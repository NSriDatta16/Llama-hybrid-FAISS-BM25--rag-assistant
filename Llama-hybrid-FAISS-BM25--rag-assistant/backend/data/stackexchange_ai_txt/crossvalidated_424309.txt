[site]: crossvalidated
[post_id]: 424309
[parent_id]: 98953
[tags]: 
For CART, you can apply the missing-in-attributes (MIA) approach. That is, for categorical predictors, you code missing as a separate category. For numerical predictors, you create two new variables for every variable with missings: one where you code missings as -Inf and one where you code missings as +Inf. Then you apply a random forest function as usual to your data. Advantages of MIA: Computationally cheap Does not yield multiple datasets and thereby models, as multiple imputation does (the imputation-of-missing-data literature generally agrees that one imputed dataset is not enough) Does not require you to choose a statistical method and/or model for imputing the data. If values are MNAR, and missingness is in fact predictive of the outcome, MIA may outperform multiple imputation Functions ctree() and cforest() from package partykit allow for applying MIA by passing ctree_control(MIA = TRUE) to their control arguments. Jerome Friedman's RuleFit program appears to use MIA for dealing with missings, see https://statweb.stanford.edu/~jhf/r-rulefit/rulefit3/RuleFit_help.html#xmiss . A description of the MIA approach can be found in Twala et al. (2008): Twala, B.E.T.H., Jones, M.C., and Hand, D.J. (2008). Good methods for coping with missing data in decision trees. Pattern Recognition Letters, 29(7), 950-956.
