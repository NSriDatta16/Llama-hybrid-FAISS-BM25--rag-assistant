[site]: crossvalidated
[post_id]: 14185
[parent_id]: 14088
[tags]: 
They are different because the lme model is forcing the variance component of id to be greater than zero. Looking at the raw anova table for all terms, we see that the mean squared error for id is less than that for the residuals. > anova(lm1 F) factor 3 0.6484 0.21614 1.3399 0.2694 id 21 3.1609 0.15052 0.9331 0.5526 Residuals 63 10.1628 0.16131 When we compute the variance components, this means that the variance due to id will be negative. My memory of expected mean squares memory is shaky, but the calculation is something like (0.15052-0.16131)/3 = -0.003597. This sounds odd but can happen. What it means is that the averages for each id are closer to each other than you would expect to each other given the amount of residual variation in the model. In contrast, using lme forces this variance to be greater than zero. > summary(lme1 This reports standard deviations, squaring to get the variance yields 9.553e-10 for the id variance and 0.1586164 for the residual variance. Now, you should know that using aov for repeated measures is only appropriate if you believe that the correlation between all pairs of repeated measures is identical; this is called compound symmetry. (Technically, sphericity is required but this is sufficient for now.) One reason to use lme over aov is that it can handle different kinds of correlation structures. In this particular data set, the estimate for this correlation is negative; this helps explain how the mean squared error for id was less than the residual squared error. A negative correlation means that if an individual's first measurement was below average, on average, their second would be above average, making the total averages for the individuals less variable than we would expect if there was a zero correlation or a positive correlation. Using lme with a random effect is equivalent to fitting a compound symmetry model where that correlation is forced to be non-negative; we can fit a model where the correlation is allowed to be negative using gls : > anova(gls1 This ANOVA table agrees with the table from the aov fit and from the lm fit. OK, so what? Well, if you believe that the variance from id and the correlation between observations should be non-negative, the lme fit is actually more appropriate than the fit using aov or lm as its estimate of the residual variance is slightly better. However, if you believe the correlation between observations could be negative, aov or lm or gls is better. You may also be interested in exploring the correlation structure further; to look at a general correlation structure, you'd do something like gls2 Here I only limit the output to the correlation structure. The values 1 to 4 represent the four levels of factor ; we see that factor 1 and factor 4 have a fairly strong negative correlation: > summary(gls2) ... Correlation Structure: General Formula: ~unclass(factor) | id Parameter estimate(s): Correlation: 1 2 3 2 0.049 3 -0.127 0.208 4 -0.400 0.146 -0.024 One way to choose between these models is with a likelihood ratio test; this shows that the random effects model and the general correlation structure model aren't statistically significantly different; when that happens the simpler model is usually preferred. > anova(lme1, gls2) Model df AIC BIC logLik Test L.Ratio p-value lme1 1 6 108.0794 122.6643 -48.03972 gls2 2 11 111.9787 138.7177 -44.98936 1 vs 2 6.100725 0.2965
