[site]: crossvalidated
[post_id]: 623416
[parent_id]: 447232
[tags]: 
We use (strictly) proper scoring rules to compare probabilistic predictions. The key requirement of propriety means that predicting the true data generating distribution will give the lowest score/loss on average. More precisely, $S : \mathcal{F} \times O \to \mathbb{R}$ is proper if $$ \mathbb{E}_{Y\sim F} \, S(F, Y) \le \mathbb{E}_{Y\sim F} \, S(G, Y) $$ for all $F, G \in \mathcal{F}$ , where $\mathcal{F}$ is some set of distributions. Due to this property, we generally want to select forecasts which achieve lower expected scores. If we have some probabilistic forecast $F_i$ and some data $y_i$ , $i=1, \ldots, n$ we want to estimate these expected scores, i.e. roughly speaking we use the sample average $ \frac{1}{n} \sum_{i=1}^{n} S(F_i, y_i) $ to estimate $\mathbb{E} S(F,Y)$ . The forecasts $F_i$ are not marginals of a joint probability distribution since most users of scoring rules do not aim to produce forecasts for this joint distribution, but only for the next time step. For instance, most weather services won't produce a joint forecast distribution $\tilde F (y_1, \ldots, y_{7})$ of the daily temperatures of a whole week. Instead, a one-dimensional forecast $F_i$ is produced each day. If you want to exactly know why this estimation is done although the $F_i$ might not be all equal and the distribution of $Y$ might also change for each $i$ , then you have to look deeper in the literature. This paper is probably the standard starting point.
