[site]: crossvalidated
[post_id]: 325008
[parent_id]: 
[tags]: 
why is XGBoost giving me seriously biased predictions with small nrounds?

I can not put my company data online, but I can provide a reproducible example here. We're modelling Insurance's frequency using Poisson distribution with exposure as offset. Here in this example, we want to model the number of claim Claims ($y_i$) with exposure Holders ($e_i$) In the traditional GLM model, we can dirrectly model $y_i$ and put $e_i$ in the offset term. This option is not available in xgboost . So the alternative is to model the rate $\frac{y_i}{e_i}$, and put $e_i$ as a the weight term ( reference ) When I do that with a lot of iteractions, the results are coherent ($\sum y_i = \sum \hat{y_i}$). But it is not the case when nrounds = 5 . I think that the equation $\sum y_i = \sum \hat{y_i}$ must be satisfied after the very first iteration. The following code is an extreme example for the sake of reproducibility. In my real case, I performed a CV on the training set (optimizing MAE), I obtained nrounds = 1200 , training MAE = testing MAE. Then I re-run a xgboost on the whole data set with 1200 iteration, I see that $\sum y_i \ne \sum \hat{y_i}$ by a large distance, this doesn't make sense, or am I missing something? So my questions are: Am I correctly specify parameters for Poisson regression with offset in xgboost ? Why such a high bias at the first iterations? Why after tuning nrounds using xgb.cv, I still have high bias? Here is the graphics plotting the ratio $\frac{\sum \hat{y_i}}{\sum y_i}$ by nrounds Code edited after the comment of @JonnyLomond library(MASS) library(caret) library(xgboost) library(dplyr) #-------- load data --------# data(Insurance) #-------- data preparation --------# #small adjustments Insurance$rate = with(Insurance, Claims/Holders) temp % predict(temp) #create xgb matrix xgbMatrix
