[site]: datascience
[post_id]: 45456
[parent_id]: 45276
[tags]: 
I've been working on this for the last few days, and I think I've answered my own question. Calculating individual word saliency is not possible with the model structure as is. This is because of the GlobalAveragePooling layer of the model. This averages the embedding matrix in the 'word' dimension, removing the ability to distinguish the effect of an individual word on the classification. This is the code I used to convince myself of what was happening, left here in the hope that the next soul to try this finds this answer. outputTensor = model.output embeddingTensor = model.layers[1].input gradientsEmbedding = tf.gradients(outputTensor, embeddingTensor) globalAverageTensor = model.layers[2].input gradientsAverage = tf.gradients(outputTensor, globalAverageTensor) idx = 1 sess = keras.backend.get_session() embedding = sess.run(embeddingTensor, feed_dict={model.input:train_data[(idx-1):idx,:]}) globalAverage = sess.run(model.layers[2].input, feed_dict={model.input:train_data[(idx-1):idx,:]}) print(np.mean(embedding, 1)) print(globalAverage) gradientMatrixEmbedding = sess.run(gradientsEmbedding, feed_dict={embeddingTensor:embedding}) gradientMatrixAverage = sess.run(gradientsAverage, feed_dict={globalAverageTensor:globalAverage}) print(np.sum(gradientMatrixEmbedding, 2)) print(gradientMatrixAverage)
