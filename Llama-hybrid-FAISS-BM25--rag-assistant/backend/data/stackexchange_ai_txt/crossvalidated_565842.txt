[site]: crossvalidated
[post_id]: 565842
[parent_id]: 
[tags]: 
Probability distribution of model accuracy over different training instances

Typically in machine learning we simply run evaluation over the test set and report the test metrics. This makes sense for assessing the trained model. However, I feel that this may not be suitable for assessing an architecture, as training the same architecture with the same objectives, but under different shuffling of data, often produces differences in test score on the order of a percent. (correct me if I'm wrong, I'm still not very experienced) So what I'm curious about is whether there are known good ways to model the test scores, e.g. accuracy, as a random variable under a probability distribution, or perhaps even compute something like a standard error of the test scores, so that we can evaluate the architecture and training scheme itself rather than a specific instance of the trained model?
