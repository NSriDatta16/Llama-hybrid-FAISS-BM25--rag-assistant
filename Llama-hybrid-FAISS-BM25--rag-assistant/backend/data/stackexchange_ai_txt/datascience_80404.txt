[site]: datascience
[post_id]: 80404
[parent_id]: 80389
[tags]: 
One approach that I've found helpful when considering autoencoders is the following result: whereas methods such as PCA identify axes of maximal variation in the input space, the introduction of non-linear activation functions in the autoencoder allows for the identification of axes of maximal variation embedded in a (potentially) non-linear transform of the space. As an example, consider data in distributed according to the function , where . Here, the goal is to store inputs as one-dimensional compressions. A PCA approach could possibly introduce significant loss (as long as the support is sufficiently large), but an autoencoder with non-linearities will be able to identify the principal embedded axis in the transform space as the one with pre-image roughly at in the input space, and therefore will introduce much less loss. You can think of the autoencoder training regime as working to approximate a transform functor which produces a transform space with a linear pre-image at . The autoencoder then works by storing inputs in terms of where they lie on the linear image of . Observe that absent the non-linear activation functions, an autoencoder essentially becomes equivalent to PCA â€” up to a change in basis. A useful exercise might be to consider why this is.
