[site]: crossvalidated
[post_id]: 616371
[parent_id]: 
[tags]: 
Compensating for anomalies in training data by diluting them with older data?

I'm having trouble finding references to (and the name of) an obvious correction procedure that surely must have been used extensively in time series forecasting, but my Google searches keep turning up references to anomaly detection (which isn't what I'm looking for). In (univariate) time series forecasting, we're always using past data to predict future data. But suppose there is a brief interval of data that is known to be somewhat inaccurate or different from what it would be normally. For example, perhaps a sensor was miscalibrated for a couple of days and it's been repaired or a temporary change in corporate policy that was later reversed affected the data collected while the policy was in effect (or COVID caused a temporary inflection, a street outside the store was under construction (which decreased customer traffic), an older device filled in for a newer one during maintenance window, etc.). If we use this period of anomalous data, as is, to predict future data as we normally would, it will derail the forecast somewhat because it will propagate these anomalies forward. We expect the target forecast interval to be more similar to the distant past than to the recent past. But we also don't want to discard the recent data altogether. It still contains some useful predictive information. We just want to "dilute" the recent, anomalous data with a corresponding interval of more typical past data. I'd like to see how other people have been doing this. How to choose the ratio, whether to keep the ratio constant within an interval or ease in and out of it with a Gaussian, whether to trigger the dilution manually or automatically (e.g., when the forecast error exceeds a threshold), etc. I think I'm just using the wrong search terms. Anybody have any references to recommend?
