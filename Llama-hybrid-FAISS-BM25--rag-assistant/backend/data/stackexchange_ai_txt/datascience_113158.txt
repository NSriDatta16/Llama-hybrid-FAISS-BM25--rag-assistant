[site]: datascience
[post_id]: 113158
[parent_id]: 113138
[tags]: 
It's because during the training, it is not only the neural network with tanh that learns but also the words' representation in Matrix C. There are 2 different parts in this model: The classic neural network can make correlations between words sequentially. The words' representation can make correlations between words in general. Without words' representation, the model couldn't make unknown correlations that haven't been learned previously. They've explained it in the paper with the words "cat" and "dog". This is obtained progressively by building a map of probability between words. For instance: The sentences "The cat is walking in the room." and "The dog is running in the bedroom." would increase the neighbor vectors (ex: "walking", "running", "room", "bedroom") in Matrix C of both "cat" and "dog", thanks to the phrases' similarity. In other words, if we increase the vector of "cat"/"walking", it would also increase the vectors of "cat"/"running" and "dog"/"walking" in a similar context, thanks to an embedding lookup. We could define the words' representation as a general probability space of all words, which is slightly modified at each learning iteration. Explanation from Yoshua B (great thanks): The reason why similar words end up having similar word embeddings is because of the smoothness of the neural net that takes these word embeddings in input. If "The cat is walking in the --- " can be completed by "room", it is also true when we replace "cat" by "dog", which puts pressure on both words to have similar word embeddings. Small change of the embeddings = small change in the output probabilities. If you think about it, the architecture is very similar to a 1-D convolutional neural network: the matrix C corresponds to a usual dot-product neural operation when the input is a one-hot vector for the word at each position (with a 1 at the position corresponding to the word symbol) using the same matrix C at every position makes sense and proceeds of the same inductive bias as in 1-D convolutional neural networks: the meaning of a word is position invariant (i.e. if we only know that a word appeared at position 3 vs 4, the meaning does not change). The idea of using such layers, with shared weights across different positions, is found not just in 1-D convnets and time-delay neural net (which pre-existed the NNLM, and which I worked on in my 1991 PhD thesis), but also in neural nets operating on symbols (which were explored among others by Geoff Hinton and his student Paccanaro a few years earlier, cited in the paper).
