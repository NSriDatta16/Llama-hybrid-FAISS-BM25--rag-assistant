[site]: crossvalidated
[post_id]: 218333
[parent_id]: 215513
[tags]: 
Keep in mind why people typically scale features prior to estimating an SVM. The notion is that the data are on different scales, and this happenstance of how things were measured might not be desirable -- for example, measuring some length quantity in meters versus kilometers. Obviously one will have a much larger range even though both represent the same physical quantity. However, there's no reason that the new scaling must be better. While it's true that the rescaled features will all vary in comparable units, it's also possible that the original scaling happened to encode the data such that some important features had more prominence in the model. Consider the example of two different versions of the Gauissian RBF kernel: $K_1(x,x^\prime)=\exp(-\gamma||x-x^\prime||^2_2).$ This is an isotropic kernel, meaning that the same scaling ( $\gamma$ ) is applied in all directions. A more general kernel function might have the form $K_2(x,x^\prime)=\exp\big(-(x-x^\prime)\Gamma(x-x^\prime)\big);$ it is anisotropic as $\Gamma$ is a diagonal PSD matrix, with each element applying a different scaling to each direction. The advantage of this kernel function is that it will vary more strongly in some directions than others. Coming back to your question, it's possible to imagine that your data have, for whatever reason, some features that are more important than others, and that this coincides with the scale on which they are measured. Placing them on the new scale where they all appear on similar scales and are all treated as equally important means that unimportant or noise features cloud the signal. As an aside, don't use accuracy as a metric for comparing models: Why is accuracy not the best measure for assessing classification models? " The Case Against Accuracy Estimation for Comparing Induction Algorithms ", Foster Provost, Tom Fawcett, Ron Kohavi
