[site]: crossvalidated
[post_id]: 517436
[parent_id]: 515553
[tags]: 
Here is an attempt at an answer. This might take a little iteration to make it match your situation directly. how to treat all experiments with the same weight as curent method biases to experiment with more observations (experiments with smaller number observations have more errors)? As you will see in my simulation, I'm not sure this is really a problem yet. But if, in your data, it is a problem, then here may be a way to mitigate this: fit the parameters on each experiment, then average the experiment-wise fits. I don't recommend this yet though. how to deal with the time serial data as the current method biases to bigger values (See Exp7 below)? This might be an artifact of "capping" the observed data as you will see in simulation. In other words, if no plants have more than 7 leaves, the model may need to treat this differently, or change the model form (e.g. 4 parameter logistic) Simulation in R: require(dplyr) require(magrittr) require(ggplot2) # simulate data # number of experiments Ne % dplyr::group_by(exp) %>% dplyr::summarize(merr = mean(y - pred), mabserr = mean(abs(y - pred)), msqerr = mean((y - pred)^2), n = length(y)) datexp ggplot(datexp, aes(x = n, y = msqerr)) + geom_point() summary(lm(msqerr ~ n, datexp)) # does the fit bias to larger values? In other words, do larger observed values have smaller error? # This shouldn't happen, but in my simulation, it does, because I capped the upper and lower ends at 7 and 1 datlarge % dplyr::mutate(err = y - pred, sqerr = (y - pred)^2) datlarge ggplot(datlarge, aes(x = y, y = sqerr)) + geom_point() summary(lm(sqerr ~ y, datlarge)) ```
