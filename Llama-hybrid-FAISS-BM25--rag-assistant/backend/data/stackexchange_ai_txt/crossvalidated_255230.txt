[site]: crossvalidated
[post_id]: 255230
[parent_id]: 
[tags]: 
How do ensemble methods outperform all their constituents?

I am a bit confused about ensemble learning. In a nutshell, it runs k models and gets the average of these k models. How can it be guaranteed that the average of the k models would be better than any of the models by themselves? I do understand that the bias is "spread out" or "averaged". However, what if there are two models in the ensemble (i.e. k = 2) and one of the is worse than the other - wouldn't the ensemble be worse than the better model?
