[site]: crossvalidated
[post_id]: 132545
[parent_id]: 
[tags]: 
maximum a posteriori vs squared loss

I am unclear about max a posteriori and squared loss. Let me assume I have $N$ images and $\mathbf{y}_i$ is the label of the image $i$, where, $\mathbf{y}_i\in \mathbb{R}^{C\times 1}$ - a binary vector of lenght $C$ (number of classes). $\mathbf{y}_{ic} = 1$ when the image $i$ belonging to class $c$. Consider the following two loss functions. \begin{align} L_1 &= -\sum_i\sum_{c=1}^C y_{ic}\, \log(P(y_{ic}|\mathcal{D})) \\ L_2 &= \sum_i \sum_{c=1}^C \left(y_{ic}-P(y_{ic}|\mathcal{D})\right)^2 \end{align} What is the difference between these functions? When should I use $L_1$ and when should I use $L_2$? I observed that in neural networks always $L_2$ is used. Why?
