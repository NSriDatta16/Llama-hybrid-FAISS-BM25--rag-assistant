[site]: crossvalidated
[post_id]: 582279
[parent_id]: 
[tags]: 
How to compare -50% and +100%

This might seem (and be) a really trivial question! I have this table: id until_2000 after_2000 1 1 1 1 2 2 1 2 3 3 2 1 Now I want to compare the relative change between the columns after_2000 and until_2000 . Just dividing the after_2000 values by the until_2000 (and substract 1?!) values yields the colum rel_change id until_2000 after_2000 rel_change 1 1 1 1 0.0 2 2 1 2 1.0 3 3 2 1 -0.5 While the first row stays as is (has a change of 0%), the second row doubles (has a relative change of 100 %). Yet the third row is half the value and thus has a relative value of -50 %. Now my trivial question is how to correctly compute and average these relative changes. In my line of thinking the values of 0, doubling, and "halfing" should result in an average change of 0. Yet adding 0 + 1 - 0.5 and dividing it by three yields a value of 0.16666. What am I conceptually understanding wrong here?
