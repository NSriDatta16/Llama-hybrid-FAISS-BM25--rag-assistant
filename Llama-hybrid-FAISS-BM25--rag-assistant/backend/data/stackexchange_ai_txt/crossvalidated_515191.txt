[site]: crossvalidated
[post_id]: 515191
[parent_id]: 49528
[tags]: 
Imagine you are going down the hill to a valley of minimum height. You may use batch gradient descent to calculate the direction to the valley once and just go there. But on that direction you may have an up hill. It's better to avoid it, and this is what stochastic gradient descent idea is about. Sometimes is better to take small steps . Just for the terminology direction batch gradient descent is not what you may intuitively think. This refers to the complete dataset pass per one iteration and in that context the entire dataset is called batch where we compute the average of the true gradient. Stochastic gradient descent is actually what we know as a multiple batches per iteration gradient descent update. This is why you may heard the word minibatch for the second.
