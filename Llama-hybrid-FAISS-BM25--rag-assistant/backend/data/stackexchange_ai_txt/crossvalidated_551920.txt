[site]: crossvalidated
[post_id]: 551920
[parent_id]: 
[tags]: 
When do I need something "fancier" than multiple regression?

Let me describe my data, without getting TOO specific. I have ~1000 words. They have all been rated on some dimension (i.e., "DV"). There are 30 properties that can either be present or absent in the words (i.e., "X1":"X30"). I want to know if any properties are more common in words that are higher (or lower) on the DV. I've taken a few swings at answering this. I have run an adaptive LASSO. I've also run a random forests model with boruta feature selection. I was told that these two approaches may be good for prediction but not so good for explanation . My main objective isn't to predict new data, but to understand which of my 30 properties are related to the DV. So I just ran a plain old multiple regression. The residuals are all distributed nicely. The VIF for each predictor is also So, is this enough? For some reason, I've gotten it in my head that it isn't, but now that I think about it, I can't explain why. I think my worry was that it is too "liberal" and that with a large enough sample some predictors are bound to be significant. So my questions are: Is there any reason that this multiple regression isn't enough? Are there ways to "augment" it if not? I've thought about stepwise regression, but in all honesty I'm not sure why I would need that if there is no multicolinearity. Edit: I will also add the detail that 69/703 pairwise correlations among my predictors are significant at p
