[site]: datascience
[post_id]: 45897
[parent_id]: 
[tags]: 
Neural Network Data Normalization Setup

I am fairly new to Neural Networks and have some questions regarding data normalization. I am trying to build a regression neural network with two neurons on the output layer using a 'mean_squared_error' loss function. The problem I am running into is that one of the outputs is on a small scale (values within 0.9 to 1.4) and the other output is on a much larger scale (values within 60 to 80). My understanding is that multiple output neurons use the following formula to calculate MSE MSE = (|op1 - targ1|^2 + |op2 - targ2|^2 ) / 2 Given that the scales are off by an order of magnitude I believe I have to normalize the target columns of the training data-set before applying the neural network to make sure they are both weighted equally and scales don't influence the outcome. I also read that ANN perform better when input data is normalized, so I went ahead and normalized all the data-set by subtracting the mean and dividing by the standard deviation (z-score normalization). Here is the code I used to build the sequential model after the input normalization: def build_model(n_layers, input_dim, units, activation):#, initializer): if isinstance(units, list): assert len(units) == n_layers else: units = [units] * n_layers model = Sequential() # Adds first hidden layer with input_dim parameter model.add(Dense(units=units[0], input_dim=input_dim, activation=activation, kernel_regularizer=l2(0), #kernel_initializer=initializer, name='h1')) model.add(BatchNormalization()) # Adds remaining hidden layers for i in range(2, n_layers + 1): model.add(Dense(units=units[i-1], activation=activation, kernel_regularizer=l2(0), #kernel_initializer=initializer, name='h{}'.format(i))) model.add(BatchNormalization()) # Adds output layer model.add(Dense(units=2,name='o'))#,kernel_initializer=initializer)) # Compiles the model optimizer = Adam(lr= 0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.05, amsgrad=False) model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=[coeff_determination]) return model And here is how I compiled the model (10 layers, 50 hidden units per layer): model = build_model(n_layers=10,input_dim=len(model_inputs),units=50,activation='relu') I finally fitted my model using: history = model.fit(x=X_norm, y=Y_norm, epochs=225, batch_size = X_norm.shape[0]) And got this output for the last epoch: Epoch 225/225 162/162 [=======================] - 0s 86us/step - loss: 1.2539e-04 I went ahead and ran a prediction on the training data-set as a sanity check using: Y_new = model.predict(X_new) and finally I "de-normalized" the prediction using the mean and standard deviation used to normalized the target columns in the first place. Here is how the actual vs prediction stacked up for a few observations: Prediction: target_1 target_2 0 82.485733 1.370714 1 80.422562 1.260928 4 81.192978 1.216307 5 67.609528 1.191042 6 71.330009 1.012126 7 75.563698 1.161521 8 80.212601 1.341668 9 76.349266 0.962544 10 81.197662 1.307808 12 79.872147 1.306849 13 82.837700 1.313502 14 73.760750 1.687881 16 75.290382 1.368603 18 70.717682 1.229838 19 76.409767 1.307446 20 85.307167 1.342816 21 70.818542 1.242142 22 78.467438 1.382735 23 75.320892 1.238882 25 89.502998 1.446245 Actuals: target_1 target_2 0 86.2700 1.243 1 81.2960 1.330 4 75.2020 1.309 5 73.8390 1.342 6 73.1020 1.296 7 79.8180 0.795 8 77.3300 1.612 9 78.4010 1.074 10 65.6410 1.457 12 83.9160 1.449 13 69.1829 1.166 14 88.9450 1.056 16 77.7440 1.392 18 71.8940 1.169 19 86.0040 1.306 20 79.8420 1.215 21 74.3650 0.977 22 66.3840 1.464 23 84.3200 1.295 25 75.8870 1.808 Questions: I feel that for such a low cost value the prediction and actual values should be a lot closer, especially for the training data-set, but maybe I am thinking about this the wrong way. Is there any downside to normalizing the inputs and target columns ? Am I carrying out the normalization correctly ? Am I re-converting the outputs of the model correctly by using the mean and standard deviation used to normalize them in the first place ? Is my setup valid for two neuron regression NN ?
