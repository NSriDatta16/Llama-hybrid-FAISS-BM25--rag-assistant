[site]: crossvalidated
[post_id]: 230426
[parent_id]: 230193
[tags]: 
More hidden layers shouldn't prevent convergence, although it becomes more challenging to get a learning rate that updates all layer weights efficiently. However, if you are using full-batch update, you should be able to determine a learning rate low enough to make your neural network progress or always decrease the objective function. Assuming that you are using full-batch update, at a given iteration, in order to guarantee sufficient objective function decrease without manually specifying a learning rate, you can perform line search to find a learning rate that satisfies the two Wolfe conditions . You can also use L-BFGS with line search to optimise your neural network efficiently. L-BFGS uses an approximation of the Hessian (second order gradient) which in a way sets a learning rate for every parameter. minFunc for Matlab and scipy.optimise for python have L-BFGS. However, before going further I would do the following two things: 1) I would check that the implementation is correct by checking that the gradient function is correct. This link 1 shows a method for checking that the gradient function is correct. First, numerically approximate the gradient for a parameter using the function value, then compare it with the value returned by the gradient function. The two values should be approximately the same. 2) I would also compare the neural network (NN) results with easy-to-use NN code available online. The autograd python library shows a quick implementation of multi-layer perceptron and its performance on a toy example. You can set the learning rate and the number of hidden layers fairly easy. Hope this helps!
