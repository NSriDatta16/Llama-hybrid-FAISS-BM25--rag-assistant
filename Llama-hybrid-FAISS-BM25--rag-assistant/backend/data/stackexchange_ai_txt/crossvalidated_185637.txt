[site]: crossvalidated
[post_id]: 185637
[parent_id]: 185624
[tags]: 
Well, in the second article there is a sentence: Note that before conducting linear regression, you should normalize the data. One way is $\frac{x_iâˆ’mean(x)}{Range(x)}$, and some use $sd(x)$ as the denominator. Both work. But is not said that it applies specifically to normal equations. And in gradient descent section there is nothing said about normalization. So I suppose it was a small mistake to include that sentence in Normal Equation section instead of Gradient Descent. Anyway Andrew Ng is pretty authoritative on machine learning topic, so you can rely on his words: Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no 'loop until convergence' like in gradient descent.
