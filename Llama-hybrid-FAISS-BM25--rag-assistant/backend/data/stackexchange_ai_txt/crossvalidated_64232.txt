[site]: crossvalidated
[post_id]: 64232
[parent_id]: 41704
[tags]: 
There are two separate issues: a) learning the right function eg k-means: the input scale basically specifies the similarity, so the clusters found depend on the scaling. regularisation - eg l2 weights regularisation - you assume each weight should be "equally small"- if your data are not scaled "appropriately" this will not be the case b) optimisation , namely by gradient descent ( eg most neural networks). For gradient descent, you need to choose the learning rate...but a good learning rate ( at least on 1st hidden layer) depends on the input scaling : small [relevant] inputs will typically require larger weights, so you would like larger learning rate for those weight ( to get there faster), and v.v for large inputs... since you only want to use a single learning rate, you rescale your inputs. ( and whitening ie decorellating is also important for the same reason)
