[site]: crossvalidated
[post_id]: 131000
[parent_id]: 130985
[tags]: 
Here's my simple explanation. When we model reality we want our models to be able not only to explain existing facts but also predict the new facts. So, the out-of-sample testing is to emulate this objective. We estimate (train) the model on some data (training set), then try to predict outside the training set and compare the predictions with the holdout sample. Obviously, this is only an exercize in prediction, not the real prediction, because the holdout sample was in fact already observed. The real test in prediction happens only when you use the model on the data, which was not observed yet. For instance, you developed machine learning program for advertising. Only when you start using it in practice, and observe its performance you'll know for sure if it works or not. However, despite the limitation of training/holdout approach, it's still informative. If your model only works in-sample, it's probably not a good model at all. So, this kind of testing helps weed out bad models. Another thing to remember: let's say you conducted training/holdout sample validation of the model. However, when you want to use the model you probably will estimate the model on the entire dataset. In this case how applicable are the results of the out-of-sample validation of the model which was estimated on the training sample?
