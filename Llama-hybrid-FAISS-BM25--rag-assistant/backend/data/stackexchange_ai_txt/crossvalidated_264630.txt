[site]: crossvalidated
[post_id]: 264630
[parent_id]: 
[tags]: 
Bayesian Ridge and Bayesian Lasso for GLMs

I had a question about Bayesian ridge and Lasso regression in the context of GLMs (and perhaps Cox models). A lot of the resources I found online talk about Bayesian ridge and Lasso regression for OLS regression but have not found much on GLMs, unless I'm doing a poor job at Google searching. I have heard that the MAP estimates from using a Gaussian prior is the same as ridge regression, and likewise with the Laplace prior and Lasso regression. My question was with the tuning parameter. In a frequentist point of view, we aim to minimize the following penalized log-likelihood: $$\hat{\beta}_{pen} = \arg \min_{\beta} -l(\beta) + \lambda \sum_{j=1}^p p(\beta_j)$$. For ridge regression, $p(\beta_j) = \beta_j^2$ and for Lasso, $p(\beta_j) = |\beta_j|$. From my understanding, and please correct me if I'm wrong, the Ridge estimates obtained by minimizing the above function for a fixed $\lambda$ would be similar (exact?) to the MAP estimates from a $N(0, 1/\lambda)$ prior. Is this always true? And if so, would the same hold for a $Laplace(0, 1/\lambda)$ and the Lasso estimates? The work I've seen for OLS regression deals with conditioning on $\sigma^2$, but for GLM's this is not the case. I would appreciate any constructive feedback and any (and all) references that can point me toward the right direction!
