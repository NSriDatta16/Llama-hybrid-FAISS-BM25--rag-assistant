[site]: datascience
[post_id]: 40968
[parent_id]: 
[tags]: 
LSTM/RNN seems to be failing at testing

I'm relatively new to ML, keras and tensorflow and I working with a dataset (kerastest.csv) that is 400 lines of this sequence,target 0.54701155 0.517513 0.4690355 0.48752165 0.46766028 0.5972366 0.61058664 0.7067254 0.7121893 0.7661711 0.77721995 0.7518825 0.68164456 0.5985321,1 0.551435 0.5153954 0.4721298 0.490865 0.47131577 0.60469216 0.6038976 0.7038085 0.7110235 0.77294403 0.7816415 0.7592487 0.67063993 0.6062528,1 0.55585843 0.5132778 0.4752241 0.49420834 0.47497123 0.6121478 0.5972085 0.70089155 0.70985764 0.7797169 0.78606296 0.7666148 0.65963525 0.61397344,1 0.5483472 0.51139927 0.4748516 0.48613134 0.47298145 0.6120353 0.5916382 0.6981726 0.69980276 0.7783837 0.78754336 0.7675861 0.6592715 0.6162258,0 0.5351824 0.5096339 0.47283685 0.47264448 0.46831745 0.6083377 0.58659786 0.6955476 0.6855371 0.77321047 0.7876306 0.7655281 0.66394836 0.61588776,0 0.52201766 0.50786847 0.47082213 0.45915762 0.46365345 0.6046401 0.5815576 0.69292253 0.67127144 0.76803726 0.7877178 0.7634701 0.6686252 0.61554974,0 0.56658393 0.49670732 0.4596451 0.49459377 0.507614 0.5795203 0.6221169 0.7199565 0.6908693 0.75352824 0.74981964 0.7737604 0.70846057 0.5648611,1 0.5691148 0.49902853 0.45811677 0.4862831 0.507614 0.5830465 0.630378 0.72070897 0.69663626 0.7547129 0.75281024 0.7726751 0.71151966 0.56712264,1 My code looks like this from keras.layers import Dense, Dropout, LSTM, Embedding from keras.preprocessing.sequence import pad_sequences from keras.models import Sequential import pandas as pd import numpy as np input_file = 'kerastest.csv' def load_data(test_split = 0.2): print ('Loading data...') df = pd.read_csv(input_file) df['sequence'] = df['sequence'].apply(lambda x: [float(e) for e in x.split()]) df = df.reindex(np.random.permutation(df.index)) train_size = int(len(df) * (1 - test_split)) X_train = df['sequence'].values[:train_size] y_train = np.array(df['target'].values[:train_size]) X_test = np.array(df['sequence'].values[train_size:]) y_test = np.array(df['target'].values[train_size:]) return pad_sequences(X_train), y_train, pad_sequences(X_test), y_test def create_model(input_length): print ('Creating model...') model = Sequential() model.add(Embedding(input_dim = 188, output_dim = 50, input_length = input_length)) model.add(LSTM(activation="sigmoid", return_sequences=True, units=256, recurrent_activation="hard_sigmoid")) model.add(Dropout(0.5)) model.add(LSTM(activation="sigmoid", units=256, recurrent_activation="hard_sigmoid")) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) print ('Compiling...') model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) return model X_train, y_train, X_test, y_test = load_data() model = create_model(len(X_train[0])) print ('Fitting model...') hist = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split = 0.1, verbose = 1) score, acc = model.evaluate(X_test, y_test, batch_size=1) print('Test score:', score) print('Test accuracy:', acc) My output (10 epochs though increasing it does't change anything) looks like Loading data... Creating model... Compiling... Fitting model... Train on 288 samples, validate on 32 samples Epoch 1/10 288/288 [==============================] - 2s 6ms/step - loss: 0.7022 - acc: 0.7396 - val_loss: 1.4764 - val_acc: 0.2812 Epoch 2/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5812 - acc: 0.7569 - val_loss: 1.0761 - val_acc: 0.2812 Epoch 3/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5750 - acc: 0.7500 - val_loss: 1.3237 - val_acc: 0.2812 Epoch 4/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5648 - acc: 0.7604 - val_loss: 1.2321 - val_acc: 0.2812 Epoch 5/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5900 - acc: 0.7500 - val_loss: 1.1174 - val_acc: 0.2812 Epoch 6/10 288/288 [==============================] - 0s 1ms/step - loss: 0.6045 - acc: 0.7465 - val_loss: 1.3299 - val_acc: 0.2812 Epoch 7/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5820 - acc: 0.7604 - val_loss: 1.3163 - val_acc: 0.2812 Epoch 8/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5778 - acc: 0.7604 - val_loss: 1.1562 - val_acc: 0.2812 Epoch 9/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5938 - acc: 0.7604 - val_loss: 0.8883 - val_acc: 0.2812 Epoch 10/10 288/288 [==============================] - 0s 1ms/step - loss: 0.5779 - acc: 0.7535 - val_loss: 1.4051 - val_acc: 0.2812 80/80 [==============================] - 0s 3ms/step Test score: 1.8908673524856567 Test accuracy: 0.0 So my question is, have I royally screwed up somewhere in configuring the network because I presume a 50% testing accuracy is just about random, but 0% testing accuracy means I must have screwed something up somewhere. Any suggestions?
