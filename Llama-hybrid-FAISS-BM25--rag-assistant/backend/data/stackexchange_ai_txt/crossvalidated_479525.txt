[site]: crossvalidated
[post_id]: 479525
[parent_id]: 478056
[tags]: 
I think because everyone is observed the same number of times, you could probably just get away with estimating the open rate for everyone and then doing something like a t-test. I've simulated this approach (see below) assuming each person has a random effect as well (I think this is a fairly valid assumption, everyone has different email habits), and it seems like the results are approximately unbiased and have the correct false positive rate when the null is true. The question then becomes about power, which can also be simulated. library(tidyverse) #Set up the data num_members $y n $p_est = members$ y/members$n p = t.test(p_est ~ group, data = members) $p.value mean_a = mean(filter(members, group=='A')$ p_est) mean_b = mean(filter(members, group=='B')$p_est) tibble(A = mean_a, B = mean_b, p = p) }) p = map_dfr(results, ~.x) %>% pull(p) mean(p >>0.045 (depending on random seed). kjetil mentions possible effects of the day. That is certainly plausible, and so what you can do is do a logistic regression in order to adjust for days. Here is another example of how you might do that, this time using glmer since the same people are observed multiple times. library(tidyverse) library(lme4) #Set up the data num_members % crossing(days = factor(c('Fri','Sat','Sun','Mon'))) #Construct random and fixed effects. X The approach will depend on what you're interested in adjusting for and what you think is appropriate.
