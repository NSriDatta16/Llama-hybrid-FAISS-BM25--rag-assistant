[site]: crossvalidated
[post_id]: 559058
[parent_id]: 559042
[tags]: 
What happens here is that you compare one model that you know to be the true one with fixed true parameters to another model that will estimate these parameters. Obviously if you fix the parameters at the true values, you will normally be better on new data than if you estimate the parameters. But with a certain probability, any model selection method will select the model with estimated parameters, so will appear to be overfitting. This is not a specific problem with LOO-CV but applies to any model selection approach. If you compare a simpler model that you know to be true with a more complex model, the more complex model can be selected with a probability that is not negligible, in which case overfitting takes place. If you set up the problem so that underfitting cannot happen, you will overfit with a certain probability and underfit with probability zero, so on average overfitting will happen. Only if you also look at what happens in case the more complex model is true you get a fair assessment of the model selection procedure. Note furthermore that LOO-CV is known to be asymptotically equivalent to the AIC, which even asymptotically will overfit, see https://www.jstor.org/stable/2984877 (Note that I haven't made an effort to check whether potential assumptions in that paper apply here.) The AIC (and equivalently LOO-CV) is however asymptotically good at finding an optimal prediction rule (for which overfitting is not as bad as underfitting). Note that in the given setup, if you just try to predict the target 1 or 0, rather than predicting their probabilities, any rule will be wrong 50% of the time, so that the "overfitted" model will not be worse than the true one when it comes to point prediction.
