[site]: crossvalidated
[post_id]: 28765
[parent_id]: 18732
[tags]: 
One solution would be to use cross-validation methods. This might be a conceptually easy (and elegant) solution because the model you have differs significantly from the model to be compared to. AIC or BIC won't really work here because the functional forms of these two models are very different -- yours is nonlinear and their model is not only linear but also based on binned data. AIC or BIC is insensitive to functional forms. I wouldn't worry about binning vs non-binning too much, since it seems to me that binning is a modeling decision that could make a model better or worse. In other words, it's a feature whose effectiveness should be tested. Now, assuming you can implement the other model, you can perform a k-fold cross-validation: Divide your data into k subsets; Iteratively leave one subset out, and train your model (without binning) and the other model (with binning) on the rest of the subsets; Compute the sum of loglikelihoods of the subset that was left out in the previous with regard to your model and the other model. This should be relatively straight-forward: in your nonlinear model, error is binomially distributed; in the other model, error is normally distribution since it's a simple linear regression; Repeat 2 and 3 until you have used each of the k subsets as the test subset (thus the name k-fold). You can then compare which model gives you the better loglikelihood (i.e. the less negative one).
