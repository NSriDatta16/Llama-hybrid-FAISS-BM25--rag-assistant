[site]: crossvalidated
[post_id]: 201540
[parent_id]: 201452
[tags]: 
As mentioned by pkubik, usually there's a regularization term for the parameters that doesn't depend on the input, for instance in tensorflow it's like # Loss function using L2 Regularization regularizer = tf.nn.l2_loss(weights) loss = tf.reduce_mean(loss + beta * regularizer) In this case averaging over the mini-batch helps keeping a fixed ratio between the cross_entropy loss and the regularizer loss while the batch size gets changed. Moreover the learning rate is also sensitive to the magnitude of the loss (gradient), so in order to normalize the result of different batch sizes, taking the average seems a better option. Update This paper by Facebook (Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour) shows that, actually scaling the learning rate according to the batch size works quite well: Linear Scaling Rule: When the minibatch size is multiplied by k, multiply the learning rate by k. which is essentially the same as to multiply the gradient by k and keep the learning rate unchanged, so I guess taking the average is not necessary.
