[site]: crossvalidated
[post_id]: 559033
[parent_id]: 
[tags]: 
Sliding dataset using window VS Using strides in CNN

Reading a paper about Conv-TasNet , model part, I misunderstood about the configuration of the encoder model, which is a simple CNN, tf.keras.layers.Conv1D() . For 8kHz 1-second segment, i.e., (8000,) data, It uses an encoder of 512 filters, length of 16, overlap size of 8. This can be easily implemented using kernel_size=16, strides=8 as arguments of the above layer. But I misunderstood by myself, making it as a point-wise convolution(which means kernel_size=1 ) and sliced each input into length of 16 and shifted by 8. (This required some works using tf.data.Dataset apis including window(), flat_map(), zip() and so on, but I could make the dataset into the form I wanted.) So my question is, what is the difference between these two methods? Both methods see the 16 lengths of the input, shift for 8 lengths. And both yields the same result. Let me give you an example to clarify. Suppose we set BATCH_SIZE=32 , and method 1 follows import numpy as np import tensorflow as tf import tensorflow.keras as keras x = tf.reshape(tf.range(8000*32, dtype=tf.float32), shape=(32, 8000, 1)) original_encoder = keras.layers.Conv1D(512, kernel_size=16, strides=8, use_bias=False) result1 = original_encoder(x) result1.shape # (32, 999, 512) And for method 2, splited_x = np.array_split(x, 32) splited_x = np.asarray(splited_x) splited_x = np.squeeze(splited_x) splited_x.shape window_length = 16 shift_length = 8 datasets_x = [] for split in splited_x: ds_x = tf.data.Dataset.from_tensor_slices(split) ds_x = ds_x.window(window_length, shift=shift_length, drop_remainder=True) ds_x = ds_x.flat_map(lambda window: window.batch(window_length)) datasets_x.append(ds_x) x_ds = tf.data.Dataset.zip(tuple(datasets_x)).map(lambda *windows: tf.stack(windows)) pointwise_conv = keras.layers.Conv1D(512, kernel_size=1, use_bias=False) result2 = tf.zeros((32, 1, 512)) for i in x_ds: i = tf.expand_dims(i, 1) res = pointwise_conv(i) result2 = tf.concat([result2, res], axis=1) result2 = result2[:, 1:, ] # discard the first zeros result2.shape # (32, 999, 512) I know this looks very ugly, but still both methods watch the same things, yield the same things. Well, checking the trainable_variables of each layer, original_encoder layer has (16, 1, 512) variables and pointwise_conv has (1, 16, 512) variables. Did I just do the same job?
