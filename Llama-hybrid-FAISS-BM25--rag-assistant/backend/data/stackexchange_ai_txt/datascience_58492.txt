[site]: datascience
[post_id]: 58492
[parent_id]: 58487
[tags]: 
Adding more and more layer indeed make the network learn better and better on the training set. However, this causes a problem called "overfit". Overfitting means that your model works extremely well on training set but works poorly on validation set or testing set. For your last question, the performance of Neural Network models is measured by the ability of the model to predict correctly for unseen data (or future data), which is the accuracy of predicting testing data. In practice, when you make your model deeper, the model will fit more completely to the training data. In this case, you increase the chance of getting an overfitted model. Therefore, the performance of the model (which is the accuracy of predicting testing data) will decrease due to overfitting into training data. Reference: Overfitting
