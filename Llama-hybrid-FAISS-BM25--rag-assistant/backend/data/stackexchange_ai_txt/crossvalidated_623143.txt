[site]: crossvalidated
[post_id]: 623143
[parent_id]: 623139
[tags]: 
This is a response to this comment : As far as I understand, you can't do any dimensionality reduction if all the variables are independent. You can perform dimensionality reduction on independent variables. Below is an example with a random data matrix $\mathbf{X}_{1000\times10}$ where we used $X_{i,j} \sim \mathcal{N}(0,1)$ as IID random variables . Then we reduce the data down to 2 dimensions using PCA. import numpy as np from sklearn.decomposition import PCA # Make some fake data np.random.seed(2018) X = np.random.normal(0,1,size=1000*10).reshape(1000,10) # PCA pca = PCA(n_components=2) Q = pca.fit_transform(X) You'll find that PCA really has reduced those 10 variables down to 2 variables, falsifying the claim/intuition that this cannot be done.
