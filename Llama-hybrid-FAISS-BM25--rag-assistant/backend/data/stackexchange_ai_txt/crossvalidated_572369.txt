[site]: crossvalidated
[post_id]: 572369
[parent_id]: 
[tags]: 
Normalizing Flows KL divergence equivalency

This question is related to the normalizing flows concept in machine learning. Let $X \sim P_X$ and $U \sim P_U$ be, respectively, the distribution of the data and a base distribution (e.g. an isotropic gaussian). We define a normalizing flow as $F: \mathcal{U} \rightarrow \mathcal{X}$ parametrized by $\theta$ . Starting with $P_U$ and then applying $F$ will induce a new distribution $P_{F(U)}$ (used to match $P_X$ ). Since normalizing flows are invertible, we can also consider the distribution $P_{F^{-1}(X)}$ . How comes that in this case $D_{KL}[P_X || P_{F(U)}] = D_{KL}[P_{F^{-1}(X)} || P_U]$ ? $D_{KL}$ being the Kullbackâ€“Leibler divergence . Let's say there are 2 scenarios: you don't have samples from $p_X(x)$ , but you can evaluate $p_X(x)$ , you have samples from $p_X(x)$ , but you cannot evaluate $p_X(x)$ . Which divergence should be used in each scenario as the objective to optimize?
