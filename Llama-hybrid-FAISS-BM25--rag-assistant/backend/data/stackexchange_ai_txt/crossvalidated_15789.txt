[site]: crossvalidated
[post_id]: 15789
[parent_id]: 15745
[tags]: 
The classic error metric for probabilistic classifiers is the cross-entropy, for a two class classifier it is $L = -\sum_{i=1}^n t_i\log(y_i) + (1-t_i)\log(1-y_i)$ where there are n test patterns, $t_i \in [0,1]$ is the target for the $i^{th}$ test pattern and $y_i$ is the outoput of the model for the $i^{th}$ test pattern. The cross-entropy is the negative log-likelihood used in fitting a logistic regression model (or kernel logistic regression or neural networks etc.) so it is fairly natural to use it as the test metric as well. Unlike the AUROC it takes the calibration of the probabilities into account rather than just the ranking (which may or may not be important depending on the application), but it goes off to infinity if the classifier gets the answer wrong with very high confidence. A closely related metric is the mean predictive information, which for a two class problem is $I = \frac{1}{n}\sum_{i=1}^n\left[t_i.*log_2(y_i) + (1-t_i).*log_2(1-y_i)\right]+1$ which is very similar, but conveniently gives result that normally lies between 0 and 1 bits, which is more easily interpretable than the cross-entropy
