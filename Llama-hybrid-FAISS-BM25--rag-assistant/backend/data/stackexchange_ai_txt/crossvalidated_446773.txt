[site]: crossvalidated
[post_id]: 446773
[parent_id]: 
[tags]: 
Understanding Overfitting in neural network

i have built a fully connected neural network in order to predict a physical quantity. The dataset that I use is composed by 18 features and 1 label. The samples are 8504. I ran a lot of time the code and I plot the chart that shows me the evolution of the training loss vs validation loss along the epochs in order to understand if it is a case of overfitting. I attacched some of this plot below. Is it a case of overfitting? because in some case the two line seem to converge. But in other case the val loss is under the training loss, even if only slightly. So the question are: 1) is it overfitting? If yes, why I can say that it is overfitting if the val loss is under the training loss? 2) what can I do in order to reduce overfitting? because I've tried the most important one, like regularizaion of weigth and bias, dropout, data augmentation. The code that I wrote is attached below. There are also the chart (the first 2 with 200 epochs, the second two with 100 epochs). Let me know if you want other details. Thanks. import pandas as pd import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.feature_selection import SelectFromModel from sklearn import preprocessing from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt import keras import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, BatchNormalization from tensorflow.keras.callbacks import EarlyStopping from keras import optimizers from keras import regularizers from keras import backend from tensorflow.keras import regularizers from keras.regularizers import l2 # ============================================================================= # Scelgo il test size # ============================================================================= test_size = 0.20 importance = 0.95 # ============================================================================= # Richiamo il dataset # ============================================================================= dataset = pd.read_csv('DataSet.csv', decimal=',', delimiter = ";") label = dataset.iloc[:,-1] features = dataset.drop(columns = ['Label']) y_max_pre_normalize = max(label) y_min_pre_normalize = min(label) def denormalize(y): final_value = y*(y_max_pre_normalize-y_min_pre_normalize)+y_min_pre_normalize return final_value # ============================================================================= # Split # ============================================================================= X_train1, X_test1, y_train1, y_test1 = train_test_split(features, label, test_size = test_size, shuffle = True) y_test2 = y_test1.to_frame() y_train2 = y_train1.to_frame() # ============================================================================= # Normalizzo # ============================================================================= scaler1 = preprocessing.MinMaxScaler() scaler2 = preprocessing.MinMaxScaler() X_train = scaler1.fit_transform(X_train1) X_test = scaler2.fit_transform(X_test1) scaler3 = preprocessing.MinMaxScaler() scaler4 = preprocessing.MinMaxScaler() y_train = scaler3.fit_transform(y_train2) y_test = scaler4.fit_transform(y_test2) # ============================================================================= # Creo la rete # ============================================================================= from keras import backend as K def r2_score(y_true, y_pred): SS_res = K.sum(K.square(y_true - y_pred)) SS_tot = K.sum(K.square(y_true - K.mean(y_true))) return ( 1 - SS_res/(SS_tot + K.epsilon()) ) from datetime import datetime logdir = "logs/" + datetime.now().strftime("%Y%m%d-%H%M%S") tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir) optimizer = tf.keras.optimizers.Adam(lr=0.001) model = Sequential() c= 1e-10 # ,kernel_regularizer=l2(c), bias_regularizer=l2(c) model.add(Dense(100, input_shape = (X_train.shape[1],), activation = 'relu',kernel_initializer='glorot_uniform')) model.add(Dropout(0.2)) model.add(Dense(100, activation = 'relu',kernel_initializer='glorot_uniform')) model.add(Dropout(0.2)) model.add(Dense(100, activation = 'relu',kernel_initializer='glorot_uniform')) model.add(Dense(1,activation = 'linear',kernel_initializer='glorot_uniform')) model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse', r2_score]) history = model.fit(X_train, y_train, epochs = 100, validation_split = 0.1, shuffle=False, batch_size=250 ) history_dict = history.history loss_values = history_dict['loss'] val_loss_values = history_dict['val_loss'] r2_score_train = history_dict['r2_score'] r2_score_val = history_dict['val_r2_score'] y_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test) y_train_pred = denormalize(y_train_pred) y_test_pred = denormalize(y_test_pred) from sklearn.metrics import r2_score print("\n\nThe R2 score on the test set is:\t{:0.3f}".format(r2_score(y_test_pred, y_test1))) print("The R2 score on the train set is:\t{:0.3f}".format(r2_score(y_train_pred, y_train1))) from sklearn import metrics # Measure MSE error. score = metrics.mean_squared_error(y_test_pred,y_test1) print("\n\nFinal score test (MSE): %0.4f" %(score)) score1 = metrics.mean_squared_error(y_train_pred,y_train1) print("Final score train (MSE): %0.4f" %(score1)) score2 = np.sqrt(metrics.mean_squared_error(y_test_pred,y_test1)) print(f"Final score test (RMSE): %0.4f" %(score2)) score3 = np.sqrt(metrics.mean_squared_error(y_train_pred,y_train1)) print(f"Final score train (RMSE): %0.4f" %(score3))
