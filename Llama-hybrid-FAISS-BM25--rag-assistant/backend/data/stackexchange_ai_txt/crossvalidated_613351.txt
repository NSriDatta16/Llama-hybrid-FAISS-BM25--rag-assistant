[site]: crossvalidated
[post_id]: 613351
[parent_id]: 613295
[tags]: 
Residual connections are used in many different types of architectures (e.g., transformers). Whenever you have a question about whether a technique is going to work, it's best to try it and see if your CV score or whatever you're using improves. The easiest way to use residual connections in a feed forward is to use the same width for all hidden layers. If you want to use different widths, you can use linear layers to reshape the inputs so that they're conformable with the outputs. For example, if your input $x$ to a layer $f$ is $N$ -dimensional, and the output is $M$ -dimensional, then you'd want to introduce an $M\times N$ array of weights $W$ , an $M$ -dimensional bias term $b$ , and use them so that your layer output is $$f(x) + (b + Wx)$$ So it's close to a residual connection, but not exactly the same thing. Also, you'll likely need to pass your input features through a linear layer so you can add them to the output of your first hidden layer.
