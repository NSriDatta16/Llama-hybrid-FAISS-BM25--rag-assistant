[site]: crossvalidated
[post_id]: 486158
[parent_id]: 
[tags]: 
reparameterization trick in VAEs, How should we do this?

I'm confused about how does reparameterization trick works. In this article shows it very simple. You learn two vectors $\sigma$ and $\mu$ , sample $\epsilon$ from $N(0, 1)$ and then your latent vector $Z$ would be (where $\odot$ is the element-wise product.): $$ Z = \mu + \sigma\odot\epsilon $$ BUT when I look at the TensorFlow tutorial code for VAEs, it is not just a simple $\odot$ . The code is this: def reparameterize(self, mean, logvar): eps = tf.random.normal(shape=mean.shape) return eps * tf.exp(logvar * .5) + mean which is showing this: $$ Z = \mu + \epsilon\times e^{0.5\times\log{}var} $$ These two are not the same and I'm confused, first why does it learn the logarithm of variance (as the name of the variable suggests) instead of learning just variance. second, why it is multiplied by 0.5? and finally, which one is the correct reparameterization trick (if they differ)?
