[site]: crossvalidated
[post_id]: 224794
[parent_id]: 
[tags]: 
Help understanding Linear Model in ESL book

Also known as "I slowly try to understand ESL (Elements of Statistical Learning)", part two (see part one ) Help me understand this (bullets added) The term $\hat{β}_0$ is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable 1 in $X$ , include $\hat{β}_0$ in the vector of coefficients $\hat{β}$ , and then write the linear model in vector form as an inner product $\hat{Y}= X^\top\hat{β}$ where $X^\top$ denotes vector or matrix transpose (X being a column vector). Here we are modeling a single output, so $\hat{Y}$ is a scalar; in general $\hat{Y}$ can be a $K$ –vector, in which case $β$ would be a $p \times K$ matrix of coefficients. In the $(p + 1)$ -dimensional input–output space, $(X, \hat{Y} )$ represents a hyperplane. If the constant is included in $X$ , then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the $Y$ -axis at the point $(0, \hat{β}_0)$ . From now on we assume that the intercept is included in $\hat{β}$ . My questions: $K$ in the context of a $K$ -vector and $p \times K$ matrix of coefficients -- that value is obviously different than $p$ ; but is $K$ different than $N$ -- the number of observations ? What does the notation $(X, \hat{Y} )$ mean? How do they mean "hyperplane"? For example, in the the 2-D example? How do they mean "subspace"? For example, in the the 2-D example? How do they mean "affine set"? For example, in the the 2-D example? When they "Assume the intercept is included" -- which did they choose, option 1 or option 2? How do they mean " $\hat{β}_0$ is the bias" -- why is that word used? Is it related to bias vs variance? Is there indeed a typo as suggested in the quoted part here ; should it be (... in which case $\mathbf{\hat{β}}$ would be a $p \times K$ matrix of coefficients...) In other words, when can $β$ take his hat off ? Guess at answers: Yes, $p$ is number of columns/variables (i.e. age, weight) $N$ is the number of rows/observations (i.e. Andy, Olly -- though this linear model operates on one-row-at-a-time) , then $K$ is yet another (orthogonal?) axis (i.e. Andy's age, and weight at age 3, age 10, age 20)? It looks like a Cartesian coordinate , but it's generic (uses $X$ and $\hat{Y}$ ). In 2-D , (1,2) represents the point on a 2-D graph. So does (x,y) represent a set of points; i.e. a line? I have trouble reconciling a scalar $x$ with a vector $X$ hyperplane; They mean it cuts the (..."space"?) into two parts. In 2-D , a line cuts (.... ${\rm I\!R^2}$ ?) space on a graph into two separate portions. Indeed, that's the whole point of the "linear model" binary classification (two parts) ; could be called a "hyperplane model" for higher dimensions. subspace ; not sure. How can I think of "If the constant is included in $X$ ", in 2-D space, where X is just a scalar? Do they mean the hyperplane coincides with a plane formed by the intersection of p dimensions? In 2-D, like a line x=0 or y=0? affine set ; they mean it does not have an origin? Because the "intercept" has moved it away from the origin, like the " $b$ " in $y=mx+b$ ? I am naive about "affine": They've gone with option 1; where the constant ( $1$ ) is included in $X$ and the intercept ( $\hat{β}_0$ ) is included in $\hat{β}$ "Bias" and "weight vector" are two relevant terms here explained at the link... I am trying to understand why they would use the word "bias"; In 2-D space the "bias" the y-intercept... is it because when "x" is 0, we know we can't actually estimate "y", but a bias suggests there is some non-zero value for "y"? It is different than bias vs variance (...?) Yes, it should be $\mathbf{\hat{β}}$ -- we only remove the hat when we start "viewing this as a function" (i.e. $f(X) = X^\topβ$ )
