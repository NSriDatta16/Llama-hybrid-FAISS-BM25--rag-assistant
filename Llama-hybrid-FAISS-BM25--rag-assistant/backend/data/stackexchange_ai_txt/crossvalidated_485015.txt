[site]: crossvalidated
[post_id]: 485015
[parent_id]: 
[tags]: 
Analysing event rate in a computer system for alerting

I'm trying to come up with an algorithm to identify how frequently specific events occur on a computer system, so that I can categorise them based upon their rarity. I can't keep a log of every event as there are too many, so I need a single record of each event and a numerical indicator for its 'Events per day/hour' etc. Let's say my threshold for notifications is 2 per day. If EventA first occurs 10 days ago (and I record that) and then doesn't occur again until today, I'll update it's 'occurrences' count to 2, and I can calculate that it occurs 'on average' once every 5 days. If it then occurs 8 times more over the next hour, my algorithm will record 10 occurrences over 10 days which is 1/day, but the most useful rate is actually 8/hour which I want to be alerted for. Is there an algorithm for this? Should I discard any events that haven't occurred for my threshold period ? i.e. if I want to know about x occurrances of a specific event per day, and that event hasn't occurred for an day, should I discard the record of it?
