[site]: crossvalidated
[post_id]: 325105
[parent_id]: 323788
[tags]: 
In my experience, problems where the input data has different sizes are usually multiple instance learning (MIL) problems in disguise. (wikipedia has 2 different entries for MIL: https://en.wikipedia.org/wiki/Multiple-instance_learning and https://en.wikipedia.org/wiki/Multiple_instance_learning ) In my opinion, the best paper to learn about MIL is Amores, Jaume (2013), "Multiple instance classification: Review, taxonomy and comparative study", Artificial Intelligence, 201: 81â€“105 , In a nutshell, in MIL you have to generate a different representation of the data. Historically the first solution was to generate features by aggregating the data in the original representation - taking mean, standard deviation, maximum and minimum values, and so on. Thus the data may have different sizes but you end up representing the data using this fixed set of aggregation measures. The second family of approaches are vocabulary based. In some way you learn "frequent sub-patterns of the data" ( if your data is an image, the sub-patterns can be patches of the image, if the data is a time series, they can be segments) and you represent (in very general terms) the data as the histogram of such sub-patterns for each of the original data. Again, the data has different sizes but the set of sub-patterns is fixed, so the histograms (number or frequency of each sub pattern appears in the data) are of fixed size (sometimes this vocabulary approach is called "bag of visual word" when the data are images). Please read Amores papers. I learned a lot from it. My experience in MIL was mainly in images and for a long time I used bag of visual words. In a time series problem, to my surprise, the aggregation approach was the best solution. Now, everybody and me also, are using deep nets to solve MIL problems!!
