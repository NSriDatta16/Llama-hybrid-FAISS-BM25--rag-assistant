[site]: datascience
[post_id]: 27070
[parent_id]: 27058
[tags]: 
It looks like you want new word discover ? Because thousand is not a big deal Just build ngrams of filenames , count them would be fine . You can use Trie to store string counts, reduce memory cost , I can provide a dict way(in python): from collections import defaultdict, Counter # for memory effcient, you would need trie here # t = Trie() t = defaultdict(int) filenames = ["sjkghkjfs skjfs kjskdfjsfkjs sahkj", "tretyer erytewr fskjdf trjk", "...."] # some preprocess, tokenize to sentence , to words , filter useless one def ngrams(s, start, end): for i in range(start, end+1): if len(s) > i: for j in range(i, len(s)): yield s[j:j+i] # Suppose you need same string with length 4 ~ 6 for s in filenames: for word in ngrams(s, 4, 7): t[word]+=1 # Suppose you need most_common 5 print(Counter(t).most_common(5)) Further more you can tokenize the filenames at first. Calculate tf-idf or word entropy to remove the useless words then do something like above . And If you have a group of traget strings , word2vec may be a good tool, you can use it to search the strings in similar domain.
