[site]: crossvalidated
[post_id]: 283114
[parent_id]: 
[tags]: 
Why do Srivastava et al. claim that "the best" theoretical regularization technique involves all possible network parameter settings?

In the original paper on Dropout by Srivastava, Hinton, Krizhevsky et al. (2014), the authors make this claim in the introduction: With unlimited computation, the best way to "regularize" a fixed-size model is to average the predictions of all possible settings of the paramaters, weighting each setting by its posterior probability given the training data. Here is how I understand that statement so far: For a single neural net architecture and hyperparameter configuration, the set of all possible trained weight configurations is $Μ$. Each individual model $μ ∈ M$ has a Bayesian posterior probability $p_μ$ given the training data $D$. For a given example $x ∈ D$, the prediction of any individual model is $f_μ(x)$ An optimally regularized system makes a prediction equal to the sum of all individual predictions, but weighted by the model's posterior $f(x) = \sum p_μ \cdot f_μ(x)$ Please correct me if I've misinterpreted there. Why is that "the best way" to regularize a particular network, given unlimited computational power? To me, trained weights’ distribution of posteriors is not clearly connected to overfitting and regularization. But I trust these authors are correct, so I would really appreciate some help seeing how those are connected.
