[site]: crossvalidated
[post_id]: 397659
[parent_id]: 397653
[tags]: 
A count data model is at the same time a rate (events per observation time) model, if you use an offset. E.g. number of events/reactions/times/whatever per time unit can be modelled with the number as the dependent variable and the observation time as an offset with a log-link. Similarly, if your rate is event/reaction/whatever out of N tries, then you can just model that as binomial (y events out of N tries) using logistic regression rather than modeling the proportion. The reason why it is usually better to model things in this manner is that you have more information, if you have seen more observation time or more tries, but if you just look at the rate or proportion, then this is not reflected (while a count or binomial model does reflect it). Having a lot of zeros is not per-se a reason to go for a zero-inflated model. The question is really are there more zeros than you would expect under a simpler model. The obvious models to try first are GLMs (Poisson or logistic regression) with an added random effect, which are GLMMs (Poisson or logistic regression) or negative binomial regression. You can fit such models in R using the glmer function in lme4 using the Poisson family with a log link or the binomial family with a logit link. Which random effects make sense depends on your experiment, which was not so clear from your description (but if you have e.g. multiple records for an individual, you would probably want at least a random individual effect on the intercept, i.e. (1|individual) ). The nice thing about a GLMM is that you can estimate a latent subject random effect (individuals that just have fewer events than others), while a zero-inflated model is similar, but there is a stark dichotomy (either an individual never has events or it follows the a usual distribution). Looking at residuals for models for discrete data is challenging (see e.g. What do the residuals in a logistic regression mean? ). Other options including (posterior) predictive checks. With nested models (e.g. a logistic regression GLMM vs. a logistic regression GLMM with zero inflation), you can also do tests of whether the more complex model is needed. However, note that deciding on your model based on your data will invalidate hypothesis tests that you conduct based on the same data. I.e. if you want to have valid hypothesis tests, p-values and so on that can be claimed to be "confirmatory", you really should be pre-specifying your analysis prior to seeing the data based on theoretical considerations and/or based on previous data that allows you to judge what model will be appropriate.
