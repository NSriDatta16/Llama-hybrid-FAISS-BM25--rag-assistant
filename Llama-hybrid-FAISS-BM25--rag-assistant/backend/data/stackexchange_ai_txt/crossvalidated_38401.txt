[site]: crossvalidated
[post_id]: 38401
[parent_id]: 
[tags]: 
Is it ok to fit a Bayesian model first, then begin weakening priors?

When doing frequentist stats, there is a long list of big no-nos, like looking at the results of statistical tests before deciding to collect more data. I'm wondering generally if there are a similar list of no-nos for the methodologies involved in Bayesian statistics, and specifically whether the following is one of them. I've realized recently that for some of the models I've been fitting, my process has been to first fit the model with informative priors to see if it works or blows up, and then weaken the priors either to uninformative or weakly informative and refit the model. My motivation for this really has to do with the fact that I'm writing these models in JAGS/Stan, and in my mind I've been treating it more like a programming task than a statistical one. So, I do a first run, sort of rigging it to converge quickly by using informative priors, making it easier to catch errors in the model I've written. Then, after debugging the model, I refit it with uninformative, or weakly informative priors. My question is whether or not I'm breaking some serious rules with this process. For example, for my inferences to be valid, and to avoid exploiting researchers' degrees of freedom, do I need to commit to specific priors before are start fitting any models?
