[site]: datascience
[post_id]: 69263
[parent_id]: 69209
[tags]: 
First, some background context: Non-contextual word embeddings (e.g. word2vec) only reflect co-occurrence statistics. The similarity between two embedded vectors may only be loosely related to their semantics (e.g. the representations for country names like "france" and "italy" may be close) or there may even be negative correlation (antonyms may be very close). BERT is subword-level, not word-level. This means that before going throug the network, there is a tokenization process that splits words into word pieces. Therefore, you obtain representations of pieces of words, not words themselves, e.g. for word "difficult" you may obtain a tokenization like "diff", "i", "cult". There is no direct way of obtaining a "combined representation" from the individual subword representations. Therefore: I recommend you not using BERT, because you are interested at word-level information, while BERT only offert subword-level stuff. I recommend you to look into ELMo , which offers word-level contextual representations.
