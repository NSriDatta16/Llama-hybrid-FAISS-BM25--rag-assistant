[site]: crossvalidated
[post_id]: 393635
[parent_id]: 393629
[tags]: 
Say that you have an urn with red, green, and blue balls, you draw $n$ balls from the urn with replacement. The distribution of the counts of the red, green, and blue balls, $(x_1, x_2, x_3)$ , would follow multinomial distribution parametrized by probabilities $(\pi_1, \pi_2, \pi_3)$ such that $\sum_{j=1}^3 \pi_j = 1$ for drawing red, green, and blue balls respectively, $$ (x_1, x_2, x_3) \sim \mathcal{M}(n, \,\pi_1, \pi_2, \pi_3) $$ The values of $\pi_i$ are unknown and you want to estimate them from your data (counts of the drawn balls). There are different ways of estimating the probabilities, for example you could take the maximum likelihood estimate $\hat\pi_i = \tfrac{x_i}{n}$ . Another possibility is to use Bayesian approach, where instead of looking only at the data, you also assume a prior for the probabilities and then use Bayes theorem to update the prior to obtain the posterior estimate of the parameters. In case of multinomial distribution, the most popular choice for prior is Dirichlet distribution , so as a prior for $\pi_i$ 's we assume $$ (\pi_1, \pi_2, \pi_3) \sim \mathcal{D}(\alpha_1, \alpha_2, \alpha_3) $$ where $\alpha_1, \alpha_2, \alpha_3$ such that $\forall\,\alpha_i > 0$ are the parameters of the Dirichlet distribution. Because this is a conjugate prior, updating the prior to posterior is straightforward, because the posterior distribution of the estimated parameters is $$ (\pi_1, \pi_2, \pi_3) \mid (x_1, x_2, x_3) \sim \mathcal{D}(\alpha_1 + x_1, \alpha_2 + x_2, \alpha_3 + x_3) $$ If you want a point estimate for the probabilities, you can take mean of the posterior distribution $$ \hat\pi_i = \frac{\alpha_i + x_i}{\sum_{j=1}^3 \alpha_j + x_j} $$ If you want a practical example where it is useful, for example in natural language processing you can use Laplace smoothing , i.e. estimate the probabilities of occurrences of words using Dirichlet-multinomial model with uniform prior. It helps for the fact that when training and then predicting using a machine learning model, if in the test set you find a word that was not seen in training set, then with maximum likelihood approach you would conclude that the probability of observing such word is zero (it was not seen in training set), while in case of Bayesian estimate it is nonzero $$ \hat\pi_i = \frac{\alpha_i + 0}{\sum_{j=1}^3 \alpha_j + x_j} $$ This makes a difference in many cases, for example with Naive Bayes algorithm you multiply all the probabilities, so multiplying by zero would zero-out everything.
