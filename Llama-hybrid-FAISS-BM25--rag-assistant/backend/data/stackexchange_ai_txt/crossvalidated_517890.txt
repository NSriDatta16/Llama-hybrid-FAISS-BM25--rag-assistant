[site]: crossvalidated
[post_id]: 517890
[parent_id]: 517883
[tags]: 
Neural networks require gradients to be trained. If you would do classification as you described, i.e., $$\hat{y} = f(s) = \begin{cases} 0 & s \leq 1 \\ 1 & s > 1 \end{cases},$$ it would be hard to define a gradient w.r.t. $s$ , which is the output of the network. Therefore, you do not want to use classification in this sense. Instead of pure classification, a trick called logistic regression is generally used. Instead of directly predicting the class, the goal is to predict the probability that a sample is in the positive class. Assuming that the binary labels are binomially distributed , logistic regression maximises the likelihood of the parameters for the given data. Therefore, you will want to use a logistic sigmoid at the end of your network in combination with the binary cross-entropy loss. With (linear) regression, the output labels are assumed to have a Gaussian distribution, which is incorrect for binary classification. Since the binomial distribution assumption is clearly much closer to reality, you should get better results with logistic regression.
