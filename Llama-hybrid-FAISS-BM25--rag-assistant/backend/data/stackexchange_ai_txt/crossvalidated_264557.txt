[site]: crossvalidated
[post_id]: 264557
[parent_id]: 264531
[tags]: 
The point is moot as the likelihood is never defined as a joint distribution over observables and parameters. ...in many textbooks, likelihood of a r.v. $X$ given an observation is taken to be the function got by treating the pdf as having variable parameters but fixed argument. The traditional definition of the likelihood function is the density of the sample $X$, at the observed value of the sample $x$, taken as a function of the unknown parameter when this parameter $\theta$ varies over the range of the parameter space. [excerpt from Fisher (1922)] If we consider a joint distribution over arguments and parameters like above, the likelihood, given observation $x_0$, appears to be this function: $$t↦f_{X,θ}(x_0,t)$$ So, in this case the constant of proportionality of (∗) is $$∫f_{X,θ^\text{old}}(x_0,t)\text{d}t∫f_{X,θ^\text{old}}(x,⋅)\text{d}x$$ This joint distribution $f_{X,\theta}$ is only meaningful in either a Bayesian or a fiducial setting, hence does not correspond to the meaning of a likelihood. (In the case of random effects and latent variables, the likelihood is the integral of a joint over the random effects or latent variables.) The function$$t↦f_{X,θ}(x_0,t)$$is then proportional to the posterior of $\theta$ given $x_0$ with normalising constant $$\int f_{X,θ}(x_0,t)\text{d}t$$but one cannot use it as a likelihood since (*) involves the multiplication of $L(\theta^{old} = \cdot; x_0)$ by the prior $f_{\theta^{old}}$.
