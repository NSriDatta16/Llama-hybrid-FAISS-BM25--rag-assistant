[site]: crossvalidated
[post_id]: 603040
[parent_id]: 603028
[tags]: 
Bias and variance are elementary properties of estimators, and they're usually introduced to early statistics students because they're well understood conceptually, and one can study the properties of the quite restricted class of unbiased estimators - namely the Cramer-Rao bound, sufficiency, asymptotic relative efficiency, etc. The fact that squared bias and variance result as a decomposition of squared error is, if anything, an elegant result. The number of questions that immediately follow are too numerous to count, is squared error loss the right loss? Under what conditions is it optimal? Does a similar result exists for other loss functions? To paraphrase Paul Erdos, "Anyone can think of an interesting problem." The first year theory approach has its problems too. Consider Hodge's superefficient estimator . It is unbiased and it beats the Cramer-Rao bound. But it turns out that the estimator is not regular. More broadly, once we start considering biased estimators, we have a much broader class of estimators with different optimality properties to consider. Concepts like admissibility, minimax, penalized or bounded loss, etc. give rise to other popular estimators as solutions to particular problems, particularly Bayes estimators, ridge estimators, and so on. These concepts would be covered in a second year statistics or probability theory class, from such texts as Ferguson "A Course in Large Sample Theory", Lehmann Casella "Theory of Point Estimation", or Wassermans' "All of Nonparametric Statistics".
