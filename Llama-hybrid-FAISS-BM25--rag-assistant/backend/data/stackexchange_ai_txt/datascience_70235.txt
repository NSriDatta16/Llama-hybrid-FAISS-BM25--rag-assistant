[site]: datascience
[post_id]: 70235
[parent_id]: 70230
[tags]: 
In Machine Learning, you are making the assumption that the training and test sets follow the same distribution . If this assumption does not stand, then your model won't be able to generalize properly. Having said that, there obviously is a chance of a test-set feature having a value slightly larger than the max of that same feature in the training set. If this is the case, all ML models will work perfectly fine for that sample having a normalized value slightly higher than $1$ . What I want to emphasize, however, is that if the training set and the test set have significantly different distributions (most commonly due to a small dataset size), then no model will be able to generalize properly and it won't be a problem of normalization.
