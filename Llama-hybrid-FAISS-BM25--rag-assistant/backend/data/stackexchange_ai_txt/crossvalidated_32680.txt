[site]: crossvalidated
[post_id]: 32680
[parent_id]: 32627
[tags]: 
As you did not provide specific details on the problem I will give a short but comprehensive review of the dimension reduction techniques. Generaly two problem statements considered: Unsupervised dimension reduction. We have a set of observations $S =\{X_i\}_{i=1}^{N}$ and need to find a reduced representation for it. Linear models. Here PCA is a the most popular technique. MDS is another approach you can try. Nonlinear models. Here we have a lot of different techiques: LTSA (I would suggest this in general), Isomap , LLE , and lots of their modifications. All this methods (and a litlle bit more) are provided by special (yet free) MatLab toolbox . Supervised dimension reduction (aka Feature Extraction). We have a set of observations (both regressors and responses) $$S =\{X_i, Y_i\}_{i=1}^{N}, X_i\in\mathbb{R}^p, Y_i\in\mathbb{R}^q$$ and need to find a reduced representation for $X$s which preserves information about $Y$s. And we assume that the foolowing model holds: $$Y=f(X)+\varepsilon=g(\tilde{X}) +\varepsilon, \tilde{X}=XB, B\in\mathbb{R}^{p\times d}, d Linear models. PLS is the most popular (and probably the only linear) technique. Nonlinear models. MAVE, OPG, SAMM, SIR and some others are employed in this problem. Yocan find some MatLab implementations here . In both problem statement dimensionalty selection is another individual problem. I can give more exhaustive comments on any of the subjects provided if you give more details on your problem and possibly choose the approach.
