[site]: datascience
[post_id]: 8891
[parent_id]: 8799
[tags]: 
K-means uses the mean . K-means is designed for least-squares. It only works reliably with (variants of) squared Euclidean distance (= sum of squared deviations). Counterexample: Assume you have the two hours 0 and 23. If they get assigned to the same cluster, k-means will compute the mean . The mean of the two values is 11.5. It is not 23.5. Abusing k-means with a cyclic "distance" may no longer converge , and will return nonsense results. But there are more cases where the concept of a cluster center is not viable on cyclic data. For example, given an event on every full hour, what is the center ? The arithmetic mean is 12 - but if you take cyclic space into account every hour is an equally good choice in cyclic space. Therefore, the concept of a "center" in cyclic space is fragile. Alternate clustering algorithms You can try e.g. PAM or DBSCAN instead, with an appropriate similarity measure. Projection techniques As pointed out by other answers, you can project the time to the unit circle via sin/cos(time/24*2pi). By computing the angle of the centroids, you can map this back to a point in time. But once you want additional attributes it gets hard to meaningfully normalize the data (to combine attributes), and you can get undefined time (e.g. if there are two points in a cluster, one at 6 and one at 18). I didn't discuss this because I wanted to point out that modifying the distance function is not a good idea for k-means.
