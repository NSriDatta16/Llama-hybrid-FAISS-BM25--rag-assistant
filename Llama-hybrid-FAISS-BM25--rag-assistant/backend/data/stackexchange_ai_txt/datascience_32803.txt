[site]: datascience
[post_id]: 32803
[parent_id]: 32694
[tags]: 
First off, a neural network is a black box model and there is no way of really knowing what each layer does in practice. There are examples that have been created that are useful for explaining the conceptual processes that could be happening. The car example and the Eigen-face examples are two that come to mind. In reality, these are carefully tailored examples to help people understand deep learning versus single layer neural nets. My guess is that some intern or grad student serendipitously found the configuration that produced the 'features' for the example. While there are tools that can output the hidden layers, this is not generally done or necessary in practice. As you note, there is no difference in how the layers function except for which activation function the developer chooses. What this means in practice is that there are no heuristics that determine the optimal shape or depth of a neural network. Going back to the cars example, from memory, it implies that the model is a three or four layer NN, but you don't know how many nodes are in each hidden layer or the activation function used in each layer. In reality, the performance of a model might be better with any number of variations in the model, like the number of epochs, number of hidden layers, nodes in each hidden layer, or activation functions at each layer. Defining the model really is more intuition and prototyping than other models (think linear regression). This is where having a good grid search can help.
