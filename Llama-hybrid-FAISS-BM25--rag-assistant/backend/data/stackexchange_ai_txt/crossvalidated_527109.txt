[site]: crossvalidated
[post_id]: 527109
[parent_id]: 
[tags]: 
Why the standard errors of logistic regression are of that form?

In Elements of Statistical Learning, page 125 it is written that the coefficients of a logistic regression converges to $\mathcal{N}(\beta, (X^T W X)^{-1})$ with: $X$ the $N \times (p+1)$ matrix of data $W$ a $N \times N$ diagonal matrix of weights with $i$ th diagonal element $p(x_i;\beta)(1-p(x_i; \beta))$ where by definition $p(x_i;\beta) = \exp(\beta^T x_i)/(1+\exp(\beta^T x_i))$ . Is there a proof/intuition for the $(X^T W X)^{-1}$ ?
