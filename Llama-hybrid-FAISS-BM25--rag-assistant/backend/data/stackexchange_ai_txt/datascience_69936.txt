[site]: datascience
[post_id]: 69936
[parent_id]: 30714
[tags]: 
Bootstrapping needs just a single transition, or a single tuple (state, action, next_state, reward) in order to perform a value (Q-value) update; thus learning can occur without a full episode of transitions. This is used in Q-learning type recursions. Since we are not waiting for a full episode to make an update, playing can be intertwined with learning. In this kind of learning, we may have a stochastic policy (say, epsilon greed), where we are using the maximum Q-value in the next state to update the current Q-value, but we may not have to take the action that actually maximizes the Q function when we get to that next state. As such, we call Q-learning an off-policy method, since learning is not guided by the policy. Sampling requires several transitions or even a full episode (sequence of transitions from initial state to terminating state) in order to perform the update. Since we hope to learn a policy, and the update is done for a single episode, we must follow the same policy throughout the episode before we can improve it (or update it). This is called policy iteration, and is use in Monte-Carlo learning, and it is on-policy because the policy guides the learning process. Because the episode must end before an update is done, learning would be very slow if episodes consists of so many transitions; thus the method cannot be used for non-episodic tasks (like games with infinite horizons).
