[site]: crossvalidated
[post_id]: 311966
[parent_id]: 311962
[tags]: 
Unless you're talking about a batch backpropagation algorithm I am unaware of (if so, post a link), I think you may be getting confused. Conceptually there are two different parts: backprogation and the mini-batch gradient descent. For each element of the the batch $\{x_1, x_2, \ldots x_n\}$, you backpropagate the error for each individual example $x_i$ and you get gradients for each weight for each example. At the end of each batch, you determine the weight adjustment by averaging over the gradients and use mini-batch gradient descent (for instance) to determine the optimal direction to adjust your weights based on the average weights.
