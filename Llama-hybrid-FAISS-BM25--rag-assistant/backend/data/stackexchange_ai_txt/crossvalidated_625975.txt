[site]: crossvalidated
[post_id]: 625975
[parent_id]: 
[tags]: 
Why am I getting negative components with my custom NIPALS algorithm

I've recently been learning about the Nonlinear Iterative Partial Least Squares (NIPALS) algorithm for computing the principal components of a dataset. I am trying to code a NIPALS class from scratch in Python to get a more intuitive understanding of the algorithm: class NIPALS: def __init__(self): """ Attributes: - self.U: after fitting returns a matrix of (n_components, n_features). - self.Y: after fitting returns a matrix of (n_samples, n_transformed_features). - self.R1: after fitting returns a list of rank 1 matrices. - self.iterations: after fitting returns an integer indicating how many iterations the fit took to converge. """ self.U = [] self.Y = [] self.R1 = [] self.n_loops = 0 def vector_len(self, a: np.ndarray): return np.sqrt(np.dot(a, a)) def normalize_vector(self, v): return v*(1/vector_len(v)) def converge(self, t, A): # step 2: compute the principal component p = ((t.T@A)/(t.T@t)).T # step 3: normalize the principal component p_norm = self.normalize_vector(p.flatten()).reshape(-1,1) # step 4: project the data onto the principal component t = A@p_norm return t, p_norm def fit(self, A): # step 1: grab the first column vector from the data matrix t = A[:,0].reshape(-1,1) # step 5: compare t from the start of the iteration to t from # the end of the iteration; continue until convergence while np.all(self.converge(t, A)[0] != t): t, p_norm = self.converge(t, A) # append the pc and the coordinates self.U.append(p_norm.T) self.Y.append(t) # count the loops self.n_loops+=1 # terminate when n_loops = n_features if self.n_loops == A.shape[1]: self.U = np.row_stack(self.U) self.Y = np.column_stack(nipals.Y) return else: # step 6: deflate the data matrix r1 = t@p_norm.T E = A-r1 self.R1.append(r1) # step 7: restart the loop self.fit(E) I am testing this class out on the Breast Cancer Wisconsin dataset and comparing it to the result I get from using PCA class from sklearn. The results are nearly identical however, some components that I get from my class are the opposite sign, examples: There doesn't seem to be a discernible pattern regarding which components are the opposite sign. Can anybody help me understand what's going on here? I know that sklearn uses the SVD to compute the PCs. Is this difference due to the difference in methods? Any answers are much appreciated!
