[site]: crossvalidated
[post_id]: 70033
[parent_id]: 70032
[tags]: 
There is a plethora of methods out there for addressing this question, but a very common method is data imputation. Here is a good Wikipedia source to get you started: http://en.wikipedia.org/wiki/Imputation_%28statistics%29 The basic premise of imputations is to construct a distribution for the missing data conditional on the actual data that you do have. Similarly, you could try to construct the full data likelihood that incorporates both the observed and missing data, i.e., $$f(y|\theta)=f(y_{\text{obs}},y_{\text{mis}}|\theta)$$ where the full data $y$ is equal to the observed data $y_{\text{obs}}$ as well as the missing data $y_{\text{mis}}$. Now if you treat this problem from a Bayesian perspective then we can think of $y_{\text{obs}}$ as being a random variable and thus we can integrate out the missing data to obtain the observed data likelihood $$f(y_{\text{obs}}|\theta)=\int_{\Theta}f(y_{\text{obs}},y_{\text{mis}}|\theta)d\theta$$ and then proceed with statistical inference using the observed data likelihood. However, the integral may be computationally expensive to deal with. Alternatively, for conditionally independent observations the full data likelihood is $$f(y|\theta) = \prod_{i=1}^n f(y_{\text{obs},i}|\theta)\prod_{i=n+1}^{n+k}f(y_{\text{mis},i}|\theta)$$ If we once again take a Bayesian perspective and place a prior on $\theta$ then we can conduct statistical inference for $\theta$ by sampling from the posterior distribution of $\theta|y$ in a MCMC fashion. For example, one possibility would be to use a Gibbs sampler where we can sample first from $$p(\theta|y_{\text{obs}},y_{\text{mis}})$$ and then sample from $$p(y_{\text{mis}}|\theta,y_{\text{obs}})$$ repeatedly until we have a sufficient number of samples of $\theta$ to conduct inference for $\theta$.
