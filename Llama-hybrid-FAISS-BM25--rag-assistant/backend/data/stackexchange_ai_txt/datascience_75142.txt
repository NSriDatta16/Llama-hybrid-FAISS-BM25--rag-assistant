[site]: datascience
[post_id]: 75142
[parent_id]: 
[tags]: 
Can all known ML algorithms be written as a sequence of matrix operations?

I keep hearing that machine learning is just linear algebra. Does that mean all known (and all possible?) ML algos, from random forest, to support-vector machines, to recursive neural networks, can be written as a sequence of basic matrix and vector operations, such as multiply, add, sum, etc? Would writing the algorithm in this way be able to capture all aspects of the model, e.g. architecture, activation functions, learning rate, optimization method? Can advanced features like convolution be expressed as basic matrix operations? Is there any ML algorithm for which this isn't true? Is matrix algebra Turing complete?
