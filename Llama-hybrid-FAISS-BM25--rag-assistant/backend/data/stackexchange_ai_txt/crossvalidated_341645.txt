[site]: crossvalidated
[post_id]: 341645
[parent_id]: 
[tags]: 
Would gradient boosting machines benefit from adaptive learning rates?

In deep learning, a big deal is made about optimizing an adaptive learning rate. There are numerous popular adaptive learning rate algorithms . The hyperparameters for all of the leading gradient boosting machines , by contrast, involve setting only a fixed learning rate. Intuitively, it could make sense to instead use a decreasing learning rate or possibly a hill-shaped learning rate. Is there any obvious mechanical limitation in how GBMs are implemented that would make an adaptive learning rate difficult? If not, is there a simple argument for why a constant rate makes more sense for a GBM than for deep learning? UPDATE: Relevant discussion in the context of LightGBM.
