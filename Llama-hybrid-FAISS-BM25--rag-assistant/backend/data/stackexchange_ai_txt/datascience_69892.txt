[site]: datascience
[post_id]: 69892
[parent_id]: 
[tags]: 
How do we determine what is learnt by a ML model?

I am trying to investigate/justify the output of a random forest regression model for a financial problem, where justification of the output is also important. In that context, for the random forest regression model that I built, I'm using the tree interpreter for justification of the output using the contributions. But is there any other way I can do this better? Is this the best approach? The idea is to be able to explain the output of the random forest regression model to a layman supported with technical points.
