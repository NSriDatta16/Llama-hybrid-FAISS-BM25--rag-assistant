[site]: crossvalidated
[post_id]: 100322
[parent_id]: 
[tags]: 
Approximating a Complementary Cumulative Distribution Function via a piece-wise function

I hope this is not too much to read, but I tried to give you a specific overview over my problem. I am currently trying to model the German electricity market, with a special focus on balancing power . Considering this setting I have a data set consisting of hourly observations of balancing power in Megawatts for the year of 2013. As the application of this data set would not represent any uncertainty in the data I wanted to use a little trick to "assume" uncertainty. While this is really only a basic approximation my model has to have some sort of "uncertainty", else there will be a big mistake. In order to do this, I divided my data set into five blocks (too many more would be to much for my calculation) of varying Megawatt-steps calculating the "probability" of a specific value to be in one of these blocks using the Complementary cumulative distribution function . What I mean by that is that I took the aforementioned function and tried to best approximate it in a (5-)piece-wise fashion. Unfortunately I had to do this in my example by a "sense of proportion". I hope the picture makes it a little more clear (On the x-axis the values are in Megawatts). EDIT: To be clear, I am not trying to best-possibly approximate the whole-function in a sense of a non parametric boundary estimation as user603 kindly described in his answer. (Or maybe this can be done using non parametric boundary estimation...?) I just want to fit the five (or whatever number) blocks best-possible to the function in a fashion that, e.g. the Sum of squared Errors is at a minimum. Also: i would love the knot location to be estimated. The following code example should further explain what I do: # This function divides the input data into 5 different blocks. # v, w, x and y are all Megawatt values dividing the data set getCall_block v & data[,2] w & data[,2] x & data[,2] y, select = c(colnames(data)[1], colnames(data)[2])) #' df_v And the getAverage function... #This function is called in the above mentioned function. #It calculates the mean average of the points of the CCDF lying in one block. #v and w are the input parameters in Megawatts. #They can be understood as starting point and end point of a given block. getAverage 0 if(w > v){ if (v > 0){ v My questions are the following: I) Does this way of approximating a CCDF sound okay for you guys? What did I overlook or completely mess up? II) Is there a way to automatically approximate the block sizes, in order to not rely on a sense of proportion. E.g. minimizing the sum of squared errors between the piece-wise function and the original function. Maybe this could be done by using a clustering algorithm like k-means? But then how to cluster this CCDF...? III) Is there a far more easier way to approximate any function in a piece-wise fashion? Thanks in advance!
