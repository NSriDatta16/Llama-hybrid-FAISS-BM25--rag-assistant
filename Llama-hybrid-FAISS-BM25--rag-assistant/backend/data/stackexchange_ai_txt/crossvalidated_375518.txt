[site]: crossvalidated
[post_id]: 375518
[parent_id]: 375346
[tags]: 
Non-negativity constraints share some fundamental similarities with regularization. Both are a means of imposing a priori knowledge about the solution in the form of constraints. Both can also be described as Bayesian priors. For example, ridge regression corresponds to a Gaussian prior, lasso corresponds to a Laplacian prior, and non-negativity constraints correspond to an improper prior that's zero for any solution with negative parameters and flat otherwise. On the other hand, one doesn't usually see non-negativity constraints described as regularization. One difference is that non-negativity constraints are typically used when the parameters are known with certainty to be non-negative (e.g. for physical reasons). Also, there are no degrees of non-negativity--either the parameters are non-negative or they're not. In contrast, regularization techniques typically have hyperparameters that adjust their strength, and the ordinary, unregularized solution is obtained as the regularization strength goes to zero. As a consequence, we can learn the proper level of regularization, and this might be seen as a weaker form of knowledge than imposed by non-negativity constraints. For example, we might impose an $\ell_1$ (i.e. lasso) penalty on the parameters if we believe that they're sparse, but we can learn the proper penalty strength if we don't know how sparse they are. And, we might discover in the end that they're not sparse at all (e.g. the optimal penalty strength is zero). Non-negativity constraints can certainly be combined with various types of regularization, as each corresponds to a different form of a priori knowledge about the solution. For example, if we believe the solution to be both non-negative and sparse, it would make sense to impose both $\ell_1$ and non-negativity constraints (e.g. see the non-negative lasso).
