[site]: crossvalidated
[post_id]: 114321
[parent_id]: 114184
[tags]: 
You can generalise the logistic regression model so that the latent distribution is something other than logistic. Using the t distribution lets you capture relationships where the data are contaminated, meaning observations from the "wrong" class appear unexpectedly far away from the decision boundary -- this would be the binary equivalent of fat tails in regression with a continuous response. IOW, you model $$ \begin{align} y_i & = \begin{cases} 1 & \text{if } z_i > 0 \\ 0 & \text{if } z_i where the latent errors $\epsilon$ are distributed as $$\epsilon_i \sim t_\nu \left(0, \frac{\nu-2}{\nu} \right) $$ with $\nu > 2$ estimated from the data. This is also called robit regression. In particular see pp.124-125 of Gelman & Hill's Data Analysis Using Regression And Multilevel/Hierarchical Models (what a mouthful); the above is eqn 6.15 in the book. You can fit it in R using package stan (which is also by Andrew Gelman and his collaborators and goes with the book), list and maybe others. You could also write code to maximise the log-likelihood directly using optim or nlminb ; I haven't tried it, but it shouldn't be hard.
