[site]: crossvalidated
[post_id]: 76747
[parent_id]: 76723
[tags]: 
MCMC changed Bayesian inference forever, allowing us to go well beyond inference based on conjugate priors. We may see MCMC as a set of tools that allows us to explore any posterior distribution, even when we don't know the analytic expression of its normalization constant (this is analogous to those situations in Physics in which we don't have an analytic expression for the partition function). MCMC gives you a sample of (generally dependent) parameter values from the posterior distribution. From this sample, with the help of the ergodic theorem, any posterior summary can be computed, such as the posterior expectation, a credible interval (more generally, a credible set), or the posterior probability of events involving any (measurable) transformation of the parameters. The existence of a plethora of MCMC methods is due to the fact that, although algorithms like Metropolis-Hastings are pretty much universal, particular algorithms may perform better in specific scenarios. To develop a taste for the need and importance of MCMC, I would do all the possible computations with conjugate priors (books by Robert, Lee, De Groot and Schervish, and others are good starting points), and then proceed slowly to more general problems (Robert and Casella is the best MCMC reference that I have seen so far).
