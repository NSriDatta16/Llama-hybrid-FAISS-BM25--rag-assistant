[site]: crossvalidated
[post_id]: 422473
[parent_id]: 422398
[tags]: 
The Wikipedia definition is ambiguous, but there are ways it can be interpreted to justify your conclusion. To begin, here's the definition: Let ${\mathcal {H}}$ be a space of functions $f:X\to Y$ called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Although fitting a linear regression model with Ordinary Least Squares does not require a "search," it can be conceived of as one, because the solution minimizes a quantity and therefore the "space of functions" must correspond to the domain of that quantity. Specifically, consider the problem of minimizing $$\sum_{i=1}^n|y_i - (x_{i1}\beta_1 + \cdots + x_{ip}\beta_p)|^2$$ by varying the real-valued parameters $\beta_j.$ In this case the search space is the set of all $(\beta_1, \ldots, \beta_p)$ and it naturally has a vector space structure, whence we may identify it with the vector space $\mathbb{R}^p$ (not just the set $\mathbb{R}\times \cdots \times \mathbb{R}$ ). So in what sense can this be considered a "space of functions"? One way is to identify each $\beta = (\beta_1, \ldots, \beta_p)\in\mathbb{R}^p$ with the linear transformation $$T_\beta: \mathbb{R}^{n} \oplus \cdots \oplus \mathbb{R}^n\to \mathbb{R}^n,\quad T_\beta(\mathbf{x}_1,\ldots, \mathbf{x}_p) = \beta_1 \mathbf{x}_1 + \cdots \beta_p \mathbf{x}_p.$$ (If you like, upon choosing a basis for $\mathbb{R}^n$ this can be represented as a set of $n \times np$ matrices. It forms a $p$ -dimensional sub-vector space of the $n^2p$ -dimensional space of such matrices.) This makes sense when your objective is to predict the values of $y$ for "future" values of the regressor variables $\mathbf{x}_j$ : that is, for arbitrary values that likely have not appeared in the dataset. Conceptually, the regression consists of finding such a linear transformation (or "hypothesis") that best fits the data, which are viewed as a noisy sample from the graph of this transformation. Although this is not the traditional statistical viewpoint (which focuses on estimating and interpreting $\beta$ ), it is appropriate for the usual machine learning objective of finding a good predictor. Here is a concrete example to cement the ideas. Consider the design matrix $$X = (\mathrm{x}_1, \mathrm{x}_2) = \pmatrix{1&0\\1&1\\1&2}$$ with $n=3$ observations and $p=2$ variables. Using the usual basis $\mathcal{E}=(\mathrm{e}_1, \mathrm{e}_2, \mathrm{e}_3)$ of $\mathbb{R}^3$ where $\mathbb{e}_1 = (1,0,0)^\prime,$ etc , we may construct a basis $\mathcal{F}=(\mathrm{f}_1, \ldots, \mathrm{f}_6)$ for $\mathbb{R}^3\oplus \mathbb{R}^3$ by setting $\mathrm{f}_1 = (\mathrm{e}_1^\prime; \mathrm{0}^\prime)^\prime = (1,0,0,0,0,0)^\prime,$ and so on through $\mathrm{f}_6 = (\mathrm{0}^\prime; \mathrm{e}_3^\prime) = (0,0,0,0,0,1)^\prime.$ Consequently $$\mathrm{x}_1 = 1\,\mathrm{f}_1 + 1\,\mathrm{f}_2 + 1\,\mathrm{f}_3;\quad \mathrm{x}_2 = 0\,\mathrm{f}_4 + 1\,\mathrm{f}_5 + 2\,\mathrm{f}_6.$$ Because the model sends $(\mathrm{x}_1;\mathrm{0})$ to the vector $(\beta_1,\beta_1,\beta_1)^\prime = \beta_1\mathrm{e}_1 + \beta_1 \mathrm{e}_2 + \beta_1\mathrm{e}_3$ and $(\mathrm{0},\mathrm{x}_2)$ to $(0,\beta_2,2\beta_2)^\prime = \beta_2\mathrm{e}_2 + 2\beta_3\mathrm{e}_3$ , for arbitrary $(z_1,\ldots,z_6)\in\mathbb{R}^6$ we obtain $$T_\beta(z_1 \mathrm{f}_1 + z_2 \mathrm{f}_2 + \cdots + z_6 \mathrm{f}_y) = \beta_1 z_1\mathrm{e}_1 + \beta_1 z_2\mathrm{e}_2 + \beta_1 z_3\mathrm{e}_3 + \beta_2 z_5 \mathrm{e}_2+ 2\beta_2 z_6\mathrm{e}_3,$$ whence its matrix representation is $$\operatorname{Mat}(T_\beta,\mathcal{F},\mathcal{E}) = \pmatrix{\beta_1&0&0&0&0&0 \\ 0&\beta_1&0&0&\beta_2&0\\0&0&\beta_1&0&0&2\beta_2}.$$ The set of such matrices clearly forms a two-dimensional subspace of the $18$ dimensional space of all matrices (under the usual addition and scalar multiplication of matrices). There is your $\mathbb{R}^2.$
