[site]: crossvalidated
[post_id]: 523745
[parent_id]: 523739
[tags]: 
The way all the machine learning algorithms work is that they learn the patterns from the data and then interpolate or extrapolate using them. If there are no patterns, thereâ€™s nothing to learn. For a simple problem like this, you could use simple linear regression , a model defined as $$ y = \alpha + \beta x + \varepsilon $$ where $\alpha$ is the intercept, $\beta$ is the slope, and $\varepsilon$ is the noise term. The model can detect only a very simple relationship, a linear one. If there is no trend, the slope is flat, hence $\beta$ would be equal to zero. In such a case, you would be left with a constant $\alpha$ that, because of using the squared loss function, would be equal to the mean of $y$ 's. In such a case, no matter what $x$ is, you are predicting the same value for $y$ The question I would ask is: how do you know there are no patterns in the data? Some patterns may not be obvious at first sight. Start with plotting $x$ vs $y$ , are the values just scattered all over the plot with no regularity whatsoever? If there aren't, you can try using machine learning algorithms, which are designed to detect such patterns. On another hand, if your dataset is small, then the algorithms may be prone to overfitting and detecting dubious patterns, in such a case, the simple approach as described above may be a safer solution.
