[site]: crossvalidated
[post_id]: 160892
[parent_id]: 160571
[tags]: 
Thank you for making me read this article :) @Did I understand the algorithm correctly? As I read the article. DRF deviates from RF in two ways. First the number of tried features(called K) in each node is selected either: a) randomly chosen from 1 to M features(M= total number features) with equal probability, or b) K is sampled from a normal distribution with mean=M^.5 and standard deviation=M/50 [and probably some rounding of numbers... and why 50?]. Thus either a) K is uniform sampled or b) K is often something close to default mtry=M^.5. Whether a or b, is decided on some information gain criteria which is not described very well. It is not described in his referenced earlier article (18) where K always is chosen as situation a. It may be described in his thesis (17), but that is written in French so I'm lost there. Ok, secondly DRF will re-evaluate total OOB-CV accuracy after each tree trained. After bootstrap, those samples being inbag will recieve a weighting(D) there inverse proportional to OOB-CV accuracy of that sample. You ask if this weighting(D) is implemented by "bootstrapping the bootstrap" accordingly to this weightng(D). That would effectively be the same as down-sampling of already predicted training examples. I think, this is not what is referred to, although I agree it is not very clearly stated. I think the weightings is used when computing gini impurity, such that highly weighted samples will have higher leverage. I speculate down-sampling and "sample-weighting" would work equally well. Because "down-sampling" and "sample-weighting" work equally well to counter class unbalanced training data, see this article . @When you are finished, if we give the forest a new input x, how do we decide its class? By majority vote? Well that would be default. Other voting regimes could be used as in any other forest, see e.g. this answer . Basicly the forest left with after training is no different from regular random forest. Only the method of how splits were made differs. xi will run through all trees and the terminal predictions will be some kind of vote decide the class. Lastly an unaccounted opinion. Boosting is a paper tiger, strong but fragile. It is likely the extra of boosting will outperform(meassured by CV) regular RF or the mtry chosen in random implementation. If the true hidden structure, our model have learned to replicate is not static. Then, when sampling a entirely new data set, this data set will represent a slightly changed structure, and for such problems classic RF is likely to yield more stable predictions. At best this boosting-bagging hybrid DRF may have overcomed this boosting Achilles heel. I think DRF is difficult to reproduce from this article alone, because the a-b criterion is not well described and neither is the weighting scheme. An actual implementation should have been submitted with the article.
