[site]: crossvalidated
[post_id]: 570172
[parent_id]: 
[tags]: 
Bootstrap optimism corrected - results interpretation

I came to know about Bootstrap validation approach for data poor settings. Currently, my problem is binary classification with 977 records and 6 features. class ratio is 77:23. Model is random forest As my dataset is small, I learnt that regular train, test split may not be a good approach and that's when I came to know about bootstrap optimism corrected approach. I am using the code from here to adapt to my random forest model. Thanks to @Demetri Pananos I used, n_bootstraps = 500 and scoring metric is f1. I will update my metric to custom metric later. Now, my objective is to understand bootstrap optimism and interpreting their results. So, I simply ran two seperate two jupyter motebooks. One with random hyperparameters of random forest and other with best hyperparameters of random forest (best params were found from regular train and validated on test split) execution 1 - best hyperparameters Optimism Corrected: 0.58 regular cv: 0.48 Wall time: 9min 49s Brier score loss = 0.18061299051614899 AUC = 85 MCC = 50 And the calibration curve looks like below execution 2 - random hyperparameters Optimism Corrected: 0.65 regular cv: 0.47 Wall time: 9min 49s Brier score loss = 0.12090 AUC = 93 MCC = 68 and the calibration curve looks like below As my data is imbalanced and I want better estimates upper estimates (ex: 0.7,0.8,0.9 etc), should I go with execution 2 though it is done using random hyperparameters? brier score loss is low for execution 2 from the calibration curve, it doesn't seem like overfitting but the difference in scores between optimisim bootstrap and regular cv makes me think that it is overfitting. Why is the overfitting pattern not shown in calibration curve when bootstrap optimism and cv score difference is huge? Am I interpreting the right way? update - best hyperparameter code
