[site]: crossvalidated
[post_id]: 214036
[parent_id]: 214024
[tags]: 
No, it isn't. Remember the definition of a proper scoring rule : it is a loss function of the actual outcome and your predictive density (in your case of a binary classification, the predictive density is just an estimated $p$), which is minimized in expectation by the true future density. (So, to be in exact accordance with the definition of a scoring rule, $S$ should be a function of an observed outcome , not the unobservable probability $c$ - but $c$ will come in when you average over many observed outcomes, so it doesn't really matter.) Note that a proper scoring rule needs to be minimized , not maximized, so your argmax is irrelevant. However, even putting a minus sign in front of your rule won't help. We can simply calculate the derivative of $S$ with respect to $p$ and check whether it is zero if $p=c$: $$ \frac{\partial S}{\partial p} = \frac{1}{\log 2}\bigg(\frac{c}{p+1}-\frac{1-c}{2-p}\bigg),$$ so $$ \frac{\partial S}{\partial p}(c,c) = \frac{1}{\log 2}\bigg(\frac{c}{c+1}-\frac{1-c}{2-c}\bigg),$$ which is nonzero for $c\neq\frac{1}{2}$. In fact, the partial derivative above is zero if and only if $$ p = \frac{1-3c}{c-3} $$ (barring any errors I made), which is usually not solved for $p=c$ and will actually be negative if $c Finally, you can plot $S$ as a function of $p$ for given $c$. For instance, for $c=\frac{1}{4}$: SS Again, this is not minimized for $p=c=\frac{1}{4}$.
