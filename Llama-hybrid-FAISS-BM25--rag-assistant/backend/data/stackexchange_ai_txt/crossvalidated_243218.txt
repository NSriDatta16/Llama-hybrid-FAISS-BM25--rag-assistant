[site]: crossvalidated
[post_id]: 243218
[parent_id]: 243184
[tags]: 
A paper by Lin and Tegmark nicely summarizes the reasons why lognormal and/or markov process distributions fail to fit data displaying critical, power law behaviors... https://ai2-s2-pdfs.s3.amazonaws.com/5ba0/3a03d844f10d7b4861d3b116818afe2b75f2.pdf . As they note, "Markov processes...fail epically by predicting exponentially decaying mutual information..." Their solution and recommendation is to employ deep learning neural networks such as long-short-term memory (LSTM) models. Being old school and neither conversant nor comfortable with NNs or LSTMs, I will give a tip of the hat to @glen_b's nonlinear approach. However, I prefer more tractable and readily accessible workarounds such as value-based quantile regression. Having used this approach on heavy tailed insurance claims, I know that it can provide a much better fit to the tails than more traditional methods, including multiplicative, log-log models. The modest challenge in using QR is finding the appropriate quantile around which to base one's model(s). Typically, this is much greater than the median. That said, I don't want to oversell this method as there remained significant lack of fit in the most extreme values of the tail. Hyndman, et al ( http://robjhyndman.com/papers/sig-alternate.pdf ), propose an alternative QR they term boosting additive quantile regression . Their approach builds models across a full range or grid of quantiles, producing probabilistic estimates or forecasts which can be evaluated with any one of the extreme value distributions, e.g., Cauchy, Levy-stable, whatever. I have yet to employ their method but it seems promising. Another approach to extreme value modeling is known as POT or peak over threshold models. This involves setting a threshold or cut-off for an empirical distribution of values and modeling only the largest values that fall above the cutoff based on a GEV or generalized extreme value distribution. The advantage to this approach is that any possible future extreme value can be calibrated or located based on the parameters from the model. However, the method has the obvious disadvantage that one is not using the full PDF. Finally, in a 2013 paper, J.P. Bouchaud proposes the RFIM (random field ising model) for modeling complex information displaying criticality and heavy tailed behaviors such as herding, trends, avalanches, and so on. Bouchaud falls in a class of polymaths that should include the likes of Mandelbrot, Shannon, Tukey, Turing, etc. I can claim to be highly intrigued by his discussion while, at the same time, being intimidated by the rigors involved with implementing his suggestions. https://www.researchgate.net/profile/Jean-Philippe_Bouchaud/publication/230788728_Crises_and_Collective_Socio-Economic_Phenomena_Simple_Models_and_Challenges/links/5682d40008ae051f9aee7ee9.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail
