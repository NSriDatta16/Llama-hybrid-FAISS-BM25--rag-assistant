[site]: crossvalidated
[post_id]: 157848
[parent_id]: 67547
[tags]: 
Gamma regression is in the GLM and so you can get many useful quantities for diagnostic purposes, such as deviance residuals, leverages, Cook's distance, and so on. They are perhaps not as nice as the corresponding quantities for log-transformed data. One thing that gamma regression avoids compared to the lognormal is transformation bias. Jensen's inequality implies that the predictions from lognormal regression will be systematically biased because it's modeling transformed data rather than the transformed expected value. Also, gamma regression (or other models for nonnegative data) can cope with a broader array of data than the lognormal due to the fact that it can have a mode at 0, such as you have with the exponential distribution, which is in the gamma family, which is impossible for the lognormal. I have read suggestions that using the Poisson likelihood as a quasi-likelihood is more stable. They're conjugates of each other. The quasi-Poisson also has the substantial benefit of being able to cope with exact 0 values, which trouble both the gamma and, especially, the lognormal.
