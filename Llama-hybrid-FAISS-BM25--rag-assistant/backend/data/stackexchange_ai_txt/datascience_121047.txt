[site]: datascience
[post_id]: 121047
[parent_id]: 
[tags]: 
Minimizing the difference between two distributions with TensorFlow

Suppose that the encoding neural network of a variational autoencoder (VAE) outputs a distribution from which latent samples will be drawn. To do this, the layer tfp.layers.MultivariateNormalTriL is used. A decoding neural network takes this latent sample z and attempts to reconstruct the input ( x ) passed to the encoder. In this VAE adaptation, the reconstructed input x_tilde is then again passed to the encoder, resulting in another latent sample z_tilde . The goal of this VAE is to minimize the difference between z and z_tilde . This has readily been attempted by Gauerhof & Gu , who named this technique the "reverse VAE". However, to minimize the aforementioned difference, they merely minimize the mean squared error (MSE) between the latent samples and try to reduce the variance of the latent samples to 1. Considering the output of tfp.layers.MultivariateNormalTriL is a distribution, it seems that we are able to use more accurate probabilistic methods than merely reducing the MSE between samples of the distribution. To attempt this, currently I am trying to maximize the covariance between distribution z and z_tilde , but I am unsure whether this is a robust technique mathematically. Another possibility would be minimizing KL-divergence, but somehow D_kl(z||z_tilde) returns extremely high values (even infinity) when using tfp.distributions.kl_divergence . TLDR: Is there a proven or correct deep learning method for minimizing the difference between two distributions?
