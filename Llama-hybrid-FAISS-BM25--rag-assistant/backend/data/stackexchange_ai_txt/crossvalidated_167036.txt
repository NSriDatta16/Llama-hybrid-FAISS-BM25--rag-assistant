[site]: crossvalidated
[post_id]: 167036
[parent_id]: 
[tags]: 
How effective is randomly/algorithmically generating exorbitant amounts of training data for a neural network?

There are some problems of which the generating of data is easy whereas the inverse is not. For example, use a 3D game engine to render some randomly generated objects with some random changes and place them throughout the scene. We already know the positions of these objects so these are the "answers" when training the neural net, and the "input" would be the rendered pixels at a particular viewpoint. For example, use a speech synthesis algorithm with randomized formant, pitch, EQ, speed, random reverb/noise/distortion, as well as randomized words as input. The "answer" is the input to the speech synthesis algorithm, and the "input" to the neural net is the output of the speech synthesis algorithm. For example, in the "blind source separation" problem, pick a ton of sounds from Youtube, or freesound, and randomly add them together in resulting sound files (preferably each source audio would have a single source in real life, like one person's voice). The resulting summed cacophonies will be training data, and the original signals are ground truth. Is there a name to technique? What is it called? Has it ever been implemented successfully without overfitting? What kind of overfitting problems would result?
