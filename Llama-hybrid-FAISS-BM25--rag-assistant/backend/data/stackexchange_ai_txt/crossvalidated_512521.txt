[site]: crossvalidated
[post_id]: 512521
[parent_id]: 512519
[tags]: 
There are two problems going on here. The first is general for very high dimensional optimisation. Think of a one-dimensional minimisation problem, minimising $f(x)$ . If you use gradient descent, you take $$x_{k+1}\rightarrow x_k-\epsilon f'(x_k)$$ and the problem is now to choose $\epsilon$ . It's not trivial, because even if you take $g(x)=cf(x)$ or $h(x)=f(k\times(x-x_{opt}))$ , simply rescaling the input or output variables, good values of $\epsilon$ will change. One way to choose $\epsilon$ is to use (an approximation to) $f''(x)$ , as in Newton-type methods. $$x_{k+1}\rightarrow x_k-\frac{ f'(x_k)}{f''(x_k)}$$ Or you might have some idea from domain knowledge about the correct scaling of the problem. But in deep neural networks you can't afford the computation to get the second derivative. So, a Newton-type algorithm might be ok if you could afford one, but you can't. And part of the point is avoiding manual feature engineering, so scaling by hand is out. On top of that, there's a problem with the specific structure of deep neural networks as a feed-forward network of individual nodes. From the chain rule, the derivative with respect to a parameter at layer $i$ involves the product of per-node derivatives at all layers from $i$ onwards. If the derivative at a later layer is near zero, the derivative wrt layer $i$ parameters will also be near zero even if those parameters are far from their optima. The later-layer node may be close to its optimum, but it also may have derivative close to zero because it's far from its optimum and so it isn't sensitive to its inputs. Having a derivative near zero is necessary for being near the optimum of a smooth function, but it is not sufficient. The classical sigmoid activation function has this problem in both tails: it flattens out for large positive and negative inputs and has exponentially small derivative with increasing distance from the optimum. The arctanh function has it slightly less serously (by a constant factor). The ReLU activation has it much more severely in the lower tail but doesn't have it in the upper tail. The leaky ReLU has the problem less severely.
