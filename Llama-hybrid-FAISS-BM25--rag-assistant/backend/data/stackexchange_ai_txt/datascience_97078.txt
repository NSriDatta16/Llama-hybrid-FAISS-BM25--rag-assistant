[site]: datascience
[post_id]: 97078
[parent_id]: 97052
[tags]: 
i) Times series learn to predict values based on many past sequences. Like other ML models, they use training and validation datasets. Consequently, y(t) is your objective data. You can either predict t+1 or several steps in the future, but in general, the further you predict, the worse is the forecast. ii) It depends on the model you use. LSTMs are designed to predict many different dynamic behaviors. On the other hand, ARIMA is a pure statistical forecasting model, and is more limited in time range than LSTM and cannot always detect seasonality (use SARIMA instead). RNNs are well suited for small data sets. Many forecasting models are sensitive to noise, reducing it could improve the results. I recommend to study their publication, they are very interesting. For instance, LSTM uses about 10 different learning algorithms. https://www.researchgate.net/publication/13853244_Long_Short-term_Memory iii) Use models that are easy to implement first, like Random Forest. Then increase the complexity using combined models like SARIMA, GRU or LSTM. Depending on your data, some models will perform best than other, so it is a good idea to do a test bench to monitor several forecasting models. Here is a good notebook that explains time series using RNN, LSTM and GRU: https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb
