[site]: crossvalidated
[post_id]: 90686
[parent_id]: 90659
[tags]: 
Why is the AUC for A better than B, when B "seems" to outperform A with respect to accuracy? Accuracy is computed at the threshold value of 0.5. While AUC is computed by adding all the "accuracies" computed for all the possible threshold values. ROC can be seen as an average (expected value) of those accuracies when are computed for all threshold values. So, how do i really judge/compare the classification performances of A and B? I mean, do i use the AUC value? do i use the acc value? and why? It depends. ROC curves tells you something about how well your model your model separates the two classes, no matter where the threshold value is. Accuracy is a measure which works well usually when classes keeps the same balance on train and test sets, and when scores are really probabilities. ROC gives you more hints on how model will behave if this assumption is violated (however is only an idea). furthermore, when i apply proper scoring rules to A and B, B outperforms A in terms of log loss, quadratic loss, and spherical loss (p I do not know. You have to understand better what you data is about. What each model is capable to understand from your data. And decide later which is the best compromise. The reason why that happens is that there is no universal metric about a classifier performance. The ROC graph for A looks very smooth (it is a curved arc), but the ROC graph for B looks like a set of connected lines. why is this? That is probably because the bayesian model gives you smooth transitions between those two classes. That is translated in many threshold values. Which means many points on ROC curve. The second model probably produce less values due to prediction with the same value on bigger regions of the input space. Basically, also the first ROC curve is made by lines, the only difference is that there are so many adjacent small lines, that you see it as a curve.
