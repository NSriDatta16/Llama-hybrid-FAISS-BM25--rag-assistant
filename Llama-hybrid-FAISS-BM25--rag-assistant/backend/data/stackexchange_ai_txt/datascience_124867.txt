[site]: datascience
[post_id]: 124867
[parent_id]: 124865
[tags]: 
Welcome to the DataScience stack exchange. ReLu is not "correct" or "incorrect" but it is just one of several popular choices for a nonlinearity in neural networks. It sounds like either you're misunderstanding something , and/or you need to clarify your question. But I'll do my best to clarify some things. gradient for the filter weights of this layer ReLu layers don't have weights, but you probably mean the weights of some layer prior to the ReLu. This means that the gradient is also positive It's true that the gradient of a ReLu layer with respect to its input is always positive (equal to one) or zero. which means that the filter weights are always decreasing. Sounds like you're talking about using backprop to update weights "through" a ReLu layer, and that statement is not correct. Filter weights can decrease because during backprop, one computes the gradient of a loss function with respect to its weights . That is not the same as the gradient of the ReLu with respect to its inputs.
