[site]: crossvalidated
[post_id]: 625461
[parent_id]: 625186
[tags]: 
This is going to be hard. In this answer of mine, I give a situation where the two categories have dramatically different distributions, but only when the entire distribution in two dimensions is considered. Just considering the marginal distributions leads to viewing the two categories as having the same distribution, even though you can see the red and blue to be extremely different. That’s in two dimensions. If there is a third feature, it would be possible to have the distributions differ in three dimensions yet have identical univariate and bivariate marginal distributions. However, it is kind of possible to plot in three dimensions, so you might be able visualize such a situation to assess differences. You don’t have three dimensions. You have $42$ , which means that the $41$ -dimensional margins could be identical for both classes, yet, analogous the the situation above with the X-shape, the two distributions do differ dramatically in the full $42$ dimensions. I see a few approaches. If you fit an extremely flexible model like a neural network, it can pick up on the funky behavior in many dimensions. If such a model fails to distinguish between the categories, then perhaps the categories have similar distributions. The trouble with such an approach is that you have to combat overfitting concerns. (It is also somewhat circular reasoning to wonder if a predictive model can distinguish between the categories and test this out by fitting a predictive model.) It is theoretically possible to calculate the mutual information between your feature vectors in $42$ dimensions and your class labels, such as with the JMI (jackknife mutual information) package in R . From my experience using that package, however, such a calculation will be prohibitively slow with the sample size you have. Dimension reduction algorithms like t-SNE and UMAP aim to reduce your features to just a few dimensions (say two for visualization) while maintaining separability characteristics from the high-dimension space. For instance, I have run t-SNE on the MNIST digit pixel values. When I plotted the results in two dimensions with a separate color for each digit, the categories all seemed to bunch together, consistent with the digits being easy to distinguish. An issue with this approach is that the dimension reduction algorithms like t-SNE risk introducing separability in low dimensions when there is not any in the full space, and separability can be destroyed in the low-dimensional space even though it does exist in the high-dimensional space. Finally, you mention clustering. This is an intriguing idea. The issue I see is that the clusters might be wildly different from the category labels. The image below demonstrates a situation where this may arise. The data come from a geyser in Yellowstone National Park and have a decent cluster in the lower left and another in the upper right. A clustering algorithm should be able to find those clusters. However, if you are using the two variables to predict the color, such a clustering algorithm is tricking you into thinking the color relates to those two variables. (Yes, you can see in this plot that the color has nothing to do with these two features, but this is just in two dimensions. As before, you have $42$ dimensions, meaning that this kind of easy visualization is off the table, unless you do a dimension reduction whose possible issues I discussed in point $\#3$ .) That is a far different situation than in the image below, where the color is predicted quite well by the two features. # To generate the Yellowstone plots # To generate the Yellowstone plot library(ggplot2) library(MASS) data("faithful") X1 $eruptions X2 waiting Y $X1 - mean(d$ X1))/sd(d $X1) + (d$ X2 - mean(d $X2))/sd(d$ X2) p
