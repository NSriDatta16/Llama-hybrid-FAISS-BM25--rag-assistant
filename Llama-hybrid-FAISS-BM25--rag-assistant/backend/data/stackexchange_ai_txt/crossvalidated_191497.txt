[site]: crossvalidated
[post_id]: 191497
[parent_id]: 191396
[tags]: 
From Wikipedia: The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, and other areas for many decades and is increasingly used in machine learning and data mining research. The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes. You can think of the two axes as costs that must be incurred in order for the binary classifier to operate. Ideally you want to incur as low a false positive rate as possible for as high a true positive rate as possible. That is you want the binary classifier to call as few false positives for as many true positives as possible. To make it concrete imagine a classifier that can detect whether a certain disease is present by measuring the amount of some biomarker. Imagine that the biomarker had a value in the range 0 (absent) to 1 (saturated). What level maximises detection of the disease? It might be the case that above some level the biomarker will classify some people as having the disease yet they don't have the disease. These are false positives. Then of course there are those who will be classified as having the disease when they do indeed have the disease. These are the true positives. The ROC assesses the proportion of true positives of all positives against the proportion of false positives by taking into account all possible threshold values.
