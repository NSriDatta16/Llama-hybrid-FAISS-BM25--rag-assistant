[site]: crossvalidated
[post_id]: 277642
[parent_id]: 
[tags]: 
Parallel gradient descent problem

I'm new to ML so please go gentle on me in case I was missing something obvious. I read that GD on parallel machines can be done by splitting points and then averaging the results. However, consider this example: Applying GD on: (2,1),(3,2),(4,3) gives theta0 = -0.997784, theta1 = 0.999308 Applying GD on: (4,5),(5,6),(6,7) gives theta0 = 0.944353, theta1 = 1.010848 However, applying an average gives theta0 ~ 0 , theta1 ~ 1 But when GD is applied on the whole data-set I get theta0 = -2.399684, theta1 = 1.599928 Any ideas on what I'm missing here?
