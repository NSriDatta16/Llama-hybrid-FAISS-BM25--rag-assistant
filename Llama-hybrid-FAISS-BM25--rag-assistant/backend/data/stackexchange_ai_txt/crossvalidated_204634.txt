[site]: crossvalidated
[post_id]: 204634
[parent_id]: 
[tags]: 
Divergence in gradient descent

I am trying to find a function h(r) that minimises a functional H(h) by a very simple gradient descent algorithm. The result of H(h) is a single number. (Basically, I have a field configuration in space and I am trying to minimise the energy due to this field). The field is discretized in space, and the gradient used is the derivative of H with respect to the field's value at each discrete point in space. I am doing this on Mathematica, but I think the code is easy to follow for non-users of Mathematica. The function Hamiltonian takes a vector of field values, the spacing of points d, and the number of points imax, and gives the value of energy. EderhSym is the function that gives a table of values for the derivatives at each point. I wrote the derivative function manually to save computation time. (The details of these two functions are probably irrelevant for the question). Hamiltonian[hvect_, d_, imax_] := Sum[(i^2/2)*d*(hvect[[i + 1]] - hvect[[i]])^2, {i, 1, imax - 1}] + Sum[i^2*d^3*Potential[hvect[[i]]], {i, 1, imax}] EderhSym[hvect_,d_,imax_]:=Join[{0},Table[2 i^2 d hvect[[i]]-i(i+1)d hvect[[i+1]] -i(i-1)d hvect[[i-1]]+i^2 d^3 Vderh[hvect[[i]]], {i, 2, imax - 1}], {0}] The code below shows a single iteration of gradient descent. hvect1 is some starting configuration that I have guessed using physical principles. Ederh = EderhSym[hvect1, d, imax]; hvect1 = hvect1 - StepSize*Ederh; The problem is that I am getting random spikes in the derivative table that grow and cause an overflow. Here is a plot of the derivative at the start, and after a few iterations (the number of iterations before overflow depends on the step size). The spikes keep growing until there is an overflow. I have tried changing the step size, I have tried using moving averages, low pass filters, Gaussian filters etc. I still get spikes that cause an overflow. Has anyone encountered this? Is it a problem with the way I am setting up gradient descent? Edit 1: It seems to be working with a much smaller step size than I had previously tried (although convergence is slow). I believe that was the problem, but I do not see why the divergences are localised at particular points. Edit 2: I am testing my gradient descent code as I will have to adapt it to a different multivariable Hamiltonian where I do the following iteratively, to find a saddle point instead. (n is an appropriately chosen small number, 20 in my case). This makes it difficult to use adaptive step sizes. Otherwise, updating step sizes would be the best course of action. For[c = 1, c
