[site]: crossvalidated
[post_id]: 609973
[parent_id]: 609933
[tags]: 
A feature transformation can be "learned" by fitting a neural network. For example, if the original features $x \in \mathbb{R}^d$ are mapped to one of $k$ classes, the mapping may be modeled using a neural network $f$ with one hidden layer (omitting bias parameters): $$ \begin{equation*} f(x; W_1, W_2) = \sigma(W_2 \text{relu}(W_1x)) \end{equation*} \\ \text{where } W_1 \in \mathbb{R}^{h \times d}, W_2 \in \mathbb{R}^{k \times h}. $$ You may very well set $h$ to be greater than $d$ . After fitting $f$ on a bunch of training data, the function $$ \begin{equation*} g(x; W_1) = W_1x \end{equation*} $$ outputs a vector in $\mathbb{R}^{h}$ which may encode information about how a given $x$ relates to the $k$ classes. This information may be treated as a more abstract or "transferrable" representation of $x$ for future tasks. Word2vec is a concrete example of this idea: an $x$ is a "one-hot" vector indicating which word in the vocabulary it is, and a neural network is learned to (literally) assign any word to a vector. This vector abstractly represents something about the "meaning" of that word. (In a literal sense, a Word2vec model doesn't necessarily increase the dimensionality of the original one-hot vector. But it illustrates the point that transformations to a vector space with any dimension can be learned.) These vectors can be used to numerically encode wordy features in external datasets, or to initialize the first layer of a language model.
