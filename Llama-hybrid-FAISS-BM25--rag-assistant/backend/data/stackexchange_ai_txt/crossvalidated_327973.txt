[site]: crossvalidated
[post_id]: 327973
[parent_id]: 327964
[tags]: 
I think you guessed it right- this is usually an example to show how overfitting/underfitting varies with choice of k. If you choose k=1, the algorithm will pick the neighbor as the point itself and have a extremely overfit decision boundary. When you do a leave one out cv the error you will thus obtain will be high. If you choose k=(no of data points), the decision boundary will be the average line for all your data points (extreme underfitting). In this case too you will obtain a high error.
