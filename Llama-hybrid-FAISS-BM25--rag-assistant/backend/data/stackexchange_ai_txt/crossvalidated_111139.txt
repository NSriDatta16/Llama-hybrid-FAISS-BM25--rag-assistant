[site]: crossvalidated
[post_id]: 111139
[parent_id]: 
[tags]: 
Is this poor transformation advice for predictive modeling?

I have gotten some advice from a PhD statistician on doing predictive modeling on large datasets (lots of variables AND lots of observations) that I should perform transformations to eliminate skewness in my numeric variables. I should first try various power transformations, then when that doesn't work, try rank transformations, then when that does not work to bin my variable into ordinal categories or in extreme cases (e.g. >95% of a variable is equal to zero) turn the variable into a nominal variable of some sort. This is all with the goal of being able to fit various types of models to the data (Neural Nets, SVMs, Logistic Regression, etc.) I am having trouble finding any advice backing this up. I have found that normalizing, scaling and box cox transformations seem to be common ways to improve models, but not this. Can someone help validate/invalidate this advice?
