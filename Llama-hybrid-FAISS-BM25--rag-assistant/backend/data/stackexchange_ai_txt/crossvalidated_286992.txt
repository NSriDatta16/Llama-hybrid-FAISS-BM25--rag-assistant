[site]: crossvalidated
[post_id]: 286992
[parent_id]: 286975
[tags]: 
Is it generally advised to remove redundant information (if highly correlated) when training neural networks? It depends. Is it necessary? No. Since a neural network with an appropriate architecture can model any (!) function, you can safely assume, that it also could first model the PAT and then do whatever it also should do -- e.g. classification, regression, etc. ( source ) and In principal, the linear transformation performed by PCA can be performed just as well by by the input layer weights of the neural network, so it isn't strictly speaking necessary ( source ) This is because Neural Nets could be used as a non-linear dimensionality reduction tool: High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors ( source ) In this context, it is also worth mentioning auto-encoders . Can it help? Yes, it speeds up things. However, as the number of weights in the network increases, the amount of data needed to be able to reliably determine the weights of the network also increases (often quite rapidly), and over-fitting becomes more of an issue (using regularisation is also a good idea). The benefit of dimensionality reduction is that it reduces the size of the network, and hence the amount of data needed to train it ( source ) Speed comes at a cost /slash/ bears a risk The disadvantage of using PCA is that the discriminative information that distinguishes one class from another might be in the low variance components, so using PCA can make performance worse ( source ) This might really be what you have experienced in your experiment.
