[site]: crossvalidated
[post_id]: 401482
[parent_id]: 313406
[tags]: 
You don't speak of likelihood of the data. Of the four possible ways to mix likelihood/probability and data/parameters, you speak of: either likelihood of the parameters (given the data) or probability or probability density of the data (given the parameters) In a Bayesian probability setting, where the parameters also follow a probability distribution, you may speak as well of (posterior/prior) probability of parameters. The difference depends on which part of the equation is considered fixed. Likelihood is considered a function of the parameters (ie theta) given a particular observation $x$ $$\mathcal{L}(\theta|x)$$ The vertical bar $|$ is used to say that $x$ is a fixed parameter and $\theta$ is variable (but of course, you can encounter different $x$ and it is only fixed for the particular case). The likelihood relates to inverse probability , which contrasts with the typical concept of probability that gives: The probability of observations given some set of parameters. This typical concept of probability is often a straightforward problem. The problem of inverse probability is to express: The probability (or something related) of the set of parameters given some observation. Which is not straightforward, or even an impossible (ill-defined) problem.
