[site]: crossvalidated
[post_id]: 322029
[parent_id]: 321711
[tags]: 
In our recent publication Kriegel, H. P., Schubert, E., & Zimek, A. (2017). The (black) art of runtime evaluation: Are we comparing algorithms or implementations? Knowledge and Information Systems, 52(2), 341-378. we discuss the performance (- differences ) of different algorithms for hierarchical clustering (for single-link there exist optimal $O(n)$ memory and $O(n^2)$ time algorithms). For other linkages, finding the optimal result supposedly is NP hard (i.e., $O(2^n)$). The standard algorithm is in $O(n^3)$, if you use a heap $O(n^2 \log n)$ but with very large constant factors that make these approaches not perform well in practice. There exist algorithms that often run $\sim n^2$, but the worst case remains $O(n^3)$ which is usually much better. Either way, except for SLINK (single-link only!) none of them scale well. As most require a distance matrix, implementations tend to run out of memory at $46341$, $2^{16}=65536$, or $65537$ instances due to (signed or unsigned) 32 bit integer overflows. It should be noted that at this point a $O(n^2)$ distance matrix with double precision needs 16 resp. 32 GB of RAM, and some implementations need multiple copies of a matrix... So if you wanted to go to say, one million points, hierarchical clustering (except single linkage) will be problematic. You can cite our publication to discuss that hierarchical clustering tends to run out of memory before 100.000 instances (which would be possible with just 38 GB of RAM, if you have a very careful implementation). At 1 million instances: 3.7 TB of RAM anybody? The fastest algorithms took about 1 minute for 100k instances with single-link (for other linkages, with a 64 bit implementation and enough memory, I would estimate 8-10 minutes). Compare this to k-means, where the fastest were 0.3 seconds (for k=10) to 10 seconds (for k=1000) and DBSCAN, where the fastest were 1.5 seconds - because of index acceleration. Note that our benchmarks also show a huge difference between different implementations. Code quality matters when benchmarking. You need to choose as large data as you can afford when investigating runtime behavior. If you look only at 1000 objects, you will get results that are completely different than with huge data, and you get a wrong story. And even with large data, it is easy to get misleading results.
