[site]: crossvalidated
[post_id]: 544843
[parent_id]: 544839
[tags]: 
When you fit a logistic regression, you do not get flat 0 predictions. Rather, you get probabilistic predictions, which are far more useful and informative than hard 0/1 predictions. Evaluate these using proper scoring rules . Do not blindly compare the predicted class membership probabilities to some threshold, unless you know what you are doing, and know that the threshold you are using is actually useful for your context. (Hint: a threshold of 0.5, or one that maximizes accuracy, will usually not be useful.) Weighting your model is really the same as oversampling the minority class, which will only bias your parameter estimates, see below. Don't do it, again, unless you know what you are doing. More information here: Classification probability threshold Are unbalanced datasets problematic, and (how) does oversampling (purport to) help? Why is accuracy not the best measure for assessing classification models?
