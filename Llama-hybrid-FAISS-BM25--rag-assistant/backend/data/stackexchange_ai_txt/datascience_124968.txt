[site]: datascience
[post_id]: 124968
[parent_id]: 
[tags]: 
why my training and validation loss curve looks like lognormal distribution?

I trained an XGBoost model and my training and validation curve looks like this? Is something weird I am doing? I have always seen it going from high to low or like a U-shape incase of overfitting. How come the model is better without any training (at iteration=0)?
