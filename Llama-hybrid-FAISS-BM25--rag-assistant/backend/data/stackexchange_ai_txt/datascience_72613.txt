[site]: datascience
[post_id]: 72613
[parent_id]: 
[tags]: 
Informed sender in Lazaridou et al. (2017) 's guessing game

I'm trying to implement the informed sender from Multi-Agent Cooperation and the Emergence of (Natural) Language ( Lazaridou et al., 2017 ). However I'm confused about the tensor shape in the forward pass. In the paper the network is described as following: The informed sender also first embeds the images into a “game-specific” space. It then applies 1-D convolutions (“filters”) on the image embeddings by treating them as different channels. The informed sender uses convolutions with kernel size 2×1 applied dimension-by-dimension to the two image embeddings (in Figure 1 , there are 4 such filters). This is followed by the sigmoid nonlinearity. The resulting feature maps are combined through another filter (kernel size f×1, where f is the number of filters on the image embeddings), to produce scores for the vocabulary symbols. Figure 1: So I assume the data should pass through the net as following: The input is 2 feature vectors of shape (None, n_features) They each get embedded via a Dense layer, output shape (None, n_game_features) Here I'm not sure but I assume that in Keras, before passing them to the convolution layer, I need to stack those two vectors, forming (None, 2, n_game_features) The vector passes through Conv1D(n_feature_filters, kernel_size=2, activation="sigmoid", data_format="channels_first") , outputting (None, n_feature_filters, n_game_features) . This passes through the vocabulary filter Conv1D(1, kernel_size=n_feature_filters, data_format="channels_first") , outputting (None, 1, n_game_features) . However, according to the paper, the output vector should be of length vocabulary_size which is independent on all the other parameters. Should I assume an additional Dense layer on the output , even though it was not mentioned in the paper? Or was there a mistake in my interpretation of the model specification?
