[site]: crossvalidated
[post_id]: 298432
[parent_id]: 298328
[tags]: 
Suppose I have a "big data" set, and in it I have the data on every event. Maybe it's a business with meticulous records on transactions. Then if I analyze the whole data set, and I get a 1% p-value, I don't interpret that as giving insight on the sample as before, I interpret it as saying there's a 1% chance that randomness in the data generating process has produced such a relationship throughout all the data. First , let's talk about what is p-value linked to. Let's say we're interested in a parameter that describes our data. I'll pick the simplest one, the mean. So, we calculated a mean using our favorite formula for averages: $$\bar x=\frac 1 n \sum_{i=1}^nx_i$$ If our data is a random sample from the population, then we'd use this formula as an estimator of the sample mean $$\hat\mu=\bar x$$ Note, that this is a very good estimator usually, it has all these nice properties we want from the estimators, which I'm not going to go through here. Now, notice that I market the estimator of the mean with a hat $\hat\mu$. The reason I'm doing this is because if there were another set of data that I could use, then the estimated value of the mean would have been different. So, the estimated value has uncertainty in it. We can use this value and the uncertainty to make statements about the population mean $\mu$, which we will never know, perhaps. So, we can make statements about how likely is the population mean $\mu$ (or true , in some sense, value) to be greater than ZERO, for instance. The statements are often accompanied by the p-values. Summarizing, p-values are used when we only have the random sample from the population, but want to make statements about the unknown population parameters while we only have their sample estimates. Now, if your data is the population , i.e. there is no other data set, the data that you have is all that is there and can exist, then you can calculate the population mean. There's no uncertainty anymore, you got $$\mu=\bar x$$ This is it, no need to guess or theorize about probabilities or likelihood, you know the population mean $\mu$. Anything you want to know about it is in its value. Hence, here we don't need p-values or hypotheses. Note, that "big data" does not mean population . It can simply be the big sample. In this case may look up "large sample statistics" keywords. Usually, p-values are associated with small samples or finite samples, while "big data" may fall into a part of statistics that deals with asymptotic and infinite samples. Also, small data set doesn't mean sample . For instance, if the question you asked was "what is the average grade in 7th grade in this particular school in Washington DC?", then you can go to school records and get grades of every student. You may get 100 observations comprising the full population , it's not a sample anymore. On the other hand, if your question was "what is the average grade of 7th graders in US?", then the same 100 records long data set is not a population anymore, but a sample. So, the fact that your data set is a random sample or the population is not defined by the size of your data set, but by the question asked, by how the data was collected, what it represents etc. Second , given what I just wrote, does your example actually represent the population? Likely, not. The fact that you brought up DGP (data generating process) usually means that this process generated a sample from the population that it could possibly generate. Otherwise, why would there be any uncertainty in it? If your data set contains every possible realization of the process, then you got the population and can calculate (as opposed to estimate ) the parameters of the process and don't need p-values anymore. Again, with DGP the bigness of your data will likely lead to asymptotic estimators working very well.
