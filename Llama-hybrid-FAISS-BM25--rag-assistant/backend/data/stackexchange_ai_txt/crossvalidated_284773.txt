[site]: crossvalidated
[post_id]: 284773
[parent_id]: 244907
[tags]: 
I have read your question and the answer above 2 times (1st time 3 month ago). I'm interested and also want to find the absolute appropriate way to do cross-validation for my data. After a lot of thinking & reading, it seems that I find the holes and here is my fix: To explain my confusion, let me try to walk through the model selection with nested cross validation method step by step. Create an outer CV loop with K-Fold. This will be used to estimate the performance of the hyper-parameters that "won" each inner CV loops. Use GridSearchCV to create an inner CV loop where in each inner loop, GSCV goes through all possible combinations of the parameter space and comes up with the best set of parameters. (Note: here I assume: data for inner loop = training data for outer loop. You may ask: why ? Answer: https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764 read the answer section by Vivek Kumar step 4) After GSCV found the "best parameters" in the inner loop (let's call it inner winner), it is tested with the test set in the outer loop to get an estimation of performance (let's call it outer_fold_score1). The outer loop then updates to the next fold as the test set and the rest as training set (to evaluate the "inner winner" on outer loop), the "inner winner" is tested again with the new test set (outer_fold_score2). Then again the outer loop updates to the next fold until the loop is completed. Scores from each fold (outer_fold_score 1,2..) will be average to get the score of the "inner winner" for the outer loop (outer_score) The outer loop then updates to the next fold as the test set and the rest as training set (to find the next "inner winner" , and 1- 4 repeats (note that when we repeat 1 we don't create the new K-fold but we use the same outer Kfold every time) . With each of 1-4 cycle, we get a "best parameters" (or "inner winner") with an outer_score. The one with the best outer_score will be the winner of the winners Reasoning: Basically your question concerns that there are many "winning parameters" for the outer loop. The thing is u didn't complete the outer loop to evaluate and find the "outer winner". Your 4th step only evaluate the "inner winner" in 1 fold in the outer loop, but u didn't "loop it". Therefore I need to replace it with my 4th step - "loop" the evaluation step in the outer loop and get the outer score (by averaging) Your 5th step did do the "looping" job in the outer loop, but it's just for building another "inner winner". It didn't loop the "evaluation" of this "inner winner" in the outer loop
