[site]: crossvalidated
[post_id]: 222415
[parent_id]: 222405
[tags]: 
Your example refers to likelihood principle that states that given a statistical model, all of the evidence in a sample relevant to model parameters is contained in the likelihood function, i.e. that we do not condition our estimates on unseen data. However it is not true that Bayesians are not interested in sampling distribution. In fact, knowledge about sampling distribution plays a pivotal role in defining your model as noticed by Gandenberger (2015): In practice, subjective Bayesians typically use methods that depend on the sampling distribution to estimate an expert’s $P_\text{old}(H)$, such as methods that involve fitting a prior distribution that is conjugate to the sampling distribution. Objective Bayesians use priors that depend on the sampling distribution in order to achieve some aim such as maximising a measure of the degree to which the posterior distribution depends on the data rather than the prior, as in the reference Bayesian approach (Berger [2006] p. 394). Some contemporary Bayesians (e.g. the authors of Gelman et al. [2003], pp. 157–96) also endorse model-checking procedures that violate the Likelihood Principle more drastically. It is worth noting that neither subjective nor objective Bayesians violate the Likelihood Principle in a different sense of ‘violates’ than the one used here, even when checking their models: they do not allow information not contained in the likelihood of the observed data to influence the inferences they draw conditional on a model (Gelman [2012]). But they generally do allow the sampling distribution of the experiment (for instance, whether the experiment is binomial or negative binomial) to influence their choice of a model, and thereby potentially influence the conclusions they reach. Gandenberger, G. (2015). A new proof of the likelihood principle. The British Journal for the Philosophy of Science, 66(3), 475-503.
