[site]: crossvalidated
[post_id]: 212688
[parent_id]: 212684
[tags]: 
The answer is yes, in this case, the standard errors are the same. Why: Your vector of estimates $\hat{\boldsymbol{\beta}} = \left[\begin{array}{c} \hat{\beta}_1\\ \hat{\beta}_2 \\ \hat{\beta}_3 \end{array} \right]$ of the true vector $\boldsymbol{\beta}$ will be a random variable. One of the key outputs of any regression will be an estimate of the covariance matrix of estimator $\hat{\boldsymbol{\beta}}$. That is: $$ Cov\left( \hat{\boldsymbol{\beta}} \right) \approx \Sigma \quad \quad \Sigma = \left[ \begin{array}{ccc} \Sigma_{1,1}, \Sigma_{1,2}, \Sigma_{1,3} \\ \Sigma_{2,1}, \Sigma_{2,2}, \Sigma_{2,3} \\ \Sigma_{3,1}, \Sigma_{3,2}, \Sigma_{3,3} \end{array} \right] $$ Your standard errors are the square root of the elements along the diagonal of the $\Sigma$ matrix. That is: $$ SE = \left[ \begin{array}{c} \sqrt{\Sigma_{1,1}} \\ \sqrt{\Sigma_{2,2}} \\ \sqrt{\Sigma_{3,3}}\end{array} \right] $$ Trivially we have that covariance of $\hat{\boldsymbol{\beta}} + \left[\begin{array}{c} 0 \\ 1 \\ 0\end{array} \right]$ is the same as the covariance of $\hat{\boldsymbol{\beta}}$ (i.e. both covariance matrices are $\approx \Sigma$, and hence the standard errors will be the same as well. Further note: A key step in the above argument is that adding a constant to a random variable does not change the variance. Quick sketch in the univariate case: $$ \begin{align*} Var(\tilde{x} + c) &= E[(\tilde{x} + c - E[\tilde{x} + c])^2] \\ &= E[(\tilde{x} + c - \left(E[\tilde{x}] + c\right) )^2] \\ &= E[(\tilde{x} - E[\tilde{x}] )^2] \\ &= Var(\tilde{x}) \end{align*}$$ Intuition: variance measures average square of deviations from the mean. Adding a constant to a random variable doesn't change deviations from the mean and hence doesn't change the variance.
