[site]: datascience
[post_id]: 74203
[parent_id]: 73605
[tags]: 
Here you can find the code to train an LSTM via keras and tune it via keras tuner, bayesian option: #2 epoch con 20 max_trials from kerastuner import BayesianOptimization def build_model(hp): model = keras.Sequential() model.add(keras.layers.LSTM(units=hp.Int('units',min_value=8, max_value=64, step=8), activation='relu', input_shape=x_train_uni.shape[-2:])) model.add(keras.layers.Dense(1)) model.compile(loss='mae', optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), metrics=['mae']) return model # define model bayesian_opt_tuner = BayesianOptimization( build_model, objective='mae', max_trials=20, executions_per_trial=1, directory=os.path.normpath('C:/keras_tuning'), project_name='timeseries_temp_ts_test_from_TF_ex', overwrite=True) EVALUATION_INTERVAL = 200 EPOCHS = 2 bayesian_opt_tuner.search(train_univariate, #X_train, y_train, epochs=EPOCHS, validation_data=val_univariate, validation_steps=50, steps_per_epoch=EVALUATION_INTERVAL #batch_size=int(len(X_train)/2) #validation_split=0.2,verbose=1) ) I did it with a temperatures dataset, changing both epochs and hyperparams combinations. I think it depends also on the dataset you are playing with, for the one I quickly tried (with no representative results since it should be repeated enough times to get results distributions for each case), I saw no much difference (we should check it via a hypothesis tester for a robust conclusion), but there you can play with it. My quick results: 20 epochs, 2 hyperparams combinations : 2 epochs, 20 hyperparams combinations :
