[site]: crossvalidated
[post_id]: 523857
[parent_id]: 
[tags]: 
The connection between linear regression and Gaussians

I am reading Kevin Murphy's Machine Learning: A Probabilistic Perspective and I am having difficulties understanding Equation 1.5. Murphy introduces the linear function $$ y(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + \epsilon = \sum_{j=1}^D w_j x_j + \epsilon, $$ where $\mathbf{w}^T\mathbf{x}$ represents the inner product between the input vector $\mathbf{x}$ and the model's weight vector $\mathbf{w}$ , and $\epsilon$ is the residual error. Then Murphy mentions that we often assume that $\epsilon$ has a Gaussian distribution with mean $\mu$ and variance $\sigma^2$ . Murphy makes the connection between linear regression and Gaussians with Equation 1.5: $$ p(y|\mathbf{x, \theta}) = \mathcal{N}(y|\mu(\mathbf{x}), \sigma^2(\mathbf{x})). $$ I read this as the probability of getting output $y$ , given a data point $\mathbf{x}$ and some model parameters $\mathbf{w}$ and $\sigma^2$ , is normally distributed with mean $y$ given the mean of $\mathbf{x}$ (I don't understand this) and variance of $\mathbf{x}$ . To me, it seems that $y$ should be normally distributed with mean $\mathbf{w}^T\mathbf{x}$ and variance $\sigma^2$ only if $\epsilon$ is normally distributed with mean $0$ variance $\sigma^2$ , i.e., if $$ \epsilon \sim \mathcal{N}(0, \sigma^2), $$ then $$ p(y|\mathbf{x, \theta}) = \mathcal{N}(\mathbf{w}^T\mathbf{x}, \sigma^2). $$
