[site]: crossvalidated
[post_id]: 461822
[parent_id]: 461816
[tags]: 
Just extend time a little bit, we can see how terrible is the polynomial fit: plot(seq(30,320), predict(poly_fit, data.frame(time = seq(30,320))), type='l', col='red') points(d $time, d$ infected) grid() From machine learning perspective, we say the polynomial fit is overfitting. For SIR model, differential equations are describing the underline physical laws and interactions between variables. But the curve fitting approach is just try to minimize the loss with many parameters that do not have physical meaning. As a result, we will get loss minimized / perfect fit for training data. But the system is not describing any physics. For pros and cons, SIR fitting vs. polynomial fitting is very similar to the discussion on "parametric model vs. non-parametric model". For example, if we are fitting data with normal distribution or using kernel density estimation. If the data is really come from normal distribution or mostly satisfy model assumptions, then fitting the data to normal distribution is better than non-parametric estimation. On the other hand, if data is far way from model assumptions, say contains a lot of outliers, then fitting data with non-parametric methods will have better results. Similar question as been as asked What's wrong to fit periodic data with polynomials? And one of the still apply to here: Intuitively you want to fit function that (in some sense) looks like your underlying process. This way you'll have the fewest number of parameters to estimate. Say you have a round hole, and need to fit a cork into it. If your cork is square it's harder to fit it well than if the cork were round.
