[site]: crossvalidated
[post_id]: 557565
[parent_id]: 
[tags]: 
Granger Causality and F statistic

I am trying to educate myself in Granger Causality reading the classic literature. From what I have understood the idea is quite simple: first, to test if $X_t$ Granger causes $Y_t$ we define two autoregressive models: $$ \begin{align}\label{eq:varmodel} \tag{Full model} Y_t &= \sum_{j=1}^p a_j Y_{t-j} + \sum_{j=1}^q b_j X_{t-j} + \epsilon_t~, \\ \tag{Reduced model} Y_t &= \sum_{j=1}^p a_j Y_{t-j} + \epsilon'_t ~, \end{align} $$ in other words in the reduced model we predict $Y$ just using its own past and in the full model we add $X$ 's past states. Granger Causality quantifies the "gain" in linear predictability and can be tested using the following statistic $$ \begin{equation} \mathcal{F}_{X\rightarrow Y} = \ln{\frac{\Sigma_R}{\Sigma_F}}~, \end{equation} $$ where $\Sigma_F= var(\epsilon_t)$ and $\Sigma_R = var(\epsilon'_t)$ . Does the quantity $\mathcal{F}$ follow the F-distribution? Because this would be crucial to calculate the p-value for a test. The problem is that I don't see where the F-distribution came from. The main problem is the presence of the logarithm. Shouldn't the F-statistic just be a ratio of two sums of residual squares? In particular, from what I have understood (but correct me if I'm wrong), in a regression problem, F-test is used to see if a full model with $(p+q)$ parameters explains more variance of a restricted model with just $p$ parameters, so my intuition says that this is the case of Granger causality. On the other hand, F statistic should be a ratio of two sums of squares $SSR$ reflecting different sources of variability, but scaled on the number of parameters $p$ and the number of points $N$ , so something like $$F = \frac{(SSR_{R}-SSR_{U})/p}{SSR_{U}/(N-p-1)},$$ where $SSR_R$ and $SSR_U$ are the residual sum of squares of restricted/unrestricted model, respectively. Finally, quoting [Geweke 1982], when referring to this object $\mathcal{F}$ , he says that If autoregressions are really of order $p$ and the disturbances $\epsilon$ are Gaussian, these are maximum likelihood estimates conditional on presample values of $X_t$ , and $Y_t$ I don't understand this point either, why is $\mathcal{F}$ is a maximum likelihood estimates ? [Geweke 1982] Geweke, John. "Measurement of linear dependence and feedback between multiple time series." Journal of the American statistical association 77.378 (1982): 304-313.
