[site]: crossvalidated
[post_id]: 495975
[parent_id]: 451868
[tags]: 
I am also following Andrew Ng's note on SVM and had the same question. Inspired by OP's description and the first answer, I believe I have found a "natural" way of arriving at the solution $$b^*=-\frac{\max_{i:y_i=-1}w^*\cdot x_i + \min_{i:y_i=1} w^*\cdot x_i}{2}$$ In question description, the OP tries to solve for $b^*$ by plugging $w^*$ into the generalized Lagrangian $\mathcal{L} (w,b,a)$ . We expect setting the partial derivative against $b$ to 0 should give us a lead on finding $b^*$ . But same as shown in the note, $\frac{\partial}{\partial b}\mathcal{L} (w,b,a)=0$ yields $\sum_i a_i y_i=0$ . Which has nothing to do with $b$ ! This is saying the value of $b$ does not affect the minimum value of $\mathcal{L} (w,b,a)$ . So we can choose an arbitrary value for $b$ ? Not really. Because minimizing $\mathcal{L} (w,b,a)$ is not the only goal $b$ is involved in. It is also involved in the constraint, $$y_i(w^* \cdot x_i + b) \ge 1, i = 1, ..., m$$ Here we already replaced $w$ with the optimal value $w^*$ . Recall that the decision boundaries are defined by Support Vectors, so there must be one or more Support Vectors on both decision boundaries. Also recall the relationship between functional margin $l$ and geometric margin $d$ is given by $d_i = \frac{l_i}{\|w^*\|}, l_i = y_i(w^* \cdot x_i + b)$ . Support Vectors are, by definition, the closest to the hyperplane $w^* \cdot x+b=0$ , meaning they have the smallest $d$ , meaning that they have the smallest $l$ . So for Support Vectors on the positive decision boundary ( $y_i=1$ ), we have $$ \min_{i;y_i=1} l_i = \min_{i;y_i=1} 1 * (w^* \cdot x_i + b) = 1 $$ Similarly, for Support Vectors on the negative decision boundary, we have $$ \min_{i;y_i=-1} -1 * (w^* \cdot x_i + b) = 1 $$ Since both equations equals to 1, we connect them together to write $$ \min_{i;y_i=1} 1 * (w^* \cdot x_i + b) = \min_{i;y_i=-1} -1 * (w^* \cdot x_i + b)\\ b = \frac{\min_{i;y_i=-1} -1 * (w^* \cdot x_i) - \min_{i;y_i=1} 1 * (w^* \cdot x_i + b)}{2} $$ Note $\min_{i;y_i=-1} -1 * w^* \cdot x_i$ is the same as $\max_{i;y_i=-1} w^* \cdot x_i$ , also pull out the negative sign to the front, we arrive at $$b = b^* = -\frac{\max_{i:y_i=-1}w^*\cdot x_i + \min_{i:y_i=1} w^*\cdot x_i}{2}$$ This conclueds my 2 cents on solving for $b^*$ , kindly let me know if you spotted something funny along this answer.
