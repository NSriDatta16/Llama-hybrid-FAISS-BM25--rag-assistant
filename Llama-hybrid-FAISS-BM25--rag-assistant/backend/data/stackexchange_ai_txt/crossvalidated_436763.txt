[site]: crossvalidated
[post_id]: 436763
[parent_id]: 436743
[tags]: 
The most important thing to grasp about most neural-network oriented machine learning frameworks/libraries is that they provide three things: An abstraction called "tensor", with which you can efficiently compute common operations (matrix multiply, relu, etc) on a wide range of different hardware (CPU/GPU/etc). A flexible automatic differentiation system which is necessary for backpropagation / optimization. Lots and lots of convenience code for any common task you might perform -- for example, implementations of SGD, common data augmentation algorithms, common weight initialization strategies. Keras is an even higher level wrapper over tensorflow, so it only does 3. If you want to explore into the "guts" of things, you might want to ditch it, since it is only an extra level of abstraction you have to work around. Tensorflow and PyTorch provide more direct access to the details imo. You can't easily (and you shouldn't need to) touch 1 or 2, but since 3 is basically "helper code", you can throw it all out and write your own. Just force yourself not to use any helper code provided by the library that you can write yourself. I would like to stop the model between each batch of training and see if the validation loss/accuracy improved. Presumably you're calling some library's "fit" or "train" function. You can just delete this function call and write your own version which you have full control over. It may help to look at the source code of the library's function.
