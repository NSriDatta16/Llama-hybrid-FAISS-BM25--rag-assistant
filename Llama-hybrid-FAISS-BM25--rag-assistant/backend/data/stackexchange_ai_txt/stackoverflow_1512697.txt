[site]: stackoverflow
[post_id]: 1512697
[parent_id]: 1512624
[tags]: 
Let me explain the procedure that the authors introduced (as I understood it): Input: Training data : users, items, and ratings of users to these items (not necessarily each user rated all items) Target user : a new user with some ratings of some items Target item : an item not rated by target user that we would like to predict a rating for it. Output: prediction for the target item by target user This can be repeated for a bunch of items, and then we return the N-top items (highest predicted ratings) Procedure: The algorithm is very similar to the naive KNN method (search all training data to find users with similar ratings to the target user, then combine their ratings to give prediction [voting]). This simple method does not scale very well, as the number of users/items increase. The algorithm proposed is to first cluster the training users into K groups (groups of people who rated items similarly), where K N ( N is the total number of users). Then we scan those clusters to find which one the target user is closest to (instead of looking at all the training users). Finally we pick l out of those and we make our prediction as an average weighted by the distance to those l clusters. Note that the similarity measure used is the correlation coefficient, and the clustering algorithm is the bisecting K-Means algorithm. We can simply use the standard kmeans , and we can use other similarity metrics as well such as Euclidean distance or cosine distance. The first formula on page 5 is the definition of the correlation: corr(x,y) = (x-mean(x))(y-mean(y)) / std(x)*std(y) The second formula is basically a weighted average: predRating = sum_i(rating_i * corr(target,user_i)) / sum(corr(target,user_i)) where i loops over the selected top-l clusters Hope this clarifies things a little bit :)
