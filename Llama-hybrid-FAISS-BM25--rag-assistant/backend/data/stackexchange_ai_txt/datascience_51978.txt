[site]: datascience
[post_id]: 51978
[parent_id]: 51976
[tags]: 
Random forest (as almost any other algorithm) is prone to selecting variables which can lead to a one-to-one relationship with the $Y$ variable. Why? Because you are leading them to be overfitted. If your variables have high cardinality, it means they form little groups (in the leaf nodes) and then your model is "learning" the individuals, not generalizing them. The more "cardinal" the variable, the more overfitted is the model. For example, if you have social security number as variable (biggest cardinality possible), this variable will for sure have the biggest feature importance. Because in the leaf nodes you will find every individual with his social sec. number and his output. But the capacity of generalization of the model is zero. Almost every task in data science looks that this doesn't happen.
