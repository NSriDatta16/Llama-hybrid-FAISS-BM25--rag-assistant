[site]: datascience
[post_id]: 113390
[parent_id]: 
[tags]: 
Minor error in Ian Goodfellow's GAN optimality proof

I've been thinking of a part of the proof of the optimality of GANs from the original paper, and I can't manage to solve what seems to be an error. The paper states that the maximum of the function $y \mapsto a\log(y)+b\log(1-y)$ is $\frac{a}{a+b}$ which is true only when $a$ and $b$ are independent from $y$ . However, in the context of the proof we have $a=p_{data}(x)$ , $b=p_g(x)$ and $y=D(x)$ . Since they all three depend on the variable $x$ they cannot be independent. Moreover, to follow that reasoning, the maximum of $a\log(y)+b\log(1-y)$ is computed differenciating and equalling to zero. That result assumes $\frac{\partial a}{\partial y}=0$ , but we have $\frac{\partial a}{\partial y}=\frac{\partial p_{data}(x)}{\partial D(x)}=\frac{\partial p_{data}}{\partial x}\frac{\partial x}{\partial D}=\frac{p'_{data}(x)}{D'(x)}$ which is different from zero since $p_{data}$ is a continuous distribution and cannot be constant. In the latter I have used the chain rule only. My question is: Is there a way to overcome this error? Can we still find a way of showing that the optimal discriminator is found at $\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$ ? In case we cannot fix the proof, how good is the approximation? Because we all know GANs work well, so there must be a way of proving that at least this is a good approximation. Edit: Forgot to mention that other explanations of the proof include the same misconception, like this one: https://srome.github.io/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/ which is very detailed. Edit2: I have noticed that $p_g$ and $p_{data}$ need not be differentiable, however, this doesn't solve the problem because the dependency between $D$ and $p_g$ , $p_{data}$ still exists.
