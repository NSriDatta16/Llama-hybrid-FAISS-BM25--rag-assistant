[site]: crossvalidated
[post_id]: 514952
[parent_id]: 
[tags]: 
For a nonlinear regression task, is either Maximum Likelihood Estimation or Least Squares easier to learn a neural network model with?

I have data (x,y) and I want to create a model f(x) that will best approximate y . Let's assume that my data may not have homoscedasticity and the relationship between x and y is definitely nonlinear, so I want to use a neural network model for this regression task. I know Maximum Likelihood Estimation (MLE) and Least Squares (LS) are sometimes equivalent, and could both be used here. However, I don't know which would be the better tool to use for this, practically. If I were to naively implement LS, for each x , I'd have it output a y' and then minimize the sum of squares (y - y')**2 . If I were to naively implement MLE, for each x , I'd have it output a mu_y', sigma_squared_y' and then maximize the log likelihood of y under the normal distribution normal(mu_y', sigma_squared_y') . Is one of those generally easier to find a better solution with? For example, MLE requires outputting more (therefore requiring a larger network), but I can also imagine it being less brittle than LS to large errors. To be clear, I am not asking about the equivalence between MLE and LS with the assumption of Gaussian noise.
