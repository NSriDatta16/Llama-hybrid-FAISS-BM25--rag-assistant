[site]: stackoverflow
[post_id]: 3428131
[parent_id]: 
[tags]: 
Extracting a set of words with the Python/NLTK, then comparing it to a standard English dictionary

I have: from __future__ import division import nltk, re, pprint f = open('/home/a/Desktop/Projects/FinnegansWake/JamesJoyce-FinnegansWake.txt') raw = f.read() tokens = nltk.wordpunct_tokenize(raw) text = nltk.Text(tokens) words = [w.lower() for w in text] f2 = open('/home/a/Desktop/Projects/FinnegansWake/catted-several-long-Russian-novels-and-the-NYT.txt') englishraw = f2.read() englishtokens = nltk.wordpunct_tokenize(englishraw) englishtext = nltk.Text(englishtokens) englishwords = [w.lower() for w in englishwords] which is straight from the NLTK manual. What I want to do next is to compare vocab to an exhaustive set of English words, like the OED, and extract the difference -- the set of Finnegans Wake words that have not, and probably never will, be in the OED. I'm much more of a verbal person than a math-oriented person, so I haven't figured out how to do that yet, and the manual goes into way too much detail about stuff I don't actually want to do. I'm assuming it's just one or two more lines of code, though.
