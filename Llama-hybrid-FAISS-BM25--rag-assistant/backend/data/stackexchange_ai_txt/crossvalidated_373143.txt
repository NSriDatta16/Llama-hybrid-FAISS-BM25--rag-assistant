[site]: crossvalidated
[post_id]: 373143
[parent_id]: 373136
[tags]: 
He or Xavier Initialization is usually what is recommended. See for example this article: I recommend simply remembering: Use Xavier Initialization in the fully-connected layers of your network. (Or layers that use softmax/tanh activation functions) Use Variance Scaling Initialization in the intermediate layer of your network that use ReLU activation functions. He Initialization and Variance Scaling Initialization is the same thing. In fact, both He and Xavier Initialization are so similar to each other that they can be considered variants of the same idea. Common wisdom in the deep learning world is that sigmoid activation is bad and shouldn't be used. So you don't need to worry about what initialization to use in that case. :) Also, for best performance in deep networks you need to investigate in batch normalization -- it is not enough to use good initializations.
