[site]: crossvalidated
[post_id]: 313566
[parent_id]: 313564
[tags]: 
While a person might be wrong in a particular moment about the likelihood of a particular thing happening, the idea behind Bayes theorem (as applied to updating your understanding in the face of new information) is that the updated probability may not be entirely right, but that it will be more right than you were when you started. I think of a sitution where I'm trying to estimate the number of sheep in a field - let's imagine that there are truly 100, but I'm going to estimate that there are no sheep at all (which is about as wrong as I can get). Then, I see a sheep, and update my estimate - now, I estimate that there's one sheep in the field! I'm still wrong, but I'm slightly less wrong than I was when I started. In this way, if you collect enough information, you can update your estimates to be closer to reality - and, indeed, by collecting enough data, you can get arbitrarily close to reality. A really good description of this (albeit a pretty technical one) is in Savage's The Foundations of Statistics . It's a great read, and he develops a way of thinking about probability that makes a lot more sense from a Bayesian perspective.
