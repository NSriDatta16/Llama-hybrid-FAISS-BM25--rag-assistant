[site]: crossvalidated
[post_id]: 387881
[parent_id]: 
[tags]: 
Machine learning to detect wear on a machine axis

I have a machine that moves with one axis in the same direction (basic position A to end position B). While driving, the torque is measured and recorded every 10 milliseconds. This looks something like this: Time [t] | Torque [Nm] 0 | 2.5 10 | 4.0 20 | 4.7 30 | 5.6 40 | 6.0 50 | 5.5 60 | 5.1 70 | 3.2 80 | 2.1 90 | 1.5 100 | 0.2 The torque is therefore almost identical for a new machine. On a "very old" machine, the situation is different: the torque is constantly increased (signs of axis wear), or there are some outliers in the record (e.g., torque greatly increased at 50 milliseconds compared to a new machine). What I need now is a prediction for wear or failure of the axis! What I have: Very much data from an axis movement coming from a new machine (the data should therefore be considered "GOOD"). What I do not have: Data of an axis movement of an old machine (defective axis, increased torque, axis wear, etc.) My basic idea is: I want to use Machine Learning to train the data to the "GOOD" state. With the help of machine learning, a prediction (probability of a defect or wear) or a classification (GOOD or BAD) should then be carried out. My approach: Regression or classification (supervised learning) My question is: Does this approach make sense, or would you choose a completely different approach? Is it even possible what I intend to do? Thanks in advance
