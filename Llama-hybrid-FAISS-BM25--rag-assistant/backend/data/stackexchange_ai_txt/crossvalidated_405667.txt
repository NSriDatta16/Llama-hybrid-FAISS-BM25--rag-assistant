[site]: crossvalidated
[post_id]: 405667
[parent_id]: 377828
[tags]: 
Bit late, but this too gave me headache... Maybe we could argue as follows (two class case). Start with the usual expression for the log-ML, being product of of joint distributions as function of distribution parameters. \begin{align} \ell(\beta) &= \log\prod_{i=1}^N p(C=1, x_i\mid\beta)^{t_i} \cdot p(C=0, x_i\mid\beta)^{1-t_i} \\[8pt] &= \log\prod_{i=1}^N \left( p(C=1\mid x_i,\beta)~p(x_i)\right)^{t_i} \cdot \left(p(C=0\mid x_i,\beta)~p(x_i)\right)^{1-t_i} \\[8pt] &= \log\prod_{i=1}^N p(C=1\mid x_i,\beta)^{t_i} \cdot p(C=0\mid x_i,\beta)^{1-t_i}~~ p(x_i) \\[8pt] &= \sum_{i=1}^N t_i\cdot \log (p(C=1\mid x_i,\beta))+ (1-t_i)\cdot \log(p(C=0\mid x_i,\beta))+\log(p(x_i)) \\[8pt] \end{align} Now, as you minimize the expression (where the sigmoid is used as ansatz for the posterior $p(C\mid x)$ ) with respect to $\beta$ the marginal distribution $p(x_i)$ is dropped and the same minimum will be found independent if you start with the posterior probabilities $p(C\mid x)$ or joint probabilities $p(C,x)$ in the expression for the likelihood. I think this is equivalent to the fact that bigger $p(C\mid x)$ implies bigger $p(C,x)$ for a class. EDIT: I opened a new post for proposing a consistent problem description/suggested solution. MLE for logistic regression, formal derivation
