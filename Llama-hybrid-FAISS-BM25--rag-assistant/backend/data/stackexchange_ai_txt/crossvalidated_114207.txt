[site]: crossvalidated
[post_id]: 114207
[parent_id]: 114204
[tags]: 
Your problem is the following assumption: LIBSVM when set to use a linear kernel is a reasonable implementation of a linear SVM LIBSVM solves any SVM training problem in the exact same way, whether you are using linear SVM or kernel SVM. This general purpose solving strategy is not efficient for linear SVM. This is also why the linear SVM solution you get by LIBSVM contains support vectors $\mathbf{SV}$, weights $\alpha$ and a bias term $\rho$ rather than just the hyperplane of interest. Prediction with any LIBSVM model is done using the following equation: $$f(\mathbf{z}) = \sum_{i\in n_{SV}} \alpha_i \kappa(\mathbf{x}_i, \mathbf{z}) + \rho.$$ As per this equation, prediction complexity is linear in the number of support vectors for all LIBSVM models. For the linear kernel $\kappa(\mathbf{x}_i, \mathbf{z}) = \mathbf{x}_i^T\mathbf{z}$ the decision function can be rewritten as follows $f(\mathbf{z}) = \mathbf{w}^T\mathbf{z} + \rho$, where $$\mathbf{w} = \alpha \times\mathbf{SV}^T.$$ This is not done in LIBSVM for linear models. For linear models you should use LIBLINEAR , not LIBSVM. This software is designed by the same people, specifically to train linear models. It behaves exactly like you expect it to.
