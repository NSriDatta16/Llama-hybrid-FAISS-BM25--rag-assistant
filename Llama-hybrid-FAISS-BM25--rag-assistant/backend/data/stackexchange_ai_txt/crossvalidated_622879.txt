[site]: crossvalidated
[post_id]: 622879
[parent_id]: 389130
[tags]: 
I think the answer provided by Will Gregory is very good, but I think it may be helpful to additionally have a more simplified, qualitative explanation of the plot: Let’s assume you performed a linear regression in a Bayesian framework and you want to predict new values based on the mean of the posterior of your regression parameters. The equivalent kernels provide an alternative way of looking at how these predictions are made. Say, you have observed values $[y(x_1),y(x_2),...y(x_N)]$ and you want to predict a new value at $y(x’)$ . The kernels tell you how each of the observed data $y(x_i)$ contribute to your newly predicted value $y(x’)$ . I think, it’s intuitively clear that if $x’$ is very close to, e.g. $x_8$ , then $y(x_8)$ will have a larger influence on $y(x’)$ than some far away data point. That’s what these kernels show you: if you want to make a prediction at $x_i$ , you should look at the kernel on top of the plot: $k(x,x_i)$ . Observed data close to $x_i$ contributes most to the prediction (the kernel has a maximum). Observed values far away from $x_i$ contribute less (the kernel $k(x,x_i)$ is almost zero at $x_k$ and $x_j$ ).
