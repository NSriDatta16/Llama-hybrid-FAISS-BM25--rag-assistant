[site]: crossvalidated
[post_id]: 484289
[parent_id]: 
[tags]: 
Does SVM suffer from curse of high dimensionality? If no, Why?

While I know that some of the classification techniques such as k-nearest neighbour classifier suffer from the curse of high dimensionality, I wonder does the same apply to the support vector machines (SVM)? I can imagine SVM would only have to take dot products of support vectors and a feature vector to classify a new data point, so it might not suffer from curse of dimensionality. But what about non-linear SVMs?
