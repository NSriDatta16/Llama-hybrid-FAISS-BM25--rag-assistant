[site]: datascience
[post_id]: 42523
[parent_id]: 41046
[tags]: 
If you know that your output are positive, I think it makes more sense to enforce the positivity in your neural network by applying relu function or softplus $\ln(1. + \exp(x))$ . You could also have a look at Generalized models which extend linear regresssion to cases where the variable to predict is only positive (Gamma regression) or between 0 and 1 (logistic regression). If you are predicting a categorical variable, you could also perform one hot encoding and transform your regression problem in a classification. Last but not least, as suggested in the last question it might be interesting to normalise your output between 0 and 1 and have a logistic regression in the last layer. Hope this help
