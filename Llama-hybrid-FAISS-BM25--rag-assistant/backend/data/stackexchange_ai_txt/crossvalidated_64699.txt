[site]: crossvalidated
[post_id]: 64699
[parent_id]: 
[tags]: 
Assumptions of two-way ANOVA and k-fold cross validation

I want to compare 3 classifiers (kNN, SVM and CT) by using their classification accuracies on 10 folds, to highlight eventual differences between them. I think it could be done by a two-way ANOVA analysis, where classifiers=factors and folds=blocks, if some assumptions on data are verified. Following wikipedia, the assumptions are: The populations from which the samples are obtained must be normally distributed. Sampling is done correctly. Observations for within and between groups must be independent. The variances among populations must be equal (homoscedastic). Data are interval or nominal. I need an help on how to verify them in my case. Do I have to verify that for every classifier, its 10 accuracies are normally distributed and/or that for each fold, the 3 accuracies are normally distributed? Observations for within groups are independent because I use a different test set for every fold. Am I wrong? Observations for between groups are independent because I suppose classifiers to act in an independent way. Aren't they? Do I have to verify that each group described in the first point has the same variance? No problems in my case. Is there a quick way to verify all of the assumptions in Matlab?
