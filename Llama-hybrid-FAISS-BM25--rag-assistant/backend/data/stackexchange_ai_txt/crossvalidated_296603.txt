[site]: crossvalidated
[post_id]: 296603
[parent_id]: 230044
[tags]: 
I'm currently taking a course on Machine learning, where we use the following definition of nonparametric models: "Nonparametric models grow in complexity with the size of the data". Parametric model To see what it mean let's have a look at linear regression, a parametric model: There we try to predict a function parametrized in $ w \in ‚Ñù ^d $: $$ f(x) = w^Tx $$ The dimensionality of w is independent of the number of observations, or the size of your data. Nonparametric models Instead kernel regression tries to predict the following function: $$ f(x) = \sum_{i=1}^n \alpha_i k(x_i, x) $$ where we have $n$ data points, $\alpha_i$ are the weights and $k(x_i, x)$ is the kernel function. Here the number of parameters $\alpha_i$ is dependent on the number of data points $n$. The same is true for the kernelized perceptron: $$ f(x) = sign( \sum_{i=1}^n \alpha_i y_i k(x_i,x))) $$ Let's come back to your definition and say d was the number of $\alpha_i$. If we let $ n \to \infty $ then $d \to \infty$. That's exactly what the wikipedia definition asks for. I took the kernel regression function from my lecture slides and the kernelized perceptron function from wikipedia: https://en.wikipedia.org/wiki/Kernel_method
