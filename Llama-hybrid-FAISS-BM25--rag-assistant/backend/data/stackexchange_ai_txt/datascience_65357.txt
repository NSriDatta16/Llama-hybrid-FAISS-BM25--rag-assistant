[site]: datascience
[post_id]: 65357
[parent_id]: 65356
[tags]: 
I don't believe that's possible, in order for the model to return 0 or 1, your activation function on the output layer would have to return 0 or 1, which would mean that the activation function is non-differentiable, and you cannot do that. Also you can simplify your transformer function to: In [20]: predictions = np.array([[0.1], [0.9], [0.3], [0.6]]) In [21]: (predictions[:, 0] > 0.5).astype(np.int8) Out[21]: array([0, 1, 0, 1], dtype=int8) So it is very little extra work, and it gives your more flexibility if you want to see how confident the model is about the prediction, and you can change the threshold if you like.
