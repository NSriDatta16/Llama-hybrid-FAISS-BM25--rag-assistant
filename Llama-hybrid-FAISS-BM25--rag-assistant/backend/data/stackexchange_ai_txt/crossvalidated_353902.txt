[site]: crossvalidated
[post_id]: 353902
[parent_id]: 247551
[tags]: 
There are actually ways of doing this using dropout. Run the evaluation with dropout enabled (it's usually disabled for evaluation but turned on when training), and run the evaluation several times. The result distribution from multiple different runs can be used as confidence intervals. See the paper " Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning " Watch the youtube presentation Andrew Rowan - Bayesian Deep Learning with Edward (and a trick using Dropout)
