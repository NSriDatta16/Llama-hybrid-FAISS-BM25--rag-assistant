[site]: datascience
[post_id]: 77154
[parent_id]: 77145
[tags]: 
@DPCII, I don't think modifying output nodes at runtime will help you. This is because, A neural network is trained on a specific dataset and to predict predefined variables only. In backpropogation ( used for optimization ), the gradients are for each weight and bias ( or any other parameters ) are calculated using the loss function ( also the objective function ). For the changing the number of output nodes, you have remove some the connections coming from the previous layer which is $L_N$ , suppose. But during training, the parameters of $L_{N-1}$ were optimized using the gradients of the activation functions of $L_N$ . Hence, removing output nodes at inference time would affect all the previous layers which in turn will affect the final output ( of the remaining nodes ). Instead, I would suggest, to construct a neural network which predicts all the possible classes. Train this NN on various samples wherein you'll require the predictions of some desired nodes. Let me explain this by an example. Suppose, we have a neural network which predicts classes $C_1, C_2,C_3,...,C_N$ . For a specific sample, we only need the predictions for classes $C_1$ and $C_2$ only, given a sample $X$ . For this sample, we set the label as $[ y_1 , y_2 , 0 , 0 , 0 , ... , 0].$ We only set the values for classes $C_1$ and $C_2$ and rest all the classes are set to zero. Similarly, we assign labels for various samples and set only those values whose predictions are needed. Rest all values are set to zero. This is actually the common classification we do for images. For a given sample of a cat, I would like to predict an array of $[1,0]$ and for a sample of dog, $[0,1]$ . Hence, we could apply this ideology instead of removing nodes and disturbing the NN.
