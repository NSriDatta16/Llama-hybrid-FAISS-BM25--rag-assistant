[site]: datascience
[post_id]: 72123
[parent_id]: 72121
[tags]: 
You can think of productList as a sentence and treat it the same way language is treated in NLP. So yes, if your set of unique products is not too big, then exploding the list and writing each product as a unique column is an approach that can work quite well. You can also look into embedding layers, which extend this idea to lists of items that are "too big". If the order of items in the list matters, you probably want to decompose the list into individual rows and look for prediction on sequences. Edit: In response to your comment here is an analogy with semantic analysis on tweets: We can think of a tweet as a list of words, e.g., "I am happy" -> ["I", "am", "happy"] . These lists vary in length but each word (presumably) comes from the English language (+ some slang and neologisms which we will conveniently ignore). We can take a dictionary of the English language, look up the position of each word in that dictionary, and replace the word with the index of the word in the said dictionary. In our running example, this might look like [23, 54, 219] . This is the same as your list of product ids relating to individual products. The dictionary only has a finite number of words in it (similarly you only have a finite number of products), so we can OneHot encode each index in the list ( [[0,0,..,1,...], [0,...,1,...,0,..], ...] ). Now there are two options: (1) the order of the vectors in the list does not matter, in which case we would sum them up to obtain a single vector for each example, with which you can proceed as described -, or (2) the order of the vectors in the list does matter, in which case you would split the array into multiple examples, one for each vector in the list, and add another feature denoting the position at which it was found in said list. You now have a dataset where a column contains a vector of the same size as every other column, which you can rewrite as a set of many columns. You can then proceed with any analysis you think is reasonable for your data, e.g., clustering using simple methods, or training a non-linear embedding.
