[site]: crossvalidated
[post_id]: 103956
[parent_id]: 
[tags]: 
How to deal with neural networks when the data is unbalanced (but unlimited)?

If you assume that you have a plentiful/unlimited supply of training data, and you are using a neural network for classification, how do you deal with a situation in which a very high proportion of the data belongs to one class? For example, if you are doing binary classification and 99% of data comes from one class and only 1% the other. When you train the network using backpropagation, it normally uses the overall average error. If you keep the data with the same skewness as it comes in, will that cause it not to learn the rarer class properly? Another option is to discard data (presumably at random) from the more common class, until you have an equal proportion of data from each class, e.g. it is 50/50 in my binary example. But if you balance the data like this, then the network is being trained on data that is not representative of real life. The assumption that training data is plentiful (or unlimited) means that balancing the data need not reduce the size of the training set. If extreme skewness is inadvisable, is there a threshold at which this occurs, e.g. is it ok for it to be 30/70 but not 1/99 ?
