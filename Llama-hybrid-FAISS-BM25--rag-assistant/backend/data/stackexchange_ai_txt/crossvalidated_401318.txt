[site]: crossvalidated
[post_id]: 401318
[parent_id]: 167482
[tags]: 
@Imjohns3 has right, if you process long sequences(size N) and limit backpropagation to last K steps, the network won't learn patterns at the begining. I have worked with long texts and use the approach where I compute loss and do backpropagation after every K steps. Let's assume that my sequence had N=1000 tokens, my RNN process first K=100 then I try to do prediction (compute loss) and backpropagate. Next while maintaining the RNN state brake the gradient chain(in pytorch->detach) and start another k=100 steps. A good example of this technique you can find here: https://github.com/ksopyla/pytorch_neural_networks/blob/master/RNN/lstm_imdb_tbptt.py
