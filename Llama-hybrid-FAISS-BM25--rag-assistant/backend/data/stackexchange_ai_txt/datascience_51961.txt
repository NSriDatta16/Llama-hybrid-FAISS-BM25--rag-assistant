[site]: datascience
[post_id]: 51961
[parent_id]: 
[tags]: 
High Training Accuracy, Poor Validation, Test Accuracy

I am a beginner exploring Deep learning. I am trying to train a classifier (9 classes) with images as the input to my CNN followed by Bidirectional LSTM architecture. My model rapidly achieves a very high training accuracy (98-100%) but the test and validation accuracy remains constant at 15-20%. In the above image, blue line is the training accuracy. Green is validation accuracy and yellow is test accuracy In the above image, the blue line is the loss which is being propagated backwards What could be the possible reason for this peculiarity? Any suggestions to overcome this poor test accuracy dilemma would be appreciated. Additional Info: I have 630 training samples, 90 test samples and 90 samples for validation. I have implemented early stopping to stop training once my validation error stops decreasing in a patience interval. # Convolutional Neural Network class ConvNet(nn.Module): def __init__(self,num_classes=9): super(ConvNet,self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2), nn.Dropout(0.25)#dropout_factor = 0.1 ) self.layer2 = nn.Sequential( nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2), nn.Dropout(0.25)#dropout_factor = 0.1 ) self.layer3 = nn.Sequential( nn.Conv2d(in_channels=32,out_channels=8,kernel_size=2,stride=1, padding=1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2), nn.Dropout(0.25)#dropout_factor = 0.1 ) self.flc = nn.Linear(8*16*32,1024) self.hidden_size = 9 self.num_layers = 2 self.blstm = nn.LSTM(input_size=1024,hidden_size=9,num_layers=2,batch_first=True, bidirectional=True) self.flc2 = nn.Linear(18,9) #(hidden_size*2, num_classes) %2 for bidirection def forward(self,x): out = self.layer1(x) out = self.layer2(out) out = self.layer3(out) out = out.reshape(out.size(0),1,4096) #Flatten tensor out = self.flc(out) h0 = torch.zeros(self.num_layers*2, out.size(0), self.hidden_size) # 2 for bidirection c0 = torch.zeros(self.num_layers*2, out.size(0), self.hidden_size) # Forward propagate LSTM out, _ = self.blstm(out, (h0, c0)) # out: tensor of shape (batch_size, seq_length, hidden_size*2) # Decode the hidden state of the last time step out = self.flc2(out[:, -1, :]) return out
