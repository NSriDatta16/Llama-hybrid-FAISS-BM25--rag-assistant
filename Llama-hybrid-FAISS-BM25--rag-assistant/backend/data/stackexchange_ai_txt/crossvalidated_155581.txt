[site]: crossvalidated
[post_id]: 155581
[parent_id]: 155580
[tags]: 
You don't have to. The loss function has the same minimum whether you include the $\frac{1}{m}$ or suppress it. If you include it though, you get the nice interpretation of minimizing (one half) the average error per datapoint. Put another way, you are minimizing the error rate instead of the total error. Consider comparing the performance on two data sets of differing sizes. The raw sum of squared errors are not directly comparable, as larger datasets tend to have more total error just due to their size. On the other hand, the average error per datapoint is . Can you elaborate a bit? Sure. Your data set is a collection of data points $\{ x_i, y_i \}$. Once you have a model $h$, the least squares error of $h$ on a single data point is $$ (h(x_i) - y_i)^2 $$ this is, of course, different for each datapoint. Now, if we simply sum up the errors (and multiply by one half for the reason you describe) we get the total error $$ \frac{1}{2} \sum_i (h(x_i) - y_i)^2 $$ but if we divide by the number of summands we get the average error per data point $$ \frac{1}{2m} \sum_i (h(x_i) - y_i)^2 $$ The benefit of the average error is that if we have two datasets $\{ x_i, y_i \}$ and $\{ x'_i, y'_i \}$ of differeing sizes , then we can compare the average errors but not the total errors. For if the second data set is, say, ten times the size of the first, then we would expect the total error to be about ten times larger for the same model. On the other hand, the average error divides out the effect of the size of the data set, and so we would expect models of similar performance to have the similar average errors on different data sets.
