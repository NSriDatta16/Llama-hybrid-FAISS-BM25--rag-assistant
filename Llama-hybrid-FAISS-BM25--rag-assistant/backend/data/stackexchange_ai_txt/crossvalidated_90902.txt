[site]: crossvalidated
[post_id]: 90902
[parent_id]: 
[tags]: 
Why is leave-one-out cross-validation (LOOCV) variance about the mean estimate for error high?

In leave-one-out cross-validation (LOOCV), each of the training sets looks very similar to the others, differing in only one observation. When you want to estimate the test error, you take the average of the errors over the folds. That average has a high variance. Is there a mathematical formula, visual, or intuitive way to understand why that average has a higher variance compared with the $k$-fold cross validation?
