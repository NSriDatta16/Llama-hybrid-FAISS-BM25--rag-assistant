[site]: crossvalidated
[post_id]: 436348
[parent_id]: 436340
[tags]: 
A somewhat intuitive argument (though one that can be made rigorous): The population variance is itself a population average. Specifically if you define a new variable to be the square of the difference of the original variable from its population mean, $Y=(X-\mu_X)^2$ , then the expected value (population mean) of the new variable is the variance of the original one. (NB when using capital letters I am referring to random variables, rather than their realizations) The n-denominator variance from the population mean is the corresponding sample average for observations from the distribution of $Y$ . Sample averages are unbiased estimators of their population counterpart - that is, the expected value of a sample average IS the population mean. So if we were able to calculate an average of samples filled with $(X_i-\mu_X)^2$ values, it would be an unbiased estimator of the variance of the $X$ -distribution (that is, correct on average, over many such samples). Let $\bar{Y}$ be the mean of a sample taken from the $Y$ distribution. Computationally speaking, what we're saying is that $E[\bar{Y}] = \mu_Y = Var(X)$ . Written out in full, $E[\sum{(X_i-\mu_X)^2}/n] = Var(X)$ . The expected value of the average of a sample from the $Y$ distribution is the variance of $X$ . This is what it means for the sample average from the $Y$ distribution to be an "unbiased" estimator of $Var(X)$ . If we were to replace $\mu_X$ with the average of the $X_i$ sample, $\bar{X}$ , that sum would always become smaller, and thus the overall expected value would become smaller. Since it's smaller than something that is unbiased (the only exception is when the variance is 0), it is therefore biased (specifically, biased downward, too small on average).
