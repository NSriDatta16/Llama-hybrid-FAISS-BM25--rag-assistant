[site]: datascience
[post_id]: 116236
[parent_id]: 116233
[tags]: 
The weights of GPT-3 are not public. You can fine-tune it but only through the interface provided by OpenAI. In any case, GPT-3 is too large to be trained on CPU. About other similar models, like GPT-J, they would not fit on a RTX 3080, because it has 10/12Gb of memory and GPT-J takes 22+ Gb for float32 parameters. It should possible to fine-tune some special versions that use int8 precision, like this one .
