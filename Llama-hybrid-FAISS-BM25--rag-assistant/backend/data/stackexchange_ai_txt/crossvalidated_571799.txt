[site]: crossvalidated
[post_id]: 571799
[parent_id]: 571798
[tags]: 
This is what supervised learn does, particularly so-called “classification” models (most of which make probability predictions, but “classification” is all but a euphemism for predicting the probability of a discrete outcome). Consider a deck of cards. I draw a card and ask you to guess the card without showing it to you. You have a $1/52$ probability of guessing the right card, so little less than $2\%$ chance. If I then show you that that the card is red, you’ve ruled out half of the cards and know that it must be a diamond or a heart. Your probability of guessing the right card increased from $1/52$ when you were complexly ignorant about the card I drew to $1/26$ when I gave you some information about the card. In machine learning or predictive modeling, those details about the cards are called features or predictors (probability some other terms, too). How to use the available features and synthesize new ones from existing features is the special sauce of accurate predictive models. If your example, in the absence of much information about the viewer, you might think there is a low chance of her seeing a tree. However, if you know that she looked in the direction of a tree in the middle of the daytime when she should be able to see, perhaps you would think there is a high probability of her seeing a tree. Conversely, if you knew that she looked at night without the help of a flashlight and on an overcast night that even had a new moon (so no moonlight), you might expect there to be a particularly low probability of her seeing a tree. How to model something like this is an open question that machine learning and predictive modeling practitioners tackle every day.
