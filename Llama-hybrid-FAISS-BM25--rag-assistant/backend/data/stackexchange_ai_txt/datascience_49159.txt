[site]: datascience
[post_id]: 49159
[parent_id]: 49150
[tags]: 
They way you tried to do it is correct and no, you should not concatenate your train and test set to get around this. Unfortunately it seems that the transformer you are trying to use just does not implement the functionality you need. From the docs : There are some imputation algorithms that are inductive, meaning they can be applied to new data after a call to solver.fit or solver.fit_transform. Currently only IterativeImputer supports the full scikit-learn API: fit, fit_transform, and transform, but we are actively looking for contributions that extend other imputers to support induction. At least the KNN and SimpleFill imputers can be extended in a straightforward manner. Getting around this by concatenating your train and test set together will cause a data leak. This happens when your model gets information from the test set during training time, which invalidates the test evaluation. And data leaks aside, what imputer will you use when you want to make predictions on new data? As I see it your options now are: Find another KNN imputation implementation that supports transform on new data Use another inductive imputer Implement KNN imputation yourself
