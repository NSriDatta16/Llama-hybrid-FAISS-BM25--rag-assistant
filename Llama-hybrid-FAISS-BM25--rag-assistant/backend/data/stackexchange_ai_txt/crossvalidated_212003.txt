[site]: crossvalidated
[post_id]: 212003
[parent_id]: 90886
[tags]: 
Comparing such models using their performance metrics does give you additional information, as mentioned with the other answers already. But in terms of e.g. models choosing, the types of models scoring so well give you additional information . The less complex the model that can solve your problem sufficiently (in your case: perfectly), the better. I would consider logistic regression to be very simple in this regard. This means: using a logistic regression model will be better than using a more e.g. an ANN or RF, and better than a SVM (except for the linear kernel, which boils down to also using a linear separation hyperplane with just a more complex fitting process, but a similar resulting separation complexity). Essentially, this also applies to features: using less features is better (be aware that this strongly depends on how well your model generalizes: you will almost always find a way to obtain perfect results with a handful of features when dealing with very few samples, even when using simple models, but the resulting model will not generalize well). If you achieve such good results with such simple models, this also tells you something about your problem and data . If you have very few samples, you should consider obtaining more, because your models probably don't generalize well yet. But if you are using a sufficient amount of samples already, this probably means your problem is easy to solve using only few features. This is where you might want to thoroughly visualize the features and features-target-variable-relations to get an in-depth understanding of their relations, and where you would want to cross-check those relations with your domain specific knowledge about the problem, to ensure that those relations actually make sense. If not, it could e.g. mean that something is wrong with your data in the first place.
