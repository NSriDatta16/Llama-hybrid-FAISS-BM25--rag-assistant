[site]: crossvalidated
[post_id]: 581523
[parent_id]: 581017
[tags]: 
In order to calculate the output Z, we need to incorporate poses, encoder output, and previous Z value. Poses are incorporated in the kernel matrix and the previous Z value is defined by hyperparameter gamma2 and ell, which is learned and updated during the backward propagation by reducing the error. Finally, the encoder output is taken from the last layer of the encoder. Now the mean of the Gaussian process regression is defined by the formula The posterior mean and variance are defined as per the definition from Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. Now the definition of mean and variance can be calculated from the below equation In the case mentioned in the research paper, the new test point is the input point itself , just that it is subjected to Gaussian regression. So K star transpose is nothing but the Kernel with respect to the input point itself. i.e $k_*^T = k(P,P') = C$ Now $ (C+\sigma^2I)^{-1} = (K+\sigma_n^2I)^{-1}$ $y = Y$ We need to calculate $(C+\sigma^2I)^{-1}Y$ , which can be framed as solving AX=B, i.e $B = Y, A = (C+\sigma^2I)$ So, X = torch.linalg.solve(Y, K+torch.exp(self.sigma2)*I) will do the trick. Once we get X, we can multiply with C, as per the formula, and get the mean which is nothing but Z.
