[site]: datascience
[post_id]: 54455
[parent_id]: 54453
[tags]: 
Do I understand correctly from your test RMSE, that the error is lower as you increase the size of your fraction used in training? If you do 5-fold cross validation using a fraction of 0.01, that implies here that you are only a total fraction of only 0.05 of your data? So 0.04 for training and 0.01 for testing. It should not come as a surprise then, that the performance improves with increased amounts of data used for training. Your RMSE values increase with fraction size - I would normally expect such a correlation (assuming your data is fairly coherent). EDIT: To answer your questions, I would argue it is a well-known relationship, even in the simplest of regression problems. Have a read here (including comments on accepted answer) for more discussion on minimum required data points for valid models. One reason highlighted is that very few data points makes any estimate of variance less meaningful I don't think there is a name given to this relationship - at least I don't know of one (and neither do people in the link above). I would just describe it as being inherent to problem that the dataset is trying to represent. For example, if we have a high-dimensional complex parameter space, and then only a handful of data-points sampled from that space, it is very unlikely that we can fully describe the original space. The more samples we have, the closer we can approximate it. This is extremely evident in data-intensive modelling techniques, such as image-based tasks (i.e. convolutional neural networks, with millions of parameters.) Other methodologies, such as Bayesian optimisation using Gaussian processes. These are computationally more intense, but perhaps offer higher data efficiency. There might be some convergence theories, stating how approximations to ground truth reach a threshold given a certain amount of data (that kind of framework would make sense to me) - but alas, I have not seen any formal analysis in this direction. Regarding number of samples, in the meantime, I just follow the mantra: the more the merrier!
