[site]: crossvalidated
[post_id]: 509843
[parent_id]: 
[tags]: 
What is the recommended way of normalizing time-series data for neural networks?

Introduction Let me begin by describing the dataset and the application that I'm currently working with. I am working on a time series binary classification problem in Keras. My current approach uses LSTMs/GRUs to analyze the data and I'm planning on building the network more as time goes. My dataset is a time-series dataset of the electricity consumption of 40,000 customers over the period of 2.5 years. About 9% of the customers are known to be dishonest and are labeled as 1 in the dataset while the honest customers are labeled as 0 . Since I am working with neural network and based on the recommendation of many research papers I need to normalize my data using MinMax normalization. That can be done in Python using the sklearn.preprocessing.MinMaxScaler class. Question Based on the description of the dataset that I have given, I have three different ways in which my data can be normalized, a diagram of them is above and a verbal description is below: I can apply the MinMax scaler to the entire dataset as whole; thus, the MinMax scaler will scale the data using the global minimum and global maximum in the dataset. I have tried this approach and have found that this can lead some consumers' consumption to be very very small, some where about 0.001 . I am afraid that this might lead the model to ignore these values as they're somewhat small I could normalize my data row-wise which means that I would normalize each consumer's consumption separately. This would mean that each consumer would have a value(s) of 0 and 1 in their consumption because the normalization is done on each customer separately. We have found that usually when the consumption is very very high that this consumer is a thief. Following this approach of normalization, a consumer's consumption that has very high values will range between 0 and 1, this masking the fact that this consumer's consumption is an outlier. The third approach that I can follow would be doing the MinMax normalization on the columns of the data. This would mean that the consumption of each day for all of the customers will be lumped together and normalized. This approach will mean that those customers with very high consumption will have values very close to 1 and customers with smaller consumption will have values closer to 0. My question is, is there a specific or recommended approach for normalization of time series data that has high variation? And, what would be the best approach to follow? Should it be done globally, row-wise, or column-wise?
