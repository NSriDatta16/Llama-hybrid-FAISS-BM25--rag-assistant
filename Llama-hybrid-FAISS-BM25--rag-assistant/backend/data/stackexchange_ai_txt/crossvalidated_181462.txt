[site]: crossvalidated
[post_id]: 181462
[parent_id]: 181455
[tags]: 
One way to describe your problem is as a Markov decision process (MDP). Here are the things that formally define an MDP, listed along with how they correspond to your example: $S$, a set of states. (In your case a state would likely describe the cash and bets available.) $A$, a set of actions available. ("Bet on Sea Biscuit," etc.) A transition function, giving the probability that one will arrive in a state, given a prior state and an action. (Your model.) A function giving the reward for a certain state. (Payouts.) A discount factor $\gamma$. Your question doesn't explicitly mention the discount factor $\gamma$, but's it's helpful in thinking through the last-dollar circumstance. The discount factor captures an agent's preference for immediate rewards over future ones, and solutions for MDPs are said to be optimal when they maximize expected discounted rewards: $$\sum_{t=0}^{\infty}\gamma^t R_t$$ (This obscures away the details of $R$, but gives the gist.) $\gamma$ allows you to encode your own preferences for future rewards. If you're on your last pound and also the protagonist of a romantic comedy who needs the Â£100 to buy a train ticket so that you can interrupt a wedding, $\gamma=0$ encodes your preference for present rewards over anything else. Meaning, the reward is only worth it if you get it right now. If you're a hobbyist or professional better who would like that stake to persist long enough to continue profitably betting, you might set $\gamma$ closer to $1$, to encode that you'd like to give more weight to future rewards. (I.e., your preference that you get to keep playing.) This aligns with the intuition that if we have the funds to continue betting and the patience to wait, we should take the on-average higher payout bet.
