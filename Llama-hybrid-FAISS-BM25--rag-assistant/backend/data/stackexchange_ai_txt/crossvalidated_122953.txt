[site]: crossvalidated
[post_id]: 122953
[parent_id]: 122463
[tags]: 
If you take a Bayesian network structure and parameterize it, then it defines a probability distribution. However before adding the parameterization, it only defines constraints on the distribution. And in general, a BN may include continuous variables - so there might be no CPTs at all! The third option - "A Bayesian network is a set of CPTs (inducing the graph)" - won't work unless you limit the possible sets of CPTs, in a way that would seem arbitrary. Not all sets of CPTs induce BN structures; there are sets of conditional independences that cannot be fully represented by a DAG. (For example, say we have variables $A, B, C$ and $D$, and only two independences: $A \perp\!\!\!\perp C | \{B,D\}$ and $B \perp\!\!\!\perp D | \{A,C\}$.) Furthermore, even if the CPTs are compatible with a DAG, they won't necessarily identify a unique model; they will only tell you the Markov equivalence class of the generating DAG. [Edit: Here is an example, which hopefully will clarify this point. Say we have the DAG $A \to B \to C$, and the following three CPTs: \begin{align} \text{CPT1:} \qquad P(A=1) = .2 \end{align} \begin{align} \text{CPT2:} \qquad P(B=1|A=1) &= .7 \\ P(B=1|A=0) &= .4 \end{align} \begin{align} \text{CPT3:} \qquad P(C=1|B=1) &= .75 \\ P(C=1|B=0) &= .5 \end{align} CPT1 and CPT2 can be combined to produce the joint pmf $P(A,B)$, like so: \begin{align} P(A,B): \qquad P(A=1\; \& \;B=1) &= .14 \\ P(A=0\; \& \;B=1) &= .32 \\ P(A=1\; \& \;B=0) &= .06 \\ P(A=0\; \& \;B=0) &= .48 \\ \end{align} The joint pmf can then be factored using the chain rule into CPT1' and CPT2': \begin{align} \text{CPT1':} \qquad P(B=1) = .46 \end{align} \begin{align} \text{CPT2':} \qquad P(A=1|B=1) &= \frac{.14}{.46} \\ P(A=1|B=0) &= \frac{.06}{.54} \end{align} Clearly $P(A,B,C)$ can be represented by either CPT1, CPT2 and CPT3, or alternatively by CPT1', CPT2' and CPT3. This illustrates that the distribution is compatible with two distinct DAGs: $A \to B \to C$, and $A \leftarrow B \to C$. This is because these two DAGs entail the same conditional independences (i.e. they are members of the same Markov equivalence class).] So I would stick with the first option. In more detail: A Bayesian network induces conditional independence constraints on all probability distributions over the nodes of the graph. In particular, a graph $G$ over a set of variables $\mathbf{X}$ entails that the probability distribution $P(\mathbf{X})$ should factor into $\Pi_i P(X_i|\mathbf{Pa}_i)$, where $\mathbf{Pa}_i$ is the set of parents of node $X_i$ in $G$. If all the variables in $\mathbf{X}$ are discrete, this means $P(\mathbf{X})$ can be characterized by a set of small CPTs. A fully parameterized discrete BN is a graph together with a set of CPTs, one for each factor in the distribution. (If you are looking for good teaching resources I recommend David Heckerman's classic A Tutorial on Learning with Bayesian Networks (pdf). There is an associated set of slides that might save some time.)
