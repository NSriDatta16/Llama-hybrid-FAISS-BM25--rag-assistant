[site]: crossvalidated
[post_id]: 374830
[parent_id]: 346582
[tags]: 
It seems that you are mixing two problems: 1) performing feature selection with an ensemble learning algorithm (e.g. random forest, RF); 2) balancing your dataset so the learning process of your algorithm is maximum. For the first one, perhaps you could take a look to this paper , in which the authors propose a modification of RF (called Guided Regularized RF) to perform feature selection as well. There is an R implementation of this algorithm here that maybe is useful. Then, the second problem is largely detached from the first one. In my experience, I have never seen a machine learning algorithm handling decently data imbalance by default (I am all ears if any reader has experienced the contrary), like a Poisson, Weibull or a Negative Binomial model would do. They are simply not fit for this task, at least in its basic form. But this does not have to be a problem: you can balance the classes yourself. You should ensure an even number of samples belonging to each class during the training phase, and you should repeat this training phase using cross-validation techniques with random selection of samples to make sure that you are capturing most of the variance of the imbalanced class(es). In this way, the subsequent process of feature selection with RF should not be biased towards the imbalanced classes.
