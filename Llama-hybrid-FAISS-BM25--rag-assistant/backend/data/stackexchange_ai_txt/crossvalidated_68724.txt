[site]: crossvalidated
[post_id]: 68724
[parent_id]: 
[tags]: 
Time series: correcting the standard errors for autocorrelation

I have performed a number of tests to detect any presence of autocorrelation in my monthly return series. The test results confirm that the standard errors are not independent. A Durbin-Watson test result shows an upper bound violation with a d-statistics of 2.16, which implicates (first order) negative autocorrelation. A second, Breusch-Godfrey test, performed to examine higher order correlation points outs that for the first 12 lags the tests fails to reject the null of no serial correlation. Isnâ€™t this strange since the test results are contradicting each other? To get a better understanding of the correlation within the (dependent) variable I also implemented a corrgram. The Q-statistic results in this case shows a significantly autocorrelated data after the first lag (see image). The additional independent variable Q-statistic results show no presence autocorrelation. What I came across so far while searching on the internet for solutions to solve the autocorrelation are a large number of solutions. In order to obtain meaningful results from my OLS-regression I thought it was best to include a lagged dependent variable in the regression and generate Newey-West standard errors. This is probably a relatively simple but accessible approach for inexperienced statisticians, like myself. My question is; Is this a correct and sufficient approach of tackling this particular autocorrelation problem in my data, or should I think about more advanced models? A link to the time series file; Time_Series_Data
