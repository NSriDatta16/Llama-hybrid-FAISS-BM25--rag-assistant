[site]: crossvalidated
[post_id]: 351921
[parent_id]: 
[tags]: 
Evaluate approximation of PCA from randomized algorithms

I have been comparing different PCA implementations (some via explicit calculation of the covariance matrix, some with randomized/truncated SVD) in terms of speed, and now wanted to compare how good different randomized algorithms approximate the exact (to the numerical precision) solution via covariance matrix. Are there any suggested metrics to evaluate the similarity of two resulting eigenvector bases, i.e. the $k$ eigenvectors returned? My intuition would be to assert that the vectors have the same "general direction" (i.e. in case the first dimension does not match directions, one vector would be flipped), and then computing either the Frobenius norm of the absolute value matrix $abs(A)=abs(X-V)$, where $X$ is the exact solution and $V$ the resulting matrix from a randomized algorithm containing the eigenvectors, or A modified/weighted version of the Euclidean distance, on the pairwise vectors. I.e., $$ \mathcal{L}(X,V)=\sum_{j=0}^k \omega_j \sum_{i=0}^n \sqrt{(a_{ji}-x_{ji})^2}$$ where $\omega_j$ is a weight for the $j$-th PC, $n$ the dimension of the eigenvectors, and $a_{ji}$ and $x_{ji}$ the elements of the eigenvector matrices, respectively. The reason for the second version could be that I want to have a better approximation for the first few components (as they explain more of the variance; maybe even weigh them according to that explained variance), and not so much care about the last percent of remaining variance spread across multiple dimensions, potentially accumulating a high loss.
