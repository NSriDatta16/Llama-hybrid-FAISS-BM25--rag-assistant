[site]: crossvalidated
[post_id]: 589641
[parent_id]: 576776
[tags]: 
I found other answers are confusing. Here is my understanding. XGBoost trees estimate the residual from the previous tree (we set a base_score as the 0th tree residual). When subsample is defined (rather than default 1.0), each tree is trained using the subsampled data, and each tree computes the residuals of all data; this makes sure you have all residuals to subsample for consequent trees. n_estimators are the number of your trees; it is not a bad idea to think of it as the iteration number in DNN, because you can also early stop the training before all trees are trained using early_stopping_rounds hpyerparameter. Hope it helps.
