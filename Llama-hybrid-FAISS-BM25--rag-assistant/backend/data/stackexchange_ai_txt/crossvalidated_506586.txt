[site]: crossvalidated
[post_id]: 506586
[parent_id]: 
[tags]: 
can you calculate the uncertainty on predictions of y(x) if you regressed x(y)?

Probably a rather naive question, but it got me quite baffled. I wonder if anyone can help. I have data for 2 variables $x$ and $y$ , and a (theoretical) function relating them, with some parameters, e.g. $y = A + x^B$ . The inverse function is of course $x = (y-A)^{1/B}$ . [BTW this is just an example to show the concept; our real formula is a bit more complicated]. Measuring $y$ is relatively cheap, whereas measuring $x$ is more expensive. So I run a non-linear regression to find $A, B$ for cases where I have measurements for both variables, and later apply the inverse formula to estimate $x$ from measurements of $y$ . Given that the plan is to use the inverse formula $x = x(y)$ , the regression is also run by considering $y$ as the independent variable and $x$ as the dependent variable. Now, the problem is that, indeed because it's cheaper to measure $y$ than $x$ , the values of $y$ have on average a much larger uncertainty than the values of $x$ . So I thought: what if I determined $A, B$ by running the regression on $y = y(x)$ , i.e. considering $x$ as the independent variable and $y$ as the dependent variable? Wouldn't that be more in line with the fact that $y$ has a larger uncertainty? Then I would still use the same parameters in the inverse formula. But then I would not know: 1) if this is a legitimate approach; 2) how I would estimate the uncertainty on the predictions of $x$ , given that my regression was run on $y(x)$ . Any ideas / suggestions / further reading / posts about this topic? Thanks!
