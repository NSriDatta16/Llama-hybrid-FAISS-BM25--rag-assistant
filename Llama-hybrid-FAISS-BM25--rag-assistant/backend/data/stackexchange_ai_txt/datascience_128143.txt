[site]: datascience
[post_id]: 128143
[parent_id]: 117018
[tags]: 
I think that the current accepted answer (Dec 2022) isn't true. If you set $\beta_2=0$ , Adam doesn't behave like Momentum, it does a different thing. Say the gradient moving average at a particular step is a vector, $v_i$ , and the gradient at that step is another vector $g_i$ . Then Adam with $\beta_2=0$ with learning rate $\alpha$ will update the parameters $\theta_i,t$ like: $\theta_{i,t+1} := \theta_{i,t}-\alpha\cdot\frac{v_i}{\text{abs}(g_i)}$ which is different from momentum: $\theta_{i,t+1} := \theta_{i,t}-\alpha\cdot v_i$ . Eyeballing PyTorch's RMSProp pseudocode, https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html it looks like RMSProp without momentum and 'centered' looks like Adam with $\beta_1=0$ , but that it behaves differently if you change these parameters.
