[site]: crossvalidated
[post_id]: 382033
[parent_id]: 381990
[tags]: 
The rest seems to be fine, but 31 patients is a very small sample for minority class, to generalize the results. If you split this to train and test sets, this gets even smaller. With such a small set, it is questionable if you should split the data. After splitting the data, both training sample is too small for training, and test set too small for validating the results. Think of the test set size, is it 15 patients from the positive class, maybe 5 of them? Better use whole data and a method that is less likely to overfit, that is, something simple, like logistic regression, but random forest with shallow trees, and prohibiting the small splits, should also do fine. You also should try reducing the number of features, because with more features then samples, risk of overfitting is greater (certain in many cases). Then conduct detailed analysis of in-sample fit, that looks at the distributions of predictions, variable importances, tries to understand the predictions, detect biases etc. With such a small sample, you could check the predictions case by case (all of the 31 patients, plus random sample from negative class), to see what has lead the algorithm to make it's decisions and if they they make sense, or are there signs of overfitting (nonsense patterns).
