[site]: crossvalidated
[post_id]: 174777
[parent_id]: 
[tags]: 
Predicting the next time realization value of a MA(1) white noise time series

Suppose $y_t$ represents the value of a white noise time series at time $t$. The values are drawn from a normal distribution. Note that this is a 1st-order moving average process, or MA(1). $$ y_t=\epsilon_t + \beta \epsilon_{t-1} $$ It is known that the predictability of this time series at $t$ is improved with knowledge of the value at $t-1$. A book I have been reading says something along the lines of: The prediction for the step $t$ would be a value that is normally distributed with the mean, $y_t^{pred}=\beta\epsilon_{t-1}$. The Variance of the predicted value would be the variance of the $\epsilon_t$, which in this case is the same as the distribution used to create the white noise time series. I don't fully understand this. Is it simply saying that the correct guess at time $t$ is simply the normal distribution? I.e., is it simply some random value in the range of the normal distribution, since, in this case, the normal distribution is the source of drawing values for the time series? It would seem that such a guess is "right", but that a better guess may be possible. Since a white noise process is perfectly mean-reverting, if the value at $t-1$ is at the mean, then guessing the normal distribution makes sense. However, if the value at $t-1$ is close to the upper or lower bound of the possible values that can be drawn from the distribution, then would a better guess be values closer to the mean? Or is this simply a misunderstanding of how to answer the statement, "predict the value at time $t$ given $t-1$." Thanks.
