[site]: datascience
[post_id]: 81481
[parent_id]: 81471
[tags]: 
I'm still not 100% sure about the setting, but based on OP's comments I understand that there is no hyperparameter tuning, so there's a single method being trained in two different ways. So if my understanding is correct: In option 1 the training data is used for CV training/testing, then the model which corresponds to the best CV run is selected and applied to the unseen test set. This would be an unusual way to use CV, since normally CV is used only for evaluation, not for extracting one of the models. Unsurprisingly the performance of the model on the unseen test data is lower than during CV, because the maximum performance during CV is likely due to chance. Option 2 is just regular CV evaluation for a single model, so I would use this result. However there is an inconsistency between the results obtained: if in option 1 the average CV accuracy is 91.5, there's no logical reason why it's 92.5 in option 2 (there's slightly more data but it's unlikely to improve that much).
