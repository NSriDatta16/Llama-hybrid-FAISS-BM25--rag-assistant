[site]: crossvalidated
[post_id]: 485178
[parent_id]: 335321
[tags]: 
"What operation is actually used (element-by-element multiplication or the dot product?) and what is the primary difference?" The actual operation is called a convolution defined as: $$ J(x,y) = K * I = \sum_{n,m}K(n,m)I(x-n,y-m) $$ Where $J$ is the convolved signal, $K$ is the kernel, $I$ is the input signal, and $n,m$ are the kernel indexes. We see by the input signal indexing $I(x-n,y-m)$ that both input signal axes are "flipped". As an aside, the kernel (a.k.a. filter) could be flipped instead. Back to your question: From the Numpy docs : the dot product numpy.dot "Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. (...)" While the sum of the element-wise multiplication returns a scalar. Making use of a previous reply - if we look at the two regions in the example: import numpy as np from scipy.signal import convolve2d A = np.array([[1,1,1], [0,1,1], [0,0,1]]) B = np.array([[1,0,1], [0,1,0], [1,0,1]]) Dot product (returns ndarray): print(np.dot(A,B)) [[2 1 2] [1 1 1] [1 0 1]] Sum of element-wise multiplication (returns scalar); print(np.sum(np.multiply(A,B))) 4 And that is the primary difference. For good measure, a convolution: D = convolve2d(A, B, 'valid') print(D) [[4]] which in this case is equal to sum of element-wise multiplication of image patch and filter. A convolution can be equivalent to sum of element-wise multiplication if the filter is symmetric, which is the case in deeplearning.ai example given. This will not be the case for asymmetric filters: B2 = np.array([[1,0,0], [0,1,0], [1,0,1]]) print(np.sum(np.multiply(A,B2))) print(convolve2d(A, B2, 'valid')) 3 [[4]] where sum of element-wise multiplication returns 3 and convolution returns 4, albeit as numpy.ndarray data type. Note the discussion on flattened arrays - TensorFlow and Deep Learning without a PhD, Part 1 (Google Cloud Next '17) - applies to fully-connected, not convolutional layers. 2020 are you still reading? :D
