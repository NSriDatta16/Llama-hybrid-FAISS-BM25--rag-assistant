[site]: crossvalidated
[post_id]: 466804
[parent_id]: 
[tags]: 
Quantifying importance of a parameter in neural networks' prediction

Say I'm given a neural network, parameterized by a $d$ -dimensional vector $\theta$ , and an input $x$ . Given the prediction of this model $f_{\theta}(x)$ , can I somehow quantify importance of each of $d$ parameters ? For example, if a parameter is attributed a low importance, setting this parameter to 0 shouldn't change the model's output too much where as doing the same thing for an important parameter should change the model's prediction rather significantly.
