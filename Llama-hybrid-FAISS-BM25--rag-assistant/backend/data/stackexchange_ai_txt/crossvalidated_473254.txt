[site]: crossvalidated
[post_id]: 473254
[parent_id]: 
[tags]: 
Optimization as sampling for stochastic functions

Given an input space $X$ and a function $f: X\rightarrow \mathbb R$ , we want to find $x^*=argmin_{x\in X} f(x)$ . One way is to cast this problem as a sampling, where we define a distribution $p(x)\propto e^{-f(x)}$ . The mode of the distribution corresponds to $x^*$ . We can draw $N$ samples from $p(x)$ and pick the one that minimizes $f(x)$ as $x^*$ . For example, if we use Metropolis-Hastings algorithm as the sampler, then we are doing something similar to simulated annealing. However, in my problem, $f(x)$ is stochastic, and we want to find the minimizer in expectation, $x^*=argmin_{x\in X} \mathbb E[f(x)]$ . I can evaluate $f(x)$ but it is a quite slow procedure, so I would prefer not to e.g. evaluate $f(x)$ 100 times and take the average. In addition, given a specific $y$ from an $f(x)$ evaluation, I don't know its probability mass/density, even up to a constant. Essentially $f(x)$ is just a black-box stochastic procedure that returns a sample after some quite expensive computation. My question is, can I still use a similar sampling idea for the optimization? A naive way is to pretend that a single $y\sim f(x)$ sample is actually $\mathbb E[f(x)]$ , and use that value in the MH-sampler. But I don't know what, if any, distribution is implicitly being sampled. Another idea is to sample jointly in the $x, y\in X, \mathbb R$ space, but since I can't evaluate the likelihood of $y$ , even up to a normalizing constant, under $f(x)$ , and running $f(x)$ multiple times is perhaps too expensive, I don't know how to write a sampler with this constraint. Any ideas are greatly appreciated!
