[site]: crossvalidated
[post_id]: 153765
[parent_id]: 153574
[tags]: 
Yes, you can combine kernels, usually through either positive linear combinations or pointwise products. Let $k_1(x, y)$ be the kernel on only the string components of inputs $x$ and $y$, and $k_2(x, y)$ the kernel on only the score components of inputs $x$ and $y$. You can apply any kernel to these subsets; adding extra (ignored) dimensions doesn't break its kernel-ness. Positive linear combinations: if $w_1, w_2 > 0$, then $(x, y) \mapsto w_1 k_1(x, y) + w_2 k_2(x, y)$ is a valid kernel function. If you think of kernels as a similarity function, this means that $x$ and $y$ are similar if either $k_1$ or $k_2$ thinks they are, and more similar if both say they are, with relative importances determined by the weights $w_1, w_2$. Pointwise products: $(x, y) \mapsto k_1(x, y) \, k_2(x, y)$ is a valid kernel. This is like an "and" condition: points are similar only if both $k_1$ and $k_2$ think they're similar. In terms of implementing this in ksvm : you can either use a matrix, or implement your own kernel function. help(ksvm) has an example of doing this (under "#### Use custom kernel").
