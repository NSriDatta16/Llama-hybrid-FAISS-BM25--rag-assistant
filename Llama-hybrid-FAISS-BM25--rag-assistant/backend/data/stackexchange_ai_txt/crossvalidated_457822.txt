[site]: crossvalidated
[post_id]: 457822
[parent_id]: 455992
[tags]: 
The simple answer would be: yes, it can and BERT already does something like this. In an RNN, you take the last hidden because it is the state the network is in after reading the entire sentence. In a transformer network, it is not clear what should the special state be. BERT does that by prepending a special token (they call it [CLS] ). BERT is trained on sentence pairs and the vector corresponding to the [CLS] is used to predict if the two sentences are adjacent in a coherent text. When BERT is fine-tuned for downstream tasks, this vector is used as a single vector representation of the input (no matter if the input is a sentence pair or just one sentence). You can certainly simulate this also in the sequence-to-sequence setup. The simplest way would be using just a vector for the (beginning of a sentence) token instead of the context vector from encoder-decoder attention. However, I doubt it would get you a better representation than BERT and his Transformer friends do.
