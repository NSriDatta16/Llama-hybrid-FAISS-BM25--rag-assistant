[site]: crossvalidated
[post_id]: 462863
[parent_id]: 18891
[tags]: 
Bagging Bootstrap AGGregatING (Bagging) is an ensemble generation method that uses variations of samples used to train base classifiers. For each classifier to be generated, Bagging selects (with repetition) N samples from the training set with size N and train a base classifier. This is repeated until the desired size of the ensemble is reached. Bagging should be used with unstable classifiers, that is, classifiers that are sensitive to variations in the training set such as Decision Trees and Perceptrons. Random Subspace is an interesting similar approach that uses variations in the features instead of variations in the samples, usually indicated on datasets with multiple dimensions and sparse feature space. Boosting Boosting generates an ensemble by adding classifiers that correctly classify “difficult samples” . For each iteration, boosting updates the weights of the samples, so that, samples that are misclassified by the ensemble can have a higher weight, and therefore, higher probability of being selected for training the new classifier. Boosting is an interesting approach but is very noise sensitive and is only effective using weak classifiers. There are several variations of Boosting techniques AdaBoost, BrownBoost (…), each one has its own weight update rule in order to avoid some specific problems (noise, class imbalance …). Stacking Stacking is a meta-learning approach in which an ensemble is used to “extract features” that will be used by another layer of the ensemble. The following image (from Kaggle Ensembling Guide ) shows how this works. First (Bottom) several different classifiers are trained with the training set, and their outputs (probabilities) are used to train the next layer (middle layer), finally, the outputs (probabilities) of the classifiers in the second layer are combined using the average (AVG). There are several strategies using cross-validation, blending and other approaches to avoid stacking overfitting. But some general rules are to avoid such an approach on small datasets and try to use diverse classifiers so that they can “complement” each other. Stacking has been used in several machine learning competitions such as Kaggle and Top Coder. It is definitely a must-know in machine learning.
