[site]: datascience
[post_id]: 65325
[parent_id]: 
[tags]: 
How can we prove that an autoassociator network will continue to perform if we zero the diagonal elements of a weight matrix?

How can we prove that an auto-associator network will continue to perform if we zero the diagonal elements of a weight matrix that has been determined by the Hebb rule? In other words, suppose that the weight matrix is determined from $W = PP^T- QI$ , where $Q$ is the number of prototype vectors. I have been given a hint: show that the prototype vectors continue to be eigenvectors of the new weight matrix. This is a question from Neural Network Design (2nd Edition) book by Martin T. Hagan, Howard B. Demuth, Mark H. Beale, Orlando De Jesus . Resource : E7.5 p 224-225
