[site]: crossvalidated
[post_id]: 234841
[parent_id]: 234838
[tags]: 
Gradient boosted trees handle missings. The popular implementations in R, xgboost and gbm , both create a missing node alongside the left and right nodes for each split. If there are no missing elements in the training data then the missing node prediction is the same as the root prediction$^1$. Otherwise the prediction for the missing node is learned. Be very careful though, as it sounds like your training data and implementation data are very different. If the model expects to always be able to see a certain powerful predictor, don't expect good performance when that predictor is later masked. Consider if there is a way to make your training data look more like the data on which you want to implement your model. Boosted trees' handling of missing is a neat convenience when your training and scoring data both have equally distributed missings, but it is not a catch-all for what you are trying to do. $^1$ Edit: apparently xgboost always sends missings to the right node if it does not encounter any in training, and fits them to the left or right node otherwise. It does not spawn third node.
