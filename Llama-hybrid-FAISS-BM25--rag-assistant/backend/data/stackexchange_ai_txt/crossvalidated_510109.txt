[site]: crossvalidated
[post_id]: 510109
[parent_id]: 
[tags]: 
Learn sentence embeddings from a sequence of token embeddings

I want to build a sentence classifier that takes the sentence as a sequence of token embeddings. I'm specifically interested in the methodology for learning the sentence embedding from the sequence of token embeddings as part of a classification model. I've looked at various ways to do this: Mean and max pool the token embeddings This isn't sufficient as I want the model to learn the embeddings during training. I am using BERT token embeddings and averaging them doesn't fully utilise their power. Transformer layer/stack I've tried using a transformer encoder and taking the first token's embedding. I believe this approach would work well but unfortunately it makes the model too large (number of trainable parameters). It also seems like there is a lot of wasted computation as the output for most of the tokens is discarded. LSTM I haven't yet tried this approach, but I plan to try this next. My BERT model was only trained with the Masked Language Modelling objective, so I don't think taking the [CLS] token's embedding will perform well. Are there other techniques I should be looking at? What do you think of the LSTM approach? Is there a paper I can refer to for this (learning sentence embedding from a series of token embeddings, ideally as part of a classification stack)? Many thanks for your help.
