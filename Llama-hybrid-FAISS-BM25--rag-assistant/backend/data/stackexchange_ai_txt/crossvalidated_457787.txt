[site]: crossvalidated
[post_id]: 457787
[parent_id]: 
[tags]: 
What does dotted line mean in ResNet?

I understand the basic idea of residual neural network (ResNet), just copy $a^{[l]}$ to the add operator at layer $[l+2]$ before the ReLU operator at layer $[l+2]$ . This image shows part of ResNet, which seems to uses 2 types of skip connections, represented by solid line and dotted line respectively. I would just like to know which one is the copy-paste to $[l+2]$ operation, the solid line or the dotted line. A post says The dotted line is there, precisely because there has been a change in the dimension of the input volume (of course a reduction because of the convolution). What does that mean? Does the dotted line pointed out by red arrow reduce from 64 to 128? I can't understand this. Please help. Here is the Figure 2. Residual learning: a building block, coming from the ResNet paper .
