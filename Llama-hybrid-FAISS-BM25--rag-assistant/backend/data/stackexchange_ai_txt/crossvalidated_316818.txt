[site]: crossvalidated
[post_id]: 316818
[parent_id]: 
[tags]: 
Why does Mean deviation = Standard deviation = Range/2?

We know that mean deviation and standard deviation are two different things but why are both of them equal to Range/2? Range ($R$) = Highest value - lowest value Mean deviation ($\text{MD}$) = average absolute distance from the mean Standard deviation ($\text{SD}$,$s_n$) $=\sqrt{\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}}$ Edit: This is for two unequal observations. My book has a lengthy algebraic proof but I don't follow it. Can anyone please help me understand this? Is there some shortcut way to show this statement is true?
