[site]: datascience
[post_id]: 103121
[parent_id]: 103117
[tags]: 
Most of the work done in data science is experimental in some way (from data acquisition to a model being used to do something), partially due to how most techniques work (hyperparameters, distributional assumptions, etc.), after all most models are very tied to the task they were built to solve, so they don't translate very well even to tasks that are very similar from human perspective, since they are directly dependent on data. There is also important aspects to consider like model size, hardware constraints (you would find difficult to embed GPT-3 anywhere), intrinsic task problems that come from data (like unbalanced classes in a classification task, outliers and missing data in any task), interpretability, etc.. Thus architectural choices themselves are subject to some of these restrictions, specially when you consider that performance is not linear with the structures used in the architecture, be it number of neurons, connections or whatever else, in other words, using less things might get better results than more things. Now answering your question more directly, intuition is never used as justification, robust and rigorous experiments are. One may start with models or values that are found on literature about similar problems, but eventually exhaustive experimentation will get to the best stuff so far and if a particularly rigorous investigation scheme was used, the architecture found may be close or even be the best for a long time. The link to the scikit flowchart displays more about what scikit can do for you in specific circumstances than what is possible to do, most of it comes from experience, but is in no way final on what is possible. There are environments where getting more examples is too expensive or even impossible, there are also domains where concept shift exists, so your models need to be able to be flexible and adapt to change in distributions, don't take these flowcharts/guides as the absolute truth, because there is not a single tool or toolkit that does everything, they are, however, a good starting point. Usually, at least in my experience, we just apply Occam's Razor and eventually use the minimal model or models (ensembles is a thing) that can get the job done because there are plenty of other concerns and constraints that are as important as solving the problem itself.
