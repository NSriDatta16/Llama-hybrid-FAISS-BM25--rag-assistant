[site]: crossvalidated
[post_id]: 184429
[parent_id]: 184272
[tags]: 
Sometimes, people will just omit the intercept in SVM, but i think the reason maybe we can penalizing intercept in order to omit it. i.e., we can modify the data $\mathbf{\hat{x}} = (\mathbf{1}, \mathbf{x})$, and $\mathbf{\hat{w}} = (w_{0}, \mathbf{w}^{T})^{T}$ so that omit the intercept $$\mathbf{x} ~ \mathbf{w} + b = \mathbf{\hat{x}} ~ \mathbf{\hat{w}} $$ As you said, similar technique can be used in kernel version. However, if we put the intercept in the weights, the objective function will slightly different with original one. That's why we call "penalize".
