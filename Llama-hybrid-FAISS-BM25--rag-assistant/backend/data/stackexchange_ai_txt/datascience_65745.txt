[site]: datascience
[post_id]: 65745
[parent_id]: 
[tags]: 
Batch Normalization vs Other Normalization Techniques

In the context of neural networks, I understand that batch normalization ensures that activation at each layer of the neural net does not 'blow-up' and cause a bias in the network. However, I don't understand why it would be used as opposed to other normalization techniques such as Cosine or Weight normalization, these achieve the same goal and don't seem to be any more computationally complex. Could someone please explain to me the advantages and disadvantages of using batch normalization vs other normalization techniques, and which contexts batch norm would be most beneficial?
