[site]: crossvalidated
[post_id]: 624195
[parent_id]: 624193
[tags]: 
Both the prediction interval and the variance-bias tradeoff indicate a minimal mean squared error dictated by the measurement noise $Var(Y)=\sigma^2$ . So I really wonder why I cannot find papers/books discussing this close relationship. The reason might be that the relationship is not that special. If both concepts use 'variance' in some way, then there's not yet a meaningful relationship. To take an extreme example, are two concepts related when they both use 'addition'? The bias-variance trade-off deals with the range of the prediction error, and how bias and variance of the estimate have competing roles. The prediction deals with the range of the prediction error, and how we can describe an interval with a certain percentage of coverage. Each can be discussed independently from the other. There's not much reason to explicitly combine them, e.g. discussing the bias-variance tradeoff in terms of the size of prediction intervals. Although, in a way there are indirectly discussions about this when different types of (prediction) intervals are compared. An example is the following discussion about the use of unbiased (no prior) versus biased (including a Bayesian prior) intervals: Are there any examples where Bayesian credible intervals are obviously inferior to frequentist confidence intervals .
