[site]: crossvalidated
[post_id]: 552769
[parent_id]: 
[tags]: 
Soft Target in Knowledge Distillation

I am currently reading the paper Distilling the Knowledge in a Neural Network and in the introduction I came across the following sentence - When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases , so the small model can often be trained on much less data than the original cumbersome model and using a much higher learning rate. If the soft targets have high entropy for some training cases, why would this lead to less variance in the gradient updates for those training cases? How did the authors conclude that the small model can be trained on much less data (in comparison to the cumbersome model's training data) based on the first point?
