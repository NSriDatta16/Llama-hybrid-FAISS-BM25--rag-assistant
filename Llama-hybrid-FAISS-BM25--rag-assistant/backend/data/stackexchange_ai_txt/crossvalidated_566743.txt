[site]: crossvalidated
[post_id]: 566743
[parent_id]: 566732
[tags]: 
Fitting Gaussian Mixture Models can be done quite straightforwardly with classes from scikit . Here are some of your options: If you want to provide the number of components in the mixture (you mentioned "2-3 Gaussians") yourself, simply use sklearn.mixture.GaussianMixture . If you want to do some model selection before figuring out the "right" number of components, check out how to apply e.g. the information criterion BIC to your problem as described e.g. here (in section 2.1.1.2). Finally, you could also go even more Bayesian by using sklearn.mixture.BayesianGaussianMixture , which is a class that tries to figure out everything automatically (but sometimes gets it wrong). If you decide to have the number of components figured out by scikit, you might get into problems because of the small number (27) of data points. It could happen that scikit will suggest to you 27 components... Having said that, maybe fitting a Gaussian mixture is not really the best option for your final goal. For e.g. Gaussians have infinite support while in your case all values are nonnegative. Fitting e.g. lognormal distributions might be preferable. However, often, a histogram or some KDE is just fine. Scikit also provides you with those, see e.g. here , or here .
