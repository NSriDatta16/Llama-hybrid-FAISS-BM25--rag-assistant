[site]: crossvalidated
[post_id]: 27274
[parent_id]: 
[tags]: 
Where did this risk exposure 'estimation-formula' come from?

I was reading a book and the authors metioned that risk exposure can be estimated scientifically using this forumula: $risk(\$) = \frac{(a + 4m + b)}{6}$ and standard deviation $\sigma = \frac{b-a}{6}$ Where: a = minimum dollar exposure m = most likely dollar exposure b = maximum dollar exposure Where a, m, b are solicited from past data or expert judgement. All this is fine, but I've never seen this formula - where did this come from? Is it founded in statistics or any scientific reference? Anyone know anything about this? Pointers/suggestions/clarifications would be greatly appreciated. I'm curious as to how does the above formula provides a risk exposure metric. Why '6' in the denomiator and why the a + 4m + b in the numerator? Similarly for $\sigma$? Any ideas? EDIT : This formula seems to be that of weighted averaging but am not sure about it's inception or the reason of giving 'm' a weight of 4 and how that influences the calculation of $\sigma$? UPDATE :Book - Making the Software Business Case. However just Googling for the numerator threw up a few results - it seems to be known as the PERT formula and is used for estimation of 'time' - but my question about the 'formula' still holds irrespective of it being used for risk... DERIVATION OF FORMULA - The reason for the formula to be what it is seems to be beautifully explained in a JSTOR article for those interested!
