[site]: crossvalidated
[post_id]: 632969
[parent_id]: 632935
[tags]: 
Necessity is the mother of all invention. In the early 1900s there were no powerful computers which can run a Bayesian computation. Therefore, alternative methods need to be devised. Historically, Bayesian inference is older than Frequentist inference, because the Bayesian approach is far more intuitive to use. But pretty soon it became evident that the Bayesian approach is impractical unless you have a very simple problem. With the lack of computers people had to invent clever ways to do inference while avoiding the intense computation that arises. This was the birth of the Frequentist school of statistics. It allows you to reduce your inference problems to calculation of sums, squares, and averages, along with some square roots. You compute your "test statistic" and then look up the $p$ -values on an appropriate table. The tables were all pre-determined, so there is no need to recalculate them again. By reducing statistical problems down to a family of sampling-distributions it made calculations much easier to do. Now that we have software which can run these Bayesian calculations why not revert back to the way people wanted to do statistics but where unable to? There are a number of reasons for this: Pedagogy. It is far easier to teach newcomers to calculate averages, squares, and look up numbers in a table/database then it is to teach them likelihood functions and Bayes theorem. The later requires more grounding in mathematics. Many researchers are uncomfortable even with logarithms. Old Habits. Once people learn how to something it is very hard to make them learn something new, even if the new thing is a lot better. It was hard learning statistics in the beginning, with all the tests, all the software. And now you are telling people to forget that an relearn statistics all over again? Why would they do that? So people will stick to inferior methodology. Research. People who have an academic profession are required to publish papers. The truth of the matter is most of those papers are trivial, not to mention most of them are probably even wrong. Once some study becomes statistically significant, or gets $p$ -hacked to become significant, then it can become eligible to get published. This is the way the academic world works to a large extent. This is how people get their career. Telling people to stop doing that, publish less, and risk their careers is not something that people are willing to do. It is sad that it is like this, but that is how it works.
