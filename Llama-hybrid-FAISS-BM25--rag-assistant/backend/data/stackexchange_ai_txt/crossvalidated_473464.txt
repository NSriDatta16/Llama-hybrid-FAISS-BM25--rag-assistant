[site]: crossvalidated
[post_id]: 473464
[parent_id]: 473395
[tags]: 
how it is possible to compare different approaches and claim a possible improvement without a statistical confidence on the claim I think that many papers overclaim. In the subfield of giant pre-trained models evaluated on GLUE: even if they are tested on the same datasets, they are usually trained on different data so it is not possible to claim that they are "better overall". A more realistic claim is that the data and the models yield better results, with all the caveats (maybe other methods trained longer or better optimised would be better, or other methods with the same data, or the improvements on the benchmarks do not reflect real progress as was shown on NLI recently, etc.). ML and NLP researchers and reviewers were more concerned about statistical significance years ago, see for example Dietterich (1998), a popular paper on the topic. The standard have dropped on that front for possibly several reasons: People realized that statistical testing and the whole p-value approach can be more harmful overall, see for example this wikipedia page on misuse of p-values and Andrew Gelman's piece. That might justify dropping hypothesis testing but not ignoring variance. Datasets have grown a lot since the 90s/00s. The large increase in train and test data have reduced the variance of the results quite a lot and the influence of parameter initialisation and randomness in the optimisation procedure is less important. That might justify ignoring variance. New researchers are less exposed to statistics as ML research distinguished itself from stats (see Breiman's "The two cultures" for example). I've noticed this personally, as a PhD student in a big "AI" public lab. While these reasons make sense, I would say the trend went way too far. You are not the only one to be concerned. Here are a few interesting papers to realize the extent of the problem or proposing concrete solutions: It is hard to say that an algorithm is better than another in general, so weaker claims are about algorithms being better on specific datasets. However, even such weak claims sometimes do not hold to scrutiny! Gorman and Bedrick (2019) showed in a replication study that these results sometimes only hold when the "standard" train/test splits are used! In general, one cannot simply reuse the numbers of another paper directly and needs to replicate the results. But a common problem is unfair comparison due to uneven optimisation of the hyperparameters. Dodge & al. (2019) proposed to make more robust comparisons by taking into account the amount of computation used. Dror & al. 2017 focused on how to make broad claims based on the results on several datasets. The problem is not specific to NLP. For example, recent work by Musgrave & al claim that recent "improvements" in the subfield of metric learning have been "at best marginal" (Figure 4 is extremely telling and concerning). References: Dietterich (1998): Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms Gelman: The problems with p-values are not just with p-values Gorman & Bedrick (2019): We Need to Talk about Standard Splits Dodge & al. (2019): Show Your Work: Improved Reporting of Experimental Results Dror & al. (2017): Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets Musgrave & al. (2020): A Metric Learning Reality Check
