[site]: datascience
[post_id]: 108255
[parent_id]: 
[tags]: 
Remedie for a stubborn recall result?

I was working on a project connected to predicting default on credit loan with 0-1 loss. The recall is a crucial measure that should be maximized in this case, while monitoring precision for sanity of the results. As it is usuall with such data, the sample was heavily unbalanced , with low frequency of default cases. Although the algorithms I have trained (bagged/boosted trees, logistic regression etc.) obtained high accuracy and high precision the recall was low . I have tried to work with probability treshold of labeling predicted cases, namely decreasing it, but a tiny increase in recall was highly costly in terms of precision and accuracy. Am I doomed because those default cases that were not recalled are inseparable, or is there a trick to work with that? I have thought of maybe doing some clustering on the data and fitting different sub models for different clusters, but I am not sure how to proceed. I have also thought of subsampling in such a way that the default and not default would be more less balanced , and repeating such procedure keeping the default cases fixed and not default changing over the sample. Later creating an ensamble .
