[site]: crossvalidated
[post_id]: 251728
[parent_id]: 251363
[tags]: 
Consider the case you have dropout with p probability where p is (0,1] , the expected value of an output feature is p*E(WT+x),as only p units are used, say if feature>=4 then class A else B, now for the same input if in test time you do not have any dropout the Expected value of the activation is: E(WT+x) as all units are used, thus to prevent the decision boundary from shifting you reweigh the weights by 1/p to keep the expected activation same at the final layer. In short you are doing weighted average(and not the addition) of the exponential set of networks learnt with dropout.
