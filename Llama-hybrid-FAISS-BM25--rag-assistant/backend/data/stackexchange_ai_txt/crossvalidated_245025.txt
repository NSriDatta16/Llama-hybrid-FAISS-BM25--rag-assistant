[site]: crossvalidated
[post_id]: 245025
[parent_id]: 
[tags]: 
What is a fair way to compare models after model selection procedure?

The question is phrased a bit awkwardly probably, but let me explain it in more detail. I am using feedforward neural nets with one hidden layer to model a variable of interest. I have a set of inputs with different physical meanings. I want to train several neural nets with different inputs, say M different input sets (physical interpretation of different input sets and therefore models is different). I plan to use repeated double cross validation (rdCV) to select an optimal number of neurons in the hidden layer for each set of inputs. Thus, I will end up with M models with different inputs (and for each I will have its own number of neurons in the hidden layer). My question is, will it be fair to compare those models between each other using their averaged performance on the test sets (outer loop of rdCV) afterwards? The reason to obtain several ‘final’ models with different inputs is to assess which influence those inputs have on the variable I’d like to model. Thank you in advance for your help.
