[site]: crossvalidated
[post_id]: 642448
[parent_id]: 
[tags]: 
Consequences of maintaining IID assumption for prediction model training, but relaxing it for model testing

Let's say you're developing a prediction model, and you are confident that your data are IID. For example, you have a dataset where each row represents a different patient, and you build a model to predict some binary outcome using logistic regression. Then it comes time to externally evaluate your model, and the (new) dataset you use to do so actually contains lots of observations per patient, sometimes just a couple, sometimes hundreds. What would be the consequences of relaxing the IID assumption on calculating model discrimination and calibration? This would mirror a realistic scenario in many circumstances, for example in hospital risk models where the same patient might end up with a new risk score every time their data are updated, and those predictions are likely conditionally dependent to some extent. I know that the best option here is probably to use a Cox regression with time-varying covariates, or some other approach designed to handle repeated measures. However, the situation described above is very common, so I'd appreciate gathering more knowledge about it. Any reading/resources would also be appreciated.
