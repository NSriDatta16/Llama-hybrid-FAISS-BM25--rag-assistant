[site]: crossvalidated
[post_id]: 352751
[parent_id]: 352600
[tags]: 
You can certainly use an 80/20 (or a different proportion) split, especially if you have a lot of data. However, there is a bit of risk involved in this method: your results will depend heavily on how your data are divided. If you have a relatively small dataset, and your test set ends up differing meaningfully from your training set, the estimate of out-of-sample error will be poor. Using k-folds cross validation lets us get some more use out of each data point. Let's consider five-fold cross validation as a simple example. We divide the data randomly into five subsets. We train the data on four of the subsets and test on the fifth, repeating until each of the five subsets has served as our test set. We evaluate the model by averaging the results obtained from each of the five training/test runs. You can think of this as being a "more efficient" method in terms of use of the data. Each data point is used in the test set once, and each data point is included in the training sets four times. By using a simple 80/20 split, we may not be getting as much information as we can out of our data: only a relatively-small subset of points are ever included in the test set. However, k-fold cross validation is not computationally efficient compared to using a simple split of the data into training and test sets. You'll need to train your model k separate times, which can be demanding if k is large and/or the model fitting is computationally intensive.
