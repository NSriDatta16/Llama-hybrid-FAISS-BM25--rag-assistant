[site]: crossvalidated
[post_id]: 632348
[parent_id]: 632200
[tags]: 
You're treating the weights in the way that you'd treat precision weights (as in traditional weighted least squares). Your uncertainty calculation assumes that observations with higher weights have lower uncertainty; that an observation with a weight of 2 has half the variance of an observation with a weight of 1. That might be what you want, if, for example, the weights tell you how many people's data went into each data point. On the other hand, you might have sampling weights, where the weights tell you how many people in the population each person in the data set is representing. That's different from precision weights because having 1 person chosen randomly to represent 100 is different from having 1 observation that's an average of 100 people's data. If you have sampling weights then the weighted means are still calculated the same way, but the denominator of the t-statistic isn't. Your calculation will not be horribly wrong, but it won't be right, either. Firstly, you don't want the pooled variance, because there's no reason to assume there's a common variance for it to estimate. You want something more like the Satterthwaite-Welch unequal-variance t-test, so the denominator variance is $$s^2_1/n_1+s^2/n_2$$ , where the $s^2$ are your weighted mean squares, rather than the pooled thing. If you only have sampling weights and your survey doesn't have clusters or strata you are now ok; you are effectively using a model-robust sandwich estimator and that works fine with sampling weights. If your survey has clusters or strata you really do need proper software: the svyttest function in the R survey package does it, or there might well be python code out there that explicitly says it uses sampling weights. (What people were hoping for in the comments was some indication as to whether you had sampling weights or precision weights)
