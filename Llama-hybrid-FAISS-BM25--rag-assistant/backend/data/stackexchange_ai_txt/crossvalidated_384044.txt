[site]: crossvalidated
[post_id]: 384044
[parent_id]: 
[tags]: 
What does 1x1 convolution mean in a neural network? (v2)

While studying the architecture of YOLO CNN I saw some 1x1 convolutional layers are included. Trying to understand what they are suppose to be I read this answer . Most of the answers to that question indicated how 1x1 conv layers are used for dimensionality reduction (or in general, a dimensionality change) in the filter dimension. Looking at YOLO architecture (for example here ), however, we can see that there are some 1x1xN conv layers applied after a MxMxN conv layer, i.e. the dimensionality is not changed. For example conv 4 is a 3x3x256 layer and conv 5 is a 1x1x256 layer, meaning that the dimensionality of the output of conv 5 is exactly the same of the one of conv 4. Having this in mind, the answer to the previous version of this question is clearly pointless. How can these layer be interpreted then? A possible explanation I gave to myself is that these can be seen as "pixel"-wise fully-connected layers. I.e. for each element in the MxM input, we map it's N values to the N new values via N neurons. Is this correct?
