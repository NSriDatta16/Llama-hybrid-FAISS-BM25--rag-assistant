[site]: crossvalidated
[post_id]: 564731
[parent_id]: 564045
[tags]: 
This is very strange. I never had any experience on the random seed being important. Specially in XBG which has no random component (in the default configuration) as far as I can remember. XGB can implement random forests, and they are probably more sensitive to random initialization, but even than it should not make a "reasonable difference". Maybe you can post details of the parametrization of the XBG and also details of the dataset - specially proportion of the classes. The only source of problems I can think now is a dataset with very few examples of some classes, and in some internal sampling of the training set (with as far as I know XGB does not do by default - parameter sampling_method ) some sets are left without examples of these minority classes. @Lerner Zhang link for reproducibility is very interesting. Finally, there is no "random seed search". there is no structure for the random seed search - if a seed of 42 yields a good result, a seed of 43 may yield a bad result, and 44 an even better result. There is no meaningful search. I think the usual practice is to fix the random seed always with a known seed - at least you can reproduce the results!
