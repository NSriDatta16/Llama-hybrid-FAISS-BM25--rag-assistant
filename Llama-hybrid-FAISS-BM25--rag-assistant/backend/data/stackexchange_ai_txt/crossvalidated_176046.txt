[site]: crossvalidated
[post_id]: 176046
[parent_id]: 176040
[tags]: 
That is correct. A consequence of using the variance is that the units of variance are the units of the variable, squared. The mathematical rules here pay precisely no attention to whether the units are in conventional use; are familiar, engaging or appealing to people; or have a simple interpretation. %$^2$ is far from being the most exotic unit possible. Without trying very hard to find weird examples, it should be clear that thermal conductivity measured in $\text{W} / (\text{m K})$ has variance measured in $\text{W}^2/ (\text{m K})^2$, and so forth. The remedy is standard, so to speak: take the square root to find the standard deviation (SD) and then the units of the SD are guaranteed to be those of the original variable, in this example on a percent scale. EDIT In comments, @Glen_b raised a good point about precisely what is meant by "units" here. Glen_b: Aren't percentages (being a ratio of whatever units divided by the same units, times a constant) unitless? Nick Cox: Yes, in the sense that they are not units of measurement as any physical scientist would recognise, for example. I have made the same point elsewhere that percent is just a display format in that (e.g.) 95% is just a conventional format for 95/100 = 0.95. But I would be impressed to discover any software which is that flexible and understanding even of such an elementary point. To see percents in software, you need to convert by multiplying by 100, i.e. pretend to the software that the number is 100 times bigger than it is and then reverse the lie by adding "%" as a character. So, any convention to use percents is similar to using a conversion factor of 100 and then needing to adjust for the consequences, e.g. that squared numbers are 10000 times too big. I take that to be the spirit of this question.
