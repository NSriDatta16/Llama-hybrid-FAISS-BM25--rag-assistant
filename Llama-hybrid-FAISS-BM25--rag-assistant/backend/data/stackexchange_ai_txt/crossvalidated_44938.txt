[site]: crossvalidated
[post_id]: 44938
[parent_id]: 23352
[tags]: 
Yes, it is well-known that there are many local minimums. How you initialize the network and how you order the data affect the performance, which means there is a large random component. Even with one hidden layer, the number of local minimums may be exponential in the number of hidden nodes. This is actually easy to see. Suppose you are approximating a function of one variable which is constant outside two small patches. A neuron which is unsaturated in both patches isn't doing much. So, neurons will tend to saturate in one patch or the other, and it is hard for a neuron to switch where it is unsaturated without going through a stage where it contributes nothing. Therefore you get a local minimum for most of the $2^n$ assignments of neurons to patches. See Erhan et al, "Why Does Unsupervised Pre-training Help Deep Learning?" section 6.3 "Visualization of Model Trajectories During Learning." Although the graph shows that the local minimums found by random initialization differ greatly from the local minimims found by pre-training, it also shows that there isn't a convergence in parameter space of the models with pre-training (or without). Figure 2 shows there is a wide dispersion of error rates even within a training method.
