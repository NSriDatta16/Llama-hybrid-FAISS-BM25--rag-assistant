[site]: datascience
[post_id]: 36884
[parent_id]: 36848
[tags]: 
If you were to have fully formed trees in your random forest, then they should be able to recreate the rules you ended up clustering by, and accuracy would be 100%. Now, if you are cutting off the depth of the trees in the random forest (which is pretty standard, let's say you set max_depth = 8), then you will have an accuracy that may be less than 100%. This means that there are salient features that you are not consistently getting information from because they are not part of the 8 levels that one or more decision trees are using to classify. My question would then be: why do you want to do this? Once you have the centroids for each cluster using k-means, it is pretty quick to be able to determine which cluster a new record would belong to. I'm not sure I understand why you would want to use a random forest in this fashion.
