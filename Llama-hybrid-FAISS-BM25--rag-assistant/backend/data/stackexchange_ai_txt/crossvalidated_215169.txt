[site]: crossvalidated
[post_id]: 215169
[parent_id]: 215165
[tags]: 
The first evaluation should be based on the normality of the data -- that is, how bell-shaped a histogram of each feature looks like. Normally-distributed features result in a bell-shaped histogram with symmetric tails on the left and right. If you run a histogram to show the occurrence frequency over the range of say, one of your features, and it has a large left or right-skewed tail, then the feature is probably log-normally distributed. [if you take the log of the feature values in this case, then generate a histogram of log(x1), it might look more symmetric]. Given the above, "parametric" tests are based on parameters such as the average $\mu$ and variance $\sigma^2$, which are parameters of the normal distribution. Non-parametric tests are commonly based on ranks, and therefore order statistics. For a simple example, say you have 4 values of a feature, which happen to be: 1,2,3,1000000. The average of these 4 values is a little above 250,000, but if ranks are first assigned, as in 1$\rightarrow$1, 2$\rightarrow$2, 3$\rightarrow$3, 1000000$\rightarrow$4, then average is 2.5. Overall, the use of ranks in non-parametric test prevents results from being biased by outliers, i.e., the value of 1000000 being next to small values of 1,2,3. The difference is clear: parametric tests are based on parameters like $\mu$ and $\sigma$, which can be biased (incorrect) in the presence of outliers. However, non-parametric tests based on ranks of numbers do not use parameters such as the average and standard deviation, they simply use order statistics and expected values of ranked values. The point of the above is that for the 4 example values provided, parametric tests would use the average value near 250,000, while non-parametric tests would have a much lower order statistic near the bulk of the data. In other words, a single very large (small) value for a feature can greatly throw off your test conclusions, while non-parametric tests (based on ranks) are not biased by large outlier values. There are distributional issues involved as well, so the level of complexity increases the more you delve into parametric vs. non-parametric. But for the student, showing the different averages near 250,000 and 2.5 really drives home the idea of how non-parametric tests are less biased. (NB: non-parametric tests have less statistical power, if applied to data for which parametric tests are appropriate -- just a caveat).
