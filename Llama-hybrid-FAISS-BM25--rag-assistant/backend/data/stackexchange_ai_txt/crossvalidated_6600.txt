[site]: crossvalidated
[post_id]: 6600
[parent_id]: 6582
[tags]: 
It sounds like you basically have a problem of model choice . I think this is best treated as a decision problem . You want to act as if the final model you select is the true model , so that you can make conclusions about your data. So in decision theory, you need to specify a loss function , which says how you are going to rank each model, and a set of alternative models which you are going to decide between. See here and here for a decision theoretical approach to hypothesis testing in inference. And here is one which uses a decision theory approach to choose a model. It sounds like you want to use the p-value as your loss function (because that's how you want to compare the models). So if this is your criterion, then you pick the model with the smallest p-value. But the criterion needs to apply to something which the models have in common, an "obvious" choice based on a statistic which measures how well the model fits the data. One example is the sum of squared errors for predicting a new set of observations which were not included in the model fitting (based on the idea that a "good" model should reproduce the data it is supposed to be describing). So, what you can do is, for each model: 1) randomly split your data into two parts, a "model part" big enough for your model, and a "test" part to check predictions (which particular partition should not matter if the model is a good model). The "model" set is usually larger than the "test" set (at least 10 times larger, depending on how much data you have) 2) Fit the model to the "model data", and then use it to predict the "test" data. 3) Calculate the sum of squared error for prediction in the "test" data. 4) repeat 1-3 as many times as you feel necessary for your data (just in case you did a "bad" or "unlucky" partition), and take the average of the sum of squared error value in step 3). It does seem as though you have already defined a class of alternative models that you are willing to consider. Just a side note: Any procedure that you use to select the model, should go into step 1, including "automatic" model selection procedures. This way you properly account for the "multiple comparisons" that the automatic procedure does. Unfortunately, you need to have an alternative (maybe one is "foward selection" one is "forward stepwise" one is "backward selection", etc.). To "keep things fair" you could keep the same set of partitions for all models.
