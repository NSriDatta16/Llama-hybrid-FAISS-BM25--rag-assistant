[site]: crossvalidated
[post_id]: 365074
[parent_id]: 364806
[tags]: 
If i'm not wrong CV increases generalisation so that our model has less bias in case the data is ordered/distributed in a certain particular way. No , your understanding is wrong here. CV in itself does not help with generalization, it can only help in measuring the generalization abilities of a model. And even that is only possible if you have a suitable figure of merit that is evaluated. Such figures of merit that measure the performance of predictions are readily availbe for supervised models. For unsupervised models such as cluster analysis it is much more difficult to formulate them. Models can differ in ways that are unimportant to the generalization ability and/or they can differ in ways that are important, and only the latter differences are of interest to us. You may also say that of all ways how a model can differ, we are only interested in a subset and the question is how to catch that subset. Predictive models have one easy to capture type of important differences: differences in the prediction, and the usual figures of merit track exactly those differences. For your cluster analysis, you'll have to specify what differences are important and what differences are unimportant/benign. This is something that needs to be done in close relation to the application (data generating processes, characteristics of your data, and the question(s) you want to answer). Using a clustering technique pretty much implies that you do not have a ground truth to compare the clustering with. This is what limits the general possibility of having figures of merit similar to the ones we have for predicitive models. However, if you do have something like a ground truth, you may derive a figure of merit based on that ground truth. Ensemble models can help with generalization in certain situations and they are linked to resampling validation (such as CV) in that they also use a resampling step. Time series cross validation: one crucial question that makes a lot of difference in how to set up the splitting is whether you are looking along time series. For prediction that is training on past events and then predict future developments of the same time series. An example in process analysis would be to train a model predicting current (or future) concentration based on past readings of some sensors. Or, whether you look across a large number of time series, e.g. series of sensor readings for past runs of a batch process against the achieved final product concentrations and then want to predict final product concentration for unknown batches based on the respective full time series of sensor readings for the batch in question. Sliding window cross validation is for the first scenario, the 2nd scenario uses each time series as its own case and then "normal" cross validation. Correlation and stack of time series. The crucial question here is not whether there's a similarity in the time series but what causes the similarities. If there's an underlying factor causing similarity (i.e. you have several pools of servers in your data and the server data is more similar with each pool than compared to a server of another pool), you need to take that into account for CV splitting. If on the other hand the similarity is not caused by other factors but is just the phenomenon you're modeling, that is fine.
