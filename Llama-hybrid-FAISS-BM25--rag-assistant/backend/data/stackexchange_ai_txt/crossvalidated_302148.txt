[site]: crossvalidated
[post_id]: 302148
[parent_id]: 302144
[tags]: 
This is on of the famous problems among the deep learning community. There are two solutions that I have come across so far. Deep Networks with Stochastic Depth ( https://arxiv.org/abs/1603.09382 ) This paper talks about a training method where you train only a set of randomly chosen layers and drop the rest with identity function. This method also works as a regularizer to avoid overfitting of the model. FreezeOut: Accelerate Training by Progressively Freezing Layers ( https://arxiv.org/abs/1706.04983 ) This paper proposes to only train the hidden layers for a set portion of the training run, freezing them out one-by-one and excluding them from the backward pass.
