[site]: crossvalidated
[post_id]: 554714
[parent_id]: 
[tags]: 
Strange behaviour of training accuracy and loss function

Firstly, I want to mention that I am not looking for suggestions to improve the training accuracy of my NN. The only purpose of this question is to know what might be causing the peculiar behaviour described herein. My CNN was initially having an issue with the training accuracy that was stuck at 0.5467. I overcame this issue by using BatchNormalization. However, now it shows something weird. After a certain number of epochs, the accuracy sharply drops from around 0.9 to the earlier figure, i.e., 0.5467. I find this strange because: The accuracy does not gradually decline; and What is so special about '0.5467' that no matter what changes I make to the CNN (architecture, optimizer, or activation function) the training accuracy is always this specific number. Here is the plot of accuracy and loss: I want to know if this a common issue among data scientists, and also if there are any clues to the probable causes. Here is the architecture I have used, along with the model parameters: model = Sequential() model.add(Conv1D(filters=16, kernel_size=2, activation='relu', input_shape=(7,1))) model.add(BatchNormalization()) model.add(Conv1D(filters=8, kernel_size=2, activation='relu')) model.add(BatchNormalization()) model.add(Conv1D(filters=4, kernel_size=2, activation='relu')) model.add(BatchNormalization()) model.add(Conv1D(filters=2, kernel_size=1, activation='relu')) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(10, activation='relu')) model.add(BatchNormalization()) model.add(Dense(4, activation='relu')) model.add(Dense(1, activation='sigmoid')) opt=keras.optimizers.Nadam(lr=0.01) model.compile(optimizer=opt, loss="binary_crossentropy", metrics=["accuracy"]) Here is the quick model summary: Model: "sequential_10" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv1d_35 (Conv1D) (None, 6, 16) 48 _________________________________________________________________ batch_normalization_21 (Batc (None, 6, 16) 64 _________________________________________________________________ conv1d_36 (Conv1D) (None, 5, 8) 264 _________________________________________________________________ batch_normalization_22 (Batc (None, 5, 8) 32 _________________________________________________________________ conv1d_37 (Conv1D) (None, 4, 4) 68 _________________________________________________________________ batch_normalization_23 (Batc (None, 4, 4) 16 _________________________________________________________________ conv1d_38 (Conv1D) (None, 4, 2) 10 _________________________________________________________________ batch_normalization_24 (Batc (None, 4, 2) 8 _________________________________________________________________ dropout_8 (Dropout) (None, 4, 2) 0 _________________________________________________________________ flatten_8 (Flatten) (None, 8) 0 _________________________________________________________________ dense_23 (Dense) (None, 10) 90 _________________________________________________________________ batch_normalization_25 (Batc (None, 10) 40 _________________________________________________________________ dense_24 (Dense) (None, 4) 44 _________________________________________________________________ dense_25 (Dense) (None, 1) 5 ================================================================= Total params: 689 Trainable params: 609 Non-trainable params: 80 _________________________________________________________________ Any hints or suggestions on this matter would be very helpful.
