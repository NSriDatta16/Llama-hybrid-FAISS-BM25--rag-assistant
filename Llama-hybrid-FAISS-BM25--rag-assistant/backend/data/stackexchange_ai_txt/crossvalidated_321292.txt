[site]: crossvalidated
[post_id]: 321292
[parent_id]: 321267
[tags]: 
If I have understood your question correctly, it sounds like your are in a situation where the machine learning field of active learning might help. The method you are proposing sounds is called uncertainty sampling in active learning. There are certainly other algorithms besides this available: for instance you might like to look at this survey on the subject or John Langford's tutorial from ICML . Is the problem misspecified? I don't think so, but I'm not quite sure how to answer that so I'll quote what I think is relevant from Burr's survey : An important question is: “does active learning work?” Most of the empirical results in the published literature suggest that it does (e.g., the majority of papers in the bibliography of this survey). Furthermore, consider that software companies and large-scale research projects such as CiteSeer, Google, IBM, Microsoft, and Siemens are increasingly using active learning technologies in a variety of realworld applications I'm not sure what the current theoretical state of affairs is, but I believe there is a proof that under reasonable conditions in the binary classification setting active learning outperforms a model trained using a dataset built by random sampling. This is discussed further here. I think there's at least one reasonable implementation of an active learning algorithm in Langford's software Vowpal Wabbit .
