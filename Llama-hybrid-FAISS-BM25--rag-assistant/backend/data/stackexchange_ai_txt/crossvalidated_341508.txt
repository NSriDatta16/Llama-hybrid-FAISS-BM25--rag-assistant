[site]: crossvalidated
[post_id]: 341508
[parent_id]: 
[tags]: 
Non-Identifiable Multivariate Normal Posterior

So I have a theoretical question about what looks like, in my opinion, a multivariate normal distribution. The issue comes with the fact that the data is distributed with likelihood: Y |θ1, θ2 ∼ N(θ1 + θ2, 1) Where both θ1, θ2 also have their own prior distributions with their own respective means and variances. I tried treating the problem like a normal multivariate distribution but I got lost in my own math. I know for a fact that the Markov chain/Gibbs sampling should not converge but I still need to find the "proper" closed-form posterior (thus showing that even though the posterior is proper, non-identifiable ones aren't helpful). Can I use the multivariate normal rules to find p(θ1|θ2, y) and p(θ2|θ1, y)? How is it supposedly possible to also find the marginal distributions of these parameters? Any help on how to go about this and understanding why the Gibbs sampling won't converge is greatly appreciated!
