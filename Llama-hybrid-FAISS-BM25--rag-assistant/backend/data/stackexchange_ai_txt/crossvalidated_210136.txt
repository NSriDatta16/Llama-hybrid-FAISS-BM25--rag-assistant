[site]: crossvalidated
[post_id]: 210136
[parent_id]: 
[tags]: 
Training accuracy suddenly drops to zero after a while in Tensorflow?

I was training a ConvNet on CIFAR10 based on the code presented in "Deep MNIST for Experts" tutorial by TensorFlow. After around a few thousand epochs with training accuracy gradually increasing as expected, it suddenly drops from 90% to 4% with cross entropy and predicted scores turns into NaN. Any ideas why this happen? My code: class TensorFlowCNN(object): def __init__(self, input_dim=(32, 32, 3), num_classes=10): self.input_dim = input_dim self.num_classes = num_classes def model(self): H, W, C = self.input_dim x = tf.placeholder(tf.float32, shape=[None, H, W, C]) y_ = tf.placeholder(tf.float32, shape=[None, self.num_classes]) # 1st conv-pool W_conv1 = weight_variable([5, 5, C, 32]) b_conv1 = bias_variable([32]) x_image = x h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) # 2nd conv-pool W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) # 3rd conv-pool W_conv3 = weight_variable([5, 5, 64, 128]) b_conv3 = bias_variable([128]) h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3) h_pool3 = max_pool_2x2(h_conv3) # FC W_fc1 = weight_variable([H/8 * W/8 * 128, 1024]) b_fc1 = bias_variable([1024]) h_pool3_flat = tf.reshape(h_pool3, [-1, H/8 * W/8 * 128]) h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1) # Dropout keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # Readout W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) return x, y_, y_conv, keep_prob def train(self, test_data, test_labels, val_data, val_labels): x, y_, y_conv, keep_prob = self.model() sess = tf.InteractiveSession() cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1])) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(tf.initialize_all_variables()) for i in xrange(20000): x_train, y_train = svhn.next_batch(100, i) if i % 100 == 0: loss = cross_entropy.eval(feed_dict={ x: x_train, y_: y_train, keep_prob: 1.0 }) train_accuracy = accuracy.eval(feed_dict={ x: x_train, y_: y_train, keep_prob: 1.0 }) val_accuracy = accuracy.eval(feed_dict={ x: val_data, y_: val_labels, keep_prob: 1.0 }) # fname = 'params/%s.txt' % i # np.savetxt(fname, y_conv.eval(feed_dict={x: x_train, keep_prob: 1.0}), fmt='%1.4e') print("step %d, loss %g, training accuracy %g, validation accuracy %g" % (i, loss, train_accuracy, val_accuracy)) train_step.run(feed_dict={x: x_train, y_: y_train, keep_prob: 0.5}) print("test accuracy %g" % accuracy.eval(feed_dict={ x: test_data, y_: test_labels, keep_prob: 1.0}))
