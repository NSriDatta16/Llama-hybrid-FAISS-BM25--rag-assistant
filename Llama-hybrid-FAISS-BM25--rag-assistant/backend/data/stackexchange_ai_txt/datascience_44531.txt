[site]: datascience
[post_id]: 44531
[parent_id]: 44324
[tags]: 
Ordinary, vanilla gradient descent (GD) is a numerical approximation method, and not inherently stochastic. A stochastic process by nature has some "randomness", which a numerical method like GD does not have (unless you want to debate that the data set is itself inherently random and GD obtains it's stochastic properties from the data). It's fairly common for numerical methods like GD to get stuck in their local minimums/maximums. I've a few lectures on optimization methods, here is one on simulated annealing . Methods like simulated annealing have an advantage over deterministic numerical methods because they allow some randomness to be involved in the global optimum search. Whereas deterministic methods can often be fooled by following the steepest direction, or the nearest optima. GD, whether done in batch or by individual sample, more than often gets stuck in a local minimum, especially with deeper networks because the cost function becomes more and more complicated. Overall, GD on its own is not a great method for truly finding a global optimum. A better approach would be to include some random perturbation in the weight update step as discussed by Robert & Casella . Their approach is actually very similar to simulated annealing. Other methods include similar Monte Carlo approaches to adding stochastic properties to the GD step in neural network learning. The Berkeley Artificial Intelligence Research (BAIR) lab has a nice blog post, Minibatch Metropolis-Hastings , of combining GD methods with Metropolis-Hastings steps. It'll be a little out of your field of interest, but it is a good example of how GD can be modified to work better.
