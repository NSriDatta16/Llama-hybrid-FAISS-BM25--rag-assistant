[site]: crossvalidated
[post_id]: 129261
[parent_id]: 
[tags]: 
How to fit a single quadratic term to a regression

I have a high dimensional multivariate model and am fitting linear weights to each of the $N$ free variables using a classic stable SVD matrix solver. This works. I want to improve the fit by using a higher order model. I could of course use a full quadratic regression by adding the $N^2$ cross terms to the regression. But this makes my model where $N\approx 1000$ far too unwieldy. Instead I first tried a classic simple eigenvector based Principle Component Analysis to find the top 10 "primary" directions of my data and used those augmented variables for $\approx 10^2$ cross terms. This worked well but I feel it may be improved more. I wonder if it's possible to mathematically find the optimal axis for a single quadratic term instead of hoping the PCA eigenvector directions are best. That is, given my many linear variates, I want to make a single extra augmented column which is the square of a weighted sum of the other columns. I want to chose those weights such that the total regression fit (linear regression columns plus this new quadratic term) is has the minimal least squared error. But I can't figure out how to determine these $N$ optimal corefficients! I suspect this is a quadratic programming problem, perhaps one that's solvable with a Lagrangian multiplier. I appreciate any hints. I am sure others have done this kind of fit "augmenting" before, but I haven't found any papers or references to guide me.
