[site]: crossvalidated
[post_id]: 625202
[parent_id]: 625119
[tags]: 
As @Dave suggests in the comments, the random forest algorithm is already creating many decision trees under the hood and letting them "vote" on which class should be chosen. Therefore, instead of iterating by hand, you can achieve the same solution by changing the size (number of decision trees) of the random forest. Is there a way to choose the iteration that had highest accuracy compared to ground truth? I think this is an interesting question, but it cannot be done, because we do not know the "ground truth". What we have is a dataset, which is a sample from the entire population and the best we can do is to train a model on it without too much overfitting. Let's say we proceeded like you suggest, created 1000 random forests and chose the one which scored the best on the test set , which is the closest to the "ground truth" we have. This would almost surely not help us, we would simply choose the random forest which by chance captures the quirks of the test set the best. However, there is no guarantee such a model would work well on new unseen data, which may not look so much like the test data which we used for model selection. In general, random forests are good at preventing overfitting , since they create many slightly different trees which are averaged in the end. We do not find out which trees (or which forest) are the closest to ground truth, the best we can do is retrain the model again given new data.
