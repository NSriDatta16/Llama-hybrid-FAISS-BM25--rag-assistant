[site]: datascience
[post_id]: 76435
[parent_id]: 55359
[tags]: 
Suppose you have a language model trained to predict the next word. One sample in your training data is hello, how, are, you so that the input is the three words "hello, how, are" and the output the word "you". Without label smoothing, you would be telling the network $$ P(\mathrm{you}|\textrm{hello},\textrm{how},\textrm{are}) = 1.0 $$ That is, "you" will always follow the three word "hello, how, are". That is wrong. There are hundreds of words that could follow "hello, how, are" (e.g "hello how are they"). In this case smoothing the labels means that the network is given better data.
