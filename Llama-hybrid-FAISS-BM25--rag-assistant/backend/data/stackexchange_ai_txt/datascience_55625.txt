[site]: datascience
[post_id]: 55625
[parent_id]: 
[tags]: 
Coefficients of Linear regression for minimizing MSE

(I asked this in mathematics site, but nobody responded, it seems the whole problem is more related to data science than math.) In a regression problem, loss function is: $$L(a,b) = {\sum_{i=1}^n (y^i - (ax^i +b))^2})$$ In order to minimize this value, we need to set the derivative of L with respect to each of its parameters, equal to zero. Hence, $\frac{dL}{db}$ would be $y^- + a \cdot x^-$ But what would $\frac{dL}{da}$ be? $$\frac{\sigma L(a,b)}{\sigma a} = {2 \cdot \sum_{i=1}^n (y^i - (ax^i +b)}) \cdot \frac{\sigma \sum(y^i - (ax^i +b))}{\sigma a}$$ $$\frac{\sigma L(a,b))}{\sigma a} = {2 \cdot \sum_{i=1}^n (y^i - (ax^i +b)}) \cdot -\sum(x^i)$$ $$\frac{\sigma L(a,b))}{\sigma a} = {2\sum(x^i) \cdot \sum_{i=1}^n (y^i) - 2\sum(x^i) \cdot \sum_{i=1}^n (ax^i) - 2\sum(x^i) \cdot \sum_{i=1}^nb}$$ $$\frac{\sigma L(a,b))}{\sigma a} = {2\sum_{i=1}^n (y^i \cdot x^i) - 2\sum_{i=1}^n (ax^i \cdot x^i) - 2\sum_{i=1}^n x^i \cdot b}$$ $$\frac{\sigma L(a,b))}{\sigma a} = {2\sum_{i=1}^n (y^i \cdot x^i) - 2\sum_{i=1}^n ((ax^i +b) \cdot x^i)} = 0$$ How this would be equal to $\frac{cov(x, y)}{\sigma^2x}$
