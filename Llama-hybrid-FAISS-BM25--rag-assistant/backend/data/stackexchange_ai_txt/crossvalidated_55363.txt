[site]: crossvalidated
[post_id]: 55363
[parent_id]: 55350
[tags]: 
You may model the running times $X_1,\dots,X_n$ of the program as conditionally independent and identically distributed, given $M=\mu$ and $\Lambda=\lambda$, with density $$ f_{X_1\mid M,\Lambda}(x_1\mid \mu,\lambda) = \lambda\,e^{-\lambda(x_1-\mu)} I_{(\mu,\infty)}(x_1) \, . $$ The likelihood of this translated exponential model is $$ L_x(\mu,\lambda)=\lambda^n \, e^{-\lambda\left(\left(\sum_{i=1}^n x_i\right) - n\mu\right)} I_{(0,x_{(1)})}(\mu) \, . $$ As a first attempt, I would try a Bayesian analysis with a Jeffreys-like prior $f_{M,\Lambda}(\mu,\lambda)\propto 1/\lambda$. One goal is to estimate $M$ by $\mathbb{E}[M\mid X=x]$. To sample from the posterior, I would try a Metropolis-Hastings algorithm proposing the next $M$ as $\mathrm{U}[0,x_{(1)}]$, and the next $\Lambda$ from a gamma distribution with expectation equal to the previous value and a tiny variance. With a working sampler, it is easy to compute the estimate and a posterior credible interval for $M$. It is not difficult to extend this analysis to a second program; with the necessary additions to the notation, the natural way to compare both programs is to compute $P(M
