[site]: stackoverflow
[post_id]: 2414421
[parent_id]: 2414030
[tags]: 
This is in fact two questions in one ;-) Feature selection Linear or not add "algorithm selection", and you probably have three most fundamental questions of classifier design. As an aside note, it's a good thing that you do not have any domain expertise which would have allowed you to guide the selection of features and/or to assert the linearity of the feature space. That's the fun of data mining : to infer such info without a priori expertise. (BTW, and while domain expertise is good to double-check the outcome of the classifier, too much a priori insight may make you miss good mining opportunities). Without any such a priori knowledge you are forced to establish sound methodologies and apply careful scrutiny to the results. It's hard to provide specific guidance, in part because many details are left out in the question, and also because I'm somewhat BS-ing my way through this ;-). Never the less I hope the following generic advice will be helpful For each algorithm you try (or more precisely for each set of parameters for a given algorithm), you will need to run many tests . Theory can be very helpful, but there will remain a lot of "trial and error". You'll find Cross-Validation a valuable technique. In a nutshell, [and depending on the size of the available training data], you randomly split the training data in several parts and train the classifier on one [or several] of these parts, and then evaluate the classifier on its performance on another [or several] parts. For each such run you measure various indicators of performance such as Mis-Classification Error (MCE) and aside from telling you how the classifier performs, these metrics, or rather their variability will provide hints as to the relevance of the features selected and/or their lack of scale or linearity. Independently of the linearity assumption, it is useful to normalize the values of numeric features. This helps with features which have an odd range etc. Within each dimension, establish the range within, say, 2.5 standard deviations on either side of the median, and convert the feature values to a percentage on the basis of this range. Convert nominal attributes to binary ones , creating as many dimensions are there are distinct values of the nominal attribute. (I think many algorithm optimizers will do this for you) Once you have identified one or a few classifiers with a relatively decent performance (say 33% MCE), perform the same test series, with such a classifier by modifying only one parameter at a time. For example remove some features, and see if the resulting, lower dimensionality classifier improves or degrades. The loss factor is a very sensitive parameter . Try and stick with one "reasonnable" but possibly suboptimal value for the bulk of the tests, fine tune the loss at the end. Learn to exploit the "dump" info provided by the SVM optimizers. These results provide very valuable info as to what the optimizer "thinks" Remember that what worked very well wih a given dataset in a given domain may perform very poorly with data from another domain... coffee's good, not too much. When all fails, make it Irish ;-)
