[site]: crossvalidated
[post_id]: 565564
[parent_id]: 565537
[tags]: 
In line with @delaney's reply: I have not seen and I'm unable to imagine a reason for doing so. Borrowing from the discussion in https://github.com/scikit-learn/scikit-learn/issues/15850#issuecomment-896285461 : One loses information by binning the response. Why would one want to do that in the first place (except data compression)? Continuous targets have an order ( Continuous targets usually have some kind of smoothness: Proximity in feature space (for continuous features) means proximity in target space. All this loss of information is accompanied by possibly more parameters in the model, e.g. logistic regression has number of coefficients proportional to number of classes. The binning obfuscates whether one is trying to predict the expectation/mean or a quantile. One can end up with a badly (conditionally) calibrated regression model, ie biased. (This can also happen for stdandard regression techniques.) From V. Fedorov, F. Mannino, Rongmei Zhang "Consequences of dichotomization" (2009) doi: 10.1002/pst.331 While the analysis of dichotomized outcomes may be easier, there are no benefits to this approach when the true outcomes can be observed and the ‘working’ model is flexible enough to describe the population at hand. Thus, dichotomization should be avoided in most cases.
