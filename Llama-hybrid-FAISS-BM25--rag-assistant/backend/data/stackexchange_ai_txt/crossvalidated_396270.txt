[site]: crossvalidated
[post_id]: 396270
[parent_id]: 396249
[tags]: 
1) and 2) Usually it's better to have a test set because it is common to overfit on the validation set (since the hyperparameters are indeed chosen to minimize the validation loss). In your case, however, I would not bother with a test set but it then becomes important not to try too many sets of hyperparameters (that's when overfitting on the validation set usually happens and you get overoptimistic loss). 3) I think nested cross val is essentially using a test set. As I mentioned, it could be just fine with a regular CV. 4) Not sure what you mean here. 5) Here I believe you are confusing a concept. You should indeed "simply report the model performance and coefficient weightings from the final tuned model from CV". The case where you would want to re-train on the whole thing (training + val) is when you have a test set and you make your final predictions on the test set. This is commonly done when you have little data but I think in deep learning these days, they just use the trained model only on the training data rather than re-training it on the training+validation if that makes sense.
