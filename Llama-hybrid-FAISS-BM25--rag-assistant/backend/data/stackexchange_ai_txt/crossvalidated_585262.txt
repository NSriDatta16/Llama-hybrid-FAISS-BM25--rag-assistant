[site]: crossvalidated
[post_id]: 585262
[parent_id]: 585257
[tags]: 
The scikit-learn team has written extensive tutorials on how to do cross validation well. You might want to give GridSearchCV a try. You can use it to cross-validate one model or many. This will make your code straightforward to extend when you want to use cross validation for model selection/hyperparameters tuning, as in your previous question . Selecting dimensionality reduction with Pipeline and GridSearchCV Cross-validation on diabetes Dataset Exercise Comparing randomized search and grid search for hyperparameter estimation from sklearn.datasets import load_diabetes from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeRegressor X, y = load_diabetes(return_X_y=True) model = DecisionTreeRegressor() # Default parameters # params = {} # or # Hard-coded parameters params = { "max_depth": [9], "min_samples_split": [2], } n_splits = 10 # The KFold default is no shuffling, # so we explicitly turn shuffling on. cv = GridSearchCV( model, params, scoring="r2", cv=KFold(n_splits, shuffle=True), ) cv.fit(X, y) # Average R-squared across the k folds cv.cv_results_["mean_test_score"] # Standard deviation of the R-squared cv.cv_results_["std_test_score"]
