[site]: crossvalidated
[post_id]: 370268
[parent_id]: 358602
[tags]: 
In Random forest, generally the feature importance is computed based on out-of-bag (OOB) error. To compute the feature importance, the random forest model is created and then the OOB error is computed. This is followed by permuting (shuffling) a feature and then again the OOB error is computed. Like wise, all features are permuted one by one. Based on the increase (which is the score) in the OOB error, the feature importance is estimated. Higher this increase, higher the importance. These scores are then divided by the standard deviation of all the increases. Apart from this, gini impurity measure can also used to estimate feature importance. For more details refer this lecture note . Which one is the correct way to go? To compute the feature importance, you can use the complete dataset as the importance estimate is computed based on the OOB observations which are actually the left out observations during the bootstrap aggregation process for each tree in a Random Forest. Second, how can I calculate if one (or several) features have significant more importance than others (p-value)? Consider the feature importance values to compare the relevance of the features. To get p-value, statistical tests such as ANOVA (for parametric) or Kruskal-Wallis test (for non-parametric) could be used.
