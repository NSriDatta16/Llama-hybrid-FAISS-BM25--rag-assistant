[site]: crossvalidated
[post_id]: 445805
[parent_id]: 
[tags]: 
Similarity measure in classification: when do you need it to be a metric?

I am currently considering using dynamic time warping (DTW) for outlier detection in time series data. I am fairly new to the technique and have been reading: That I shoud be cautious using DTW since it is not a metric (since no triangular inequality) That DTW used a similarity measure with k-NN for classification or in hierarchical agglomerative clustering (HAC) can yield excellent performance for a handful of time series problems. I hence have two questions: From a broader perspective, what are the properties a given "similarity measure" must fulfill to be used in k-NN or HAC for example? In many problems, we simply use the Euclidian distance which is a metric and then proceed without fear. Are there specific comparison operations that do not require the whole set of metric properties (non-negativity, symmetry, identity and triangular inequality)? In particular, in which situations is the triangle inequality (which in my understanding ensures transitivity for "closeness": if B is close from A and C then A and C are close) not mandatory? Are there any resources that clarify this mess between metric, pseudo-metric, similarity measures, etc. in machine learning/classification? Regarding DTW specifically: which comparison operations are allowed (or not allowed) with DTW? From what I understand so far, I am fearful of using it in the wrong way and producing nonsensical results.
