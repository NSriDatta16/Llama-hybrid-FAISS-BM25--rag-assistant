[site]: datascience
[post_id]: 51477
[parent_id]: 25722
[tags]: 
Both Dropout and Batch Normalization can be used with convolutional layers; but it recommended to use BN and not Dropout (see links below). Several tutorials apply BatchNormalization between Conv2D and Activation, before the MaxPooling2D Like this: model.add(Conv2D(64, 3, padding = "same")) model.add(BatchNormalization()) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2,2))) BN may not speed up convergence; but it does (on average) improve generalization power (i.e. test accuracy). See this and this . Here is the TMI version .
