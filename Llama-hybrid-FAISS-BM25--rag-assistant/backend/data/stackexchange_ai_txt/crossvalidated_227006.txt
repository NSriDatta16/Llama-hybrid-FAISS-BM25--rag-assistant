[site]: crossvalidated
[post_id]: 227006
[parent_id]: 226977
[tags]: 
A probability space is defined as a tripple $(\Omega, \mathcal{F},P)$, where $\Omega$ is the set of possible outcomes, $\mathcal{F}$ a $\sigma$-algebra on $\Omega$ and $P$ a probability measure on $\mathcal{F}$. As you say, a random variable $X$ is a map from $\Omega$ to $\mathbb{R}$ such that for any Borel set $B$ in $\mathbb{R}$ it holds that $X^{-1}(B) \in \mathcal{F}$. Because of the latter propery, it holds that, for any Borel set $B$, there exists an event $E \in \mathcal{F}$ such that $X(E)=B$. Therefore we can measure any Borel set with a measure (depending on $X$) $\mu_X$ by defining $\mu_X(B)=P(E)$ where $E$ is the event supra. In other words, to ''measure'' B, look for its inverse image under $X^{-1}$, which (by definition of $X$) belongs to $\mathcal{F}$, and, as this inverse $E=X^{-1}(B)$ is in $\mathcal{F}$, we can measure it with the probability measure $P$, i.e. $\mu_X(B)=P(E)=P(X^{-1}(B))$ If $\mathcal{B}$ is the set of all Borel sets, then it can be shown that $(\mathbb{R}, \mathcal{B},\mu_X)$ is also a probability space. The map $X$ is called a random variable, and the function $\mu_X$ is called the distribution of $X$. This formally defines a random variable and its distribution. Examples are a normal random variable, Binomial random variables, etc. (see this link for detail on the Binomial random variable). Let us take the normal random variable, with mean $\mu$ and standard deviation $\sigma$ as an example i.e. $X \sim N(\mu, \sigma$). A (random) sample of size $n$ are just $n$ random outcomes $x_1, x_2, \dots, x_n$ from the distribution of $X$. These outcomes are random, so if we redo the random draws, we will find ''other'' values $y_1, y_2, \dots, y_n$. Therefore the sample average $\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i$ is also ''random''. This sample average is a so-called ''test statistic'' and it is a special case of a random variable, namely a random variable that is derived from the normal random variable $X$. Let's see how these two are defined formally, and let's , for simplicity, say that the Borel sets are all the intervals in $\mathbb{R}$. Then the normal random variable $X$ has $\Omega=\mathbb{R}$, $\mathcal{F}$ is the set of all intervals (I simplified here, it should be a $\sigma$-algebra), and the measure $P$ for an interval $[a,b]$ is $P([a,b]=\int_a^b f(x) dx$ where $f$ is the density of a normal variable with mean $\mu$ and standard deviation $\sigma$. Your test statistic $\bar{X}$ is another random variable, derived from $X$, with the same $\Omega$, the same $\sigma$-algebra, but with another measure $P'$ where $P'([a,b]=\int_a^b f'(x) dx$ and $f'$ is the density of a normal variable with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$. So, to answer your question, a test statistic is a special case of a random variable. The test statistic is thus a random variable, related to the random variable from which the sample (used to compute the test statistic) was drawn. I would not go that far as saying that a sample relates to a test statistic like an outcome relates to a random variable. A test statistic is a random variable (cfr supra) however in my opinion the sample ($x_1, x_2, \dots , x_n$) is not the outcome of a test statistic ($\bar{X}$). I would say that the sample average is an outcome of the random variable $\bar{X}$. The sample itself is an outcome of another random variable with outcomes in the product probability space with outcome set $\Omega \times \Omega \times \dots \times \Omega$ (and ''induced'' sigma-algebra and probability measures).
