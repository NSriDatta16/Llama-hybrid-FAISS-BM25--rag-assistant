[site]: crossvalidated
[post_id]: 580084
[parent_id]: 578443
[tags]: 
"Not doing multilevel modeling causes an underestimation of standard error" is an imprecise statement as it doesn't specify which standard error we underestimate if we ignore the hierarchical structure. In fact, it's an incorrect statement. The residual (so called unexplained) variation is lower when we fit a better model; so we overestimate the residual standard error if we fit a single-level model when we should fit a multi-level model instead. To understand this better, it's sufficient to consider one individual-level predictor and one group-level predictor. Note : R code to reproduce the analysis is attached at the end. The single-level model is a linear regression with two predictors, $x$ and $w$ , and three regression coefficients, $\alpha_0,\beta_x,\beta_w$ . \begin{aligned} \text{(M1)} \quad\quad \operatorname{y}_i &\sim N \left(\alpha_0 + \beta_x\operatorname{x}_i + \beta_w\operatorname{w}_{c[i]}, \sigma^2 \right) \end{aligned} I write $\operatorname{w}_{c[i]}$ rather than $\operatorname{w}_i$ because every individual in cluster $c$ has the same $w$ value. # single-level model M1 term estimate std.error #> (Intercept) 0.0245 0.0329 #> x 0.495 0.0332 #> w 0.198 0.0344 # residual standard error sigma(M1) #> 1.646398 The multi-level model is a random-intercept model and has two components. Each cluster $c$ has an (unobserved) random intercept $\alpha_c$ which is modeled with a simple linear regression on the group-level predictor $w$ . Each observation $y_i$ is modeled with a simple linear regression on the random intercept $\alpha_{c[i]}$ and the individual-level predictor $x$ . \begin{aligned} \text{(M2)} \quad\quad \operatorname{y}_i &\sim N \left(\alpha_{c[i]} + \beta_x\operatorname{x}_i, \sigma^2 \right) \\ \alpha_c &\sim N \left(\alpha_0 + \beta_w\operatorname{w}_c, \sigma^2_{\alpha} \right) \end{aligned} # multi-level model M2 term estimate std.error #> (Intercept) 0.0148 0.0742 #> x 0.494 0.0277 #> w 0.161 0.0791 # residual standard error sigma(M2) #> 1.326981 We observe: The residual standard error is smaller in the hierarchical model. The standard error of the individual-level predictor x is lower in the hierarchical model M2 . The standard error of the group-level predictor w is lower in the single-level model M1 . Check that these statements also hold for the individual-level predictors x1, x2, x3 and the group-level predictors w1, w2 in the bigger models in your original example. There is no loss of generality in considering a simpler model; but it makes plotting the data easier. Now let's show that the hierarchical model does a better job at explaining the differences between clusters, by computing the "remainder" of $y$ after accounting for $w$ . From the single-level model M1 we compute $y_i - (\hat{\alpha}_0 + \hat{\beta}_ww_{c[i]})$ ; and from the multi-level model M2 we compute $y_i - \hat{\alpha}_{c[i]}$ . In the figure below I plot the first 16 clusters in separate panels and add two horizontal lines: For each cluster $c$ , the "fixed intercept" $(\hat{\alpha}_0 + \hat{\beta}_ww_c)$ is the gray solid line; the random intercept $\hat{\alpha}_{c[i]}$ is the black dotted line. It's a bit hard to see but the gray lines don't intercept the y axis at exactly the same value. However, there is too little variability to account for the observed differences between clusters. Visually, the dotted lines are closer to the average $y$ in each panel. Once we subtract the "explanation" due to $w$ , we plot the remainder of $y$ as a function of $x$ (for 16 clusters only). There is the same number of points in the two side-by-side plots and they have the same $x$ values. However, the range and variability of the $y$ remainder is smaller in the hierarchical model. This shows right away that M2 has smaller residual variance $\hat{\sigma}^2$ than M1. Since the standard errors of regression coefficients are a function of $\hat{\sigma}^2$ , it also shows why the individual-level coefficient $\beta_x$ has smaller standard error in M2 even though $x$ is the same in M1 and M2. On the other hand, the group-level coefficient $\beta_w$ has bigger standard error in the hierarchical model M2. Intuitively, this happens because M2 estimates $\beta_w$ from a smaller regression of the random intercepts $\alpha_c$ on $w$ while M1 estimates $\beta_w$ from the bigger regression of the observations $y_i$ on $w$ (and $x$ of course). There are fewer clusters than observations, so M2 has more uncertainty about $\beta_w$ . M1 is too optimistic about its estimate (it underestimates the variance of $\beta_w$ ) as it assumes — incorrectly — that there are $n$ independent observations. The R code in all its gory details: library("broom.mixed") library("lme4") library("tidyverse") set.seed(1234) data(Demo.twolevel, package = "lavaan") # data for analysis Demo.twolevel % as_tibble() %>% select( y = y1, x = x1, w = w1, cluster ) %>% mutate( across(cluster, as.character) ) # subset for scatterplot Demo.twolevel.c16 % filter( cluster %in% as.character(seq(16)) ) # single-level model m1 % select(term, estimate, `std.error`) # residual standard error sigma(m1) # multi-level model m2 % select(term, estimate, `std.error`) # residual standard error sigma(m2) demo_scatterplot % ggplot( aes({{ xvar }}, {{ yvar }}) ) + geom_point( shape = 1 ) + scale_x_continuous( limits = c(-2.5, 3) ) + scale_y_continuous( limits = c(-5, 5.5) ) } # Compute (alpha0 + beta1 * w) from the single-level model M1 Demo.twolevel.c16 % mutate( fixed_intercept = coef(m1)["(Intercept)"] + coef(m1)["w"] * w ) # Extract alpha_c from the multi-level model M2 Demo.twolevel.c16 % select( cluster = level, random_intercept = estimate ) %>% inner_join( Demo.twolevel.c16, by = c("cluster") ) p % demo_scatterplot(x, y - fixed_intercept) p + labs( y = expression(paste( "y - (", β[0], "-", β[1], "w)" )), title = expression(paste( "Remainder ", italic(y), " variance in M1" )) ) p % demo_scatterplot(x, y) p + geom_hline( aes(yintercept = fixed_intercept), linetype = 1, color = "gray" ) + geom_hline( aes(yintercept = random_intercept), linetype = 3 ) + facet_wrap( ~cluster ) + labs( title = expression(paste( "Variance in ", italic(y), " within clusters 1 to 16, ", "after accounting for ", italic(w) )) ) p % demo_scatterplot(x, y - random_intercept) p + labs( y = expression(paste("y - ", α[c])), title = expression(paste( "Remainder ", italic(y), " variance in M2" )) )
