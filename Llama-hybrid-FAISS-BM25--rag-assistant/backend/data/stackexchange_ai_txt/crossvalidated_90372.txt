[site]: crossvalidated
[post_id]: 90372
[parent_id]: 90331
[tags]: 
After exploring this exercise you can try the easier ways in R. There are two popular functions for doing PCA: princomp and prcomp . The princomp function does the eigenvalue decomposition as done in your exercise. The prcomp function uses singular value decomposition. Both methods will give the same results almost all of the time: this answer explains the differences in R, whereas this answer explains the math . (Thanks to TooTone for comments now integrated into this post.) Here we use both to reproduce the exercise in R. First using princomp : d = data.frame(x=c(2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1), y=c(2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9)) # compute PCs p = princomp(d,center=TRUE,retx=TRUE) # use loadings and scores to reproduce with only first PC loadings = t(p$loadings[,1]) scores = p$scores[,1] reproduce = scores %*% loadings + colMeans(d) # plots plot(reproduce,pch=3,ylim=c(-1,4),xlim=c(-1,4)) abline(h=0,v=0,lty=3) mtext("Original data restored using only a single eigenvector",side=3,cex=0.7) biplot(p) Second using prcomp : d = data.frame(x=c(2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1), y=c(2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9)) # compute PCs p = prcomp(d,center=TRUE,retx=TRUE) # use loadings and scores to reproduce with only first PC loadings = t(p$rotation[,1]) scores = p$x[,1] reproduce = scores %*% loadings + colMeans(d) # plots plot(reproduce,pch=3,ylim=c(-1,4),xlim=c(-1,4)) abline(h=0,v=0,lty=3) mtext("Original data restored using only a single eigenvector",side=3,cex=0.7) biplot(p) Clearly the signs are flipped but the explanation of variation is equivalent.
