[site]: datascience
[post_id]: 68140
[parent_id]: 60318
[tags]: 
Suppose I take a part of the data as validation data, which contains whole blocks. If I split the remaining rows randomly in training and test data, the accuracy of the learned Random Forest is very high on training and test data but very low on the validation data. This is due to the fact that the Random Forest learns in this case to identify the individual blocks and as the validation data contains some unknown blocks, the accuracy drops. This can be also seen in the resulting importance for the features: the most important ones are those features that allow an identification of a cycle very easily. Splitting the remaining data into training and test data and leaving the blocks whole helps a bit but doesn't fix the problem completely. The latter approach is better; at least your test set will be more representative of your desired use-case, and so the scores will be more relevant. sklearn has this built-in, with GroupKFold . Another approach would be to remove all features that allow the identification of blocks. But this is difficult a priori and some of these features could have important information for my problem. Indeed, this seems problematic. There's interesting related work, "domain adaptive neural networks," which essentially try to simultaneously learn the predictive trends while unlearning the block-specific information; but I'm not sure how relevant that is here, or whether there are similar non-NN approaches. A simple way to overcome the problems would be to take the row mean in each block and use the resulting data for the classification problem. However, in this case you loose a lot of information. You could try to extract other relevant features from the blocks. This could work especially well if you have some domain knowledge to guide the feature engineering. In general, if the blocks are associated with a label each (rather than each row having its own label), you're dealing with "Multiple Instance Learning." How to deal with that depends on the specifics of how the blocks are generated. The wikipedia page , especially the sections Assumptions and Algorithms, is a good place to start.
