[site]: datascience
[post_id]: 58363
[parent_id]: 
[tags]: 
Time-series prediction: Model & data assumptions in AI/ML models vs conventional models

I was wondering if there was a good paper out there that informs about model and data assumptions in AI/ML approaches. For example, if you look at Time Series Modelling (Estimation or Prediction) with Linear models or (G)ARCH/ARMA processes, there are a lot of data assumptions that have to be satisfied to meet the underlying model assumptions: Linear Regression No Autocorrelation in your observations, often when dealt with level data (--> ACF) Stationarity (Unit-Roots --> Spurious Regressions) Homoscedasticity Assumptions about error term distribution "Normaldist" (mean = 0, and some finite variance) etc. Autoregressive Models stationarity squared error autocorrelation ... When dealing with ML/AI approaches, it feels like you can throw whatever you like as an input (my subjective perception). You are satisfied with the result as long as some prediction/estimation error measurement is good enough (similar to a high, but often misleading RÂ²). What assumptions have to be satisfied for an RNN, CNN or LSTM model that find application in time-series prediction? Any thoughts? ADDED Good Article describing my question/thoughts. Medium Article discussing model assumptions + tests, but not in the context of more advanced models I read the 100-page ML Book - Unfortunately almost no content about model assumptions or how to test for them.
