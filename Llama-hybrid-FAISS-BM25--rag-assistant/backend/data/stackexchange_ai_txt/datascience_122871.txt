[site]: datascience
[post_id]: 122871
[parent_id]: 122865
[tags]: 
When the order of input sequence is changed, the positions of self attention values change (rows interchanged), but the values remain same (like you mentioned head.forward('AB')='BC' and head.forward('BA')='CB'. However, in sequential networks, in most cases the position also adds meaning and is crucial for capturing the relationships between different words . For example, consider these sentences, "A hit B" and "B hit A". In these sentences, the relation between the words is different, in the first one, A is hitting while in the second one A is being hit, but the self attention values will be same corresponding to each word. After the attention layer, there is no mechanism by which model can incorporate the sequence order or position value in calculating the output. The model will output the same values for both the sentences. That's why positional encoding is needed. Order of input sequence does not matter for self attention heads -> This simply means that the self-attention value corresponding to each input word remains the same if its position is changed when positional encoding is not used.
