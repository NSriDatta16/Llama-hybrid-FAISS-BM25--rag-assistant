[site]: crossvalidated
[post_id]: 83374
[parent_id]: 
[tags]: 
Simulate microarray technical error

I need to simulate some microarray experiment datasets. I have the levels of expression of a set of synthetic genes in different experimental conditions. For simplicity of the method this levels are relative intensities (0%-100%). I want to use the error model developed by Rocke et al. (link below): $y=\alpha+\mu e^{\eta}+\epsilon$ where $y$ is the measured gene expression, $\alpha$ is the background level of expression, $\mu$ is the actual level of expression, $\eta \, \tilde{} \, \mathcal{N}\left(0, \sigma_{\eta} \right)$ is a multiplicative error proportional to the expression level that dominates when the actual level of expression is high and $\epsilon \, \tilde{} \, \mathcal{N}\left(0, \sigma_{\epsilon} \right)$ is an additive error that dominates when the actual level of expression is low. To get a microarray expression dataset I need to set these parameters and I would like to get log expression values in range [0,15] I tried setting $\sigma_{\eta}=0.5$ and $\sigma_{\epsilon}=1$ but in general the range of values obtained depends on the parameters' values. I cannot find a way to be sure that the expression values are always in that range. Any suggestions? http://online.liebertpub.com/doi/abs/10.1089%2F106652701753307485
