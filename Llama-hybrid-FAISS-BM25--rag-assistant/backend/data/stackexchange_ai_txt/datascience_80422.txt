[site]: datascience
[post_id]: 80422
[parent_id]: 80421
[tags]: 
I was about to post my answer on the other forum, but it's migrated to here. There are a few crucial things that you should keep in mind: It’s not who has the best algorithm that wins. It’s who has the most data. (Banko and Brill, 2001) Bank and Brill in 2001 made a comparison among 4 different algorithms, they kept increasing the Training Set Size to millions and came up with the above-quoted conclusion. And your data is too few ! Whenever you talk about Linear Models, just remember their enemy -- The Outliers . If you plot your data, you can see that clearly. cross_val_score returns the R^2 by default for almost any Linear Model (i.e Regressor). The best value of this metric = 1 (i.e. totally fit), or = 0 (i.e. horizontal line), or it can be negative (i.e. worse than a horizontal line). More info here . Next in the experiment I conducted, you'll see how the results are valid. An alternative model would be the Multi-layer Perceptron Regressor ; with number of layers = 3, the model would map any complicated function. Cross-Validation would best serve if you have enough data. However in your case, the CV scores vary noticeably. Please ponder the results of the following self-explanatory Experiment: from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score from sklearn.neural_network import MLPRegressor from scipy.stats import pearsonr import numpy as np import matplotlib.pyplot as plt X = np.array([2494.98,2912.6,3397.5,2678,4310,2086.28,3061.8,4095,3997, 6503.88,6732,940.1,1543,5903.85,2861.61,3682.76,2802,3032, 2635,3749,4300.5,9722,3823.33,1648.21,24575,3926,3228,4062,1316.24, 2497.99,12123.94,2057.5,2495,3770.73,864,774.71]).reshape(-1, 1) y = np.array([857951.27,694473.11,1310529.72,199688.14,1377366.95,569312.33,660803.42,1187732.61, 1304793.08,1659629.13,1264178.31,172497.94,598772.4,809681.19,333983.85,1430771.5,1145812.21, 356840.54,543912.8,1004940.27,1889560.55,2137376.95,891633.5,335115.4,19273129.14,1223803.28, 874000,1090000,332718.54,519398.7,2504783.69,957042.37,857951.27,1743978.85,251269.48,192487.26]) X_, y_ = zip(*sorted(zip(X, y))) plt.plot(X_, y_, '-x') plt.title("Plot of Dataset") plt.show() print("Linear Regression :: Before Removing An Outlier") reg = LinearRegression() print(np.average(cross_val_score(reg, X, y, cv=3))) X, y = X_[:-1], y_[:-1] plt.plot(X, y, '-x') plt.title("Plot of Dataset After Removing Outlier") plt.show() print("Linear Regression :: After Removing An Outlier") reg = LinearRegression() print(np.average(cross_val_score(reg, np.array(X).reshape(-1, 1), y, cv=3))) print("Multi-layer Perceptron Regressor :: The Effect of Mapping Complicated / Non-Linear Function") mlp = MLPRegressor(hidden_layer_sizes=(16, 16, 16), random_state=2020, activation='identity', max_iter=1000) print(np.average(cross_val_score(mlp, np.array(X).reshape(-1, 1), y, cv=3))) RESULTS This after removing only one extreme value ( without further exploration nor doing any fancy work like utilizing any outliers detector ). As you can see, there would be no single line that fits all points. Linear Regression :: Before Removing An Outlier Average CVs Score: -1.7085612243433703 Linear Regression :: After Removing An Outlier Average CVs Score: -0.12386365189238795 Multi-layer Perceptron Regressor :: The Effect of Mapping Complicated / Non-Linear Function Average CVs Score: 0.16131374234257037
