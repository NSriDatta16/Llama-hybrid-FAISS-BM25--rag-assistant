[site]: datascience
[post_id]: 36302
[parent_id]: 36285
[tags]: 
First of all, just to be clear, you shouldn't evaluate the performance of your models on the balanced data set. What you should do is to split your dataset into a train and a test set with ideally the same degree of imbalance. The evaluation should be performed exclusively on the test set, while the balancing on the training set. As for your question, any macro averaged metric should do just fine for proving that your balancing technique is effective. To calculate such a metric (let's say accuracy for simplicity), you just need to compute the accuracies of each class individually and then average them. Example : We trained two models m1 and m2 , the first without balancing the dataset and the second after using SMOTE to balance the dataset. Actual values : 0, 0, 0, 0, 0, 0, 0, 0, 1, 1 Predicted m1 : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 Predicted m2 : 1, 0, 0, 1, 0, 1, 0, 0, 1, 1 How would we normally calculate accuracy? $acc = \frac{correct \, predictions}{total \, predictions}$ How do our two models perform on this metric? $acc_1 = \frac{8}{10} = 80\%$ $acc_2 = \frac{7}{10} = 70\%$ According to this performance metric, m2 is better than m1 . However, this isn't necessarily the case as m1 just predicts the majority class! In order to show how m2 is better than m1 , we need a metric that treats the two clases as equals. We' ll now try to calculate a macro-averaged accuracy. How? First we'll calculate the accuracy for each class separately, and then we'll average them: For m1 : $acc_1^0 = \frac{8}{8} = 100\%$ m1 's accuracy on class 0 $acc_1^1 = \frac{0}{2} = 0\%$ m1 's accuracy on class 1 $macro\_acc_1 = \frac{acc_1^0 + acc_1^1}{2} = \frac{100\% + 0\%}{2} = 50\%$ For m2 : $acc_2^0 = \frac{5}{8} = 62.5\%$ m2 's accuracy on class 0 $acc_2^1 = \frac{2}{2} = 100\%$ m2 's accuracy on class 1 $macro\_acc_2 = \frac{acc_2^0 + acc_2^1}{2} = \frac{62.5\% + 100\%}{2} = 81.25\%$ Notes : Macro averaging can be applied to any metric you want, however it is most common in confusion matrix metrics (e.g precision, recall, f1). You don't need to implement this by yourself, many libraries already have it (e.g. sklearn's f1_score has a parameter called average , which can be set to "macro" )
