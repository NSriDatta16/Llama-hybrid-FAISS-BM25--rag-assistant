[site]: crossvalidated
[post_id]: 643253
[parent_id]: 
[tags]: 
Sensible neural network architecture for image stitching?

I am currently trying to design a neural network for image stitching, but I am having trouble coming up with a neural network architecture that seems suitable for the job. The input to my neural network will be the two images in the top row, which are overlapping in some way (the mask below gives the overlap region where the yellow part is the overlap) My desired output is also the two masks (or just one of them to begin with since in this simple case the other one can be trivially found from it) To begin with I work with the following simplifications which might change later (both images are of the same size, and scale, both images are oriented the same way, there is no noise) Now my initial idea for this was just to use a standard U-net architecture and just stack the channels (in this case the images are grayscale so they each only have one channel, so my input is just a 2 channel image). However, this does not work, which is probably due to a combination of two things. 1) The two images are immediately completely mixed when using a 2D convolution, so maybe I should use something like https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html option of groups=2 to keep the convolutions separate to begin with. 2) The bigger problem is likely that the things I want to actually compare are far from each other in these two images. I am looking for identical features in the two images, but separated by large pixel distances. My current network looks like this: class UNet(nn.Module): def __init__(self, n_channels, n_classes, bilinear=False, groups=1): super(UNet, self).__init__() self.n_channels = n_channels self.n_classes = n_classes self.bilinear = bilinear self.inc = (DoubleConv(n_channels, 64, groups=groups)) self.down1 = (Down(64, 128,groups=groups)) self.down2 = (Down(128, 256, groups=groups)) self.down3 = (Down(256, 512, groups=groups)) factor = 2 if bilinear else 1 self.down4 = (Down(512, 1024 // factor, groups=groups)) self.up1 = (Up(1024, 512 // factor, bilinear, groups=groups)) self.up2 = (Up(512, 256 // factor, bilinear, groups=groups)) self.up3 = (Up(256, 128 // factor, bilinear, groups=groups)) self.up4 = (Up(128, 64, bilinear, groups=groups)) # self.outc = (OutConv(64, n_classes)) self.out = nn.Conv2d(64, n_classes, kernel_size=1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): x1 = self.inc(x) x2 = self.down1(x1) x3 = self.down2(x2) x4 = self.down3(x3) x5 = self.down4(x4) x = self.up1(x5, x4) x = self.up2(x, x3) x = self.up3(x, x2) x = self.up4(x, x1) x = self.out(x) y0 = self.sigmoid(x) y1 = y0.flip(dims=(2,3)) y = torch.cat([y0,y1],dim=1) return y class DoubleConv(nn.Module): """(convolution => [BN] => ReLU) * 2""" def __init__(self, in_channels, out_channels, mid_channels=None, groups=1): super().__init__() if not mid_channels: mid_channels = out_channels self.double_conv = nn.Sequential( nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False,groups=groups), nn.BatchNorm2d(mid_channels), nn.ReLU(inplace=True), nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False, groups=groups), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True) ) def forward(self, x): return self.double_conv(x) class Down(nn.Module): """Downscaling with maxpool then double conv""" def __init__(self, in_channels, out_channels, groups=1): super().__init__() self.maxpool_conv = nn.Sequential( nn.MaxPool2d(2), DoubleConv(in_channels, out_channels, groups=groups) ) def forward(self, x): return self.maxpool_conv(x) class Up(nn.Module): """Upscaling then double conv""" def __init__(self, in_channels, out_channels, bilinear=True, groups=1): super().__init__() # if bilinear, use the normal convolutions to reduce the number of channels if bilinear: self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, groups=groups) else: self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2, groups=groups) self.conv = DoubleConv(in_channels, out_channels, groups=groups) def forward(self, x1, x2): x1 = self.up(x1) # input is CHW diffY = x2.size()[2] - x1.size()[2] diffX = x2.size()[3] - x1.size()[3] x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2]) # if you have padding issues, see # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd x = torch.cat([x2, x1], dim=1) return self.conv(x) Does anyone have any good idea for what kind of neural network to use for this?
