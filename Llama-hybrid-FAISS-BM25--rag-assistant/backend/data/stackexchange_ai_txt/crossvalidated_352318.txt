[site]: crossvalidated
[post_id]: 352318
[parent_id]: 352082
[tags]: 
Dimension reduction is always a trade-off between resources and precision. You can use PCA (setting a high value of n, say 5000) and use the measure of exactly how much variation is addressed by each component to try and determine the number of principle components to keep. Maybe 80% is enough, but if you've only explained 30% of your data's variance after 5000 principle components, then maybe you need an alternative. One way to plot this visually is to plot the number of components as x, and the explained variance as y. You're likely to get a curve that starts off shallow, and gets steeper at some point (or starts steep, and levels off to shallow, depending on which way your x-axis is ordered). Finding the "elbow" of this curve will deliver the point at which the cost of adding an additional component yields less and less explanatory value. It's not a perfect answer, but will give you an indication of roughly where to set your n . sklearn.decomposition.PCA has an output that shows the explained variance after fitting - use this to help get a better feel for your data. So the short version of this is, you can run PCA with any number n you want, but only pick the top k components where in combination, they do a good enough job (where "good enough" is defined by you) of explaining the variance in your data. Alternately, look into building an auto-encoder using a convolutional neural net (Keras/Tensorflow), these tend to do very good job of compression, especially where more linear methods (like PCA) find the task more challenging. [EDIT] Also, to address your final point - PCA wont ever reduce the feature space to a value less than the number of observations. That is, where you're feeding it with a small number of observations vs a large number of features, you can't force PCA to reduce the feature-size to a value lower than your observation count. Either build up more data, or generate it using your initial dataset as a starting point, and duplicating it (sensibly, adding some random variation perhaps) so that you can reliably squirt in a large enough dataset. This itself might pose you some problems, since now you'll have to deal with a larger memory footprint due to the input length, rather than the feature-width. Brute-forcing it so that your PCA squeezes your feature-width down to only 50 components would limit your issue in the first place, and perhaps your resultant feature space will continue to provide you with enough explanation of variance to be useful. It's common to only use maybe 5 components for example, but with an associated loss of precision. The truth lies in your data.
