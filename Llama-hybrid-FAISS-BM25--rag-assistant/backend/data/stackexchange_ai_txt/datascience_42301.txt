[site]: datascience
[post_id]: 42301
[parent_id]: 42287
[tags]: 
Neural networks are different from other Machine Learning techniques in that they have to learn by repetition. In Neural Network parlance, the repetition is called an epoch. During an epoch, each instance, in your case each word, is evaluated and the error is applied backwards through the layers and the weights are adjusted. The changes to the weights can happen more or less aggressively based on the hyperparameters, but they tend to only move a little at a time. You have trained your word2vec model for two epochs. I didn't see how you initialized the weights for your model, but typically they are set to random values or set to a zeros. Given your results look a little random, I am sure it is because the model has barely begun to learn. In order to get a well trained word embedding model, or any other Neural Network, you may need to train for hundreds, thousands, or even millions of epochs. This is why the pre-trained word embedding models are so valuable even though they are freely available. If you just want a well trained embedding model, download it. If you need to incorporate some domain specific usage, you can further train the downloaded embeddings. HTH
