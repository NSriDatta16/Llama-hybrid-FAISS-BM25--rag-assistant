[site]: crossvalidated
[post_id]: 280032
[parent_id]: 
[tags]: 
How embeddings relates to weights in skip-gram model (word2vec)?

I'm trying to understand how a skip-gram model trains itself. Using the input embedding we predict the probability of the output word. And then using the gradient of the cost function we slightly update embedding of the output. But also we update weights. So I cannot understand how we simultaneously update weights and embeddings? Do they use different gradients or there is some sort of chaining? I'm trying understand the logic of training weights and embeddings using this example ( github ).
