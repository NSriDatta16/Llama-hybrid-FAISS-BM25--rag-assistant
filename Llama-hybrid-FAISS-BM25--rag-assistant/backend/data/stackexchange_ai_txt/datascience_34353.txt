[site]: datascience
[post_id]: 34353
[parent_id]: 32846
[tags]: 
In my experience, with very high dimensional climate data, if I do something as easy as k-means clustering, I would probably first look at the silhouette values of the clusters. As you would probably know, a high silhouette value for the clusters would mean that they are well classified and distinctly different from each other, while a low silhouette value or even a negative one would mean the opposite. In case these values are reasonable, you can then try to reduce the dimensionality of your data set by doing something like PCA (for starters or even something like t-SNE ) and see if your clustering results are still that good. If they are so, then you have a sense that those components (PCs in case of PCA or the retained components in case of t-SNE) are probably dominating the feature space that is used for clustering. In my very personal opinion, I would try to reduce(dimensionality) as much as I can and go about doing the clustering until I can visualize them in 3D or 2D. With t-SNE this can be done quite successfully for the right kind of data sets. Sometimes, if you are lucky, even comparing L2 distances between cluster centers and samples may yield something fruitful but that's hardly ever the case for high dimensional data. I guess you're question is a little open ended and a discussion on such would be great. But, once again, if I understand it correctly, extracting the part of the feature space that dominates the " distance" metric for clustering is difficult to find algorithmically in my limited knowledge but some sense can be extracted if you try these exercises.
