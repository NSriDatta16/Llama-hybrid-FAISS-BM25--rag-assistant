[site]: crossvalidated
[post_id]: 151568
[parent_id]: 
[tags]: 
Efficiently processing a large MxNx2 logistic regression, only interactions matter

I'm working with a large 3-way contingency table (roughly $180 \times 40 \times 2$) — both independent variables are categorical and the response is binary. One independent variable (X) represents a classification of experimental test objects into types, and the other (Y) a set of test conditions. The experiment is only balanced for Y - that is, each test object was tested in all Y conditions, but each test object is only one X type, and the number of objects assigned to each type varies by three orders of magnitude (40­–69600). Response to the test is either "normal" or "anomalous", and, critically, I know a priori that only interactions between X and Y are interesting. I know it's necessary to model the main effects of X and Y as well, but (in the context of the larger experiment) they are noise factors. So the thing that I care about is identifying all pairs of levels of X and Y such that the interaction is significant (given some threshold). In R-ese, the natural model is glm(cbind(anom, total-anom) ~ X * Y, family="binomial", data=apt.d) where apt.d (a $10\times10\times2$ downsample) is at the end of this question. Now, I have two problems: summary(fit object) tells me that this model leaves me with zero residual degrees of freedom and a residual deviance of $4 \times 10^{-10}$, which makes me worry about overfitting, but I don't see how I could structure the model any other way. Actually running the fit on the full data frame is painfully slow -- nearly an hour just to generate the fit object. And then it says that the fit didn't converge and "fitted probabilities numerically 0 or 1 occurred". Is there an alternative implementation of logistic regression in R that would be more appropriate for this size of data set and/or this sort of unbalanced data with widely varying levels of anomalous response? Downsampled data: apt.d
