[site]: crossvalidated
[post_id]: 243588
[parent_id]: 
[tags]: 
How to apply Softmax as Activation function in multi-layer Perceptron in scikit-learn?

I need to apply the Softmax activation function to the multi-layer Perceptron in scikit. The scikit documantation on the topic of Neural network models (supervised) says "MLPClassifier supports multi-class classification by applying Softmax as the output function." The question is how to apply the function? In the code snip below, when I add the Softmax under the activation parameter it does not accepts. MLPClassifier(activation='Softmax', alpha=1e-05, batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant', learning_rate_init=0.001, max_iter=200, momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False) The error code is: ValueError: The activation 'Softmax' is not supported. Supported activations are ('identity', 'logistic', 'tanh', 'relu'). Is there a way to apply the Softmax activation function for multi-class classification in scikit-learn?
