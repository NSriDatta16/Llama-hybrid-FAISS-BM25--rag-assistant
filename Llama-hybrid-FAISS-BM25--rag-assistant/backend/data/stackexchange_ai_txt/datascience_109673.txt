[site]: datascience
[post_id]: 109673
[parent_id]: 
[tags]: 
How to deal with words out of the vocabulary CBOW implementation

I'm studying word2vec theory, and I decided to implement the Continuous Bag of Words model from zero. I know the primary pipeline for this: Preprocess a corpus: remove stopwords, lemmatization, etc. Creates a vocabulary from the preprocess corpus (this vocabulary may have fewer tokens than the entire corpus). Coding the neural network architecture using Pytorch, Keras, etc. Then, training the model using the preprocess corpus. Finally, check the quality of the word embedding using a downstream NLP task, such as analogy, word similarity, etc. The inputs for the CBOW are the target word and its contexts. But what happens when the algorithm reviews a sequence of texts and encounters a target word or contexts words that are out of the vocabulary? Does the model ignore these words? Thank!
