[site]: crossvalidated
[post_id]: 611132
[parent_id]: 357466
[tags]: 
I generally agree with your premise that there is an over-fixation on balancing classes, and that it is usually not necessary to do so. Your examples of when it is appropriate to do so are goods ones. However, I disagree with your statement: I conclude that unbalanced classes are not a problem, and that oversampling does not alleviate this non-problem, but gratuitously introduces bias and worse predictions. The problem in your predictions is not the oversampling procedure, it is the failure to correct for the fact that the base-rate for positives in the "over-sampled" (50/50) regression is 50%, while in the data it is closer to 2%. Following King and Zeng ("Logistic Regression in Rare Events Data", 2001, Political Analysis , PDF here ), let the population base rate be given by $\tau$ . We estimate $\tau$ as the proportion of positives in the training sample: $$ \tau = \frac{1}{N}\sum_{i=1}^N y_i $$ And let $\bar{y}$ be the proportion of positives in the over-sampled set, $\bar{y}=0.5$ . This is by construction since you use a balanced 50/50 sample in the over-sampled regression. Then, after using the predict command to generate predicted probabilities $P(y|x,d)$ we adjust these probabilities using the formula in King and Zeng, appendix B.2 to find the probability under the population base rate. This probability is given by $P(y=1|x,d)A_1B$ . In the case of two classes: $$ P(y=1|x,d)A_1B = \frac{P(y=1|x,d) \frac{\tau}{\bar{y}}}{P(y=1|x,d) \frac{\tau}{\bar{y}} + P(y=0|x,d) \frac{1-\tau}{1-\bar{y}}} $$ Since $\bar{y}=0.5$ this simplifies to: $$ P(y=1|x,d)A_1B = \frac{P(y=1|x,d) \tau}{P(y=1|x,d) \tau + P(y=0|x,d) (1-\tau)} $$ Modifying your code in the relevant places, we now have very similar Brier scores between the two approaches, despite the fact that the over-sampled training sample uses an order of magnitude less data than the raw training sample (in most cases, roughly 450 data points vs. 10,000). So, in this Monte Carlo study, we see that balancing the training sample does not harm predictive accuracy (as judged by Brier score), but it also does not provide any meaningful increase in accuracy. The only benefit of balancing the training sample in this particular application is to reduce the computational burden of estimating the binary predictor. In the present case, we only need ~450 data points instead of 10,000. The reduction in computational burden would be much more substantial if we were dealing with millions of observations in the raw data. The modified code is given below: library(randomForest) library(beanplot) nn_train 0 ) break } dataset_train
