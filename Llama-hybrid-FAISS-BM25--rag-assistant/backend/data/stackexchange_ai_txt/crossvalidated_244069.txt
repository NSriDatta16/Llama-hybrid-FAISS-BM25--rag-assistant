[site]: crossvalidated
[post_id]: 244069
[parent_id]: 
[tags]: 
Best way to choose the parameter set from unstable repeated cross-validition results

I try to predict a regression task using random forest. My data set is relatively small (little above 1000 observations in the training set). In order to tune the model I perform repeated cross-validation (t = 10 and k = 10) and GridSearch (mtry and bootstrap sample size). This result into $k \times t$ best parameter sets. Among these parameter sets I get about 4 different parameter sets that appear often as the best model, e.g. mtry is sometimes 20 and sometimes 50. Since the training set is not large there is also a quite big variance in the (RMSE) scores. How do I choose the final parameter set from such a result. Three Options I thought of: I take the parameter set that most frequently appears as the best set in the repeated $k$-folds I take the parameter set with the best average prediction (RMSE) score among the best models in the repeated $k$-folds I run another outer cv loop (nested cv) and choose the parameter set that has the best average score there. Which option is preferable or are there any other solutions to this?
