[site]: crossvalidated
[post_id]: 475033
[parent_id]: 
[tags]: 
Is there any "maximizing" in a Bayesian model?

I am not a statistician, so please bear that in mind. Suppose you have a simple model: $$ y_i \sim Normal(\mu, \sigma) \quad i = 1 \ldots n \\ \mu \sim f(\cdot) $$ where $f$ is some distribution for $\mu$ (and assume its not a conjugate distribution). The posterior distribution of $\mu$ can be found using Bayes Theorem, i.e. $$ p(\mu | y_1, y_2, \ldots, y_n) \propto p(y_1, y_2, \ldots, y_n | \mu) f(\mu) $$ where the right hand side product terms are the likelyhood and the prior. To find the posterior distribution, one can make use of an MCMC algorithm, for instance the Metropolis Hastings algorithm. In this algorithm, a Markov Chain is constructed so that the stationary distribution is the posterior distribution, and samples can be drawn using this MC. My question is at which point does the observed data $y = (y_1^*, y_2^*, \ldots, y_n^* )$ get used to accurately predict the posterior? I was thinking something along the lines of sampling from the posterior distribution from $\mu$ , using this drawn sample to sample $y_i$ and comparing against the observed value $y_i^*$ . For example, in this JAGS pseudocode: for i = 1:n y[i] ~ dnorm(mu, sigma=10) end mu ~ dnorm(0, 1) Here the y (the data) gets sampled from the Normal distribution, but no comparison is made against the observed value. My interpretation: I am guessing that the observed values are used only once in determining the formula for the posterior distribution. After the expression for the posterior is derived, MCMC takes over and data is no longer used. I guess I am a little shocked that the likelihood function can contain so much information to yield the posterior this way.
