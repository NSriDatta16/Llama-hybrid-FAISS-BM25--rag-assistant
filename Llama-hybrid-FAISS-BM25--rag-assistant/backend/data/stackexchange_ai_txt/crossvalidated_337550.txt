[site]: crossvalidated
[post_id]: 337550
[parent_id]: 337262
[tags]: 
Note: I use capital letters to refer to an entire vector, and lower case letters to refer to a specific observation of a vector. Hopefully this isn't confusing. I think the answer ought to be no . I offer an intuitive explanation, as well as some quick and dirty (emphasis on dirty ) R code that supports my assertion. The variance of the $Y$ values is increasing with $x$ , because even though the range of $Y$ appears to be somewhat constant over the domain of $X$ , the average distance of the points from their mean (as a function of $x$ ) is actually increasing. You can see this because the density of $Y$ in regions between its extents is actually decreasing with increasing $x$ , and therefore the average squared deviation (variance) from the mean is increasing. In other words, pretty much all the $y$ values' separation from the mean (conditional on $x$ ) are "really large" for large $x$ , while at smaller $x$ , they're anywhere from zero to "really large." Anyway, I wouldn't be surprised if folks find that explanation really confusing. If so, just run the following in R, and observe the resulting plot. set.seed(999) n.lines = (i-1))] var(y) }) par(new=TRUE) plot(x=buckets, y=var.y, col='blue', type='l', ylim=ylim, xlim=xlim, ylab=ylab, xlab=xlab) This is far from exact, but the data look a lot like yours. I bucketed $Y$ into about ten different intervals (based on integer values of $X$ ) and computed their variance; the blue line shows this in the plot. As you can see, it is increasing with $x$ , and therefore the data are not homoscedastic.
