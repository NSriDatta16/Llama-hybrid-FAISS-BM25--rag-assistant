[site]: crossvalidated
[post_id]: 239344
[parent_id]: 193833
[tags]: 
If you use RNNs, static features can be fed just like varying features. As noted, with most out-of-the box implementations (if any could be called that) you'll feed the same values over and over again. I think we need to be implementation- and data-specific to get a useful discussion about how this can be approached, or if it's worth approaching. Depending on the size of your data and your level of skill this performance gain will most likely be outweighed by the time it takes to implement. It all comes down to how much much you like abstraction, which implementations you are familiar with etc. If you are predicting multiple time series with the same step it sounds like you have a regression problem in each step. Let your network have many outputs and set the loss function as sum of squares $\textrm{loss}_t = \sum_k (y_k(t)-\hat{y}_k(t))^2$ . You could also embed it into some kernel and learn the parameters in that space, like letting your network output the parameters in a gaussian. To get useful answers I'd recommend you to give the dimensions of the data: How long is the longest sequence? How many sequences do you have? What's the distribution of sequence lengths? How large temporal dependencies do you expect to need? etc
