[site]: datascience
[post_id]: 64557
[parent_id]: 56476
[tags]: 
For your first part of the question as to which question generation approaches are good - Neural question generation is being pretty popular (as of 2018/2019) among NLP enthusiasts but not all systems are great enough to be used directly in production. However, here are a few recent ones which reported the state-of-art performances in 2019 and have shared their codes too: https://github.com/ZhangShiyue/QGforQA https://github.com/PrekshaNema25/RefNet-QG This one is a 2020 one (now that NLP performances have been improved with Transformers) 3. https://github.com/patil-suraj/question_generation Besides, if you want more control as to understand and fix for wrongly generated questions, I would suggest the more traditional rule-based approach like the below which is more reliable than the above neural ones ones and generates a larger amount of question-answer pairs than the above 2: http://www.cs.cmu.edu/~ark/mheilman/questions/ https://bitbucket.org/kaustubhdhole/syn-qg/src/master/ To answer your second question, if your QG model is generating an answer, then it makes sense to use cosine similarity. Assuming your question generation is at the sentence level, you will mostly have short answer spans and hence averaging Glove or Paragram word vectors might serve you better results than the Universal Sentence Encoder.
