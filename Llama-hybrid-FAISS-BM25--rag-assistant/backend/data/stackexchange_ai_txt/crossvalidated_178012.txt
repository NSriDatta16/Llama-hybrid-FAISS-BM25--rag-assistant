[site]: crossvalidated
[post_id]: 178012
[parent_id]: 
[tags]: 
Definition of complexity of a tree in xgboost

Doing research about the xgboost algorithm I went through the documentation . In this approach trees are regularized using the complexity definition $$ \Omega(f) = \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 $$ where $\gamma$ and $\lambda$ are parameters, $T$ is the number of terminal leaves and $w_j$ is the score in each leaf. I wonder: how does this define complexity? $T$, the number of terminal nodes, seems natural to me. But the sum of final scores squared? Maybe overfitting is meant. Meaning that very large scores give too much confidence? Is it chosen to get a weak learner? What is a natural explanation for this choice of the complexity function?
