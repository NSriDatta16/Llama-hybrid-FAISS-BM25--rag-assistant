[site]: crossvalidated
[post_id]: 633715
[parent_id]: 
[tags]: 
Linear regression performs considerably better than (linear) neural network

I looked through other questions on stats.exchange, but could not quite find a similar question, this post probably comes the closest to it. Since (fully-connected) neural networks with only one layer and no activation function is linear regression, even though it is trained with some form of gradient descent, I expect the performance to be relatively similar to a linear regression performed by ordinary least squares. But in my case they differ by a factor of up to 2 or 3! Is this to be expected? A bit of background: I'm using neural networks to predict ocean wave heights from some points along the coast to other points. The further you go offshore (and if you don't include certain physical interactions in your calculations) the more linear the problem gets. That is why I started to compare my results with a (OLS) linear regression and found that it outperforms my best neural networks (by around a factor of 2). Which was shocking at first, but somehow explicable given the strong linear relationship for most of the domain. However, when analyzing the errors I can see that linear regression performs worse close to the shore, where non-linearity is the strongest, but if I train a (non-linear) neural network only for this region it still performs worse than a simple linear regression! But I also want to note that the neural networks they don't perform bad at all. Quite in contrast, the results are very promising, it's just surprising that they get outperformed so easily even in places where the setting is more non-linear than linear. Data set: Input is of shape (2918, 69) of which 80% is used for training, 20% is used for validation. Each row consists of the wave height at a certain location in the domain. The output is of shape (2918, 3773) so the neural network and linear regression are performing super-resolution. Equally, each row consists of the wave height at (different) locations in the domain. Neural Network Architecture As mentioned above, I try normal linear regression with OLS. A simple neural network with one layer and without activation function (so again, a linear regression) with either SGD or ADAM, where I trained different learning rates and many, many epochs (going up to 5000). The batch sizes that I tried are 16, 32, and 512 (just to go wildly different). Weights are updated after each batch. The two other networks are not too important for the question, since I am wondering more about the different between the linear approaches, but I use either: 1. A fully-connected neural network with multiple hidden layers (ranging from 2 to 5) with varying unit sizes (128 to 2056), and ReLU activation. 2. A graph neural network that performs multiple Chebyshev Convolutions on the input, than upsamples it to the output size and performs yet 2-3 additional convolutions. Both non-linear networks perform well and are more or less equal, but the linear regression is still best. Questions Is it to be expected that a linear neural network performs that much worse than a linear regression, even though it's theoretically the same? Is it to be expected that the linear regression outperforms the neural network even in cases where the setting is mostly non-linear (even though some linear regions exist) What I tried Since it looked like a convergence problem or a bug, I tried different learning rates, different optimizers (SGD with and without momentum, ADAM), different loss functions (l1 and l2 loss) and implemented everything in 2 different libraries (pytorch and tensorflow), with different neural network architectures (fully-connected and graph neural networks). I tried to normalize and not normalize my data, there's still a large discrepancy. I let everything run also for multiple epochs Is there something obvious maybe that I missed or that I could try? Are the results surprising? Especially the discrepancy between OLS linear regression and gradient descent linear regression? I know there's no code or images associated to this question, but I hope the overall problem setting is still clear. Thank you already a lot for any answers!
