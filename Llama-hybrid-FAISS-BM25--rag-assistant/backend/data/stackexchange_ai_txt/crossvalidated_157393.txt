[site]: crossvalidated
[post_id]: 157393
[parent_id]: 157391
[tags]: 
Evaluating your models based on a test set with only one class is a bad idea. What do you expect to learn from this? Secondly, discrete metrics like accuracy, precision, recall are poor choices to assess which model is best. It is far better to use metrics like area under the ROC/PR curve. Assuming you are talking about the same data set, then based on your description it is quite clear that the LIBLINEAR model is less conservative, e.g. has a lower threshold for positive predictions (that is, it predicts more positives). Hence, the LIBLINEAR model has higher TPR (= higher 'accuracy' on your first test set with only positives) and higher FPR (= lower 'accuracy' on your second test set with only negatives). For all you know, both models might be identical except for a translation (this is quite common for linear SVM), so in terms of AUC they would be identical, while discrete measures like accuracy may mislead you into believing that the models themselves are substantially different. Additionally, did you use the same hyperparameters when making these comparisons (that is, the same regularization and misclassification penalties per class in both toolboxes)? If your models are parameterized differently, then ofcourse their performance is different. That has nothing to do with the software, though.
