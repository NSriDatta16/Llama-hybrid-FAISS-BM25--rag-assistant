[site]: crossvalidated
[post_id]: 71008
[parent_id]: 70994
[tags]: 
@Hotaka's answer is quite correct. Ranking makes transformation unnecessary; it is itself a transformation that ignores exact values except in so far as they lead to differences in rank. In fact, a little thought, or some example calculations, will show that the results after ranking logarithms or square roots or any other monotonic transformation are exactly the same as those after ranking the original data. But more can be said. The either-or thinking Either my data are normally distributed, and I can use standard or classical procedures. Or I need to resort to rank-based tests. is a little stark, and (it may be suggested) over-simplified. Although it's hard to suggest exactly what you should be doing without some sight of your data and your precise goals, there are other perspectives: Many users of statistics look at marginal (univariate) distributions and assess whether they are close to normality, but that may not even be relevant. For example, marginal normality is not required for regression-type procedures. For many procedures, it's how the means behave, not how the data behave, that is more important and closer to the main assumptions. Even (say) a significant result at conventional levels for a Shapiro-Wilk test is equivocal in terms of guiding later analysis. It just says "your distribution is detectably different from a normal distribution". That itself does not imply that the degree of non-normality you have makes whatever you have in mind invalid or absurd. It may just mean: go carefully, as underlying assumptions are not exactly satisfied. (In practice, they never are exactly satisfied, any way.) The habit to cultivate is that of thinking that all P-values are approximations. (Even when assumptions about distributions are not being made, assumptions about sampling or independence or error-free measurement are usually implicit.) Although many texts and courses imply otherwise, non-parametric statistics is something of a glorious dead end: there are a bundle of sometimes useful tests, but in practice you give up on most of the useful modelling that is central to modern statistics. Outliers are mentioned here, and they always deserve close attention. They should never be omitted just because they are inconvenient or appear to be the reason why assumptions are not satisfied. Sometimes analysis on a transformed scale is the best way forward. Sometimes a few mild outliers are not as problematic as less experienced users of statistics fear. With small samples, data will often look ragged or lumpy, even if the generating process is quite well-behaved; with large samples, a single outlier need not dominate the rest of the data. There is always the option of doing both kinds of tests, e.g. Student's t and Mann-Whitney-Wilcoxon. They don't ask exactly the same question, but it is often easy to see if they point in the same direction. That is, if a t test and the other test both give clear signals that two groups are different, you have some reassurance that your conclusion is well supported (and some defence against the sceptic who distrusts one or other procedure given a whiff of non-normality). If the two tests give very different answers, this in itself is useful evidence that you need to think very carefully about how best to analyse data. (Perhaps that massive outlier really does determine which way the answer comes out.) With experience, users of statistics are often more informal than texts or courses imply they should be. If you talked through an analysis with them, you would often find that they make quick judgements such as "Sure, the box plots show some mild outliers, but with data like this analysis of variance should work fine" or "With skew that marked, a logarithmic scale is the only sensible choice". I don't think you will often find them choosing techniques based on whether a Shapiro-Wilk test is or is not significant at $P
