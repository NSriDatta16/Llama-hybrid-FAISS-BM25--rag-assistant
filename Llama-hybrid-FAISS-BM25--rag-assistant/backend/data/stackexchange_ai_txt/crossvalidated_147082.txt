[site]: crossvalidated
[post_id]: 147082
[parent_id]: 147079
[tags]: 
The leave-one-out method is a cross-validation technique. If you have a data set with N=100 samples, you will run 100 train/test iterations where, in each iteration, the train set will have 99 test examples and 1 training example. This way you find out, for each sample, what the generated classification is for the sample when training on all other samples. You get a final error value by averaging the error over all iterations. It is a particular case of k-fold cross validation ( k=N ) where you divide your data set into k equal-sized subsets and train on k-1 of them while you test on the remaining subset, and repeat that scenario for all combinations of k-1 training subsets and 1 test subset. You then get an error value by averaging the error of all iterations.
