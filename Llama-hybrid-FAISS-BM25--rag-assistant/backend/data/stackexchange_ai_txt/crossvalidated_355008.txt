[site]: crossvalidated
[post_id]: 355008
[parent_id]: 354417
[tags]: 
Well, Yolo is a rather successful approach to this problem, so I would suggest looking into why it doesn't work a bit more. Perhaps there is a bug in implementation or something of the sort. Anyway, one thing you could also consider in order to help balance loss terms is to take their max. For instance, if $L_c(D)$ is the classification loss and $L_\ell(D)$ is the localization loss on some set of data $D$, consider using: $$ L(D) = \max\{ \alpha L_c(D), \beta L_\ell(D) \} $$ for some $\alpha,\beta\in\mathbb{R}$. This means that if the network focuses too much on minimizing one loss, the other loss will grow bigger and become the focus of the algorithm instead. This guarantees it cannot simply ignore one of the loss terms. This sort of approach is reasonably common (e.g. it used in FoldingNet's autoencoder to balance the halves of the Chamfer distance).
