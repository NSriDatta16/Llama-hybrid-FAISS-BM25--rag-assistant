[site]: crossvalidated
[post_id]: 113996
[parent_id]: 104210
[tags]: 
From my experience, this sounds like a bad idea. Anecdotal evidence: On a similarly dimensioned (artificial) data-set I once compared the optimization of a logistic regression by gradient descent, fmincg, and downhill-simplex. Gradient descent and fmincg did a good job in under a minute, arriving at almost identical solutions. Downhill simplex thrashed around for half an hour, and reported that it was "done" without accomplishing anything useful. So I suggest that, if at all possible, you calculate that missing gradient. It shouldn't be difficult: Automatic-differentiation packages exist in just about every programming language. Also note that the standard cross-entropy/maximum-likelihood cost of the logistic regression has the nice property that it is convex, making it easy to optimize. Many other cost functions (square-error for example) do not have this property.
