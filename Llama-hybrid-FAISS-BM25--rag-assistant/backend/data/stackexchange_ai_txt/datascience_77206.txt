[site]: datascience
[post_id]: 77206
[parent_id]: 
[tags]: 
Does BERT pretrain only on masked tokens?

I was a bit confused on the details of the Masked Language Model in BERT pretraining. Does the model only predict the masked tokens for the purposes of pretraining or does it predict it for all tokens?
