[site]: crossvalidated
[post_id]: 277040
[parent_id]: 
[tags]: 
Cross-validation: set aside the validation set based on items or participants?

Here is the structure of my data set: group A and B rated 256 items (each group used a different dependent measure) and I found a significant correlation between their ratings. Now I want to do a cross-validation on this data set but am not sure whether I should set aside some items (e.g., 30%) or some participants from each group for validation. Which is the right practice, or does it depend on my purpose? Thanks! Edit: Two groups of people saw 256 binary sequences (e.g., HHTTHTHT). Group A (40 people) classified each sequence into random/nonrandom while group B (40 people) classified each as "generated by a human/natural phenomenon". So, each sequence has a "nonrandom score" (the proportion of people classified it as "nonrandom") and an "agent score" (the proportion of people classified it as "generated by a human). Looking at the 256 sequences, I found a strong correlation between the two scores. In order to test how robust the correlation is, should I build a model lm(agent~nonrandom, data=myData) based on, say, 180 randomly selected sequences and test how well it works on the rest of the 76 sequence? Alternatively, should I build lm(agent~nonrandom, data=myData) based on 28 people in group A and 28 people in group B and see how it extends to the rest of 12 people in each group? Thanks!
