[site]: crossvalidated
[post_id]: 602835
[parent_id]: 
[tags]: 
Expected value of the max of scaled powers of the same random variable

I've been looking at order statistics and the behavior of expectations when max is involved, but that literature always discussed iid random variables whereas I have a strange situation where I have only one random variable that's being scaled and exponentiated by deterministic quantities so it's not clear to me if that's relevant. I have a random variable $X \sim Beta(\alpha, \beta)$ , and I want to compute $$\mu = E[\max \lbrace a_1 X^{b_1}, a_2 X^{b_2}, \cdots, a_n X^{b_n} \rbrace]$$ for known $a_i$ and $b_i$ . If it helps, my $a_i \in (0, 1]$ are strictly increasing and $b_i \in (0, 1]$ also strictly increasing. I know that $E[X^c] = \frac{B(\alpha + c, \beta)}{B(\alpha, \beta)}$ (where $B(\cdot, \cdot)$ is the Beta function), and I can convince myself numerically that if all $a_i = 1$ , then $\mu = \max_i \lbrace E[X^{b_i}] \rbrace$ . But I'm not sure how to show this analytically so I'm not sure what to do when I see that numerically, $\mu \neq \max_i \lbrace a_i E[X^{b_i}] \rbrace$ , there's a small but regular bias in my Monte Carlo simulation. I feel like this case, where I just have a single random variable, should be quite straightforward to handle but obviously I'm missing some perspective in handling situations like this. Can I make progress with this? Some Python code leveraging Numpy and Scipy to reproduce the results above with some example numbers followsâ€” from scipy.stats import beta as betarv from scipy.special import gamma import numpy as np def monteCarloTest(alpha=2, beta=7, size=1_000_000, n=5): xs = betarv.rvs(alpha, beta, size=size) bvec = (np.arange(n + 1) / (n + 1 - 1))[1:] arr = xs**(bvec[:, np.newaxis]) monteCarloA1 = np.mean(np.max(arr, axis=0)) analyticalMeans = gamma(alpha + beta) * gamma(alpha + bvec) / ( gamma(alpha) * gamma(alpha + beta + bvec)) analyticalA1 = np.max(analyticalMeans) # Ok that makes sense, E[max(X ** b)] = max(E[X ** b]), here b is known deterministic vector and X is Beta rv # what about E[max(a * (X ** b))] for a and b known vectors? avec = np.linspace(.1, .9, n) monteCarlo = np.mean(np.max(avec[:, np.newaxis] * arr, axis=0)) analytical = np.max(analyticalMeans * avec) print(f'{monteCarloA1=:.5f}, {analyticalA1=:.5f}; {monteCarlo=:.5f}, {analytical=:.5f}') monteCarloTest() monteCarloTest() monteCarloTest() monteCarloTest() monteCarloTest() monteCarloTest() Running the above produces the following output for me: monteCarloA1=0.71648, analyticalA1=0.71640; monteCarlo=0.21708, analytical=0.20414 monteCarloA1=0.71641, analyticalA1=0.71640; monteCarlo=0.21696, analytical=0.20414 monteCarloA1=0.71653, analyticalA1=0.71640; monteCarlo=0.21703, analytical=0.20414 monteCarloA1=0.71646, analyticalA1=0.71640; monteCarlo=0.21705, analytical=0.20414 monteCarloA1=0.71639, analyticalA1=0.71640; monteCarlo=0.21689, analytical=0.20414 monteCarloA1=0.71637, analyticalA1=0.71640; monteCarlo=0.21690, analytical=0.20414 The first two numbers on each line are the Monte Carlo vs "analytical" expectation for all $a_i=1$ ; these are in tight agreement which is what leads me to my belief that the max can be moved into the expectation for this simpler case. The second two numbers have $a_i$ varying, and give the Monte Carlo and my straightforward-but-incorrect analytical expectation. I note the latter is persistently below the former, indicating the mismatch.
