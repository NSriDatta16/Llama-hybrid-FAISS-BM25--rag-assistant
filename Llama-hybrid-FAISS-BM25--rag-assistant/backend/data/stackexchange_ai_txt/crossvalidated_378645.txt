[site]: crossvalidated
[post_id]: 378645
[parent_id]: 378503
[tags]: 
There is a lot of options, it depends what exactly do you want. Feature importance or permutation importance Both methods tells you which features are most important for the model. It is a number for each feature. It is calculated after the model is fitted. It doesn't tell you anything about which values of a feature implies what scores. In sklearn most modelz has model.feature_importances_ . Sum of all feature importances is 1. Permutation importance is calculated for a fitted model. It tells you how much the metric worsens if you shuffle the feature column. Pseudo-code: model.fit() base_score = model.score(x_dev, y_dev) for i in range(nr_features): x_dev_copy = copy(x_dev) x_dev_copy[:, i] = shuffle(x_dev_copy[:, i]) perm_score = model.score(x_dev_copy, y_dev) perm_imp[i] = (perm_score - base_score) / base_score You can read more about permutation importance here . Partial Dependence Plots tells you what values of a feature increases/decreases the values of prediction. It looks like this: More info on Kaggle: Partial Dependence Plots or go straight to the library PDPbox GitHub . SHAP value explains why the model gives particular prediction for given instance. It plots the following graph which tells you which feature values moved the prediction from an average value to current value for the current instance. Check SHAP library for more details.
