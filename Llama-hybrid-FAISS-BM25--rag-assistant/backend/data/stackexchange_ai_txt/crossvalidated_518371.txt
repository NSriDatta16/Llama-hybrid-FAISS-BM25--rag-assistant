[site]: crossvalidated
[post_id]: 518371
[parent_id]: 515588
[tags]: 
I would say that you do not necessarily have an over-fitting problem - it rather depends on what you take over-fitting to mean. Some would define over-fitting as meaning the performance on the training data is substantially better than the performance on the validation/test data. However this is not a particularly useful definition as many machine learning methods (e.g. SVMs or deep neural networks) give their best generalisation performance while giving (near) zero error on the training set (see the recent work on "benign overfitting" - see also this paper ). This seems especially try for high-dimensional problems, such as image recognition. Most learning algorithms start of minimising the training loss by learning the large scale structure of the data, which also tends to minimise the loss on unseen data (hence improving generalisation), and only later by making changes that are memorising the random variation in the training sample. This usually has the effect of making generalisation performance worse and worse. As a result, we tend to see something like the diagram below (from Wikipedia), where the training error decreases monotonically, but the error on unseen data initially goes down as the underlying structure is learned, but then starts to rise again as the model is learning the random variation in the training set. This is the idea behind "early stopping". I would define over-fitting as meaning that the model had started to learn the random variability in the data, at the expense of an decrease in generalisation performance. By this definition, I would suggest that your model probably is not overfitting as the generalisation performance is not getting worse. ETA: the validation loss is increasing slightly, so it appears that may be some mild over-fitting going on.
