[site]: datascience
[post_id]: 39897
[parent_id]: 39404
[tags]: 
I think I have enough experience on doing feature engineering on the data for the training of the models in LightGBM. When feature engineering, it is wise to logically reason about the features that you create and how they should effect your model, just like you do; you logically expect that the ratio will work. However, the results may not be as you expected, as you experience now. 1) Always visualize your data after processing; divisions by zero can break your model, or create harmful outliers by incredibly small divisors instead of zero. Therefore, ratios can be dangerous as features Alternative ideas can variate around the difference between two, square difference, their sum, their square sum or their multiplication. I'd also recommend you to visualize statistical properties of the features by plotting the distribution or using a tool like pandas.Describe() . Then you will be able to see whether there are outliers. ( for example, 90% of the features are distributed in the range 10 to 170, where some are higher than 10,000 or so) 2) A new feature may harm your model by its own, but new patterns can occur with several new features combined together that your ML model can explicitly see. First create the features in your mind, then try their combinations. 3) Always print your training and test multi_error during the iterations, verbose_eval = 10 is a viable parameter that you can give to the train() function of LightGBM; you will see the training and test errors at every 10 iterations upon training. By doing that, you will be able to observe the changes on the performance by the features without even finishing up the iterations, and whether your model overfits. Hope I could help.
