[site]: crossvalidated
[post_id]: 643875
[parent_id]: 324992
[tags]: 
one way to envision the implementation (not sure this is how Keras/Pytorch or other frameworks implement it), such that the weight could be "learned" by back propagation, is to map the Embedding Matrix (n_vocab x n_emb_dim) as the weight between the input layer where word/token are one-hot encoded, and the embedded-layer where each token is represented as a n_emb_dim vector. Given that the One-hot layer dimension is n_vocab, and the embedded layer dimension is n_emb_dim, the number of weights for the full connections between these two layers is n_vocab x n_emb_dim, which is the same as the "lookup table of weights". This is only for one token, what if our input is a sequence of token, like a sentence? We could easily duplicate and concatenate the embedding layer vertically, with weight-sharing, so the weights count remain the same regardless of the sequence length.
