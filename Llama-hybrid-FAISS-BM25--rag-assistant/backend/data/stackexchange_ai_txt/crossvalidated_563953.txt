[site]: crossvalidated
[post_id]: 563953
[parent_id]: 
[tags]: 
Is it better to train constantly unique data or make epoch of a dataset?

Imagine a dataset containing billions of data. Some of them will never be used because of its length. That leads to my question: is it better for a Deep Learning model to train constantly new data of this huge dataset, or in opposite, take a reduced amount of data and create x epochs of them?
