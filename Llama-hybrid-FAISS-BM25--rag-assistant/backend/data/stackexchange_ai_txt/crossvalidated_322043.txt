[site]: crossvalidated
[post_id]: 322043
[parent_id]: 321851
[tags]: 
I think the following theorem that you allude to is considered to be pretty fundamental in statistical learning. Theorem (Vapnik and Chervonenkis, 1971) Let $H$ be a hypothesis class of functions from a domain $X$ to $\{0, 1\}$ and let the loss function be the $0 − 1$ loss. Then, the following are equivalent: $H$ has the uniform convergence property. $H$ is PAC learnable. $H$ has a finite VC-dimension. Proved in a quantitive version here: V.N. Vapnik and A.Y. Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2): 264–280, 1971. The version formulated above along with an nice exposition of other results from learning theory is available here : Shalev-Shwartz, Shai, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
