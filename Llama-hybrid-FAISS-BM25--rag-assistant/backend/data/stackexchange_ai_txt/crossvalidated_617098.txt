[site]: crossvalidated
[post_id]: 617098
[parent_id]: 
[tags]: 
Word2Vec with some Levenshtein metric

I'm building a search engine of technical documents based on Word2Vec, using cosine similarity metric. This search engine is very specific because it is meant to work with technical writings written in both French and English, to be used by end-users who don't necessarily know the exact technical words to look for. Using context through word embedding makes it possible to infer synonyms, which is great for this purpose. The issue is I have only a few thousands (rather short) documents to train the word embedding, so the signal/noise ratio is not great and there is a fair deal of manual cleanup to do to help the model converge to something general enough. To give the Word2Vec learning some reference points between languages, I resorted to conservatively stem words with regexs. That's because French and English share a fair deal of words ("action", "profession", etc.) and word's roots (activity/activit√©, apply/appliquer), so if you prune enough suffixes from both languages, you can converge to a fairly similar synthetic language in a way that gives Word2Vec a bit more generality to work with, where translations should be seen as synonyms, thus neighbours in the model. For words out of the dictionnary, I use Peter Norvig's spellchecking algo (using Levensthein distance) against the Word2Vec database, and use the vector of the closest word found in dictionnary. In this context, this can be seen as a typo corrector as well as a fuzzy "nearest translation". I have not been able to find litterature regarding a possible way to directly merge the words morphologic likeliness (represented by Levenshtein distance) within the vector cosine similarity from the Word2Vec fitting. Is there a such thing ?
