[site]: datascience
[post_id]: 120131
[parent_id]: 120073
[tags]: 
I see multiple possibilities, here: In General Some general remarks first: When designing you model, you should take reoccurring patterns into account: There will probably be a 24h pattern (for example: being at work has every 24h a similar probability, while 12h after being at work, the probability to be at work will be quite low). There might also be a 7 day pattern (e.g. every Wednesday evening one might go for sports). This can be done by extracting different features from the timestamp (the hour, the weekday) or choosing a suitable kernel / distance function. There are variants: you can either predict all locations you might visit in the next 2 weeks (independent of when) or you might predict for each time (e.g. each day / each hour or even every 5 minutes) where you might be. With many approaches, both variants might be possible. Finite set of locations This is basically your option 1. Just some more ideas: You might consider to to apply some clustering on your recorded data to find reoccurring locations and use these as target locations. You do not need to transform the temporal dimension into features. There are techniques that can handle time series as input (e.g. recurrent neural networks like LSTMs or transformer networks) Rasterization You could put a raster over you area of interest (e.g. your city). It depends on you to choose an appropriate cell size. Now you can predict for each cell the probability that you will visit it. This will create kind of a heat-map. Choosing the raster-approach allows you to handle your data as a series of images, which allows for techniques such as CNNs. Gaussian Processes / Kriging Gaussian Processes (a.k.a Kriging in the field of geostatistics) allow to learn a probability distribution over the spatiotemporal space (which seem to be waht you are looking for). Unfortunately, they come with some disadvantages: Gaussian Processes are better with interpolation than with extrapolation. You might get some strong uncertainties. Gaussian Processes are computationally expensive. You probably have too much data and might have to subsample or compress it. Originally, they are used for unbound regression with Gaussian distributions. You are looking more for classifications (will you be there). This can also be done, but requires some extra steps. Note : These are just some approaches and directions to look into.
