[site]: crossvalidated
[post_id]: 600440
[parent_id]: 600427
[tags]: 
Depending on how you define $R^2$ , it can be equivalent to minimizing the sum of squared residuals. While this minimization is famous from OLS linear regression, nothing about such a minimization is special for linear models, and it is fine to minimize the sum of squared residuals in other regression models. It might even be fair to say it is common to minimize the sum of squared residuals for regression problems. A typical way to define $R^2$ (that is used in your software) is as follows. $$ R^2=1-\dfrac{ \sum_{i=1}^n\left( y_i-\hat y_i \right)^2 }{ \sum_{i=1}^n\left( y_i-\bar y \right)^2 } $$ Note that maximizing this value is equivalent to minimizing the numerator, which is exactly the sum of squares residuals. (The denominator is a constant that depends on the data, not the model. You get the same denominator whether you fit a linear model, a gradient boosting model, or a deep learning neural network.) An advantage of $R^2$ is that it gives a context to the sum of squared errors by comparing to the performance of a baseline “must beat” model (despite my qualms with the exact software function, which I discuss in this link). A drawback of $R^2$ is that it lacks the natural units that are given by the squared errors, which can be interpreted. In your example, the developers evidently value the former in their model evaluation.
