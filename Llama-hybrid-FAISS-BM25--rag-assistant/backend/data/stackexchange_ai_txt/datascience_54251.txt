[site]: datascience
[post_id]: 54251
[parent_id]: 54237
[tags]: 
It depends on how you formulate the problem. Let's say you have a time-series of measurements X and are trying to predict some derived series of values (mood) Y into the future: X = [x0, x1, x2,.....] Y = [y0, y1, y2,.....] Now, if your model has no memory, that is, it is merely mapping xN -> yN , then the model does not care what order it sees x and y in. Feed-forward neural networks, linear regressors etc. are memory-less models. But, if your model has memory, that is, it is mapping a sequence of features to the next days moods xN-2, xN-1, XN... -> yN-2, yN-1, yN... , then the ordering matters. The model in this is keeping track of what it has seen before. The model's internal parameters are changing and persisting with each new example it sees. The current prediction depends on the last prediction. Recurrent neural networks have memory, so order matters. You can get around the memory requirement by restructuring your dataset. You can concatenate consecutive features and map them to next day's mood instantaneously (xN-2, xN-1, xN) -> yN . In this way, your input feature will incorporate information about the past, but the model won't care about the order since all the temporal information is encoded in the new feature, and not the model. Your new dataset will look like: Xnew = [(x0, x1, x2), (x1, x2, x3), (x2, x3, x4),...] Y = [ y2, y3, y4,...]
