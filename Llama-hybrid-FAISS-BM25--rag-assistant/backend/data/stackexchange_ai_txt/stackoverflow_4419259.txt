[site]: stackoverflow
[post_id]: 4419259
[parent_id]: 4416744
[tags]: 
@psmears is right, for the case where all 3 values are equally likely. However, if they were not equally likely, or were not independent, if you had a long enough string of them, you could just use your 2-bit or any other coding and run gzip on it. That should compress it down to about the theoretical limit. Like in the limit where all the values were 0, it should come out being not much more than the log of the length of the string. BTW: We're talking about entropy here. A simple definition in this case is -P(0)logP(0) - P(1)logP(1) - P(null)logP(null). So, for example, if P(0) = P(1) = 1/2, and P(null) = 0, then the entropy is 1 bit. If P(0) = 1/2, P(1) = 1/4, P(null) = 1/4, then the entropy is 1/2 * 1 + 1/4 * 2 + 1/4 * 2 also = 1 bit. If the probabilities are 1022/1024, 1/1024, 1/1024, then the entropy is (almost 1)*(almost 0) + 10/1024 + 10/1024 which is about equal to 20/1024 or about 2 hundredths of a bit ! The more certain something is, the less it tells you when it occurs, so the less storage it needs.
