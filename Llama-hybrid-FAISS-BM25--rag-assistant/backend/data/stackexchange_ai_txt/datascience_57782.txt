[site]: datascience
[post_id]: 57782
[parent_id]: 
[tags]: 
Weird distribution of neural network outputs

I've faced an unusual behavior during training a neural network. The problem is to predict if a sample of 1st class or 2nd class. (2-class classification). Classes are imbalanced (~ 5 / 95). I use weighted crossentropy. What do I see is probabilities of two classes for each sample, and it's seems weird to me that no matter how long I wait (number of epochs), the probabilities are in range [0, 0.9) and there are no samples close to probability 1.0. I guess, the imbalance and weighted loss + ADAM with low starting learning rate could led to this, but I am not sure. Has anyone faced the same?
