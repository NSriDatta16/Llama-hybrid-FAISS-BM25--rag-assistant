[site]: datascience
[post_id]: 118848
[parent_id]: 118823
[tags]: 
I built a PC specifically for ML last year and from my research at that time here are some of my conclusions. My datasets are quite a bit smaller than yours so I didn't spend too much time thinking about RAM/storage, but as for CPU/GPU: Intel's MKL that is relevant for certain Python packages like NumPy seems to achieve appreciable performance improvements compared to using say OpenBLAS on an AMD CPU. Support for AMD GPUs on certain ML packages like PyTorch is relatively recent, and as a result is probably less stable and less optimized than using an Nvidia GPU for the same task. Intel's ARC is even newer - I would say save yourself time and headache and grab an Nvidia GPU. In the end I ended up getting an i5-12600K and RTX 3080 12G for these reasons. I don't think getting say, a 4090Ti will be worth the extra cost over a 4090 since your main concern is just getting as much VRAM as possible (bigger batches = better!). If you can really splurge the RTX A6000 has 48GB of VRAM but it might just be cheaper to get 2x 4090s and try to run them in parallel.
