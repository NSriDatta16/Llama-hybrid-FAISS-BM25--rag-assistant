[site]: crossvalidated
[post_id]: 512059
[parent_id]: 511191
[tags]: 
Hi there I first wanted to add a comment before answering on this question, but it seems I have not enough reputation, to do this. Thus, I'll give my answer with a few assumptions and because of that, it will be longer as a comment. Maybe you will confound them instantly ;-) When looking at the insights of your data, it seems that your two groups are already clustered, due to performance? Is it correct, that one group are the high performers and one group are the low performers? Or is this just coincidence? Or does this still has to be tested? When looking at your data, it reminds me of a finished kmeans cluster analysis. When doing cluster analysis we scale the original data, before putting them into the analysis. YOu only have two dimensions and as you say more than two groups. So in general taking a mean of a scaled values seems not harmful. Now to the question of the residuals Looking at residuals/error of a model is usually a somehow a model agnostic(that means real knowdledge revealing) approach. In ML for instance we may use permutation importance to measure the importance of a variable/feature after permutating the feature(random shuffling its values) and looking if the error of the model is higher than before permutating, which indicates the feature had some meaning. The thing is that these approaches are independent of the indiviudal method chosen, e.g. coefficients of a linear regression or a logisitic regression (log odds) generate different importances and means. Thus, I think that look at the residuals is somehow always better, as it shows, how close something was to what we expected, this is a better measure as how it performs, as the latter has no inherent benchmark. Thus the approach with the residuals may be ok imho, but only if you scaled your exogenous variables before attempting your model for your groups. I hope you found my answer somewhat useful. If not, then we can maybe elaborate more together. -----------UPDATE--------------- Is every row a point of time? Do you know the points of time? Is any individual in your dataset represented more than once, e.g. one time in t0 and one time in t1 or something like that? Because this reminds me of the fact that we arent allowed to do t-test or Anova on different groups if observations were gathered over time, and are ""not indepedent"" from each other: see also: Using 2 sample T test in time series data If that is the case in your part please tell me, maybe we should instead of scaling choose a useful rolling window size for both groups over time. Then you would compare different means over time between both groups. That is allowed and useful. https://www.investopedia.com/ask/answers/difference-between-simple-exponential-moving-average/ You can compare simple rolling averages or rolling averages which focus more on actual data, thus on specific time points with weights. So in case you really have not independent data and observations over time rolling comparions might do the trick, instead of scaling.
