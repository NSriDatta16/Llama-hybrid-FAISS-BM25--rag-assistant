[site]: crossvalidated
[post_id]: 422513
[parent_id]: 422430
[tags]: 
There is no one 'right' way to turn wordvectors back into words. The issue is that the words themselves form a discrete set of points in the embedding space, and so the output of a model is very unlikely to be exactly equal to the location of any word. Typically if your model emits a vector $v$ then interpreting it as a word is done by finding a word $w$ with embedding $v_w$ such that $d(v, v_w)$ is small, i.e. $v$ is 'close' to the embedding of $w$ . Choosing the distance function $d$ is up to you, although typically the cosine similarity is used. Depending on the application, you could also consider showing the top- $k$ similar words to your wordvector, which could offer a bit more diversity.
