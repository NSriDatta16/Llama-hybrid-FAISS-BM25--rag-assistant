[site]: crossvalidated
[post_id]: 565974
[parent_id]: 565943
[tags]: 
Nested cross validation can indeed include feature selection. It is similar to preprocessing as you have mentioned. I feel that the errors from this can be still used as a valid generalization error estimate, stating that a model built using this particular procedure of feature selection and parameter estimation is expected to have such as such error on new data. This is the idea as nested cv won't produce a single model. So, it makes sense. And it can be extended to other concepts. Though flat cross validation can be preferred in case we also want to get a final model. I understand the concerns of the authors here: ... for a fixed number of variables end up averaging over models with different model complexities But, this actually aims to answer the question "What can we do with this set of features?"
