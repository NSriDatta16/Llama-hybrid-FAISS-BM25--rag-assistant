[site]: crossvalidated
[post_id]: 570016
[parent_id]: 569901
[tags]: 
I have found my own theory on what is wrong with this approach. Please correct me if I am wrong! Like suggested by Kutz in https://youtu.be/NoQV1lc7OlU?t=416 or his book on pdf-page 192 http://databookuw.com/databook.pdf , cross-validation does not serve any model building purpose. It only gets used to get a better idea of the generalization error. Dividing the data-set up into k subsets, performing regression on them and averaging their coefficients is not a great idea. This is because for small datasets, the weights of the data-points get disrupted ( Running regression on the entire dataset vs running on smaller dataset then take average of coefficient? ). This way a lot of bias gets introduced. If the sets consist out of more data, this effect will be lower. But by having fewer sets (assuming an unchanged set of data), the variance-reduction effect will also be less. I believe that this is simply trading some unknown amount of bias for lower variance. It probably is better to just go for a biased regression straight away.
