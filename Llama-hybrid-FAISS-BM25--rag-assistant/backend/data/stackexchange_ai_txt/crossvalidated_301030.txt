[site]: crossvalidated
[post_id]: 301030
[parent_id]: 300933
[tags]: 
In their PPCA paper, Tipping and Bishop give a closed form expression for the maximum likelihood solution to PPCA. I would guess that the material you're reading is referring to this (in contrast to the iterative, expectation maximization algorithm that Tipping and Bishop also describe in their paper). PPCA is exactly what it sounds like--a probabilistic version of ordinary PCA. It's a Gaussian latent variable model, and is described quite well in the paper you linked in your question. scikit-learn is a curious case. It performs regular PCA, but also has some functionality related to PPCA. For example: It can compute the maximum likelihood PPCA covariance matrix and noise variance, and PPCA log likelihoods. It can also use a Bayesian approach to select the number of components, assuming a PPCA model. However, the transform() method simply projects the centered data onto the eigenvectors of the sample covariance matrix. This corresponds to regular PCA. It could also be interepreted as PPCA in the limit of zero noise variance (where PCA and PPCA are equivalent, but the probability distribution becomes undefined). Otherwise, scikit-learn doesn't seem to give a way to compute the PPCA latent representations (assuming nonzero noise variance). But, you could compute this manually from the information scikit-learn does return, using equation 9 from the Tipping & Bishop paper you linked.
