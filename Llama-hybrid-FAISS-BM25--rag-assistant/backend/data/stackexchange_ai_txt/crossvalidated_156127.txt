[site]: crossvalidated
[post_id]: 156127
[parent_id]: 
[tags]: 
Estimation Error when neglecting $\mu$ while computing Covariance-Matrix

Error when neglecting $\mu$ while computing Covariance-Matrix I would like to quantify the estimation error I have to accept when estimating the Covariance matrix based on $T$ observations of $x_i\in\mathbb{R}^N$ (multivariate normal distributed with mean vector $\mu\in\mathbb{R}^N$). Define $$X:=\left( \begin{array}{c} x_1' \\ \vdots \\ x_T'\end{array} \right)\in\mathbb{R}^{T \times N}$$ The standard approach is to compute $$\hat{\Sigma}=\frac{1}{T-1}X'X-\frac{T}{T-1}\hat{\mu}\hat{\mu}'.$$ where $\hat{\mu}\in\mathbb{R}^N$ is the empirical mean. I make the following observation: I sampled 5000 times multivariate normal datapoints with $N=10$ and $T=500$ observations with mean around $0.005$ and high positive correlation. When computing the maximum relative difference between $\hat{\Sigma}$ and $\frac{1}{T-1}X'X$ I obtained on average a low error (around 0.05). Changing the sampling variance to the identity matrix ceteris paribus leads me towards a very different result with errors around 15. Now I am wondering how the Correlation structure affects this estimation error. Every Idea is appreciated!
