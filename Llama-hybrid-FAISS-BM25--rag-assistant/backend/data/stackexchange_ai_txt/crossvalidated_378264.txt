[site]: crossvalidated
[post_id]: 378264
[parent_id]: 132774
[tags]: 
Other than eyeballing structures from correlation matrix, there are 3 better ways to do it, especially when your dimensionality is higher than what you could hand-pick features from. variance inflation factor (VIF). https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html instead of showing the correlation of a feature to every other feature, VIF gives you the collinearity between one feature and a linear model made of all the other features. you can easily set up a threshold for the VIF value and exclude the features that doesn't give more information to the linear combination of other features. regularization / shrinkage https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py The bad thing about collinearity is that it makes the within-class covariance matrix close to singular matrix, resulting in impossibility or inaccuracy of calculating inverse matrix. This problem can be circumvented by having a shrinkage, i.e. averaging the covariance matrix with a diagonal matrix. It also helps the problem of having small sample size and prevents overfitting. PCA before LDA https://www.sciencedirect.com/science/article/pii/S0031320302000481 it has been a common practice. After PCA the collinear features will be combined into components without losing information, if you pick enough components. Hope this is helpful.
