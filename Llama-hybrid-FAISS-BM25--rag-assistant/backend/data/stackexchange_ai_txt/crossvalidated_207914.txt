[site]: crossvalidated
[post_id]: 207914
[parent_id]: 
[tags]: 
Difficulty learning parameters in RNN?

I'm implementing an LSTM using the RNN package in Torch. I've been able to get very simple models to converge (like learning the relation f(x) = x), but haven't been able to get basic things like predicting whether the last word was one of the 300 most common English words. Initially, the training would run into an exploding gradient problem. I've tried several approaches to fixing this Different optimizers (adam, adagrad) Clipping gradients (threshold on L2 norm) Clipping gradients seems to help a little, but after some point in training, the training loss starts rising instead of falling and continues rising. Are there any other common tricks used to get LSTM models to train well?
