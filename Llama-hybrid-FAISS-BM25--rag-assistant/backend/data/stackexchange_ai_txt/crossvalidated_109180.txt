[site]: crossvalidated
[post_id]: 109180
[parent_id]: 
[tags]: 
Estimate confidence in multiple choice test

There is multiple-choice test and for every possible answer option my algorithm gives some score of how much it is likely to be the right one and picks the one with the maximal value as the answer. If there are 4 options A B C and D and the corresponding scores are for example 3.34, 4.01, 2.78 and 3.01 then the answer should be B. On the other hand, if score are, say, 3.34, 100.01, 2.78 and 3.01 then the answer is still B but the algorithm looks more “confident” I was wondering how to numerically capture this confidence so that I can for example debug the algorithm and see if it is improving. Like for example the maximal one minus the second biggest value or something like that for one question - and the average of this value for the whole set of questions. So I wanted to ask if there is some “standard” commonly-used approach for this.
