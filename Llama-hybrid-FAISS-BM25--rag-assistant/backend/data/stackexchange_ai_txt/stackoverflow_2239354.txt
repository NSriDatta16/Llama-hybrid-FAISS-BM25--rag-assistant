[site]: stackoverflow
[post_id]: 2239354
[parent_id]: 2239266
[tags]: 
Your unit tests need to employ some kind of fuzz factor , either by accepting approximations, or using some kind of probabilistic checks. For example, if you have some function that returns a floating point result, it is almost impossible to write a test that works correctly across all platforms. Your checks would need to perform the approximation. TEST_ALMOST_EQ(result, 4.0); Above TEST_ALMOST_EQ might verify that result is between 3.9 and 4.1 (for example). Alternatively, if your machine learning algorithms are probabilistic, your tests will need to accommodate for it by taking the average of multiple runs and expecting it to be within some range. x = 0; for (100 times) { x += result_probabilistic_test(); } avg = x/100; TEST_RANGE(avg, 10.0, 15.0); Ofcourse, the tests are non-deterministic, so you will need to tune them such that you can get non-flaky tests with a high probability. (E.g., increase the number of trials, or increase the range of error). You can also use mocks for this (e.g, a mock random number generator for your probabilistic algorithms), and they usually help for deterministically testing specific code paths, but they are a lot of effort to maintain. Ideally, you would use a combination of fuzzy testing and mocks. HTH.
