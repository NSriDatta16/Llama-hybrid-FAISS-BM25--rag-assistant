[site]: crossvalidated
[post_id]: 606138
[parent_id]: 
[tags]: 
How to determine the shape of a fully connected layer in Squeeze-and-Excitation Networks?

Given the following network with input $x=H\times W \times C$ where $H$ is the width, $W$ is the width and $C$ is the number of channels (as $x$ is originally output of a convolutional layer). For $C$ channels, the dimension of average pooling would be $1 \times 1 \times C$ . Thne after passing it to the fully contented layer, the dimension is reduced by a ratio $r$ so that it becomes $1 \times 1 \times \frac{C}{r}$ . Question : why the dimension of fully connected layer 1 becomes $1 \times 1 \times \frac{C}{r}$ please? From my understanding, the fully connected layer does not do any reduction of incoming input. This comes from J. Hu, L. Shen and G. Sun, "Squeeze-and-Excitation Networks," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 7132-7141, doi: 10.1109/CVPR.2018.00745. A preprint is on arXiv: https://arxiv.org/pdf/1709.01507.pdf
