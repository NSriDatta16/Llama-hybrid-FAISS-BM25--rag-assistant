[site]: crossvalidated
[post_id]: 328233
[parent_id]: 328222
[tags]: 
If your data is too tall, then a standard technique is batching, where you update the loss function for say, 1000 points at a time. This is how stochastic gradient descent works. If your data is also too wide, then I would think a similar kind of batching procedure would work, where you also select a subset of features to update at any given time. This would be analogous to how dropout works in neural networks.
