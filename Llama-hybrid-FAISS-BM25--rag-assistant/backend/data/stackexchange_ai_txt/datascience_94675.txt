[site]: datascience
[post_id]: 94675
[parent_id]: 94517
[tags]: 
In theory maybe yes, but you would probably need to reimplement the model yourself. In practice, with the current implementations, probably no. (Judging from the documentation snippet, you use Huggingface Transformers.) The documentation says it expects a LongTensor , i.e., a tensor with integer values. Internally, the attention mask is used to compute sequence lengths, but summing the mask along dimension 1. This would need to be fixed and there might many other places in the code just assume the mask values are zeros and ones.
