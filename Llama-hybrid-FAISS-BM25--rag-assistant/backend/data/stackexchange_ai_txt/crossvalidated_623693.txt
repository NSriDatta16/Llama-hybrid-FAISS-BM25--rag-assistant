[site]: crossvalidated
[post_id]: 623693
[parent_id]: 
[tags]: 
Cost complexity pruning in random forests

When choosing the optimal alpha for cost complexity pruning in a single decision tree, we can directly look at the subset of effective alphas. However, in the context of random forests, there isn't an easy way to define what the effective alphas should be. Although in random forests such a pruning is not as necessary as in decision trees (and indeed the algorithm works well with fully grown trees), I'd like to test how it improves the score. What technique would you suggest to reduce the range of alpha candidates? There are of course stochastic ways that avoid the use of a brute-force grid search. But apart from those, is there a way "similar" to the one we use in single decision trees?
