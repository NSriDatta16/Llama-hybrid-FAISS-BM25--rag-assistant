[site]: crossvalidated
[post_id]: 525239
[parent_id]: 
[tags]: 
Bayesian optmization when the relationship on some variables linear, and non-linear on other features

I am working on a Bayesian Optimization problem, where the approximator is GP, and the utility function is UCB1. My problem is the output of the function is linear on some features while non-linear on other features. Meaning: Let's say we have six features in total where the behavior of the function is non-linear on the first three features and completely linear on the remaining three features (keeping all variables the same and changing single linear feature will result in 100% correlation between changes of that variable and output of the function) as you can imagine, with 100% correlation optimum for linear features always the upper bound values provided by me. My problem is Bayesian optimization with the setup GP (with Mattern kernel) and UCB1 fails to converge to upper bound of the linear features explained above. Bounds for all features are between [-0.15, +0.15]. I know that: We can remove linear features from optimization since we know where the optimum lies Or we can set bounds accordingly for linear features so we restrict the space But I am looking for a solution other than above 2, also I want to understand reasoning behind this result I am getting
