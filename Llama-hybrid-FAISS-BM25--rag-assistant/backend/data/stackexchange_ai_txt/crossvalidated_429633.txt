[site]: crossvalidated
[post_id]: 429633
[parent_id]: 429623
[tags]: 
As Noah says but just with formulas ... Consider logistic regression $$ \Pr(Y=1) = \frac{\exp(\beta_0 + \mathbf x^\top\beta)}{1+ \exp(\beta_0 + \mathbf x^\top\beta)}$$ and then offcourse $$ \Pr(Y=0) = 1- \Pr(Y=1)=1 - \frac{\exp(\beta_0 + \mathbf x^\top\beta)}{1+ \exp(\beta_0 + \mathbf x^\top\beta)} = \frac{1}{1+\exp(\beta_0 + \mathbf x^\top\beta)}$$ Assuming that you are using demeaned raw variables $\mathbf z$ to get covariates $$\mathbf x = \mathbf z - \mathbf{ \bar z}$$ then $\mathbf x= 0$ is equivalent to $\mathbf z = \mathbf {\bar z}$ . Inserting $\mathbf x = 0$ in the formulas above the probabilities reduce to $$\Pr(Y=1) = \exp(\beta_0) /(1+\exp(\beta_0)) \phantom{xxx}\wedge \phantom{xxx}\Pr(Y=0) = 1 /(1+\exp(\beta_0))$$ hence odds at the mean $$\frac{\Pr(Y=1)}{\Pr(Y=0)}\biggr\rvert_{\mathbf z=\mathbf { \bar z}} = \exp(\beta_0)$$ and log odds at the mean $$\log \frac{\Pr(Y=1)}{\Pr(Y=0)}\biggr\rvert_{\mathbf z=\mathbf { \bar z}} =\beta_0$$ Compare this to the case where evaluation is not at the mean and assume for simplicity that $\mathbf x$ only includes one covariate such that $$\log \frac{\Pr(Y=1)}{\Pr(Y=0)}=\beta_0 + \beta_1 x_1$$ it then makes sense in the case where $x_1$ is a continuous covariate to differentiate the log odds with respect to $x_1$ to get $\beta_1$ . This is never the case with the intercept because it is not a coefficient of a continuous regressor, hence it never makes sense to speak of the intercept as marginal log odds in the sense here used.
