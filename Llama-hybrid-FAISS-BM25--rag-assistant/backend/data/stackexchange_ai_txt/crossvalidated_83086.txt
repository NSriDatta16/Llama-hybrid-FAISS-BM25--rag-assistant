[site]: crossvalidated
[post_id]: 83086
[parent_id]: 82923
[tags]: 
Replicating my answer from http://www.quora.com/Machine-Learning/What-are-good-ways-to-handle-discrete-and-continuous-inputs-together/answer/Arun-Iyer-1 Rescale bounded continuous features: All continuous input that are bounded, rescale them to $[-1, 1]$ through $x = \frac{2x - \max - \min}{\max - \min}$. Standardize all continuous features: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean ($\mu$) and standard deviation ($\sigma$) and do $x = \frac{x - \mu}{\sigma}$. Binarize categorical/discrete features: For all categorical features, represent them as multiple boolean features. For example, instead of having one feature called marriage_status, have 3 boolean features - married_status_single, married_status_married, married_status_divorced and appropriately set these features to 1 or -1. As you can see, for every categorical feature, you are adding k binary feature where k is the number of values that the categorical feature takes. Now, you can represent all the features in a single vector which we can assume to be embedded in $\mathbb{R}^n$ and start using off-the-shelf packages for classification/regression etc. Addendum: If you use Kernel Based Methods, you can avoid this explicit embedding to $\mathbb{R}^n$ and focus on designing custom kernels for your feature vectors. You can even split your kernel into multiple kernels and use MKL models to learn weights over them. However, you may want to ensure positive semi-definiteness of your kernel so that the solver doesn't have any problems. However, if you are unsure of whether you can design custom kernels, you can just follow the earlier embedding approach.
