[site]: datascience
[post_id]: 112071
[parent_id]: 
[tags]: 
Mix of time-dependent and constant features for a transformer

I'm using the transformer architecture to predict future time-points from previous time-points. Each item of the input sequence is a vector of [ temperature, time, sunlight ] Each item of the output sequence is simply a temperature . I want to also provide a contextual, non time-dependent feature input (e.g. altitude ) to improve the prediction. What's the best way to plug in that additional meta feature to the transformer? I've considered the following: Simply appending the constant feature to each item of the input sequence: [ temperature, time, sunlight, altitude ] Passing altitude through a couple of dense layers and using the output as a latent space representation to be inputted to one of the attention heads in every encoder layer. Basically replicating the decoder layers but with the altitude encoding as foreign input. Passing altitude through a couple of dense layers and using the output to balance the output of the encoder layers. Is there an agreed upon best practice or available implementation for this use case?
