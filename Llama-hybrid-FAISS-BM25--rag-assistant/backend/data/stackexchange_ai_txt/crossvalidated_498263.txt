[site]: crossvalidated
[post_id]: 498263
[parent_id]: 
[tags]: 
Any natural interpretation for "hopfield network is all you need"?

I am studying deep learning and came across to the paper claiming "so-called attention mechanism in deep learning is equivalent to hopfield network", in the paper called "Hopfield network is all you need". ( https://arxiv.org/pdf/2008.02217.pdf ) However, I still doesn't fully get the physical intuition of the correspondence. In my opinion, equation 2 and 3 in the paper should come from transition probability of MCMC or something, but I quite don't understand why. In particular, could someone explain where $\beta^{-1} \log N$ term and $\frac{1}{2}M^2$ term come from relationship between transition of $\xi$ and MCMC I would really appreciate your help,
