[site]: crossvalidated
[post_id]: 269588
[parent_id]: 267600
[tags]: 
Your architecture looks fine. I mean, it's straight out of MNIST lenet. It's a good solid network to start from. You can then evolve it over time, according to your loss curves, by adding capacity, ie layers, channels per layer, etc. You could also consider adding dropout, for regularization. As far as convergence... it's pretty much impossible not to converge, unless you are using too high a learning rate. So, divide your learning rate by 10, until it starts converging. You can just pick some tiny subset of eg 32 images, and just train on those images, using smaller and smaller learning rates, until the error on those 32 images drops to zero (which it should, because you'll overfit them, easily). Then, once the loss on 32 images is dropping to zero, ie you've picked a small enough learning rate, fixed any bugs etc, then you can add more and more data, and then start increasing capacity of your network, ie adding layers etc. And you probalby want to add dropout, it's really good at encouraging generalization to test data. Edit: oh, you're using Adadelta etc, which should handle learning rate for you. Well... I've mostly used SGD, and SGD is somewhat standard for deep nets (though gradually falling out of favor a bit recently). You might consider trying SGD, with a small learning rate, and seeing what happens.
