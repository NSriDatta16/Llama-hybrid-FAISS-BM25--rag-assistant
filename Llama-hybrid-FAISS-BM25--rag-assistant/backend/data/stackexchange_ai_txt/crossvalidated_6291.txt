[site]: crossvalidated
[post_id]: 6291
[parent_id]: 6279
[tags]: 
The large value of "B" would be a coefficient of a variable usually called "X" in your model. Usually, "X" has a real world meaning (could be income, could be a measured volume of something, etc.). So the job is to interpret this "B" in terms of "X". The usual definition (in ordinary least squares) is that a ONE UNIT increase in "X" corresponds to a "B" increase in "Y" (where "Y" is the dependent variable which you are modeling). It is similar (but not exact) when interpreting for a logistic regression: a ONE UNIT increase in "X" corresponds to a B increase in the log-odds (which is "Y" in this case). Therefore EXP(B) tells you the proportional increase in the odds for a ONE UNIT increase in "X". So the question which may help is "What does a ONE UNIT increase in X mean in the real world?" This may make the apparent extreme value seem more sensible. Another thing to ask is what is the range of X values in your data? for if this is much smaller than 1 then EXP(B) is effectively extrapolating well beyond what you have observed in the "X space" (something which is generally not recommended because the relationship may be different). Put more succinctly the numerical value of your betas is related to the scale at which you measure your X variables (or independent variables). The easiest mathematical way to see this is the form of the simple ordinary least squares estimate which is B=(standard deviation of Y) / (standard deviation of X) * (correlation between Y and X) This result is not exact in logistic regression, but it does approximately happen. I would also recommend that replacing the observed proportions with (r_i+1)/(n_i+2) is not a bad way to go before plugging them into the logistic function, because it can help guard against creating extreme logit values (which are only an artifact of your choice of "small number"). Extreme logit values can create outliers and influential points in the regression, and this can make your regression coefficients highly sensitive to these observations, or to be more precise, to your particular choice of "small number". This has been called "Laplace's rule of succession" (add 1 success and 1 failure to what was observed) and it effectively "pulls" the proportion back towards 1/2, with less "pulling" the greater n_i is. In Bayesian language it corresponds to the prior information that it is possible for both of the binary outcomes to occur.
