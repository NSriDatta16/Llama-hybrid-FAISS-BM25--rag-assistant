[site]: crossvalidated
[post_id]: 301342
[parent_id]: 103942
[tags]: 
There are two general approaches that can be used. First is Bag of Words model which basically transforms documents to counts of words. Its drawback is that it does not capture semantic similarity. Latent Semantic Analysis and Latent Dirichlet Allocation can be thought of extensions of this model. Scikit-learn contains algorithms that encode data using BoW approach (CountVectorizer, TfidfVectorizer etc), and in fact it contains an example (Clustering text documents using k-means) that seems to deal with problem that is very similar to yours. The second one approach is to use word embeddings . These are dense representations of words that capture cooccurrence patterns. These are really good at capturing semantic similarity. They have a drawback of being harder to understand, and also to use them you either need pretrained word vectors or train the model, what can be pretty time-consuming. Gensim contains algorithms for word embeddings, and also for embeddings of whole documents (from what I've read just averaging word vectors for documents works OK, but I don't have any reference for that). Using either approaches yields encoding of texts that can be directly used as input to clustering algorithms or similarity queries. If you want to read on the theory: chapters 15-16 of Manning and Jurafsky's Speech and Language Processing cover this.
