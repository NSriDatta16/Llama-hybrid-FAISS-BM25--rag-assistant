[site]: crossvalidated
[post_id]: 622284
[parent_id]: 620718
[tags]: 
Here is some data: When $x we have a very small probability that $y = 1$ . When $x > 11$ we have a very large probability that $y=1$ . In between things are murkier. We could get an approximation of the probability by looking at the empirical probability within a "moving window" of size 1: In the interval $x \in [9,10]$ there are 20 samples, of which $5$ have $y=1$ . So the height of the green point at $x = 9.5$ is $\frac{5}{20} = 0.25$ . We decide to fit a sigmoid curve to this: This is a function of the form $$ p_{m,b}(x) = \frac{1}{1+e^{-(mx+b)}} $$ We fit logistic regression using maximum likelyhood estimation . Out of all of the different choices of the parameters $m$ and $b$ in our model, we are looking for the choices which will maximize the probability that our sample was generated by those parameters. For an individual data point $(x,y)$ with $y = 1$ , the probability that the model generated that data point is $p_{m,b}(x)$ . For an individual data point $(x,y)$ with $y = 0$ , the probability that the model generated that data point is $1 - p_{m,b}(x)$ . So the probability that the model generated your sample is just the product of all of these: $$ \prod_{i} \begin{cases} p_{m,b}(x_i) \textrm{ if $y_i = 1$}\\ 1 - p_{m,b}(x_i) \textrm{ if $y_i = 0$} \end{cases} $$ We try to find the values of $m$ and $b$ which make this as large as possible. In practice we instead take the negative logarithm of this quantity, call it the logistic loss, and try to minimize it. This is equivalent, but nicer for numerical reasons. $$ \begin{align*} -\log \left(\prod_{i} \begin{cases} p_{m,b}(x_i) \textrm{ if $y_i = 1$}\\ 1 - p_{m,b}(x_i) \textrm{ if $y_i = 0$} \end{cases}\right) & = -\sum_{i} \begin{cases} \log(p_{m,b}(x_i)) \textrm{ if $y_i = 1$}\\ \log(1 - p_{m,b}(x_i)) \textrm{ if $y_i = 0$} \end{cases} \end{align*} $$ Now we can use a notational trick: since $y_i$ is either $0$ or $1$ we can "multiply by $y_i$ to do case analysis". $$ \ell(m,b)= -\sum_i y_i\log(p_{m,b}(x_1)) +(1- y_i)\log(1 - p_{m,b}(x_1) ) $$ When $y_i = 1$ this simplifies to the first case and when $y_i = 0$ it simplifies to the second case. This is logistic loss! Hopefully it makes sense why you would want to minimize it now. Minimizing this quantity maximizes the likelihood that your data was generated by the distribution $p_{m,b}(x)$ !
