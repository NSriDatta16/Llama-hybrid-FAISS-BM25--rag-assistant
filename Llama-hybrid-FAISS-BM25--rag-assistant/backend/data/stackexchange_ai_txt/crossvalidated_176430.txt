[site]: crossvalidated
[post_id]: 176430
[parent_id]: 
[tags]: 
Understanding Bagged Logistic Regression (and a Python Implementation)

I am trying to understand an academic article I am reading regarding bagged logistic regression for marketing attribution -- http://www.turn.com.akadns.net/sites/default/files/whitepapers/TURN_Tech_WP_Data-driven_Multi-touch_Attribution_Models.pdf Particularly, this paragraph: Step 1. For a given data set, sample a proportion (ps) of all the sample observations and a proportion (pc) of all the covariates. Fit a logistic regression model on the sampled covariates and the sampled data. Record the estimated coefficients -- we recommend to choose ps and pc to take values around 0.5 if both the variability and the accuracy are of the concern Can someone please explain what this means in (hopefully) plain english? Based on my understanding, the idea is to just keep running the logistic regression on .5 random subsets of the sample data and then average all of the log odd coefficients that meet a .5 selection threshold? Completely Optional Bonus points 1 : On a side note, is this implementation similar to the idea of randomized logistic regression in scikit learn for python? If not, what is the difference? http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html Completely Optional Bonus points 2 : is there a way to incorporate ordered effects into a bagged logistic regression model (e.g. the order in which the predictor variables, in this case advertisements, appeared -- however this is of seconday concern to the primary question)
