[site]: crossvalidated
[post_id]: 198902
[parent_id]: 
[tags]: 
Simple NN with Lasagne and NoLearn

I wanted to test nolearn and Lasagne with a simple neural network and train the XOR problem, but I run into odd problems. The code below fails with the error ValueError: Cannot have number of folds n_folds=5 greater than the number of samples: 4. import numpy as np from lasagne.layers import DenseLayer from lasagne.layers import InputLayer from lasagne.nonlinearities import softmax, sigmoid from nolearn.lasagne import TrainSplit from nolearn.lasagne import NeuralNet data_set = np.array([ [0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0], ]) X = data_set[:, :2] y = data_set[:, 2:] X[0] X = np.array(X).astype(np.float32) y = np.array(y).ravel().astype(np.int32) layers = [ (InputLayer, {'shape': (None, X.shape[1],)}), (DenseLayer, {'num_units': 2}), (DenseLayer, {'num_units': 2, 'nonlinearity': softmax}), ] net1 = NeuralNet( layers=layers, max_epochs=100, update_learning_rate=0.01, train_split=TrainSplit(eval_size=0), verbose=3, ) net1.fit(X, y) and if I double the training set (extending the data_set array by the same set), the network trains but doesn't converge. Why is that the case? # Neural Network with 6 learnable parameters ## Layer information # name size --- ------ ------ 0 input0 2 1 dense1 2 epoch train loss valid loss train/val valid acc dur ------- ------------ ------------ ----------- ----------- ----- 1 nan nan nan 0.50000 0.02s 2 nan nan nan 0.50000 0.01s 3 nan nan nan 0.50000 0.00s 4 nan nan nan 0.50000 0.00s 5 nan nan nan 0.50000 0.00s 6 nan nan nan 0.50000 0.00s 7 nan nan nan 0.50000 0.00s 8 nan nan nan 0.50000 0.00s 9 nan nan nan 0.50000 0.00s 10 nan nan nan 0.50000 0.00s Out[217]: NeuralNet(X_tensor_type=None, batch_iterator_test= , batch_iterator_train= , custom_scores=None, layers=[( , {'shape': (None, 2)}), ( , {'num_units': 2, 'nonlinearity': })], loss=None, max_epochs=10, more_params={}, objective= , objective_loss_function= , on_batch_finished=[], on_epoch_finished=[ ], on_training_finished=[], on_training_started=[ ], regression=False, train_split= , update= , update_learning_rate=0.01, use_label_encoder=False, verbose=1, y_tensor_type=TensorType(int32, vector)) What am I misunderstanding about the simple lasagne setup?
