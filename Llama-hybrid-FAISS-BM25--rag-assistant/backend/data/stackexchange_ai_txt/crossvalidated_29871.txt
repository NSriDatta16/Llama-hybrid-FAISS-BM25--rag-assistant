[site]: crossvalidated
[post_id]: 29871
[parent_id]: 29863
[tags]: 
You might consider shrinkage estimation of $P$ given $X$. One straightforward way to do this is, for each row $i$, $P_{i}^* = m\frac{c}{c+x_i} + \frac{x_i}{n_i}\frac{x_i}{x_i+c}$, where $x/n = P$, the proportion of successes $x =$ the number of successes $m =$ the mean proportion across the whole data set (which, based on the data above, is 0.303) $c =$ a scaling constant (the higher this is, the more shrinkage there is) The intuition is that you are creating a weighted average $P^*$ which shrinks each proportion to the mean on the basis of how many successes it has. The left term establishes the base value, which will be more or less of a factor based on the degree to which $x_i$ differs from $c$. The right term pushes the value from $P_i^*$ away from that average, once again based on how $x_i$ differs from $c$. The downside if that you have to choose an arbitrary scaling constant $c$. But you can justify this based on how many successes values $x_i$ it would take you to "trust" the proportion estimate $p_i$. A canonical example of this in action is the imdb top 250 , for which movie ratings are scaled to be higher (or lower, for the bottom 100 ) on the basis of how many votes they have. (By the way, it's not clear to me why you trust entries with high $X$ more than high $N$, but you clearly have more domain-specific knowledge.)
