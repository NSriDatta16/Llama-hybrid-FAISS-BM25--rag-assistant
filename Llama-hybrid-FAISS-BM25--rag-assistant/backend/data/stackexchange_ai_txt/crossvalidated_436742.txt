[site]: crossvalidated
[post_id]: 436742
[parent_id]: 
[tags]: 
learning time series by machine learning

I am trying to learn a mapping between coordinates x,y,z,d , with d=sqrt(x**2 + y**2 + z**2) , and a scalar. Each x,y,z,d has an associated scalar; putting together the scalars corresponding to different x,y,z,d , one obtains a time-series. If I sort, for example, the coordinates by increasing distance d , and I standardize the coordinate d , the time-series, reordered to follow the sorted order of the distance d , appears, as a function of the standardized coordinate d , as follows: What would be the best machine learning algorithm to learn this mapping? If for example, a Gaussian Process, what would it be its kernel?
