[site]: crossvalidated
[post_id]: 97574
[parent_id]: 97426
[tags]: 
As per Bill Huber's comments and answer elsewhere , the trick is to remove the influence of the independent variables on each other whenever producing each sequential regression. In other words instead of: lm(lm(x ~ y1)$residuals ~ y2) We want: lm(lm(x ~ y1)$residuals ~ lm(y2 ~ y1)$residuals) In this case, we DO get back to the multiple regression: Moreover, we can show the coefficients are the same: > round(coef(lm(lm(it30 ~ itpc1)$residuals ~ lm(itpc2 ~ itpc1)$residuals)), 5) (Intercept) lm(itpc2 ~ itpc1)$residuals #$ 0.00000 -0.21846 > round(coef(lm(lm(it30 ~ itpc2)$residuals ~ lm(itpc1 ~ itpc2)$residuals)), 5) (Intercept) lm(itpc1 ~ itpc2)$residuals #$ 0.00000 0.29197 > round(coef(lm(it30 ~ itpc1 + itpc2)), 5) (Intercept) itpc1 itpc2 0.01186 0.29197 -0.21846 Interestingly, and as expected, if the independent variables are orthogonal as in PCA regression, then we do not need to take out the influence of each of the regressors against each other. In this case it is true that: lm(lm(x ~ y1)$residuals ~ y2)$residuals is perfectly correlated with: lm(x ~ y1 + y2)$residuals as can be seen here: This is because the orthogonal principal components have a zero-slope regression line and thus the residuals are equal to the dependent variable (with a vertical translation to mean=0).
