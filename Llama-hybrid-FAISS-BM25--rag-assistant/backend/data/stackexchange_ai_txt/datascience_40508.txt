[site]: datascience
[post_id]: 40508
[parent_id]: 40488
[tags]: 
This process is called fine tuning frequently in the neural network literature. If you already have the trained network, you simply run additional training steps on the new data. You'll need to balance the weight of the prior training with that of the new observations to some degree, and that decision is parameterized by the number of free variables in the network, the learning rate of the new training steps, and the number of new training steps. However, your particular case is very complicated by the presence of new variables. Maintaining the form of the prior network is difficult in this case. You may want to simply ensemble a newly trained network on the new data and variables with the old network to combine the predictions.The simplest ensemble (but maybe not the best) would be averaging the results of the two models.
