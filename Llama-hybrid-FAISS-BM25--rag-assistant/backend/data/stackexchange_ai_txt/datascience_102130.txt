[site]: datascience
[post_id]: 102130
[parent_id]: 17074
[tags]: 
The init score in LGB(LightGBM) is similar to the base score in XGB(XGBoost). The function of this value is used to initialize the gradient boosting algorithm. This is mainly done to speed up convergence. If you have enough time to wait for your model to converge, you do not have to use this parameter. In my personal experience, set this one to your training label average values, you will save time and get a no bad result. Here is a comparison among default settings, using the base score and using focal loss. # Focal loss [100] fit's focal_loss: 0.00191792 val's focal_loss: 0.00359477 [200] fit's focal_loss: 0.00082206 val's focal_loss: 0.00287949 [300] fit's focal_loss: 0.000403044 val's focal_loss: 0.00262565 [400] fit's focal_loss: 0.000212066 val's focal_loss: 0.00258044 [500] fit's focal_loss: 0.000117509 val's focal_loss: 0.00265192 Test's ROC AUC: 0.98168 Test's logloss: 0.00242 # Base score [100] fit's logloss: 0.00191083 val's logloss: 0.00358371 [200] fit's logloss: 0.000825181 val's logloss: 0.00286873 [300] fit's logloss: 0.000403679 val's logloss: 0.00262094 [400] fit's logloss: 0.000212998 val's logloss: 0.00257289 [500] fit's logloss: 0.0001183 val's logloss: 0.00262522 Test's ROC AUC: 0.98170 Test's logloss: 0.00242 # default [100] fit's focal_loss: 0.203631 val's focal_loss: 0.203803 [200] fit's focal_loss: 0.0710043 val's focal_loss: 0.0712822 [300] fit's focal_loss: 0.0263409 val's focal_loss: 0.0267795 [400] fit's focal_loss: 0.0103281 val's focal_loss: 0.011038 [500] fit's focal_loss: 0.00422448 val's focal_loss: 0.00539362 Test's ROC AUC: 0.95715 Test's logloss: 0.00688 Among performance on AUC and log loss, Focal loss or base score is better than the default one. The minimal example CoLab Notebook （focal_loss_on_init_score.ipynb）is inspired on this post 。
