[site]: crossvalidated
[post_id]: 307813
[parent_id]: 
[tags]: 
Ratio-of-averages vs average-of-ratios with "implicit weight"

I have some precision/recall data for an algorithm, run against multiple inputs $i$, where: $\text{precision}_i = \frac{\text{matches}_i}{\text{generated}_i}$ $\text{recall}_i = \frac{\text{matches}_i}{\text{possible}_i}$ Where $i$ is a particular test input, and for each input we have an associated "desired"/ground-truth output. $\text{possible}_i$ is the size of the "desired" output set for input $i$, $\text{generated}_i$ is the size of the actual output the algorithm gave for input $i$, and $\text{matches}_i$ is how many elements of the actual output match elements of the "desired" output. There are two ways to average these: the average of the ratios: $\text{precision}_A = \frac{\sum_i{\text{precision}_i}}{i}$ Or the ratio of the averages: $\text{precision}_B = \frac{\sum_i{\text{matches}_i}}{\sum_i{\text{generated}_i}}$ The important thing to consider is that the sizes $\text{possible}_i$ may vary a lot, so we may get precisions like $\frac{1}{2}$ and $\frac{10}{20}$. To predict how it will perform against unknown inputs, latter seems to "carry more weight" than the former, since it's based on larger data sets, which makes me think that the ratio of the averages is the most appropriate. Does this seem reasonable? What would be an appropriate error to give these values? The variance or standard deviation seems to make more sense with regards to the average of the ratios, rather than the ratio of averages.
