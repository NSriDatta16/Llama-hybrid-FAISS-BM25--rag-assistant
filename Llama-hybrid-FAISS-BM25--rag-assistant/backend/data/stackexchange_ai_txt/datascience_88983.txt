[site]: datascience
[post_id]: 88983
[parent_id]: 88981
[tags]: 
Following your example: The source sequence would be How are you The input to the encoder would be How are you . Note that there is no token here. The target sequence would be I am fine . The output of the decoder will be compared against this in the training. The input to the decoder would be I am fine . Notice that the input to the decoder is the target sequence shifted one position to the right by the token that signals the beginning of the sentence. The logic of this is that the output at each position should receive the previous tokens (and not the token at the same position, of course), which is achieved with this shift together with the self-attention mask.
