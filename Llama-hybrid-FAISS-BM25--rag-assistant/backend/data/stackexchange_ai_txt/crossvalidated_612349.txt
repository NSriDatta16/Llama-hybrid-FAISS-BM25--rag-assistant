[site]: crossvalidated
[post_id]: 612349
[parent_id]: 45784
[tags]: 
@SergeyBushmanov points out that sometimes you need to give maximum likelihood estimation a hand by providing appropriate starting point and lower/upper bounds for the optimization algorithm to find the MLE. Another way to give the likelihood a hand is to augment it with a (weakly) informative prior and do Bayesian estimation. This approach is most applicable if we have domain information to help us formulate a useful prior. I'll illustrate one Bayesian solution to the $t$ distribution fitting problem in the setting where @kjetilbhalvorsen demonstrates that the MLE approach fails: sample size $n = 10$ , location $\mu = 0$ , scale $\sigma = 1$ and degrees of freedom $\nu = 2.5$ . I'll use the brms package to do the Bayesian model fitting and two different priors for the degrees of freedom $\nu$ : strongly informative prior: $\nu \sim \operatorname{Gamma}(1, 0.3)$ and informative prior: $\nu \sim \operatorname{Gamma}(1, 0.1)$ , with the constraint $\nu > 1$ (otherwise the mean of the $t$ distribution is undefined.) Both choices indicate there is high prior probability the degrees of freedom are less than 5: under the informative prior, $\operatorname{Pr}(\nu and under the strongly informative prior, $\operatorname{Pr}(\nu . So both priors are quite "opinionated", which we would like to avoid in general. [The default brms prior on the degrees of freedom is $\operatorname{Gamma}(2, 0.1)$ .] On the other hand, in general we would also probably collect more than $n = 10$ observations for a study where we know there is a lot of variability in the outcome. And here is how to do Bayesian $t$ distribution fitting with brms::brm . First we simulate a sample of size $n$ from $t(\mu=0,\sigma=1,\nu=2.5)$ where $\mu$ is the location, $\sigma$ is the scale and $\nu$ are the degrees of freedom. n Then we fit an intercept-only $t$ -family model. fit.brm While the posterior distributions of the location $\mu$ and scale $\sigma$ are reasonably symmetric, the posterior of the degrees of freedom $\nu$ is very skewed. So I will use the posterior mode (rather than the posterior mean or the posterior median) to estimate the parameters. round( apply(posterior, 2, mode), 3 ) #> μ σ ν #> 0.161 0.785 2.164 And finally, I repeat the three analyses (MLE, Bayesian with weakly informative prior, Bayesian with informative prior) 200 times. Each analysis estimates all three parameters (location $\mu$ , scale $\sigma$ and degrees of freedom $\nu$ ) but below I plot histograms of the estimated degrees of freedom only. The true $\nu$ is 2.5. Maximum likelihood estimation fails dramatically more than half of the time (100 is the upper bound for $\nu$ in the optimization). Bayesian estimation doesn't fail in any of the 200 simulations but the estimate is biased upwards unless the prior information indicates strongly that we expect a priori only a few degrees of freedom.
