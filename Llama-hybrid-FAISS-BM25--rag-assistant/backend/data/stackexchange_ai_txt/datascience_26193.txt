[site]: datascience
[post_id]: 26193
[parent_id]: 26183
[tags]: 
The word "prediction" does not belong to any specific type of machine learning. There is nothing wrong with "predicting" new data to the cluster it belongs to; (e.g. there are many applications that place new customers into pre-discovered market segments). A conditional probability, like that used in classification, is not "stronger" than an unsupervised approach, as it rests its assumption on properly labelled classes; something that is not guaranteed. This is why there are packages that provide a predict function to clustering algorithms. Here is an example using the flexclust package with the kcaa function. That being said, the prediction step is usually handled by a supervised classifier, so the approach would be to sit a classifier on top of your learned clusters (treating cluster assignments as "labels"). You just have to reason about your weaknesses . As stated above, the weakness in classification is the assumption that labelled data is tagged correctly, whereas the weakness in clustering is that your discovered clusters are assumed to be valid. Unsupervised approached cannot be validated the same way it is done with classification. Clustering requires a variety of cluster validity techniques along with domain experience (e.g. show campaign managers your market segments to validate customer types). Ultimately, you are just matching an incoming vector (new data) to the cluster most similar. For example, in k-means this could be accomplished by finding the smallest distance between the incoming vector and all the centroids of your clusters. This kind of pattern matching depends on the data you are using. This works best for clustering techniques that have well-defined cluster objects with exemplars in the center, like k-means. Using hierarchical techniques means you would need to cut the tree to obtain flat clusters, then use the "label" assignment to run a classifier on top. This comes baked with a lot of assumptions, so you need to make sure you understand your data very well, and validate any clusters with non-technical users that have deep domain experience. POSSIBLE APPROACH If you're bent on using hierarchical clustering, then here is the general approach. Note I am not suggesting this is the best way. Every approach comes baked with a number of assumptions. You will need to work to understand your data, attempt many models, validate with stakeholders, etc. Readers can use the tutorial by Jörn Hees to get started in hierarchical clustering if needed: Create some example data: from matplotlib import pyplot as plt from scipy.cluster.hierarchy import dendrogram, linkage import numpy as np np.random.seed(42) a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,]) b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,]) X = np.concatenate((a, b),) Confirm clusters exist in synthetic data: plt.scatter(X[:,0], X[:,1]) plt.show() Generate the linkage matrix using the Ward variance minimization algorithm : (This assumes your data should be be clustered to minimize the overall intra-cluster variance in euclidean space. If not, try Manhattan, cosine or hamming. You can also try different linking options). Z = linkage(X, 'ward') Check the Cophenetic Correlation Coefficient to assess quality of clusters: from scipy.cluster.hierarchy import cophenet from scipy.spatial.distance import pdist c, coph_dists = cophenet(Z, pdist(X)) 0.98001483875742679 Calculate full dendrogram: plt.figure(figsize=(25, 10)) plt.title('Hierarchical Clustering Dendrogram') plt.xlabel('sample index') plt.ylabel('distance') dendrogram( Z, leaf_rotation=90., # rotates the x axis labels leaf_font_size=8., # font size for the x axis labels ) plt.show() Determine the number of clusters (e.g. can be done manually by looking for any large jumps in the dendrogram...see Jörn's blog for plotting function): Retrieve clusters: (using our max distance determined from reading dendrogram) from scipy.cluster.hierarchy import fcluster max_d = 50 clusters = fcluster(Z, max_d, criterion='distance') Map cluster assignments back to original frame: import pandas as pd def add_clusters_to_frame(or_data, clusters): or_frame = pd.DataFrame(data=or_data) or_frame_labelled = pd.concat([or_frame, pd.DataFrame(clusters)], axis=1) return(or_frame_labelled) df = add_clusters_to_frame(X, clusters) df.columns = ['A', 'B', 'cluster'] df.head() Build a classifier using this "labelled" data: Here, I'll just use the original data and the assigned clusters along with a knn classifier: np.random.seed(42) indices = np.random.permutation(len(X)) X_train = X[indices[:-10]] y_train = clusters[indices[:-10]] X_test = X[indices[-10:]] y_test = clusters[indices[-10:]] # Create and fit a nearest-neighbor classifier from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) res = knn.predict(X_test) print(res) print(y_test) predicted labels : [2 2 1 1 2 2 1 2 2 1] test labels : [2 2 1 1 2 2 1 2 2 1] As with any classifier, your incoming data needs to be in the same representation as your training data. As new data arrives you run it against the predict function provided by your classifier (here we use sci-kit learn's knn.predict). This effectively assign new data to the cluster it belongs. Ongoing cluster validation would be required in the model monitoring step of the machine learning workflow. New data can change the distribution and results of your approach. BUT, this isn't unique to unsupervised as all machine learning approaches will suffer from this (all models eventually go stale). As argued by Jörn in the reference above, manual inspection typically trumps automated approaches, so regular visual/manual inspection of the flat clusters is recommended.
