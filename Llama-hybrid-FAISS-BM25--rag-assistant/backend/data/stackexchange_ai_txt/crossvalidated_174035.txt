[site]: crossvalidated
[post_id]: 174035
[parent_id]: 174033
[tags]: 
The short answer is variance. The long answer is variance and also generalization. Decision trees have high variance: a slight change in the training data can cause a big change in how the splits occur, and therefore the predictions aren't very stable. Before RF, there was a lot of attention paid to pruning decision trees and so on to get better generalization. But by taking the average of many identically distributed decision trees (a random forest), that variation is averaged out, and we get to have our cake and eat it by having a low bias, low variance classifier with excellent out-of-sample generalization. This is explained in more detail in Elements of Statistical Learning .
