[site]: crossvalidated
[post_id]: 190873
[parent_id]: 
[tags]: 
Noninformative prior for variance

In Bishop's Pattern Recognition and Machine learning, he states that if you want a noninformative prior for scale,$$\int_A^Bp(\sigma)d\sigma = \int_{A/c}^{B/c}p(\sigma)d\sigma = \int_A^Bp\left(\frac{1}{c}\sigma\right)\frac{1}{c}d\sigma$$ This means that the probability that $\sigma$ is between $A$ and $B$ needs to be the same as if you scaled the standard deviation by a scaling factor $c$ (first quality). This is accomplished by scaling down the density at each of those values of $\frac{1}{c}\sigma$ by a factor of $c$ (second equality). Intuitively, this makes sense to me. Then, he equates the integrands to get $$p(\sigma) = p\left(\frac{1}{c}\sigma\right)\frac{1}{c}$$ and concludes that $$p(\sigma) \propto \frac{1}{\sigma}$$ My questions are: How can you equate the integrands just because the integrals are equal? How do you go from that equality to the statement of proportionality?
