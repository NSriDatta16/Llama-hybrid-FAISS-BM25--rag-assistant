[site]: crossvalidated
[post_id]: 60446
[parent_id]: 
[tags]: 
What's the problem with model identifiability?

I understand that in a decision perspective, identifiability of a model is needed to ensure the convergence (with increasing number of observations) of the parameters to estimate through a single value. But, if the non-identifiability of a given model is not a modeling artifacts but clearly characterises some "inaccessible knowledge" about the system under study, is it valid to perform bayesian inference on a non-identifiable model ? Here is a simple example. $$ x_i =t a y_i + \epsilon_i $$ with $(\epsilon_i)$ iid $$ \epsilon_i \sim N(0,1) $$ and an informative prior for $t$: $$ t\sim N(1,0.1) $$ and a non-informative prior for $a$ (let says, that one chooses a uniform...) $$ a \sim U(0,1000) $$ One observes $(x_i)$ and $(y_i)$ are exogenous parameters and one wants to compute : $$ p(a | (x_i); (y_i)) $$ As I understand it, the model is not identifiable as all the densities $p((x_i) | a,t;(y_i))$ described by the pairs $(a,t)$ such that $a.t=k$ ($k \in R$) are the same. Obviously in such a case the choice of $p(t)$ has a strong implication but if it is physically supported, I see no reason to invalidate the meaning of an HPD interval obtained from such a non-identifiable model. On the other hand, I do not manage to find any reference about that... so thanks for your expertise.
