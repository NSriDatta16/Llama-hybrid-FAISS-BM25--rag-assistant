[site]: datascience
[post_id]: 103277
[parent_id]: 103208
[tags]: 
The motivation is that the [CLS] embedding should contain "a summary" of both sentences to be able to decide if they follow each other or not. However, in follow-up papers such as RoBERTa or XLNet , only the masked LM objective is used and they reach better results than the original BERT. Here is the table with with results from the RoBERTa paper (Table 2 on page 5) that specifically measures the effect of the next-sentence-prediction (NSP) loss.NS
