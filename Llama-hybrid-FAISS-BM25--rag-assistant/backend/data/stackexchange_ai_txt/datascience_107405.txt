[site]: datascience
[post_id]: 107405
[parent_id]: 107402
[tags]: 
In K fold cross validation, the training data is divided into K folds. In each iteration of training fold K-1 subset are used for training the model and one of the subsets is used for validation.The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting , and also significantly reduces variance as most of the data is also being used in validation set.
