[site]: crossvalidated
[post_id]: 180129
[parent_id]: 
[tags]: 
Do Monte Carlo perturbations capture all the uncertainty in prediction?

I have a model $M$ that I use to predict a value $y = M(\vec x)$. I have known one-$\sigma$ error bars on each input $x_i \in \vec x$. I want to know the one-$\sigma$ error bar on my prediction $y$. Since in my special case I know all uncertainties $\vec \sigma$ on my inputs $\vec x$, I can perturb each input $x_i \in \vec x$ with random noise having mean 0 and standard deviation $\sigma_i$. I can do this $n$ times to obtain $\epsilon_1,\ldots,\epsilon_n$. I can then apply my model to each of these randomly perturbed inputs to obtain $$y_j = M(\vec x + \vec \epsilon_j)\qquad j=1,\ldots,n.$$ Now I can take the median and standard deviation of $\vec y$, and I have one-$\sigma$ error bars on $y$. But wait! Doesn't this only account for the uncertainty in my inputs $\vec x$, and not the uncertainty in my model $M$? If $M$ is an oracle and gives the correct answer every time, this seems fine. But in practice, this won't be the case. So how do I account for the uncertainty in each prediction made by $M$? Does it differ depending on whether I am using a linear model, a neural network, a random forest regressor, etc? I imagine what I really need to do is obtain a prediction interval for each $y$ and then take medians of the lower, middle, and upper bound of the interval. Can anyone comment on this?
