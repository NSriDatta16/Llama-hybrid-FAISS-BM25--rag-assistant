[site]: crossvalidated
[post_id]: 592726
[parent_id]: 
[tags]: 
With 2 ReLU activated layers, if the 2nd layer has all weights initialized to < 0, the network is always stillborn?

I've built on my own neural network library with keras-like syntax. I noticed that when using 2 consecutive ReLU activated layers, and the 2nd of those layers has its weights initialized to negative values, the network will always output the same value and can not learn? Since ReLU output is always positive, with negative weights it seems like the 2nd layer always outputs 0 for every neuron for every sample. Is the only solution to use leaky ReLU? If my conclusion is correct, I wonder why ReLU is used at all with random weight initialization. Even if just a single ReLU activated neuron has only negative weights (instead of the entire layer), it seems to me like it will always output 0 and is thus useless. Would like to hear your thoughts!
