[site]: datascience
[post_id]: 81169
[parent_id]: 
[tags]: 
Why is the cosine distance used to measure the similatiry between word embeddings?

While computing the similarity between the words, cosine similarity or distance is computed on word vectors. Why aren't other distance metrics such as Euclidean distance suitable for this task. Let us consider 2 vectors a and b . Where, a = [-1,2,-3] and b = [-3,6,-9] , here b = 3*a , i.e, both the vectors have same direction but different magnitude. The cosine similarity between a and b is 1, indicating they are identical. While the euclidean distance between a and b is 7.48. Does this mean the magnitude of the vectors is irrelevant for computing the similarity in the word vectors?
