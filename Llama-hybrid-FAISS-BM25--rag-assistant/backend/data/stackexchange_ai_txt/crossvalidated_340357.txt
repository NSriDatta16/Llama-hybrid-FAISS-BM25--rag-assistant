[site]: crossvalidated
[post_id]: 340357
[parent_id]: 340355
[tags]: 
You cannot see the relative importance of (input) features in your NN from just looking at its parameters. Estimating the importance of features is a branch of research in itself. It is called Sensitivity Analysis . In the case of neural network models, a lot of papers recently introduced tools to do (most of the time) local Sensitivity Analysis to understand the importance of each part of the input on the output. Among them, one could cite the widely used LIME ( Ribeiro et al., 2016 ), and the SHAP values ( Lundberg et al., 2017 ) which are an improvement over the LIME method.
