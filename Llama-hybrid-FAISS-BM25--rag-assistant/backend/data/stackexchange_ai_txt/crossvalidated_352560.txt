[site]: crossvalidated
[post_id]: 352560
[parent_id]: 352558
[tags]: 
Neural networks are typically positioned as classifiers -- so your final output is Yes/No -- or possibly a class with more than two options. But let's suppose that it's only a binary class. In that case, you are modelling a lot of Bernouilli trials. The maximum likelihood will contain terms like $y\log(p) + (1-y)\log(1-p)$, where $y$ is 0 or 1, and $p$ is the probability of a success for that individual. On the other hand, you could simply slap a bunch of squared error losses together, like $(y-\hat{y})^2$, where $\hat{y}$ would be an estimated probability of success. Back in the day, when I learned statistics, before logistic regression was invented, we regressed variables against binary outcomes by using 0 and 1 as responses and blasting a regression line at the problem. In other words, we used squared error loss. This isn't such a bad thing when the probabilities are in the mid range. Things get bad when the probabilities are close to 0 or 1. But to answer your question, you are not dealing with normal errors here and the maximum likelihood differs from squared error loss.
