[site]: crossvalidated
[post_id]: 273273
[parent_id]: 
[tags]: 
Bayesian spike and slab versus penalized methods

I'm reading Steven Scott's slides about BSTS R package (You can find them here: slides ). At some point, when talking about including many regressors in the structural time series model he introduces the spike and slab priors of regression coefficients and says that they are better compared to penalized methods. Scott says, referring to an example of a dataset with 100 predictors: Penalized methods make a single decision about which variables are included/excluded,that means that they decide one subset of predictors i.e. one model among the $2^{100}$ possible ones. "Lasso (and related) priors are not sparse, they induce sparsity at the mode but not in the posterior distribution" At this point he introduces the Spike and Slab priors. I think I got the intuition, but I want to be sure about it: Are they better in the sense that they basically use a brute force approach testing each possible subset of regressors to include? Is the drawback the computation time in doing so? What do you think he means when saying "Lasso (and related)...but not in the posterior distribution"?
