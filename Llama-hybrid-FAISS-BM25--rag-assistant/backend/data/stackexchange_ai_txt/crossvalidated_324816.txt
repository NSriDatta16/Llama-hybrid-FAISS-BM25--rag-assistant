[site]: crossvalidated
[post_id]: 324816
[parent_id]: 324813
[tags]: 
You should not confuse a zero value in a node in the input layer (can very well affect the connected nodes to it) with a connection with weight zero (does not affect the node at the end of the connection). He is talking about the former and you seem to have misunderstood it as the second thing. Since with enough layers and nodes a neural network can approximate arbitrarily complex transformations, it can eventually figure out that zeros should be treated totally differently than any other values. In that sense the statement is right, if no observed value can ever be zero. Of course you could use any other value that cannot ever occur in the data. Of course, there may be the question of whether this is the most efficient approach and if you could do some quite good imputation first, that may be a good idea if your training set is quite small. E.g. the neural net may need a lot of training data to learn how to deal with such missing data and it could be problematic if there is no or hardly any missing data in the training data, but then the neural net encounters this in practice. In that case things could go severly wrong - e.g. if the neural net has learnt a somewhat linear relationship between e.g. property size (input) and property value (what we are trying to predict) and in the training data property size was always at least 200 square meters, then I would not want to know what a neural net would predict if you code an unknown property size as 0 (perhaps close to zero value for the property, perhaps a negative value perhaps some quite low positive number, who knows...). On the other hand, if you training data is massively large and any possible missingness occurs in a good number of cases, then this may well no longer be an issue.
