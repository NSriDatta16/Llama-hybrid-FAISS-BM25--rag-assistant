[site]: crossvalidated
[post_id]: 393835
[parent_id]: 361723
[tags]: 
Short answer: Take the activation map corresponding to a particular weight matrix, take the mean of all the activations, and then average this mean over all images. Then divide the weight matrix and the bias by this average. And yes it makes sense to do it sequentially. Long answer: (Using the notation used in the paper you cited) The convolution operator for the $i^{th}$ feature map performs an inner product with image patches $x_j$ : $$ max\{0,\ w_i^{l} \bullet x_{j} + b_j^{l}\} = F_{ij}^l$$ They take the mean of activations over all images $\chi$ and all spatial locations $j$ (let's call that $s_i$ ) $$ s_i^{l} \equiv \mathbf{E}_{\chi, j}[max\{0,\ w_i^{l} \bullet x_{j} + b_j^{l}\}] = \frac{1}{KM_l} \sum_{\chi} \sum_{j=1}^{M_l} F_{ij}^l$$ Here $K$ is the number of images in the dataset. Now you just scale $w_i^{l}$ and $b_j^{l}$ by $\frac{1}{s_i^{l}}$ , giving you: $$\mathbf{E}_{\chi, j}[ max\{0,\ \frac{w_i^{l}}{s_i^{l}} \bullet x_{j} + \frac{b_j^{l}}{s_i^{l}}\}] = 1$$ This also ensures that activations that were zero earlier, after passing through the RELU nonlinearity, remain so, i.e. $$w_i^{l} \bullet x_{j} + b_j^{l}
