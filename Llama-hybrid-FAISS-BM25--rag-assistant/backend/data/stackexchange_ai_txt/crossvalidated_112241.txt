[site]: crossvalidated
[post_id]: 112241
[parent_id]: 
[tags]: 
testing logistic regression coefficients using $t$ and residual deviance degrees of freedom

Summary: Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution? Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a "degrees of freedom" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$ This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that $z$ is "approximately" normally distributed; this approximation might be poor for small sample sizes; nevertheless it cannot be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression. Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically "$t$-like", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following: Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models? If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution? More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible? R code: summary(glm(y ~ x, data=dat, family=binomial)) R output: Call: glm(formula = y ~ x, family = binomial, data = dat) Deviance Residuals: Min 1Q Median 3Q Max -1.352 -1.243 1.025 1.068 1.156 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.22800 0.06725 3.390 0.000698 *** x -0.17966 0.10841 -1.657 0.097462 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1235.6 on 899 degrees of freedom Residual deviance: 1232.9 on 898 degrees of freedom AIC: 1236.9 Number of Fisher Scoring iterations: 4 SAS code: proc glimmix data=logitDat; model y(event='1') = x / dist=binomial solution; run; SAS output (edited/abbreviated): The GLIMMIX Procedure Fit Statistics -2 Log Likelihood 1232.87 AIC (smaller is better) 1236.87 AICC (smaller is better) 1236.88 BIC (smaller is better) 1246.47 CAIC (smaller is better) 1248.47 HQIC (smaller is better) 1240.54 Pearson Chi-Square 900.08 Pearson Chi-Square / DF 1.00 Parameter Estimates Standard Effect Estimate Error DF t Value Pr > |t| Intercept 0.2280 0.06725 898 3.39 0.0007 x -0.1797 0.1084 898 -1.66 0.0978 $^1$Actually I first noticed this about mixed-effects logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with "vanilla" logistic regression. $^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$
