[site]: datascience
[post_id]: 17819
[parent_id]: 
[tags]: 
HMM probability and log probability always Infinity!

I have a list of time series data, which I want to classified them with HMM. for this Iâ€™m using Jahmm . the data are binary vectors and they ate transformed to Observation vectors for more simplicity. Now, I want to do a binary classification. As an example: obs1 = [1,0,0,0,1,0,1] obs2 = [1,0,1,0,1,0,1] obs3 = [0,0,0,0,1,1,1] obs4 = [0,1,1,0,1,0,1] first I have created K-means learner and optimize the learning process with Baum welch (as its said in jahmm), then when I want to calculate the Hmm probability which the trained observation, its always Infinity, I have also tried with log and scaling methods and the LSE. none of them change the Infinity. here is my code OpdfMultiGaussianFactory opdfMGF = new OpdfMultiGaussianFactory(hcodeDim); KMeansLearner kml = new KMeansLearner (hSNr, opdfMGF, trainData); RegularHmmBase trainHmm = kml.iterate(); System.out.println("Result TrainHmm " + trainHmm); /* Baum-Welch learning */ RegularBaumWelchScaledLearnerBase bwl = new RegularBaumWelchScaledLearnerBase(); bwl.setNbIterations(100); Hmm learntHmmTrain = bwl.learn(trainHmm, trainData); System.out.println("Resulting HMM:\n" + learntHmmTrain); System.out.println("probability at learnt hmm " +learntHmmTrain.probability(trainData.firstElement())); // This object measures the distance between two HMMs KullbackLeiblerDistanceCalculator klc = new KullbackLeiblerDistanceCalculator(); // Incrementally improve the solution for (int i = 0; i I have to mention that the klc.distance in always NaN. does anybody have experience with this problem in jahmm? Thanks for your help.
