[site]: stackoverflow
[post_id]: 5551058
[parent_id]: 5549021
[tags]: 
fwiw the microbenchmarks above seem a little unsafe, you'd expect hotspot to spot that val never changes & is never used. The other thing to bear in mind is that there are times when the averages (of 2 implementations) might be close together in absolute terms but where 1 has a pretty bad tail cost compared to the other, e.g. your 90th percentile value might be pretty similar but the last 10% is much worse. For example, changing it to use a different value each time and dumping the value to stderr produces an somewhat higher average cost (~3300ns vs ~2500ns for the case where the value is reused) on my box. This is much higher than the other posts presumably because it takes some amount of time to actually get the time so the measurement is artificially inflated. This just shows one of the difficulties in doing a good microbenchmark though. It might also be worth noting I can't measure that effect I've suggested might be present, e.g. if it were present then you might expect it to get optimised away completely. I suppose you could see what is going on via LogCompilation if you're really keen. int runs = 10000000; long totalTime = 0; for (int i = 0; i
