[site]: crossvalidated
[post_id]: 545401
[parent_id]: 545134
[tags]: 
Let me take a step back and try to frame the problem into some existing theory; would love to get a statistician's take on it, too. It sounds like the objective of the analysis is to consider $k$ parameters, each of which is measured many times by two, different, computers $c$ during the course of a flight, giving $n_p$ samples for each computer and parameter combination, where $p = 1..k$ and we have $n_p$ samples from each computer, across 100 airplanes ( $a = 1..100$ ). The measurements for a given combination of parameter, computer and plane are time series, will not be independent and identically distributed (subsequent measurements are not statistically independent of previous measurements, nor are they sampling from the same distribution because, e.g., position and speed are always changing), but we expect measurements between the two computers to be highly correlated and, in fact, we'll try to test if the error (difference) between the computers is zero. I'll assume that the two computers take measurements at the same times and that, for each parameter, the errors between them at each point in time are independent and identically distributed both across the planes (error between computers on one plane is not a function of error between computers on another plane, and, in fact, comes from the same distribution for every plane) as well as over time (error between computers is not a function of what point in time we're looking at during a flight and, in fact, comes from the same distribution over time.) This last assumption may not hold if, e.g., the error in speed measurements is a function of the plane's speed and the planes traveled at different speeds, or at different speeds at different times of their flights; in practice, this is probably a second-order consideration, but if you're making an important decision, it's probably something to consider. The approach I would suggest is to calculate the error in the measurements between the two computers for each parameter and then take the mean of this error (again, for each parameter.) This will give 100 samples of the mean error between the two computers for each parameter (one sample from each plane). One can then perform a two-sided t -test for each parameter at a chosen significance level, with the null hypothesis that the mean error between the computers is zero. So, for a given parameter $p$ with $n_p$ measurements $X_{i, p, c, a}$ ( $i$ indexing measurements, $c$ indexing computers, $a$ indexing planes), the mean error between the computers is $$\epsilon_{p, a} = \frac{1}{n_p} \sum_i ( X_{i, p, 2, a} - X_{i, p, 1, a} ).$$ To perform a one-sample t -test, calculate $$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}},$$ where $\mu_0 = 0$ because we're testing the hypothesis that the population mean of the error between the two computers is zero, $\bar{x}$ is the sample mean of the error across the 100 planes, $s$ is the sample standard deviation of the error across the 100 planes, and $n = 100$ . If this value of $t$ were outside the acceptance region for your chosen significance level and degrees of freedom, you would reject the null hypothesis and declare that the error between the two computers was statistically significant greater than zero. For example, for a significance level of $\alpha = .05$ and $\nu = 99$ (degrees of freedom $\nu = n - 1$ , where $n = 100$ airplanes), R tells me that the acceptance region is $(-1.98 . A significance level of $\alpha = .05$ means that there is a 5% chance of a Type I error, meaning, if this experiment were repeated an infinite number of times, the null hypothesis would be incorrectly rejected 5% of the time. To come up with one, final decision on whether or not the computers are in good agreement, a crude approach would be to look at what proportion of the hypothesis tests rejected the null hypothesis, and if that proportion were "meaningfully" larger than the chosen significance level, then one would conclude that the computers are in disagreement at a higher rate than mere chance alone would suggest. A better approach would be to use existing theory on combining the results of multiple hypothesis tests, but that's beyond my skill level.
