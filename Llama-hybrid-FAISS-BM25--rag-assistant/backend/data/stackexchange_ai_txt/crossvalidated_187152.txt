[site]: crossvalidated
[post_id]: 187152
[parent_id]: 187000
[tags]: 
Short answer: The effect of the regularization term on the trained model changes not only by the size of the training set, but, also by the specific set of training examples. In batch updates different batches affect the model differently, but, what matters is the average effect of all the batches. Longer answer: Training objective functions usually have two terms that are linearly combined together: 1) the regularization term (let's call it $R$), and 2) the loss term (let's call it $L$). Moreover, the loss term is typically a sum of partial losses, one for each training data, and for a training set of size $n$ we have $L = \sum_{i=1}^n L_i$. Let $w$ denote the parameters of the model. We can write a general training objective as follows: $$ \begin{align} O(w) &= \lambda R(w) + L \\ &= \lambda R(w) + \sum_{i=1}^n L_i \tag{1} \end{align} $$ As you can see, the size of the training set affects the balance between the two terms. This is regardless of the type of the loss function $R(w)$ and holds true for $\ell_2$, $\ell_1$, etc. Sometimes the loss term in $(1)$ is normalized by the number of training examples $n$ and we have $L = \frac{1}{n} \sum_{i=1}^n L_i$. Even in this case, addition or removal of examples to the training set will change how the regularization affects the training (although we normalized for the size of the dataset). I can elaborate on this more if needed. However, in batch updates (which is usually used, for example, in stochastic gradient descent) the update step is stochastic and, as such, one single update step need not (and will not) match the behavior of (or represent the characteristics of) the complete objective function. Some batches may result in regularizing the model more than the others. What is important in stochastic optimization, however, is that in expectation the updates represent the true training objective in its whole integrity. For more information about stochastic gradient descent of SVMs see this tutorial .
