[site]: crossvalidated
[post_id]: 301033
[parent_id]: 300881
[tags]: 
Word2Vec is a technique for embedding words into a latent space of vectors, which ends up having similar-used words being close to each other. Thus you'd expect $v_{yale}$ to be close to $v_{harvard}$ and $v_{stanford}$. More Specifically word2vec actually preserves analogies: "king is to queen" as "man is to woman" is quantified as: $$v_{king}-v_{queen} \approx v_{man}-v_{woman},$$ which allows you to say things like, $$v_{queen}\approx v_{king}-v_{man}+v_{woman}.$$ The individual components of each $v$ do not carry an obvious meaning, but they are not noise , due to the above relations. The issue is interpretability . You could for example cluster word2vec embeddings to see which words occur close to eachother. The biggest issue here is that the embedding is calculated from random initial conditions, and is rotation and scale invariant. So if you do want to extract meaning from directions , you could do PCA on your dataset and then look at how words cluster at opposite ends of each direction.
