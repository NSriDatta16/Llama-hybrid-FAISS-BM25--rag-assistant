[site]: stackoverflow
[post_id]: 1909003
[parent_id]: 1902061
[tags]: 
Thanks to the other answerers for their insight. It sounds like from other questions around the web that NTFS can handle the sizes , but Windows Explorer and network operations will potentially choke at much lower thresholds. I ran a simulation of a very even random distribution similar to what SHA-1 would produce for a random set of 1,000,000 "files". Windows Explorer definitely did not like a directory width of 4 as it very quickly approached the maximum (65536) for that level. I tweaked the top two directory lengths to be 3 each (4096 max), and put the remaining 34 digits in the third level to attempt to balance depth versus probability of too many directories per level. This seems to allow Windows Explorer to handle browsing the structure. Here's my simulation: const string Root = @"C:\_Sha1Buckets"; using (TextWriter writer = File.CreateText(@"C:\_Sha1Buckets.txt")) { // simulate a very even distribution like SHA-1 would produce RandomNumberGenerator rand = RandomNumberGenerator.Create(); byte[] sha1 = new byte[20]; Stopwatch watch = Stopwatch.StartNew(); for (int i=0; i After about fifty thousand, the simulation appears to slow down a bit (15-20 sec per 5000) but stays at that rate. The delete at the end took over 30 min on my machine! The distributions work out like this for 1 million hashes: 1st level has 4096 folders 2nd level has average of 250 folders 3rd level has an average of 1 folder That is very manageable within Windows Explorer and doesn't seem to get too deep or wide. Obviously if the distribution weren't this even, then we could run into problems, but only at the third level. The first two levels are bounded at 4096. I suppose if the target set were larger, we could add an additional level and gain a lot of growth potential. For my application 1 million is a very reasonable upper bound. Anyone have any thoughts on the validity of such a test for determining directory structure heuristics?
