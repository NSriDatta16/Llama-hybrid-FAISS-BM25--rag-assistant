[site]: crossvalidated
[post_id]: 443610
[parent_id]: 443550
[tags]: 
Ok, so this was fun. Luckily I have the book on hand and so if you follow some equations of Section 3.3.1 then (I think) it becomes quite easy. Let's write a class to do all the stuff we need. import numpy as np import matplotlib.pyplot as plt def make_design(x, s=0.1, p = 9): design = np.zeros(shape = (x.size, p)) design[:,0] = 1 for i in np.arange(1,p): design[:,i] = np.exp(-(x-i/10)**2/(2*s)) return design class BayesianLinearModel(): def __init__(self, alpha=.010, beta=(1/.3)**2): self.alpha = alpha self.beta = beta def fit(self,x,t): Phi = make_design(x) r,c = Phi.shape self.SN = np.linalg.pinv(self.alpha*np.eye(c) + self.beta*Phi.T@Phi) self.mN = self.beta*self.SN@Phi.T@t return self def predict(self, x, return_predictive = True): Phi = make_design(x) preds = Phi@self.mN cov_mat = 1/self.beta + Phi@self.SN@Phi.T if return_predictive: sig = np.sqrt(np.diag(cov_mat)) return preds , sig else: return preds Everything is quite easy now. true_func = lambda x: np.sin(2*np.pi*x) x = np.random.rand(25) y = true_func(x) + np.random.normal(0, 0.3, size = x.size) X = np.linspace(0,1,1001) model = BayesianLinearModel().fit(x,y) preds,lims = model.predict(X, return_predictive = True) fig, ax = plt.subplots(dpi=120) plt.fill_between(X, preds - 2*lims, preds + 2*lims, color = 'red', alpha = 0.25) plt.plot(X, preds, color ='red', label = 'prediction') plt.plot(X, true_func(X) , color = 'green', label = 'truth') plt.scatter(x,y, marker = 'o', alpha = 0.5, edgecolor = 'k', facecolor ='None') plt.legend() No guarantees this is correct, but it certainly looks correct. EDIT: Re: Your questions when fitting the model we used training data to build the design matrix and to build SN, mN. When predicting we use the new data to build another design matrix and use it to predict. How/when do I know when to use which x? You only use the training x (in my code x ) once, and you use it in the fit call. You only have to fit once. When you want to predict on new data (in my code, X ) then you pass that to the predict method. Could elaborate on the drawing from the pred. distr.? From what I understand in ur code, we only use the computed mean to draw and only use the variance to obtain the uncertainty? I thought I would have to plug those values into a normal distr. You are technically right. We can draw from the predictive distribution which induces a distribution over curves. I've not implemented that here because I'm lazy. The reason I've made the predictive distribution as I have is because our choice of prior and likelihood make it so our posterior is Gaussian. I can thus summarize the predictive distribution as mean +/- two standard deviations (which is exactly what I've shown in the shaded region).
