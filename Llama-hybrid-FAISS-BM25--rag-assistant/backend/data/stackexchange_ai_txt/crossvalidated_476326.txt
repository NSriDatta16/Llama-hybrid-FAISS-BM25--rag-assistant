[site]: crossvalidated
[post_id]: 476326
[parent_id]: 469799
[tags]: 
This has not to do with that specific log loss function. That loss function is related to binomial/binary regression and not specifically to the logistic regression. With other loss functions you would get the same 'problem'. So what is the case instead? Logistic regression is a special case of this binomial/binary regression and it is the logistic link function that has the asymptotic nature. In addition the 'overfitting' is mostly problematic for cases with perfect separation. Perfect separation and fitting with sigmoid curves If the samples are perfectly separated then the sigmoid shape of the logistic link function can make the fit 'perfect' (zero residuals and overfitted) by increasing the coefficients (to infinity). For instance, in the image below the true model is: $$p(x) = \frac{1}{1 + e^{-2x}}$$ But the data points, which are not equal or close to $p(x)$ but have values 0 or 1, happen to be perfectly separated classes (on one side they are all 0 and on the other side they are all 1), and as a result the fitted values $\hat{p}(x)$ are also fitted equal to 0 and 1 (which the sigmoid function allows by letting $b \to \infty$ ) $$\hat{p}(x) = \frac{1}{1 + e^{-bx}}$$ An analogous example, with a similar tendency to over fit, would be $y_i = sin(b \cdot x_i) + \epsilon_i$ So this is not so much dependent on the type of loss function (or the error distribution) and it is more about the model prediction being able to approach a perfect fit. In the example with this sin-wave you get the overfitting when you do not limit the frequency, in the case with logistic regression you get the over-fitting when you have perfect separation. Why does regularization work You can solve it with regularization, but you should have some good ways to know/estimate by what extent you wish to regularize. In the high-dimensional case it 'works' because the over-fitting (with features that link only to one or a few points/individuals) requires many parameters to be high in value. This will increase the regularization part of the cost function quickly. The regularization will make your fit tend towards 'using less features'. And that corresponds with your prior knowledge/believe that would be that your model should rely on only a few features, instead of a large collection of many itsy-bitsy tiny bits (which could easily be noise). Example For instance, say you wish to predict the probability to become president of the USA, then you might do well with some generalizing parameters like education, parents, money/wealth, gender, age. However your fitted classification model, if it is not regularized, might give weight to the many unique features from each single observation/president (and potentially reach perfect score/separation in the training set, but is not generalizing) and instead of putting weight on a single parameter like 'age' it might use instead things like 'smokes sigars and likes skinny dipping' (and many of them to account for each single president in the observed set). This fitting with overly many different parameters is reduced by regularization, because you might get a better (regularized) loss when there are less parameters with high values (which means that you make the model prefer the more general parameters). This regularization is actually a 'good thing' to do, even without the case of perfect separation.
