[site]: crossvalidated
[post_id]: 288279
[parent_id]: 288273
[tags]: 
AFAIU from your comment, you're talking about recursive feature elimination , specifically, using linear regression. As described here : Recursive feature elimination is based on the idea to repeatedly construct a model (for example an SVM or a regression model) and choose either the best or worst performing feature (for example based on coefficients), setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. Features are then ranked according to when they were eliminated. As such, it is a greedy optimization for finding the best performing subset of features. For your case, you could do the following with sklearn : from sklearn.feature_selection import RFE from sklearn.linear_model import LinearRegression features = ['X1', 'X2', 'X3'] X = df[features] y = df['Y'] prd = LinearRegression() rfe = RFE(prd, n_features_to_select=1) rfe.fit(X, y) You should be able to see the ranks in rfe.ranking_ .
