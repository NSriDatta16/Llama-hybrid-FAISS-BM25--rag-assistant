[site]: crossvalidated
[post_id]: 365340
[parent_id]: 238576
[tags]: 
It's actually an immediate consequence of the fact that $R_e(h)$ is a Monte Carlo estimator for $R(h)$ ( for fixed h ). This is evident if, instead of the terrible notation often used in some introductory books to Machine Learning, where "datasets" are considered, we more properly consider a random vector $\mathbf{X}$ whose $n$ components are iid. The random vector has a probability distribution $$p(\mathbf{X})=p(X_1,\dots,X_n)$$ Now, obviously $R_e(h(X_1),\dots,h(X_n))=f(\mathbf{X})$ is a random variable and we really want to compute its expectation: $$\mathbb{E}_{\mathbf{X}\sim p(\mathbf{X})}[R_e(h)]$$ But this is immediate if we just notice that $$f(\mathbf{X})=\frac{1}{n} \sum_{i=1}^n \mathcal{L}(X_i, h(X_i))=\frac{1}{n} \sum_{i=1}^n g(X_i)=\frac{1}{n} \sum_{i=1}^n Y_i$$ is nothing more than the Monte Carlo estimator for the mean of $Y=g(X)$, a random variable whose mean is nothing more than the true risk. Proof: all $Y_i$ are iid and we have $$\mathbb{E}[Y]=\mathbb{E}_{X\sim p(X)}[g(X)]=\mathbb{E}_{X\sim p(X)}[\mathcal{L}(X, h(X))]=R(h)$$ Now, the Monte Carlo estimator has many interesting properties , but we only need two (actually one, but thanks to the second one I'll also show you an interesting property of Empirical Risk, you didn't ask about): it is an unbiased estimator of true risk, i.e., its mean is equal to the mean of $Y$. As a matter of fact, $$\mathbb{E}_{\mathbf{X}\sim p(\mathbf{X})}[R_e(h(X_1),\dots,h(X_n))]=\mathbb{E}[Y]=R(h)$$ it is a consistent estimator of true risk, i.e., the Monte Carlo estimator converges a.s. to the mean of $Y$ for the sample size $n\to\infty$. In other words $$R_e(h)\overset{a.s.}\to R(h) \ \text{as} \ n\to\infty$$
