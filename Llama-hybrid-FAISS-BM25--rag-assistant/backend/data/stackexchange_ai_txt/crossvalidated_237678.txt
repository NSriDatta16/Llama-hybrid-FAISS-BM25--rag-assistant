[site]: crossvalidated
[post_id]: 237678
[parent_id]: 
[tags]: 
Many questions and thoughts on the relation between machine learning and econometrics

I am looking at a setting where you only have observational data, i.e., non-experimental data and all the predictors $\{X_k\}_{k=1}^p$ are not manipulable, i.e., you view the data set $\{(y_i,\mathbf{X}_i)\}_{i=1}^n$ as $n$ i.i.d. draw from the distribution $P_{\mathbf{x},y}$. I was told that machine learning primarily focus on prediction and may not care about inference----how $y$ changes as some $X_i$ changes. My first question is: in this setting, is knowing inference useful? As none of the predictors are under the researcher's control, non-manipulable, I feel that knowing how $y$ changes as some $X_i$ changes is not that useful. And you cannot draw any causal relation, so I guess in this setting, the goal is probably just prediction. You can argue there are some structural relationship but you cannot test that. Let's say for some reason we still want to do inference, which is also listed as one of the difference between machine learning and econometric. First, it's hard to tell "how $y$ changes as some $X_i$ changes". But there is a well-defined notion of "how $y$ changes as some $X_i$ changes on average", which is closely related to the conditional mean $E[Y|X]$ (with this, we can derive the partiall effect of $x_i$ on $E[y]$). Note that if we have enough data, then we can recover $E[Y|X]$ using data non-parametrically. If we know $E[Y|X]$, then the inference problem is solved. (A digress, "big data" is a relative notion, if the dimension of $\mathbf{X}$ is large, then we need big Big BIG data to estimate $E[Y|X]$ robustly. That's why we still need models like neural nets rather than just non-parametrically estimate $E[Y|X]$, since we can't always do that). My second question is: in econometrics, in most cases, we write down a linear model $y_i=\mathbf{\beta'x_i}+u_i$. Then the textbook spent a lot of ink talk about consistency of OLS estimator and endogenity. Here are a set of specific questions: let's say the data is really generated from $y_i=\mathbf{\beta'x_i}+u_i$ and further, assume $E[\mathbf{x}_iu_i]=0$ (which is not necessarily $E[u_i|\mathbf{x}_i]=0$), then we know OLS estimator of $\beta$ is consistent (no endogenity issue). However, in this case, what is the interpretation of $\beta$? Note that $E[y|\mathbf{x}]=\beta'\mathbf{x}+E[u_i|\mathbf{x}]\neq \beta'\mathbf{x}$. In this case, even if OLS will give you the "true $\beta$" in the limit, but this true $\beta$ does not help you to do inference. This is an interesting case, you can cook the data such that $E[\mathbf{x}_iu_i]=0$ but $E[u_i|\mathbf{x}_i]\neq 0$, in this case, OLS estimator is pretty close to the true $\beta$ (assume large dataset), then the residual $\hat{e}_i\equiv y_i-\hat{\beta}'_{ols}\mathbf{x_i}$ is very close to $u_i$. Hence $E[u_i|\mathbf{x}_i]\neq 0$ means that for some values of $x_i$ (think about $\mathbf{x}$ is one dimension), $u_i$ is negative and for some value of $x_i$, $u_i$ is positive (imagine a U shape pattern fit with a line). Then you probably want to add a quadratic term in your original linear regression model, since that will probably better capture the pattern in the data. However, as I cook the data in this way such that the true model is linear and your OLS estimator is consistent. Everything is perfect, but still a "wrong" model (i.e., adding a quadratic term) will probably have a better predictive power than your true linear model. My question is, in practice, this could also occur, then how can you tell whether a "U" shape data pattern is not coming from a linear model used by nature with $E[\mathbf{x}_iu_i]=0$ but $E[u_i|\mathbf{x}_i]\neq 0$? This is an example where nature (you) generates data using a linear model but with specific joint distribution of $x_i$ and $u_i$ such that $E[\mathbf{x}_iu_i]=0$ but $E[u_i|\mathbf{x}_i]\neq 0$, which makes the data looks like $U$ shape and hence the conditional expectation $E[Y|X]$ is better captured by a quadratic model, i.e., $E[Y|X]=a+bx_1+c x^2_1$. Note that if you use OLS to run the linear regression with 2 predictors, $x_1$ and $x^2_1$, then you get residual $\hat{e}'_i\equiv y_i-\hat{a}_{ols}-\hat{b}_{ols}x_i-\hat{c}_{ols}x^2_1$, you can check whether $E[\hat{e}'_i|x_i]=0$ or not. If it is true, then I would say your model capture the functional relation $E[y|x]$. Then we don't have to worry about endogeneity issue, since the goal is to estimate $E[y|x]$ correctly. In this example, running $y_i=\beta_0+\beta_1x+u_i$ will not have endogeneity issue but it is a terrible estimator of $E[y|x]$. I call this case, you win a battle by running the true linear model with just $x$ and consistently estimated your $\beta$'s but lose a war, since the true $\beta$'s in the case are not helpful to explain $E[y|x]$. how to check whether $E[u_i\mathbf{x}_i]=0$ or $E[u_i|\mathbf{x}_i]$? I don't think that there is a way to do that since $u_i$ is unobservable. (note that here $u_i$ is NOT the residual after you run the OLS and use $y_i$ to subtract $\hat{\beta}'_{ols}\mathbf{x}_i$. $u_i$ comes from the true model $y_i=\beta'\mathbf{x}_i+u_i$ and you need to figure out $\beta$ from data.) My professor said that "If a reviewer just wants to reject an empirical paper anyhow, then the reviewer will just raise the question: did you check the endogenity". I guess this is as I just explained, i.e., there is no way to check whether $E[u_i\mathbf{x}_i]=0$ or $E[u_i|\mathbf{x}_i]$. You can use your domain knowledge but you never know what's has been left into $u_i$. Then my question is: when you read a paper show your R summary table with all those $\beta$'s, how should you trust those $\beta$'s and how should you interpret those $\beta$'s, like in question 1, if just $E[u_iX_i]= 0$ but if $E[u_i|X_i]\neq 0$, then even if you have "true $\beta$", that $\beta$ is not the partial effect of $\mathbf{x}$ on $y$. And if $E[u_iX_i]\neq 0$ (which you never know), then you are not even obtaining the "true $\beta$" but OLS will always produce something, then how should we interpret that something? Like in question 1, a U shape data pattern fitted with a line, then what's the meaning of the slope? liner model gives great interpretability which machine learning model usually don't have, but relies on very strong assumptions that is not testable. In practice, how should we make use of those linear model wisely and with cautious? In practice, what I will do is the following: with the data set, I will try many different types of transformation of the predictors and/or the response variable such as take high order or log or whatever and then call the set of transformed predictors $\mathbf{w}$, then I will run a linear regression of possibly transformed $y$ over $\mathbf{w}$, then I get my residuals $\hat{e}_i=y_i-\beta'\mathbf{w}$, then I check whether $E[\hat{e}_i|\mathbf{w}]=0$ (if it is possible, I check it for each value of vector $\mathbf{w}$), then if it is roughly true, I claim that $E[y|\mathbf{w}]=\beta'\mathbf{w}$ and there will be no endogeneity issues. But it seems that this method has a problem. If nature does generate the data using model $y_i=\beta_0+\beta_1x_i+u_i$such that $x_i$ and $u_i$ are correlated, then I wonder if there is always a transformation of $x_i$ (call it $w_i$) and/or $y_i$ such that after we run OLS for the transformed data, we can get $E[\hat{e}_i|w_i]=0$? I guess not.
