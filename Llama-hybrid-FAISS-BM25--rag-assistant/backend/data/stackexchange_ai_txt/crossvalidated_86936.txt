[site]: crossvalidated
[post_id]: 86936
[parent_id]: 
[tags]: 
ROC graph interpretation

I'm reading Fawcett's 2004 paper on ROC graphs for machine learning algorithms, which can be found here . On page 7-8 he shows a simple ROC example and makes some interpretations that I don't understand. Below is the ROC graph: And here is what he wrote: Although the test set is very small, we can make some tentative observations about the classifier. It appears to perform better in the more conservative region of the graph; the ROC point at (0.1,0.5) produces its highest accuracy (70%). This is equivalent to saying that the classifier is better at identifying likely positives than at identifying likely negatives. Note also that the classifier’s best accuracy occurs at a threshold of ≥ .54, rather than at ≥ .5 as we might expect with a balanced distribution. I don't understand how he derived his interpretations. the ROC point at (0.1,0.5) produces its highest accuracy (70%) How is the highest accuracy of 70% for point (0.1, 0.5) found from that graph, and how do we know it's the highest accuracy? This is equivalent to saying that the classifier is better at identifying likely positives than at identifying likely negatives. I don't see how that interpretation is determined. Note also that the classifier’s best accuracy occurs at a threshold of ≥ .54 How was this found? rather than at ≥ .5 as we might expect with a balanced distribution Why would we expect that? Thank you for any help.
