[site]: crossvalidated
[post_id]: 289473
[parent_id]: 
[tags]: 
Neural Network in classification task behaves "weird"

I am trying to segment an image by first dividing it into superpixel, and than classifying those superpixel to merge them together. If I am doing it for example with a SVM or a Decision Tree , it works fairly well, but is still doing some mistakes. Now I wanted to use an Neural Network , to do the task, but after a few iterations it almost always gets stuck in the local minima and "maps every input to the same class". It behaves in general really weird, because sometimes the accuracy first increases, but than steadily decreases again, till it reaches one of the minima from above (I am talking about the train accuracy, not the test accuracy, so overfitting is not an explanation, I also don't use Stochastic gradient descent but use all train data for an weight update, so fluctuations cant really explain the decreasing accuracy) That looks like this: 617/617 [==============================] - 1s - loss: 1.1174 - acc: 0.2690 Epoch 2/1000 617/617 [==============================] - 0s - loss: 0.8662 - acc: 0.8169 Epoch 3/1000 617/617 [==============================] - 0s - loss: 0.5829 - acc: 0.8136 Epoch 4/1000 617/617 [==============================] - 0s - loss: 6.3685 - acc: 0.9060 Epoch 5/1000 617/617 [==============================] - 0s - loss: 9.0494 - acc: 0.4036 Epoch 6/1000 617/617 [==============================] - 0s - loss: 4.4840 - acc: 0.1297 Epoch 7/1000 617/617 [==============================] - 0s - loss: 1.9398 - acc: 0.1167 Epoch 8/1000 617/617 [==============================] - 0s - loss: 1.8809 - acc: 0.1167 (It stays at the same loss and acc from there) The NN I am using looks like this (even though I also tried many others): from keras.models import Sequential from keras.layers import Dense, Dropout from keras.optimizers import SGD enc = OneHotEncoder() labelOH = enc.fit_transform(label.reshape(-1,1)).A clfN = Sequential() clfN.add(Dense(32, input_dim=feature_array.shape[1])) clfN.add(Dense(64, activation="relu")) clfN.add(Dense(64, activation="relu")) clfN.add(Dense(32, activation="relu")) clfN.add(Dense(labelOH.shape[1])) sgd = SGD(lr=0.0001, decay=1e-10, momentum=0.9, nesterov=True) clfN.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) clfN.summary() clfN.fit(feature_array, labelOH, epochs=1000, batch_size=617) What could be the cause for this behaviour, and what could I do about it?
