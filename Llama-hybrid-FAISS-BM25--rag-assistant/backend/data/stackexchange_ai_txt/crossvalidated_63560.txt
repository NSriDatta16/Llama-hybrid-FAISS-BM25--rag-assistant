[site]: crossvalidated
[post_id]: 63560
[parent_id]: 63552
[tags]: 
You will rarely encounter the word “validated” in the psychometrics literature. Current thinking is that validation is an on-going process on which many type of evidence can be brought to bear. The validity of a particular measurement instrument also depends on the interpretations you want to draw from it and the specific population you are studying (see e.g. Messick, 1995). Consequently, a questionnaire can hardly be validated once and for all. Many techniques that could be relevant (say factor analysis) also often require quite a large sample size to yield stable results, so if you plan to run a relatively small study (either because it is convenient and accepted in your field or better yet because a power analysis suggests it is appropriate), it would not necessarily be very useful to try to “validate” the scale again with these techniques as any discrepant result could/should reasonably be explained away as a consequence of the sample size and you will have extremely low power to empirically distinguish between various potential models. Regarding reliability, there are some authors who indeed recommend computing reliability estimates on your own data instead of relying on published estimates, see Reliability and validity of a standard survey . The idea is that reliability is not so much a property of a scale per se as it a property of the scores (i.e. a set of measures on a specific population). At the same time, computing and reporting usability estimates is fine but what do you intend to do with them? There are many ways to use them of course (correcting other estimates, redesigning the scale…) but they are not always relevant in typical psychology experiments. It is also important to note that reliability is not limited to item-related variance (see generalizability theory for a systematic account) and that popular thresholds are completely arbitrary (see Lance, Butts, & Michels, 2006). In fact, published guidelines vary quite a lot and I can provide well-known “authorities” recommending anything from .5 to .9 as a minimum and even some other authorities arguing against internal consistency as a valuable characteristic for psychological scales. Computing Cronbach's $\alpha$ and writing “it's over .8 so I'm fine”, as many people do, therefore does not seem very useful. All this would seem to make the question moot. You do need to think about the appropriateness of the measurement procedures for your purpose but even if you had the inclination and resources to conduct an ambitious validation study, you would not conclude that your scale is absolutely “valid”, just that it has this or that structure, is related to this or that other measure, etc. It is therefore more fruitful to think about the meaning of your measures and the inferences warranted by your study than to worry about “validating” or not “validating”. One final point to consider is that using well-known scales also increases the comparability between different studies. Even if a particular questionnaire has some well-known limitations, it can be reasonable to use it for that reason alone (possibly together with other scales). References cited: Lance, C.E., Butts, M.M., & Michels, L.C. (2006). The sources of four commonly reported cutoff criteria: What did they really say? Organizational Research Methods, 9 (2), 202-220. Messick, S. (1995). Validity of psychological assessment: Validation of inferences from persons' responses and performances as scientific inquiry into score meaning. American Psychologist, 50 (9), 741-749.
