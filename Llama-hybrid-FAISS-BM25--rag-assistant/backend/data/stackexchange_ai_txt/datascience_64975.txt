[site]: datascience
[post_id]: 64975
[parent_id]: 45503
[tags]: 
I went ahead, and testing a lot of stuff out and I came up with this network which seems to work alright as far as I have tested it def __init__(self, hidden_dim, ms_dim, embeddings): super(LSTMRegressor, self).__init__() self.hidden_dim = hidden_dim # load pretrained embeddings, freeze them self.word_embeddings = nn.Embedding.from_pretrained(embeddings) embed_size = embeddings.shape[1] self.word_embeddings.weight.requires_grad = False self.w2v_lstm = nn.LSTM(embed_size, hidden_dim, bidirectional=True) self.ms_lstm = nn.LSTM(ms_dim, hidden_dim, bidirectional=True) self.linear = nn.Linear(hidden_dim, 1) self.relu = nn.LeakyReLU() def forward(self, batch_size, sentence_input, ms_input): # 1. Embeddings embeds = self.word_embeddings(sentence_input) w2v_out, _ = self.w2v_lstm(embeds.view(-1, batch_size, embeds.size(2))) # separate bidirectional output into first/last, then sum them w2v_first_bi = w2v_out[:, :, :self.hidden_dim] w2v_last_bi = w2v_out[:, :, self.hidden_dim:] w2v_sum_bi = w2v_first_bi + w2v_last_bi # 2. Other features ms_out, _ = self.ms_lstm(ms_input.view(-1, batch_size, ms_input.size(1))) ms_first_bi = ms_out[:, :, :self.hidden_dim] ms_last_bi = ms_out[:, :, self.hidden_dim:] ms_sum_bi = ms_first_bi + ms_last_bi # 3. Concatenate LSTM outputs summed = torch.cat((w2v_sum_bi, ms_sum_bi)) # 4. Only use the last item of the sequence's output summed = summed[-1, :, :] # 5. Send output to linear layer, then ReLU regression = self.linear(summed) regression = self.relu(regression) return regression
