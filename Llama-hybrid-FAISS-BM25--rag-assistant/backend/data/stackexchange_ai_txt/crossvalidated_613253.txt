[site]: crossvalidated
[post_id]: 613253
[parent_id]: 613248
[tags]: 
To eliminate any potential coding bugs, I've rewritten the script to use standard python libraries for machine learning. I also changed the data generation process to have a gap in the middle, similar to the diagram in OP's question. The data in OP's "expected" diagram has a big gap in the middle, but OP's code uses linspace , so there are no big gaps in OP's data. Also, the gaps are the places where the polynomial interpolation swings wildly, buy OP has systematically excluded some of the swings by only plotting the function at the training points. So I generate random data and also plot the true function at regular intervals. import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler, PolynomialFeatures from sklearn.linear_model import LinearRegression np.random.seed(42) n = 500 # Create data set # X = np.linspace(-5, 5, n).reshape(-1, 1) X_small = np.random.randn(5) + 4 X_large = np.random.randn(n - X_small.size) - 3 X = np.hstack((X_small, X_large)) def true_fn(x): return 2.0 * x**3.0 + 5.0 * x**2.0 + 10.0 * x def noisy_fn(x): return true_fn(x) + 30.0 * np.random.randn(x.size) y = noisy_fn(X).reshape((-1, 1)) X = X.reshape((-1, 1)) print(X.shape) print(y.shape) plt.scatter(X, y, s=15, alpha=0.8) plt.xlabel("X") plt.ylabel("y") plt.title("Nonlinear Data") plt.show() degree = 13 X_poly = PolynomialFeatures(degree=degree, include_bias=False) X_scaler = StandardScaler() X_poly_train = X_poly.fit_transform(X) X_poly_train = X_scaler.fit_transform(X_poly_train) lingreg = LinearRegression() lingreg.fit(X=X_poly_train, y=y) theta_fitted = lingreg.coef_ print(f"rank: {lingreg.rank_}") print(f"singular: {lingreg.singular_}") print(theta_fitted) x_test = np.linspace(-5, 5, 1000).reshape((-1,1)) x_test_poly =X_poly.transform(x_test) x_test_poly = X_scaler.transform(x_test_poly) y_test_pred = lingreg.predict(X=x_test_poly) plt.figure(figsize=(10, 5)) plt.plot( x_test, y_test_pred, color="red", label="Fitted Values" ) # Specify label for legend plt.scatter(X, y, s=15) plt.xlabel("Predictor", fontsize=16) plt.ylabel("Target", fontsize=16) # Add legend plt.legend() plt.show() The data looks like this. We can get the desired result with a degree 13 polynomial. The nice thing about this plot is that it also shows that the polynomial is fine in the places where the data are most dense . But the interpolation behavior in sparse regions oscillates. I was never able to get the GD to work -- it always diverged. However, I suspect that it does not produce an optimal solution. You can take a moment to compare the coefficients that you get from gradient descent to a reference implementation, such as sklearn 's LinearRegression . If your GD results have a larger RSS than the reference implementation, then you know you haven't found the optimal fit to the training data! Relatedly, stopping optimization before the optimizer has found the exact minimum is called early-stopping , and can have a regularizing effect similar to penalizing the $L^2$ norm of the coefficient vector.
