[site]: crossvalidated
[post_id]: 578567
[parent_id]: 407921
[tags]: 
The time complexity of gradient descent is: $O(knd)$ where: $k$ is number of iterations $n$ is number of samples $d$ is number of features (or equally number of parameters that are being updated iteratively during the gradient descent) As Valentina Sánchez Bermúdez already pointed out (which is indeed based on the Machine Learning course by Stanford University ), the time complexity of gradient descent is defined as $O(kn^2)$ there. I find such definition less intuitive and seemingly incorrect. In either case, as the number of features grows (say $d > 10^4$ ), then it becomes computationally difficult to calculate $\theta = (X^TX)^{-1}X^Ty$ matrix (aka. the Normal Equation) in the linear regression (particularly the $(X^TX)^{-1}$ part). In those cases, we tend to use the gradient descend method to find the optimal parameters of the linear regression. This is also pointed out in the above course .
