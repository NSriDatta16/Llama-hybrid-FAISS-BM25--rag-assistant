[site]: crossvalidated
[post_id]: 278730
[parent_id]: 278725
[tags]: 
Sounds like you've got this problem: \begin{eqnarray} \mathrm{var_1} &=& y\\ \langle \mathrm{var_2,var_3,var_4} \rangle &=& \vec{x} \end{eqnarray} You're data matrix isn't very large, just 25 x 3: \begin{eqnarray} X &=& \left( \begin{array}{c} \vec{x}_1 \\ \vec{x}_2 \\ \dots \\ \vec{x}_{25}\end{array}\right) \end{eqnarray} with a covariance matrix something like this: \begin{eqnarray} \rho_{ij} = \Sigma_{ij}/\Sigma_{ii}&=& \left( \begin{array}{c} 1 & .97 & .48 \\ .97 & 1 & .5 \\ .48 & .5 & 1\end{array}\right) \end{eqnarray} If the first two components of your predicting variables -- $var_2$ and $var_3$ -- are so correlated, you've got a couple choices: Regularize the regression -- Ridge Regression -- with an L1 or L2 norm. (The L1, or "LASSO", will probably just choose between $var_2$ and $var_3$ if sufficiently strong). Do PCA, as you said, and regress on the resulting features (should make it 3 or less) but you'll probably get similar results as to the option above, with a lot more complexity. The features -- and regression coefficients $\vec{\beta}$ -- will no longer be interpretable. Just drop $var_2$ or $var_3$, based on which one has less incremental benefit -- reduction in $\chi^2$ or RSS -- after fitting a model with only $var_4$, then ($var_2$, $var_4$) vs. ($var_3$, $var_4$)
