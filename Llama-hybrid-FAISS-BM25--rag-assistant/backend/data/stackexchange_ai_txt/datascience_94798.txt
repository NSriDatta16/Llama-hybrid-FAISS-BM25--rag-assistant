[site]: datascience
[post_id]: 94798
[parent_id]: 94401
[tags]: 
There are cutting edge research available on open class classification, however a workaround which is commonly used in production is to use the probability as prediction confidence and only predict class where model is confident. For example when you predict for 'fhjakdlfsah' it should predict the class with a low probability compared to a correct prediction. So you would always have some identified subset of data which your model cannot predict, you may call it fallout/fallback as commonly termed in chatbots. In chatbots the fallback is generally handled by human agents. Key question is what should be the cut-off of probability above which predictions are generally correct? Should it be 0.7/ 0.8/ anything else? Answer to that would come from your specific use case. On a couple of test datasets you need to predict class and probability. Then plot how fallout% and accuracy of non fallout portion change with different choices of probability cutoff. Choose a cutoff which suits your accuracy need as well as manageable volume of fallout. You need to be cautious about probability scale in case your model is not well calibrated. Probabilities may generally be on the higher side. But still the above explained plot should give you an answer.
