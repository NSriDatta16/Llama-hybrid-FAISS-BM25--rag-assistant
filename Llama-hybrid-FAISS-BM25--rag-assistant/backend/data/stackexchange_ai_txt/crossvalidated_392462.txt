[site]: crossvalidated
[post_id]: 392462
[parent_id]: 
[tags]: 
norm of SVM's weights vector

From solving a hard-margin SVM primal problem we get: $$ w = \sum{\alpha_i y_i x_i} \\ \sum{a_i y_i} = 0 $$ Where $\alpha$ is the lagrangian multiplier vector. After solving for $w$ (using the dual problem) we can also solve for $b$ (the bias) with one of the support vectors by $$y_i(wx_i+b)=1$$ Given that, I'm looking for a way to prove that $$\lVert w \rVert ^2 = \sum{\alpha_i}$$ Thanks.
