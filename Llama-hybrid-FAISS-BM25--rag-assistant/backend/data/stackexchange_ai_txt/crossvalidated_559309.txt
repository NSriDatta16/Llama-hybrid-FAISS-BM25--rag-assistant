[site]: crossvalidated
[post_id]: 559309
[parent_id]: 559288
[tags]: 
ORIGINAL ANSWER: All bets are off without knowing what MATLAB is doing here under the heading (robust fit). The results show quite different models: one forced through the origin (intercept zero) and the other omitting a predictor that the first model declares strongly significant. It's implausible -- despite the otherwise appealing figures of merit -- that either model is really a good summary of the data. My guesses (expanding on remarks by others) are that $R^2$ is here added by analogy; the usual interpretation that each regression can be thought of as maximising $R^2$ just does not apply. A better answer is to be found in a closer inspection of your data, with attention to correlations among predictors and also the possibility of extreme skewness and/or outliers. Most of the P-values are too good to be true. The dataset is perhaps a little too large for you to give here, although that would be ideal. We at least need a scatter plot matrix for the five variables concerned. If you can post a listing of your data, please don't use an image but a portable listing with say a line of column headers and then data that are separated by spaces or commas. I'd guess that most people active here don't use MATLAB. EDIT: You have added a listing giving a sight of your data, for which thanks. Presumably y here is y in your first results and x1 x2 x3 are three of the columns column_1 column_2 column_3 column_4 . From a largely graphical analysis I can't see that multiple regression makes sense at all -- at least without a subtle, domain-specific analysis that draws upon a physical (chemical, engineering, whatever) understanding of the set-up. The context calibration of sensors leads me to hope for some simple or least strong relationships. The two graphs following are (1) quantile plots for each variable (just the ordered values against a cumulative probability scale) and (2) a scatter plot matrix. I imagine that these are, or should be, easy in MATLAB. Simple cautions start with noticing Two outliers on y (informally) and hence marked skewness. See the quantile plots. The fact that x2 is almost constant. This may make complete sense in context but such a variable can be awkward at best as a predictor. The presence of puzzling structure in plots of y versus x1 x2 x3 individually and the absence of even rough linearity. I would guess further that the data are a pooling of results from quite different circumstances. None of these facts rule out multiple regression absolutely, but unfortunately they make it less surprising that you get some weird and even contradictory results. Whatever "robust fit" means, the main idea of any robust linear regression should be to get fair results if $y = Xb$ is in essence a good idea, but there are awkward complications in the data. But $y = Xb$ has to be in essence a good idea. I very tentatively ran a quantile regression on this (which may not qualify as "robust", according to people's definitions) and got the calibration plot below. Here calibration plot is sometimes used in what I read to mean a plot of observed response versus fitted or predicted response, but may not correspond to any usage in your field. A pessimist could only bounce this back for more guidance. An optimist might see a hint of a model that might work well for some of the data with a mix of many data points that behave quite differently. (I used Stata, but don't think anything hinges on that. I haven't tried to look for explanations of what robust fit means here. "Robust regression" can mean many different things depending on who is talking, including just plain or vanilla regression with Huber-Eicker-White-sandwich standard errors.)
