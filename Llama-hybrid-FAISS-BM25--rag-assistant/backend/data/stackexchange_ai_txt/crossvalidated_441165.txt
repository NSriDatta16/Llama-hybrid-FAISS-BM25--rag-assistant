[site]: crossvalidated
[post_id]: 441165
[parent_id]: 419294
[tags]: 
Yes, there is, but in practice it may be meaninglessly small. Consider an ARMA process with unknown mean as a subset of the more general problem of a non-diagonal, possibly heteroskedastic covariance matrix in a linear regression of the form: $$ y = \beta X + \epsilon, \,\,\epsilon \sim \text{Normal}(0,\Sigma)$$ In your case, $X$ consists of a single column of ones, i.e., the intercept, and $\beta$ is the mean. In ARMA modeling, $\Sigma$ has a lot of structure, and can be written in terms of a finite number (this is important for asymptotics) of parameters $\theta$ (for the AR part) and $\phi$ (for the MA part): $\Sigma(\theta, \phi)$ . To see how the parameters of an ARMA process define a covariance matrix, consider the autocorrelation function (ACF) commonly used to help identify the lag structure of $\theta$ and $\phi$ . This function gives the correlation between $\epsilon_t$ and $\epsilon_{t+h}$ for all $h$ ; clearly the relationship between this function and the ARMA parameters is is sufficient to define the correlation matrix of $\epsilon$ in terms of $\theta$ and $\phi$ , and is one small step from defining the covariance matrix of $\epsilon$ in terms of $\theta$ and $\phi$ . For example, the correlation matrix associated with an AR(1) process is defined by: $$\rho_{ij} = \theta^{|i-j|}$$ and that of an MA(1) process by: $$\rho_{ii} = 1,\,\rho_{i,i+1} = \rho_{i,i-1} = {-\phi \over 1+\phi^2}$$ with other entries equal to $0$ . The two-step process described in your question essentially estimates $\beta$ via OLS, then the parameters of $\Sigma(\theta, \phi)$ via, for example, MLE. However, the Gauss-Markov theorem tells us that the OLS estimate of $\beta$ is not efficient when $\Sigma$ is known, instead, $\tilde{\beta} = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}y$ is the BLUE (Best Linear Unbiased Estimator). This is known as the Generalized Least Squares (GLS) estimator. This is also the MLE under the assumption of Normality of $\epsilon$ . Since we have to estimate $\Sigma$ , Gauss-Markov doesn't apply, and we have to resort to some form of Feasible Generalized Least Squares (FGLS.) However, it turns out that the FGLS estimator will have the same asymptotic distribution as the GLS estimator if: $$\text{plim} {X^T(\hat{\Sigma}^{-1} - \Sigma^{-1})X \over N} = 0 $$ and $$\text{plim} {X^T(\hat{\Sigma}^{-1} - \Sigma^{-1})\epsilon \over \sqrt{N}} = 0 $$ Given that there are a finite number of parameters in $\Sigma$ , and $X$ is just a vector of ones, these are not particularly strenuous assumptions to make. These are sufficient conditions, and, clearly, if they hold, the joint estimator is superior asymptotically to the two-step estimator. When doing ARMA modeling, you would just use the standard time series algorithms for estimating ARMA parameters, not try to estimate the covariance matrix directly. In practical applications, though, we are often not in the happy asymptotic world, and it's not so clear in finite samples that there is anything to be gained. Here's an example comparing the two procedures on a zero-mean ARMA(2,2) process with 250 observations: parms $coef) res_two_step[i,5] coef) } mse_one_step with results: > sqrt(mse_one_step) [1] 0.3933203 0.1974865 0.4008505 0.1942562 0.1434249 > sqrt(mse_two_step) [1] 0.3936735 0.1977207 0.4011226 0.1942403 0.1434510 > sqrt(mse_two_step)/sqrt(mse_one_step) [1] 1.0008980 1.0011859 1.0006788 0.9999183 1.0001815 Essentially no difference. The fact that we are estimating the parameters of the true process instead of going through an identification or model selection step is unlikely to have made a difference, given how similar the results are.
