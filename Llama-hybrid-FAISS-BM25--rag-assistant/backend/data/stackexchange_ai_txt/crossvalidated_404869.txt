[site]: crossvalidated
[post_id]: 404869
[parent_id]: 404840
[tags]: 
The subject you are referring to is called Multicollinearity . When some features of your input data are correlated, it might decrease your model performance. Nevertheless, the impact highly depends on your data and the type of model you are using. You can read more about it here : Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results. Luckily, decision trees and boosted trees algorithms are immune to multicollinearity by nature. When they decide to split, the tree will choose only one of the perfectly correlated features. However, other algorithms like Logistic Regression or Linear Regression are not immune to that problem and you should fix it before training the model.
