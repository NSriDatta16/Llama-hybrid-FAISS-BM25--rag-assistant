[site]: datascience
[post_id]: 25468
[parent_id]: 25467
[tags]: 
Welcome to the site! If I understand your question correctly you want to know why a model would perform worse when a new feature is added? So every time you do feature engineering (add new columns, derive columns, standardize the data, normalize the data, etc) there is always a flip side of the coin. If you add some features and if those features explain something about the target variable then it would aid in increasing the accuracy; on the other hand, if the feature doesn't have much relation to the target variable then it doesn't aid in increasing the accuracy. Now before going to modeling you can go through a couple of things like (assuming that you have done all these things): Eliminating Unnecessary Features using Business Understanding Removing Outliers Imputing Missing Data (XGBoost is not prone to missing values) Standardizing data Correlation analysis with the target variable and within the variables too, eliminate if there is high correlation between variables (as you need independent variables), eliminate the variables which are not much related to the target variable. Once the above steps are done you can get a model which explains the data well. To improve the accuracy you need to do more feature engineering and try to find if there are some external factors which might effect your model. There are many reasons why a model doesn't work well. Some of the above stated might be the reason why your model is not performing well in your scenario. Do have a closer look on the data -- this might give you a better idea. This is a top level view.
