[site]: crossvalidated
[post_id]: 313708
[parent_id]: 313698
[tags]: 
There are two major differences between what you have described and how dropout is being used in neural networks. Neural networks have typically multiple layers, each using the features extracted by the previous one, building a hierarchy of more complex, distributed features. Dropout is used not only on the input features, but also after (every) layer of the network, therefore it cannot be simply replaced by a different training set 1 . Let's say we only talk about a single layer neural network (disregarding my 1. point). Then there still is a major difference: all the trained predictors $h^1 \ldots h^B$ share their weights : they are not independent classifiers as your decision trees. Instead, every unit of the network (which is not dropped at the given iteration) is updated, learning not to rely too much on particular features, thus preventing what authors call "feature co-adaptation". 1 Actually, there is a paper by Xavier Bouthillier et al. "Dropout as data augmentation" describing how one can modify the input data to simulate the effect of dropout in deeper layers, so in certain scenarios you could disregard the point 1. altogether.
