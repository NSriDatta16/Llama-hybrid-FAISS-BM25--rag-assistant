[site]: crossvalidated
[post_id]: 233152
[parent_id]: 
[tags]: 
Universal Kriging

In universal kriging the general model is $$y(x)=h(x)^{t}\boldsymbol\beta + f(x)+\epsilon(x)$$ where $h(x)^{t}$ is a regression function such as $\left [1,x,x^{2} \right ]$ and $\boldsymbol\beta$ are the set of regression coefficients and $$f_{i}(x)\sim GP(0,K(x,x^{'}))$$ and $\epsilon(x)$ is normal with zero mean and $\sigma^{2}$ variance I am wondering whether $h(x)^{t}\boldsymbol\beta$ is assumed deterministic and thus to find $\boldsymbol\beta$ we need to optimize the the multivariate gaussian likelihood function to find the hyperparameters in $K(x,x^{'})$ and $\boldsymbol\beta$ where the joint distribution of $y(x)$ is $$y(x)\sim GP(h(x)^{t}\boldsymbol\beta,K(x,x^{'})+\sigma^{2}I)$$ and thus the predictive distribution is $$y^{*}|\mathbf{y} \sim N(K_{*}K^{-1}(\mathbf{y}-h(\textbf{x})^{t}\hat{\boldsymbol\beta}),K_{**}-K_{*}K^{-1}K_{*}^{T})$$ However, In the famous Gaussain process for machine learning book Ch2 page 28 , it seems that $\boldsymbol\beta$ is a random variable and the likelihood function and predictive distribution are different. I need help understanding the distinction between the method I described above (if valid) and the approach in the book. The main related text is attached as a pic $$$$
