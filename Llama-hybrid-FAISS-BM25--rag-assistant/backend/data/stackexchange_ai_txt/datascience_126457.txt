[site]: datascience
[post_id]: 126457
[parent_id]: 
[tags]: 
How to treat "ignore" boxes during object detector evaluation?

I want to evaluate the performance of multiple object detectors for comparison, each of them has been trained on this data. This dataset has images labeled, where each image can have multiple labels. Additionally, it has an attribute called "ignore" which indicates this particular crop of the image is not too obvious to be taken as either a positive or a negative example. Following is an example of this labeling: front front front I know how to treat these "ignore" boxes during train time, but I don't understand how I treat them during evaluation. I am particularly interested in calculating precision, recall, and average precision. I am working with binary class classification (true, false). How do I include this "ignore" boxes' information in the evaluation? I am particularly interested in doing this with some libraries. I preferably don't want to implement this myself as any faulty implementation can lead to misleading interpretation. So, I want to keep the implementation workload as low as possible on my side. However, couldn't find any appropriate library to do it. Moreover, I have been evaluating this as a part of the grid search, so I need it in the form of a library method, where I will pass the necessary information (ground truth, ignore boxes, prediction boxes) and the method will return me the metrics.
