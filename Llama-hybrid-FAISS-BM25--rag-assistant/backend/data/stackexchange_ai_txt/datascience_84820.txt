[site]: datascience
[post_id]: 84820
[parent_id]: 84810
[tags]: 
GANs do not provide any guarantee on the distribution of the generated data. On the contrary, they are notorious for their mode collapse problems (i.e. generating always the same values). Therefore, I doubt that they are a reliable way of systematically generating synthetic data for other systems to train. Oversampling techniques like SMOTE are normally much better suited for the likes of your scenario. There are other techniques like providing class weights (see this ). Apart from handling the imbalance, I think the most important aspect here is to use an evaluation measure that behaves well in this scenario and don't lead you to think your model is better than it actually is. Some alternatives for this are the area under the ROC curve (AUC) or the precision-recall AUC. Update: regarding the updated information in the question, I think that, while knowing what kind of analysis we want to perform is crucial for determining what preprocessing techniques are acceptable, creating artificial data (with GANs or with any other method) would totally ruin any analysis you may want to apply, as you may be altering key aspects like the data distribution.
