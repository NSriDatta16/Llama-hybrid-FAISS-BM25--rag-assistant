[site]: crossvalidated
[post_id]: 552259
[parent_id]: 86472
[tags]: 
Bayes theorem is $$ p(A|B) = \frac{ p(B|A) \, p(A) }{ p(B) } $$ Based on prior knowledge you can define a prior, it can be updated using the data, then via Bayesian updating you could use the posterior as a prior for a next update, etc. Think of the prior as a way of augmenting your data with artificial data . In such a case, thereâ€™s nothing strange if you have two different sets of data for the final result to be an average of those two datasets. This is a "feature" of Bayes theorem, we want the prior to influence the result and we want the end result to be somewhere "in between" prior and the data, weighting both by how strong information they provide. In many cases you may be interested in running the prior predictive checks before running the analysis, i.e. simulating predictions with parameters sampled from the priors and comparing them to the data. It would help you avoid surprises like this.
