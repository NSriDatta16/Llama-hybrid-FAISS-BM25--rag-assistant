[site]: datascience
[post_id]: 52925
[parent_id]: 
[tags]: 
Fewer observations & larger documents vs More observations & smaller documents

Let's suppose that I have a dataset of 1000 documents. Each document is a restaurant review (so relatively short text) and it has labels {Negative, Indifferent, Positive}. Let's suppose that the dataset has 600 positive reviews, 200 indifferent reviews and 200 negative reviews. I want to train a classifier to classify a review as Negative or Indifferent or Positive based on the text of the review. I am not thinking about using any word embeddings for now so I will probably use a TF or TF-IDF model (even though this may be a bit off topic for current question). Let's suppose that in my case I split (in a stratified way) my dataset into a training set of 800 observations and into a test set of 200 observations. My question is the following: Is it better to have 800 separate documents in my training set or to merge these documents based on its categories/labels and create 3 very big documents? There 800 separate documents of any of the 3 labels or 3 big documents of each of the labels is the best way to go and why? My question stems from the fact that in the latter case if for example I do TF-IDF then this will be applied based on different categories/labels since each document will be about a category/label. On the other hand, if I do (as we usually do actually) like in the former case then the TF-IDF will be categories/labels-agnostic and I do not know this helps things. Is the answer simply that this an interesting but pretty bad idea because in this way you simply massively decrease the number of the observations with which the model/algorithm is trained and so you make much harder for him to figure out how to successfully classify things?
