[site]: datascience
[post_id]: 8332
[parent_id]: 
[tags]: 
Is my Lasagne/Theano neural network running too slow?

So I'm a newcomer to the world of neural networks, and I've been getting a little familiar with the field and have started playing with my own networks. I'm using Lasagne , and I'm finding that the training of my NN is taking an infeasible amount of time. Maybe the problem I'm looking at is simply biting more than my computer can chew; I just don't know, because I'm not familiar enough with how these things go. I'm trying to train a 1D convolutional NN, which has two 1D convolution layers, and two max-pooling layers, and two fully connected layers. All in all, the NN has around 95,000 parameters (weights + biases). I'm running this on an Amazon EC2 GPU instance. What I'm finding is that during training, it's taking 0.3 sec/sample. By contrast, when I run the Lasagne MNIST CNN example, it's taking about 0.0003 sec/sample, 1000 times faster! Now to be fair, my input samples are 18000x3 dimensional, and the MNIST samples are 28x28 dimensional; but the difference there is a factor of 68. And the MNIST CNN actually has more parameters than mine, with 160,000 or so parameters. Does this indicate anything wrong with what I'm doing?
