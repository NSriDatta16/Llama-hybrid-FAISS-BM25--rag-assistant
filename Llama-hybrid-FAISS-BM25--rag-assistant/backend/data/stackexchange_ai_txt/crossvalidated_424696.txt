[site]: crossvalidated
[post_id]: 424696
[parent_id]: 
[tags]: 
Trying to improve the generalization ability of decision tree with bagging or random forest

We are fitting a regression tree on a sample of about 5000 observations with 5 predictors. The data stems from 2 sources. There are reasons to suspect that the tree will not have exaggerated generalization capability and will have increased variance. Therefore, row-wise bootstrapping or random forests might be an option for us. We would see this as a form of regularization of the original problem. We could apply these techniques both on the sample of 5000 as well as on a subsample of 500 observations which stems for our original data source, which is more representative of the future application sample. Question: Are there rules of thumb, assumptions or experiences regarding on the size of the dataset when such methods (bootstrapping, Random forest) improve over simple regression trees ? Or does it solely depend on the data at hand, such that it has to be found by oos/oot validation? Appendix: to give some background: the task is to build an LGD (loss given default used in banking) model based on internal (the smaller set) as well as external data (the bigger one).
