[site]: crossvalidated
[post_id]: 310587
[parent_id]: 
[tags]: 
What is the most efficient feature engeenering approach for Kaggle-like data?

I am wondering what could be the most efficient approach to feature engineering with data consisting of huge number of anonymous variables of unknown, or blurry meaning? To not make it too broad, let's focus on numerical and one-hot-encoded categorical variables, and ignore working with textual data. By efficient I mean here such approach that could possibly lead to greatest improvement out-of-the box and is unlikely to do harm. Obviously there is lots of things that could be done with such data and lots of ways how you could waste your time producing literally hundreds of meaningless features (say polynomials of all the features and their interactions), so I'm asking for the approach that is unlikely for you to lead to wasting your time, just to get very minor improvements in the predictions quality. Is there any safe recommendation for data like this? I am not looking for opinions, but for something that has proven accuracy.
