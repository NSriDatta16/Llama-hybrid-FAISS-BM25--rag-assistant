[site]: crossvalidated
[post_id]: 357247
[parent_id]: 120080
[tags]: 
The currently accepted answer by @bayerj states that the weights of a linear autoencoder span the same subspace as the principal components found by PCA, but they are not the same vectors. In particular, they are not an orthogonal basis. This is true, however we can easily recover the principal components loading vectors from the autoencoder weights. A little bit of notation: let $\{\mathbf{x}_i \in \mathbb{R}^n \}_{i=1}^N $ be a set of $N$ $n-$dimensional vectors, for which we wish to compute the PCA, and let $X$ be the matrix whose columns are $\mathbf{x}_1,\dots,\mathbf{x}_N$. Then, let's define a linear autoencoder as the one-hidden layer neural network defined by the following equations: $$ \begin{align} \mathbf{h}_1 & = \mathbf{W}_1\mathbf{x} + \mathbf{b}_1 \\ \hat{\mathbf{x}} & = \mathbf{W}_2\mathbf{h}_1 + \mathbf{b}_2 \end{align} $$ where $\hat{\mathbf{x}}$ is the output of the (linear) autoencoder, denoted with a hat in order to stress the fact that the output of an autoencoder is a "reconstruction" of the input. Note that, as it's most common with autoencoders, the hidden layer has less units than the input layer, i.e., $W_1\in \mathbb{R}^{n \times m}$ and $W_2\in \mathbb{R}^{m \times n}$ with $m Now, after training your linear autoencoder, compute the first $m$ singular vectors of $W_2$. It's possible to prove that these singular vectors are actually the first $m$ principal components of $X$, and the proof is in Plaut, E., From Principal Subspaces to Principal Components with Linear Autoencoders , Arxiv.org:1804.10253. Since SVD is actually the algorithm commonly used to compute PCA, it could seem meaningless to first train a linear autoencoder and then apply SVD to $W_2$ in order to recover then first $m$ loading vectors, rather than directly applying SVD to $X$. The point is that $X$ is a $n \times N$ matrix, while a $W_2$ is $m\times n$. Now, the time complexity of SVD for $W_2$ is $O(m^2n)$, while for $X$ is $O(n^2N)$ with $m
