[site]: crossvalidated
[post_id]: 366076
[parent_id]: 
[tags]: 
Understanding a derivation of bias correction for the Adam optimizer

I'm reading paper about the Adam optimizer and went up until the bias-correction section; in the paper they estimate the bias of the moving average of the squared gradient. These are the equations they use: Where $v_{t}$ is the moving average of gradient squared, $g_{i}$ is gradient at the $i^{th}$ iteration and $\beta_{2}$ is a moving average parameter. I don't understand how they make any of these transitions from the first row to the second and on to the third one. Could someone explain what rules are allowing them to do so ?
