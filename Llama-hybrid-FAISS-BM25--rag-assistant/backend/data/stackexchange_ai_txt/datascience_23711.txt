[site]: datascience
[post_id]: 23711
[parent_id]: 23709
[tags]: 
Yes, sequence 2 sequence models attempt to do this. This can be used in a number of domains, from typo fixing to machine translation. They are encoder -> decoder based, which means you have a part that encodes your input and then a decoder that generates a new sequence based on this encoding (and usually some attention). In this case your encoder would likely be two recurrent neural networks of which the output would be concatenated and then a decoder that takes this concatenated output and turns this into a new sequence. If you want to use attention you need to adapt the standard attention a bit because you have two textual inputs, but if you understand how it works this would not be too difficult to adapt.
