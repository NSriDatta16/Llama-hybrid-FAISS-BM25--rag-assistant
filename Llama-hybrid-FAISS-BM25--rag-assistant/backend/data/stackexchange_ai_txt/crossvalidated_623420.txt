[site]: crossvalidated
[post_id]: 623420
[parent_id]: 
[tags]: 
Best way to show one Bayesian model is more certain and accurate than another, based on simulated data?

I'm trying to compare performance of two bayesian models $A$ and $B$ on simulated data. It's a recruitment curve fitting problem and I'm interested in how accurate these models are in estimating only one parameter of the curve, let's call that parameter $q$ . I run $10$ trials (10 different simulations). For every trial, I have the true value for $q$ , as well as the HPDI credible intervals and MAP estimates for $q$ from models $A$ and $B$ . Accuracy: The naive way would be to compare these models based on mean absolute error or mean squared error between their MAP estimates and true values of $q$ , but I feel like this is not very Bayesian. I'd like to use credible intervals instead. Is it possible to come up with a metric which uses credible intervals? I want to quantify how reliable model A's credible interval is to model B's. Uncertainity: The naive (and possibly incorrect) way would be to measure the length of their credible intervals -- but one could could have a tighter interval and not contain the true estimate. Ideally, I would like to come up with a metric that combines both accuracy and uncertainity. Is there any existing theory/research on this? Any help would be highly appreciated.
