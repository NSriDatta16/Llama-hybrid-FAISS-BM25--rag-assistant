[site]: crossvalidated
[post_id]: 270639
[parent_id]: 
[tags]: 
Consequences of the Gaussian correlation inequality for computing joint confidence intervals

According to this very interesting article in Quanta Magazine: "A Long-Sought Proof, Found and Almost Lost" , -- it has been proved that given a vector $\mathbf{x}=(x_1,\dots,x_n)$ having a multivariate Gaussian distribution, and given intervals $I_1,\dots,I_n $ centered around the means of the corresponding components of $\mathbf{x}$ , then $$p(x_1\in I_1, \dots, x_n\in I_n)\geq \prod_{i=1}^n p(x_i\in I_i) $$ (Gaussian correlation inequality or GCI; see https://arxiv.org/pdf/1512.08776.pdf for the more general formulation). This seems really nice and simple, and the article says it has consequences for joint confidence intervals. However, it seems quite useless in that respect to me. Suppose we are estimating parameters $\theta_1,\dots,\theta_n$, and we found estimators $\hat{\theta_1},\dots,\hat{\theta_n}$ which are (maybe asymptotically) jointly normal (for example, the MLE estimator). Then, if I compute 95%-confidence intervals for each parameter, the GCI guarantees that the hypercube $I_1\times\dots I_n$ is a joint confidence region with coverage not less than $(0.95)^n $...which is quite low coverage even for moderate $n$. Thus, it doesn't seem a smart way to find joint confidence regions: the usual confidence region for a multivariate Gaussian, i.e., an hyperellipsoid, is not hard to find if the covariance matrix is known and it's sharper. Maybe it could be useful to find confidence regions when the covariance matrix is unknown? Can you show me an example of the relevance of GCI to the computation of joint confidence regions?
