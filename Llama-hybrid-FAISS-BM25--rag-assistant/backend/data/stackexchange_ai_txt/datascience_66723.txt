[site]: datascience
[post_id]: 66723
[parent_id]: 
[tags]: 
What is the role of $W_{ax}, W_{aa}, W_{ay}$ in forward propagation in RNN? Are they hyperparameters? Why are they needed?

In RNN introduction in Coursera sequence model course, the following formula for forward propagation in RNN was introduced. What exactly is the role of $W_{ax}, W_{aa}, W_{ay}$ ? What do they do? In the lecture, it was told that: $W_{ax}$ : parameter governing connection from $x$ to hidden layer (not sure what exactly does that mean: what happens if it is not governed?) $W_{aa}$ : governing activations (Why govern activations? What happens if it is not governed?) $W_{ay}$ governs output prediction (What is the point of governing that? What happens if it is not governed?) In standard neural network these were the formula of forward propogation $$ z_1 = w_1 X_1+b_1 \\ A_1 = g(Z_1) $$ Consider there were only 4 layers then last layer $$ z_4 = w_4X_4 + b_4 \\ A_4 \text{ or } \hat{y} = g(z4) $$ I'm able to correlate these equation with $a^{ }$ in RNN but I am unable to correlate the role of $W_{ya}$ present in $\hat{y}^{ }$ highlighted in yellow. Please explain in very simple terms with an example on what is the job of $W_{ax}, W_{aa}, W_{ay}$ in forward pass in standard RNN.
