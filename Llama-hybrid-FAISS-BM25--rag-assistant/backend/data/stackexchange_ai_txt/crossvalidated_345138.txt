[site]: crossvalidated
[post_id]: 345138
[parent_id]: 255877
[tags]: 
You're absolutely correct. The log likelihood of most GAN models on most datasets or true distributions is $-\infty$, and the probability density under the trained model for any actual image is almost always zero. This is easy to see. Suppose a GAN has a 200-dimensional latent variable and is generating 200x200 grayscale images. Then the space of all images is 40,000-dimensional, and the GAN can only ever generate images on a 200-dimensional submanifold of this 40,000-dimensional space. Real images will almost always lie off this submanifold, and so have a probability density of zero under the GAN. This argument holds whenever the output space has higher dimension than the latent space, which is typically the case (e.g. Nvidia's recent progressive GANs used a 512-dimensional latent space to generate 1024x1024x3 images). Whether this is a problem or not depends on what you want the generative model for; GANs certainly generate visually attractive samples in many cases.
