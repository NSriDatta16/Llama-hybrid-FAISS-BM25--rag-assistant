[site]: crossvalidated
[post_id]: 443578
[parent_id]: 
[tags]: 
Computing Gradients for a [-1, 1]-valued RBM

The gradient derivation for a binary-valued RBM with values $\in\{0,1\}$ is well-documented, for example in Goodfellow, et al and here on Cross Validated . However, in some works (e.g., associative GANs and their quantum variants ), it is popular to use binary RBM variables in the range $\{-1, 1\}$ instead. How would this change in variable range affect the gradient derivation? I know we can still exploit the bipartite structure of the RBM but am not sure how the rest of the simplifications and subsequent equations would fare. As an example, the gradients of a popular TensorFlow implementation are h0_props = self.propup(visibles) w_positive_grad = tf.matmul(tf.transpose(visibles), h0_props) w_negative_grad = tf.matmul(tf.transpose(v_samples), h_samples) w_grad = (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(visibles)[0]) hb_grad = tf.reduce_mean(h0_props - h_samples, 0) vb_grad = tf.reduce_mean(visibles - v_samples, 0) where propup computes $P(h|v)$ . Changing the input range (and removing ReLU activations when updating either $\bf{v}$ or $\bf{h}$ ) makes the RBM fluctuate wildly, leading me to assume the gradients are no longer valid. I was hoping someone more familiar with the math could elucidate.
