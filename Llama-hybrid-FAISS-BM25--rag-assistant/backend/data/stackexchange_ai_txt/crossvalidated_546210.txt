[site]: crossvalidated
[post_id]: 546210
[parent_id]: 546183
[tags]: 
You have encountered a very common phenomenon that we call overfitting ! Pretty much every statistical model fits its training data better than new, unseen test data. This is because the model is the best possible fit to the training data . Even if there is a real relationship between the inputs and the response, it is probably not exactly what you've estimated from the training data. So you should expect to see a lower $R^2$ on test data than on training data. This same thing happens when there is actually no true relationship between the inputs and the response . In that case, the model fitting will overfit to any apparent pattern that it sees in the random noise. Then, when you try to apply this model to test data, its random noise will be different, and you may find that your model is worse than just predicting the new data will match its average. In that case, you can have a negative $R^2$ on the test data.
