[site]: datascience
[post_id]: 115694
[parent_id]: 115655
[tags]: 
In my opinion, it does not make sense from a hypothetical perspective. The Boosting models and Recurrent Neural Network models are designed to serve different purposes. If we consider natural language processing-based tasks like the one you mentioned regarding text classification, we were previously solving text classification problems using the bagging models, then we brought boosting models, then we brought Recurrent Neural Networks, and now we are utilizing Transformer based architectures using an attention mechanism. If you already have used an advanced model in the previous stage which did great work in converting textual features into representative numeric features while preserving the context of the data, it is not a costly thing to use a Sigmoid or Softmax layer to classify such text. Bringing another boosting or all-alone machine learning architecture does not make sense in this case. If that would make sense Google Research, Microsoft Research, NeurIPS papers, and ICML papers would not bring novel architectures based on new concepts rather they would have randomly stacked several machine learning methods over one another to solve real-world problems.
