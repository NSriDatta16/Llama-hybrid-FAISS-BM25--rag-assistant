[site]: crossvalidated
[post_id]: 402958
[parent_id]: 378268
[tags]: 
In a classical machine learning (i.e. statistical learning theory) setup, the number of parameters usually enters via the Vapnikâ€“Chervonenkis (VC) dimension and the number of observations via the PAC bound. Very roughly speaking, this says that for classification problems, the worst-case difference in 0-1 loss between training and test set is of the order $\sqrt{D/N}$ with $N$ number of observations and $D$ the VC dimension. Usually, the VC dimension increases in the number of parameters (how exactly depends on the model class). This result can be generalized beyond the binary classification setting. For neural networks, a quick Google scholar search gives for example Size-Independent Sample Complexity of Neural Networks . More recent results support the idea that as the number of parameters passes a threshold of perfect (over)fitting, the test error decreases again since a model with more parameters is more expressive and able to fit the data using smoother functions. This is probably what is happening in your example. See for example Reconciling modern machine learning and the bias-variance trade-off or Generalization in Machine Learning via Analytical Learning Theory
