[site]: datascience
[post_id]: 53184
[parent_id]: 53181
[tags]: 
Hi and welcome to the forum. Just as an idea: The strategy you suggested (extracting important classes in the X and leave the rest as "other") may work and it‘s worth a try. Make sure you don‘t throw away information by keeping the levels in the "other" category. The alternative is (of course) to one-hot encode all classes. In any case, I would check if regulation (by L2 or L1 norm) is useful. Some background: L1 regulation can shrink features to zero, L2 regulation can shrink features but they never become zero. So say you have a lot of one-hot encoded features but you have no idea which one are really important. Just let the computer do the job of selecting features by L1 regulation. Method wise, the problem sounds like a candidate for boosting. Boosting is similar to random forest, but in addition, the algorithm tried to focus on especially hard to predict rows/observations. This is done by growing a lot of very small trees. Prominent algorithms are LightGBM, Catboost, XGboost. They all offer support for regulation. So I would start with "a lot" one-hot encoded features and boost with L1 regulation to get rid of irrelevant features.
