[site]: crossvalidated
[post_id]: 194253
[parent_id]: 
[tags]: 
What does entropy tell us?

I am reading about entropy and am having a hard time conceptualizing what it means in the continuous case. The wiki page states the following: The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose expected value is the average amount of information, or entropy, generated by this distribution. So if I calculate the entropy associated with a probability distribution that is continuous, what is that really telling me? They give an example about flipping coins, so the discrete case, but if there is an intuitive way to explain through an example like that in the continuous case, that would be great! If it helps, the definition of entropy for a continuous random variable $X$ is the following: $$H(X)=-\int P(x)\log_b P(x)dx$$ where $P(x)$ is a probability distribution function. To try and make this more concrete, consider the case of $X\sim \text{Gamma}(\alpha,\beta)$, then, according to Wikipedia , the entropy is \begin{align} H(X)&=\mathbb{E}[-\ln(P(X))]\\ &=\mathbb{E}[-\alpha\ln(\beta)+\ln(\Gamma(\alpha))+\ln(\Gamma(\alpha))-(\alpha-1)\ln(X)+\beta X]\\ &=\alpha-\ln(\beta)+\ln(\Gamma(\alpha))+(1-\alpha)\left(\frac{d}{d\alpha}\ln(\Gamma(\alpha))\right) \end{align} And so now we have calculated the entropy for a continuous distribution (the Gamma distribution) and so if I now evaluate that expression, $H(X)$, given $\alpha$ and $\beta$, what does that quantity actually tell me?
