[site]: crossvalidated
[post_id]: 326398
[parent_id]: 326343
[tags]: 
After some searching around pretty hard I found a solid answer to my question of why not just use plain old ML instead of EM algorithm in a book called Pattern Recognition and ML by Christopher Bishop . Excerpt copied exactly below from pages 433,434 (hopefully this is OK to do since I referenced/gave credit to the source). Note he has a figure reference that helps. Before discussing how to maximize [a GMM likelihood] function, it is worth emphasizing that there is a significant problem associated with the maximum likelihood framework applied to Gaussian mixture models, due to the presence of singularities. For simplicity, consider a Gaussian mixture whose components have covariance matrices given by $\sum{k}=\sigma^2I$ , where $I$ is the unit matrix, although the conclusions will hold for general covariance matrices. Suppose that one of the components of the mixture model, let us say the j-th component, has its mean $u_j$ exactly equal to one of the data points so that $µ_j = x_n$ for some value of n. This data point will then contribute a term in the likelihood function of the form $N(x_n|x_n, σ^2_j I) = \frac{1}{(2π)^{1/2}}\frac{1}{σ_j}$ . If we consider the limit $σ_j → 0$ , then we see that this term goes to infinity and so the log likelihood function will also go to infinity. Thus the maximization of the log likelihood function is not a well posed problem because such singularities will always be present and will occur whenever one of the Gaussian components ‘collapses’ onto a specific data point. Recall that this problem did not arise in the case of a single Gaussian distribution. To understand the difference, note that if a single Gaussian collapses onto a data point it will contribute multiplicative factors to the likelihood function arising from the other data points and these factors will go to zero exponentially fast, giving an overall likelihood that goes to zero rather than infinity. However, once we have (at least) two components in the mixture, one of the components can have a finite variance and therefore assign finite probability to all of the data points while the other component can shrink onto one specific data point and thereby contribute an ever increasing additive value to the log likelihood. This is illustrated in Figure 9.7. These singularities provide another example of the severe over-fitting that can occur in a maximum likelihood approach. We shall see Section 10.1 that this difficulty does not occur if we adopt a Bayesian approach. For the moment, however, we simply note that in applying maximum likelihood to Gaussian mixture models we must take steps to avoid finding such pathological solutions and instead seek local maxima of the likelihood function that are well behaved.
