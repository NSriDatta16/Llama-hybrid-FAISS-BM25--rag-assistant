[site]: datascience
[post_id]: 85737
[parent_id]: 27236
[tags]: 
An ensemble of 15 CNNs classify Kaggle's MNIST digits after training on Kaggle's 42,000 images plus 25 million more images created by rotating, scaling, and shifting Kaggle's images. Learning from 25,042,000 images, this ensemble of CNNs achieves 99.75% classification accuracy. Techniques include data augmentation, nonlinear convolution layers, learnable pooling layers, ReLU activation, ensembling, bagging, decaying learning rates, dropout, batch normalization, and adam optimization. GM CDeotte explains the arch in great details here: https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist The SOTA link shared in the other answer uses homogeneous vector capsules generated from the final filters and gives 99.85% - a full 0.1% increase. However it is not a plain vanilla Apple to Apple conversion as the Kaggle solution involves using only a subset of the MNist images provided by the hosts. Possibly using it on the full MINST database could take it closer to the SOTA score. The real question though is - how does this compare with the accuracy of the best SOTA CNN architecture in the universe - the human mind. While there is no formal study of the same, 99.8% was the 'claimed' human benchmark and it seems to have been broken now
