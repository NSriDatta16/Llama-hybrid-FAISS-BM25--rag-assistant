[site]: crossvalidated
[post_id]: 470278
[parent_id]: 
[tags]: 
What does it mean to represent a word as an "indicator vector" in natural language processing?

I'm reading the paper A Neural Attention Model for Sentence Summarization (Rush et al., EMNLP 2015) and came across the following: Let the input consist of a sequence of $M$ words $\mathbf{x}_1, \dots , \mathbf{x}_M$ coming form a fixed vocabulary $\mathcal{V}$ of size $\vert \mathcal{V} \vert = V$ . We will represent each word as an indicator vector $\mathbf{x}_i \in \{0, 1\}^V$ for $i \in \{1, \dots, M\}$ , sentences as a sequence of indicators, and $\mathcal{X}$ as the set of possible inputs. What does it mean to represent words as "indicator vectors?" The Wikipedia page for indicator vectors says that an indicator vector is basically a vector for a subset $T$ of set $S$ that is $0$ or $1$ depending on whether that element in $T$ is also in $S$ . Does the paper mean that each word is a one hot vector for the vocabulary? Thanks.
