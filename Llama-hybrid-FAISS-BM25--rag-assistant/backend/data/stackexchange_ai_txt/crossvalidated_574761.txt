[site]: crossvalidated
[post_id]: 574761
[parent_id]: 
[tags]: 
What is this one-hot-like encoding of ordinal variables called and is it feasible?

Common understanding seems to be that when we're doing machine learning (classification or regression) using linear models, there are basically two ways to encode ordinal variables: Ordinal scale: 1, 2, 3 One-Hot: [1,0,0], [0,1,0], [0,0,1] I don't remember where I've learnt this, but this is how someone told me to do it: | | v_1 | v_2 | v_3 | v_4 | ------------------------------------- | very bad | 0 | 0 | 0 | 0 | | bad | 1 | 0 | 0 | 0 | | neutral | 1 | 1 | 0 | 0 | | good | 1 | 1 | 1 | 0 | | very good | 1 | 1 | 1 | 1 | This way we preserve the ordering of the values from "very bad" to "very good" (as long the weights of these dummy variables are non-negative), but we don't imply that the difference between "neutral" and "good" is the same as between "good" and "very good". I've found only one source in German for this. They are slides from a lecture that call it "Ordinale Entflechtung" (I have no idea on how to translate this, maybe "ordinal disentanglement"?), but there is not a single other source using this term. Is this a known concept? If yes, what is it called? And is it feasible or something one should avoid?
