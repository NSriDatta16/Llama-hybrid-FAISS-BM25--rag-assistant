[site]: datascience
[post_id]: 53120
[parent_id]: 
[tags]: 
An epoch step is taking too much time on GPU!

I am trying to train a CNN on phoneme recognition. My dataset is composed up of 50 Million sample, each sample has 120 feature (signal parameters). My input's shape is (11,120) and the target is a phoneme class. The CNN architecture is as follows: CONV (32 activation maps / ReLU activation) - POOL (max pooling) CONV (64 activation maps / ReLU activation) - POOL (max pooling) 3 DENSE layers (each contains 1024 neurons / ReLU activation function / dropout =0.5) The output is a softmax layer for 36 class. I used the f it_generator as I generate my data batch-by-batch. My batch size = 512. I tried to train the model on 10 Million of sample = almost 20K of batches however an epoch took me more than 3 hours. The server i'm using has 2 GPUs: Is that normal with such GPU characteristics? If yes, what can I do to accelerate my model training phase? Especially since I would like to train it on the whole dataset?
