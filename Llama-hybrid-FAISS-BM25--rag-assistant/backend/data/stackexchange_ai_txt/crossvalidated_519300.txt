[site]: crossvalidated
[post_id]: 519300
[parent_id]: 
[tags]: 
Model Explanation

I have a problem where I am trying to predict something from historic tabular data. We have features {X1,X2,..Xn} and I have a prediction {Y} I am trying to find a way to explain why the model came up with a prediction. In other words, which features were impacting the most that specific output. I know I can use algorithms such as Feature Importance or Elastic Net to know which features were more important to the model in general (X features), but I want to know given a single specific input what is more important. Example: I have a client that wants to predict the budget of certain work that they do. I input some variables they give me and my trained model (let's say a DT or Random Forest regressors) comes up with a prediction value. I know that for the model I have X features that are more important but I can't tell my client every time they ask me to come up with a budget that the budget is for the same reasons(X features) than all of the other works. I know I can plot the Decision Logic of a decision tree regressor with Graphviz for example, but that's okay only for me to understand. My client would want to see a more user friendly way of looking at the reasons behind that prediction. The final tool should be something like they being able to enter the features in fields, then the budget prediction would come up with lets say percentage circles of impact of their input in the prediction. Do you know about any ways this type of tool can be implemented at least for experiments? Do you know about any ways of displaying the specific decision path followed by a decision tree regressor of only one prediction?
