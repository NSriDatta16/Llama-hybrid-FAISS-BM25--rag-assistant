[site]: crossvalidated
[post_id]: 85465
[parent_id]: 
[tags]: 
Theoretically, why do we not need to compute a marginal distribution constant for finding a Bayesian posterior?

For most of my time in stats, I have been able to ignore the marginal distribution that is usually present at the denominator of any bayesian posterior distribution. For example, if we write down $L_x(\theta)\pi(\theta)$ and recognize that this function of $\theta$ looks like a distribution of $\theta$ but with an incorrect normalizing constant, I usually just mix and match till I get it since $$ \pi(x|\theta) \propto L_x(\theta)\pi(\theta) $$ HOWEVER, why can I do this, are there any cases where this breaks down?
