[site]: crossvalidated
[post_id]: 295645
[parent_id]: 
[tags]: 
In a real application, how to improve the CNN performance?

I'm doing two class classification. I have a special dataset with most negative cases (absence of an object I want to classify) and a few positive cases (presence of an object). Since I have only less than 100 positive cases for training and validation, I downloaded some images(around 450) from Flickr and used them for training. I separate my real dataset into two halves, one for training and one for validation. The models(Alexnet and VGG) I tried could only reach around 70% accuracy (probably due to the background cluster and occlusion). I have two obsevations. First, The object I try to classify only occupies a small portion of the image size. Second, since the images I used are of the size 2688*1512, I resized the images to 224*224 and 227*227 for VGG16 and Alexnet, respectively. Thus, I suspect that the resizing process filters some useful information out before training. In order to retain more information for training, I tried to reuse only the structure and the weights of VGG16â€™s convoultional layers and rebuilt the fully connected layers on top of it. So I could feed pixel size 1024*1024 to changed model. The performance, as shown here , is unconvergent. The second try is to crop the images first and then resize to 224*224 or 227*227. I'm thinking to feed more object information into models by cropping background out before resizing. So hopefully models could see more useful object information. Note I only manually crop my training dataset, not those downloaded from flickr. Unexpected, while feeding cropped images into models, the models simply couldn't learn anything. The accuracy after 100 epochs is still around 50%, random guess. I don't quite understand why the models fail when feeding cropped images. I'm thinking there are two potentials. One is that the blur (Because the captured object is moving) in the cropped images prevents the model learning. After cropping the background, more blurred pixels are fed into the models. So the models stop learning? The other one is that the inconsistency of the object size between training and validation. Because of cropping, the object size in the training is much bigger than that in the validation. I could try to adjust augmentation parameters. Any thoughts and suggestions? Thanks a lot.
