[site]: crossvalidated
[post_id]: 254658
[parent_id]: 23391
[tags]: 
Ryan Zotti's answer explains the motivation behind the maximization of the decision boundaries, carlosdc's answer gives some similarities and differences with respect to other classifiers. I'll give in this answer a brief mathematical overview of how SVMs are trained and used. Notations In the following, scalars are denoted with italic lowercases (e.g., $y,\, b$ ), vectors with bold lowercases (e.g., $\mathbf{w},\, \mathbf{x}$ ), and matrices with italic uppercases (e.g., $W$ ). $\mathbf{w^T}$ is the transpose of $\mathbf{w}$ , and $\|\mathbf{w}\| = \mathbf{w}^T\mathbf{w}$ . Let: $\mathbf{x}$ be a feature vector (i.e., the input of the SVM). $\mathbf{x} \in \mathbb{R}^n$ , where $n$ is the dimension of the feature vector. $y$ be the class (i.e., the output of the SVM). $y \in \{ -1,1\}$ , i.e. the classification task is binary. $\mathbf{w}$ and $b$ be the parameters of the SVM: we need to learn them using the training set. $(\mathbf{x}^{(i)}, y^{(i)})$ be the $i^ {\text {th}}$ sample in the dataset. Let's assume we have $N$ samples in the training set. With $n=2$ , one can represent the SVM's decision boundaries as follows: The class $y$ is determined as follows: $$ y^{(i)}=\left\{ \begin{array}{ll} -1 &\text{ if } \mathbf{w^T}\mathbf{x}^{(i)}+b \leq -1 \\ 1 &\text{ if } \mathbf{w^T}\mathbf{x}^{(i)}+b \ge 1 \\ \end{array} \right. $$ which can be more concisely written as $y^{(i)} (\mathbf{w^T}\mathbf{x}^{(i)}+b) \ge 1$ . Goal The SVM aims at satisfying two requirements: The SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to maximize the distance between the hyperplane defined by $\mathbf{w^T}\mathbf{x}+b = -1$ and the hyperplane defined by $\mathbf{w^T}\mathbf{x}+b = 1$ . This distance is equal to $\frac{2}{\|\mathbf{w}\|}$ . This means we want to solve $\underset{\mathbf{w}}{\operatorname{max}} \frac{2}{\|\mathbf{w}\|}$ . Equivalently we want $\underset{\mathbf{w}}{\operatorname{min}} \frac{\|\mathbf{w}\|}{2}$ . The SVM should also correctly classify all $\mathbf{x}^{(i)}$ , which means $y^{(i)} (\mathbf{w^T}\mathbf{x}^{(i)}+b) \ge 1, \forall i \in \{1,\dots,N\}$ Which leads us to the following quadratic optimization problem: $$\begin{align} \min_{\mathbf{w},b}\quad &\frac{\|\mathbf{w}\|}{2}, \\ s.t.\quad&y^{(i)} (\mathbf{w^T}\mathbf{x}^{(i)}+b) \ge 1 &\forall i \in \{1,\dots,N\} \end{align}$$ This is the hard-margin SVM , as this quadratic optimization problem admits a solution iff the data is linearly separable. One can relax the constraints by introducing so-called slack variables $\xi^{(i)}$ . Note that each sample of the training set has its own slack variable. This gives us the following quadratic optimization problem: $$\begin{align} \min_{\mathbf{w},b}\quad &\frac{\|\mathbf{w}\|}{2}+ C \sum_{i=1}^{N} \xi^{(i)}, \\ s.t.\quad&y^{(i)} (\mathbf{w^T}\mathbf{x}^{(i)}+b) \ge 1 - \xi^{(i)},&\forall i \in \{1,\dots,N\} \\ \quad&\xi^{(i)}\ge0, &\forall i \in \{1,\dots,N\} \end{align}$$ This is the soft-margin SVM . $C$ is a hyperparameter called penalty of the error term . ( What is the influence of C in SVMs with linear kernel? and Which search range for determining SVM optimal parameters? ). One can add even more flexibility by introducing a function $\phi$ that maps the original feature space to a higher dimensional feature space. This allows non-linear decision boundaries. The quadratic optimization problem becomes: $$\begin{align} \min_{\mathbf{w},b}\quad &\frac{\|\mathbf{w}\|}{2}+ C \sum_{i=1}^{N} \xi^{(i)}, \\ s.t.\quad&y^{(i)} (\mathbf{w^T}\phi \left(\mathbf{x}^{(i)}\right)+b) \ge 1 - \xi^{(i)},&\forall i \in \{1,\dots,N\} \\ \quad&\xi^{(i)}\ge0, &\forall i \in \{1,\dots,N\} \end{align}$$ Optimization The quadratic optimization problem can be transformed into another optimization problem named the Lagrangian dual problem (the previous problem is called the primal ): $$\begin{align} \max_{\mathbf{\alpha}} \quad &\min_{\mathbf{w},b} \frac{\|\mathbf{w}\|}{2}+ C \sum_{i=1}^{N} \alpha^{(i)} \left(1-\mathbf{w^T}\phi \left(\mathbf{x}^{(i)}\right)+b)\right), \\ s.t. \quad&0 \leq \alpha^{(i)} \leq C, &\forall i \in \{1,\dots,N\} \end{align}$$ This optimization problem can be simplified (by setting some gradients to $0$ ) to: $$\begin{align} \max_{\mathbf{\alpha}} \quad & \sum_{i=1}^{N} \alpha^{(i)} - \sum_{i=1}^{N}\sum_{j=1}^{N} \left( y^{(i)}\alpha^{(i)}\phi\left(\mathbf{x}^{(i)}\right)^T \phi\left(\mathbf{x}^{(j)}\right) y^{(j)}\alpha^{(j)} \right), \\ s.t. \quad&0 \leq \alpha^{(i)} \leq C, &\forall i \in \{1,\dots,N\} \end{align}$$ $\mathbf{w}$ doesn't appear as $\mathbf{w}=\sum_{i =1}^{N}\alpha^{(i)}y^{(i)}\phi\left(x^{(i)}\right)$ (as stated by the representer theorem ). We therefore learn the $\alpha^{(i)}$ using the $(\mathbf{x}^{(i)}, y^{(i)})$ of the training set. (FYI: Why bother with the dual problem when fitting SVM? short answer: faster computation + allows to use the kernel trick, though there exist some good methods to train SVM in the primal e.g. see {1}) Making a prediction Once the $\alpha^{(i)}$ are learned, one can predict the class of a new sample with the feature vector $\mathbf{x}^{\text {test}}$ as follows: \begin{align*} y^{\text {test}}&=\text {sign}\left(\mathbf{w^T}\phi\left(\mathbf{x}^{\text {test}}\right)+b\right) \\ &= \text {sign}\left(\sum_{i =1}^{N}\alpha^{(i)}y^{(i)}\phi\left(x^{(i)}\right)^T\phi\left(\mathbf{x}^{\text {test}}\right)+b \right) \end{align*} The summation $\sum_{i =1}^{N}$ could seem overwhelming, since it means one has to sum over all the training samples, but the vast majority of $\alpha^{(i)}$ are $0$ (see Why are the Lagrange multipliers sparse for SVMs? ) so in practice it isn't an issue. (note that one can construct special cases where all $\alpha^{{(i)}} > 0$ .) $\alpha^{{(i)}}=0$ iff $x^{{(i)}}$ is a support vector . The illustration above has 3 support vectors. Kernel trick One can observe that the optimization problem uses the $\phi\left(\mathbf{x}^{(i)}\right)$ only in the inner product $\phi\left(\mathbf{x}^{(i)}\right)^T \phi\left(\mathbf{x}^{(j)}\right)$ . The function that maps $\left(\mathbf{x}^{(i)},\mathbf{x}^{(j)}\right)$ to the inner product $\phi\left(\mathbf{x}^{(i)}\right)^T \phi\left(\mathbf{x}^{(j)}\right)$ is called a kernel , a.k.a. kernel function, often denoted by $k$ . One can choose $k$ so that the inner product is efficient to compute. This allows to use a potentially high feature space at a low computational cost. That is called the kernel trick . For a kernel function to be valid , i.e. usable with the kernel trick, it should satisfy two key properties . There exist many kernel functions to choose from . As a side note, the kernel trick may be applied to other machine learning models , in which case they are referred as kernelized . Going further Some interesting QAs on SVMs: Best way to perform multiclass SVM Support vector machines and regression Understanding the different formulations for SVM What's the difference between $\ell_1$ -SVM, $\ell_2$ -SVM and LS-SVM loss functions? To deal with an unbalanced dataset: Best way to handle unbalanced multiclass dataset with SVM A priori selection of SVM class weights How does one interpret SVM feature weights? Interpretating the C value in a linear SVM Generalization bounds on SVM General formula for the VC Dimension of a SVM What does the "machine" in "support vector machine" and "restricted Boltzmann machine" mean? How are SVMs = Template Matching? Single layer NeuralNetwork with ReLU activation equal to SVM? Comparing SVM and logistic regression Other links: Least squares support vector machine References: {1} Chapelle, Olivier. "Training a support vector machine in the primal." Neural computation 19, no. 5 (2007): 1155-1178. https://scholar.google.com/scholar?cluster=469291847682573606&hl=en&as_sdt=0,22 ; http://www.chapelle.cc/olivier/pub/neco07.pdf
