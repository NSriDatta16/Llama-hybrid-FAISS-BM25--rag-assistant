[site]: crossvalidated
[post_id]: 291311
[parent_id]: 290953
[tags]: 
Is the model not predicting unseen data because the model is overfit to the train data Possibly. But it may also just be a mix of the relative frequency of your classes is about 8 : 1 = 11 % of class Yes in the test data you show accidentally bad single split into train and test: class Yes overrepresented in the test set and underrepresented in the train/optimization sets (you don't say that you stratified the splitting) There is no indication how the working point (threshold for assigning class labels) was chosen. With so unequal relative frequencies and comparably low numbers of cases, you need to think about this. Also, the area under the ROC may not be the value you want to optimize - it may be better to optimize a figure of merit that is tailored to your application and that includes fixing the working point. If you want to optimze hyperparameters like mtry, go for a proper scoring rule. What is the point in k fold Cross Validation if overfitting is still an issue - i.e. why not just use the traditional method of hold one out with a train/test split? The advantages of cross validation over a single random split are that you can check stability and that it does make more efficient use of the (few) cases you have: precision of cross validation estimates is better as more cases are tested. cross-validation is not a magic wand against overfitting. If the overfitting is caused by clusters in the data (i.e. not all rows are statistically independent), then random splits will not be able to guard against such overfitting. Neither via cross validation nor via random splits into hold out sets. A good test set (e.g. produced by a separate experiment) will be able to spot this issue. Cross validation reduces overfitting compared to training-set performance estimates (i.e. pure goodness-of-fit), and it is somewhat better than single split hold out because of the reduced variance (which translates to overfitting bias after systematic optimization). BTW, neither is the random forest a magic wand: bagging helps only if the issue is instability of the model - if that isn't the issue, then bagging doesn't improve predictions. So do a sanity check: does the random forest predict mostly probabilities very close to 0 and 1 or do you get intermediate values as well?
