[site]: crossvalidated
[post_id]: 478972
[parent_id]: 478963
[tags]: 
Things like separate train/test sets that make a lot of sense in machine learning on large data sets don't always translate well to smaller-scale studies. Unless you have many thousands of cases a split into train and test cases runs a risk of both losing power in training and losing sensitivity in testing. You throw away information by making some cases unavailable for training. With relatively few cases for testing your estimates of performance will necessarily be of low precision and might be highly dependent on the vagaries of the single train/test split. For situations like your example with 150 cases it can make more sense to use the entire data set both for training and for evaluating the modeling performance, in a principled way that takes advantage of resampling. For example, simple 5-fold cross-validation is the equivalent of an 80/20 train/test split done 5 times; combining multiple runs with different random splits into 5 groups will improve performance. That's a good way to tune hyper-parameters for a model. In no one case are you testing on the training data, but the repetition and randomization takes advantage of all the data for both training and testing. You can check modeling reliability by repeating all the modeling steps on multiple bootstrap samples of the data then checking on the full data set. That follows the bootstrap principle : the resamples are to the original data sample as the original data sample is to the underlying population. So a modeling approach based on bootstrap samples that works well on your full data sample might be expected to work well when you use your full data sample to model the underlying population.
