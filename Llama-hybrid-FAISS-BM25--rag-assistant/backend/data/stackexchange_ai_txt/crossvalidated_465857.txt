[site]: crossvalidated
[post_id]: 465857
[parent_id]: 465849
[tags]: 
Random thoughts: In the following I'm assuming that your logistic regression provides as good enough fit that $\hat{p}$ from the logistic regression is a good approximation of the true success probability $p$ . (1) Definitely $Z = (Y-p)/\sqrt{np(1-p)}$ won't converge in distribution to a normal distribution, because $Y$ is a single, Bernoulli random variable and does not represent a sequence of random variables. Central Limit Theorem does not apply at all in this case. ( $Z$ has a shifted and scaled Bernoulli distribution with only two possible values, it's very much not normally distributed). (2) As you mentioned, if your data values $Y$ represent a collection of $n$ observations averaged together all with the same (under the logistic model) success probability $\hat{p}$ , then, yes, if $n$ is fairly large (usual rule of thumb is that $np > 7$ and $n(1-p) > 7$ , though this varies), $Y$ will be approximately normally distributed and $(Y-\hat{p})/\sqrt{n\hat{p}(1-\hat{p})}$ will be roughly $N(0,1)$ . However, this won't be very satisfying, because (i) you might not have any two observations with the same predicted success probability, so you would have to pool observations with similar success probabilities and (ii) this rather ruins the idea of looking for outliers, because pooling an outlier with a bunch of non-outliers will probably result in an innocent-looking residual. This works if the original experiment was based on groups with the same regressor values and you want do detect which groups are anomalous, but not helpful in determining which individual observations are anomalous. (3) You actually can use Pearson residuals such as $Z=(Y-\hat{p})/\sqrt{\hat{p}(1-\hat{p})}$ . They won't have a normal distribution, but you can still get a concept of outliers by using bounds based on the Chebyshef Inequality, so, for instance, $|Z| > 5$ would be flagged as an outlier (not adjusting for multiple comparisons).
