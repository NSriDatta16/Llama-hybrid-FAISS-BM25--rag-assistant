[site]: crossvalidated
[post_id]: 389668
[parent_id]: 389545
[tags]: 
I'll focus on the question of statistical significance since Dason already covered the modeling part. I am unfamiliar with any formal tests for this (which I am sure exist), so I'll just throw some ideas out there (and I'll probably add R code and technical details later). First, it is convenient to infer the classes. Presuming you have two lines fit to the data, you can approximately reconstruct the two classes by assigning each point to the class of the line closest to it. For points near the intersection, you will run into issues, but for now just ignore those (there may be a way to get around this, but for now just hope that this won't change much). The way to do this is to choose $x_{l}$ and $x_{r}$ (soil pH values) with $x_{l} \leq x_{r}$ such that the parts left of $x_{l}$ are sufficiently separated and the parts right of $x_{r}$ are sufficiently separated (the closest point where the distributions don't overlap). Then there are two natural ways I see to go about doing this. The less fun way is to just run your original dataset combined with the inferred class labels through a linear regression as in Demetri's answer. A more interesting way to do so would be through a modified version of ANOVA. The point is to create an artificial dataset that represents the two lines (with similar spread between them) and then apply ANOVA. Technically, you need to do this once for the left side, and once for the right (i.e. you'll have two artificial datasets). We start with the left, and apply a simple averaging approach to get two groups. Basically, each point in say the first class is of the form $$ y^{(i)}_{1} = \beta_{1,1} x_{1}^{(i)} + \beta_{1,0} + e_{1}^{(i)}$$ so we are going to replace the linear expression $\beta_{1,1} x_{1}^{(i)} + \beta_{1,0}$ by a constant, namely the average value of the linear term or $$ \beta_{1,1} x^{\mathrm{avg}} + \beta_{1, 0}$$ where $x^{\mathrm{avg}}_{l}$ is literally the average $x$ value for the left side (importantly, this is over both classes, since that makes things more consistent). That is, we replace $y_{1}^{(i)}$ with $$ \tilde{y}_{1}^{(i)} = \beta_{1,1} x^{\mathrm{avg}} + \beta_{1, 0} + e_{1}^{(i)},$$ and we similarly for the second class. That is, your new dataset consists of the collection of $\tilde{y}_{1}^{(i)}$ and similarly $\tilde{y}_{2}^{(i)}$ . Note that both approaches naturally generalize to $N$ classes.
