[site]: crossvalidated
[post_id]: 481734
[parent_id]: 481275
[tags]: 
Interesting question, which I think depends on the application in question and the modeling approach used. On one hand Spam Filters do a pretty good job of handling concept drift. Indeed would probably stop working all together if they weren't able to handle concept drift in the first place. On the other hand, more recent NLP models like ELMO and BERT assume by definition that a given is language is stable, and hence concept drift is minimal, since they assume that embedding layers can be used and reused over long periods of time. As to how to detect concept drift? As mentioned above, Spam filters have their own approaches . Presumably, this applies to some other areas of NLP as well. Another answer is to look at the general field of ML Ops , which among other things, aims at dealing with concept drift in production for ML in general, and can be applied to text data based apps. The genera philosophy of MLOps is to continuously monitor the statistical distributions of both the input features and the output classes in production. E.g. if you are dealing with binary classification problem and your rate of true positives went from being 10% to 50%, then you should bring in a data scientist to look into the root causes and also retrain and recalibrate your models accordingly.
