[site]: datascience
[post_id]: 45053
[parent_id]: 43972
[tags]: 
StandardScaler and MinMaxScaler are more common when dealing with continuous numerical data. One possible preprocessing approach for OneHotEncoding scaling is "soft-binarizing" the dummy variables by converting softb(0) = 0.1, softb(1) = 0.9 . From my experience with feedforward Neural Networks this was found to be quite useful, so I expect it to be also benefitial for your MLPClassifier. StandardScaler is useful for the features that follow a Normal distribution . This is clearly illustrated in the image below ( source ). MinMaxScaler may be used when the upper and lower boundaries are well known from domain knowledge (e.g. pixel intensities that go from 0 to 255 in the RGB color range).
