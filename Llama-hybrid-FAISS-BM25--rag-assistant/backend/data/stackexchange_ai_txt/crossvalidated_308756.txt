[site]: crossvalidated
[post_id]: 308756
[parent_id]: 92383
[tags]: 
The original paper describing this can be found here In section 4.4, they discuss the ways in which the algorithm can be implemented. The best implementation that they discovered initially was to not reset any Markov Chains, to do one full Gibbs update on each Markov Chain for each gradient estimate, and to use a number of Markov Chains equal to the number of training data points in a mini-batch. Section 3 might give you some intuition about the key idea behind PCD.
