[site]: crossvalidated
[post_id]: 329553
[parent_id]: 74082
[tags]: 
The Bayesian estimate is Bayesian inference while the MLE is a type of frequentist inference method. According to the Bayesian inference, $f(x_1,...,x_n; \theta) = \frac{f(\theta; x_1,...,x_n) * f(x_1,...,x_n)}{f(\theta)}$ holds, that is $likelihood = \frac{posterior * evidence}{prior}$ . Notice that the maximum likelihood estimate treats the ratio of evidence to prior as a constant(setting the prior distribution as uniform distribution/diffuse prior/uninformative prior, $p(\theta) = 1/6$ in playing a dice for instance), which omits the prior beliefs, thus MLE is considered to be a frequentist technique(rather than Bayesian). And the prior can be not the same in this scenario, because if the size of the sample is large enough MLE amounts to MAP(for detailed deduction please refer to this answer ). MLE's alternative in Bayesian inference is called maximum a posteriori estimation(MAP for short), and actually MLE is a special case of MAP where the prior is uniform, as we see above and as stated in Wikipedia : From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. For details please refer to this awesome article: MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation . And one more difference is that maximum likelihood is overfitting-prone, but if you adopt the Bayesian approach the over-fitting problem can be avoided.
