[site]: crossvalidated
[post_id]: 113008
[parent_id]: 21623
[tags]: 
I think there are two problems here: The huge state space, The fact that many agents are involved. I have no experience with (2), but I guess if all the agents can share their knowledge (e.g. their observations) then this is no different than treating all different agents as a single agent, and learn sth like a "swarm policy". If this is not the case, you might need to search for "distributed reinforcement learning" or "multi agent reinforcement learning". For (1), you might need to find a representation of the action/state space which is more compact. Some ideas follow. You say that there are 1000 locations. Does it make sense to try to find a low dimensional embedding for them? E.g. are you able to find a suitable distance measure between them? If so, you can use multidimension scaling to embed them in a continuous, k-dimensional space with $k Another approach would be to use policy gradients . The idea is that you use a parametrized policy, $$ \pi: \Theta \times S \mapsto A $$ where each $\theta \in \Theta$ is a point in parameter space which defines the policy. This policy can then be optimized with gradient-based methods. An example would be that you have a neural network which takes the current state as an input, and directly puts out "move object i to location j". You will not need to enumerate all possible actions explicitly. Nevertheless, I doubt this approach will work without serious work. Even when using PGs, you will need to reduce your action/state space.
