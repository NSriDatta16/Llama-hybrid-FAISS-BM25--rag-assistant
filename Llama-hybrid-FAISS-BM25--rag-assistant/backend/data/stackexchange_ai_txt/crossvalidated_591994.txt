[site]: crossvalidated
[post_id]: 591994
[parent_id]: 591970
[tags]: 
If you want to discover associations between many variables and a binary outcome, you can fit tens or hundreds or thousands of bivariate logistic regression models, but for any given model's p-value to mean anything, you have to control the familywise error rate. It wouldn't make sense in this case to include all the variables in a single model. This is the only setting you described in which a p-value is being used for its true purpose: as a measure of significance with a well defined null-hypothesis. Alternately, if the goal were to predict the group membership, the goal is now prediction rather than inference. You could throw all the variables into a single model. In that case, we wouldn't care what the p-value was for any variable because the p-value is an imprecise measure of predictiveness. You could enforce parsimony by performing backward (or forward) stepwise model selection, thereby deleting non-significant variables. This model will have lower internal validity, but hopefully a higher external validity. But again, the p-value is only incidentally being used to encourage generalizability. You may just as well do the same thing using penalized regression. In general, feature selection is not a shortcut to statistical inference. A variable may be predictive but not statistically significant, or vice versa. A p-value becomes increasingly hard to conceptualize when the space of possible hypotheses grows at $2^K$ where $K$ is the number of features considered in an analysis.
