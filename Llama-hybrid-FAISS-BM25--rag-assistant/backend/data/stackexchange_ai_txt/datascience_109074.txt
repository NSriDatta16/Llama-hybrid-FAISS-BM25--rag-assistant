[site]: datascience
[post_id]: 109074
[parent_id]: 109065
[tags]: 
It really depends on the model/library you want to use. I will make you two examples here. sklearn In sklearn you can train your model by applying a transformation converting your text data into numbers ( e.g. tfidf) and then using a classifier ( e.g. random forest). from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.ensemble import RandomForestClassifier # note data are a list of strings data = ["Hi there", "i'm eating breakfast u?", "okay talk to you later"] labels = [1, 0, 1] pipeline = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', RandomForestClassifier()), ]) # train classifier pipeline.fit(data, labels) # evaluate all steps on new data predicted = pipeline.predict('Hello predict this') tensorflow Analogously in tf, you have plenty of possibilities (even pass directly strings to the model and then use a conversion layer). I strongly advise having a look at the documentation or at tutorials available . Here I will show you one with an LSTM-based neural network classifier. import tensorflow as tf max_len = 6 # all the sentence must have the same lenght data = np.array(["Hi there", "i'm eating breakfast u?", "okay talk to you later"]) labels = np.array([1, 0, 1]]) tokeniser = tf.keras.preprocessing.text.Tokenizer() tokeniser.fit_on_texts(data) tokenised_text = tokeniser.texts_to_sequences(data) tokenised_text = tf.keras.prepocessing.sequences.pad_sequences(tokenised_text, maxlen=max_len) vocab_size = len(tokeniser.word_index) + 1 # This is a model I am inventing right now, you have to change it according to your problem def build_model(emb_dim, dropout_rate, n_units, n_labels = len(labels)): inputs = tf.keras.Input(input_shape = (max_len,)) x = tf.keras.layers.Embedding(vocab_size, output_dim = emb_dim)(inputs) x = tf.keras.layers.LSTM(n_units)(x) x = tf.keras.layers.Dense(n_labels, 'softmax')(x) model = tf.keras.Model(inputs = inputs, outputs = x) model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model then, you can fit your model on tokenised sequences model = build_model(emb_dim, dropout_rate, n_units) model.fit(tokenised_text, labels, epochs = 10) In tf you can really give several different kinds of inputs, but you have to build your model accordingly.
