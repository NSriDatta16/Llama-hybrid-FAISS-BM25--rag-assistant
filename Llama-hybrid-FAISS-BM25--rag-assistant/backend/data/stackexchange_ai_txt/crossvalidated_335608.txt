[site]: crossvalidated
[post_id]: 335608
[parent_id]: 335594
[tags]: 
The most important task for my machine learning model, is to optimize the sensitivity (I want to "catch" all the "Yes" people, the 3%). Taking this sentence literally, the baseline (guess "Yes" for everybody) is the best possible method - this will get 100% sensitivity and there is no way to improve. Obviously you want to also get good specificity without compromising sensitivity but how much of a compromise is still worth it? (e.g. will you be willing to reduce sensitivity to 95% to get 100% specificity?) There is no single good answer, it really depends on your case and sensitivity alone is impossible to interpret. Your question IMHO illustrates a wider problem with thinking in terms of sensitivity and specificity. I would suggest that you define a cost function - what is the cost/utility of true positives, true negatives, false positives and false negatives (those all can have dramatically different costs!) and then try to find a classifier that minimizes expected cost (maximizes expected utility). You can then compare which of the baseline classifiers (in your case just giving the same answer for all inputs) has lower expected cost and use this one. Frank Harrell has some more thoughts on this topic: http://www.fharrell.com/post/classification/
