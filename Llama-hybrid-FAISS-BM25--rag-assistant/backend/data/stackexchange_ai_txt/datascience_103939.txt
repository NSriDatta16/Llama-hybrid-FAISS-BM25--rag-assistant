[site]: datascience
[post_id]: 103939
[parent_id]: 103924
[tags]: 
Background: The concept of Cross Entropy is inherited from Information theory where it is applied to understand and measure the difference in the distributions of two or more events. Events as you would appreciate are a discrete concept and translate to classes in the case of a ML classification problems. This is the reason that Cross Entropy is only applicable to Bernoulli/Multinoulli (categorical distributions). Regarding your question: It is not clear why you mention Logistic regression and raise a question on the applicability of Cross Entropy (aka LogLoss in case of Logistic regression) to regression problems (the name may have confused you?). Since, Logistic regression is a classification model, all seems to fit well in place. EDIT 1: If you take a normal distribution (hence, continuous) and discretize it using bins, you convert it into a multinoulli distribution where the area under the curve of individual bins acts as pi of the events/classes. Now you can easily calculate cross entropy for this transformed distribution, however, it is no more a normal distribution.
