[site]: datascience
[post_id]: 63940
[parent_id]: 63817
[tags]: 
While the bagging of random forests is meant to reduce overfitting, they generally will overfit more than e.g. a parametric model like logistic regression. Having a larger gap between training and testing scores is not necessarily a problem; you may still prefer the model if its testing score is higher than other models'. In a purely statistical perfect world, the test score is all you should care about. In real problems, there are some other things to consider: a more-overfit tree model is likely less interpretable, potentially takes longer to train and make predictions, may be more susceptible to concept drift, etc. You should, as you have, see if reducing the model's capacity can increase your testing score (and maybe see if doing so can reduce the train-test gap even if it costs a little on test score, see above). With min.node.size , I would suggest using a fraction of the training set size rather than the small fixed integers that are default in ranger . Note that ranger does also have a max.depth parameter if you'd prefer to fix that directly. Reducing mtry is also a good approach, but 1 strikes me personally as too random, and I'd worry that you'd need a ton of trees to stabilize results; using mtry in combination with a tree complexity parameter hopefully produces a higher test score without too outrageous of a train-test gap.
