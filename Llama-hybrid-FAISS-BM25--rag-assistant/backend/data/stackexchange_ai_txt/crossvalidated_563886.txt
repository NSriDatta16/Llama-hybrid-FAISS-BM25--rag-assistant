[site]: crossvalidated
[post_id]: 563886
[parent_id]: 563798
[tags]: 
If I could assume that OP is a physicist, then the most straightforward and even obvious explanation of the energy function (including the minus sign) would have been to reference an Ising chain for a ferromagnetic and its Hamiltonian . Then the answer could have been a one liner, perhaps. That was my first attempt at answering, after which I realized that assuming everyone studied theoretical physics before learning about RBM is simply ridiculous. Thus my seconds attempt. It will not be succinct, because I am not assuming any statistical mechanics background this time. Let's start with a simple two node network, and denote the nodes $v$ and $h$ , each holding a single bit. There's one connection with a weight $w$ and two biases $a,b$ for nodes $v,h$ . Let's first see how the energy looks like before interpreting it: $E(v,h)=-(av+bh+vwh)$ . Let's plug this energy into Boltzmann distribution probabilities $p(v,h)\propto e^{-E(v,h)}$ , noting that this system has exactly four states: $(v,h):[(0,0),(0,1),(1,0),(1,1)]$ . We want to see what is conditional probability $p(h|v)$ , i.e. given node $v$ value what is a probability of a node $h$ value. First, we get the scaling constant (partition in mechanics): $$Q=e^{0}+e^{b}+e^{a}+e^{a+b+w}=1+e^{b}+e^{a}+e^{a+b+w}$$ then the conditional probability $$p(h=1|v)=\frac{p(v,h=1)}{p(v,h=0)+p(v,h=1)}$$ $$p(h=1|v)=\frac{e^{av+b+vw}}{e^{av}+e^{av+b+vw}}=\frac{e^{b+vw}}{1+e^{b+vw}}$$ Wow! Can this be an accident? Of course, no, because clearly the energy was constructed so that we ended up with a softmax function for the probabilities. You can workout the equation to show that this works exactly the same for $v$ node given the values of $h$ node: $$p(v=1|h)=\frac{e^{a+wh}}{1+e^{a+wh}}$$ So, what is then this energy thing $E(v,h)=-(av+bh+vwh)$ ? Suppose we are looking for some hidden meaning $h$ of visible data $v$ . In our silly single bit network, the meaning of two possible states 0 and 1 of $v$ can be only $h=v$ . In this case, whenever $w the energy increases when both bits are set: $E(1,1)=-a-b-w$ . This formulation of energy will lead to learning that the weights are positive. Biases here are not important: as usual, they catch the intercepts so things that should add up actually do. Once we learned that the parameters of RBM, i.e. $w,a,b$ , we can calculate energy for any combination of visible and hidden nodes, e.g. $$E(0,1)=-b\\E(1,1)=-a-b-w\\a>0\Rightarrow E(0,1)>E(1,1)$$ . In Physics the systems always try to achieve the lowest energy possible. Boltzmann distribution allows to calculate the probabilities of states with different energies. When something prevents the system from staying in its lowest possible energy state, this distribution explains the probability distribution of observed states. For instance, in Ising spin chain (by which RBM and BM are clearly inspired) the system often is trapped in suboptimal local optimum and can't escape it because it would first need to increase the system energy - and that is lower probability - to get out of the hole. So, when you heat it up, the probabilities start to change (perhaps, the difference between different energy state probabilities narrow), and the system is able to escape the local minimum. This explains what happens when you heat up the magnet, then cool it down. Hence, somewhat disappointingly, the energy function in RBM can be thought of as a deliberate construct that was inspired by statistical mechanics and its purpose was to drag the math apparatus to support Boltzmann distribution application to some learning problems. Once you map a key concept to something used in physics, it gives an idea of how and what else can be done.
