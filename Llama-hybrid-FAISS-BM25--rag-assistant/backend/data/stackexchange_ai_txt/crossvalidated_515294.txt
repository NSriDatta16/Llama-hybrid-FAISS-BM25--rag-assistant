[site]: crossvalidated
[post_id]: 515294
[parent_id]: 514522
[tags]: 
The statement the authors intended is (A) for the following reasons. Addressing the confusion. Much of your reluctance to consider $(A)$ as the correct statement amounted to insufficient attentiveness concerning the following basic distinction between: $$\mathbb{P} \left( \sup_{f \in \mathcal{F}} \lvert \hat{R}_n(f) - R(f) \rvert \geq \epsilon \right) = \mathbb{P} \left( \bigcup_{f \in \mathcal{F}} \{\lvert \hat{R}_n(f) - R(f) \rvert \geq \epsilon \} \right)$$ and $$\begin{align} \mathbb{P} \left( \sup_{f \in \mathcal{F}} \lvert \hat{R}_n(f) - R(f) \rvert \leq \epsilon \right) &= \mathbb{P} \left( \bigcap_{f \in \mathcal{F}} \{ \lvert \hat{R}_n(f) - R(f) \rvert \leq \epsilon \} \right) \\ &= \mathbb{P} \left( \forall f \in \mathcal{F}, \space \lvert \hat{R}_n(f) - R(f) \rvert \leq \epsilon \right) \end{align}$$ Consider the events within each statement. In the first statement, the event that the "maximum absolute deviation between the empirical risk and the risk over the function class $\mathcal{F}$ exceeds $\epsilon$ " is the same as the compound event that " there is at least one function $f$ whose absolute deviation between the empirical risk and risk exceeds $\epsilon$ ". In the second statement, the event that the "maximum absolute deviation between the empirical risk and the risk over the function class $\mathcal{F}$ does not exceed $\epsilon$ " is the same as the compound event that the "absolute deviation between the empirical risk and the risk does not exceed $\epsilon$ for all functions $f \in \mathcal{F}$ . Difference between statements $(A)$ and $(B)$ . The reason why the statement the authors intended is not statement $(B)$ goes back to the argument about the insufficiency of concentration inequalities for understanding generalisation error, and why we are instead interested in bounding uniform deviations. Statement $(B)$ is of the same flavour as a concentration inequality applied to every $f \in \mathcal{F}$ , except it is much looser than the one derived using Hoeffding in step 1, due to the inclusion of the $\ln \lvert \mathcal{F} \rvert$ term: $$ \space \mathbb{P} \left( \lvert \hat{R}_n(f) - R(f) \rvert \leq \sqrt{\frac{\ln \lvert \mathcal{F} \rvert + \ln (2 / \delta)}{2n}} \right) \geq 1 - \delta, \quad \forall f \in \mathcal{F}$$ Setting the looseness of the bound aside, this is qualitatively distinct from statement $(A)$ . Statement $(B)$ is saying that for every single function $f \in \mathcal{F}$ , there exists a set of training samples associated with that function $f$ , which we define as $S_f$ , such that $\hat{R}_n(f) - R(f) \leq \sqrt{\frac{\ln \lvert \mathcal{F} \rvert + \ln (2 / \delta)}{2n}}$ , and that the probability of encountering this sample exceeds $1 - \delta$ , that is, $P(S_f) \geq 1 - \delta$ . Even though for all $f \in \mathcal{F}$ , the probability $P(S_f) \geq 1 -\delta$ , this does not change the fact that in general, each training sample $S_f$ for which $\hat{R}_n(f) - R(f) \leq \sqrt{\frac{\ln \lvert \mathcal{F} \rvert + \ln (2 / \delta)}{2n}}$ holds is different for different functions $f \in \mathcal{F}$ . Hence what is required is something stronger than this. What instead is required is the probability of a set of training samples $S$ for which every absolute deviation $\lvert \hat{R}_n(f) - R(f) \rvert \leq \sqrt{\frac{\ln \lvert \mathcal{F} \rvert + \ln (2 / \delta)}{2n}}$ . Only statement $(A)$ captures this sense. Key take away: The position of the qualifier $\forall f \in \mathcal{F}$ is of the utmost importance. Illustrations. To see this graphically, statement $(B)$ is a statement concerning a pointwise deviation for every function $f \in \mathcal{F}$ . That is, it only bounds the vertical distances in the graph below as individual events. Statement $(A)$ is a uniform bound across the entire function class. That is, it bounds the probability of the compound event that the entire blue curve lies within the "white river", and does not cross over onto the "grey river bank". Other sources. Statements equivalent to $(A)$ , though not written explicitly as probability statements, and with some mild idiosyncratic differences in notation, can be found in: Anthony, M., & Bartlett, P. (1999). Neural Network Learning: Theoretical Foundations. Cambridge: Cambridge University Press. p22 doi:10.1017/CBO9780511624216 Bousquet O., Boucheron S., Lugosi G. (2004) Introduction to Statistical Learning Theory. In: Bousquet O., von Luxburg U., RÃ¤tsch G. (eds) Advanced Lectures on Machine Learning. ML 2003. Lecture Notes in Computer Science, vol 3176. Springer, Berlin, Heidelberg. p179 https://doi.org/10.1007/978-3-540-28650-9_8
