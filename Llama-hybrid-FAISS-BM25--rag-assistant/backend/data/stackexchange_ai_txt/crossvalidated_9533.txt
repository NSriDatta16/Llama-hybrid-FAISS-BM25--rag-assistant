[site]: crossvalidated
[post_id]: 9533
[parent_id]: 
[tags]: 
Learning a univariate transform (kernel?) for novelty detection

I have 150 observations, 500 features, and I am interested in novelty detection (outlier detection): given a new observation (let's say 'patient') I want to know if it is different from the previous ones (let's call it 'control'). If I had a lot of data, I would probably be using statistical testing at the univariate parameter level, but, because of multiple testing issues, I end up exploring the tails of the control distribution to achieve significance, and I do not have enough data for non parametric tests for such small p-values. I am doing one class SVMs that alleviate this issue by learning a global decision strategy. The limitations of this approach are it is very 'blackboxy' it works poorly if the data is very 'anisotropic', i.e. the marginal distributions of the control are very dissimilar in different directions. A trick to work around problem 2 is to center an norm the univariate parameters (this is often called creating 'Z scores'). Ideally, one would like to whiten the data using the control covariance, but there is not enough data to compute it. The values fed in the OC-SVM then can be interpreted as a univariate test statistic (under a normal null distribution for the controls). In my case, I can see from the histograms that the control's distribution is heavy tailed. I would like to learn a univariate transform making it closer to a standard normal. By the way, I have no reference about such practices. I have learned them empirically, and from lab discussions. Any pointer would be welcome, even if they don't directly answer my question.
