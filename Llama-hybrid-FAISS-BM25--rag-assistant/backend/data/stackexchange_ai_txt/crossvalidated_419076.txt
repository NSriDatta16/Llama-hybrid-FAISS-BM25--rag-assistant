[site]: crossvalidated
[post_id]: 419076
[parent_id]: 
[tags]: 
Active data acquisition with small datasets / Active Design of Experiment

Validated, I am facing a problem at work and I hope to find some light here, I have long worked with numerical methods, simulations and data analysis, but only recently I started studying and working with learning methods, including machine learning. Right now, I am trying to find the "smartest", and therefore cheapest, way to sequentially evaluate a data-set in which each evaluation require an expensive computer simulation to be executed. Hopefully, this would allow me not to evaluate every single point of the data-set before getting a reasonable approximation for the output of all the query points. The literature I've found till now mentioned topics such as active data-acquisition, active learning and adaptive design of experiment [1,2,3]. Each data-point is characterized by a feature vector $\mathbb{x} = ({\mathbb x}_1,{\mathbb x}_2)$ , and the evaluation returns an output vector ${\mathbb y} = f({\mathbb x}_1,{\mathbb x}_2)$ . The data-set of points do be queried is of the form $\mathcal{D} = \bigcup_{k=0}^{N_1-1} \{X_1^k\} \times \{X_2^{k,0}, X_2^{k,1},X_2^{k,2},...,X_2^{k,N_2}\}$ , being "line" samples, sometimes but not always it is possible to split the data-set as a Cartesian product $\mathbb{X}_1 \times \mathbb{X_2}$ . The total number of points on set $\mathcal{D}$ range from a few hundred up to a few thousand, and the dimensionality of the feature vector is around 10-20. If a given number $n$ of samples is already it would be possible to interpolate the remainder values of $\mathbb y$ via a regression method (e.g. Gaussian Processes Regression comes to mind). In this case it would be possible to compute a posteriori distribution on the un-queried points, including the expected value and the uncertainty over it. In principle, it should be possible to run an optimization algorithm [4] (maybe even bayesian optimization) that would yield the point which would maximize the "information yield" from the next point to be queried among the available options, but I am not yet sure how to properly formulate this problem. The closest thing I found were papers describing sensor optimal sensor placement [5]. Other papers focused a lot more on either problems with a lot of points, which is not my case, or problems in which I would choose a point inside a continuum, the freedom for which I don't have. I haven't followed through with this idea yet, but my intuition is that I should simply run GPs over then not yet queried points and, since there is only a finite and small number of them, choose the point with the biggest variance and evaluate it, even though it may be overly simplistic. My problem is that I have not found any literature to that exactly fits my problem, and therefore I can't properly back my intuition at the moment. I believe I have missed something, and this is why I am coming here to ask for help. Is my reasoning sound, or not, and is there any alternative ways to approach the problem I am describing here? Thanks in advance for all that are reading. Some papers I've found till the present moment: [1] A Tutorial on Adaptive Design Optimization - Myung, Cavagnaro and Pitt [2] Adaptive Design Optimization - A Mutual Information-Based Approach to Model Discrimination in Cognitive Science - Cavagnaro, Myung, Pitt and Kujala [3] Data acquisition and cost-effective predictive modeling - targeting offers for electronic commerce - Provost, Melville and Saar-Tsechansky [4] Information-Based Objective functions for Active Data Selection - David J C MacKay [5] Near-optimal sensor placements - maximizing information while minimizing communication cost - Krause
