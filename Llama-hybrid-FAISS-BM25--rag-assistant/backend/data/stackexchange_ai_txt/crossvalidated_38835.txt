[site]: crossvalidated
[post_id]: 38835
[parent_id]: 
[tags]: 
Clear description of PCA using SVD of covariance matrix

After reading thousands of articles on PCA and SVD, using them in a number of programming frameworks and even implementing similar techniques (like Random Indexing) I found out that I still have doubts about some parts of PCA for dimension reduction. So let me show what I know and what I have doubts about. Let's say we have $N$ observations of $M$-dimensional data, organized as matrix $A \in R^{N * M}$. To perform PCA we should first compute MLE estimate of covariance matrix $\Sigma \in R^{M*M}$: $$\Sigma=\frac{1}{N}\sum_{i=1}^N(x_i - \bar x)(x_i - \bar x)^T$$ where $x_i \in R^M$ is $i$-th observation, $\bar x = \frac{1}{N}\sum_{k=1}^{N}x_k \in R^M$ is mean observation. Then we can decompose $\Sigma$ using SVD as follows: $$\Sigma = USV^T$$ Now here are several things I'm not sure about: What are dimensions of $U$, $S$ and $V^T$? In $USV^T$ what exactly is considered as eigenvalues and which of them should I use as principal components ? How can I project original observations $x_i$ onto new reduced space and vice versa? UPD. There's a different way to compute PCA using SVD - by factorizing data matrix ($A$ here) instead of covariance matrix ($\Sigma = AA^T$). Good description for this process may be found in this answer.
