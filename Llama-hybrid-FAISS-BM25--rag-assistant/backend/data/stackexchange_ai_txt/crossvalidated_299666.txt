[site]: crossvalidated
[post_id]: 299666
[parent_id]: 
[tags]: 
Consistency of ReLU gradient

In Goodfellow et al.'s Deep Learning , the authors write, "rectified linear units use the activation function $g(z) = \max\{0,z\}$... The gradients are not only large but consistent" (187). What does it mean for a gradient to be consistent? I always understood consistency to mean that for an estimator of a parameter, the estimator converges in probability to the true parameter value as the number of data points goes to infinity. How does this definition relate to the gradient? Does this mean that if you use SGD, the stochastic gradient converges to the true gradient as the number of data points goes to infinity, and if so, isn't this a general property of SGD rather than of ReLU?
