[site]: stackoverflow
[post_id]: 4725178
[parent_id]: 4724387
[tags]: 
Basically how much cycle CPU spent. I assume this will be always a same for the same piece of code in the same computer and not effected by other processes. There might be some fundamental stuff I'm missing in here, if so please enlighten me in the comments or answers. CPU time used by a function is a really squishy concept. Does it include I/O performed anywhere beneath it in the call tree? Is it only "self time" or inclusive of callees? (In serious code, self time is usually about zero.) Is it averaged over all invocations? Define "all". Who consumes this information, for what purpose? To find so-called "bottlenecks"? Or just some kind of regression-tracking purpose? If the purpose is not just measurement, but to find code worth optimizing, I think a more useful concept is Percent Of Time On Stack . An easy way to collect that information is to read the function call stack at random wall-clock times (during the interval you care about). This has the properties: It tells inclusive time percent. It gives line-level (not just function-level) percent, so it pinpoints costly lines of code, whether or not they are function calls. It includes I/O as well as CPU time. (In serious software it is not wise to exclude I/O time, because you really don't know what's spending time several layers below in the call tree.) It is relatively insensitive to competition for the CPU by other processes, or to CPU speed. It does not require a high sample rate or a large number of samples to find costly code. ( This is a common misconception. ) Each additional digit of measurement precision requires roughly 100 times more samples, but that does not locate the costly code any more precisely. A profiler that works on this principle is Zoom . On the other hand, if the goal is simply to measure, so the user can see if changes have helped or hurt performance, then the CPU environment needs to be controlled, and simple overall time measurement is what I'd recommend.
