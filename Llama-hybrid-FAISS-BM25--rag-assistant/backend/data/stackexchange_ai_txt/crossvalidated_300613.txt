[site]: crossvalidated
[post_id]: 300613
[parent_id]: 300543
[tags]: 
In standard logistic regression we have 1 output in the final layer. However with a single hidden layer neural network, we can have multiple intermediate values each of which can be thought of as an output of a different logistic regression model i.e. we are not just performing the same logistic regression again and again. It is then not a large jump to think that it is possible that the combination of these has greater expressive capabilities than the standard logistic regression model (and also has been shown in practice and theory ). You also mention in the comments about how these nodes have different values in the same layer if they have the same inputs? This is because they should have different weights. Each node in a neural network takes $N$ inputs and produces a value $\displaystyle y_j = f\left(\sum_{i = 1}^N w_{ji} \cdot x_i + b_j\right)$ where $f$ is some chosen function, in our case the sigmoid, $w_{ji}$ are the weights, $x_i$ are the inputs, and $b_j$ is some bias. The weights are chosen by an optimisation algorithm to optimise our objective e.g. minimise classification error. Initialisation is very important for the gradient descent algorithms that are usually used to optimise the weights. See https://intoli.com/blog/neural-network-initialization/ where if all the weights start off at 0, the network is unable to learn.
