[site]: crossvalidated
[post_id]: 64905
[parent_id]: 64901
[tags]: 
The simplest thing to do would probably be a sign test. The null hypothesis is that each result has equal probability of being positive or negative (like flipping a fair coin). Your goal is to determine whether the observed results would be unlikely enough under this null hypothesis that you can reject it. What's the probability of getting 80 or more heads out of 100 flips of a fair coin? You can calculate this using the binomial distribution. In R , the relevant function is called pbinom , and you could get a (one-sided) p-value using the following line of code: pbinom(80, size = 100, prob = 0.5, lower.tail = FALSE) According to this test, your intuition is correct, you'd be exceedingly unlikely to get 80 positive results by chance if the treatment had no effect. A closely related option would be to use something like the Wilcoxon signed rank test . A better approach, if you actually want to estimate the size of the effect (rather than just determine whether it tends to be greater than zero or not), would probably be a hierarchical ("mixed") model. Here, the model says that your 100 individuals' results come from a distribution, and your goal is to see where the mean of that distribution is (along with confidence intervals). Mixed models let you say quite a bit more about your effect sizes: after fitting the model, you could say something like "we estimate that our treatment tends to improve outcomes by an average of three units, although the data is consistent with the true average effect size being anywhere from 1.5 to 4.5 units. Also, there's some variation among individuals, so a given person might see an effect anywhere from -0.5 to +6.5 units". That's a very precise and useful set of statements--much better than just "the effect is probably positive, on average", which is why this approach tends to be favored by statisticians. But if you don't need all that detail, the first approach I mentioned could be fine too.
