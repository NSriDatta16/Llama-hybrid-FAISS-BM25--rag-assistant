[site]: crossvalidated
[post_id]: 59864
[parent_id]: 59472
[tags]: 
OK. Well, if you are looking for alternative correlation measures, maybe we should start with some basic time series stuff - which the author of this blog seems unaware of. Of course, getting a quantitative measure of “highly correlated” to match with my intuitive understanding has never worked out so well. Correlated, for me, tends to mean: “oscillates with the same periodicity in the same overall trend.” A daily correlation measure, as defined mathematically, is actually a measure of how divergent two time-series’s noise is (where noise is the divergence of a time-series from its average trend). These definitions certainly don’t line up. Hoffstein's intuition of correlation corresponds fairly closely to my intuition of what a spectral decomposition is. Spectral decomposition ... and I'm glad you asked ... represents how closely a series resembles a sine/cosine function of given periodicity. It's derived from the Fourier transform of the series. This is called a frequency domain analysis. Correlation is related to the extent to which two entities are linearly related. Correlations belong to the so-called time domain analysis. When you look at an individual time series, you look at its correlation with itself at previous time points - that's the auto-correlation function. The cross-correlation function looks at how $X(t)$ relates to $Y(t-L)$ for lags $L$. Hoffstein seems to be shooting for the cross-correlation of lag 0, but he misses. All of these series that look at correlations (or spectra) assume that the time series have mean 0. That means that if they don't have mean 0, you subtract the mean. And furthermore, if there is a fixed linear trend, you estimate it and remove it. Otherwise, as he points out, you are confounding the trend effect and the stochastic effect. There's more. The sample auto-correlations only converge if the relationship between subsequent values is sufficiently small - and the correlations have to dampen out with time. The estimates don't converge with something like a random walk $X_t=X_{t-1}+\epsilon_t$, so the first thing you do (if you suspect random walk type models) is take the first differences of the series and analyse those. If Hoffstein took the first differences of the two plots he has, the remainders would probably be ARMA (auto-regressive moving average processes), and he could sensibly look at correlations and cross-correlations. I am trying to cram a whole course in time series into one posting. The point is that series like Hoffstein's can arise from two (at least) models. You can assume a linear function of time with random (correlated) error - or you can assume a fully stochastic model (like random walk). There are fully stochastic models whose trajectories mimic deterministic models fairly well. Either way, a straight-up autocorrelation or cross-correlation will not work. You can see from your own workings, and Hoffstein's, that your correlations are seriously not converging. I don't know from your code why you got a different answer from H. - I am not familiar with the functions in quantmod --- but you can see that whatever it is, it's a bad idea. I think the problem is that $\bar{x}$ and $\bar{y}$ are not converging, since they are being taken over trending time frames. Hoffstein says We’ve blogged about how it can be a deceiving metric before, but never offered up more of a solution than simply, “make sure you assume a zero-mean.” Well, no. It's not important to assume a zero mean. It's important to have a zero mean - and if you need to difference your series to get it, then that's what you do. There is more wrong with Hoffstein. Why does he take moving averages of his series before looking at correlations? The moving average induces autocorrelations into the series. If the series had been white noise prior to his manipulations, they would end up with aucorrelations after the fact. This is called the Slutsky-Yule effect. I don't understand what he is doing with his "% difference from price - whether this is a % difference within the same series (Hello, Messers. Slutsky and Yule), or if he is taking the % of one series again the other (which would induce cross-correlations) --- but it's messy and horrible and I don't like it. The starting point has to be the model - but I am old-fashioned. Ask yourself what you know about the data; ask yourself what you would like to know; then build a model and estimate the parameters. Otherwise, it looks a lot like puddling, although one might call it exploratory data analysis if one wanted to put a good face on it. If Hoffstein wants to relate SPY and GLD, he could possibly just regress one on the other - perhaps with some iterative process for the errors (which would be correlated). You would need a weighted least squares to account for the error structure - but there are ways of doing that.
