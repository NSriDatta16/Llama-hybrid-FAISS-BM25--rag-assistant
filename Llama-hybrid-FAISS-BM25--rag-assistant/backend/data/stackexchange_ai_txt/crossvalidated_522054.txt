[site]: crossvalidated
[post_id]: 522054
[parent_id]: 515741
[tags]: 
Yes, you can simply add the losses. That being said, since you only want to have the final layer depend on the explanatory tasks, you might as well first fully train your network on the explanatory tasks. Then you could just run a simple logistic regression for your final tasks. If you think about it, your last fully-connected layer is equivalent to a logistic regression on a set of variables (your explanatory task variables). Logistic regressions are indeed considered a good resource in case you want to have explainable outcomes, because the weights give indeed a rudimentary explanation. That being said, the whole reason for using neural networks is to avoid the need of hand-crafting explanatory tasks to start with. You should ask yourself: can you define all then necessary explanatory tasks that will allow you to train the network optimally on your downstream task in the final layer? (This would depend on your particular tasks, but often it is not the case.)
