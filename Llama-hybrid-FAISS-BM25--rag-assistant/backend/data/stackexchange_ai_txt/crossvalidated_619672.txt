[site]: crossvalidated
[post_id]: 619672
[parent_id]: 
[tags]: 
Simple Linear Regression - Unbiasedness

We consider the residuals $\varepsilon_i$ as random variables drawn independently from some distribution with mean zero. In other words, for each value of $x$ , the corresponding value of $y$ is generated as a mean response $\alpha + \beta x$ plus an additional random variable $\varepsilon$ called the error term, equal to zero on average. Under such interpretation, the least-squares estimators $\hat \alpha$ and $\hat \beta$ will themselves be random variables whose means will equal the "true values" $\alpha$ and $\beta$ . Wikipedia Is there a conceptual way to understand how is it that the normal distribution of the error (which is somewhat acceptable) turns $\alpha$ and $\beta$ into normally distributed?
