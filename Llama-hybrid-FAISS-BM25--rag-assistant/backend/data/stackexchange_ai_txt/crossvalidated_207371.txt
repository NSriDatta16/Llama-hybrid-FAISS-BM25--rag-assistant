[site]: crossvalidated
[post_id]: 207371
[parent_id]: 182331
[tags]: 
I actually wrote my first paper in machine learning on this topic. In it, we identified that when your classifier outputs calibrated probabilities (as they should for logistic regression) the optimal threshold is approximately 1/2 the F1 score that it achieves. This gives you some intuition. The optimal threshold will never be more than .5. If your F1 is .5 and the threshold is .5, then you should expect to improve F1 by lowering the threshold. On the other hand, if the F1 were .5 and the threshold were .1, you should probably increase the threshold to improve F1. The paper with all details and a discussion of why F1 may or may not be a good measure to optimize (in both single and multilabel case) can be found here: https://arxiv.org/abs/1402.1892 Sorry that it took 9 months for this post to come to my attention. Hope that you still find the information useful!
