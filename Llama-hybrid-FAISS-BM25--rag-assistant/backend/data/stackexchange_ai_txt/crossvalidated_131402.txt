[site]: crossvalidated
[post_id]: 131402
[parent_id]: 129472
[tags]: 
I tested XOR w/ RPROP, i=2, h=3, o=1, MSE, sigmoid, start weights [0:1]. Training data (while mse > 1%, epoch float trainingDataArr[] = { 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 0.0f, 0.0f, 1.0f, 1.0f, 1.0f, 1.0f, 0.0f }; This network converged only ~83% of the time (out of 500 iterations). I'm not sure where your 0.35 is coming from (is that o average? o=0.35 regardless of i?, final rms?) But, I can tell you that 80% convergence that you're experiencing is normal w/ 3 hidden nodes . I'd recommend using a minimum of 4 (96% convergence) or even 5 (98% convergence) nodes in your hidden layer. These results were generated using an RPROP algorithm as outlined in A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm by Professor Riedmiller[1]. Hope this helps and sorry I can't shed more light on the local minima. http://deeplearning.cs.cmu.edu/pdfs/Rprop.pdf UPDATE: Getting 93% convergence now using range [-1:1] for the random weight generation but I did have to normalize the inputs to get the std. deviation to 1 and mean to 0.0. Is it possible the 0.35 minima is occurring due to error translating between domain A with range 1 (0:1) and domain B with range 2 (-1:1) ? void NeuralNet::NormalizeInputs(float* trainingData, int batchSize) { float max = FLT_MIN, min = FLT_MAX; for(int row = 0; row max) max = trainingData[row+x]; if(trainingData[row+x]
