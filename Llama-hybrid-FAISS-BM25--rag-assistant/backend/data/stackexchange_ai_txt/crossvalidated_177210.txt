[site]: crossvalidated
[post_id]: 177210
[parent_id]: 
[tags]: 
Why is Laplace prior producing sparse solutions?

I was looking through the literature on regularization, and often see paragraphs that links L2 regulatization with Gaussian prior, and L1 with Laplace centered on zero. I know how these priors look like, but I don't understand, how it translates to, for example, weights in linear model. In L1, if I understand correctly, we expect sparse solutions, i.e. some weights will be pushed to exactly zero. And in L2 we get small weights but not zero weights. But why does it happen? Please comment if I need to provide more information or clarify my path of thinking.
