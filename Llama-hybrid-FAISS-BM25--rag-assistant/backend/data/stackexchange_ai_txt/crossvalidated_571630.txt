[site]: crossvalidated
[post_id]: 571630
[parent_id]: 568507
[tags]: 
This matrix is called the Gram matrix and it is the matrix whose $(i, j)$ entry is the inner product of vectors $x_i$ and $x_j$ (if $X$ is the matrix whose $i$ -th row is $x_i$ ). If you want to normalize the row-vectors before taking inner products (by removing the means), you can consider it a covariance matrix as well. Why would you do this? Well, in many cases the columns of a matrix are equally as meaningful as the rows. Imagine you have a matrix whose rows correspond to Documents (like scientific publications or news articles) and whose columns correspond to Words (like "President", "biology", or "Twitter"). You could write a $1$ in the $(i, j)$ entry of the matrix if the $j$ -th word appears in the $i$ -th document, and a $0$ otherwise. Now you can imagine how determining the covariance of Documents could be useful (perhaps in document clustering with PCA) and how determining the covariance of Words could also be useful (perhaps in knowledge graph creation). A more broad field of study in this direction could be called co-clustering. The idea is that Documents are similar to one another if they contain the same word; Words are similar to one another if they appear in the same document; Documents are similar to one another if they contain similar words; Words are similar to one another if they appear in similar documents; And so on!
