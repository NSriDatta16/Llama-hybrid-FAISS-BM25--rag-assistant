[site]: datascience
[post_id]: 48194
[parent_id]: 31109
[tags]: 
A similar question was asked here . This Google Developers blog post says: Well, the following "formula" provides a general rule of thumb about the number of embedding dimensions: embedding_dimensions = number_of_categories**0.25 That is, the embedding vector dimension should be the 4th root of the number of categories. Interestingly, the Word2vec Wikipedia article says (emphasis mine): Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions , a window size of 15 and 10 negative samples seems to be a good parameter setting. Assuming a standard-ish sized vocabulary of 1.5 million words, this rule of thumb comes surprisingly close: 50 == 1.5e6 ** 0.2751
