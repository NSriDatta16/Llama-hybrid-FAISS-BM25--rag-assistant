[site]: crossvalidated
[post_id]: 363275
[parent_id]: 
[tags]: 
does including "neighborhood income" as a feature increase or decrease bias (in the discriminatory, cultural sense)?

This is a question about reducing bias in models, in the cultural sense of unfair discrimination. I'm aware that adding features to a model will tend to decrease its statistical bias. The models I'm thinking of are machine learning models, so the answer to my question may depend on which algorithm and how the features are included. With those caveats, my question: In Cathy O'Neil's book "Weapons of Math Destruction" she has an example (p.118-119) of Xerox building a model for employee churn. They realize that the variable "distance from the job" correlated with churn; this makes sense because most of us don't like long commutes. However, after digging into the data they realized "Many of the people suffering those long commutes were coming from poor neighborhoods." So Xerox removed the variable from their model. They saw that "distance from the job" was serving as a proxy variable for something like "neighborhood income level", and, since it correlated with higher churn, including this variable would train a model that was discriminatory against folks in that neighborhood. Q: Is there a way to decouple these variables, to still include the useful trend that longer commutes make employees more likely to quit? By including another variable, like "neighborhood income level" (assuming I can get this data), wouldn't the model be able to learn to distinguish the four possibilities (poor neighborhood + long commute; poor neighborhood + short commute; rich neighborhood + long commute; rich neighborhood + short commute) and thereby become less discriminatory?
