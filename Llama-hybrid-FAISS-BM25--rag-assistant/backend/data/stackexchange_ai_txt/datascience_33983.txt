[site]: datascience
[post_id]: 33983
[parent_id]: 33977
[tags]: 
Cross-validation An extremely important concept to understand is that: Cross-validation works independently of the model you use. Cross-validation is just the process of splitting your data into multiple pairs of training and test sets. Once this is done, you can train that data using whatever model you like. In sklearn, which I assume you are using given your tags, you can use cross-validation with any classifier/regressor you'd like. Cross-validation is way to evaluate how well the model will perform on data that it hasn't seen before. When you train your model on all your data, you do not have any data that the model hasn't seen before to test on. This is why you cross-validate so that you can get multiple estimates of what the performance might be. This is more robust than just splitting your data once between a training and test set because in this case, it might happen that your test set is easier very difficult or very hard. By cross-validating, you smooth this out. Linear vs Logistic regression Apparently, your output is ages, and therefore, you could use linear or logistic regression. If you use linear regression, you will try to guess the age directly. If you use logistic regression, you could round the output to the closest integer to obtain the proper target classes. I would advise you to use logistic regression because it will allow your model to learn better. If you use linear regression, your classes are all as different as each other (from a mathematical point of view). However, my guess is that the 13 class is closer to 14 than it is to 20+ .
