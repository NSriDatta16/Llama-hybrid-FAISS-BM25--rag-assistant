[site]: crossvalidated
[post_id]: 274060
[parent_id]: 
[tags]: 
On the independence of errors in time series

I recently started to learn statistics, and I've stumbled upon the idea of autoregressive time series. My question relates to the assumption of independence of the time series white noise process: let $$ y_t = \alpha \, y_{t-1} + u_t$$ be an (at least) second order stationary time series, where $u$ is white noise. Reading online, it seems like all the $u$'s are to be taken as i.i.d., $u\sim \mathcal N(0, \sigma^2)$, so I'd expect $\langle u_t\,u_s\rangle = \sigma\,\delta_{i,j}$. However, playing around with the defining equation above, I get $$ \langle u_t\,u_{t+1}\rangle = \langle(y_t - \alpha\,y_{t-1})(y_{t+1} - \alpha\,y_{t})\rangle\\ =(1+\alpha^2)\langle y_t\,y_{t+1}\rangle - \alpha\langle y_t\,y_{t+2}\rangle\\ =\sigma^2\,\frac{\alpha}{1-\alpha^2} $$ where I used the standard formulae for the correlation of $y_t\,y_s$, the fact that $\langle y_t \rangle=0$ and the wide-sense stationariety of the process. From this few equality it looks like the errors $u$ are really independent only if alpha vanishes, in which case the entire process is white noise. In the more general case where $|\alpha|
