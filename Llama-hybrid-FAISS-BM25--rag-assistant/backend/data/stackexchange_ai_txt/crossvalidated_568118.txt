[site]: crossvalidated
[post_id]: 568118
[parent_id]: 
[tags]: 
Do I need linear output in a neural netework?

I tried to solve the XOR problem with 2 almost identical neural networks, see R Keras code below. What differs between the NN is that in the second network, I have added a linear output node, which if I understood the NN-litterature correctly, is how feedforward NN usually are designed (eventually more than one output node depending on problem). By intuition, I would guess that the optimization of these NN should almost be equally hard, but running these codes gives notably different results. Result after 50 epochs from first network, no linear out Result after 50 epochs from first network + linear out As we can see, the network with the extra linear output has a much harder time to converge. I ran the code multiple times and got similar results. If I run the second code with more epochs, it eventually do converge (but not as smooth as the first code) Result after 200 epochs from first network + linear out If I understand my code correctly, the two networks looks like this: $$\text{2 Inputs} \rightarrow \textbf{W}^{1} \rightarrow \text{10 ReLU nodes} \rightarrow \textbf{W}^{2} \rightarrow \text{1 Sigmoid node} \rightarrow \text{1 Output}$$ and $$\text{2 Inputs} \rightarrow \textbf{W}^{1} \rightarrow \text{10 ReLU nodes} \rightarrow \textbf{W}^{2} \rightarrow \text{1 Sigmoid node} \rightarrow \textbf{W}^{3} \rightarrow \text{1 Linear node} \rightarrow \text{Output}$$ where weights are $\textbf{W}^{1}: 2\times10$ , $\textbf{W}^{2}: 10\times1$ , $\textbf{W}^{3}: 1\times1$ . Thats is, by adding one extra linear output node the convergence has changes drastically. So to my questions. Is this standard behaviour? I recently started to play around with NN so I don't know what to expect really. Should I use this extra linear output , or is the sigmoid enough? Eventually, when should I use an extra linear output layer? Data library(keras) library(tidyverse) XORdata = data.frame(output = c(0,1,1,0), x1 = c(0,0,1,1), x2 = c(0,1,0,1)) X = model.matrix(output ~ . -1, data = XORdata) y = XORdata$output ggplot(data = XORdata) + geom_point(aes(x = x1, y = x2, shape = as.factor(output)), size = 4) + scale_shape_manual(name = "Output", values=c(4, 16)) Neural network 1, sigmoid output, no linear output myNN1 % layer_dense(units = 10, activation = "relu", input_shape = ncol(x)) %>% layer_dense(units = 1, activation = "sigmoid") myNN1 %>% compile(loss = "mse", optimizer = optimizer_sgd(learning_rate = 0.5), metrics = list("mean_absolute_error")) history1 % fit(X, y, epochs = 50, batch_size = 1) Neural network 1, sigmoid output + linear output myNN2 % layer_dense(units = 10, activation = "relu", input_shape = ncol(x)) %>% layer_dense(units = 1, activation = "sigmoid") %>% layer_dense(units = 1) myNN2 %>% compile(loss = "mse", optimizer = optimizer_sgd(learning_rate = 0.5), metrics = list("mean_absolute_error")) history2 % fit(X, y, epochs = 50, batch_size = 1)
