[site]: crossvalidated
[post_id]: 614809
[parent_id]: 
[tags]: 
Can probabilistic (e.g., Fisher, Shannon) and non-probabilistic (e.g., Hartley, Kolmogorov) information types be jointly useful?

Suppose you draw a random sample from a probability distribution, with the objective of gaining information about a parameter of that distribution. The inferential usefulness of the probabilistic information (Fisher information about the parameter) of the sample is obvious. Likewise, if we examine a set of messages sent over a noisy channel, we can quantify their entropy to draw inferences about the error source, the population of messages, and the messages as originally composed. Alternatively, we could use non-probabilistic measures in either scenario (or at least in some versions of them). These include exact, non-parametric, combinatorial, or algorithmic measures. These measures would yield, respectively, exact information, non-parametric Fisher information, combinatorial entropy, and algorithmic information or complexity. I can't recall ever seeing an analytical approach that uses the two types of information (probabilistic and non-probabilistic) in a complementary or supplementary way. They seem to be mutually exclusive. For example, we would never estimate a p -value when it is practical to compute an exact test. Non-parametric tests are inferior to (less informative than and redundant with) parametric tests when assumptions hold but are superior otherwise. Or else, non-parametric tests answer different classes of questions entirely (e.g., quantifying monotonic vs. linear relationships). Combinatorial entropy is equivalent to Shannon entropy under a discrete, uniform distribution, and the respective measures yield the same results in that case. Algorithmic information/Kolmogorov complexity describes individual strings exactly as they are, while Shannon entropy describes average/expected strings or the noise-generating distribution acting on a string. Are there ways that we could, and actually would prefer to, use both probabilistic and non-probabilistic information types to supplement one another, when answering a single question? I'm specifically interested in cases where the two information types are measured on a single data set, and where the question being answered is not multifaceted. It would be trivial to conceive of a question with several parts, some of which are probabilistic in nature and others not, or some requiring greater robustness than others. It's also easy to concoct a scenario where there are multiple data sources or multiple samples from one source, each with different data types or assumptions, respectively. In contrast, it is hard (at least for me) to think of a question as simple as "What is the mean of the population from which this single sample has been drawn?" that would benefit from using both.
