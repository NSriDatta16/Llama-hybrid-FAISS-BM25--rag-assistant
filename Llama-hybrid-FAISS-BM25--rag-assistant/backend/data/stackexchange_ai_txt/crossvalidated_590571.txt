[site]: crossvalidated
[post_id]: 590571
[parent_id]: 590477
[tags]: 
If you want to frame it as a time series problem, then you might want to go the autoregressive decoder route, where you feed in the sequence of past "outputs" (using teacher forcing, as is commonly used to parallelize transformer model training) in addition to your regular input features in order to help the model predict the next location. Note that the time series formulation and using the historical data as ground truth assumes that the historical placements and timings were optimal, which is a shaky assumption. However, you should be able to approximate human performance provided you have supplied your model with enough information. Note that this is likely a high perplexity task so don't expect your training losses to get very low-- but even an RNN should be a good fit for this. Recurrent memories retain information about likely candidates and can pivot to another close or "next-best" option if the model's top-1 pick in the current timeframe isn't the "best fit" to the historical data. In any case, it sounds to me like your problem has an inherent autoregressive property. If you wanted to flatten out the time element, the problem becomes something like, "given the currently deployed machines and the other input features, predict a heatmap, top-k or a single-best location for future machines", for which a variety of architectures could be reasonable candidates.
