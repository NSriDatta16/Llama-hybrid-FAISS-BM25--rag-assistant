[site]: datascience
[post_id]: 117427
[parent_id]: 117411
[tags]: 
In almost all circumstances, sound statistics disputes that class imbalance is a problem. Consequently, you probably should not do anything besides keep doing good data science that is grounded in correct statistics. (After all, this is what you were doing the whole time, right?) Imbalance often appears to be a problem because models can achieve high accuracy scores by always or almost always classifying as the majority class. The trouble with such a statement is that most models do not do classification. Despite what software methods might exist, models like logistic regressions and neural networks do not do classification. They output predictions on a continuum that can be binned into categories, but they do not have to be, and the way to separate the continuous predictions into discrete classes could involve a threshold other than the usual software-default of a probability of $0.5$ or even more discrete categories than there are classes. By evaluating models on statistically sound values like log-loss (“crossentropy loss” or “negative log likelihood” in some circles) or Brier score, almost any apparent issue related to class imbalance turns out not to be a problem. Since class imbalance then is not a problem, there is no need to use methods like oversampling, undersampling, or synthesis of new points in order to solve a non-problem. In fact, since synthesis of new points need not produce reasonable new points , creating synthetic points might create problems just because the user tried to solve a non-problem. We have an entire statistics meta post with oodles of good links related to class imbalance and why it is not the problem it appears to be.
