[site]: crossvalidated
[post_id]: 287795
[parent_id]: 287773
[tags]: 
Using 'old school' techniques for building neural networks, there are many problems with making deep networks. Nowadays, there are many modern techniques designed to address these. In no particular order Vanishing gradients: As the gradient is fed through layer after layer, and some neurons have saturated activations, the gradients get smaller and smaller. This make the pace of learning for early layers very small. Sometimes the opposite can happen and the gradients explode. Residual and/or skip connections have largely overcome this problem. Overfitting: As you add more parameters to your model, you may end up training something that works really well in sample, but does not generalize out of sample. This is because you have 'fit the noise'. Techniques such as L1 and L2 regularization and drop out help address this problem. Internal Covariate Shift: Each layer can be thought of (loosely) as trying to learn from the layer before it. But as you train your model, the distribution of activations of earlier layers keep changing, and then later layers are trying to learn from a moving target.: Techniques such as batch normalization help to reduce this problem. There are plenty of others, but those are three big ones.
