[site]: datascience
[post_id]: 56600
[parent_id]: 
[tags]: 
Can I turn any binary classification algorithms into multiclass algorithms using softmax and cross-entropy loss?

Softmax + cross-entropy loss for multiclass classification is used in ML algorithms such as softmax regression and (last layer of) neural networks. I wonder if this method could turn any binary classification algorithms into a multiclass one? For instance, If I am using a polynomial function for binary classification, the decision step being 'predict positive if the output of the polynomial is greater than 0, otherwise predict negative', then I could use k of such polynomials $\dagger$ for k-class classification, each polynomial $f_i(X)$ has its own set of parameters to be learned; the objective is then to minimize the KL divergence between the sample distribution of the one-hot label and $e^{f_i(X)}/\sum_{i=1}^{k} e^{f_i(X)}$ or equivalently, cross-entropy. Now if $f(X)$ is linear, this is exactly softmax regression algorithm, would it work if $f(X)$ is a polynomial or factorization machine or any classification algorithms that output a real number? Some disadvantages of this approach I have in mind: the parameters scale linearly with the number of classes the loss function may be non-convex and hard to optimize the theoretical properties/guarantees of the original binary classifiers may be lost And how does this compare to classical 1-v-1 or 1-v-all approaches? $\dagger$ Only $k-1$ is needed since softmax is over-parameterized
