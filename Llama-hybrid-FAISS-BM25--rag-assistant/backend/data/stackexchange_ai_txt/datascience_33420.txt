[site]: datascience
[post_id]: 33420
[parent_id]: 33419
[tags]: 
You are correct in saying that it would be unfair - and if avoidable, you shouldn't do it. In order to truly be able to claim (in a statistical sense) that a model achieves e.g. 90% accuracy, the test must be performed on unseen data. That is where your test data should be used. Training a neural network requires the validation data (as you mentioned, within the fit_generator method of a Keras model), in order to compute errors and steer the weights in the right direction. The final accuracy you report needs to be on data to which the the training pipeline has never been exposed. Creating a training/validation/test split is advised where possible; however, it can sometimes be a challenge, due to things like lack of data and imbalanced datasets . You can try things such as cross-validation - here is an example using Keras . Her is another example , in a similar question.
