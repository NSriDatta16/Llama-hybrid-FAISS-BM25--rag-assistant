[site]: stackoverflow
[post_id]: 2000939
[parent_id]: 1971662
[tags]: 
Here's an idea: Compute the average usage frequency of every word in the English language. Put these in a lookup table. You will likely want to only keep the most common words. Pick a number of words (say, 5000), or a minimum frequency, that makes sense. You will likely still want to have a list of words that you never show. If you sort your list of words by frequency, it won't take you long to look through them and choose which words to always exclude. To compute the frequency, you need an input sample. Your choice of input sample will affect the outcome. For example, Twitter could use every twitter message that has ever been posted as its input sample. Topics that are constantly being discussed on Twitter (for example, "Twitter" itself) would lose importance. If you want Twitter-specific topics to keep their significance, then find another input sample. The algorithm for computing frequency is simple: For each word in the sample: Look that word up in the dictionary Add one to a counter associated with that word To normalize the data, divide the frequency of each word by the number of words in the input sample. Now, if you run the same algorithm on today's Twitter posts, you can compare today's word frequencies with expected word frequencies.
