[site]: datascience
[post_id]: 46524
[parent_id]: 46523
[tags]: 
If your model is still improving (according to the validation loss ), then more epochs are better. You can confirm this by using a hold-out test set to compare model checkpoints e.g. at epoch 100, 200, 400, 500. Normally the amount of improvement reduces with time ("diminishing returns"), so it is common to stop once the curves is pretty-much flat, for example using EarlyStopping callback. Different model requires different times to trains, depending on their size/architecture, and the dateset. Some examples of large models being trained on the ImageNet dataset (~1,000,000 labelled images of ~1000 classes): the original YOLO model trained in 160 epochs the ResNet model can be trained in 35 epoch fully-conneted DenseNet model trained in 300 epochs The number of epochs you require will depend on the size of your model and the variation in your dataset. The size of your model can be a rough proxy for the complexity that it is able to express (or learn). So a huge model can represent produce more nuanced models for datasets with higher diversity in the data, however would probably take longer to train i.e. more epochs. Whilst training, I would recommend plotting the training and validation loss and keeping an eye on how they progress over epochs and also in relation to one another. You should of course expect both values to decrease, but you need to stop training once the lines start diverging - meaning that you are over-fitting to your specific dataset . That is likely to happen if you train a large CNN for many epochs, and the graph could look something like this: Image source
