[site]: datascience
[post_id]: 40838
[parent_id]: 
[tags]: 
How to learn word embedding from a context on the fly?

Consider the fictional word tahiliuk in the sentence “We found a small, fluffy tahiliuk running around our garden.” While hearing a new word used in context, people are remarkably adept at inferring a basic notion of a its meaning. Question: How the tahiliuk's vector could be learned from the context? Assumption: a fixed input vocabulary, mapped to a set of word vectors. Remarks: Tahiliuk is a completely new word. There is now such word within training data. Using approaches like "fastText" will not work here because of tahiliuk is a random word. One possible way is to initialize the embedding as the sum of their context words and then rapidly refine the embedding with a high learning rate. This approach is used in the following paper, but this approach seems to be not perfect Herbelot, A., & Baroni, M. (2017). High-risk learning: acquiring new word vectors from tiny data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.
