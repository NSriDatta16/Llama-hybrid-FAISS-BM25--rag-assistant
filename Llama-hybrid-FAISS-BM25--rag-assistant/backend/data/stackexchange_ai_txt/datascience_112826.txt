[site]: datascience
[post_id]: 112826
[parent_id]: 
[tags]: 
Limitations of NLP BERT model for sentiment analysis

I am reading a paper , where the authors assess online public sentiment in China in response tot the government's policies during Covid-19, using a Chinese BERT model. The author's objective is not only to learn whether a given online post is critical or supportive, but also learning to whom each post was directed at (e.g. CCP, local governments, health ministry, etc). To achieve this, the authors further state in pages 8 through 9, that they,"To train the classifer, we randomly sample approximately 5,000 posts from each dataset (10,541 posts in total), stratified by post creation data. This sample is used for a number of analyses, and we refer to it as the Hand-Annotated Sample." My question here is what's the value of using human-annotated posts in combination with a BERT sentiment analysis model? Specifically, my understanding of BERT as a technique is that it eliminates or at least minimizes the need for pre-labelling a sample of text for sentiment analysis purposes, and it's not clear to me why we still need hand-annotated text by humans even when using BERT.
