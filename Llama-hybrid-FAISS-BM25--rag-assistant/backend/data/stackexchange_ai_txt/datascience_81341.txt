[site]: datascience
[post_id]: 81341
[parent_id]: 81249
[tags]: 
Here is a great answer to this question. I'll summarize: The code example was taken from a "buggy" repository on GitHub and is not typical of robust solutions. Robust solutions actually do use the first word as a target word. If the context window is length 10, then the method uses the next 5 words as the context and the first word as the target (it won't actually have a context of size 10 since the first half of the context doesn't exist). Even though the first few words in the sentence are used as target words, they still will not appear in as many contexts. This issue is mitigated because they appear in smaller contexts. Since the contexts they appear in are smaller, they have more impact on the context and, therefore, more significance in back propagation. Many of the more robust implementations will use an entire paragraph or document as opposed to a sentence (some even include punctuation). This make sense because the ending of one sentence may give context for the beginning of another sentence. When this approach is implemented, there are far fewer start/ending words, which reduces the issue. The answer linked above has some other helpful details and is worth reading.
