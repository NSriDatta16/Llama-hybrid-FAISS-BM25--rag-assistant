[site]: crossvalidated
[post_id]: 125268
[parent_id]: 125261
[tags]: 
This will greatly depend on your problem and the type of features you use. It is generally not the number of features itself that will decide if you overfit or not but rather the method you use to train your model with. However, overfitting is more prone to happen if your data is sparse unless you do something to prevent it. My recommendation would be to use as many features as you can and then let the model decide which of these will add to it's general performance. If you use a linear model such as logistic regression you can use L1-regularization to select which variables are of value during training (preferably using cross-validation or similar to estimate your models true performance). If you use R there is a package called glmnet that can do this for you. The python library scikit-learn can do the same. An alternative is to evaluate the performance in a step-wise manner adding or removing variables depending on some model metric. But in my experience it is both easier and more flexible to use regularization.
