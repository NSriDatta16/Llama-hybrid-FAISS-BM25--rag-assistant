[site]: crossvalidated
[post_id]: 321898
[parent_id]: 321890
[tags]: 
The problem is my data here is non-standardized because some of it is income, some of it is age etc. It's also not scaled. I could scale it to go from 0-1 but some of the values have extremely high ranges, from 0 to 99999. That's exactly why someone would use standardization/scaling. Since you mentioned you use scikit-learn - actually its PCA implementation does standardization and mean-centering as preprocessing by default. Load the data, remove the target variable, then run PCA only on the values that i converted from being Nominal. Re-add these PCA-generated columns to the original numeric data. Seems like a valid approach - also you could also use some different dimensionality reduction method for this data, for example maybe some of these features don't influence prediction or are redundant (you can check out contingency tables with pandas.crosstab or try sklearn.feature_selection algorithms on your one-hot encoded data). Load the data, remove the target variable, then run PCA separately for each attribute that used to be nominal, so for example run it for education-College, education-High School, generate n number of principal components, re-add to the original columns and then run it again for another ex-nominal variable, keep repeating until done. That's not particularly good idea, since PCA finds useful features as combinations of input features. For example if you have two perfectly correlated binary features PCA will find it out, but under 3rd approach it won't be possible.
