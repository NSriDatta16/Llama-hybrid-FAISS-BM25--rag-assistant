[site]: datascience
[post_id]: 126118
[parent_id]: 126108
[tags]: 
The initial prediction value $\hat{y}^{(0)}$ does not matter for gradient boosting. Typically it is set to the mean of the target variable for regression or the log odds of the class probabilities for classification. But any reasonable constant value works fine as the starting point. The key is that subsequent trees fit to the residuals will update and improve on this initial prediction. It may affect the algorithm's convergence time if a terrible initial value is selected. We fit trees to the pseudo-residuals rather than the target classes because this allows each tree to focus on correcting the errors from the model ensemble so far. The pseudo-residuals represent the gap between the current model predictions and the true target. So fitting a tree to predict these residuals lets the tree specialize in hard examples that current trees are getting wrong. This complementary specialization of trees to focus on residuals is what makes gradient boosting so effective. Trees fit on residuals reduce bias while trees fit directly on the target may overfit and increase variance. I highly recommend checking the following videos from the StatQuest channel that clearly explain the boosting algorithms. EDIT : Explaining the following statement Trees fit on residuals reduce bias while trees fit directly on the target may overfit and increase variance. When we fit decision trees directly to the target variable (e.g. directly predicting home prices), they can overfit the training data quite easily. A single tree is already a complex high-variance model, and fitting it directly to noisy targets often results in overspecializing to noise and outliers. This overfitting increases variance without necessarily reducing bias. So we end up with a very complex, wiggly model that fails to generalize. However, when we fit trees to model the residuals, we change the objective. Instead of memorizing the noisy targets, the tree is focused on explaining the current model's errors. The residuals represent our model's bias - examples it is consistently getting wrong. By having trees correct these residuals, we reduce bias while limiting variance growth because the trees cannot overfit relative to what came before them. The residual-fitting forces the trees to focus on systematic errors rather than noise. The ensembling smooths out variance over many trees, leaving us with a strong bias reduction that generalizes much better. This is why GBDTs beat random forests despite both using trees - the residual-focused learning prevents overfitting.
