[site]: datascience
[post_id]: 112536
[parent_id]: 
[tags]: 
Low validation accuracy when not using shuffled datasets

First I tried creating the training/testing datasets using sklearn train_test_split function like the following, x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.5, random_state = 1) And on the second test I tried splitting the two datasets by half without any kind of randomization... x_train, x_test, y_train, y_test = x_scaled[:int(total_rows/2)],x_scaled[int(total_rows/2):],y[:int(total_rows/2)],y[int(total_rows/2):] On first test, The model accuracy was like the following, loss: 0.1951 - accuracy: 0.7057 - val_loss: 0.2101 - val_accuracy: 0.6540 and classification report was like this, precision recall f1-score support 0.0 0.55 0.76 0.64 864 1.0 0.78 0.56 0.65 1263 accuracy 0.65 2127 macro avg 0.66 0.66 0.65 2127 weighted avg 0.68 0.65 0.65 2127 On the second test, when I used splitted datasets, the model accuracy was pretty good, but test accuracy was below average... loss: 0.1558 - accuracy: 0.7875 - val_loss: 0.5014 - val_accuracy: 0.5026 Classification report, precision recall f1-score support 0.0 0.47 0.80 0.59 965 1.0 0.60 0.26 0.36 1162 accuracy 0.50 2127 macro avg 0.54 0.53 0.48 2127 weighted avg 0.54 0.50 0.47 2127 I understand the second model is overfitting, that's why I'm getting poor test results... But in the real world the structure is gonna be kind of same... Like I'll have to use the model on fresh data while training it on older data... ( The rows are sorted or indexed by datetime in the datasets ) I'm kinda new to machine learning. So lil bit confused on this... Does the second test mean the model is not gonna perform that well like the first test in real world? Or what I'm doing wrong?
