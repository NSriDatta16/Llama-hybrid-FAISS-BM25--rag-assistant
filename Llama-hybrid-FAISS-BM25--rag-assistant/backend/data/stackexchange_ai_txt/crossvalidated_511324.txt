[site]: crossvalidated
[post_id]: 511324
[parent_id]: 511281
[tags]: 
Why is that as the time steps increase, =0 always yields average reward equal to 1? It does not always yield 1, but when averaged over many instances of the problem (2000 used in the plots), the mean reward is close to 1. The expected reward per time step in each instance of the environment can vary widely, and is limited by $\text{max}_a q_*(a)$ . For instance it is possible (with probability of 1 in 1024) that all 10 actions are assigned a mean less than zero. In which case, the best an agent can do is find the least negative action. Why is it, firstly, non-zero, and why is it 1? You would expect a random agent to have a zero mean result on the example environment. Greedy agents are not as effective as other strategies, as shown by the graph, but they are often significantly better than random agents. The mean reward is non-zero because a greedy agent will still refine its estimates and because the estimates may change which the preferred action is, it is able to correct a proportion of early mistakes. Here are two ways in which a greedy agent will prefer actions with a positive mean value: When pulled for the first time (and thus setting the initial estimate for that bandit), an action with a negative mean value is more likely to return a negative reward. Once the agent has experienced a negative value from the first pull, it will avoid that action until all other estimates are below it. If by chance the first chosen action with a negative mean reward returns the best reward so far (due to variance in individual rewards), it will get chosen again on next actions. Over successive time steps, the estimated mean will become closer to the true mean for that action. As that true mean is negative, it will drop below initial estimates of zero. Provided at least one other action has not been chosen yet, that other action will become preferred by the agent. There can be many more complex interplays between initial estimates when using a greedy agent. In general, any initial estimate that is by chance too high and cause the agent to select that action will self-correct becasue it will get sampled again. That in turn may reduce the estimate to where it is below some other action's estimate, making a different action the preferred greedy choice. This sequence of events will preferentially select better actions on average. The greedy agent does not easily correct estimates that start too low such that the actions never get selected again - it is common to see underestimates of the best few actions where those estimates are lower than the true value of the $n^{th}$ best action, so it is that $n^{th}$ best action that gets chosen in the long term. However, there is still a bias towards low values of $n$ , with $n=1$ (i.e. choosing the best action) being most likely. The same chapter of Sutton & Barto covers optimistic initialisation, which can take advantage of this behaviour of greedy action selection to drive a form of exploration. The specific value close to $1.0$ seen in the experiment is mostly coincidence. It depends a lot on $k$ , the number of possible actions, as well as the mean and standard deviation of course. I am not aware of any simple derivation from theory, and would be very surprised if the value was exactly $1.0$
