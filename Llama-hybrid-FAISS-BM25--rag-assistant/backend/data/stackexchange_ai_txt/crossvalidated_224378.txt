[site]: crossvalidated
[post_id]: 224378
[parent_id]: 
[tags]: 
Does an optimally designed neural network contain zero "dead" ReLU neurons when trained?

In general should I retrain my neural network with fewer neurons so that it has fewer dead ReLU neurons? I've read conflicting opinions about dead ReLUs. Some sources say dead ReLUs are good because they encourage sparsity. Others say they're bad because dead ReLUs are dead forever and inhibit learning. Is there a happy medium?
