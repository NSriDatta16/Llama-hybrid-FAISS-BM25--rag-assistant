[site]: crossvalidated
[post_id]: 176896
[parent_id]: 176808
[tags]: 
Assessing the quality of a multinomial classifier based on the modal class (the class that has the highest probability) is indeed sub-optimal. This results in the richer posterior class distribution (which is much more informative) to, essentially, go to waste. The multinomial (generalised) use case for the Bayesian Information Reward (BIR) caters for that. In the Bayesian Information Reward paper, the authors highlight the problem you mention on Pg.3 (between equations 2b and 3) and continue to propose a solution. $I_i^+ = 1-log\frac{pi}{p'i}$ $ I_i^- = 1-log\frac{1-pi}{1- p'i}$ $ BIR = \frac{\sum_i^k{I_i}}{k} $ where, $pi$ is the predicted probability of class i $p'i$ is the prior probability of class i $k$ is the number of class labels $I_i^+$ and $I_i^-$ are the rewards for correct and incorrect classifications respectively. The $BIR$ for a single observation is calculated as the average of rewards across all labels. In the context of Neural Networks training, Cross Entropy is commonly used. This is certainly better than training based on accuracy. However, note that $BIR$ is better than cross entropy since it takes into account class priors.
