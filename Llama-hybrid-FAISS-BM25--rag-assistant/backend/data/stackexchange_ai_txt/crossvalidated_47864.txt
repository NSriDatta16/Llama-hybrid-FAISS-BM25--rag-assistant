[site]: crossvalidated
[post_id]: 47864
[parent_id]: 
[tags]: 
Handling poor inter-rater reliability while minimizing the loss of data

Edit: While I do appreciate Peter Flom's suggestions and the subsequent discussion (and upvoted his answer), I am opening a bounty to solicit an answer which offers a specific, formal statistical approach to handling these inconsistencies while minimizing the loss of data. I strongly suspect this issue has inspired statistical research along the lines of my bullet points below (or maybe completely different!) but I am not aware of it and am hoping to rouse the attention of someone who is. I would consider an answer that is little more than a reference and a short description to be worthy of the bounty ; there's no need to derive or implement the approach for me. I'd also accept an answer that gives a clever approach, with some details, without a reference. I think @Momo may be onto something with the comments below, so maybe this bounty will draw forth more details/background info :) Edit: For the purposes of building a tractable model, I am willing to assume that the inconsistencies arise "randomly", e.g. due to randomly mis-reading the question or mis-clicking an answer on a tablet/computer, so that the errors could be conceived of as independent of any auxiliary variables, in contrast to the example given by @whuber in the comments. I was recently approach with a statistical question about having inconsistent answers to questions. I think this is the same basic question as when you have multiple raters that disagree on how to rate a particular item, and I thought this language may be more familiar, which is why I chose this title. A toy example: Q1: Do you smoke? A1: No Q2: How many cigarettes a day do you smoke? A2: 5 Of course, a "skip pattern" in the questionnaire would have prevented this but that ship has sailed. The real situation is more complex than this and involves more questions with more subtle inconsistencies but the basic issue is the same. The query of the person who asked me this can be simply stated as: Are there methods for eliminating some degree of inconsistency while still preserving as many samples as possible? I am very aware that, if you want to be "safe" then the only thing to do is throw out any samples with inconsistent replies, but that's not the answer I'm looking for. In particular, there are some cases where there is good evidence that a particular response is a mistake and I'm looking for principled ways to use that evidence - as an extreme example, suppose ten questions measure the same overall construct and nine of the ten agree - then it is quite likely that the one that disagrees was an error (e.g. a mis-read of the question or a mis-click on the computer/tablet used to administer the survey) The basic thoughts I have on the subject bring to mind two general ideas: Try to build a model that estimates the probability that a particular item is a mistake and switch responses whose "mistake probability" is very high. My concerns are that a) this is not tractable without making wild assumptions about the "error rate" and b) if there are only a few (say 3 or 4) questions for each construct, this approach would be basically useless. Try to select some "reliable subset" of the questions (i.e. try to determine whether the disagreement regularly arises from a particular subset of the questions). This way I can get away with deleting columns from the data set rather than rows. This seems reasonable but would be more of an ad hoc procedure that I'm not sure how to formalize. I'm not at all familiar with this field, and it seems like this kind of issue would come up occasionally in statistical consulting, so I wanted to know how this is handled (other than throwing the data out) before I try to "reinvent the wheel".
