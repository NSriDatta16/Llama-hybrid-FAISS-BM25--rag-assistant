[site]: crossvalidated
[post_id]: 389416
[parent_id]: 389412
[tags]: 
This is the distinction between having an objective, and the methodology you use to achieve an objective. In fitting a model, your objective (used as a synonym here for goal, i.e. what you hope to achieve) is to minimize some loss function . In the case of linear regression, that loss function is the least squares loss, but other loss functions are used in other situation. The important point is that the process of fitting the model is synonymous with the goal of minimizing this function. Once you have a goal, you can employ multiple techniques to achieve that goal. First Technique: Compute the partial derivative of the loss function, then try to solve the resulting equations for the zeros of the derivative. This works, sometimes, if the algebraic form of the derivative is simple enough. Often this is just not possible, algebraically (as is the case in logistic regression). Second Technique: Use gradient descent with some stopping criteria to approximate the zeros. This is almost always applicable, even though it does not achieve an exact solution, and allows us little leverage to analyze the properties of the solution. So you see, both techniques attack the same goal, but they offer different pros and cons in actually achieving that goal.
