[site]: crossvalidated
[post_id]: 576754
[parent_id]: 
[tags]: 
Is it possible to use attention in non sequential data in neural networks?

I'm still trying to understand the attention mechanism. It is not clear to me what query, key, and value mean yet, for example. However, my main issue is regarding how to apply attention in my use cases. Most of the examples that I found using attention were for dealing with sequential data, like tokens in sentences. For example, I have a use case in which I need to classify the relationship between pairs of objects, represented by 100 features each. Thus, I was thinking about building a NN architecture with two inputs (each input will represent a description of some object) and one output (the relationship between those objects). Is it possible to use attention mechanisms in this case?
