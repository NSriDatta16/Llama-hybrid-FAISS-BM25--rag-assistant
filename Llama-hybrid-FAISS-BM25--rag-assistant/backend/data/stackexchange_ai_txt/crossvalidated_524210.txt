[site]: crossvalidated
[post_id]: 524210
[parent_id]: 523904
[tags]: 
Expanding on the comment from @BenReiniger, XGBoost can be deterministic, but it probably shouldn't be run that way. According to the XGBoost parameter settings , the default for subsample , the "subsample ratio of the training instances," is 1. With that setting, all cases in the training set will be used at each iteration--deterministic. In contrast, the default for the corresponding bag.fraction in the R gbm package is 0.5, with only half of the training cases used at each iteration--random. Also for XGBoost, the defaults for the subsampling of features, colsample_bytree , colsample_bylevel , and colsample_bynode are all equal to 1, for no subsampling of features. So with the default XGBoost parameter settings, there is no subsampling either of cases or features. You thus should get deterministic results. With those settings, however, you lose many of the protections against overfitting and thus lose the generally better application to new instances that subsampling both cases and features in XGBoost provides. I can't rule out your having a peculiar data set as the answer from @EngrStudent suggests, but change your settings for subsampling of cases or features and see what happens.
