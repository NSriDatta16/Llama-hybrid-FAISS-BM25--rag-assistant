[site]: crossvalidated
[post_id]: 281026
[parent_id]: 
[tags]: 
Why is a fully marginalized likelihood so much more difficult to computer than a partially marginalized posterior?

I have a question in terms of Bayesian computation. Let's contrast parameter estimation and model selection. When performing a Bayesian hypothesis test, this involves computing fully marginalized likelihoods. When performing (Bayesian) parameter estimation, one computes partially marginalized posteriors. The latter is normally far more easy to actually compute than the former. Why exactly is that? How could it be so much easier to computer the marginalization of all of your parameters except one versus marginalizing all of the parameters? Is there a computational intuition behind why this is?
