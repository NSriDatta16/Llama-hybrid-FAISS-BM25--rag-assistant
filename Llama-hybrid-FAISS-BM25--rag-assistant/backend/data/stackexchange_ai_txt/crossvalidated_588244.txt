[site]: crossvalidated
[post_id]: 588244
[parent_id]: 133931
[tags]: 
It does not matter for law of large numbers I observe that with increasing number of simulations (i.e. increasing the repetitions of the sampling event), the mean of means and mean of variance more closely agree with the theoretical values (1/lambda). Your application and observation of the behaviour of the mean relates more to the law of large numbers than the central limit theorem. For the mean of means, it is irrelevant how you create the average. If you average all your samples or first average groups does not matter, because the end result is the same. Example Say your sample is $1,2,3,4$ then you could group it and compute average of group averages, but the result is the same (if the group's sizes are equal). $$\frac{(2+3)/2 + (4+1)/2}{2} = \frac{2+3+4+1}{4}$$ Also note that the mean of the estimated standard deviations may approach some value for larger nsim , but that doesn't need to relate to the simulations following approximately a normal distribution. The behaviour that this value approaches some constant is due to the law of large numbers. It matters for central limit theorem The CLT relates to the distribution of a sample average. When you standardize this based on the population mean and deviation, then the final result approaches a Normal distribution under the right circumstances (independence, finite variance, etc.). If your question would have been about the central limit theorem or about approximations with a normal distribution then the size of the samples matters (not the number of repetitions, although this may have an effect when you make a histogram). There is already a question about this: Why does increasing the sample size of coin flips not improve the normal curve approximation? It deals with the issue of the two different sizes (the sample size and the repetitions size), as well as with problems of drawing a histogram.
