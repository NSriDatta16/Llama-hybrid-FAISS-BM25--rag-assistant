[site]: crossvalidated
[post_id]: 22281
[parent_id]: 22223
[tags]: 
There are different smoothing techniques you can apply to your data to sidestep the 0-observation case you are explaining here. In fact, this page here describes how to handle your exact case, ie. "probability 0" events when calculating the KL divergence using "absolute discounting". This PDF enumerates a number of other smoothing methods you might consider in the NLP context, as well.
