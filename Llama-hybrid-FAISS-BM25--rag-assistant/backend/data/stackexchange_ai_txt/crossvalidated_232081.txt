[site]: crossvalidated
[post_id]: 232081
[parent_id]: 231997
[tags]: 
One way to see it: larger kernels = more parameters = allows for more expressive power. Another way to see it, using texts as inputs to the CNN: In this example, $x_i$ corresponds to the $i^{th}$ word. The kernel's width is 3: the larger its width, the more words it "sees" at once, which allows for more expressive power. Smaller: risk of underfit; larger: risk of overfit. One needs to choose a size between too small and too large by simply trying. There is no method of finding the optimal number: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. To choose the kernel size, one intuition: ask yourself how much contiguous data you would like to use if you had to engineer features. For example, if you use CNN for sentiment analysis and think that typically 3-grams or below are enough to capture the sentiment, then a kernel with or 3 would be a good choice.
