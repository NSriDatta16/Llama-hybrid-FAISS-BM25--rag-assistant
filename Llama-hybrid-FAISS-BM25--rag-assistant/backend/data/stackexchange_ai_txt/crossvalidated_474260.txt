[site]: crossvalidated
[post_id]: 474260
[parent_id]: 474250
[tags]: 
The reason for doing power calculations is to determine ahead of time how large a sample ( $n$ ) would be needed to detect a particular difference, which we can call effect size. From Power Analysis, Statistical Significance, & Effect Size , one reads (at the end of it) "How do I estimate effect size for calculating power? Because effect size can only be calculated [ sic , estimated] after you collect data from program participants, you will have to use an estimate for the power analysis. Common practice is to use a value of 0.5 as it indicates a moderate to large difference." After collecting data, one can calculate what effect size is (should have been) detectable. This becomes important when no significant effect is detected in the experiment. Because that information is missing in the question, i.e., Was the result of the difference A) significant or B) not significant? If not significant, then what difference would have been detected becomes an issue. For example, suppose that the difference between two different measurement methods of renal clearance is 0 where the average value for each is 454 ml/min. Does this mean that those methods are the same? Hardly, and this is a common error that experimenters make, to take a not significant result and claim significance. Here is an example from the literature , "To justify their claim of 100% renal clearance at 48 h, Pentikainen et al set total clearance equal to renal clearance. However, they did not substantiate that equality numerically. Although their average value of total clearance was only 5.7 ml/min greater than renal clearance, the 95% confidence interval for this difference is from âˆ’173 to 184 ml/min, such that their insignificant difference in clearance by Student's $t$ test did not exclude a huge potential range of clinically significant difference." So does this answer the question, or, what is the question, anyway?
