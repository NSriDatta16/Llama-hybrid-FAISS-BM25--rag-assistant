[site]: crossvalidated
[post_id]: 207297
[parent_id]: 
[tags]: 
Understanding the approach behind variable importance returned with Xgboost method in R package caret

I recently implemented the R package caret, for a binary categorical outcome regarding a transcriptomic microarray dataset. As i used the method from the xgboost package(method="xgbtree"), then i used the varImp() function, in order to get the variable importance of my predictors. But, i noticed, that only a subset of my features is returned !!Thus, i would like to ask if anyone if familiar with the notion behind xgboost: that is, has a similar logic to gradient stochastic boosting ? That is sum the relative importances for each feature regarding the total boosting iterations ? Moreover, it returns only a subset of my features, meaning that only these were actually used in the final model after implementing the train() function ?
