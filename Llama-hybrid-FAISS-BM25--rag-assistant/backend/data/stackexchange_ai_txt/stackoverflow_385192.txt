[site]: stackoverflow
[post_id]: 385192
[parent_id]: 385069
[tags]: 
Added comments: I know you can't completely protect something that a normal user should be able to see. I've been on both sides of the problem :) From a developer side what do you think is best ratio of time spent versus protected cases? I'd guess some simple user-agent checks would remove half or more of the potential crawlers, and I know you can spend months developing to protect from the last 1% Again, from a service provider point of view I'm also interested that one user (crawler) doesn't consume cpu/bandwidth for others so any good bandwidth/request limiters you can point out? response to comment: Platform specifications: Application based on JBoss Seam running on JBoss AS. However there is an apache2 in front of it. (running on linux)
