[site]: crossvalidated
[post_id]: 639926
[parent_id]: 639823
[tags]: 
As Cliff AB's answer noted, we need to distinguish between survival (a.k.a. time-to-event) data and specific models for that data (like Cox-PH, accelerated failure time, first passage models you mentioned and many others). First consider a simple analogy: we have unbounded continous measurements and we are considering whether to model it with a normal, student-t or Gumbel distribution. The difference is that each makes different assumptions about the data. Similarly, different models for time-to-event data make different assumptions about the underlying process. They also provide different interpretation of the data and analysis results. As any other model, they will perform well when their assumptions are met and poorly if they are grossly violated. Additionally, simpler/constrained models will tend to provide more precise estimates than complex/flexible models using the same amount of data. To be specific, the first passage time model you mention is likely to be appealing if a convincing case can be made that there indeed is an underlying Wiener process. It will be particularly useful if we know the predictors act on the drift of the model, because this will provide a nice interpretation of model coefficients. So e.g. modelling time to a stock hitting a value? Plausible. Time it takes a cell to complete a cell cycle? Maybe. Modelling a neurodegenerative disease? Unlikely as we know the underlying process is monotonous and thus not Wiener. More generally the independent increments assumption is going to be hard to justify for many real world processes as many processes have some form of momentum/memory/inner state. The biggest possible advantage is that this model is quite restricted so if it is roughly correct, it will make good use of your data. The biggest disadvantage is that it is quite restricted and will thus be misleading/overconfident when a more flexible model is needed. By contrast, Cox-PH model assumes that there is a fixed baseline hazard and any predictors act additively on the log-hazard. I don't think this has a very good direct physical interpretation, but the model is quite flexible and highly mathematically appealing and computationally tractable. It also naturally allows for predictors that change over time, multiple event types and other extensions that are hard to square with both first passage and accelerated failure time models. It is true that the classical version of Cox-PH doesn't let you make direct predictions for time to event, but that can be ameliorated. One can restrict the class of baseline hazard functions to a semi-parametric form (e.g. a penalized spline for the log of baseline hazard) and then full predictions are possible. Indeed this is what Bayesian implementations of Cox-PH do. One reason Cox-PH is often used is that the loss of power/precision due to its flexibility tends to be small for reasonably big datasets while the bias from an overly restricted model never goes away. To some extent, you might be able to choose a good model based on data (e.g. via tests rejecting specific distributional forms, residual plots, comparing performance in cross-validation or posterior predictive check in the Bayesian context) but that will always be limited, so understanding how your data was collected and what it represents is crucial.
