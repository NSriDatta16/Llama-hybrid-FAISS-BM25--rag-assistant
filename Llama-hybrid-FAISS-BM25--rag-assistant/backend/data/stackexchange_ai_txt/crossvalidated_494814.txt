[site]: crossvalidated
[post_id]: 494814
[parent_id]: 493211
[tags]: 
Short answer: There is no explicit way of proving causality. All claims of causality must be logically derived, i.e. through common sense (theory). Imagine having an operator (like correlation) which would return causality or non-causality between variables: you would be able to perfectly identify the sources and relations of anything in the universe (e.g. what/who would an interest rise have an impact on; which chemical would cure cancer etc.). Clearly, this is idealistic. All conclusions of causality are made through (smart) inferences from observations. Long answer : The question of which variables cause another is a philosophical one, in the sense that it must be logically determined. For me, the clearest way to see this is through the 2 classical examples of a controlled vs non-controlled experiment. I will go through these while emphasizing how much is statistics and how much is common sense (logic). 1. Controlled experiment: fertilizer Assume you have an agricultural field divided into parcels (squares). There are parcels on which crops $(y)$ grow with and without sunlight $(X_1)$ , with and without good nutrients $(X_2)$ . We wish to see if a certain fertilizer ( $X_3$ ) has an impact or not on the crop yield $y$ . Let the DGP be: $y_i = \beta_0+\beta_1 X_{1i}+\beta_2 X_{2i}+\beta_3 X_{3i} +\varepsilon_i$ . Here $\varepsilon_i$ represents the inherent randomness of the process, i.e. the randomness that we would have in predicting crop yield, even if this true DGP were known. Exogeneity: [skip if clear] The strong exogeneity assumption $E[\varepsilon_i|\textbf{X}]=0$ that you mention is needed in order for the coefficients estimated by OLS $\hat\beta$ to be unbiased (not causal). If $E[\varepsilon_i|\textbf{X}]=c$ where $c$ is any constant, all $\hat{\beta_j}$ except for the intercept $\hat{\beta_0}$ are still unbiased. Since we are interested in $\beta_3$ this is sufficient. (Side note: other weaker assumptions such as weak exogeneity and orthogonality between $X$ and $\varepsilon$ are sufficient for unbiasedness.) Saying that $E[X|Z]=c$ for any 2 random variables $X$ and $Z$ means that $X$ is not systematically dependent in the mean on $Z$ , i.e. if I take the mean ( $\to\infty$ ) of $X$ , for any pair of $(X,Z)$ I will get (approx.) the same value each time, so knowing $Z$ does not help at all in predicting the mean of $X$ (e.g. $E[X|Z=10]=E[X|Z=10000]=E[X|Z=-5]=E[X]=c$ ) Why is this interesting? Remember, we want to know if the fertilizer $X_3$ has an impact or not ( $\beta_3=0?$ ) on the crop yield $y$ . By spraying fertilizer on random parcels, we implicitly "force" exogeneity of $X_3$ compared to all other regressors. How? Well, if we randomly spray fertilizer on a parcel, no matter if it has sunlight or not, if it has good nutrients or not and if we then take the mean value of fertilizer for sunny parcels, it will be the same as the mean value for non-sunny parcels. Same with nutrient-rich parcels. E.g: the results of the table below hold approx. for large numbers. It makes sense after all that, if $X_3$ is independent of $X_1$ , its mean should not change (significantly) as $X_1$ changes. So, in other words $X_3$ is exogenous wrt $X_1,X_2$ , i.e. $E[X_3|X_1,X_2]=c$ . This means that effectively, if we want to estimate $\beta_3$ unbiasedly, we don't need $X_1,X_2$ . Hence these two variables (sun, nutrients) can be treated as randomness and incorporated into the noise term, giving the regression: $y_i = \beta_0 + \beta_3 X_{3i} + \epsilon_i$ , where $\epsilon_i = \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i$ . Hence, the noise term can also be interpreted as a collection of all other variables that influence the response $y$ , but not in a systematic fashion in the mean. (Note that $\hat\beta_0$ is biased; further note that exogeneity is weaker than independence, since the variables could be related in a higher moment instead of the mean, such as the variance, but exogeneity would still hold, see heteroskedasticity). Causality: Now where does causality come into play? So far we have only shown that randomly distributing fertilizer on better or worse parcels lets us look at crop yield and fertilizer alone, without taking into account the other variables (sun, nutrients), i.e. "forcing" exogeneity of fertilizer and thus all other variables into the noise term. Causality itself was and will not be proven. However, if $\hat\beta_3$ turns out to be significant, we can logically conclude that, since the randomization of fertilizer effectively "de-relates" it from all other variables (in the mean), it must have an impact on crop yield, since all other variables have no systematic impact in this setting. In other words: 1) we used exogeneity to statistically prove that this is the condition we need for unbiased estimators (for OLS); 2) we used randomization to get this exogeneity and get rid of other uninteresting variables; 3) we logically concluded that, since there is a positive relation, it must be a causal one. Notice that 3) is just a common sense conclusion, no statistics involved as in 1) or 2). It could theoretically be wrong, since e.g. it could have been that the fertilizer was actually a 'placebo' ( $\beta_3=0$ ) but was distributed only on the sunny and nutrient-rich parcels by pure chance. Then the regression would wrongly show a significant coefficient because the fertilizer would get all the credit from the good parcels, when in fact it does nothing. However, with a large number of parcels this is so unlikely that it is very reasonable to conclude causality. 2. Uncontrolled experiment: wage and education [I will eventually (?) return with an edit to continue here later; topics to be addressed OVB,Granger-causality and instantaneous causality in VAR processes] This question is precisely the reason why I started learning statistics/data science - shrinking the real world into a model. Truth/ common sense/ logic are the essence. Great question.
