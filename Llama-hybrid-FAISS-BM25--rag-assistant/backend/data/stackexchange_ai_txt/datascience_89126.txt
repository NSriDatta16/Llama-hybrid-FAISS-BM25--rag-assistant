[site]: datascience
[post_id]: 89126
[parent_id]: 89026
[tags]: 
The teacher/student model approach (at least as I understand it) could be used. It is normally used to replace all layers, but there is nothing stopping you applying it to a subset of layers. First you create training data, by running your training data through the (trained) network, and recording the inputs to layer i, as your new training data ( x ) and the outputs from layer j as your desired answer ( y ). You now simply (!) want to design a model that takes x and produces y . Ideally using fewer weights than layers i to j currently use. If those layers were a fully-connected NN, then this is the idea behind autoencoders. If a Transformer then try with one attention + FCNN layer, and see how low the error will go. If it flatlines at an unacceptable error rate, maybe you will need to try two layers. If a CNN, then I am less sure it will work; my hunch would be would have better success keeping the same number of layers and convolution sizes, and just using fewer features in each layer. Anyway, when you have trained this student model, you can plug it in to your original model to replace layers i to j. (Then maybe run some fine-tuning training.) Note: if your layers have residual connections, you will have to think carefully about how they handled. Taking the i-1th output and feeding it to the j+1th layer, when it previously expected the output of the jth layer, may go badly. Especially if the dimension of the data is different.
