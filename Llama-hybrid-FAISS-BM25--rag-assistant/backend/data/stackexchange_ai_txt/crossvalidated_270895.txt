[site]: crossvalidated
[post_id]: 270895
[parent_id]: 
[tags]: 
Transition matrix for continuous state-action space reinforcement learning

Imagine a rigid solid, just subjected to an external upwards effort and gravity. I am trying to calculate by RL the effort needed along time so that the solid reaches and maintains at a certain height. I am using: - heights of each moment as states - three possible actions (positive fixed effort, negative fixed effort and 0 effort). My doubt consists in how to create the transition matrix. I am following a value iteration policy, and it is defined as: the probability of getting from a state s to s' with an action a. If I'm using three different actions, do I need to create three different matrix? I'm a bit lost.
