[site]: crossvalidated
[post_id]: 526611
[parent_id]: 
[tags]: 
Trained model returns constant value independent of input

I am trying to train a simple model on data that has seven inputs and one output. My dataframe basically looks like this: x1 y1 x2 y2 x3 y3 Re mean Cd 0 15 21 15 15 21 21 200 0.007524 1 15 21 15 15 21 21 400 0.007722 2 15 21 15 15 21 21 600 0.007892 3 15 21 15 15 21 23 200 0.010272 4 15 21 15 15 21 23 400 0.009638 ... ... ... ... ... ... ... ... ... From there I want to train a very simple model like this: # Load the dataframe that was saved in read_files.py df = pd.read_csv("tmp/df.csv",index_col=0) # Split data into train and test sets train, test = train_test_split(df, test_size=0.2) # Create a shallow copy of the data train_features = train.copy() test_features = test.copy() # Remove the "mean Cd" column from the features and use it as label train_labels = train_features.pop('mean Cd') test_labels = test_features.pop('mean Cd') # Only one hidden layer will be used # 4 nodes are used because of the rule of thumb: # Hidden layer nodes = (No. of inputs + No. of outputs) / 2 model = Sequential( [ layers.Dense(4, activation="sigmoid", name="hidden_layer"), layers.Dense(1, name="output_layer"), ] ) # Compile the model to static graph model.compile(optimizer="Adam", loss="mse") # Train the model using 80% of the training data model.fit( train_features, train_labels, epochs=100, # Calculate validation results on 20% of the training data validation_split = 0.2) # Test the model on up to now unused test data # Loss on test data should be close to loss on # on validation data during training, otherwise # the model is overfitted print("Test:") model.evaluate(test_features,test_labels) # Save the model for later use model.save('tmp/model') At first it looks like the training actually went well. The loss is going down nicely and I have similar loss on the test dataset which I did not use for the training. However if I now test my model by just plugging in different inputs, the output only changes slightly, although that was not the case for the original data. E.g. the first 5 lines of the data get predicted like this: model.predict(x=[[15,21,15,15,21,21,200]]) array([[0.00913189]], dtype=float32) model.predict(x=[[15,21,15,15,21,21,400]]) array([[0.00913231]], dtype=float32) model.predict(x=[[15,21,15,15,21,21,600]]) array([[0.00913231]], dtype=float32) model.predict(x=[[15,21,15,15,21,23,200]]) array([[0.00913177]], dtype=float32) model.predict(x=[[15,21,15,15,21,23,400]]) array([[0.00913231]], dtype=float32) which is far away of the range of results that can be observed in the original data from 0.0075 to 0.010. Why is my model not sensitive enough regarding the inputs. How can I get better model behavior? EDIT: For the better understanding of the data I made a small Dash app. Now if I change one parameter, e.g. like here x3 I can observe, that increasing x3 reduces the drag for the given combination of all other parameters. So there should be a relationship in the data, in my opinion. However it is definitely highly non-linear as Re might switch its influence when I change e.g. y3 : I also tried to use more nodes (16 in each layer) and two layers, however the results then look something like this for all parameter combinations: What else can I do to correctly reflect the behavior of the original data? EDIT 2: I added 6 additional inputs in the form of df["x1Re"] = (df["x1"]-np.mean(df["x1"]))*(df["Re"]-np.mean(df["Re"])) for all parameters x1 , x2 and so on. With two hidden layers and 16 nodes in each layer, the estimation now looks like this: It also looks seem to look like this for all input combinations, so it is not sensitive to the input.
