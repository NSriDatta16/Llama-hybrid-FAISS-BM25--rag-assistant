[site]: datascience
[post_id]: 97855
[parent_id]: 97833
[tags]: 
The problem you describe looks very close to standard information retrieval: given a predefined set of documents $D$ and an input string $s$ , find the most similar document $d\in D$ to $s$ (alternatively find the top $n$ documents $d$ most similar to $s$ ). The approach you describe is good, except that in general the input string $s$ is not part of the TFIDF matrix: indeed the full set of predefined documents is encoded as a TFIDF matrix, but then any input string $s$ is simply encoded using the same vocabulary and weights. The advantage is that you don't need to recompute the matrix for every different string $s$ (the matrix can be pre-computed and stored for efficiency reasons). There is no disadvantage because any word in $s$ which is not in the vocabulary cannot be used in the calculation of the similarity anyway. Indeed the standard method for matching or ranking the documents with respect to $s$ is to calculate a similarity score (e.g. cosine) for every $d$ against $s$ , and then pick the highest similarity score.
