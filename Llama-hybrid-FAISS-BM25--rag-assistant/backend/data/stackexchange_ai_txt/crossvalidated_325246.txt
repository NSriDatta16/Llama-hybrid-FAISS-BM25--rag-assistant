[site]: crossvalidated
[post_id]: 325246
[parent_id]: 181183
[tags]: 
Interactions are needed explicitly in regression models because the formula does not include any interactions per se. More precisely, a regression model will always be linear in its input, whereas an interaction $X_i * X_j$ is a nonlinear combination of the features. The simplest way to see this is through the XOR-Problem, a regression model without any interactions cannot solve this, as it requires a nonlinear combination. KNNs and SVMs on the other hand (and many other models also) are universal function approximators. This means, that they cannot only combine their inputs in a linear fashion, but also in any possible non-linear way. That is given enough layers or a suitable kernel, they can basically "create" their own interactions, exactly as they need them. If you know or expect specific interactions to be important, though, you can still use them as an input to guide the models in the right direction. Similarly, tree-based models can be interpreted as only consisting of interactions. Basically, a split in a tree-based model creates a specific interaction with all previous variables. So for deciding which interactions to use, for sufficiently "high-power" models (i.e. those which are universal function approximators), you don't need them and you can let the model do its own magic. For other models it depends. There are some techniques available to guide the decision, like CHAID or step-wise regression. CHAID also works with a large number of features, for step-wise regression it may get lost in the number of possible interactions. Given that if you have $N$ features, there are $2^N$ possible interactions (counting not only two-way but also higher order interactions).
