[site]: crossvalidated
[post_id]: 234797
[parent_id]: 
[tags]: 
Bayesian online changepoint detection (marginal predictive distribution)

I am reading the Bayesian online changepoint detection paper by Adams and MacKay ( link ). The authors start by writing the marginal predictive distribution: $$ P(x_{t+1} | \textbf{x}_{1:t}) = \sum_{r_t} P(x_{t+1} | r_t, \textbf{x}_t^{(r)}) P(r_t | \textbf{x}_{1:t}) \qquad \qquad (1) $$ where $x_t$ is the observation at time $t$; $\textbf{x}_{1:t}$ denotes the set of observation until time $t$; $r_t \in \mathbb{N}$ is the current runlength (time since last changepoint, can be 0); and $\textbf{x}_t^{(r)}$ is the set of observations associated with run $r_t$. Eq. 1 is formally correct (see the reply below by @JuhoKokkala), but my understanding is that if you want to actually make a prediction about $x_{t+1}$ you'd need to expand it as follows: $$ P(x_{t+1} | \textbf{x}_{1:t}) = \sum_{r_t, r_{t+1}} P(x_{t+1} | r_{t+1}, \textbf{x}_t^{(r)}) P(r_t | \textbf{x}_{1:t}) P(r_{t+1} | r_t) \qquad (1\text{b}) $$ My reasoning is that there might well be a changepoint at (future) time $t+1$, but the posterior $P(r_t | \textbf{x}_{1:t})$ only covers until $t$. The point is, the authors in the paper make us of Eq. 1 as is (see Eqs. 3 and 11 in the paper), and not 1b. So, they seemingly ignore the possibility of a changepoint at time $t+1$ when predicting $x_{t+1}$ from data available at time $t$. At the beginning of Section 2 they say en passant We assume that we can compute the predictive distribution [for $x_{t+1}$] conditional on a given run length $r_t$. which perhaps is where the trick is. But in general, this predictive distribution should look something like Eq. 1b; which is not what they do (Eq. 11). So, I am not sure I understand what's going on. Perhaps there is something funny going on with the notation. Reference Adams, R. P., & MacKay, D. J. (2007). Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742.
