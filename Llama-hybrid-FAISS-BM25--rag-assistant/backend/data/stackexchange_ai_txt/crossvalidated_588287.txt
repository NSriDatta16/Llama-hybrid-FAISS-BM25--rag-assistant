[site]: crossvalidated
[post_id]: 588287
[parent_id]: 521006
[tags]: 
You are just one step away! Following Lei Mao's derivation $\eqref{eq1}$ , we can replace the second term in the right hand with $\eqref{eq2}$ . $$L^{'} = \sum_{i=1}^n \left[ (1-\epsilon)H_i(p, q_\theta) + \epsilon H_i(u, q_\theta)\right] \tag{1}\label{eq1}$$ $$H(u, q) = D_{KL}(u || q) + H(u) \tag{2} \label{eq2}$$ Voil√†, we have $\eqref{eq3}$ in which the term $H(u)$ is a constant during the optimization when $u$ is a uniform distribution. Therefore, we can neglect it. Eventually, $\eqref{eq4}$ is what the paper Regularizing Neural Networks By Penalizing Confident Output Distributions actually refers to. $$L^{'} = \sum_{i=1}^n \left[ (1-\epsilon)H_i(p, q_\theta) + \epsilon D_{KL}(u || q_\theta) + \epsilon H(u) \right] \tag{3}\label{eq3}$$ $$L^{'} = \sum_{i=1}^n \left[ (1-\epsilon)H_i(p, q_\theta) + \epsilon D_{KL}(u || q_\theta) \right] \tag{4}\label{eq4}$$ However, the equation in Section 3.2 of the paper 1 omitted the smoothing factor for unknown reasons. It would be more understandable if reformulated as $\eqref{eq5}$ which is quite close to $\eqref{eq4}$ . Hope this clarifies your confusion ;) $$\begin{equation}\begin{aligned} L(\theta) &= - \sum \left[ \text{log} q_{\theta}(\mathbf{y} | \mathbf{x}) - D_{KL}(u || q_\theta (\mathbf{y} | \mathbf{x})) \right] \\ &= \sum \left[ - \text{log} q_{\theta}(\mathbf{y} | \mathbf{x}) + D_{KL}(u || q_\theta (\mathbf{y} | \mathbf{x})) \right] \\ &= \sum \left[ H(p, q_\theta(\mathbf{y} | \mathbf{x})) + D_{KL}(u || q_\theta (\mathbf{y} | \mathbf{x})) \right] \end{aligned}\end{equation}\tag{5}\label{eq5}$$ Last but not least, back to your final question, both of your statements hold when the noise distribution $u$ is a uniform distribution.
