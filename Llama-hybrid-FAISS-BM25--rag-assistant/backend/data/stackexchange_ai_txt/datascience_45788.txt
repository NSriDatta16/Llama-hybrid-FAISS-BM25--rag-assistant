[site]: datascience
[post_id]: 45788
[parent_id]: 45762
[tags]: 
Training a network is analogous to fitting a function to some scalar data. If the data is linear, fitting a linear function is appropriate and will work well. In the case of deep learning, the data is rich and non-linear, so we apply non-linear activation functions to make the model more complex. Another reason we use activation functions on intermediate layers is to keep the weight and output values close to 0 and "kind of" Gaussian, for optimization reasons.
