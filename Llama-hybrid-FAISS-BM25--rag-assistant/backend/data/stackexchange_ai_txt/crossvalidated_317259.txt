[site]: crossvalidated
[post_id]: 317259
[parent_id]: 317239
[tags]: 
The problem is not as easy as it seems. When rolling a dice or flipping a coin: the parameter is $p$: theoretical probability to get a 6 you known $X$: the number of 6s on $N$ trials you want to estimate $p$. The distribution of $X$ is a binomial distribution $B(N,p)$. What is the best estimator for $p$? The notion of the best estimator is easier in the Bayesian approach: the estimator with least expected squared error is the conditional mean $\hat p=E(p|X)$ . With a uniform prior it is $\hat p=\frac{X+1}{N+2}$. See: https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair . You can calculate this for the prior that you want. With a Beta prior with parameters $\alpha,\beta$ it is $\hat p=\frac{X+\alpha}{N+\alpha+\beta}$, which is equivalent to the frequentist estimator having seen $\alpha$ 6s on $\alpha+\beta$ trials before. Your simple solution is the special case $\alpha=0$. In the frequentist approach, the notion of the best estimator is nothing simple. The estimator used is usually $\hat p=\frac{X}{N}$. You can compute confidence intervals. See https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval Now, from a machine learning point of view, I recommend analysing the sentence: The main application I am looking for is to not overestimate the probability of certain events that may only be due to the small sample size. How do you quantify the consequences of overestimation? This kind of analysis leads to find the best estimator for your specific purpose. You can look at it for example: https://en.wikipedia.org/wiki/Multi-armed_bandit .
