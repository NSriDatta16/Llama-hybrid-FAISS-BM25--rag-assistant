[site]: datascience
[post_id]: 109179
[parent_id]: 30876
[tags]: 
The evaluation metric depends on the goals of the project. Which outcomes are better or which outcomes worse? Some projects value precision over recall and other projects value recall over precision. After you have clarity on project goals, pick a single metric to provide a consistent scorecard when comparing different algorithms and hyperparameters combinations. One common evaluation metric for multi-class classification is F-score . F-score has a Î² hyperparameter which weights recall and precision differently. You will have to choose between micro-averaging (biased by class frequency) or macro-averaging (taking all classes as equally important). For macro-averaging, two different formulas can be used: The F-score of (arithmetic) class-wise precision and recall means. The arithmetic mean of class-wise F-scores (often more desirable).
