[site]: crossvalidated
[post_id]: 569392
[parent_id]: 556070
[tags]: 
I want to add to the accepted answer and the question itself. I think there is a confusion as to how GRU solves the vanishing gradient issue (title of the question but, not the actual question itself) when z=r=0 which makes ∂hi/∂hi−1 = 0 and therefore, ∂Lt/∂Uz = 0. From the backward pass equations in the given links, it is in fact true that ∂Lt/∂Uz = 0 when z=r=0 because ∂hi/∂hi−1 = 0. One thing to keep in mind that this does not necessarily mean that the entire gradient vanishes because the entire gradient is not ∂Lt/∂Uz but, ∂L/∂Uz which is the sum of all ∂Lt/∂Uz at different time steps. You need to repeat the above equation for all ∂Lt/∂Uz at multiple time steps because each time step has its own loss (error). This is true if you have a label y (ground truth) for each time step and the error is some form of a loss function that involves this label y. If you only have a label at the very end of the sequence (a classification type problem), then I believe you only have a single error ∂Lt/∂Uz for that sequence (or time series) hence the gradient in fact vanishes if z=r=0 at any point in time but, your hope is that you can find a path (set of weights) where z and r are not equal to zero (at least not both r and z) for the entire sequence so information from previous time steps keep moving forward but, that might be very difficult if you have a very long sequence and the longer the sequence is the harder it becomes to find these set of weights that avoids the vanishing gradient issue. So, technically from what I observed so far, you can run into the vanishing gradient issues in GRUs depending on the weights you find during training and once you run in to the vanishing gradient issue the training ultimately stops because the weights no longer change or you get stuck at a local minima and no matter how many iterations you run, it won't help unless of course, you are using some fancy optimizer that let's you escape the local minima (which I don't know about). Also, what if such set of weights do not exist for the type of problem you are solving (no one can guarantee that it does exist - at least not without running some serious analysis). You should also not ignore the context of the problem: whether you are solving a classification type problem (many-to-one) or regression type problem (many-to-many) and watch your gradients closely. I also want to share this wonderful and intuitive paper which explains the derivation of the GRU gradients via BPTT and when & why the gradients vanish or explode (mostly in the context of gating mechanisms): Rehmer, A., & Kroll, A. (2020). On the vanishing and exploding gradient problem in gated recurrent units. IFAC-PapersOnLine, 53(2), 1243–1248. https://doi.org/10.1016/j.ifacol.2020.12.1342 I'm not sure if you can access the paper directly but, there should be a version of the paper available by the authors online which you can read. I did not want to provide a link to avoid future issues since the paper may not be open access. I hope this answer resolves some of the confusion and helps someone else that come across this post as well.
