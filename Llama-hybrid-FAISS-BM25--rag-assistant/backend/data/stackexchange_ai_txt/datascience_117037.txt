[site]: datascience
[post_id]: 117037
[parent_id]: 
[tags]: 
Logistic Regression Modeling & Interpretation

I'm building a logistic regression model to predict the credit risk of lending company customers. I'm using dataset from kaggle : https://www.kaggle.com/datasets/ranadeep/credit-risk-dataset/code First question: When i'm doing EDA to gain some business insights, i found some insight using interest rate feature. But when im going through preprocessing step i found that interest rate was a redundant feature with grade feature. Grade actually is ordinal categorical feature ('A','B','C','D','E') and im using label encoding to make the value range from 0-6. So to make sure which feature that i will decided to drop, im checking each feature correlation to the TARGET ( loan_status ). and found out the grade feature have more high correlation to the TARGET compared int_rate feature. And i decided to drop int_rate feature. So it make me confused that i have to drop the feature that i'm using to gain good insight because it was redundant to another feature only to make better performance for machine learning. Is it irony and weird? I need suggestion in this part. Second question : I think i make a good prediction using logistic regression algorithm. This is the confussion matrix : I also want to interpreted the feature on logistic regression that contributed the most to the independent variable using statsmodel. But the problem is i have certain features that have very high coefficient (have coefficient make odds_ratio also high). filtered feature that have pvalue I just think that doesnt make any sense when having very high coefficient, but on the other hand this algorithm make good prediction. So can i say that my model is wrong eventhough the performance of prediction is good? Or i can only use my model for make prediction but not good when i want to interpreted through the feature coefficient?
