[site]: crossvalidated
[post_id]: 393915
[parent_id]: 393909
[tags]: 
It seems as a valid approach, but there is an important caveat -- you need to take precaution to avoid data snooping (that is, leakage, or "using data twice"). Namely, when marking a sales value as an outlier you must do it separately for training and testing datasets. If not, the predictive performance of your method will be inflated since the outliers were determined on the whole data set (training outliers are based also on values from the test set). More precisely, I'd do the following: (1) divide the data in training and test sets. (2) on the training set use the distribution of sales values to mark each value as an outlier (for example check Tukey's method). (3) do the same as in (2) on the test set (using only test data to form a distribution and decide outliers on test set) Then, you can repeat (1)-(3) a number of times to evaluate average performance on the test sets.
