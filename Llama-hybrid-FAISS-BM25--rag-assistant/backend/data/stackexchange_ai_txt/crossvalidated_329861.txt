[site]: crossvalidated
[post_id]: 329861
[parent_id]: 
[tags]: 
what happens when a model is having more parameters than training samples

In a simple neural network, say, for example, the number of parameters is kept small compared to number of samples available for training and this perhaps forces the model to learn the patterns in the data. Right? My question is that what repercussions could we have in a scenario where the number of parameters in a model are more than the number of training instances available ? Can such a model lead to over-fit? What effect can those extra parameters bring about in the model performance? Kindly shed some light on this. I believe that it is only the data representation (number of hidden layers, number of neurons in each layer etc.) that governs the number of parameters in the model. Is my understanding correct ?
