[site]: crossvalidated
[post_id]: 369484
[parent_id]: 
[tags]: 
Multi-agent actor-critic MADDPG algorithm confusion

I am trying to understand the paper from openAI called Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments In the paper, they mention that they combat the problem of environment non-stationarity by sampling from sub-policies: I am confused about: (1) how subsampling would resolve the non-stationarity problem and (2) why would the individual agents have more than one possible (sub) policy - shouldn't there be a single optimum policy for each agent?
