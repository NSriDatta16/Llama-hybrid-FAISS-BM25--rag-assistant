[site]: crossvalidated
[post_id]: 65749
[parent_id]: 65744
[tags]: 
Part of the difficulty might result from the conflation of the Fisher and Neyman-Pearson approaches to testing. As far as I understand the problem (and I haven't read the original sources and certainly don't claim to be an expert), interpretation of the p -value and the “data more extreme” and “or something unlikely happened” language comes from Fisher but he has no concept of power and not much to say about experiments that fail to reject the null hypothesis. On the other hand, in the Neyman-Pearson framework, interpretation is strictly about long-run frequencies and there is really nothing to be said about a single experiment. Under that view, the only thing you can say is that if the effect size were actually d , the test would lead you to accept the null hypothesis at most 5% of the time (that's $\beta$ or 1 - power). Both $\alpha$ and $\beta$ must be set beforehand. Your description of a significant result feels quite Fisherian, even if it mentions $\alpha$. This would suggest that it is impossible to create a rigorous analogous statement about non-significant results as it requires the concepts of power and type II error ($\beta$), which only completely make sense in the Neyman-Pearson framework. It's precisely because all this is quite frustrating that it's very difficult to avoid any pseudo-Bayesian interpretation (optionally prefaced with a “that's not quite right” if you want to show some sophistication but still don't know what else to say). But like I said, I am not sure that I fully grasp all the issues here and I am happy to be corrected! Some literature on this (not sure how “authoritative” you would consider it): Hubbard, R. (2004). Alphabet soup: Blurring the distinctions between p's and $\alpha$'s in psychological research. Theory and Psychology, 14 (3), 295-327. Hubbard, R., & Lindsay, R.M. (2008). Why P values are not a useful measure of evidence in statistical significance testing. Theory and Psychology, 18 (1), 69-88.
