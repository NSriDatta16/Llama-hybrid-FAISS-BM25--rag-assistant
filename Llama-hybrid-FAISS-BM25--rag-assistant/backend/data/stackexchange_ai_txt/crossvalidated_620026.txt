[site]: crossvalidated
[post_id]: 620026
[parent_id]: 371650
[tags]: 
Class imbalance is almost certainly not a problem for proper statistical methods. Therefore, throwing away precious data just to fix a non-problem seems like the worst approach one can take. Additionally, by throwing away many instances of the majority category, you alter the prior probability of membership in that category, which will affect the posterior (predicted) probability through Bayes' theorem. It is possible to fix this , but why would you throw away precious data to solve a non-problem just to have to fix a real problem later that only exists because you tried to solve a non-problem? From King and Zeng (2001), it can make sense to downsample at the stage of collecting data. If you have too large of a data set to run it on your hardware, it might make sense to apply ideas like those of King and Zeng (2001), giving your hardware a break without discarding especially precious observations from the minority category (and then calibrating the predictions to account for this). However, getting rid of precious data just to solve a non-problem seems like a poor approach. REFERENCE King, Gary, and Langche Zeng. "Logistic regression in rare events data." Political analysis 9.2 (2001): 137-163.
