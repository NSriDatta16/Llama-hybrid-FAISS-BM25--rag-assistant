[site]: crossvalidated
[post_id]: 375717
[parent_id]: 
[tags]: 
How to quantify the uncertainty in a single value obtained from a large number of simulations

Im a physics student working on a project in which I have to simulate a machine that would order a bunch of molecules according to their mass. I want to get a quantitative measure of how well the simulated machine is ordering the molecules as I tweak various parameters, and I'd also like to know the uncertainty of that quantity. The way I'm quantifying the error rate is by counting the number of nearest-neighbor swaps needed to get the ordered list of molecules returned by the machine to match the correctly ordered list. I am interested specifically in a set of 18 molecules, and the number of swaps needed to correct the machine's output is usually around 0-2. So I run the simulation ~100 times for a given set of parameters and find the average error rate. The problem comes when I try to calculate the uncertainty in the rate. Here's an example to make clear what I am talking about. If I run 5 sets of 100 simulations I get error rates of [7.11%, 8.00%, 7.78%, 7.22%, 7.56%], the standard deviation of this list is 0.37%, but if I look within one of those sets of 100 simulations it looks something like [0%, 5.56%, 0%, 11.11%, 11.11%, 5.56%, 5.56%, 5.56%, 5.56%...] with a mean of 7.11% and a standard deviation of 6.09%. So which is the uncertainty in my measurement, 0.37% or 6.09%? It seems like it's obviously 0.37%, but I feel like I should be able to obtain that number somehow just from a single set of 100 simulations.
