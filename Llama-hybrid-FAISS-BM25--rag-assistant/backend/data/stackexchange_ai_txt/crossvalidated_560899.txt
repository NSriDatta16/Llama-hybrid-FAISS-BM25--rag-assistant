[site]: crossvalidated
[post_id]: 560899
[parent_id]: 
[tags]: 
Calibration of an agent based epidemic model on an HPC

I am working on HIV modeling with the EpiModel R package and we are trying to smooth our calibration process by efficiently using the Slurm clusters we have available. For reference, this is one of the last projects we made . Most of my parameters govern one target statistic, like the probability of being tested at each time step influencing directly the portion of the infected that are diagnosed. But I also have one case of 3 parameters influencing 3 target statistics: a scaling factor for the transmission probability for each of the 3 subpopulations and the HIV prevalence in each subgroup. (Increasing the prevalence in subgroup 1 through parameter 1 will increase the prevalence in the other 3 groups as well as there is contact between the populations). My current method is as follow: Test a range of values for the parameters Run ~200 simulations per value Manually assess the best subrange (by looking at the median outcome for each parameter value) shrink the range rinse and repeat until good enough One of the issues is the noise in our outcomes. That's why we run 200 simulations per parameter value. I can automate my process quite easily but I was wondering if there was a good method already existing for this kind of problem. I have looked at different optimization methods but most are limited to single response functions and do not really benefit from parallelization. Thanks
