[site]: crossvalidated
[post_id]: 81579
[parent_id]: 
[tags]: 
What is the relationship between graphical models and hierarchical Bayesian models?

I've searched a good bunch of literature but have failed to find an exact distinction between the two. My impression is that in the Machine Learning literature you'll find allusions to hierarchical Bayesian modeling, but in the Statistics literature you'll seldom find allusions to PGMs. Hopefully you guys will be able to allay my confusion. I have a few specific questions, but would be more than happy to simply have someone with experience explain their own intuition to me. (I'm about to start studying this stuff seriously and this is like "paving the way" in my brain, or something to that effect.) Since Hierarchical Bayesian models are also DAGs, can you use the message passing algorithms of PGMs (junction tree etc.) in this context? Bayesian Networks usually represents random variables as conditional probability tables, which could be filled out by counting (Maximum Likelihood). Is it correct to think of Hierarchical Bayes as somehow more computationally flexible versions of this idea, where instead of CPTs you have mathematical functions (parametric probability distributions) that you can query instead? What is the role of hyperparameters/hyperpriors in this context? Is it correct that your inference procedure concerns itself with inferring the hyperparameters after observing the data, and setting these then propagates down the hierarchy in a statistically useful way? (This sounds like you make a kind of clustering model every time you abstract away a node in the hierarchy with a parent). Any literature describing these two concepts would be much appreciated. Thanks so much!
