[site]: crossvalidated
[post_id]: 358786
[parent_id]: 
[tags]: 
Mean or sum of gradients for weight updates in SGD

I am using single observation to compute losses using neural network implementation in PyTorch. I am confused in a small detail of SGD. If I compute loss and do loss.backward() , I am accumulating gradients. If I do this on 100 observations and then run optimizer.step() , should I average out the gradients? This is what I am doing as of now: def compute_loss(training_data): for data in training_data: loss = F.mse_loss(data[0], data[1]) loss.backward() def optimize(sample): optimizer.zero_grad() compute_loss(sample) optimizer.step() Should it be rather: def compute_loss(training_data): for data in training_data: loss = F.mse_loss(data[0], data[1]) loss.backward(torch.Tensor([1.0/len(training_data)]))
