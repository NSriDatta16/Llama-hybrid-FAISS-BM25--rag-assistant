[site]: crossvalidated
[post_id]: 464287
[parent_id]: 455560
[tags]: 
If we learn the log variance or log standard deviation , then the variance would always be positive. This has to be guaranteed with network output. People usually output mean and variance together as a vector. However, mean is unconstrained compared to variance. That's why people use the log of variance/std. You can also learn some parameter $\gamma$ with neural network alongside the mean $\mu$ and then transform the $\gamma$ to $\sigma$ via a softplus function(or smt else like simply applying log), a differentiable approximation to the ReLU function given as: $$ f(x) = \log(1 + exp(x)) $$ All of them are same eventually. Then, to generate the latent variable, just invert the transformation. log is both for training stability and need for log sigma in ELBO. Hope this helps
