[site]: crossvalidated
[post_id]: 514825
[parent_id]: 514118
[tags]: 
Important to know: When you select an action $a$ you actually draw it from the policy $\pi(a|s)$ and the notation is $\pi(s)$ . How is that important ? To compute $V$ you then do the average over all possibles actions (expected value of V), that is why in the first image you see the term $\pi(a|s)$ In the second image you can notice that $p$ is now conditioned on $\pi(s)$ and not $a$ , because you are looking at a specific action that was taken at random from $\pi(a|s)$ , and which is noted $\pi(s)$ The 2 images actually do not serve the same purpose as their titles indicates. Mainly, in the second image one can see that the policy $\pi(a|s)$ is actually unknown (or uniform) at the beginning because this is the algorithm to find it. Finally, the answer: the second image (policy iteration) describes a deterministic policy which simply assign 1 action to 1 state through $\pi(a|s)$ . Thus there is no need for an average because all other terms, the ones that do not correspond to the action assigned to state s, are 0. So the sum is not there, simply because there is no distribution over $a$ . Hope this helps !
