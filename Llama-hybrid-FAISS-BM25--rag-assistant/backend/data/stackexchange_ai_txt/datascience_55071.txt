[site]: datascience
[post_id]: 55071
[parent_id]: 55049
[tags]: 
From http://www.deeplearningbook.org/contents/convnets.html , The only reason to ﬂip the kernel is to obtain the commutative property. While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation. Instead, many neural network libraries implement a related function called the cross-correlation, which is the same as convolution but without ﬂipping the kernel. The discrete cross correlation function with 1-based indexing for $$I = U*V\ and\ K=M*N$$ is given by $$C(x,y)=\sum_{m=1}^M\sum_{n=1}^NI(x+m-1,y+n-1)K(m,n)$$ where $$x\in(U-M+1)\ and\ y\in(V-N+1)$$ When applying 2-D convolutions, many neural network implementations will reduce the size of the output . To retain the original size in output and to retain information at the borders, many practical use cases add a padding of zeros to the input image . Coming back to your specific question, in the case where indices go out of bounds, either on the positive side or on the negative side, (which happens when you try to retain the size of the input in your output), values corresponding to them are usually taken as zeros , which is akin to zero-padding in implementation. Discrete convolutions are actually given by $$(f * g)[n] = \sum_{m=-\infty}^\infty f[n-m] g[m]$$ In practice the limits are made finite because of the same assumption of zero amplitude signal for out of bound indices .
