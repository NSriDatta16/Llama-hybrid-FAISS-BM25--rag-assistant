[site]: crossvalidated
[post_id]: 622541
[parent_id]: 
[tags]: 
Best way or rules of thumbs for evaluating 1d integrals from randomly sampled grid points

I have a 1D domain (let's say the interval $(0,1)$ ) on which I randomly sample $N$ points from the uniform distribution. I have a function $f\colon (0,1) \to \mathbb{R}$ which is integrable. What is the best way to numerically integrate $f$ based on these grid points, given that I want $N$ to be as low as possible? My $f$ comes from a neural network and to evaluate the loss I need to perform the integral. So I want to keep $N$ as low as possible (eg. 1000). I've read about the Monte Carlo integral and the trapezoidal rule. Is there a rule of thumb for which is better given that my grid points are uniformly distributed and are there better ways? I don't know if there is a better way of taking grid points (than uniformly distributed) -- but I'd like to get them randomly because it helps train my model. But maybe there is a different better distribution of points to get in order to give a more accurate integral. Also would your suggestions change if I had a 2d domain like the unit square instead of the unit integral? From what I read, MC is not good for low dimensional problems (but it's very easy to implement in Pytorch). I am aware that there is a methodology to check how many grid points one needs for a good MC integral (which involves finding the std etc), but this seems too messy for my setting.
