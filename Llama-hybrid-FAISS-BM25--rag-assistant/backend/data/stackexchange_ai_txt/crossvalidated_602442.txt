[site]: crossvalidated
[post_id]: 602442
[parent_id]: 
[tags]: 
Randomness in a Gaussian Process

I'm somewhat confused about the standard setup of Gaussian Processes in the machine learning literature. In the classic setup, we have a dataset $D=\{\mathbf{x}, y_i \}_{i=1}^N$ and we have that $y=f(\mathbf{x}_i)$ (which may or may not be corrupted by noise). I am confused exactly as to whether or not $\mathbf{x}$ is a random variable or not. It's usually stated that $\mathcal{X}$ is the index set so that, $\{f(\mathbf{x}):\mathbf{x}\in \mathcal{X}\}$ is a collection of random variables. From this perspective, it does not seem to make sense to think of $\mathbf{x}$ as a random variable. However, we also derive posterior distributions such as $p(f|y,\mathbf{x})$ , where here we are now conditioning on $\mathbf{x}$ , which does not make sense unless $\mathbf{x}$ is the realization of some random variable. How can I reconcile these two points together? Is writing the posterior distribution in this way just an abuse of notation?
