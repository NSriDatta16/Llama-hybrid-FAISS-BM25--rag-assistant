[site]: crossvalidated
[post_id]: 347977
[parent_id]: 
[tags]: 
Optimal subset from training data used in Random Forest

I have a set of say 10,000 spatial locations with associated values of a soil property (e.g. soil clay). In addition, I have 100 spatial covariates (e.g. elevation) which cover entirely my study area. My objective is to build a random forest model for soil clay with a subset of spatial locations (e.g. using 100 locations) selected optimally for my RF model. Intuitively, I use the quantile regression forest and I run simulated annealing to select 100 locations from the existing 10,000 locations, so as to minimize the spatial average width of the 90% RF prediction intervals (predicted over the whole area). The problem is that the optimized 100 locations are actually the locations which offer the smallest data variance (i.e. low percentage of soil clay). I understand that RF is data driven and that minimizing the averaged width of the quantile regression forest leads to minimizing the variability among the data. However, I am trying to find a way to optimize my sampling design using an objective function derived from RF, while keeping the optimized data variability close to that of the 10,000 locations. Which RF objective function could I minimize to find an optimal design from a subset of the training (10,000) locations ?
