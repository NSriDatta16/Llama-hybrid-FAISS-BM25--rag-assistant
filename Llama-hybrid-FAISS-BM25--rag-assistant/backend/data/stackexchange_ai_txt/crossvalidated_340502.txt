[site]: crossvalidated
[post_id]: 340502
[parent_id]: 
[tags]: 
Transfer learning for image classification

When working with transfer learning for image classification, I would like to freeze only a part of the convolutional base of a pretrained model while adding a classifier (some shallow network) on top of the convolutional base. I fail to understand why I have to pretrain the classifier while the whole convolutional base is frozen and then unfreeze the top layers of the convolutional base and re-train the (unfrozen layers+ classifier that is already trained). The literature states that if the classifier isn't already trained, the error signal propragating throough the network during training will be too large and the representations previously learned by the layers being fine-tuned will be destroyed. Would you mind helping me understanding why the error signal would be so big if we just train (the classifier added on top+the unfrozen layers)at once?
