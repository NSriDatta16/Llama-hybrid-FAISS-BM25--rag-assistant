[site]: crossvalidated
[post_id]: 71842
[parent_id]: 71782
[tags]: 
Section 8.4 of The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is "Relationship Between the Bootstrap and Bayesian Inference." That might be just what you are looking for. I believe that this book is freely available through a Stanford website, although I don't have the link on hand. Edit: Here is a link to the book, which the authors have made freely available online: http://www-stat.stanford.edu/~tibs/ElemStatLearn/ On page 272, the authors write: In this sense, the bootstrap distribution represents an (approximate) nonparametric, noninformative posterior distribution for our parameter. But this bootstrap distribution is obtained painlessly — without having to formally specify a prior and without having to sample from the posterior distribution. Hence we might think of the bootstrap distribution as a “poor man’s” Bayes posterior. By perturbing the data, the bootstrap approximates the Bayesian effect of perturbing the parameters, and is typically much simpler to carry out. One more piece of the puzzle is found in this cross validated question which mentions the Dvoretzky–Kiefer–Wolfowitz inequality that "shows [...] that the empirical distribution function converges uniformly to the true distribution function exponentially fast in probability." So all in all the non-parametric bootstrap could be seen as an asymptotic method that produces "an (approximate) nonparametric, noninformative posterior distribution for our parameter" and where this approximation gets better "exponentially fast" as the number of samples increases.
