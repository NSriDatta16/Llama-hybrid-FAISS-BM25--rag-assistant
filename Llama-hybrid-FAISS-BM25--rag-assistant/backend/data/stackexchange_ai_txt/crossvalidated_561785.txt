[site]: crossvalidated
[post_id]: 561785
[parent_id]: 561267
[tags]: 
You will have a problem with estimating fixed group effects. Especially, when you have just one observation per group. To illustrate: Group D value 1 1 10 2 0 0 3 1 11 4 0 1 If you fit a regression model like $\text{value}_i \sim \alpha_\text{group} + \beta * D + \epsilon_i$ , then the parameters of the model are not-identifiable. E.g. $\hat{\alpha}_1 = 10$ , $\hat{\alpha}_2 = 0$ , $\hat{\alpha}_3 = 11$ , $\hat{\alpha}_4 = 1$ and $\hat{\beta}=0$ gives a perfect fit/maximizes the likelihood, but so does e.g. $\hat{\alpha}_1 = 5$ , $\hat{\alpha}_2 = 0$ , $\hat{\alpha}_3 = 6$ , $\hat{\alpha}_4 = 1$ and $\hat{\beta}=5$ , as well as an infinite set of other solutions. If you then want to estimate a within group variation and want to assume that the within group variation varies across groups, things only get worse (with one observation per group, how just cannot tell apart between record and between group variability). How does one resolve that issue? Options: Use random effects that make assumptions about how intercepts vary across groups (ideally after adjusting for all key aspects in which groups could differ in a way that would lead to a different outcome). This kind of setting with many observations and very few records per observation are one of the key motivations for random effects models. Key terms to look for are "exchangability". Note that random effects are not limited to assuming normally distributed random effects, but software is typically the most mature for that setting. Go Bayesian and use prior information to set at least weakly identifiable priors for all model parameters, which resolves the unidentifiability in he sense of lading to e.g. unique maximum-a-posteriori estimates. A combination and (1) and (2). Using some form of regularization (e.g. ridge, LASSO, elastic net regression etc.). This will in some sense be equivalent to Bayesian maximum a-posteriori estimation with a particular prior. Some kind of embedding approach as seen e.g. in the machine learning literature, where you find some kind of low level numeric representation for groups (e.g. via 1-, 2-, 3- or more continuous covariates instead of a fixed effect for every single group, e.g. group 1 might be [0.732, -0.442, 2.013] and group 2 [-1.234, 2.224, 0.0277] and so on). Such embeddings can sometimes be created via other tasks or in a neural network (with an embedding layer) when training for the task at hand (however, this would again need regularization in your case). Ad-hoc solutions like combining small groups e.g. based on some kind of similarity criteria. In some particular circumstances this can be quite reasonable.
