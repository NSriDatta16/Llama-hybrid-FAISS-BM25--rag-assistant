[site]: crossvalidated
[post_id]: 187784
[parent_id]: 187683
[tags]: 
A conclusive answer would require digging into the C source for classRF, but from background knowledge of how random forests work, and a bit of reading between the lines in ?randomForest, here's what I think is happening. Despite my comment above I was able to reproduce this behavior as follows: library(randomForest) set.seed(291874590) fit1 randomForest() randomly selects a subset of the data to train each tree on. Rows that are not selected serve as OOB data for that tree and end up contributing to the confusion matrix. The key phrase from the help is [ntree] should not be set to too small a number, to ensure that every input row gets predicted at least a few times. You can see which ones were not selected in any of the trees by looking at fit2$predicted . 6 of the Setosa rows have NA for the prediction. EDIT: If you want to see how many times each row ended up in the OOB data, set the argument norm.votes=FALSE in the call to randomForest() , and look at fit2$votes. As pointed out by user777 the reason the classification error for setosa doesn't change even though the number of cases changes is that setosa can be perfectly seperated from the other two classes by any pair of the variables. The classification error does change for the other two classes.
