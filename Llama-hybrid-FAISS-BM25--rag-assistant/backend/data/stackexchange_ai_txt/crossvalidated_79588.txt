[site]: crossvalidated
[post_id]: 79588
[parent_id]: 69921
[tags]: 
As I understand the problem, of the 70000 samples available, you have 90 samples where you have the set of 5 numeric values AND the target values; and of the remaining 69910 samples where you have the 5 numeric values ONLY. You want to predict the unobserved target values of the 69910 samples. Have you considered just building a classifier/regressor using the 90 complete samples alone? This would have the advantage of being pretty quick to do and having lots of methods available you could try. With 90 samples you can easily use non-parametric methods (k-nearest neighbours, SVMS, parzen windows to classify, gaussian processes of svm regression if the target variables are real valued). Cross validate, or use bayesian methods, to tune the hyperparameters. The downside is, of course, you have very few samples available, and they may not be enough to build a good model on their own. If, and only if, that is the case, it may be worth using your 69910 samples without target variable values as part of your trainign set. In that case... You are using a semi-supervised learning method. In fact, if you don't want your eventual solution to generalise to other, novel samples, you are using transductive learning. Approach with caution. Semi-supervised methods can be brittle, and make you results worse than if you were learning from the labelled dataset alone. They depend a great deal on the ratio of labelled to unlabelled data, and may exhibit sharp transitions in behaviour as this ratio grows (so a techniques which is initially improved by adding some unlabelled data may suddenly get worse when you add some more). However, they can work. Things to bear in mind: If you are performing model fitting, then getting a good match between model and underlying distribution is really important. Performance is much, much better if you try to ensure this (Castelli & Cover, 1995 and 1996, showed this very clearly). Using the unlabelled data to change your feature space in an unsupervised way, then projecting your labelled data into that new feature space and doing all your learning in that, is an approach I have heard some good things about (though cannot at this moment find the references I wanted to cite). Self-organising maps ought to be able to do this. Kernel methods to do non-linear dimensionality reduction, or things like Isomap, are also good . If you decide to use graph based methods, look into using the graph laplacian . It can do some pretty magic stuff if used carefully. Look at section 2 of Zhu, X. et. al, 2003, "Combining Active Learning and Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions" (.pdf can be found here ) for the best description of what it does that I'm aware of. You must hold aside part of your labelled data to cross-validate with. It really is the only way to be moderately sure including the unlabelled data is making your learning algorithm better and not worse. If this is a classification task and you have some prior knowledge of the class proportions, then Expectation regularisation (Mann & McCallum, 2007, "Simple, Robust, Scalable Semi-supervised Learning via Expectation Regularization") is surprisingly effective for being such a simple idea. SVMlite can perform transductive smv learning - it probably should be your first stop if you want to investigate the feasibility of using TSVM methods. You must hold aside part of your labelled data to cross-validate with. It can't be said too many times. Zhu (wikipedia reference) wrote an excellent review of the semi-supervised field a while ago. Have a look in that for things which might fit with what you are doing. But seriously, if the 90 complete samples alone get you good enough results, just use them.
