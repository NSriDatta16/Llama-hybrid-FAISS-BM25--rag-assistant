[site]: crossvalidated
[post_id]: 555153
[parent_id]: 
[tags]: 
Gradient descent implementation of logistic regression

Objective Seeking for help, advise why the gradient descent implementation does not work below. Background Working on the task below to implement the logistic regression. Gradient descent Derived the gradient descent as in the picture. Typo fixed as in the red in the picture. The cross entropy log loss is $- \left [ylog(z) + (1-y)log(1-z) \right ]$ Implemented the code, however it says incorrect. import numpy as np def LogitRegression(arr): # code goes here x = arr[0] y = arr[1] a = arr[2] b = arr[3] z = 1.0 / (1.0 + np.exp(-a * x - b)) lr = 1.0 a = a - x * (z-y) a = np.round(a, decimals=3) b = b - (z-y) b = np.round(b, decimals=3) return ", ".join([str(a), str(b)]) # keep this function call here print(LogitRegression(input())) If I reverted the sign of the gradient update, it works. However, not sure why. # a = a - x * (z-y) a = a + x * (z-y) a = np.round(a, decimals=3) # b = b - (z-y) b = b + (z-y) b = np.round(b, decimals=3)
