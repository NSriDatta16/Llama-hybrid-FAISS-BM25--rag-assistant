[site]: crossvalidated
[post_id]: 191090
[parent_id]: 110088
[tags]: 
For decision trees, is it better to use the full train data set to construct the tree? It is always better to have more data to train your model. But if you use all data that you have in hand, then you have no idea about your test error (of course you can indirectly estimate it but estimations remain estimations), and furthermore, it is hard to know if you overfit your data or not. So in my experience, it is not recommended to use all data to fit your model. In a random forest, is k-fold cross-validation necessary? I thought you could use OOB error? OOB error can be used to tune your parameters, once it's done, OOB is no longer a valid test set for evaluating your model.
