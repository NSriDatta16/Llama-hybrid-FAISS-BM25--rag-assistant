[site]: crossvalidated
[post_id]: 448703
[parent_id]: 446594
[tags]: 
Regarding your first question: "How does parameter sharing help the model?" The main purpose of sharing is the radical reduction of parameters, one of the main selling points was to reduce the impractical size of Bert. The authors do mention that the parameter reduction also acts as regularisation/generalisation. It does not improve the performance of the model though ( "This approach slightly diminishes the accuracy, but the more compact size is well worth the tradeoff." ). As to your second question, "How did they find out that network often performed similar operations at different layers?" - I don't know, but I can guess that they might have used some technique to inspect the network. Van Aken et al (2019) "present a layer-wise analysis of BERTâ€™s hidden states" by applying "probing tasks that reveal the information stored in each representation layer". Another example for probing into Bert is presented by Tenney et al (2019) . Note that these do not particularly attempt to measure existing redundancy and don't seem to make explicit claims about it. However, there are a few papers which notice redundancy when looking at attention heads in particular and what they attend to (e.g. certain heads mostly attend to special tokens or punctuation, other heads attend to tokens that relate to syntactic relations, e.g. direct objects attend to their verbs). Clark et al (2019) find that there are groups of heads behaving quite similarly, in particular heads within the same layer. Voita et al (2019) and Michel et al (2019) show that many heads can be pruned away without hurting model performance. I do not know if the redundancy that the Albert authors mention is the same that the above papers found. What's inconsistent is that the most redundancy was found between attention heads within one layer, as opposed to across different layers. But it is still something worth considering.
