[site]: crossvalidated
[post_id]: 180367
[parent_id]: 
[tags]: 
Incorporating evidence to improve a robot's controller

I have a little robot, and have created a controller based on a MDP. Its transition function is based on a set of training data we have, giving each action a learned set of probabilities which reflect its possible outcomes. I now want to update these probabilities based on the robot's actual performance. Say for example we have our navigation action, from our data we know if we execute this action it has 70% chance of reaching its destination, a 20% chance of encountering a failure of type 1, and a 10% chance of encountering failure of type 2. I want to take this prior information and incorporate the robots real experience into them to produce more realistic estimates of each action e.g. succeed 5/10 times, fail1 4/10 and fail2 1/10. This seems like it should be bayesian inference from reading around, but I can't quite get my head around how it should be used in this situation. Looking through tutorials and posts here most are based around having just two outcomes and using Beta distributions and Bernoulli-esq trials, and I end up getting lost. It has apparently been far too long since my last statistics course! To keep it simple I have 1 action, 3 outcomes, prior knowledge over these outcomes, and an increasing set of empirical evidence. How do I go about combining these?
