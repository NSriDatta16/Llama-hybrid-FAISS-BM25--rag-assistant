[site]: datascience
[post_id]: 14091
[parent_id]: 14084
[tags]: 
There are two steps to this problem: feature selection/ dimensionality reduction and selecting the predictive model. Selecting the 'best' features to use in the model will often improve the accuracy, and there are quite a few methods you can use. When you are working with continuous data, you can use a regularization method such as Lasso or ElasticNet to select the best features for the model. These are both available in sklearn, but they do require some parameter tuning to find the right hyperparameters to get the best results. You can also take a look at sklearn.feature_selection . These are more statistical approaches to select features, generaly based on their p-values. This also only works with continuous data. Another approach is calculating the Gini importance from decision tree models, such as RandomForest. This is useful when you have continuous and categorical data. Here is an example. After you have selected the best features, you want to choose the right model for binary classification. The go-to model in this case is logistic regression. There are multiple hyperparameters in sklearn.linear_model.LogisticRegression and in order to get the best results, you may have to perform some grid searches to find the right parameters. Here are some additional resources that may prove helpful: http://blog.nycdatascience.com/student-works/bnp-claims/ http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/ http://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/ I added the last link, discussing bias vs. variance tradeoff, because I think it is important to understand when testing your models.
