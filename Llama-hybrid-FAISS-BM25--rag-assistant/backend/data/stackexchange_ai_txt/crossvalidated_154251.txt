[site]: crossvalidated
[post_id]: 154251
[parent_id]: 
[tags]: 
What makes the recommendation problem unable to be solved by traditional machine learning algorithms directly?

We have collaborative filtering and content based algorithms there for recommendation. What stops traditional algorithms from directly being used to find missing values in the Utility matrix formulation of recommendation? Is it the sparsity of the matrix? If the matrix wasnâ€™t sparse, what other algorithms also work? Do regressions work then? Say given ratings for $n$ items for $m$ users, predicting the $n^{th}$ rating for a user with ratings of only $n-1$ items. Would collaborative filtering and content based algorithms still do better given less sparsity? Is there some way in which SVMs, Neural Nets be used for recommendation directly? A naive Bayes approach to say whether a product will be bought or not? In my opinion wouldn't work because products are similar and can't be treated as independent variables. Also, if these algorithms (collaborative filtering and content-based approaches) work nicely on sparse matrices with a lot of features, can it be used for prediction problems for sparse matrices and solve the high dimension less data problem? I know these questions are a little spread and may be really naive, but I think an underlying difference between the two approaches of prediction would solve most of my questions. Is there a survey paper which compares machine learning algorithms with content-based and collaborative filtering-based approaches to prediction?
