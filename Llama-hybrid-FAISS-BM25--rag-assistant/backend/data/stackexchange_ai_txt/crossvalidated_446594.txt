[site]: crossvalidated
[post_id]: 446594
[parent_id]: 
[tags]: 
Cross layer parameter sharing in ALBERT Model

I am reading the paper "ALBERT: LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS". ALBERT uses cross-layer parameter sharing to improve model performance. I don't understand how does parameter sharing helps the model? In a Google AI blog , they say that "the network often learned to perform similar operations at various layers, using different parameters of the network. This possible redundancy is eliminated in ALBERT by parameter-sharing across the layers, i.e., the same layer is applied on top of each other.". How did they find out that networks often performed similar operations at different layers?. Any help would be appreciated.
