[site]: crossvalidated
[post_id]: 610542
[parent_id]: 
[tags]: 
How to compare the semantic similarity of text generated by large language models (GPT-3, BLOOM etc) to reference text?

Large language models, such as GPT-3, BLOOM etc, can generate open-ended text. Say I want to prompt these models to answer a question. How can I compare the semantic similarity of the answer it provides me with a reference question? Eg, if the prompt is, 'What should I eat for breakfast?' , an example response that BLOOM will output is 'when should i eat lunch? etc.). Is there any way to get some suggestions from microsoft?' Say the reference answer to this question is 'You should eat a full English breakfast' . How can I measure how different the BLOOM response is to the reference answer, in terms of its content? Currently, I am encoding the text into BERT embeddings, and then computing cosine similarity between the output text and reference text. But the cosine similarity scores are quite low, because, as you can see, the BLOOM answer is not really on task. Should I be finetuning the BLOOM model somehow before trying to test semantic similarity? The original BLOOM/GPT-3 models did quite well on SQuaD, but I don't seem to find that these models are answering questions very well without finetuning.
