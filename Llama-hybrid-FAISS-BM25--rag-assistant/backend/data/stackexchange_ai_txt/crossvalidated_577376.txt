[site]: crossvalidated
[post_id]: 577376
[parent_id]: 577306
[tags]: 
First, to clarify terminology, current best practice in the context of regression is to reserve the word "multivariate" for models having multiple outcomes; see Hidalgo and Goodman , for example. What you are describing seems to be regressions with single outcomes having multiple versus single predictor variables. Those might be called multiple versus simple, or multivariable versus single-variable, regressions. The fundamental danger in single-variable regression is omitted-variable bias . This can happen in ordinary least squares regression if omitted outcome-related predictors are correlated with predictors in the model. In real-life applications such correlations are common. The Wikipedia page on Simpson's paradox shows how you can even get the direction of associations between predictors and outcomes wrong if you omit critical variables. With generalized linear models like binary regressions, or with survival models, you can have such bias even if omitted predictors are uncorrelated with included predictors . Nothing in the previous paragraph has anything to do with interaction terms in regression models. Omitting even additive terms for predictors associated with outcome can get you into big trouble with inference. You caution about overfitting is well founded. You should also be cautious about automated model selection, which is not a good idea . It's generally best practice to include as many outcome-related predictors as possible without overfitting, including interactions that you expect to be important and modeling continuous predictors flexibly, for example with regression splines. Then take steps to evaluate and correct for overfitting that might have been introduced. Frank Harrell's course notes and book provide much useful guidance.
