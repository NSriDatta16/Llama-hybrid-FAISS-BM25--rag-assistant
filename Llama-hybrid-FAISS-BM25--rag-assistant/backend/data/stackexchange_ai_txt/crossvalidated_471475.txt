[site]: crossvalidated
[post_id]: 471475
[parent_id]: 
[tags]: 
Determining variance due to measurement errors between two data sets of the same population

I have a set of n objects, and for each object i , the property C(i) has been measured. The measuring device has low precision, so the value actually recorded from the measurement is C(i) + X(i) , where X(i) is a random variable. C is also expected to be randomly-distributed around a mean if you pick a random object, but every individual object's C(i) is fixed. C(i) has been measured twice for each object, which results in two recorded values, which we'll call C(i) + X(i,1) and C(i) + X(i,2) . (I don't understand the notation well enough to describe this more correctly.) The problem I'm trying to solve is to estimate how much of the variance in these 2n observations (two measurements for each object) is due to the variance in the objects' properties-- C(i) --and how much is due to the measurement device-- X(i) . The data can be assumed to be normally-distributed. The only two estimates I could come up with are not mathematically rigorous, and I'm not sure what the actual way to do it is. Here's what I tried: 1) Finding the variance of an n -observation set consisting of the average of the two measurements for each object, then finding the variance of the original 2n -observation data set, and finally dividing the former value by the latter to get a % variance explained by X(i) . 2) Individually finding the variance of [ C(i) + X(i,1) , C(i) + X(i,2) ] for every i and averaging all of those variances together. Divide that by the variance of the original 2n -observation data set to get a % variance explained by X(i) .
