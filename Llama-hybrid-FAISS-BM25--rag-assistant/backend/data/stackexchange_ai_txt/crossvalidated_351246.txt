[site]: crossvalidated
[post_id]: 351246
[parent_id]: 351233
[tags]: 
There is a lot in this question, but one thing for certain that you are doing incorrectly is performing feature selection on the training data and then building your model using the same training data. This could lead to very optimistically biased error rates. Without getting too involved, I would suggest treating feature selection as one in the same as hyper parameter optimization. The choice of number of trees to use in a random forest is no different than the choice of using random forest with variables x,y,z vs. only x and y. The important thing is to do this properly using nested cross-validation which there are many questions related to on this site. If you want to use some manner of feature selection other than a wrapper prior to the grid search in order to minimize the length of the grid search, you need to either have it self contained in its own cross validation loop or simply hold out a part of your data that is only used for feature selection.
