[site]: crossvalidated
[post_id]: 161198
[parent_id]: 161101
[tags]: 
Suppose you have a set of binary observations $Y_i$ for $i=1,\ldots,n$ and, for each observation, an associated explanatory variable $X_i$. Logistic regression assumes $$ Y_i \stackrel{ind}{\sim} Ber(\pi_i), \quad \ln\left(\frac{\pi_i}{1-\pi_i}\right)=\beta_0+\beta_1 X_i.$$ If you are obtaining point estimates of the parameters via maximum likelihood, then you just use the assumptions above. But, if you are obtaining estimates of the parameters using a Bayesian approach, then you need to define a prior for $\beta_0$ and $\beta_1$, call it $p(\beta_0,\beta_1)$. This prior along with the logistic regression assumptions above is Bayesian logistic regression.
