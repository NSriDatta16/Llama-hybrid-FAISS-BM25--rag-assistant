[site]: crossvalidated
[post_id]: 617489
[parent_id]: 526583
[tags]: 
Here is a drawing of a two-layer neural network. The blue, red, purple, and grey lines represet network weights, and the black line is a bias. Assume the pink output neuron to have sigmoid activation function so it gives a predicted probability. Then the equation is: $$ p = \text{sigmoid}\left( b + \omega_{blue}x_{blue} + \omega_{red}x_{red} + \omega_{purple}x_{purple} + \omega_{grey}x_{grey} \right)\\ \iff\\ \log\left( \dfrac{ p }{ 1 - p } \right) = b + \omega_{blue}x_{blue} + \omega_{red}x_{red} + \omega_{purple}x_{purple} + \omega_{grey}x_{grey} $$ The second equation is that of a logistic regression (save for some statistical technicalities). Since there is no limit to how many input neurons there could be, and the sigmoid activation function could have been the inverse of any of the link functions from generalized linear models, the Keras claim is not as strong as it could be. Indeed, generalized linear models (including OLS linear regression) can be expressed as two-layer neural networks.
