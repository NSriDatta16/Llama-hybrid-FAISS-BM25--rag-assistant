[site]: crossvalidated
[post_id]: 517161
[parent_id]: 517051
[tags]: 
The short answer is yesâ€”what you've stumbled upon is a case of variable elimination. You might as well change the $n+1$ s to $n$ , just to reduce clutter: $$ P(X_{n} = k) = \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{1} = i)P(X_{1} = i) \text{.} $$ Short proof: $$ \begin{align} P(X_{n} = k) &= \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{1} = i)P(X_{1} = i) & \text{Your starting point} \\ P(X_{n} = k) &= \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{1} = i) \sum_{j \in \mathcal{S}} P(X_{1} = i \mid X_{0} = j) P(X_0 = j) & \text{Definition of } P(X_1) \text{ from Ross}\\ P(X_{n} = k) &= \sum_{j \in \mathcal{S}} \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{1} = i) P(X_{1} = i \mid X_{0} = j) P(X_0 = j) & \text{Push constant inside second summation}\\ P(X_{n} = k) &= \sum_{j \in \mathcal{S}} \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{1} = i, X_{0}=j) P(X_{1} = i \mid X_{0} = j) P(X_0 = j) & \text{Markov assumption}\\ P(X_{n} = k) &= \sum_{j \in \mathcal{S}} P(X_{n} = k \mid X_{0} = j) P(X_0 = j) & \text{Definition of marginal probability}\\ \end{align} $$ Some discussion: You've provided the following formula from the Ross book. The formula is correct, but it's one of those 'Why would you ever write it this way?' things. $$P(X_{n} = k) = \sum_{i\in \mathcal{S}} P(X_{n} = k | X_{0} = i)P(X_{0} = i)$$ All of the fun of the Markov chain is in this term: $P(X_{n} = k | X_{0} = i)$ , which we tend to decompose with the chain rule and the Markov conditional independence assumption: $P(x_n | x_0) = \sum_{x_{n-1}} \cdots \sum_{x_1} P(x_n | x_{n-1}) \ldots P(x_1 | x_0)$ . Usually we'll write it explicitly. But Ross's equation is still correct.
