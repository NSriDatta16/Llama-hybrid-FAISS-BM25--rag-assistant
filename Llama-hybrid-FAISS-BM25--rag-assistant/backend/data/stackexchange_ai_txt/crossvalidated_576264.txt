[site]: crossvalidated
[post_id]: 576264
[parent_id]: 576035
[tags]: 
Q1 See ?gamObject and in particular note the $sp and $full.sp components of the fitted GAM model object and note their description. As your data_gdp_gamm object is of class "gamm" , which is a list with two components $lme the equivalent mixed model object $gam the equivalent GAM object (which won't have all of the components described in ?gamObject ) you'll want to find the smoothing parameters ( $\boldsymbol{\lambda}_j$ ) within the $gam component; data_gdp_gamm$gam$sp for example. Q2 This is pretty well covered in ?gamm and its Details section, although it is unreasonable to expect some quite hefty theory to be presented in the documentation of a function, so you'll need to do some further reading of the references cited in that section. What is happening here is that the theory showing that penalized splines and mixed effects models are two sides of the same coin is being used to fit the GAM as a linear mixed effects model. This Bayesian view of smoothing is that the penalty matrix(es) $\mathbf{S}_{\lambda}$ can be viewed as imposing a Gaussian prior on the model coefficients, and the resulting Bayesian (smoothing model) marginal likelihood has exactly the same form as the REML criterion of a mixed model. The smoothing parameters $\boldsymbol{\lambda}_j$ , in this framework, are proportional to the precision (inverse of the variance) of the random effect variances in the mixed model form. As such, selection of the smoothing parameters becomes a problem of jointly estimating the model coefficients for part of the smooth (coefs for the basis functions that are in the penalty null space, which are treated as fixed effects) and the posterior modes of the the random effects (whose model matrix contains the wiggly basis functions of the smooth) using either REML or ML. So, it is the Bayesian marginal log likelihood (=== REML criterion) $$ \mathcal{V}_{\text{r}}(\boldsymbol{\lambda}) = \log \int f(\mathbf{y} | \beta) f(\beta) d \beta $$ is being maximised (from Wood, 2017, p263). As this is not a fully Bayesian model (no prior on the smoothing parameters), this kind of approach is also known as empirical Bayes . Wood, S.N., 2017. Generalized Additive Models: An Introduction with R, Second Edition. CRC Press.
