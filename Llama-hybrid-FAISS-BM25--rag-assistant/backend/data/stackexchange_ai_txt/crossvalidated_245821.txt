[site]: crossvalidated
[post_id]: 245821
[parent_id]: 245806
[tags]: 
When you transform a discrete random variable, all you do is move the spikes of probability around. If the probability at $X=x$ is $p(x)$ and you transform $Y=t(X)$, then at $y=t(x)$ you have probability $p(x)$. If the transformation is not injective then where more than value is mapped to a single transformed value the probabilities will all add. Apart from the fact that both 0 and 1 map to 0, the transformation $Y=X\log(X)$ where $X$ can take the values $\{0,1,2,...,n\}$ is otherwise monotonic (it's monotonic in the positive integers; as a transformation of a continuous variable it would be monotonic for $x > 1/e$). So you have x 0 1 2 3 ... n p p(0) p(1) p(2) p(3) ... p(n) \ / | | | y 0 2log2 3log3 ... nlogn p p(0)+p(1) p(2) p(3) ... p(n) It's possible to write the probabilities in closed form for $y = 2\log 2$ and up but it's not really more enlightening than just working with probabilities for $x$ and then shifting the locations. That is we can write $P(Y=0)= (1-p)^{n-1}(1 + (n-1) p)$ and $ P(Y=y) = {n \choose e^{W(y)}} p^{e^{W(y)}} (1-p)^{n-{W(y)}}$, $y=2\log 2,3\log 3, ...,n\log n$, where $W$ is the Lambert $W$-function -- but it's less clear than writing the cases after the first one as $P(Y=y) = {n \choose x} p^{x} (1-p)^{n-x}\,$, $x=2,3,...,n$ where $y=x\log x$.
