[site]: crossvalidated
[post_id]: 608639
[parent_id]: 608297
[tags]: 
There's situations where you can bootstrap. If your sample size is large, then bootstrapping when possible is really convenient for a number of reasons: If it works, then it works for almost any metric you can define, while frequentist analytical solutions tend to be derived for one metric at a time (bad luck if your metric of interest is not covered) It's easy to implement (assuming a straightforward bootstrapping scheme is appropriate for the situation and the metric is easy to calculate for a sample). Where bootstrapping has problems is When sample size is small, then the discreteness of bootstrapping can make it inefficient. When another simple solution is easily available/already implemented, why do it, when it often takes longer to run/is a little bit more complex to implement than running a well-validated package that someone else already wrote? Sometimes you just cannot bootstrap in a valid way. E.g. multiple-rate-multiple-case (MRMC) studies tend to make it very hard to bootstrap (some solutions still have a bootstrap component, but it's nightmarishly complicated*), because everything is in a way correlated with everything (you have both the same raters looking at cases, as well as the same cases being looked). As the other answer mentioned, one approach is a form of mixed effects model that describes the whole process. We took that approach for a MRMC study, where there were 3 possible diagnoses, recently. The Methods Section of the article outlines the approach: We analyzed the primary outcome using a Bayesian model that jointly modeled each patient’s true disease status (ie, expert panel diagnosis) as a categorical random variable and the diagnoses given for each patient by each physician or algorithm (determined using multinomial logistic regression). The multinomial logistic regression model included a separate intercept term for each combination of disease (asthma, COPD, and ACO) and group (PCPs, pulmonologists, and the AC/DC tool), as well as a random case and random rater effect. We took this approach, because we could not find any published methods that applied to the metrics our clinical colleagues wanted to look at (most MRMC literature is about AuROC and as mentioned, simple bootstrapping does not work). We ended up fitting this in a Bayesian way using Stan, because that deals with the uncertainties sensibly. It might be possible to fit such a model with something like PROC NLMIXED in SAS and to get confidence intervals that way (they have a decent automatic procedure for SEs there that uses the delta method) in a large study, but the Bayesian approach deals more naturally with (nearly) empty cells. * Regarding "nightmarishly complicated", look at the complexity in these papers for getting AuROC estimates with CIs from MRMC studies: Brandon D. Gallas, Gene A. Pennello, and Kyle J. Myers. Multireader multicase variance analysis for binary data. J. Opt. Soc. Am. A, 24(12): B70–B80, Dec 2007. doi: 10.1364/JOSAA.24.000B70 . Brandon D. Gallas, Andriy Bandos, Frank W. Samuelson, and Robert F. Wagner. A framework for random-effects roc analysis: Biases with the boot- strap and other variance estimators. Communications in Statistics - Theory and Methods, 38(15):2586–2603, 2009. doi: 10.1080/03610920802610084 .
