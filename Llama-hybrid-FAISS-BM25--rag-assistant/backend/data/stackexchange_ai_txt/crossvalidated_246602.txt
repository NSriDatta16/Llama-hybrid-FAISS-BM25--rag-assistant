[site]: crossvalidated
[post_id]: 246602
[parent_id]: 244919
[tags]: 
I see that you initialize weights with init_weight : [-1, 1], init_bias : [-1, 1], I am not sure what does it do exactly, but i guess your weights are initialized to 1 or -1. If that is the case, then thats your first problem. Since the all weights starts at the same number, they will all produce the same error and therefore will be updated by the same amount. So you cannot really learn any pattern, because all your neurons will be doing the same. This is not a problem with a linear activation function, because layer full of linear neurons will produce the same output as just the one linear neuron regardless. Because the linear combination of linear functions is still just a linear function. I thought that maybe you have some bug and you are not updating all the weights but only the bias, which would explain why you will end up with the mean function. This was however not true with linear activation function, which was learning the output correctly. Next I noticed that with sigma activation function something was happening to the outputs in the beginning of the epoch but not in the later stages and also that the network was working reasonably fine if I use only 10 inputs instead of yours default 100. That usually happen if you don't standardize the inputs before the training. If the inputs are too big, your sigmoid activation function will produce outputs close to 1, however the slope of the sigmoid at this point is close to 0, so your weights wont get updated, because the gradients are close to 0. You can fight this by standardizing the inputs, so by subtracting the mean of the vector and dividing it by its standard deviation. So your input vector will have mean 0 and std 1. This is also related problem to vanishing/exploding gradient problem, where the weights tends to grow too big/too small and you will get the same issue of the neuron outputing values close to 0/1 and therefore not updating the weights. I think you combat this by some form of regularization, for example force all weights to sum to 1 or something like that. I tried few things in scikit learn implementation of SGD NN and indeed, neural network fail if the inputs are not standardized. However I couldn't find in your code where the heck is actual input vector, so I couldn't try if this also solves your problem.
