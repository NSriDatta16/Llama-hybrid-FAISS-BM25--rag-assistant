[site]: crossvalidated
[post_id]: 251790
[parent_id]: 251745
[tags]: 
I would recommend reading Eicher, Lenkoski and Raftery's article on it at http://faculty.washington.edu/te/papers/elr.pdf I would like to make two side notes however. The first is on Bayesian Model Averaging(MA) versus Bayesian Model Selection(MS). The article makes good use of MA because it extracts so much information, but I want to put in a pitch for a possible combination of MA and MS. I tested 78 models of bankruptcy and found that the cumulative posterior probability for 76 of them was slightly less than one ten thousandth of one percent. The other two had posterior probabilities of approximately fifty-four and forty-six percent respectively. I averaged the remaining two models based on their posterior probabilities. The model was amazingly accurate. It used CRSP data from 1950- and used up to the prior business cycle as its training set and the last business cycle as its test set. The thing to keep in mind in Bayesian methods is that any "test" is just some integration that removes nuisance parameters out of the posterior to leave the variable of interest. The second note is on the problem of Bayesian optimality. All Bayesian procedures are optimal procedures, even the bad ones. People can get confused on the meaning. When comparing a Bayesian procedure and a non-Bayesian procedure, a non-Bayesian procedure is only valid if it either provides the same solution as the Bayesian procedure or maps to a valid Bayesian procedure at the limit. This can create a false sense of security. A Bayesian model is optimal in the sense that for the specification, there is no way to extract more information from the data. The hitch is in the phrase "for the specification." If you construct a fragile model, then you will get an optimally fragile model. Likewise, if you construct a robust model, then you will get an optimally robust model. I believe combining MS with MA gives you the best of both worlds. MS causes you to lose the information from highly improbable models, but adds to robustness since those models are likely far from the data generating function. By averaging over the probable models, you have an indirect tool to exclude highly unlikely models, while extracting as much information as possible over the likely models.
