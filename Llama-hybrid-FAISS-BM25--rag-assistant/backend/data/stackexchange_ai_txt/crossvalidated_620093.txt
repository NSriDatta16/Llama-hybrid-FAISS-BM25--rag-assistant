[site]: crossvalidated
[post_id]: 620093
[parent_id]: 
[tags]: 
Word embedding and Euclidean distance

Does a transformation exist that allows to use of the Euclidean distance with the word embeddings? The Cosine distance could be a problem in my case. For example, what if I translate the vector to a polar coordinate system from the cartesian one? Does it make sense? The idea is reduce the dimensionality of such vector using PCA or Autoencoders. In my understanding, using PCA on word embeddings means to produce random data. How can I reduce the dimensionality of the word-embeddings saving the meaning the have?
