[site]: crossvalidated
[post_id]: 32174
[parent_id]: 
[tags]: 
PCA objective function: what is the connection between maximizing variance and minimizing error?

The PCA algorithm can be formulated in terms of the correlation matrix (assume the data $X$ has already been normalized and we are only considering projection onto the first PC). The objective function can be written as: $$ \max_w (Xw)^T(Xw)\; \: \text{s.t.} \: \:w^Tw = 1. $$ This is fine, and we use Lagrangian multipliers to solve it, i.e. rewriting it as: $$ \max_w [(Xw)^T(Xw) - \lambda w^Tw], $$ which is equivalent to $$ \max_w \frac{ (Xw)^T(Xw) }{w^Tw},$$ and hence ( see here on Mathworld ) seems to be equal to $$\max_w \sum_{i=1}^n \text{(distance from point $x_i$ to line $w$)}^2.$$ But this is saying to maximize the distance between point and line, and from what I've read here , this is incorrect -- it should be $\min$, not $\max$. Where is my error? Or, can someone show me the link between maximizing variance in projected space and minimizing distance between point and line?
