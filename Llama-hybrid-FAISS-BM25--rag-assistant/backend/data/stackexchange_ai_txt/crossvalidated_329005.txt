[site]: crossvalidated
[post_id]: 329005
[parent_id]: 327611
[tags]: 
OOB is good as proxy for the generalization error, but probably you will get more accurate results, if you do a 5-fold or 10-fold cross-validation and repeat this several times. But this procedure also takes more time. You can see the OOB Error as an estimation for each observation with all trees except of the trees where the observation was used for training. So on average exp(-1) * number_of_trees are used for each prediction of an observation. If you have learned enough trees, this is not a problem, as random forest converges, see the Breiman, 2001, paper. Furthermore I think OOB-Error is most comparable to the Leave-One-Out Crossvalidation. For a discussion between LOOCV and k-fold-CV see here: 10-fold Cross-validation vs leave-one-out cross-validation
