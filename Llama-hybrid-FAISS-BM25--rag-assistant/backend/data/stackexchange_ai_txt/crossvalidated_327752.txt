[site]: crossvalidated
[post_id]: 327752
[parent_id]: 327534
[tags]: 
There has been research on dealing with this kind of issues. Specifically in your case I'd outline the paper "Character-Aware Neural Language Models" by Kim at al, which encodes the input on the character level , but predicts the output on the word level . This turns out to work very well on languages which use a lot of forms of the same word (e.g. prefixes, suffixes, endings) depending on the context. But it also shows great results in English, where training sentences contain rare or misspelled words. For example, the learned embeddings allow to match the word looooook with look , and thus handle a lot of OVV words. If your data contains a lot of words like that, you might benefit a lot from the character-aware model.
