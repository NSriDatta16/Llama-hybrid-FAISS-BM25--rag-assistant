[site]: crossvalidated
[post_id]: 20502
[parent_id]: 18862
[tags]: 
I wanted to make a clarification regarding the original question. In item response theory, the discrimination (i.e. item slope or factor loading) is not indicative of difficulty. Using a model that allows for varying discrimination for each item is effectively weighting them according to their estimated correlation to the latent variable, not by their difficulty. In other words, a more difficult item could be weighted down if it estimated to be fairly uncorrelated with the dimension of interest and vice versa, an easier item could be weighted up if is estimated to be highly correlated. I agree with previous answers that point to (a) the lack of awareness of item response methods among practitioners, (b) the fact that using these models require some technical expertise even if one is aware of their advantages (specially the ability of evaluating the fit of the measurement model), (c) the student's expectations as pointed out by @rolando2, and last but not least (d) the theoretical considerations that instructors may have for weighting different items differently. However, I did want to mention that: Not all item response theory models allow for variation of the discrimination parameter, where the Rasch model is probably the best known example of a model where the discriminations across items are held constant. Under the Rasch family of models, the sum score is a sufficient statistic for the item response score, therefore, there will be no difference in the order of the respondents, and the only practical differences will be appreciated if the 'distances' between the score groups are considered. There are researchers that defend the use of classical test theory (which relies on the traditional use of sum scores or average correct) for both theoretical and empirical reasons. Perhaps the most used argument is the fact that scores generated under item response theory are effectively very similar to the ones produced under classical test theory. See for example the work by Xu & Stone (2011), Using IRT Trait Estimates Versus Summated Scores in Predicting Outcomes , Educational and Psychological Measurement , where they report correlations over .97 under a wide array of conditions.
