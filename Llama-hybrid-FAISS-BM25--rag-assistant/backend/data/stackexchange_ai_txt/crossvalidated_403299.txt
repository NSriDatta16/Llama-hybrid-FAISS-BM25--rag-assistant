[site]: crossvalidated
[post_id]: 403299
[parent_id]: 401366
[tags]: 
So after a few weeks of experimenting, I have reached a new solution. Since I had suspected that the bottleneck of my problem was probably my small target dataset, I tried to use data augmentation to increase the data. I tried simple shuffling and insertion of random words, but it didn't really help me with this problem since my problem is very domain specific, so I tried to use generative models in order to generate texts similar to what I have in the target dataset. I used the awesome textgenrnn library and trained two different models for my two classes, and generated 1000 texts of 300 words long for each class. Then I used these generated texts alongside a portion of my original data to train a linearSVC, on the features I extracted with the TF-IDF * WordEmbedding approach that I have mentioned in my original post, and tested the system only on the original data. The results were improved by 5% and I got 89% accuracy. The next steps will be to train the text generator on a bigger dataset (that 70k dataset for example) and then finetune it for a few epochs on the target data. Also the approach that bj√∂rn suggests sounds really interesting, I shall experiment with that too.
