[site]: crossvalidated
[post_id]: 453505
[parent_id]: 452819
[tags]: 
Wow, really good question. Let me see if I can add something. My question is: If results start to pile up in one rejection region after a lot of runs, how long will we believe in the plausibility of 0 and thus the relevance of ? I don't think this is a Frequentist question. To review, probability is the long term relative frequency of an event. To quantify plausibility in a hypothesis sounds, at least to me, very Bayesian. The mechanics of a hypothesis test force you to make an assumption about the world. In reality, H0 is strictly false (no two populations have precisely the same mean ) but it can be a useful approximation. It is up to the investigator to determine if that null hypothesis is a useful approximation conditioned on the experiment, the question, past experiments, etc. So to answer your question, there is no number we can place on the hypothesis as Frequentists. The plausibility of the null would be a scientific question, not a statistical one. Doesn't it just become unreasonable to believe in the possibility of 0 and the realistic chance of a type I error if results repeatedly suggest otherwise? I suppose this is the intended purpose of replication . A single rejection of the null does not constitute proof that the null is false (else, the type 1 error would be 0). Repeated rejection of the null through replication would likely lead to people believing the difference is real. One can see this happening even today as theories like General Relativity continuously receive empirical support for their theories. I suppose that is a more a concern for philosophers of science, and I'm sure I am making some philosophers role in their grave, but I find this argument compelling.
