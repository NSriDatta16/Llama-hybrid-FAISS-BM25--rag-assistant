[site]: datascience
[post_id]: 126923
[parent_id]: 126915
[tags]: 
Here are some differences: Computational complexity: LSTMs have linear complexity $O(n)$ , because you need to process input tokens one by one, while transformers have constant $O(1)$ complexity because all tokens are processed at the same time. Memory: LSTMs have linear memory consumption because you only need to store the last hidden state, while transformers have quadratic memory consumption. Training task: bidirectional LSTMs are trained on a (causal) language modeling task, while Transformer encoders are usually trained on a masked language modeling task. Quality of the embeddings: in general, transformer-based embeddings are preferred quality-wise. Research activity: LSTM's are a much less active area of research and so are LSTM-based embeddings, while transformers have been all the rage since 2017. This means that there are a lot of publicly available pretrained transformer models for embeddings and close to zero LSTM-based models for embeddings.
