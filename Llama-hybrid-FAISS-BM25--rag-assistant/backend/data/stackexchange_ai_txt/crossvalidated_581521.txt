[site]: crossvalidated
[post_id]: 581521
[parent_id]: 565672
[tags]: 
For your statements 1), 2) and 3), yes! Although, as I think you have recognised, these are very simplistic explanations. I would advise you to look at the corresponding Wikipedia pages for the gradient and the Hessian matrix . In short, from Wikipedia, for some $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ , the gradient $\nabla f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is defined at the the point $p=(x_1, x_2, \dots x_n)$ as $$ \nabla f = \begin{bmatrix} \frac{\partial f}{\partial x_1}(p) \\ \vdots \\ \frac{\partial f}{\partial x_n}(p) \\ \end{bmatrix} \, . $$ For the Hessian matrix see the following screenshot from Wikipedia: Assuming that the functions you provide are correct, they should just be implemented functions of what is provided above. As XGBoost evaluate one row at a time think of the case where $n = 1$ , i.e. $f: \mathbb{R} \rightarrow \mathbb{R}$ , then we have by the chain rule that $$ \frac{d}{dx}\frac{1}{2}(\log(x + 1) - \log(y + 1))^2 = \frac{\log(x + 1) - \log(y + 1)}{x+1} \, . $$ Just take the derivative of this again with respect to $x$ to get the Hessian, in the $n=1$ case it will just be a number. The implementations you provide above do elementwise operations so there are no vector operations there in the mathematical sense if that was unclear. Also, np.log1p(x) calculates $\log(1+x)$ , see here . Hope this helps :)
