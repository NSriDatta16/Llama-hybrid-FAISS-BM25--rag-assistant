[site]: crossvalidated
[post_id]: 515314
[parent_id]: 
[tags]: 
Why is my training loss going up and down all the time?

My training loss goes down and then up again. I am totally new to pytorch and neural networks in general so I guess it makes sense to explain my whole model. I probably messed up something really obvious. I am trying to predict 3 continuous values using 10 input variables. My target variables are days, hours and minutes. I was hoping to be able to make reliable predicitions regarding the days and the hours, I don't expect the NN to be exact regarding the minutes. My NN-Architecture looks like this: class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden1 = torch.nn.Linear(10,1000) self.predict = torch.nn.Linear(1000,3) def forward(self, x): x = F.relu(self.hidden1(x)) x = self.predict(x) return x net= Net(n_feature=10, n_hidden=1000, n_output=3) optimizer = optim.Adam(net.parameters(), lr=0.001) criterion = torch.nn.MSELoss() EPOCHS = 50 I was already playing around using different number of hidden layers, neurons, higher or lower learning rate, different batch sizes, more and less data and so on. But I couldn't identify any significant improvement regarding the loss value. The predicted output is actually not that bad but I don't feel like it's learning what it's supposed to. I guess it's not about certain adjustments but more whether the architecture of my NN is appropriate for what I'm trying to do, so if anyone reads this, thanks a lot!
