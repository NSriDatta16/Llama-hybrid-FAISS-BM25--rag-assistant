[site]: crossvalidated
[post_id]: 633379
[parent_id]: 633372
[tags]: 
If you want to manipulate model into displaying desired feature importances regardless of the importance, that would have resulted from an honest training, you may try to tinker with Cost Efficient Gradient Boosting. The feature is available in LightGBM and CatBoost (demo) , I am not sure about XGBoost. The idea is to specify feature costs, which would be taken into account during split search: a feature would be used for a split only if its gain outweights its cost. Setting high enough costs makes features unlikely to be selected, despite their actual gains. If you want to make certain features more likely to be selected (without guarantee that they would end up most important), you may Specify forced splits for certain features. Those would be applied at the root of each tree, before tree fitting begins ( forcedsplits_filename in LightGBM ) Fit separate models on the set of all features and subset(s) of traditional features only, then combine them into ensemble Increase number of histogram bins for selected features ( max_bin ) [Added] Monotone constraints is another way to incorporate knowledge about relationships between the target and predictors. iirc XGBoost enforces monotonicity strictly, and LightGBM uses penalization, which is much more flexible.
