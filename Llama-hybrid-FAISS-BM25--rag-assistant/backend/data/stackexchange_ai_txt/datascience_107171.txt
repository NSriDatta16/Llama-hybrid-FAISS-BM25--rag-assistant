[site]: datascience
[post_id]: 107171
[parent_id]: 49612
[tags]: 
While other answers are perfectly correct, I want to give you a somewhat simpler (not very philosophically deep) explanation. In machine learning models have two sets of parameters: parameters of the model (which are fitted by a learning algorithm, e.g. gradient descent); and hyperparameters of the model (which are typically chosen via a some kind of grid search) Now, you do need to have separate subsets of data to estimate both of these parameters sets: train set to fit parameters (e.g. weights of the linear regression), validation set to choose hyperparameters (alpha of the l1/l2 regularization); and finally, you need to have a test set, to get and estimate of model performance.
