[site]: crossvalidated
[post_id]: 174247
[parent_id]: 174230
[tags]: 
In the second model, when you apply ordinary linear squares (OLS) regression, the binary variable CO is dummy coded : $X$ will either take the value $0$ or $1$. From the point of view of linear algebra, your second model will be as follows: $$\Tiny \begin {bmatrix} Fst_1\\ Fst_2\\Fst_3\\\vdots\\ Fst_n \end {bmatrix} = \Tiny \begin {bmatrix} 1&GEO_1&TEMP_1&CO_1\\ 1&GEO_2&TEMP2&CO_2\\1&GEO_3&TEMP_3&CO_3\\1&\vdots&\vdots&\vdots\\ 1&GEO_n&TEMP_n&CO_n \end {bmatrix} \small \begin {bmatrix} \beta_0\\ \beta_1\\\beta_2\\ \beta_n \end {bmatrix} $$ ... Just like any other system. But while all the other variable values in the model matrix (i.e. $\small GEO$ and $\small TEMP$ are numeric, continuous realizations of random variables with typically a wide range of real values, $\small CO$ will be a bunch of zeros and ones depending on whether each particular observation or subject (remember that observations or subjects are points in the data cloud, and correspond to individual rows in the model matrix... I guess specimens of the marine species you are studying) is "connected" ($X=1$, or in our case, $\small CO_i=1$) or "not connected" ($\small CO_i=0$), whatever that means in your dataset. Effectively, then, $\beta_n$ acts like a switch (when $\small CO_i=0$, the switch is off, and we don't add any contribution from the coefficient of $\small CO$ (i.e. $\small \beta_n$); when $\small CO_i=1$ the effect of $\small \beta_n$ is added to the prediction). Therefore, when $X=1$, we are just adding a constant to the intercept (or subtracting) - in effect, the intercept will be $\beta_0+\beta_n$, instead of just $\beta_0$. As @Glen_b was explaining, in logistic regression the idea is completely unrelated: we are modelling a system where a number of regressors are trying to explain the odds of something happening or not happening, a binary outcome, on the left-hand side of the equation. On the LHS we have the $ln\,(odds)$ and on the RHS we have a system pretty much as above with or without dummy variables. We assume that the outcome ($Y=1$) follows a binomial distribution with probability $pi$, and come up with the logit function $ln(\frac{\pi}{1-\pi})$ for the LHS.
