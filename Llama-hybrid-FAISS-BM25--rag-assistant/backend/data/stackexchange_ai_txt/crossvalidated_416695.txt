[site]: crossvalidated
[post_id]: 416695
[parent_id]: 
[tags]: 
Proving that an SVM problem with a complex loss function is convex

The end goal, along with proving that the problem is convex, is to be able to get the problem into a form that can be coded in CVX. I have m positively labeled data points $x_i$ $\in$ $\mathbb{R}^n, i = 1,2,...m$ and a negative class summarized by a random variabe $x$ $\in$ $\mathbb{R}^n$ with mean $\hat{x}$ $\in$ $\mathbb{R}^n$ , and covariance matrix C The objective function is of the form: $\min_{w,b} L(w,b) + p(w)$ The loss function L(w,b) is a sum of: 1) the mean empirical hinge-loss error on the positive class; and 2) the worst case (w.r.t the class of random variables x with mean $\hat{x}$ and covariance matrix C) mean error on the negative class I.e. $L(w,b) =$ ${1}\over{m}$ $\sum_{i=1}^m (1 - x_i^Tw - b)_+ + sup_{x \sim (\hat{x},C)} E_x(1 - x_i^Tw - b)_+$ And the penalty function p(w) = $\lambda ||w||_1, \lambda \geq 0 $ I guess the obvious approach is to prove that the Hessian of the objective function is p.s.d, but I'm having trouble deriving the Hessian. Could anyone help me with how to proceed on this?
