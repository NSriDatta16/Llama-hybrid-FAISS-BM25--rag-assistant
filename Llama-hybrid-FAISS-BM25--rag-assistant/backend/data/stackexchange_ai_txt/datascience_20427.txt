[site]: datascience
[post_id]: 20427
[parent_id]: 11852
[tags]: 
The full answer to your question is yes and no . Let me elaborate: Yes, you can train such a model with SGD: Yes , you can train a model like you described. This is well known under the name "Autoencoder". The goal of an autoencoder is to find a (usually smaller) representation of the input data, which can then be used to reconstruct the data. This looks as follows: Image taken from standford.edu We try to make $\hat{x}_1$ equal to $x_1$ . In your code, you add the additional constraint that $W_1$ (the weights from the input layer to the hidden layer) are equal to $W_2$ (the weights from the hidden layer to the output layer). The reconstruction error is usually either the squared error or the cross-entropy. Such an autoencoder can be trained with SGD, as you proposed. So, the answer is: yes, what you propose does work and is known as Autoencoder. No, you can't train an RBM with SGD: In the previous section, I said that training an RBM with SGD, as described in your question, works. This is not the full truth: this is NOT an RBM anymore! As described in the answer by Quittend a restricted Boltzmann machine models the probabilistic distribution. The goal of RBM training is not to exactly reconstruct the input, but to learn the probability distribution of the data. The reconstruction is thus not exactly the same as the input, but is a sample from the same probability distribution. The goal is to assign a high probability to the input data we train on. The probability of an input vector $\mathbf{v}$ is given by $$p(\mathbf{v}) = \frac 1Z \sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}$$ where $E$ is the energy function, and $Z$ is the partition function (to normalize $p$ such that it is a valid probability). During training, we want to increase $p(\mathbf v)$ for all $\mathbf v$ in our training data, so we have to calculate the derivative $$\frac{\partial \log p(\mathbf v)}{\partial w_{ij}}$$ and change the weights $w_{ij}$ accordingly. This is what we do with the contrastive divergence (CD- k ) algorithm, and this is not possible with gradient descent (SGD). tl;dr: Training an unsupervised neural network with SGD exists and is known as autoencoder. An RBM is a different concept, which models the probability distribution and does not strictly reconstruct the input. RBMs can not be trained with SGD.
