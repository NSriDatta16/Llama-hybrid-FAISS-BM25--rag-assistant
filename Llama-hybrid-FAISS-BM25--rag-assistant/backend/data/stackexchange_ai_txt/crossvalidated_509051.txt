[site]: crossvalidated
[post_id]: 509051
[parent_id]: 
[tags]: 
What is the marginal log-likelihood in a Multi-Head Model?

I have to study the model described here . Given a passage text and a question about it, the model tries to find the correct answer to the question. This model follows a multi-head approach: each head is a small module that takes the contextualized representations (coming out from BERT or RoBERTa) as input and computes a probability distribution over answers. Thus, the model probability for an answer is a weighted sum on every head. I made a diagram of how I suppose this model works: Even if the model describes the training loss function used for every single module, there is no clue about an overall training loss. Checking the official implementation code (full repository here ), I see that the loss refers to something called marginal_log_likelihood. Unfortunately, I don't know what does it means, and the paper does not mention it. What is the marginal log-likelihood? How does it work in a model like this?
