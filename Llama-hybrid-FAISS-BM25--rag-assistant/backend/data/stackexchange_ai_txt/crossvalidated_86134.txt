[site]: crossvalidated
[post_id]: 86134
[parent_id]: 86125
[tags]: 
You might achieve what you're really after (if it's not exactly what you've asked, which is interesting in its own right; +1 and welcome to CV!) rather simply by fitting a confidence interval (CI) around the correlation (I see @Glen_b suggested this in a comment too). If your correlation is significantly negative, a 95% CI would exclude positive values (and zero) with 95% confidence, which is usually enough for many statistical applications (e.g., in the social sciences, from whence I come brandishing a PhD). See also: When are confidence intervals useful? I don't know if it's legit to just keep increasing (or decreasing) your confidence levels until your upper bound exceeds zero, but I'm curious enough myself that I'll offer this idea, risk a little rep, and eagerly await any critical comments the community might have for us. I.e., I don't see why you couldn't just take the confidence level at which your correlation estimate's CI touches zero as your estimate of $1-p$ for a test of whether your estimate is on the proper side of zero, but also below the other, more extreme bound ...which means I still haven't answered your question exactly. Still, even if your estimate is above zero, you could calculate the level of confidence with which you can say future samples from the same distribution would exhibit correlations that are also above zero and below the upper bound of your CI ... This idea is due in part to my general preference for CIs over significance tests, which itself is due partly to a recent book (Cumming, 2012) I haven't actually read, to be honest—I've heard some pretty credible praise from those who have though—enough to recommend it myself, whether that's wise or otherwise. Speaking of "credible", if you like the CI idea, you might also consider calculating credible intervals —the Bayesian approach to estimating the probability given the fixed data of a random population parameter value being within the interval, as opposed to the CI's probability of the random data given a fixed population parameter...but I'm no Bayesian (yet), so I can't speak to that, or even be certain that I've described the credible interval interpretation with precise accuracy. You may prefer to see these questions: What, precisely, is a confidence interval? Possible dupe of ^: What does a confidence interval (vs. a credible interval) actually express? Clarification on interpreting confidence intervals? Interpreting a confidence interval. Confidence intervals when using Bayes' theorem What's the difference between a confidence interval and a credible interval? Should I report credible intervals instead of confidence intervals? Are there any examples where Bayesian credible intervals are obviously inferior to frequentist confidence intervals As you can see, there's a lot of confusion about these matters, and many ways of explaining them. Reference Cumming, G. (2012). Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis . New York: Routledge.
