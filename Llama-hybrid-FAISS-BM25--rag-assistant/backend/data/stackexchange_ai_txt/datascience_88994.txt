[site]: datascience
[post_id]: 88994
[parent_id]: 88936
[tags]: 
I think that the "neurons" analogy is not very helpful to understand what is going on with artificial neural networks. Neural networks are not comprised by "neurons", but by differentiable operations. These operations are arbitrary, e.g. convolutions, indexing (in embeddings), pooling, etc. What you proposed is a perfectly valid building block of a hypothetical neural network. The classical "neuron" analogy is used for explaining the multilayer perceptron (MLP), which is a mere sequence of fully connected layers with non-linear activations in between. As soon as you depart from the simple MLP, applying the neuron analogy becomes more cumbersome.
