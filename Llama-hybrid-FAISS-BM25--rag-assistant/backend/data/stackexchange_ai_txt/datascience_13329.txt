[site]: datascience
[post_id]: 13329
[parent_id]: 13248
[tags]: 
I would train multiple machine learning models based on information that is available. First of all I would standardize the data per product, so that the average is 0. That way you can better compare different price categories. I would add the mean price known to your set however, standardized over your whole training data, because I can imagine policies being different for different price groups. Now you can train different models for different products. The more features you use, the less data you have so this is a trade-off that you have to test. Here is an example of what I'm talking about (non-standardized): ID Price1 Price2 Price3 Feat1 Feat2 1 1 2 NA 3 8 2 4 NA 4 6 9 3 2 NA 3 5 3 4 NA 2 NA 3 4 5 5 1 NA 4 5 6 NA 3 5 2 3 7 8 5 7 4 1 Now to impute the missing Price1 values you could use: Price1 = F(Price2, Price3, Feat1, Feat2) using only 1 row (ID: 7) Price1 = F(Price2, Feat1, Feat2) using 3 rows (IDs: 1, 5, 7) Price1 = F(Feat1, Feat2) using 5 rows (IDs: 1, 2, 3, 5, 7) In this very short example the third one is probably better, but with more data using more features will lead to better results. To compare imputation models you could per price remove a number of known entities, train on the rest and compare the output to the known price. You can use a crossvalidation scheme to get all the prices in there once if the process is fully automated. I would do this at the standardized prices so it's about the relative mistakes and not absolute.
