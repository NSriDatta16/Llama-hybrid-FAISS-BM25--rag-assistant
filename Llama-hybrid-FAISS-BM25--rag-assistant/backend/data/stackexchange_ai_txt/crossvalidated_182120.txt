[site]: crossvalidated
[post_id]: 182120
[parent_id]: 182109
[tags]: 
Here's an example. Suppose you have a set of n data points from a normal distribution. We assume that the observations are independent. $$ X_1, X_2, \cdots, X_n \sim N(\mu,\sigma^2) $$ We want to estimate $\mu$. The usual way is a sample average $\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$. The sample average estimator $(\hat{\mu} = \overline{x})$ is consistent and unbiased for the normal distribution. We could define other estimators however, for instance we could just use the last observation for our estimate of the mean $(\hat{\mu} = x_n)$. The sample average $\overline{x}$ is an unbiased and consistent estimator for $\mu$. The `latest-value' estimator $x_n$ is unbiased but not consistent, the expected value of this estimator is $\mu$ but the variance does not decrease with more observations. Bias and consistency are not directly related. Neither implies the other. However if the bias remains non-zero and does not decrease with more observations then the estimator must be inconsistent. In your supplied document, the author showed that the estimator had a non-zero bias which does not decrease with $n$. It doesn't make sense to call a parameter or an estimator biased. An estimator is biased with respect to a parameter.
