[site]: stackoverflow
[post_id]: 4368977
[parent_id]: 4357518
[tags]: 
If it runs relatively fast and you're trying to get an average time by running it on a bunch of random inputs, use: long totalTime = 0; long start = System.nanoTime(); for(int i=0;i This gives you your average time in nanoseconds. Finding the average running time of GCD is a very interesting and complex problem. In the worst case, the inputs have a ratio which is close to the golden mean (such as consecutive Fibonacci numbers) and then the running time is O(log n). But it's still possible to have extremely large inputs and end up with essentially constant time. I'd be curious to know your results.
