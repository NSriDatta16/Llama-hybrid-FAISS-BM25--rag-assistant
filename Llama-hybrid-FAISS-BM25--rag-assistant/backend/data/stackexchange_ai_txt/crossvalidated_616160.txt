[site]: crossvalidated
[post_id]: 616160
[parent_id]: 616152
[tags]: 
When you “estimate” a Bayesian model most often what you do is you sample from the posterior distribution. Posterior is, by Bayes theorem, basically a product of the priors and the likelihood. If you have very little data, or it does not provide much valuable information, the posterior would be dominated by the priors. In extreme cases, you would be sampling just from the priors. If you are willing to accept your guesses about the parameters as “estimates”, then you can estimate the parameters of any model with no effort. I am not saying that to ridicule the Bayesian approach, I'm a great fan of it. What I'm trying to say is that plugging in some model to the MCMC algorithm is the easiest part and your job is far from done at this stage. The least you need to do after it is to check if the results make sense, e.g. are not completely dominated by the priors (are completely “random”).
