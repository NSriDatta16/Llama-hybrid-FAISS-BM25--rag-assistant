[site]: crossvalidated
[post_id]: 642693
[parent_id]: 642687
[tags]: 
Unfortunately, almost every step you are taking warrants constructive criticism. It is not obvious that splitting is the way to go. Frank Harrell advocates for bootstrapping the entire pipeline, especially when the sample size is fairly small (he’s given $20,000$ as a loose threshold). Skewed features aren’t necessarily problematic. There can be good reasons to transform features, but there are minimal assumptions about feature distributions in machine learning. Transformations are mostly to capture a particular relationship between the features and the outcome, e.g., quadratic. This might be reasonable, but it is worth considering if you should scale your features at all. This is done in deep learning to help the numerical optimization of such a complicated loss function. You are not in such a situation (though scaling features on different scales might impact the regularization). Constant features have no predictive utility and should be dropped. Near constant features might be quite informative when they deviate from “business as usual” and aren’t necessarily good to drop. Feature selection is surprisingly unstable, and it isn’t clear that you will benefit from doing so. Regularization can handle $p>n$ situations, and domain knowledge might be able to reduce the features, too. Removing correlated features isn’t necessarily a good idea. The information might be redundant, but each might have unique information about the outcome. Fair enough It is quite likely that the majority category will usually have the highest predicted probability, so you might wind up with a ton of “classifications” that way, and this will affect your classification metrics. Perhaps better and often advocated by statisticians is to directly evaluate the raw predictions with a strictly proper scoring rule like log loss or Brier score.
