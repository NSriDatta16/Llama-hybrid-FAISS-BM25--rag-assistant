[site]: crossvalidated
[post_id]: 527191
[parent_id]: 
[tags]: 
Callibration after oversampling

We have build a credit scoring model with OptBinning library in python. In the process we oversampled the minority class and now we want to callibrate it back to the dataset before oversampling. This table shows some dummy data of the resulting model, since I cannot show the full model here: As described in this paper Section 2.2 here , we will run a logistic regression on the dataset before oversampling with just one variable to explain the binary target variable. This "one variable" can either be the sum of the Points or we could use $$ {\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x)}}} $$ the probability as calculated on the oversampled dataset. $beta_1 x$ is the sum over WoE * Coefficient for each variable. What makes more sense in this case and why?
