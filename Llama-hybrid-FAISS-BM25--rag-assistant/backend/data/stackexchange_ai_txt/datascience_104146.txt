[site]: datascience
[post_id]: 104146
[parent_id]: 
[tags]: 
Building a linear regression model for every combination vs only one Machine Learning model

So my question is more on the conceptual side. Given a dataset, I want to predict a given continuous variable Y. Now, there are 3 features , 2 categorical and one numerical (integer only) . I know that if I create a combination of the 2 categorical features I can use the numerical feature as an independent variable in the linear regression model to predict Y. For instance, for a combination of the 2 categorical features I build a linear model where the numerical feature is the independent variable. This yields good results because the relationship between Y and the numerical feature for a given combination will always be linear. However this also means that I might have to build 1000 linear regressions, one per combination. This of course sounds a bit strange to me, since I can use for instance a decision tree model without the need of creating the combinations of the categorical features. I'm trying to see the pros and cons of each methodology, but I'm having a hard time. Can anyone shed some light into this problem? Example: Imagine this dataset is from a massive bakery. The dataset is very large, over 100k instances. The categorical values are a machine ID and the recipe that it does. So the combinations are (machine, recipe). And the numerical feature is the number of times you do the recipe. Therefore I'm trying to predict the how long a given process will take. Now obviously, for every combination, the more you do the recipe, the more time it will take. The thing is, it seems super odd to just create a linear equation for every combination. Sure it works, but you end up with a enormous amount of linear equations which seems like it will take a lot more computational power than one single model.
