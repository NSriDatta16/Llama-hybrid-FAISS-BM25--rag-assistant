[site]: crossvalidated
[post_id]: 282609
[parent_id]: 282602
[tags]: 
I see a few conceptual mistakes here. First of all, if $Z$ and $W$ are two variables in logistic regression, there should be no constraint on their input value. Logistic regression is a discriminative model, so no assumption is made about the distribution of the input variables. Therefore the values of $Z$ and $W$ should not be mutually exclusive. You are allowed to have an entry for both, which you neglected to allow when listing your values. Thus you have nine possible entries, not six. Secondly, as you noted with one variable, each categorical variable need only one less binary variable than the number of classes. So with two categorical variables, each with three classes, you have 5 inputs. One bias (which is always one), and four binary variables representing the two 3-class categorical variables. We can look at the model as having five parameters. $\beta_B,$ $\beta_C,$ $\beta_F,$ $\beta_G,$ and $\beta_0,$ where each subscript represents the categorical value it represents, and $\beta_0$ is the bias term which represents the "reference" values $Z=A$ and $W=E.$ (This is an arbitrary choice.) In general, the bias term absorbs the reference value of every categorical variable in the model. Now, let's denote $l = \beta' X$ as the log-odds of an observation given $X$ (where $X$ is a five-vector of binary variables representing an observation and $\beta$ is the vector of five parameters). (Note that by "odds of an observation", what I mean is that it's the odds ratio of a "one" observation of your target variable given $X$ with respect to a "zero" observation given $X$.) Suppose we have $X_1 = (1,0,0,0,0)'$ and $X_2 = (1,0,0,1,0)'.$ This corresponds to $(Z,W) = (A,E)$ and $(Z,W) = (A,F)$ respectively. As well, $l_1 = \beta' X_1$ and $l_2 = \beta' X_2.$ Then, $$ \frac{\text{odds}(X_2)}{\text{odds}(X_1)} = e^{l_2 - l_1} = e^{\beta_F}. $$ So $\beta_F$ is the log of the odds ratios between observations $(Z,W) = (A,F)$ and $(Z,W) = (A,E).$ However, it's easy to see that this relationship would hold independent of what the value of $Z$ is. That is, if $Z=Z_0,$ then using the same analysis, we would find that $\beta_F$ is the log of the odds ration between observations $(Z_0,F)$ and $(Z_0,E),$ no matter what the value of $Z_0.$ Thus, $\beta_F$ is the log of the odds ratio between $W=F$ and $W=E.$ This is simply because of the choice of $E$ as the base line value. You can easily extend this argument to any value of a categorical variable, with respect to the choice of reference value. In general, if $Z$ is a categorical input variable in logistic regression with $k$ levels, then it contains $k-1$ parameters, each of which represents the log-odds of an observation given a particular value of $Z$ with respect to an observation given the reference value of $Z.$ This is true regardless how however many other categorical variables are in the model.
