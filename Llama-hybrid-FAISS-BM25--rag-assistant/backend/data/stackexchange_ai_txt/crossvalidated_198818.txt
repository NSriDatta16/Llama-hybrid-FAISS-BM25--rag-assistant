[site]: crossvalidated
[post_id]: 198818
[parent_id]: 173728
[tags]: 
There's been some work recently on dynamically assigning word2vec (skip gram) dimension using Boltzmann machines. Check out this paper: "Infinite dimensional word embeddings" -Nalsnick, Ravi The basic idea is to let your training set dictate the dimensionality of your word2vec model, which is penalized by a regularization term that's related to the dimension size. The above paper does this for words, and I'd be curious to see how well this performs with phrases.
