[site]: crossvalidated
[post_id]: 453543
[parent_id]: 
[tags]: 
To difference to not difference, that is the question

Definitions Skip if you know what are second order stationary processes and what we mean by integrated processes. Def.1: Suppose that we have a set of $K$ input variables and a target variables $(X_t, y_t)$ that evolve over time. Let me now define second order stationary processes. A process $(Z_t)$ is second order stationnary iff The unconditional expected value of the process is finite and does not depend on time, i.e., $E(Z_t) = m ; The unconditional variable of the process is finite and does not depend on time, i.e., $Var(Z_t) = \sigma_z^2 The unconditional $h$ period autocovariance function is finite, does not depend on time, but only depends on the number of periods $h$ , i.e. $Cov(Z_t, Z_{t+h}) = f(h) . Def. 2: A process $(Z_t)$ is integrated of order $d$ , noted as $(d)$ , iff $(1-L)^dZ_t$ is second order stationary where the lag operator is defined as $L^j Z_t:= Z_{t-j}$ . We'll limit ourselves to cases where $d \in \{0,1\}$ . Now that we're clear about the definitions, here is my question. Question Suppose that at least some variables $(X_{t,k})$ within the set of inputs $(X_t)$ are I(1) and the target $(Y_t)$ is I(1) and that my intention is to forecast the level of $(Y_t)$ . Let's call $Y_{t+h|t}$ my $h$ -steps ahead forecast for the target variable given observations up to time $t$ and let's say that I use a square loss function as the relevant performance metric, i.e., $L(Y_{t+h}, Y_{t+h|t}) := E \left( (Y_{t+h} - Y_{t+h|t})^2 \right) $ . From an econometric point of view, standard practice would be to transform all inputs and the target using first differences where necessary so that we only work with $I(0)$ variables. There are some exceptions. For example, if you work with a linear model of the form $Y_{t+h} = \beta'X_t + \epsilon_{t+h}$ and it so happens that $Y_{t+h} - \beta' X_t$ is $I(0)$ (i.e., you have a cointegration relationship), you can use OLS directly on levels data. The problem with OLS will arise only if you can't "clean" the stochastic trend behavior in $Y_t$ : if your residuals is $I(1)$ , your OLS estimator behaves randomnly even in the limit $T \rightarrow \infty$ and you get, well, nonsense. So, my question is essentially, what do people do in Machine Learning? I know that many people will do the same thing: stationnarize the data by taking first differences or the first difference of logarithms (to get growth rates) and then work out a model. I'm just wondering if there aren't papers on what happens when you use non-linear models like Neural Networks, kernelized SVR or Kernel Ridge Regression on data that isn't second order stationnary. An example for context would be trying to forecast industrial production in the US. From a statistical perspective, if you do have data that is exactly $I(1)$ , the transformation is either trivially the first difference or it doesn't matter. But $I(1)$ might be an overkill sometimes -- i.e., it's going to dampen frequencies too far away from 0. The issue is that you don't know if $I(1)$ is just enough or if it is too much. Maybe you'd need $d=0.87$ , to borrow from the AFIRMA type of models. Or maybe there is a better decomposition of long term and short term fluctuation that you should consider -- i.e., you could first pass a linear regression with an $n^th$ order polynomial in time and, then, model the residual using a machine learning model. So, any references and comments would be greatly appreciated.
