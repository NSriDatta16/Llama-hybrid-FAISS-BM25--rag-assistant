[site]: datascience
[post_id]: 15883
[parent_id]: 13760
[tags]: 
ARI and Coleman-Liau are basically the same thing with different numbers. Gunning Fog and Dale-Chall are similar in that they're simple functions of how "complicated" the words and sentences are, though at least they appeal to a notion of "complicated words", though that just pushes the problem somewhere else. There are more and more like this, and I know you're not looking for this. It may be stating the obvious, but if I were building such a measure, I'd look at things like misspellings and grammar errors. These are fairly possible to check automatically. Given the success of RNNs in reproducing text in the style of given input, I suspect you could quite successfully apply deep learning to learn what about a text makes it readable, or at least find whether models trained on high or low readability texts seem to find a given new input more probable.
