[site]: crossvalidated
[post_id]: 457258
[parent_id]: 
[tags]: 
Why do earlier hidden layers learn slower?

I'm reading chapter 5 of Nielsen's textbook about vanishing gradients . He states: In at least some deep neural networks, the gradient tends to get smaller as we move backward through the hidden layers. This means that neurons in the earlier layers learn much more slowly than neurons in later layers. And while we've seen this in just a single network, there are fundamental reasons why this happens in many neural networks. The phenomenon is known as the vanishing gradient problem. He goes on to say: One response to vanishing (or unstable) gradients is to wonder if they're really such a problem. Momentarily stepping away from neural nets, imagine we were trying to numerically minimize a function f(x) of a single variable. Wouldn't it be good news if the derivative f′(x) was small? Wouldn't that mean we were already near an extremum? In a similar way, might the small gradient in early layers of a deep network mean that we don't need to do much adjustment of the weights and biases? My confusion with the vanishing gradient problem stems from this particular statement: In at least some deep neural networks, the gradient tends to get smaller as we move backward through the hidden layers. This means that neurons in the earlier layers learn much more slowly than neurons in later layers. And my question relates to this particular question: Wouldn't it be good news if the derivative f′(x) was small? Wouldn't that mean we were already near an extremum? Specifically, don't small derivatives indicate that the parameters have essentially converged and, therefore, finished learning? Since the gradients of the earlier layers are smaller compared to later layers, shouldn't this mean that earlier layers learn faster?
