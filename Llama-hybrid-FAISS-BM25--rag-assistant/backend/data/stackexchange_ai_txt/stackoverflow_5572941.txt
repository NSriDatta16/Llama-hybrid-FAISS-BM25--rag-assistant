[site]: stackoverflow
[post_id]: 5572941
[parent_id]: 5571519
[tags]: 
There is vast of Information Extraction algorithms, to name a few: regular expressions, statical methods, machine learning based, dictionaries, etc. You can find a complete overview on methods in this survey . Yes, it is hard to build a tool, which find tags with high precision, because it requires a lot of testing and tuning. The -- easiest to implement -- algorithm for finding tags will consists of two steps: Extract candidates for tags Find most significant tags - most disti. In the first step you can take one of two approaches: Use entity names to use as tag candidates (here you need to use Information Extraction framework) Use nouns or noun groups as tag candidates (here you need to use part-of-speech tagger) In the second step, you should use tf-idf to weight tags across document corpus and discard all tags which has tf-idf weight below a given trash-hold If you need a more powerful algorithm look for topic detection frameworks or research papers on this topic . Check also LSA , after wikipedia: Latent semantic analysis (LSA) is a technique in natural language processing, in particular in vectorial semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.
