[site]: crossvalidated
[post_id]: 591219
[parent_id]: 590140
[tags]: 
For clustering large datasets in high-dimensional spaces, it is pretty standard to proceed in two steps, first reducing the dimension and then clustering the new data points in the dimension-reduced space. This is mainly done for performance reasons and because clustering is quite sensitive to the curse of dimensionality . In your case, you want to do the first step (dimensionality reduction) with random projection and the second step (clustering) with LSH. Thus, in principle, this is a reasonable plan. How good this works depends of course on the data set and the particular methods you use for random projection and for LSH. The problem is that it is difficult to evaluate the results, because there is no definition of "correct" clustering, and the clustering evaluation metrics are, in my experience, not very helpful. Anyway, maybe you have a way to check the quality of your clustering, and if it is only via evaluation metrics of your higher-level goal (CTR, ROI, ...). Of course, this could involve an unjustifiable effort. Concerning your question about redundancy: Hashing is often very fast. If, in addition, you know that your LSH is not particularly sensitive to the curse of dimensionality, the reduction in dimension before hashing might not be necessary. Some final remarks : LSH methods are, in my experience, not optimal clustering methods, but if you want to substitute them with standard clustering methods , runtime performance may quickly become a problem. I have had good experiences in similar situations with UMAP followed by e.g. HDBSCAN . UMAP is very good in preserving the grouping of the data, also with strong dimensionality reduction. This is even the case when reducing to two dimensions, which is why it is also a popular visualization tool. Because it is nonlinear, it does this much better than random projections. Another possibility would be t-SNE . However, both are, of course, much slower than random projections. Very fast clustering, appropriate for high dimensional spaces, is also provided by BIRCH if the option n_clusters is set to None . Dimensionality reduction always reduces the information in your dataset. If $n$ is the dimension of the reduced space and $m$ the size of your data set, then, according to the Johnsonâ€“Lindenstrauss lemma , the ratio $\frac{n}{log \;m}$ should not become too small.
