[site]: crossvalidated
[post_id]: 319360
[parent_id]: 
[tags]: 
In GLMs, why do we solve score(beta)=0 instead of just minimizing the negative log-likelihood?

When we search for a numerical way to find $\hat{\beta}$ in a GLM (say, a logistic regression), we could do a numerical optimization (minimization) of the negative log-likelihood. But instead, we go one step further, and first compute the score function, to then equvialently look for the value of $\beta$ that sets $s(\beta) = 0$. Why do we do this extra step? They are both iterative numerical procedures. Is solving this equation system computationally easier than minimizing the negative log-likelihood?
