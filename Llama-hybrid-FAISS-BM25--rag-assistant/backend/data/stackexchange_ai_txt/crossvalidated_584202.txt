[site]: crossvalidated
[post_id]: 584202
[parent_id]: 
[tags]: 
Is data leakage a concern when using an ensemble of leave-one-out predictions?

I am new to stacking. I have a dataset with N samples and 7 tables corresponding to different data types, plus a binary label. Some tables have dozens of features, other have many thousands. I train 10 ML models (SVM, KNN, XGB etc.) separately on each table, excluding one sample and forming a prediction on the excluded sample. Repeating for each sample, I get a Nx70 '1st-level' matrix, where each feature is the prediction for a particular model on a particular table. Performances vary, up to about 0.8 AUROC. Now, when I train a random forest on this matrix, again in a leave-one-out fashion, I get about 0.94 AUROC, which is suspiciously higher than the state-of-the-art for my problem. I am concerned because in some sense, the prediction of sample 1 is using the 1st-level prediction for sample 2, sample 3 etc., which have been formed during the first step, using also sample 1. However I cannot imagine how the label could propagate through the predictions. I would like to test this on a dummy dataset but I am not sure how to design this experiment. The proper x-validation scheme (exclude one sample from the start) would take too long to compute on the real data (takes about 12 hours on my laptop, and would need to be repeated N times). Any comment or help welcome! Thanks a lot
