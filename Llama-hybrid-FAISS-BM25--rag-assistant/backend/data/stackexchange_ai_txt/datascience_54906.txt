[site]: datascience
[post_id]: 54906
[parent_id]: 
[tags]: 
Predicting word from a set of words

My task is to predict relevant words based on a short description of an idea. for example "SQL is a domain-specific language used in programming and designed for managing data held in a relational database" should produce words like "mysql", "Oracle", "Sybase", "Microsoft SQL Server" etc... My thinking is to treat the initial text as a set of words (after lemmatization and stop words removal) and predict words that should be in that set. I can then take all of my texts (of which I have a lot), remove a single word and learn to predict it. My initial thought was to use word2vec and find words similar to my set. But this doesn't work very well, as I don't want similar words but words that go together in many sentences, which is sort of the task that word2vec is training on to create its embedding... How would you model this problem? I don't think RNNs are relevant here because I want to use a set of words - they do not have any order, I'm not trying to predict the next word in any way. However, the size of the set could vary I think... What kind of NN architecture would you use for this sort of problem? UPDATE: To further explain the purpose of the predictor and why word2vec doesn't work for me: I did train a word2vec model on my data (which is very domain specific and I have lots of data on it). Now my problem is that it produces words that are similar to the ones I input to it... and not ones that could be in the context. e.g: I input "computer manufacturer" to get most similar words and i get: (u'equipment_manufacturer', 0.7463390827178955), (u'manufacture', 0.7410058975219727), (u'manufacturer_distributor', 0.7196526527404785), (u'distributor', 0.6950951814651489), (u'desktop_computer', 0.6632821559906006) These are terms that have similar meaning... But I'm not looking for similar meaning. I would be more interested in getting "Dell" for example as the output. Another example is to associate "Apple_inc" with informatics, OS, hardware.. - not similar words, but related ones... So my thinking now is to build a word2vec CBOW model, but not to use just the embeddings that are created but actually use the network to predict new words from context...
