[site]: crossvalidated
[post_id]: 230636
[parent_id]: 
[tags]: 
How to Deal with Large Number of Dummy Variables in Machine Learning?

I have a cross-sectional real estate dataset with information on roughly 100000 properties, including rental price, square meter size, number of bedrooms etc. In addition, the dataset contains information about the region for each property. The total number of regions in the dataset is 400. I would like to play around with different machine learning methods in the caret package to predict rental prices using the relevant features in the dataset. I obviously need to incorporate regional information to capture region-specific price effects. Update: To be more formal, I have the following two model specifications in mind: $y_i=\beta_0+\beta_1x_{i,1}+\beta_2x_{i,2}+\beta_3D_{i,1}+\beta_4D_{i,2} + \epsilon_i$ where $y_i$ refers to the rental price of property $i$, $x_{i,1}$ denotes square meter size, $x_{i,2}$ denotes the number of rooms, $D_1$ is a matrix of dummy variables capturing different characteristics like balcony, kitchen, cellar and so on, and $D_2$ is a matrix containing the 399 regional dummies. Alternatively, the model could be specified as follows: $y_i=\beta_0+\beta_1x_{i,1}+\beta_2x_{i,2}+\beta_3D_{i,1}+\gamma d_{j} + \epsilon_i$ where $d$ is a vector containing the median or average square meter price in each region $j$. I guess inclusion of interaction effects could also make sense (see comment below by seanv507), but I am not sure about them right now. The obvious advantage of the last equation is that I only need one variable in my model to proxy for a regional effect. However, I also do not see too much of a problem for the first equation given the relatively large number of observations in the dataset. I have two questions: Which of the two specifications would be preferable or are there better alternative ways to deal with this issue? Which machine learning methods could be promising for this type of model?
