[site]: crossvalidated
[post_id]: 291503
[parent_id]: 291436
[tags]: 
As per the comments above, the question is unclear. Unless there is strong evidence that the model should consist of two line segments, it is most often a mistake to model data that way. In order to prove that such a procedure is correct, one has to offer evidence to that effect. Now the counterargument to that claim is that the change-point exists by "assumption." But is that really statistics or a mathematical exercise in futility? I claim the latter, and unless someone can show a concrete example of a physical model process that results in such a horrible assumption, I will stand my ground. Later in this post I show, as an example, how a biexponential, i.e., a single function, fits the data, and it has only four parameters, whereas two line segments have 5, because the cut point is a variable. Now to be clear, one can continue the process of cutting the data into segments and solve for exact line segments connecting each and every adjacent point. Then the residuals are zero, the correlations are all ~1, and we have made a model that is a fit of a perfect fit with no noise. What's wrong with that picture? 1) It isn't a proper data model. 2) I could have solved for each point with a Lagrange polynomial so it is not unique. 3) It is highly unusual to have a continuous model of some physical process that has a discontinuous derivative at a point. Thus, without proof amounting to an unusual and changed model at some cut point, we have to assume that the model is posited in error. As there is no evidence provided to the effect that two separate physical processes have given rise to the data, most likely such a model is incorrect. And, if there is any such indication please provide it. Without that proof of principle, the question is ambiguous and should be closed. Let us assume we have a 2D point, which is the literal interpretation of a "change point," i.e., $(x_c,y_c)$ . Now suppose we do not and we only know $x_c$ , then let $y_c=\dfrac{y_L(x_c)+y_R(x_c)}{2}$ , where $y_L=m_L\,x+B_L$ for the left hand line and $y_R=m_R\,x+B_R$ is the right hand line. Suppose we do not like that, then do something else, like a weighted average. Anyway, we now assume that by hook or by crook we have $(x_c,y_c)$ . Now we need new lines; $y'_{L}$ and $y'_{R}$ , and since we already have one point, we only need one more point and one logical extra point is to require each respective line to go through its data centroid $(\bar{x}_{L},\bar{y}_{L})$ , and $(\bar{x}_{R},\bar{y}_{R})$ . Now, those centroids are merely the mean $x$ and $y$ coordinates of the left hand data, and the right hand data. Then for the left hand new line we have simultaneous equations $$y'_{L}-y_{c}=m'_{L}(x'_{L}-x_{c}),$$ and $$y'_{L}-\bar{y}_{L}=m'_{L}(x'_{L}-\bar{x}_{L})$$ whence $$m'_{L}=\dfrac{y_{c}-\bar{y}_{L}}{x_{c}-\bar{x}_{L}}$$ thus $$y'_{L}=\dfrac{y_{c}-\bar{y}_{L}}{x_{c}-\bar{x}_{L}}(x'_{L}-\bar{x}_{L})+\bar{y}_{L}$$ and similarly $$y'_{R}=\dfrac{\bar{y}_{R}-y_{c}}{\bar{x}_{R}-x_{c}}(x'_{R}-\bar{x}_{R})+\bar{y}_{R}$$ Try it and note there are many many answers to your question. @whuber There are assumptions being made that are being ignored. The fitting in the question does not pass my eyeball test, never mind more sophisticated testing. To show what my eyeball is seeing I have put in some hand drawn eyeball regression lines. My claim is that these lines are as spurious as the actual OLS regression lines, maybe less. Compare this illustration with the OP's and see if you can see what I see. I have two sizable problems with the original regressions shown. The first is that both regression lines have visually obviously flattened slopes. The second is that the left line does not have homoscedastic residuals. Now, to proceed with calculation of slope confidence intervals, where I, admittedly personally, have no confidence in the aptness of the regression lines shown is derivative enough to give me pause for thought. In particular, there is so-called omitted variable bias , and this is more pronounced at the end points of the regressions. I would suggest using Theil regression to reduce this bias. However, as stated the problem has no exact solution, and all that can be done is to do a median (approximate) regression for this bivariate problem. Supply the actual data, and I will show this. The results are very dependent on what the cut point is, and the regression method used. For example, if I use @Scortchi 's data and a slightly lesser x-value cut point and Passing-Bablok conversion regression (type III) for the left hand points and Passing-Bablok regression type I for the right hand points I get significant slopes and intercepts as below: Now, the left hand points would not have either significant slope nor intercept using ordinary least squares regression, but do using Passing-Bablok regression. If I move the cut point to the right, as for Scortchi this significance for the left hand regression disappears. Next thought, I think the data is inadequate to prefer two lines over a single function with local terms; let us say a mixture of two exponential functions, e.g.,
