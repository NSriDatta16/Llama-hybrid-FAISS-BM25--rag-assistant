[site]: crossvalidated
[post_id]: 330559
[parent_id]: 
[tags]: 
Why is tanh almost always better than sigmoid as an activation function?

In Andrew Ng's Neural Networks and Deep Learning course on Coursera he says that using $tanh$ is almost always preferable to using $sigmoid$. The reason he gives is that the outputs using $tanh$ centre around 0 rather than $sigmoid$'s 0.5, and this "makes learning for the next layer a little bit easier". Why does centring the activation's output speed learning? I assume he's referring to the previous layer as learning happens during backprop? Are there any other features that make $tanh$ preferable? Would the steeper gradient delay vanishing gradients? Are there any situations where $sigmoid$ would be preferable? Math-light, intuitive answers preferred.
