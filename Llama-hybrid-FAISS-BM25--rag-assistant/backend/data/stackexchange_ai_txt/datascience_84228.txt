[site]: datascience
[post_id]: 84228
[parent_id]: 
[tags]: 
Does the output of the Sequence-to-Sequence encoder model exist in the same semantic space as the inputs (Word2vec)?

Does the output generated from the LSTM encoder module exist in the same semantic space as the original word vectors? If so, say for example we have a sentence and we pass it through the encoder to get an encoded output and then we also calculate the average of word vectors for the same sentence separately, will the two new vectors (encoded and average) be comparable? Will their euclidean distance be relatively small?
