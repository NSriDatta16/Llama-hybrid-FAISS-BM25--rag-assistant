[site]: datascience
[post_id]: 38763
[parent_id]: 
[tags]: 
Handling imbalanced data by deleting over represented rows vs. adding under represented rows

I am currently working with a very imbalanced data set (frauded credit card data from kaggle, which has 492 rows of frauded cards and over 280,000 rows of non-frauded cards). As much as I know, there are 2 ways to handle imbalanced data without adjusting penalty: over-sampling the under-represented data - by creating a lot of copies of the frauded data under-sampling the over-represented data - by not using a lot of the non-frauded data Right now I go with under-sampling (since my computer is not that strong and I do not want to waste hours for answers). Technically what I do, is create a 50-50 dataframe, which has all of the under-represented rows and randomly selected over-represented rows. Problem is, every time I run the notebook, the algorithms use different random okay data and there are different results. When tweaking xgboost, there are different optimal parameters every time I run the notebook. What should I do? Should I save as csv one of the balanced df I created and call it a day? Should I creat a lot of copies from the frauded cards? What is the best way to tackle the situation?
