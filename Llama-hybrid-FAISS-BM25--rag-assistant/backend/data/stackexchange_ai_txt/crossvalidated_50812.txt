[site]: crossvalidated
[post_id]: 50812
[parent_id]: 
[tags]: 
Variance explained for averaged values

I am trying to work out how good my particular model is at explaining some observed data. The problem here is that the observed data takes the form of averaged (mean) values for each of my predictive scores. When performing a simple correlation, I get a really high R-squared value (and this is repeated for independent data sets), which I am assuming means that there is a consistent relationship between my predictive value and the observed data. However, if I want to estimate how much of the variance in observed data I explain with my predictive score, how would I do this with averaged observed values? My underlying concern is that although I have a good correlation in all cases, there may be many underlying factors that also drive my observed data that are simply being 'averaged out' within each category, and as such, my r-squared value is meaningless. Just as an example, if my underlying observed data was: x I get r-squared of 0.911. However, if I average my observed data (which is all I can measure in my case), I would get: x1 Thus telling me that my predictive score explains almost all of the variation in the observed data, when a better representation above (x vs y) tells me I am explaining 91% of the variation. Thanks.
