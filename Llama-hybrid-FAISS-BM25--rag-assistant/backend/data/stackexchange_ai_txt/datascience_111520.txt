[site]: datascience
[post_id]: 111520
[parent_id]: 46052
[tags]: 
Higher polynomial degrees correspond to more parameters. Typically, a model with more parameters will fit the data better, as it will have higher likelihood (and the goal is to maximize the log-likelihood of the parameters). Yes it will overfit, but overfitting should still mean higher accuracy on the training data. So why would more parameters stop fitting the training data? That's because of the Bayesian Occam's razor effect. Models with more parameters do not necessarily have higher marginal likelihood. Think about it as follows; as your parameters increase, the model needs to spread the probability mass over the solution space ever more thinly, so the model will tend to be flat (i.e. counter intuitively it will underfit). This is referred to as the conservation of probability mass principle. For more on this, refer to Kevin P. Murphy's "Machine Learning: A Probabilistic Perspective" 5.3.1 Another suspect would be the curse of dimensionality , as the solution space will grow exponentially as you add more parameters. So although it might seem that the model suddenly underfits the training data, the solution space has grown too much after increasing the degree beyond a certain threshold.
