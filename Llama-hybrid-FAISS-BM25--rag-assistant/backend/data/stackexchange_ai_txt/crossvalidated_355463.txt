[site]: crossvalidated
[post_id]: 355463
[parent_id]: 355448
[tags]: 
Non-Bayesian methods may be non-probabilistic, or they may define a probability distribution on the data that depends on some fixed (but perhaps unknown) parameters. Bayesian methods are distinct in that they use probability distributions to express knowledge/uncertainty about both the data and the model/parameters. In a Bayesian approach, we can think of the data as having been drawn from some probability distribution. But, we don't know know which one. To express this uncertainty, we consider a set of possible data-generating distributions. We then define a distribution over these possibilities, which represents our degree of belief in each. Before we see the data, this distribution is called the prior, and it represents our pre-existing knowledge about the problem. After seeing the data, we update this distribution using Bayes' rule, then call it the posterior. But then isn't this just the definition of supervised machine learning in general? As above, supervised learning need not be Bayesian. For example, we may learn the parameters of a classifier, but not consider a distribution over many possible classifiers. Even if we fit many classifiers (as in ensemble methods), we need not treat this as a probability distribution, or update it using Bayes' rule. And, as Peter Flom pointed out, Bayesian methods are not limited to supervised learning.
