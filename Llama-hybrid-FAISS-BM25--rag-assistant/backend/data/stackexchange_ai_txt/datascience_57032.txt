[site]: datascience
[post_id]: 57032
[parent_id]: 57023
[tags]: 
In principle: yes, you will have the same problem as with OLS. However, since xgboost is tree-based (and by that non-parametric), you may get relatively accurate estimates, meaning that values which are below zero or above one would be rare (at least the problem should be less severe than with OLS). In this case you could simply restrict results to $\hat{y} \in [0,1]$ . An alternative would be to do a multiclass classification task where you have 100 classes $y=[1\%,2\%,...,100\%]$ . Boosting usually performs well on classification tasks. Just a hint: xgboost can be somewhat "heavy" in data handling. Boosting tools such as lightgbm are a good alternative. I prefer lightgbm over xgboost .
