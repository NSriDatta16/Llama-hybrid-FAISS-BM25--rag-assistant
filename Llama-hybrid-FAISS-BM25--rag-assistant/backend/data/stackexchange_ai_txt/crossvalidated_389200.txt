[site]: crossvalidated
[post_id]: 389200
[parent_id]: 
[tags]: 
Importance of the prior

The maximum a posteriori objective can be written as $$\widehat{\theta}_\textrm{MAP} = \operatorname*{argmax}_\theta \log P(y\mid\theta) + \log P(\theta)$$ where $\log P(\theta)$ ––the prior––is a term that does not depend on the data. I can see how the prior has regularization effects in the context of optimization, but I'm wondering what its importance is in Bayesian statistics. If priors are supposed to encode "a priori" information of parameters that might be useful during training, then why is it that they are often drawn from probability distributions? For example, Goodfellow et al. suggests that priors should come from high-entropy distributions, such as the normal distribution, because this reflects uncertainty in the parameters. However, it seems counterintuitive to me to introduce a "random" prior if the point of the prior is to reduce uncertainty. How can these two claims be resolved?
