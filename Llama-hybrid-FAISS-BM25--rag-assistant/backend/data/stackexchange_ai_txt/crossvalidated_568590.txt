[site]: crossvalidated
[post_id]: 568590
[parent_id]: 
[tags]: 
Why are my random forest regression predicts valid probability distributions?

I have tabular input data where the labels correspond a probability distribution on five actions, E.g. a row might look like: x_0, x_1, ...., x_n, .1, .1, .3, .0, .5 I am using sklearn's Random forest regressor for this. It handles it well enough, in the sense that the cross-validated cross entropy loss is significantly lower than for a randomly generated prob. distribution. It also always outputs a pdf, i.e. the sum of the 5 predictions always = 1. Frankly, I don't understand why that is happening, since it's just doing linear regression, right? I don't think this is the right way to do it, still. Surely the splitting is not being done to minimize cross entropy loss, which is the right metric. Also the fact that the output sums to 1 is disconcerting. I've considered doing regression on the 4 softmax logits of the target prob. distro instead, but I haven't tried it since I wrote this post (also I don't see why that would work better, other than clearly guaranteeing a prob distro as output). Is there a way to create a custom splitting criterion that will minimize CEL instead of MSE or whatever the regression trees are using currently? Any comments are appreciated
