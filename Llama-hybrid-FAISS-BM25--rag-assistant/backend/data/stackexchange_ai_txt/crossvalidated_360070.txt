[site]: crossvalidated
[post_id]: 360070
[parent_id]: 359725
[tags]: 
The XGBoost portion of the question is answered here: How does gradient boosting calculate probability estimates? Peter Flom's outline, that you can use boolean logic to represent the decision tree output, is correct. The same procedure generalizes to random forests. In the case that each tree votes (that is, makes a binary decision), another perspective is to think of each tree as producing a 1-hot sparse vector. An ensemble of $T$ such trees produces a $T$ hot sparse vector. The average of that vector is the average over each ensemble member. In the case that each tree produces a proportional score in $(0,1)$, the result is a sparse vector with one float, and you can acquire average scores in the same manner.
