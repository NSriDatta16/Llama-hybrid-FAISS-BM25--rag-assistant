[site]: crossvalidated
[post_id]: 586910
[parent_id]: 586906
[tags]: 
You've got this backwards. You don't (typically) look at the data and come up with a hypothesis based on this. This is a recipe for p-hacking and unreliable inferences. Instead, you make your hypothesis first, and then evaluate it with the data. That is a simplification, of course. You can do exploratory data analysis and use your data to generate hypotheses that can be tested on future datasets. Or if you have a large enough dataset, you can split it into two (randomly in the simplest case, but there are ways to do this if there is hierarchical structure in the data as well). You use one half to explore and generate hypotheses, and then test the hypotheses on the other half that you have not examined. All this assumes you are using a Null Hypothesis Statistical Testing framework (which I assume based on your question). The general workflow would be different if you were using a Bayesian framework.
