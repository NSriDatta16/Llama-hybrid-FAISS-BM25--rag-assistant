[site]: datascience
[post_id]: 128194
[parent_id]: 
[tags]: 
How do I automate testing and comparison of the performance of models with different layer depths, layer types, and unit counts?

I am testing the effects of different layer counts/depths, unit counts, and layer types for natural language processing. I made a Kaggle notebook where I manually create different layers and then train them on the same input. This is the full notebook . This is the code snippet for building the models: def generate_model( model_optimizer, embedding_dimension=32, layer_count=1, unit_count=1, activation_function='selu', kernel_initializer_function='glorot_normal', ): input_layer = tf.keras.Input(shape=(sequence_max_len,)) output_layer = tf.keras.layers.Embedding( input_dim=vocabulary_size, # input_length=sequence_max_len, output_dim=embedding_dimension, mask_zero=True, )(input_layer) output_layer = make_layer_block( output_layer, layer_count, unit_count, activation_function, kernel_initializer_function ) output_layer = tf.keras.layers.GRU( units=unit_count, activation='selu', kernel_initializer='glorot_normal', return_sequences=False )(output_layer) output_layer = tf.keras.layers.Flatten()(output_layer) output_layer = tf.keras.layers.Dense(units=32, activation='selu')(output_layer) output_layer = tf.keras.layers.Dense(units=16, activation='selu')(output_layer) output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(output_layer) model = tf.keras.Model(inputs=input_layer, outputs=output_layer) model.compile( optimizer=model_optimizer, loss='binary_crossentropy', metrics=['accuracy'] ) print(model.summary()) return model This code builds several different models and stores their history. # Python RNG import random # TF RNG from tensorflow.python.framework import random_seed layer_counts = [1, 3, 6, 9, 12] unit_counts = [4, 8, 16, 32, 64] model_performances = dict() for layer_count in layer_counts: for layer_units_count in unit_counts: random_seed.set_seed(2) np.random.seed(2) optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.005) model = generate_model( optimizer_adam, 32, layer_count, layer_units_count ) model_history = model.fit( tokenized_train_text, train_split_df['target'].to_numpy(), validation_data=( tokenized_crossval_text, crossval_split_df['target'].to_numpy() ), epochs=30, callbacks=[ tf.keras.callbacks.EarlyStopping( monitor='val_accuracy', min_delta=0.01, patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True, start_from_epoch=0 ) ] ) name = f"layer_{layer_count}+unit_{layer_units_count}" model_performances[name] = model_history The problem is I have to go through the history dictionary to check the model performances. I also need to specify the layer type. It would also require a big change to make different combinations of layers. That isn't possible with the code I have. Is there any way to automate building models with different layer types, layer unit counts, and layer counts or depths? I am open to using TensorFlow, Weights and Biases, and MLFlow to compare model performance. If you have suggestions other than those, please include code snippets or links to examples I can try now.
