[site]: stackoverflow
[post_id]: 5398795
[parent_id]: 5394200
[tags]: 
You don't say exactly how big the tables are, what kind of tables they are, how the are being populated and how they are being used. So, I'm just going to give some random thoughts :) When you are reporting over large amounts of data, you are basically limited to the speed of your disk system, i.e at what rate your disks deliver the data to MySQL. This rate is usually measured in megabytes/second. So if you can get 100mb/s, then you cannot perform a select sum() or count(*) on a table bigger than 100mb if you want subsecond response time (completely ignoring the DB cache for a moment). Please note that 100mb would be something like 20 million records with a rowsize of 50 bytes. This works up to a point and then everything just dies. Usually when the size of the database becomes larger than available memory and the number of concurrent users increases. You will want to investigate the possibility to create aggregate tables , so that you can reduce the nr of megabytes you need to scan through. It can best be explained by an example. Say that your current measure table looks something like this: measures( user_id ,timestamp ,action ) For every single action performed (logged in, logged out, clicked this, farted, clicked that) you store the ID of the user and the timestamp when it happened. If you want to plot the daily nr of logins from the start of the year, you would have to perform a count(*) over all 100,000,000 million rows and group by the day(timestamp) . Instead, you could provide a precalculated table such as: daily_actions( day ,action ,occured ,primary key(day, action) ) That table would typically be loaded with something like: select day(timestamp) ,action ,count(*) from measures group by day(timestamp) ,action If you had 100 possible actions, you would need only 36,500 rows to store the activities of an entire year. Users running statistics, charts, reports and what not on that data wouldn't be any heavier than your typical OLTP transactions. Of course, you could store it on hourly basis as well (or instead) and arrive at 876,000 rows for a year. You can also report on weekly, monthly, tertial or yearly figures using the above table. IF you can group your user actions into categories of actions, say "Fun", "Not so fun", potentially harmful" and "flat out wrong" you could reduce the storage further from 100 possible actions, down to 4. Obviously, your data is more complicated than this, but you can almost always come up with a suitable nr of aggregate tables that can answer almost any question on an high aggregate level. Once you have "drilled down" through the aggregate tables, you can use all those filters and then you might find it is very possible to select against the lowest detailed table using a specific date , and a specific action .
