[site]: crossvalidated
[post_id]: 320989
[parent_id]: 320984
[tags]: 
PCA simply does projections onto certain linear subspaces. This can always be done, so I don't really see much content to the statement that you quote. The spaces are eigenspaces of the covariance matrix, selected to have eigenvectors of highest value. The better the data distribution shapes itself "around" these principal spaces, the closer the projected data will be to the original ones. Think of a Gaussian in 2D with covariance matrix $\Sigma$ having eigenvalues $\lambda_1 = 4$ and $\lambda_2 = 0.1$. The level curves of the Gaussian will be ellipses with principal axes given by the eigenvectors of $\Sigma$. If you project on the eigenspace $\operatorname{span}\{v_1\}$, your new 1D data will be close to the original data in some sense. So: if the original data is roughly concentrated around ellipsoids flattened around some direction or hyperplanes, the projected data will be close. See here for some theory on PCA and other techniques for dimension reduction.
