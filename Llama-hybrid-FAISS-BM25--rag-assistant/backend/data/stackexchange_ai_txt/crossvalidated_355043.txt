[site]: crossvalidated
[post_id]: 355043
[parent_id]: 39243
[tags]: 
I am trying to interpret the variable weights given by fitting a linear SVM. A good way to understand how the weights are calculated and how to interpret them in the case of linear SVM is to perform the calculations by hand on a very simple example. Example Consider the following dataset which is linearly separable import numpy as np X = np.array([[3,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3]] ) y = np.array([-1,-1, -1, 1, 1 , 1 ]) Solving the SVM problem by inspection By inspection we can see that the boundary line that separates the points with the largest "margin" is the line $x_2 = x_1 - 3$ . Since the weights of the SVM are proportional to the equation of this decision line (hyperplane in higher dimensions) using $w^T x + b = 0$ a first guess of the parameters would be $$ w = [1,-1] \ \ b = -3$$ SVM theory tells us that the "width" of the margin is given by $ \frac{2}{||w||}$ . Using the above guess we would obtain a width of $\frac{2}{\sqrt{2}} = \sqrt{2}$ . which, by inspection is incorrect. The width is $4 \sqrt{2}$ Recall that scaling the boundary by a factor of $c$ does not change the boundary line, hence we can generalize the equation as $$ cx_1 - cx_2 - 3c = 0$$ $$ w = [c,-c] \ \ b = -3c$$ Plugging back into the equation for the width we get \begin{aligned} \frac{2}{||w||} & = 4 \sqrt{2} \\ \frac{2}{\sqrt{2}c} & = 4 \sqrt{2} \\ c = \frac{1}{4} \end{aligned} Hence the parameters (or coefficients) are in fact $$ w = [\frac{1}{4},-\frac{1}{4}] \ \ b = -\frac{3}{4}$$ (I'm using scikit-learn) So am I, here's some code to check our manual calculations from sklearn.svm import SVC clf = SVC(C = 1e5, kernel = 'linear') clf.fit(X, y) print('w = ',clf.coef_) print('b = ',clf.intercept_) print('Indices of support vectors = ', clf.support_) print('Support vectors = ', clf.support_vectors_) print('Number of support vectors for each class = ', clf.n_support_) print('Coefficients of the support vector in the decision function = ', np.abs(clf.dual_coef_)) w = [[ 0.25 -0.25]] b = [-0.75] Indices of support vectors = [2 3] Support vectors = [[ 2. 3.] [ 6. -1.]] Number of support vectors for each class = [1 1] Coefficients of the support vector in the decision function = [[0.0625 0.0625]] Does the sign of the weight have anything to do with class? Not really, the sign of the weights has to do with the equation of the boundary plane. Source https://ai6034.mit.edu/wiki/images/SVM_and_Boosting.pdf
