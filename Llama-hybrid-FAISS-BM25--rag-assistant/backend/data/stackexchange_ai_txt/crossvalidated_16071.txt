[site]: crossvalidated
[post_id]: 16071
[parent_id]: 16069
[tags]: 
If you intend to use the Neyman-Pearson approach then you definitely cannot set the cutoff for significance after the data has been analysed. However, that is not the only approach to statistical inference, and in many cases it is not the best approach. N-P is certainly not well matched to a task that you specify as hypothesis generation. N-P allows you to specify a maximally acceptable rate of false positive results, the alpha level that is most often unthinkingly set to 0.05. The N-P approach mostly deals with decisions about what to do next (significant, discard the null; not significant, accept the null) rather than dealing directly with the evidential meaning of the results. Fisher's approach is incompatible with N-P and treats the data as evidence: it yields a p value that is an index of evidence against the null hypothesis. It is far more often compatible with the needs of scientific experimentation than the N-P approach, in my opinion, in so far as it allows the evidence from an experiment to be considered in light of any other information before any decision is made about what to do next. In contrast to the all-or-none results of an N-P analysis, it encourages experiments to be repeated or refined. Specify the exact p values that you obtained from the experiment and interpret the results thoughtfully. If an interesting finding comes from the data rather than a pre-experiment hypothesis then the results should be taken as preliminary and, if sufficiently interesting, it may be worth repeating the experiment. (You should note that it is fairly common to see statistical analyses and interpretations that are a hybrid of N-P and Fisher: the hybrid is always inappropriate.) To answer your specific questions, I will do so (obliquely) as a pharmacologist: it is unlikely that all of thousands of chemicals will affect cell growth at low concentrations, but certain that all chemicals will do so at a high concentration. Paracelsus famously said (in Greek, I assume) "All drugs are poisons, dose determines effect." If your doses are large then it is not scientifically interesting to find that they are toxic. Perhaps you should test them at a wide range of concentrations (geometrical spacing of concentrations is efficient). The concentration at which a chemical has biological effects is at least as interesting as the magnitude of the effect, and much more interesting than the significance level obtained in an experiment. Make sure that you don't convert a biochemical and experimental design question into a question about statistical significance.
