[site]: datascience
[post_id]: 72114
[parent_id]: 
[tags]: 
PyTorch time series prediction beyond test data

I am currently playing around with pytorch models for time series prediction. I have managed to successfully run a model to predict test data. I was wondering how can I use it to predict beyond test data? I will attach my code below. I essentially want the model to continue running for say 1000 more points after the test data. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import pandas as pd import numpy as np import matplotlib.pyplot as plt import sys from torch.autograd import Variable from sklearn.preprocessing import MinMaxScaler # Load the data set data_test = np.loadtxt('data.txt') training_set = np.reshape(data,(len(data),1)) def sliding_windows(data, seq_length): x = [] y = [] for i in range(len(data)-seq_length-1): _x = data[i:(i+seq_length)] _y = data[i+seq_length] x.append(_x) y.append(_y) return np.array(x),np.array(y) sc = MinMaxScaler() training_data = sc.fit_transform(training_set) batch_size = 4 x, y = sliding_windows(training_data, batch_size) train_size = int(len(y) * 0.5) test_size = len(y) - train_size data_X = Variable(torch.Tensor(np.array(x))) data_Y = Variable(torch.Tensor(np.array(y))) training_set_X = Variable(torch.Tensor(np.array(x[0:train_size]))) training_set_Y = Variable(torch.Tensor(np.array(y[0:train_size]))) testing_set_X = Variable(torch.Tensor(np.array(x[train_size:len(x)]))) testing_set_Y = Variable(torch.Tensor(np.array(y[train_size:len(y)]))) class LSTMModel(nn.Module): def __init__(self, input_dim, hidden_dim, layer_dim, output_dim): super(LSTMModel, self).__init__() # Hidden dimensions self.hidden_dim = hidden_dim # Number of hidden layers self.layer_dim = layer_dim # Inlcude dropout if you want (decided not to since it did not seem to affect results very much) #self.dropout = nn.Dropout(p=0.5) # Builds the LSTM self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) # Readout layer # Linear = Applies a linear transformation to the incoming data: y = xW^T + b self.fc = nn.Linear(hidden_dim, output_dim) def forward(self, x): # Initialize hidden state h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_() # Initialize cell state c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_() # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach())) # Index hidden state of last time step # out[:, -1, :] --> Last time step hidden state out = self.fc(out[:, -1, :]) return out input_dim = 1 hidden_dim = 50 layer_dim = 1 output_dim = 1 num_epochs = 100 lr = 0.1 model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim) criterion = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=lr) decayRate = 1 lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = decayRate) for epoch in range(num_epochs): outputs = model(training_set_X) optimizer.zero_grad() loss = criterion(outputs, training_set_Y) loss.backward() optimizer.step() lr_scheduler.step() model.eval() train_predict = model(data_X) data_predict = train_predict.data.numpy() dataY_plot = data_Y.data.numpy() data_predict = sc.inverse_transform(data_predict) dataY_plot = sc.inverse_transform(dataY_plot) dataY_plot = np.array(dataY_plot) data_predict = np.array(data_predict) ```
