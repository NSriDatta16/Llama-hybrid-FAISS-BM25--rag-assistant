[site]: crossvalidated
[post_id]: 155191
[parent_id]: 
[tags]: 
Is there an supervised learning method (a classifier) that can account for unobserved heterogeneity like a mixed logit can?

I'm just starting to teach myself various machine learning techniques. My background is in more "classical" statistics. I've got an analysis that I've done using a mixed logit to predict linkage in a social network. I'm interested in probability of linkage as a function of geographic distance as well as covariates, where the effect of geographic distance is specified to be random. The model is $$ pr(link_{ij}) = \Lambda\left[\alpha+\left(\beta+\beta_j+\beta_i\right)\text{Distance}_{ij}+X_{ij}'\gamma+\epsilon_{ij}\right] $$ This allows some respondents to know others at a greater distance, and to be known at a greater distance (I am assuming a directed graph). Is there a tree-based (or other) method that could account for these random effects? What is the sense behind it? In essence, random effects models are shrunken versions of fixed-effects models -- can one apply shrinkage to a tree model? If this does indeed make sense (which it might not), how does one implement it? And can one extract probabilities from it? (I'm more interested in the conditional expectation than the best prediction.)
