[site]: crossvalidated
[post_id]: 414222
[parent_id]: 414220
[tags]: 
So it all boils down to the number of parameters in a certain network. More parameters means a higher capacity for a model, i.e. it can approximate more complex functions (or have more complex decision boundaries as you say). On the other hand, less parameters means a lower capacity for the model. The problem that, ideally, you want the model to have just the right capacity to model all useful aspects of the data, while not having enough capacity to model the noise in the data. In the present case, if we have two models a CNN and a Fully-Connected (FC) NN, the latter has many more parameters and thus a higher capacity. However, if the CNN is capable of solving the problem, the more complex FC network is more prone to overfit (because it has a higher capacity and can model the underlying noise). You can also think of it like this. A sufficiently-high capacity network has the ability to memorize datasets (i.e. learn every single one of training samples without having the capability to generalize). FC networks, due to the fact that they have more parameters, are more prone to this than CNNs. Now, the last part has to do with the size of the dataset . Smaller datasets are easier to memorize (and thus more prone to overfitting on), while larger ones are harder. I mentioned previously that FC networks can memorize datasets; this easier in smaller datasets . If fact, you can expect a FC network to most-certainly overfit on small datasets. For more on generalization, I'd recommend reading this post , where I analyze generalization in a bit more detailm
