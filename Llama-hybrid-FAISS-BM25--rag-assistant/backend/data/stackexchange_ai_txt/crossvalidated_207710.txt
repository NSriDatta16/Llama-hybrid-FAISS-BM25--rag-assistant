[site]: crossvalidated
[post_id]: 207710
[parent_id]: 
[tags]: 
Newton's method for regression analysis without second derivative

In regression analysis, instead of gradient descent, Newton's method can be used for minimizing the cost function. However, in Newton's method, we need to calculate second derivative too. For example, to minimize a cost function $F(x)$, we need to find $x_0$ such that $F'(x0) = 0$, which means that we need to find the zeroes of $F'(x)$. And for that, we can use Newton's method: $$ x_1 = x_0 - \frac{F'(x_0)}{F"(x_0)} $$ In the case of linear regression, the cost function is: $$ J = [h(x) - y]^2 $$ In the case of logistic regression, the cost function is: $$ J = y \log(h(x)) + (1 - y)(1 - \log(h(x))) $$ In both the cases, since the cost function's minimum value is $0$, why can't we directly find the zeroes of the function using Newton's method, thus avoiding the calculation of the second derivative?
