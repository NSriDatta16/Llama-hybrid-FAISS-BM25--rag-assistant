[site]: crossvalidated
[post_id]: 192303
[parent_id]: 175302
[tags]: 
An autoencoder is unsupervised since it's not using labeled data . The goal is to minimize reconstruction error based on a loss function, such as the mean squared error: $\mathcal{L}(\mathbf{x},\mathbf{x'})=\|\mathbf{x}-\mathbf{x'}\|^2=\|\mathbf{x}-f(\mathbf{W'}(f(\mathbf{Wx}+\mathbf{b}))+\mathbf{b'})\|^2$ All algorithms that do not use labeled data (targets) are unsupervised. Clustering algorithms are unsupervised. They generate natural groupings of data. Autoencoders are typically used for dimensionality reduction. You can think of them as non-linear PCA . Autoencoders consist of an encoder and a decoder. They kind of fit a zip and unzip functions for compression, learned from the dataset. In the image below there is just one hidden layer. The output $x'$ is the corrupted version of $x$ (some noise is added -- this makes the compression more robust). After the training is performed and the lower dimensional representation is learned, you can get rid of the decoder. Now, with your encoding function you can transform your data set into a lower dimensionality one. With the new dataset now you can repeat the process with an even lower dimensionality. This is the basic idea of stacked autoencoders.
