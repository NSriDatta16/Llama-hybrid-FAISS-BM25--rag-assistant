[site]: datascience
[post_id]: 27454
[parent_id]: 
[tags]: 
Learning a logical function with a 2 layer BDN network - manual weight setting rule question?

So I am trying to construct a 2-layer network of binary decision neurons as proposed by McCullough and Pitts (1943) to learn a logical function (a composition of AND's and OR's) such as: $((\neg x_1\land x_3) \lor (\neg x_2 \land x_1 \land x_3))$ I have gotten as far as attempting to construct a 2-layer network which I believe should be able to learn this function, each neuron $h_1$ through $h_3$ performing a single logical operation. However my lack of understanding in both logic and BDNs as a whole has led me to believe that my network is incorrect when considering the weight setting rules proposed by McCullough Pitts (j is neuron index, k is input index): $w_{jk} = -1$ if $F(x_k)=\neg x_k $ and $w_{jk} = 1$ if $F(x_k)=x_k $ As far as I understood it, you set the weight for each input to -1 if that input is negated in the neuron's formula and 1 otherwise. I was able to calculate these for the first layer with ease, but got thrown by a hole in my knowledge of how the second layer works. I assumed that $h_3$ takes $y_1$ and $y_2$ as input, as opposed to an $x_k$, so I don't see how the above rule can be applied. I was hoping maybe someone with a stronger understanding of neural networks could perhaps explain to me how the weight setting rule is applied to the second layer, I currently assumed it meant $w=1$ if $F(y_i)=y_i$ and visa versa, but I believe this is wrong.
