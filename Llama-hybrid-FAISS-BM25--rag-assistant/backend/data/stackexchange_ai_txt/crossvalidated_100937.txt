[site]: crossvalidated
[post_id]: 100937
[parent_id]: 60190
[tags]: 
Okay, I'm not sure if anyone's still checking this (since it's from a year ago), but here's my response to your question: "I can follow the definitions but the article doesn't really explain why the calculations should be different in the different contexts. Is anyone able to provide an explanation?" The calculations are different because of the purpose of each of these measurements. I'll go through them one by one. For each one, let's consider you taking a test. We know that in the general population, the average score for a certain measurement equals a specific value (let's say 100, to keep it simple). There's an associated standard deviation (let's say, 15, to keep it consistent with your chart) that describes how the data is distributed around that mean. So if you measure and graph a large population, you should get a curve centered at 100 with a certain spread defined by the standard deviation. Let's also define a few terms, to make sure we're clear on what they all mean: "True score" - the sample's true value for this score (i.e. what a perfectly accurate machine will spit out as a value when applied to this sample) "Actual score" - a machine's actual output when applied to a sample Standard Error of Measurement This is basically a measurement of how accurate the machine's scoring is. If, for example, a sample's true score was 90, a perfectly accurate machine would give you a score of 90 every time. The lower the reliability of the machine, though, the more varied would be the responses when you measured the sample. A somewhat accurate machine might give you scores of 85, 87, 91, 90, 92 for five tries. A less accurate machine might give you 93, 81, 96, 88, 89 for 5 tries. Consider this as a new curve based off of measuring the same person a bunch of times. That person has one "true score", but the machine will create a spread of "Actual scores". The less reliable the machine, the more spread out the actual scores. Standard Error of Estimation This description is a little confusing. It says "The standard error of estimation is an estimate of the variability of a candidateâ€™s true score, given their actual score." A true score doesn't vary - it's fixed. I think what this is trying to say is if you have a bunch of people with the same actual score, then this measurement would be a measurement of the variability of their true scores. Here, the more reliable the machine, the less variation among those people's actual scores. This is also (though counterintuitively for many people) true if the measurement machine is really unreliable. If the machine is really unreliable, we basically have no idea where those people should really be scoring, so they are all likely coming from the middle of the true distribution (that's where the most people are) and so there won't be as much variation among them. Standard Error of Prediction The standard error of prediction can be pictured this way. You take one measurement of the sample with your machine. Let's say it gives you a value of 95. You then say, "I predict that if I measure this sample again, I will get a score of x." If the machine is perfectly reliable, you can say you'll get a score of 95. But the more unreliable the machine, the less likely your prediction. Most often, you'd say something like "I predict that if I measure this sample again, I will get a score between x and y." The less reliable the machine, the wider range you have to give to be confident in your prediction. It's higher because, as you say in a comment above, you have two sources of unreliability - your initial measurement and the second (upcoming) measurement. I hope that his helps (and gets seen!).
