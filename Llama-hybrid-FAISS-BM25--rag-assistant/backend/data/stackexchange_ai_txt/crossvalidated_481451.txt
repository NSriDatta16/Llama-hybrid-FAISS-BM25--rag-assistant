[site]: crossvalidated
[post_id]: 481451
[parent_id]: 476942
[tags]: 
If you want a probabilistic inference on the location of $k$ (the change point), mcp is well suited for cases like this. It infers the parameters of change point models using Bayesian Inference ( see details here ). First, let's set things up: df = data.frame(x, y) library(mcp) Now we specify the two-segment model you have in mind (two formulas): model = list( y ~ 1 + x, # Intercept and slope ~ 0 # Joined flat line ) Then we fit it as a poisson model and plot it: fit = mcp(model, data = df, family = poisson()) plot(fit) The blue curves are the posterior distributions of the change point. mcp contains many functions to summarise and check ( summary(fit) , pp_check(fit) ), predict ( fitted(fit) , predict(fit) ), hypothesis test ( hypothesis(fit, "cp_1 > 10") ), etc. on this fit. See the mcp website for more: https://lindeloev.github.io/mcp/ INITIAL ANSWER: If the intercept should be allowed to change at the change point, this could be your model: model = list( y ~ 1 + x, # Intercept and slope ~ 1 # Intercept only (flat line) ) We use the prior to set the constraint that the second intercept can only be a negative change relative to segment 1. It's a Normal(0, 1) which is truncated to maximally be the value of segment 1 ( int_1 is the intercept; x_1 is the slope on x ) at the change point ( cp_1 ): prior = list(int_2 = "dnorm(0, 1) T(, int_1 + x_1 * cp_1)") fit = mcp(model, data = df, prior = prior, family = poisson(), iter = 5000) plot(fit) Notice the weird shape which is definitely non-normal. This is much richer than least-squares methods. Change points often have these kinds of distributions because they largely depend on just a few data points in their vicinity.
