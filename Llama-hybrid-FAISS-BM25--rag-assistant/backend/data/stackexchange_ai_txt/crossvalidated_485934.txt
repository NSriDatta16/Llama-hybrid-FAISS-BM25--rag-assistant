[site]: crossvalidated
[post_id]: 485934
[parent_id]: 485710
[tags]: 
Matching and weighting are both methods of pre-processing observational data to reduce or eliminate bias in an effect estimate. Both methods allow you to estimate average marginal effects in a population. With standard propensity score matching, the estimand is typically the average treatment effect in the treated (ATT), but if a caliper is used, the estimand is the average treatment effect in the matched sample (ATM). With weighting, the ATT and ATM are possible estimands, but others, including the average treatment effect in the population (ATE) or average treatment effect in the overlap (ATO) are possible. When the target estimands of both methods are the same, the effect is interpreted the same way regardless of the method used to estimate it. Your primary question is about the difference in the estimation performance between the two methods. Unfortunately, there is no general answer. The quality of each method depends on the unobserved specifics of the sample it is being applied on. There have been some studies comparing matching and weighting methods in the same dataset, but it is unwise to generalize these results to all datasets. Most simulation studies do not consider the actual ways in which the methods are used, which includes an iterative process of fitting and balance checking without involving the outcome. Here are a few key differences to consider when considering matching and weighting: Asymptotic theory is far better developed for weighting. We know how to estimate valid asymptotic standard errors for propensity score weighted estimates using M-estimation (although we typically use a conservative approximation to these standard errors). We know the theoretical properties of weighting and doubly robust methods that incorporate propensity score weights and we know their their asymptotic standard errors as well, even when using nonparametric (e.g., machine learning) methods to estimate them. While we know some theoretical results for propensity score matching, they are fairly limited to the case of matching imputation (used primarily in economics) and not to propensity score matching as subset selection as it is used in medicine and education. However, mostly thanks to P. C. Austin's extensive simulation work, we have a lot of information about the empirical performance of propensity score matching estimators and their standard error estimators in finite samples. Matching is generally more robust to misspecification of the propensity score model than is weighting. Two propensity score specifications might yield the same matched set, but will generally yield different weighted samples. Although this seems like a blessing for matching, it can also be a limitation, because correct specification of the model might yield a poor quality matched set due to the inherent limitations of the members of the sample, whereas weighting is less affected by the specific units in the sample. Matching is a discrete, nonsmooth method, while weighting is continuous and smooth. There are many more ways to perform matching that can improve its performance. For example, you can exact match on some variables, place calipers on others, and use a variety of distance metrics to pair units that may or may not depend on the propensity score. This can lead to artisanal matching solutions with great robustness properties and potentially high precision even if units are discarded. Typically, propensity score matching is not used in this way, but it is possible to do so. There are not as many ways to perform weighting, though there are many ways to estimate the weights that imbue the weights with certain properties. For example, it's straightforward to estimate weights that yield exact mean balance on chosen covariates using entropy balancing. With stable balancing weights, it's possible to request specific approximate balancing properties while maximizing the effective sample size of the weights (though this method is very rarely used, simply because it has not received exposure). A new weighting method, energy balancing, also makes it easy to (approximately) balance the entire joint covariate distribution without specifying a propensity score model. Generally, if you have a thought, like, "I want my matching/weighting method to do this ", there is a new matching weighting method that does it, though each has its own compromises. Matching methods tend to satisfy requirements by discarding units, which can reduce precision and change the estimand. Weighting methods tend to satisfy requirements by potentially dramatically reducing precision, though they are more effective at retaining the original estimand. There is no clear way to decide between matching or weighting prior to collecting your data, and even with the data in hand, it's not always straightforward to decide which will perform better. It's worth it to try both and see which gives you the best chance at a precise, unbiased estimate based on their observable performance. You want a method that yields excellent balance. You want a method that retains precision. You (often) want a method that retains the target estimand. You want a method that is robust to possible misspecifications of whatever model is implied by the method (though generally this cannot be ascertained in your dataset). Which method will be most desirable depends on too many specifics to make any preemptive recommendation. To try a variety of matching methods, check out the MatchIt package in R, which offers a great deal of customizability. I'm in the process of updating it to improve its performance and expand some of its capabilities. To try a variety of weighting methods, check out the WeightIt package. I wrote it specifically to make it easy to try and compare a variety of weighting methods, and it uses the same syntax as MatchIt . To compare methods side-by-side based on their balancing capabilities, check out the cobalt package. I wrote it specifically to compare many different preprocessing methods and it is compatible with most R packages for matching in weighting, including both MatchIt and WeightIt . See here for an example of it being used to compare propensity score matching and weighting on the same dataset. *I often include citations in my answers but didn't feel like it this time. If you would like any references for any points I made, let me know and I'll find them.
