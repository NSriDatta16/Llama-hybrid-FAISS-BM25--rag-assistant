[site]: crossvalidated
[post_id]: 158777
[parent_id]: 158759
[tags]: 
One plausible explanation is the high correlation of your features. I am by no means an expert on NLP, but you could check the following hypothesis: tf-idf is proportional to the frequency a word appears in a document. Through stemming that frequency is higher, since you do not differentiate flexions, conjugations and so on, of verbs and nouns. The algorithms you mention, like Multinomial Naive Bayes and Random Forest are sensitive to highly correlated features, and their performance degrades with it. SVMs, on the contrary, are much more robust against it. This interesting paper (Tackling the Poor Assumptions of Naive Bayes Text Classifiers) explains why multinomial naive Bayes is sensitive to correlations, and ways to overcome it. Understanding why random forest are sensitive to correlated features is more involved. This paper (Correlation and variable importance in random forests) explains in great detail why the measure used for measuring the relevance of a feature, based on a permutation test, is sensitive to it. This presentation gives you some advice on how to tackle this problem with random forest.
