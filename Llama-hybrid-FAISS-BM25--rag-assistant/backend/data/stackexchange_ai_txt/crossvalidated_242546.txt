[site]: crossvalidated
[post_id]: 242546
[parent_id]: 242541
[tags]: 
I think this means that pretrained embeddings (e.g. using Word2Vec) are not used. Correct Are there any variables/weights (e.g. adjusted during back prop) associated with the word_embedding layer in this RNN? Or is it purely a static lookup table to randomly assigned variables? First option: there are any variables/weights (e.g. adjusted during back prop) associated with the word_embedding layer in this RNN. Do random word embeddings have any advantages over just adding a regular hidden layer with the same dimensions as the randomly initialized word embedding layer? It is the equivalent: adding a random word embedding layer means adding a regular hidden layer.
