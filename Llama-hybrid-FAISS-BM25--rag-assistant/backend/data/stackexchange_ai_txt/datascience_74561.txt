[site]: datascience
[post_id]: 74561
[parent_id]: 62556
[tags]: 
1. Data Preparation By default, train_test_split will assume your train:test split to be 75:25 if you don't declare otherwise for either the test_size or train_size parameters. See here . For your decision tree, you don't declare this and so you test on 25% of the data, whereas you explicitly state in your data preparation for the random forest algorithm that the train size should be 33%. Correcting for this ensures a fair comparison of the two algorithms in this scenario. I have assumed you wish to use test_size=0.33 . 2. Model Differences The random forest classifier is an ensemble method. As its name implies, this method generates a 'forest' of many decision tree classifiers that each train on a different subset of samples and features from the training data. The n_estimators , max_samples and max_features parameters control this. See here . The results of each tree are then averaged with the aim of providing a more robust estimator. In order for the two classifiers to produce the same output, I can set the same random_state and max_depth whilst allowing all features to be used (since by default all samples are used in every tree generated) in the random forest: tree.DecisionTreeClassifier(random_state=0, max_depth=3) ensemble.RandomForestClassifier(random_state=0, max_depth=3, max_features=None) Both now achieve an identical test accuracy of 0.9551 because both classifiers generate the same identical tree. Removing max_features=None allows the random forest classifier to randomly select features for each subset trained upon by each respective tree in the random forest: ensemble.RandomForestClassifier(random_state=0, max_depth=3) Resultantly, the random forest classifier now achieves a test accuracy of 0.9121. Hopefully, this explains why you were getting different results for the decision tree and random forest classifiers.
