[site]: datascience
[post_id]: 27833
[parent_id]: 13429
[tags]: 
The original Seq2Seq paper uses the technique of passing the time delayed output sequence with the encoded input, this technique is termed teacher forcing . There exists a simplified architecture in which fixed length encoded input vector is passed to each time step in decoder (analogy-wise, we can say, decoder peeks the encoded input at each time step). The paper " Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation " combines these both techniques (so it passes encoded input vector along with time delayed output sequence as inputs to decoder).
