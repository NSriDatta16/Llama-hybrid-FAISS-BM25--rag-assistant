[site]: crossvalidated
[post_id]: 6449
[parent_id]: 6444
[tags]: 
Your are trying to estimate model \begin{align} X_1&=\alpha_0+\alpha_2X_2+...+\alpha_nX_n+\varepsilon,\\ X_n&=\beta_0+\beta_1X_1+...+\beta_{n-1}X_{n-1}+\eta. \end{align} For such model ordinary least squares will give biased estimates. Assuming that $X_2,...,X_{n-1}$ are either deterministic or independent from $\varepsilon$ and $\eta$ we have \begin{align} EX_n\varepsilon&=E\left(\beta_0+\beta_1X_1+...+\beta_{n-1}X_{n-1}+\eta\right)\varepsilon\\ &=\beta_1EX_1\varepsilon+E\varepsilon\eta=\\ &=\beta_1E(\alpha_0+\alpha_2X_2+...+\alpha_nX_n+\varepsilon)\varepsilon+E\varepsilon\eta\\ &=\alpha_n\beta_1EX_n\varepsilon+\beta_1E\varepsilon^2 +E\varepsilon\eta. \end{align} So \begin{align} EX_n\varepsilon=\frac{\beta_1E\varepsilon^2+E\varepsilon\eta}{1-\alpha_n\beta_1} \end{align} The main assumption of linear regression is that the error is not correlated to the regressors. Without this assumption the estimates are biased and inconsistent. As we see in this case the assumption is violated, so it is entirely natural to expect unexpected results. The are ways to get statistically sound estimates of the coefficients. This is an extremely common problem in econometrics called endogeneity . Most popular solution is two-stage least squares. In general these type of models are called simultaneous equations . Update In the comments below the attention was drawn to the fact that the OP is just trying to fit two regressions and the model given above might be inappropriate. @onestop provided excellent reference about data modelling vs algorithmic modelling cultures. Nonetheless there is an important point I want to make in this case. Taking into account @whuber answer we can restrict ourselves to the case where we have only two variables, $Y$ and $X$. Suppose having the sample $(y_i,x_i)$ we fit two linear regressions $Y$ vs $X$ and $X$ vs $Y$. The least squares estimates are the following: \begin{align} \beta_{YX}&=\frac{\sum_{i=1}^nx_iy_i}{\sum_{i=1}x_i^2} \\ \beta_{XY}&=\frac{\sum_{i=1}^nx_iy_i}{\sum_{i=1}y_i^2} \end{align} Now from the formulas it is clear that $\beta_{XY}$ and $\beta_{YX}$ coincide only in the case when $\sum_{i=1}^ny_i^2=\sum_{i=1}^nx_i^2$. The corresponding $t$-statistics are then \begin{align} t_{YX}&=\frac{\beta_{YX}}{\sigma_{\beta_{XY}}}, \quad \sigma_{\beta_{YX}}^2=\frac{ \sigma_{YX}^2}{\sum_{i=1}^nx_i^2}, \quad \sigma_{YX}^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\beta_{YX}x_i)^2 \\ t_{XY}&=\frac{\beta_{XY}}{\sigma_{XY}}, \quad \quad \sigma_{\beta_{XY}}^2=\frac{ \sigma_{XY}^2}{\sum_{i=1}^nx_i^2}, \quad \sigma_{XY}^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\beta_{XY}x_i)^2 \end{align} After a bit of manipulation we get the amazing fact (in my opinion) that \begin{align} t_{XY}=t_{YX}=\frac{\sum_{i=1}^nx_iy_i}{\sqrt{\frac{1}{n-1}\left(\sum_{i=1}^ny_i^2\sum_{i=1}^nx_i^2-\left(\sum_{i=1}^nx_iy_i\right)^2\right)}} \end{align} So this illustrates that OP clearly has a problem, since the $t$-statistics must coincide in this case. This of course was pointed out by @whuber. So far so good. The problem arises when we want to get the distribution of this statistic. This is needed since $t$-statistic formally test the hypothesis that the coefficient $\beta_{XY}$ is zero. Without assuming any model, suppose that we want to test the null hypothesis that $cov(X,Y)=0$. We can also assume that we have $EX=EY=0$. Rewrite the $t$ statistic as follows: \begin{align} t=\frac{\frac{1}{\sqrt{n-1}}\sum_{i=1}^nx_iy_i}{\sqrt{\frac{1}{n-1}\sum_{i=1}^ny_i^2\frac{1}{n-1}\sum_{i=1}^nx_i^2-\left(\frac{1}{n-1}\sum_{i=1}^nx_iy_i\right)^2}} \end{align} Now since we have a sample due to law of large numbers we have \begin{align*} \frac{1}{n-1}\sum_{i=1}^ny_i^2\frac{1}{n-1}\sum_{i=1}^nx_i^2\xrightarrow{P}var(X)var(Y)\\ \frac{1}{n-1}\sum_{i=1}^nx_iy_i\xrightarrow{P}cov(X,Y), \end{align*} where $\xrightarrow{P}$ denotes convergence in probability. So denominator of $t$-statistic converges to $\sqrt{var(X)var(Y)}$ under null hypothesis of $cov(X,Y)=0$. Under null hypothesis due to central limit theorem we have that \begin{align} \frac{1}{\sqrt{n-1}}\sum_{i=1}^nx_iy_i\xrightarrow{D}N(0,var(XY)), \end{align} where $\xrightarrow{D}$ denotes convergence in distribution. So we get that under null-hypothesis of no correlation the $t$-statistic converges to \begin{align} t\xrightarrow{D}N\left(0,\frac{var(XY)}{var(X)var(Y)}\right) \end{align} Now if $(X,Y)$ are bivariate normal we have that $\frac{var(XY)}{var(X)var(Y)}=1$, under null hypothesis of $cov(X,Y)=0$, since zero correlation implies independence for normal random variables. The quantity $\frac{var(XY)}{var(X)var(Y)}$ is one, when $X$ is deterministic as is usually the case in linear regression, but then the null hypothesis $cov(X,Y)=0$ makes no sense. Now if we assume that we have a model $Y=X\beta+\varepsilon$, under null hypothesis that $\beta=0$ and usual assumption $E(\varepsilon^2|X)=\sigma^2$ we get that \begin{align} t\xrightarrow{D}N(0,1), \end{align} since \begin{align} \frac{var(\varepsilon X)}{var(X)var(\varepsilon)}=\frac{\sigma^2var(X)}{\sigma^2var(X)}=1 \end{align} But in the OP question we have two regressions, so if we allow model for one, we must allow model for the other and we arrive at the problem which I described in my initial answer. So I hope that my lengthy update illustrates that if we do not assume usual model in doing regression, we cannot assume that usual statistics will have the same distributions.
