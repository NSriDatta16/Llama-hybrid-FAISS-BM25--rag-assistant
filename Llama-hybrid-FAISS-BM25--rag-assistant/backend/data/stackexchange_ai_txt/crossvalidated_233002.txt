[site]: crossvalidated
[post_id]: 233002
[parent_id]: 
[tags]: 
Weights to combine different models

I have built different classification models (logistic regression, randomforest, and xgboost) for a dataset. I would like to combine the prediction of all the models to reduce the variance and increase the robustness. I read that just averaging the predictions of all the models will not be the best way to combine. Also, good performing models should ideally be weighted more compared to the other low performing models. Can someone suggest me how to assign the weights to each model? Or should I iterate from 0.01 to 0.9 for all 3 models and decide the weights based on miss-classification rate for each combination?
