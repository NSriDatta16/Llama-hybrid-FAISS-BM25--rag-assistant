[site]: crossvalidated
[post_id]: 519382
[parent_id]: 312838
[tags]: 
Convolutional and fully connected layers are the building blocks of most neural networks. They are the units (layers) that most NNs are constructed from. Convolutional and fully connected layers are multiplication parameters that connect one layer of neural network to subsequent layers, thereby making each layer’s weights as a linear combination of its previous layer (nonlinear after applying Relu or Tanh). They differ however in the way that they connect two layers of a neural network. As the name suggests, fully connected layers connect each neuron of the output layer to each and every neuron of the input layer. They can serve in order to express any general pattern in the input layer. This enables fully connected layers best to be capable of recognizing global patterns in a NN layer. Therefore, they are well suited for being applied to wrap up all the patterns recognized with all previous layers (as the final layer of NNs). Also, the size of the layers at the final layers of NN is often relatively small, which means that the high number of parameters within a fully connected layer is less of an issue when it comes to learning. However, the abundance of the number of the weights (that should be learned independently in learning the NN), as well as potential problems such as over-fitting can hinder them from being the ubiquitous solution in NN architectures. On the other hand, in convolutional connects each neuron of the output layer merely to a handful of the neurons (locally) in the previous layer via a universal “filter” that makes the number of the parameters to be learned really less than a fully-connected layer. Additionally, the uniqueness of the filter applied to the whole neuron layer allows for the convolutional layer to recognize recurrence of the same pattern in different input layers (coincidence of letters in text processing and corners in image recognition). Accordingly, They can more easily be stacked in order to construct deep neural networks with affordable learning processes due to the low number of parameters to learn.
