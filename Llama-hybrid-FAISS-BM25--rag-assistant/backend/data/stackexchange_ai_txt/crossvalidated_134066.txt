[site]: crossvalidated
[post_id]: 134066
[parent_id]: 134010
[tags]: 
The $R^2$ of a multivariate regression model is such an asymmetric measure. The regression model of $Y$ on $X$ leads to a different $R^2$ than the regression model of $X$ on $Y$. This is because the value is computed using the proportion of vertical distance from the mean accounted for by the line of best fit using the conditional mean of $Y$ on $X$ to predict average potential outcomes for $Y$. EDIT: In the discourse below, it was shown that the $R^2$ may be conserved and "symmetric". The $R^2$ has a particular application and is useful for summarizing high dimensional predictive models. In general, dependence is a sophisticated mathematical concept, and you can rarely inform much about the dependence between two variables without making strong (and often untestable) assumptions. I think for conveying aspects of the interrelationship between two variables in an applied setting, the term "association" is much better. For smaller models, simply using the coefficient and its 95% confidence interval from a linear regression model is sufficient for reporting the first order trend in those data. These are well established association measures. Even if the trend is possibly nonlinear, a linear regression model has a coefficient that is taken to be a "rule-of-thumb" difference in outcomes for a unit difference in some regressor. These will necessarily be different for regression models treating a $Y$ variable as an outcome or a regressor. I see models of this form presented often in the literature with as many as 20 adjustment variables in large sample sizes.
