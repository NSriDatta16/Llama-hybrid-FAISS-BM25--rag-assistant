[site]: crossvalidated
[post_id]: 507207
[parent_id]: 
[tags]: 
Deriving the Backpropagation Matrix formulas for a Neural Network - Matrix dimensions don't work out

I try to really internalize the way backpropagation works. I made up different networks with increasing complexity and wrote the formulas to it. However, I have some difficulties with the matrix notation. Hope anyone can help me. My network has 2 input, 3 hidden and 2 output neurons. I wrote up the matrices The loss is MSE: $ L = \frac {1}{2} \sum (y_{hat} - y_{true})^2$ The derivative of the Loss with respect to the weight matrix $W^{(3)}$ should have the same dimensions like $W^{(3)}$ to update each entry with (stochastic) gradient descent. $\frac {\partial L}{\partial W^{(3)}} = \frac {\partial L}{\partial a^{(3)}} \frac {\partial a^{(3)}}{\partial z^{(3)}} \frac {\partial z^{(3)}}{\partial W^{(3)}} = (a^{(3)} - y) \odot a^{(3)} \odot (1 - a^{(3)}) a^{(2)T}$ First question , is that correct to transpose $a^{(2)}$ since otherwise the dimension would not work out? Now for the second weight matrix, where I cannot figure out what is wrong with the dimensions : $\frac {\partial L}{\partial W^{(3)}} = \frac {\partial L}{\partial a^{(3)}} \frac {\partial a^{(3)}}{\partial z^{(3)}} \frac {\partial z^{(3)}}{\partial a^{(2)}} \frac {\partial a^{(2)}}{\partial z^{(2)}} \frac {\partial a^{(2)}}{\partial z^{(2)}} \frac {\partial z^{(2)}}{\partial W^{(2)}} = (a^{(3)} - y) \odot a^{(3)} \odot (1 - a^{(3)}) W^{(3)} (1,1,1)^T a^{(1)T}$ I get 2x1 2x3 3x1 1x2 ... I wrote just $(1,1,1)$ assuming that the the $z = (z_1, z_2, z_3)$ are greater than 0.
