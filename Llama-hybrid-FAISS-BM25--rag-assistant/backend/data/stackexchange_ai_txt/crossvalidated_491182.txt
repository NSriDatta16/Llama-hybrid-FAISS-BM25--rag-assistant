[site]: crossvalidated
[post_id]: 491182
[parent_id]: 
[tags]: 
Log-transforming values on different scales

I'm trying to train a neural network to classify time series that represent the times between the sales of consumer products. I am training a single model across multiple products (spanning across multiple different categories) as I think the sales patterns do have things in common, so the idea is that the model can learn common behaviour across the items. The DeepAR paper is doing something very similar. In Section 3.3 (Scale handling) they say that it is necessary to apply a per-training-example normalization scheme, otherwise the model has to learn to scale the inputs and then inverse scale the outputs. What they suggest is scaling each training sequence by its mean. For my problem, the times between sales follow an exponential distribution, so it makes sense to apply a log transformation before feeding them into the NN. Here's an example of a product: (The values are in seconds, histogram on the right) But different products have different sale frequencies. For example milk might sell on average once every hour, while a book can sell once every few days or even weeks. So taking the log will still leave me with a scale problem. One thing I thought of, was to take the $log_{seq\_median}(values)$ where $seq\_median$ is the median of the values of a particular sequence. So the base of the log would vary - sequences with higher values (eg book) would have a higher base. This does work, and it seems to be doing better than the fixed base log. I am wondering if this transformation makes sense, or if there's anything better that I can do ? For example, would it make sense to take the $log(seq)$ and then min-max scale that or scale that by the mean ?
