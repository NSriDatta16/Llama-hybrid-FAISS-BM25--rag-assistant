[site]: crossvalidated
[post_id]: 507929
[parent_id]: 507908
[tags]: 
There are two aspects to your question: Are the limits of the original VAE, with a simple prior of $ \mathcal{N}(0,1) $ , recognized by the community? Yes. A lot of research is being done towards developing more expressive probabilistic generative models. Examples are normalizing flows for generative modeling [1], Wasserstein autoencoders [2] or back constrained Gaussian process latent variable models [3]. Can VAEs be useful in practical applications? Yes. There are tricks to make the training more robust. One useful example is annealing [4]. Consider the variational loss in VAEs, which is called the evidence lower bound (ELBO). It has two components, a regularization and a reconstruction term: $$ \mathcal{L}\big(\theta,\phi|x\big) \;\; = \;\; \underbrace{KL\Big(\,q_\phi(z) \,\big|\big|\, p_(z)\,\Big)}_{\textrm{Regularization term}} \;\; + \quad \underbrace{\mathbb{E}_{q_\phi(z)}\Big[ \log p_\theta(x|z) \Big]}_{\textrm{Reconstruction term}} $$ where $q_\phi(z)$ is the variational distribution and $\phi$ are the variational parameters; $p_\theta(x|z)$ is the likelihood and $\theta$ are the likelihood parameters; and $p(z) = \mathcal{N}(z\,|\,0,1)$ is the prior. Clearly, reconstruction is a very difficult task, and the model may have trouble learning both the reconstruction and the regularization term at the same time and from scratch. Annealing solves this in the following way: it multiplies the regularization term by a weight $w_r \in [0,1]$ that is set to 0 at the beginning of the training, and slowly grows all the way up to 1 as the training progresses. This way, the VAE can focus on learning to reconstruct during the first epochs. For example, one relatively atypical application where annealing was used to train VAEs is molecular generation [5]. This is usually considered a challenging problem, much more so than MNIST. References [1] Normalizing Flows for Probabilistic Modeling and Inference. Papamakarios et al. https://arxiv.org/abs/1912.02762 [2] Wasserstein Auto-Encoders. Tolstikhin et al. https://arxiv.org/abs/1711.01558 [3] Local distance preservation in the GP-LVM through back constraints. Lawrence et al. https://doi.org/10.1145/1143844.1143909 [4] Generating Sentences from a Continuous Space. Bowman et al. https://arxiv.org/abs/1511.06349 [5] Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. GÃ³mez-Bombarelli et al. https://doi.org/10.1021/acscentsci.7b00572
