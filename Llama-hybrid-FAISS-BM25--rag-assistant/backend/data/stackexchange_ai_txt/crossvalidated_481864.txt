[site]: crossvalidated
[post_id]: 481864
[parent_id]: 
[tags]: 
Deep Autoencoder for Narrow Dataset Feature Extraction

#Background# Hello, just to preface my question, I am relatively new to python and to machine/deep learning in general. Part of my job is now to implement an Autoencoder that reduces the dimensionality of a dataset provided by Excel. #Research Done# I have been researching this fervently. Like I said I am new and this is my fist time really implementing a Machine Learning algorithm. I have looked on this forum, colab, stackoverflow, and pages on pages of google. I have not been able to find a good example of dimensionality reduction on anything but MNIST. I have seen the MNIST examples. Please don't link them and say review, unless you really believe it will help. ##Coding Platform information## Code done in Python 3 using Google Colab. ##Data## In the current form, I am just using dummy data. I only have 8 columns/variables I am feeding into the Autoencoder via Pandas. ##Model## The Autoencoder is using the Sequential Model. Layer structure is using all dense layers where the number of neurons is 8->4->2->4->8. ##Purpose## The true purpose of the Autoencoder is to extract the important features from the latent/code/bottleneck layer and interpret those back to the original variables. Basically identifying the most significant variables from the input data. ##Problem## I have the Autoencoder running and training. However, when I extract the features, the other layers and the output of the AE, I think I have an error. The reason I believe there to be an issue is when I extract my code/latent/bottleneck layer output I will sometimes get an output (tensor) of all zeros. Additionally, when I review my output, it does not seem to always reconstruct my input. If anyone could help or point me in the right direction. I would be extremely grateful. ##Code## import pandas as pd import numpy as np import pickle import matplotlib.pyplot as plt from scipy import stats import tensorflow as tf import datetime, os import seaborn as sns from pylab import rcParams from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from tensorflow.keras.models import Model, load_model, Sequential from tensorflow.keras.layers import Input, Dense from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard from tensorflow.keras import regularizers from tensorflow.keras.utils import plot_model %matplotlib inline %reload_ext tensorboard sns.set(style='whitegrid', palette='muted', font_scale=1.5) rcParams['figure.figsize'] = 14, 8 #Setup Random Seed RANDOM_SEED = 13 df = pd.read_csv("autoencoder_ts1_nolabel_Reworked.csv") X = df.iloc[:, :8].values y = df.iloc[:, :8].values # Copy initial "df" data frame to "data" data frame. data = df # Split data 80% Training / 20% Testing -> picked at random using RANDOM_SEED value from the Pre-amble. X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=RANDOM_SEED) #Large range of values from each variable (column) require a scaled version of the dataset for loss to converge. scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Confirm Intended Train Shape (5034 * .80) = 4027 Rows X_train.shape # Determine the Number of items in the # Shape: [7] -> 8 -> 4 -> 2 -> 4 -> 8 #number of neurons in each hidden layer. progression above shows desired # of neurons. input_size = 8 hidden_size = 4 code_size = 2 #Model declaration using Sequential and Dense Layers autoencoder = tf.keras.models.Sequential() ip = autoencoder.add(Dense(input_size, input_shape = (8,), activation = 'relu',name = 'input_layer')) #Input - Dense layer with input shape encode = autoencoder.add(Dense(hidden_size,activation ='relu', name = 'encode_layer')) # Encoder - Dense layer from 8 neurons to 4 code = autoencoder.add(Dense(code_size, activation = 'relu', name = 'code_layer')) # Latent/code layer reducing neurons from 4 to 2 decode = autoencoder.add(Dense(hidden_size, activation = 'relu', name = 'decode_layer')) # Decode layer - expanding from 2 to 4 neurons op =autoencoder.add(Dense(input_size,activation = 'sigmoid', name = 'output_layer')) # Ouput layer - expands from 4 to 8 neurons. Should recreate input #Sub-models - Used to capture features/outputs from each layer in the autoencoder input_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('input_layer').output) input_layer_output = input_layer_model(X_test) intermediate_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('encode_layer').output) intermediate_output = intermediate_layer_model(X_test) code_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('code_layer').output) code_output = code_layer_model(X_test) output_layer_model = tf.keras.Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('output_layer').output) output_layer_output = output_layer_model(X_test) # The number of times to cycle through the data. epoch = 128 # The number of records (samples) to group together batch_size = 256 # Compile Neural Network Model # - optimizer: method for changing attributes to minimize loss # - loss: how the model's performance will be judged # - metrics: how the loss will be judged autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy']) logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S")) tensorboard_callback = TensorBoard(logdir, histogram_freq=1) #Train the model autoencoder.fit(x=X_train,y=X_train,epochs=epoch, shuffle = True, callbacks=[tensorboard_callback]) #Evaluation methods: autoencoder.evaluate(X_test,X_test) #Shows loss and accuracy of the test set from the trained model autoencoder.summary() #Shows layers and number of parameters in the model code_output #This is the output of the latent/bottleneck layer autoencoder.predict(X_test) #Autoencoder output prediction of train model. should match input X_test # Input to check against the prediction above plot_model(autoencoder, to_file='autoencoder_plot.png', show_shapes=True, show_layer_names=True) #this was used to verify connection of the layers in the model. Since I believe there to be an error, I think it is in the way I am using the model layers.
