[site]: crossvalidated
[post_id]: 533936
[parent_id]: 111856
[tags]: 
Basically, all nonparametric bootstrap procedures underestimate the variance of the sampling distribution. This issue doesn't have a consistent name in the literature, but I like "narrowness bias" from "What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum" by Tim Hesterberg. The reason the parametric bootstrap doesn't have this issue is because when you are building a parameterized model based on some data, you'll use the unbiased estimator for the variance (which is corrected by $ {n/(n-1)}$ ). Because the non-parametric bootstrap distribution is essentially the same thing as the plug-in estimate, its estimate of the variance of the sampling distribution (and thus the standard error) is too small. This is also why the issue goes away with very large sample sizes. If you are stuck with small sample sizes, however, there are a two ways to get good non-parametric intervals. If you can ask your question in the form of a two-sample hypothesis test, inverting a permutation test will get you intervals with the proper coverage. If you can ask your question in the form of a one-sample hypothesis test, you will get decent intervals with a sign-change permutation test (or the wild bootstrap, which is essentially the same thing).
