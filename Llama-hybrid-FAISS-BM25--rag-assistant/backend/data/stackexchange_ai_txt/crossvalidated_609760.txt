[site]: crossvalidated
[post_id]: 609760
[parent_id]: 593708
[tags]: 
Out-of-sample testing is the standard way to do this. Train your model on most but not all of your data Even better might be to have multiple out-of-sample groups (something like cross validation). Benavoli et al (2017) discuss a number of ways to do statistical inference based on model performance in such groups. While the Benavoli paper argues in favor of Bayesian methods, the paper also discusses competing frequentist methods. There are, however, a few issues with out-of-sample testing. You withhold precious training data. There can be instability depending on how you split the data into training and holdout sets. This problem is worse the smaller the sample size. (Harrell (2015), for instance, recommends not to use holdout sets unless there are at least 20,000 observations.) If you do this, like your out-of-sample performance, and then train your model on all of the data combined, you lack holdout data to validate the final model that is trained on all data. Harrell (2015) advocates for bootstrapping to address this. REFERENCES Benavoli, Alessio, et al. "Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis." The Journal of Machine Learning Research 18.1 (2017): 2653-2688. Harrell, Frank E. "Regression modeling strategies with applications to linear models, logistic and ordinal regression, and survival analysis." (2015).
