[site]: crossvalidated
[post_id]: 417598
[parent_id]: 417584
[tags]: 
I think there are two problems going on here. First, I'll hazard a guess that your model is far too complex—all of these nestings are probably the source of the errors. Second, and more importantly, you are fitting different data to the same outcome variables many times over. Your outcome (please correct me if I am wrong here) is drug overdose deaths in Ohio during a given month, so you have 60 outcome datapoints. Also, while these deaths per month are technically count data, if they are "big enough" you could reasonably ditch the poisson assumption. I think a better strategy might be to try a little feature engineering to make some new variables that are more informative. For example, why not transform your variables weight , county and date from their current form (130,000 rows) into something that summarizes the total amount of drugs seized across the state in a particular month, like total_grams_of_drugs_siezed ? (forgive my verbose variable names). And maybe sum up all of your Have_Fentanyl instances during a given month into a new variable total_fentanyl_seizures ? Then you could have something like deaths ~ total_grams + total_fentanyl_seizures in a plain, boring old lm() . That's a good start. If you wanted to get fancy, you could apply some longitudinal techniques (a GAM with a s(time) term, maybe?) but I'd be worried that you're actually regressing out something you care about—yes, drug overdose deaths are changing over time, but don't you think they might be changing because there are more fentanyl-laced drugs ? A final note, another type of variable that you might want to try constructing would be a "time-lagged" variable—drug deaths in October might be predicted by drug deaths in September, for example.
