[site]: datascience
[post_id]: 118879
[parent_id]: 118873
[tags]: 
This is a decision you have to make depending on your model and use case. Here are a few points that you might find useful: What you are referring to is called online learning . This is the idea that you receive batches of data, learn from them, then never see them again. Extensive research exists about this topic. Most deep learning models suffer from what is called catastrophic forgetting , which refers to the fact that a model tends to forget what it has learned in previous iterations when presented with new training examples. LSTMs have been proposed as a method to alleviate this problem, although they do not entirely solve it. More recent research exists on the topic so you might want to look at that as well. As your dataset grow, if you have the resources to do so, you might consider retraining your model with old + new data. However, as you said, this does not scale well and it might not be want you want either. Depending on your use case, new data might be more useful that old data (eg: stock price prediction) but again, this might not be the case (for example in multilingual machine translation or language modeling)
