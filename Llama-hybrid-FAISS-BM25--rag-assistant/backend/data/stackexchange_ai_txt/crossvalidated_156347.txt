[site]: crossvalidated
[post_id]: 156347
[parent_id]: 141005
[tags]: 
I work on similar models using the same book so I encountered the same issues - thanks for interesting questions. 1. Can posterior predictive checks be performed using both fitted and out-of-sample data? Yes - at least some authors propose to do both of these in the statistical literature (e.g., http://www.math.ntnu.no/~daniesi/INLA/Papers/festschrift.pdf ). But they refer more specifically to CPO (conditional predictive ordinate) and PIT (probability integral transform) However, Bayesian p-values and out-of-sample-based predictive probabilities seem to measure slightly different things . The latter ones measure your ability to predict new data, which is a blend of goodness of fit and model complexity (keep in mind that in some contexts, IT measures favouring parsimony such as AIC are asymptotically equivalent to leave-one-out CV). The Bayesian P-value $\text{Pr}(T(Y^{rep})>T(y)|y,\theta)$ (using the fitted data only) would be instead the equivalent of your $R^2$ , a simple measure of fit, except that rather than using only a point prediction from the fitted model, it uses the whole posterior distribution of $y$ values. Digging a little through the citation pipeline, the above paper refers to Dawid, A. P. (1984). Statistical theory: The prequential approach, Journal of the Royal Statistical Society. Series A (General) 147: 278-292. Pettit, L. I. (1990). The conditional predictive ordinate for the normal distribution, Journal of the Royal Statistical Society: Series B 52: 175-184. Marshall, E. C. and Spiegelhalter, D. J. (2003). Approximate cross-validatory predictive checks in disease mapping models. Statistics in Medicine 22, 1649–1660. Many papers including a complicated model seem to have their own specific predictive measure based on out of sample data, thus perhaps that's appropriate for you too. Which brings us to what you're trying to do by using cross-validation: selecting a model balancing goodness of fit with complexity. On that point the following review might be up-to-date Hooten, M. B., & Hobbs, N. T. (2015). A guide to Bayesian model selection for ecologists. Ecological Monographs, 85(1), 3-28. I don't know if $\chi^2$ is the best distance/discrepancy measure, not entirely sure either if that makes sense to plot the independently obtained discrepancy values with those from the fitted data (but perhaps it does). 0.5 Correction. $\text{Var}(Y_i)$ is likely to be above 0.5 if $N>>1$, which in turn depends on $\lambda$. You can always use 0.05, 0.1, 0.2 instead to check. Measure of fit. You seem to be looking for a $R^2$. But even with such measure, good or bad is context dependent, i.e. $75 \%$ will appear barely believable in some fields and ridiculously low in others. Here's what I found on interpreting Bayesian p-values http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.310.145&rep=rep1&type=pdf (statisticians seem to debate their distribution btw). One practical solution would be simulate data according to a model which you know to be close to your situation (similar sample sizes, number of sites, approximate abundances/occupancy etc.), then fit the simulated model to simulated data, and use the resulting Bayesian P-values as measures of "good" (as good as it is going to get) when interpreting those from your real data. Lack-of-fit ratio: Looks like a comparison of mean squared errors. Isn't it mean(simulations$fit) / mean(simulations$fit.new) , rather? The same simulation approach could work to get sensible reference values. Also, if we were comparing to sets of replicated data, the discrepancy measures should follow two $\chi^2$ distributions which means the ratio follows an F - and gives access to a different kind of P-value. [just throwing in ideas here] The comparison to the null model makes more sense to me when you compare the predictive abilities of the models using the independent data. EDIT 10/06 The discrepancy measure of Kéry and Schaub is in fact $\sum_{i=1}^{n}{\frac{(Y_i-E(Y_i))^2}{E(Y_i)+0.5}}$, no variance term below. EDIT2 10/06 For similar hierarchical models, I also get Bayesian P-values always above 0.5 (no matter how good the fit looks graphically) and very often close to 1, with Lack-of-fit ratios just below 1 (e.g. 0.8 or 0.9)
