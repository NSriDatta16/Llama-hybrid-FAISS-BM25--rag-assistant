[site]: crossvalidated
[post_id]: 617102
[parent_id]: 617095
[tags]: 
A likelihood is a strange thing: it is not a probability and does not need to sum or integrate to $1$ . In fact likelihood is only measured up to proportionality, with a positive multiplicative constant which becomes an additive constant if you take the logarithm. So you can find the ratios of likelihoods of different values of the parameter, i.e the difference for log-likelihoods, and you can which value of the parameter maximises the likelihood, but the likelihood itself not fixed absolutely; in Bayesian analysis the constant of proportionality integrates out. If you are just looking at the overall figures from your data, you get $\frac{y}{n}=\frac{44}{371}\approx 0.1186$ and this is going to be the maximum likelihood proportion $\hat p$ . In R: phat The differences in the log-likelihoods come from binomial coefficients involving $y$ and $n$ but not $\hat p$ . Here are three ways of calculating it, giving three different values: Your: $$\log_e\left(\hat p^y (1-\hat p)^{n-y}\right)$$ log(phat^y * (1-phat)^(n-y)) # -135.0896 the binomial: $$\log_e\left({n \choose y} \hat p^y (1-\hat p)^{n-y}\right) = \log_e{n \choose y}+\log_e\left(\hat p^y (1-\hat p)^{n-y}\right)$$ log(choose(n,y) * phat^y * (1-phat)^(n-y)) # -2.749837 the binomials for each set of miners as used by glm : $$\log_e\left(\prod\limits_i{\text{miners}_i \choose \text{cases}_i} \hat p^{\text{cases}_i} (1-\hat p)^{\text{miners}_i-\text{cases}_i}\right)\\ = \left(\sum\limits_i \log_e {\text{miners}_i \choose \text{cases}_i}\right) + \log_e\left(\hat p^y (1-\hat p)^{n-y}\right)$$ log(prod(choose(miners,cases) * phat^cases * (1-phat)^(miners-cases))) # -39.8646 These are all log-likelihoods from the same data, with the difference between the first and second being log(choose(n,y)) about $132.3398$ , and between the first and third sum(log(choose(miners,cases))) about $95.22504$ , none of which involve $\hat p$ .
