[site]: crossvalidated
[post_id]: 529519
[parent_id]: 529513
[tags]: 
Doing a joint optimization for two metrics is an interesting problem, but unfortunately a bit out my expertise. However, making a decision based on a single metric is straight forward enough. Here is a simulated example of how this might be done using a Bayesian lense. Let's first simulate some data library(tidyverse) set.seed(0) N = 10000 groups = sample(LETTERS[1:3], size = N, replace = T) # Generate fake data X = model.matrix(~groups) true_beta = c(qlogis(0.1), qlogis(0.12) - qlogis(0.1), qlogis(0.09) - qlogis(0.1)) true_p = plogis(X %*% true_beta) # The observations y = rbinom(N, 1, true_p) d = tibble(groups, y) In our fake experiment, we have allocated approximately 3300 people to one of 3 groups and measured a binary outcome. Here is a summary of the data. groups N y 1 A 3385 359 2 B 3311 415 3 C 3304 297 To begin with, we need a loss function , essentially a function which tells us what I would stand to lose were I to make a decision. Each action, including the do nothing scenario (where we just keep the control group as the main exposure) is a decision, so we need to know what we would lose if that were the wrong decision. We really don't want to hurt the rate/metric at which the binary outcome happens (whatever that rate/metric is). One loss function we might find interesting is the following $$ \mathcal{L}(x, p_a, p_b, p_c) = \begin{cases} \operatorname{max}(\operatorname{max}(p_b, p_c) - p_a, 0) & \quad x=a \\ \operatorname{max}(\operatorname{max}(p_a, p_c) - p_b, 0) & \quad x=b\\ \operatorname{max}(\operatorname{max}(p_a, p_b) - p_c, 0) & \quad x=c \end{cases}$$ Here, $x$ is the variant we choose to implement, and $p_i$ is the rate for group $i$ . The function $\mathcal{L}$ tells us how much we would stand to hurt our metric if we chose the wrong variant. So for example, let's say we choose variant $a$ . In the case where $a$ truly is the superior variant ( $p_a>p_b$ and $p_a>p_c$ ) then $\max{\max{p_b, p_c} - p_a, 0} = 0$ and so we lose out on a total 0 change to the metric. But suppose that $p_b>p_a$ , then choosing $a$ means we lose out on $p_b-p_a$ . Had we chosen $b$ , our metric would have been higher, but since we didn't we lose out on that increase. The thing is that $p_a$ , $p_b$ , and $p_c$ are all unknown, so we have to estimate them from our data. We can do that fairly easily using R and some tools for logistic regression (I am using Bayesian methods because it makes the decision process very easy as we will see). Using the data created above, we can estimate a logistic regression with # Bayesian models library library(brms) library(tidybayes) # Modify data so we have success/trials as our coliumns modified_d = group_by(d, groups) %>% summarise(N = n(), y = sum(y)) #Fit the model model = brm(y|trials(N)~groups, data = modified_d, family = binomial()) From the model, we can extract posterior samples of each variant's rate # .value column gives the posterior rates. draws = tibble(groups = LETTERS[1:3], N=1) %>% add_fitted_draws(model) draws groups N .row .chain .iteration .draw .value 1 A 1 1 NA NA 1004 0.107 2 A 1 1 NA NA 704 0.105 3 A 1 1 NA NA 377 0.103 4 B 1 2 NA NA 1465 0.134 5 B 1 2 NA NA 46 0.120 6 B 1 2 NA NA 324 0.124 7 C 1 3 NA NA 1623 0.0906 8 C 1 3 NA NA 3421 0.0900 9 C 1 3 NA NA 788 0.0868 Now, all that is needed is to compute our expected loss by using these draws to compute our loss function and averaging over the results. Here is how you could do it in R. # Extract posterior estimates for each variant A = filter(draws, groups=='A') $.value B = filter(draws, groups=='B')$ .value C = filter(draws, groups=='C')$.value # Loss function in R code loss = function(choose, ignore_1, ignore_2) pmax(pmax(ignore_1, ignore_2) - choose, 0) # Compute the expected loss over posterior samples # Loss if I chose A loss_A = mean(loss(A, B, C)) > 0.019 # May be different from run to run # If I chose B loss_B = mean(loss(B, A , C)) > 2.3e-05 # If I chose C loss_C = mean(loss(C, A, B)) >0.035465 Now all we do is pick the variant which minimizes the loss. Variant B has lowest expected loss. If we chose B and we were wrong, then we estimate we lose out on a potential 0.0023% units to our metric. Compare this with A in which we lose out on a possible 2% to our metric. In fact, when I simulated the data, I made sure B had the largest rate. So our decision would be a good one in this case. One thing I have not talked about is priors. If you do this approach, you may want to think about good priors to use for your model which is as much an art is it is a science. That being said, with enough data the priors matter less and less (especially for a simple experiment like this).
