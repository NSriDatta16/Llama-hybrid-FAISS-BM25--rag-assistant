[site]: datascience
[post_id]: 71658
[parent_id]: 
[tags]: 
In "Attention Is All You Need", why are the FFNs in (2) the same as two convolutions with kernel size 1?

In addition, why do we need a FFN in each layer when we already have attention? Here's a screenshot of the relevant section from Vaswani et al. (2017):
