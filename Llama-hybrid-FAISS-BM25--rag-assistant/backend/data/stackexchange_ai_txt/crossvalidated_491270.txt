[site]: crossvalidated
[post_id]: 491270
[parent_id]: 
[tags]: 
Different input size for training and prediction in CNN for image segmentation?

I’m relatively unexperienced when it comes to deep learning and I’m trying to reimplement a CNN architecture for segmentation of medical images based on a paper. In the paper they state that they use input images that are of size 448x448. Further they state that they crop random sub-images which are 224x224 in order to have more data to train on. Due to my lack of experience I’m not sure what the most likely interpretation of this is. Does this mean that they have trained the network with input size 224x224 and when using it on unseen data they are cut the input images into 4 pieces and feed it to the network or do they resize the input layers to 448x448 and reuse the weights from the network trained on 224x224? Or is there some other more likely interpretation that I’m unaware of?
