[site]: datascience
[post_id]: 29270
[parent_id]: 29264
[tags]: 
Yes, actually what usually people do is to map the unique tokens in a space with fixed dimensionality, obtaining what is called "words embeddings". And actually, using already trained word embeddings like GloVE is usually it's a best practice: those vectors are trained on huge datasets like Wikipedia or Common Crawl. A nicety of those vectors is that, thanks to the way they are built, they include also the relation between the words, a sort of semantic. This is the way I definitely suggest you to start.
