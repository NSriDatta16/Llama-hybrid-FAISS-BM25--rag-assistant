[site]: crossvalidated
[post_id]: 49573
[parent_id]: 49473
[tags]: 
Well first things first, you can try eyeballing the residuals and see if one or other model makes them more even, but I suspect given that you haven't seen a clear winner, that won't split them either. In which case I think this really comes down to your definition of "better". Because you are using the same explanatory variables for both models, and the same number of parameters etc. then it is simply a matter of comparing the goodness of fit of the two models. You simply have to define what you mean by a good fit. For example, if your utility is to minimise the mean square error of predictions, then pick the model that does that best - which should be the gaussian. If on the otherhand you want the one that minimses the mean square log odds error, then pick the one that does that best (this should be the binomial). Of course, since you are choosing a model for empirical pragmatic reasons, you needn't be restricted to this. You could choose the model that minimises the chi-squared error, or something even more exotic. What are you going to use the model for when it is fitted? Really you want to think about what the cost to the user of the model is, when the model is wrong. This is related in spirit to this question about cost functions: Cost function for validating Poisson regression models
