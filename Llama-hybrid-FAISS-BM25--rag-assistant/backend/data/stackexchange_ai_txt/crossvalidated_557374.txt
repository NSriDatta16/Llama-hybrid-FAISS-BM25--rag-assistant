[site]: crossvalidated
[post_id]: 557374
[parent_id]: 
[tags]: 
What if learning rate is too small or large in machine learning problems?

Can we say that if our learning is too small then vanishing gradient will occur. And if our learning rate is too large then exploding gradient will occur? very small learning rate == vanishing gradient, very large learning rate == exploding gradient Is this concept right or wrong?
