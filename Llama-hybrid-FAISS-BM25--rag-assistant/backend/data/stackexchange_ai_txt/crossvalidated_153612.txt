[site]: crossvalidated
[post_id]: 153612
[parent_id]: 153413
[tags]: 
In the random effects model, I would consider using random slopes: otherwise, you are implying that each bird in each group learns at the exact same rate. But just a word of advice: I analyze behavioral data like this quite often, and I've grown to dislike standard mixed effects models for this type of data. The reason for this data will show many aspects that are problematic for simple linear mixed effects model. Such as: the real learning effect is very non-linear: there's usually a large learning effect early on which then flatlines. Secondly, as subjects learn more, the variance of time until success reduces substantially (in fact, it makes sense to treat this as time to event data rather than simple linear regression). Thirdly, there's an annoying slope-intercept interaction; subjects who preform very well early on typically show very little improvement. So if you're just comparing simple slopes, subjects who "get it" on their first attempt don't improve much at all, showing little learning effect. This is logical, but not what you're really interested in the study; I don't think you want to be reporting that groups who "got it" right away were learning slower than those who didn't get it all early on but had lots of room for improvement. So there's various directions you can take this. You can make a really complicated mixed effects model. But with only 6 observations per subject, I'm not such a fan. Or, if your question of interest is really simple (overall, how do males compare to females) and that you have a nice balanced design (equal measurements per subject), you can make it much simpler. The simple model I personally like is at each level (i.e. trial 1, 2, ...), replace the scores with their ranks (so subject who finished first = 1, second = 2, etc.). Then, for each subject, compute a (potentially weighted) mean rank score. Now we can do a simple t-test, linear regression, etc. on this mean rank score (one per individual, each one is independent). The reason I prefer using the ranks over the raw scores is this tackles the issue that there is higher variance on the earlier scores than the later scores. Otherwise, the mean scores can be completely dominated by the first score, which is not what we are interested in. Moreover, if you're interested in a metric that's more reflective of ability after learning, you can take a weighted average that places heavier weight on later scores; I find that linear ranks (weight of first = 1, weight of second = 2, etc.) work well in the datasets I've seen. Based on power simulations I've run, even if the model is simulated exactly according the mixed model scheme, this ranked metric results in very little power loss: if the mixed effects model has power = 0.5, then the mean rank statistic will have power, say, 0.48. And once you start simulating data that deviates from the simple model (i.e. declining learning effect), the mean rank statistic displays much, much higher power. Of course, the mean ranks doesn't really give individual trajectories. But I'm not so sure you're that concerned with this; in the studies I've seen, the researcher typically is mostly just interested in showing there is a difference between groups.
