[site]: stackoverflow
[post_id]: 3302019
[parent_id]: 3301983
[tags]: 
But my understanding of Big O notation is that it's the worst-case runtime time, not best. Unfortunately, there is no "standard" for Big-O when describing algorithms. Often, it's used to describe the general or average case - not the worst case. From Wikipedia : ...this notation is now frequently also used in the analysis of algorithms to describe an algorithm's usage of computational resources: the worst case or average case... In this case, it's describing a standard case, given proper hashing. If you have proper hashing in place, the limiting behavior will be constant for size N, hence O(1).
