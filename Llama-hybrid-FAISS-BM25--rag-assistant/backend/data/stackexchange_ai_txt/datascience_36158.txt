[site]: datascience
[post_id]: 36158
[parent_id]: 
[tags]: 
Convergence of vanilla or natural policy gradients (e.g. REINFORCE)

I am reading in a lot of places that policy gradients, especially vanilla and natural, are at least guaranteed to converge to a local optimum (see, e.g., pg. 2 of Policy Gradient Methods for Robotics by Peters and Schaal ), though the convergence rates are not specified. I, however, cannot at all find a proof for this, and need to know if and why it is true. I am wondering the following: What is the proof of this? Can someone point me to a reference? What is the best convergence rate out there that's known (if any are proven)? If I am reading at least the Peters/Schaal paper correctly, this only holds for discounted reward. Is there any formulation that works for the mean reward of a rollout? I am in a situation where I am running many simulations of a robot and most directly want to optimize its mean reward.
