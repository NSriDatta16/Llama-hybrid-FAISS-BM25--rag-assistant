[site]: crossvalidated
[post_id]: 605820
[parent_id]: 
[tags]: 
Factor Analysis: Simulating observations from predetermined factor loadings with beta-distributed variables

I have a question about simulating data in the context of (exploratory) factor analysis. I need to simulate n observations of p measurable variables derived from k latent variables given factor loadings for each measurable variable. That is, given a p x k matrix of factor loadings, I'd like to generate a set of n plausible observations. The output should be an n x p matrix of simulated observations. When factor analysis is performed on the simulated data, I should get values similar to the original factor loadings. One hitch, the values in each column of the n x p simulated observations matrix should (roughly) follow a beta distribution. I'm not sure how to go about doing this, but with dubious help from Google and ChatGPT, I put together some code. These are the steps that ChatGPT told me follow: define the p x k matrix of factor loadings (I'll call it loadings ). generate an n x k matrix of random, normally distributed factor scores, which I'll call factors . generate an n x p matrix of random, beta distributed observations, which I'll call observations . take the dot product of factors and loadings (transposed) and add observations . The result is the (more or less) final simulated observations. I'm not sure I really understand what ChatGPT is trying to do here, but it doesn't work. When I perform a new factor analysis on the simulated observations, the factor loadings are nothing like those in loadings and the variables don't load the way they should either (even given the possibility of poles flipping). So, I have two questions: Has ChatGPT put me on the right track and I'm just missing something silly? If no, does anyone have a better way to do this? Python code below: import numpy as np import pandas as pd from factor_analyzer import Rotator, FactorAnalyzer # Set the random seed for reproducibility np.random.seed(123) # Define the number of observations and variables n = 1000 p = 6 # Define the factor loadings matrix loadings = np.array([[0.6, 0.4, 0.0], [0.4, 0.5, 0.1], [0.3, 0.2, 0.7], [0.0, 0.8, 0.4], [0.7, 0.0, 0.5], [0.0, 0.6, 0.4]]) # Define the number of factors k = loadings.shape[1] # Generate the latent factor scores factors = np.random.normal(0,.1,size=(n, k)) # Generate the standardized observed variables with beta distribution observed = np.random.beta(2, 5, size=(n, p)) # Compute the observed variables as a linear combination of the latent factors observed = np.dot(factors, loadings.T) + observed # Test the simulated data by fitting a new FA model # If the simulation worked, the loadings from the new model should be similar to the loadings # specified above. # Fit factor analysis model to the simulated data # Method='principal' to use OLS instead of MLE (vars are not normally distributed) fa = FactorAnalyzer(rotation='varimax', method='principal') fa.fit(observed) # Print the standardized factor loadings print(fa.loadings_) The output: [[-0.01295672 0.87588893 -0.02321431] [ 0.28134231 0.49612987 0.19610379] [ 0.21686871 -0.11089673 0.76680512] [ 0.72838621 0.1062238 0.12636419] [-0.06024928 0.28948084 0.73527448] [ 0.77707454 0.05032242 0.01255114]] As you can see, the matrix is different enough from loadings to indicate something went seriously wrong. Go easy on me folks. I'm a linguist.
