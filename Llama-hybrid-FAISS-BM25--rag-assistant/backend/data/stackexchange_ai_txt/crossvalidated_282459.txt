[site]: crossvalidated
[post_id]: 282459
[parent_id]: 
[tags]: 
XGBoost vs Python Sklearn gradient boosted trees

I am trying to understand how XGBoost works. I already understand how gradient boosted trees work on Python sklearn. What is not clear to me is if XGBoost works the same way, but faster, or if there are fundamental differences between it and the python implementation. When I read this paper http://learningsys.org/papers/LearningSys_2015_paper_32.pdf It looks to me like the end result coming out of XGboost is the same as in the Python implementation, however the main difference is how XGboost finds the best split to make in each regression tree. Basically, XGBoost gives the same result, but it is faster. Is this correct, or is there something else I am missing ?
