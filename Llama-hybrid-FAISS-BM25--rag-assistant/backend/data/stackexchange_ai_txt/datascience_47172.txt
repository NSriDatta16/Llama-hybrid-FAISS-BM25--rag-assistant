[site]: datascience
[post_id]: 47172
[parent_id]: 47142
[tags]: 
Well, you picked optimizers which are used in neural networks, those optimizers do use gradient based algorithms. Most of the times gradient based algorithms are used in neural networks. Why is that? Well, would you prefer trying to find the minimum knowing the slope of a curve or without knowing it? When you can't compute the gradient then you'll fall back to Derivative-free optimization . That being said, there are cases when even though you have info about the gradient, it's better to use a gradient-free method. This is usually the case with functions which have a lot of local minima. Population based algorithms such as evolutionary strategies and genetic algorithms have the upper hand here. And there's also the branch of combinatorial optimization where a whole different set of tools is used.
