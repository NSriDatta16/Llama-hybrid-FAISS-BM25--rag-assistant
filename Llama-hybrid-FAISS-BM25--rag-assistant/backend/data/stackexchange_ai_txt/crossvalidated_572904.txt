[site]: crossvalidated
[post_id]: 572904
[parent_id]: 
[tags]: 
Hold neural network distribution nearly constant to help training

I have a neural network $F(X)$ which outputs $Y$ . The output $Y$ goes to another separate neural network $G(X)$ and is used as a label training example. I minimize $min{(G(X)-F(X))^2}$ . The output of both networks is unconstrained and real valued. Additionally, $G(X)$ is not updated at the same moment as $F(X)$ , but is fixed during training of $G(X)$ . $F(X)$ is trained separately, with a different objective and loss function (which I can't publish here). I have noticed that the second network struggles, because the first network changes distribution of outputs very often. How could I enforce or restrain the first network $F(X)$ , to not change the distribution so much ?
