[site]: crossvalidated
[post_id]: 94931
[parent_id]: 
[tags]: 
Understanding the tradeoff with regularization in SVMs

In the linear SVM model, one may have the following equation to describe how to achieve a maximal margin while still classifying the data into 2 groups: \begin{equation} L(w, \epsilon) = w\cdot w + \lambda\sum\limits_{i=1}^R \epsilon_i \end{equation} In the above, $w$ is the weight vector and the $\epsilon_i$ are the error distances for a mis-classification of $x_i$. There are $R$ mis-classifications total. My textbook tells me that if $\lambda$ is small we prefer a large Margin over a few erros and vice versa. I do not understand this reasoning at all. I do not see how varying $\lambda$ affects the minimization of $L$. Could someone please explain to me how varying $\lambda$, changes what is being minimized in $L$?
