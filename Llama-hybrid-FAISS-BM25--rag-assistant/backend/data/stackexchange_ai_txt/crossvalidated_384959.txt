[site]: crossvalidated
[post_id]: 384959
[parent_id]: 291981
[tags]: 
Q1. because BIC is an approximation to the likelihood of a model and the likelihood should never decrease when the model becomes more complex There might be some confusion of the marginal likelihood $P(D|S^h)$ and a likelihood $P(D|\theta_s, S^h)$ . When you are doing a point estimate of the weight $\theta_s$ given the model $S^h$ (e.g. Maximum likelihood or MAP or SGD in NN) and get $\tilde{\theta_s}$ , usually, the likelihood or fitness $P(D|\tilde{\theta_s}, S^h)$ will increase as the model becomes more complicated. However, what we are using for model comparison is the "marginal likelihood" in which the $\theta_s$ variable is integrated out. The marginal likelihood over the hypothesis/model $S^h$ does not necessarily increase as the model family includes more possible functions or more parameters. Intuitively, you just need a single parameter point in the space to yield good fitness to get a high likelihood, but need many parameter settings to yield acceptable fitness to get a high marginal likelihood. Another intuition from bishop's PRML, Chapter 3.4: From a sampling perspective, the marginal likelihood can be viewed as the probability of generating the dataset D from a model whose parameters are sampled at random from the prior. Q2. Why the log hessian $\log |A|$ grows as $d \log N$ You have already demonstrated in your question how eq.3.41 is derived by using the Laplace approximation of the posterior $P(\theta|D) = \frac{P(D, \theta)}{Z}$ (by taking the 2-order taylor approximation of $g(\theta) = \log P(D,\theta)$ around the mode $\tilde{\theta}$ ). To understand the BIC criterion, which is a very loose approximation of eq.3.41, you can add a very broad Gaussian prior for $\theta$ , which means $\log P(\theta) = - \frac{(\theta - \theta_0)^T \Lambda (\theta-\theta_0)}{2}+ \mbox{const}$ , and $\Lambda$ is very small, then assume all N data in the dataset is i.i.d, we have: $A = - \nabla^2_{\theta} [\log P(D|\theta) + \log P(\theta)] |_{\theta = \tilde{\theta}} = - \nabla^2_{\theta} \log P(D|\theta)|_{\theta = \tilde{\theta}} + \Lambda \approx - \sum_{x \in D}\nabla^2_{\theta} \log P(x|\theta)|_{\theta = \tilde{\theta}} = -\sum_{x \in D} H(x_i)$ . As we know, $\det(c H) = c^{d} \det{H}$ , $d$ is the dimension of $H$ , $c$ is a constant, so $\log{\det{A}} \approx \log{(\det{(- \sum_{x \in D} H(x_i))})} \sim O(d\log N)$ . However, in practice, the penalize term $d\log N$ in BIC criterion could be overly strong, as $H(x_i)$ is not so full-rank in practice, so that many $\det{H(x_i)}$ will be close to 0. As a result, BIC tends to show overly favour to simple models. Q3. The BIC is an approximation to the likelihood, not the posterior, so it shouldn't have a preference for simpler models. Actually, we should use model posterior $P(S^h|D)$ for model comparison, however, we usually assume an equal prior probability over all the models $S^h$ , so we can just use the marginal likelihood $P(D|S^h)$ . And, yes, this marginal likelihood will favor expressive and simple model, as illustrated intuitively in the answer of Q1, also shown by the formula eq 3.41 you provided and the approximated BIC criterion. You should refer to Chapter 3.4 of Bishop's Pattern Recognition and Machine Learning for why marginal likelihood can be used to do model comparison, and Chapter 4.4 for some discussion about how BIC criterion is approximated derived. I'm not an expert, answering this inactive question just because i'm learning about this topic too. Welcome to point out any imprecise statement or errors in my answer.
