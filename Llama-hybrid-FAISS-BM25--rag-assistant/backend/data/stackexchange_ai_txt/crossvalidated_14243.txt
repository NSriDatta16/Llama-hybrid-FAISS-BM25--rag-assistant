[site]: crossvalidated
[post_id]: 14243
[parent_id]: 8630
[tags]: 
If we start with the premise that all variables have been centred (standard practice in PCA), then the total variance in the data is just the sum of squares: $$T=\sum_{i}(A_{i}^{2}+B_{i}^{2}+C_{i}^{2}+D_{i}^{2}+E_{i}^{2}+F_{i}^{2})$$ This is equal to the trace of the covariance matrix of the variables, which equals the sum of the eigenvalues of the covariance matrix. This is the same quantity that PCA speaks of in terms of "explaining the data" - i.e. you want your PCs to explain the greatest proportion of the diagonal elements of the covariance matrix. Now if we make this an objective function for a set of predicted values like so: $$S=\sum_{i}\left(\left[A_{i}-\hat{A}_{i}\right]^{2}+\dots+\left[F_{i}-\hat{F}_{i}\right]^{2}\right)$$ Then the first principal component minimises $S$ among all rank 1 fitted values $(\hat{A}_{i},\dots,\hat{F}_{i})$. So it would seem like the appropriate quantity you are after is $$P=1-\frac{S}{T}$$ To use your example $A+2B+5C$, we need to turn this equation into rank 1 predictions. First you need to normalise the weights to have sum of squares 1. So we replace $(1,2,5,0,0,0)$ (sum of squares $30$) with $\left(\frac{1}{\sqrt{30}},\frac{2}{\sqrt{30}},\frac{5}{\sqrt{30}},0,0,0\right)$. Next we "score" each observation according to the normalised weights: $$Z_{i}=\frac{1}{\sqrt{30}}A_{i}+\frac{2}{\sqrt{30}}B_{i}+\frac{5}{\sqrt{30}}C_{i}$$ Then we multiply the scores by the weight vector to get our rank 1 prediction. $$\begin{pmatrix} \hat{A}_{i} \\ \hat{B}_{i} \\ \hat{C}_{i} \\ \hat{D}_{i} \\ \hat{E}_{i} \\ \hat{F}_{i}\end{pmatrix} =Z_{i}\times\begin{pmatrix} \frac{1}{\sqrt{30}} \\ \frac{2}{\sqrt{30}} \\ \frac{5}{\sqrt{30}} \\ 0 \\ 0 \\ 0\end{pmatrix}$$ Then we plug these estimates into $S$ calculate $P$. You can also put this into matrix norm notation, which may suggest a different generalisation. If we set $O$ as the $N\times q$ matrix of observed values of the variables ($q=6$ in your case), and $E$ as a corresponding matrix of predictions. We can define the proportion of variance explained as: $$\frac{||O||_{2}^{2}-||O-E||_{2}^{2}}{||O||_{2}^{2}}$$ Where $||.||_{2}$ is the Frobenius matrix norm . So you could "generalise" this to be some other kind of matrix norm, and you will get a difference measure of "variation explained", although it won't be "variance" per se unless it is sum of squares.
