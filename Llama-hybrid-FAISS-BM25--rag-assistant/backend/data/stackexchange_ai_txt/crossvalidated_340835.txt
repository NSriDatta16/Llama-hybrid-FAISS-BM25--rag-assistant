[site]: crossvalidated
[post_id]: 340835
[parent_id]: 340099
[tags]: 
This is an algebraic counterpart to @Martijn's beautiful geometric answer. First of all, the limit of $$\hat{\boldsymbol\beta}_\lambda^* = \arg\min\Big\{\|\mathbf y - \mathbf X \boldsymbol \beta\|^2+\lambda\|\boldsymbol\beta\|^2\Big\} \:\:\text{s.t.}\:\: \|\mathbf X \boldsymbol\beta\|^2=1$$ when $\lambda\to\infty$ is very simple to obtain: in the limit, the first term in the loss function becomes negligible and can thus be disregarded. The optimization problem becomes $$\lim_{\lambda\to\infty}\hat{\boldsymbol\beta}_\lambda^* = \hat{\boldsymbol\beta}_\infty^* = \operatorname*{arg\,min}_{\|\mathbf X \boldsymbol\beta\|^2=1}\|\boldsymbol\beta\|^2 \sim \operatorname*{arg\,max}_{\| \boldsymbol\beta\|^2=1}\|\mathbf X\boldsymbol\beta\|^2,$$ which is the first principal component of $\mathbf X$ (appropriately scaled). This answers the question. Now let us consider the solution for any value of $\lambda$ that I referred to in point #2 of my question. Adding to the loss function the Lagrange multiplier $\mu(\|\mathbf X\boldsymbol\beta\|^2-1)$ and differentiating, we obtain $$\hat{\boldsymbol\beta}_\lambda^*=\big((1+\mu)\mathbf X^\top \mathbf X + \lambda \mathbf I\big)^{-1}\mathbf X^\top \mathbf y\:\:\text{with $\mu$ needed to satisfy the constraint}.$$ How does this solution behave when $\lambda$ grows from zero to infinity? When $\lambda=0$, we obtain a scaled version of the OLS solution: $$\hat{\boldsymbol\beta}_0^* \sim \hat{\boldsymbol\beta}_0.$$ For positive but small values of $\lambda$, the solution is a scaled version of some ridge estimator: $$\hat{\boldsymbol\beta}_\lambda^* \sim \hat{\boldsymbol\beta}_{\lambda^*}.$$ When $\lambda=\|\mathbf X\mathbf X^\top \mathbf y\|$, the value of $(1+\mu)$ needed to satisfy the constraint is $0$. This means that the solution is a scaled version of the first PLS component (meaning that $\lambda^*$ of the corresponding ridge estimator is $\infty$): $$\hat{\boldsymbol\beta}_{\|\mathbf X\mathbf X^\top \mathbf y\|}^* \sim \mathbf X^\top \mathbf y.$$ When $\lambda$ becomes larger than that, the necessary $(1+\mu)$ term becomes negative. From now on, the solution is a scaled version of a pseudo-ridge estimator with negative regularization parameter ( negative ridge ). In terms of directions, we are now past ridge regression with infinite lambda. When $\lambda\to\infty$, the term $\big((1+\mu)\mathbf X^\top \mathbf X + \lambda \mathbf I\big)^{-1}$ would go to zero (or diverge to infinity) unless $\mu = -\lambda/ s^2_\mathrm{max} + \alpha$ where $s_\mathrm{max}$ is the largest singular value of $\mathbf X=\mathbf{USV}^\top$. This will make $\hat{\boldsymbol\beta}_\lambda^*$ finite and proportionate to the first principal axis $\mathbf V_1$. We need to set $\mu = -\lambda/ s^2_\mathrm{max} + \mathbf U_1^\top \mathbf y -1$ to satisfy the constraint. Thus, we obtain that $$\hat{\boldsymbol\beta}_\infty^* \sim \mathbf V_1.$$ Overall, we see that this constrained minimization problem encompasses unit-variance versions of OLS, RR, PLS, and PCA on the following spectrum: $$\boxed{\text{OLS} \to \text{RR} \to \text{PLS} \to \text{negative RR} \to \text{PCA}}$$ This seems to be equivalent to an obscure (?) chemometrics framework called "continuum regression" (see https://scholar.google.de/scholar?q="continuum+regression" , in particular Stone & Brooks 1990, Sundberg 1993, Björkström & Sundberg 1999, etc.) which allows the same unification by maximizing an ad hoc criterion $$\mathcal T = \operatorname{corr}^2(\mathbf y, \mathbf X \boldsymbol\beta)\cdot \operatorname{Var}^\gamma(\mathbf X\boldsymbol\beta) \;\;\text{s.t.}\;\;\|\boldsymbol\beta\|=1.$$ This obviously yields scaled OLS when $\gamma=0$, PLS when $\gamma=1$, PCA when $\gamma\to\infty$, and can be shown to yield scaled RR for $0 Despite having quite a bit of experience with RR/PLS/PCA/etc, I have to admit I have never heard about "continuum regression" before. I should also say that I dislike this term. A schematic that I did based on the @Martijn's one: Update: Figure updated with the negative ridge path, huge thanks to @Martijn for suggesting how it should look. See my answer in Understanding negative ridge regression for more details.
