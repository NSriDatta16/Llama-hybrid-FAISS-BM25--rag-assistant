[site]: datascience
[post_id]: 85999
[parent_id]: 49468
[tags]: 
Let me try to keep it more intuitive and less mathematical Prior to 2014, RNNs used to perform badly if the sequence was beyond a certain size. After all RNNs encode all steps in the sequence and give out a final output which is 'supposed' to be something of a sequence embedding. This works well for short sequences but beyond a certain length, it starts 'forgetting' things. To fix this, Bahdanau et al came up with a landmark paper in 2014. They used all the hidden states of the encoder (instead of just the last state) in the model at the decoder end. But the best part was - they made the model give particular 'attention' to certain hidden states when decoding each word. They let the model itself 'learn' which words to give Attention to and which ones to ignore during translation of each word at the decoder. This approach worked brilliantly and for 4 years, various forms of attentions were proposed. RNN coupled with Attention seems to have solved a long pending problem in NLP. Now the scene shifts to 2018 when a team at Google presented what was a gamechanger for NLP. The name of the paper was 'Attention is all you need' and they claimed that Attention was all that was needed to encode sequences. No RNNs and serial processing any more. Chuck out the LSTMs and GRU and just use attention for the encoding. Of course for this they made several changes to the way attention is applied. They used self-attention models which is largely inspired by a paper by Cheng et al https://arxiv.org/pdf/1601.06733.pdf . In self-attention, the concept of attention is used to encode sequences instead of RNNs. So both the encoder and decoder now dont have RNNs and instead use attention mechanisms. In itself simplest form - each word in the sequence attends to every other word in the same sequence and in this way relationship between words in the sequence are captured. So to summarize the difference - traditional Attention was used in combination with RNNs to improve their performance. Self-attention is used INSTEAD OF RNNs and they do a much better job and are also much faster. So in that sense they are pretty different.
