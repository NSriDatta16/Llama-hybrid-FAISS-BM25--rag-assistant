[site]: crossvalidated
[post_id]: 12995
[parent_id]: 12641
[tags]: 
One of the interesting things I find in the "Model Uncertainty" world is this notion of a "true model". This implicitly means that our "model propositions" are of the form: $$M_i^{(1)}:\text{The ith model is the true model}$$ From which we calculate the posterior probabilities $P(M_i^{(1)}|DI)$. This procedure seems highly dubious at a conceptual level to me. It is a big call (or an impossible calculation) to suppose that the $M_i^{(1)}$ propositions are exhaustive. For any set of models you can produce, there is sure to be an alternative model you haven't thought of yet. And so goes the infinite regress... Exhaustiveness is crucial here, because this ensures the probabilities add to 1, which means we can marginalise out the model. But this is all at the conceptual level - model averaging has good performance. So this means there must be a better concept. Personally, I view models as tools, like a hammer or a drill. Models are mental constructs used for making predictions about or describing things we can observe. It sounds very odd to speak of a "true hammer", and equally bizzare to speak of a "true mental construct". Based on this, the notion of a "true model" seems weird to me. It seems much more natural to think of "good" models and "bad" models, rather than "right" models and "wrong" models. Taking this viewpoint, we could equally well be uncertain as to the "best" model to use, from a selection of models. So suppose we instead reason about the propostion: $$M_i^{(2)}:\text{Out of all the models that have been specified,}$$ $$\text{the ith model is best model to use}$$ Now this is a much better way to think about "model uncertainty" I think. We are uncertain about which model to use, rather than which model is "right". This also makes the model averaging seem like a better thing to do (to me anyways). And as far as I can tell, the posterior for $M_{i}^{(2)}$ using BIC is perfectly fine as a rough, easy approximation. And further, the propositions $M_{i}^{(2)}$ are exhaustive in addition to being exclusive . In this approach however, you do need some sort of goodness of fit measure, in order to gauge how good your "best" model is. This can be done in two ways, by testing against "sure thing" models, which amounts to the usual GoF statistics (KL divergence, Chi-square, etc.). Another way to gauge this is to include an extremely flexible model in your class of models - perhaps a normal mixture model with hundreds of components, or a Dirichlet process mixture. If this model comes out as the best, then it is likely that your other models are inadequate. This paper has a good theoretical discussion, and goes through, step by step, an example of how you actually do model selection.
