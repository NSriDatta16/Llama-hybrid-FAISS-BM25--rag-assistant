[site]: crossvalidated
[post_id]: 365048
[parent_id]: 361687
[tags]: 
how does lasso method find which features are redundant to shrink their coefficients to zero? The Lasso method on its own does not find which features to shrink. Instead, it is a combination of Lasso and Cross Validation (CV) which allows us to determine the best Lasso parameter $\lambda$. Different $\lambda$ may shrink different parameters to zero, and choosing the right $\lambda$ can sometimes be easy (e.g.) in the graphic below, and sometimes be difficult/subjective task. A good introduction to combining Lasso and cross validation is provided by the inventor of the Lasso, Robert Tibshirani , pages 15 and 16 here . Also here: Lasso cross validation and here What happens when features are correlated The case of correlated features and Lasso is more subtle and is well explained here: How does LASSO select among collinear predictors? . But in practice there is some arbitrariness in which features are included in the model and when/why. Quoting EdM In practice, the choice among correlated predictors in final models with either method is highly sample dependent, as can be checked by repeating these model-building processes on bootstrap samples of the same data. If there aren't too many predictors, and your primary interest is in prediction on new data sets, ridge regression, which tends to keep all predictors, may be a better choice.
