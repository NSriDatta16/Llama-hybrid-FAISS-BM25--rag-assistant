[site]: datascience
[post_id]: 81797
[parent_id]: 
[tags]: 
How can a ML algorithm learn to classify fake news?

I am new in Machine learning techniques and in fake news detection by using these algorithms (SVM, nn, logistic regression,..). I would like to understand how an algorithm can learn from a training set which include news and fake news, what it will be necessary to have (target), what type of information can be relevant for a good analysis and learning ... I read many papers and most of them do not mention the type of news used, the criteria for building the classifiers, but they only show the results of the prediction when a new news is generated. This could be perfectly fine, but it could be also easily to say: I got this amazing results!, without understanding how these results were got. I would appreciate if you could show me an example of how an algorithm can learn from text (not necessarily news, but also tweets, or something else would be fine). Following discussion with Erwan: one of his previous answer partially has answered my question. However I would like to understand the following. One needs to have a corpus, then label news/tweets in fake/not fake, then run the model. But how the algorithm works on texts and takes relevant words or features for detecting fake news? So my question is: If I have a corpus on Trump, would the algorithm be able to detect fake news on Vitamin C, without any words (verb, adj, noun,..) in common between the two dataset, except stopwords?
