[site]: crossvalidated
[post_id]: 452205
[parent_id]: 451750
[tags]: 
This is perfectly reasonable question, which is actually related to purely technical aspect of deep learning frameworks. I assume you know how Backpropagation Through Time (BPTT) works since you're asking about RNNs. TL;DR - Keras was built to support frameworks like Theano and Tensorflow which supported only static computation graphs. From my understanding, one of the advantages of sequence models like RNNs is that they can handle variable length input sequences. This is true, but in practice you wouldn't actually use inputs of arbitrary lengths, since standard RNN vanishing/exploding gradient problems you don't actually need arbitrary length because your dataset will have a longest input you can easily adapt shorter sequences to longer - by padding them Why would you want to adapt all sequences to have same length? That's basically because it will allow you to do batching. Why would you want to keep the size of tensors constant when running your data by batches? This is no longer required (for example in PyTorch and MXNet you can unroll your network dynamically), but older frameworks like Theano and Tensorflow (until recently) required you to write static computation graphs. Basically this means they required you to know the shape of data before you feed it into NN - creating a single computation graph and then rerunning it was significantly faster than creating a graph each time you execute it.
