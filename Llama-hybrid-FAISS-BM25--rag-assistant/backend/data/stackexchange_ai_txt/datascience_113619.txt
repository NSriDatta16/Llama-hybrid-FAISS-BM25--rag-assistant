[site]: datascience
[post_id]: 113619
[parent_id]: 
[tags]: 
Why my neural network model is not able to learn a simple linear function?

I'm trying, using a neural network, to predict a simple relation $ f(x) = 1-x $ . I write my function this way: $f(x) = 1 - x^{+} + (-x)^{+} $ or in a more data-scientistic way: $f(x)= 1-Relu(x)+Relu(-x)$ .Thus, I have an idea about the optimal weights my model should find and an appropriate architecture for the latter. So I choosed this architecture: X = np.random.uniform(-5., 5., size=(50000, 1)) y = 1-X model = keras.models.Sequential() model.add(keras.layers.Dense(2, activation='relu', input_shape=(1, ))) model.add(keras.layers.Dense(1)) model.compile(loss='mse', optimizer='sgd') model.fit(X, y, epochs=200, validation_split=0.2, callbacks=[keras.callbacks.EarlyStopping(patience=10)]) My model is struggling to predict this relation and is, by far, outperformed by a 'simple' linear regression ( got a rmse of $10^{-13}$ versus a rmse of $10^{-31}$ for linear regression ) I am wondering why my model can't reach the optimal weights. I tried to increase the number of epochs and use a validation set to avoid overfitting but the optimal weights are not reached. Can someone please advise me how to get rid of this problem and make my Neural Network predicts this linear relation? Thanks
