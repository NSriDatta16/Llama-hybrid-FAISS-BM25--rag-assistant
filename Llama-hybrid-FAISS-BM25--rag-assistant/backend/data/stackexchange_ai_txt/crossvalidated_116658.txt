[site]: crossvalidated
[post_id]: 116658
[parent_id]: 116655
[tags]: 
There are several combinations on the size of $n$ and $p$: small $n$ - large $p$, small $p$ - large $n$, large $n$ - large $p$ ... See Johnstone & Titterington, 2009, Statistical challenges of high-dimensional data for an overview. In your case, it appears that you have a small $p=1$ and relatively small $n$, with high dimension of the dependent variable. It is likely that your independent variable may not contain enough information to properly model $300$ responses. The justification of this claim is as follows. If you use a GLM for your data, then you have $20$ samples to estimate $300+$ parameters in the covariance matrix of the errors. This may induce over-fitting, and the precision of the estimators will be unnecessarily vague (in the sense that confidence intervals for these parameters might be too wide) and inaccurate (far from the true value). However, if you restrict the structure of the covariance matrix, then it may be possible to estimate the parameters more accurately (How to restrict the structure of the covariance matrix? That's a big question which depends on the context). Moreover, the fewer covariates you use, the more "responsibility" the residual errors carry to explain the unobserved variability. This may, for instance, inflate the variances or induce the need for more flexible distributions than normal for modelling the residual errors. Additional references of possible interest: West, 2003, Bayesian Factor Regression Models in the “Large p, Small n” Paradigm CV question: Summary of "Large p, Small n" results
