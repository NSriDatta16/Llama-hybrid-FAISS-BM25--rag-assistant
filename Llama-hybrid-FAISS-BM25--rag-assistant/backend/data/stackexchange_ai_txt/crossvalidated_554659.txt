[site]: crossvalidated
[post_id]: 554659
[parent_id]: 
[tags]: 
I need help understanding the meaning of the loss values of a WGAN with Gradient Penalty

I am currently working on training a Auxiliary Classifier Wasserstein GAN with Gradient Penalty. I based my implementation off of https://keras.io/examples/generative/wgan_gp/ (to which I added the Auxiliary Classifier functionality). I have a model trained off of quite a few epochs of emoji images, and I am now trying to filter out bad (or vice versa, good) samples from the set of generated examples. I understand that the best way to automatically do this is to utilize the trained discriminator and its loss values to evaluate the "fakeness" of the images in order to be able to select ones that are able to fool the discriminator. Now, I understand, that the Discriminator, using the Wasserstein Distance attempts to separate the loss values of fake samples as much as possible from the ones of real samples. Given that my Discriminator output for a randomly chosen set of generated images looks something like the below table, I think that the images with maximum or minimum values should be the highest and lowest graded examples. -22.96732 -12.37780 -23.39248 -44.45711 -14.15668 -11.19169 -35.99777 -9.65943 -16.71531 -9.35125 -25.98240 -4.36232 -8.58446 -24.78805 -7.47653 -19.14746 -28.33695 -30.18404 2.67499 -8.63077 This should mean that my most "fake" and most "real" generated image are the samples with -44.4 ... or 2.67... , but neither of these outliers are particularly more "real" looking than the other and both look much worse than randomly chosen examples (see here: https://i.stack.imgur.com/klYnD.jpg ) How should I interpret these loss values? Would it make sense to go for a median loss value to get more of the good looking "average loss" images? If so, why? Would it make sense to run a 2-means clustering algorithm on the losses to try and separate realistic and fake samples? Anyways, thanks in advance for your help
