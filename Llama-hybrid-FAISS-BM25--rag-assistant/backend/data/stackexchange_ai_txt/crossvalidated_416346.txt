[site]: crossvalidated
[post_id]: 416346
[parent_id]: 
[tags]: 
Why is it a good idea to initialize the bias vector to have small values in feedforward neural networks?

I was reading the Deep Learning book and had a question regarding Chapter 6.3.1 Rectified Linear Units and Their Generalizations . The book states that: Rectified linear units are typically used on top of an affine transformation: $$\pmb{h} = g(\pmb{W}^T\pmb{x} + \pmb{b})$$ When initializing the parameters of the affine transformation, it can be a good practice to set all elements of $\pmb{b}$ to a small positive value, such as $0.1$ . Doing so makes it very likely that the rectified linear units will be initially active for most inputs in the training set and allow the derivatives to pass through. My confusion regarding the passage is the reason given for initializing $\pmb{b}$ 's elements to be small values. Do the authors mean that it's a good idea because you typically want the derivatives to propagate through the network in the beginning stages of training? What is the normal intuition behind this practice? Thank you.
