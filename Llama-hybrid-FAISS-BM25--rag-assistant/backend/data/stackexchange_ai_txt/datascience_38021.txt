[site]: datascience
[post_id]: 38021
[parent_id]: 
[tags]: 
Replacing null with average in pyspark

I have a problem during upsampling operation in PySpark. My dataframe is: df_upsampled.show() +-------------------+------------------+ | et| average| +-------------------+------------------+ |2018-08-15 00:10:00| 4.165999948978424| |2018-08-15 00:15:00| null| |2018-08-15 00:20:00|3.6580000072717667| |2018-08-15 00:25:00| null| |2018-08-15 00:30:00|0.9999999925494194| What I want to do is that by using Spark functions, replace the nulls in the "sum" column with the mean value of the previous and next variable in the "sum" column. Wherever there is a null in column "sum", it should be replaced with the mean of the previous and next value in the same column "sum". In this case, first null should be replaced by (4.16599 + 3.658)/2 = 3.91 and so on for the rest nulls.. What would be a good way to do this?
