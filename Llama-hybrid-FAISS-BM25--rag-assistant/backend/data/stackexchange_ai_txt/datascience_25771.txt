[site]: datascience
[post_id]: 25771
[parent_id]: 
[tags]: 
How to drop input channels or neurons at the inference phase

I understand that dropout is used to drop some weights during training, is there a way one can conventionally train a neural network with full weights and then drop some weights at the inference? My understanding is that during testing, for instance using MNIST data-sets, the test accuracy is computed as: test_accuracy = accuracy.eval ({X: mnist.test.images, Y: mnist.test.labels}) How can I modify the code so that some weights are set to zero?
