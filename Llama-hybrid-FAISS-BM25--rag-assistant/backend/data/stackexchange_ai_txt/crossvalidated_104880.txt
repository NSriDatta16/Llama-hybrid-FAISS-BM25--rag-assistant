[site]: crossvalidated
[post_id]: 104880
[parent_id]: 103849
[tags]: 
The basic idea As per Chen, Linton, and Robinson (2001) , the "default" technique for conditional univariate kernel density estimation is to find, for bandwidths $a,b,c$, $$\frac{\hat{f}_{ab}(y,z)}{\hat{f}_c(z)}=\hat{f}_{abc}(y|z)$$ Then, with numerator bandwidth $(a,b)$ and denominator bandwidth $c$ and $a=b=c$, the following central limit result holds under certain independence and consistency assumptions (which are only really restrictive when $y=x_t,z=x_{t-1}$): $$ \sqrt{na^2}\left(\hat{f}_{abc=aaa}(y|z)-f(y|z)\right)\xrightarrow{d}N(0,V) $$ where $$\begin{align} \hat{V}&=\left(\int K(u)^2du\right)^2\cdot\frac{\hat{f}_{aaa}(y|z)}{\hat{f}_a(z)}\\&=\left(\int K(u)^2du\right)^2\cdot\hat{f}_{aa }(y,z) \end{align}$$ Although I've never see a frequentist weighted model (even intro-stats WLS) make an attempt to account for the variance of the estimated weights. For now I'm going to follow that convention but if I get results here I'll see if I can work it into a fully Bayesian model that will propagate uncertainty more honestly. So yes, estimating the conditional density by estimating the joint and marginal densities is standard procedure. Applicability to my case It's not explicitly clear from that paper is how this generalizes to the case when when $y=x_t$ and $z=\left(x_s\right)_{s=1}^{t-1}$, and $x_s=\left(\begin{smallmatrix} x_{s,1}\\ \ddots \\x_{s,D} \end{smallmatrix}\right)$. But I guess this is really just the same thing as one big long sequence $x=\left(\left(x_{s,d}\right)_{d=1}^{D}\right)_{s=1}^{t-1}$ which seems perfectly manageable according to Robinson (1983) (cited in Chen, et al). Again, using Bayes' rule to estimate the conditional density seems perfectly acceptable. Bandwidth The final issue is bandwidth selection. Bandwidth is now a block matrix of the form $$ B=\left(\begin{matrix} B^{numerator}&0\\0&B^{denominator} \end{matrix}\right)=\left(\begin{matrix} \left(\begin{matrix}a_{1,1}&&B^{num}_1\\&\ddots&\\B^{num}_2&&a_{t,D}\end{matrix}\right)&0\\0&\left(\begin{matrix}c_{1,1}&&B^{denom}_1\\&\ddots&\\B^{denom}_2&&c_{t-1,D}\end{matrix}\right) \end{matrix}\right) $$ which is a mess. When bandwidth $H=hH_0$ such that $|H_0|=1$, then $b^*\sim\sqrt[4+D]{N}$, but this result would apply separately to $B^{num}$ and $B^{denom}$ rather than to $B$ as a whole ( source , someone's lecture notes). Chen et al find an optimal bandwidth $a=b=c$ (in their 2-d case) for a given level of $z$ that looks like it generalizes to the case when $y$ and $z$ are multivariate. They suggest setting $z=\mu$ where $\mu$ is the theoretical mean that would be induced under joint normality, and they derive $\hat{a}(\mu)$. A more general version of the same result is in another section of those lecture notes, called "rule-of-thumb" bandwidth. They also derive optimal bandwidth as a function of a general cross-validation procedure. Computation I have a 7-dimensional treatment over 3 time periods, so I have up to a 21-dimensional density to estimate. And I forgot about baseline covariates. I have something like 30 baseline covariates, so I would end up trying to estimate a 51-dimensional distribution, a 44-dimensional distribution, and a 37-dimensional distribution. And that's not to mention that the extreme dimensionality will require an impossibly large sample. Scott & Wand (1991) report that a sample size of 50 in one dimension is equivalent to well over 1 million in 8 dimensions... no mention of 30. No amount of these can express how I feel right now. Conclusion So I just wasted a week of my life on this. Oh well. Instead, I'm going to use MCMC to fit parametric treatment and outcome models simultaneously, so that the IPT weights end up being a function of the posterior predictive densities from the treatment model. Then I'll step through linear, quadratic, and cubic forms for the treatment model and see which one fits the best.
