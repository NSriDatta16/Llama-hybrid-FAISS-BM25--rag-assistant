[site]: datascience
[post_id]: 112911
[parent_id]: 
[tags]: 
How to guide exploration in reinforcement leanring/model predictive control/dual control problem

Consider the following optimization/control problem: We aim to maximize the cumulative reward R during the horizon H by every day allocating a portion of total budget B to our two different investment options inv1 and inv2 and the same day seeing the response/reward for that day. Lets say we also have an maximum we can spend everyday. I.e $$ \max_{inv1t, inv2t} \sum_{t=t}^{t=t+H} r(inv1t, inv2t, independent \; variables) \\s.t \sum_{t=t}^{t=t+H} inv1t + inv2t = B \\ inv1t + inv2t The reward function will be modelled as bayesian linear regression with weak priors, and lets say we left out the other independent variables: $$ \begin{equation} r(inv1t, inv2t) = c1 * inv1t + c2 * inv2t \\ where \; as \\ c1 \sim \mathcal{N}(\mu,\,\sigma^{2})\,. \\ c2 \sim \mathcal{N}(\mu,\,\sigma^{2})\,. \end{equation} $$ To our hand we have daily historical data that spans for lets say 3 months back. This data can show great exploration where as we have tried alot of different budgets but it could also be constant for both the channels where as vast exploration is needed in the algorithm for us to learn about the environment. Algorithm to consider: Fit our reward function to the historical data by using bayes rule. Sample an set of parameter values. Use the above parameter values as ground truth and maximize the reward function over the whole horizon s.t to the budget constraints. Set the allocation for the current timestep. See the reward for the current timestep. Add the allocation and reward to our dataset. Repeat step 1. In this case however. The signal-to-noise ratio might be really low so we are really keen to get good reliable estimates of the coefficients for the regression model. The environment might change as well so we want to keep exploring even though we are really sure about our estimates of the coefficients. My idea of sampling the parameter values at step 2 of the algorithm seems suboptimal. There must be an smarter way of guiding the exploration. Dual control seems to be a community that have tried to tackle similar stuff before but i cant find my actual case anywhere. Can anyone guide me on good literature or give it a go themselves?
