[site]: crossvalidated
[post_id]: 571255
[parent_id]: 548505
[tags]: 
The residual connection adds the sequence of input tokens to the sequence of output tokens. It works on a token by token base. So the resulting token at position n is simply the output token of the masked multihead attention at position n, plus the input token at position n (ONLY). Since the output token has no look ahead bias thanks to the masking and the input token has no look ahead bias, as it's only the unprocessed input token at position n, the residual connection doesn't introduce any look ahead bias. (This is also true for stacked attention layers as long as look ahead masks are used in each layer)
