[site]: crossvalidated
[post_id]: 616317
[parent_id]: 
[tags]: 
Too good results with linear regression on a non-linear dataset due to training on seen data?

I plotted some time series data that looks non-linear as can be seen below. ![Text] [ Looks pretty non-linear, but I decided to implement a linear regression model for learnings sake. train_data = df["High"].iloc[:math.ceil(10382*0.8)].to_numpy().reshape(8306, 1) y_data = df["High"].iloc[1:math.ceil(10382*0.8)+1].to_numpy().reshape(8306, 1) As you can see, I have actually decided to try using the exact same series data as the X and Y values. Just I have shifted the positions of data by 1. from sklearn.linear_model import LinearRegression model_lr = LinearRegression().fit(train_data, y_data) When I scored the training results, model_lr.score(train_data, y_data) , the coefficient of determination is good (0.9996742517879881), nearly 1. I thought it was just overfitting but when I predicted values, pred = model_lr.predict(test_data) model_lr.score(pred, y_test_data) The coefficient is good as well (0.9992371497078137). If the data I used was non-linear, is the problem here that I am using the same series data to model it? As I don't think a linear model should be performing well on non-linear data. I thought it would be ok using the same series data to model if it mimicks actual inflow of stock price data. So as data flows in, just plug that into the model. Is the linear regression model simply taking the previous series data and adding a small bias only? Is that how it is getting so accurate? I plotted the predictions to the test data, and it looks like this weird straight line. Note also that this is not multiple regression.
