[site]: datascience
[post_id]: 30411
[parent_id]: 30239
[tags]: 
Transferring an original image to a different style of image is called style transfer . Although you can use a GAN for such a case, I am not convinced this is the best approach. Furthermore, there have been many papers recently published suggesting very interesting ways for which to train models for style transfer. An amazing implementation of a file transfer algorithm which can convert video in real time can be found here . There is another method that I have come across more recently which uses the same approach but a different loss function. The paper can be found here . The underlining method These methods use a combination of two networks. The first network acts as a transformation function $f_W$ which maps your image $x$ to a transformed image $\hat{y}$. The purpose is to train this network such that the output $\hat{y}$ is the original input image in the new style, in your case as line art. To train this network we will use the second network on the right side which learns how to mix in the Style Target and the Content Target . We want the transformed image $\hat{y}$ is still similar in content, but we want the transformed image to have a similar style as the target. To train this network we will use a massive amount of input images using some online database source. This training process will be very long. However, once it is complete then we can strip the network on the right side and you will only need the network which approximates $f_W$. You will then be able to put a new image into the input of this network and the output will be the line art version of the input image. You will need to tune the extent at which the input image is line artified by tuning the weight on the style loss during the training.
