[site]: crossvalidated
[post_id]: 286529
[parent_id]: 286522
[tags]: 
Talking about feature scaling - it's optional, and need of it depends on data values. The motivation behind feature scaling is to speed up your backpropagation. Because to update your weights you will use some algo (i.e. gradient descend). And most of this algorithms works better on scaled features (in fact, performs worse on non-scaled). So, depending on your data values you may not even see the speed difference, but more your feature values differs, the more it affects the backptop performance. And, by the way, idea to map words into categorical integers may end up with bad ANN accuracy (with high odds). Because integers are linear, not categorical discrete. I.e. $dog \ngtr cat$ , while $2 > 1$ . And your ANN probably will be "tricked" with this. In some cases, it will increase your performance/accuracy, but in most cases it will hurt. More "classic" approach for this is to map categories into sparse vectors, just like with termvectors in NLP - single dimension for single category.
