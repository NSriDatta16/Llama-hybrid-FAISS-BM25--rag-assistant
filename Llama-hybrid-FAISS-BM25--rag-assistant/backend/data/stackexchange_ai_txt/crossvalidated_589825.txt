[site]: crossvalidated
[post_id]: 589825
[parent_id]: 
[tags]: 
How to prove that L2 loss converges faster than L1 loss

I see many sources claim this result, however, I was not able to find a proof for it. I think this should be given in some paper or book. Can someone point me to some resources, or even better, show me the proof? Thanks in advance! To clarify my question, for a given dataset of $\{x_i, y_i\}$ , where $x_i$ represents the data point, and $y_i$ represents the labels, I want to show that using the loss function $$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}(f_\theta(x_i) - y_i)^2$$ makes $f_\theta(x_i)$ learn faster using gradient descent than when using the loss function $$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}|f_\theta(x_i) - y_i|.$$ Faster is referring to the number of iterations required to achieve $$\frac{1}{N}\sum_{i=1}^{N}|f_\theta(x_i) - y_i| \leq \epsilon$$ with $\epsilon$ being a loss threshold. In both cases, the neural network will be initialized with the same weights.
