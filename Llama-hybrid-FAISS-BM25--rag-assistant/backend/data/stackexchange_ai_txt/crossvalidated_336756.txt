[site]: crossvalidated
[post_id]: 336756
[parent_id]: 336045
[tags]: 
Since the activation is applied not directly on the input layer, but after the first linear transformation -- that is, $\text{relu}(Wx)$ instead of $W\cdot \text{relu}(x)$, relu will give you the nonlinearities you want. And it makes sense for the final activation to be relu too in this case, because you are autoencoding strictly positive values. Scaling and normalization is still important, because the initialization of neural network weights is carefully chosen so that for reasonably scaled inputs, the optimization process is greatly eased. A simple scaling of the inputs to around [0,1] should do the trick.
