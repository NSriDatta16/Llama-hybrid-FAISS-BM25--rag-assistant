[site]: crossvalidated
[post_id]: 606056
[parent_id]: 
[tags]: 
Simulating dataset for class

Not sure whether what I’m after is possible, but thought I would ask. I am trying to create a database with a dichotomized dependent variables and a bunch of binary (1/0) independent variables. I'd like to pre-set some associations between the independent variables and the outcome. Doing so is relatively easy - I basically run a binomial function to predict the outcome, where the #number_of_test=1 and the probability of an event = expit(b0+b1 var1+b2 var2.....), where b0=frequency of the outcome, and b1,b2,....=my pre-defined odds ratio for each covariate. What I realize however is that the prespecified ORs are never fully retrievable when I run a simple GLM model on the final simulated data. This makes sense, since the process of switching from a probability of having the outcome to a dichotomized 1/0 variable has some randomness to it. One can see this for example by running (in R): Y and then mean(Y) multiple times - the results change across individual runs. All of this means that for each database I would generate, the obtained ORs are close to the ones I a-priori defined, but not exact. Does that mean that there is never really a way to retrieve a prespecified OR from a single simulated database? I completely understand that one way here would be simulate the data many times and run the GLM on each simulated database – on average the prespecified ORs should be obtained. But is there a way of ensuring one gets the correct/prespecified ORs with only one simulated database?
