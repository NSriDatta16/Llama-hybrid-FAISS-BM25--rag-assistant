[site]: datascience
[post_id]: 84673
[parent_id]: 
[tags]: 
SVM on BERT-Embeddings with very small dataset does not converge

I am trying to reproduce the results from this paper where they use a linear SVM on top of BERT-Embeddings for text-classification. They use the average of the token-embeddings which results in a 768 dimensional vector for each sample. As this is active-learning, the number of samples used is very small (between 40 and 500). So: nb_samples . I thought that a SVM converges most of the times (see this question ), and surely in the cases where nb_samples . However, this is not the case, even with only 40 samples, the SVM (LinearSVC from sklearn) does not converge. I am quite sure my embeddings are somehow meaningful, they look also ok (values between between -1 and 1) no need for normalization etc. What could be my problem? I tried dual=True and dual=False, increasing max_iter to 10000 etc.
