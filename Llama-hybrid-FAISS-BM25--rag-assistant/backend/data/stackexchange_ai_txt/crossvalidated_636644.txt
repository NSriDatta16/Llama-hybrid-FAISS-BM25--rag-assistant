[site]: crossvalidated
[post_id]: 636644
[parent_id]: 440595
[tags]: 
Here's an idea I am currently pursuing. I have large biological data sets (in about 20,000 dimensions, one for each gene). The mental model is that the actual data lives on (or at least near) a manifold of dimension smaller than 20,000. There are some existing mathematical/statistical tools to try to compute the "local dimension" of that manifold near any data point. (See Ellis and McDermott, Computational Statistics & Data Analysis, Volume 17, Issue 3, March 1994, Pages 317-326.) These frequently give local estimates in the range of 20-30 or more dimensions. In that case, we know that shrinking things down to 2 dimensions will destroy some (possibly all) of the topological structure of the manifold. By the Whitney Embedding Theorem, we can embed any N-dimensional real manifold into 2N-dimensional Euclidean space. So, there must exist a way to put those 20-30 dimensional data manifolds into 40-60 dimensional space. So, there's your potential mathematical justification. In practice, finding such an embedding may be an open problem. I found your question because I was searching for an implementation of either t-SNE or UMAP that would reduce from large dimensions down to moderate dimensions. (The existing implementations, at least in R, seem to only believe in 2 or 3 for visualization purposes.) In my case, I can easily reduce from 20,000 dimensions to M-dimensions, where M is the number of samples. (Just use principal components analysis, and that is the maximum number of components you can get.) It's not clear what happoens when you take fewer PCs, since explainng less variance may fail to be an embedding of the data manifold.
