[site]: crossvalidated
[post_id]: 29592
[parent_id]: 29580
[tags]: 
" Sparse Algorithms are not Stable: A No-free-lunch Theorem " I guess the title says a lot, as you pointed out. [...] a sparse algorithm can have non-unique optimal solutions, and is therefore ill-posed Check out randomized lasso , and the talk by Peter Buhlmann . Update: I found this paper easier to follow than the paper by Meinshausen and Buhlmann called "Stability Selection". In " Random Lasso ", the authors consider the two important drawbacks of the lasso for large $p$, small $n$ problems, that is, In the case where there exist several correlated variables, lasso only picks one or a few, thus leading to the instability that you talk about Lasso cannot select more variables than the sample size $n$ which is a problem for many models The main idea for random lasso, that is able to deal with both drawbacks of lasso is the following If several independent data sets were generated from the same distribution, then we would expect lasso to select nonidentical subsets of those highly correlated important variables from different data sets, and our final collection may be most, or perhaps even all, of those highly correlated important variables by taking a union of selected variables from different data sets. Such a process may yield more than $n$ variables, overcoming the other limitation of lasso. Bootstrap samples are drawn to simulate multiple data sets. The final coefficients are obtained by averaging over the results of each bootstrap sample. It would be great if somebody could elaborate on and explain this algorithm further in the answers.
