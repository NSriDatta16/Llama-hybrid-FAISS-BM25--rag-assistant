[site]: crossvalidated
[post_id]: 243103
[parent_id]: 87182
[tags]: 
This question was raised two years ago and there have been many awesome answers already, but I'd like to add mine which helped myself a lot. The question is What purpose does the logarithm serve in this equation? The logarithm(usually based on 2) is because of the Kraft's Inequality . $\sum_{i=1}^m 2^{-l_i} we can intuit it this way: sum of the probability of all the code with length $l_i$ is smaller than 1. From the inequality we can derive the following result that for every code length function $L_x$ of a uniquely decodable code, there is a distribution $P(x)$ such that $P(x) = 2^{-L(x)} $, And hence $L_{(x)} = -logP(x)$ and $P(x)$ is the probability of the code with length $L_{(x)}$. The Shannon's entropy is defined as the average length of all code. Since the probability of every code with lenght $L_{(x)}$ is $P(x)$, the average length(or Shannon's entropy) is $-P(x)logP(x)$ . An intuitive illustration and a visual answer(as you required, but more specifically for the Kraft's Inequality) is articulated in this paper Code Tree, and Kraft's Inequality .
