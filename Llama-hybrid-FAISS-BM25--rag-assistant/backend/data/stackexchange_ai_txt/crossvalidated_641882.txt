[site]: crossvalidated
[post_id]: 641882
[parent_id]: 
[tags]: 
Softened logical implication using statistics

In a machine learning application, I have the following boolean implications $$\forall\,x,y:I_b(x,y)\rightarrow E_b(x,y),$$ where $I_b(x,y)$ being true means $x$ and $y$ are isomorphic, and $E_b(x,y)$ being true means that the distance between the embeddings generated by a model of $x$ and $y$ is zero. Now, I want to soften this condition by introducing, $I(x,y)$ which is now a distance between $x$ and $y$ such that $I(x, y)$ is 0 when $x$ and $y$ are isomorphic, while the number is small and positive for "almost" isomorphic $x'$ and $y'$ , and large and positive for other cases. I'll also introduce $E(x,y)$ which is just the $L_2$ -distance between embeddings of $x$ and $y$ . With this, I can still state that $$\forall\,x,y:\text{Low } I(x,y)\rightarrow\text{Low }E(x,y).$$ I have the values of $I(x,y)$ and $E(x,y)$ for all pairs of $x$ and $y$ in my dataset. How can I statistically verify that the above implication holds within my dataset. I thought of using correlation, but correlation is symmetric while the implication is one-directional. Also, I know I have to do some sort of thresholding on $I(x,y)$ and $E(x,y)$ to make them booleans as well, but how does this thresholding value affect the truth of the implication? I have no clue how to approach this last question. I also plotted the values of $I_{\text{similarity}}(x,y)=e^{-I(x,y)}$ on x-axis and $E(x,y)$ on y-axis (because my colleague computed $I_{\text{similarity}}(x,y)$ instead of raw $I(x,y)$ ) and I should see negative correlation towards high values of $I_\text{similarity}(x,y)$ and $E(x,y)$ , however I cannot understand the graph as of now. Any direction towards solving this is appreciated.
