[site]: crossvalidated
[post_id]: 191729
[parent_id]: 
[tags]: 
Why is optimal learning rate obtained from analyzing gradient descent algorithm rarely (never) used in practice?

Why is optimal learning rate obtained from analyzing gradient descent algorithm rarely (never) used in practice? Gradient descent procedure is to iteratively do $a(k+1) = a(k) - \eta(k)\nabla J(a(k))$. Expanding $J(a(k+1))$ using $2^{nd}$ order Taylor expansion and taking the derivative with respect to $\eta$, one obtain the optimal learning rate of $$\eta^{opt} = \frac{||\nabla J||^2}{\nabla J^T H \nabla J}$$ where $H$ is the second order derivative of the cost function. However, I have not seen this being used in any learning algorithm that employs gradient descent like SVM or perceptron. Is there any reason for that? Or is it implicitly employed in a way that I am not aware of. If so, can anyone illustrate the math involved?
