[site]: crossvalidated
[post_id]: 312876
[parent_id]: 
[tags]: 
Why are random Fourier features non-negative?

Random Fourier features provide approximations to kernel functions. They're used for various kernel methods, like SVMs and Gaussian processes. Today, I tried using the TensorFlow implementation and I got negative values for half of my features. As I understand it, this shouldn't happen. So I went back to the original paper , which---like I expected---says that the features should live in [0,1]. But its explanation (highlighted below) doesn't make sense to me: the cosine function can produce values anywhere in [-1,1] and most of the points displayed have negative cosine values. I'm probably missing something obvious, but would appreciate it if someone can point out what it is.
