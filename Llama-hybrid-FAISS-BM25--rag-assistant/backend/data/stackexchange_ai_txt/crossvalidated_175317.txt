[site]: crossvalidated
[post_id]: 175317
[parent_id]: 
[tags]: 
Obtaining uncertainties from an errors-in-variables machine learning algorithm

In my field, every value reported comes with a 1-sigma uncertainty value. I'm using random forest regressors to estimate a value. All of my inputs have 1-sigma uncertainty information with them. My random forest regressor is trained on error-free simulations, and now I want to estimate a value from real data. In a monte carlo fashion, I perturb all of my inputs within their 1-sigma errors 10,000 times and feed them to my random forest. I get out 10,000 estimates of the value. My question: in order to obtain 1-sigma uncertainties for my predicted value, assuming the values I get out are distributed normally, is it OK to just take the mean and standard deviation of the 10,000 values? My intuition is no: that I need to obtain prediction intervals for each of the 10,000 estimates, and somehow combine them. Is that right? And if so, how?
