[site]: crossvalidated
[post_id]: 241122
[parent_id]: 
[tags]: 
What is the preferred way to use deep learning to learn output distributions instead of point estimates?

I was wondering what would be the preferred model to use deep learning to learn an output distribution instead of a point estimate. Specifically, there might be the case that certain types of datapoint have associated output quantities (labels or regressed vectors) that are more uncertain (read: noisy) than other types of datapoint. It would be interesting to model that uncertainty so that for a new datapoint a distribution (specified by gaussian parameters mu and sigma) is produced instead of a fixed output quantity (point estimate). A probabilistic interpretation for a regular neural network is that the error is modeled via a gaussian that is centered in the output vector and has fixed sigma (homoscedastic), and this would lead to our typical RMSE / L2norm loss function. Would replacing the fixed sigma with a variable quantity that is output from the neural network viable? This would be similar to the output of the encoder of a Variational Auto Encoder (just before the sampling/reparameterization trick stage). Has this been extensively explored in literature? Does it ease the learning and lead to better accuracies? I would be grateful if you can provide some pointers to literature. I found something such as: Training Neural Networks with Implicit Variance and Mixture Density Networks . I wonder if there is more.
