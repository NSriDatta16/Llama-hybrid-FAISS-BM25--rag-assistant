[site]: crossvalidated
[post_id]: 563554
[parent_id]: 563527
[tags]: 
I'm not aware of a standard term, and agree that it's slightly annoying. A quick glance at Cover & Thomas seems to confirm that they don't discuss this. Wikipedia calls $H(Y \mid X=x)$ the "entropy of $Y$ conditioned on $X$ taking the value $x$ ". I personally tend to use something like "the entropy of the conditional distribution" (when it's clear which distribution we're talking about) or "the conditional entropy of $Y$ given $X=x$ ". The latter is similar (modulo capitalization) to MacKay's terminology in Information theory, inference, and learning algorithms . From section 8.1: The conditional entropy of $X$ given $y=b_k$ is the entropy of the probability distribution $P(X \mid y=b_k).$ $$H(X \mid y=b_k) \equiv \sum_{x \in \mathcal{A}_X} P(x \mid y=b_k) \log \frac{1}{P(x \mid y=b_k)} \tag{8.3}$$ In the next sentence, he also uses the apparent shorthand "conditional entropy of $X$ given $y$ ": The conditional entropy of $X$ given $Y$ is the average, over $y$ , of the conditional entropy of $X$ given $y$ . $$H(X \mid Y) \equiv \sum_{y \in \mathcal{A}_Y} P(y) \left[ \sum_{x \in \mathcal{A}_X} P(x \mid y) \log \frac{1}{P(x \mid y)} \right] \tag{8.4}$$ But, I think this shorthand risks confusion because it relies on a small difference in capitalization ( $Y$ vs. $y$ ) to indicate a big difference in meaning (whether or not we're averaging over $y$ values).
