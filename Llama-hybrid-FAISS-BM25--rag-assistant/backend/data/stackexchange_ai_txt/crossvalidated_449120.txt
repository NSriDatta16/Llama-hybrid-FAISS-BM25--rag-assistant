[site]: crossvalidated
[post_id]: 449120
[parent_id]: 449111
[tags]: 
The answer will depend on who you ask, the experimental design, the analysis plan, and potentially even the journal you are publishing in. Frequentists and Bayesian have different philosophical beliefs about multiple testing. Observational studies are more at risk of data-driven bias than controlled experiments or trials. Different journals use different statistical guidelines that affect reporting. With that said, let's consider two common scenarios: Differential gene expression screens usually screen 20,000 or more variables (genes) for differential expression between two conditions. This means 20,000 hypotheses are usually tested. These screens are often used to generate hypotheses about potential genes of interest. Because the intent is hypothesis generation, we know multiple comparisons will lower our power and miss some genes, but we are OK with this because there are 20,000+ genes and we want to maintain an approximate error rate so we can determine how much money we should expect to spend on true/false positives and prioritize candidates for further research. For settings with an extreme number of tests, I think most would agree some sort of multiple comparisons (or bayesian shrinkage) is needed. Randomized controlled trials (RCTs) usually test a set pre-registered questions that were developed before scientists had access to the data. RCTs usually test around 5 primary hypotheses, a number far from the 20,000 hypotheses tested in gene expression screens. Here the issue becomes more blurry ; some statisticians such as Frank Harrell argue that pre-specified questions shouldn't be adjusted for multiple comparisons while the NEJM's Statistical Guidelines advocate When comparing outcomes in two or more groups in confirmatory analyses, investigators should use the testing procedures specified in the protocol and [statistical analysis plans] to control overall type I error — for example, Bonferroni adjustments or prespecified hierarchical procedures. These two scenarios represent different sides of the analysis spectrum. Gene expression screens are exploratory in nature, while RCTs are confirmatory. Studies that are exploratory and test a large number of hypotheses should probably adjust for multiple comparisons because the goal is usually to prioritize hypotheses for further investigation. Confirmatory studies with pre-registered questions are less prone to data-driven bias so it is less clear when to adjust for multiple comparisons in a confirmatory setting. This doesn't mean pre-define 1000 hypotheses for your confirmatory study, but instead think twice about if your experiment with two pre-registered, unrelated hypotheses needs multiple comparisons adjustment. In terms of what level (per section, paper, dataset etc) should you adjust for multiple comparisons, most people adjust for a set of related hypothesis tests pertaining to a single analysis. This analysis could encompass multiple datasets and may or may not take up multiple sections of the paper. Think about what parts of your paper are exploratory vs confirmatory and then think about whether hypotheses were pre-specified or data driven and how many tests were conducted. It's always helpful to talk to a statistician about your specific research to get better insight into these issues. Here are a few articles I suggest reading for further insight into this topic: No Adjustments Are Needed For Multiple Comparisons Kenneth Rothman; Epidemiology Multiplicity Considerations in the Design and Analysis of Clinical Trials Cook and Farewell; Journal of the Royal Statistical Society Frequentist versus Bayesian approaches to multiple testing Sjölander and Vansteelandt; European Journal of Epidemiology
