[site]: crossvalidated
[post_id]: 499426
[parent_id]: 
[tags]: 
Minimize the sum of squared perpendicular distances while computing PCA

Problem (PCA): Assume that p = 2 and the the predictors are centered. Show that the sum of squared perpendicular distances from ( $x_{i1}, x_{i2})$ , i = 1, 2, . . . , n to the line $a_{2}x_{1}−a_{1}x_{2}=0$ with a $a_1^2 + a_2^2 = 1$ is minimized when $a = φ_{11}$ and $b = φ_{21}$ , where $φ_1 = (φ_{11}, φ_{21})$ is a norm-one eigenvector associated with the eigenvalue $λ_1$ . I know that maximizing the variance along the principal component is equivalent to minimizing the reconstruction error,i.e., the sum of the squares of the perpendicular distance from the component. I don't know how can I integrate this idea here. I was wondering if you could give me some hints to solve this problem.
