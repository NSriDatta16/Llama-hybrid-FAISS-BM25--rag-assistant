[site]: crossvalidated
[post_id]: 23444
[parent_id]: 23439
[tags]: 
The implicit question here is how can you determine the topology/structure of a neural network or machine learning model so that the model is "of the right size" and not overfitting/underfitting. Since cascade correlation back in 1990, there has been a whole host of methods for doing this now, many of them with much better statistical or computational properties: boosting: train a weak learner at a time, with each weak learner given a reweighted training set so that it learns things that past learners haven't learnt. sparsity inducing regularization like lasso or automatic relevance determination: start with a large model/network, and use a regularizer that encourages the unneeded units to get "turned off", leaving those that are useful active. Bayesian nonparametrics: forget trying to find the "right" model size. Just use one big model, and be careful with regularizing/being Bayesian, so you don't overfit. For example, a neural network with an infinite number of units and Gaussian priors can be derived to be a Gaussian process, which turns out to be much simpler to train. Deep learning: as noted in another answer, train a deep network one layer at a time. This doesn't actually solve the problem of determining the number of units per layer - often this is still set by hand or cross-validation.
