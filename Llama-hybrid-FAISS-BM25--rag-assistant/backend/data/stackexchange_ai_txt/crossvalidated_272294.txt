[site]: crossvalidated
[post_id]: 272294
[parent_id]: 
[tags]: 
How to deal with variables which have large amount of NAs

currently I got three datasets: DS-A, DS-B and DS-C. Basically I hope to use these data to find out some typical/potential customers/targets for a certain group to make business plan with the help of some kinds of machine learning methods. Take DS-A as an example : tun_account 0.7))] %>% ncol() [1] 73 tun_account[, which(apply(tun_account, 2, function(x) sum(is.na(x))/length(x) > 0.9))] %>% ncol() [1] 57 As shown above, DS-A is a 233273x258 dataset, and among its 258 variables, there are 73 variables which have the number of NAs outnumbered a certain threshold (here are 0.7 and 0.9, but it could be changed). I understand that usually, we could impute NAs by applying lots of fancy methods like KNN, means, and so on. However, my question is, in my current context, do I need to impute these variables (will it be necessary to do that?) or just drop them (I am not sure whether these variables they are important or not)
