[site]: datascience
[post_id]: 39275
[parent_id]: 39266
[tags]: 
What is the meaning of: Non-trainable params: 454? Should this ideally be 0? If so, how can this be made 0. I guess it is due to using batch normalisation . Try to ascertain by investigating the math of that. If you see the paper, you can easily find out that the bias terms and $\beta$ , if I remember, both get added. Consequently, one can be ignored and won't be trained because it is not needed. Dense layer should be added at the end (in the part you mentioned in your comment) or can it be added in previous layers also? Not really, dense layers should be employed after conv layers. What they do is classifying the extracted features obtained by conv layers. About conv layers, they are employed for reducing the number of parameters and finding local patterns. There is no consensus on how to change the number of filters in convolutional layers, at least as far as I know. But there is a point here. In the following lines of your code, you've employed a kind of pooling layer just before dense layer. If the number of activations coming from conv layer is many, you can use it but consider that by doing so, you ignore important features. I suggest you not doing that, especially, for the last conv layer. Also, try to increase the number of neurons in a dense layer or add extra layers for better accuracy. conv2d_4 (Conv2D) (None, 16, 16, 64) 73792 _________________________________________________________________ global_average_pooling2d_1 ( (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 13) 845 ================================================================= About using batch norm, it is used due to a kind of problem which is called Covariat Shift . It simply tries to keep the distribution of the outputs of different layers in order to facilitate the learning process. Based on your questions, I highly recommend you watching professor Andrew Ng's course about ConvNets in Coursera.
