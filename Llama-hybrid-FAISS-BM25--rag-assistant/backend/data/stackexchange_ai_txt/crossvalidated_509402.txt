[site]: crossvalidated
[post_id]: 509402
[parent_id]: 508736
[tags]: 
So let me give you a somewhat more straightforward alternative to the ROPE method in Kruschke’s paper. You would need to solve the convolution of the two distributions to get to where you want to be. There is an easier solution, though it is less comparable to a Frequentist procedure. I would like to make an observation about Fisher’s “No Effect” hypothesis. In this case, the null hypothesis would be that group membership has no effect on the population parameter’s location. In other words, $\mu_1-\mu_2\equiv{0}$ . If this were a regression such as $z=\beta_xx+\beta_yy+\alpha$ , and the belief was that the variable $y$ had no impact on z, then the null would be that $\beta_y=0$ . However, there is an alternative specification that would also work as they are both equivalents. That would be to have the hypothesis that $z=\beta_xx+\alpha.$ The Bayesian equivalent is not to determine if a parameter is zero, but whether you can simply drop the variable as it is not probable that the variable is part of how nature generates the data. The two Bayesian hypotheses would be: $$H_1:x\text{ is drawn from a single population}$$ versus $$H2:x_1\text{ and }x_2\text{ are drawn from different populations.}$$ In the first model, the group designation means nothing. If you flipped a coin to divide kindergarteners’ classroom in half and measured their height, the heads versus tails groups would be artificial and were not part of how nature chooses sizes. In the second model, the group designation is meaningful. If you divided a group of same age adults from the same population by birth sex, you would find that they were different populations because nature does assign height by sex. As it is a Bayesian model, you will have to assign a prior probability to model one and model two. It can either be your prior probabilities or, if you have an opponent, then you could use their prior probabilities. If $f$ is the log-normal likelihood function, then the joint likelihood for model one is $$\prod_{i=1}^If(x_i|\mu;\sigma^2).$$ If $c$ is a categorical variable denoting group membership, then the join likelihood for model two is $$\prod_{j=1}^Jf(x_j|\mu_1;\sigma^2_1;c_j=1)\prod_{k=1}^Kf(x_k|\mu_2;\sigma^2_2,c_k=2), J+K=I.$$ You still need to set your prior distributions. I also assumed independence of draws. I would point out a couple of warnings. First, when you use uniform prior distributions, there is a chance, particularly as the dimensionality of variables becomes three or higher, that the posterior will not sum to one. You are comparing two groups, so it isn’t an issue here. Second, a uniform distribution in the raw space is not uniform in log space. You would be adding information, possibly unintentionally. Third, it no longer matters about finding some area around zero. You will not be subtracting parameters anymore. Fourth, you would not be comparing mean values anymore. You can have two lognormal distributions with the same population mean, but with different values for $\mu$ and $\sigma^2$ . The Bayesian method would not determine if $$\mu_1+\frac{1}{2}\sigma_1^2=\mu_2+\frac{1}{2}\sigma_2^2.$$ Instead, it would directly determine if they are from the same population. After all, variables could be drawn from different populations but would have the same population mean if $$\mu_1=\mu_2+\frac{1}{2}\sigma_2^2-\frac{1}{2}\sigma_1^2.$$ Remember that $\mu$ is the mean only after logarithmic transformation; otherwise, it is the mode. Suppose you set $\mu_1=1$ , $\mu_2=1.1$ , $\sigma_1=1$ and $\sigma_2=0.8944272$ . In that case, the t-test will say they are the same if performed in the raw data space, without the log transform. Simultaneously, the Bayesian method will say they are different, with just a modest sample size.
