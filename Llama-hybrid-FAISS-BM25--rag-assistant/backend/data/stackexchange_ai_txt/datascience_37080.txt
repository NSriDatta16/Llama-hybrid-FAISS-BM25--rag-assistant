[site]: datascience
[post_id]: 37080
[parent_id]: 37079
[tags]: 
It can approximate, but as the input grows the error of that approximated function will also grow exponentially, right? You are right, the neural network uses ReLu functions to approximate the output $f(x) = x^2$ and there is some error incurred, but you are forgetting that Non-linear functions also do approximation since we use only some subset of activation functions and incur some error which can be as huge as with ReLus There is also a number of neurons to consider. The more neurons/layers you have - the better the approximation can be (since more small-interval ReLus are fit to better approach the shape of $x^2$). If your input size grows, you should also consider increasing the number of neurons/layers, in this way the approximation will always be close enough. Now x² is a simple curve. How can ReLU perform better for real data which will be way more complicated than x²? The main reason for this is that even though other activation functions can approximate $x^2$ or something else better, they are not optimised as fast. Generally speaking, non-linear activation function decision surface is more complex than the loss created by ReLus and can contain better global minimum but this minimum is harder to find, and, thus, with the less sophisticated loss created by ReLus, we can find a better optima using some gradient descent procedure, and it will be much faster.
