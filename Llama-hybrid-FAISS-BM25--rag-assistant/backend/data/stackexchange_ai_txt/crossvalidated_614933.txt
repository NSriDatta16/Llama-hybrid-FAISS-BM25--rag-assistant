[site]: crossvalidated
[post_id]: 614933
[parent_id]: 
[tags]: 
Loss function for estimating the conditional variance by fitting $y_i^2$

I'm trying to detect anomolies in a dataset $i \in \{1,2,...,N\}$ where a random variable $y_i$ is expected to be drawn from a normal distribution with mean $\mu_i=0$ and variance $\sigma_i^2 (X_i)$ totally determined by (conditioned on) the multiple features $X_i$ . My hope is that I can use a Z-score threshold such that anomolies are marked by: $$Anomolies=\{i \in \{1,2,...,N\} \space | \space |y_i|/\sigma_i > Z_{thresh} \} $$ I am wondering if there is a "good" (e.g. maximum likelihood) way to formulate this as a regression problem in which any machine learning algorithm could be fit on $y^2_i$ given $X_i$ with a suitably-chosen loss function. In this case, presumably the predictions would correspond to estimates of $\sigma_i^2$ . But what loss function matches the probabilistic assumption that $y_i$ is drawn from $N(\mu_i=0, \sigma^2_i=f(X_i))$ for some function $f$ ? The inspiration for this question is that I was using natural-gradient-based methods like NGBoost to simultaneously fit $\mu_i$ and $\sigma_i$ . I've also tried quantile loss tree methods that use the quantile loss function. But here, since I only need to fit $\sigma_i$ , it seems there should be a way to formulate fitting $\sigma_i$ as a regular regression problem with a suitable loss function. A similar question has a response that states Linex Loss with a chosen parameter yields a prediction corresponding to the sum of a mean and variance, but I'm looking for only the variance. This and other questions don't seem to assume that fitting is being done to the reformulated target $y_i^2$ , or don't ask about a suitable loss function for this case. An argument against fitting to $y_i^2$ would also be a helpful answer.
