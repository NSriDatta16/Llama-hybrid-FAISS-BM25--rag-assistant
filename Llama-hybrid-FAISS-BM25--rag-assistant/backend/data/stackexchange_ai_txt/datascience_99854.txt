[site]: datascience
[post_id]: 99854
[parent_id]: 
[tags]: 
Building machine learning models whilst penalizing them for complexity

I come from a predictive modelling background, where it's common to use differential equations to model physical or chemical or biological processes. Commonly to avoid overfitting people use AIC and penalise models for the number of free parameters they have. This is useful - because it allows us to compare models across levels of complexity - for example, a model with twice the number of free parameters may reduce the RSS, but not to justify its extra parameters. Is there an equivalent method for Decision trees? Or for neural networks? In either case, both sets of models can always get better, when you add complexity, and is there an ML package that allows you to build models of increasing complexity whilst penalising them for increasing complexity?
