[site]: datascience
[post_id]: 116657
[parent_id]: 116656
[tags]: 
To sketch one approach, you may define the loss as: $$ loss \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right) = \left( w + 1 \right) \operatorname{CE} \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right), \quad w \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right) = \frac{ \left| \arg \max_{i} \boldsymbol{y} - \arg \max_{i} \hat{\boldsymbol{y}} \right| }{K - 1} $$ Where: $K$ - The number of classes (2 for Binary Classification). $\boldsymbol{y}$ - A vector in $\mathbb{R}^{K}$ which is the ground truth probabilities per class. $\hat{\boldsymbol{y}}$ - A vector in $\mathbb{R}^{K}$ which is the estimation of probabilities per class of the classifier. $\operatorname{CE} \left( \cdot, \cdot \right)$ - The Cross Entropy loss function. $\arg \max_{i} \hat{\boldsymbol{y}}$ - Extracts the index of the class with the highest probability. Basically the class represented by the vector. In the function $ w \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right) $ you may raise the distance between the class indices as you want. I think it should work well for you in the context of Deep Learning. For the matrix case, you may use the same loss as above and set: $$ w \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right) = {L}_{\arg \max_{i} \boldsymbol{y}, \arg \max_{i} \hat{\boldsymbol{y}}} $$ Where in my convention $ \boldsymbol{y} $ and $ \hat{\boldsymbol{y}} $ are vectors of the discrete distribution over classes. One simple extension would be: $$ w \left( \boldsymbol{y}, \hat{\boldsymbol{y}} \right) = \frac{ {\left| \arg \max_{i} \boldsymbol{y} - \arg \max_{i} \hat{\boldsymbol{y}} \right|}^{p} }{K - 1} $$ Where $ p \geq 1 $ .
