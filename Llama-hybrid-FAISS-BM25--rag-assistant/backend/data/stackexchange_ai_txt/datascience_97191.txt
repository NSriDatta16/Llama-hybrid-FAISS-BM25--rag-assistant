[site]: datascience
[post_id]: 97191
[parent_id]: 97180
[tags]: 
First, you need to understand what Sequential Modeling is? There are two categories in which Sequential Modeling falls in Suppose the data itself is a Sequence of the stream like audio, time-series data, textual data. Other is if you have the model who is working in sequential manner , what it means? Suppose you are giving a model training data but the model has not the capability to take the input at once, like in RNN you give the input in a sequence to the model first $x_1$ at Cell No.1 then $x_2$ at cell no2 and go on. So, your model itself hasn't the capability which could take the input at once, like in Neural Network, CNN, we give the input training examples at once in a batch. You are asking about the problem that how many models are there who has the capability to deal with sequential data? You have studied LSTM and GRU. These two are the core models before going to advance architectures. Besides these models, we have statistical models which deal with Sequential data, like n-gram model (deals with language modeling) Hidden Markov Model (for parts of speech tagging), etc. So, now understand what is Transformers basically? If you have studied LSTM and GRU well, then there are two big problems with them that are the model works in a sequential manner it means we can't take advantage of GPUs and the other problem is transfer learning is not possible using GRU and LSTM because of their recurrence. To, sort out these problems "Attention all you need" in this paper they proposed a model name "Transformer" and the technique which is used to enable the solutions for the above problem was "Attention Mechanism". The Architecture itself isn't Sequential but it can train and gives better results than LSTM and GRU on Sequential Data. Besides this many state-of-the-art models are build on top of the Transformer Model like "BERT" , "T-5" , "Roberta" , etc. These all work for sequential data training.
