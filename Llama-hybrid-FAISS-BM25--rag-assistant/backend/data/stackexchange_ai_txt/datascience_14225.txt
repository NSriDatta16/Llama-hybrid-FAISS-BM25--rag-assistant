[site]: datascience
[post_id]: 14225
[parent_id]: 11693
[tags]: 
Just averaging them might not be good because that would assume that they they have the same weight, and that is probably not the case if the capitalized and uncapitilized version appear with very different frequencies in your training data. An incremental improvement would be to average them proportionally to their frequency in the corpus. So say Earth appears 159 times and earth 1239 times do something like: v(Earth & earth ) = 159/(159+1239) * v(Earth) + 1239/(159+1239) *v(earth). The vectors are supposed to encode semantics linearly, so this should give you a resonable approximation.
