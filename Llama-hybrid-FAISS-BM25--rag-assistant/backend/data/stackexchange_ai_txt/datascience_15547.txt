[site]: datascience
[post_id]: 15547
[parent_id]: 15540
[tags]: 
This question is very common in the automation when machine learning used to perform specific tasks. Guaranteeing the quality is always a must. Evaluating the model while it is in production is not an easy task. the reason, why? In order to evaluate the model in production you need to have the ground truth. This ground truth is not available (if it is available no need to have the model). Getting the ground truth (by using human for example) is not a good solution: 1- It is very expensive, 2- Again, if you will generate the ground truth for data in production then no need to have a model. BUT how to deal with this problem in reality? I have worked recently on a prediction model that is used to predict the vehicles (Makes, Models), since every year we may have new models, makes, it is a good question to ask how often do I have to repeat the training process? ImpThree different ways I used to answer this question: I analyzed the changes in the training data I have year by year. based on this variation I can estimate the number of new makes and models appear every year and the number of makes and models disappear every year, and thus I can estimate the expected degradation in performance. I did several experiments using the data from 1990-2014 to predict 2015. Using Data from 1991-2015 to predict 2016. This helps me a lot in understanding how much my model is invariant from year to year. Instead of scanning all the data from production, you can randomly sample. The used distribution can be adaptive, such that the number of sampled records increases gradually with time. The reason why the distribution is adaptive because we are expecting the model performance to deviate from the expected performance with time.
