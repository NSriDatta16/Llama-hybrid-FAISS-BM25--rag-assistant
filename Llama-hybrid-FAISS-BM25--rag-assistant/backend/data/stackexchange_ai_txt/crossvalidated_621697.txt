[site]: crossvalidated
[post_id]: 621697
[parent_id]: 453183
[tags]: 
Disclaimer: This post is pretty verbose; sorry about this in advance, but I thought that it would be instructional for myself and others. I realize that I am late to this by just over 3 years; however, I thought that the following would be useful for future readers. We require to reason why the formula $s_{\ell_{i+1}} = s_{\ell_{i}} + (kernelsize - 1) * dilationfactor$ that user "samirzach" mentioned is correct. For brevity, we will let $K = kernelsize$ . So $K = 3$ , for example, denotes a $3 \times 3$ convolution filter. We will instead write this as $s_{\ell+ 1} = s_{\ell} + (K-1)\cdot 2^{\ell}$ , where we have gotten rid of the subscript $i$ and thus $s_{\ell + 1}$ means layer $\ell + 1$ and so on; this is just for convenience. Note that $s_0 = 1$ as a base case as we shall elaborate more on later. Moreover, we have explicitly substituted in the dilation factor $2^{\ell}$ for layer $\ell + 1$ . This just follows the definition of the exponentially-increasing dilation scheme that we are analyzing. We will prove this recurrence by strong induction. Unlike most proofs by induction, I will argue that the inductive step is actually constructive for one's intuition as to why the recurrence actually holds (i.e. how it was derived). Base Case: $\ell = 0$ . This is trivially $s_{0} = 1$ as each element can be viewed as being a receptive field of $1 \times 1$ when no convolution fields are applied. Base Case: $\ell = 1$ . We will consider this base case for instructive purposes. In this case, we are just considering a basic $K \times K$ with no dilation (i.e. a dilation of $1$ reduces to a standard convolution). Thus, the receptive field of an element is simply $K \times K$ . Note, by our proposed recurrence, $s_1 = s_0 + (K-1) \cdot 2^{0} = 1 + (K-1) = K$ as required. Inductive Hypothesis: We will assume that our recurrence for $s_{\ell}$ holds for layers $i \in \{0, 1, \ldots, \ell\}$ where $\ell \geq 1$ by our base cases. Inductive Step: Consider layer $\ell + 1$ . We require to find the receptive field of an element after layer $\ell + 1$ has been applied. Note that, by definition, the convolution is a $K \times K$ filter applied to $K \cdot K$ activation outputs of layer $\ell$ that each have a receptive field of $s_{\ell}$ by definition (note that we have made no assumption as to what $s_{\ell}$ is). Let us now consider what size the square formed by the combined receptive field of these $K \times K$ layer $\ell$ activations will form. To make this easier to follow, I would recommend visualizing what a dilated filter looks like, i.e. (image from Pyramid-dilated deep convolutional neural network for crowd counting by Wang et al, 2021): Let us assume, for the first part, that the receptive field fully overlaps; i.e. the receptive fields of the $K \cdot K$ activations from layer $\ell$ fully overlap and form a new, combined receptive field square (with no isolated individual squares), which is the receptive field of the arbitrary element outputted by layer $\ell + 1$ that we are considering. That is to say, the receptive field of the arbitrary output element of layer $\ell + 1$ that we are considering is formed by taking the union of the $K \cdot K = K^{2}$ output elements of layer $\ell$ that each have a receptive field of $s_{\ell}$ respectively. The side length of this square will first include $K$ (which is the number of activations from layer $\ell$ , which each have a receptive field of $s_{\ell}$ respectively, in the side length of the chosen filter) as well as the "gaps" between the respective squares (recall that we assumed that the receptive fields overlap). The number of squares in these "gaps" is $(K-1) \cdot (2^{\ell} - 1)$ since we have $(K-1)$ "gaps" and the number of spaces between each pair of adjacent squares in the filter is $2^{\ell} - 1$ , since a dilation of $1$ is considered to have no gaps. So we know that the side-length is at least $K + (K-1)\cdot(2^{\ell} -1)$ . Is this the exact answer? No. Note that we forgot to add the "extra bit" to the receptive field that is contributed to by the outputs on the boundaries of the filter. These add $s_{\ell} - 1$ to the side length; each square on the opposite sides of the boundary adds $\frac{s_{\ell} - 1}{2}$ . So, we have that $s_{\ell + 1} = 2\cdot\frac{s_{\ell} - 1}{2} + K + (K-1)\cdot(2^{\ell} - 1) = s_{\ell} + (K-1) \cdot 2^{\ell}$ , which matches our recurrence relation! At this point, alarm bells should be ringing since we never used the inductive hypothesis. This is due to our prior assumption that the receptive fields actually overlap, which we have to obviously prove. To do so, we have to prove that $2^{\ell} - 1 \leq s_{\ell} - 1 \iff 2^{\ell} \leq s_{\ell}$ , where $s_{\ell} - 1$ comes from the fact that we know that each of the $2$ elements will contribute at least $\frac{s_{\ell} - 1}{2}$ from their receptive fields (i.e. this is the distance from the center of the receptive field to the right/left horizontal boundary) and $2^{\ell} - 1$ is simply the number of squares that we have to account for between the adjacent pair. By our inductive hypothesis and unrolling the recurrence, we have that $$ s_{\ell} = 1 + (K-1)\sum_{i=0}^{\ell - 1}2^{i} = 1 + (K-1)(2^{\ell} -1) = (K-1)2^{\ell} + (2-K) $$ Thus, $s_{\ell} \geq 2^{\ell} \iff (K-1)2^{\ell} + (2-K) \geq 2^{\ell} \iff (K-2)2^{\ell} \geq (K-2)$ , which is trivially true as $2^{\ell} \geq 1$ , $\forall \ell \geq 0$ . Note that we have therefore proven the corresponding recurrence by induction. Unrolling the recurrence that we have proven, we get that for layer $\ell$ with a filter of $K \times K$ , the effective receptive field of the element is $1 + (K-1)(2^{\ell} -1)$ , which is clearly exponential in $\ell$ , even though the number of parameters grows linearly with $\ell$ ! Hence, an activation within layer $\ell$ is actually a function of an exponential number of inputs!
