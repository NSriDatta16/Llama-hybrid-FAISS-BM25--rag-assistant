[site]: crossvalidated
[post_id]: 332338
[parent_id]: 332334
[tags]: 
MCMC is not needed to find the posterior mode. I think the approach you propose is correct in principle, but unfeasible for all but the simplest tasks. What you suggest is a particularly naive optimization scheme for $\theta$, the problem is that a) If the data (likelihood) provides non-negligible information, the posterior density will actually be very close to zero on most of the parameter values admitted by the prior. It is hard to guess the range where posterior has non-negligible density. b) The posterior mode can be arbitrarily narrow, requiring you to run a lot of samples to have a chance of hitting close to it. Of course this all gets exponentially worse as the dimension of $\theta$ grows. Basically any classical optimization method (simulated annealing, gradient descent, ...) is likely to outperform your approach. In fact the Stan probabilistic language has a very fast optimization mode (with L-BFGS). Note however that the posterior mode is a problematic quantity as it does change under reparametrization - unlike maximum likelihood or full Bayes. You correctly state that MCMC is useful to evaluate the full posterior density (and expectations over it). In fact it is the only known practical way to get full posterior density in non-trivial models. The following question also has some related thoughts: Bayesian posterior: is multiplying likelihood by prior (rather than simulation) an acceptable approach?
