[site]: datascience
[post_id]: 60385
[parent_id]: 60370
[tags]: 
ELMO cannot be used to predict the next word. The reason is that ELMO is like two language models combined: one normal language model that predicts the next word based on the previous ones and another language model in the reverse direction. The hidden representations obtained from both directions are combined to generate the word predictions. This way, ELMo cannot be used as is for next word prediction because it is designed to receive the whole sentence and is not trained in a causal language model loss. In order to select a model for next word prediction, you should focus in normal (causal) language models, not bidirectional LMs (e.g. ELMo ), masked LMs (e.g. BERT , XLM ) or permutation LMs (e.g. XLNet ). Some examples of popular causal language models are OpenAI's GPT and GPT-2 , or Transformer-XL . I would recommend taking a look at Hugging Face's PyTorch transformers github repo , which makes it super easy to load pretrained weights for renowned LMs
