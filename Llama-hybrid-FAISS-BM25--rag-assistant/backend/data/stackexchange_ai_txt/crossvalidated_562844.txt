[site]: crossvalidated
[post_id]: 562844
[parent_id]: 
[tags]: 
product of log-normal variables

I'm trying to figure out some "paradox" that I don't understand. I want to model the evolution of some quantity (eg price), my assumption is that from one time step to the next, I will multiply this quantity by some random variable that is from a log-normal distribution, with mean 1. However after several time steps, I always get something very close to 0. What I am missing ? I'm thinking the expected value at time n should be the initial expected value. So I would expect that the value at time n is reasonably close to the initial value (or same magnitude) Here's a python snippet to explain: variance = 0.5 mu = - variance / 2 data = [math.exp(np.random.normal(mu, math.sqrt(variance))) for _ in range(100)] np.mean(data), np.product(data) (1.0281333043113792, 1.6729502153887247e-08) On lots of retry I always get something similar : the average is 1 (as expected), but the product becomes very small. What am I missing here ? Alternatively how could I simply model the evolution of this quantity if I want it to change but keep the same expectation across time ?
