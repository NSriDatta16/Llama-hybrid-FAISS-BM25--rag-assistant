[site]: crossvalidated
[post_id]: 491150
[parent_id]: 481156
[tags]: 
In fact, the distribution that is equivalent to the softmax function is the Boltzman distribution , which is in the list of common distributions you linked to. (The Wikipedia entry focusses on its original use in statistical mechanics and you would obviously ignore the $kT$ denominator for machine learning uses, though it's worth noting that softmax is not invariant under scale ). I can't answer why the Dirichlet PDF (as a function) is not as popular in NNs, though I suspect that this answer comparing softmax to normalisation is the reason. It suggests that softmax has the nice property of effectively cancelling out the log in cross entropy loss, leading to faster learning when the loss is larger.
