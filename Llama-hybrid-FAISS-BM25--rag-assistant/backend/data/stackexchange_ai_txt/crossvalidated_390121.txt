[site]: crossvalidated
[post_id]: 390121
[parent_id]: 
[tags]: 
ANN webpage classification based on visual segmentation: sample size and parameterization

I am trying to classify web-pages based on their visual layout. My application visits a categorized webpage, catches certain elements' bounding boxes (rect in web-element term) and label them. Label are derived from html element's properties. I call this processes segmentation of the web-page. The resulting data to be used in training is a numpy array of (2048, 1024) (because the page captured at 1024x2048 dimensions) where some parts of the array is filled as different numbers as results of labeling web elements. The model could be visualized as such: I build a keras Neural Network model with a hidden layer of 50 nodes. The input data to this model didn't fit into my GPU memory and i decided to shrink the model. Before that i tried decreasing hidden-layer's node numbers or reducing batch-size but these attempts game me poorer results. So I took following steps for shrinking my model Iterate a window over the array by given dimensions (say 4x4). Reduce the array to the most dominant value. Ex: [1, 1] [1, 2] becomes [1] [3, 3] [1, 2] becomes [3] For feature scaling (box label values) i used min-max scaling and used SGD as optimizer. My questions are: What is good sample number for training such a model? As far as i understand with ML, larger samples sizes makes it performs better. In this app sample size is between 4k-5k, is it good enough? Is there a rule of thumb to determine the good to go sample size based on model or ML algorithm? Does down-sampling looks right? would you recommend something else? Based on this type of input, is there a parameter which has very significant role? for example, activation function, batch-size, epoch size, adding another hidden layer, increasing node number of hidden later or using another optimizer?
