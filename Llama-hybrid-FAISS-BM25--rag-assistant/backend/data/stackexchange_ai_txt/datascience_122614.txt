[site]: datascience
[post_id]: 122614
[parent_id]: 
[tags]: 
Seq2seq Transformer Autoencoder returns same results when unfolded using only memory and initial value

I have numeric signals from two sensors, and I would like to create a mapping using sequence-to-sequence autoencoder. I used the transformer architecture, and it seems to be learning - the loss is getting lower both for the training and validation sets over time, and when using the decoder on both inputs - memory (that is, $encoder(source)$ ) and $target$ , the decoder would return a value, very similar to $target$ (and also low $MSE$ values). Nevertheless, in the inference stage, results look very much regardless of the input I add. My current approach for inference, when we don't see $target$ , is: Encode $source$ to get $memory$ Feed the decoder $momory$ (which is supposedly encodes the entire $source$ and projects it into some latent space). $sos$ - start of sequence token. Even though I work with numeric signals, I borrowed $sos$ from the NLP domain to tell my decoder it is time to start decoding. Do 2 iteratively, take the last predicted value, append it to my $result$ sequence, and stop after $T$ times. Both my $source$ signals and $target$ signals are normalized to $[-1,1]$ . Am I doing the inference wrong?
