[site]: crossvalidated
[post_id]: 406501
[parent_id]: 
[tags]: 
Logistic regression produces well calibrated models. Is that true for neural nets trained in batches?

This is an earlier discussion about LR producing well calibrated models: Some people equate neural net based prediction models (even deep NN or deep+sparse NN) to be equivalent to logistic regression. We train them with adagrad (or some other methods) but always update weights by optimizing cost function on limited batch. Q1: Is it true that Neural Nets have properties of logistic regression? Q2: When we train with batches, weights that were calculated in the first batch and produce 'well calibrated' output, probably will be updated quite differently when processing subsequent batch. Intuitively, I think after a few updates this 'well-calibrated' property wouldn't hold (and especially for sparse neural nets where some embedding table is used). Is this correct?
