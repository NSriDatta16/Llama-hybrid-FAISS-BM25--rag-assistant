[site]: crossvalidated
[post_id]: 597295
[parent_id]: 597002
[tags]: 
Your question is, basically, all of the field of item response theory. A set of questionnaire items along with the algorithm to combine them is called a "scale". There are many popular scale examples, like the CES-D of depression or the 3MSE of cognition. Before a "scale" is usable for research purposes, people are often interested to know whether the scale is validated. This form of validation is separate from the forms of model validation often discussed by machine learning and data science folks. Unfortunately, the same data shouldn't be used to both validate a scale and produce meaningful summaries - not that you won't find examples of this being done in the literature, but the weight of evidence should be considered "shifted" by a keen reviewer. Whether a set of questionnaire items can develop a powerfully predictive scale, and whether the sum of those ordinal scales is an adequately powerful predictor in its own right are separate questions. Cumulative link models are a nice conceptual starting place because they open the door to consider latent variable modeling - where the nuisance parameters, that is the "Intercepts", allow an unknown but positive score to separately predict the probability of achieving the next response level. That said, it's the completely wrong place to look, and you often find the most problematic aspect of the design are the particular questions themselves and how they should be scored. Consider that surveys sometimes mix questions with "reverse anchoring": i.e. q1: My doctor cares about my feelings (1 worst - 10 best) q2: my doctor is late (1 none of the time - 10 all of the time). The Rasch model (particularly the polytomous Rasch - which is perhaps inappropriately named as they are referring to ordinal outcomes) are a super flexible way of analyzing "Likert" style responses. In practice, I find there's rarely much difference between the results there and, say, a continuous analysis, or dichotomizing a Likert variable using a "top box" response (i.e. 4 or 5 out of a 1-5 scale). I don't believe in spurious dichotomization, but seeing a billion Likert answers, people usually do not give intermediate responses, or, if they do, most analysts agree to treat a "3" as a negative result.
