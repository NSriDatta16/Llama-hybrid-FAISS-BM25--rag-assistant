[site]: crossvalidated
[post_id]: 267686
[parent_id]: 205902
[tags]: 
Sometimes modelers separate variable selection into a distinct step in model development. For instance, they would first perform exploratory analysis, research the academic literature and industry practices then come up with a list of candidate variables. They'd call this step variable selection . Next, they'd run a bunch of different specifications with many different variable combinations such as OLS model: $$y_i=\sum_{j_m} X_{ij_m}\beta_{j_m}+\varepsilon_i,$$ where $j_m$ denotes variable $j$ in a model $m$. They'd pick the best model out of all models $m$ manually or in an automated routines. So, these people would call the latter stage model selection . This is similar to how in machine learning people talk about feature engineering , when they come up with variables. You plug the features into LASSO or similar frameworks where you build a model using these features (variables). In this context it makes a sense to separate out the variable selection into a distinct step, because you let the algorithm to pick the right coefficients for variables, and don't eliminate any variables. Your judgment (in regard to which variable goes into a model) is isolated in the variable selection step, then the rest is up to the fitting algorithm. In the context of the paper you cited, this is all irrelevant. The paper uses BIC or AIC to select between different model specifications. It doesn't matter whether you had the variable selection as a separate step in this case. All that matters is which variables are in any particular model specification $m$, then you look at their BIC/AIC to pick the best. They account for sample sizes and number of variables.
