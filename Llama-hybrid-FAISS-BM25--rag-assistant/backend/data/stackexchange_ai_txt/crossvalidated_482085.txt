[site]: crossvalidated
[post_id]: 482085
[parent_id]: 482053
[tags]: 
The entire (Bayesian and classical) analysis of a generalised linear model is conditional on the regressor vector $X=(x_1,\ldots,x_n)$ . The joint distribution of the $Y_i$ 's in the logit model $$p(y|x,\theta) = \prod_{i=1}^m[\frac{1}{1+e^{-x_i\theta}}]^{y_i}[1-\frac{1}{1+e^{-x_i\theta}}]^{1-y_i}$$ where $$y=(y_1,\ldots,y_n)\quad\text{and}\quad x=(x_1,\ldots,x_n)$$ is a valid joint pmf [on the components of $Y$ ] conditional on the vector $X=(x_1,\ldots,x_n)$ , assuming the $Y_i$ 's are independent given $X$ and that $$\mathbb P(Y_i=1|X=x,\theta)=\frac{1}{1+e^{-x_i\theta}}$$ As a joint distribution, it defines a likelihood function $$\ell(\theta|X,Y)=\prod_{i=1}^m[\frac{1}{1+e^{-x_i\theta}}]^{y_i}[1-\frac{1}{1+e^{-x_i\theta}}]^{1-y_i}$$ that can be used in a Bayesian analysis. As a function of $\theta$ , the likelihood is not a pdf , it does not integrate to one except in some specific cases (not including the logit model). The same applies when given a prior $\pi(\theta)$ one considers the product $\ell(\theta)\pi(\theta)$ : it does not integrate to one. The joint distribution of $\theta$ and $Y$ is $$p(y|\theta)\pi(\theta)$$ which integrates to one in $(y,\theta)$ and the conditional distribution of $\theta$ given $Y=y$ (and $X$ ) is $$\dfrac{p(y|\theta)\pi(\theta)}{\int_\Theta p(y|\eta)\pi(\eta)\,\text{d}\eta}$$ which integrates to one in $\theta$ . The marginal $$\int_\Theta p(y|\eta)\pi(\eta)\,\text{d}\eta$$ integrates to one in $y$ [except that it is a summation since $Y$ is discrete].
