[site]: datascience
[post_id]: 128489
[parent_id]: 128488
[tags]: 
Firstly, why do we use dropout in the first place? Dropout is a regularization technique designed to improve generalization and prevent overfitting. With this in mind, you should not necessarily expect dropout to improve your validation accuracy. If you see a large divergence in train/val accuracy (indicative of overfitting), you should probably add more dropout. However, that may have the effect of lowering train accuracy without lowering validation accuracy. There is not necessarily a relationship between applying dropout and improving validation performance. Secondly, consider the distinction between the location of dropout and the magnitude of dropout. Two scenarios: Apply 90% dropout only in the model head Apply 0.1% dropout after every layer The first is likely to be more aggressive than the second, as it applies more dropout overall despite being applied in fewer places. In terms of where to apply it, it depends on what model architecture you are going for. Wide resnet tends to go bn/relu/conv/bn/relu/dropout/conv/residual . A more simple cnn might go conv/dropout/bn/relu . There are many configurations. You can try add dropout wherever you see fit, at a low dropout percentage. See how that impact the model, train/val accuracy, and tune params.
