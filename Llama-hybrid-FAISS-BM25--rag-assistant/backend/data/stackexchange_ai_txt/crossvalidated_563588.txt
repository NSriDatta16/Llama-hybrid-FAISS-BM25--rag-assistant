[site]: crossvalidated
[post_id]: 563588
[parent_id]: 
[tags]: 
xgboost - difference between XGBClassifier.feature_importances_ and XGBClassifier.get_booster().get_fscore()

What is the difference between get_fscore() and feature_importances? Both are explained as feature importance but the importance values are different. # model_smote = XGBClassifier() # model_smote.fit(X_train_smote, y_train_smote) model_smote = GridSearchCV( estimator = XGBClassifier(), param_grid=parameters, scoring='roc_auc', cv=5, verbose=2, return_train_score=True ) model_smote.fit(X_train_smote, y_train_smote) xgb = model_smote.best_estimator_ feature_importances = pd.Series(xgb.get_booster().get_fscore()).sort_values(ascending=False) feature_importances.plot(kind='bar', title='Feature Importances', figsize=(10, 10)) get_fscore() class xgboost.Booster(params=None, cache=(), model_file=None) - get_fscore(fmap='') Get feature importance of each feature. feature_importances_ class xgboost.XGBClassifier - property feature_importances_: numpy.ndarray feature_importances = pd.DataFrame( data=xgb.feature_importances_, index=xgb.get_booster().feature_names, columns=['Importance'] ).sort_values('Importance', ascending=False) feature_importances.plot(kind='bar', title='Feature Importances', figsize=(10, 10))
