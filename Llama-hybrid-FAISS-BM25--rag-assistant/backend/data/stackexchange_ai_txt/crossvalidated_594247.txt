[site]: crossvalidated
[post_id]: 594247
[parent_id]: 113717
[tags]: 
Logistic regression is mostly likely used because of historic convenience, well studied convergence properties, relative data-frugality in comparison with other ML learners as well as being readily available pretty much everywhere. Also the resulting probabilities are usually "well-calibrated" out-of-the-box and this is helpful as it does not lead to under-/over-estimation of the probability to receive treatment as well as makes the occurrence of "extreme probabilities" (near 0 or 1) less likely. GBMs specifically, do not give very well-calibrated probabilities out of the box, I provided relevant material and commentary in this CV.SE tread on: Biased prediction (overestimation) for xgboost . Similarly simply using a tree would not be strongly advisable as it would lead to discontinuities and non-strictly monotonic probabilities that could mess up ordering because of the ties. Finally SVMs are a bit of red-herring as strictly speaking they do not provide probabilities natively but we need to use Platt scaling to get a similar output. This brings us to the last point: we can always post-process our output to make it better calibrated. The success of that step will be crucial but to avoid getting it "very wrong" logistic regression presents a safe bet. I recently read A tutorial on calibration measurements and calibration models for clinical prediction models by Huang et al. and I found it very informative if you want to explore that point further.
