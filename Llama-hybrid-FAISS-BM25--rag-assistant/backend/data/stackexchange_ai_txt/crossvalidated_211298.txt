[site]: crossvalidated
[post_id]: 211298
[parent_id]: 64825
[tags]: 
Just as an addendum to the answers here, I've got two links that really helped me understand why this isn't a good procedure: http://nbviewer.jupyter.org/github/cs109/content/blob/master/lec_10_cross_val.ipynb https://www.youtube.com/watch?v=S06JpVoNaA0 Edit: as requested, a brief explanation of the contents of the links: Suppose I'm training a classifier, and I have a dataset of 1000 samples, with 1 million features each. I cannot process them all, so I need less features (say, I can compute 300 features). I also have a held-out test set of 100 samples to accurately estimate my out-of-sample, real-world accuracy. If I filter my 1 million features down to 300, by selecting those features with a highest correlation to the targets of my whole dataset, I am making a mistake (because I'm introducing overfitting which cannot be detected by Cross Validation later on). My held-out set will show this by spitting back a bad accuracy value. According to the above links, the correct way to do it is to divide my dataset into a training set and Cross-Validation set, and then tune my model (filtering out features, etc) based on this training set and it's associated CV score. If I'm using K-folds, I must tune from scratch each time I make a split/fold, and then average the results. Programatically, you do the following: Keep aside a part of your dataset as a hold-out set. Split the remainder of your dataset (henceforth called T1) into K-folds. In a for-loop from i=1 to K, do the following: select the i'th fold as your CV set, and the remaining samples as your training set (henceforth called Ti). Do whatever feature engineering and feature selection you want: filter features, combine them etc. Convert both your CV set (the current fold, called CVi) and your current training set Ti to one with the appropriate features. Train your model on the training set Ti Get the score from the current fold, CVi. Append this score to a list holding all the scores. Now, your list has the score of each fold, so you average it, getting the K-folds score. It's really important that you perform the feature engineering inside the loop, on the sub-training set, Ti, rather than on the full training set, T1. The reason for this is that when you fit/feature engineer for Ti, you test on CVi, which is unseen for that model. Whereas, if you fit/feature engineer on T1, any CV you choose has to be a subset T1, and so you will be optimistically biased, i.e. you will overfit, because you're training and testing on the same data samples. A really good StackExchange answer is this one, which really explains it more in depth and with an example of the code. Also see this as an addendum.
