[site]: crossvalidated
[post_id]: 618517
[parent_id]: 618496
[tags]: 
Based on a bit of research, it seems like this is still not a settled science. The resnet implementation in Pytorch includes a batch normalization layer for every layer until the very last pooling and FC layer. This appears to be standard at least among most resnets I am familiar with - if not simply because designing the blocks in this way is easier rather than a bespoke definition for each block (and has fewer degrees of freedom to tune). Based on some other threads I was able to find , it appears that in some other applications (such as GANs), they found value in removing the BN from layers closest to the "raw image" which would correspond to the first layers as well as removing from the front and the end. This seems generally atypical in the implementations I have seen. My recommendation would be to try to experiment with some removal of the batch normalization if you feel that you have the time, but if not, go with the standard approach of BN up to the end.
