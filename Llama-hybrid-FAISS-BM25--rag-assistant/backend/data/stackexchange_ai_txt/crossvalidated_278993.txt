[site]: crossvalidated
[post_id]: 278993
[parent_id]: 278988
[tags]: 
Because a neural network is not perfect. First of all, it will always 'approach' the goal through the gradient. Second of all, your network will squash the values it receives with an activation function . If that is sigmoid function, it will never hit 1 (in your case, 100%). It always approaches 1. You also have to take the learning rate into account.
