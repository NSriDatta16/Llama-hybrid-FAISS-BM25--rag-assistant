[site]: crossvalidated
[post_id]: 80904
[parent_id]: 67618
[tags]: 
I would say it is well worth the time of a person with a personal stake to investigate data on the individual level. What can be learned by comparing averages is severely limited, as was noted by Poisson et al when "numerical" methods were first being applied to medicine nearly 200 years ago: In the field of statistics, that is to say in the various attempts at numerical assessment of facts, the first task is to lose sight of the individual seen in isolation, to consider him only as a fraction of the species. He must be stripped of his individuality so as to eliminate anything accidental that this individuality might introduce into the issue in hand. In applied medicine, on the contrary, the problem is always individual, facts to which a solution must be found only present themselves one by one; it is always the patient's individual personality that is in question, and in the end it is always a single man with all his idiosyncrasies that the physician must treat. For us, the masses are quite irrelevant to the issue. Calculations of probability, in general, show that, all other things being equal, the truth or the laws that are to be determined are all the better approached if the observations used embrace a large number of facts or individuals at once. These laws, then, by the very manner in which they are derived, no longer have any individual character; therefore it is impossible to apply them to the individual chances of a single man, without exposing oneself to numerous errors. Statistical research on conditions caused by calculi by Doctor Civiale. 1835. Int J Epidemiol. 2001 Dec;30(6):1246-9. Reprint of the classical report by Poisson, Dulong, Larrey, Double. The thing to do with individual data is to look for patterns and theorize as to what mechanisms could be responsible the patterns. Group averages often hide the individual patterns and can lead to bad inference regarding the underlying mechanism under many circumstances. Another issue with analyzing group level data is that individual differences are treated as noise, and if it is repeated measures then the intra-individual differences are also treated as noise. The idea that biological data consists of a deterministic signal + random noise is a rather unjustified assumption. It simplifies the analysis but also has caused many researchers to ignore studying the variability which is not necessarily "random". One field that studies variability is cardiovascular physiology: Heart Rate Variability Standards of Measurement, Physiological Interpretation, and Clinical Use Circulation. 1996; 93: 1043-1065 doi: 10.1161/â€‹01.CIR.93.5.1043 Another is the study of motor systems: Variability and Determinism in Motor Behavior Michael A. Riley and M. T. Turvey Journal of Motor Behavior, 2002, Vol. 34, No. 2, 99-125 So what you could do is plot the data for each parameter over time, and investigate the structure of the variability for patterns. Even just eyeballing it may work, you may also be able to find methods from the fields mentioned above. One of your goals is probably to compare treatments. The best "n=1" experiments are to measure baseline, give treatment, take away treatment, then give treatment again. If the treatment is doing what you expect you should expect the effect to appear, disappear, then appear again. This is not possible in your case, but perhaps there are similarities and differences of the putative mechanisms behind the treatments that could play a similar role.
