[site]: crossvalidated
[post_id]: 468914
[parent_id]: 
[tags]: 
Benefits of saturation in activation functions

It's known that saturation of activation functions in neural networks leads to vanishing gradients or dead units, so modern practice often avoids them, instead opting for e.g. ReLUs , Leaky ReLUs and other variants that don't saturate. Perhaps now in retrospect, it seems obvious that saturation and null gradients are undesirable, but it's surprising that it took so long for non-saturating activation functions to become widespread considering the long history of neural networks, and the fact that differentiation was fundamental to their training. What was a motivation behind their original use? Was saturation a desirable property at the time perhaps because people used to not "stack layers" as much? (in the last layer one often expects bounded quantities, e.g. softmax for getting probabilities). Or was / is there any other benefit to saturation?
