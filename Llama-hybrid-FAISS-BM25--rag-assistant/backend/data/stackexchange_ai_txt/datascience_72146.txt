[site]: datascience
[post_id]: 72146
[parent_id]: 72130
[tags]: 
Concerning the dimensionality reduction I encourage you to check non-linear dimensionality reduction if PCA does not give satisfying results. It can happen that some low dimensionality manifold hides behind a much higher number of features. The excellent sklearn's guide explains manifold learning in detail. Concerning the algorithm Almost any algorithm can manage 15 features with similar variance . It is always the case when you standardize your data before feeding the algorithm with it. You might want to train any model on a sample of the data and check the results. It can highly reduce the computation time without hurting too much the performance. (there is a saturation of performance with respect to data for a given complexity (features + model's parameters)). Feature Engineering importance When performance seems to ceil when changing models, you should consider improving your feature engineering. (products of features, polynomial features, ...)
