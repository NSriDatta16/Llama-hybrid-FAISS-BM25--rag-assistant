[site]: crossvalidated
[post_id]: 308494
[parent_id]: 308109
[tags]: 
I believe characters will reduce the size of the vectors. Well it depends what features you will use. For sure if you use one-hot encoding then there are only a couple dozen characters, but you won't usually see people using one-hot encoded words, since the dimensionality of such vectors is huge. What is commonly used for words is called word embeddings . Could someone tell me what are the tradeoffs using words vs characters. Apart from the fact, that you can use one-hot vectors for characters in practice, the other difference is that character-level models handle unknown words better - these language models can infer something about unseen words based on patterns in known words. For more approaches (for example hybrid approaches that use subword features) and for references see Goldberg's Primer on Neural Network Models for Natural Language Processing section 5.5.5.
