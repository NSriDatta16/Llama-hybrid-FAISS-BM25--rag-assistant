[site]: crossvalidated
[post_id]: 17323
[parent_id]: 17321
[tags]: 
You are asking for a typical "feature selection" analysis; Principal Component Analysis , or PCA , is perhaps the most common (and i think the most straightforward) technique for this type of anlysis. It is based on an eigenvector decomposition of the covariance matrix (of the original data). In the example below, i use Python and NumPy , the most widely used numerical computation library for Python. The syntax is very similar to Matlab's. # generate a data set (4 explanatory variables + 1 response variable) import numpy as NP X = NP.random.rand(75).reshape(15, 5) # calculate the correlation matrix of this data matrix, gives a 4 x 4 matrix C = NP.corrcoef(X[:,:-1], rowvar=0) # calculate the eigenvalues of the covariance matrix: from numpy import linalg as LA eva, evc = LA.eig(C) # the key step # sort the eigenvalue array in descending order: eva1 = NP.sort(eva)[::-1] # get value proportion of each eigenvalue: eva2 = NP.cumsum(eva1/NP.sum(eva1)) # print/display this intermediate result in table: NP.set_printoptions(precision=3, suppress=True) evas = NP.arange(1, 5) evas = evas.reshape(4,1) eva1 = eva1.reshape(4,1) eva2 = eva2.reshape(4,1) q = NP.hstack((evas, eva1, eva2)) q = NP.array(q, dtype=float) title1 = "ev value proportion" print(title1) print( "{0}".format("-"*len(title1)) ) for row in q : print("{0:1d} {1:2f} {2:2f}".format(int(row[0]), row[1], row[2])) ev value proportion ------------------- 1 1.596746 0.399186 2 1.379002 0.743937 3 0.843466 0.954803 4 0.180787 1.000000 In other words, according to Table immediately above: slightly less than 40% of the variability in the data is accounted for by the first variable, 74% , by the first two , and 95% of the variability is accounted for by the first three variables. When the data from the table above is plotted using a bars to show percent, the result is usually referred to as a scree plot . Finally, i would not use regression analysis for this problem. To begin with, it's not necessary, and second the result from applying those techniques tell you the importance of variables in your regression model , not in the data itself (which is their purpose).
