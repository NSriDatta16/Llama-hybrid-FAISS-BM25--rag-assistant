[site]: crossvalidated
[post_id]: 214027
[parent_id]: 213955
[tags]: 
It's hard to say without knowing more about the nature of your data. But, here are some possibilities: Transform your vectors to a fixed-length representation, then use any standard method that takes fixed-length vectors as input. You'd have to define the transformation. Use a method that acts on pairwise relations between objects. A simple example would be k nearest neighbors regression/classification, or any other distance-based approach. In this case, you'd define a distance function that takes two variable-length vectors as input and outputs a single value quantifying how distant they are. You could also consider kernel methods (e.g. support vector regression/classification). In this case, you'd define a kernel function that takes two variable-length vectors as input and outputs a single value measuring their 'similarity'. You'd then work with the kernel matrix $K$, where $K_{ij}$ measures the similarity between data points $i$ and $j$. The interpretation is that $K_{ij}$ gives the dot products between data points $i$ and $j$ after implicitly mapping them into a high dimensional feature space. Note that $K$ has to be positive semidefinite. Recurrent neural networks can deal with variable-length sequences. If your vectors represent a sequence (or could be interpreted as one), this could be a good option. The analogy to the bag-of-words approach for real data could be something like a histogram. In this approach, you'd calculate a histogram for each of your data points, using a common set of bins. Then, use the bin counts as the feature vector to pass to downstream learning algorithms.
