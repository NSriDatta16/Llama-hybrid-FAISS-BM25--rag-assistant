[site]: crossvalidated
[post_id]: 302964
[parent_id]: 302887
[tags]: 
What you propose is to select variables via LASSO first, then use the selected variables in a second unpenalized regression model to try to estimate their "actual effects." The comments from @Repmat would certainly apply if you were working directly with the LASSO-derived coefficients; coefficients from LASSO are deliberately biased. Your approach appears to be an attempt to remove that bias. Yes, there are ways to estimate standard errors in this type of approach, but they have to take into account the pre-selection and the discontinuities introduced by the variable-selection process. The 2014 paper by Efron that you cite provides one such approach. Efron's method, however, is more than the simple bootstrap that you propose. It repeats the model selection process over the set of bootstrap samples, while your proposal limited to your initial LASSO-based selection takes neither the pre-selection nor the associated discontinuities into account. Note that Efron introduced the bootstrap in a 1977 lecture ( The Annals of Statistics 7:1-26, 1979 ; doi: 10.1214/aos/1176344552), and it took him 35 more years to come up with a principled way to use it to estimate standard errors in such variable-selection schemes. There is no reason that you could not simply follow Efron's 2014 method, if you wish; the non-parametric approaches seem like they should work with underlying heteroskedasticity. (Parametric bootstraps typically require a well-specified model.) Related references involving OLS following LASSO can be found linked from this page . What I fear in such approaches is omitted-variable bias. In addition to its Wikipedia page , a search on this site today provides 307 results. If you omit important predictors from a model, the coefficients of the remaining coefficients may be biased from their "actual effects." LASSO tends to pick only a few of multiple correlated predictors, even if all of them have appreciable "actual effects." So the "actual effects" of the remaining variables in your regression-after-LASSO approach run the risk of having such omitted-variable bias. (Note that this omitted-variable bias is distinct from the coefficient bias inherently introduced by LASSO.) A further caution is the variability of the set of selected variables among multiple samples. Try repeating the entire LASSO modeling approach on multiple bootstrap samples from your data (say, 200 or more) and see how different the selected sets of variables can be. (This is related to the "discontinuity" issue noted above.) In that case, which predictors have the "actual effects"? This is also a useful way to estimate the quality of your modeling process : for example, use each of the models derived from the bootstrapped samples to predict the transfer prices in the original data set to estimate the standard errors of the predictions . With your interest in predictions, those estimates of prediction errors are likely to be much more useful than any estimates of the errors in the coefficients themselves. Note that the basic approach in the Efron 2014 paper has some similarity to my recommendation in the previous paragraph: the approach is to smooth out the discontinuities from variable selection by averaging results over a set of models each based on a separate bootstrap sample. These models in general will differ in the number or identity of the predictors used. Also, Efron explicitly notes in his parametric LASSO example that "estimation of the regression coefficients [is] a more difficult task" than estimating a single statistic like your transfer prices (page 19 of the linked PMC manuscript, page 1001 of the published JASA article: v109,p991).
