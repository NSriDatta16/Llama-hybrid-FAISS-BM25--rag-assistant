[site]: crossvalidated
[post_id]: 558874
[parent_id]: 
[tags]: 
About update procedure in data incremental learning

As far as I understood, the idea of data incremental learning consists of keeping the model always up to date. Suppose that we trained a model for user recognition using voice as input. Therefore, the input is the voice of users and the output is the label of users (user 1, 2, ...). After a certain time (say years for example), there could be changes in the input distribution of users, therefore we need to adapt our base model. The idea seems me to be the idea of stochastic gradient learning (in deep learning) where we only use one data point at a time to update model parameters. However, my question is, in order to update the model with new test data we have to have the label of tests? In real case scenarios, how this can be possible? Edit 1: An idea comes to my mind, maybe the solution is this (not sure at all)? One idea comes to my mind. Suppose that we have a deep model trained for user 1, 2 and 3. Then, there is a new input arrived. Our model predicts it (with the highest probability, consider softmax result of a deep network for example) as user 2. Therefore, in loss, while backpropagation, as true label for that new coming data (we have not that label in reality) we use the predicted label as ground truth. So, suppose that the softmax outputs: 0.1 for belonging in class of user 1 0.7 for belonging in class of user 2 0.2 for belonging in class of user 3 user 2 has the highest probability and we consider the true labels for that incoming data as: 0 for class of user 1 1 for class of user 2 0 for class of user 3 Therefore the cross entropy loss is calculated as follows: Loss = - (0*ln(0.1) + 1*ln(0.7) + 0*ln(0.2)) = -ln(0.7) = 0.357 Then we backpropagate this error throughout the network. I need verification, is this what is done for data incremental learning processes in real life?
