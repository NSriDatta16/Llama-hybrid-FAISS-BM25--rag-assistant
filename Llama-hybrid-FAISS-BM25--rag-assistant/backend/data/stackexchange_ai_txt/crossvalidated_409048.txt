[site]: crossvalidated
[post_id]: 409048
[parent_id]: 405712
[tags]: 
Yes there is a theory for it, it's called ensemble learning . The method of bagging (bootstrap aggregating) relies on it. This is used for example in Random Forests. The intuitive idea is that by averaging models that have a very low bias but a high variance, you can reduce that variance while still keeping the bias low. This is what happens with random forests where you usually use deep trees that can overfit (i.e. low bias-high variance), but averaging their prediction reduces this overfitting. This of course works best if the training sets of all the models are independent but in practice you use bagging. In DL models the diversity in the ensemble comes from different hyperparameters: they highlight here different initialization, dropout levels, BN or not. As for the second part of your question, I think Cowboy Trader answered it best. However, the ensembling also works with outputs that are not probabilities, like for example in the case of regression.
