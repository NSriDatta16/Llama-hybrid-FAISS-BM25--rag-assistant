[site]: crossvalidated
[post_id]: 589019
[parent_id]: 
[tags]: 
How to account for varying levels of “confidence” among data points during training

I am training a neural network (VGG16) on some image data for an image classification task. Each image has a “number of clicks” and a “number of views” feature. From these features, I’m defining a binary label as a threshold value on clicks/views. The issue is that not all images have been viewed the same number of times. For example, we could have two images, one with 10 clicks out of 100 views and another with 1000 clicks out of 10000 views. Both have the same clicks/views ratio but the latter has much lower variance. It seems this variance (or confidence in the success rate) is meaningful and should somehow be accounted for in the training process. Is there a standard way to incorporate this information?
