[site]: crossvalidated
[post_id]: 126951
[parent_id]: 126945
[tags]: 
what could be the reason for SVM to overfit to the data? First, the SVM may be overfitting because you are not regularizing it enough. Try decreasing the C parameter in the scikit-learn SVC constructor. (This parameter controls how much the classifier tries to prevent classification errors on the training set, as opposed to coming up with a simpler model.) Generally when training an SVM one will pick a number of different C parameters, evaluate the training error using e.g. 10-fold cross-validation, and pick the C that yields the lowest generalization error. In scikit-learn you can automate this using grid search . Another possible (related) problem with SVM performance is that SVMs are scale-sensitive (that is, if you multiply one of your features by a constant it will produce a different SVM). You may get better results from scaling and centering your data, if some of your features are typically much larger than others. As to your second part: What can I use to increase the precision and recall of rarer event more There's an intrinsic trade-off between precision and recall which basically corresponds to making a classifier more or less biased towards predicting that an instance is positive. (For instance, in the limit, you could predict every instance as being positive and have 100% recall, or predict no instances as being positive and have 100% precision.) So it's not surprising that you find it hard to raise both precision and recall at the same time--this really just corresponds to finding a better predictive model. Hopefully the two suggestions above will help you with that! EDIT: If you want to increase recall and don't mind sacrificing some precision (or vice-versa), then you can lower (or raise) the threshold at which the SVM (or RF) decides to classify things as positive. To do this you can look at the output of decision_function (for SVM) or predict_proba (for RF) and pick a different cutoff to compare them to (the defaults are 0 and 0.5 respectively, I believe, but you should check this). You can plot how precision and recall vary with your choice of threshold and pick one that looks good yourself, or use ROC analysis based on an estimate of the cost of misclassifying positive vs. negative instances (but that's a subject for a different answer).
