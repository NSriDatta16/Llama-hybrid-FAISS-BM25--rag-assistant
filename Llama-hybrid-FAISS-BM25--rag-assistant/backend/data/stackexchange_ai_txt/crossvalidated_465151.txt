[site]: crossvalidated
[post_id]: 465151
[parent_id]: 
[tags]: 
Sum of SVM hinge losses always a convex function?

The data loss function for a multi class SVM may take the following expression: \begin{equation} L=\frac{1}{N}\sum_{i}\sum_{j\neq y_i}\left[ \max(0,w_j^Tx_i-w_{y_i}^Tx_i+1)\right] \end{equation} where $N$ is the number of training examples, $w_j$ is the weight vector of the linear classifier for class $j$ , and $y_i$ is the actual label of training point $x_i$ . In an attempt to clarify its shape with a simple example, let's consider the data points are 1D (i.e. $x_i, w_j \in\mathbb{R}$ ) and there are only three labels to classify our data ( $N=3$ ). Then: \begin{align} L=&\frac{1}{3}\left[ \underbrace{\max(0,w_1^Tx_0-w_0^Tx_0+1) + \max(0,w_2^Tx_0-w_0^Tx_0+1)}_{\equiv L_0} + \\ +\underbrace{\max(0,w_0^Tx_1-w_1^Tx_1+1) + \max(0,w_2^Tx_1-w_1^Tx_1+1)}_{\equiv L_1} + \\ +\underbrace{\max(0,w_0^Tx_2-w_2^Tx_2+1) + \max(0,w_1^Tx_2-w_2^Tx_2+1)}_{\equiv L_2} \right]= \\ =& \frac{1}{3}(L_0+L_1+L_2) \end{align} When minimizing that, we'd be looking for example at the partial derivative w.r.t. $w_0$ . The loss as a function of only $w_0$ (i.e. keeping the other variables constant) is the sum of three linear functions (just zeroed at the hinges). It is claimed that, since the sign in front of $w_0$ is negative in $L_0$ , and positive in both $L_1$ and $L_2$ , the hinges are oriented towards the left and right respectively, and therefore the sum has a bowl-like shape (i.e. it's convex). It is also said that as we move on to deep neural networks the loss function loses this nice property and we have to deal with a non-convex one. My question is why the orientation of the hinges is determined in such a way, when in reality the slope of those linear functions is not given only by the explicit sign in front of the terms in $\boldsymbol{w_0}$ . Aren't the slopes the full coefficients of $w_0$ (i.e. $-x_0$ , $x_1$ and $x_2$ ) for the three functions respectively? What about the sign of the datapoint themselves, which can a priori be anything? If it happened the three slopes where negative, all hinges would be on the same side, and the sum wouldn't look like a convex function at all...
