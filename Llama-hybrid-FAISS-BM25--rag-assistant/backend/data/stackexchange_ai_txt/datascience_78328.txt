[site]: datascience
[post_id]: 78328
[parent_id]: 77651
[tags]: 
The framing of your problem is close to so-called language modeling task. Because your input data is fixed-length samples, you can use a seq2seq model with fixed-size context embedding. What this means is you would essentially have an encoder, Bi-LSTM for example which encodes your input into a fixed representation (by concatenating the final output states of forward and backward LSTM), and a decoder, for example LSTM, which sequentially produces the output tokens. Your objective function could be a mean of cross-entropy loss over each output token, or a more complex loss like CTC. You can also simplify it by just predicting the masked tokens, instead of the whole sentence, as output of your neural network. The fact that your tokens are integers makes no difference and actually simplifies the embedding. You can simply feed the data as is to an embedding layer in Keras or PyTorch. If you use PyTorch, there is this tutorial that I would recommend, using transformer instead of LSTM.
