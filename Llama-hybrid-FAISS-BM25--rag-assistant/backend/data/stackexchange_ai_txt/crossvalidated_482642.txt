[site]: crossvalidated
[post_id]: 482642
[parent_id]: 482079
[tags]: 
Multiple imputation plays reasonably nicely with Bayesian inference. You just fit the Bayesian model on each imputation (making sure there's not too few, e.g. do at least 100 imputations or so) and then put the posterior samples together (=you use the mixture of the posteriors as the overall posterior). However, doing a good multiple imputation requires a multiple imputation tool that is aware of the left-censoring (if you ignore that, MI would more likely impute values like the non-censored observations). Technically, I think it would be valid to do multiple imputation and only select the imputation, for which values are below the limit of detection, but you very quickly get to where none of 1000s of imputations fulfill the criterion. The substitution rule you mention apparently does not do too badly, if the censored quantity is the dependent variable in a model (see e.g. this paper for a list of references on the topic). How does it do for a covariate? No idea. I'd speculate it might be okay, if there's very few censored values. However, you have quite a few values that are censored. The other obvious approach mentioned by Tom Minka is joint modeling of the covariate and the outcome of interest. I tried to really spell this out in Stan for an example like yours with a bit of made up data. I suspect that as usual my Stan program is not as efficiently written as it could be, but at least I hope it is reasonably clear. library(rstan) stancode = " data { int N_obs; // Number of observation real y[N_obs]; // Observed y-values real x[N_obs]; // observed value or limit below which x is left-censored when x_censored=1 int x_censored[N_obs]; // 1=left-censored, 0=not censored, 2=right-censored real measurement_error[N_obs]; // measurement error we know for the covariates } parameters { real mu; // intercept for the regression model for y real sigma; // residual SD for the regression model for y real beta; // regression coefficient for x in the regression model for y real x_randomeff[N_obs]; // A random effect we use to capture the underlying true value // (obtained by multiplying by sigmax and adding mux - for more on the rationale for this parameterization look "non-centralized parameterization") real mux; // True population mean of the covariate values real sigmax; // True population SD of the covariate values } transformed parameters { real x_imputed[N_obs]; // Imputed values for x (or rather log(x)) for (r in 1:N_obs){ x_imputed[r] = mux + x_randomeff[r] * sigmax; } } model { // Specifying some wide weakly informative priors mu ~ normal(0, 100); sigma ~ normal(0, 100); beta ~ normal(0, 100); mux ~ normal(0, 10); sigmax ~ normal(0, 10); x_randomeff ~ normal(0,1); for (r in 1:N_obs){ // Dealing with the covariate model if (x_censored[r]==1){ target += normal_lcdf(x[r] | x_imputed[r], measurement_error[r]); } else if (x_censored[r]==2){ target += normal_lccdf(x[r] | x_imputed[r], measurement_error[r]); } else { x[r] ~ normal(x_imputed[r], measurement_error[r]); } // fitting the regression model for y y[r] ~ normal(mu + x_imputed[r]*beta, sigma); } } " sfit = stan(model_code = stancode, data=list(N_obs=12, y=c(44, 40, 37, 33, 31, 27, 24, 19, 16, 13, 9, 6), x=log( c(15, 7, 5, 3, 0.9, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5) ), x_censored = c(rep(0,5), rep(1, 7)), measurement_error=rep(0.1, 12)), control=list(adapt_delta=0.95)) summary(sfit)$summary As you can see the model even outputs what it imputed for the missing values. There's probably other ways of doing this, but this seemed reasonably intuitive to me. At the moment, I am using $log(x)\times \beta$ in the regression equation, but you could change that by exponentiating x_imputed[r] . Update : this paper just popped up in my Twitter feed.
