[site]: crossvalidated
[post_id]: 306729
[parent_id]: 306526
[tags]: 
Looks like your sample size is not a lot bigger than the dimensionality of the data (feature set size). That can be a problem for LDA and it can overfit. Since it relies on computing the within-class scatter matrix which requires the scenario of N >> p (# samples >> # features). One quick way to check if you are overfitting with LDA is to look at the projections. As a result of LDA you have C-1 projection vectors. I would try projecting data on those vectors one by one and visualize it. If LDA indeed overfitted - you will see that the classes separate almost perfectly and are clustered around separate points on the projected axis. (In case of p > N all the samples would get projected onto C different points with classes separated perfectly). This effect was termed "data piling" by J. S. Marron in his paper Distance Weighted Discrimination . For reference of how it might look like you can check the figures in that paper. So assuming that is what happening I would do one of the following: 1) Use a regularized version of LDA. The simplest idea is probably just adding some constant to the diagonal of within-class scatter matrix in order to increase the variance in all directions. But there are a lot of different way you can regularize LDA. 2) Use another method for dimensionality reduction that is adapted to your scenario of small sample size. Distance Weighted Discrimination (DWD) might be a good choice here. 3) Get more samples (always recommended) [1] Distance-Weighted Discrimination . J. S. Marron, Michael J. Todd and Jeongyoun Ahn. Journal of the American Statistical Association Vol. 102, No. 480 (Dec., 2007), pp. 1267-1271
