[site]: crossvalidated
[post_id]: 420219
[parent_id]: 
[tags]: 
When does a confidence interval "make sense" but the corresponding credible interval does not?

It is often the case that a confidence interval with 95% coverage is very similar to a credible interval that contains 95% of the posterior density. This happens when the prior is uniform or near uniform in the latter case. Thus a confidence interval can often be used to approximate a credible interval and vice versa. Importantly, we can conclude from this that the much maligned misinterpretation of a confidence interval as a credible interval has little to no practical importance for many simple use cases. There are a number of examples out there of cases where this does not happen, however they all seem to be cherrypicked by proponents of Bayesian stats in an attempt to prove there is something wrong with the frequentist approach. In these examples, we see the confidence interval contains impossible values, etc which is supposed to show that they are nonsense. I don't want to go back over those examples, or a philosophical discussion of Bayesian vs Frequentist. I am just looking for examples of the opposite. Are there any cases where the confidence and credible intervals are substantially different, and the interval provided by the confidence procedure is clearly superior? To clarify: This is about the situation when the credible interval is usually expected to coincide with the corresponding confidence interval, ie when using flat, uniform, etc priors. I am not interested in the case where someone chooses an arbitrarily bad prior. EDIT: In response to @JaeHyeok Shin's answer below, I must disagree that his example uses the correct likelihood. I used approximate bayesian computation to estimate the correct posterior distribution for theta below in R: ### Methods ### # Packages require(HDInterval) # Define the likelihood like k if(n %% n_print == 0){ print(c(n, sqrt(n)*abs(x_bar))) } } return(x) } # Plot results plot_res This is the 95% credible interval: > as.numeric(hdi(chain[, 1], credMass = 0.95)) [1] -1.400304 1.527371 EDIT #2: Here is an update after @JaeHyeok Shin's comments. I'm trying to keep it as simple as possible but the script got a bit more complicated. Main changes: Now using a tolerance of 0.001 for the mean (it was 1) Increased number of steps to 500k to account for smaller tolerance Decreased the sd of the proposal distribution to 1 to account for smaller tolerance (it was 10) Added the simple rnorm likelihood with n = 2k for comparison Added the sample size (n) as a summary statistic, set tolerance to 0.5*n_target Here is the code: ### Methods ### # Packages require(HDInterval) # Define the likelihood like k if(!rule){ rule = ifelse(n > n_max, TRUE, FALSE) } if(n %% n_print == 0){ print(c(n, sqrt(n)*abs(x_bar))) } } return(x) } # Define the likelihood 2 like2 $theta[1] = chain2$ theta[1] = rnorm(1, 0, 1) # Run ABC for(i in 2:nStep){ # Chain 1 theta1 = rnorm(1, chain[i - 1, 1], 1) prop = like(theta = theta1, n_max = n*(1 + tol $n)) m_prop = mean(prop) n_prop = length(prop) if(abs(m_prop - m) m && abs(n_prop - n) $mean) | is.na(chain2$ mean))) chain = chain[ -(1:nBurn), ] chain2 = chain2[-(1:nBurn), ] # Results plot_res(chain, chain2, nrow(chain), main = main) hdi1 = as.numeric(hdi(chain[, 1], credMass = 0.95)) hdi2 = as.numeric(hdi(chain2[, 1], credMass = 0.95)) 2*1.96/sqrt(2e3) diff(hdi1) diff(hdi2) The results, where hdi1 is my "likelihood" and hdi2 is the simple rnorm(n, theta, 1): > 2*1.96/sqrt(2e3) [1] 0.08765386 > diff(hdi1) [1] 1.087125 > diff(hdi2) [1] 0.07499163 So after lowering the tolerance sufficiently, and at the expense of many more MCMC steps, we can see the expected CrI width for the rnorm model.
