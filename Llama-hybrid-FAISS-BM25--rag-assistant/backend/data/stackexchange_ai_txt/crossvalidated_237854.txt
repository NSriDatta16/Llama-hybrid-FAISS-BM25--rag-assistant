[site]: crossvalidated
[post_id]: 237854
[parent_id]: 4284
[tags]: 
Here is a very simple explanation. Imagine you have a scatter plot of points {x_i,y_i} that were sampled from some distribution. You want to fit some model to it. You can choose a linear curve or a higher order polynomial curve or something else. Whatever you choose is going to be applied to predict new y values for a set of {x_i} points. Let's call these the validation set. Let's assume that you also know their true {y_i} values and we are using these just to test out model. The predicted values are going to be different from the real values. We can measure the properties of their differences. Let's just consider a single validation point. Call it x_v and choose some model. Let's make a set of predictions for that one validation point by using say 100 different random samples for training the model. So we are going to get 100 y values. The difference between the mean of those values and the true value is called the bias. The variance of the distribution is the variance. Depending on what model we use we can trade off between these two. Let's consider the two extremes. The lowest variance model is one where completely ignore the data. Let's say we simply predict 42 for every x. That model has zero variance across different training samples at every point. However it is clearly biased. The bias is simply 42-y_v. One the other extreme we can choose a model which overfits as much as possible. For example fit a 100 degree polynomial to 100 data points. Or alternatively, linearly interpolate between nearest neighbors. This has low bias. Why? Because for any random sample the neighboring points to x_v will fluctuate widely but they will interpolate higher just about as often as they will interpolate low. So on average across the samples, they will cancel out and the bias will therefore be very low unless the true curve has lots of high frequency variation. Hoever these overfit models have large variance across the random samples because they are not smoothing the data. The interpolation model just uses two data points to predict the intermediate one and these therefore create a lot of noise. Note that the bias is measured at a single point. It doesn't matter if it is positive or negative. It is still a bias at any given x. The biases averaged over all the x values will probably be small but that doesn't make it unbiased. One more example. Say you are trying to predict the temperature at set of locations in the US at some time. Let's assume you have 10,000 training points. Again, you can get a low variance model by doing something simple by just returning the average. But this will be biased low in the state of Florida and biased high in the state of Alaska. You'd be better if you used the average for each state. But even then, you will be biased high in the winter and low in the summer. So now you include the month in your model. But you're still going to be biased low in Death Valley and high on Mt Shasta. So now you go to the zip code level of granularity. But eventually if you keep doing this to reduce the bias, you run out of data points. Maybe for a given zip code and month, you have only one data point. Clearly this is going to create lots of variance. So you see having a more complicated model lowers the bias at the expense of variance. So you see there is a trade off. Models that are smoother have lower variance across training samples but don't capture the real shape of the curve as well. Models that are less smooth can better capture the curve but at the expense of being noisier. Somewhere in the middle is a Goldilocks model that makes an acceptable tradeoff between the two.
