[site]: crossvalidated
[post_id]: 350921
[parent_id]: 
[tags]: 
Variational autoencoder with Gaussian mixture model

A variational autoencoder (VAE) provides a way of learning the probability distribution $p(x,z)$ relating an input $x$ to its latent representation $z$ . In particular, the encoder $e$ maps an input $x$ to a distribution on $z$ . A typical encoder will output parameters $(\mu,\sigma)=e(x)$ , representing the Gaussian distribution $\mathcal{N}(\mu,\sigma)$ ; this distribution is used as our approximation for $p(z|x)$ . Has anyone considered a VAE where the output is a Gaussian mixture model, rather than a Gaussian? Is this useful? Are there tasks where this is significantly more effective than a simple Gaussian distribution? Or does it provide little benefit?
