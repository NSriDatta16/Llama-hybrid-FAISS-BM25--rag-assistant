[site]: crossvalidated
[post_id]: 80523
[parent_id]: 80516
[tags]: 
"Scaling a random variable" may be used in various situations for various reasons and in various ways, but related to the Central Limit Theorem, the intuition is straightforward. Consider the quantity $$\bar X_T-\mu$$ where $\bar X_T$ is the sample average (and as a function it depends on the sample size $T$), from an i.i.d sample, and also it is an estimator of the population mean $\mu$. Now $\bar X_T$ is a consistent estimator, meaning that as $T\rightarrow \infty$, $\bar X_T$ tends to $\mu$ in probability, or essentially, it stops being a random variable and "collapses to the constant $\mu$", and so $(\bar X_T-\mu)$ collapses to zero. Now, if it collapses to zero, it cannot have a distribution, right? "But this is good - uncertainty is eliminated"... no it is not good, because uncertainty is eliminated at an infeasible situation -actual infinite sample size. So we need something "intermediate", we need our estimator to maintain the identity of a non-constant random variable, so that we are able to "describe" the structure of unavoidable uncertainty through the properties of its distribution. So we need to do something to prevent $(\bar X_T-\mu)$ from collapsing to zero -but at the same time, this "something" must not be such as make $(\bar X_T-\mu)$ explode to infinity. It has been found that if we multiply the shrinking difference $(\bar X_T-\mu)$ by the growing factor $\sqrt T$ then the product $\sqrt T(\bar X_T-\mu)$ neither collapses to zero, nor it explodes to infinity as $T\rightarrow \infty$, but it remains a random variable that asymptotically has a Normal Distribution.
