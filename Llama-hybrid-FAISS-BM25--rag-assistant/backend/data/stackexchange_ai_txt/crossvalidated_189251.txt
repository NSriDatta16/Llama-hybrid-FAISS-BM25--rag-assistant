[site]: crossvalidated
[post_id]: 189251
[parent_id]: 174708
[tags]: 
Using average or sum are equivalent, in the sense that there exist pairs of learning rates for which they produce the same update. To confirm this, first recall the update rule: $$\Delta w_{ij} = -\alpha \frac{\partial E}{\partial w_{ij}}$$ Then, let $\mu_E$ be the average error for a dataset of size $n$ over an epoch. The sum of error is then $n\mu_E$, and because $n$ doesn't depend on $w$, this holds: $$\Delta w_{ij} = -\alpha \frac{\partial (n\mu)}{\partial w_{ij}}= -\alpha n\frac{\partial \mu}{\partial w_{ij}}$$ To your second question, the phrase "accumulating the delta weights" would imply that one of these methods retains weight updates. That isn't the case: Batch learning accumulates error . There's only one, single $\Delta w$ vector in a given epoch. (Your pseudocode code omits the step of updating the weights , after which one can discard $\Delta w$.)
