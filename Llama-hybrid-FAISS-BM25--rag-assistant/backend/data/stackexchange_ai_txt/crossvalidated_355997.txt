[site]: crossvalidated
[post_id]: 355997
[parent_id]: 
[tags]: 
When to stop training of neural network when validation loss is still decreasing but gap with training loss is increasing?

During training of CNNs, I often come across this case for training and validation loss : X axis is epochs, Y axis is cross entropy loss. I would like to keep the "best model", meaning the one which generalizes the best (I wish to use it for fine-tuning on another dataset). So up until now, I wass keeping the model at the "green line" point : the one with the lowest validation loss. However, I'm concerned because before that point, the gap between training loss and validation loss has increased, which indicates overfitting. So should I rather be keeping the model at the "red line", before overfitting gets too much out of hand ?
