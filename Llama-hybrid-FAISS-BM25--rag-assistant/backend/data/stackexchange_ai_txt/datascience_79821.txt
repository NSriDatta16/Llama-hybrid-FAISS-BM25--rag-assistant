[site]: datascience
[post_id]: 79821
[parent_id]: 79787
[tags]: 
In short A cell is in essence a forward neural network consisting of neurons and so $cell_{RNN}\neq neuron_{NN}$ . Rationale In the context of recurrent neural networks, a layer consists of cells e.g. LSTM cells. Although they seem to position similarly in terms of architecture (figure), there is a fundamental difference between an RNN_cell and a NN_neuron . The number of RNN cells is generally equal to the sequence length (e.g. number of words in a sentence) or embedding output dimension (figure below). Conversely, the number of neurons in forward neural networks can generally be of any number. Each RNN_cell acts much like a NN itself. Specifically in LSTMs, each cell consists of hidden state $W_{hidden}$ and cell state $W_{cell}$ matrices. These are used to store weights and perform transformations in the various gates (forget, input, output) of the LSTM cell. In essence, the transformations that take place in each cell are matrix multiplications of $W_{hidden}$ and $W_{cell}$ much like those in a forward NN. And so, the size of $W_{hidden}$ and $W_{cell}$ must be equal to the number of neurons of that NN which essentially determine the memory of the LSTM cells and model itself.
