[site]: datascience
[post_id]: 123247
[parent_id]: 
[tags]: 
Using random forest regression for time series forecasting - valid approach?

I'm working on predicting 7-day sales for a product using time series data. Though I primarily have experience in NLP and CV, I ventured into using ARIMA models for this task. However, the results didn't align well with my holdout set, as the ARIMA model was giving me a constant value for predictions on all 7 future days. I then explored transforming the time series into a regression problem, using lagged features (e.g., lagging sales by days or weeks). With a RandomForestRegressor, I achieved acceptable RMSE and respectable Mean Bias Deviation. This approach was reminiscent of one used in the Walmart sales forecasting Kaggle challenge, and I wonder if others have successfully applied it to their time series data, and how well or poorly it worked on their dataset? I've resampled my daily data to a weekly level and used TimeSeriesSplit from scikit-learn with n_splits=6. However, I haven't used max_train_size or test_size, and I'm concerned this might be causing data leakage, as my results seem overly optimistic. Here's an outline of my methodology: Lagged Features: Created 8 lags on a weekly level. Random Forest Model: Trained and evaluated using TimeSeriesSplit, with hyperparameter tuning for 'n_estimators', 'max_depth', 'min_samples_split', and 'min_samples_leaf'. Can anyone share insights or experiences on transforming time series into a regression problem, especially regarding the potential data leakage issue with TimeSeriesSplit? Your expertise would be much appreciated! Thank you! Code: # Function to create lagged features- here I create 8 lags on Week level def create_lagged_features(series, lags=8): lagged_data = pd.concat([series.shift(i) for i in range(lags, 0, -1)], axis=1) lagged_data.columns = [f"lag_{i}" for i in range(lags, 0, -1)] lagged_data['target'] = series.values return lagged_data.dropna() # Function to train and evaluate Random Forest model def train_random_forest(X, y): # Time series cross-validation tscv = TimeSeriesSplit(n_splits=6) # Random Forest model rf_model = RandomForestRegressor() # Hyperparameter tuning param_dist = { 'n_estimators': [50, 100, 150, 200], 'max_depth': [10, 20, 30, 40, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4] } search_cv = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, scoring='neg_root_mean_squared_error', cv=tscv, verbose=2, n_jobs=-1, n_iter=50) search_cv.fit(X, y) ```
