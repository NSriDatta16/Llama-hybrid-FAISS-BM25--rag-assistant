[site]: crossvalidated
[post_id]: 420495
[parent_id]: 
[tags]: 
Multivariate post-hoc tests and multiple comparisons

My research group is currently working on a medical imaging project in which we are studying brain perfusion in patients affected and unaffected by a particular type of injury using a technique called PWI-ASL (an MRI sequence). We have a machine learning technique for identifying regions in each subject's brain and calculating perfusion in those regions. We have two categorical independent variables (patient sex, disease state) with two levels each (male/female, affected/unaffected) and 62 brain regions (62 response variables). We want to compare perfusion in these regions between the four groups, with certain predetermined hypotheses about what will be higher, etc. The problem we are running into is with post-hoc testing. We are consistently finding significant differences between the sexes and affected/unaffected individuals on MANOVA/adonis, etc., but by the time we perform any type of post-hoc testing, all differences are absolutely crushed by adjustments for multiple comparisons because we have so many with the 62 brain regions. Someone in our group proposed using permutation testing (our data seem to fit its requirements) and performing permutation tests on each brain region individually, but should we not be adjusting the p values from each of the 62 comparisons? If so, we are basically left with data that will never produce anything significant because of the multiple comparisons adjustments. Is there another way to analyze these data, or are we out of luck? To summarize: 1. When performing permutation tests on multiple dependent variables, should the p values be adjusted for multiple comparisons? (I think so) 2. Does anyone have a recommendation for analyzing a large data set with multiple dependent variables that would not require such a massive multiple comparison adjustment?
