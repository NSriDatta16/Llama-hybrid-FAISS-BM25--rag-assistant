[site]: crossvalidated
[post_id]: 337201
[parent_id]: 337200
[tags]: 
In practice you shouldn't use decision trees, but random forests (decision trees are prone to overfitting), at least if you're interested in high classification accuracy. RFs are not the easiest methods to interpret, although there are some approaches to visualize feature importance . You could also try Gradient Boosting Trees for that and use the same method for them. What would be probably the easiest method to interpret is logistic regression - when using LASSO regularization with them it is possible to drive irrelevant components to zero, thus giving a model which predicts an outcome only based on some subset of features (you'd have to run your model with different $\lambda$ values though). EDIT: Oliver Angelil mentioned a very important aspect of comparing/interpreting different models, see his comment below.
