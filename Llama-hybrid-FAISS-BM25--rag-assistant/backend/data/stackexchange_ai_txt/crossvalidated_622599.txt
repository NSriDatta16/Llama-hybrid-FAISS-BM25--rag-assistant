[site]: crossvalidated
[post_id]: 622599
[parent_id]: 252936
[tags]: 
The MLE is invariant by transformation of the data $(X_1,...,X_n)$ by a strictly increasing transformation. Concretely, if $g$ is strictly increasing and $Y_i=g(X_i)$ and the $X_i$ 's have density $f_{\theta}$ , then the $Y_i$ 's have density $$ F_\theta: y\mapsto f_\theta(g^{-1}(y)) (g^{-1})'(y). $$ Minimizing the negative log-likelihood for $(X_1,...,X_n)$ gives $\hat \theta = \text{argmin}_{\theta} -\sum_{i=1}^n \log f_\theta(X_i)$ . Minimizing the negative log-likelihood for $(Y_1,...,Y_n)$ gives the same MLE because $$ -\sum_{i=1}^n \log F_\theta(Y_i) = -\sum_{i=1}^n \log f_\theta(X_i) + \log((g^{-1})'(Y_i) $$ so that the second term does not depend on $\theta$ : Minimizing $-\sum_{i=1}^n \log F_\theta(Y_i)$ is the same as minimizing $-\sum_{i=1}^n \log f_\theta(X_i)$ . So the MLE $\hat \theta$ is invariant under monotonic transformation. The same cannot be said for method of moments estimators, for which monotonic transformation can lead to terrible results. Consider $X_i\sim N(\theta,1)$ for an unknown parameter $\theta>0$ so that the MLE is the average $\hat\theta=\frac1n\sum_{i=1}^nX_i$ . The method of moments for $Y_1,...,Y_n$ where $Y_i=g(X_i)$ for $g(t)=t^2$ suggests to consider an estimator $\tilde\theta$ that solves the equation $$ \theta^2 + 1 = E_\theta[X^2] = \frac 1 n \sum_{i=1}^n X_i^2, $$ so that $\tilde\theta = (\frac 1 n\sum_{i=1}^n X_i^2 - 1)^{1/2}$ . By the law of large numbers, $\frac 1 n\sum_{i=1}^n X_i^2\to^P \theta^2+1$ so that $\tilde \theta \to^P \theta$ by the continuous mapping theorem. But the asymptotic variance of $\tilde\theta$ can be arbitrarily bad: we have $\frac{1}{\sqrt n}\sum_{i=1}^n(X_i^2 - 1)-\theta^2 \to^d N(0, 2)$ since $Var[\chi^2_1]=2$ (variance of $X_i$ which has the chi-square distribution with 1 degree of freedom). The delta-method applied the function $s(u)=\sqrt{u}$ function then gives $$ \frac{1}{\sqrt n}(\tilde \theta - \theta) \to^d N(0, 2s'(\theta^2)^2) = N(0, 1/(2\theta)). $$ The asymptotic variance can get arbitrarily bad as $\theta$ approaches 0, while the asymptotic variance of the MLE is always 1, i.e., $n^{-1/2}(\hat\theta-\theta)=^d N(0,1)$ . Summary The MLE enjoys optimal asymptotic variance thanks to the Cramer-Rao lower bound, and the MLE is invariant by monotonic transformation of $(X_1,...,X_n)$ . On the other hand, method of moments estimates can be rendered arbitrarily bad (in terms of asymptotic variance) by certain monotonic transformation of $(X_1,...,X_n)$ .
