[site]: datascience
[post_id]: 2595
[parent_id]: 2582
[tags]: 
Within each class, you'll have distributions of values for the features. That in itself is not a reason for concern. From a slightly theoretical point of view, you can ask yourself why you should scale your features and why you should scale them in exactly the chosen way. One reason may be that your particular training algorithm is known to converge faster (better) with values around 0 - 1 than with features which cover other orders of magnitude. In that case, you're probably fine. My guess is that your SVM is fine: you want to avoid too large numbers because of the inner product, but a max of 1.2 vs. a max of 1.0 won't make much of a difference. (OTOH, if you e.g. knew your algorithm to not accept negative values you'd obviously be in trouble. ) The practical question is whether your model performs well for cases that are slightly out of the range covered by training. This I believe can best and possibly only be answered by testing with such cases / inspecting test results for performance drop for cases outside the training domain. It is a valid concern and looking into this would be part of the validation of your model. Observing differences of the size you describe is IMHO a reason to have a pretty close look at model stability.
