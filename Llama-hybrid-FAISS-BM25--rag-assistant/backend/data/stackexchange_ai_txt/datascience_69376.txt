[site]: datascience
[post_id]: 69376
[parent_id]: 69349
[tags]: 
For the sentences you provided the nltk sentence tokeniser would work just fine. from nltk.tokenize import sent_tokenize sentences = ['Datascience exchange is a wonderful platform to get answers to datascience related queries and it helps to learn various concepts too','Can company1 buy company2? What will be their total turnover then?','Coronavirus was originated in china. After that it is spreading all over the world. To prevent it everyone has to take care of cleanliness and prefer vegetarians.'] for sent in sentences: print(sent_tokenize(sent)) out: ['Datascience exchange is a wonderful platform to get answers to datascience related queries and it helps to learn various concepts too'] ['Can company1 buy company2?', 'What will be their total turnover then?'] ['Coronavirus was originated in china.', 'After that it is spreading all over the world.', 'To prevent it everyone has to take care of cleanliness and prefer vegetarians.'] It is true that these rule based tokeniser do suffer from several issues (see: https://github.com/nltk/nltk/issues/494 ). If you are specifically working on a sequential model for sentence slitting then an LSTM with attention mechanism would surely be a good choice, you can find many repositories on GitHub which implement the code of the original paper 'attention is all you need' (among others: https://github.com/kaushalshetty/Positional-Encoding )
