[site]: crossvalidated
[post_id]: 379884
[parent_id]: 
[tags]: 
Why can't a single ReLU learn a ReLU?

As a follow-up to My neural network can't even learn Euclidean distance I simplified even more and tried to train a single ReLU (with random weight) to a single ReLU. This is the simplest network there is, and yet half the time it fails to converge. If the initial guess is in the same orientation as the target, it learns quickly and converges to the correct weight of 1: If the initial guess is "backwards", it gets stuck at a weight of zero and never goes through it to the region of lower loss: I don't understand why. Shouldn't gradient descent easily follow the loss curve to the global minima? Example code: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, ReLU from tensorflow import keras import numpy as np import matplotlib.pyplot as plt batch = 1000 def tests(): while True: test = np.random.randn(batch) # Generate ReLU test case X = test Y = test.copy() Y[Y Similar things happen if I add bias: 2D loss function is smooth and simple, but if the relu starts upside down, it circles around and gets stuck (red starting points), and doesn't follow the gradient down to the minimum (like it does for blue starting points): Similar things happen if I add output weight and bias, too. (It will flip left-to-right, or down-to-up, but not both.)
