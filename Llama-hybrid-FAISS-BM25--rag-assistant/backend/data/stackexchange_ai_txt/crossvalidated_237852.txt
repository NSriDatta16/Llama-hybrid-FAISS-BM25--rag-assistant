[site]: crossvalidated
[post_id]: 237852
[parent_id]: 208932
[tags]: 
Statistical reasoning provides an elegant solution. Because the integral of $f$ is used to define inverse trig functions, one is immediately tempted to interpret $X=\sin^2(A)$ for a random variable $A$ ranging from (say) $0$ to $\pi/2$. Substituting $\sin(a)$ for $x$ in $f$ gives $$f(x)\,\mathrm{d}x = f(\sin^2(a))\mathrm{d}\left(\sin^2(a)\right) = \frac{2\sin(a)\cos(a)\,\mathrm{d}a}{\pi\sqrt{\sin^2(a)(1-\sin^2(a))}}=\frac{2}{\pi}\mathrm{d}a.$$ This reveals $X$ as the squared sine of a uniformly distributed angle on $[0,\pi/2)$. Consequently $1-X=\cos^2(A)$ is its squared cosine. Recall (this is familiar from the study of the Normal distribution and related distributions of statistical importance) that an Exponential variable $Y$ has the same distribution as half the sum of squares of two independent standard Normal variables $Z_1$ and $Z_2$. In the plane, the ordered pair $\mathbf{Z}=(Z_1,Z_2)$ has a standard bivariate Normal distribution, showing that $Y$ is half the squared length of $\mathbf{Z}$, $$Y=1/2\,|\mathbf{Z}|^2.$$ Consequently $$U=XY = 1/2\,\sin^2(A)|\mathbf{Z}|^2 = 1/2\,\left(\sin(a)|\mathbf{Z}|\right)^2$$ and $$V=(1-X)Y = 1/2\,\cos^2(A)|\mathbf{Z}|^2 = 1/2\,\left(\cos(a)|\mathbf{Z}|\right)^2.$$ Those expressions that have been squared are the very components of $\mathbf{Z}$ itself: $$U = 1/2\,Z_1^2,\ V=1/2\,Z_2^2.$$ Apparently $U$ and $V$ are independent and their distributions are both--by definition--half a $\chi^2(1)$ distribution. It is now easy to write down their joint distribution any way you wish: as a PDF, CDF, characteristic function, moment-generating function, cumulant-generating function, etc. But it's probably most revealing to have expressed them in this familiar statistical form. A quick simulation supports these conclusions: by simulating $U$ and $V$ independently as proportional to $\chi^2(1)$ variates and solving $$Y=U+V;\ X=U/Y$$ we can see whether $X$ and $Y$ have the distributions originally assumed of them. A quick check--which could be formally verified with a goodness of fit test (like a chi-squared test)--is to overplot the histograms of the simulated $X$ and $Y$ with the density functions. They should match, up to a small amount of random variation in the areas of the histogram bars. They do. Here is the R code that made this figure. n
