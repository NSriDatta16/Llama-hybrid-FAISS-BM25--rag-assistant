[site]: crossvalidated
[post_id]: 242617
[parent_id]: 
[tags]: 
Comparing Perplexities With Different Data Set Sizes

I am currently doing research comparing language modelling in English to language modelling in programming languages (namely Java) using perplexity as the metric for the language model being used. My question is whether different data set sizes will invalidate the comparison of the perplexities. I am using the Penn Treebank as the English data set and it contains approximately 1,000,000 words. The other data sets I am using each contain about 200,000 to 500,000 words. All of the data sets have constant vocabulary size of 10,000. If it matters at all, I am using a recurrent neural network. Would comparing perplexities be invalidated by the different data set sizes? Thanks.
