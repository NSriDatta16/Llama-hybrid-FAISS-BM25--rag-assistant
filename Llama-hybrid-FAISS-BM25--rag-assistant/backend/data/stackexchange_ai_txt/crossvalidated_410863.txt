[site]: crossvalidated
[post_id]: 410863
[parent_id]: 
[tags]: 
What is wrong with tuning parameters on training set as opposed to validation set?

When creating a machine learning model it is suggested to split your data into train, validation, and test sets. Here is my understanding of what they are for. Train: Use this to train the different models that you want to use. Validation: Use this to tune each of these models, such as the regularization parameter Test: Use this to compare the final models and select which one is best based on some type of error analysis. An example in a machine learning course I am taking is Train the models of interest: $$\hat{y} = \beta_0 + \beta_1x_1 $$ $$\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 $$ $$\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3$$ Adjust $\beta$ 's for each model using parameter regularization with the validation data. Then test each of these models against the test data where you can then select the most optimal one. I understand why you do not want to use the test data when tuning the model using regularization because then you are essentially fitting to the test data set. However, I am wondering what can go wrong when tuning parameter on the training dataset instead of the validation data set? It is not immediately clear to me what issues can arise.
