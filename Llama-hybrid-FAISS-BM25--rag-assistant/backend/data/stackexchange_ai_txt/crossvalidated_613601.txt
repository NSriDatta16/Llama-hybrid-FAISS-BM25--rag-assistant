[site]: crossvalidated
[post_id]: 613601
[parent_id]: 613597
[tags]: 
In statistical learning theory, a quantity of great interest is the gap between empirical risk (expected training error) and population risk (expected test error) when train and test sets are sampled IID from the same population. In this context, overfitting refers to the size of the gap. No gap means there's no overfitting, large gap = method is overfitting. This is a property of the method rather than model, and refers to expected size of the gap averaged over all train/test pairs, rather than for a specific train/test dataset. However, you can use gap between specific train/test pair to infer the expected size of the gap. IE, if you see a large difference in train/test errors, then probably the average over all train/test pairs is also large. No overfitting + low train error also implies low test error.
