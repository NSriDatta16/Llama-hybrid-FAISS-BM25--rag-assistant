[site]: datascience
[post_id]: 10588
[parent_id]: 10479
[tags]: 
Sigmoid kernels owe their popularity to neural networks, which traditionally used the sigmoid activation function. Sigmoid kernels de-emphasize extreme correlation. In a way they behave a bit like correlation coefficients, which also has a limited range, emphasizing similarity in orientation. $c$ shifts the operating point on the sigmoid, affecting the relative emphasis of the angle between the inputs. Perhaps this visualization (for $c=0$) might help mentally visualize this: Your first reference states that sigmoid kernels behave like RBFs for certain parameters. This makes them suited to nonlinear classification. You probably know that the sigmoid kernel is only conditionally PSD, and thus sometimes does not correspond to the kernel function of any implicit feature map per Mercer's theorem . Someone this interested in kernel methods should have a copy of Learning with Kernels !
