[site]: crossvalidated
[post_id]: 543406
[parent_id]: 543401
[tags]: 
Yes, you can! One way of taking into account the risk is to use distributional reinforcement learning, where you learn the entire distribution of the rewards (in each state) rather than just keeping track of the expected rewards as is standard in reinforcement learning. Once you have access to the distribution of rewards for each arm, you can design whatever policy you desire that takes into account the risk factor. Consider this simple example. Say, you want the agent to avoid all options that can lead to negative rewards. If you have access to the distributions of values for each arm, you can map the value distributions to a (scalar) metric that is not the expected reward, but rather: $V(x_i) \rightarrow -\infty$ if $\int_{-\infty}^{0} p(V(x_i)) dV > \epsilon$ , else $V(x_i) = \int V(x_i) p(V(x_i)) dV = mean(V(x_i))$ , where $\epsilon$ is an arbitrarily small number. In words, for all options that have negative rewards, you artificially set the scalar value which you will use to choose an action to $-\infty$ while using the mean value for all other options that do not have negative rewards. There are two proposed methods of learning the value distribution rather than the expected values that I know of. One of them is from Dabney et al. 2020 , and the other one from Tano et al. 2020 . The code for both is linked in the respective papers, but for a quick breakdown: Deepmind's code uses a mathematically simple code to keep track of each expectile of the value distribution individually, but it is non-local, which can be viewed as slightly problematic for the implementing the updates (especially if using neural nets for the value function), as opposed to Tano's Laplace code, which is local.
