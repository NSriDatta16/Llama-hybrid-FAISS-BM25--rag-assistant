[site]: crossvalidated
[post_id]: 457374
[parent_id]: 
[tags]: 
How to implement Batch Norm to Deep learning Neural Networks?

I'm studying at coursea.com Neural Networks with deep learning course. I have a problem with implementing A Batch Norm to Mini-Batch Gradient descent. More accurately, in gamma and beta hyper-parameters. As Andrew Ng said, they are learnable params and they are modified in gradient descent as weights,bias,etc. My question is: how should I initialize the Gamma and Beta -> Randomly or there are some default values and in which shape they will be? and should I initialize them where I do it with weights or bias or in forward prop?
