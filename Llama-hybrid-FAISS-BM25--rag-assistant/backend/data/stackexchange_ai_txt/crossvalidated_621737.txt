[site]: crossvalidated
[post_id]: 621737
[parent_id]: 621726
[tags]: 
I would read the text provided on the slide as follows: Either they use some kind of probabilistic model. Such models are commonly used, there is nothing strange about it. Alternatively, they use fancy language to say that they are using squared error to fit the model. This would be equivalent to using maximum likelihood with a Gaussian distribution, so "you can think of" the results as of fitting Gaussians to the data. The description is however too vague to tell what it is exactly and definitely is not written by an expert in this area. From the description you simply cannot tell what is the model and if it makes sense or not. Answering the question from your title, yes, it is pretty common to interpret neural networks in probabilistic terms. For example, Goodfellow et al (2016) in the classic handbook on deep learning says Given features $h$ , a layer of linear output units produces a vector $\hat y = W^\top h + b$ . Linear output layers are often used to produce the mean of a conditional Gaussian distribution: $$ p(y | X) = \mathcal{N}(y; \hat y, I) \tag{6.17} $$ Maximizing the log-likelihood is then equivalent to minimizing the mean squared error. [...]
