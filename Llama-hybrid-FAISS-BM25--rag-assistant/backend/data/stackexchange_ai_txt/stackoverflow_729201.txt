[site]: stackoverflow
[post_id]: 729201
[parent_id]: 729167
[tags]: 
You can use a list of User-Agent strings that the common bots use. You can use some form of rate-detection and determine that a very high rate of requests will probably be a spider (or someone leeching your entire site). There might also be lists of IP adresses used by common bots, but a fool-proof detection system is most-likely impossible. You could create a link on your pages that a real visitor would never click and flag anyone that does follow the link as a spider. You will get some people clicking the link anyway but curiosity cannot be avoided.
