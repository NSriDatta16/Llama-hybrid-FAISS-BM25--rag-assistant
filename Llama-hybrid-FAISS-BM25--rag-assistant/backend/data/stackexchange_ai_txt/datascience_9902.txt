[site]: datascience
[post_id]: 9902
[parent_id]: 9886
[tags]: 
Assigning ~3 of 70 categories means you would be performing multi-label classification . In the end, it doesn't make much difference if you use Naive Bayes or SVM; they are both families of algorithms that translate provided independent variables (your feature space) into hopefully correct dependent variables (target classes). The question is how to construct a good feature space. The state of the art approaches in text mining are (or were) first tokenizing words, stripping punctuation and stop words , stemming or lemmatizing them, creating a bag-of-words model of those words' relative frequencies and perhaps the frequencies of those words' bigrams or trigrams . Then run your classification learners on that. Assume the resulting feature space table might get really wide (lots of words and combinations of words), so you might want to consider some form of dimensionality reduction . Of course, you will have to repeat the same filtering process with exact same parameters for each new survey you want to classify. Here's another good batch of answers on multi-label text classification .
