[site]: datascience
[post_id]: 117224
[parent_id]: 102851
[tags]: 
There are typically many discrete stages in a natural language processing (NLP) pipeline. Two possible stages are normalization and classification. Strings are normalized before being classified. Normalization (aka, standardization) makes string data more consistent. Mapping spelling variates to the same representation is an example. Mapping spelling variations to a consistent representation does not require machine learning. It can be with something like edit distance look-up in a dictionary. After normalization, classification is more straightforward. Classification models depend on the quality and quantity of features, aka feature engineering. Predicting a city most likely won't happen at the character-level. Modeling at the token-level would probably be more useful.
