[site]: crossvalidated
[post_id]: 309969
[parent_id]: 309919
[tags]: 
Regression spline example The first example uses splines, which involve a basis expansion of each covariate. There is generally a basis function in the spline basis that is confounded with the model intercept term. Unless some form of identifiability constraint is imposed on the spline, we could add a constant to the effect of the spline and subtract it from the model intercept and not change the fit of the model. There is then an infinite set of solutions to the model, which isn't helpful. To make progress some constraint is required. Typically that constraint takes the form of a summation to zero constraint; summed over the range of the covariate, the effect, or value, of the spline sums to zero. This has the undesirable effect of forcing the confidence interval to be zero at the point at which the spline cross zero on the y-axis. Note that I suspect in the figure, some transformation of the y-axis has been performed (say by adding the intercept) so that the zero-width point doesn't occur at y = 0. Clearly this interval is wrong in that it's coverage properties are not 95% around the narrow area. There are solutions to this problem. For the penalized likelihood generalised additive models in the mgcv package for R by Simon Wood and he and his collaborator's research, they show that you can achieve good coverage properties by putting the identifiability constraints on all the other splines except the spline of interest, and then compute the confidence interval. This solves the problem of interval shrinking to zero at y = 0, but otherwise makes little difference to the interval if the spline lies somewhat away from the null space of the spline penalty (i.e. in the default setting of a spline with second order derivative penalty, the null space is a linear function, hence if the spline is somewhat more wiggly than a linear fit). If the fitted spline is close to the penalty null space (i.e. it is close to being linear) the adjustment can widen the intervals markedly. Gaussian process example For the second example, I think this is an case where there is no noise or error in the simulated data. Hence the GP is setup to pass through the data points, and, as there is no noise/error the uncertainty about the function at the observed points is zero. It is very common in the GP world to introduce the basic concepts with known example data that have no error.
