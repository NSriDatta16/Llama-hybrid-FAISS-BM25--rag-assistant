[site]: datascience
[post_id]: 97399
[parent_id]: 97361
[tags]: 
First I think it's worth mentioning that in the context of an exploratory study with a small dataset, manual analysis is certainly as useful as applying NLP methods (if not more) since: Small size is an advantage for manual study and a disadvantage for automatic methods. There's no particular goal other than uncovering general patterns or insights, so it's unlikely that the results of an automatic unsupervised method would exhibit anything not directly observable. That being said one can always apply automatic methods indeed, if only for the sake of observing what they can capture or not. Observing frequency (point 1) can always be useful. You may consider variants with/without stop words and using document frequency (number of documents containing a term) instead of term frequency. points 3 and 5 are closely related: LDA essentially clusters the sentences by their similarity using conditional words probabilities as hidden variable. But the small size makes things difficult for any probabilistic method, and there could be many sentences which have little in common with any other. Syntactic analysis with dependency parsing can perfectly be applied to any sentence, but the question is what for? As far as I know this kind of advanced analysis is not used for exploratory study, it's used for specific applications where one needs to obtain a detailed representation of the full sentence. Traditionally this was used for higher-level tasks involving semantics, often together with semantic role labeling and/or relation extraction . I'm not even sure that this kind of symbolic representation is still in use now that end-to-end neural methods have become state of the art in most applications. I agree that summarizing a short sentence is pointless. You could try to summarize the whole set of sentences though, if that makes sense. In the logic of playing with any possible NLP method, you could add a few things to your list: Lemmatizing the words, this can actually be useful as preprocessing. Using embeddings or not: on the one hand this can help finding semantic similarities through the embedding space, on the other hand the small size makes it questionable to project the data in a high dimension space. Finding colocations (words which tend to appear together in the same sentence) with association measures such as Pointwise Mutual Information . Spelling correction and/or matching similar words with string similarity measures. It's unlikely that there's any interest in it but there are also stylometry methods, i.e. studying the style of the text instead of the content. These range from general style like detecting the level of formality or readability to trying to predict whether two texts were authored by the same person.
