[site]: crossvalidated
[post_id]: 311931
[parent_id]: 
[tags]: 
Semantic word embedding for relationships?

Observation [1] Word2vec is pretty good at comparing "subjects", e.g.: $$\langle dog, pup \rangle \simeq 0.81$$ $$\langle pants, trousers \rangle \simeq 0.75$$ But it appears to be not great at encoding "relationships" in a way that preserves semantics, e.g.: $$\langle spouse, wife \rangle \simeq 0.52$$ $$\langle employer, boss \rangle \simeq 0.27$$ $$\langle own, posess \rangle \simeq 0.26 $$ Questions Is there any obvious intuition for why this is the case? Is there any well-known way to encoding these relationship-type words in a way that the cosine distance would actually successfully measure semantic similarity? [1] I computed these examples with the webapp here
