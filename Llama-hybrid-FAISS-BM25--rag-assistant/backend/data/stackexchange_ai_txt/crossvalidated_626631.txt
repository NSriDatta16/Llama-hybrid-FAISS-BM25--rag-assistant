[site]: crossvalidated
[post_id]: 626631
[parent_id]: 81778
[tags]: 
The problem exists only if you don't use Bayesian inference. Bayesian posterior distributions completely cut through these confusing issues. As an example suppose that one is interested in quantifying evidence that a new drug lowers blood pressure. Let $\Delta$ be the parameter in the regression model associated with the difference in mean systolic blood pressure (SBP). A Bayesian regression model (which would also handle random effects without any approximations needed, by the way) gives rise to a posterior distribution for $\Delta$ . Suppose that the minimum clinically important difference in SBP is 7 mmHg (the difference one would really not want to miss; the basis for Bayesian or frequentist power calculations) and that the minimum detectable change is 3 mmHg, i.e., we consider that a change in mean SBP $\Pr(\Delta > 0)$ $\Pr(\Delta > 3)$ $\Pr(\Delta > 7)$ and likewise for all possible values of $\Delta$ . Bayes provides evidence in favor of any interval of $\Delta$ whereas frequentist inference mainly provides evidence against one particular $\Delta$ (zero). In interpreting the study I'd concentrate of the first and second posterior probabilities, which represent evidence for any SBP reduction and evidence for a non-trivial reduction. That's how Bayesian inference deals with practical significance.
