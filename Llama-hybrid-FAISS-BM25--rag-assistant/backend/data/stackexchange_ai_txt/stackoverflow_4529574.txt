[site]: stackoverflow
[post_id]: 4529574
[parent_id]: 
[tags]: 
Bypassing Windows Copy design

I have been trying to figure out a way to streamline copying files from one drive (network or external in my case) to my main system drives. Short of creating a program that I will have to activate each time then choosing all the info in it, I have no really good design to bypass it. I have been wondering if there was a way to intercept a straight-forward windows copy with a program to do it my way. The basic design would be that upon grabbing the copy (actually for this to be efficient, a group of different copies), the program would organize all the separate copies into a single stream of copies. I've been wanting to do this as recently I have been needing to make backups of a lot of data and move it a lot as all my drives seem to be failing the past few months. EDIT For the moment, let's assume I am copying from a fully defragmented external drive (because I was). Let's also assume I am copying to a 99% defragmented drive (because I was). If I attempt to move every file at once, then the Windows copy method works great as it only needs to stream a contiguous file from one drive to the other (likely keeping the copy contiguous). This is basically best case, I need everything from there to move to here. So the copy method gets to grab the files in some algorithmicly logical order (typically by filename) and read then write the data to the new location. Now for the reality of the situation, I instead copy a bunch of files from different locations (let's still assume fully defragmented as this is the case of my external), to different file locations on the second drive. I don't wish to wait for each one to finish before starting the next one, so I go through the folders picking and choosing what goes where and soon my screen is covered in COPY windows displaying the status of all the copies attempting to run at the same time. This presents a slight slowing down of the copy as a whole as each copy progresses independently of the others. That means that instead of a single contiguous file taking the total bandwidth available, all these files share the bandwidth approximately equally (this also adds in read and write delays to the disks as now the system has to grab a little from each file before circling back to the first). Still assuming best case scenario, this slower than if all the files were in one location and moved to one location. What I want to do is intercept all the separate downloads and organize them into a list of single downloads that would happen one after another.
