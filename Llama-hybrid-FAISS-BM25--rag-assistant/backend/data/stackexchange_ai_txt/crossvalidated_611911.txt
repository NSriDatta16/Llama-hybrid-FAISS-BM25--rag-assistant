[site]: crossvalidated
[post_id]: 611911
[parent_id]: 325942
[tags]: 
Much of the point of convolutional neural networks is to use clever weight-sharing and dropping of connections (setting weights equal to zero). My drawings from here may be useful. Apply the filter to the upper left 2x2 array. Apply the filter to the upper right 2x2 array. Apply the filter to the bottom left 2x2 array. Apply the filter to the bottom right 2x2 array. Here is the entire layer, with the 3x3 input image mapping to four neurons for the four positions in the image where convolution occurs. A fully-connected layer would have each feature/pixel on the left connected to each neuron on the right for a total of $36$ different connections, so $36$ colors instead of $4$ . For an image, this sharing of weights and dropping of connections has a reasonable interpretation as looking for local patterns in the image. If you just have a collection of features that lack the obvious relationship to each other like the pixels of an image do, you can shoehorn your features into a convolutional neural network by arranging the features in a rectangle and acting like that is an image, and your code will run, but the weight-sharing and connection-dropping lacks the clever interpretation from image data. This does not make it ridiculous to apply a convolutional neural network to such data, particularly if you can validate that the model achieves good performance, but the cleverness is lacking, and other approaches to lowering the parameter count might make more sense (e.g., dropout). Neural networks in general could make more sense, however. There is not necessarily a clever architecture for general features like images have convolutional neural networks, but various universal approximation theorems say that, speaking loosely, any decent function can be approximated arbitrarily closely by a neural network of sufficient size, so if you believe there to be a decent function that uses your features to explain the outcome ( $y$ ) of interest, a a neural network has a reasonable shot of approximating it. (What the universal approximation theorems do not say is how large of a network you need to approximate a function or how a large parameter count puts you at a high risk of overfitting to the noise that inevitably exists in your data.)
