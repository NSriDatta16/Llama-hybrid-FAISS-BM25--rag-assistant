[site]: crossvalidated
[post_id]: 514620
[parent_id]: 
[tags]: 
Selecting number of input variables in a PCA-LDA model

Let's say that you have a dataset with a huge number of variables, so that a linear discriminant analysis on the original data may not be a good idea. If you first use PCA a dimensionality reduction method, you can use a certain number of PCA axes as the input variables for your LDA model (I think it's called PCA-LDA). But then, is there any usual criterion to know (and justify) what would be the optimal number of principal components to keep in your model? As an example, I did that: Perform PCA on original data. For $p$ from 2 to [some reasonable number], use the $p$ first axes of the PCA as the input variables in an LDA model, and compute the classification error in k-fold cross-validation for each model size. (Side note: maybe I should rather recompute PCA at each iteration because it's considered to be part of the model?) I got the following results: Starting from here, it seems that maximal accuracy in cross-validation is reached for 20 principal components. But it also seems that retaining only 5 principal axes leads to a similar classification accuracy, with a very smaller model. In such a case, have we some "standard" measure to make a compromise between model size and model accuracy, that would allow to know how many components I should keep? (I mean, something like adjusted R-squared in regression, or Breiman's "1-SE rule" for CART algorithm, etc.) Thanks!
