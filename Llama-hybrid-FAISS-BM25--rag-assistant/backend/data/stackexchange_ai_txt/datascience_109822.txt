[site]: datascience
[post_id]: 109822
[parent_id]: 
[tags]: 
NLP Basic input doubt

I actually have a basic doubt in NLP, When we consider traditional models like Decision trees, The feature column order is important, Like first column is fixed with some particular attribute. So If, I have Tf-Idf Each word will have some fixed index and the model can learn. But in the case of LSTM, Sentences can be jumbled. For eg: "There is heavy rain", "Heavy rain is there" In the above 2 sentences, The word heavy occurs in different places. So in order for the model to understand that we have passed the word "There", We would require some unique representations for the word "there". Either a One-Hot or Word2vec. Is my understanding so far right? My final doubt is, If I use tfidf for the above, How will it work? How will the model understand that "heavy" word is passed? This doubt has been bugging me for long. Kindly clarify this! Thanks a ton!
