[site]: crossvalidated
[post_id]: 506440
[parent_id]: 506397
[tags]: 
The beauty of ridge regression (L2 norm / quadratic penalty / Bayesian model with normal priors) is that it doesn't waste any predictive information trying to determine which variables have exactly zero effect. Any attempt to select variables after the fact will result in worse predictions. The set of "selected" variables will have a zero probability of being the truly predictive variable. If you truly believe that some of the unknown coefficients are exactly zero (why?) you can formulate a better model by using Bayesian priors for regression coefficients that reflect your beliefs. For example if you knew that 1/5th of the unknown coefficients are exactly zero but didn't know which 1/5th, you could form a prior that is normal (as with ridge) but is mixed with a spike at zero such that $\Pr(\beta = 0) = \frac{1}{5}$ . A great advantage of the Bayesian approach is that you automatically get exact inference when you're done whereas traditional inference after traditional feature selection is very hard to do. Note that it is not valid to use simple methods after using shrinkage. You need to stay within the shrinkage model context.
