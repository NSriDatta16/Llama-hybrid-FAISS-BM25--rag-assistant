[site]: crossvalidated
[post_id]: 104478
[parent_id]: 104426
[tags]: 
MCMC works like this: You need to draw independent samples from a distribution, but it's computationally intractable to do so. It might not even be easy to draw dependent samples. Your next best approach is to cleverly construct up a Markov chain with the property that, if you run it for a very long time, you can expect it to take values that look like draws from the distribution you care about. That is MCMC. In some cases, you can make those draws approximately independent. So in this case the $\theta$ draws are computed by initializing the Markov chain, "burning it in" for several thousand iterations to start moving towards limiting behavior, then just watching it bounce around. $\theta$ is the value of the chain at each sample. The relationship between samples is the transition relationship in the Markov chain. So initial values don't matter. Three chains (as opposed to 5 or 8) is completely arbitrary. More is better, since chains are guaranteed to be independent from each other but not within themselves. There are post-hoc "effective sample size" calculations you can do but there isn't a good way I'm aware of to decide the number of chains and iterations ex ante. But many simpler MCMC problems are well-behaved enough that you don't need necessarily to run 10,000 iterations on 8 chains each.
