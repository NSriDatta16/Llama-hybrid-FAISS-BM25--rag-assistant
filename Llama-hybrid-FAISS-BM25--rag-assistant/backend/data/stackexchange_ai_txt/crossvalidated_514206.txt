[site]: crossvalidated
[post_id]: 514206
[parent_id]: 
[tags]: 
Slow feature analysis - why are zero mean and unit variance required

In their article "Slow feature analysis: Unsupervised learning of invariances" Wiskott and Sejnowski give the following learning problem on pg. 719: What I'm failing to understand here is that why exactly are zero mean and unit variance necessary? The text says that these constraints are in place to avoid trivial constant solution, but why would the trivial constant solution come about if the constraints were not fulfilled? There is also a short chapter about the method in the Goodfellow's "Deep Learning" book starting from pg. 490 which elaborates that: The constraint that the learned feature have zero mean is necessary to make the problem have a unique solution; otherwise we could add a constant to all feature values and obtain a diï¬€erent solution with equal value of the slowness objective. The constraint that the features have unit variance is necessary to prevent the pathological solution where all features collapse to 0. However, even after reading that, I still do not understand the justification why are zero mean and unit variance necessary. If we didn't have a zero mean, what about it would cause the problem not to have an unique solution and how exactly does having zero mean avoid that situation? Similarly, why does having unit variance avoid the situation of a constant solution? If we did not have unit variance, what would cause the constant output?
