[site]: crossvalidated
[post_id]: 314329
[parent_id]: 
[tags]: 
Can support vector machine be used in large data?

With the limited knowledge I have on SVM, it is good for a short and fat data matrix $X$ , (lots of features, and not too many instances), but not for big data. I understand one reason is the Kernel Matrix $K$ is a $n \times n$ matrix where, $n$ is number of instances in the data. If we have say, 100K data, the kernel matrix $K$ will have $10^{10}$ elements, and may take ~80G of memory. Is there any modification of SVM that can be used in large data? (Say on the scale of 100K to 1M data points?)
