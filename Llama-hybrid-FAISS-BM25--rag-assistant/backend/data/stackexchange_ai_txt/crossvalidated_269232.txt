[site]: crossvalidated
[post_id]: 269232
[parent_id]: 
[tags]: 
If I use dropout in a neural network and run it for a large number of steps, do I risk deleting all the units?

I'm still trying to understand dropout completely, but this is what think is happening so far: At each step there is a chance p of a unit being set to zero. If a rectified linear unit (ReLU) is used for activation, then a weight of zero can often result in 'dead' units. If I run a network for a long time (i.e. towards infinity), will all my units become stuck at zero? I think the answer is no, but I'm not sure why or what process is involved.
