[site]: stackoverflow
[post_id]: 5058577
[parent_id]: 
[tags]: 
Will I run into load problems with this application stack?

I am designing a file download network. The ultimate goal is to have an API that lets you directly upload a file to a storage server (no gateway or something). The file is then stored and referenced in a database. When the file is requsted a server that currently holds the file is selected from the database and a http redirect is done (or an API gives the currently valid direct URL). Background jobs take care of desired replication of the file for durability/scaling purposes. Background jobs also move files around to ensure even workload on the servers regarding disk and bandwidth usage. There is no Raid or something at any point. Every drive ist just hung into the server as JBOD. All the replication is at application level. If one server breaks down it is just marked as broken in the database and the background jobs take care of replication from healthy sources until the desired redundancy is reached again. The system also needs accurate stats for monitoring / balancing and maby later billing. So I thought about the following setup. The environment is a classic Ubuntu, Apache2, PHP, MySql LAMP stack. An url that hits the currently storage server is generated by the API (thats no problem far. Just a classic PHP website and MySQL Database) Now it gets interesting... The Storage server runs Apache2 and a PHP script catches the request. URL parameters (secure token hash) are validated. IP, Timestamp and filename are validated so the request is authorized. (No database connection required, just a PHP script that knows a secret token). The PHP script sets the file hader to use apache2 mod_xsendfile Apache delivers the file passed by mod_xsendfile and is configured to have the access log piped to another PHP script Apache runs mod_logio and an access log is in Combined I/O log format but additionally estended with the %D variable (The time taken to serve the request, in microseconds.) to calculate the transfer speed spot bottlenecks int he network and stuff. The piped access log then goes to a PHP script that parses the url (first folder is a "bucked" just as google storage or amazon s3 that is assigned one client. So the client is known) counts input/output traffic and increases database fields. For performance reasons i thought about having daily fields, and updating them like traffic = traffic+X and if no row has been updated create it. I have to mention that the server will be low budget servers with massive strage. The can have a close look at the intended setup in this thread on serverfault . The key data is that the systems will have Gigabit throughput (maxed out 24/7) and the fiel requests will be rather large (so no images or loads of small files that produce high load by lots of log lines and requests). Maby on average 500MB or something! The currently planned setup runs on a cheap consumer mainboard (asus), 2 GB DDR3 RAM and a AMD Athlon II X2 220, 2x 2.80GHz tray cpu. Of course download managers and range requests will be an issue, but I think the average size of an access will be around at least 50 megs or so. So my questions are: Do I have any sever bottleneck in this flow? Can you spot any problems? Am I right in assuming that mysql_affected_rows() can be directly read from the last request and does not do another request to the mysql server? Do you think the system with the specs given above can handle this? If not, how could I improve? I think the first bottleneck would be the CPU wouldnt it? What do you think about it? Do you have any suggestions for improvement? Maby something completely different? I thought about using Lighttpd and the mod_secdownload module. Unfortunately it cant check IP adress and I am not so flexible. It would have the advantage that the download validation would not need a php process to fire. But as it only runs short and doesnt read and output the data itself i think this is ok. Do you? I once did download using lighttpd on old throwaway pcs and the performance was awesome. I also thought about using nginx, but I have no experience with that. But What do you think ab out the piped logging to a script that directly updates the database? Should I rather write requests to a job queue and update them in the database in a 2nd process that can handle delays? Or not do it at all but parse the log files at night? My thought that i would like to have it as real time as possible and dont have accumulated data somehwere else than in the central database. I also don't want to keep track on jobs running on all the servers. This could be a mess to maintain. There should be a simple unit test that generates a secured link, downlads it and checks whether everything worked and the logging has taken place. Any further suggestions? I am happy for any input you may have! I am also planning to open soure all of this. I just think there needs to be an open source alternative to the expensive storage services as amazon s3 that is oriented on file downloads. I really searched a lot but didnt find anything like this out there that. Of course I would re use an existing solution. Preferrably open source. Do you know of anything like that?
