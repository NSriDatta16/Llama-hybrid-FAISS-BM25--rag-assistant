[site]: crossvalidated
[post_id]: 526024
[parent_id]: 501280
[tags]: 
Your question actually motivated me to writeup a recently published Stanford paper - link . Here's the writeup if you're curious. Note that you'll still see the performance hits because it uses NCV, but this method seems statistically robust. Finally, I think Cross Validated was missing info about computing the SE because the method didn't exist. Dependence between folds has been a long-standing issue with cross validation that, in my experience, is just ignored. "Roughly speaking, we expect the standard CV intervals to perform better when n/p is larger and when more regularization is used. In our experiments, we saw that even in the mundane linear model with n/p = 10 , the miscoverage rate of standard CV was about 50% larger than the nominal rate." - section 7 Paper Info (in case the link dies): Name: Cross-validation: what does it estimate and how well does it do it? Authors: Stephen Bates, Trevor Hastie, and Robert Tibshirani Year: 2021 Key conclusions: "We have made two main contributions. First, we discussed point estimates of prediction error via subsampling techniques. Our primary result is that common estimates of prediction error—cross-validation, bootstrap, data splitting, and covariance penalties—cannot be viewed as estimates of the prediction error of the final model fit on the whole data. ... Secondly, we discuss inference for cross-validation, deriving an estimator for the MSE of the CV point estimate, nested CV."
