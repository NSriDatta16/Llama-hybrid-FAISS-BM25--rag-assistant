[site]: crossvalidated
[post_id]: 531725
[parent_id]: 
[tags]: 
Formulation of Recurrent Neural Networks

In the book Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville , is said that the classical form of a dynamical system is: $$s^{t}=f(s^{t-1};\theta)$$ $s^{t}$ is state of system. Therefore, the next state $s^{t}$ is determined through a function $f$ that always uses the same parameters $\theta$ and information about the previous state $s^{t-1}$ . Then it is said: Consider a dynamical system driven by an external signal $x^{t}$ , $$s^{t}=f(s^{t-1}, x^{t};\theta)$$ where we see that the state now contains information about the whole past sequence. I don't understand this conceptual passage, what is meant by being driven by an external signal? $x^{t}$ shouldn't it be the value at time $t$ of the input sequence? Why is it said that the state now contains information about the whole past sequence? I want to understand in this sense how one comes to define the model of an RNN.
