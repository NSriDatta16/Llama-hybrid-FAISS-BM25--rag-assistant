[site]: crossvalidated
[post_id]: 135440
[parent_id]: 135406
[tags]: 
When doing analysis, one first always splits the data in two groups, a training set and a test set. You should not look at the test set ever, until the very end. Usually the split it around 70-80% training, and 20-30% test. Then, you want to find the best model using the data in the training set. To figure out which of the models (or model parameters) lead to the best results, you can split your training set again, now into a validation set, and a (smaller) training set. You then fit all models on the smaller training set, and try to predict instances in the validation set. This way, you can find the model that works best out of sample (on the validation set). However, people came up with another trick. Instead of splitting the training set into a validation set and training set once, we repeat this many times. This way, we hope to get a more accurate estimate of the model performance. This is called cross-validation (or CV). To keep things simple, I would advise you to skip this step. One more word of caution: I don't know what data you are using, but if the data is on one specific location for a period of time, you have to be very careful in splitting the data. In particular, you have to ensure that your train sample contains observations before the validation and test sets. Otherwise, you might leak information from the future, causing you to be too optimistic. As a simple example, suppose I want to predict the weather tomorrow, as that is in my validation set, I better only use information before tomorrow. If I know it rains today AND the day after tomorrow, I might be much more inclined to say it rains tomorrow as well, then when I only know it rains today. As long as your first observations are your training set, your second set of observations are your validation set, and your last observations are the test set, everything will be fine. However, doing proper CV in this case is difficult. Okay, now given that you split your data in three sets (train, validation and test), it's time to think about a model. For example, suppose you have data on just one location, and we want to predict pressure. We could look at the last $k$ observations of pressure and predict the average of those pressures. In general this is known as the k-nearest neighbor model. The question is, what value of $k$ should we pick? That is, should we look only at the previous pressure reading, or the last two, or three? To do this, we simply try all of them, and use our validation set to pick the best one. Once we have found the best value of $k$, we can fit the model on both the training and validation set, and then use that to predict the data in the test set. We can use this to estimate the accuracy of our final model. You might wonder, why do we use three test sets? Can we not use the validation score? Here is the problem: Suppose for simplicity we want to predict some value and to do this we use $m$ different models. Furthermore, suppose there is just 1 observation in the validation set. Now every model predicts a random set value, no matter what the predictor variables are. For example, the first model always predicts 0.32. Now one of these models will perform best on the validation observation, but not because it is better than the others, but simply by random chance is was lucky and close to the validation observation. We cannot expect this random model to always be this lucky, hence, the actual performance of this model is overestimated if we only use the validation set. While this was an extreme example, the same logical holds in general. Now let me briefly try to explain the difference between a parameter and an estimator: Many models are specified by a parameter. For example, we can consider the uniform distribution over the interval $[0, \theta]$. This means random variables from this distribution take on any value between $0$ and $\theta$ and each value is equally likely. Here $\theta$ is the parameter. A fixed (but unknown) quantity. An estimator tries to estimate the value of the parameter $theta$. For example, given the following data on the uniform model defined above, we might see 0.2, 0.42, 0.72, 0.34, 0.12, 0.62 Now we wonder, what value of $\theta$ is most likely? We could say: well we know $\theta$ must be at least 0.72, but it is probably not 3000. An estimator is thus a function of the data, and predicts what the parameter is. Hope this helps, let me know if things are unclear!
