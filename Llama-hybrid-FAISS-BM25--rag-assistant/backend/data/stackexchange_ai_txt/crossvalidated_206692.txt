[site]: crossvalidated
[post_id]: 206692
[parent_id]: 203103
[tags]: 
Here is a distribution-free option, since it seems that's what you're looking for anyway. It is not particular to the field of circular statistics, of which I am fairly ignorant, but it is applicable here and in many other settings. Let your directional variable be $X$. Let the other variable be $Y$, which can lie in $\mathbb R^d$ for any $d \ge 1$ (or, indeed, any type of object on which a useful kernel can be defined: graphs, strings, images, probability distributions, samples from probability distributions, ...). Define $Z := (X, Y)$, and suppose you have $m$ observations $z_i = (x_i, y_i)$. Now, conduct a test using the Hilbert Schmidt Independence Criterion (HSIC), as in the following paper: Gretton, Fukumizu, Teo, Song, Sch√∂lkopf, and Smola. A Kernel Statistical Test of Independence. NIPS 2008. ( pdf ) That is: Define a kernel $k$ for $X$. Here we mean a kernel in the sense of a kernel method , i.e. a kernel of an RKHS . One choice is to represent $X$ on the unit circle in $\mathbb R^2$ (as in Kelvin's edit), and use the Gaussian kernel $k(x, x') = \exp\left( - \frac{1}{2 \sigma^2} \lVert x - x' \rVert^2 \right)$. Here $\sigma$ defines the smoothness of your space; setting it to the median distance between points in $X$ is often good enough. Another option is to represent $X$ as an angle, say in $[-\pi, \pi]$, and use the von Mises kernel $k(x, x') = \exp\left( \kappa \cos(x - x') \right)$. Here $\kappa$ is a smoothness paramater. 1 Define a kernel $l$ for $Y$, similarly. For $Y$ in $\mathbb R^n$ the Gaussian kernel, above, is a reasonable default. Let $H$, $K$, and $L$ be $m \times m$ matrices such that $K_{ij} = k(x_i, x_j)$, $L_{ij} = l(y_i, y_j)$, and $H$ is the centering matrix $H = I - \frac1m 1 1^T$. Then the test statistic $\frac{1}{m^2} \mathrm{tr}\left( K H L H \right)$ has some nice properties when used as an independence test. Its null distribution can be approximated either by moment-matching to a gamma distribution (computationally efficient), or by bootstrapping (more accurate for small sample sizes). Matlab code for carrying this out with RBF kernels is available from the first author here . This approach is nice because it is general and tends to perform well. The main drawbacks are: $m^2$ computational complexity to compute the test statistic; this can be reduced with kernel approximations if it's a problem. The complicated null distribution. For large-ish $m$, the gamma approximation is good and not too onerous; for small $m$, bootstrapping is necessary. Kernel choice. As presented above, the kernels $k$ and $l$ must be selected heuristically. This paper gives a non-optimal criterion for selecting the kernel; this paper presents a good method for a large-data version of the test that unfortunately loses statistical power. Some work is ongoing right now for a near-optimal criterion in this setting, but unfortunately it's not ready for public consumption yet. 1. This is frequently used as a smoothing kernel for circular data, but I haven't in a quick search found anyone using it as an RKHS kernel. Nonetheless, it is positive-definite by Bochner's theorem , since the shift-invariant form $k(x - x')$ is proportional to the pdf of a von Mises distribution with mean 0, whose characteristic function is proportional to a uniform distribution on its support $[-\pi, \pi]$.
