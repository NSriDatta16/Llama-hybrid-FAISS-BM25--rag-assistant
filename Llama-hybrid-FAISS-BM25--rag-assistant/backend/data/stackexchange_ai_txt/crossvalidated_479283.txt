[site]: crossvalidated
[post_id]: 479283
[parent_id]: 
[tags]: 
The Bayes' Theorem Components of the Probability Output of a Classifier

Let's give a simple setup. I have $500$ photos of dogs and $500$ photos of cats, all labeled. From these, I want to build a classifier of photos. For each photo, the classifier outputs a probability of being a dog (which I deem to be class $1$ ). $$P(\text{dog }\vert\text{ photo})$$ We can reverse the conditioning with Bayes' theorem. $$P(\text{dog }\vert\text{ photo}) = \dfrac{P(\text{photo }\vert \text{ dog}) P(\text{dog})}{P(\text{photo})}$$ I can interpret the $ P(\text{dog})$ as the prior probability of a photo being of a dog. Since the classes are balanced, I would call this $ P(\text{dog}) = 0.5$ . Then the probability output of the classifier, $P(\text{dog }\vert\text{ photo})$ is the posterior probability of the photo being of a dog. What are the interpretations of $P(\text{photo }\vert \text{ dog})$ and $P(\text{photo})?$ Each individually seems like it could be zero, so perhaps the better interpretation would be the ratio $\dfrac{P(\text{photo }\vert \text{ dog})}{P(\text{photo})}$ . In that case, what is the interpretation of the ratio? Either the ratio, or the numerator or denominator on its own, must have something to do with the particular model (e.g. convolutional neural network vs logistic regression), right?
