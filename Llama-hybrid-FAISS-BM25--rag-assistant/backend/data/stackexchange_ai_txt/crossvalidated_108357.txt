[site]: crossvalidated
[post_id]: 108357
[parent_id]: 108344
[tags]: 
To me this looks like the perfect setup for logistic regression because the variable $s(x)$ is 0 or 1 and $m(x)$ is numeric. Briefly, you can assume that the probability that $s(x) = 1$ follows the model below saying that the relationship between $s(x)$ and $m(x)$ is monotonic and S-shaped (more precisely logistic). $$P(s(x) = 1) = \frac{1}{1+e^{\beta_0 + \beta_1 m(x)}}.$$ You can make the model more complex and add a variable $g(x)$, say, which is 1 when the element $x$ belongs to group A. The model becomes the following. $$P(s(x) = 1) = \frac{1}{1+e^{\beta_0 + \beta_1 m(x) + \beta_2 g(x)}}.$$ The null hypothesis in your case can be reformulated as $\beta_2 = 0$. If you are not familiar with linear models, this may be a little technical. Fortunately you do not need to be an ace to use the native R library. Below I show an example with fake data that illustrates how to use it. # Create some (reproducible) random fake data. set.seed(123) m Now we add an uninformative grouping function $g(x)$ and perform the test. g The last command outputs the following. Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -4.000e+00 8.570e-01 -4.668 3.05e-06 *** m 1.000e-01 1.949e-02 5.132 2.87e-07 *** gTRUE 1.077e-12 6.924e-01 0.000 1 See that the last line shows a z-value equal to 0 and a p-value equal to 1, which means that the null hypothesis is not rejected. With real data, this is never so clear, but you can replace s , m and g with your data and obtain the answer to your question in the same way. EDIT: Following the comments of @MichaelMayer I will try to discuss whether a more complex model is required, more particularly whether it should have an interaction term of the form $\beta_3 m(x) g(x)$. Here are the reasons that were on my mind at the time of writing the answer above. It seemed to me that the poster was not familiar with logistic regression so I opted for a simple but operative answer to give him a place to start. Hands on experience shows that if the relationship between $m$ and $s$ is different among groups, $\beta_2$ will often be significantly different from 0, even if this model is mis-specified. Without further information about the dataset and the kind of difference the poster is looking for, I opted for the most coarse-grained model. If $\beta_2 \neq 0$, the S-shaped curve will be shifted in one of the groups (the probability that $m$ is 1 goes uniformly higher or lower). If $\beta_3 \neq 0$, the most pronounced effect is that the S will get flatter or steeper in one of the groups. An example will explain better. The R lines below create an ideal dataset with such an interaction term. beta_0 For this dataset, $\beta_2=0$ (it is not even used in the specification). Let us see what happens when testing for its nullity. We do it exactly as above. model This time we obtain the following. Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -4.32243 0.96382 -4.485 7.3e-06 *** m 0.10816 0.02242 4.825 1.4e-06 *** gTRUE 2.44032 0.89081 2.739 0.00615 ** The hypothesis $\beta_2=0$ is rejected. Of course, this is the wrong conclusion because $\beta_2$ is 0 by construction but... wait a minute, the groups are different! So even if we got the statistical modeling framework completely wrong, we got to the right conclusion about the groups. If there is a difference between the groups, it will often “leak” into the test for $\beta_2$ even if the relationship is not the right one (the model with $\beta_2$ is mis-sepcified, but still has predictive power). Don’t get me wrong, there is no guarantee that this will happen, and I am not saying that interaction terms are useless in this context. If you are really interested in the flatness of the S-curve, you should incorporate the interaction term and specifically test its nullity. But from the context of the question, I gathered that the focus is really on the groups and whether or not they are different. I thought that this simple approach would already bring you a long way.
