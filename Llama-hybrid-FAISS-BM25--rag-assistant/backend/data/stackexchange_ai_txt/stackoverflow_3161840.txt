[site]: stackoverflow
[post_id]: 3161840
[parent_id]: 3161549
[tags]: 
Looks like there is some O(n^k) part of VC++ with k>1 when parsing array initializers... That would qualify as a logical bug you cannot do much about, but something that may work is unsigned char bdata[][100] = { { 0x01, 0x02, ... , 0x63} , { 0x64, 0x65, ... , 0xC7} , { 0xC8, 0xC9, ... , 0x2B} , ... }; unsigned char *data = &(bdata[0][0]); that is breaking the data in 100-bytes rows... MAY BE this will be parsed/compiled a lot faster by VC (just a suspect I have given the symptoms) and it shouldn't change your build process by much. I don't use VC++2010 so I cannot check. Just pay attention that sizeof(data) in this case will be just the size of a pointer and sizeof(bdata) will be instead the size of the image but rounded up to a multiple of the size of the row . If this version runs at the same speed the unfortunately the code is O(n^k) in the number of bytes and you're basically doomed if you want that to be compiled as an array. Another option could be using a huge string literal... the compiler may work better on that (may be they coded a special code path for string literals because "big" literals are not so uncommon), but your code generator will have to handle escaping of special chars.
