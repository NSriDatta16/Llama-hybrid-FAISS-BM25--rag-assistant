[site]: datascience
[post_id]: 61610
[parent_id]: 61603
[tags]: 
Yep, you're right! As you know, it's difficult for machine learning models to use natural language directly, so it helps to transform words into some meaningful numeric representation. This process is called word embedding, and finding word embeddings is the task of the keras Embedding layer. Ideally, word embeddings will be semantically meaningful, so that relationships between words are preserved in the embedding space. Word2Vec is a particular "brand" of word embedding algorithm that seeks to embed words such that words often found in similar context are located near one another in the embedding space. The technical details are described in this paper . The generic keras Embedding layer also creates word embeddings, but the mechanism is a bit different than Word2Vec. Like any other layer, it is parameterized by a set of weights. The weights are randomly-initialized, then updated during training using the back-propagation algorithm. So, the resultant word embeddings are guided by your loss function. To summarize, both Word2Vec and keras Embedding convert words (or word indices) to a hopefully meaningful numeric representation. Word2Vec is an unsupervised method that seeks to place words with similar context close together in the embedding space. Keras Embedding is a supervised method that finds custom embeddings while training your model.
