[site]: datascience
[post_id]: 126411
[parent_id]: 
[tags]: 
Basic neural network to solve ordinary differential equation

There is a related question in the forum: Solving an ODE using neural networks (via Tensorflow) But I am trying an idea to solve ODE by considering that the solution would be a polynomial and the differential equation is of 2nd order and non-linear So basically I was trying to solve the equation of a single pendulum $$\frac{d^2\theta}{dt^2}+\frac{g\sin\theta}{l}=0$$ I collected dataset from simulation where I considered time, angular position, angular velocity, angular acceleration, mass of the bob, length of the string, damping. My plan was to implement a neural network model from scratch such that there are 4 hidden layers and the output layer consists of 21 units. Among these 21 units 20 are the coefficient of the variable(time) and the other unit is basically the constant term of the polynomial (output layer is activated by linear function) I will be predicting $$\theta_{pred}(t) = a_1t+a_2t^2+......+a_{20}t^{20}+a_{21} $$ $$\omega_{pred}(t) = a_1+2a_2t+......+20a_{20}t^{19}$$ $$\alpha_{pred}(t) = 2a_2+......+380a_{20}t^{18}$$ and I customized the loss function such that $$L = \frac{1}{2m}*((\alpha_{actual}-\alpha_{pred})^2+(\theta_{0(actual)}-\theta_{0(pred)})^2+(\omega_{0(actual)}-\omega_{0(pred)})^2))$$ and in hidden layers I tried applying tanh activation function. But no matter what the epoch or learning rate I set the loss keeps increasing instead of decreasing. Since this pendulum motion follows some sort of a pattern I didn't regularize the model intentionally so that model overfits. I also tried decreasing and increasing the number of units per layer but no matter what I do, the loss never decreases. I need suggestion so that I could implement the idea. Thank you
