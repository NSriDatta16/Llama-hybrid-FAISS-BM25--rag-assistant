[site]: crossvalidated
[post_id]: 580261
[parent_id]: 580160
[tags]: 
You commented that pinball loss might be hard to interpret compared to something like $R^2$ . Fortunately, we can draw an analogy to $R^2$ to get a metric that is similar. In linear regression with square loss, we are estimating the conditional mean of $y$ . In the absence of any information that could explain variability in $y$ (no features), a reasonable naïve model is to predict the pooled mean of $y$ every time, so $\bar y$ . This gives some kind of baseline performance to which we can compare models that want to tighten up the estimation of the conditional mean, and $R^2$ has an interpretation as this. $$ R^2=1-\dfrac{\text{ Square loss of model of interest }}{\text{ Square loss of baseline model }} $$ Do something similar with your pinball loss, where the baseline model always predicts the pooled $75$ th percentile of $y$ . $$ R^2=1-\dfrac{\text{ Pinball loss of model of interest }}{\text{ Pinball loss of baseline model }} $$ This idea of comparing your performance to the performance of a baseline model is used elsewhere, such as in McFadden’s $R^2$ for logistic regression.
