[site]: datascience
[post_id]: 110180
[parent_id]: 
[tags]: 
Why can't positions in transformers be simply appended to the input to preserve the positional information instead of using positional encodings?

I saw in an intro to transformers in this video that positional encodings need to be used to preserve positional information, otherwise word order may not be understood by the neural network. They also explained why we cannot simply add positions to the original inputs or add other functions of positions to the input. However, what if we simply appended positions to the original input to preserve positional information? In fact, if directly using positional numbers does not suffice, couldn't they also just append positional encodings (containing the sine and cosine functions of differing frequencies) to the input? This would also solve the problem of loss of information which happens when one adds the positional encodings to the original input.
