[site]: crossvalidated
[post_id]: 550907
[parent_id]: 378197
[tags]: 
I would always assess this using a holdout sample. Fit models with and without the predictor and see how much your forecasts improve. I realize my recommendation makes most sense in the context of forecasting. If you are just looking for in-sample fit, you can do the same thing. If you are indeed forecasting, it would also make sense to use forecasted values of predictors. A predictor that does a great job explaining your time series but that can't be forecasted accurately is not really very helpful. Of course, the effect of a predictor may depend on whether or not another predictor is in the model. For instance, the effect of weather information will be much lower if we already have a seasonal model than if our model is nonseasonal. Thus, it is doubtful whether we can reasonably speak of "the" effect of a predictor. And yes, if we want to look at all possible combinations of predictors, that may lead to a combinatorial explosion. It may make sense to just test the full model with $k$ predictors against $k$ submodels that each drop just one predictor. Finally, note that whether one or the other forecast are "better" depends on your accuracy measure ( Kolassa, 2020 ).
