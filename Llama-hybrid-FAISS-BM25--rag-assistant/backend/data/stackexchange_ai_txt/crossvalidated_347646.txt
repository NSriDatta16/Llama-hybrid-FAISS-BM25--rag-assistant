[site]: crossvalidated
[post_id]: 347646
[parent_id]: 
[tags]: 
Is it possible to have a basis for a covariance matrix such that the greatest variance is greater than the variance of the first eigenvector?

Suppose we have a covariance matrix $C$. Define the eigendecomposition, $C = Q^{-1} \Lambda Q$ and some other arbitrary basis $C = B^{-1} D B$. Define $V_\text{PCA} = \text{diag}(QCQ^{-1})$ and $V_B = diag(BCB^{-1})$, and assume they're both sorted by magnitude. Define $T_\text{PCA} = \text{sum}(V_\text{PCA})$ and $T_B = \text{sum}(V_B)$. Is it possible that $V_\text{PCA}[i] / T_\text{PCA} I can understand how the inequality holds for $i > 0$, due to the orthogonality constraint of PCA which may not result in the "optimal" vectors being found in the variance-explained sense. I'm trying to figure out if there's an error in my code, so please forgive the convoluted notation, but I want to be as clear as possible about what I'm seeing.
