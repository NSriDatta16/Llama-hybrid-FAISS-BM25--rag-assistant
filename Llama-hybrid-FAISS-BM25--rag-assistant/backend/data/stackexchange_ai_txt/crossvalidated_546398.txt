[site]: crossvalidated
[post_id]: 546398
[parent_id]: 546396
[tags]: 
Cross-validation is a means of estimating the performance of a method for fitting a model, rather than of the model itself, so all steps in fitting the model (including feature selection and optimising the hyper-parameters) need to be performed independently in each fold of the cross-validation procedure. If you don't do this, then you will end up with an optimistically biased performance estimate. See my (with Mrs Marsupial) paper on this topic GC Cawley and NLC Talbot, "On over-fitting in model selection and subsequent selection bias in performance evaluation", The Journal of Machine Learning Research 11, 2079-2107 ( pdf ) I tend to use nested cross-validation to get an unbiased performance estimate, but if you don't need an unbiased performance estimate, just choose between competing methods (that don't have too many degrees of freedom, i.e. not feature selection!) then that often isn't necessary in practice, see Wainer and Cawley J Wainer and G Cawley, "Nested cross-validation when selecting classifiers is overzealous for most practical applications", Expert Systems with Applications 182, 115 ( doi:10.1016/j.eswa.2021.115222 ) Once you have that performance estimate, then retrain the model on the whole dataset, repeating the feature and model selection procedures once more. Also I would advise against feature selection if the aim is to improve performance (rather than identifying relevant features itself being the goal). Using a regularised model will often perform better.
