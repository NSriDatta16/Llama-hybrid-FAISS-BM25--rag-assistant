[site]: crossvalidated
[post_id]: 255083
[parent_id]: 171174
[tags]: 
The answer to "how capable are neural nets" is, in short, "very". It has been shown that neural networks can, given enough structure, approximate any nonlinear function (universal function approximation). https://en.wikipedia.org/wiki/Universal_approximation_theorem This says that even a single layer with enough neurons can represent any function. However, the number of neurons needed in a single layer may be very large, and techniques like convolutional nets (CNNs) favor having multiple (in some cases, dozens or hundreds) of layers instead of a single massive hidden layer. For your problem in particular, it is small enough that it may be easily solved by a network with a single, fully-connected hidden layer with a few dozen neurons. For the output layer, softmax would be the typical approach with one neuron for each digit you want to recognize. For this approach (or any other classifier, for that matter), you would need to feed in enough training data examples (including translations, scalings, and rotations of the digits you're trying to classify, depending on how robust you want the classifier to be to these sorts of permutations) to allow it to learn a broad array of data conditions. Feature selection has historically been used to effectively reduce the dimensionality of the representation fed into classifiers. There are many techniques that have been used to find low-dimensional representation of data sets as a precursor to classification tasks. However, in recent years, convolutional neural nets with deep learning architectures without any explicit feature selection have proven to out-perform most other methods for state-of-the-art image recognition. This is not to say that in specific cases you couldn't find specific feature extraction techniques as a precursor to a smaller neural network classifier that would work quite well for your problem, but this requires hand-tuning and trial and error to extract adequate representations that support downstream classification. In some sense, CNN architectures are doing the same thing automatically - they are learning nonlinear feature extractions that best support classification in a low-dimensional space.
