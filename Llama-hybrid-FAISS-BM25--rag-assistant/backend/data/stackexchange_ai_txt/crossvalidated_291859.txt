[site]: crossvalidated
[post_id]: 291859
[parent_id]: 
[tags]: 
Neural networks converge after only several steps

I am working on a sequence classification problem and have a pretty large dataset (100k examples). The inputs are binary vectors of length 100 and the output is a binary yes/no vector. I have tried several different architectures including fully-connected feed-forward NN, CNN and RNN. The problem is that after choosing the appropriate learning rate and using the Adam optimizer all networks converge after only 10-20 steps (200 samples per step) to smth like 0.65 auc. Tuning parameters or switching architectures improves only convergence, but not the final score. Does it mean that my problem is not well-defined? Am I using wrong features? Is it the best result I can get with this dataset? Thank you!
