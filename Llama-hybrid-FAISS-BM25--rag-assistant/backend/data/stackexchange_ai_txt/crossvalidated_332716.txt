[site]: crossvalidated
[post_id]: 332716
[parent_id]: 
[tags]: 
Averaging data leads to smaller standard deviation. Where is this visible in the formulas?

I need to compare two series of accidents counts from two cities. The comparison should establish whether the two series change at a similar rate. My way to answer this would be to calculate for every two consecutive years the rate of change for each series. Then I will create a new series of the ratio of their rate of change. The closer the mean of that series would be to 1, (and the smaller the variance), the stronger will be the conclusion that the two series A and B change at a similar rate. I can decide to compare on a monthly basis ("resolution"), or to sum the monthes, and to take a year as the unit of time. Now I come to the main quandry : Comparison that uses a year (rather than a month) as a unit of time, makes the rate of change of the two series to seem to be more similar. It seems that always when I average/sum a series of data, the standard deviation becomes smaller. However, obviously information is lost if summing\averaging across a year. My question is where does this loss of information appear in the mathematical formulas ? Usually loss of information is expressed in the formulas...
