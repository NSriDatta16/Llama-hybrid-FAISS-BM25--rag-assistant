[site]: datascience
[post_id]: 79853
[parent_id]: 79844
[tags]: 
A neural network is usually trained on a large set of paired example data (supervised learning). For each example in the set, the best known optimization for each weight is calculated, but it is then multiplied by the learning rate, which is a very small number. If a learning rate was not used, you would make large adjustments to each weight, only to destroy any positive benefit from those changes by making more large adjustments on the next example. By the time you reached the bottom of an iteration (epoch), your first few optimizations would be a distant memory, completely overwritten by later changes. If we only make small adjustments to the weights after looking at each example, we can go over the training set again and again with the goal of achieving an average optimization for the whole set, not just any one example. This also helps to protect against overfitting, which is where the network learns the noise in the training data and hence performs great on the training set but poorly in the wild.
