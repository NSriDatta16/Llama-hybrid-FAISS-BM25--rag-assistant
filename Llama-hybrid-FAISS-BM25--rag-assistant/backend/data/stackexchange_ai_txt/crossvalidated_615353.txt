[site]: crossvalidated
[post_id]: 615353
[parent_id]: 615344
[tags]: 
Notice that the sample covariance is $S_n = \frac{1}{n}\sum_i x_i x_i^T=\frac{1}{n}X^TX$ . If the features are drawn from a distribution with covariance matrix $\Sigma$ , then $S_n$ converges in probability to $\Sigma$ when $n$ approaches infinity. In other words, $(X^TX)^{-1} \to n^{-1} \Sigma^{-1} \to 0$ when $n \to \infty$ . In linear regression the features are usually considered as fixed. In particular, the distribution of the estimator $\hat W \sim \mathcal N(W,\sigma^2(X^TX)^{-1})$ is due to the randomness of $y$ only. (Intuitively, it is the distribution you will get by sampling $y$ over and over with the same features). If you would like to consider how $\hat W$ varies due to randomness of the features, you will have to additionally model the distribution of $x$ , which will result in a much more complicated distribution of $\hat W$ (and probably impossible to find in closed form). The estimator $\hat W$ is fixed by the data, so the expression $p(\hat W|data)$ doesn't make a lot of sense (it is just equal to 1). To make sense of it you would have to treat $W$ itself as the random variable, which is exactly the Bayesian approach.
