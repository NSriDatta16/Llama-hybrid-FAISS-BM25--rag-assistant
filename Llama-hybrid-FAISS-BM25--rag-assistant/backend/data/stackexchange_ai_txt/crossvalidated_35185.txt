[site]: crossvalidated
[post_id]: 35185
[parent_id]: 
[tags]: 
Dimensionality reduction (SVD or PCA) on a large, sparse matrix

/edit: Further follow up now you can use irlba::prcomp_irlba /edit: following up on my own post. irlba now has "center" and "scale" arguments, which let you use it to calculate principle components, e.g: pc I have a large, sparse Matrix of features I would like to use in a machine learning algorithm: library(Matrix) set.seed(42) rows Because this matrix has many columns, I would like to reduce its dimensionality to something more manageable. I can use the excellent irlba package to perform SVD and return the first n principal components (5 shown here; I'll probably use 100 or 500 on my actual dataset): library(irlba) pc However, I've read that prior to performing PCA, one should center the matrix (subtract the column mean from each column). This is very difficult to do on my dataset, and furthermore would destroy the sparsity of the matrix. How "bad" is it to perform SVD on the un-scaled data, and feed it straight into a machine learning algorithm? Are there any efficient ways I could scale this data, while preserving the sparsity of the matrix? /edit: A brought to my attention by B_miner, the "PCs" should really be: pc Also, I think whuber's answer should be pretty easy to implement, via the crossprod function, which is extremely fast on sparse matrices: system.time(M_Mt Now I'm not quite sure what to do to the means vector before subtracting from M_Mt , but will post as soon as I figure it out. /edit3: Here's modified version of whuber's code, using sparse matrix operations for each step of the process. If you can store the entire sparse matrix in memory, it works very quickly: library('Matrix') library('irlba') set.seed(42) m If you set the number of columns to 10,000 and the number of principal components to 25, the irlba -based PCA takes about 17 minutes to calculate 50 approximate principal components and consumes about 6GB of RAM, which isn't too bad.
