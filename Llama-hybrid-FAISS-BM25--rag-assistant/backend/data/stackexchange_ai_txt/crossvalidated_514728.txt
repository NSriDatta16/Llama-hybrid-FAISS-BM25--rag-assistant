[site]: crossvalidated
[post_id]: 514728
[parent_id]: 514333
[tags]: 
The answer is to use $X You find directions which explain the most variation in the data. You choose a few of them (eg. enough to explain 85% of the variance) and you project your data onto those directions. What you end up with is a lower dimensional dataset which captures much of the variance in the original dataset. This is what you want to use. From Introduction to Statistical Learning . For instance, the first two principal components of a data set span the plane that is closest to the n observations, in terms of average squared Euclidean distance [...] The first three principal components of a data set span the three-dimensional hyperplane that is closest to the n observations, and so forth. Your data, projected onto the principal component directions, is given by $x of the result from the prcomp function. From ?prcomp, the description for the value of x is "if retx is true the value of the rotated data (the centred (and scaled if requested) data multiplied by the rotation matrix) is returned." You can just select the first n columns of x in order to get the first n principal components. Also see Introduction to Statistical Learning page 403 Also, see this answer to another question about PCA using prcomp results.
