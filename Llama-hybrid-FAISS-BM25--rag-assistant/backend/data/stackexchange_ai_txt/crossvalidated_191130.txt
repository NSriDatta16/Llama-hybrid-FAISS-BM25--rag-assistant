[site]: crossvalidated
[post_id]: 191130
[parent_id]: 
[tags]: 
Cross validated penalized logistic regression - one standard deviation rule

I am new to this topic and would like to understand it better. I want to build a binary classifier based on penalized logistic regression. I have 10 features and 23 observations: 16 from class "0" and 7 from the second class "1" (yes, it is pretty unbalanced). I work with matlab and I used the following code to learn the classifier: >> [B,FitInfo] = lassoglm(X,y,'binomial','CV',5,'Weights',weightsFunc(y),'Options',opt) with: >> weightsFunc = @(y) 0.5*[ones(numel(find(~y)),1)/numel(find(~y)); ones(numel(find(y)),1)/numel(find(y))] which computes weights of each point (due to unbalanced data sets). X is the design matrix 23x10 and y - vector of zeros and ones. The problem is how to interpret and understand the result. Cross validation does find the minimal value of lambda (green circle), but in order to select the relevant features, we should use the "one standard deviation" rule and this rule gives all coefficients are zero (blue circle). Questions: How to work with unbalanced data sets? Is my procedure correct (by assigning weights to observations)? I tried to oversample the class with less observations (simply by doubling the observations), but it produces slightly different result. How to select features after the penalization, if one-std rule penalizes everything?
