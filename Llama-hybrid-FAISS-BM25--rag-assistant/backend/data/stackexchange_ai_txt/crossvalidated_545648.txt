[site]: crossvalidated
[post_id]: 545648
[parent_id]: 
[tags]: 
The second-derivative of the log-likelihood function in logistic regression (page 120) model

In textbook The Elements of Statistical Learning, for logistic regression (page 120) model, the log-likelihood function can be written as $$\ell(\beta)=\sum_{i=1}^N(y_i\beta^Tx_i-\log(1+e^{\beta^Tx_i}))$$ This is a $(p+1)$ nonlinear equations in $\beta$ . To solve this problem, we use the Newton-Raphson algorithm, which requires the second-derivative $$\frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta^T}=-\sum_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta))$$ where $p(x_i;\beta)=1-\frac{1}{1+e^{\beta^Tx_i}}$ . My questions are (1) why the second derivate is not defined by $$\frac{\partial^2 \ell(\beta)}{\partial \beta ^2}?$$ (2) What is the result of $$\frac{\partial \ell(\beta)}{\partial \beta^T}=\frac{e^{\beta^Tx_i}x_i}{(1+e^{\beta^Tx_i})^2}?$$ Is it same as $\frac{\partial \ell(\beta)}{\partial \beta}?$
