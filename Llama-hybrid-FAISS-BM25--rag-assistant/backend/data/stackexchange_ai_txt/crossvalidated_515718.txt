[site]: crossvalidated
[post_id]: 515718
[parent_id]: 496393
[tags]: 
Permutation test. Suppose I am given the vectors x1 and x2 in R, each with $n = 20$ counts of flaws in 20 specimens of a product, for two different production procedures. Data are tallied and plotted below: table(x1); mean(x1) x1 1 2 3 7 7 6 [1] 1.95 table(x2); mean(x2) x2 0 1 2 3 4 5 6 1 1 4 8 4 1 1 [1] 3 x = c(x1, x2); g = rep(1:2, each=20) boxplot(x~g, col="skyblue2", horizontal=T) Ordinarily, I might consider testing for a difference between the two population means with a pooled or Welch two-sample t test or a two-sample Wilcoxon signed rank test. But the data are hardly normal, so I have doubts about using a t test, and the Wilcoxon test in R gives a warning message about ties. So I wonder if I can get a reliable P-value from either procedure. My concern is not that a pooled t test statistic is an unreasonable way to measure the difference in sample means, but with the P-value, which depends on that statistic having Student's t distribution with $\nu = 38$ degrees of freedom. In order to perform a simulated permutation test, I will scramble the 40 counts at random between the two groups (production procedures), and find the pooled t statistic. By doing this repeatedly (say $m = 10\,000$ times), I can get a good idea of the true distribution of the t statistic. Then I can compare the value of the t statistic for the actual data to get the P-value of the permutation test. [By tedious combinatorial methods I might be able to get the exact permutation distribution. In the permutation distribution, there are ${40\choose 20}$ possible ways to evaluate $T,$ not all leading to different results. However, $m$ permutations of the data are enough for useful results.] Details as we begin: The R function sample (without extra parameters) randomly scrambles the elements of its argument; two such permutations are show. For the original (unperuted) data, the observed value of the t statistic t.obs is $-2.99,$ sample(g) [1] 2 1 1 1 1 2 1 1 1 2 1 2 2 1 2 1 2 2 2 1 [21] 1 1 1 2 2 2 1 1 1 2 1 2 1 1 2 2 2 2 2 2 sample(g) [1] 2 1 2 2 1 1 2 1 2 1 2 1 2 2 1 1 2 2 1 2 [21] 1 2 2 1 1 2 1 1 1 2 1 1 2 2 2 2 1 2 1 1 t.obs = t.test(x ~ g, var.eq=T)$stat; t.obs t -2.987193 From $m = 10\,000$ random scramblings, we get $m$ values of the t statistic, these make up our simulated permutation distribution. Then the proportion of those $m$ values that is more extreme than t.obs is the P-value of the simulated permutation test. set.seed(325) t.prm = replicate(10^4, t.test(x~sample(g), var.eq.=T)$stat) mean(abs(t.prm) >= abs(t.obs)) [1] 0.0054 # Simulated P-value hdr = "Permutation Dist'n of t Statistic" hist(t.prm, prob=T, col="skyblue2", main=hdr) abline(v = c(-t.obs,t.obs), col="red", lwd=2, lty="dotted") Although there are about 127 trillion possible permutations, because of the many ties only $52$ uniquely different values of `t.perm' were observed. Nevertheless, P-values from several runs had values between 0.0054 and 0.0082, so there is no doubt about rejecting $H_0: \mu = 0;$ the two processes are giving different results. Using a larger $m$ might result in slightly more consistent simulated P-values, but with the same decision. Nonparametric bootstrap confidence interval. Suppose I have $n = 100$ observations from an unknown distribution (for which I believe the mean $\mu$ exists) in the vector x , which are summarized and plotted below. summary(x); length(x); sd(x) Min. 1st Qu. Median Mean 3rd Qu. Max. 50.17 59.66 69.89 70.45 80.69 90.94 [1] 100 # sample size [1] 11.29149 # sample standard deviation hist(x, prob = T, col="skyblue2"); rug(x) From the summary and histogram, I suppose $\mu \approx 70,$ but I want a 95% confidence interval to give me some idea of the variability of that estimate. If I knew the distribution of the quantity $\bar X - \mu,$ then I could find values $L$ and $U$ that cut probability 0.025 from the lower and upper tails, respectively, of that distribution, so that $P(L \le \bar X - \mu \le U) = 0.95.$ Then I could "pivot" to get the 95% CI $(\bar X - U,\, \bar X - L)$ for $\mu.$ Let $A_{obs} =\bar X_{obs} = 70.45203$ be the sample mean of the data as provided. mean(x) [1] 70.45203 Not knowing the distribution I enter the 'bootstrap world', re-sampling values of $\bar X$ to get realizations $D^*$ of $D.$ One iteration of this re-sampling goes as follows: re-sample $n = 100$ values from the data x with replacement, to find $\bar X^*,$ temporarily use $A_{obs}$ as a proxy for unknown $\mu,$ and obtain a realization of $D^* = \bar X^* - A_{obs}.$ [Notice that there is no use sampling $n$ items from x without replacement; I'd just scramble the values in x to get the same average every time. In R, to do sampling with replacement, use the argument rep=T with the sample procedure.] Obtain a few thousand, say $B = 3000$ such re-sampled values $D^*$ to get an idea of the distribution of $\bar X - \mu,$ and finally estimate $L$ by the $L^*,$ the lower 0.025 quantile of this bootstrap distribution of $D.$ Similarly, use estimate $U$ by $U^*,$ the 0.975 quantile of the bootstrap distribution. Now, emerging from the 'bootstrap world, $A_{obs}$ returns to its original role as the observed sample mean of x , and use $\left(A_{obs} - U^*, A_{obs} - L^*\right).$ In the code for the 95% nonparametric bootstrap CI below, suffixes .re replace the * 's above to indicate re-sampled quantities. The code assumes vector x is already available in R. [It is always important to distinguish real data from re-sampled values because re-sampling is an analytic device, not a way to get additional information about the population.] The resulting 95% nonparametric CI for $\mu$ is $(68.34, 72.60).$ set.seed(2021) a.obs = mean(x) d.re = replicate(3000, mean(sample(x, rep=T)) - a.obs) UL.re = quantile(d.re, c(.975,.025)) a.obs - UL.re 97.5% 2.5% 68.33898 72.60076 Note: The R code below shows how the data vectors for the permutation test and the bootstrap CI were simulated in R. Of course, in a real application you would not know the true parameter values. # permutation test set.seed(123) x1 = rbinom(20, 3, .7) x2 = rbinom(20, 7, .4) # bootstrap set.seed(123) x = c(rnorm(50, 60, 5), rnorm(50, 80, 5))
