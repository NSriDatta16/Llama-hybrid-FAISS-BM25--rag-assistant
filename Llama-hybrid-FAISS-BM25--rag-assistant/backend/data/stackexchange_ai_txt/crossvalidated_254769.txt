[site]: crossvalidated
[post_id]: 254769
[parent_id]: 254632
[tags]: 
Each of the 19000 genes has three characteristics you are considering: expression level, tumor/normal expression ratio, and presence of mutation. If you think that these characteristics are related in ways that might be important for your project, then some initial exploration of those characteristics (without reference to the outcome data) seems in order. Exploration of distributions of predictors, and relations among predictors, is a standard way to start to reduce dimensionality in the scenario of $p\gg n$ (many more predictors than cases). The example in ISLR of using random forests to relate gene expression to predict cancer types (pages 320-321) only considered the 500 genes (among 20000 genes total) with the largest variance in expression among the samples. Pre-clustering several associated predictors into individual predictors is another accepted approach to this problem; see, for example, section 4.7 of Harrell's Regression Modeling Strategies . In your particular application the task might be fairly simple. Unless your cancers are of a highly hypermutated type (e.g., patients with DNA polymerase deficiencies or microsatellite instability), very few of the 19000 genes are likely to have mutations in more than 1 or 2 of your 300 individuals. If you restrict your list of mutated genes to those that are mutated in more than, say, 2% of individuals then almost all of the data columns on mutation status can be eliminated. Typically gene expression data are normalized in some way among the samples (to account for differences in sample mass, assay conditions, etc.) although it's not clear from your question whether that has been done. If not, standard normalization among samples should be done first. Once normalized gene expression data from tumors is available, it's not clear what additional information the tumor/normal expression ratio will add. If you take the standard approach of analyzing tumor gene expression and tumor/normal expression ratio data in log scales, then the log tumor/normal expression ratio is just the difference between the normalized log expression in the tumor (which you already have) and the normalized log expression in whatever normal tissue was analyzed. So working with the ratio is pretty much equivalent to working with the normalized normal-tissue expression instead. If you don't think that normal-tissue expression levels per se matter to outcome, then you might not need to include the ratio information in your model. After the initial dimension reduction, however, you will still face the challenges of $p \gg n$. Random forests provide one way to approach this, but intelligent paring down of your number of predictors first, based on subject-matter knowledge, is highly preferable to just throwing all the data into a random forest.
