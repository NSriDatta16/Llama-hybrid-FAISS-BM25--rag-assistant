[site]: crossvalidated
[post_id]: 260604
[parent_id]: 
[tags]: 
Estimating error on oscilloscope

I'm trying to understand a technique used for estimating error from an oscilloscope. I'm assuming the noise on the scope is normal, so can except a bell-curve for the error I see. If I see a horizontal line, a technique I have learned to estimate the one sigma error is to find the max and min visually on the scope and find the vertical distance. Then you take this distance and say it is 3-sigma since it is about 99.7% of the data in that line. So you take that 3-sigma and divide by 3 to get one sigma. I have also heard other people say you divide by 6 since you are looking 3 sigmas in both directions from the average, so to find one sigma, you divide by 3. For example, consider the following oscilloscope line: And lets pretend you measure from the maximum of this line to the minimum and find it has a total spread (distance from max to min) of 6 mV. Lets also say the line is centered at 0 mV, and has a mean of 0 mV. So the max point is at 3 mV and bottom point at -3 mV. I have learned and seen people do the following to calculate 1 sigma to use as an uncertainty: say that 6 mV is 3 sigma since it is 99.7% of the data, so then 2 mV is 1 sigma. I have also seen people do and say that 3 sigma is just from 0 mV to 3 mV, so going the full range of -3 mV to +3 mV is 6 sigma, making 1 sigma be 1 mV. What is correct? I believe taking it from -3 mV to +3 mV is 6 sigma because if you look at a bell curve: Then going from +3 sigma to -3 sigma is the 99.7%. So I believe you would divide by 6, not 3.
