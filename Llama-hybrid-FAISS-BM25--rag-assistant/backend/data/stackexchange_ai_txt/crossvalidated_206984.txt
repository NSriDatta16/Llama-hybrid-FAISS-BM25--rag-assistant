[site]: crossvalidated
[post_id]: 206984
[parent_id]: 206807
[tags]: 
First of all, you're right about the similarities: they are all types of resampling-based error estimates. Now about the differences. cross validation vs. out-of-bootstrap : cross validation (as well as random splitting procedures known as set validation or hold-out validation) use resampling without replacement whereas bootstrap procedures resample with replacement . In cross validation, the resampling without replacement is done in a way that ensures each sample is tested exactly once per "run" of the cross validation. (There are also non-random splitting procedures for cross validation such as venetian blinds or contiguous blocks which are used in special situations.) Which of these procedures is best for your model validation depends e.g. on your sample situation and on the model you're validating (bootstrap is of no use with a modeling algorithm that deletes duplicates). Out-of-bag estimates are very different from the "normal" out-of-bootstrap or cross validation estimates, because they do not estimate the generalization error of single models (typically the one model built on the whole data set) but the generalization error of an aggregated (ensemble) model (bag = bootstrap aggregation). So this difference is like the difference between a single decision tree and a random forest. Again, you can also use resampling without replacement (e.g. cross validation) to generate the model ensemble for the aggregation - the principle works exactly the same. (Ask me if you need a literature example) @benmaq already pointed out, bootstrapping is often used for other purposes than validation, particularly to estimate variability due to the random process of sampling that lead to the sample at hand. An analogous procedure with resampling without replacement or more precisely, using the surrogate models of leave-one-out cross validation is known as jackknifing .
