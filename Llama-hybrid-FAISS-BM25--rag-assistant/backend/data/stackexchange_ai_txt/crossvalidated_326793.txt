[site]: crossvalidated
[post_id]: 326793
[parent_id]: 326710
[tags]: 
For a shallow network, of only a few layers, standard deviation of the Normal distribution with a mean of 0.0, can be 0.01, or even 1.0. But, that kind of a "fixed" standard deviation is not okay for deep neural networks (NN), that have many layers. If you want to use the Uniform distribution, then it translates to the range of [-0.01, 0.01], or [-1.0, 1.0]. I'll use Normal distribution in this answer. Let's take $\tanh$ as activation function. What you'd get in a deep NN with that initialization, in the case of $\sigma = 0.01$, is the so-called "vanishing gradients" problem. Deep layers would have all activations close to zero, and that's not good for backward pass. Namely, deep layers would have $\sigma$ near zero, because you keep multiplying by small values layer after layer, and thus all activations come close to zero. In the backward pass, we now have gradients that are close to zero in all layers, so they don't flow, and the network is unable to learn. We can try to fix that by using a larger value for $\sigma$, say 1.0. But, now all neurons become saturated. Their activations will be either -1.0, or +1.0. What's the gradient of a constant? Zero. The answer is somewhere in between, but it's not so easy to find. If a neuron is connected to more neurons from the previous layer, its output variance will be larger than when it's connected to fewer neurons. If we divide its weight vector by $\sqrt{n}$, we normalize its $\sigma$ to 1. I would just say that n is the number of inputs to the layer we're initializing, i.e., its number of rows, or number of columns of the previous layer, and not number of outputs of the current layer, as you said. This makes all neurons in a network have approximately the same output distribution in the beginning. This is a heuristic, that's been found to work well in practice. It helps the convergence process. So, we don't fix our range to a particular value, but rather scale initial $\sigma$ of each neuron to 1. But, this doesn't work for the ReLU activation function. Using ReLU units leads to $\sigma$ being close to 0 in deep layers, thus making activations being close to 0, which in turn leads to gradients being close to 0. This means we have to compensate for that, by dividing weights by $\sqrt{n/2}$ instead of by $\sqrt{n}$. Then, it works fine. Xavier Glorot et al. proposed a similar heuristic, in 2010.: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf . For example, TensorFlow has it: tf.contrib.layers.xavier_initializer . References: CS231n Lecture Notes CS231n video The references explain this in more detail, so if you're interested, you can check them out. I tried to be concise.
