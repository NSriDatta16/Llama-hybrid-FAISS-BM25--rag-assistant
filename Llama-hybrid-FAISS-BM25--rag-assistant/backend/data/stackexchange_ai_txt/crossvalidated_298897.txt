[site]: crossvalidated
[post_id]: 298897
[parent_id]: 
[tags]: 
Robust Principal Component Analysis (RPCA) for collaborative filtering

Consider the robust completion problem of a matrix $X$ where $X_{ij}$ are observed matrix entries for $(i, j)$ ∈ $Ω_{obs}$ i.e, $\min_{L,S}$ $\text{rank}(L) + \mu ||S||_{0} $ subject to $X_{ij} = L_{ij}+S_{ij}$ ; $(i,j) ∈ Ω_{obs} $ with convex relaxation $\min_{L,S}$ $||L||_* + \lambda ||S||_{1} $ subject to $X_{ij} = L_{ij}+S_{ij}$ ; $(i,j) ∈ Ω_{obs} $. Why for this specific task RPCA is sometimes preferred to standard low rank matrix completion? My guess would be that the model takes in account the uncertainty of the observations represented by the sparse noise term S and that the recovery is more accurate in the case of outliers/noisiness in the observed entries. Can somebody confirm/reject and explain why?
