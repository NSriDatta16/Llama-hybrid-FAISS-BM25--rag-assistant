[site]: datascience
[post_id]: 128379
[parent_id]: 
[tags]: 
Question about contextual embeddings?

How do BERT and RoBERTa generate contextual embeddings? The articles I've read keep saying that transformer encoders work bidirectionally. Because of self-attention, they can look at every token, unlike RNN/LSTM, which can only process the previous hidden state. Is it true ? I'm not sure how BERT and RoBERTa accomplish that.
