[site]: datascience
[post_id]: 52392
[parent_id]: 51983
[tags]: 
IMO decision trees are - by definition - designed so that the single "best" split is chosen in each step (Introduction to Statistical Learning, Ch. 8.1). I think you need to split on values low, medium, high, in which case a first split would e.g. occur at (low) vs. (medium, high) and a later split would be between (medium, high), whatever gives the best fit. Single decision trees often do not have a very good predictive capacity (see. Introduction to Statistical Learning, Ch. 8.2). If you are interested in accuracy of prediction, you should go a step further and grow a random forest with "bagging" or even better "boosting" on many trees (or "ensambles of trees"). In this case, many trees are grown, and they all together make a "vote" on how to predict some outcome. Random Forest in scikit-learn: https://scikit-learn.org/stable/modules/ensemble.html Prominent boosting methods are e.g. catboost ( https://catboost.ai/docs/concepts/about.html ) or lightGBM ( https://lightgbm.readthedocs.io/en/latest/ ). Alternative (non-tree based) models will be able to make a differentiation by three classes without any problem (ISL, Ch. 4). One example is a logistic model (or "logit") of form risk = b0 + b1*savings . In this models you can also calculate marginal effects, telling you, in case someone moves from class A to class B, by how much will the probability of being a "bad risk" change (marginal effects). https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html You can find a bunch of good code directly covering topics mentionned here and discussed in the book "Introduction to Statistical Learning" online: https://github.com/JWarmenhoven/ISLR-python In summary: If you are interested in prediction, don't stick to simple decision trees, but move on to something else.
