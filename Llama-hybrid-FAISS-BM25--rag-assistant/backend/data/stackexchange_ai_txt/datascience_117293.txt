[site]: datascience
[post_id]: 117293
[parent_id]: 
[tags]: 
Combining sentence embeddings of two different models (sBERT and mBERT)

I am working on a chatbot that helps students. So, I wanted to make use of bert model which has better performance on mathematics, which lead to me to math-bert, but the paper on it said that it was trained only on mathematical corpus, which means it wont have great performance on general sentences (example in image), so is there a method to combine sentence-bert and math-bert? [![enter image description here][1]][1] Or, the only way is to train bert model from scratch using corpus used for sentence-bert and math-bert.
