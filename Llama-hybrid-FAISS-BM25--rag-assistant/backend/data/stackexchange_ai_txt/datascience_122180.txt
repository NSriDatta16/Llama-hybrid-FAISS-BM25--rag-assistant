[site]: datascience
[post_id]: 122180
[parent_id]: 
[tags]: 
Recommendation for ML framework that allows running untrusted code to train models while maintaining data privacy?

We are a deep learning company looking to find a way to allow contractors to create and run training code that we can then run on private datasets, without giving the contractors access to all the data. The ideal solution would be one where they can publish and run code without oversight, get some feedback (e.g. accuracy over time, error reports if something crashes), but not gain access to the data being trained on. A few thoughts: The presumption is that the actors are not malicious, but we want to defend against the case of coming across a malicious actor or one that becomes malicious. One approach would be that we audit their code before we run it on our servers, but this allows for vulnerability if our audit isn't good and the actor becomes malicious. Another thought is to disable all or almost all network traffic, then start their script, and only once the script ends, restore network traffic. We could whitelist certain APIs for the reporting they're allowed to see. The latter seems a good approach, but, before we do the work to implement it, we wanted to see if this problem hasn't already been solved.
