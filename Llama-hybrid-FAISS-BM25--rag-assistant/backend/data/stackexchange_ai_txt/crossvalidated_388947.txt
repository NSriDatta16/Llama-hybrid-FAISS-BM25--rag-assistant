[site]: crossvalidated
[post_id]: 388947
[parent_id]: 
[tags]: 
Why can having more "model" parameters (weights) in neural networks lead to overfitting?

In http://cs231n.github.io/convolutional-networks/ , it states that a "huge number of parameters would quickly lead to overfitting" in neural networks. I don't think I quite understand this. The thought that I have is more model parameters = ability to represent more complex functions. So when training, having more model parameters could mean that you're fitting a function to the training data very closely such that it leads to overfitting and loss of generalization. Is this the idea or am I totally off?
