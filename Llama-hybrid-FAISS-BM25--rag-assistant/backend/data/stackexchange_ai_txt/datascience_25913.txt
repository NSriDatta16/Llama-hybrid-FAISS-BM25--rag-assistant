[site]: datascience
[post_id]: 25913
[parent_id]: 
[tags]: 
Do I need to use Bayes to combine a sample's class probability with the performance of the overall model?

For a classification algorithm that gives the predicted probabilities for each class (ie random forest in sklearn), by default the classes are separated with a score of 0.5. After evaluating the metrics, it looks like there would be a better precision/recall balance for my needs with a higher score. If I use a split of 0.6 score, which results in a 0.8 precision and 0.9 recall, how do I communicate the overall confidence of future sample scores? For example, if a sample returns a predicted probability of 1 vs one that returns a 0.75. Based on the threshold chosen, both are going to be considered as part of the positive class. However, it's estimated that only 80% of the time that will be true. On top of that, the 1 and 0.75 scores seem to indicate another level of confidence for the model. This feels like Bayes should be used here to summarize the scores for interpretation. But, I am not sure. So, given the model has an estimated performance based on a score threshold, and given some scores, how do you give an overall percentage of the samples actually being in the positive class? And how would one communicate this to non-technical people?
