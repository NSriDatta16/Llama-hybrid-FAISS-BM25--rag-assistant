[site]: datascience
[post_id]: 114771
[parent_id]: 111943
[tags]: 
That is the case with VAE training in general. It all boils down to the task you are working on. If you want your model to generate meaningful reconstructions and it does that then I wouldn't worry. If you want to generate novel samples and that is failing maybe some hyper-parameter tuning is needed. Specifically, for your case you might also want to try early-stopping. As I understand from the log-file KL starts increasing and thus affecting your overall loss after epoch 220. It also seems to me that your reconstruction loss is stable around that epoch mark. In terms of battling KL divergence issues I would also refer you to the beta-VAE due to Higgins et. al ( https://openreview.net/forum?id=Sy2fzU9gl ). In a nutshell they add a term (beta) that regularizes the KL term of the ELBO. You can experiment with that and see how it affects training. Finally, there is a very helpful post from MSR where they introduce annealing regimes for the the beta hyper-parameter in beta-VAEs and how they affect training ( https://www.microsoft.com/en-us/research/blog/less-pain-more-gain-a-simple-method-for-vae-training-with-less-of-that-kl-vanishing-agony/ ).
