[site]: stackoverflow
[post_id]: 3790461
[parent_id]: 3487927
[tags]: 
I'm curious, and thinking out loud here -- betting it's not on the dev side, actually more suited for ServerFault, being sort of a networking problem? I say so not to discourage your question, but perhaps help you think outside of the usual suspects. So this is a stab in the dark, but given that you're firing this off from a browser, I'd guess you could also do it with Wget? I've run some HTTP requests that are coded properly and as a safeguard should exit after 1000 or so iterations, but for whatever reason they do not (even modifying the FF config for it). However, running same HTTP request from Wget works like a champ. From the Wikipedia entry on Wget: Wget has been designed for robustness over slow or unstable network connections. If a download does not complete due to a network problem, Wget will automatically try to continue the download from where it left off, and repeat this until the whole file has been retrieved. It was one of the first clients to make use of the then-new Range HTTP header to support this feature. What do you think? I don't offer this with conviction, but on a hunch. If I'm wrong, maybe it's a step outside the box, closer to the solution? Good luck fixing it! I'm eager to know what it is. :)
