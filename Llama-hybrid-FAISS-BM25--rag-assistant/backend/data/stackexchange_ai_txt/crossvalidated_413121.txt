[site]: crossvalidated
[post_id]: 413121
[parent_id]: 413112
[tags]: 
Terminology use around sets in model building is inconsistent, so I will refer to Wiki on sets and SE Data Science question where 'test set' is specifically a held out set only assessed at the very end. In this syntax then what we are discussing is the 'validation set', used as an intermediate check on the calibration of the model using the training set (and to tune hyperparameters). Am I overfitting because my final objective is to have the best generalization accuracy? Thanks to the ever present issue of bias and noise overfitting is unavoidable, it is generally a question of whether it is with acceptable limits. Imagine the ideal; an unbiased dataset where the only errors in the data are completely random and independent between samples (or any other hierarchy within your dataset). Even in this scenario you will still get cross talk between samples due to random overlap in noise. For example Gaussian noise between independent measurements reduces by the square root of the number of measurements taken. This means that any chance coincidences between the two sets will be selected for using the training set even though the validation set was not used to get coefficients for a model. Now, if random noise is low (or sample numbers quadrupled to halve the noise, since noise cancels by square root) in this scenario the overfitting would be minimal, but this is a ideal dataset far removed from any real world. If your validation set is simply a subset of your full dataset cordoned off from the training phase then the validation set will share the same biases as the training set. This means that the test set will further ingrain these in the model, but then again it would never be able to identify these common biases if used once. In such as case your cannot predict any generalization capability, because you are not generalising your model on independent data. Do I have some sort of data leakage because of my hyperparameters searching objective is defined based on having the highest generalization accuracy? Yes, biases will leak strongly, especially where the validation set is not completely independent of the training set. In such a case biases will not be controlled for and will give a falsely optimistic performance estimation. Independent random noise will leak but can be minimised by using large sample numbers. There is no such thing as a blank-cheque generalisation licence for models, you do not prove boundless generalisability. Strictly speaking a model can only be considered validated for generalised use in a situation/population for which an independent data collection has been carried out. Furthermore, the utility is only validated for the limits of the independent set. E.g. a model built using East Asian data will not be able to demonstrate generalisation outside of East Asia. Before knowing if it is relevant to North America it needs validation on North American data. You can replace East Asia/North America with any combo relevant to your model (Male/Female, working age/retired, poor/rich cats/dogs etc). This is because different limits may have different underlying data generating processes (control variables, covariance structures, biases etc) that fall outside the calibrated range, i.e. you are extrapolating the model beyond its expected limits.
