[site]: crossvalidated
[post_id]: 430339
[parent_id]: 430301
[tags]: 
Aren't the mean and stds that are created in encoder's part which are as large as the embedding size, supposed to specify each classes valid values? That is for example, all the variables that can create a 1, will have this mean and this std! This is not correct. The encoder defines a mean and a covariance (std) for each sample individuallyâ€”not all ones get encoded to the same gaussian. The VAE learns to encode different samples to different subregions of the prior density, so that over the training dataset, KL divergence between the priors and posteriors is low, but at the same time, each sample's encoding contains enough information to reconstruct it. I recommend reading the article Density Estimation: Variational Autoencoders by Rui Shu, which demonstrates it with the following figure: An example of a 2D latent space encoding of MNIST shows it too: Overall, the latent space density is gaussian, but different classes occupy different, non-gaussian subregions (image from A Tutorial on Variational Autoencoders with a Concise Keras Implementation ):
