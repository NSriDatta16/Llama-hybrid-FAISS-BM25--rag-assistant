[site]: crossvalidated
[post_id]: 56566
[parent_id]: 56562
[tags]: 
From my reading, I understand that usually what you would do would be to: - take, say, 40 chunks of 100 replies as your sample, - for each sample, determine what percentage is written by bots - calculate the mean and standard deviation of the percentages of all 40 chunks - calculate the standard error for the sample, based on the standard deviation / sqrt of 40 (chunks) - calculate 95% significance of the mean percentage via mean of sample chunks +/- 1.96 x standard error It's almost certainly not necessary to break it up into chunks. To what I understand, this can be done because we expect the data to be roughly normal. Not when your proportion is very close to zero or 1. If the expected count of not-written-by-bots is no more than a handful, assuming normality isn't tenable. I can't really say that I am 99.999% confident that the % of bots for the population is 100% right? There's a commonly used approximate rule to get bounds on the proportion of "successes" when you observe only failures... sometimes called the " rule of 3 ". This rule says that if no successes occur in $n$ observations, then $[0, 3/n]$ is an approximate 95% confidence interval for the proportion of successes in the population. There are a number of ways of arriving at this rule, including using a Bayesian derivation of a credible interval. If you get all of your sample of 4000 being bot-posts, you would have a 95% interval for the population proportion of non-bot posts of about $[0, 0.00075]$, or between 99.925% and 100% bot-posts in the population. To do something more sophisticated, I'd suggest considering Nick Sabbe's comment relating to a Bayesian approach. If you have any prior information about the proportion you can incorporate it (not that it will make much difference with a large sample size); you can also incorporate homogeneity of proportion in this framework relatively easily.
