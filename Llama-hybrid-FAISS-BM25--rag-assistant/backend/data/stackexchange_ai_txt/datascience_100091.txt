[site]: datascience
[post_id]: 100091
[parent_id]: 100089
[tags]: 
I'll try to make it as simple as possible. Underfitting is when you have high bias and high variance in your model. So the model learns nothing from the training data (low training score aka high bias) and predicts poorly on the test data (low variance). You get underfitting when your model is too simple for the data or the data is too complex for your model to understand. Here is an example of underfitting:- As we can see both train and test scores are poor which means the model learns nothing from the data and performs/predicts nothing on the test set. Techniques to reduce underfitting : Increase model complexity Increase number of features, performing feature engineering Remove noise from the data. Increase the number of epochs or increase the duration of training to get better results. Overfitting is when you have low bias and high variance. So the model learns everything from the training dataset (high train score aka low bias) but is not able to perform good on the test set (low test score aka high variance) You get overfitting when your model is too complex for the data or your data is too simple for the model. Here is an example of overfitting:- As we can see, the training loss decreases initially (low bias) but the test/validation loss, after decreasing to a certain point starts gradually increasing. Also apparent is the large gap between train and test lines. Techniques to reduce overfitting : Increase training data. Reduce model complexity. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training). Ridge Regularization and Lasso Regularization Use dropout for neural networks to tackle overfitting. Hope that clears the confusion!
