[site]: crossvalidated
[post_id]: 313725
[parent_id]: 
[tags]: 
Text-classification: techniques for matching train, test, production feature spaces?

You've got a supervised text-classification system that involves significant preprocessing of the source text (case-normalization, removing stop words, stemming, etc.) followed by some routine to produce a numerical document vectors for representing some language model (let's keep it simple and say BoW). Your document vector matrix for train/test data: | doc-id | "act" | "bat" | "cat" | ... | label | |--------|-------|-------|-------|-----|-------| | d1 | 0 | 1 | 0 | ... | True | | ... | ... | ... | ... | ... | ... | | dn | 1 | 1 | 0 | ... | False | You've trained, tested, iterated. You're getting good precision-recall. Now you're ready for production... Are there common techniques, or any theory/conceptual approaches, ensuring that the production data produces features within the same feature space? That is, how does the production system's preprocessing and vectorization steps produce new features (e.g. "after") that the system didn't produce during training/testing? | doc-id | "act" | "after" | "bat" | "cat" | ... | label | |--------|-------|---------|-------|-------|-----| | d1 | 0 | 1 | 0 | 0 | ... | True | | ... | ... | ... | ... | ... | ... | ... | | dn | 0 | 1 | 0 | 1 | ... | False | It is easy to imagine lower-level programmatic means of handling this: I'm interested in higher-level conceptual approaches, if they exist. .
