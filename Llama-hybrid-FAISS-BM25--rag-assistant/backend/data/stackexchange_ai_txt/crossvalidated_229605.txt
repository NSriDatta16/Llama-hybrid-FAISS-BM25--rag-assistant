[site]: crossvalidated
[post_id]: 229605
[parent_id]: 229592
[tags]: 
Typically learning with neural networks uses the back-propagation algorithm to minimize SSE of the errors. This will generally find a local minimum ( https://en.wikipedia.org/wiki/Local_optimum ) of SSE and is not guaranteed to find the global minimum. Depending on the initialization of parameters the algorithm may converge to a different local minimum. In your example, when training the full model it seems that the algorithm has got stuck on a local minimum, as there is clearly another model (using a subset of available data) that has smaller SSE.
