[site]: crossvalidated
[post_id]: 385596
[parent_id]: 367397
[tags]: 
Maybe the answers to this question need a quick update. It seems like SGD yields a global minimum also in the non-convex case (convex is just a special case of that): SGD Converges To Global Minimum In Deep Learning via Star-Convex Path, Anonymous authors , Paper under double-blind review at ICLR 2019 https://openreview.net/pdf?id=BylIciRcYQ The authors establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. The argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately); 2) SGD follows a star-convex path. In such a context, although SGD has long been considered as a randomized algorithm, the paper reveals that it converges in an intrinsically deterministic manner to a global minimum. This should be taken with a grain of salt though. The paper is still under review. The notion of star-convex path gives a hint about towards where the gradient would point at each iteration.
