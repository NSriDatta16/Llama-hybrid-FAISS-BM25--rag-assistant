[site]: crossvalidated
[post_id]: 123748
[parent_id]: 123367
[tags]: 
I will try an answer, even if I am not completely clear about the situation. Formulas will have to be adapted! The problem of estimation of $N$ in the binomial distribution is old, and there are multiple relevant papers. I will give some references at the end. Let there be $R$ regions (in OP example $R=2$ ), with $T$ samples (from disjoint time intervals of equal length) from each region. The observed variables is $x_{it}$ which are independent binomial random variables, each with the distribution $\text{Bin}(N_i,p)$ both unknown. The log-likelihood function becomes $$ \ell ( N_i , p ) = \sum \ln \binom{N_i}{x_{it}} + \ln p \cdot \sum x_{it} + \ln (1-p) \cdot \sum ( N_i - x_{it} ) $$ Note that, in the usual problem when $N_i$ is known so that only $p$ is unknown, then the sum (or the mean) of the binomial counts $x_{it}$ is a sufficient summary, so the analysis can be done in terms of the binomial distribution of the sum. In our problem, however, because of the first term in the log-likelihood function, such is not the case, and the log likelihood depends on each of the counts individually! So what you propose, to reduce to the sum of the counts (over $i$ ), SHOULD NOT BE DONE, as that will lose information (how much, I don't know, but that can be investigated ...). Let us try to understand this a little better. First, we see below that $\max_t(x_{it})$ is a consistent estimator of $N_i$ , but this consistent estimator is not a function of the summed counts. That is one clear indication that summation looses information! Note also that the mean is an unbiased estimator of its expectation which is $N_i p$ , but doesn't seem to hold information about $N_i$ and $p$ individually, when nothing is known about the other parameter. That indicates that if there is useful information about $N_i$ in the likelihood function, that must be contained in the spread of the values $x_{i1}\dots, x_{iT}$ , again indicating that summation is bad. The Olkin et al paper referenced below shows indeed that the method-of-moments estimator in many cases is better than maximum likelihood! and that uses the empirical variance of the $x_{i1}\dots, x_{iT}$ , so couldn't be calculated from the summed data. This problem is known to be unstable. Let us try to understand why. In the usual problem, estimating $p$ when $N_i$ in known, the estimation can be done from some gross characteristic of the data, the mean. When trying to estimate both $N_i$ and $p$ , we use much finer properties of the log-likelihood function (thus of the data). To see why, remember that we can obtain the Poisson distribution as a limit of the binomial when $p$ goes to zero and $N$ grows without bounds, with a constant positive product. So, if $p$ is small and $N$ large, the binomial distribution will be quite close to that limit. Take two cases: (A) $N=100, p=0.01$ , (B) $N=20, p=0.05$ . Draw histograms for the two (binomial) distributions: zapsmall(cbind(0:20, pA, pB)) pA pB [1,] 0 0.366032 0.358486 [2,] 1 0.369730 0.377354 [3,] 2 0.184865 0.188677 [4,] 3 0.060999 0.059582 [5,] 4 0.014942 0.013328 [6,] 5 0.002898 0.002245 [7,] 6 0.000463 0.000295 [8,] 7 0.000063 0.000031 [9,] 8 0.000007 0.000003 [10,] 9 0.000001 0.000000 [11,] 10 0.000000 0.000000 [12,] 11 0.000000 0.000000 [13,] 12 0.000000 0.000000 [14,] 13 0.000000 0.000000 [15,] 14 0.000000 0.000000 [16,] 15 0.000000 0.000000 [17,] 16 0.000000 0.000000 [18,] 17 0.000000 0.000000 [19,] 18 0.000000 0.000000 [20,] 19 0.000000 0.000000 [21,] 20 0.000000 0.000000 Above a table of this probabilities. To detect from observed data which of this two distributions one have, is what it takes to decide, in this case, if $N=100$ or if $N=20$ . It is obviously quite hard, and the instability of the resulting estimators is only to be expected. This example also indicated that the instability is mainly for small $p$ . You say you expect $p$ around 0.7, so the problem might be more stable then. You could investigate that for your data by finding the maximum likelihood estimator as a function of a known $p$ , and plotting that for $p$ in some confidence interval. Or you could go full Bayes, this is a case where even some rather vague prior information could be helpful. The parameters are indeed estimable. It is clear that $N_i \ge \max_t(x_{it})$ , so it is possible to use that maximum count as an estimator of $N$ . That estimator will be strongly consistent, and a parameter with a consistent estimator must be estimable. But, as the above example shows, the estimability is almost a formality; in practice distributions with very different $N$ are very close, so $N$ is very weakly estimable. I will not give details of the estimation methods here, but give a few references you can check out: Ingram Olkin, A John Petkau, James V Zidek: A comparison of N estimators for the Binomial Distribution. JASA 1981. This is a classic paper which develops and analyzes ML and moment estimators, and some stabler variants. It also shows, interestingly, that in many cases the method-of-moments estimator is better than the ML estimator! Raymond J Carrol and F Lombard: A note on N estimators for the binomial distribution. JASA 1985. Develops an alternative, stabler & maybe better estimator, based in integrating $p$ out of the likelihood. Also notes the lack of sufficiency of the summed counts. J Andrew Royle: N_Mixture Models for Estimating Population Size from Spatially Replicated Counts. Biometrics, 2004. This gives another, alternative Bayesian approach which you may try. Back to your concrete question. You SHOULD NOT sum the counts over your two regions! That will lose information. If you introduce $N=N_1 + N_2$ then the log-likelihood function can be written as a function of $N$ , $p$ and $N_1$ (or $N_2$ ). Then the extra parameter $N_1$ should be eliminated by some procedure. I will come back to that, but no there is no time!
