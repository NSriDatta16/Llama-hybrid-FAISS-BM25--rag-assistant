[site]: crossvalidated
[post_id]: 502047
[parent_id]: 
[tags]: 
Neural Network Regularization and derivation

I donâ€™t really understand two concepts in Neural Network. Firstly is the regularization, why the summation is from $l=1$ to $l= L-1$ ? (highlighted in the photo). I understand if it is from $l=0$ to $l=L-1$ , but it starts from $l=1$ which means if we have $4$ layers, we only sum the regularization for $3$ layers. Secondly, for the derivation of the backpropagation, I understand the notation $j$ is for next layer and $k$ is the cuerent layer. But what is "p"? and how can the derivation of $W_{jp}$ over $W_{jk}$ become $\delta_{kp}$ ? The link for full derivation can be found here Thank you so much!
