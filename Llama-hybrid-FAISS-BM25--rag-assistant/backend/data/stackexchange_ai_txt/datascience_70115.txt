[site]: datascience
[post_id]: 70115
[parent_id]: 70075
[tags]: 
Question 1: To do so, I would use the Gensim wrapper of FastText because Gensim has a predict_output_word which does exactly what you want. Given a list of context words, it provides the most fitting words. Question 2: It is up to the user. FastText isn't inherently CBOW or Skipgram. See this Question 3: Yes, even though CBOW and SkipGram are different training procedures, they share a common goal. Both will generate word embeddings where (hopefully) words that are semantically close also have embeddings that are close. The main difference between SkipGram and CBOW is the inherent heuristic used for semantic closeness.
