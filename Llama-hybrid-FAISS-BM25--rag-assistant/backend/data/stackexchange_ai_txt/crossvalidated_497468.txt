[site]: crossvalidated
[post_id]: 497468
[parent_id]: 469226
[tags]: 
as I understand, BPE is just used for word segmentation (compress size of the dictionary) so how will it produce a numeric vector? BPE(an illustration can be found here ) (and unigram language model) is a subword algorithm, and it just tokenizes the text into subwords which can be just treated as words separated by spaces because they will be transformed into ordinal ids by a mapping dictionary or (feature) hashing. The embedding matrix will be trainable(when you pretrain the model) or untrainable(in some conditions for instance transfer learning afterwards for some downstream tasks). I understand that two embedding layers could be shared but why "the pre-softmax linear transformation" should be the same matrix to the embedding? It doesn't make sense to me In this paper: Using the Output Embedding to Improve Language Models the author found that sharing the parameters would benefit the performance. The author also found that in Word2vec the $W'$ , which is also an embedding matrix, is kind of inferior to the standard embedding matrix $W$ . Some guys tried to tie these two matrices but that leads to the detriment. But in large neural network language models, such parameter sharing is reportedly benefitial. Source: https://stackoverflow.com/a/42392407/3552975
