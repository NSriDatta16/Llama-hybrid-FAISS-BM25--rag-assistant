[site]: crossvalidated
[post_id]: 37479
[parent_id]: 37411
[tags]: 
As Robert put it correctly, Accuracy is the way to go. I just want to add that it is possible to do calculate it with ROCR. Take a look at help(performance) to select different measures. For example, in ROCR only one decision threshold is used which is called cutoff . The following code plots accuracy vs cutoff and extracts the cutoff for maximum accuracy. require(ROCR) # Prepare data for plotting data(ROCR.simple) pred which results in To operate with two thresholds in order to create a middle region of uncertainty (which is a valid way to go if the circumstances / target application allows it) one can create two performance objects with ROCR cutoff vs True Positive Rate (tpr) aka precision for the positive class cutoff vs True Negative Rate (tnr) aka precision for the negative class Select a suitable cutoff from the performance vectors (using the R method which) and combine them to achieve the desired balance. This should be straightforward, hence I leave it as an exercise to the reader. One last note: What is the difference between Accuracy and calculating precision for both classes separately and e.g. combine them in a (weighted) average? Accuracy calculates a weighted average, where the weight for class c is equivalent to number of instances with class c. This means that if you suffer a heavy class skew (98% negatives for example) on can simply "optimize" the accuracy by setting predict the label negative for all instances. In such a case a non-weighted plain average of both class precisions prevents the gaming of the metric. In the case of a balanced classes both calculation methods lead of course to the same result.
