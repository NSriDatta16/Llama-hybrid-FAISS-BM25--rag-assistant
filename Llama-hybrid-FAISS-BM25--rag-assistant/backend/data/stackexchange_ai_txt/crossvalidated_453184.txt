[site]: crossvalidated
[post_id]: 453184
[parent_id]: 453164
[tags]: 
In economic (mostly macroeconomic) and financial application, it is not uncommon to have time series of different lengths. Some variables have been recorded for a longer time than other. This way you get a data matrix with ragged edges at the top (the oldest observations). It is perhaps less common to have ragged edges at the bottom (the newest observations), but it happens e.g. in macroeconomics where the newest observations are reported at different times for different variables during a quarter or a month. So within a quarter or a month, you will already have observations of some variables but not others. Similar examples can be found in finance, just the frequency of the time series is typically higher. Time series are usually trimmed by cutting the oldest (and less often, newest) observations from variables that have more old (new) observations than other variables. This way one achieves a data matrix with flat top (bottom) that* can be readily used for estimation with least-squares based routines (such as feasible GLS or equation by equation OLS). This is your option 1. Zero-padding (your option 2) is tricking yourself; you note correctly that it messes up the coefficients. I would not recommend that and I have not seen it done in serious work (I am mostly familiar with macroeconomics and finance). Imputations here and there (option 3) also mess up inference, but it may be worth it. When the number of time series is large and quite a few of them have a few missing observations at different time periods, suddenly there are lots of lines with missing data. If deleted, they can substantially reduce the sample size. Moreover, they bring other lines with them; a single missing data point is part of lagged time series for different lags and as such lies in different lines of the data matrix augmented by lags. So sometimes people do such imputation at the expense of messing the inference a bit. There are estimation routines (EM algorithm) and frameworks (Bayesian) where keeping rows with missing data does not mess up inference as the missing data points are not used as data for estimating coefficients but are only estimated from the model. This should work fine for missing data points here and there, but I do not think you would gain any estimation precision by applying them to cases with severely ragged top or bottom of the data matrix. *when augmented with lags (which again requires trimming the oldest observations from the lower-order lags to remove a ragged top edge and achieve a flat-top matrix)
