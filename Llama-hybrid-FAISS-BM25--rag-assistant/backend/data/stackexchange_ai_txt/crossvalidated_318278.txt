[site]: crossvalidated
[post_id]: 318278
[parent_id]: 
[tags]: 
Generalized additive models (GAMs), interactions, and covariates

I've been exploring a number of tools for forecasting, and have found Generalized Additive Models (GAMs) to have the most potential for this purpose. GAMs are great! They allow for complex models to be specified very succinctly. However, that same succinctness is causing me some confusion, specifically in regard to how GAMs conceive of interaction terms and covariates. Consider an example data set (reproducible code at end of post) in which y is a monotonic function perturbed by a couple of gaussians, plus some noise: The data set has a few predictor variables: x : The index of the data (1-100). w : A secondary feature that marks out the sections of y where a gaussian is present. w has values of 1-20 where x is between 11 and 30, and 51 to 70. Otherwise, w is 0. w2 : w + 1 , so that there are no 0 values. R's mgcv package makes it easy to specify a number of possible models for these data: Models 1 and 2 are fairly intuitive. Predicting y only from the index value in x at default smoothness produces something vaguely correct, but too smooth. Predicting y only from w results in a model of the "average gaussian" present in y , and no "awareness" of the other data points, all of which have a w value of 0. Model 3 uses both x and w as 1D smooths, producing a nice fit. Model 4 uses x and w in a 2D smooth, also giving a nice fit. These two models are very similar, though not identical. Model 5 models x "by" w . Model 6 does the opposite. mgcv 's documentation states that "the by argument ensures that the smooth function gets multiplied by [the covariate given in the 'by' argument]". So shouldn't Models 5 & 6 be equivalent? Models 7 and 8 use one of the predictors as a linear term. These make intuitive sense to me, as they are simply doing what a GLM would do with these predictors, and then adding the effect to the rest of the model. Lastly, Model 9 is the same as Model 5, except that x is smoothed "by" w2 (which is w + 1 ). What's strange to me here is that the absence of zeros in w2 produces a remarkably different effect in the "by" interaction. So, my questions are these: What is the difference between the specifications in Models 3 and 4? Is there some other example that would draw out the difference more clearly? What, exactly, is "by" doing here? Much of what I've read in Wood's book and this website suggests that "by" produces a multiplicative effect, but I'm having trouble grasping the intuition of it. Why would there be such a notable difference between Models 5 and 9? Reprex follows, written in R. library(magrittr) library(tidyverse) library(mgcv) set.seed(1222) data.ex % mutate(facet = sprintf('%i: %s', model, formula)) %>% ggplot(data = ., aes(x = x, y = y)) + geom_point() + geom_line(aes(y = predicted), color = 'red') + facet_wrap(facets = ~facet) print(plot.models)
