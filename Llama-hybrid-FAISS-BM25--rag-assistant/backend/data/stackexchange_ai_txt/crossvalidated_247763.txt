[site]: crossvalidated
[post_id]: 247763
[parent_id]: 
[tags]: 
Activation function for forward propagation

I am building a single layer recurrent neural network and I am training on large sets of pure text (ASCII characters only). I would like to use softmax for the output layer and Tanh for the hidden layer. However, I am not sure where to start to derive the proper activation equations at time step $t$ for the hidden and output layer during forward propagation. For instance, with regards to the softmax, is it $P(y=j|\mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}$ or should I be using something different? The reason I don't think it is this equation is because this does not include the time step.
