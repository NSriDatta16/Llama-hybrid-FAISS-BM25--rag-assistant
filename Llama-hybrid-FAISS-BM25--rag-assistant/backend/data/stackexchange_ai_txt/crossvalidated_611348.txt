[site]: crossvalidated
[post_id]: 611348
[parent_id]: 611221
[tags]: 
In Bayesian terminology, the marginal likelihood is the prior predictive density $$m(x)=\int_\Theta f(x|\theta)\pi(\theta)\,\text d\theta$$ where $f(\cdot|\cdot)$ is the sampling density, $\ell(\theta|x)=f(x|\theta)$ is the standard likelihood. This marginal likelihood integrates to one over the sample space $\mathcal X$ : $$\int_\mathcal X m(x)\,\text dx=\int_\mathcal X \int_\Theta f(x|\theta)\pi(\theta)\,\text d\theta\,\text dx=1$$ It is also the normalising factor for the posterior density $$\pi(\theta|x) = \frac{f(x|\theta)\pi(\theta)}{m(x)}$$ and simulating from the posterior is feasible without deriving $m(x)$ , provided $f(x|\theta)$ is known up to a constant (that is, not a constant indexed by $\theta$ ). This is also the case for tempered versions of the posterior density, e.g. $$\pi_i(\theta|x) = \frac{f(x|\theta)^{\tau_i}\pi(\theta)}{m_i(x)}\quad 0\le\tau_i\le1$$ SMC and other Monte Carlo methods (nested sampling, bridge sampling, harmonic mean, umbrella sampling, path sampling, &tc.) exploit a sample from $\pi_i(\theta|x)$ or from another distribution to approximate the normalising constant $m_i(x)$ . When comparing several models through marginal likelihood ratios, the corresponding (standard) likelihoods must all be completely available (or up to the same multiplicative constant).
