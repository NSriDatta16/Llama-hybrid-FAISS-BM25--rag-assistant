[site]: datascience
[post_id]: 103912
[parent_id]: 103898
[tags]: 
Randomized sparse models: Random forest and boosting tree already have the feature (column) sampling, are two effects repeated? This question is unclear. Random Forest and GBDT are not randomized sparse models. Unless it is a nomenclature that I am not familiar about. In the random forest doc , max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto” The number of features to consider when looking for the best split: This is the feature selection of random forest, it also avoids overfitting. It also appears in GBDT Recursive feature elimination: Does it something like the stepwise regression? For the info about RFE you can have a look at sklearn docs Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. About your question, you are making some unjustified claims that are not true Tree model are used to select the importance of features by Mean decrease impurity(let's ignore Permutation) -- Tree models are not used to select importance of features, but they are predictors. You can use them later to see what are the most important features that they have selected. And more....
