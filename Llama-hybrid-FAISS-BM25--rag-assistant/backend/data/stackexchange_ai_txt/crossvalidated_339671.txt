[site]: crossvalidated
[post_id]: 339671
[parent_id]: 
[tags]: 
Significance test of model performance

I have a data set with 350,000 entries and three models: a feedforward neural network, a recurrent neural network and a LSTM. To determine how well each model performs I used K-fold cross validation with k = 8. The error metric that I preferably use is the median absolute percentage error (MdAPE) (because of the presence of very large outliers) or otherwise the mean absolute percentage errorr (MAPE). The distributions of the percentages errors are furthermore heavily positively skewed for all models. I now want to know whether the performance of the models differs significantly. However, I do not know if I should use the aggregated percentage errors of the predictions on the test set as the sample for the significance test (meaning sample size = 350,000) or the median (or mean) of the percentage errors of each fold (meaning sample size = 8). In the former, the sample size is enormous leading almost certainly to a significant result, while in the latter the sample size is extremely small. Also, after selecting on the composition of the sample (and possibly the metric), what would be the preferred significance test?
