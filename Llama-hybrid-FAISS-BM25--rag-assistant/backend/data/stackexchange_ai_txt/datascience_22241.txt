[site]: datascience
[post_id]: 22241
[parent_id]: 22153
[tags]: 
Given 35 numeric features and 3 classes, perhaps first try a variant of SVM or random forest to get a handle on the data - neither of these are deep learning, but they are fast and good for benchmarking/troubleshooting: SVMs: @Media mentions a linear support vector machine, but if your data isn't linearly separable then you can also employ a nonlinear kernel or soft margin classifier: http://scikit-learn.org/stable/modules/svm.html Random Forests: an ensemble method fitting decision trees to your data, there are a ton of variants (bagging, boosting): https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm If you've already done that, you could use a restricted boltzmann machine to build a generative model of the data, or an unsupervised algorithm like an autoencoder or self-organising map to reduce the dimensionality of the data, then use that as the input to your MLP/SVM/softmax classifier. Restricted Boltzmann Machines: good brief description of workflow here How to use RBM for classification? and more detail on training from Hinton here: https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf Self-organising Maps: Good page on what: http://www.ai-junkie.com/ann/som/som1.html and implemented in tensorflow here: https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/ but of course you can choose the structure of your output. There are other options out there but it might be worth finding out what sort of results you get on the faster and more explanatory algorithms before building a full CNN etc.
