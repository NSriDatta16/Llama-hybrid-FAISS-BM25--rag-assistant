[site]: datascience
[post_id]: 116975
[parent_id]: 
[tags]: 
Tips for making an autoencoder converge?

I am currently trying to make an autoencoder that compresses a 3D volume where each value represents the density of said volume. The architecture is a UNet without skip connections. The optimizer is AdamW with a learning rate of 0.0002 and beta1 of 0.5. I use three losses: MSE between the input and reconstructed, the 3d frobenius norm of the reconstructed density and a segmentation loss where I want to areas with non-zeros density to match. My issue is that the autoencoder does not seem to converge. After a few epochs, it will get results that are closer to the initial input, but if I train it for too long, the loss shoots up and the output looks nothing like the input. From your experience, what could be causing this? What are the tools/tips that I can use to mitigate non-convergence? What questions should I be asking myself? Here's a plot of the MSE error for my validation set.
