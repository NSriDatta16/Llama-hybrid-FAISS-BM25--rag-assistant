[site]: crossvalidated
[post_id]: 515741
[parent_id]: 
[tags]: 
Explainable neural network through supervision of penultimate layer

Generally, most deep learning (DL) models are considered opaque black boxes, and post-hoc explanations may not be satisfactory to users, especially for use cases in the legal or clinical world where users may need to then justify their belief in the DL model to other experts. While it's exciting to think about discovering completely unexpectedly important predictors, most likely, explanations would be most trusted if it aligns with the domain knowledge/mental model of users. So I started thinking about how one could incorporate "plausible" explanations derived from domain knowledge into the DL model. For simplicity's sake, say the primary task is binary classification, as are the set of explanatory tasks. The output of the last layer would be compared to the primary task and the outputs of the second-to-last layer would be compared to the explanatory tasks. Is it proper to simply add (with some weighting schema) the losses from the primary task (last layer) and the explanatory tasks (intermediate layer) and backprop or do losses based on intermediate layers require special treatment? Would you "buy" the proposition that for any given sample, the output of the second-to-last layer multiplied by their respective weights in the last layer is a measure of feature importance of the respective "plausible explanations"?
