[site]: crossvalidated
[post_id]: 593615
[parent_id]: 
[tags]: 
Does image data augmentation make sense when fine-tuning a transformer-based encoder-decoder model (Donut) on a small dataset (~100 samples)?

I am trying to fine-tune Donut model on ~100 (training) labelled data samples (pairs of images and json files). (Donut is a transformer-based encoder-decoder model, the encoder encodes images and the decoder takes the input of the encoder and the textual tokens to predict next textual tokens.) Will the classical image data augmentation techniques (rotation, translation, shearing) help in this case to improve on the standard evaluation metrics (e.g. F1 score)? On one hand, I would imagine that if such image augmentation would be useful, they would mention it in their paper and/or provide implementation for such augmentation in their GitHub repo. I can also imagine that it might cause some overfitting on the labels in the training set, but I am not sure. On the other hand, by having a quick glance at this paper , they do mention image data augmentation as an valuable technique for improving the model's performance. I am just interested in the intuition about this -- does it make sense to peform image data augmentation in this case? Any thoughts would be appreciated!
