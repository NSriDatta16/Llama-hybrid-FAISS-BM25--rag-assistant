[site]: crossvalidated
[post_id]: 515654
[parent_id]: 
[tags]: 
How can we introduce an anomaly class as a positive class to sklearn IsolationForest?

I inspired by this notebook , and I'm experimenting IsolationForest (IF) algorithm using scikit-learn==0.22.2.post1 for anomaly detection context on the SF version of KDDCUP99 dataset , including 4 attributes. The data is directly fetched from sklearn and after preprocessing (label encoding the categorical feature) passed to the IF algorithm with the default setup. Perhaps we can call our task is binary classification. The full code is here . There are two possibilities to demonstrate and evaluate IF results in the context of Anomaly/outlier detection. I think option A is my case, and I expect that TP+FN = Anomaly/positive class and TN+FP = Normal/negative class . Problem : If you consider the following results on testset I realized that Sklearn IF algorithm treats normal data as positive class by default while the anomaly class would be better to be considered as positive class for evaluation via confusion matrix . This result in incorrect output in confusion matrix as it is mentioned here and here . You see that even though I tried IF with the real anomaly rate is 0.5% from documentation( I set contamination=0.005 ), the number of FP surprisingly increased (maybe it is FN )!! The legend on the left side has been designed based on results of confusion_matrix (from documentation example ) as it is printed at the bottom of each confusion matrix in the above picture. tn, fp, fn, tp = confusion_matrix(y_test_sf, y_pred_test).ravel() print("TN: ",tn,"\nFP: ", fp,"\nFN: " ,fn,"\nTP: ", tp) print("TP+FN: ",tp+fn,"\nTN+FP: ", tn+fp) Quick solution: is to introduce a legend for the confusion matrix, not to confuse us more! Expected solution: is to understand how to adjust the IF algorithm from the root to treat the Anomaly class as a positive class . I don't need to include legend and other calculation like TP+FN = Anomaly class and TN+FP = Normal class per se. I'm not sure it is the right way to force it to see those data with labels of -1 normal, which are Anomaly and vice versa. my last try was to play with pos_label=-1 due to Score is biased towards the Anomaly class like we do when we configure GridSearchCV : scoring = {'AUC': 'roc_auc', 'f1_score': make_scorer(f1_score, pos_label=-1)} Probably I am missing something here, and perhaps there is an elegant way to fix it, and any help will be highly appreciated.
