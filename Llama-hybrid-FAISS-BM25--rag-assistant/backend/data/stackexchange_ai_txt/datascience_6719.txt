[site]: datascience
[post_id]: 6719
[parent_id]: 
[tags]: 
Value Updation Dynamic Programming Reinforcement learning

Regarding Value Iteration of Dynamic Programming(reinforcement learning) in grid world, the value updation of each state is given by: Now Suppose i am in say box (3,2). I can go to (4,2)(up) (3,3)(right) and (1,3)(left) and none of these are my final state so i get a reward of -0.1 for going in each of the states. The present value of all states are 0. The probability of going north is 0.8, and going left/right is 0.1 each. So since going left/right gives me more reward(as reward*probability will be negative) i go left or right. Is this the mechanism. Am I correct? But In the formula there is a summation term given. So I basically cannot understand this formula. Can anyone explain me with an example?
