[site]: crossvalidated
[post_id]: 73651
[parent_id]: 73646
[tags]: 
(Answer partially updated in 2023.) This is a very hard problem in general, though your variables are apparently only 1d so that helps. Of course, the first step (when possible) should be to plot the data and see if anything pops out at you; you're in 2d so this should be easy. Here are a few approaches that work in $\mathbb{R}^d$ or even more general settings, to match the general title of the question. One general category is, related to the suggestion here, to estimate the mutual information: Estimate mutual information via entropies, as mentioned. In low dimensions with sufficient samples, histograms / KDE / nearest-neighbour estimators should work okay, but expect them to behave very poorly as the dimension increases. In particular, the following simple estimator has finite-sample bounds (compared to most approaches' asymptotic-only properties): Sricharan, Raich, and Hero. Empirical estimation of entropy functionals with confidence. arXiv:1012.4188 [math.ST] Similar direct estimators of mutual information, e.g. the following based on nearest neighbours: Pál, Póczos, and Svepesári. Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs , NeurIPS 2010. Variational estimators of mutual information, based on optimizing some function parameterized typically as a neural network; this is probably the "default" modern approach in high dimensions. The following paper gives a nice overview of the relationship between various estimators. Be aware, however, that these approaches are highly dependent on the neural network class and optimization scheme, and can have particularly surprising behaviour in their bias/variance tradeoffs . Poole, Ozair, van den Oord, Alemi, and Tucker. On Variational Bounds of Mutual Information , ICML 2019. There are also other approaches, based on measures other than the mutual information. The Schweizer-Wolff approach is a classic one based on copula transformations, and so is invariant to monotone increasing transformations. I'm not very familiar with this one, but I think it's computationally simpler but also maybe less powerful than most of the other approaches here. (I vaguely expect it can be framed as a special case of some of the other approaches but haven't really thought about it.) Schweizer and Wolff, On Nonparametric Measures of Dependence for Random Variables , Annals of Statistics 1981. The Hilbert-Schmidt independence criterion (HSIC): a kernel (in the sense of RKHS, not KDE)-based approach, based on measuring the norm of $\operatorname{Cov}(\phi(X), \psi(Y))$ for kernel features $\phi$ and $\psi$ . In fact, the HSIC with kernels defined by a deep network is related to one of the more common variational estimators, InfoNCE; see discussion here . Gretton, Bousqet, Smola, and Schölkopf, Measuring Statistical Independence with Hilbert-Schmidt Norms , Algorithmic Learning Theory 2005. Statisticians are probably more familiar with the distance covariance/correlation as mentioned here previously; this is in fact a special case of the HSIC with a particular choice of kernel, but that choice is maybe often a better kernel choice than the default Gaussian kernel typically used for HSIC. Székely, Rizzo, and Bakirov, Measuring and testing dependence by correlation of distances , Annals of Statistics 2007.
