[site]: crossvalidated
[post_id]: 417466
[parent_id]: 
[tags]: 
Why consider the variance rather than the entropy of estimators?

It is a rather common thing to be concerned with the variance of an estimator . For instance, confidence intervals for the mean can be constructed based on the standard error. Often, however, we look at minimizing the variance of an estimator . This leads us to the minimum variance unbiased estimator , for example, or to the use of control variates or baselines (in reinforcement learning and optimal control). In machine learning, the variance of the stochastic gradient estimator is a major concern as well (especially when dealing with those from e.g. REINFORCE ). My question is why we look at the variance specifically, rather than the entropy , especially when minimization is concerned. I'd be interested in both practical and theoretical reasons. Ideally, we want the estimator to form a low entropy Dirac delta around the true value (or if the target is random, closely approximate it with zero density in areas away from the target's distribution). I feel that entropy is a more principled measure of "noisiness" and uncertainty than variance (for instance, an answer here has a few reasons for this). I can think of two reasons why we are concerned with the variance rather than the entropy: Entropy is hard to estimate. This matters for practical rather than theoretical reasons, I suppose. Variance gives us uncertainty/dispersion that is (a) about the mean and (b) in the same units (once square rooted anyway) as the quantity itself. Since the mean is usually the estimator itself, the variance measures how spread out the data are from it. For (b), being in the same units lets us relate the uncertainty back to the original values. Nevertheless, it's not clear either of these matter for optimization. Are there other reasons to use variance, or conversely, some interesting reasons why looking at entropy makes sense?
