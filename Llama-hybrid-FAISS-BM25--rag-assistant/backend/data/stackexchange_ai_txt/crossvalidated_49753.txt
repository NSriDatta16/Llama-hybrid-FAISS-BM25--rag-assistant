[site]: crossvalidated
[post_id]: 49753
[parent_id]: 49692
[tags]: 
The main reason is that the k-fold cross-validation estimator has a lower variance than a single hold-out set estimator, which can be very important if the amount of data available is limited. If you have a single hold out set, where 90% of data are used for training and 10% used for testing, the test set is very small, so there will be a lot of variation in the performance estimate for different samples of data, or for different partitions of the data to form training and test sets. k-fold validation reduces this variance by averaging over k different partitions, so the performance estimate is less sensitive to the partitioning of the data. You can go even further by repeated k-fold cross-validation, where the cross-validation is performed using different partitionings of the data to form k sub-sets, and then taking the average over that as well. Note however, all steps of the model fitting procedure (model selection, feature selection etc.) must be performed independently in each fold of the cross-validation procedure, or the resulting performance estimate will be optimistically biased.
