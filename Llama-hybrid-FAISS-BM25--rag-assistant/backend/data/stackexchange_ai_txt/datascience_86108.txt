[site]: datascience
[post_id]: 86108
[parent_id]: 86104
[tags]: 
The name provides a clue. BERT (Bidirectional Encoder Representations from Transformers): So basically BERT = Transformer Minus the Decoder BERT ends with the final representation of the words after the encoder is done processing it. In Transformer, the above is used in the decoder. That piece of architecture is not there in BERT
