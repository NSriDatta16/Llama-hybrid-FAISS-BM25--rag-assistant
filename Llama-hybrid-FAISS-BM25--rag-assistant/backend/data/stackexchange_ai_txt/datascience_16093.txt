[site]: datascience
[post_id]: 16093
[parent_id]: 16087
[tags]: 
"Premature optimization is the root of all evil". --Knuth You could do this, but why? You definitely don't want to do this, if you're going to feed the result into a classifier: most classifiers will perform worse after this transformation. There's no point to do this, if you're trying to save space in a database: hard disks can store hundreds of gigabytes, so a measly 15 columns probably won't make any noticeable difference. 15 additional columns is tiny . Just save the additional 15 columns and spare yourself headaches. The cost of your time to program this up and troubleshoot all of the problems it causes downstream will almost surely exceed the miniscule cost of computation or storage to store the data in the natural format. And if you seriously have big data where you truly need to do some optimization, the very first step is to measure : measure how much space/time you're actually consuming, and what the dominant contributors to that total cost is, and then focus on optimizing those dominant factors. Optimizations should be guided by data ; otherwise you risk implementing complicated optimizations that make little difference overall but add needless complexity and epicycles to your data processing workflow.
