[site]: datascience
[post_id]: 66665
[parent_id]: 66635
[tags]: 
My guess: I agree with your colleagues. I see no reason to do anything other than a single neural network with multiple outputs. If necessary, increase the capacity of that single neural network until you see no further improvement. An stacking ensemble where you have a few neural nets whose inputs are fed as input into another neural network is itself equivalent to one bigger neural net. I don't see any reason to expect the stacking approach to train more effectively or to be better for some other reason. However, this is ultimately an empirical field. The only way to find out what will work best is to try different approaches and see. There's not a huge amount of theory that enables us to make predictions about what approaches will be most effective. You might consider whether you can obtain more training data, particularly in the regimes of particular interest. Often better training data sets offer higher gains than modifying the neural network architecture.
