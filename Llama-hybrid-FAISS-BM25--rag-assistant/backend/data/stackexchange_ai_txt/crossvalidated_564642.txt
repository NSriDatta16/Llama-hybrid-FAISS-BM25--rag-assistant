[site]: crossvalidated
[post_id]: 564642
[parent_id]: 
[tags]: 
Linear Regression and Quantile Regression

Linear regression using the method of least squares estimates the conditional mean of the response variable across values of the predictor variables. Quantile regression estimates a conditional quantile of the response variable across values of the predictor variables. The least squares method minimises the sum of squared residuals, and quantile regression minimises a loss function, $\rho(\tau)$ , that depends on the quantile of interest. I think I understand the methods. My problem: I have a model for the failure times of a material, $\log(T) \sim N(\mu(x),\sigma)$ , where $\mu(x) = \beta_0 + \beta_1 x_1 +\beta x_2 + \dots + \beta_n x_n$ is the mean, $x_1, \dots, x_n$ are covariates, and $\sigma$ is the standard deviation. I usually approach a problem from a Bayesian perspective (well, I call myself a Bayesian but I often implement a model using Stan and uninformative priors so I am not sure if I am classed as a Bayesian). I specified the likelihood and prior distributions and obtained estimates for the model parameters (conditional on the covariates). This worked fine. I am able to obtain any quantile of interest for the (log) failure times from the posterior distribution. I was happy with this analysis until I started to overthink the problem. "When you changed from linear regression to quantile regression you had to change the loss function. You have performed a Bayesian analysis like usual, you must adjust for quantile regression". I think my above thoughts are incorrect. If least squares estimates provided parameter estimates for the log failure times, one could then obtain any estimate of interest. However, least squares estimates the conditional mean $E[\log(T)]$ , and not $\log(T)$ itself. Therefore, I have to change the loss function of interest to $\rho(\tau)$ , if I want the $\tau$ quantile of $\log(T)$ . Another loss function would be required if I want the $\tau^*$ quantile. This is because these methods estimate quantities of interest only, and not the random variable of interest. The Bayesian approach (I am not saying only a Bayesian approach can do this. ML estimates with bootstrap or something would also provide estimates with uncertainty for $\log(T)$ . I am comparing a Bayesian approach to least squares and quantile regression.) estimates the distribution of $\log(T)$ and not of $E[\log(T)]$ , and hence I do not need to adjust anything, like when going from linear regression to quantile regression. Can anyone please confirm my understanding. I think I was just overthinking for a moment because I had to adjust the loss function when moving from linear regression to quantile regression and thought "a standard Bayesian approach must need to be adjusted when you're interested in quantile regression". I hope this makes sense.
