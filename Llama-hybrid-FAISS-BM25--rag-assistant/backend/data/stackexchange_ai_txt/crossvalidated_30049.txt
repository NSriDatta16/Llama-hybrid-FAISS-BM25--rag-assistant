[site]: crossvalidated
[post_id]: 30049
[parent_id]: 30033
[tags]: 
As your data likely is very noisy, you can try to improve the PCA performance using a robust variation of it, see Wikipedia for details. But in general I do share your concern. Because in complex data sets such as genetic data, different clusters may show different correlations that cannot be adequately represented by global PCA. The quality of using PCA for dimensionality reduction (e.g. to 2D or 3D for visualization) does depend a lot on the amount of variance captured. But you can't go by the direct relative shares. If we have 1000 dimensions, and the first two explain $10\%$ this can (I did not test this) be quite significant. In 10 dimensions, it is completely meaningless, for uniform i.i.d. data the first single eigenvector will necessarily already explain more than this. A better control is the value $$\frac{\text{explained variance}}{\text{expected explained variance}}$$. Just a few days ago I posted a question here about the expected distribution of eigenvalues. If we find some distribution for this, we can test whether the result is significant: Estimated distribution of eigenvalues for i.i.d. (uniform or normal) data If you look at the example I posted, it is not unusual to see eigenvalues range from $0.119$ to $0.006$ (a difference of factor 20!) in a uniform i.i.d. dataset with just 20 dimensions, at least when the sample is small. So the eigenvalues seem to be rather unreliable when it comes to indicating whether the projection is actually capturing something. Feature selection will not cover rotations. Which is when PCA gets interesting: did it actually rotate the data much, or did it just select a number of features (i.e. low angle between one of the axes and one of the eigenvectors)? Try plotting the axes of the original attributes in your visualization to show the relationship to the original data and the attributes used by PCA.
