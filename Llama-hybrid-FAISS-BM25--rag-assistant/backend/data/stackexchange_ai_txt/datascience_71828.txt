[site]: datascience
[post_id]: 71828
[parent_id]: 
[tags]: 
Negative correlation between OOB statistics and test set statistics during tuning of a RandomForest

I am tuning the parameters of a binary random forest classifier using a random search with a priority queue for training. After training with a fixed number of estimators (3000), the strategy is to reduce priority (and eventually pause) of configurations with performances lower than the median of all the other running configurations with the same number of estimators, while I keep adding new estimators to the best performing configurations, until a total number of estimators is reached. The maximum number of estimators is used to constraint computational requirements and the scheduling is used as a means to invest computational resources in the most promising configurations earlier. As the performance criterion I use the average precision score computed on the OOB predictions, I also check the average precision of the trained models on a held out test set. I have run this setup several times, but every single time the OOB score and the test score are negatively correlated, meaning that the better a model performs on the OOB, the worse performs on a held out set. The thing is also the opposite is true!!! In the picture, the color is according to the OOB average precision. I thought that this phenomenon would be caused by an unfortunate train/test split, but even with a random split for each model this still happens. I also run a smaller training experiment in which I used a 10-fold cross-validated average precision score instead of oob, but it is still negatively correlated to the test statistic. What do you think could be the cause of this behaviour?
