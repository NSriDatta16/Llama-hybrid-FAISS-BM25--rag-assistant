[site]: crossvalidated
[post_id]: 620711
[parent_id]: 620661
[tags]: 
The term "Random Forest" is broad and describes simply an ensemble of randomized decision trees . "Ensemble" means that you somehow construct individual models (here: trees) and then somehow combine their predictions. "Randomized" means two things Unlike Decision Trees, we do not consider all possibilities when looking for a split direction/dimension and threshold for a node. Instead, we pick at random a sample of candidate split directions and evaluate only these. Each individual tree is grown not on the full dataset but a subsample of it. The randomization means that individual trees will be diverse and uncorrelated. This is essentially the reason why Random Forests can outperform Decision Trees. As you already indicated, a Random Forest prediction is the combination of the individual tree's predictions. This can be done using majority voting or another method, depending on the setup. Usually, it will be some form of mean, however (arithmetic, geometric, harmonic, ...). How does majority voting fit in there? You can understand these means as centroids. And the majority vote is the centroid w.r.t. to the 0-1-Loss / symmetric difference metric. So, no, as far as I know in conventional RFs there is no other mechanism that uses concepts of Entropy or Information Gain. As for learning sources, one standard reference that is often recommended is "Elements of Statistical Learning", available online.
