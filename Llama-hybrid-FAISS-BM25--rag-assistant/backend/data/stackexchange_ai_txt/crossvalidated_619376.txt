[site]: crossvalidated
[post_id]: 619376
[parent_id]: 
[tags]: 
How normalizing data cause not problem in prediction?

In algorithms that perform better with data normalization or deep learning problems such as classification, how normalizing data does not bias our algorithm? I mean, in training or even testing, we can normalize data and columns with very big values compared to others, will become like them. But, in prediction for one query, how does algorithm can predicts correctly without any bias, with such a big value compared to others? Does it even make sense or I am missing sth here?
