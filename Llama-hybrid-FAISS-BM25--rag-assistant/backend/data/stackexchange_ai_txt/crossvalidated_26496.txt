[site]: crossvalidated
[post_id]: 26496
[parent_id]: 26144
[tags]: 
1 . Should I still scale the input features using feature scaling? What range? Scaling does not make anything worse. Read this answer from Sarle's neural network FAQ: Subject: Should I normalize/standardize/rescale the data? . 2 . What transformation function should I use in place of the sigmoid? You could use logistic sigmoid or tanh as activation function. That doesn't matter. You don't have to change the learning algorithm. You just have to scale the outputs of your training set down to the range of the output layer activation function ($[0,1]$ or $[-1,1]$) and when you trained your network, you have to scale the output of your network to $[-5,5]$. You really don't have to change anything else.
