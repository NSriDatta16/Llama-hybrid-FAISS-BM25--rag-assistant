[site]: crossvalidated
[post_id]: 291359
[parent_id]: 198629
[tags]: 
I think the authors are simply using the word "overfitting" to refer to overparametrizing. In the case of neural networks, overfitting is a consequence of overtraining an overparametrized (i.e. overly complex) model. As far as I can tell, there is no difference between an overtrained and an overfitted model, insofar as the prefix "over" already implies that a line has been crossed. Moreover, "fit" and "training" are basically the same thing. A good indication of this is the fact that Wikipedia redirects you to overfitting when you search for overtraining. Overfitting can occur when the model is too complex. However, a very complex but well regularized model can still do well, as the function space it searches within is restricted or penalized in certain regions. In fact, some regard deep neural networks as overparametrized but carefully trained and regularized models. Cross validation can help you get a more or less reliable estimate of how your model is likely to do in the wild. More than avoiding overfitting, it prevents you from relying on the overly optimistic prospect brought by the error obtained by an overfitted model on the training set. It can also help you find a set of hyperparameters that will prevent your model from overfitting, by trying to get good performance on the validation splits.
