[site]: crossvalidated
[post_id]: 581014
[parent_id]: 
[tags]: 
What do flattening learning curves indicate and when to stop training of a ML model in that case?

I am training CNNs for image segmentation on a limited dataset and apply some on-the-fly data augmentation. I measure mean intersection over union (mean IoU) to evaluate the training and select models. This metric increases at the beginning but it slows down strongly and almost stagnates after a while (see TensorBoard or figures below). However, it appears to continue growing (although very slowly), presumably for a long time to come. At this point (300 epochs and about 15 h into the training), it does not make the impression that we soon reach a point where clear over fitting becomes visible. When I look at validation loss, however, the point of over fitting seems to haven been passed 250 epochs ago. It should be noted, though, that loss is weighed categorical crossentropy and class frequencies are very different between training and validation data. Where do I draw the line now? Am I underfitting the model by cutting the training at 300 epochs, or is it already overfitting? In case it is underfitting, does it make sense to wait until validation mean IoU starts to decrease, or would you expect the remaining increase of validation mean IoU negligible (since the curve becomes almost horizontal towards the end)? Fig. 1 Mean IoU for training (orange) and validation data (blue) Fig. 2 Epoch loss for training (orange) and validation data (blue)
