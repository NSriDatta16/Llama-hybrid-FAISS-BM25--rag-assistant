[site]: crossvalidated
[post_id]: 617662
[parent_id]: 
[tags]: 
Model returns low training F1 Score, but high Testing and Validation F1 score

I am currently working on a very imbalanced dataset: 24 million transactions (rows of data) 30,000 fraudulent transactions (0.1% of total transactions) and I am using XGBoost as the model to predict whether a transaction is fraudulent or not. After tuning some hyperparameters via optuna, I have received such results F1 Score on Training Data : 0.57417479049085 F1 Score on Testing Data : 0.8719438392641008 PR AUC score on Training Data : 0.9918559271777408 PR AUC score on Testing Data : 0.9077624174590952 Training report precision recall f1-score support 0 1.00 1.00 1.00 20579668 1 0.47 1.00 0.64 25179 accuracy 1.00 20604847 macro avg 0.73 1.00 0.82 20604847 weighted avg 1.00 1.00 1.00 20604847 Test report precision recall f1-score support 0 1.00 1.00 1.00 2058351 1 0.83 0.93 0.87 2087 accuracy 1.00 2060438 macro avg 0.91 0.96 0.94 2060438 weighted avg 1.00 1.00 1.00 2060438 The following is my loss, learning curves and classification matrix Loss data validation_0 is the trainingset, validation_1 is the testing set [0] validation_0-aucpr:0.75831 validation_0-logloss:0.67418 validation_1-aucpr:0.17989 validation_1-logloss:0.67417 [10] validation_0-aucpr:0.78157 validation_0-logloss:0.52305 validation_1-aucpr:0.42574 validation_1-logloss:0.51965 [20] validation_0-aucpr:0.83228 validation_0-logloss:0.41181 validation_1-aucpr:0.79299 validation_1-logloss:0.40593 [30] validation_0-aucpr:0.84335 validation_0-logloss:0.32956 validation_1-aucpr:0.82845 validation_1-logloss:0.32171 [40] validation_0-aucpr:0.86026 validation_0-logloss:0.26683 validation_1-aucpr:0.86401 validation_1-logloss:0.25788 [50] validation_0-aucpr:0.87519 validation_0-logloss:0.21770 validation_1-aucpr:0.86298 validation_1-logloss:0.20919 [60] validation_0-aucpr:0.88714 validation_0-logloss:0.17906 validation_1-aucpr:0.86130 validation_1-logloss:0.17034 [70] validation_0-aucpr:0.89531 validation_0-logloss:0.14839 validation_1-aucpr:0.86285 validation_1-logloss:0.14016 [80] validation_0-aucpr:0.89770 validation_0-logloss:0.12463 validation_1-aucpr:0.86329 validation_1-logloss:0.11545 [90] validation_0-aucpr:0.90004 validation_0-logloss:0.10519 validation_1-aucpr:0.86052 validation_1-logloss:0.09647 [100] validation_0-aucpr:0.90534 validation_0-logloss:0.08897 validation_1-aucpr:0.87044 validation_1-logloss:0.07986 [110] validation_0-aucpr:0.91044 validation_0-logloss:0.07617 validation_1-aucpr:0.86994 validation_1-logloss:0.06662 [120] validation_0-aucpr:0.91458 validation_0-logloss:0.06538 validation_1-aucpr:0.86962 validation_1-logloss:0.05589 [130] validation_0-aucpr:0.91902 validation_0-logloss:0.05645 validation_1-aucpr:0.87092 validation_1-logloss:0.04684 [140] validation_0-aucpr:0.92276 validation_0-logloss:0.04895 validation_1-aucpr:0.87258 validation_1-logloss:0.03967 [150] validation_0-aucpr:0.92713 validation_0-logloss:0.04308 validation_1-aucpr:0.87285 validation_1-logloss:0.03377 [160] validation_0-aucpr:0.93179 validation_0-logloss:0.03788 validation_1-aucpr:0.87703 validation_1-logloss:0.02851 [170] validation_0-aucpr:0.93487 validation_0-logloss:0.03361 validation_1-aucpr:0.87967 validation_1-logloss:0.02426 [180] validation_0-aucpr:0.93875 validation_0-logloss:0.03013 validation_1-aucpr:0.88027 validation_1-logloss:0.02093 [190] validation_0-aucpr:0.94333 validation_0-logloss:0.02688 validation_1-aucpr:0.88284 validation_1-logloss:0.01781 [200] validation_0-aucpr:0.94592 validation_0-logloss:0.02454 validation_1-aucpr:0.88497 validation_1-logloss:0.01577 [210] validation_0-aucpr:0.95043 validation_0-logloss:0.02236 validation_1-aucpr:0.89025 validation_1-logloss:0.01363 [220] validation_0-aucpr:0.95464 validation_0-logloss:0.02033 validation_1-aucpr:0.89146 validation_1-logloss:0.01172 [230] validation_0-aucpr:0.95761 validation_0-logloss:0.01880 validation_1-aucpr:0.89327 validation_1-logloss:0.01044 [240] validation_0-aucpr:0.96080 validation_0-logloss:0.01747 validation_1-aucpr:0.89531 validation_1-logloss:0.00912 [250] validation_0-aucpr:0.96417 validation_0-logloss:0.01625 validation_1-aucpr:0.89891 validation_1-logloss:0.00802 [260] validation_0-aucpr:0.96675 validation_0-logloss:0.01519 validation_1-aucpr:0.90279 validation_1-logloss:0.00712 [270] validation_0-aucpr:0.96898 validation_0-logloss:0.01434 validation_1-aucpr:0.90530 validation_1-logloss:0.00645 [280] validation_0-aucpr:0.97143 validation_0-logloss:0.01353 validation_1-aucpr:0.90629 validation_1-logloss:0.00573 [290] validation_0-aucpr:0.97334 validation_0-logloss:0.01284 validation_1-aucpr:0.90836 validation_1-logloss:0.00520 [300] validation_0-aucpr:0.97506 validation_0-logloss:0.01216 validation_1-aucpr:0.90954 validation_1-logloss:0.00468 [310] validation_0-aucpr:0.97660 validation_0-logloss:0.01161 validation_1-aucpr:0.91150 validation_1-logloss:0.00427 [320] validation_0-aucpr:0.97800 validation_0-logloss:0.01108 validation_1-aucpr:0.91411 validation_1-logloss:0.00386 [330] validation_0-aucpr:0.97927 validation_0-logloss:0.01068 validation_1-aucpr:0.91551 validation_1-logloss:0.00361 [340] validation_0-aucpr:0.98054 validation_0-logloss:0.01019 validation_1-aucpr:0.91600 validation_1-logloss:0.00323 [350] validation_0-aucpr:0.98177 validation_0-logloss:0.00977 validation_1-aucpr:0.91776 validation_1-logloss:0.00299 [360] validation_0-aucpr:0.98272 validation_0-logloss:0.00938 validation_1-aucpr:0.92028 validation_1-logloss:0.00275 [370] validation_0-aucpr:0.98370 validation_0-logloss:0.00903 validation_1-aucpr:0.92015 validation_1-logloss:0.00256 [380] validation_0-aucpr:0.98444 validation_0-logloss:0.00877 validation_1-aucpr:0.92196 validation_1-logloss:0.00242 [390] validation_0-aucpr:0.98514 validation_0-logloss:0.00851 validation_1-aucpr:0.92389 validation_1-logloss:0.00229 [400] validation_0-aucpr:0.98580 validation_0-logloss:0.00828 validation_1-aucpr:0.92348 validation_1-logloss:0.00219 [410] validation_0-aucpr:0.98643 validation_0-logloss:0.00801 validation_1-aucpr:0.92514 validation_1-logloss:0.00203 [420] validation_0-aucpr:0.98711 validation_0-logloss:0.00774 validation_1-aucpr:0.92575 validation_1-logloss:0.00189 [430] validation_0-aucpr:0.98774 validation_0-logloss:0.00750 validation_1-aucpr:0.92427 validation_1-logloss:0.00177 [440] validation_0-aucpr:0.98832 validation_0-logloss:0.00725 validation_1-aucpr:0.92531 validation_1-logloss:0.00164 [450] validation_0-aucpr:0.98887 validation_0-logloss:0.00708 validation_1-aucpr:0.92623 validation_1-logloss:0.00160 [460] validation_0-aucpr:0.98931 validation_0-logloss:0.00690 validation_1-aucpr:0.92806 validation_1-logloss:0.00151 [470] validation_0-aucpr:0.98963 validation_0-logloss:0.00674 validation_1-aucpr:0.92860 validation_1-logloss:0.00146 [480] validation_0-aucpr:0.99005 validation_0-logloss:0.00656 validation_1-aucpr:0.92980 validation_1-logloss:0.00140 [490] validation_0-aucpr:0.99038 validation_0-logloss:0.00642 validation_1-aucpr:0.93051 validation_1-logloss:0.00135 [500] validation_0-aucpr:0.99077 validation_0-logloss:0.00628 validation_1-aucpr:0.93089 validation_1-logloss:0.00131 [510] validation_0-aucpr:0.99108 validation_0-logloss:0.00613 validation_1-aucpr:0.93270 validation_1-logloss:0.00126 [520] validation_0-aucpr:0.99138 validation_0-logloss:0.00601 validation_1-aucpr:0.93254 validation_1-logloss:0.00122 [530] validation_0-aucpr:0.99166 validation_0-logloss:0.00590 validation_1-aucpr:0.93199 validation_1-logloss:0.00119 [540] validation_0-aucpr:0.99197 validation_0-logloss:0.00577 validation_1-aucpr:0.93318 validation_1-logloss:0.00116 [550] validation_0-aucpr:0.99224 validation_0-logloss:0.00566 validation_1-aucpr:0.93408 validation_1-logloss:0.00112 [560] validation_0-aucpr:0.99250 validation_0-logloss:0.00554 validation_1-aucpr:0.93327 validation_1-logloss:0.00109 [570] validation_0-aucpr:0.99278 validation_0-logloss:0.00542 validation_1-aucpr:0.93397 validation_1-logloss:0.00106 [580] validation_0-aucpr:0.99300 validation_0-logloss:0.00530 validation_1-aucpr:0.93339 validation_1-logloss:0.00102 [590] validation_0-aucpr:0.99324 validation_0-logloss:0.00521 validation_1-aucpr:0.93372 validation_1-logloss:0.00100 [599] validation_0-aucpr:0.99338 validation_0-logloss:0.00513 validation_1-aucpr:0.93378 validation_1-logloss:0.00099 Confusion matrices of the trainig and testing sets Learning curve Although the PR AUC score are quite high, the F1 score of my trainig data is quite low and its PR AUC score is abnormally high.When intrepreting the loss and the learning curve, I see that the model is learning and generalizing well on the two sets (Although I have only included the testing sets here, results on the validation sets perform similarly). Is it safe to assume that my model is not overfitting, or is there something wrong with my intrepretation? I understand that overfitting means the data performs well on the training data, but can not generalize well into unseen data, and underfitting means that the model is unable to learn patterns from the training data, and their predictions suffer, but in this situation, it seems that my model is performing badly on the training data, but performs quite well on the testing sets. If it is not overfitting, what is wrong with my data, or my model, or are these results acceptable?
