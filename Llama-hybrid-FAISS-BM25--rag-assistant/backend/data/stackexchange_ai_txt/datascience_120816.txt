[site]: datascience
[post_id]: 120816
[parent_id]: 
[tags]: 
Does the Transformer model has memory to store the state accross different data injection sequences(segments)?

I've trained a transformer model based on the pytorch tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html , But I found I've difficulties to understant this model's input and output connections. According to its training code, the model's input data is: t[0], t[1], t[2],...,t[n], and its output target value should be t[1], t[2],...,t[n], t[n+1]. input: t[0], t[1], t[2],...,t[n-1], t[n] output:t[1], t[2], t[3],...,t[n], t[n+1]. And based on my unerstanding, t[1] depends on t[0], t[2] depends on t[1], t[0], and so on, t[n+1] depends on t[n], t[n-1], ..., t[0]. My question is, since we need cut a long tokens list into multiple segments, and input these segments into the transformer model one by one, let's assume one segment has n tokens, is there any connection exists between two segments? e.g. does any state connection exist between the t[2n+2] and t[0]? Simply to say, is the 2nd segments target value t[2n+2] decided by t[0], t[1], ..., t[n], t[n+1], t[n+2]...,t[2n+1]?
