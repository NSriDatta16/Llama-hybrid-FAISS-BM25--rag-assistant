[site]: crossvalidated
[post_id]: 244982
[parent_id]: 
[tags]: 
Conditional probability from decision tree based on biased sampling

I have a very simple binary decision tree trained with a depth of 2 as shown below. There are two labels (e.g. a , b ). For example, In the root node, there are Na samples labeled as a , and Nb samples labeled as b . The same naming convention goes with variables in the two children nodes. My goal is to obtain a probability of a sample being of label a given that it meets the particular condition, C (i.e. f I thought it would be just about simple bayesian statistics and calculated as P(a|C) = P(a, C) / P(C) where P(a, C) = La / (Na + Nb) and P(C) = (La + Lb) / (Na + Nb) so P(a|C) = La / (La + Lb) Nice and clean. I thought I could conclude that given a new sample and C is met, then the probability of it being of label a would be La / (La + Lb) However, I realized that the sampling process is biased in the data collection process. In particular, samples of Label a is over-sampled , so I thought P(a, C) and P(C) as calculated above won't reflect the true distribution anymore. My question is that will my analysis above still be valid? If not, is there a way to correct the bias introduced during the sampling process?
