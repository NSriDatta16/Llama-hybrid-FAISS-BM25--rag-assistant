[site]: crossvalidated
[post_id]: 369206
[parent_id]: 369169
[tags]: 
In think you are conflating the idea of the policy, with how some types of policy are learned. The two are usually kept separate, and you can either have the concept of a "current policy" $\pi(a|s)$ or you might use a subscript $\pi_t(a|s)$ if you are explaining how policy changes from step to step in an online environment. Not all RL agents work purely online, or have a simple way to express how they change from step to step though. In fact not all policies have anything to do with RL - the existence of a policy function is independent of how that function is arrived at. The current policy may well be determined through experience of the agent, but that is not usually considered "part of" the policy function. Instead the policy is viewed as a function that can be changed arbitrarily, by changing its parameters. In RL this is usually done either by directly modelling the function (as in policy gradient methods like REINFORCE) or via a learned value function (as in value methods, like Q learning), and then learning through updates based on experience. There is no requirement for a policy to depend on experience in any particular way, and RL theory does usually not require tracking the dependency of policy on history to the degree where your suggested notation would be useful. To be complete, your suggested notation may also need the history of all the actions, the starting policy and/or its parameters, any hyper-parameters of learning, the type of RL algorithm being used. In addition, the influence of all this history is already summarised in the Q table or estimated q function or parameters of the policy function - reminding yourself of that in the notation is really optional, because how this dependency works is often already captured by shorter notation. You may see, for policy gradient methods, the notation $\pi(a|s, \theta)$ or $u(s, \theta)$ or similar to represent the policy. The parameters $\theta$ of the function being learned are made explicit in those cases precisely because this is a useful detail to be reminded of. Of course $\theta$ is very likely dependent on the history of the agent etc as before, but there is little benefit from making this explicit, when $\theta$ completely sums up the influence that this history has had.
