[site]: datascience
[post_id]: 124277
[parent_id]: 124265
[tags]: 
At least one approach to this question would be to create word embeddings, apply PCA, and then use TSNE or K-means to cluster words with similar meanings. Word embeddings can be created using Word2Vec or GloVE . from gensim.models import word2vec ... model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=2, workers=16) keys = ["markets", "exchanges", "otc","stocks", "equity","indices", "ipo","commodity", "mortgages", "abs", "derivatives"] embedding_clusters = [] word_clusters = [] for word in keys: embeddings = [] words = [] for similar_word, _ in model.wv.most_similar(word, topn=15): words.append(similar_word) embeddings.append(model.wv[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words) ... embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape tsne_model_en_2d = TSNE(perplexity=20, n_components=2, init='pca', n_iter=3500, random_state=42) embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2) def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, alpha, filename=None): plt.figure(figsize=(16, 9)) colors = cm.rainbow(np.linspace(0, 1, len(labels))) for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:, 0] y = embeddings[:, 1] plt.scatter(x, y, color=color, alpha=alpha, label=label) for i, word in enumerate(words): plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom', size=8) plt.legend(loc=4) plt.title(title) plt.grid(True) if filename: plt.savefig(filename, format='png', dpi=150, bbox_inches='tight') plt.show() tsne_plot_similar_words('Similar words', keys, embeddings_en_2d, word_clusters, 0.7, 'other words.png')
