[site]: crossvalidated
[post_id]: 238600
[parent_id]: 
[tags]: 
Union of good feature sets degrades accuracy

I am doing binary text classification and I have some feature sets (unigrams, bigrams, dependencies, etc.) and each one of these performs very good individually. For example unigrams alone achieve 89% accuracy and bigrams alone 87% accuracy with 10 Fold CV. But the problem is that when I combine them the accuracy is around 88% which is worse than unigrams alone. My idea was that by combining good features I will get better results. Here is one simple Pipeline combining unigrams and bigrams: Pipeline([ ('features', FeatureUnion( transformer_list=[ ('bot-1', Pipeline([ ('ext', TextFeatureExtractor(feat_type="bot", term="word", negation=True)), ('vec', CountVectorizer(min_df=0.003, max_df=0.9, lowercase=False)), ("fs", SelectKBest(score_func=mutual_info_classif, k=10000)), ('bin', Binarizer()), ('trans', TfidfTransformer(sublinear_tf=True, smooth_idf=True, use_idf=True)), ])), ('bot-2', Pipeline([ ('ext', TextFeatureExtractor(feat_type="bot", term="lemma", negation=True)), ('vec', CountVectorizer(min_df=0.002, max_df=0.9, lowercase=False, ngram_range=(2, 2))), ("fs", SelectKBest(score_func=mutual_info_classif, k=15000)), ('bin', Binarizer()), ('trans', TfidfTransformer(sublinear_tf=True, smooth_idf=True, use_idf=True)), ])), ], )), ('classifier', svm.LinearSVC(C=.5)), ]) As you can see I do MI Feature Selection and after that I apply tf-idf transformation on the selected features. Am I doing something wrong? The things I have tried so far: Scale them using StandardScaler(with_mean=False) before classification Scale them using MaxAbsScaler() before classification apply TruncatedSVD with n_components = 100-300 after the TfidfTransformer and also tried normalization after that. Am I doing something wrong?
