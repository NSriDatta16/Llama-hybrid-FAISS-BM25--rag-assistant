[site]: datascience
[post_id]: 76877
[parent_id]: 
[tags]: 
Why are Neural Network predictions "correct", but offset from true value? Not using any past lagged values

I recently asked a similar question, but didn't get a response that really addressed/fixed the issue. Additionally, I've done some more work since then. I'm sorry for the long question below, I just want to make it very clear what I've done so far and why I'm so perplexed. I am working with a neural network with the Keras package in R, trying to predict hourly Bitcoin price, 24 hours ahead. Here is the code for my model: batch_size = 2 model % layer_dense(units=13, batch_input_shape = c(batch_size, 1, 13), use_bias = TRUE) %>% layer_dense(units=17, batch_input_shape = c(batch_size, 1, 13)) %>% layer_dense(units=1) model %>% compile( loss = 'mean_squared_error', optimizer = optimizer_adam(lr= 0.000025, decay = 0.0000015), metrics = c('mean_squared_error') ) summary(model) Epochs % fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=TRUE) #model %>% reset_states() } You may notice that I am working on a time-series problem, but not using LSTM. This is because none of my inputs are time-series values. They are all exogenous variables. You'll also notice that I commented out the line "model %>% reset_states()". I'm not sure if that's the right thing to do here, but from what I read, that line is for LSTM models and since I'm not using one anymore, I commented it out. Again, because I have no time-series inputs, I also set "shuffle=" to TRUE. So, below are the predictions in blue vs. true value in red: You can see that the predictions very often (but not always) lag behind the true value. Additionally, this lag is not constant. Let me again emphasize that all of the variables are exogenous. There is not past-price input that the model could be using in order to generate these late predictions. AND don't forget I set shuffle= TRUE, which confuses me even further as to how the model could be giving such results if there's no way (that I know of) that it can "see" past values in order to replicate them. Here is the graph of the training data fit: It's harder to tell, but the lag exists in the training data as well. I'll also say that if I change how far ahead I'm trying to predict, the apparent prediction lag changes too. If I try to predict 0 hours ahead (so I'm "predicting" current price given current conditions), there is no lag in prediction. I checked to make sure the tables/columns were set up right so that the model is trained on "current" conditions predicting price 24 hours ahead. I've also played around with the network architecture and batch size. The only thing that seems to affect this lag is how far ahead I'm trying to predict - that's to say, how many rows I shifted my Bitcoin Price column by so that "future" price rows are matched with past predictor rows. Something else weird I've noticed is that with this non-LSTM model, it has a rather high error when training (MSE=0.07) which it reaches after only 5-9 Epochs and then doesn't go any lower. I don't think this is relevant because the LSTM model I used before achieved MSE=0.005 and still had the same lag issue, but I figured I'd mention it. Any advice, tips or links would be enormously appreciated. I can't for the life of me figure out what's going on.
