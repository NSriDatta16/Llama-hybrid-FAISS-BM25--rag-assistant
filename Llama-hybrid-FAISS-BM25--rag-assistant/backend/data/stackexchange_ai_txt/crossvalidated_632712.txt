[site]: crossvalidated
[post_id]: 632712
[parent_id]: 
[tags]: 
How does the training set size affect the uncertainty (variance) of performance estimation?

I am reading this paper which discusses the factors that affect the uncertainty (variance) in the performance estimation of a learner. The authors say (p. 2, "The monotonicity of the learning curve of a learner"): The true performance of the Tree Model will be varying around the point on the curve on the 90% x axis point. There is variance due to the exact training set feeding the Tree Learner : different training sets will spit out different Tree Models with better or worse true average performance around the mean value on the curve. In addition, our performance estimate will have additional variance because our test set is not infinite. and In fact, the smaller the test set, the larger the variance of our performance estimate, but the better performance our model will exhibit due to a larger train set and vice versa, as shown in Figure 1. So the larger the training set, the smaller the test set (since we consider a random sample of fixed size from the population) and as such (based on the last quote from the paper), the larger the variance. I can understand this, since if the test set is very small, we can get samples where we have very good predictions or samples where we have very bad predictions. As such the estimation of the performance will vary very much (in the extreme case consider just 1 test example). Question In Figure 1 of the paper, the factors that affect the uncertainty in performance estimation are the following 3: Random sampling of the dataset from the whole population Random partition to train and test Size of test set Why the training set size isn't mention? What if we choose to have a very small training set (e.g. partitioning dataset of $N$ data points into 1% training and 99% test)? Isn't the variance also high in this case?
