[site]: crossvalidated
[post_id]: 44942
[parent_id]: 
[tags]: 
Does the average distance in K-means have to be monotone decreasing?

I'm implementing the k-means algorithm myself. I don't see any obvious mistake in my code and it seems to work well. However, there's something I don't understand. My algorithm, working on vectors $x_i \in X$, cetroids $c_j \in C$ and labels $l_i \in L$ looks like: choose centers $C=\{c_1,c_2,...,c_k\}$ from $X_i$ randomly (no repetitions) label each vector with the index of nearest centroid $$l_i = \text{arg}\min_{j \in \{1..k\}}(||x_i-c_j||, c_j \in C)$$ find new means of each cluster $$c_j = \text{mean}\{x \in X_i \;\text{and}\; l_i=j\}\;\text{for}\;j=1..k $$ if the centroids have moved signifficantly in the previous step, goto 2 A plain vanilla k-means, I guess. However, when I make a plot of the average of point-to-respective-centroid distances after each iteration, $$\text{mean}\{||x_i-c_j||\;\text{and}\;j=l_i\;\text{for each}\;x_i \in X \}, $$ I can see that the mean distance is not monotone decreasing (although in general it is decreasing and converging nicely): https://i.stack.imgur.com/ySgWf.png (sorry it won't allow me embed the image) I remember to be told it should be always decreasing, and my intuition is the same. What's wrong? My algorithm (implementation) or my assumption about the mean of distances?
