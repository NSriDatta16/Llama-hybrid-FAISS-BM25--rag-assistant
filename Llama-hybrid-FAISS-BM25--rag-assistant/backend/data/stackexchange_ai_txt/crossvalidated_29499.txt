[site]: crossvalidated
[post_id]: 29499
[parent_id]: 28734
[tags]: 
To generalize your question, I believe you're saying you oversampled a high influence region of a predictor and you're now wondering why your model performance measures seem much, much better. If this were a linear regression model, it would make perfect sense since the measures of model performance there are generally improved when the variance of the sampling distribution of the input variable is increased. I think your neural network is no different. Oversampling can be done for efficiency reasons, such as the above scenario. However I would be very leery of doing so based on a preliminary analysis of the data since the confirmatory model will be subject to overfitting or bias. If you're interested in the population, you don't want a select group to drive the trend. Consider SES and health disparities. If you oversample high earning families, you will also capture their considerably better health outcomes. That could "torque" the least squares regression slope about the central mean of the bivariate distribution of SES/health measures and make the fitted mean health outcomes for low income families seem much lower than their actual mean(this is a case of model misspecification). There are methods of stratified analysis in which you might sample systematically according to a prespecified "importance sampling distribution" or weighting scheme. But to do inference on the population of interest, you need to consider how to reweight your analysis to account for this sampling strategy.
