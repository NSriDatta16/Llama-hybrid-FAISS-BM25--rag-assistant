[site]: crossvalidated
[post_id]: 623594
[parent_id]: 
[tags]: 
Geometric intuition of kernel trick

I would like to understand better the geometry underlying the Kernel trick with the Gaussian Kernel. In particular my question is: How the Kernel trick can be interpreted geometrically, in particular for the case of the Gaussian Kernel? For example if kernelize linear SVM we would consider an hypothetical feature map $\Phi$ in some $R^k$ and draw linear boundaries on the embedded space. The set of all possible decision boundaries in the embedded space are hyperplanes. Consider an initial $2D$ (two dimensional) feature space. How can we get a characterization/intution of how the set of possible decision boundaries looks like in the original feature space? E.g. can we show simple things like if they will always be connected or not? Some random thoughts. The Kernel trick with the Guassian Kernel amounts to the replacement: $\Phi(x)\cdot \Phi(y) \rightarrow K(x,y)=\exp(|x-y|^2/2)$ Some consequences: if $x=y$ than this implies $|\Phi(x)|^2=\exp(0)=1$ . So the feature map maps everything to a sphere. If the original space if 2 dimensional and $k=3$ (just to get some intution) I can assume that there are more hyperplanes dividing the data in the 2-dim sphere rather than in the original 2D plane, since the sphere has a curvature. Now if $\Phi$ is such a map we should have that the mapping induces a new distance in the original feature space $d(x,y)=||\Phi(x)-\Phi(y)||$ (*). In Kernel terms $d(x,y)=\sqrt{K(x,x)+K(y,y)-2K(x,y)}$ . For the Guassian Kernel this implies that $d(x,y)=\sqrt{2(1-\exp(|x-y|^2/2))}$ should be a distance. But this distance is quite weird and difficult to visualize since points very far apart initially get compressed to a finite distance of $\sqrt{2}$ (I guess this is a consequence of point 1). *e.g. triangular inequality $d(x,z)=||\Phi(x)-\Phi(z)||\le ||\Phi(x)-\Phi(y)||+||\Phi(y)-\Phi(z)||=d(x,y)+d(y,z).$
