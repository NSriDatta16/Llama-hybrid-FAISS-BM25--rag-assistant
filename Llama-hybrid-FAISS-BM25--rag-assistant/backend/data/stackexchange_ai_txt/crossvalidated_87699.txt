[site]: crossvalidated
[post_id]: 87699
[parent_id]: 87698
[tags]: 
The price you pay for the Kernel Trick in general, for linear methods, is having worse generalization bounds. For a linear model its VC dimension is also linear in terms of the number of dimensions (e.g. VC dimension for a Perceptron is d + 1 ). Now, if you will perform a complex non-linear transform to a high dimensional space the VC dimension of your hypothesis set is significantly larger, as it's now linear in terms of the number of dimensions in the new, high dimensional space. And with it, the generalization bound goes up. Support Vector Machines exploit the Kernel Trick in the most efficient way, by doing two things: the generalization bound for hard-margin SVM models is related to the number of Support Vectors, and for soft-margin is related to the norm of the weight vector - so it can be irrelevant in the first case, and almost irrelevant in the second case. No matter how "big" the kernel target space is, you don't loose anything/much in terms of the generalization (references: (i) C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273â€“297, 1995 ; (ii) Shawe-Taylor, J.; Cristianini, N., "On the generalization of soft margin algorithms," Information Theory, IEEE Transactions on , vol.48, no.10, pp.2721,2735, Oct 2002 ). SVMs find the separation plane that maximizes the margin, and this further simplifies the hypothesis set (we don't consider every possible separating plane, just those that maximize the margin). Simple hypothesis set also leads to better generalization bounds (this is related to the first point, but it's more intuitive).
