[site]: crossvalidated
[post_id]: 238295
[parent_id]: 238276
[tags]: 
Somewhat sloppy definitions: Marginal probability You have two dice. Let $X$ be a random variable denoting the roll of the first die. Let $Y$ be a random variable denoting the roll of the second die. Let $P(X=x,Y=x)$ be a function giving the probability that $X=x$ and $Y=y$. $P$ is called the joint probability mass function. This function defines the joint probability distribution over the two dice rolls. $P(X=x)$ is called a marginal probability . You come to a marginal probability by summing or integrating the joint probability distribution. $$ P(X=x) = \sum_{y=1}^6 P(X=x, Y=y) $$ Eg. The probability your first die roll is a 2 is the probability you rolled 2 and a 1 plus the probability you rolled a 2 and a 2 plus the probability you rolled a 2 and a 3 etc... Basically, the joint probability distribution is the distribution over all your random variables. And a marginal probability distribution is a distribution that's over fewer than all your variables. Bayesian Prior Let $\theta$ be some parameter that represents your beliefs, some prior beliefs before you see the data. $P(\theta)$ is called a prior .
