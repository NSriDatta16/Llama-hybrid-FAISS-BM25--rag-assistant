[site]: crossvalidated
[post_id]: 160939
[parent_id]: 160937
[tags]: 
The relation you posit is true only when certain conditions are satisfied. Unfortunately, imposing these conditions tends to destroy the "indicatorness" of the predictors. We can start from the equation satisfied by the linear regression solution $$ X^{t} X \beta = X^{t} y $$ Lets work out the intercept first. If we call the model predictions $P = X \beta$, then we can rewrite the equation as $$ X^{t} P = X^{t} y$$ The first row of $X^{t}$ is completely populated with $1$s, so the equation corresponding to this row is $$ \frac{1}{n} \sum_i p_i = \frac{1}{n} \sum_i y_i $$ Now let's expand the predictions in terms of the other parameters $$ p_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_n x_{ni} $$ So the sum average of the $p_i$s is $$ \frac{1}{n} \sum_i p_i = \beta_0 + \frac{\beta_1}{n} \sum_i x_{1i} + \cdots + \frac{\beta_n}{n} \sum_i x_{ni} $$ But we're assuming the predictor are centered, so this becomes $$ \frac{1}{n} \sum_i p_i = \beta_0 + 0 + \cdots + 0 = \beta_0 $$ Now going back to the average of the responses we get $$ \beta_0 = \frac{1}{n} \sum_i y_i $$ so that's our first relation. This holds as long as the independent variables are centered. To derive a relation for the independent variables, we must assume that they are standardized and uncorrelated. Since we know the relation for the intercept, we may as well center the response $y$, so that $\beta_0 = 0$. Then, ignoring the intercept, we have $$ \frac{1}{n} X^{t} X = I $$ so the regression equation is $$ \beta = \frac{1}{n} X^{t} y $$ Now, if we had our raw indicators still, we could deduce that the parameter estimate was the mean of the responses in a given class. But we've centered the indicators, so we cannot! Instead, we've made the following replacements for the predictor $x_j$ $$ 1 \rightarrow 1 - \frac{1}{n} \sum_{i:x_{ij} = 1} 1 = 1 - \frac{ \# \{i : x_{ij} = 1\} }{n} $$ $$ 0 \rightarrow - \frac{1}{n} \sum_{i:x_{ij} = 1} 1 = - \frac{ \# \{i : x_{ij} = 1\} }{n}$$ So what the $j$th row of this matrix equation actually says is $$ \beta_j = \left( 1 - \frac{ \# \{i : x_{ij} = 1\} }{n} \right) \sum_{i:x_{ij} = 1} y_i - \left( \frac{ \# \{i : x_{ij} = 1\} }{n} \right) \sum_{i:x_{ij} = 0} y_i $$ Which is a rather complicated relation! In the general case $$ \frac{1}{n} X^{t} X \beta = \frac{1}{n} X^{t} y $$ the correlation between the independent variables forces a complicated "mixing" between the parameter estimates before becoming equal to the average of the response within the classes. There's little general that can be said.
