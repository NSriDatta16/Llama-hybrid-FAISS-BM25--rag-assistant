[site]: crossvalidated
[post_id]: 330494
[parent_id]: 225433
[tags]: 
I can't imagine how manipulating with your test set could lead to anything more then cheating yourself that your results are better then in reality. The idea of cross-validation and testing your results on unseen data, is to approximate the situation where your model would be applied to some future, unknown data. This approximation may be better or worse depending on how similar is the data you have to the future data. The idea of bootstrap is that you sample from your data the same way as you'd sample from the population, so to approximate the sampling process and estimate the variability caused by it. First thing to notice is that such procedure does not let you learn anything about possible performance if the data you have is not similar to the future data. Second, the sampling is ought to imitate the sampling process, so rather then resampling your test set, you should instead make multiple random splits to train and test set (i.e. use k-fold cross-validation). Finally, bootstrap is designed for estimating the possible variability, not for correcting it. Bradley Efron himself discouraged from such useage of bootstrap. The completely different story, is to bootstrap resample the train set and then aggregate the results, i.e. use bagging -- this would help to reduce the variance of the predictions.
