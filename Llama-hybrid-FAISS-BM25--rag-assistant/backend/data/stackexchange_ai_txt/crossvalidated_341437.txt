[site]: crossvalidated
[post_id]: 341437
[parent_id]: 340898
[tags]: 
There is a bug in your code, since the first half of your constructed examples are positive and the rest are negative, but keras does not shuffle before splitting the data into train and val, which means all of the val set is negative, and the train set is biased towards positive, which is why you got strange results such as 0 accuracy (worse than chance). In addition, I tweaked some parameters (such as the learning rate, number of epochs, and batch size) to make sure training always converged. Finally, I ran only for 5 and 100 time steps to save on computation. Curiously, the LSTM doesn't train properly, although the GRU almost does as well as the RNN. I tried on a slightly more difficult task: in positive sequences, the sign of the first element and an element halfway through the sequence is the same (both +1 or both -1), in negative sequences, the signs are different. I was hoping that the additional memory cell in the LSTM would benefit here It ended up working better than RNN, but only marginally, and the GRU wins out for some reason. I don't have a complete answer to why the RNN does better than the LSTM on the simple task. I think it must be that we haven't found the right hyperparameters to properly train the LSTM, in addition to the fact that the problem is easy for a simple RNN. Possibly, a model with so few parameters is also more prone to getting stuck in local minimum. The modified code
