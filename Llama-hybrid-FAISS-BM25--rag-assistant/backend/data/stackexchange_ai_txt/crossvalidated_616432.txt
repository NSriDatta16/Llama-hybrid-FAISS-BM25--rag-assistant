[site]: crossvalidated
[post_id]: 616432
[parent_id]: 
[tags]: 
How to prove that my neural network satisfies convergence assumptions of the Adam optimizer?

Recently, I read a paper ( this paper ) that provides convergence analysis of the Adam optimizer without assuming the smoothness condition of the loss function. Suppose I have a neural network $f(\theta)$ with a loss function $L$ , and I want to verify if my network satisfy the assumptions (S1)-(S3) and (A1)-(A2). I am confused about how to prove that a specific network $f(\theta)$ and a specific loss function $L$ satisfies those assumptions. How should I get started? Also, do these assumptions depend on the network architecture? Note: Maybe this question seems a bit silly because I'm still relatively new to neural network analysis.
