[site]: crossvalidated
[post_id]: 503678
[parent_id]: 503677
[tags]: 
The reason to use an out-of-sample validation or test set is to mimic the real application of your machine learning model: releasing it into the wild. As far as you’re concerned, once you split the data, only the training data exist. Whatever method you use to fill in missing values or engineer features in the training data, go do that for the out-of-sample observations. You don’t really even think of the out-of-sample data as a set. While you won’t program this way, think of it as making sequential predictions. You have this awesome machine learning model, and a customer comes to you with new data, desperate to know your prediction. “Doc, what do you think?” Make your prediction, using what you’ve learned on the training data. Then move to the next observation that the next customer brings.
