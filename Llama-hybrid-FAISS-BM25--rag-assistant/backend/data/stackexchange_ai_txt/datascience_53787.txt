[site]: datascience
[post_id]: 53787
[parent_id]: 53773
[tags]: 
I'm not 100% sure what you mean. In k-fold CV, you partition the training set into $k$ subsets of equal size. Holding out one of these folds at a time, you train the model on the remaining $k-1$ folds to make a prediction for the held-out fold. Thus, in the end, you have one prediction for each observation in your training data. Now, you can compute average accuracy in two equivalent ways: for each fold, compute the average accuracy, then average the k averages. Or, you average accuracy for every single observation. The two values are the same (up to rounding errors), since all you do is take the intermediate step of calculating averages for each fold. The advantage is that you save on memory because you only have to retain $k$ values (average accuracy per fold) instead of $N$ values (one value for each observation in the training set).
