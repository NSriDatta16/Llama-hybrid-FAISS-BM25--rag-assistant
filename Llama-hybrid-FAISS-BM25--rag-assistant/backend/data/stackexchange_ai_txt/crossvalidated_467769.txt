[site]: crossvalidated
[post_id]: 467769
[parent_id]: 
[tags]: 
Calibration in linear, logistic and Poisson regression

I read the following in Google's Rules of ML : In linear, logistic, or Poisson regression, there are subsets of the data where the average predicted expectation equals the average label ( calibrated ). This is true assuming that you have no regularization and that your algorithm has converged, and it is approximately true in general. If you have a feature which is either 1 or 0 for each example, then the set of 3 examples where that feature is 1 is calibrated. Also, if you have a feature that is 1 for every example, then the set of all examples is calibrated. I don't quite understand that. For example: Why would these specific models be calibrated? (i.e. why single them out?) Are such regression models always calibrated by simple virtue of the fact that the fitting algorithm converged and there was no regularization? What do they mean by this happening in subsets of the data ? Are they saying that the model would be calibrated at least for a few instances? If so, I thought you could only make a statement about calibration when looking at your predictor under the distribution of the input. In other words, if it's calibrated, it's calibrated for the full input, not for some subsets. What am I missing? How can the value of a single feature say anything about calibration?
