[site]: crossvalidated
[post_id]: 196758
[parent_id]: 196750
[tags]: 
The simplest way to stack your predictions is to take the average. Linear regression is certainly an alternative. Here is a link to a video in which Phil Brierley describes using regularized regression instead of linear regression to combine model predictions. You could also look at accounts by other Kaggle winners. For example, the winner of the Bulldozer Price Prediction contest combined models "using a neural network". I think the best thing to do is to look at things like Kaggle contest writeups and see what people did (or look at their code.) As for your question, if you are only interested in prediction (which you probably are if you are stacking) then it doesn't matter if the mixture weights don't add up to 1.
