[site]: datascience
[post_id]: 44293
[parent_id]: 
[tags]: 
How does dropout work during testing in neural network?

The below paragraph is picked from the textbook Hands-On Machine Learning with sci-kit learn & Tensorflow . I couldn't understand what the author is trying to convey. It would be really grateful if someone can provide an explanation for below paragraph with an example. I have referred these links Dropout in Deep Neural Networks , https://www.coursera.org/lecture/deep-neural-network/dropout-regularization-eM33A , that somehow aligns with the same objective. However, i am seeking good explanations. Please help me. Suppose p = 50%, in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. To compensate for this fact, we need to multiply each neuron's input connection weights by 0.5 after training. if we don't, each neuron will get a total input signal roughly twice as large as what the network was trained on, and it unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability(1-p) after training. Alternatively, we can divide each neuron's output by keep probability during training (these alternatives are not perfectly equivalent, but they work equally well)
