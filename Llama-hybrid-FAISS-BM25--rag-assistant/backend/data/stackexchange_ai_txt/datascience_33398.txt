[site]: datascience
[post_id]: 33398
[parent_id]: 
[tags]: 
Random Forest Classifier - KFold CV Tunes Very Deep Trees --> Overfitting?

I'm tuning a random forest in python and am wondering if/why my model is overfit. The dataset is described below: 1700 Positive Cases / 54000 total cases ~ 3.2% (unbalanced) 50 Numerical Features,~450 label/hot encoded features (post data reduction) 10Fold CV using 85% of data, with 15% hold out for final test Classification Metrics = AUC or F1 (as data is imbalanced) The results I get tend to suggest using very deep trees i.e depth 18 with no restriction on number of samples per split = 2(default). In this case, Train AUC was 99.9% , Max Test AUC was 84%. My scores are also almost monotonically increasing in max depth of trees. Given the results and how deep the trees are - I suspect the model is overfit? If this is the case then why would I not observe some sort of out of sample reduction in AUC as depth and min_samples_split typically constrain the random forest? Or have I overlooked anything in tuning? My ranges in CV Grid Search are more or less: n_estimates : range(100,1000,by=100) max_features : {sqrt(p),0.3,0.4,0.5} max_depth : range(2,20,by=1) min_samples_split : range(2,50,by=1) class_weights : {balanced,None} Thanks
