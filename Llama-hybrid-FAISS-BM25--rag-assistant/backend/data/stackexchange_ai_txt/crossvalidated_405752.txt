[site]: crossvalidated
[post_id]: 405752
[parent_id]: 
[tags]: 
1D Bayesian Inference clarification

I'd like some help making sure I understand a 1D Bayesian inference problem. Say I have a data vector which is an array of the number of flu cases reported weekly in California for the past 10 years. I want to compare two models which describe this data, via the odds ratio: $$\mathcal{O}_{ij} = \frac{P(M_i|D,I)}{P(M_j|D,I)} = \frac{P(M_i|I)\,\mathcal{L}(M_i)}{P(M_j|I)\,\mathcal{L}(M_j)}$$ where $$\mathcal{L}(M) = \int d\theta P(\theta|M_i)\mathcal{L}(\theta)|D,M)$$ borrowing the notation I've found. $M$ is model, $D$ is data, I'm not sure what $I$ represents. I've developed two models, each of which are 1D vectors, one of which has parameters. I'm new to Bayesian statistics. My confusion here is: what are $\mathcal{L}(\theta)|D,M)$ , $P(M|I)$ , and $P(D|I)$ in relation to the dataset and the two models? I Think $P(D|I)$ is just the data. $P(M|I)$ is the prior but I don't know what that relates too, unless that's the model, in which case I don't know what the likelihood function $\mathcal{L}$ is. Could someone help clarify this for me?
