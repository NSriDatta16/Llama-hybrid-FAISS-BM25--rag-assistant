[site]: crossvalidated
[post_id]: 618963
[parent_id]: 
[tags]: 
Multivariate Normal Bayesian Updating with Conjugate Priors but Non-Standard Likelihood

I am trying to solve for the posterior of two parameters $\theta_1$ and $\theta_2$ . I have priors $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$ for $\theta_1$ and $\theta_2$ respectively, where $\mathrm{Corr}[\theta_1,\theta_2] = \rho$ (i.e., $(\theta_1, \theta_2)$ is a bivariate normal with correlation $\rho$ ). I also observe data $$ y_1 \mid \theta_1,\nu_1^{2} \sim N(\theta_1, \nu_1^{2}) \\ y_2 \mid \theta_2,\nu_2^{2} \sim N(\theta_2, \nu_2^{2}) $$ where $\nu_1^{2}$ , $\nu_2^{2}$ , and $\rho$ are all known. Solving for posteriors is straightforward in this context--I can just plugin the closed-form expression for a multivariate normal with conjugate priors with known variance-covariance matrix (follow page 71 from http://www.stat.columbia.edu/~gelman/book/ ). My trouble comes from another fact. I also observe $$ y_3 \mid \theta_1, \theta_2, \nu^{2}_3 \sim N(a_0 + a_1\theta_1 + a_2 \theta_2, \nu^{2}_3) $$ where $a_0$ , $a_1$ , $a_2$ , and $\nu_3^{2}$ are known. What is the posterior after seeing a draw $y_3$ ? I think that I can just relabel $a_0 + a_1\theta_1 + a_2 \theta_2$ as $\theta_3$ and treat it as a third parameter. I could solve for the joint prior of $(\theta_1, \theta_2, \theta_3)$ and proceed in the standard fashion. After doing this, I would expect that the posterior for $(\theta_1, \theta_2)$ would be the same as the conditional posterior $(\theta_1, \theta_2)\mid \theta_3$ . Is that right? Is any of this correct, or am I way off? EDIT: I tried just including $\theta_3$ and proceeding in the standard fashion, but I ran into some issues when I simulated it. Specifically, the posteriors for $\theta_1$ , $\theta_2$ , and $\theta_3$ do not always agree. In particular, I would expect to have posteriors $\theta_{N3} = a_0 + a_1\theta_{N1} + a_2\theta_{N2}$ . If this procedure used all of the available information (i.e., $a_0$ , $a_1$ , and $a_2$ ), this equality should always hold. But it does not. Below is my simulation code in Julia that demonstrates this issue. It is especially apparent if you set $\nu_1^2$ or $\nu_2^2$ to be large (e.g., 100). using Statistics using Distributions # Generate θ's μ1 = 0 μ2 = 0 σ2_1 = .5 σ2_2 = 1 ρ12 = .6 θ1, θ2 = rand(MvNormal([μ1, μ2], [σ2_1 ρ12; ρ12 σ2_2])) a0, a1, a2 = 1, 2, 1 θ3 = a0 + a1*θ1 + a2*θ2 # Construct prior for θ3 cov12 = ρ12 * (sqrt(σ2_1)*sqrt(σ2_2)) σ2_3 = a1^2*σ2_1 + a2^2*σ2_2 + a1*a2*2*cov12 ρ13 = (a1*σ2_1 + a2*cov12)/(sqrt(σ2_1)*sqrt(σ2_3)) ρ23 = (a1*cov12 + a2*σ2_2)/(sqrt(σ2_2)*sqrt(σ2_3)) # Prior variance covariance matrix Λ = [σ2_1 ρ12 ρ13; ρ12 σ2_2 ρ23; ρ13 ρ23 σ2_3] # Prior means vector μ = [μ1, μ2, a0 + a1*μ1 + a2*μ2] # Simulate N draws of y data. N = 10_000 ν2_1 = 2 ν2_2 = 3 ν2_3 = 1 Σ = [ν2_1 0 0; 0 ν2_2 0; 0 0 ν2_3] y = rand(MvNormal([θ1, θ2, θ3], Σ), N) y1, y2, y3 = y[1, :], y[2, :], y[3, :] # Get posterior θN = (Λ^(-1) + N*Σ^(-1))^(-1)*(Λ^(-1)*μ + N*Σ^(-1)*[mean(y1), mean(y2), mean(y3)]) ΛN = (Λ^(-1) + N*Σ^(-1))^(-1) # This is often false isapprox(θN[3], a0 + a1*θN[1] + a2*θN[2]; atol = .01) ```
