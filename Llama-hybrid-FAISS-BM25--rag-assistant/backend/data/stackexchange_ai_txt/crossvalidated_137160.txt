[site]: crossvalidated
[post_id]: 137160
[parent_id]: 
[tags]: 
Similarity metric for 2 sets of vectors

I'm trying to determine the similarity between two sentences. I have vectors for each word in a corpus, and using cosine distance of the two vectors, I can get quite a good "similarity" score between two words. I wish now to extrapolate that out to two sentences, where each sentence is a set of word vectors. Since the word-to-word cosine distance works pretty well, I think I'm looking for an extrapolation of the vector cosine distance to matrices. In other words, given two sentences of M x N words, I have a M x N matrix of word vectors, and can derive an M x N matrix of similarity scores. I'd like to come up with a score for the matrix, analogous to the scores I have between each word vector, so as to compare it to other matrices (other sentence pairs). I've thought about: Take the mean vector of the vectors in sentence A and the mean vector of the vectors in sentence B, and then perform cosine similarity on the 2 resultant vectors. Compute the cosine similarity of each word vector in both sets, resulting in a matrix of m x n elements, and then normalize the matrices using something like L2 or Frobenius and compare the differences. Compare the element-wise average similarity of the matrix A with B. I realize this is an open research problem, and I am not taking into account things like word order, or tf-idf, etc. I'm just looking for general guidelines on what I'm trying to do. For example, taking the cosine distance of the mean vectors of each set is very different than taking the mean similarity score of the M x N matrix, and it's unclear to me what the tradeoffs of those approaches are. Thanks a lot.
