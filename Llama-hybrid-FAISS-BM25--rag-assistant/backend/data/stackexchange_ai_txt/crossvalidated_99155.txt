[site]: crossvalidated
[post_id]: 99155
[parent_id]: 20010
[tags]: 
Many important points have been covered in the excellent answers that are already given. Lately, I've developed this personal check list for statistical independence of test data: Split data at highest level of data hierarchy (e.g. patient-wise splitting) Split also independently for known or suspected confounders, such as day-to-day variation in instruments etc. (DoE should take care of random sequence of measurements**) All calculation steps beginning with the first (usually pre-processing) step that involves more than one patient* need to be redone for each surrogate model in resampling validation. For hold-out / independent test set valdiation, test patients need to be separated before this step. This is regardless whether the calculation is called preprocessing or is considered part of the actual model. Typical culprits: mean centering, variance scaling (usually only mild influence), dimensionality reduction such as PCA or PLS (can cause heavy bias, e.g. underestimate no of errors by an order of magnitude) Any kind of data-driven optimization or model selection needs another (outer) testing to independently validate the final model. There are some types of generalization performance that can only be measured by particular independent test sets, e.g. how predictive performance deteriorates for cases measured in future (I'm not dealing with time series forecasting, just with instrument drift). But this needs a properly designed validation study. There's another peculiar type of data leak in my field: we do spatially resolved spectroscopy of biological tissues. The reference labelling of the test spectra needs to be blinded against the spectroscopic information, even if it is tempting to use a cluster analysis and then just find out which class each cluster belongs to (that would be semi-supervised test data which isn't independent at all). Last but certainly not least: When coding resampling validation, I actually check whether the calculated indices into the data set to not lead to grabbing test rows from training patients, days etc. Note that the "splitting not done in order to ensure independence" and "split before any calculation occurs that involves more than one case" can also happen with testing that claims to use an independent test set, and the latter even if the data analyst is blinded to the reference of the test cases. These mistakes cannot happen if the test data is withheld until the final model is presented. * I'm using patients as the topmost hierarchy in data just for the ease of description. ** I'm analytical chemist: Instrument drift is a known problem. In fact, part of the validation of chemical analysis methods is determining how often calibrations need to be checked against validation samples, and how often the calibration needs to be redone. FWIW: In practice, I deal with applications where $p$ is in the order of magnitude of $10^2 - 10^3$, $n_{rows}$ is usually larger than $p$, but $n_{biol. replicates}$ or $n_{patients}$ is $\ll p$ (order of magnitude: $10^0 - 10^1$, rarely $10^2$) depending on the spectroscopic measurement method, all rows of one, say, patient may be very similar or rather dissimilar because different types of spectra have signal-to-noise ratio (instrument error) also varying by an order of magnitude or so Personally, I've yet to meet the application where for classifier development I get enough independent cases to allow setting aside a proper independent test set. Thus, I've come to the conclusion that properly done resampling validation is the better alternative while the method is still under development. Proper validation studies will need to be done eventually, but it is a huge waste of resources (or results will carry no useful information because of variance) doing that while the method development is in a stage where things still change.
