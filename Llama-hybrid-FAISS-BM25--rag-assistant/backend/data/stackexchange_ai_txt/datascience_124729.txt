[site]: datascience
[post_id]: 124729
[parent_id]: 72321
[tags]: 
I think it depends on the size of your data and the predictive power of your features, what your reference optimization metric is and data quality. Using class weights or resampling would increase your model's performance in predicting the positive/minority classes but might also bring extra noise - false positives. On top of that, you do not always have to make a perfectly balanced 50:50 dataset. For example for me, I was dealing with severe imbalance and without class weights or SMOTE, the performance of my model was really never good enough. However, one disadvantage of your Options 2-3 is that your predictive probabilities will be inflated depending on how much you apply them. So if you are after predicted probabilities (which can be the case in marketing use cases) then you might have to make a compromise between how much you want to improve your model's performance vs how much you want to sacrifice the validity of your probabilities. Lastly, you can also try Logistic Regression if it works as well as others, it is computationally much simpler and you can interpret your coefficients more easily too to get business insights.
