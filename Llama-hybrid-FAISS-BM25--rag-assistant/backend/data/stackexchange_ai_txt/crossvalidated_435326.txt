[site]: crossvalidated
[post_id]: 435326
[parent_id]: 435298
[tags]: 
Let me use the linear regression example, that you mentioned. The simple linear regression model is $$ y_i = \alpha + \beta x_i + \varepsilon_i $$ with noise being independent, normally distributed random variables $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ . This is equivalent of stating the model in terms of normal likelihood function $$ y_i \sim \mathcal{N}(\alpha + \beta x_i, \;\sigma^2) $$ The assumptions that we make follow from the probabilistic model that we defined: we assumed that the model is linear, we assumed i.i.d. variables, variance $\sigma^2$ is the same for every $i$ -th observation, so the homoscedasticity, we assumed that the likelihood (or noise, in first formulation) follows normal distribution, so we do not expect to see heavy tails etc. Plus some more "technical" things like no multicollinearity, that follow from the choice of method for estimating the parameters (ordinary least squares). (Notice that those assumptions are needed for things like confidence intervals, and testing, not for the least squares linear regression. For details check What is a complete list of the usual assumptions for linear regression? ) The only thing that changes with Bayesian linear regression , is that instead of using optimization to find point estimates for the parameters, we treat them as random variables, assign priors for them, and use Bayes theorem to derive the posterior distribution. So Bayesian model would inherit all the assumptions we made for frequentist model, since those are the assumptions about the likelihood function. Basically, the assumptions that we make, are that the likelihood function that we've chosen is a reasonable representation of the data . As about priors, we do not make assumptions about priors, since priors are our a priori assumptions that we made about the parameters.
