[site]: datascience
[post_id]: 24083
[parent_id]: 24081
[tags]: 
Graph embedding learns a mapping from a network to a vector space, while preserving relevant network properties. Vector spaces are more amenable to data science than graphs. Graphs contain edges and nodes, those network relationships can only use a specific subset of mathematics, statistics, and machine learning. Vector spaces have a richer toolset from those domains. Additionally, vector operations are often simpler and faster than the equivalent graph operations. One example is finding nearest neighbors. You can perform "hops" from node to another node in a graph. In many real-world graphs after a couple of hops, there is little meaningful information (e.g., recommendations from friends of friends of friends). However, in vector spaces, you can use distance metrics to get quantitative results (e.g., Euclidian distance or Cosine Similarity). If you have quantitative distance metrics in a meaningful vector space, finding nearest neighbors is straightforward. " Graph Embedding Techniques, Applications, and Performance: A Survey " is an overview article that goes into greater detail.
