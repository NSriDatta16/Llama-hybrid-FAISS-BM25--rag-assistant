[site]: crossvalidated
[post_id]: 549886
[parent_id]: 
[tags]: 
Is it always possible to find a joint distribution $p(x,y)$ consistent with the results of both labs?

I am reading "Bayesian Reasoning And Machine Learning" and doing exercise 4.8 and would like to check if the following reasoning is correct. Two research labs work independently on the relationship between discrete variables $x$ and $y$ . Lab A proudly announces that they have ascertained distribution $p_A(x|y)$ from data. Lab B proudly announces that they have ascertained $p_B(y|x)$ from data. Is it always possible to find a joint distribution $p(x,y)$ consistent with the results of both labs? Is it possible to define consistent marginals $p(x)$ and $p(y)$ , in the sense that $p(x)=\sum_yp_A(x|y)p(y)$ and $p(y)=\sum_xp_B(y|x)p(x)$ ? If so, explain how to find such marginals. If not, explain why not. My attempt: First I want to say that I'm slightly confused as to why there is such a question, when the chapter is about Markov Networks/Chain Graphs/Factor Graphs. This doesn't seem connected to me. Could someone explain exactly how this is connected to those topics? So my answer to the first question is: Assuming the distribution of the sampe sets $S=\{(x,y)\}$ is random, it may not be possible to always come to the same conclusion about the relationship of the the variables because sometimes there may be a sample which doesn't properly represent the general trend in the data. Answer to 2: No, since as above stated, there may be misrepresentative samples. Could someone please say is this correctly answered?
