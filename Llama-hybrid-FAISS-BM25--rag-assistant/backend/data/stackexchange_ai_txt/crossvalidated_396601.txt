[site]: crossvalidated
[post_id]: 396601
[parent_id]: 172810
[tags]: 
I'm a bit (hum!) late, but you may look at (chronologicaly) : Robert D Gibbons, Giles Hooker, Matthew D Finkelman, David J Weiss, Paul A Pilkonis, Ellen Frank, Tara Moore, and David J Kupfer. The computerized adaptive diagnostic test for major depressive disorder (cad-mdd): a screening tool for depression. The Journal of clinical psychiatry, 74(7):1{478, 2013. Yichen Zhou and Giles Hooker. Interpreting models via single tree approximation . arXiv preprint arXiv:1610.09036, 2016. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining Black Box Models . ACM Comput. Surv. 51, 5, Article 93 (August 2018), 42 pages. (especialy the table on page 93:26) To be a little bit more specific about the oracle methods : among the authors referenced by Guidotti & alii above, Craven [1996, 2003] and Melville & Mooney [2006] uses gaussian distribution postulating zero covariance between features, but generating data as they build the tree (different estimation of variance for different branches of the tree). It's also possible to generate new data for the whole tree through a multivariate gaussian with mean and covariance matrix estimated on the whole training set, but the number of needed data quickly becomes very large. One may limit that problem by performing PCA on the training set and drawing the new data on a lower dimension subspace. One may also use deterministic methods. For instance (inspired by the Munge method from Bucilua & alii 2006) taking the middle point between every pair of data point in the training set (which obviously generate n^2 points if n is the size of the training set). For those interested (and for those wiling to correct my mistakes) I've given it a couple try and loaded the notebooks on Github here https://github.com/ljmdeb/GSTA
