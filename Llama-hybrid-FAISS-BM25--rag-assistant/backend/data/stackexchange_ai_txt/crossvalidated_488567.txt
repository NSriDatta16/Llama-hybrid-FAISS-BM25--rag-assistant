[site]: crossvalidated
[post_id]: 488567
[parent_id]: 450801
[tags]: 
Yes, it's the same concept and fundamental challenge for transfer learning (in terms of both human learning and machine learning. As Lerner mentioned, a good reference will be Characterizing and Avoiding Negative Transfer . Below are quick answers to your question. To understand the root cause of transfer learning, we need to first understand its objective: given a label-scarce target domain, we aim to borrow relevant samples from a label-rich source domain and build a model that performs better on the target domain than using the target-domain data alone. Thus, a fundamental task is how to select the relevant samples from the source domain. The selected samples are called either positive or negative samples, depending whether it improve or degrade the model performance on the target domain, respectively. The inclusion of irrelevant negative samples is called "negative transfer", as it doesn't help and even hurt the learning in the target domain. Why do we need to choose relevant samples? Because there is discrepancy in the joint distributions between the source and target domains. This is the root cause of negative transfer problem. I don't think negative transfer is something that can be prevented, but we need to select the relevant samples carefully to minimize its impact. For detailed sample selection methods, you can refer to "Domain Adversarial Network" and "Discriminator Gate" as mentioned in the above literature. Recently, there are also reinforcement learning based methods. Hope it clarifies you doubts.
