[site]: crossvalidated
[post_id]: 330255
[parent_id]: 273281
[tags]: 
In a system for which all of the physical occurrences have been properly modeled, the left over would be noise. However, there is generally more structure in the error of a model to data than just noise. For example, modelling bias and noise alone do not explain curvilinear residuals, i.e., unmodelled data structure. The totality of unexplained fraction is $1-R^2$, which can consist of misrepresentation of the physics as well as bias and noise of known structure. If by bias we mean only the error in estimating mean $y$, by "irreducible error" we mean noise, and by variance we mean the systemic physical error of the model, then the sum of bias (squared) and systemic physical error is not any special anything, it is merely the error that is not noise. The term (squared) misregistration might be used for this in a specific context, see below. If you want to say error independent of $n$, versus error that is a function of $n$ , say that. IMHO, neither error is irreducible, so that the irreducibility property misleads to such an extent that it confuses more than it illuminates. Why do i not like the term "reducibility"? It smacks of a self-referential tautology as in the Axiom of reducibility . I agree with Russell 1919 that "I do not see any reason to believe that the axiom of reducibility is logically necessary, which is what would be meant by saying that it is true in all possible worlds. The admission of this axiom into a system of logic is therefore a defect ... a dubious assumption." Below is an example of structured residuals due to incomplete physical modelling. This represents residuals from ordinary least squares fitting of a scaled gamma distribution, i.e., a gamma variate (GV), to blood plasma samples of radioactivity of a renal glomerular filtered radiopharmaceutical [ 1 ]. Note that the more data that is discarded ($n=36$ for each time-sample), the better the model becomes so that reducibility deproves with more sample range. It is notable, that as one drops the first sample at five minutes, the physics improves as it does sequentially as one continues to drop early samples out to 60 min. This shows that although the GV eventually forms a good model for plasma concentration of the drug, something else is going on during early times. Indeed, if one convolves two gamma distributions, one for early time, circulatory delivery of the drug, and one for organ clearance, this type of error, physical modeling error, can be reduced to less than $1\%$ [ 2 ]. Next is an illustration of that convolution. From that latter example, for a square root of counts versus time graph, the $y$-axis deviations are standardized deviations in sense of Poisson noise error. Such a graph is an image for which errors of fit are image misregistration from distortion or warping. In that context, and only that context, misregistration is bias plus modelling error, and total error is misregistration plus noise error.
