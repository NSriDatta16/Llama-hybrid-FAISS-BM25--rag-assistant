[site]: crossvalidated
[post_id]: 296733
[parent_id]: 231585
[tags]: 
Since an ounce of algebra is equal to a ton of words, let me write some formulas. Notation Denote $k( \cdot, \cdot )$ some covariance function, assume we have $m$ observations $(\mathbf x_i, y_i )_{i=1}^m$. Denote $$ \Sigma = \begin{bmatrix} k( \mathbf x_1 , \mathbf x_1 ) & \dots & k( \mathbf x_1 , \mathbf x_m ) \\ k( \mathbf x_2 , \mathbf x_1 ) & \dots & k( \mathbf x_2, \mathbf x_m ) \\ \vdots & & \vdots \\ k( \mathbf x_m , \mathbf x_1 ) & \dots & k( \mathbf x_m , \mathbf x_m ) \\ \end{bmatrix} \in \mathbb{R}^{m \times m }, \ k(\mathbf x) = \begin{bmatrix} k(\mathbf x, \mathbf x_1 ) \\ \vdots \\ k(\mathbf x, \mathbf x_m ) \end{bmatrix} \in \mathbb{R}^m,\ \mathbf y = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix} \in \mathbb{R}^m $$ and $$ X = \begin{bmatrix} ------ & \mathbf x_1^t & ------ \\ & \vdots & \\ ------ & \mathbf x_m^t & ------ \end{bmatrix}. $$ Gaussian Process Regression Gaussian Process Regression (GPR) gives the poserior for $\mathbf x$ as $$ y \sim \mathcal{N} (k^t(\mathbf x) \Sigma^{-1} \mathbf y, k(\mathbf x,\mathbf x) - k^t(\mathbf x) \Sigma^{-1} k(\mathbf x ) ). $$ This arises by assuming $(\mathbf y, y )$ are all jointly Gaussian with zero mean and a covariance structure specified by $k( \cdot, \cdot )$. That's the main idea and the rest is calculations using Schur complements. You would (probably) want to make a prediction based on either the posterior mean or the posterior mode. Luckily, in this case they are the same. You would predict, for a given $\mathbf x$: $$ y^{\star} = k^t(\mathbf x ) \Sigma^{-1} \mathbf y. $$ General Linear Model A General Linear Model (GLM) arises when you try to find the best linear model to describe observations with a given covariance structure (specified by $\Sigma$). You assume $$ \mathbf y = X \beta + \epsilon, \epsilon \sim \mathcal{N}(0,\Sigma). $$ Then the log-likelihood is $$ \log p(\mathbf y|X,\beta x) = -\frac{1}{2} (X \beta - \mathbf y )^t\Sigma^{-1} (X\beta - \mathbf y), $$ up to an additive constant. Then the following $\beta^{\star}$ is a Maximum Likelihood Estimator for $\beta$: $$ \beta^{\star} := \arg \min_{\beta} \| \Sigma^{-1/2} (X\beta - \mathbf y) \|_2^2 \\ = ( (\Sigma^{-1/2} X)^t (\Sigma^{-1/2} X) )^{-1} (\Sigma^{-1/2} X)^t \Sigma^{-1/2}\mathbf y \\ = (X^t \Sigma^{-1} X)^{-1} X^t \Sigma^{-1} \mathbf y. $$ Now, a prediction is made using this linear model as follows: $$ y^{\star} = \mathbf x^t \beta^{\star} = \mathbf x^t (X^t \Sigma^{-1} X)^{-1} X^t \Sigma^{-1} \mathbf y. $$ Conclusion The formulas for the posterior mean for GPR and the GLM predictor are clearly different, so this answers your question. A Few Comments One key difference is that a GLM does not take into account the covariance between $\mathbf x$ and $\mathbf x_i$, for any $i$. In the GPR model, this information on $\mathbf x$ enters via the vector $k(\mathbf x)$. Expanding on this point you can think of either one of these models as a weigting scheme used to get from $\mathbf y$ to $y$. In the GLM case, your weights are a linear function of $\mathbf x$ itself. In the GPR case, these weights are a still linear, but now in $k(\mathbf x, \cdot )$! More on this is the book, chapter 2. http://www.gaussianprocess.org/gpml/ The Gaussian Process model is Bayesian. It gives you a posterior distribution (of which you take the mean for prediction). The GLM is frequentist - no posterior distribution, just point estimates (for $\beta^{\star}$ and for $y^{\star}$).
