[site]: crossvalidated
[post_id]: 472064
[parent_id]: 471955
[tags]: 
Usually the losses are averaged over the minibatches; check the implementation of the specific loss function you're using in Keras for details. This answer might help. I think your interpretation is correct. Furthermore, I don't see a reason why this type of plot (train/val error over epochs) would ever be over something that isn't the entire training split. So by intra-training loss, you mean end-of-epoch loss over the split; by post-training loss, you mean end-of-training loss over the split. So since these losses should decrease over time, there's no reason to believe that your intra-training losses are good estimates of the post-training loss, given that you should (at least on train w/ sufficient batch size) always be asymptotically crawling towards the minimal loss, simply by virtue of the design of SGD/SGD-style iterative optimizers. I'm also keeping around my old answer to 3, since it seemed to be somewhat helpful. Old answer to 3: I'm unsure what you mean exactly by intra-training train and test loss vs. post-training difference, but I think I have a general idea based off your previous questions -- forgive me if I'm misunderstanding you. I think by intra-training loss, you mean the loss on a particular minibatch. Depending on the batch size, that'll affect how "good" of an estimator it is for the "post-training loss," which I think means the loss at the end of an epoch. This is a simple sample-size vs. variance argument. In practice, based on my experience, you should only have to worry about the end-of-epoch loss, so you don't have to worry about this question. Let me know if I'm interpreting that correctly. As for why your validation loss is lower than training loss -- this simply happens sometimes. There is no mathematical rule that says that this cannot happen; in fact, the theorems from stat ML (which might not even apply in the case of a DNN) merely provide a worst case bound (i.e. train/empirical error can be no more than O(something) above val/generalization error) given the suitable assumptions. This is probably a sign that your model is generalizing well on this particular dataset.
