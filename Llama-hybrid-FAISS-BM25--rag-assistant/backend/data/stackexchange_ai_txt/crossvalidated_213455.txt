[site]: crossvalidated
[post_id]: 213455
[parent_id]: 213201
[tags]: 
A glance at the manual for auto.arima shows that an explanation of precisely why it found the solution it did in this case would be complicated: depending on the fitting algorithm (conditional least squares by default); on the details of the stepwise selection procedure, & the criteria used (approximate AICc by default); & on the particular configuration & the sequence of the stationarity tests carried out. Nevertheless, I daresay that ARIMA(2,1,1) is a perfectly good model for one-step-ahead forecasts , because the evident smoothness of the seasonality means that knowing the observed value from exactly one year ago will give you little more information about this week's likely value than just knowing the last couple of weeks' observations. If you want to forecast over the next year or two , you will need to take seasonality into account. Rather than taking seasonal differences, or looking for seasonal autoregressive or moving average terms, or estimating 52 fixed seasonal effects; you might be better off taking advantage of the smoothness & regressing on a few Fourier terms, as explained by @Glen_b here . (This also makes it easy to allow for non-integer frequenciesâ€”as @RichardHardy ponts out might be appropriate in this case (if the extra day or two each year weren't simply dropped).) Your model will then have deterministic, but smoothed, seasonal effects, & you can still allow ARIMA errors: $$ y_t = \alpha + \sum_{i=1}^k \beta_i \sin \left(i \cdot \frac{2 \pi}{\omega}\cdot t\right) + \sum_{i=1}^k \gamma_i \cos\left(i \cdot \frac{2 \pi}{\omega} \cdot t \right) + \frac{1+\sum_{i=1}^q \theta_i B^i}{(1-B)^d\left(1-\sum_{i=1}^p \phi_i B^i\right)}\cdot\varepsilon_t $$ $$ \varepsilon\sim\mathrm{WN}(\sigma^2) $$ The frequency $\omega$ is assumed known. The parameters you need to estimate are: $\alpha$, the intercept; the $\beta$s & $\gamma$s, the coefficients of the sine & cosine terms terms; the $\phi$s & $\theta$'s, the autoregressive & moving average coefficients; & $\sigma^2$, the variance of the white noise innovations. The orders for the $\mathrm{ARIMA}(p,d,q)$ errors & the number of harmonics $k$ can be chosen heuristically or by using significance tests/information criteria. The forecast package has a function to create the Fourier terms; here's an illustration: library(astsa) library(forecast) # make a training set & test set window(cmort, start=c(1970,1), end=c(1977,40)) -> cmort.train window(cmort, start=c(1977,41), end=c(1979,40)) -> cmort.test # fit non-seasonal & seasonally-differenced models for comparison Arima(cmort.train, order=c(2,1,1)) -> mod.ns Arima(cmort.train, seasonal=c(1,1,0), include.drift=T) -> mod.sd # make predictors fourier(cmort, K=10) -> fourier.terms fourier.terms.train Up to the third harmonic seems to be enough, and you still need to difference. # fit models Arima(cmort.train, order=c(0,1,0), xreg=(fourier.terms.train[,1:6])) -> mod.ft3.df acf(residuals(mod.ft3.df), ci.type="ma", lag.max=52) acf(residuals(mod.ft3.df), type="partial", lag.max=52) Arima(cmort.train, order=c(2,1,0), xreg=(fourier.terms.train[,1:6])) -> mod.ft3.df.ar2 acf(residuals(mod.ft3.df.ar2), lag.max=52) acf(residuals(mod.ft3.df.ar2), type="partial", lag.max=52) Examining correlograms suggests a couple of AR terms will do to leave the residuals looking like white noise. Or you can investigate the number of harmonics to use, as well as the ARMA structure, usng auto.arima : # auto.arima auto.arima(cmort.train, xreg=fourier.terms.train[,1:2], seasonal=F, d=1, allowdrift, stepwise=F, approximation=F) -> auto1 auto.arima(cmort.train, xreg=fourier.terms.train[,1:4], seasonal=F, d=1, allowdrift, stepwise=F, approximation=F) -> auto2 auto.arima(cmort.train, xreg=fourier.terms.train[,1:6], seasonal=F, d=1, allowdrift, stepwise=F, approximation=F) -> auto3 auto.arima(cmort.train, xreg=fourier.terms.train[,1:8], seasonal=F, d=1, allowdrift, stepwise=F, approximation=F) -> auto4 auto.arima(cmort.train, xreg=fourier.terms.train[,1:10], seasonal=F, d=1, allowdrift=T, stepwise=F, approximation=F) -> auto5 auto1 auto2 auto3 auto4 auto5 The ARMA(2,1) structure is found by auto.arima 's minimizing the AICc for the model with 4 harmonics on differenced observations, & has the lowest AICc overall. # look at accuracy on test set plot(cmort.test, col="grey") lines(forecast(mod.ns,h=length(cmort.test))$mean, col="red") lines(forecast(mod.sd,h=length(cmort.test))$mean, col="darkorange") lines(forecast(mod.ft3.df.ar2,h=length(cmort.test), xreg=fourier.terms.test[,1:6])$mean, col="green") lines(forecast(auto4,h=length(cmort.test), xreg=fourier.terms.test[,1:8])$mean, col="blue") accuracy(forecast(mod.ns,h=length(cmort.test)),cmort.test) accuracy(forecast(mod.sd,h=length(cmort.test)),cmort.test) accuracy(forecast(mod.ft3.df.ar2,h=length(cmort.test),xreg=fourier.terms.test[,1:6]),cmort.test) accuracy(forecast(auto4,h=length(cmort.test),xreg=fourier.terms.test[,1:8]),cmort.test) Summarizing the root mean square errors shows that both the harmonic models perform better on the test set, with little to choose between them: $$ \begin{array}{l,l,l} &\text{Training} & \text{Test}\\ \mathrm{ARIMA}(2,1,1) & 5.729 & 7.657\\ \mathrm{SARIMA}(1,1,0)_{52}\text{ with drift} & 6.481 & 7.390\\ \text{3 harmonics, }\mathrm{ARIMA}(2,1,0) & 5.578 & 5.151\\ \text{4 harmonics, }\mathrm{ARIMA}(2,1,1) & 5.219 & 5.188 \end{array} $$
