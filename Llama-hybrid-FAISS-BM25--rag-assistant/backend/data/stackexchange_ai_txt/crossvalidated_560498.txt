[site]: crossvalidated
[post_id]: 560498
[parent_id]: 
[tags]: 
Training loss, validation loss and WER decrease, then increase

I am trying to use Hugginface Datasets for speech recognition using transformers using this tutorial , epochs=30, steps=400, train_batch_size=16. Training loss, validation loss and WER decrease, and then increase: [6321/7500 17:25:12 Is this because I have too many epochs? Overfitting? Or does it have to do with steps/batch_size?
