[site]: crossvalidated
[post_id]: 402434
[parent_id]: 342635
[tags]: 
The square root transformation is a totally acceptable way to proceed. I'd drop the weights = 1/(y+epsilon) idea. Good thinking but this is a hack that could depend pretty heavily on this choice of arbitrary epsilon. Check out the gls function from nlme . It will likely have what you need. > glmfit2 summary(glmfit2) Generalized least squares fit by REML Model: y ~ 0 + x Data: data AIC BIC logLik 632.624 640.4093 -313.312 Variance function: Structure: Power of variance covariate Formula: ~fitted(.) Parameter estimates: power 0.5014736 Coefficients: Value Std.Error t-value p-value x 1.022571 0.01231209 83.05423 0 Standardized residuals: Min Q1 Med Q3 Max -2.6004374 -0.7538303 0.1200103 0.6402228 2.2430706 Residual standard error: 0.8599757 Degrees of freedom: 100 total; 99 residual Look at that power estimate! Need to ensure your coefficient is greater than 0? Then reparameterize $\beta = \exp(\theta)$ and use the nonlinear version of gls, gnls : > glmfit3 summary(glmfit3) Generalized nonlinear least squares fit Model: y ~ exp(theta) * x Data: data AIC BIC logLik 625.6613 633.4768 -309.8306 Variance function: Structure: Power of variance covariate Formula: ~fitted(.) Parameter estimates: power 0.49894 Coefficients: Value Std.Error t-value p-value theta 0.02232537 0.01202577 1.856461 0.0664 Standardized residuals: Min Q1 Med Q3 Max -2.5979856 -0.7547549 0.1201196 0.6395667 2.2459128 Residual standard error: 0.8679793 Degrees of freedom: 100 total; 99 residual > exp(.022325) [1] 1.022576 You mention OLS is "more accurate" than GLS for your example. Let the seed go and run the simulation 1000 times. See which is closer to the truth on average.
