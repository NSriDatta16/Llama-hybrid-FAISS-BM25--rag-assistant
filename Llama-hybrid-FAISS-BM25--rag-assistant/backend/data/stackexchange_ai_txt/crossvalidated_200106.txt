[site]: crossvalidated
[post_id]: 200106
[parent_id]: 200102
[tags]: 
The simple answer is that both "cases" and "controls" are used in building a model. You need to know characteristics associated with both in order to distinguish them. Once the model is built, if you know the odds, say, of having the disease then there is no difficulty in figuring the odds of not having the disease. I'm unaware of any approaches to regression/classification that will be more or less efficient computationally depending on which is chosen as the "target" class. There is one practical consideration, however: the number of variables you can reasonably include in a model is limited by the least-frequent class. For example, in standard logistic regression you should not consider more than 1 predictor variable per 15 cases in the least-frequent class. Your question, however, raises the important and sometimes overlooked issue of whether a particular analysis should care more about "cases" or "controls." The usual "classification accuracy" scores based on numbers of mis-classifications or areas under receiver operating characteristic curves have an implicit assumption that both types of mis-classifications are equally important. That's seldom the case in practice, even with the same model. For example, say you had a model predicting the probability that a company would fail within 5 years, and that failures are the "cases." If you are a conservative investment banker, you would probably want to avoid "cases" in your portfolio. If you were a venture capitalist, you would probably want to factor in the potential gains along with the probability of being a "case." Same model, different uses. That's one reason why on this site you will often find recommendations to build models that predict probabilities of class membership, as with logistic regression, rather than a simple yes-no classification scheme. This page and its links go into more depth.
