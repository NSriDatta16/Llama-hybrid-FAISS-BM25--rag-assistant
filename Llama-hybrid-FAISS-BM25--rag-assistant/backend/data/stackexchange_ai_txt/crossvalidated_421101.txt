[site]: crossvalidated
[post_id]: 421101
[parent_id]: 421098
[tags]: 
First of all, there's no assumption made when you fit a linear regression, i.e. optimize its coefficients to minimize some risk function. There are a few assumptions made when you want to apply tests over the coefficients. However, I realise linear regression and GLM are not appropriate for time series. Why not though? What assumptions are violated if one used such models on times series data with autocorrelation? You said it yourself. Independence of error terms is often violated when you perform a hypothesis test on the value of the coefficient. There are remedies to this though. This already works quite well for predicting future values of $y$ If this was your objective all along, it's performing well on out of sample data, and you do not intend to perform hypothesis tests, then by all means keep with the linear regression.
