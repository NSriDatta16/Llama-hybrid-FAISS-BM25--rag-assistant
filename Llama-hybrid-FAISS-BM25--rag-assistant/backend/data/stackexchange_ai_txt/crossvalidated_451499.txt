[site]: crossvalidated
[post_id]: 451499
[parent_id]: 
[tags]: 
Do Recurrent Convolutional Layers share parameters?

I'm reading the paper Recurrent Convolutional Neural Networks for Object Recognition . I don't understand the idea of weight sharing explained in the article. I am confused between the two following explanations : (I've omitted the bias in equations and the images are taken from the article above, where I drew the red abominations) From the left image : There are two sets of weights. The feed-forward ones ( $W_f$ ) and the one in the hidden recurrent unit ( $W_r$ ) where : $t_0 = W_f*x$ $t_n = W_f*x + W_r*t_{n-1}$ From the right image : The feed-forward weights stay the same but every time there is a new "time step", there are new weights. $t_0 = W_f*x$ $t_n = W_f*x + W_{rn}*t_{n-1}$ I have a feeling the answer is the left image, but I don't understand how doing this would be useful (so, I guess that's a part two of my original question).
