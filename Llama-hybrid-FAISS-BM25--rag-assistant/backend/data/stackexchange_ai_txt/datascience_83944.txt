[site]: datascience
[post_id]: 83944
[parent_id]: 83934
[tags]: 
Neural tools trained on Universal Dependencies corpora use learned models for tokenization and sentence-spliting. Two I know of are: UDPipe – developed at Charles University in Prague. Gets very good results (at least for parsing), but has a little unintuitive API. Stanza – developed at Stanford University. The API is quite similar to Spacy. However, they are quite slow compared to regex-based sentence-spliting.
