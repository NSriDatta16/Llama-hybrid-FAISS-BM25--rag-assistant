[site]: datascience
[post_id]: 126907
[parent_id]: 126904
[tags]: 
Large beam sizes do not lead to improvements but to degradation in the generated text quality, as described in the article Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models . Also, article On NMT Search Errors and Model Errors: Cat Got Your Tongue? gives a nice insight into the problems of beam search for translation. In general, the beam size is a hyperparameter that must be tuned. However, given that the "good values" are in a small range, the same value is normally used (for each task) instead of exploring different options. In the first mentioned article, you can find a table with an analysis of different beam sizes for different tasks: In machine translation, the typical value used in most papers is 4-5. For instance, in the article that proposed the Transformer architecture ( Attention is all you need ), the beam size was 4. Note that it is possible to mitigate the problems of large beam sizes by e.g. normalizing scores by sentence length. For instance, in Six Challenges for Neural Machine Translation we can see that, in machine translation for some language pairs (see German-English, in Figure 10), there is no degradation with increasingly large beam sizes. Normally, the beam size value is not changed for different input distributions (e.g. different domains). At least there seems to be no literature on the matter. Nevertheless, there are other popular decoding strategies apart from beam search. For instance, in LLMs, it's typical to use temperature sampling (the generated token is sampled from the probability distribution after applying a temperature factor $\alpha$ , which can either flatten the distribution or sharpen it) or top_p/nucleus sampling (you sample from the probability distribution, but only consider the top probability tokens that add up to a specific cumulative probability p). You can check this answer for more decoding strategies.
