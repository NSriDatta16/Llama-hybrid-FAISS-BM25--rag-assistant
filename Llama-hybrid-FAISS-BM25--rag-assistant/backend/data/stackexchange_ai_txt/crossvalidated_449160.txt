[site]: crossvalidated
[post_id]: 449160
[parent_id]: 
[tags]: 
test error lower than training error for several algorithms

I just read this question but all the answers are focused on why this is happening when using a neural network. I'm using random forest, elastic net and Cubist. Both elastic net and Cubist have lower test set error than training set error. I'm using the tidymodels package in R to split the data, make recipe and all that. So I'm pretty confident that there is no data leakage or that the data is not split randomly. I split the data 60-40 and use 3 repeats of 10-fold cross-validation. The total size of the data is 10.000 observations and 90 variables of which 80 are dummy variables that were created out of categorical variables in the original dataset. The distribution for the outcome is identically distributed between train and test set and no outliers in the training set, but possibly one in the test set. Is there any reason why the test set error is lower than the training set error in my case?
