[site]: crossvalidated
[post_id]: 980
[parent_id]: 
[tags]: 
How do I reduce the number of data points in a series?

I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand. Anyway, what I want to do is reduce the number of data points in a series. The x-axis is number of milliseconds since start of measurement and the y-axis is the reading for that point. Often there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points? What is the process called? (So I can google it) Are there any prefered algorithms (I will implement it in C#) Hope you got some clues. Sorry for my lack of proper terminology. Edit: More details comes here: The raw data I got is heart rate data, and in the form of number of milliseconds since last beat. Before plotting the data I calculate number of milliseconds from first sample, and the bpm (beats per minute) at each data point (60000/timesincelastbeat). I want to visualize the data, i.e. plot it in a line graph. I want to reduce the number of points in the graph from thousands to some hundreds. One option would be to calculate the average bpm for every second in the series, or maybe every 5 seconds or so. That would have been quite easy if I knew I would have at least one sample for each of those periods (seconds of 5-seconds-intervals).
