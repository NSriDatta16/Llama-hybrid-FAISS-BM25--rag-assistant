[site]: crossvalidated
[post_id]: 431373
[parent_id]: 429924
[tags]: 
For a given question and a given examinee, the probability of a correct answer is $logit(a(\theta - b))$ . Most precise estimations of a probability parameter come when it is near to 0.5, so its logit should be near 0, so here $\theta$ should be near to $b$ . Of course, as $\theta$ is assumed to distribute as a standard normal hidden variable, estimations of $b$ are most precise when $b$ is close to 0; any value far from 0 (they come from questions with a lot of imbalance between correct and incorrect answeres) generally has a bigger standard error. I want to point out that when $b$ is close to 0 also estimates of $a$ get better, to put it in other words: better balance between correct and incorrect answers brings to better estimation of association between questions. On the other hand, with this formulation of the model you can see by yourself that $a$ heavily affects estimated value of $b$ . In contrast to that, I don't quite agree with the second claim. If $\theta$ presents a big range, this means that it has heavy tails, or outliers, points with great leverage that have the power to invalid the model. On the other hand, since variance of $\theta$ is fixed, when it has more outliers, its normal values are shrinked, so, in my understanding any good effect on $\hat a$ should be annulled. Let's see if someone brings up some good explanation for that claim. Standard errors of $a$ and $b$ is given by observed Fisher information, no closed form exists for it.
