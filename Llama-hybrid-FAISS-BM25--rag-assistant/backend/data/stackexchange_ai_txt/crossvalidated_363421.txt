[site]: crossvalidated
[post_id]: 363421
[parent_id]: 363406
[tags]: 
This is a constrained optimization problem. Practically speaking when looking at solving general form convex optimization problems, one first converts them to an unconstrained optimization problem (e.g., using the penalty method , interior point method , or some other approach) and then solving that problem - for example, using gradient descent, LBFGS, or other technique. If the constraints have a "nice" form, you can also use projection (see e.g. proximal gradient method ). There are also very efficient stochastic approaches, which tend to optimize worse, but generalize better (i.e., have better performance at classifying new data). As well, your formulation doesn't appear to be correct. Generally one has $\alpha_i \leq C$ for hinge-loss SVM. If one uses e.g. square loss, then that constraint wouldn't be present, but your objective would be different.
