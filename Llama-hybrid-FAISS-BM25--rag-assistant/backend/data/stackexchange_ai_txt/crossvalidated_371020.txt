[site]: crossvalidated
[post_id]: 371020
[parent_id]: 370978
[tags]: 
You can perform the Bayesian updating of the parameters as you would with an unconstrained $\lambda$ , then adjust the posterior to reflect the constraints by limiting the range of $\lambda$ to $[0.2,0.7]$ and renormalizing appropriately. In your case, as you realized, you'd have to be able to integrate the posterior Beta distribution to find the normalization constant. If, by good fortune or planning, you've picked a prior on $\lambda$ with integer parameters $a$ and $b$ (as you have done in the question), then the posterior will have integer parameters $a'$ and $b'$ , and there is a closed form solution for the incomplete beta function: $$I_x(a,b) = {1\over \text{B}(a,b)}\int_0^xp^a(1-p)^b\text{d}p=\sum_{j=a}^{a+b-1}{a+b-1\choose j}x^j(1-x)^{a+b-1-j}$$ ... admittedly awkward if $a$ and/or $b$ are large. A check of the formula, using R: Ix and comparison with the cumulative Beta distribution in the same language: > Ix(0.4,3,5) [1] 0.580096 > pbeta(0.4,3,5) [1] 0.580096 In your case, the normalization of the posterior would involve dividing by $I_{0.7}(a',b')-I_{0.2}(a',b')$ . If, on the other hand, you don't have integer parameters, you will be forced to resort to using an infinite series expansion or numerical integration; the latter will probably work well enough given the smoothness of the functions. Of course, languages such as R and Python / scikit have functions that will evaluate the CDF of the Beta distribution for you, so you can avoid the whole issue by simply using the available canned routines to find the normalization constant.
