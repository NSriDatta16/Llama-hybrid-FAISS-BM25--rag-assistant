[site]: crossvalidated
[post_id]: 599789
[parent_id]: 
[tags]: 
Does pruning Deep Neural Networks at initialization even make sense?

I recently started exploring pruning methods for Deep Neural Networks and stumbled on some interesting papers suggesting algorithms for unstructured pruning at initialization (e.g. SNIP ), i.e. removing single weights (elements in the connection matrices) instead of full neurons (rows/columns of the matrix, depending on notation). Most research papers state that pruning leads to reductions in training time and memory requirements, which I could not verify in PyTorch. I tried using a (very basic) magnitude based pruning approach at initialization to verify the assumptions that the training and inference time is reduced via pruning. However, both increase significantly when pruning is applied. Follow the link to see a minimal working example as reference. Pruning is implemented via a binary mask that is element-wise applied in each forward pass (the pruned weights appear to still be updated in the backward pass). The same appears to be the case in Tensorflow. Naturally, adding a matrix multiplication per layer leads to an increase in computational time, which I also observed in the notebook provided above. When reading the literature, I expected that pruning makes use of the resulting sparsity structure in some way, but it appears as if the necessary implementations are still missing. My question now is: Are the stated gains in efficiency from the papers purely theoretical (for now)? Or is there a way to actually use pruning at initialization to decrease the training time and subsequently the inference time? Thanks in advance!
