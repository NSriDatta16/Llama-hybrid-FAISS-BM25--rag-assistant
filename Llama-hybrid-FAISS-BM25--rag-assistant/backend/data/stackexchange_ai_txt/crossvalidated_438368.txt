[site]: crossvalidated
[post_id]: 438368
[parent_id]: 
[tags]: 
FastText or GloVe for code-mixed sentiment analysis?

I am currently working on a project for code-mixed sentiment analysis (English+Spanish). I've been using the GloVe Twitter word embeddings so far but I realized that even though this representation is multilingual, it is most likely not aligned between languages , which I think impacts the quality of the sentence representation. So I am trying out the FastText aligned word embeddings downloaded here and I've been computing cosine similarities of English/Spanish words within GloVe and same with the respective aligned files, and it shows stronger similarity in fastText for actually closer words, and stronger dissimilarity between opposite words (see code below). What I need is an opinion on these assumptions: - Results below are a good indicator that I should rather use fastText embeddings which will allow better representation. - If yes, should I use fastText embeddings by concatenating Spanish-English files? - If yes, I should deal with duplicate words by averaging their vectors in order to keep it simple. code: >>> from fasttext import FastVector >>> #loading embeddings >>> e = FastVector(vector_file='./fastText/wiki.en.align.vec') reading word vectors from ./fastText/wiki.en.align.vec >>> s = FastVector(vector_file='./fastText/wiki.es.align.vec') reading word vectors from ./fastText/wiki.es.align.vec >>> g = FastVector(vector_file='./glove.twitter.27B/glove.twitter.27B.200d.txt') reading word vectors from ./glove.twitter.27B/glove.twitter.27B.200d.txt >>> #compute cosine similarity >>> def f(emb1, emb2, w1, w2): ... w_en = emb1[w1] ... w_es = smb2[w2] ... print(FastVector.cosine_similarity(w_en, w_es)) ... >>> f(e, s, "dog", "perro") 0.5871925259757504 >>> f(g, g, "dog", "perro") 0.36327244845041556 >>> f(e, s, "dog", "gato") 0.43565656007837456 >>> f(g, g, "dog", "gato") 0.31090089925549624 >>> f(e, s, "murder", "alegria") -0.05121494642496044 >>> f(g, g, "murder", "alegria") -0.011452790766085442 >>> f(e, s, "murder", "asesinato") 0.5567438582337103 >>> f(g, g, "murder", "asesinato") 0.2374400637804554
