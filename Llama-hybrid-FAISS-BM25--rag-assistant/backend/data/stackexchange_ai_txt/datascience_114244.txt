[site]: datascience
[post_id]: 114244
[parent_id]: 
[tags]: 
Multi head self attention output size for batches with different sequence length

I have a question regarding the self attention layer of transformers. When dealing with sequences of varying lengths in a mini-batch, we pad sequences so that all sequences in the batch have the same length. Let's say that that most sequences in a dataset are
