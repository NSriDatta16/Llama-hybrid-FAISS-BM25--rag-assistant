[site]: datascience
[post_id]: 70031
[parent_id]: 
[tags]: 
What is the better model architecture and setting when using merge layers?

I am building a deep learning model with dense, dropout, and merge layers. The inputs will be N sentences' feature encoded by BERT (768 dim) and then each will go into the same dense layer as the first downstream then following by dropout. N inputs are done at this moment and since assuming maximum only 1 label will exist in every sentence but multi labels for the sentences set. I use the merged layer at the end, hope to summarize the results from the results. An example of input features and label as below: Features Labels Vector 1, Vector 2, ..., Vector N (1*768 for each) A, B Vector 1, Vector 2, ..., Vector N (1*768 for each) J Vector 1, Vector 2, ..., Vector N (1*768 for each) H, Y, Z Currently, the model I used is like class TestModel(keras.Model): def __init__(self, input_dim=(768,), max_sents=6, num_cls=1200): super(TestModel, self).__init__() self.Hidden_1 = keras.layers.Dense(256, activation='relu', name='BERT_DOWSTREAM') self.Hidden_2 = keras.layers.Dropout(.25) self.LayOut = keras.layers.Dense(num_cls, activation='sigmoid', name='OUTPUT') self.Combines = keras.layers.Maximum() self.inputs = [keras.layers.Input(shape=input_dim) for i in range(max_sents)] tmp = [self.Hidden_1(inpt) for inpt in self.inputs] tmp = [self.Hidden_2(inpt) for inpt in self.inputs] tmp = [self.LayOut(inpt) for inpt in self.inputs] self.outputs = self.Combines(tmp) _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= BERT_DOWSTREAM (Dense) (None, 256) 196864 _________________________________________________________________ dropout (Dropout) (None, 768) 0 _________________________________________________________________ OUTPUT (Dense) (None, 1200) 922800 _________________________________________________________________ maximum (Maximum) (None, 1200) 0 ================================================================= Total params: 1,119,664 Trainable params: 1,119,664 Non-trainable params: 0 _________________________________________________________________ My questions are Is this good to use sigmoid in self.LayOut or softmax? Is this the correct way to set the merge layer (maximum) there? or any better way to do it? (A bit out of question) Any better way to solve this kind of problem? Many thanks for any ideas, suggestion or relative research sources posted.
