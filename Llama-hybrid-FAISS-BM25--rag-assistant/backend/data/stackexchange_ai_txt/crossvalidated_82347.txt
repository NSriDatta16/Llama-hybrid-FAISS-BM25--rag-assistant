[site]: crossvalidated
[post_id]: 82347
[parent_id]: 82309
[tags]: 
Sampling problems of this type are usually approached in one of two ways: Tabular sampling, which exploits cumulative distribution function: compute the values of the posterior over a grid of many points and set the total probability of all the points in the grid together equal to one. Then draw values from the uniform distribution over the unit interval, and select the posterior value at the nearest grid point. Add some uniformly-distributed noise over the uniform $[-a, a]$ interval where $a$ is the distance between adjacent points in the grid in order to jitter your draw to "fill in" the parameter space between grid points. This is described in Gelman et al., Bayesian Data Analysis , 3rd Ed, p. 23. Numerically summing over the density less than some value will give you the probability of drawing a value below that threshold. Monte Carlo methods, e.g. Gibbs Sampling. Any explanation I write would be worse than a published reference, so I would recommend Gelman again starting on page 275. These methods implicitly compute the integral in the sense that the proportion of samples less than some value is approximately the probability of a value less than that value. You raise an interesting point with numerical integration. Runge-Kutta algorithms and the like compute the integral directly. Problems arise with computational efficiency, selecting step-size, and knowing where in multidimensional space you want to start integrating from (Gelman 262). Adding dimensions to the parameter space exacerbates all of these problems, but for a single-parameter problem it should work well enough.
