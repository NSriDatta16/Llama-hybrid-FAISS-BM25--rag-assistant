[site]: crossvalidated
[post_id]: 275430
[parent_id]: 239341
[tags]: 
The gradient of sigmoid function is $$ (1-\sigma) \sigma $$ So if the input is $[0,1]$ then the output of the gradients on $w$ could only be all positive or all negative. You can check more on the notebook . Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x>0$ elementwise in $f=w^Tx+b$)), then the gradient on the weights ww will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$).
