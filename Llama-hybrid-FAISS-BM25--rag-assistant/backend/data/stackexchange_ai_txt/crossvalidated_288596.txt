[site]: crossvalidated
[post_id]: 288596
[parent_id]: 
[tags]: 
Relationship between RNNs and n-gram language models

Is there a correspondence between the hidden states of RNNs (LSTMs and GRUs) and the n-gram models?. Karpathy et al gave an empirical evidence that the hidden state of LSTM does keeps track of structure in the sequence. Also these models are unable to capture features beyond a certain number of time steps (due to vanishing gradient), which is something similar to what an n-gram model does (keeps track of previous few timesteps only).
