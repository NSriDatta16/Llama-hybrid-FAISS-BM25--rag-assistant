[site]: crossvalidated
[post_id]: 320803
[parent_id]: 320774
[tags]: 
It one sense, it can: Your second equation is the update rule for Q-learning. (See the definitions in Ch. 6 of Sutton and Barto's draft edition of Reinforcement Learning , along with the relation $V(S) = \max_a Q(S,a)$. I don't have a written source for this at hand, but it's covered in early sections of this course .) The major difference is that SARSA is on-policy : It learns the $Q$ values of the policy that it's following. Off-policy learners, Q-learning included, improve a policy different from that used to generate the data. Here's a summary the differing advantages between on- and off-policy, again from Sutton and Barto: On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge. On the other hand, off-policy methods are more powerful and general. They include on-policy methods as the special case in which the target and behavior policies are the same. Off-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. Ch. 6 of the draft, very much worth a read, also includes a comparison of SARSA and Q-learning from several perspectives, to include performance on an example and conditions for convergence to the optimal policy.
