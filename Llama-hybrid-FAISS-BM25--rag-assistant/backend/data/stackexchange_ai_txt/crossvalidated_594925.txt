[site]: crossvalidated
[post_id]: 594925
[parent_id]: 
[tags]: 
PCA evaluation with Shannon Entropy

I'm working on a fairly large dataset (5e5 samples in a 22 dimensional space); I'm reducing the dimensionality of this dataset with PCA. Since a lot of variables are cross-correlated, I group the variables that are highly correlated and then I collapse them in one variable by projecting them on thier first PC [1]. I want to evaluate how much information I lose in this process, so I evaluate the shannon entropy of the dataset before and after dimensionality reduction. I estimate the Shannon entropy in the following way: I estimate the probability densitiy function of the dataset with KDE; I estimate the Shannon entropy as: $H = \sum_i pdf(x_i)\log({pdf(x_i)^{-1}})$ ; For computational reasons, I use the first 1000 samples to estimate the entropy. To my surprise, the entropy is higher in the reduced dataset; I'd expect to lose information in dimensionality reduction. Is my code wrong, or the increase in entropy is due to the fact that -by looking at the first 1000 samples- I'm looking at a sort of "density of information" that increases because PCA better condensates information? Below is my code: Imports: import numpy as np import pandas as pd from matplotlib import pyplot as plt import math as m from sklearn.neighbors import KernelDensity as KDE from sklearn.decomposition import PCA Entropy estimation: def pdfKDE(X): kde = KDE(bandwidth=1,kernel='gaussian').fit(X) return kde.score_samples(X) def H(X): pdf = pdfKDE(X) pdf = np.exp(pdf) h = 0. for p in pdf: h += p*m.log(p**-1) return h PCA aggregation of correlated variables: # corrMatrix is the Pearson's correlation factor matrix between variables def pcaDatasetReduction(X,corrMatrix,treshold=0.9): dumpedVariables = [] aggregatedVariables = [] reducedDataset = [] pca = PCA(n_components=1) # loop through each row for i in range(corrMatrix.shape[0]): # if the variable was already reduced, ignore if i in dumpedVariables: continue # loop through each column for j in range(i+1,corrMatrix.shape[0]): # if a variable is too highly correlated if corrMatrix[i,j] >= treshold: if j not in dumpedVariables: dumpedVariables.append(j) aggregatedVariables.append(j) # if some correlated variables are found # aggregate them if aggregatedVariables != []: dumpedVariables.append(i) aggregatedVariables = [i] + aggregatedVariables # extract correlated variables corrVar = X[:,aggregatedVariables] # Find PC pca.fit(corrVar) # Project variables on PC projVar = pca.transform(corrVar) # save reduced dataset reducedDataset.append(projVar) aggregatedVariables = [] # else, if the variable was not merged, keep the variable in the dataset elif i not in dumpedVariables: reducedDataset.append(X[:,i].T) else: pass reducedDatasetNP = reducedDataset[0] for array in reducedDataset[1:]: reducedDatasetNP = np.column_stack((reducedDatasetNP,array)) return reducedDatasetNP [1] I used this strategy because some variables in my dataset are strongly correlated, while others are uncorrelated. So, instead of projecting all data on the first $p$ principal components, I decided to keep unmodified the variables that are uncorrelated to all of the others and to aggregate the correlated ones. Is this a good approach?
