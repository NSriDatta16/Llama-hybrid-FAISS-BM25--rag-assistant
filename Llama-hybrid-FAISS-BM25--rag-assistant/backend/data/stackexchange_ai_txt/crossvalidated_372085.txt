[site]: crossvalidated
[post_id]: 372085
[parent_id]: 372080
[tags]: 
Your premise is wrong, it is not true that every (supervised or not) machine learning model assumes that the data consists or independent and identically distributed observations. If it was true, we wouldn't be able to model things like time-series, where the subsequent observations depend on each other, or other cases where the observations are correlated. For such cases we design models that assume dependence and take it into consideration, e.g. the time-series models, or recurrent neural networks. Moreover, you don't need RNN's to find dependence assumption in neural network architectures: in convolutional neural networks you use kernels that share weights and slide through the picture (or something else), so basically you assume that the collection of pixels within the kernel forms some kind of pattern i.e. depends on each other. You may also find the following thread interesting: On the importance of the i.i.d. assumption in statistical learning
