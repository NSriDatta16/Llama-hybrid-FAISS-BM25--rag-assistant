[site]: crossvalidated
[post_id]: 400753
[parent_id]: 400751
[tags]: 
The loss function is not just a yes/no prediction, it takes into account how confident the model was about correct and wrong predictions. As you may realize, a sigmoid acitivation actually predicts something that is a lot like a probability (for a neural network it is usually not nicely calibrated like a proper predicted probability). In contrast, accuracy is just a proportion of correctly classified items. Correctly classified in the context simply means those cases, for which the predicted probability is above the chosen threshold for positive examples and below for negative examples; you are probably just using a threshold of 0.5. So, if for the images your model gets wrong, it keeps being more and more confident about its wrong predictions (and/or less confident about the correct ones), your loss function can get worse without the accuracy notably changing.
