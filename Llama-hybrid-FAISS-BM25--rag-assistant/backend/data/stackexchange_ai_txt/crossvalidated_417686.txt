[site]: crossvalidated
[post_id]: 417686
[parent_id]: 
[tags]: 
Combining together principal components from PCA performed on different subsets of a large dataset

I'm trying to QA a process in which the data has over a million rows with approx 60,000 variables in a binary form. The aim of the process was to perform k-means clustering, but prior to this, the 60,000 variables were put through PCA to reduce dimensionality. My issue is that the data was split into batches of 5000 variables, so there were 12 separate PCAs conducted on 5000 variables each, then 500 PC's were kept from the 12 and then merged together. My knowledge of PCA doesn't extend too far from the basics taught at university, but I just have a bad feeling that we may be missing out on too many of the correlations between variables that are separated into their own PCA. Am I right to be concerned? Is there another approach that would be better, or is there a way to quantify what we may be losing out by doing this?
