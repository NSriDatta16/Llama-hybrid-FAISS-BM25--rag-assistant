[site]: crossvalidated
[post_id]: 202126
[parent_id]: 
[tags]: 
How to infer the likelihood ratio between two examples shown to a Neural Network?

How can I construct a neural net enabling me to efficiently estimate the likelihood of some input relative to some other input? (i.e. a likelihood ratio) An example : Let's say we train this network on unlabeled pictures of animals. It just so happens that there are four times as many depictions of foxes in the training set as there are of hens. Now we present this network input from a test set: One image of a fox, and one image of a hen. We are then able to glance from the network their relative likelihood (a ratio of 4, in this case, assuming the network has learnt the relevant concepts correctly). Relative likelihood of the input seems to be encoded in the derivative of the model's log density for some models, notably those models involving Score Matching or Ratio Matching. The specifics elude me though: What kind of architectures can be used? Which training criteria are suitable? How to obtain a likelihood ratio at test time? Which papers/resources are relevant to this topic? When all is said and done I'd like to apply this technique to a deep convolutional autoencoder. I suspect that the intractability of the partition function may make an estimate of the absolute likelihood either more complex or more expensive, and for my application a likelihood ratio is sufficient anyway.
