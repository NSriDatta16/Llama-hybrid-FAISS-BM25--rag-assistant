[site]: crossvalidated
[post_id]: 641554
[parent_id]: 378454
[tags]: 
You are asking for a situation where "proportional likelihoods would lead one to markedly different (and equally defensible) inferences" with such a circumstance leading to a violation of the likelihood principle. However, in my view, the idea that it is a violation of the principle for inferences to differ when the likelihood functions are proportional requires a misconstrued likelihood principle. Instead of paraphrasing the likelihood principle as saying that when the likelihood functions are proportional then the inferences are the same, we should say that it is the evidence that is the same, and that that evidence is the evidence in the data concerning parameter(s) of interest in the statistical model, according to that statistical model. How to go from evidence to inference is, in my considered view, outside the scope of the likelihood principle. How can I justify such an assertion. Well, for one thing, the statements of the likelihood principle by Birnbaum (1962 and 1969) both write about evidential equivalence of results that yield equal likelihood functions and do not mention inference at all. A much more straightforward definition of the likelihood principle, and one that I prefer for that straightforwardness is that of Edwards (1972, p.31): Within the framework of a statistical model, all of the information which the data provide concerning the relative merits of two hypotheses is contained in the likelihood ratio of those hypotheses on the data, and the likelihood ratio is to be interpreted as the degree to which the data support one hypothesis against the other. Note that the "hypotheses" mentioned by Edwards are nothing more than values that might be taken by the parameter(s) of interest in the statistical model. A likelihood function that does not say how inferences should be formed is much more limited in scope than one that does, but it would be a much better match to most inferential practices. Consider inferences being made by a Bayesian. They are informed by the evidence in the form of likelihood functions, but also by the priors. Consider the inferences being made by a scientist; they are commonly informed by the results of statistical analyses (rarely in the form of a likelihood function, in my experience), but should also be informed by all of the relevant information including the results of other analyses past and present, as well as theory. That extra information might be incorporated formally via, for example, an expert prior, but it is also often incorporated informally. Now consider the 'inferences' of a frequentist using the Neyman–Pearsonian methods. Those inferences take the form of discarding, or not, the null hypothesis and they are decided using an algorithm that is designed to protect the long run, global, error rates rather than to respond to local evidence in the data concerning hypotheses of interest. Neyman and Pearson were explicit about that. (Yes, Pearson later moved away from that approach.) I only bring up the Neyman–Pearsonian methods because all of the examples where there is some conflict with the likelihood principle seem to involve those methods. The binomial/negative binomial case that is included in the original question is an excellent example. Proportional likelihoods with different p-values that might lead to different significant/not significant outcomes: horror! Experiments where there has been repeated 'testing' that terminate at the whim of the experimenter: double horror! The evidence in those results concerning values of the parameter(s) of interest are in the likelihood functions, but there is no compulsion to make any particular inference. The evidence expressed in a likelihood function may or may not differ between a model that takes the sampling procedure into account and one that does not, but that evidence does not force any particular inference according to the likelihood principles of Birnbaum and Edwards. The distinction between testing hypotheses in a way that privileges global error rates and ways that expose the local evidence in the data is important, but is rarely mentioned. I therefore have found many occasions to have linked to my own work on that topic and will do so again here , as it is relevant to experienced statisticians as it is to the scientist-statisticians that were the intended audience of that particular work. I recognise that a principle that merely tells us what equivalent evidence looks like would be much less influential and consequential than a principle that tells us how to make inferences, but that really is all that the likelihood principle can be. References Birnbaum, A. (1962), ‘On the foundations of statistical inference’, Journal of the American Statistical Association 57(298), 269–306. Birnbaum, A. (1969), Concepts of statistical evidence, in ‘Essays in honor of Ernest Nagel: Philosophy, science, and method’, St. Martin’s Press, New York. Edwards, A.W.F. (1972), Likelihood: an account of the statistical concept of likelihood and its application to scientific inference, Cambridge University Press, Cambridge.
