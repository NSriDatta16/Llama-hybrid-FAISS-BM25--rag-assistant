[site]: crossvalidated
[post_id]: 122108
[parent_id]: 122103
[tags]: 
I'm not sure if this will constitute a complete answer for you, but it may help break free the conceptual logjam. There seem to be two misconceptions in your account: Bear in mind that ordinary least squares (OLS--'linear') regression is a special case of the generalized linear model. Thus, when you say "[t]ransforming a response variable does NOT equate to doing a GLM", this is incorrect. Fitting a linear model or transforming the response variable and then fitting a linear model both constitute 'doing a GLM'. In the standard formulation of GLMs, what you call " $u$ " (which is often represented by $\mu$ , but this is just a matter of preference) is the mean of the conditional response distribution at a specific location in the covariate space (i.e., $X$ ). Thus, when you say "where $u$ is just another symbol for $y$ ", this is also incorrect. In the OLS formulation, $Y$ is a random variable and/or $y_i$ is a realized value of $Y$ for observation / study unit $i$ . That is, $y$ (more generically) represents data , not a parameter . (I don't mean to be harping on mistakes, I just suspect that these may be causing your confusion.) There is also another aspect of the generalized linear model that I don't see you mentioning. That is that we specify a response distribution. In the case of OLS regression, the response distribution is Gaussian (normal) and the link function is the identity function. In the case of, say, logistic regression (which may be what people first think of when they think of GLMs), the response distribution is the Bernoulli (/ binomial) and the link function is the logit. When using transformations to ensure the assumptions for OLS are met, we are often trying to make the conditional response distribution acceptably normal. However, no such transformation will make the Bernoulli distribution acceptably normal.
