[site]: crossvalidated
[post_id]: 502818
[parent_id]: 
[tags]: 
Confidence intervals for fitting parameters

I have a problem with how confidence intervals are defined in Bayesian linear regression, as it seems to lead to an nonsensical result. Say that I have data set $(x_i,y_i)$ where $y_i \sim \mathcal{N}(\beta_0 + x_i\beta_1,\,\sigma^{2}) = \mathcal{N}(X\beta,\,\sigma^{2})$ , where the $i$ -th column of $X$ is $[1, x_i]$ and $\beta$ is the vector of fitting parameters. Let's suppose for now that $\sigma$ is known. The likelihood function for $\beta$ is then: $$L(\beta) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}}$$ The maximum-likelihood values of $\beta$ are given as $\beta =(X^{T}X)^{-1}X^{T}y$ , while the variance is given by $Var(\beta) = \sigma^2 (X^{T}X)^{-1}$ (see e.g. here ). In this case the previous general result simplifies to: $$Var(\beta)=\sigma ^{2}\frac{1}{n\sum {(x_{i}-{\bar {x}})^{2}}}\begin{pmatrix}\sum x_{i}^{2}&-\sum x_{i}\\-\sum x_{i}&n\end{pmatrix}$$ where $n$ is the number of data-points in the set. Moreover, if $\sum x_i=0$ the off-diagonal terms drop off and we have $Var(\beta_0) = \dfrac{\sigma^2}{n}$ and $Var(\beta_1) = \dfrac{\sigma^2}{\sum x_i^2}$ . While at first glance this seems ok (it gives the $\sim \frac{1}{\sqrt{N}}$ for the uncertainty) it can lead to some strange conclusions. As en experimentalist I would never trust an intercept with an uncertainty much lower than the data from which it was determined. Also, if I have a data-set with a sufficient number of points (to capture all of it's features) I don't see how adding more equally "bad" points should help. As an illustration for these two points have a look at the following graph, where $y_i \sim \mathcal{N}(3 + 2x_i,\,\sigma=0.4)$ : Lastly we can imagine a paradoxical situation. Imagine that I perform a (noisy) measurement (e.g. measure the mains voltage) $n$ times, and assign an $x$ value to each of the measurements (such that $\sum x =0$ !). These are, by design, uncorrelated, and by doing a "linear fit" I can determine actual voltage with an uncertainty by a factor of $\frac{1}{\sqrt{n}}$ lower than what my instrument is capable of (which is given simply by the standard deviation of the data). What I'm missing here??? The actual problem that I'm trying to solve is a bit more nuanced than this, but the gist is the same: how to properly determine the error bars for some fitting parameters, when different datasets are of different size but of the same "quality". I'm not a mathematician so the language used might seem a bit "off". Thanks in advance!
