[site]: crossvalidated
[post_id]: 535169
[parent_id]: 534861
[tags]: 
TL;DR -- The code in the question maximizes model misfit because it uses "minimize" in conjunction with neg_mean_squared_error . The correct usage is "maximize" in conjunction with "neg_mean_squared_error" . We know this because the sklearn documentation says so, and also because we can demonstrate it with a simple script. In every machine learning problem, the task is to minimize the error. One way to measure error in regression is to take the square of the difference between the observed data and the model's predictions, i.e. minimizing $(y - \hat{y})^2$ . On the other hand, the package sklearn adopted the convention of maximizing a "score". In skelarn 's parlance, a "score" is an objective function that you want to maximize. From the documentation : For the most common use cases, you can designate a scorer object with the scoring parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error , are available as neg_mean_squared_error which return the negated value of the metric. We can verify that sklearn works as described in the documentation with a simple script. from sklearn.datasets import load_boston from sklearn.linear_model import Lasso from sklearn.model_selection import cross_validate X, y = load_boston(return_X_y=True) lasso = Lasso(random_state=0, max_iter=10000) foo = cross_validate(lasso, X=X,y=y,scoring="neg_mean_squared_error") print((foo["test_score"] Which prints True . This shows that the return from cross_validate with "neg_mean_squared_error" must be the negative of the square error (because squaring a real number is non-negative). If you negate the function that you're minimizing, the minimization transforms into a maximization. Therefore, the code in the question is maximizing model misfit.
