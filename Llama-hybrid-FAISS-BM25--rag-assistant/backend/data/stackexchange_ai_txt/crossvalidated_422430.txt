[site]: crossvalidated
[post_id]: 422430
[parent_id]: 
[tags]: 
Inverse word embedding: vector to word

I'm building a generative text model, and the output of one of the final layers is a word embedding (vector) of the generated word. I'm left with the task of converting this vector back to the actual word. Is there a good algorithm for doing this inversion? I'm thinking of using a fully-connected/dense layer, but then it's decoupled from the original (forward) embedding layer. Ideally, I'd think it's better to make use of parameters of the embedding layer somehow for the inversion.
