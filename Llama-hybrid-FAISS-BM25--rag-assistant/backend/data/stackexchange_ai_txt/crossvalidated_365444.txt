[site]: crossvalidated
[post_id]: 365444
[parent_id]: 
[tags]: 
Why is a 0-1 loss function intractable?

In Ian Goodfellow's Deep Learning book, it is written that Sometimes, the loss function we actually care about (say, classification error) is not one that can be optimized efficiently. For example, exactly minimizing expected 0-1 loss is typically intractable (exponential in the input dimension), even for a linear classifier. In such situations, one typically optimizes a surrogate loss function instead, which acts as a proxy but has advantages. Why is 0-1 loss intractable, or how is it exponential in the input dimensions?
