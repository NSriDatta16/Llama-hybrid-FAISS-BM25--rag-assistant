[site]: crossvalidated
[post_id]: 345609
[parent_id]: 340119
[tags]: 
I've found that for simple regression problems with neural networks, tanh can be superior to ReLU. An example would be input = x, output = sin(x), over a limited domain such as [-pi,pi]. Any function approximated with ReLU activation functions is going to be piecewise linear. So, it takes a lot of piecewise linear functions to fit to a smooth function like sin. Meanwhile, tanh is very smooth, and it doesn't take as many tanh primitives to build something that closely resembles a sine wave. Note that for classification problems, especially using multiply convolution layers, ReLU and its minor variants are hard to beat.
