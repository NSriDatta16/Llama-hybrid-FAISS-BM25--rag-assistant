[site]: crossvalidated
[post_id]: 174569
[parent_id]: 169666
[tags]: 
I'm not disagreeing with the answer provided by jlimahaverford, but I feel a bit wider explanation would be useful. The context is variable selection in (hierarchical) regression models estimated by Bayesian inference. In this context I would say that: No, it is actually rather uncommon to do a direct variable selection via CV. I guess you could do it without great harm, but the issue is a) it is not very Bayesian and therefore potentially difficult to justify b) CV on the posterior or the MAP? If MAP, why work Bayesian at all? Plus, may be difficult to get stable MAP estimates for hierarchical models c) most importantly: it is probably rather slow. IF you can really run the analysis on all possible models, the fractional Bayes factor, which is in some sense very similar to CV, seems preferable to me, because it has a straighforward Bayesian interpretation. See O'Hagan, A. (1995) Fractional Bayes Factors for Model Comparison. J. Roy. Stat. Soc. B Met., 57, 99-138. If this is not the case, you can use regularization such as the Bayesian ridge or lasso, which is what jlimahaverford refers to. In this case, you don't select variables, you use the full model, but with a penalty on parameter estimates different from 0 encoded in the prior. The Penalty may or may not be optimized by CV or some other method, see, e.g. Park, T. & Casella, G. (2008) The Bayesian Lasso. J. Am. Stat. Assoc., 103, 681-686. A related regularization approach that is often discussed (also here on CV) are spike-and-slab priors. See Ishwaran, H. & Rao, J. S. (2005) Spike and Slab Variable Selection: Frequentist and Bayesian Strategies. The Annals of Statistics, 33, pp. 730-773. I think Fabian Scheipl http://www.statistik.lmu.de/~scheipl/research.html has also done some interesting work in this area, maybe also regarding the comparison to the Bayesian lasso.
