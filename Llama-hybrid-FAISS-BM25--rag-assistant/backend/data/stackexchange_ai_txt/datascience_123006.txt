[site]: datascience
[post_id]: 123006
[parent_id]: 122996
[tags]: 
I would assume the classical L2 regularization would already do so. In the following, I will give some mathematical reasoning for that. Perfectly correlated columns Let's for the sake of simplicity for the moment stick to your extreme case of $r=1$ , i.e. the two columns (let's call them $x_1$ and $x_2$ ) are directly bound by a linear mapping, e.g. $$x_2 = ax_1 + b$$ . If now the weights of $n$ variables are given by $(w_i)_{i=1,\ldots,n}$ and the intercept is given by $w_0$ , then the L2-Loss with factor $\lambda$ is given by $$L_{2}=\lambda\sum_{i=0}^nw_i^2$$ Given the linear binding between $x_1$ and $x_2$ , we can compensate every change of the weight $w_1$ be an change of the weight $w_2$ and the intercept $w_1$ so that the prediction of the model does not change. Let $w_i^\prime$ ( $i=0,1,2$ ) be the updated weight. If the prediction shall not change, we get: $$\begin{align} w_0^\prime + w_1^\prime x_1 + w_2^\prime x_2 + \sum_{i=3}^nw_ix_i &= w_0 + w_1 x_1 + w_2 x_2 + \sum_{i=3}^nw_ix_i\\ w_0^\prime + w_1^\prime x_1 + w_2^\prime x_2 &= w_0 + w_1 x_1 + w_2 x_2\\ w_0^\prime + w_1^\prime x_1 + w_2^\prime (ax_1+b) &= w_0 + w_1 x_1 + w_2 (ax_1+b)\\ w_0^\prime + w_2^\prime b + (w_1^\prime + aw_2^\prime)x_1 &= w_0 + w_2 b + (w_1 + aw_2)x_1 \end{align}$$ From this we get: $$ w_0^\prime + w_2^\prime b = w_0 + w_2 b \quad\wedge\quad w_1^\prime + aw_2^\prime = w_1 + aw_2$$ or: $$\begin{align} w_2^\prime &= \alpha-\frac{w_1^\prime}{a}\\ w_0^\prime &= \beta + \frac{b}{a}w_1^\prime \end{align}$$ with $\alpha = \frac{w_1 + aw_2}{a}$ and $\beta=w_0 + b(w_2-\alpha)$ This gives us the updated loss as a function of $z=w_1^\prime$ : $$L_2(z) = \left(\beta + \frac{b}{a}z\right)^2 + z^2 + \left(\alpha-\frac{z}{a}\right)^2 + \sum_{i=3}^nw_i^2$$ This function has its minimum at $$ z = \frac{a(\alpha - \beta b)}{b^2 + a^2 + 1}$$ From this, one can compute the $w_i^\prime$ that minimize the loss without changing the predictions for a given $w_i$ ( $i=0,1,2$ ). I will skip the computation of the optimum, here! (since it is just a specific detail and not necessary for the answer). For the special case $a=1$ and $b=0$ , we get the minimal loss for: $$w_0^\prime = w_0, \qquad w_1^\prime=w_2^\prime = \frac{w_1+w_2}{2}$$ Summary Using the L2-Loss for regularization leads already to a well-defined share between the weights. In case if identical features, this is the average of their features. Real World In the real world, we do not have perfect correlation. Still, be minimizing the squared sum of the weights, the L2-regularization pulls the weights of correlated features towards a defined state. Here, the regularization factor $\lambda$ controls how much small predictive gains will pull the weights away from this state (as for every regularization)
