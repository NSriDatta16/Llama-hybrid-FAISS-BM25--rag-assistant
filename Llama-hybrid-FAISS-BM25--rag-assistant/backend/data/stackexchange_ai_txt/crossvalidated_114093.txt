[site]: crossvalidated
[post_id]: 114093
[parent_id]: 114078
[tags]: 
I would aggregate the data to weekly aggregate numbers, assuming that great / bad agents have some what consistent call center performance over the six months. Sometimes aggregating erases the effects of outliers before they can be classified as such. This would account for shifts in performance across the total 6 month period as well. When it comes to sampling using 80% of data points to develop model and 20% to validate would be a good start. Can adjust those numbers depending on how big a data set you are dealing with. I utilize Iowa State papers some times. Here is a good one on the basics ( pdf ). Hope You have fun!! Update: Just so we are clear you are aggregating by week per customer service rep right? Both models don't fit good. You can tell variable fits using the coefficients section of the results. Significant variables have the stars next to there P value (more stars equals more significant typically and lower P value). Based on that none of your variables are actually It's good that you are comparing the model vs actual results. ROC curves capture the model differences pretty well. Try running this and post what you get. library(pROC) g Update : Its weekly aggregates, population wise(i.e the attrite population and the active population),didn do it agentwise because we will have cases when a agent leaves when he was at his peak performance but those are exceptional cases so i thought it would be better to compare the two populations, please advise if that's not the correct way of thinking SO AW1 is first weeks performance metric aggregates for Attrites, similarly NAW1 is first weeks performance metric aggregates for Non-attrites/Active agents. Ran the "step" fuction(Selects a formula-based model by AIC) on the bayesglm model and the results are as below; Aggregating all the agent results together will mean you essentially are over fitting to match the total population metrics and not the agent's performance. Recommend that you tie in the agent level results. You mentioned there being a chance that a great agent leaves unexpectedly but for a well run unit that should be a rarity. Also, recommend you change model family parameter to family = binomial(link = "probit") This should give you probability of default for each agent. This would
