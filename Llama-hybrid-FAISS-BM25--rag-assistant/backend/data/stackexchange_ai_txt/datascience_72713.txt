[site]: datascience
[post_id]: 72713
[parent_id]: 
[tags]: 
L1 vs. L2 Loss in XGBoost

I understand the concepts of L1 and L2 loss (i.e. L1 loss will force some parameter coefficients to zero while L2 will only make them approach zero). What do these do when implemented in XGBoost? Does L1 loss prune the tree more significantly than L2 loss?
