[site]: crossvalidated
[post_id]: 335806
[parent_id]: 8201
[tags]: 
Deep Sets and PointNet provide two (fairly recent) examples of permutation-invariant deep learning architectures. Both these methods provided state-of-the-art results at their time of introduction and are continuous (in senses described in their papers) with respect to perturbations of the underlying sets. The permutation-invariant Deep Sets neural architecture can be written as: $$ F_{DS}(A) = \rho\left( \sum_{a\in A} \phi(a) \right) $$ and a (simplified) PointNet architecture can be written as: $$ F_{PN}(A) = \rho\left( \max_{a\in A} \phi(a) \right) $$ where $\phi:\mathbb{R}^n\to\mathbb{R}^m$ creates features for each point in the set $A$ and $\rho:\mathbb{R}^m\to\mathbb{R}$ combines these features into a single real-valued output (here the $\max$ of a vector is taken to be the component-wise maximum and $\rho$ and $\phi$ are continuous). Since taking sums and max are permutation-invariant, so is the whole network. Ideally we would choose $\rho$ and $\phi$ to be the functions that together minimize the error among all such continuous functions, but that is monstrously intractable. Instead we model $\rho$ and $\phi$ as neural networks whose joint set of parameters $\Theta$ are chosen to minimize the regression error. With respect to your context and notation for $|X_i|=3$ , these models look like: $$ F_{DS}(X_i) = \rho\Big(\phi(x_{i1}) + \phi(x_{i2}) + \phi(x_{i3})\Big) $$ $$ F_{PN}(X_i) = \rho\Big( \max\big\{\phi(x_{i1}),\phi(x_{i2}),\phi(x_{i3})\big\} \Big) $$ Bonus: Deep Sets actually also describes a permutation- equivariant architecture as well.
