[site]: crossvalidated
[post_id]: 613933
[parent_id]: 602062
[tags]: 
The decoder computes the probability of the next word given the words that have already been decoded. Given words $1, \ldots, n$ , the model computes a categorical distribution over the output vocabulary and decides the next word. This word is then prepended to the input, so we have words $1, \ldots, n+1$ , which can be used to predict the $(n + 2)$ -nd word. This is repeated until the [EOS] token is generated. There are various strategies for how the next token is decided: the most common strategies are greedily taking the most probable word or using beam search. At training time, this autoregressive self-feeding is simulated by providing the ground truth sentence as an input with prepended [SOS] symbol and the same sentence as the output with appended [EOS] symbols. Therefore, we know the decoder's entire input and output at training time. This way of training is called teacher forcing. This cause that for $(n-1)$ -th input word, the probability of the $n$ -th word is predicted. Additionally, at training time, there is a mask to prevent self-attention only to consider the left context.
