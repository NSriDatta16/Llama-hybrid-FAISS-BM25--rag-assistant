[site]: crossvalidated
[post_id]: 575856
[parent_id]: 
[tags]: 
What is the difference between inducing location and minibatch data points in Stochastic Variational Gaussian Process?

I'm reading about Gaussian Processes (GP) and specifically about scalable Gaussian processes, namely Stochastic Variational Gaussian Process (SVGP). I've been using the GPFlow-library for implementing GP regression for big data, and the example code I've been using can be found from this source: https://gpflow.github.io/GPflow/develop/notebooks/advanced/gps_for_big_data.html I see the term "inducing locations" being used in this context, which are explained in the tutorial as, quote: The main idea behind SVGP is to approximate the true GP posterior with a GP conditioned on a small set of “inducing” values. This smaller set can be thought of as summarizing the larger dataset. Later on in the same example code, the authors talk about "minibatches", which I assume is the same idea as in deep learning context: you use a small subset of the data to perform gradient descent -like of iterative training for the model, because you can't use all the data at once due to computational issues. My confusion comes from the difference between the "inducing location" and "minibatch" data points. It is not fully clear to me the differing roles of these subsets of the data, because they seem very similar in their purpose. I think this would come clear for me if I read couple of papers and a book on the subject, but maybe I could save weeks worth of time and get the general idea from the community :) So my question is: What is the difference between the minibatch and the inducing location -subsets of the data in SVGP ? Can they be equal?
