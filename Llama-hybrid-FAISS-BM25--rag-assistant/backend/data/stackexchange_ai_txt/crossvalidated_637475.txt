[site]: crossvalidated
[post_id]: 637475
[parent_id]: 
[tags]: 
Imaginary numbers in PCA output

Using PCA manually on correlation matrix, I'm getting imaginary numbers in both eigenvalues and eigenvectors. Is this expected behavior? I understand that when interpreting a matrix as a linear transformation, the eigenvectors can be thought of as the "principal effects" of the transformation. The 'limit', one might say, is when the transformation is recursively applied to some arbitrary input vector for an infinite number of iterations and this vector is pulled into the span of the leading eigenvector (the other effects are simply washed out.) I use this mental framework as it helps me make sense of rotational matrices. There is no principal effect; the arbitrary vector is rotated infinitely but never converges on a real span (hence the imaginary numbers.) So I'm left wondering, if the output of PCA (using correlation matrix) has imaginary numbers in its eigenvectors and eigenvalues then it might be a rotational matrix - aka no span convergence. In this light, I'm doubtful that such eigenvectors can be of any use in some downstream task and simply removing the imaginary numbers sounds like a perilous choice. Response to comments/answers: I checked for symmetry using def check_symmetric(a, rtol=1e-05, atol=1e-08): return np.allclose(a, a.T, rtol=rtol, atol=atol) check_symmetric(correlation_matrix) The response was true. Additionally, I used np.corrcoef for correlation matrix and numpy.linalg.eig for eigen decomposition. Having verified these conditions, I wonder what else might be wrong... One other side note is that the minimum correlation is 0.58, which seems a bit high, perhaps pointing to an issue with the input data? (The max correlation is of course 1.0)
