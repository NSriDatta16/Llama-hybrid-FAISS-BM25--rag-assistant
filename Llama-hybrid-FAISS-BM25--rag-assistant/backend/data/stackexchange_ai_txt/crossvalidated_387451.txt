[site]: crossvalidated
[post_id]: 387451
[parent_id]: 387445
[tags]: 
A classifier's actual output isn't a sample's class but the probability of a sample belonging to each class. However, we usually care about the classes, so we check which class has the highest probability and consider that to be the model's prediction. Then let's say we want to see how many predictions the classifier got right, so we'll measure the accuracy of the predicted classes with the actual ones. The probability that the classifier assigns to each class can be viewed as how confident it is for a prediction. For example a class assigned a $99\%$ probability means that the classifier is very sure the sample belongs to that class. During training, though, we want our classifier make correct predictions with high confidence and wrong predictions with low confidence . We want to penalize our model more if it is very confident about a wrong prediction. For this reason we need a continuous scale for measuring the model's performance during training. Example Suppose you want to train a classifier to predict a person's sex from his height . Intuitively, tall people will be men and shorter people will be women. Because the classifier will many more samples of males with high heights, it will learn to produce a high probability for men in these heights (i.e. high confidence that tall people are males). Likewise, it will see more samples of females with low heights and will produce a high probability for them (i.e. high confidence that short people are females). Finally, it will see a balanced number of males and females in intermediate heights. This will cause it to be unsure (i.e. low confidence - probabilities around $50\%$ ) for people with an average height.
