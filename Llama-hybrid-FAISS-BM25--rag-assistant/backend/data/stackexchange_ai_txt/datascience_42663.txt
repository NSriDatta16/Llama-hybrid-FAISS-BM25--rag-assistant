[site]: datascience
[post_id]: 42663
[parent_id]: 42657
[tags]: 
To deal with overfitting, you need to use regularization during the training: Weight regularization - The first thing you have to do (practically always) is to use regularization on the weights of the model. L1 or L2 regularization update the general loss function by adding another term known as the regularization term. As a result thee values of weights decrease because it assumes that a neural network with smaller weights leads to simpler models. Therefore, it will also reduce overfitting. If you are not sure what you need, just use L2. Keras - Usage of regularizers Dropout - Add dropout layers after dense layers (by the way, there are also advantages to using dropout after the convolution layers, it helps with occlusions). Just make sure not to use it at the final dense layer (the one with the same size as the number of classes). Data Augmentation - The simplest way to reduce overfitting is to increase the size of the training data. Use data augmentation to potentially expend your training set to "infinity". Keras's data augmentation is really simple an easy to use: Keras Image Preprocessing If you implement these 3 steps, you will see drastic improvements (probably even just after the first one). Further corrections and improvements (nothing to do with overfitting): Your batch normalization layer should come after the non-linear activation, or more accurately, it needs to come before the next convolution layer. Add an additional dense layer or 2 (only if the results are not good enough).
