[site]: datascience
[post_id]: 72109
[parent_id]: 40874
[tags]: 
Kari 's answer is concise and very informative. However, the explanation for Equation 4 is slightly incorrect. (final section of the post) $$\phi_j(\tau):=\text{ReLU}(\sum\limits^{n-1}_{i=0}\cos(\pi i\tau)w_{ij}+b_j)$$ $w_{ij}$ is indeed a matrix and $b_j$ is a vector. It's better to interpret the equation by layers: Let's say we have a single $\tau$ , and we want to perform a single forward pass to get its value $Z_\tau(x_t,a_t)$ . Input ( $\tau$ , $|\tau|=1$ ) We have a scalar $\tau$ . $$\tau\sim U([0,1])$$ Intermediate result ( $I_i$ , $|I|=n=64$ ) Expand the scalar $\tau$ to a vector by $\cos(\pi i\tau)$ for $i\in[0,n-1]$ . $$I_i=\cos(\pi i\tau)$$ The number 64 is used in the paper. Embedding of current $\tau$ ( $\phi_j$ , $|\phi|=256$ ) Use fully connected layer to expand $I$ to $\phi$ with $|W|=i\cdot j$ and $|b|=j$ . $$\phi_j(\tau):=\text{ReLU}(\sum\limits^{n-1}_{i=0}I_i w_{ij}+b_j)$$ The number 256 is the number of neurons of the layer after convolutions in DQN architecture. Then we conduct element-wise multiplication $\psi\odot\phi$ and continues pass it into DQN's last layer. For multiple passes, we can simply treat all $\tau$ as a batch and run the forward pass in parallel. The concept above is Universal Value Function Approximator (UVFA), while the cosine term is chosen since it results in good empirical results.
