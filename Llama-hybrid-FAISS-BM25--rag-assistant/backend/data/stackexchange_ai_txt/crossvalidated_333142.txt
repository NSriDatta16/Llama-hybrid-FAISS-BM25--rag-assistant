[site]: crossvalidated
[post_id]: 333142
[parent_id]: 333078
[tags]: 
If we use "the one true model" and "true priors" reflecting some appropriately captured prior information, then as far as I am aware a Bayesian truly does not have an overfitting problem and that posterior predictive distribution given very little data will be suitably uncertain. However, if we use some kind of pragmatically chosen model (i.e. we have decided that e.g. the hazard rate is constant over time and an exponential model is appropriate or e.g. that some covariate is not in the model = point prior of coefficient zero) with some default uninformative or regularizing priors, then we really do not know whether this still applies. In that case the choice of (hyper-)priors has some arbitrariness to it that may or may not result in good out of sample predictions. Thus, it is then very reasonable to ask the question whether the hyperparameter choice(=parameters of the hyperpriors) in combination with the chosen likelihood will perform well. In fact, you could easily decide that it is a good idea to tune your hyperparameters to obtain some desired prediction performance. From that perspective a validation set (or cross-validation) to tune hyperparameters and test set to confirm performance make perfect sense. I think this is closely related to a number of discussions of Andrew Gelman on his blog (see e.g. blog entry 1 , blog entry 2 , blog entry 3 on LOO for Stan and discusions on posterior predictive checks), where he discusses his concerns around the (in some sense correct) claims that a Bayesian should not check whether their model makes sense and about practical Bayesian model evaluation. Of course, we very often are the most interested in using Bayesian methods in settings, where there is little prior information and we want to use somewhat informative priors. At that point it may become somewhat tricky to have enough data to get anywhere with validation and evaluation on a test set.
