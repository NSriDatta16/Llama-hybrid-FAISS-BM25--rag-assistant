[site]: crossvalidated
[post_id]: 218659
[parent_id]: 217703
[tags]: 
A big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have a zero gradient (because of neural network implementation details and limitations). We can use two approaches: Divide by a constant. We are just dividing everything before the learning and multiply after. Use log-normalization. It makes multiplication into addition: \begin{align} m &= x \cdot y\\ &\Rightarrow \\ \ln(m) &= \ln(x) + \ln(y) \end{align}
