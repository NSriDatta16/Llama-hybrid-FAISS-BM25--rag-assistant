[site]: datascience
[post_id]: 31506
[parent_id]: 31494
[tags]: 
What you are looking for is a probabilistic classification for 2+ classes (multi class) i.e. evaluating a probability of being associated to a list of k classes given the observation. k = 4 blocks/classes for your example. Its a one vs many situation. Binary classifiers (like logistic regression) have been extended to multi class problems. One of the strategy used is to train/test your model k times, each time keeping one class in predicted block intact and turning other three predicted class to a single dummy class. Then use a binary probabilistic classifier(eg. logistic regression) to predict class probabilities. The order of class preference will be in decreasing order of class probabilities. For logistic regression and for other classifiers, this is inherently built in by scikit and applied by default: lr = linear_model.LogisticRegression(multi_class = 'ovr') # ovr is "one vs rest" lr.predict_proba(test_features) refer to Multi-class capability built in scikit Yes, you can even use a pruned decision tree to get the class probabilities. But most probably you will not be able to get 2nd, 3rd... best predictions for most of your observations from a single tree due to the underlying splitting mechanism of algorithm. dt = tree.DecisionTreeClassifier(min_samples_split=25) dt.predict_proba(test_features) Therefore, rather then relying on a single decision three, better option would be to use a randomforest classifier to obtain proportion of votes going to each class while making a prediction for an observation. In short, any multi-class classifier (or ensemble) which spits out a likelihood over multiple classes will do. Another eg. XGBoost as pointed out in below comment by bradS.
