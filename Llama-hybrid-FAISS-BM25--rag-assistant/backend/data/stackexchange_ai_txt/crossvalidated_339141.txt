[site]: crossvalidated
[post_id]: 339141
[parent_id]: 
[tags]: 
Why don't we average Confidence Intervals?

For example, if we have a 95% confidence interval for a parameter of interest then we will have an interval such that if we were to take a large number of such intervals, about 95% of them will contain the parameter of interest and about 5% won't. So why is it that we don't just take some respectable number of such intervals, say 20, and in order to mitigate the effect of the problematic $\approx$5%, average them? Intuition tells me that if it was possible to make so many intervals, then we'd be better off just taking a sample that's 20 times as big as our original one and therefore getting a better interval due to our significantly bigger sample size, but that wouldn't help us avoid the problem that we might have an interval from the 5% that doesn't contain the parameter of interest, which I suspect that averaging would mitigate the effect of (therefore giving us an interval that should contain the parameter of interest with high probability).
