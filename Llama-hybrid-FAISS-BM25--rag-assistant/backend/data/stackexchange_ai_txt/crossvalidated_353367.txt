[site]: crossvalidated
[post_id]: 353367
[parent_id]: 353298
[tags]: 
vectors that are similar under the original measure have small Euclidean distance under the embedding This is the goal of dimensionality reduction, especially the nonlinear dimensionality reduction, where it is the only goal (because they cannot in general enforce that distances between points that are far apart will be undistorted, if you're interested in proof you can find it here ). Some approaches: Multidimensional scaling Isomap (this is nonlinear method that uses MDS for distances retrieved from kNN graph) Kernel PCA (uses kernel trick to do PCA in embedding space) graph-based dimensionality reduction (your distance matrix defines a graph, and graphs give rise to useful matrices, check out A tutorial on Spectral Clustering . In Python megaman implements Spectral Embedding ) tSNE (reduces dimensionality trying to preserve distribution of distances) If you're interested in Python in particular, then almost all of these methods are implemented in scikit-learn , especially in manifold module.
