[site]: datascience
[post_id]: 37223
[parent_id]: 37221
[tags]: 
Normalisation is performed to balance weights and make parameters universal. In the cases you mention this is interval normalisation (thus numbers, thus a set that has standard order and distance metric). Just for the contrast, string normalisation might involve stemming, converting to uppercase or a number of other techniques, depending on the context. Since scaling (multiplying by the same factor) is a linear operation, it preserves order and magnitude relations, among other properties. Thus, in the general case, normalisation does not loose information about relationships within the parameter/dimension. If we use a simple clustering algorithm for the sake of the example, consider that age might have a range (roughly) of (0-100) and salaries could be something like the interval (1-10^6). A naive approach would be not to perform normalisation and to use the default Euclidean distance metric. However, variances of salaries of up to $100 are negligible. Thus your salary information would dominate your age information, resulting distances between data points (and thus clusters) being mostly determined by the salary only. The secondary role of age would become negligible for the clustering. However, if we normalise the two to the range of (0-1), we might get somewhat comparable scales and the factor influence would be incomparably smaller (while still to be considered). Due to the last remark in the brackets, most advanced machine learning algorithms (including neural networks, support vector machines) actually handle different scales. When you use these, you don't need to normalise data, because the algorithm learns normalisation along with other properties of the data. The decision of when to normalise your data depends on your problem context and on the approach you are planning to take. Let's say that normalising data makes it easier from the first view to be able to tell whether a value is low or high.
