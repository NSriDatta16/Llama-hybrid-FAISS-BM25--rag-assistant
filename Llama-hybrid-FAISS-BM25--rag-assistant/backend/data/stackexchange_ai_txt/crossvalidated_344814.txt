[site]: crossvalidated
[post_id]: 344814
[parent_id]: 344805
[tags]: 
Logistic regression models probability of success $\mathbb{P}(Y=1 \mid X)$, and uses a Bernoulli likelihood function $$ Y \sim \mathcal{B}\big(g^{-1}(X\beta)\big) $$ and, most often , a logistic link function $g^{-1}$. For more then two categories, we use categorical distribution and multinomial regression (recall that categorical distribution is a special case of multinomial distribution with $n=1$), to model probability of observing $j$-th category $\mathbb{P}(Y=j\mid X)$. The likelihood is $$ Y \sim \mathcal{C}\big(\pi_1,\pi_2,\dots,\pi_K\big) $$ where the probabilities are calculated using softmax as a link function $$ \pi_j = \frac{ e^{x^T w_j} }{ \sum_{k=1}^K e^{x^T w_k} } $$ Since the vector of parameters $w$ does not really differ from the parameters that we'd see for linear regression , the priors won't differ either. So you can use any priors that you'd find reasonable (e.g. following normal distributions). You can find many examples of such models implemented in Stan or PyMC3 , this could possibly help you with concerns about implementing it. As about the possible distribution for the $\pi_1,\pi_2,\dots,\pi_K$ probabilities, the two common distributions for them could be Dirichlet , or in this case Multivarite log-normal distribution , yet this is not something that you'd care, since they do not come into model definition. Saying this differently, you don't put any prior for those probabilities, since they are estimated from the data, you put priors on the regression parameters. For the intercept-only model, you could use the conjugate Dirichlet-multinomial model .
