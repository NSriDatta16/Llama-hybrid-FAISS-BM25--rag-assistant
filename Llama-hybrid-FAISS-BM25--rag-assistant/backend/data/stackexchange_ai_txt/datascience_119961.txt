[site]: datascience
[post_id]: 119961
[parent_id]: 119952
[tags]: 
NLP neural networks don't use word tokens any more. It's been a while since the norm is using subwords. Usual approaches to define the subword vocabulary are byte-pair encoding (BPE) , word pieces or unigram tokenization . GPT-3 uses BPE tokenization . According to the OpenAI's tokenizer tool website: Codex models use a different set of encodings that handle whitespace more efficiently From this, I understand that they use BPE but with a different vocabulary. This is supported by this javascript tokenizer that was created by extracting the BPE vocabulary from OpenAI's own online tokenizer tool.
