[site]: crossvalidated
[post_id]: 194390
[parent_id]: 193674
[tags]: 
Summary: Q: Grow a decision tree such that $se \ge 0.95$ A: Substitute the tree decision evaluation criterion by one that has a tuning parameter that controls how much to reward sensitivity. Test this decision tree (using k-fold cross-validation) and measure the sensitivity. If $se \ge 0.95$ is not true, repeat the process but with more aggressive parameters for the criterion until you get a tree that satisfies $se \ge 0.95$. Overview on decision trees: Suppose that you choose C4.5 as your decision tree generation algorithm (which is also known to generate small trees in case your end-users like small trees). Growing C4.5 trees is generally simple: you perform a series of splits until you reach a stopping condition. A stopping condition could be things like: Purity of the leaf node (e.g. entropy of samples in the leaf node with respect to their target variable is small enough). Maximum tree branch depth is reached (this is a parameter to ensure that your model does not overfit the training samples). Leaf boundary tightness around samples that are inside it (Google for "tree grafting"). Etc. Other stopping criteria exist and you may wish to look at them, but this is not our main focus. The main focus here is identifying which binary split is the optimal split that satisfies your $se \ge 0.95$ constraint. To address this we need to look at how it's done with our example decision tree, the C4.5 algorithm. Generally, it: All possible split using all available features are evaluated. Then split that causes maximum Information Gain will be chosen as the 1st split. Now we have split our dataset into two subsets: subset right, and subset left. Then, for each of those subsets, we evaluate all the possible splits, and then choose one that gives us maximum Information Gain. This process is repeated recursively for the subsequent subsets as we keep splitting the leaf nodes further down the tree (until we reach the stopping conditions above). A classical split evaluation criterion: Information Gain Let $\mathcal{X}$ be your dataset, $\mathcal{Y}$ be their corresponding labels, and $f:\mathcal{X} \rightarrow \mathcal{Y}$ be sample-to-label mapping function. Suppose that for any sample $x \in \mathcal{X}$, $x$ is represented as a $k$ dimensional vector, where, for any $i \in \{1, 2, \ldots, k\}$, $x[i]$ represents the value of the $i^{th}$ component or feature. Also, we define $s_{i,v}$ as a decision split on feature $i$ by using value $v \in \mathbb{R}$. An example of such decision split is: \begin{equation} s_{i,v}(x) = \begin{cases} \textbf{True} & \text{if } x[i] \ge v\\ \textbf{False} & \text{else} \end{cases} \end{equation} Then, if we apply split $s_{i,v}$ on samples in set $\mathcal{X}$, we obtain subsets $\mathcal{X}_{left} = \{x:x\in\mathcal{X}, s_{i,v}(x) = \textbf{True}\}$ and $\mathcal{X}_{right} = \{x:x\in\mathcal{X}, s_{i,v}(x) = \textbf{False}\}$. Finally, we calculate the Information Gain of $s_{i,v}$ on $\mathcal{X}$ as follows: $IG(\mathcal{X}, s_{i,v}) = H(\mathcal{X}) - \big(\frac{1}{|\mathcal{X}_{left}|}H(\mathcal{X}_{left}) + \frac{1}{|\mathcal{X}_{right}|}H(\mathcal{X}_{right})\big)$, where $H$ is Shannon's entropy (not gonna explain it here). The process above is repeated for all possible splits $s_{i,v}$ on $\mathcal{X}$. I.e., all evaluatable values of $i$ and $v$ are evaluated exhaustively. Then, the split that has maximum IG is chosen. Once the above happens, we repeat all that again, but this time with the subsets that are created by apply the optimal split above. This process is repeated recursively. A new split evaluation criterion to satisfy $se \ge 0.95$ Let $se:\mathcal{X} \rightarrow [0,1]$ and $sp:\mathcal{X} \rightarrow [0,1]$ be functions that output sensitivity and specificity of their input sets. Then we define our split evaluation criterion as follows: \begin{equation}\begin{split} New(\mathcal{X}, s_{i,v}, t) &= \begin{split}&\left(\begin{split}&\frac{1}{\mathcal{X}_{left}}\Big(t \times se(\mathcal{X}_{left}) + (1-t) \times sp(\mathcal{X}_{left})\Big)\\ &+\\ &\frac{1}{\mathcal{X}_{right}}\Big(t \times se(\mathcal{X}_{right}) + (1-t) \times sp(\mathcal{X}_{right})\Big) \end{split}\right)\end{split}\\ \end{split} \end{equation} where $t \in [0,1]$ is a threshold that favors splits that return higher specificity the higher it is, and favors splits that return higher sensitivity the lower it is. If you replace IG (or Gini index, or whatever criterion) by $New$, and set $t$ large enough, then as the individual splits in your tree are biased towards better sensitivity, the sensitivity of your whole tree should too be biased towards better sensitivity. If $t$ is large enough, then your tree as a whole should satisfy your $se \ge 0.95$ constraint. Extra notes You may wish to explore ensembles of decision trees, such as Random Forests, Extra Trees, etc as they are more accurate than C4.5 in general. However, a challenge would be representing such ensemble of trees as a single tree to your end users.
