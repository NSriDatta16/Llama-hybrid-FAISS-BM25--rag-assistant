[site]: crossvalidated
[post_id]: 453956
[parent_id]: 
[tags]: 
Bayesian inference and testable implications

I have one question regarding testable implications of a model and Bayesian inference. My main doubt is how to exploit testable implications to reject a model. Here is a simple example. Suppose my model is that I have an iid sample from two gaussians with means $\mu_1$ and $\mu_2$ (and known variance) and I impose in the model the restriction that $\mu_1 = c \mu_2$ where $c$ is a known constant. Note this model imposes restrictions on the observed data---the mean of the two samples can't be more than $c$ times apart, barring, of course, sampling error. Now suppose the data is discrepant with the posited model. Is there a principled way in Bayesian inference to reject this model given discrepant data? PS: The generic comments saying that we could use posterior predictive checks, bayes factors and what not are ok, but not very useful in practice. Please also show how you would actually solve this toy problem. Edit for the bounty: I will give the bounty to an answer that compares the three current suggestions (or more) both theoretically and more importantly with numerical examples of the toy problem . The three suggestions are: (i) posterior predictive checks; (ii) bayes factors; (iii) credible intervals (with or without ROPE). For people potentially interested in answering: it would be helpful to actually perform a posterior predictive check and the hypothesis test or whatever you choose in your answer. The answer that does this and compares approaches will get the bounty.
