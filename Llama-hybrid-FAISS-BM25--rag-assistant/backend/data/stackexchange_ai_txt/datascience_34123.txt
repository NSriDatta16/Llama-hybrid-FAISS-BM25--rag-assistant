[site]: datascience
[post_id]: 34123
[parent_id]: 34120
[tags]: 
Keep in mind, downsampling is always lossy . I will give you two hints. Nyquist-Shannon Theorem. What it says in short is that the max frequency that you may observe in a sampled signal is half of the sampling frequency . This means that if you downsample the data every 1 hour, you can observe dynamic phenomena with period of 2 hours (or more). Another way of looking at the theorem is "you have to sample the signal at least twice as fast as the maximum frequency that you expect to see in it" . Engineering rule of thumb: Sample the signal at least 10 times faster than the expected highest frequency, not just 2 times. Histogram Representation . Since you have a huge database, it might make sense to convert your variables into histograms! In this way, you will greatly reduce their size. You discretize every real-valued variable into bins and then instead of storing the exact value of the variable you just increase the count of the appropriate bin by 1. There are a lot of Machine Learning algorithms that deal with histogram representations, such as this: https://pdfs.semanticscholar.org/b8c8/347f9c33935b97703ecd35a67af5c5508487.pdf Hope it helps :) !
