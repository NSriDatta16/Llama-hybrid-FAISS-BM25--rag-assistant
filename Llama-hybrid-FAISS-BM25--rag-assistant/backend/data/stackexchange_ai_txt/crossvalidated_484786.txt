[site]: crossvalidated
[post_id]: 484786
[parent_id]: 
[tags]: 
Best practice/Ideas for clustering Event Sequence Embeddings?

My dataset consist of around 40 000 samples of event sequences. Sample of data [[Event 1, Event 2, Event 4, Event 5], [Event 1, Event 3, Event 4], [...]] I apply Sequence Graph Transform as described in https://arxiv.org/abs/1608.03533 in order to embed the event sequences into a vector of length N N where N is the set of events in the dataset,(in this case N = 17 and N N = 289). What SGT does is embedding the sequences based on the occurence and position of the subsequences in the sequence. What I end up with is a DataFrame of size (40 000,289) with quite sparse array where most of the elements are often 0. I would now like to cluster these sequence embeddings in order to see if I can detect any clusters and/or outliers. Based on my previous experience I am more comfortable with PCA and K-Means. I have a few ideas without knowing which fits best for the problem. Apply K-Means directly on the 289 dimensional arrays. But the problem with high dimensional data is that distances become less precise due to the curse of dimensionality. Apply PCA on the data, find the number of components that describe around 80-90% of the variance and then perform K-Means on the components. This would reduce the number of dimensions greatly (around 10 components are needed for 85% of the variance explained). But as I understod it, PCA is not good on embeddings because it subtracts the mean. Same as 2) except use TruncatedSVD instead of PCA. Same as 2) or 3) but use another distance metric than euclidean distance. Some other approach? I would greatly appreciate some guidance here if anyone feels that they are familiar with these situations.
