[site]: crossvalidated
[post_id]: 139019
[parent_id]: 139005
[tags]: 
I'm afraid that you will likely need to dig into the source code, since this does not seem to be documented in the help pages. For the methods you list - all straightforward forecasting methods, nothing involving integer values or intermittency - I'm 99% sure that they use a normal or a t distribution. With enough (30 or more) observations, you can safely treat the t distribution as "close enough" to a normal. So your question really boils down to extracting the variance the methods use for each future predictive distribution. And there again I'm afraid you will need to dig into the source code, since to my experience they only report prediction intervals, not forecast variances. EDIT in response to @RichardHardy's comment ), specifically: If innovations/errors are assumed to follow a t-distribution, the degrees of freedom parameter of that t-distribution is not related to the number of observations in a time series. Thus I am struggling to understand the logic of your statement in the second paragraph. I went and dug through a few textbooks I had lying around here ( Time Series Analysis - Forecasting and Control , 3rd ed., by Box, Jenkins & Reinsel; Principles of Business Forecasting by Ord & Fildes), but unfortunately, both are rather vague in terms of specifying predictive distributions for ARIMA models. Next, I looked at the source code of forecast.Arima() in the forecast R package. It seems like it uses a normal distribution for prediction intervals, which certainly does not make sense. If you assume normally distributed errors in the original series, you need to take estimation uncertainty into account in prediction intervals, just like in predicting from Ordinary Least Squares. For OLS, this implies that the correct predictive distribution is a t distribution, assuming a correctly specified model. (Note that I am not assuming that the original errors follow a t distribution, as @RichardHardy seems to read into my answer.) Here is a little simulation. We simulate 100,000 ARMA(1,1) time series with 11 observation each, all with AR and MA coefficients of 0.5. In each case, we fit a model on the first 10 observations. We use arima() , not auto.arima() , since for now we do not want to deal with misspecified models; instead, we force the fitted model to have the correct ARMA(1,1) structure. We forecast a single value out and store the error. Finally, we create a normal q-q-plot of the errors. require(forecast) nn The plot clearly shows that the errors are heavier-tailed than the normal distribution. The reason is that we do not take parameter estimation uncertainty into account. However, if we change the script above slightly, to n.hist instead of n.hist , that is, using 50 instead of 10 historical observations, we get the following q-q-plot: This looks much more normal. Indeed, both plots together look very much like what we would expect from a standard OLS model: a heavy-tailed forecast error distribution for smaller numbers of observations (that is, a t distribution), which approximates a normal distribution as the number of observations increases. So, even if my textbooks are vague on this point, I strongly suspect that the correct predictive distribution does involve a t distribution, with degrees of freedom related to the number of observations. And if the number of observations increases, we can safely treat the t distribution with large df as a normal distribution.
