[site]: crossvalidated
[post_id]: 96881
[parent_id]: 92101
[tags]: 
It seems that gram matrix that you use for predictions is wrong. Once you fit the SVM its prediction for $x$ is: $ y = \text{sign} \langle w,\phi(x)\rangle $ It sure is possible that one cannot compute $\phi(x)$ but needs to use a pre-computed gram matrix instead. Substitute $ w = \sum_i^m\alpha_i\phi(x_i)$ into the above prediction expression: $$ y = \text{sign } \langle \sum_i^m\alpha\phi(x_i),\phi(x) \rangle = \text{sign } \sum_i^m\alpha_i\langle\phi(x_i),\phi(x) \rangle = \text{sign } \sum_i^m\alpha_i K(x_i,x) \rangle = \text{sign } (Xx)^T\alpha = \text{sign } {G}\alpha $$ Where $X$ is the design matrix for the training data (row $i$ of $X$ is $\phi(x_i)$) and gram matrix $G=(Xx)^T$ (note that G is not symmetric) You can use this as a reference: from sklearn.datasets import load_digits from sklearn.svm import SVC from sklearn.utils import shuffle from sklearn.metrics import accuracy_score import numpy as np digits = load_digits() X, y = shuffle(digits.data, digits.target) X_train, X_test = X[:1000, :], X[100:, :] y_train, y_test = y[:1000], y[100:] svc = SVC(kernel='precomputed') kernel_train = np.dot(X_train, X_train.T) # linear kernel svc.fit(kernel_train, y_train) #kernel_test = np.dot(X_test, X_train[svc.support_, :].T) kernel_test = np.dot(X_test, X_train.T) y_pred = svc.predict(kernel_test) print 'accuracy score: %0.3f' % accuracy_score(y_test, y_pred)
