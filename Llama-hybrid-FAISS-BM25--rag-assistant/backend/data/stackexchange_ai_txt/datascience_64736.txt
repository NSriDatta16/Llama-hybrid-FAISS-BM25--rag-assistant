[site]: datascience
[post_id]: 64736
[parent_id]: 64717
[tags]: 
You can use the nearest neighbor loss directly. You can backpropagate through that, though its gradient isn't very stable. Perhaps better is to use a soft nearest neighbor loss, e.g., see Salakhutdinov & Hinton, "Learning a nonlinear embedding by preserving class neighbourhood structure", 2007. This gives a more stable gradient. Both of these will be very slow, because they require comparing each point to every other point in the training set. A hack to speed it up is to only compare to other samples in the same mini-batch. There may be many other approaches as well.
