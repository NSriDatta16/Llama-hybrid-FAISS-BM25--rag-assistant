[site]: crossvalidated
[post_id]: 322232
[parent_id]: 117953
[tags]: 
I think you should have a relook at the requirement for analysis at the "seconds" level. What purpose does it solve? For example if the analysis is to mark out anomalies in the time series, does it give enough reaction time for operations team to drill down, analyze and take corrective actions? If this is for prediction purposes, does it help a user to predict a variable for next n seconds? Having worked in the operations analytics space, I can say that anything more than a 15-20 minutes granularity for predictions/classifications in time series data does more harm than good, like: 1. Higher granularity decreases the signal to noise ratio and your techniques will need to be highly robust to noise. 2. Doesnt solve much purpose (unless it is AML kind of problems) 3. Puts enormous load on your hardware especially if dealing with multi variate cases (example tracking 1000 metrics of an application and flagging abnormal conditions for the entire application)
