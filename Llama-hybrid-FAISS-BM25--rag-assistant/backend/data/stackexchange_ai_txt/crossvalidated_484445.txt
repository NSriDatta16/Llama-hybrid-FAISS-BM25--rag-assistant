[site]: crossvalidated
[post_id]: 484445
[parent_id]: 
[tags]: 
Why does LightGBM Classifier gives some folks a probability of 1 of belonging in a class with log-loss?

I'm trying to use the LightGBM package in python for a multi-class classification problem and I'm baffled by its results. For a minority of the population, LightGBM predicts a probability of 1 (absolute certainty) that the individual belongs to a specific class. I am explicitly using a log-loss function, so if the algorithm is wrong with even one of these folks, my loss will be infinite. I tried tweaking the parameters, changing the features, switching the boosting to random forest, etc. but it seems impossible to avoid this result. Strangely enough this issue appears specific to LightGBM: I tried other packages like XGBoost, CatBoost, H20, etc. and they all provide probabilities that exclude 0 and 1. Is there something I'm missing? Maybe a parameter I'm not setting right? Or, maybe it is a bug with LightGBM? Example: param = {'objective': 'multiclass', 'metric': 'multi_logloss', 'num_class':21} num_round = 20 model = lightgbm.train(param, train_data, num_boost_round=num_round) preds = model.predict(X_test[features]) sum(sum(preds == 1)) Results: 70 individuals have one of their probabilities set to 1.
