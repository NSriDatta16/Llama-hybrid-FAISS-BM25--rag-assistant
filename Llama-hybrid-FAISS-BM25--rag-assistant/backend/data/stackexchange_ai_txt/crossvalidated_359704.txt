[site]: crossvalidated
[post_id]: 359704
[parent_id]: 302987
[tags]: 
You're trying to have your cake and eat it, too. Logically, if I compute it with maximum depth I always get a probability of 1 or 0 (discrete classifier) for a classification. I red https://datascience.stackexchange.com/questions/323/how-can-we-calculate-auc-for-a-simple-decision-tree on how to get probabilities from the ration in the end node, but this would mean, that I would have to crop the tree, by limiting the depth. But its often those last decisions that can be crucial for a correct classification. A tree that has probabilities in the leafs compute the probability by taking the proportion of each class in that leaf. If you want pure leafs, then your probabilities are going to be $\mathbb{P}(class = 1)=1$ or $\mathbb{P}(class = 1)=0$ by construction. If you want predicted probabilities that are greater than 0 and less than 1, then you need impure leafs. You can't have both. If you're willing to use an ensemble of trees, then you have more flexibility. Random forests, for example, take the average predictions over randomized trees trained on randomized data, so predicted probabilities can be found by averaging over binary votes of fully-grown trees.
