[site]: datascience
[post_id]: 111002
[parent_id]: 110995
[tags]: 
The logic seems completely fine. In your place, I would save all the model's performances inside a dictionary to keep track and use any of them in case I need them. seed = 7 # prepare models models = [('LR', LogisticRegression()), ('LDA', LinearDiscriminantAnalysis()), ('KNN', KNeighborsClassifier()), ('CART', DecisionTreeClassifier()), ('NB', GaussianNB()), ('SVM', SVC())] # evaluate each model in turn results = [] names = [] scoring = 'accuracy' models_dict = {} for name, model in models: kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed) cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring) results.append(cv_results) names.append(name) msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std()) print(msg) models_dict[name] = {"binary":model,"avg_performance":cv_results.mean(),"std_perfirmance":cv_results.std()} best_model_dict = list(models_dict.values())[np.argmax([x["avg_performance"] for x in list(models_dict.values())])] best_model = best_model_dict["binary"].fit(X,y) # boxplot algorithm comparison fig = plt.figure() fig.suptitle('Algorithm Comparison') ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show() # Saving the model for usage in the Heroku app joblib.dump(best_model, 'model.pkl') print("Model Saved.")
