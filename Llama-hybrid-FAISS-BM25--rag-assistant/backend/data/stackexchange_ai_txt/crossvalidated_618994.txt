[site]: crossvalidated
[post_id]: 618994
[parent_id]: 
[tags]: 
Temporal leakage or different phenomena?

I've following problem/toy-example: every week I sample data describing users (one row is one user) I want to predict that in next three weeks user will be a fraud 1 or not 0, so basically binary classification, but regression setting is also possible I can't make connections between users across weeks (anonymity), but we can assume that new users appear/old users disappear in slow pace, so there is 90% overlap between weeks, Features are: demograhics like age, but majority of them describe user 'transactions', e.g. amounts of transaction in last week, last three weeks, last six weeks, analogously average value of transaction per last week, three weeks, six weeks etc. I have about 100 weeks of training data I've checked two approaches: A Training data first 50 weeks, validation data next 50 weeks. I've also checked approach that validation starts at 53 weeks, to check if there significant leakage on overlapping between training and validation targets, but it is neglible B I've trained three models on following data: model 1: weeks 1, 4, 7, 10 , ..., till 50 model 2: weeks 2, 5, 8, 11, ..., till 50 model 3: weeks 3, 6, 9, 12, ... ,till 50 there is no reason to think that users behavior differs on data partitioned in such way, so my final model is just an average of three model predictions (model1 + model2 + model3)/3. With the same validation set, B approach gives better results. I wonder if this is just coincidence or in A approach I overfit due to target leakage? I've searched literature but I didn't find similar case.
