[site]: crossvalidated
[post_id]: 492948
[parent_id]: 
[tags]: 
Proving the predictive distribution in "Pattern Recognition and Machine Learning" by Bishop

I'm studying this book as it is the teaching material for one of the courses I'm taking now. In page 31, it says the predictive distribution is given by the integral of the likelihood function times the posterior probability over weights w: $p(t \mid x, \mathbf{x}, \mathbf{t})=\int p(t \mid x, \mathbf{w}) p(\mathbf{w} \mid \mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{w}$ Moreover, the equation above transforms all the way into: $p(t \mid x, \mathbf{x}, \mathbf{t})=\mathcal{N}\left(t \mid m(x), s^{2}(x)\right)$ with mean and variance given by $m(x)=\beta \boldsymbol{\phi}(x)^{\mathrm{T}} \mathbf{S} \sum^{N} \boldsymbol{\phi}\left(x_{n}\right) t_{n}$ , $s^{2}(x)=\beta^{-1}+\phi(x)^{\mathrm{T}} \mathbf{S} \phi(x)$ , and the matrix S is $\mathbf{S}^{-1}=\alpha \mathbf{I}+\beta \sum_{n=1}^{N} \phi\left(x_{n}\right) \phi(x)^{\mathrm{T}}$ Alpha and beta is hyper parameters controlling the precision of the distributions, and phi is the basis function. My question is how can I work out the integral, and why can it be expressed in the form of another Gaussian equation as shown above?
