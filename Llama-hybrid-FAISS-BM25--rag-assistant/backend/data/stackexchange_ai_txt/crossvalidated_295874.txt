[site]: crossvalidated
[post_id]: 295874
[parent_id]: 
[tags]: 
Feature Selection for Machine Learning problem

Have been working on a dataset called Dorothea hosted at UCI. It contains 100,000 features. If I use PCA the top 10 components explain only 4.10% of the variance. Having concluded that PCA will not reduce dimensionality in any significant way, I looked at the dataset in a number of different ways. I found that close to 17000 features are duplicates of one other feature (I took into account the entire data ...i.e. training+validation+test). Is it safe to delete these features? Has anyone else worked on this dataset. I would like to know what approach did they take.
