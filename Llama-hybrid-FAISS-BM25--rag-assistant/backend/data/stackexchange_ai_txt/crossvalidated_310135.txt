[site]: crossvalidated
[post_id]: 310135
[parent_id]: 309966
[tags]: 
Standardizing is a technical trick. It speeds up convergence. It doesn't change anything fundamentally in a reasonable model. Think of using 100mm, 10cm or 1m as a length of the box. Would a model depend on the unit of measure? A sensible model will not. PCA is used to get orthogonal factors, but not in the context of machine learning, usually. In ML it's most common application is dimensionality reduction. It's a reasonable approach to highly correlated data, but in many cases autoencoder could be preferable. So, it's not a must do step. I never used VIF even in regression. Multi collinearity is not a big issue in machine learning. In severe cases where you have loads of almost identical variables, it can help with performance to remove collinear variables. If you're learning ML I wouldn't bother too much about feature engineering in the beginning. Just feed all variables into the model and see what happens.
