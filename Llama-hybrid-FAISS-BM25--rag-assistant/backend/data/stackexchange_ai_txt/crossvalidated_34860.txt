[site]: crossvalidated
[post_id]: 34860
[parent_id]: 34844
[tags]: 
I believe it is not possible in general. I don't have a proof right now, but even the famous expectation maximization algorithm (which is the general approach for your problem) makes use of a variational lower bound to the likelihood instead of the true likelihood. This guarantees to increase the likelihood as well for most cases. I suggest you approach the problem in the following way: Use an EM style variational lower bound to obtain a substitute for the likelihood, Differentiate that, Ascent the gradient. My knowledge is based on chapter 11.2 of David Barber's "Bayesian Reasoning and Machine Learning" of which you can get a free ebook here. It should contain everything you need to know to make the above work.
