[site]: datascience
[post_id]: 17712
[parent_id]: 17710
[tags]: 
What is the value of doing feature engineering using XGBoost? Performance maybe? (Note we don't use XGBoost, but another gradient boosting library - though XGBoost's performance probably also depends on the dimensionality of the data in some way.) We have a dataset where each item consists of 3 signals, each 6000 samples long - that's 18k features. Using these features directly takes ages (days), so we did some manual feature engineering to reduce the number of features to about 200. Now training (including parameter tuning) is a matter of a few hours. For comparison: a short time ago we also started training ConvNets with the same data and the whole 18k features (no feature engineering). They reach the same accuracy as the gradient boosting models after only about 2 hours of training.
