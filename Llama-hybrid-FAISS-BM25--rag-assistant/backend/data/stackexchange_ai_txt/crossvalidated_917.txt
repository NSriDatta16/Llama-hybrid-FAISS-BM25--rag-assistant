[site]: crossvalidated
[post_id]: 917
[parent_id]: 868
[tags]: 
Like drknexus said, for a logistic regression, your outcome measure needs to be 0 and 1. I'd go back and recode your outcome as 0 (didn't like it), or 1 (did like it). Then, abandon excel and load the data into R (it's really not as intimidating as it looks). Your regression will look something like this: glm(Liked ~ Visually.Stunning + Exhilarating + Artistic + Sporty, family = binomial, data = data) The regression will return betas for each feature in terms of log-odds. So, for every 1 point increase in Artistic , for instance, you'll have a value for how much that increases or decreases the log-odds of your enjoyment. Most of the betas will be positive, unless you dislike sporty games or something. Now, you'll have to ask yourself some interesting questions. The assumption of the model is that the values on each of these scores affect your enjoyment independently , which probably isn't true! A game that is very Visually.Stunning and Exhilarating is probably way better than you would expect given those component parts. And it's probably the case that if a game gets scores of 1 on all features except Sporty, which gets a 4, that high Sporty score is worth less than if the other scores were higher. That is, many or all of your features probably interact . To fit an accurate model, then, you'll want to add in these interactions. That formula would look like this: glm(Liked ~ Visually.Stunning * Exhilarating * Artistic * Sporty, family = binomial, data = data) Now, there are two points of difficulty here. First, you need to have more data to fit a good model with this many interactions than the pure independence model. Second, you risk overfitting, which means that the model will very accurately describe the original data, but will be less good at making accurate predictions for future data. Needless to say, some people spend all day fitting and refitting models like this one.
