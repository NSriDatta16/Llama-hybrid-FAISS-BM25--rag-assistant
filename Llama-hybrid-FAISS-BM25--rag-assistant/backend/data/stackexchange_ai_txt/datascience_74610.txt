[site]: datascience
[post_id]: 74610
[parent_id]: 
[tags]: 
Decision Tree Induction using Information Gain and Entropy

I’m trying to build a decision tree algorithm, but I think I misinterpreted how information gain works. Let’s say we have a balanced classification problem. So, the initial entropy should equal 1. Let’s define information gain as follows: info_gain = initial_entropy weighted_average(entropy(left_node)+entropy(right_node)) We gain information if we decrease the initial entropy, that is, if info_gain > 0. If info_gain == 0 that means weighted_average(entropy(left_node) + entropy(right_node)) == initial_entropy. Let’s say we have 4 features such that weighted_average(entropy(left_node) + entropy(right_node)) is in this order wa_of_feature_0 > wa_of_feature_1 > … > wa_of_feature_4. The more info_gain is larger than 0, the more the feature makes order in system. So, based on our 4 features maximum information gain will be info_gain_max = initial_entropy - wa_of_feature_4 since that would give us bigger number than using wa_of_feature_n where 1 Is this the correct interpretation?
