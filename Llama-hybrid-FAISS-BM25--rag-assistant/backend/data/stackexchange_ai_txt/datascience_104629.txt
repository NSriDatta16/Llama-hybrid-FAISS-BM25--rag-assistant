[site]: datascience
[post_id]: 104629
[parent_id]: 
[tags]: 
When to use GloVe vocabulary vs. building a vocabulary from the training data?

While studying some (pytorch) examples that use pretrained GloVe vectors I came across two variants: Use the vocabulary of the GloVe vectors and thus initialize the embedding layer with the pretrained GloVe vectors. Build a vocabulary from the corpus and then only use the pretrained GloVe vectors that correspond to that vocabulary to initialize the embedding layer. To me it seems that by using the vocabulary of the GloVe vectors there is a chance that some of the tokens in the training set may not have corresponding GloVe vectors and are thus excluded from the vocabulary. Therefore, you may miss out on tokens that are significant for the task. On the other hand, building a vocabulary from the corpus implies that the model cannot handle unseen words (as far as I understand that correctly). Therefore, I was wondering: When should one use the GloVe vocabulary vs. building a vocabulary from the training data? And would it make sense to use the "union" of the two vocabularies instead?
