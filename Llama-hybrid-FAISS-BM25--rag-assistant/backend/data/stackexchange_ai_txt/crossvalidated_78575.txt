[site]: crossvalidated
[post_id]: 78575
[parent_id]: 71822
[tags]: 
Your solution changes because you scale all features but you neglect to rescale the misclassification penalty $C$. The SVM cost function is as follows: $$ \min \|\mathbf{w}\|^2 + C \sum_i \xi_i $$ where $\mathbf{w}$ is the separating hyperplane, $\xi$ is a vector of slack variables associated to misclassification of training instances and $C$ is a parameter you must set as a user. When you multiply all features by $1000$, $\|\mathbf{w}\|^2$ will be $1000^2$ times larger. You forgot to change $C$, which basically means that misclassifications of training instances are no longer penalized appropriately. If you rescale $C$ correctly you will end up with the same model. If $C$ is far too small, the minimization problem becomes almost identical to minimizing $\|\mathbf{w}\|$ (which is a useless result). The result is a very simple model which isn't informative e.g. $\|\mathbf{w}\|\approx 0$. If $C$ is far too large, the resulting model will be overly complex (e.g. $\|\mathbf{w}\|$ very large). Such a model overfits the training set and exhibits poor generalization performance. This is symptomized by high training set accuracy and poor test set accuracy (which is also a useless result).
