[site]: crossvalidated
[post_id]: 402507
[parent_id]: 402391
[tags]: 
Given how you've written your code, this is expected behavior. You've set the seed of the random forest explicitly. This means that the same randomization is used. Part of the randomization procedure of a random forest is to construct boot-strapped samples of the data. The way that sklearn accomplishes bootstrapping is to assign weights to indices of samples in $X$ . If you change the order of the data in $X$ , but fix the seed of the random forest, the same indices will be given bootstrap weights. However, in your code snippet, those indices correspond to different samples (because the data are shuffled), hence the data provided to each tree will be different between runs. Providing different data to a decision tree results in a very different tree, because trees are high-variance estimators. If you need to consistent results between different runs, you'll need to impose a fixed ordering on the data in addition to fixing the seed of the random forest. If you don't need exact correspondence between different runs, but just want results to be approximately the same between runs, you can just increase the number of trees. Increasing the number of trees in a random forest will tend to reduce the variance of the random forest procedure .
