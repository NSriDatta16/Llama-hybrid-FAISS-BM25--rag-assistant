[site]: crossvalidated
[post_id]: 544562
[parent_id]: 544548
[tags]: 
A fully specified regression model (of any kind) gives you a parameterized model of the regression function $E[Y | X]$ from which you can construct a model of the distribution $P(Y | X)$ , perhaps together with other information. In logistic regression $P(Y | X)$ is assumed to be Bernoulli (or Binomial) so $E[Y | X]=P(Y | X)$ (the conditional mean just is the conditional probability) so that is all you need to characterize the Bernoulli (or Binomial) PMF. In regression $P(Y | X)$ might rather be Normal, so you'll still get $E[Y | X]$ (and it's still a conditional mean) but you'll also need (and get) a model of $\text{Var}[Y | X]$ (a separate conditional variance). Together these quantities are enough to characterize any Normal PDF. There are, of course, lots of ways to evaluate probabilistic predictions, but the details will always depend on the distribution in question. For discrete regressions like Bernoulli: calibration, precision, recall, etc. For continuous ones like Normal: mean square error, predictive checks, etc. The bottom line is that you're getting $P(Y | X)$ from all regression models, one way or another. It just looks different depending what you assume about the nature of Y. So you evaluate those probabilities in a way that makes sense for Y. if you don't want to evaluate the confidence of a prediction but rather just express it, then @adri√†-luz 's suggestion is a natural one. That boils down the conditional distribution to a mean and an interval that is expected to contain some proportion, e.g. 95% of observations of Y for a particular value of X. Prediction intervals try to fold in parameter estimation uncertainty into the construction of the $P(Y | X)$ model but it's the mean plus interval construction that's doing the expressive work.
