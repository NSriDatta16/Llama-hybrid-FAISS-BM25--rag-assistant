[site]: crossvalidated
[post_id]: 440877
[parent_id]: 
[tags]: 
Is it good idea to generate features from data points similarity comparison?

I know about polynomial features in machine learning, which can introduce nonlinearity to original dataset. I also heard about binning, which also allows us to create new features from existing ones. However now, I am thinking whether it is possible to generate new features by comparing data points similarity. My idea is having a feature which tells something like "what is cosine distance of current datapoint's feature vector and feature vector of datapoint n". This way I could compare all data points and have many new features (for each datapoint I would have features comparing it to all datapoints - number of new features would be equal to number of datapoints). When I would do a prediction for new datapoint, I would have to compare it with all training datapoints to generate these additional features. My questions are: 1) is this idea good or bad? 2) is this idea somehow related to kernels? I was not able to understand them, but from what I read, it seems for me that they operate with this idea. 3) is this idea or something similar already implemented somewhere, if it's not completely stupid (in other words: am I not trying to invent wheel?) Thanks
