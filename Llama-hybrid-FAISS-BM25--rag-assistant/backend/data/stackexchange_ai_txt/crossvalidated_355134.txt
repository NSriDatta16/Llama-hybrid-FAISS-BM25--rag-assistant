[site]: crossvalidated
[post_id]: 355134
[parent_id]: 355054
[tags]: 
As an addition to @bookmins answer the concept of how to get to the term $S_W^{-1}S_B$ can be looked up here and in Marsland, S. (2015). Machine Learning an Algorithmic Perspective. 2nd ed. Boca Raton: CRC Press. p.132 as well. Additionally, here is stated, that finding the maximum of $$\frac{\boldsymbol{w}^T S_B \boldsymbol{w}}{\boldsymbol{w}^T S_W \boldsymbol{w}}$$ is the same as maximizing the nominator while keeping the denominator constant and therewith can be denoted as kind of a constrained optimization problem with: $\max_\limits{w}\boldsymbol{w}^TS_B\boldsymbol{w}$ with the constraint $\boldsymbol{w}^TS_W\boldsymbol{w}=K$ Bringing this constrained optimization problem into Lagrangian form gives: $$L=\boldsymbol{w}^T S_B \boldsymbol{w}-\lambda(\boldsymbol{w}^T S_W \boldsymbol{w}-K)$$ Finding the maximum of a funcion can be accomplished by calculating and setting the derivative equal to zero. $$\frac{\delta L}{\delta \boldsymbol{w}}=S_B\boldsymbol{w}-\lambda S_W\boldsymbol{w}=\boldsymbol{0}$$ or $$S_B\boldsymbol{w}=\lambda S_W \boldsymbol{w}$$ This is a generalized Eigenvalue problem and can (providing that $S_W^{-1}$ exists) be written as: $$S_W^{-1}S_B\boldsymbol{w}=\lambda\boldsymbol{w}$$ $$=$$ $$(S_W^{-1}S_B-\lambda\boldsymbol{I})\boldsymbol{w}=0$$ Solving this equation gives us the Eigenvalues ($\lambda$) and Eigenvectors ($\boldsymbol{w}$) and can be accomplished using numpy.linalg.eig(a) setting $S_W^{-1}S_B$ for a or manually by calculating $det(S_W^{-1}S_B-\lambda\boldsymbol{I})=0$, solving for $\lambda$ which gives us the Eigenvalues. Inserting these Eigenvalues ($\lambda$) into $(S_W^{-1}S_B-\lambda\boldsymbol{I})\boldsymbol{w}=0$ gives us a linear set of equations and solving these equations for $\boldsymbol{w}$ gives us the corresponding Eigenvectors.
