[site]: datascience
[post_id]: 24732
[parent_id]: 24728
[tags]: 
In general simpler models are more robust to noise in the input. The strength of neural networks is also their biggest 'gotcha' - they are extremely expressive. This means they easily overfit and can be sensitive to noise in inputs. The strategy of simplifying a model to make it more robust to noise is called regularization. There are many types: use smaller hidden layers (i.e. 20 instead of 200 nodes) use dropout, where during training inputs are randomly set to 0 during training (makes the network more robust to noise overall) stop training earlier - 'early stopping' use L1 or L2 regularization, which imposes a cost on the weights All of these can be done from with Keras. You want to make your network more robust to noise without decreasing your validation quality. To do this, I would try the above suggestions in order. You can measure overfitting by looking at the difference between predictive accuracy on your training data and validation data. If they are very different - your model has learned structure in your training data that is not in your validation that is NOT what you want. Try to fiddle regularization nobs until (1) your train-validation AUC or accuracy is very similar, (2) your validation AUC/accuracy is still sufficiently high.
