[site]: crossvalidated
[post_id]: 322031
[parent_id]: 
[tags]: 
How do I Estimate Joint Entropy Using a Histogram?

I am trying to estimate the entropy for two time series, defined by random variables $X$ and $Y$, each distributed according to an unknown PDF which is to be estimated empirically (using a histogram approach, and binning data equally according to Sturgeâ€™s rule ) In one dimension, I believe that we can use the estimated PDFs to then estimate the entropy of $X$, which is defined in this case for $n$ bins by: $H(X)= -\sum \limits_{i=1}^{n} P(x_i)\log P(x_i)$ Am I correct in my understanding that, considering the histogram approach, we can simply take the proportion of the total samples which lie within the bin as $P(x_i)$, multiplied by the log of the same, and perform a weighted sum across all bins using the midpoint value within each bin as the weight? However, to calculate the joint entropy between X and Y, we have multiple dimensions:$H(X,Y) = - \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}p(x,y)\log p(x,y)$ I am not sure that performing the same procedure as above, only now in the $X$ and $Y$ direction, quite achieves this. Is the approach correct? Should we perhaps only consider the bins on the diagonal? (i.e. $i=j$) Finally, as I am performing an autoregressive analysis, I also have additional variables representing lagged time series (e.g. $X - \Delta$, $X - 2\Delta$ etc.). I therefore need to be able to perform the calculation in the N-dimensional case. I am using Pandas data frames in Python to store the time series and perform the calculations, so answers with code samples are welcomed!
