[site]: crossvalidated
[post_id]: 642904
[parent_id]: 
[tags]: 
Transformer with just one input vector

I have a problem where I am mapping from 1D input sequences of length L to 1D output sequences, also of length L. These sequences contain numerical data. The input sequence is the time evolution of a particular physical quantity of a material, and the output sequence is the time evolution of a related physical quantity of the material. When I try an encoder-only transformer architecture, the performance is rather poor. (This is also true for an encoder-decoder transformer architecture). However, curiously, if instead of a [batch_size, L,1] tensor, I pass in a [batch_size, 1, L] vector to the encoder-only transformer, i.e. single vectors with dimension L (and outputs are also [1,L]), the performance is much better. I have two questions: My understanding was that the true power of the attention mechanisms is realised when operating on sequences of vectors, allowing the model to dynamically weigh the importance of different sequence elements relative to each other. Why might this not be the case here? Especially since the input and output sequences are time series? What even does the attention mechanism do if there is just a single vector input to the transformer?
