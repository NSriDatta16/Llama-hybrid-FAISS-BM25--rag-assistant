[site]: crossvalidated
[post_id]: 569381
[parent_id]: 
[tags]: 
Why use regularization instead of feature selection for logistic regression?

For a non-linearly separable problem, when there are enough features, we can make the data linearly separable. It seems to me that for logistic regression, the reason of overfitting is always excessive number of features. So why people mostly use l1, specially l2 regularization to shrink $w$ but not use feature selection? With the correct features (that can't perfectly sperate the data), does $w$ also become large?
