[site]: datascience
[post_id]: 48405
[parent_id]: 
[tags]: 
BERT stands for Bidirectional Encoder Representations from Transformers and is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers
