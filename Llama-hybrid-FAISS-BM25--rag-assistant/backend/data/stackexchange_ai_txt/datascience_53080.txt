[site]: datascience
[post_id]: 53080
[parent_id]: 
[tags]: 
Why using a frozen embedding layer in an LSTM model

I'm studying this LSTM mode: https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis They use a frozen embedding layer which uses an predefined matrix with for each word a 300 dim vector which represents the meaning of the words. As you can see here: embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False) The embedding layer is frozen which means that the weights are not changed during training. Why is this done?
