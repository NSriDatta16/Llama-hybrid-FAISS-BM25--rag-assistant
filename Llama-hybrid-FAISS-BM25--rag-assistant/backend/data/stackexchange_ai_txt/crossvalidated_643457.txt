[site]: crossvalidated
[post_id]: 643457
[parent_id]: 
[tags]: 
Are LLMs stateful or stateless during the generation process?

Are LLMs like OpenAI's gpt-* stateful or stateless during the generation of the response? I've read a couple of articles like this but am still not quite sure about that. I understand they are generating the response token by token. In each step: do that process basically throw away all computations except the generated tokens and start from scratch, but this time predicting the next token based on the original input plus the already generated partial response? (That's what I mean with stateless.) Or does the process keep some calculated activations and use them in the next step? I find that interesting for the intuition about LLMs. If you "humanize" LLMs a bit for your mental model, this question would basically be: does a LLM form some kind of plan for the generated text or next sentence (= stateful) and follow that, or does it basically guess in each step what it wanted to do with the partial response it generated so far and extend it with the next token (= stateless)? From what I read I suppose it's stateless in that sense, as it seems much easier to train an LLM this way, but it seems a little surprising and probably a bit wasteful, so I'd rather be sure. Thanks a lot!
