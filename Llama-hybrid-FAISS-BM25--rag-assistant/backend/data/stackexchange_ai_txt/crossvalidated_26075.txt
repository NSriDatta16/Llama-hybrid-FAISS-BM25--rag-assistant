[site]: crossvalidated
[post_id]: 26075
[parent_id]: 26047
[tags]: 
There is a somewhat similar idea, called a back-off language model. For these, you set a threshold $k$ and count up the number of n-grams in your training data. For n-grams that occur at least $k$ times, you estimate their probability in the normal way. Otherwise, you "back off" to the (n-1) gram's probability. I'm pretty sure there's a bit about this in Manning and Schutz's NLP book. People (or at least I) generally think about this as a way of avoiding over-fitting the training data, rather than representing any deep linguistic wisdom though. There's also the "varigram" approach, where the context length (i.e., the $n$ in $n$-gram) isn't fixed. This might be a little closer to your/Henry's suggestion, since it would let you capture collocations/stock phrases/etc of different lengths. I think the idea is at least partly due to Kneser (1996) . I'm not sure if your approach will save much storage space, either. For an $N$ word vocabulary, there are $N^2$ possible bigrams, $N^3$ possible trigrams, and $N^4$ quad-grams. Under your "paired bigram" model, there are still $(N^2) \cdot (N^2)=N^4$ possible pairs, though the sparseness might be a bit different. You may be better off storing the models as a (pruned) trie or something, if space is at a premium.
