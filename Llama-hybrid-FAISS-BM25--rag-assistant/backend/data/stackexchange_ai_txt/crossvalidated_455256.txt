[site]: crossvalidated
[post_id]: 455256
[parent_id]: 454348
[tags]: 
There are two questions raised in the post. A. what is a good metric to achieve in an imbalanced problem and B. what improvements are too be expected with more data become available. Both are rather broad questions but I will try to be be somewhat succinct. You are absolutely correct that Precision & Recall are potentially misleading metrics in an imbalanced problem. The CV.SE thread on: " When is unbalanced data really a problem in Machine Learning? " is an excellent read on the matter to get one started. Regarding your particular setting: using AUC-ROC at first instance is a good step because it gives a realistic baseline (0.50), how better we do compared to chance. Similar metrics like Cohen's kappa and lift-curves are also very helpful. I would suggest incorporating them in your analysis. (When optimising a model using AUC-PR is also a potentially good idea but it is harder to interpret directly as it has a variable baseline). A major angle when dealing with an imbalanced datasets is the cost of misclassification, this explored with the context of cost-sensitive learning . Especially for a customer churn model this is very prominent. Aside the standard adage: " A False Positive can have a different misclassification cost that a False Negative ", customers themselves have different misclassification costs. A young super-market customer who spends on average \$20-25 per week is less valuable than an older customer who spends on average $150 per week when doing their weekly shopping. We can define cost-sensitive functions to evaluate against with XGBoost and when using other frameworks too. Improvement coming from additional data can be presented by using learning curves ; they allow us to estimate how much gains (in terms of our metric of interest) we should expect when adding more training data. The 1994 paper " Learning Curves: Asymptotic Values and Rate of Convergence " by Cortes et al. is a seminal on the matter. Figueroa et al. " Predicting sample size required for classification performance (2012) offers a more modern an exposition (as well as a more modern way to calculate learning curves). CV.SE has an interesting thread on the matter here: " How to know if a learning curve from SVM model suffers from bias or variance? " it covers SVM but the learnings of it apply for any classifier. So to recap: There is not single realistic precision (or recall) to aim when dealing with imbalanced learning task. It is far more reasonable to utilise misclassification costs and/or use a simple model as a baseline and show improvements from there one. The impact of getting more data can be use explored through learning curves. If they do not show additional improvements, do not be alarmed. This is potentially a hint that the question asked might need to be changed. And on that matter another CV.SE gem: " How to know that your machine learning problem is hopeless? ".
