[site]: datascience
[post_id]: 44786
[parent_id]: 44768
[tags]: 
I believe this statement can be supported with the concept of VC dimension . This blogpost provides a simplified explanation of the term. VC dimension can be understood as an ability of a classifier to learn complex dependencies in the data. From other hand, models with a huge VC bound tend to overfit on small amounts of data. However, the plot you provided here depicts what is happening when our training set grows indefinitely - the win situation for models with a large VC dimension. The VC bound of neural networks is something like polynomial from number of weights and connections between them. While VC dimension of a SVM classifier is linear to the dimensionality of the space it operates in. Thus, neural networks can benefit more from larger amounts of training data. And the more weights you have, the better will be the result. Of course, given that the number of training samples is at least 10 times more than number of weights (a rule of thumb), so you do not face overfitting. That is why data augmentation and regularization are so important in deep learning.
