[site]: datascience
[post_id]: 12995
[parent_id]: 12992
[tags]: 
Based on this paper (Qi, Szummer, and Minka. 2005. Bayesian Conditional Random Fields): In this paper, we propose Bayesian Conditional Random Fields (BCRF), a novel Bayesian approach to training and inference for conditional random fields We can see that the original CRF model is presumably not Bayesian, since this paper's contribution is a novel Bayesian approach to the CRF model. Based on this excerpt of the book : Each of the five tribes of machine learning has its own master algorithm, a general-purpose learner that you can in principle use to discover knowledge from data in any domain. The symbolists’ master algorithm is inverse deduction, the connectionists’ is backpropagation, the evolutionaries’ is genetic programming, the Bayesians’ is Bayesian inference, and the analogizers’ is the support vector machine. It seems that CRF can be put into connectionists (since CRF calculates optimizes a function based on the gradients) or analogizers (since CRF uses "soft-examples" calculated by softmax to separate the good output and bad outputs, as compared to the support vectors in (structured) SVM calculated by max. For example, this paper shows that the difference CRF and SSVM are the addition of cost function and change from softmax to max)
