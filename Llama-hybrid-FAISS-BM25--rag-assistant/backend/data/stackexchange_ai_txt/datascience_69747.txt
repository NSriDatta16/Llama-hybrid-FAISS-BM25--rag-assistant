[site]: datascience
[post_id]: 69747
[parent_id]: 69743
[tags]: 
SGD just requires the first derivative, but Newton-Raphson ends up requiring the second derivative, which might be hard or impossible to compute. It will also need more computation as a result. For non-convex problems, note that the derivative is 0 at maxima as well as minima, and at saddle points, which (as I understand) takes a little more care to get right with numerical methods. But yes for logistic regression, which is convex and for which second derivates are not complex, Newton's method would be fine too. I believe it's usually optimized with L-BFGS, not simply SGD, which is in a way more like Newton's method. I think it was explained with SGD just to keep it simple.
