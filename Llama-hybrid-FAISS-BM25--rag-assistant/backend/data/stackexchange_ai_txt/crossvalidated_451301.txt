[site]: crossvalidated
[post_id]: 451301
[parent_id]: 450983
[tags]: 
As it stands the two runs are not directly comparable. For example, off the bat, we can see that: catboost uses a stronger $L_2$ regularisation on the weights ( $3$ instead of $1$ in xgboost ). catboost uses different bagging/subsampling in terms of rows ( $0.8$ instead of $1.0$ in xgboost ). catboost uses a different way than xgboost to built its trees; symmetric instead of best-first in XGBoost. catboost uses an extra regularisation parameter ( bayesian_matrix_reg ) to regularise leaf values calculation (the non-diagonal ones when computing the hessian on the leafs); strictly speaking xgboost do not have the same functionality. Please note that the above points are not even a very in-depth look. While we might expect the same qualitative behaviour between different implementations of a general computational framework (e.g. lowering the depth of the base-learner trees decreases the chance of over-fitting), the effect size might not be comparable. As these implementations incorporate a number of computational techniques to accelerate and/or make them more stable, the direct comparison of their behaviour is unreliable.
