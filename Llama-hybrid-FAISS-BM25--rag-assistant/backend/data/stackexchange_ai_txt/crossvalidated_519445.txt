[site]: crossvalidated
[post_id]: 519445
[parent_id]: 519442
[tags]: 
Maximum likelihood estimators in logistic regression are one example. The MLE is infinite with non-zero probability, which stays non-zero for all $n$ , though it decreases exponentially with $n$ . $E[(W_n-\theta)^2]$ is infinite, and does not converge to zero. If you don't like estimators that can be non-finite, take $W_n$ as mean of a sample of size $n$ from a $t_2$ distribution. The distribution has a well-defined mean, zero, but no finite variance, so $E[(W_n-\theta)^2]=E[W_n^2]$ is always infinite. I don't know of any naturally occurring examples where $E[(W_n-\theta)^2]$ is finite but doesn't converge to zero, but it's easy to rig up examples if $W_n$ is allowed to be silly. For example, if you have $n$ iid observations from $N(\mu,1)$ and take $W_n=\bar X_n$ with probability $1-1/n$ and $W_n=n^{2/3}$ with probability $1/n$ , then $W_n$ is consistent for $\mu$ (because $P(W_n=\bar X_n)\to 1$ ) but the variance doesn't vanish. Basically, what you need is that a vanishing fraction of 'wrong' values of $W_n$ can be very very wrong, but most of the values of $W_n$ are close to $\theta$ .
