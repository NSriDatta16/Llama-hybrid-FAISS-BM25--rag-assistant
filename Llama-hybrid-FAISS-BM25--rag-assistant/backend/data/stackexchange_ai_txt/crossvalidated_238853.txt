[site]: crossvalidated
[post_id]: 238853
[parent_id]: 
[tags]: 
Concatenation of weights in Neural Networks

I'm training a special neural network for Natural Language Processing which has an embedding layer (this layer takes a one-hot vector of each word and output it's embedding vector through an embedding matrix). So, what I want to use is two embedding matrix for the same input. Formally, if we have $x \in \mathbb{R}^{1 \times n}$ as input and two embedding matrix $w_1 \in \mathbb{R}^{n \times d1}$ and $w_2 \in \mathbb{R}^{n \times d2}$ I want an output $e \in \mathbb{R}^{1 \times (d1+d2)}$ that contents the two embedding vectors. To do so, we can $x \cdot w_1$ and concatenate it with $x \cdot w_2$ and thus the NN can learn the weights separately. But what I thought is that I can create a "super"-embedding matrix $\in \mathbb{R}^{2n \times (d1+d2)}$ filling with 0's as: \begin{bmatrix} w_1 & 0\\ 0 & w_2\\ \end{bmatrix} and do the dot product with the concatenation of [$x$; $x$]. Finally, I got the same result but I have twice questions: Which is the most complex (computationally)? The second is bigger but only require one dot product and the concatenation is before the layer. ‌Does the weight filled with zeros change its value in the NN learning phase? ‌Thanks in advance!
