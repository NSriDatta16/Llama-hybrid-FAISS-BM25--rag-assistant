[site]: datascience
[post_id]: 22218
[parent_id]: 
[tags]: 
What should I do in case the resolution of the image varies in the dataset?

Suppose that I train my image dataset on CNN, but the resolution of the image varies significantly on the dataset. In this case, should I scale the images up to the image that has the maximum resolution, or scale down to the lowest resolution? In that case should I scale up/down the whole images even if the highest likelihood of the whole samples are somewhere in the middle of the distribution of the resolution? Or should I use another technique to deal with the varying resolution problem?
