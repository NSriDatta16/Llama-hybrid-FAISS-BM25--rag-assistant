[site]: crossvalidated
[post_id]: 427107
[parent_id]: 
[tags]: 
Bias Variance Decomposition

In "Elements of Statistical Learning" by Hastie et al. I am confused about the notation they use. (p.24 Second edition) For $Y=f(X)=e^{-8\| X\|^2}$ they use the 1-NN approximation to predict $y_0$ at the point $x_0=0$ . The prediction will be notated as $\hat y_0$ . The training set is denoted $\mathcal{T}$ . They decompose the mean squared error into $MSE(x_0)=E_\mathcal{T}(f(x_0) -\hat y_0)^2$ I don't understand this expression at all. First, how can you take an expectation over triaing sets? How is this even defined? I only know of expecations for random variables. Second, what is this expecation taken over? There are only deterministic quantities in this expression. EDIT: I have made some progress in understanding: The expectation is taken over different training sets. This means we sample e.g. $n$ pairs $x_1,...,x_n$ and their targets $y_1,...,y_n$ and then sample again in the same way until every possible sample has been obtained. The expectation then corresponds to (at least I think) an average of the trained (on different sets) models. I don't understand how the expectation is defined formally, because not for every model exists a closed form for obtaining the trained parameters. I therefore view this expecation as a rather abstract quantity which intuitively stands for the "mean" of the model.
