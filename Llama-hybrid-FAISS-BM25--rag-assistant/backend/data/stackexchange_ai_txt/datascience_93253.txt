[site]: datascience
[post_id]: 93253
[parent_id]: 
[tags]: 
Why is averaging the vectors required in word2vec?

While implementing word2vec using gensim by following few tutorials online, one thing that I couldn't understand is the reason why word vectors are averaged once the model is trained. Few example links below. https://www.kaggle.com/ananyabioinfo/text-classification-using-word2vec/notebook?scriptVersionId=11358361&cellId=7 https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec?scriptVersionId=2185653&cellId=15 My questions are: Is it just to create a single vector instead of vectors of the dimension size or to increase the accuracy or is there any reason behind this? Is it mandatory to take average of the vectors or are there any alternatives to this. I have gone through the original paper on word2vec but that doesn't give clear explanation on this.
