[site]: datascience
[post_id]: 40617
[parent_id]: 21912
[tags]: 
Your edit is not right. from the keras documentation you can actually understand the difference between timesteps and batches. I take your examples: For the first example. You have 4 instances, or samples, or sequences. The length of each sequence is 30, now you actually take 3 batches, each of these batch is composed by 4 instances, or samples, or subsequences, of length 10(timesteps). your batch size is not 10 but 4, the timesteps don't affect the batch size. the subsequences of each batch can have the length that you want, this doesn't change the fact that the batch size is 4. For the second example is the same. Online learning doesn't mean that the number of timesteps of each subsequence is 1, you can take the number of timesteps you want, but when you update the parameters of the network you should consider only 1 subsequence at the time. You can actually confirm this from here: http://philipperemy.github.io/keras-stateful-lstm/ In this part: "Said differently, whenever you train or test your LSTM, you first have to build your input matrix X of shape nb_samples, timesteps, input_dim where your batch size divides nb_samples. For instance, if nb_samples=1024 and batch_size=64, it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector." So the timesteps don't affect the batch size.
