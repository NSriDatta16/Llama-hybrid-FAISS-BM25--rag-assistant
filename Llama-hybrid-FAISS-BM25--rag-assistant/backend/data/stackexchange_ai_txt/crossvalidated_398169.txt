[site]: crossvalidated
[post_id]: 398169
[parent_id]: 
[tags]: 
Interpreting gradient descent as a constrained optimization problem- Reinforcement learning

I' m studying the lectures of Sergey Levine in reinforcement learning, specifically the TRPO algorithm, during his explanation we claims that gradient descent is the same as doing this. He does not show the proof to why that is. I intuitively understand that we have a local linearization of the objective function and we can only use that approximation arround $\theta$ , therefore using the euclidean distance between $\theta$ and $\theta'$ and limiting by $\epsilon$ we guarantee that we don't update the parameters to much to the point where the approximation has less precision. Can someone help figure out how he gets the result for $\theta'$ . I tried to construct the Lagragian an setting its' gradient to zero, but I got lost in the computation. Thank you very much in advance.
