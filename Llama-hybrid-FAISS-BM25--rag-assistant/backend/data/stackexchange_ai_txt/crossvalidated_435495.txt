[site]: crossvalidated
[post_id]: 435495
[parent_id]: 435481
[tags]: 
sklearn is, unfortunately, very much built with a supervised mindset. This shows in many is places, such as the y variable even being present at all in unsupervised "learners" (that don't learn either) fit(X, y=None) methods. Not only is the default None - it will also be ignored by k means if set to anything else. Normally, you wouldn't do cross validation on unsupervised methods at all - because of you don't have labels, how could you validate? One of exception is kmeans. Because k-means can actually "predict" cluster numbers for new objects (i.e., find the nearest center), you can perform some kind of validation here: if the model is good, you'd expect unseen objects to have a similar distance to the means as seen objects. If the model is overfitting the training data, then you'd expect the distances to be much smaller for training than for test. Now the plot you have has some issues: (1) it's inertia, not average inertia; the training setnis much larger than the test set and thus the inertia will be several times larger (more precisely, 9 times with 10x CV). (2) the values are expected to drop with k, hence the plot will have this drop-off curve that by itself does not mean a lot. It would likely make more sense to look at average inertia / (k-1)... Or the quotient of test vs. train average inertia. But even with these modifications I doubt you'll get some useful insights. First of all, the shape of the curve already suggests rather poor performance (no clear "elbow", but rather the exponential drop you get on random data), and secondly the test average inertia will likely always be quite a bit bigger than the average training inertia - so when does it overfit, when not?
