[site]: crossvalidated
[post_id]: 613862
[parent_id]: 612933
[tags]: 
Formally speaking no, but we can fake it. In particular we can make use of capabilities of the fit method to continue training an old model. As such we do the following: We make $X_{\text{whole}}$ to store the data from dataset $X_1$ as well as $X_2$ . $X_1$ features are available data at the start of the project. $X_2$ features are unavailable so we explicitly sets them a fixed value (say 42 ). It is important that all of them are fixed because in that way: a. they don't covary with any of the known features in $X_1$ and b. any splitting algorithm will ignore them as no splitting can ever result in a reduction to our loss. We train for the first $k$ iterations normally using $X_{\text{whole}}$ such that $X_{\text{whole}} = [X_1 , X_2]$ . Our booster up until that iteration uses (by necessity) only information from $X_1$ . After $k$ iteration have passed/move to new environment, etc. $X_2$ 's feature information are now available. We update our $X_{\text{whole}}$ such that it now holds the relevant $X_1$ features it did before but now $X_2$ contains the newly available information. We train for the rest $k+1$ to whatever total number of iteration we expect using the updated $X_{\text{whole}}$ . The first $k$ trees are preserved doing splits only with information from the features available in $X_1$ but the subsequent tress starting from $k+1$ can using features from $X_1$ or $X_2$ depending on the feature's informativeness. To my knowledge all three major Python GBM implementations have training continuation capabilities in their fit methods. XGBoost , LightGBM and CatBoost suggest that in their documentation. (Links provided, albeit with slightly different ways of achieving this - LightGBM & CatBoost use a init_model argument, while XGBoost straight-up retrains an existing booster.)
