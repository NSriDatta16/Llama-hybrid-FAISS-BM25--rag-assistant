[site]: crossvalidated
[post_id]: 546953
[parent_id]: 99214
[tags]: 
I would normally opt for something more like the first procedure, but for many regularised models (e.g. ridge regression), leave-one-out cross-validation performance can often be evaluated (or at least approximated) as a by-product of fitting the model. This is much more efficient as you don't need to train the model K times, just the once. This includes linear models, radial basis function neural networks, kernel learning methods, splines etc. Now both of these methods will give biased performance estimators because the model is being trained on a subset of the available data, rather than all of it. The generalisation of most methods improve with more training data, so this will be a pessimistic bias. This source of bias is likely to be greatest for the second method, where only 60% of the data is used to train the model. For K = 3 or more, each model will be trained on more data using the first method than the second. The other source of bias is because the hyper-parameters of the model have been tuned using the same data on which it is evaluated. This only affects method one and method two has a separate test set. The variance of method two is likely to be higher than that of method one. This is because (i) the model is trained on less data, so the estimates of the parameters will be noisy (and vary for different partitions of the data) (ii) because the data used to select the hyper-parameters is very small (only 20%) and (iii) the data used to evaluate the generalisation error is very small (again only 20%). By re-sampling, the cross-validation in method one is making better use of the data, it is all being used to select the hyper-parameters and it is all being used for performance evaluation. However, if you want low bias and low variance, I wouldn't use either method. I would use nested cross-validation, where the outer cross-valdaition is used to get an unbiased performance estimate (or at least to eliminate the bias due to optimising the hyper-parameters) and witing each fold of the outer-cross-validation, and inner cross-validation is used to tune the hyper-parameters. It is better to think of cross-validation as a evaluating a method of fitting a model, rather than the model itself. See the paper I wrote with Mrs Marsupial: G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, vol. 11, pp. 2079-2107, July 2010 ( pdf )
