[site]: crossvalidated
[post_id]: 193962
[parent_id]: 193818
[tags]: 
Let's begin with the definition of entropy Citing from wikipedia The entropy rate of a data source means the average number of bits per symbol needed to encode it. If we use a fair coin, we will need a bit per case in order to store the outcomes. If we use a coin X whose probability of head is 0.999 we can use way less bits E(X) = -(0.999*log(0.999,2)+0.001*log(0.001,2)) ~ 0.01 You can use techniques like Huffman coding in order to store the outcome efficiently. Mutual information MI(X,Y) measures how many bits will you need in order to store the outcomes Y given that you know the value of X. The bits/nits comes from the base of the log used in the entropy and mutual information formulas. If you use log based 2, you get bits. If you use log based e (ln), you gets nits. Since we store data on computers that use a binary system, bits are the common and more intuitive unit.
