[site]: datascience
[post_id]: 52775
[parent_id]: 
[tags]: 
A robust metric in the presence of class imbalance

When evaluating the performance of a multiclass classification problem, on a highly imbalanced dataset, what is the most robust metric for this purpose? I read a paper that states: "Average precision is a robust metric in the presence of class imbalance since it excludes the ‘true negatives’ constituent in specificity, focusing instead on precision, or positive predictive value." The confusing part, for me, is the applied methodology in this paper. They used balancing techniques (SMOTE, class_weigh, random sampling) to oversample minority classes, but they are still concerned about the evaluation metrics and 'true negatives'.
