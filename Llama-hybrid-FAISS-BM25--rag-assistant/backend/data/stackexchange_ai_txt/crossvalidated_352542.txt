[site]: crossvalidated
[post_id]: 352542
[parent_id]: 352452
[tags]: 
The simple, almost generic, answer to such question is that the difference is that Bayesians define models in probabilistic terms, so the parameters are considered as random variables and they assume priors for such parameters. In classical setting, your model has some parameters and you are using some kind of optimization to find such parameters that best fit your data. In neural networks this is done almost exclusively by starting with some randomly initialized parameters and then using some variant of gradient descent to update the parameters, so that the loss is minimized. In Bayesian setting, instead of point estimates of the parameters, we want to estimate the distributions of the parameters, because we consider them to be random variables. This is done by starting with some a priori distributions assumed for the parameters, that are updated using Bayes theorem when confronted with data. This is usually done either with some kind of optimization, or by Markov Chain Monte Carlo sampling (simulating draws from the posterior distribution). So gradient descent is an optimization algorithm, while Bayesian approach is about defining the models differently (see also this comparison of maximum likelihood and gradient descent ). Using gradient descent to minimize some kind of loss function does not have much to do with Bayesian approach because it does not consider any priors.
