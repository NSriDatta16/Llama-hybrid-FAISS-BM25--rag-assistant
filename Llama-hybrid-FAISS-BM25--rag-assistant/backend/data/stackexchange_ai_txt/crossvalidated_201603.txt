[site]: crossvalidated
[post_id]: 201603
[parent_id]: 201599
[tags]: 
Multicollinearity doesn't make estimators biased, rather it increases their variances. You could also think of it as reducing the effective sample size. Also, in presence of cointegrated time series the models are often formulated in first differences and stationary combinations of variables. E.g. the vector error correction model has first differences of the dependent variables on the left hand side and both stationary combinations (the error correction terms) and first differences (lags of the dependent variables) on the right hand side. This makes multicollinearity in levels redundant. Also, if cointegrated series are used in levels, superconsistency of estimators acts in opposite to multicollinearity, the former effectively increasing the sample size and the latter effectively reducing it. I am not sure of the relative effects of the two (which is stronger and which is weaker), but if the effect of superconsistency is at least as strong as that of multicollinearity, then the latter should be of limited importance in practice. I don't have an answer regarding the Johansen test. Edit: a good point raised by @Sympa is that multicollinearity regards the regressors as a group but not the relationship between the regressors and the regressand. So, for example, "multicollinearity" between $y$ and $x$ in a model $y=\beta_0+\beta_1 x+\varepsilon$ is not really an example of multicollinearity and is by no means harmful. Also, multicollinearity can be encountered simultaneously with cointegration in cases where several regressors are cointegrated and multicollinear. (For completeness, multicollinearity would be irrelevant in case there is only one regressor that is cointegrated with the regressand, just as in the equation above.)
