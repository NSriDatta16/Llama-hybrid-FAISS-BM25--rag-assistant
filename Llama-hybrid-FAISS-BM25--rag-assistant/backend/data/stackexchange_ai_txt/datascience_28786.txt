[site]: datascience
[post_id]: 28786
[parent_id]: 25463
[tags]: 
Actually the shapes are for simplification. If you want to know the correct behavior you have to take a look at the formulas of each LSTM cell. To answer your first question, There may be different answers. What the pictures are depicting belongs to tasks which are many to many, and for each input you need exactly one output. There are different tasks for sequences that can be defined: One to one One to many Many to one Many to many You can take a look at here . For illustrating the formulas of each LSTM cell I've given the following picture from professor Andrew Ng course about deep learning: As you can see, each node in the LSTM cell can be connected indirectly to the output of the adjacent cells of the previous time-step. It's indirect because there are gates between them. Also consider that LSTM cell shares the weights for all the inputs of different time steps. Consequently each neuron in LSTM cell is dependent to the input of the current time-step and the output of the adjacent nodes of the previous time-steps. About the third question, the input size will be equal to the number of features of the input for each time-step. The number of the outputs depends on your task as I referred to at first. Take a look at the first link.
