[site]: crossvalidated
[post_id]: 485033
[parent_id]: 485014
[tags]: 
With XGBoost, you already have the feature importance and ability of handling missing values. For handling missing values, the original paper explains well: It is important to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node. When a value is missing in the sparse matrix, the instance is classified into the default direction. There are two choices of default direction in each branch. The optimal default directions are learnt from the data... The key improvement is to only visit the non-missing entries. The presented algorithm treats the non-presence as a missing value and learns the best direction to handle missing values. And, if the missing percentage of a feature is too high to provide any valuable information, why not drop it? If you want to manually impute the missing data, this is very much problem dependent, like missing patterns, data distributions... For feature importance, you have a tutorial from the official doc . 250 features VS 4 million of samples sounds a good deal, even if imbalance exists. If necessary, you can try either upsampling (random repeat, SMOTE, ADASYN...) the minor class or downsampling the major class.
