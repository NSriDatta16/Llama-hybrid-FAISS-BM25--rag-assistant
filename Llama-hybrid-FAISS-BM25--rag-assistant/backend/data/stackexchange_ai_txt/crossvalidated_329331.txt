[site]: crossvalidated
[post_id]: 329331
[parent_id]: 329223
[tags]: 
I'm not sure what do you mean by 'preferring' some activations, but there are ways to 'encourage' something similar. Prefer activations that are bigger than a certain threshold, e.g, positive activations If you use ReLU (Rectified Linear Unit) as activation function, you'll only get positive values. Prefer activations that are within a certain range, e.g., [-0.5, 0.5] You can use activation function that clips activations to your range. It would be also helpful if you shared something on context, since there may be other techniques that work in a similar way (for example Batch Normalization sort of encourages activations to be standardized, and weight decay also has some effect on activations because it penalizes 'big' weights).
