[site]: crossvalidated
[post_id]: 491208
[parent_id]: 
[tags]: 
What role does the true distribution of data play in machine learning?

It seems to be a very simple question: Does the "true distribution" or "natural distribution" of training data matter in machine learning? The motivation for asking this question comes from the classification of imbalanced data. To handle the imbalanced data problem in classification, we can use methods such as under-sampling or up-sampling to generate a relatively balanced training data set, which can help the model perform better. In this case, haven't we changed the distribution of the data? Furthermore, can we arbitrarily change the distribution of data to observe the performance of the model, and then select the best model? This seems very unreasonable, but where is the problem?
