[site]: crossvalidated
[post_id]: 442218
[parent_id]: 
[tags]: 
Why using gradient decent if we can just minimize function in closed form?

I don't really understand why gradient descent is so important in neural networks? Wouldn't it be much easier to define an objective function in a way to do 0=parameters of objective function ? This way you would get the most perfect parameters out imidiately.
