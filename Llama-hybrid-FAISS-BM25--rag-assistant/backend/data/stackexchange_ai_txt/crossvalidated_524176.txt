[site]: crossvalidated
[post_id]: 524176
[parent_id]: 223256
[tags]: 
Yes, sure. What is Cross-Entropy? Let's think about what is Cross-Entropy (CE). CE cost in the context of PyTorch or another Frameworks can mean a different thing compare to MATH. Originally cross-entropy is some form of KL-divergence between distributions: https://sites.google.com/site/burlachenkok/articles/properties-of-kl-divergence Reasons: In the context of Machine Learning, the very often first argument for CE is typically vector from probability simplex such that it has one component equal to one. So input for CE is probability mass function (p.m.f.) in a discrete case. Inside PyTorch let's say there is an extra transformation called in STATS as "symmetric logistics transform" which $$\dfrac{exp(f_i(x)}{exp(f_1(x))+exp(f_2(x))+\dots + exp(f_k(x))}$$ What it's interesting it's a bijective mapping from Euclidane space into probabilistic simplex. Now, what is a regression? Regression in the context of probability theory means $E_{z}[y(x,z)|x]$ where by z I denote unobserved variables. In the context of machine learning, it means any predictor with an output single scalar variable or multiple scalar variables and it is not necessary conditional expectation. So as you see - there is already too much confusion with various terminology and basic terms in STATS and ML. If your model provides K scalar outputs (called sometimes logits) it can be plugged into CE with the symmetric logistic transformation. For the record - logits are just unbound scores from $\mathbb{R}$ such that class with maximum score is your prediction. I think the answer you can do whatever you want, and people do crazy things with Loss in STATS, Optimization, and Deep Learning Applications. I can not give you an exact answer because CE very often raised with classification models e.g. in Deep Learning or with Decision Trees. But if your question because you design such a system - it's better to allow more expressive power in Loss construction.
