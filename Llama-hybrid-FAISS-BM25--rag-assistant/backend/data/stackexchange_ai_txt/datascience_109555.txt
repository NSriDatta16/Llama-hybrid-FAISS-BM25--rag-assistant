[site]: datascience
[post_id]: 109555
[parent_id]: 109302
[tags]: 
Just wanted to follow up on this with what worked. While I liked Erwan's answers, I couldn't find a good way to truly implement it. Jamie's suggestion of SVD was helpful, but it did smear over groups as I was worried and ultimately more afforded me some computational complexity reduction for early runs, but not the final answer. Ultimately just using a standard clustering algorithm was effective, but more specifically: Aglomerative Clustering - It can reasonably handle a large number of samples as well as a large number of resulting clusters. Additionally it turns out there was a hierarchical structure in my data, which this indeed works well with. Cosine Distance - Cosine similarities and distances are well-suited to sparse datasets (after all they're often used on OH-encoded word tokens in NLP problems), so this was versatile to my needs. So ultimately this did end up being a fairly standard clustering problem and my dimensionality/sparsity wasn't too terrible, though I did have less cardinality than I originally supposed (30K rows, 800 columns).
