[site]: datascience
[post_id]: 20006
[parent_id]: 
[tags]: 
How to improve the accuracy of a Doc2Vec model (Gensim) in case of a toy-sized data set?

I'm building an NLP question-answering application using Doc2Vec technique in gensim package of Python. My training questions is very small, only 20 documents and I am getting very inaccurate and different similarities even for same document while running at multiple instances. Almost all the sources which I referred trained data set containing thousands of documents. So I infer the reason behind my model's inaccuracy is the size of my data set. Is there any way to improve the similarity between documents, maybe by changing parameters or feature engineering? If yes, what are those parameters and by what ratio should I change them? If no, what are other ways or perhaps other neural network models to tackle the problem?
