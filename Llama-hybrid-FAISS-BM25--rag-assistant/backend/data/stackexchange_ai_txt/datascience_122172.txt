[site]: datascience
[post_id]: 122172
[parent_id]: 
[tags]: 
Why shouldn't the attention matrices $W^Q$, $W^K$, $W^V$ be the same?

My question is why the equally shaped attention head matrices $W^Q$ , $W^K$ , $W^V$ should not be the same $W = W^Q =W^K= W^V$ . In my understanding of transformer-based language models one attention head is responsible for one syntactic or semantic relation between any two words in the context. One might think that such a relation is represented by one matrix $W$ that projects the full word embeddings $x_i$ from their full semantic space to a semantic subspace. Here we could - in principle - calculate scores $\sigma_{ij}$ as "similiarities" between two projected words $Wx_i$ and $Wx_j$ and then calculate the weighted sum of the projected tokens $Wx_k$ . I wonder why this would not work, and why we need three different matrices. The other way around: What does it mean to calculate the score as the dot-product of two vectors from two different semantic subspaces? Is this still some kind of similiarity (which lies at the heart of word embeddings)? And doesn't it sound like comparing apples and pears?
