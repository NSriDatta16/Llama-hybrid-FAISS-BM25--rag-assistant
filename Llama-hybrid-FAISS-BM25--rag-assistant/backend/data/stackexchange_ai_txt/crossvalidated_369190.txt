[site]: crossvalidated
[post_id]: 369190
[parent_id]: 
[tags]: 
focusing on hard examples in neural networks, like in gradient boosting?

gradient boosting can be seen as focusing on the hard examples (the training set examples where the prediction is still far from the true label, and the gradient is still big). is there a similar notion for neural networks? as in selecting the next batch of examples from the hard examples? (note that this is a different question than Boosting neural networks - where they talk about using neural network as the weak learner)
