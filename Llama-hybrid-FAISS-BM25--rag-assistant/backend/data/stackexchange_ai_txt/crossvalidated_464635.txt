[site]: crossvalidated
[post_id]: 464635
[parent_id]: 464634
[tags]: 
One case is when you deal with high parametric case and use penalised estimators, in you question it could be logistic regression with lasso. The shrinking decreeses variance by killing some features (possibly significant), but at the same time it reduces the bias. Another case which comes to my mind is consistent model selection (in regression setup, though, for example, with BIC): with probability going to one we choose correct model, though for a moderate data set the selected model can be "smaller", which could give a big bias.
