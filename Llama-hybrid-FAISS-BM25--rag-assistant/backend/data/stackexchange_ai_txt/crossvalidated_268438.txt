[site]: crossvalidated
[post_id]: 268438
[parent_id]: 200445
[tags]: 
I'd like to explain a little bit the perceptron. Let's say the calculated output $h(x)$ is $sign(W^T \cdot X)$ , where Pay attention that the calculated output is the sign (it's 1 if $W^TX \gt 0$ and -1 otherwise) rather than the product value. We consider only when the classifier makes mistakes, which means that $h(x) - y$ (y is the label in observations) is not 0 and $h(x) * y$ is $-1$ (the label is 1 but the calculated output is -1, orthe label is -1 but the calculated output is 1). If $y$ is $-1$ but the prediction is $1$ . The red vector is the incorrect old weight vector and the dotted vector is $yx$ . $y$ is $-1$ so it has an opposite direction as $x$ , and the prediction, the sign(positive) of the product of $w$ and $x$ with an angle between 0 and 90, is 1. We update the $w$ by $yx$ and make it the blue vector. After the update the angle of the blue vector(new weights) and the black vector(input) is between 90 and 180, making the sign (which is the same as the label $y$ ) opposite to that between the red vector $w$ and the black vector $x$ If $y$ is $1$ but the prediction is $-1$ . In this case we need to correct the weight vector to gain an angle with the input vector between 0 and 90 to obtain an inner product that has the same sign of the label(1). We can apply the same to the old weight vector by adding the negative $yx$ . To sum up, the update always drags the weight vector a little bit to make the sign of inner product has the same sign of the label by add a $yx$ (the dotted vector in each case). Hope I have cleared up the issue a bit and may this explanation help. EDIT As described above, the perceptron is the normal perceptron. And an other perceptron which is often called soft perceptron is the same as logistic regression(binomial). Let me reduce $$\Delta z_i = \text{learningRate} \cdot (\hat{y} - y) \cdot x_i$$ for you from the sigmoid function. $$h(W^TX) = \frac{1}{1+e^{-W^TX}}$$ \begin{cases} P(y=1; X, W) = h(W^TX) \\ P(y=0; X, W) = 1-h(W^TX) & \end{cases} We can combine these two into one, $$P(y;X,W)=h(W^TX)^y(1-h(W^TX))^{1-y}$$ And for all cases, $$L(W)=\prod(W^Tx^{(i)})^{y^{(i)}}(1-h(W^Tx^{(i)}))^{(1-y^{(i)})}$$ And we take log of both sides and get, $$l(W) = logL(W)=\sum_{i=1}^my^{(i)}logh(W^Tx^{(i)})+(1-y^{(i)})log(1-h(W^Tx^{(i)}))$$ To get the gradient(or update value) we take the partial derivative with respect to $W$ and get, $$\frac{\partial l}{\partial W}=\sum_{i=1}^m[y^{(i)}-h(W^Tx^{(i)})]x^{(i)}$$ Which is the same as your function. For the $xy$ case, the activation function is sgn, meaning above a threshhold we take 1 and below it we take -1, but for the latter one, the soft perceptron, the activation is the sigmoid function, thus the gradient is softer. And for the latter one( $(\hat y- y)*x$ ) we are actually evaluating the maximum likelihood.
