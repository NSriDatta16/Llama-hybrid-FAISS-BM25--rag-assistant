[site]: crossvalidated
[post_id]: 625615
[parent_id]: 
[tags]: 
Sorting step in Decision and Regression Trees

Several papers and implementations require the tree building algoirithms specifically the split finding to have values of individual features in sorted order. For example, the XGBoost manuscript says the following on page 5: "The most time consuming part of tree learning is to get the data into sorted order. In order to reduce the cost of sorting, we propose to store the data in in-memory units, which we called block ..." However, I am having trouble understanding why the feature values has to be in sorted order for efficiency. Let's say we have we have n different values for a given feature, e.g. $x_1, x_2, x_3, \ldots , x_n$ and we only have continuous feature values. Most implementations of the CART algorithm will first sort the feature values and then use the average of each two features to test the thresholds. However, what I am proposing and don't understand how it's less efficient is why we can't just use the following implementation where we just loop over each feature value and then split the dataset up to test the impurity measure (or gain value) of each split. I use python for simplicity: def get_best_split(self, X, Y): ''' function to find the best split ''' # dictionary to store the best split imp_dec, feat_ind, thresh = -1, -1, -1 # loop over all the features for feature_index in range(X.shape[1]): feature_values = X[:, feature_index] # loop over all the feature values present in the data for threshold in feature_values: # get current split and split array into two parts left_bool = X[:, feature_index] =1 and len(Y_right)>=1: # compute information gain (impurity decrease) N_p, N_L, N_R = len(Y), len(Y_left), len(Y_right) impurity_decrease = np.var(Y)-N_L/N_p*np.var(Y_left)-N_R/N_p*np.var(Y_right)) if impurity_decrease > imp_dec: imp_dec = impurity_decrease feat_ind = feature_index thresh = threshold return (imp_dec, feat_ind, thresh) Most implementations however would sort the feature_values array and either use those or the boundary between those (average of the two feature values in order). I am wondering how this would be more efficient for the feature values in the algorithm to be in sorted order? Thanks.
