[site]: crossvalidated
[post_id]: 70842
[parent_id]: 70839
[tags]: 
Biostatistician here. We encounter truncation most often in lab assays where values "EDL" or are "UDL" (exceed / under limit of detection), with various plays on the acronym. This is not to be confused with censoring which refers to time-to-event analyses in which the subject was known to be at risk for a certain failure but ceased to remain under observation due to drop-out, disenrolling, or administrative censoring (the end of the study). An example of truncation is that the progression of disease in HIV patients who are monitored for CD4 counts. Levels below 100 to 400 (depending on the type of assay) return a value of 100 (or 400) or less when the values meet that criteria. As a result, the histogram of uncoded results looks roughly like a skewed distribution with a point mass at the limit of detection. Treating the distribution as is is a cardinal sin, you lose considerable power and the point estimates of endpoints are nondifferentially biased. A better approach is to infer what those values could have been by using some parametric assumptions about their distribution. In a frequentist setting, one can use maximum likelihood with the EM algorithm which uses iterative weighting to estimate the parametric parameters without bias. An approximate Bayesian solution involves estimating those parameters and drawing untruncated samples, then performing inference with Gibb's sampling . These two techniques are approximately the same. Both can be implemented in statistical software using either R or BUGS. The assumption of IID in the truncated variables is important because either scenario is an exact method and, I think, has no large sample consistency which is robust against parametric assumptions. The last question doesn't make much sense to me.
