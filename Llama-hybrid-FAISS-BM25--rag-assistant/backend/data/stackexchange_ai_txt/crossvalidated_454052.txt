[site]: crossvalidated
[post_id]: 454052
[parent_id]: 453995
[tags]: 
I suspect that a major problem is the time-series nature of the data. As the Wikipedia page says, inference with linear regression assumes that "errors of the response variables [around the values predicted by the linear model] are uncorrelated with each other." That is often not the case with time series, for which the errors around the predicted values for observations nearby in time are often correlated with each other. This autocorrelation means that the independence assumptions underlying interpretation of p-values no longer hold. While that might not have affected your model without the interaction term, including an interaction term between 2 predictors that are already highly correlated might have led to this problem. Another thing to think about in models with interaction terms (even absent autocorrelation issues) is that the p-values for coefficients other than the interaction term often aren't very useful. As usually reported, they represent coefficients for a situation in which all categorical predictors are at their reference levels and continuous-valued predictors have values of 0. For example, the coefficient and p-value reported for SOT.angle is for a situation in which APmag is 0, which is far out of the range of actual APmag values. So do you really care whether the coefficient for SOT.angle is different from 0 under those circumstances? That's all that the p-value for that coefficient tells you. That doesn't explain the low p-values for the model as a whole or for the interaction term itself; those probably come from autocorrelations among error terms in your model.
