[site]: crossvalidated
[post_id]: 451181
[parent_id]: 
[tags]: 
Why is logisitic regression predicting TRUE values at a much higher rate than in the training data?

I am trying to use logistic regression to make predictions in R. I am confused as to why a model is predicting TRUE for 90% of predictions, when the training data had only a 50% probability of being TRUE . Here is some test data: set.seed(10) x1 If I model y as continuous, the mean prediction is identical to the mean value of y . For example: cont_mod mean(y) [1] 602.8305 > mean(cont_pred) [1] 602.8305 Thus, my intuition is that in a logistic regression the probability of the model predicting a TRUE value should be equal to the probability of a TRUE value in the training dataset. However, when I use logistic regression to model y as a binary outcome y_bin , the model predicts TRUE values at a very high rate. y_bin mean(y_bin) [1] 0.5 > mean(prob > 0.5) [1] 0.896 After doing some searching here and on the internet, I found some people suggesting that a logistic regression model will make such predictions when it is fit/trained on unbalanced data. But it is making these predictions even when I ensure that 50% of the values of y_bin are true! So, why does logistic regression predict 90% of outcomes as TRUE , when only 50% of the training data had a TRUE outcome? Additionally, if I should use a different value besides 0.5 as the cutoff between a TRUE and a FALSE prediction, how do I go about selecting the cutoff value?
