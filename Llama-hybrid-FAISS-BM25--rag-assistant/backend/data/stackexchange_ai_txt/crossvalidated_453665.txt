[site]: crossvalidated
[post_id]: 453665
[parent_id]: 452555
[tags]: 
This is a non-exhaustive answer. My guess would be, that a direct contact to the author(s) might be a lot better suited to discuss the issue. That being said, my take on the section about classification is that the authors try to evaluate the cost-vs-benefit of computing the second derivatives (the Hessian). They actually argue that the Hessian is close to zero, suggesting a linear model. Whether this is a general feature of the MAML, or just of a particular choice I can't judge. Not considering the particular properties of mentioned MAML, the evaluation of the second order derivatives possibly presents a quick way to train MLPs. This type of description is part of the Neural Network literature, like Bishops "Neural Networks for Pattern Recognition" from 1996. There are several papers on this as well, from Bishop, as well as from LeCun. I can't remember which one(s) exactly, but they are listed on their homepages. So in general there is no need to calculate the Hessian (or any product). It depends on the type of network and the type of learning. To my knowledge, training of classification networks benefit most from this approach. In certain cases, training can happen in a single step (e.g.: Bishop, "Exact Calculation of the Hessian Matrix for the Multi-Layer Perceptron", 1992); it might just be expensive in terms of computation.
