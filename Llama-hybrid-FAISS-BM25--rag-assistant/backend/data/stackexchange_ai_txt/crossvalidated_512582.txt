[site]: crossvalidated
[post_id]: 512582
[parent_id]: 
[tags]: 
Does it make sense theoretically to lag my dependent variable?

I am currently doing a research on the correlation between the duration of conflicts and religious organisations. For this, I am doing a multiple linear regression, with my dependent variable being the duration of conflicts in years (interval-ratio, assuming a value of y>0), and my independent variable is religion in the public discourse (dichotomous). After running the regression, I noticed that my Durbin-Watson value was very low, so I reorganised cases in chronological order, which improved it but still did not make it acceptable. Then I tried lagging the dependent variable, which worked and raised my Durbin-Watson value to 1.510. My question is, does lagging my dependent variable make sense theoretically? From what I have read and been taught in my courses this is better suited for other cases, where including cases registered as time-series results in autocorrelation. However, does it make sense when looking at the duration of conflicts in years? (The dataset I am using registers conflicts on a year-basis, so each case corresponds to one particular year, and thus prolonged conflicts are divided in multiple cases. However, I do not think that the problem which may derive from this aspect really apply to my regression.) The model equation without the lagged dependent variable is: duration_conlfict = -4.323 + 6.87 religion_in_public + 0.319 regime_type + 1.769*GDP With the lagged dependent variable: duration_conlfict = 0.264 + 0.244 religion_in_public + 0.000 regime_type + 0.117 GDP + 0.977 LAG_duration However when I tried doing all these operations again from the initial dataset, my results changed, especially in regards to the Durbin-Watson score (which is 1.919 with the LAG_duration), and the significance of my predictors (which has decreased significantly compared to the previous attempts).
