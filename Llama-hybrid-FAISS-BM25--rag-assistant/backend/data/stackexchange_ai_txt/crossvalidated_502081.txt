[site]: crossvalidated
[post_id]: 502081
[parent_id]: 
[tags]: 
Choice of Smoothing Kernel in ABC

In Approximate Bayesian Computation, one approximates an intractable likelihood by convolving it with some smoothing kernel $K$ as \begin{align} \ell^{\text{ABC}} ( x | \theta ) = \int \ell ( z | \theta ) \, K (x, z) \, dz. \end{align} Common choices for $K$ include $K(x, z) = \mathbf{I} \left[ d(x, z) \leqslant \varepsilon \right]$ and $K(x, z) = \exp \left( - \frac{d (x, z)^2 }{2 \varepsilon^2 }\right)$ , where $d$ is some sort of distance-like function, and $\varepsilon$ is some 'tolerance / threshold' parameter. My question is: in practice, what guides the choice of $K$ ? For example, which features of a problem should encourage a user to prefer a uniform kernel to a Gaussian kernel? Ignore the choice of summary statistics, threshold parameter, and so on. I am aware of the 'model error' perspective of R. Wilkinson (as in this paper ), which is very useful in some circumstances. Have alternative perspectives arisen in the interim? e.g. are there purely algorithmic reasons to prefer a specific kernel?
