[site]: datascience
[post_id]: 52043
[parent_id]: 52036
[tags]: 
Negative R2 values can be observed when using it in the context of model validation (where we have data that is withheld from the model) because in this context, SST $\ne$ SSE + SSR. That is, this constraint does not exist due to the data splitting. This is because in the context of model validation, the value of SST is solely calculated using the observations held in the test set only (it is just the observed variance of y in the test set only, multiplied by a factor of n-1 typically), whereas, the SSE is calculated using your trained models predictions (model is of course trained on a separate data set) and the actual values of y in your test set. Thus, it is entirely possible that SSE $>$ SST if your model is extremely poor at predicting the test set, forcing R2 = 1 - $\frac{SSE}{SST}$ to be negative. You can basically interpret a negative R2 as your model having a very low R2 in general. This is not too surprising to see from a random forest in particular which loves to fit the training set extremely well due to how exhaustive the algorithm is (often, random forests tend to fit training sets perfectly as you have seen) but do considerably worse on held out data (though still often good enough, depending on the context. In your case, clearly not good enough).
