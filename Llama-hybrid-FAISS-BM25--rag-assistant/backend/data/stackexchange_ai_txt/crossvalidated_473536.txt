[site]: crossvalidated
[post_id]: 473536
[parent_id]: 473262
[tags]: 
Think I figured this one out. The question of (a) how higher df reduces uncertainty is a separate question from (b) the simultaneous equations and equations/constraints being substituted. The former relates to higher df allowing you to use critical values from a less fat tailed distribution, as well as reducing the variance around estimated model parameters which vary inversely with df. The combination hence reduces the width of confidence and prediction intervals. The latter goes to how you accurately calculate the (sample) residual variance as an unbiased estimator of (true) error variance. The residual terms will be y1 - y_hat, y2 - y_hat, ..., yn - y_hat. Every additional parameter you estimate in the y_hat model, you add a simultaneous equation or constraint relating your yn variables, so you can substitute into subsequent residual terms and write more of them as functions of already decided variables. These subsequent residual terms are thus not free to vary and not independent of the earlier decided residual terms, so your average squared residual should really have a smaller number in the denominator, hence a higher MSE which turns out to be an unbiased estimator of the true error variance. True error = y - f(x) while sample residuals are y - f(x)_hat. See Bessel's correction for further details.
