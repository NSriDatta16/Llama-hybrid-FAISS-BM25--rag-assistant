[site]: crossvalidated
[post_id]: 362588
[parent_id]: 
[tags]: 
How can a network with only ReLU nodes output negative values?

I'm trying to use an api with a feedforward neural network for time series forecasting. For dense aggregate data it works fine, but for sparse data it sometimes forecasts negative values, even though my historical data has only positive values. The source code is very dense, and I might be missing a line or two, but as far as I can tell, the input layers and the hidden layers are all ReLU nodes. Assuming I am correct, how can a network with only ReLU layers lead to negative values, especially if none of the training data has negative values?
