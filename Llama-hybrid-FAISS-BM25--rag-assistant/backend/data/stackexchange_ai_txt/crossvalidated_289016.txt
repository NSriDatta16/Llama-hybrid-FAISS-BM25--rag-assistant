[site]: crossvalidated
[post_id]: 289016
[parent_id]: 
[tags]: 
How to use simulation to check the correctness of my Bayesian model?

To demonstrate the correctness of a frequentist estimator, it is common to simulate an experiment N times (with N being large), then show that 95% of the resulting N confidence intervals cover the true parameter values. What's the equivalent simulation exercise for Bayesian model? The Bayesian credible interval quantifies my belief about the parameter given this one particular dataset that I got , so it doesn't make sense to simulate N experiments for N new datasets. That's where I got stuck in my thinking. What I want to achieve specifically : I want to check whether my Stan model is implemented correctly. As recommended by the Stan manual, I generate mock data with a known DGP and fit my Stan model to it. Sometimes the resulting 95% credible interval covers the true value, sometimes not. My first reaction is to re-run this process N times and to check whether 95% of N times, my credible interval covers the true value. Is this valid?
