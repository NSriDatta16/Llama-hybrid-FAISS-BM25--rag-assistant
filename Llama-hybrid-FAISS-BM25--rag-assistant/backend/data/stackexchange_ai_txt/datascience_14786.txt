[site]: datascience
[post_id]: 14786
[parent_id]: 
[tags]: 
Why would you split your train data to compute a value on half of the data to then fill the Nan values on the other half?

I was checking a kernel written in python from the Bosch kaggle competition ( kaggle link to python kernel ) and I came across with a weird (at least to me) way to fill Nan values. The train data is split into two halves and then some kind of average is computed in one half by using the non-Nan values of a field along with the target value and then fill Nan values on the other half with these computed values. Then, when training the model after filling Nan values, the model is only trained on the half data where the Nan values have been replaced. The question is, why would you split the data into two halves to compute on one and then fill the other? Are you introducing some kind of leakage when mean values are related to the target value and that's the reason why just the half part with filled values (the one in which you haven't computed anything, just filled Nans) is used to train? Is this procedure prone to overfitting if you perform this operation on all the train data? Thanks in advanced.
