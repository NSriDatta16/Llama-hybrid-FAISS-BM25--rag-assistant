[site]: crossvalidated
[post_id]: 522545
[parent_id]: 522422
[tags]: 
Mutual information is linked to the concept of entropy of a random variable that quantifies the expected " amount of information " held in a random variable, or the amount of uncertainty about the outcome of a random variable. The key point is that it's about a random variable , such as $Weather$ or $Sky$ . A random variable consists of a finite number of events or outcomes whose occurrence is by random with a probability. For example, outcomes of $Weather$ are: $\mathcal{W} = \{\text{rainy}, \text{sunny}\}$ and outcomes of $Sky$ are: $\mathcal{S} = \{\text{cloudy}, \text{half-cloudy}, \text{clear}\}$ each with a probability. A random variable like $Weather$ has the highest entropy when all outcomes have equal probability. Then you can't predict that whether tomorrow is likely rainy or sunny, and any outcome is a surprise for you. For these two outcomes, we need $1$ bit (0 or 1) to report the weather of each day, which is the maximum entropy value in this case. However, if $P(\text{sunny})$ is much higher than $P(\text{rainy})$ , predicting tomorrow is $\text{sunny}$ has less information content compared to predicting that it's $\text{rainy}$ . So, it's not a news for you if one says tomorrow is sunny. The entropy for a variable averages the information content of all outcomes. If always is sunny, we have no entropy. We need zero bit to transfer information for every day. The mutual information quantifies the amount of information or uncertainty that remains for a random variable like $Weather$ when we know the outcome of another variable like $Sky$ . So if you know that $Sky=\text{clear}$ it affects the probabilities of $Weather$ outcomes. Then the probability of $\text{rainy}$ can decrease! It shows a type of dependence. It's a non-negative value and it's zero when two random variables are independent. It has various formulas, one is: $$I(X;Y) = H(X) - H(X | Y)= H(Y) - H(Y|X)$$ $H(X)$ is the entropy of $X$ and $H(X | Y)$ is the conditional entropy of two variables $X$ and $Y$ . When they are independent, then $H(X) = H(X|Y)$ , which means knowing about $Y$ has no effect on the entropy of $X$ . Also as the accepted answer pointed out, there is a measure called pointwise mutual information , which measures the mutual information between two single events, such as rainy weather and cloudy sky. The mutual information is the expected value of PMI among pairs of outcomes of two random variables.
