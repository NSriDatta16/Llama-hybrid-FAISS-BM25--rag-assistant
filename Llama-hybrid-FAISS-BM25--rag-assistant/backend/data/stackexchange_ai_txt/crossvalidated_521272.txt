[site]: crossvalidated
[post_id]: 521272
[parent_id]: 520288
[tags]: 
In the context of standard linear (ridge) regression, the diagonal entries of the 'hat' matrix correspond to the (ridge) leverage scores. These can be interpreted as the influence that the corresponding input point has on the prediction at the training input locations. Suppose we condition (train) on data $X\in\mathbb{R}^{n\times d}$ , $y\in\mathbb{R}^n$ with a regulariser $\lambda\geq0$ (taking care of rank considerations with $\lambda = 0$ ) and obtain weights $\beta = (X^TX + \lambda I)^{-1}X^Ty$ such that $$ \hat{y} = X\beta = X(X^TX+\lambda I)^{-1} X^T y = Py.$$ The matrix $P$ is then the 'hat' matrix you refer to. Now to see how much influence a point $X_i$ has, with a corresponding output $y_i$ , we could see how much the prediction at $X_i$ changes if we change the observation $y_i$ . This is the derivative $d\hat{y}_i/d y_i$ , and it is immediate from the above that it is equal to $[P]_{ii}$ . The above might need to be modified in the context of logistic regression, but the intuition should carry over. As to what you can do with such values, they're used throughout both theoretical and applied statistics for selecting 'informative subsets'. Their sum (the trace of $P$ ) is a quantity known as the 'effective dimension' of the regressor, and characterises the difficulty of the regression problem (roughly the number of near-orthogonal directions).
