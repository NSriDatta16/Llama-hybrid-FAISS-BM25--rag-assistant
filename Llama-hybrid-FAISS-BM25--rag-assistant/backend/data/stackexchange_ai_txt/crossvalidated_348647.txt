[site]: crossvalidated
[post_id]: 348647
[parent_id]: 348643
[tags]: 
In the blog post Reflections on Kitchen Sinks , Ali Rahimi and Ben Recht conducted an experiment to assess the effectiveness of several gradient methods and Levenbergâ€“Marquardt at solving a simple neural network: two linear fully-connected layers in a feedforward network. (The "trick" here is that the matrix they are approximating has very high condition number.) Clearly, using LM is a huge improvement over the gradient-only methods. However, it is more expensive because it requires computing a large, dense matrix and then solving a linear system to apply a single update. The only reason people don't use higher-order gradient information is that modern neural networks have so many parameters as to make this impractical. Re-computing the inverse Hessian (e.g. Newton's method) at each step for millions of parameters is going to be hard unless the matrix is very special ( diagonal being the easiest case). But excepting the computational challenge, this can be very effective. It would be a huge breakthrough in the field if someone could do both: (1) use higher order gradient information and (2) compute the update quickly. But this is really hard to do.
