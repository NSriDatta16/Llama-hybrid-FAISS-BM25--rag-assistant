[site]: crossvalidated
[post_id]: 309134
[parent_id]: 306805
[tags]: 
An alternative approach could be to use a word embedding , possibly learned by a neural network. The idea is to represent each team as a low-dimensional vector (perhaps 1 or 2 dimensions are enough). So, the design could be like this: you have two shared embeddings layers, one for teamA and one for teamB. Then you could take the difference of those two layers (here is an example in Keras) : basically this means that you are doing (teamA - teamB) in the embedded space. The next layer could simply be logistic regression. The target variable could be 1 if teamA won and 0 if team B won. I would train the network with both: two examples for each game. In fact, I wonder how your logistic regression managed to work so well. It could have just set all the $\beta$s to zero and have a large bias towards 1 (maybe regularisation saved the day?). If you do the embedding in 1D, each team would be mapped to a single number, and this should be equivalent to what you currently have. The advantage could be that here you could try to embed the teams in 2D for example. Also, you could try to concatenate the layers rather than taking the difference and, of course, add more layers. This would also be very simple to extend concatenating more features to the embeddings.
