[site]: crossvalidated
[post_id]: 144858
[parent_id]: 129937
[tags]: 
Update: Revisiting my 'youthful' answer, I agree, this stitching approach is not the right way to compute R-squared metric. Stitching may be useful for visual inspection of residuals. I leave answer as is, as it is mentioned in other answers. @Is it the averaged R squared value of the 5 models? -No, it is computed as seen below. You predict k-fold observations, stitch them together to a ordered vector where obs#1 is first and obs#last is last. Calculate then the squared pearson product moment correlation (R²) of this k-fold prediction vector to the response vector(y). CV-correlation to response(y) is lower than a direct MLR-fit. In the example below R²(CV) = .63 and R²(direct fit)=.82. This would suggests simple MLR here is slightly overfitted, and if this bothers you could try to do somewhat better with PLS, ridge-regression or PCR. I have not heard of any 2% rule. library(foreach) obs=250 vars=72 nfolds=5 #a test data set X = data.frame(replicate(vars,rnorm(obs))) true.coefs = runif(vars,-1,1) y_signal = apply(t(t(X) * true.coefs),1,sum) y_noise = rnorm(obs,sd=sd(y_signal)*0.5) y = y_signal + y_noise #split obs randomly in nfold partitions folds = split(sample(obs),1:nfolds) #run nfold loops, train, predict.. #use cbind to stich together predictions of each test set to one test.preds = foreach(i = folds,.combine=cbind) %do% { Data.train = data.frame(X=X[-i,],y=y[-i]) Data.test = data.frame(X=X[i ,],y=y[ i]) lmf = lm(y~.,Data.train) test.pred = rep(0,obs) test.pred[i] = predict(lmf,Data.test) return(test.pred) } CVpreds = apply(test.preds,1,sum) cat(nfolds,"-fold CV, pearson R^2=",cor(CVpreds,y)^2,sep="") cat("simple MLR fit, pearson R^2=",cor(lm(y~.,data.frame(y,X))$fit,y)^2,sep="")
