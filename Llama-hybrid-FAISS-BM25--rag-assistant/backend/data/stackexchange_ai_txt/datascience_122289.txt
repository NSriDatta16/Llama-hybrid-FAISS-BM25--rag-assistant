[site]: datascience
[post_id]: 122289
[parent_id]: 
[tags]: 
Consistency error in visualization of policy improvement in Sutton & Barto's book?

Sutton & Barto introduce in their foundational book on "Reinforcement Learning: An Introduction" in the context of Dynamic Programming algorithms for policy evaluation and improvement. Within an example they show a grid world environment with a probabilisitic policy, for which every action is initially equiprobable for every traversable state the agent can be in. They visualize it using four arrows pointing into each of the four cardinal directions of the grid world. Upon conducting policy evaluation and improvement over successive generations, they claim to be able to retrieve a optimal 'greedy' policy. However, their policy improvement algorithm still allows for the retrieved policy to allow in certain states multiple actions which are equiprobable (indicated by two arrows pointing into orthogonal cardinal directions). This is evident along the states on the direct diagonal between the two terminal states, as well the diagonal perpendicular to it. Which makes me wonder, whether they really did optimized for a greedy policy. The argmax in their policy improvement algorithm would likely not allow for such behavior and would automatically break the tie between any equiprobable actions. The only way I can see that this works out is either by the argmax operation sampling randomly uniform if the set of actions A* that maximize the state-action value function Q(s,a) is greater than |A*|>1, or instead that policy improvement is conducted by sampling the successor action a* in a roulette-wheel fashion based the height of the values of the state-action value function Q(s,a) for each action a. The latter seems more likely to me, as the former would also quickly result into degenerating into a normal argmax operation as soon as initial ties between actions are broken.
