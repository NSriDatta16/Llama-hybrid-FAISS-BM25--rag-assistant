[site]: crossvalidated
[post_id]: 204157
[parent_id]: 
[tags]: 
Get rid of values that pollute (or obfuscate) aggregate statistics

I consider myself a beginner in statistical analysis, and I want some advice on a problem that I have encountered in a data analysis task. I am monitoring a system's latency for producing results, when different methods are used for one of the system's components. To that end, I have several processes reporting latency every 1 sec and in the end I end up with multiple log files with the following format: # timestamp, throughput, latency 1458750383262,1115,1680 1458750384265,1118,1553 1458750385266,1460,1279 1458750386270,1317,1024 1458750387270,1705,999 1458750388270,1673,1426 1458750389270,2188,1318 1458750390273,1543,1038 1458750391273,2076,3 1458750392273,1888,3 1458750393273,1896,2 1458750394273,2934,17 1458750395273,3573,27 1458750396274,2177,1289 1458750397274,2636,269 1458750398274,3066,282 1458750399275,3699,9 1458750400276,4009,63 1458750401276,4202,92 1458750402276,4302,342 1458750403276,4488,157 1458750404276,4229,31 1458750405276,4761,4 1458750406276,4327,190 1458750407276,4280,644 1458750408276,3974,710 1458750409276,3854,7 1458750410276,3963,404 1458750411276,4550,237 1458750412276,4260,343 1458750413276,4860,247 1458750414284,4846,244 1458750415284,4131,201 1458750416284,4239,300 1458750417284,4569,415 1458750418284,4310,669 1458750419284,3353,53 1458750420285,4465,667 1458750421285,4266,23 1458750422285,3895,7 1458750423285,4092,209 1458750424285,4029,71 1458750425285,4244,3 1458750426285,4407,202 1458750427285,4825,525 1458750428286,3740,4 1458750429287,3943,4 1458750430287,4468,318 1458750431287,4381,4 1458750432287,2501,4 1458750433288,2489,2 1458750434288,2515,3 1458750435288,2501,2 1458750436289,2500,4 1458750437290,2503,4 1458750438290,2499,3 1458750439290,2499,4 1458750440290,2501,5 1458750441290,2501,4 1458750442290,2496,4 1458750443290,2503,2 1458750444290,2500,4 1458750445290,2496,2 1458750446290,2504,4 1458750447290,2499,4 1458750448290,2498,7 1458750449290,2505,2 1458750450290,2499,4 1458750451291,2502,4 1458750452291,2496,2 ... 1458750491300,2502,3 1458750492300,2499,4 1458750493300,2502,2 1458750494300,2500,2 1458750495300,2499,2 1458750496300,2500,3 1458750497300,2502,3 1458750498300,2500,2 1458750499300,2497,3 1458750500301,2501,4 1458750501302,2502,4 1458750502302,2503,4 1458750503302,2500,4 1458750504302,2499,3 1458750505302,2500,4 1458750506302,2502,4 1458750507303,2500,3 In order to make a decision on which method achieves the lowest latency, I plot the average latency (along with the standard-error bars) from every data point reported by each process. Unfortunately, as can be seen from the sample data, after a brief starting period in which the system experiences high latency values (warm up phase), the majority of latency values are between 1 and 10. Therefore, I wanted to ask which is a good statistical metric (or graph) to see the actual difference, without considering the initial data points which are polluting my results. Thanks, Nick
