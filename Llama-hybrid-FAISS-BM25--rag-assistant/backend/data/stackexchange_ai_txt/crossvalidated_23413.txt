[site]: crossvalidated
[post_id]: 23413
[parent_id]: 20010
[tags]: 
I'd say "k-fold cross validation" is the right answer from the theoretical point of view, but your question seems more about organizational and teaching stuff so I'll answer differently. When people are "still learning" it's often thought as if they're learning how to "quickly and dirtily" apply the algorithms and all the "extra" knowledge (problem motivation, dataset preparation, validation, error analysis, practical gotchas and so on) will be learned "later" when they're "more prepared". This is utterly wrong. If we want a student or whoever to understand the difference between a test set and a training set, the worst thing will be to give the two sets to two different guys as if we think that "at this stage" the "extra knowledge" is harmful. This is like waterfall approach in software development - few months of pure design, then few month of pure coding, then few months of pure testing and a pity throwaway result in the end. Learning should not go as waterfall. All parts of learning - problem motivation, algorithm, practical gotchas, result evaluation - must come together, in small steps. (Like agile approach in software development). Perhaps everyone here has gone through Andrew Ng's ml-class.org - I'd put his course as an example of a robust "agile", if you will, style of learning - the one which would never yield a question of "how to ensure that test data doesn't leak into training data". Note that I may have completely misunderstood your question, so apologies! :)
