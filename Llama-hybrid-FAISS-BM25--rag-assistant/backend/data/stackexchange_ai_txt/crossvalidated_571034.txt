[site]: crossvalidated
[post_id]: 571034
[parent_id]: 570068
[tags]: 
I have found the Kullback–Leibler divergence (a measure of statistical distance) suggested by @krkeane in the accepted answer to be an effective and computationally cheap solution. This implementation does not actually require the order statistics, and in fact it requires less computation to compute it on the unordered sample set. Pre-sorting the samples requires $O(N*log(N))$ for the total computation, but computing this measure on the unordered samples requires only $O(N*log(B))$ in total, where $B$ is the number of bins and the optimal $B \ll N$ (as I show below). $O(N*log(B))$ is due to performing a $log(B)$ binary search over the bins for each unordered point in $N$ . I've also found that $B$ need not be an odd value, as originally proposed; even values also work fine. I have performed a series of experiments to determine the optimal $B$ value for discriminating between random normal samples ( $\mathcal{N}$ ), random uniform samples ( $\mathcal{U}$ ), and a 50%-50% mixture of the two distributions ( $\mathcal{N}\mathcal{U}$ ). I performed these experiments for $N=1k$ , $N=10k$ , and $N=100k$ sample points. I averaged the results over 500 trials for each tested number of bins ( $B$ ). Somewhat unexpectedly, I found that optimal $B$ does not appear to depend on $N$ , and always seems to be around 10 or 11 (at least for discriminating $\mathcal{N}$ vs. $\mathcal{U}$ vs. $\mathcal{N}\mathcal{U}$ , with $N = [1k, 100k]$ ). I optimized $B$ against a certain measure of discrimination: $\displaystyle \operatorname*{argmax}_B min(ratio, 1-ratio)$ , where $ratio = \left.\frac{(D_\text{KL}(\mathcal{N}\mathcal{U} \parallel Q) - D_\text{KL}(\mathcal{N} \parallel Q))}{(D_\text{KL}(\mathcal{U} \parallel Q) - D_\text{KL}(\mathcal{N} \parallel Q))}\right|_{bins=B, D_\text{KL}(\mathcal{U} \parallel Q) \ge D_\text{KL}(\mathcal{N}\mathcal{U} \parallel Q) \ge D_\text{KL}(\mathcal{N} \parallel Q)}$ and $ratio = 0$ was assigned if the expected ranking of the three $D_\text{KL}$ values was violated. A $min(ratio, 1-ratio) = 0.0$ represents the worst possible discrimination between the three sample distributions, and $min(ratio, 1-ratio) = 0.5$ represents the best possible discrimination. I found that higher $N$ lowered the variance of the results and increased the maximum possible discrimination $min(ratio, 1-ratio)$ , but did not affect the optimal $B$ at all. Please see the charts below for a summary of the results. Having optimal $B \approx 10$ means that this measure can be computed very efficiently over an unordered sample set for a huge range of $N$ , which was precisely the objective. A typical result with $N=100k$ and $B=10\\ D_\text{KL}(\mathcal{N} \parallel Q) = 0.000036\\ D_\text{KL}(\mathcal{N}\mathcal{U} \parallel Q) = 0.015816\\ D_\text{KL}(\mathcal{U} \parallel Q) = 0.028814$ UPDATE After further testing, I have learned that the optimal $B$ actually depends on exactly how the two differing distributions overlap. For example: $\mathcal{N}(\mu=0,\,\sigma^{2}=1), \mathcal{U}[-1, 1]$ : gives optimal $B=10$ (and the charts given above). $\mathcal{N}(\mu=0,\,\sigma^{2}=1), \mathcal{U}[-2, 2]$ : gives optimal $B=36$ (and a skewed appearance to the charts). However, in all cases tested so far, the $B$ "sweet spot" has been between 10 and ≈40, and does not appear to depend on $N$ . And $B=10$ still performs well, across the board, even if it is not always the exact optimal value for the two given differing distributions.
