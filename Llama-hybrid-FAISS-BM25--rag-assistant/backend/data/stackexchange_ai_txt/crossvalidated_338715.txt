[site]: crossvalidated
[post_id]: 338715
[parent_id]: 4961
[tags]: 
In simple term, Regularization is a technique to avoid over-fitting when training machine learning algorithms. If you have an algorithm with enough free parameters you can interpolate with great detail your sample, but examples coming outside the sample might not follow this detail interpolation as it just captured noise or random irregularities in the sample instead of the true trend. Over-fitting is avoided by limiting the absolute value of the parameters in the model.This can be done by adding a term to the cost function that imposes a penalty based on the magnitude of the model parameters. If the magnitude is measured in L1 norm this is called "L1 regularization" (and usually results in sparse models), if it is measured in L2 norm this is called "L2 regularization", and so on.
