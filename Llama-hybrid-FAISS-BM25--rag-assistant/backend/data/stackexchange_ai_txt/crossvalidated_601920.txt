[site]: crossvalidated
[post_id]: 601920
[parent_id]: 600807
[tags]: 
This is a great question (I had the same question but you asking it made me experiment a bit). The answer is yes, it changes based on the context. You should not extract the embeddings and re-use them (at least for most of the problems). I'm checking the embedding for word bank in two cases: (1) when it comes separately and (2) when it comes with a context (river bank). The embeddings that I'm getting are different from each other (they have a cosine distance of ~0.4). from transformers import TFBertModel, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') model = TFBertModel.from_pretrained('bert-base-uncased') print('bank is the second word in tokenization (index=1):', tokenizer.decode([i for i in tokenizer.encode('bank')])) print('bank is the third word in tokenization (index=2):', tokenizer.decode([i for i in tokenizer.encode('river bank')])) ###output: bank is the second word in tokenization (index=1): [CLS] bank [SEP] ###output: bank is the third word in tokenization (index=2): [CLS] river bank [SEP] bank_bank = model(tf.constant(tokenizer.encode('bank'))[None,:])[0][0,1,:] #use the index based on the tokenizer output above river_bank_bank = model(tf.constant(tokenizer.encode('river bank'))[None,:])[0][0,2,:] #use the index based on the tokenizer output above are_equal = np.allclose(bank_bank, river_bank_bank) print(are_equal) ### output: False
