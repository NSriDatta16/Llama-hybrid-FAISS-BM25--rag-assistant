[site]: crossvalidated
[post_id]: 121939
[parent_id]: 21551
[tags]: 
Here is some discuss of coursera forum thread about confusion matrix and multi-class precision/recall measurement. The basic idea is to compute all precision and recall of all the classes, then average them to get a single real number measurement. Confusion matrix make it easy to compute precision and recall of a class. Below is some basic explain about confusion matrix, copied from that thread: A confusion matrix is a way of classifying true positives, true negatives, false positives, and false negatives, when there are more than 2 classes. It's used for computing the precision and recall and hence f1-score for multi class problems. The actual values are represented by columns. The predicted values are represented by rows. Examples: 10 training examples that are actually 8, are classified (predicted) incorrectly as 5 13 training examples that are actually 4, are classified incorrectly as 9 Confusion Matrix cm = 0 1 2 3 4 5 6 7 8 9 10 1 298 2 1 0 1 1 3 1 1 0 2 0 293 7 4 1 0 5 2 0 0 3 1 3 263 0 8 0 0 3 0 2 4 1 5 0 261 4 0 3 2 0 1 5 0 0 10 0 254 3 0 10 2 1 6 0 4 1 1 4 300 0 1 0 0 7 1 3 2 0 0 0 264 0 7 1 8 3 5 3 1 7 1 0 289 1 0 9 0 1 3 13 1 0 11 1 289 0 10 0 6 0 1 6 1 2 1 4 304 For class x: True positive: diagonal position, cm(x, x). False positive: sum of column x (without main diagonal), sum(cm(:, x))-cm(x, x). False negative: sum of row x (without main diagonal), sum(cm(x, :), 2)-cm(x, x). You can compute precision, recall and F1 score following course formula. Averaging over all classes (with or without weighting) gives values for the entire model.
