[site]: datascience
[post_id]: 113184
[parent_id]: 113181
[tags]: 
What you are describing is respectively: a normal (causal) language model (e.g. GPT-2), which computes the probability of a token based on the previous tokens. a masked language model (e.g. BERT), which computes the probability of a masked token in the middle of the sentence. Beware, however, that modern language models don't tend to use words as tokens. Instead, they use subword tokens, where a single word can be composed of multiple tokens. Therefore, you would probably need to train your own word-level causal and masked language models.
