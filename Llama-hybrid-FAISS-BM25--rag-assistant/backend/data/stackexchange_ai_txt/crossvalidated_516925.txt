[site]: crossvalidated
[post_id]: 516925
[parent_id]: 
[tags]: 
How to tell if a model is overfitting or underfitting or the problem is something entirely different

I'm a complete beginner and I'm trying to do a multi-label classification on the well known dataset ChestX-ray14, which contains about 112 thousand x-ray images from about 31 thousand patients, the images are either normal or can belong to one or more of the 14 categories. Since more than half of the images are "normal", I've removed those for simplicity's sake, leaving about 52 thousand instances to work with. The authors of the dataset provides official train_val and test subsets which I'm using, and I've split the train_val set to training and validation sets with 4:1 ratio, in a stratified manner by labels, although the original splits were patient-wise. Training set has around 28.8k, validation set has around 7.2k and the test set has 15.7k instances. Below is the model I'm using: def create_resnet50(): base_resnet50 = ResNet50(input_shape = t_x.shape[1:], include_top = False, weights = None) model = Sequential() model.add(base_resnet50) model.add(GlobalAveragePooling2D()) model.add(Dense(len(labels), activation = 'sigmoid')) return model resnet50 = create_resnet50() resnet50.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy', 'mae']) The original images are 1024 x 1024, but I'm using 224 x 224 size. I'm also using the following ImageDataGenerator to augment training, validation and test splits: idg = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True, horizontal_flip = True, vertical_flip = False, height_shift_range= 0.05, width_shift_range=0.1, rotation_range = 5, fill_mode = 'constant', cval = 0, zoom_range=0.15) And a training set iterator: train_iter = idg.flow_from_dataframe(dataframe=train_df, directory = IMG_DIR, x_col = 'Image Name', y_col = 'Finding Labels', class_mode = 'categorical', classes = labels, color_mode = 'grayscale', target_size = IMG_SIZE, batch_size = 32) Below is the learning curve I'm getting after training, the training loss is not only much higher than the validation loss, it's even on a different scale.. And the following image shows the ROC curve is what I get from the predictions from the test set, which is unsatisfactory to say the least. From these images I can't tell whether the model is overfitting, underfitting or the problem is something else.
