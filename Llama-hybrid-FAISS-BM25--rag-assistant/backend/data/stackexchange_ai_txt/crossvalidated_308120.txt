[site]: crossvalidated
[post_id]: 308120
[parent_id]: 
[tags]: 
Is mean or median a better way to find central tendency of a season of television?

What is better to use for calculating the score of a season of television: the mean or the median? I will say from the outset that my question may be somewhat subjective. That being said, my question is also based on calculating central tendency so I posted it here. A little bit of background: I enjoy watching episodes of television and then rating them on a scale from 0.1 to 10 (nothing is allowed to be a 0). After I see an entire season of television my past methodology has been to add up all of the episode ratings in the season and then divide by the number of episodes. Essentially I come up with a mean score for the season. After I came up with the mean score, I then come up with what I call a "satisfaction rating". This rating is determined by my overall feelings at the end of the season and how I feel about it looking back on it. (The reason I don't just use the satisfaction rating is that the mean score serves to remind me how I felt, on average, during the entire season. Sometimes, even looking back on my notes, it can be difficult to remember how I felt on average so I still calculate the mean rating because it serves as a reminder to how I felt, on average, episode by episode.) With the mean score calculated and the satisfaction rating determined, I then find the mean of both of those scores. For example: if I calculated the mean of a season to be 7.5 but my satisfaction rating was a 7.3 then the final score would be a 7.4. This was working well for me until I realized that maybe the mean wasn't the best way to calculate the central tendency. As the Laerd Statistics site puts it: Another time when we usually prefer the median over the mean (or mode) is when our data is skewed (i.e., the frequency distribution for our data is skewed). There could be said to be some amount of skewed distribution if some of the episodes got much higher scores for others. For instance, say there are ten episodes in a season. Six of the episodes received a rating ranging from 7 to 7.8. However, three episodes received a rating ranging from 1 to 3 and the last two episodes received a rating ranging from 9 to 10. Here is a sample dataset based on these parameters (in order of how large the value is, not the chronological progression of episodes): 1.1, 2.1, 2.8, 7.3, 7.4, 7.5, 7.5, 7.6, 7.7, 9.2, 9.7 The mean of these numbers would be (rounded to the nearest tenth): 6.4 The median of these numbers would be: 7.5 The difference between these scores is quite different! Sure, for finding the most central value we would be better off using the median. But would the mean perhaps be better for evaluating the average experience of watching episode to episode? The Laerd website says about the mean: An important property of the mean is that it includes every value in your data set as part of the calculation. It would be similar to how grades are calculated. When a teacher wants to find your overall grade, they don't find the median score of all of your assignments- they add up all of the scores and then divide them by the number of assignments. A good grade can help outweigh the bad, just as a bad can help outweigh the good. So here is my conundrum: should I use the median, which is more emblematic of the average experience of watching an episode? Or should I use a mean which measures the average experience by allowing for a great episode to make up for mediocre ones and a bad episode to sour decent ones?
