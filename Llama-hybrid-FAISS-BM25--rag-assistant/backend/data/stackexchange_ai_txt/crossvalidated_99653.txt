[site]: crossvalidated
[post_id]: 99653
[parent_id]: 97938
[tags]: 
Having a probability distribution where $Q_i=0$ for any $i$ means that you are certain that $Q_i$ can not occur. Therefore if a $Q_i$ were ever obeserved it would represent infinite surprise/information, which is what Shannon information represents. KL diveregence represents the amount of additional surprise (ie information lost) per observation if the distribution $Q$ is used as an approximation for the distribution $P$. If the approximation predicts 0 probability for an event that has a postive probability in reality, then you will experience infinite surprise some percentage of the time and thus infinite surprise on average. The solution is to never allow 0 or 1 probabilities in estimated distributions. This is usually achieved by some form of smoothing like Good-Turing smoothing, Dirichlet smoothing or Laplace smoothing.
