[site]: crossvalidated
[post_id]: 621450
[parent_id]: 230120
[tags]: 
Franck's answer is completely correct. If you'd pay more attention to his message you'd realize you're stating the same thing as he is, just you're omitting one important aspect. Firstly, an epoch does indeed refer to a complete pass (forward & backward) of all the training examples . This should not be a debatable topic. Secondly, when using mini-batch gradient descent, the parameters are indeed updated at each pass . Franck and you, both stated the same thing, just differently. However, I believe your confusion originates from the fact that, when using mini-batch gradient descent (or stochastic gradient descent), multiple iterations are required to complete an epoch. Aspect clearly outlined by Franck. For clarity, if there are n training samples, there are n / ( # of mini-batches ) iterations required for mini-batch GD to complete an epoch, n iterations for SGD, and evidently 1 for batch GD. I believe that, in general, this is a confusing topic and it is important for others to have a clear understanding of the meaning of a batch, mini-batch, epoch, pass, and iteration. Thus, confusing messages like yours should be highlighted and corrected.
