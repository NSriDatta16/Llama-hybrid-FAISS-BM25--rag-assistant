[site]: crossvalidated
[post_id]: 69604
[parent_id]: 64075
[tags]: 
IMO, averaging 20 runs and then comparing the averages of each program is a fine start. You might also want to record some measure of dispersion for each set of runs; a range, standard deviation, etc. What's important is to ensure that you're dealing with a clean benchmarking environment. I.e., that each run of the program is performed over as similar a background state as possible. This is a trickier issue, and is difficult to achieve perfectly on most computers. This blog post explaining the criterion library might be a worthwhile read.
