[site]: crossvalidated
[post_id]: 497562
[parent_id]: 
[tags]: 
Information gain for unequally sized time series

I have some time series data, $\mathbf{x}_{1:T} = \{ x_1, \dots, x_T \}$ where the observation at time $t$ , $X_t$ , is a continuous random variable. Let $Y_t$ denote a discrete random variable at time $t$ that, conditioned on the previous $t$ observations, has support over $t$ values. (It is an estimate for each of the previous time points.) Is this mutual information well-defined? $$ \text{MI}(Y_t, X_t) = \mathbb{H}[p(\color{red}{Y_{t-1}} \mid \mathbf{x}_{1:t-1})] - \mathbb{E}_{X_t}[p(Y_{t} \mid \mathbf{x}_{1:t-1}, X_t = x_t)]. $$ In words, I want to know how much information I gain about $Y_t$ by observing $X_t$ . I ask for two reasons: The maximum entropy of a discrete distribution with support over $N$ points is a uniform distribution, so $- \sum_{n=1}^N p(x_n) \log p(x_n) = - \log 1/N = \log N$ . The LHS of my equation has support over $N-1$ points; the RHS has support over $N$ points. It seems like these are on the different "scales", but I'm not sure how to handle this. When I approximate $\text{MI}(X_t, Y_t)$ using some code, I get a slightly different answer (always slightly larger). And sometimes the value for $\text{MI}(Y_t, X_t)$ is negative, and I know that MI is non-negative.
