[site]: datascience
[post_id]: 86417
[parent_id]: 86327
[tags]: 
Inside SimpleRNN the input (of dimensionality 256) is projected onto a representation space of dimensionality 128, by means of a matrix multiplication. The RNN operations are with these vectors of size 128. If you take a look at the source code of SimpleRNN , you can see that the projection matrix is is stored in a member variable called kernel . You can see how in method SimpleRNNCell.call one of the first things is to project the inputs with K.dot(inputs, self.kernel) . P.D.: To me, the "neurons" analogy has always been misleading. I like to think about neural network in terms of differentiable matrix operations: matrix multiplication, matrix addition, position-wise transformations like sigmoid, hyperbolic tangent, ReLU, etc. This makes it easier to reason about the dimensionality of the input and output of each computation step.
