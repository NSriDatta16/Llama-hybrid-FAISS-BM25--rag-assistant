[site]: crossvalidated
[post_id]: 579964
[parent_id]: 
[tags]: 
What is the exact role of model $p_\theta$ in Diffusion models for the reverse process?

I'm reading this interesting blog post explaining Diffusion probabilistic models and trying to understand the following. In order to compute the reverse process , we need to consider the posterior distribution $q(\textbf{x}_{t-1} | \textbf{x}_t)$ which is said to be intractable ' because it needs to use the entire dataset and therefore we need to learn a model $p_\theta$ to approximate these conditional probabilities in order to run the reverse diffusion process' . If we use Bayes theorem we have $$q(\textbf{x}_{t-1} | \textbf{x}_t) = \frac{q(\textbf{x}_t |\textbf{x}_{t-1})q(\textbf{x}_{t-1})}{q(\textbf{x}_t)}$$ I understand that indeed we don't have any prior knowledge of $q(\textbf{x}_{t-1})$ or $q(\textbf{x}_t)$ since this would mean already having the distribution we are trying to estimate. Is this correct? The above posterior becomes tractable when conditioned on $\textbf{x}_0$ and we obtain $$q(\textbf{x}_{t-1} | \textbf{x}_t , \textbf{x}_0) = \mathcal{N}(\tilde{\bf{\mu}}(\textbf{x}_t , \textbf{x}_0) \, , \, \tilde{\beta}_t \textbf{I})$$ So apparently we obtain a posterior that can be calculated in closed form when we condition on the original data $\textbf{x}_0$ . At this point, I don't understand the role of the model $p_\theta$ : why do we need to tune the parameters of a model if we can already obtain our posterior?
