[site]: crossvalidated
[post_id]: 191931
[parent_id]: 191767
[tags]: 
In general, to boil down a forest to a single tree works really well for low noise step function shaped data structures... There will be a cost of increased model bias and/or variance for most practical problems, that led you to train a forest model in the first place. If you're lucky you end up with one or a few trees, that are adequately small/few to comprehend and adequately fit the data right. But many times the gap just will not be bridged. Here's an example: Learning accurate and interpretable models based on regularized random forests regression Not sure if your seeding trick will work. Must confess I don't get it entirely :) But prove the world wrong by posting a prototype! Maybe you could write a randomForest-wrapper controlling seedings. Anyways keep in mind the decision trees and the forest are just a representation of a model structure. Well yes, that specific representation matching how the model was built. *But, that does not mean, that trees are the best representation to convey the overall model structure. You could try invent an entirely new representation both true to model structure and easy to comprehend. To think out of the black-box!... well maybe :) You can see your trained model structure as a mapping function connecting your feature space with your target. In the simple case of regression, the target space is simply a 1D numeric scale. This regression mapping function has a geometrical shape and can be visualized, with partial dependence plots as iceBOX , rMiner , randomForest::partialPlot . Sry for only mentioning R packages (what is used in python?) . I wrote forestFloor also covering probabilistic classification, latent interaction dectections and quantification of how well a given low-dimensional 2D/3D visualization represents the true high-dimensional model structure.
