[site]: datascience
[post_id]: 22783
[parent_id]: 
[tags]: 
Why is a perceptron initialized with a random line?

I'm setting up a single perceptron for doing linear classification. Why is the perceptron initialized with random weights and a random bias instead of just having all of the weights set to zero initially? Wouldn't the loss optimization happening in the back propagation work more effectively on average if these values were all set to zero, as (I think, intuitively) this would be the average of all values that these parameters could take, so starting there is essentially like starting at the "middle" of the space of all perceptron solutions? Why is random a better choice than zero?
