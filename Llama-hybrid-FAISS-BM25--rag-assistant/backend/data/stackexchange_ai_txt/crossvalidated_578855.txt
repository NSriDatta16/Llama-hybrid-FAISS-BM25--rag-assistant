[site]: crossvalidated
[post_id]: 578855
[parent_id]: 578849
[tags]: 
If you had oversampled one class randomly (e.g. if instead of 10% of examples in some class you had picked 50% from that class), this would be easy to deal with. You'd simply have to adjust metrics accordingly. E.g. let's say you oversampled class 1, you want some confusion matrix based metrics, and the numbers in a confusion matrix are like this: True class Predicted class 1 Predicted class 2 Totals 1 90 10 100 (50%) 2 10 90 100 (50%) Then, you'd create a modified confusion matrix like this: True class Predicted class 1 Predicted class 2 Totals 1 9.99... 1.11... 11.11... (10%) 2 10 90 100 (90%) And then you calculate metrics based on this. Of course, you could also do this through some appropriate models (e.g. for accuracy a logistic regression with an offset). Obviously, this would mess with any confidence intervals you'd want to create so that would be harder to get. However, you did not sample randomly and it's hard to see any realistic way you could adjust for your sampling method due to its rather ad-hoc nature. Sampling methods that can be described in a model (e.g. random sampling), you can deal with through some modeling to get metrics. Active learning may or may not be perfectly fine to do, but you should only do it on your training data, not on your evaluation data. Ideally, you'd not even want to oversample on your evaluation data, but at least if you do, you could correct for it (some metrics, as you say, will be heavily skewed by oversampling some class).
