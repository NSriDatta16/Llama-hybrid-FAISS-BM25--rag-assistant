[site]: crossvalidated
[post_id]: 140624
[parent_id]: 
[tags]: 
How do you assess data validity when sample size varies over time

Imagine that you have a customer feedback system where after a process people can press one of 4 buttons: (very happy, happy, sad, very sad) and an aggregation system reports how many people per hour pressed each button during each day. If the experience being measured can change during the day, and the number of people passing through it changes significantly during the day, how can you decide at what minimum hourly sample size to reject the hourly sample as unrepresentative - and how can a confidence level be expressed? For example 3 people per hour would not be likely to yield a meaningful result; 50 people per hour may, 1000 people per hour very likely - but what's the maths here, given that the actual "experience" - eg a wait time for service - will vary during the day as well, so the average answer is not fixed?
