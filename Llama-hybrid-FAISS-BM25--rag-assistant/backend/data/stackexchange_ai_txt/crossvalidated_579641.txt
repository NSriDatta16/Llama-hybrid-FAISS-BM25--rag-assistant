[site]: crossvalidated
[post_id]: 579641
[parent_id]: 
[tags]: 
Why does latent dirichlet allocation (LDA) fail when dealing with large and heavy-tailed vocabularies?

I'm reading the 2019 paper Topic Modeling in Embedding Spaces which claims that the embedded topic model improves on these limitations of LDA. But why does LDA have these limitationsâ€”why does it fail on large vocabs and on heavy-tailed vocabs?
