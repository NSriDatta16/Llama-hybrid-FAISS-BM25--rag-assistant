[site]: datascience
[post_id]: 57093
[parent_id]: 
[tags]: 
30% accuracy for training set, 80% for test set with a 0.3 split

I have a time series dataset on which I am training. For some reason, the training accuracy is 30% while the test accuracy is about 88% after about 10 epochs. Is this at all normal? I should point out that the loss is decreasing for both the training and test data set. [EDIT] It's changing. The data does have quite an amount of padding. I'm using LSTM. The padding layers are [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] and the padding target is [0,0,0]. The problem is the time series has between 30 and 170 steps with the median being about 40 steps. I have no easy way of reducing the timesteps. Also of importance, I don't have any Dropout layers. So this doesn't seem to be the explanation. I'd go with the padding though. [UPDATE] Seems OK. I've made a custom callback checking the window that I am interested in. accuracy for that is decent.
