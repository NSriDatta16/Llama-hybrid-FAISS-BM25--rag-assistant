[site]: crossvalidated
[post_id]: 392492
[parent_id]: 120068
[tags]: 
Posting more fun information for posterity. There is an older post that discusses a similar problem regarding the use of count data as an independent variable for logistic regressions. Here it is: Does using count data as independent variable violate any of GLM assumptions? As Glen mentioned if you are simply trying to predict a dichotomous outcome it is possible that you may be able to use the untransformed count data as a direct component of your logistic regression model. However, a note of caution: When an independent variable (IV) is both poisson distributed AND ranges over many orders of magnitude using the raw values may result in highly influential points, which in turn can bias your model. If this is the case it may be useful to perform a transformation to your IV's to obtain a more robust model. Transformations such as the square root, or log can augment the relation between the IV and the odds ratio. For example, if changes in X by three entire orders of magnitude (away from the median X value) corresponded with a mere 0.1 change in the probability of Y occuring (away from 0.5), then it's pretty safe to assume that any model discrepancies will lead to significant bias due to the extreme leverage from outlier X values. To further illustrate, imagine we wanted to use the Scoville rating of various chili peppers ( domain[X] = {0, 3.2 million} ) to predict the probability that a person classifies the pepper as "uncomfortably spicy" ( range[Y] = {1 = yes, 0 = no}) after eating a pepper of corresponding rating X. https://en.wikipedia.org/wiki/Scoville_scale If you look at the chart of scoville ratings you can see that a log transform of the raw Scoville ratings would give you a closer approximation to the subjective (1-10) ratings of each chili. So in this case, if we wanted make a more robust model that captures the true relation between raw Scoville ratings and subjective heat rating, we could perform a logarithmic transformation on X values. By doing this we reduce the impact of the excessively large X domain, by effectively "shrinking" the distance between values that differ by orders of magnitude, and consequently reducing the weight any X outliers (e.g. those capsaicin intolerant and/or crazy spice fiends!!!) have on our predictions. Hope this adds some fun context!
