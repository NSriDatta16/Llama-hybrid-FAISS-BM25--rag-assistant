[site]: datascience
[post_id]: 62881
[parent_id]: 
[tags]: 
Why activation functions used in neural networks generally have limited range?

Why do we generally use activation functions with only limited range in neural networks? for e.g. $sigmoid$ activation function has range $[0, 1]$ $tanh$ activation function has range $[-1, 1]$ Q1) Suppose I use some other non-linear activation function like $f(x)=x^2$ , that don't have any such limited range then what can be potential problems in training such a neural network? Q2) Suppose I use $f(x)=x^2$ as activation function in neural network and I am Normalizing the layers (to avoid values keep multiplying to higher values) then would such a Neural Network work? (This is again in reference to the question I posted in heading that "Why do we generally use activation functions with only limited range in neural networks?")
