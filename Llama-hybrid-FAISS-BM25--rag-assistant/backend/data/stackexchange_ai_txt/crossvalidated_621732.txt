[site]: crossvalidated
[post_id]: 621732
[parent_id]: 
[tags]: 
Why do we need $a:\mathcal{X} \to \mathbb{R}$ to be positive here?

This is exercise 6.1 from the book Foundations of Machine Learning: Let $K: \mathcal{X}\times \mathcal{X} \to \mathbb{R}$ be a PDS kernel, and let $a: \mathcal{X}\to \mathbb{R}$ be a positive function. Show that the kernel $K'$ defined for all $x, y \in \mathcal{X}$ by $K'(x,y)=\frac{K(x,y)}{a(x)a(y)}$ is a PDS kernel. I was able to show this by considering a feature map $\Phi$ associated to $K$ and noticing that for $\mathbf{c}\in \mathbb{R}^m$ and $\{x_1,\dots,x_m\} \subseteq \mathcal{X}$ we have $$\mathbf{c}^T\mathbf{K'}\mathbf{c}=\sum_{i,j}\frac{c_ic_jK(x_i,x_j)}{a(x_i)a(x_j)} =\left\|\sum_{i=1}^m \frac{c_i \Phi(x_i)}{a(x_i)}\right\|_{\mathbb{H}}^2 \geq 0$$ which along with symmetry proves that $K'$ is a PDS kernel. However, nowhere in this proof did I use the fact that $a$ is a positive function (only non-zero for the definition of $K'$ to make sense). Is my solution wrong or can the assumption be relaxed from positive to non-zero? EDIT: A kernel function $K: \mathcal{X}\times \mathcal{X} \to \mathbb{R}$ is called PDS if for every $m \in \mathbb{N}$ and $\{x_1,\dots, x_m\} \subseteq \mathcal{X}$ the matrix $\mathbf{K}=[K(x_i, x_j)]_{i,j \in [m]}$ is symmetric positive semi-definite
