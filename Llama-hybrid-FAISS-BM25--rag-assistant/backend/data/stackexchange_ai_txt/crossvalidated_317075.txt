[site]: crossvalidated
[post_id]: 317075
[parent_id]: 
[tags]: 
Keras autoencoder example â€“ why ReLU?

I'm toying around with autoencoders and tried the tutorial from the Keras blog (first section "Let's build the simplest possible autoencoder" only). For the encoding layer they use ReLus, while sigmoids are used for the decoding layer. I wondered why that is and changed encoded = Dense(encoding_dim, activation='relu')(input_img) to encoded = Dense(encoding_dim, activation='sigmoid')(input_img) Instead of a val_loss of I'm trying to understand why that is but can't find any information in the post. Does anyone know why changing the activation layer from relu to sigmoid has such a negative effect in this case?
