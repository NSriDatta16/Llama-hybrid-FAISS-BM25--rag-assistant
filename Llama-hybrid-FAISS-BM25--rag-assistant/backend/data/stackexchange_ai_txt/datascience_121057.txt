[site]: datascience
[post_id]: 121057
[parent_id]: 121003
[tags]: 
One example of real-world imbalanced data is credit card fraud . Here is code showing empirically better performance for SMOTE: import imblearn from imblearn.pipeline import make_pipeline import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import balanced_accuracy_score from sklearn.model_selection import train_test_split # Load data and split data = pd.read_csv("creditcard.csv", header=1).values X, y = data[:, :-1], data[:, -1] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) # Without SMOTE lr = LogisticRegression(solver='liblinear', class_weight=None) lr.fit(X_train, y_train) y_pred = lr.predict(X_test) print(balanced_accuracy_score(y_test, y_pred)) The balanced accuracy for non-SMOTE is ~ 0.774. # With SMOTE pipe = make_pipeline(imblearn.over_sampling.SMOTE(), LogisticRegression(solver='liblinear', class_weight=None)) pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) print(balanced_accuracy_score(y_test, y_pred)) The balanced accuracy for SMOTE is ~0.935. Addendum : It is often possible to maximize regular accuracy for an imbalanced dataset by always predicting the majority class (not fitting a machine learning model or resampling). import numpy as np from sklearn.metrics import accuracy_score # Always predicting majority class y_pred = np.zeros(len(y_test)) print(accuracy_score(y_test, y_pred)) The regular accuracy for always predicting the majority class is ~0.998.
