[site]: crossvalidated
[post_id]: 300192
[parent_id]: 300105
[tags]: 
If you duplicate any variable (of m variables) in multivariate data you do not expand the intrinsic (real) dimensionality which remains and is equal to the number of positive eigenvalues of the covariance matrix. By duplicating a variable you simply are adding one zero eigenvalue; you are also inflating positive m eigenvalues because you are adding variability to data by adding more values. If you duplicated all the m variables equal number of times k you expanded the size of the data cloud (i.e. its dispersion about the centroid) without altering the shape of the data cloud: it still occupies the m subspace but it is bigger now: euclidean distances got longer, but proportionally - there was no bias introduced in the distance matrix (e.g., with single variable values, say, -1 and 1 the distance between the 2 points is 2 ; if you take that variable twice the distance is sqrt(8)=2.83 ). K-means done on original m and on new k*m variables will always yield same result if the same points are used as initial cluster centres. In fact, K-means works only within the real dimensionality where all data lie in, and so if initial centres do not depart from that subspace then centres being updated by the data will never leave that (sub)space. And because no bias with distances occured you arrive at the same clusters. If you prefer to do K-means on PCA scores of all the m componets extracted from the k*m variables, results will still not change (provided you use still same points for initial centers). The following scatterplot shows distances within some data; X axis is distances in original data and Y axis is data with each variable duplicated. You can see that "nothing changed" except proportional increase. If you duplicated only some of the m variables you are still in m intrinsic dimensionality, as before; however, the data cloud got expanded distortedly so euclidean distances increased not all proportionally. Some points (pairs) might have become relatively closer, some - relatively farther. That can influence the results of clustering, such as k-means. Irrespective whether you do the clustering based on all the many varables or on only the all m data's pr. components. You introduce more bias to distances if you duplicate just few variables many times. For example, here is two variables: id v1 v2 1 1.00 .50 2 .50 .50 3 -1.00 -.50 4 -.50 .00 Eigenvalues of the cov. matrix and Euclidean distances: 1.045900805 .016599195 .000000000 .500000000 2.236067978 1.581138830 .500000000 .000000000 1.802775638 1.118033989 2.236067978 1.802775638 .000000000 .707106781 1.581138830 1.118033989 .707106781 .000000000 Add 6 copies of v1 (but not v2) to the data. Now, 6.042387457 .020112543 .000000000 .000000000 .000000000 .000000000 .000000000 .000000000 .000000000 1.322875656 5.385164807 4.000000000 1.322875656 .000000000 4.092676386 2.692582404 5.385164807 4.092676386 .000000000 1.414213562 4.000000000 2.692582404 1.414213562 .000000000 Observe bias in distances. In old data, d(1,2)=.500 and d(3,4)=.707 , so .707/.500 = 1.41 . In new data, d(1,2)=1.323 and d(3,4)=1.414 , so 1.414/1.323 = 1.07 . Points 3-4 have become relatively closer to each other and are now almost as close as points 1-2, while before the duplicating they were farther apart than 1-2. Duplicating some but not all of variables amounts to their (their importance) differential weighting. As it was explained, that of course tells upon distances differentially. The following scatterplot shows distances within some data; X axis is distances in original data and Y axis is data with some variables duplicated: some thrice, some twice (and some not duplicated). The shape is dispersed above the diagonal. The majority of clustering algorithms will be affected by such altered distances. PCA scores (and clustering base on them) will be affected too, sure, despite that the intrinsic dimensionality doesn't change.
