[site]: datascience
[post_id]: 86360
[parent_id]: 28542
[tags]: 
To Q1: I think the answer is yes, but the performance is not guaranteed to generalize to new unseen data, if the data is not sampled from the same underlying distribution. Or if a lot of information has "leaked" through model selection, configuration, early stopping etc.. (I saw a synthetic example recently where the information totally "broke out", in a case of feature selection.) Generalization is closely related to overfitting. As long as you will not apply the trained model to the training data in the future, "overfitting" to the training data should be of no concern, as long as you get good generalization to the validation data. Good generalization implies the overfitting is not severe. Good generalization is the big goal. For Q2, I think it is in principle most correct to start from scratch, to avoid any bias toward local minima that overfit to a subset. I don't think the initial weights have to be the same for every training run, though. I just reinitialize them to random values. The training process is noisy anyway. For Q3: I have the same question. First: Adding a grid dimension is very expensive, as it multiplies the number of training runs by the number of points along the new dimension. Random fluctuations between runs matter more the more runs you are comparing too. Adding an epochs dimension also seems unnecessary, as the validation loss in each "fold" of the CV can inform early stopping. The problem is, as you state, that the final training has no validation loss. I guess one candidate solution (as you outline) is to log the number of epochs (before early stopping) for all the folds, and fix the number of epochs to the average multiplied by (N-1)/N, to let the fresh model train for just as many steps as the folds did on average. Round up or down? Down protects against overfitting, possibly at the cost of accepting some underfitting. Maybe instead multiply each fold's number of epochs with (N-1)/N first, round them and then calculate the median (could be halfway between numbers) or the mode. Or model the rounded values as a sample from some discrete probability distribution and obtain the mode of that distribution. Starting to reek overkill for something which nevertheless does not guarantee success.
