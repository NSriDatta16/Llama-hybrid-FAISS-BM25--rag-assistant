[site]: datascience
[post_id]: 16569
[parent_id]: 
[tags]: 
Modeling pixel intensity with the normal distribution

I was reading a machine learning book with applications in computer vision. In it, it mentioned that "in vision, it is common to ignore the fact that the intensity of a pixel is quantized and model it with continuous normal distribution." So my question is why do we choose to do that?
