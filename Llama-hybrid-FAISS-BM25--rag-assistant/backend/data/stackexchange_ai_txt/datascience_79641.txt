[site]: datascience
[post_id]: 79641
[parent_id]: 79607
[tags]: 
As I mentioned in my comment this is a likely a result from initializing your network's weights to the same values. This is why it's important to use random weights as it breaks the symmetry. See also this post or this more in-depth deeplearning.ai post on weights initialization methods for neural networks.
