[site]: crossvalidated
[post_id]: 484007
[parent_id]: 483999
[tags]: 
By correctly specified, this means that the vector of coefficients $\hat{\beta}$ and the predictions $\hat{Y} = X\hat{\beta}$ are unbiased typically. This follows from the assumption as follows: \begin{align} \hat{\beta} &= (X^TX)^{-1}X^TY\\ &= (X^TX)^{-1}X^T(X\beta + \epsilon)\\ &= \beta +(X^TX)^{-1}X^T\epsilon\\ \implies E[\hat{\beta}] &= \beta + E[(X^TX)^{-1}X^T\epsilon]\\ &= \beta + E[E[(X^TX)^{-1}X^T\epsilon|X]]\\ &= \beta + E[(X^TX)^{-1}X^TE[\epsilon|X]]\\ &= \beta \end{align} So you see if you do not have the assumption met, there will be an additional term which will not necessarily be 0 and the estimate (and thus predictions) will be biased. More intuitively, think about breaking the outcome ( $Y$ ) into two parts, a structural part related to the covariates $E[Y|X]$ and noise $\epsilon$ . The assumption $E[\epsilon|X] =0$ says that given the information from the covariates the noise term averages out to nothing. Or that on average you get the conditional mean correct. If this is not true and the covariates $X$ are correlated with the error term, then your regression is misspecified or there is structure you are missing for the noise to average out. If the error is systematically on one side or the other, fitting a line (or hyperplane) with respect to the variables $X$ is maybe not the best thing to do, which is what OLS is doing. Not necessarily, it depends on what the true relationship is between your variables. You only need to add variables into $X$ such that $E[\epsilon|X] = 0$ , in some cases this may mean polynomials terms and interactions etc, but not necessarily. If for example \begin{align} Y = X_1\beta_1 +X_2\beta_2 + X_1X_2\beta_3 + \epsilon \end{align} is the truth with $E[\epsilon|X_1,X_2] = 0$ , but you fit the regression $Y = X_1\beta_1 +X_2\beta_2 + \upsilon$ , you will get bias because $E[\upsilon| X_1, X_2] = E[X_1 X_2\beta_3 + \epsilon| X_1, X_2] = X_1X_2\beta_3 \neq 0$ . If $\beta_3$ is zero, there is no interaction in the true underlying conditional mean and you would no need to fit any interaction terms to achieve unbiasedness. If your goal is in-sample prediction, there might be an argument for adding many higher-order terms and interactions, you will always achieve at least as good of an in-sample prediction. If you expect the conditional mean to be highly non-linear, this may be a good idea. This doesn't mean that one should simply add higher order terms and interactions without thinking. There are ways to increase the bias of coefficients by adding more variables, increase the variance of the estimates, and decrease the accuracy of our out-of-sample prediction by adding higher terms gratuitously. Domain knowledge and theory related to the problem at hand should be consulted as much as possible for these decisions. Yes. \begin{align} E[Y|X] &= E[X\beta + \epsilon|X]\\ &= X\beta + E[\epsilon|X]\\ &= X\beta \end{align} Which is linear.
