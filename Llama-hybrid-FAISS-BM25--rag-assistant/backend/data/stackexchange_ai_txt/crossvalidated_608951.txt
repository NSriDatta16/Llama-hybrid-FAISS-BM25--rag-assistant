[site]: crossvalidated
[post_id]: 608951
[parent_id]: 
[tags]: 
Dropping a hierarchical linear model intercept when centering the outcome at 0?

Suppose a hierarchical linear model with "random intercepts" $\mu_i$ fit to some raw (unscaled) data: $$y_i \sim N(\mu_0 + \mu_i, \sigma) \\ \mu_i \sim N(0,\sigma)$$ If I rescale $y_i$ by subtracting the sample mean of $y$ from $y_i$ , I know that $\mu_0 = 0$ . Is there any reason I shouldn't drop $\mu_0$ from the model after de-meaning $y_i$ since the value of $\mu_0$ is known? I ask since I'm trying to understand whether it is possible to set the location of $\mu_i$ without explicitly including a $\mu_0$ in the model. If I don't drop $\mu_0$ and make Bayesian inferences, I need a prior on $\mu_0$ . While it should be centered at 0, should the variance be almost 0 since we know $\mu_0 = 0$ in sample, provided that we expect the sample mean to be close to the population mean?
