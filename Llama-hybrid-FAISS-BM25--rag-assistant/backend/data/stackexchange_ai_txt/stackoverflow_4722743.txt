[site]: stackoverflow
[post_id]: 4722743
[parent_id]: 
[tags]: 
How to deal with a very large text file?

I'm currently writing something that needs to handle very large text files (a few GiB at least). What's needed here (and this is fixed) is: CSV-based, following RFC 4180 with the exception of embedded line breaks random read access to lines, though mostly line by line and near the end appending lines at the end (changing lines). Obviously that calls for the rest of the file to be rewritten, it's also rare, so not particularly important at the moment The size of the file forbids keeping it completely in memory (which is also not desirable, since when appending the changes should be persisted as soon as possible). I have thought of using a memory-mapped region as a window into the file which gets moved around if a line outside its range is requested. Of course, at that stage I still have no abstraction above the byte level. To actually work with the contents I have a CharsetDecoder giving me a CharBuffer . Now the problem is, I can deal with lines of text probably just fine in the CharBuffer , but I also need to know the byte offset of that line within the file (to keep a cache of line indexes and offsets so I don't have to scan the file from the beginning again to find a specific line). Is there a way to map the offsets in a CharBuffer to offsets in the matching ByteBuffer at all? It's obviously trivial with ASCII or ISO-8859-*, less so with UTF-8 and with ISO 2022 or BOCU-1 things would get downright ugly (not that I actually expect the latter two, but UTF-8 should be the default here â€“ and still poses problems). I guess I could just convert a portion of the CharBuffer to bytes again and use the length. Either it works or I get problems with diacritics in which case I could probably mandate the use of NFC or NFD to assure that the text is always unambiguously encoded. Still, I wonder if that is even the way to go here. Are there better options? ETA: Some replies to common questions and suggestions here: This is a data storage for simulation runs, intended to be a small-ish local alternative to a full-blown database. We do have database backends as well and they are used, but for cases where they are unavailable or not applicable we do want this. I'm also only supporting a subset of CSV (without embedded line breaks), but that's ok for now. The problematic points here are pretty much that I cannot predict how long the lines are and thus need to create a rough map of the file. As for what I outlined above: The problem I was pondering was that I can easily determine the end of a line on the character level (U+000D + U+000A), but I didn't want to assume that this looks like 0A 0D on the byte level (which already fails for UTF-16, for example, where it's either 0D 00 0A 00 or 00 0D 00 0A ). My thoughts were that I could make the character encoding changable by not hard-coding details of the encoding I currently use. But I guess I could just stick to UTF-8 and ingore everything else. Feels wrong, somehow, though.
