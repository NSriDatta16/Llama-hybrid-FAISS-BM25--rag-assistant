[site]: crossvalidated
[post_id]: 509832
[parent_id]: 83731
[tags]: 
I am not sure it is a relevant question because it requires more definition than the math itself requires. Because the math itself does not require it, I am not sure asking which Bayesian interpretation is correct has a lot of meaning. Imagine two parallel universes. They are identical in the sense that the sequence of physical events in both universes unfolds in the same way. In other words, of the sample space $\chi$ , the Universe, $U\subset\chi$ is the same in every respect, $U_1=U_2$ . Now, in Universe One, every observer believes that Nature draws fixed points, $\theta_0$ , from the parameter space at the start of time, $t=0$ . These fixed points are $\theta_0\subset\Theta$ . An observer denoted $i$ , explains their initial uncertainty about its location with a probability distribution, $\pi_i(\theta_0)$ and upon seeing the data $X_i\subset{U}$ revises their uncertainty to $\pi(\theta_0|X_i)$ . Now, in Universe Two, every observer believes that Nature draws values for parameters, $\theta_t$ , from a distribution which is believed to be approximated by $\pi_i(\theta_t)$ at time $t=\tau,$ which is when observer $i$ gathers the data. The draws are random. This differs from the concept of heteroskedasticity or stationary variables. Such a person would define either in a different manner. Upon seeing the data $X_\tau\subset{U}$ at time $\tau,$ they use this additional information to improve the description of that distribution of $\theta_t$ to $\pi_i(\theta_t|X_\tau)$ . Do note that in the second case, it isn't really helpful to bring in Frequentist definitions of things like time series, heteroskedasticity, or stationary variables because their ideas are predicated on fixed points. Also, there is nothing in Universe Two that prohibits the distribution from being a Dirac Delta function. However, nothing prevents a prior in Universe One from being one either, and as such, one could completely miss $\theta_0$ . If you drop the needless subsidiary notation, you end up with $\pi(\theta|X)\propto\pi(\theta)f(X|\theta)$ . The math provides no mechanism to be able to distinguish a world with fixed but unobservable parameters and a mechanism to describe that uncertainty from a world where the parameters truly are random variables. Which is it? Who knows? That Frequentist methods in some sense "work," doesn't provide a solution either. There is nothing about countably additive sets that makes them better than finitely additive sets. It is true that there are use cases where only a null hypothesis method or only a Bayesian method could possibly work. They are not the general case. Interestingly, in those handfuls of cases where only one method could be thought of as suitable, the problem isn't resolved. For example, if the critical element of your method boils down to a sharp null hypothesis such as $$H_0:\beta_1,\beta_2,\dots\beta_k=0$$ it does depend on on a mathematical conditioning of those parameters at zero, as if it were the true fixed point. Nature isn't required to listen. Indeed, if nature were sometimes drawing parameters instead and causing false positives or negatives, the method would not be able to tell. Likewise, if the critical element of your method is setting gambling odds, there is no way to distinguish either world. You have to use a Bayesian method, but either conceptualization will work either well.
