[site]: crossvalidated
[post_id]: 533328
[parent_id]: 
[tags]: 
Why we dont need any hidden layers at all if the data is linearly separable?

According to the answer here: How to choose the number of hidden layers and nodes in a feedforward neural network? How many hidden layers? Well, if your data is linearly separable (which you often know by the time you begin coding a NN) then you don't need any hidden layers at all. Why this is true? If the data is linearly separable: 2.1 Do we need only to use input and output layers? 2.2 Does the activation function on the output layer will do the logic of the separation? (Is it enough)?
