[site]: crossvalidated
[post_id]: 314686
[parent_id]: 314607
[tags]: 
This response is really a comment but it quickly became too long for that format. The discussion with @stephenkolassa makes the OPs query sound more like a bad variant of the classic TSP (Traveling Salesman Problem) which, with more than a few nodes, quickly becomes an Np-hard problem in terms of computational complexity. Nearest neighbor algorithms are commonly used in TSP solutions. So, +1 to Stephen for this suggestion. I do have some points of difference. First of all, my view is that treating lat/long as continuous features is a mistake. To me, it would make more sense to discretize them in two possible ways (at least): first, treat lat/long in terms of 'distance traveled' or, second, by assigning them to a zip code. Both approaches would require digging up one of the widely available lat/long distance calculators (e.g., here ... http://www.nhc.noaa.gov/gccalc.shtml ). This NOAA function can also be treated as a formula and coded up in your software for the large number of computations required. Assignment of lat/long to a zip code is a little trickier insofar as it requires using the same distance or comparison function to find the zip code closest to each lat/long (starting and ending). In this case, the two lat/longs would be evaluated separately against the zip code lat/longs at their 'centroid.' Publicly available, accessible and free databases are out there that have zip code centroid lat/longs extracted from US census data (e.g., here ... https://stackoverflow.com/questions/19819418/up-to-date-zip-code-database ). In addition, many of them have geodemographic enhancements such as median hhold income, population size, ethnicity, etc., which can also be leveraged for descriptive purposes. The caveat to this second approach is that zip code is a massively categorical feature, even if only using the approximately 36,000 residential zip codes. Treating a cross-product matrix of this size in a classic, closed form, ANOVA-type model would be prohibitive. Few machines on the planet would have sufficient RAM to decompose such a matrix. A hierachical bayesian workaround to this challenge is proposed in this paper by Steenburgh and Ainslie Massively Categorical Variables: Revealing the Information in Zip Codes (here ... http://bear.warrington.ufl.edu/centers/MKS/abstracts/vol22/no1/aab6f24ee7_abstract.pdf ). A frequentist approach would be to leverage one of the many variants of divide and conquer algorithms out there which would reduce the problem to a more manageable size. The first, discretizing solution is probably the way to go.
