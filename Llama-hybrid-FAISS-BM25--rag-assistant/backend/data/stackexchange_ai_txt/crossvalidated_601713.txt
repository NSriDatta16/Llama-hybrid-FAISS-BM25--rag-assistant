[site]: crossvalidated
[post_id]: 601713
[parent_id]: 
[tags]: 
How does/can Machine Learning use input data uncertainty? A Bayesian perspective?

Datasets in the natural sciences often contain data that has been processed from raw measurements into features accompanied by a measurement of uncertainty, for example a standard deviation. So we don't just assume that our input measurements are noisy, but instead have actual information about how noisy the input features are. From my understanding the Bayesian approach is to consider the Dataset to be a fixed instance but places a prior and distribution over model parameters to express uncertainty and obtain a posterior distribution that allows a predictive distribution for new data points and thereby express epistemic uncertainty. However, it seems that the available information about aleatoric uncertainty over the input data is very valuable and should somehow be utilized. But I am unclear how or whether this is done from a Bayesian perspective. In Bayesian Neural Networks or approximations thereof like the Laplace Approximation, the goal is to obtain a distribution over your network parameters. How could the actual information of uncertainty in the input data be utilized? I am aware of error propagation techniques in natural sciences, but am interested how this could be done with "modern techniques" like Neural Networks.
