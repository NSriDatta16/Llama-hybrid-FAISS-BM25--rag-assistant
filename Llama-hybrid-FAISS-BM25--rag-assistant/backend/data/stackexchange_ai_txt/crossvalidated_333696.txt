[site]: crossvalidated
[post_id]: 333696
[parent_id]: 
[tags]: 
How can I minimize future long-term reward in deep Q-learning?

I’m trying to implement deep Q-learning on a problem were the rewards the agent receives are errors from another model. The RL agents job is to minimize the long-term reward (error) instead of maximizing it as is typically the case in Q-learning. So what I wonder is, how can the conventional DQN target value equation: $$\mathrm{Target} = r + \gamma * \max(Q(s’))$$ be rewritten for RL problems were the feedback from the environment (reward) should be minimized instead of maximized? (I realize that multiplying the error with negative 1 converts any minimization problem to a maximization task, but for sake of brevity I won’t dwell on why that’s not ideal to do in my situation).
