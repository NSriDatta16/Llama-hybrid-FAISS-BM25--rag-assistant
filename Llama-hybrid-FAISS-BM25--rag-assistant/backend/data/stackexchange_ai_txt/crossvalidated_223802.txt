[site]: crossvalidated
[post_id]: 223802
[parent_id]: 
[tags]: 
Random forest: overfitting even OOB error is low?

Is there any case that OOB ( out of bag) error fails to indicate overfitting? For example OOB is still good but the RF is overfitted. More specifically,I got low OOB error (8%) with a data set with a lot of wrong labels (i.e. Two samples with very identical feature values may be in different classes and vice versa). The wrong label rate is around 20% of the data set of 7000 samples. The OOB was calculated during training and thus it was based on the wrong labels as well. One possibility may be that the RF was able to learn a very nonlinear cut between two wrong classes even the two classes have a significant overlap. But I want to know if there are any other posibilities. Thank you.
