[site]: crossvalidated
[post_id]: 409541
[parent_id]: 163414
[tags]: 
I'd like to share with you an intuitive example to explain the following statement: we desire that the estimated class probabilities are reflective of the true underlying probability of the sample. That is, the predicted class probability (or probability-like value) needs to be well-calibrated. To be well-calibrated, the probabilities must effectively reflect the true likelihood of the event of interest. Source: Applied Predictive Modeling on page 249 For a particular case it is hard to illustrate the probability, but when it comes to large size of cases the effect of calibration appears. For instance, the airline company use some algorithms, for instance the logistic regression( why? ), to predict whether the passenger will show up on that day. They actually don't care about if the particular person will turn up or not, and what they concern is how many shows would be in total. What they do can be just adding all the probabilities of the predictions. If the sum is bellow the number of the seats they can overbook flights. Some classifiers are not well-calibrated for instance SVM. That's the score we get while predicting is not the true probability.
