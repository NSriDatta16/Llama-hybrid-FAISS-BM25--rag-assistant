[site]: crossvalidated
[post_id]: 345431
[parent_id]: 175523
[tags]: 
The answer is that it depends. I suggest you try both random forest and extra trees on your problem. Try large forest (1000 - 3000 trees/estimators, n_estimators in sklearn) and tune the number of features considered at each split (max_features in sklearn) as well as the the minimum samples per split (min_samples_split in sklearn) and the maximum tree depth (max_depth in sklearn). That said, you should keep in mind that over tuning can be a form of overfitting. Here are two problems I worked on personally where extra trees proved useful with very noisy data: Decision forests for machine learning classification of large, noisy seafloor feature sets An efficient distributed protein disorder prediction with pasted samples
