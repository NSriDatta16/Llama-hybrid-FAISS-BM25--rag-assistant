[site]: crossvalidated
[post_id]: 450385
[parent_id]: 450229
[tags]: 
It seems to me that the link you are missing here is to the probabilistic / information theory interpretation of VAEs. When the capacity of your networks is large enough you will reach a point where the solution with a larger latent space does not keep more information than a smaller one. This is possible in VAEs because they produce a noisy representation inside. To clarify things: First the bits/dim metric is per dimension of the input. You can read more about this metric in links collected here: What is bits per dimension (bits/dim) exactly (in pixel CNN papers)? Maybe the limit for infinitely big networks and infinite data is instructive here: VAEs optimize a variational bound on the evidence for the model. This is bounded from above by the true entropy of your data. At or near this point your bits/dim will converge and adding more complexity anywhere will no longer improve performance. With limited data this point will come earlier. As you appear to think in terms of bottlenecks & auto-encoders: For VAEs the bottleneck is not really the number of dimensions in the latent space, but the noise. Without noise even a single continuous number has infinite capacity. As VAEs are allowed to adjust the noise on their representation they may very well represent the same amount of information in fewer dimensions with less noise. Thus, the number of latent dimensions much less informative about the capacity of the encoding than for classic autoencoders. Indeed VAEs regularly have latent space units who converge towards always being equal to the prior, i.e. not carrying any information about images. In practice more latent units become harder to train and costly, such that you would avoid using too many latent units which "die", but from a theoretical viewpoint many dimensions does not equal an open bottleneck for VAEs. Thus, overall I would say the dimensionality of z for VAEs is one of many knobs changing the expressiveness/complexity of the encoder/decoder and does not affect the reconstruction loss directly beyond this.
