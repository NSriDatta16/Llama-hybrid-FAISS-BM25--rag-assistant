[site]: crossvalidated
[post_id]: 64839
[parent_id]: 64836
[tags]: 
This question is quite vague. It all depends on how you define big data . Since you added machine-learning , I will talk a bit about that. In terms of machine learning, large-scale learning routinely refers to tasks where your data sets won't fit in RAM anymore (hundreds of gigabytes). The challenge is simple: get anything done (preferably efficiently). Some like to say that the main computational constraint in large-scale learning is time . Many nonlinear techniques become unfeasible when the number of instances/dimensions becomes large. Kernel methods are particularly susceptible to time inflation when the number of training instances gets huge (millions or more). Often the only solution is to resort to subsampling or linear techniques and accept the performance penalty as a necessary evil.
