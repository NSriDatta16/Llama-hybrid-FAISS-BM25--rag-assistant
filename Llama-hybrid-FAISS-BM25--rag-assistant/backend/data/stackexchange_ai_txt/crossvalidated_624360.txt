[site]: crossvalidated
[post_id]: 624360
[parent_id]: 
[tags]: 
Seeking recommendations for feature selection methods before applying a random forest model to high-dimensional data

I'm seeking recommendations for feature selection methods before applying a random forest model to high-dimensional data, specifically with over 60,000 features and only 1,000 samples. My concern is that directly inputting all 60,000 features into the random forest may impact prediction performance and be computationally intensive. I've explored the Boruta algorithm, but it's too computationally expensive given the number of features. I'm also not sure about using simple filtering methods like univariate filtering, as they might overlook non-linear and complex relationships. My current plan is to initially run a random forest model with all 60,000 features and then select the top N features (perhaps around 2,000) for further model training and tuning. I am wondering does this make sense? Btw,we've ruled out PCA due to interpretability concerns. I'd greatly appreciate any advice!
