[site]: crossvalidated
[post_id]: 475208
[parent_id]: 
[tags]: 
BERT masking - why does it require sampling, and how does it mitigate the mismatch of the [MASK] token when fine-tuning?

I'm reading the BERT paper and jalammar's illustrative guide for BERT . I don't understand 2 things about the method's crux - the masked language model: why does masking requires us to sample (take only 15%) of the words? can't we use the same sentence several times, each time masking another word? E.g. turn I am a student to I [mask] a student and I am a [mask] ? the authors were worried that the [mask] token itself would be used by BERT when training and then confuse it later at the fine-tuning stage (or even the inference stage). I don't understand the mitigation they use - in 10% of the times they replace [mask] with a random word, and in 10% they replace it back to the original word. How is that mitigating the problem? and if it does, why do they use such low percentages? thanks, Ido
