[site]: crossvalidated
[post_id]: 643361
[parent_id]: 
[tags]: 
Question on Best Transformation (Negative, Zero, Positive Values) + Missing Data

I have a dataset with $5000$ observations, and 10 explanatory and 1 response variable (binary 0 or 1), and my task is to make a logistic regression model for prediction (but also needs to provide some understanding of the situation), and right now I am at the EDA stage. My issue is as follows, all explanatory variables are extremely skewed, some only have positive values which lend themselves nicely to a log transform, while others include negative values, zero values or both. There are several methods and transformations I came across, as well as the bestNormalize package in R, and the top transformations I found were Yeo-Johnson, arcsinh, power transforms (such as cube root) and orderNorm. One final suggestion from the package is that for variables with zero and positive values, one should add a small factor and then log it. orderNorm performs the best, however it is difficult to interpret and apparently it has issues when using it on new unseen data. Power transforms were very weak, and Yeo-Johnson (extension of Box-Cox) seems like the best candidate so far. Arcsinh doesn't seem to have any glaring weaknesses either. Given this, I wanted to ask for advice and possible suggestions. In particular about the case where you add a small factor, and then log transform. Many sources state this introduces bias and is in general bad practice, while others argue the effects are negligible. I have added a density plot below to show just how severe the problem is. One other questions I have, is about missing values. I am aware of the classifications MCAR, MAR and MNAR, and how it is virtually impossible to distinguish between MAR and MNAR. I wanted to ask if there is ever a reason to delete observations (for MCAR), as opposed to using imputation.
