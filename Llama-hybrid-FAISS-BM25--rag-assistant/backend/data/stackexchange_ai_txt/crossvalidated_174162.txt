[site]: crossvalidated
[post_id]: 174162
[parent_id]: 
[tags]: 
How to work a data set with a huge number of features?

I'm new to machine learning so I apologize in advance if the answer is obvious. I have a dataset that includes an address feature. I was hoping to use one hot encoding to create a feature for each street that appears in the dataset. So if my dataset looked like: Miles| Address 4 | 111 First St 2 | 456 Grenada Ave 9 | 789 Grenada Ave It would become: Miles| Address | First_St | Grenada_Ave 4 | 111 First St | 1 | 0 2 | 456 Grenada Ave | 0 | 1 9 | 789 Grenada Ave | 0 | 1 Doing this blows up the number of features out to over 1500 and puts both the train and test files from around 90 mb to over 5gb. I was hoping to apply a RandomForest to these new datasets, but when I call forest.fit (using Python/sklearn) the memory usage balloons out to 60gb and then the Python process dies. Couple of questions: Is it considered bad practice to create this many features? If not - how does one go about modeling with data this size?
