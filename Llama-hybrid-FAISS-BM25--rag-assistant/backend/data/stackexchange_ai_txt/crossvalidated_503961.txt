[site]: crossvalidated
[post_id]: 503961
[parent_id]: 
[tags]: 
Small loss magnitude

I'm training a model that has "a priori" very small magnitudes of the loss function (MSE for regression purposes). The model fails to learn which I suppose is due to the fact that the loss is very small and hence the gradients will also be terrible small which can be considered a vanishing gradient problem even though I'm using a fully connected neural network with just 2 hidden layers and 40 neurons each. How would you solve this problem? Is there any loss function other than MSE that somehow scales the loss to proper values in order to avoid terrible small magnitudes or even scaling the gradients by multiplying them with a proper constant?
