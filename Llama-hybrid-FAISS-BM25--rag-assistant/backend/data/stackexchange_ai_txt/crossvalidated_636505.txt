[site]: crossvalidated
[post_id]: 636505
[parent_id]: 633226
[tags]: 
Embeddings are trained from scratch. I think of it as a hash map / dict in python that maps token to a 1d vector (embedding of a token). They are initialized from noise. During the training phase the backpropagation algorithm computes loss with respect to those embeddings and that is how they being trained. During inference the embeddings are frozen. Imaging you are given a sentence, it first is split into tokens by tokenizer, then those tokens are converted into embeddings using the "dict" I mentioned earlier. The fact that in classic algorithms such as word2vec there are two sets of embeddings might confuse you as there is only one set in transformers.
