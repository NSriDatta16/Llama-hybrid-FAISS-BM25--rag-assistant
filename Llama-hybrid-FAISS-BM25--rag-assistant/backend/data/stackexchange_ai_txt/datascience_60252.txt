[site]: datascience
[post_id]: 60252
[parent_id]: 57547
[tags]: 
You included that probability-calibration tag, which is prescient: there are a few techniques, all called "probability calibration," which adjust the scores output by a model to better fit observed probabilities. After this, the scores should be close to representing real probabilities, and should therefore be directly comparable. The most common methods are Platt scaling and isotonic regression. There is a third and more recent method, beta calibration, and there are a few more exotic ones around. The three ones I've named all fit to a new dataset a univariate function with inputs your model's scores and outputs the actual observed labels. Platt scaling fits a sigmoid function, beta calibration fits a parametric model that is more general than sigmoid, and isotonic fits a nonparametric, arbitrary non-decreasing function. XGBoost's outputs are biased away from 0 and 1, so the sigmoid is generally ill-suited, so in this case go with beta or isotonic (or find something else to your liking). Isotonic, being more well-known, has more open-source implementations.
