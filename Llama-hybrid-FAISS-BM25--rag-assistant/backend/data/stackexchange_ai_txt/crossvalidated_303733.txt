[site]: crossvalidated
[post_id]: 303733
[parent_id]: 303716
[tags]: 
The interpretation is motivated by considering how the model predictions change when controlled, simple changes are induced in the original variables. Let's frame this a little abstractly because it doesn't make the situation any more complicated while revealing the essence of the matter. If we denote those variables by $u=(u_1, \ldots, u_m)$, say, then we may write the regressors --by which I mean the variables that actually are involved in the regression--as specified functions $f_1,f_2, \ldots, f_p$ of $u$. For example, $m=3$ numerical variables plus an interaction between the first two, would produce $p=4$ regressors; namely, $$\eqalign{x_1 &= f_1(u) = u_1,\\ x_2 &= f_2(u) = u_2,\\ x_3&=f_3(u)=u_3, \text{ and}\\x_4 &= f_4(u)=u_1u_2\ (\text{the interaction}).\tag{*}}$$ The fitted model based on estimated parameters $b=(b_0, b_1, b_2, \ldots, b_p)$ (an "intercept" is hereby included as $b_0$) fits or "predicts" the response $y$ associated with regressor values $x_1, \ldots, x_p$ to be $$\hat y = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_p x_p.$$ One common way of interpreting this asks how $\hat y$ would change when, say, the original set of variables $u$ is changed by adding a fixed amount $\delta$ to just one variable $u_j$, becoming $u_j + \delta$, thereby creating a new set of values $u^\prime$. Those not appealing to the Calculus will compute the difference directly, as follows. Let $\hat y$ be the fitted value for regressors associated with $u^\prime$. Subtracting off $\hat y$ and organizing the result by the index $1, 2, \ldots, p$ exhibits the change in response as a linear combination of changes in the regressors: $$\hat y^\prime - \hat y = b_1(f_1(u^\prime)-f_1(u)) + \cdots + b_p(f_p(u^\prime)-f_p(u)).\tag{**}$$ The form of this expression highlights the (obvious) fact that changing $u_j$ affects only the terms for which $f_i(u)$ actually depends on $u_j$. (This is the one aspect of this analysis you might want to commit to memory.) In example $(*)$, for instance, if $j=3$ then only $x_3$ is changed when $u_3$ is changed, thereby becoming $$x_3^\prime - x_3 = f_3(u^\prime) - f_3(u) = u^\prime_3 - u_3 = (u_3 + \delta) - u_3 = \delta.$$ In all other cases $i\ne 3$, $x_i^\prime-x_i=f_i(u^\prime) - f_i(u)=0$: there is no change. Plugging these changes into $(**)$ simplifies it right down to $$\hat y^\prime - \hat y = b_3\,\delta.$$ Interpretation : "changing $u_3$ by $\delta$ (while fixing all the other $u_i$) changes the response $y$ by $b_3$ times $\delta$." Most readers of this site will appreciate that this is intended only as an English description of the foregoing mathematical relationships; in particular, it is not a causal claim. It says nothing about what will happen in the world to $y$ if somehow an observation could be altered to change $u_3$ to $u_3+\delta$ (if that is even possible). Note that $b_3$ does not depend on whatever values the $u_i$ might have: it is a "constant." This makes the interpretation particularly simple. Continuing the example, suppose instead that $u_1+\delta$ is used in the model instead of $u_1$. Now two of the $x_i$ in $(*)$ are affected: $x_1$ increases by $\delta$ while $x_4$ increases by $\delta u_2$. Consequently $(**)$ yields $$\hat y^\prime - \hat y = b_1\delta + b_4 \delta u_2 = (b_1 + b_4 u_2)\delta.$$ Interpretation: "changing $u_1$ by $\delta$ (while fixing all the other $u_i$) changes the response $y$ by $b_1 + b_4 u_2$ times $\delta$." There is the interaction: the change in predicted value depends on the values of the underlying variable $u_2$. Notice that $u_3$ is not involved. The answer to the original question should now be clear from the very forms of $(*)$ and $(**)$. This method of analysis applies not only to interactions, but--by virtue of the abstract specification of the functions $f_j$--for all other models that combine the $u_i$ in any manner whatsoever. (This includes polynomial models, higher-order interactions, and other nonlinear models that might involve exponential growth, sinusoidal variation, and more.) In particular, the interpretation of any interaction does not depend on any variables that are not directly involved in that interaction.
