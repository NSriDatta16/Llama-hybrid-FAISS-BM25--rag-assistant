[site]: crossvalidated
[post_id]: 485639
[parent_id]: 
[tags]: 
Comparing different machine learning methods over multiple test datasets with different number of samples

Say, I have an image dataset (for example, imagenet) and I am training two image recognition models on it. I train a resnet with 10 layers 3 times on it (each time with different random weight initialization), each time for 20 epochs. For last 5 epochs of training, the accuracy on test datasets does not change very much, but oscillates around. At each of the last 5 epochs, I save the current weights (at that epoch) of the model. I also have a resnet with 20 layers. Let's say I train it 4 times for 20 epochs on the same dataset, and simiarly save the weights at the final 5 epochs for each training. I also have 10 test image datasets, coming from various sources, maybe from internet, web cameras, street cameras, screenshots from movies, etc. Each of the the datasets has varying number of images in them, ranging from 20 to 20000. I evaluate all the models (2*(3+4)*5=70) on all the datasets. Now given the above info, I have these questions: What is the probability that a resnet with 20 layers is on average better on these datasets than a resnet with 10 layers? (on average, as in calculating the accuracy on each of the ten datasets, and then taking the mean of the ten resultant values). And what are the confidence intervals (or credible intervals) around that probability value? There are multiple sources of variance here: variance due to test dataset sizes, variance due to different weight initializations, variance due to accuracy oscillating from one epoch to next. How do you account for all these sources of variance to come up with a single number which would indicate the probability that one method is better than the other? And finally, imagine that you did these tests, and you noticed that on one of the ten datasets the accuracy difference is the largest between these two methods. How can you quantify whether such accuracy difference is by chance or because it indeed is the case that one of the methods is better on this particular dataset? (the concern here is the multiple hypothesis testing and how to account for it, while taking care of all the other sources of variance as well).
