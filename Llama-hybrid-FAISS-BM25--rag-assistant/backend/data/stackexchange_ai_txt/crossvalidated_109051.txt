[site]: crossvalidated
[post_id]: 109051
[parent_id]: 108343
[tags]: 
I believe there is a serious overfitting going one. When you adjust the hype-parameters using the hold out test set, you get 70% accuracy, when you run with "new" data (the 10-fold CV) you get a much lower accuracy - which is the textbook description of overfitting. So what could be the source of the overfitting? "Old" neural networks had a big source of overfitting because of the early stop learning and thus the need for a validation set, and so on. I believe that either you are aware of this and dealt with it, or it is dealt internally in the RSNNS package The other source of overfitting is a methodolgical one. Th protocol you used is not a (most) correct one. You did: a) a 4/5 1/5 hold out of teh whole dataset to select the hyperparameters (also known as "model selection") b) and than you did a 10-fold on the SAME dataset to evaluate the final model's accuracy. This is not the most correct protocol. You need a nested cross validation here to select the hyperparameters and then calculate the accuracy of the selected model. I do not have the time at this moment to write about nested cross validation, but there has been some discussions and explanations about it in CV. Dikram's answer to Use of nested cross-validation is one good example (and has a link to a somewhat difficult to read paper on the topic). Internal vs external cross-validation and model selection also has great answers. One thing that is bothering me is the magnitude of your overfitting. I have not studied this in details but in my limited experience methodological overfitting is not that high!. The only place I found this large difference in (methodological) overfitting is this one http://www.biomedcentral.com/1471-2105/7/91 but there they use artificial data where there is NO correct classifier. In that case, due to this methodological overfitting they get accuracies of 65% (average) when the true accuracy if 50%. Could this be happening in your case? There is no good classifier for your data, and the methodological error hallucinates this 70% accurate classifier!
