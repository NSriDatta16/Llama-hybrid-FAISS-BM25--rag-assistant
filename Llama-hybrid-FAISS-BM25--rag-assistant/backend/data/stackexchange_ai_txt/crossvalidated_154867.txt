[site]: crossvalidated
[post_id]: 154867
[parent_id]: 154860
[tags]: 
The main advantage of shared weights, is that you can substantially lower the degrees of freedom of your problem. Take the simplest case, think of a tied autoencoder, where the input weights are $W_{x} \in \mathbb{R}^d$ and the output weights are $W_{x}^T$. You have lowered the parameters of your model by half from $2d \rightarrow d$. You can see some visualizations here: link . Similar results would be obtained in a Conv Net. This way you can get the following results: less parameters to optimize, which means faster convergence to some minima, at the expense of making your model less flexible. It is interesting to note that, this "less flexibility" can work as a regularizer many times and avoiding overfitting as the weights are shared with some other neurons. Therefore, it is a nice tweak to experiment with and I would suggest you to try both. I've seen cases where sharing information (sharing weights), has paved the way to better performance, and others, that made my model become significantly less flexible.
