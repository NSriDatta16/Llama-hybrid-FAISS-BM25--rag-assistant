[site]: crossvalidated
[post_id]: 310952
[parent_id]: 
[tags]: 
How the probability threshold of a classifier can be adjusted in case of multiple classes?

The above is a very simple example of having a probability classifier output for a binary-class case either 0 or 1 based on some probabilities. In addition it is straightforward how you can change the threshold. You set the threshold either higher or lower of 50% to change the precision/recall balance and thus optimize for your own unique situation. However when we try to have the same thinking for a multiclass scenario, even as little as three classes as is shown in the picture below (imagine that these are probabilities) How do you begin to think how to shift the threshold? The default is to take the class with the largest probability (here is class 3). If you want to take this balance (to affect precision/recall) what could you do? One idea could be to take the first most dominant classes re-normalize them and consider putting a threshold among these two, but this does not sound like an elegant solution. Is there a solid methodology to follow?
