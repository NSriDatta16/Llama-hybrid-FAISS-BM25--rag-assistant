[site]: datascience
[post_id]: 89414
[parent_id]: 
[tags]: 
Search for redundant filters(channels) in CNN

When training a CNN one specifies in each layer the number of channels. In the input we have 1 channel for grayscale image and 3 for RGB image, and then usually the image resolution is decreased, whereas the number of channels increases (64, 128, 512). These filters capture some patterns, useful for solving a particular task - classification, segmentation, detection, whatever. However, the resulting collection of numbers in the filter does not have a particular structure, and I may happen, and I suppose, mostly happens, that there is a lot of redundancy in the channels. If one is interested to make the CNN as small as possible in order to use less memory or speedup inference procedure, what are the best procedures to eliminate the redundant, useless filters? One possibility I see is to measure the correlation between different channels in ordet to determine, whether they are linearly dependent. But I suppose, that It can get help to get rid only of very limited number of channels. Another possibility, I would think about us to perform variational dropout https://arxiv.org/pdf/1701.05369.pdf but a filterwise version to determine noisy channels. And then eliiminate them. Third, since filters are $N$ -dimensional arrays, there exist certain decompositions, namely HOSVD https://en.wikipedia.org/wiki/Higher-order_singular_value_decomposition . But the layers are connected with each other, and truncation needs to be performed in some self-consistent way. Which approaches turned out to be succesful in CNN and CV tasks?
