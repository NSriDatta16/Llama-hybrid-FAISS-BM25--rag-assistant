[site]: crossvalidated
[post_id]: 531651
[parent_id]: 359365
[tags]: 
Random forest algorithms for regression in classification are nearly the same. The important difference is the loss function used for the training. For classification, they typically use entropy, or Gini impurity, for regression squared or absolute error. You can use squared error for classification tasks, it even has a special name: Brier score . Recently there was an interesting paper by paper by Hui and Belkin (2020) showing that using squared error as a loss function for a neural network may give as good if not better results as compared to the "default" log loss. However, keep in mind that using different loss functions may lead to different results. But why would you do that? Random forest classifier does predict the probabilities, scikit-learn has the predict_proba function for that purpose. Moreover, if you care more about the probabilities than the hard classification, you should validate and calibrate the probabilities . For more details check questions tagged as calibration .
