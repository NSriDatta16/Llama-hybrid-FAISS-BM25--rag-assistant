[site]: crossvalidated
[post_id]: 349992
[parent_id]: 349989
[tags]: 
Least squares optimizer has an elegant solution using linear algebra. You are solving the system $A\hat x=\hat b$, where be is A is your matrix ( [[1,x0,z0],[1,x1,y2],...] ), $b$ is a column of [z0; z1; ;..] and $x$ is a vector containing the estimated parameters which your solving for. The vector $b$ is NOT in the column space of $A$, so there is no solution, so you need to decompose the vector $b$ vector into the sum of the projection of $b$ onto the column space of the matrix $A$ and the orthogonal component $e$ given by the following: \begin{equation} \label{2} b=proj_{Col(A)} + e \end{equation} Where $e$ is a vector containing errors orthogonal to the column space of $A$. Instead of solving $A x= b$, we solve the equation that best estimates $ b$. \begin{equation} Ax=proj_{Col(A)} \end{equation} Since, the $proj_{Col(A)}$ (read as the projection of b onto the column space of $A$) is in the column space of $A$ there will now be a solution to the system, where there wasn't one previously one! To find a the "best fit" solution start by combining the previous equations: \begin{equation} A x= b - e \end{equation} Here comes the trick! Multiply each term by $A^T$, which is the transposed matrix of A where the columns now become rows. \begin{equation} A^T A x=A^T b -A^T e \end{equation} Where $e$ is orthogonal to the row space of $A^T$, and therefore $ e$ is in the null space of $A^T$. This means term $A^T e $ becomes the zero vector $ 0$. What's left is the least squares solution to $A x=b$ given by : \begin{equation} A^T A x=A^T b \end{equation} Now you want to know how to code this... I will solve the 2 variable case: Multiplying everything out we get: To solve for the estimators, the matrix should be augmented and row reduced. The row reduction starts by switching row 1 and row 2. Then multiply row 1 by $-\frac{n}{\sum_{i=1}^{n} x_i}$ and add to row 2. This will result in a $0$ in the second row and first column. A total of two pivots for two rows means the matrix has full rank and $\hat b_0$ and $\hat b_1$ can be solved for.
