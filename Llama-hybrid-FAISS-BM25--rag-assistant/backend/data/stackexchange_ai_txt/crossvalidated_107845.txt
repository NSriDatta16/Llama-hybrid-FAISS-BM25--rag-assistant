[site]: crossvalidated
[post_id]: 107845
[parent_id]: 
[tags]: 
When is it appropriate to use PCA as a preprocessing step?

I understand that PCA is used for dimensionality reduction to be able to plot datasets in 2D or 3D. But I have also seen people applying PCA as a preprocessing step in classification scenarios where they apply PCA to reduce the number of features, then they use some Principal Components (the eigenvectors of the covariance matrix) as the new features. My questions: What effects does that do to the classification performance? When to apply such a preprocessing step? I have a dataset with 10 features as real numbers and 600 binary features that represent categorical features, using one-to-many encoding to represent them. Would applying PCA here make sense and make a better results? p.s. if the question is too broad, I would be thankful if you provide a paper or tutorials that explains better the details of using PCA in that manner. p.s. after reading a little, i found that it could be better to use Latent Semantic Analysis to reduce the number of binary features for the categorical attributes? So I don't touch the real-valued features, but only preprocess the binary features and then combine the real-valued features with the new features and train my classifier. What do you think?
