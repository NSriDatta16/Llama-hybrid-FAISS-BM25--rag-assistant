[site]: crossvalidated
[post_id]: 373870
[parent_id]: 373835
[tags]: 
Differently from the other answer here which I think is a good answer, I think a linear regression suffices in your exact situation. As a matter of principle, it might be wrong because of the hard bounds on the outcome but a plot of your data suggests why it is a good enough approximation. At both extremes, the mean of your data is far enough from the extremes of the outcome, hence, there is no bending or curving of the relationship to respect the bounds on the original outcome. In short, your predictions appear to be linearly related to the outcome, the major requirement for linear regression. If the above is true, the advantages of the linear model are then: ease of interpretation, the coefficients you posted make sense on arrival one can obtain a relatively simple measure of variability around the fitted line from the regression standard deviation if one cares. I began by exploring the data: dat The blue line is a generalized additive model smoother. The red line is the linear fit. One can observe some curvature at the ends but outwards, not inwards. So your predictions are not exactly linearly related to the outcome. Since you have enough data points, this is probably not arbitrary. A logit transformation of $y$ is unlikely to help here, rather, a logit transformation of $x$ since it bends outwards. We can shrink the middle $x$ values somewhat and expand the larger $x$ values: ggplot(dat, aes(log(x / (1 - x)), y)) + geom_point(shape = 1) + theme_bw() + geom_smooth() + geom_smooth(method = "lm", se = FALSE, col = "red") And this time, the linear fit approximates the smoothed fit quite well, not so well at the tails, but good enough for most applications. And homoskedasticity is a plausible assumption. So the regression model of choice is then: coef(summary(fit.lm |t|) (Intercept) 0.50116288 0.002897402 172.96973 0.000000e+00 log(x/(1 - x)) 0.05446613 0.002045560 26.62652 6.846189e-124 When the predicted log-odds are zero, we expect the win probability to be about 50%. A log-odd higher, and expectation is 5% higher. As seen in the second plot above, the log-odds don't go any further than 5 in either direction. So all the predicted values of $y$ are bound between about 25% and 75%. The regression effect is clear enough and at a large enough sample size that I trust that the inference is not misleading overall. There are always alternatives for better precision. We can also get a sense of the error about the fitted line. sigma(fit.lm) [1] 0.09958555 Given a prediction, about 95% of values should be within about $\pm20\%$ . The interval when added to the minimum and maximum predicted $y$ also lies within bounds. The justification for the linear approach is its simplicity and its adequacy in this particular application.
