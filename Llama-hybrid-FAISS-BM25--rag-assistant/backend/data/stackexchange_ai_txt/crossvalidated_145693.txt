[site]: crossvalidated
[post_id]: 145693
[parent_id]: 
[tags]: 
Proper cross validation for stacking models

Lets assume that we have dataset that contains continuous variable $Y$ which we want to predict and 10 predictors $X_{1}, ..., X_{10}$. The number of observations is $n=1000$. I have questions about proper cross validation in two following situations: I want to add variable $X_{11}$ which is equal to average of $Y$ from 10 nearest observations (the metric is not important). On this extended dataset I would like to make linear regression. What is the proper way of CV (kfold for $k=5$)? Add $X_{11}$ using whole data set and then do 'normal' kfold cross validation? In such situation some information about test set will be included in training part so there will be bias in error. Add $X_{11}$ separately in each fold using only training data. But then the question is how to add $X_{11}$ in test set, should only test set be used? Because of number of cases in test dataset the variable $X_{11}$ might be biased. Two models were built (for example: random forest and gradient boosting machine) and now want to make linear blending of those two models. What predictions from the model should be put as predictiors? One solution is: Split data set into train/test (800/200), build those two models using training dataset and the blend those probabilities and test final predictions on test dataset. Repeat that 5 times for different folds in test dataset. I believe that this solution might not be perfect because random forest tends to overfit on training data set. I feel that it would be better to blend probabilities not from training data set. To overcome this one might do following: Split data set into train/test (800/200). Then do kfold CV on the training data set and use as predictor out of bag estimates (so the split is 640/160). This is much more time consuming solution, but should be more reliable. The drawback is that if we want to make kfold cross validation for k=5 then in the models that are input for blending we will have 16/25 example in the training dataset. I strongly believe that both cases are well known, but I would like to know what is the state of the art in that matter.
