[site]: crossvalidated
[post_id]: 420245
[parent_id]: 412977
[tags]: 
The typical way to give some intuition for reproducing kernel spaces (and, in particular, the kernel trick), is the application area of support vector machines. The aim is to linearly separate two classes of points in $\mathbb R^n$ , which works fine if they actually are linearly separable. If they are not, the kernel trick provides (in certain situations) a possibility to transform the data points into another space, the so-called reproducing kernel Hilbert space or feature space, where the transformed points become linearly separable. A good description can be found here . Of course, this is just one of hundreds of applications of the kernel trick (or RKHS in general), but it is one which hopefully clarifies its power and justifies its usefulness.
