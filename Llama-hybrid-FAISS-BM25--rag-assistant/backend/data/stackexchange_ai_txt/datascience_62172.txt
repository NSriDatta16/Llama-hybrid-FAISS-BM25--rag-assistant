[site]: datascience
[post_id]: 62172
[parent_id]: 62167
[tags]: 
The value function you want to learn should be relatively simple, but I would expect optimal action values to be quite similar between moving left or right, at least when that is away from a wall. Recall that action values measure the expected reward for taking the selected action, then following the policy from that point on. So in state [0 0 8 5 0 0] , the action value under an optimal policy for "move right" should be +1, but the action value for "move left" is not much different, at 0.9801 . . . because under an optimal policy the agent will move right twice afterwards when following the policy. That means the neural network will have to learn fine detail differences between estimates. You have made this harder than it needs to be if you are using the representation as "raw" input to the neural network. Neural networks learn badly when the input number range is high. In addition your learning rates seem quite high. My suggestions: Scale the inputs. You should have code that turns the state representation into neural network features. I'd suggest the neural network will function nicely with +1 for goal and -1 for agent, if you must work with this state representation. Try lower learning rates. A rate of $0.01$ is high for the Adam optimiser. Reduce discount factor $\gamma$ - that will make a clearer difference in this short episode-length environment between returns for faster and slower routes.
