[site]: crossvalidated
[post_id]: 627628
[parent_id]: 
[tags]: 
Which regression best suits double bounded outcomes that aren't binary?

I'm considering some projects in the future which require modeling literacy outcomes as composites (such as word reading scores), which will naturally never have negative values (it's impossible to read negative words on a page), and is capped by the maximum score one can attain (for example somebody reading $50/50$ words). Because of this, fitting a Gaussian model doesn't seem accurate because the data for a composite will normally be something like $0 \le x \le \text{max}_x$ . However, I am not as experienced in this issue and have only used bounded data in one analysis so far, which used binary outcomes. This is quite straightforward to model with logistic regression because it is simply bounded between the two values $0$ and $1$ and can be fitted with the typical link function $g^{-1}(u) = \text{logit}^{-1}(u)$ . This is however not quite the same as what I need. Some alternatives that I have seen that look potentially more realistic: Poisson regression : typically used for counts anyway. As I have read, it uses the link $g(u) = \text{exp}(u)$ to achieve some of the same things logistic regression does. But what I normally see with it's application is right skewed data, such as the Prussian army horse data . I've seen plenty of composites used in my area that follow a more normal distribution, and so I feel that fitting Poisson regressions to this outcome data would yield overdispersion, as the variation may be greater than expected in the model. So far, I am undecided on if this method should be chosen . Logistic regression : seems to achieve the same thing by modeling proportions of successes rather than just the outcomes with each trial. This would make sense for a composite measure as they are technically proportions anyway (successes/trials). If simply fitting the data as a logistic regression the typical way works for this, then it immediately solves some of my problems, as I already have some experience with this approach. This so far seems to be the best option . Probit regression : appears to model the data with a "normal distribution", where $\text{Pr}(y_i = 1) = \Phi(X_i \beta) $ . However, what this actually means in practice to me is lost because I keep reading that it's usually only appropriate for binary outcomes. I have also read that for many purposes, logit/probit can be treated pretty similarly. This is my least favorite option because functionally it doesn't seem to add much and may confuse reviewers for no reason . Typically I never see this issue considered at all in my field, so I don't even know if it's worth any serious thought. However, given what reading I have done on this subject, treating this kind of data with a Gaussian-distributed error model seems either limited or hazardous. Edit I tried simulating this kind of data to see if it was reasonable. Below is some R code that I used: #### Simulate Data #### library(tidyverse) set.seed(123) n % ggplot(aes(x=x,y=dv))+ geom_point()+ stat_smooth( method = "glm", method.args = list(family=quasibinomial), formula = y ~ x )+ labs(x="X", y="Y", title="Simulated Quasibinomial Regression")+ theme_bw() Which shows the fit seems reasonable: But when I check the fitness measures, some of them look poor when I run plot(fit) :
