[site]: crossvalidated
[post_id]: 623997
[parent_id]: 623996
[tags]: 
From the comments, it sounds like you have a fairly standard multi-label problem and want the model predictions to be the probabilities of class membership instead of the predicted classes. The good news is that the math already gives you this, and if youâ€™re only getting predicted categories from your code, the path forward should be to change your code. For instance, the predict method in sklearn will give your the category with the highest predicted probability, yet predict_proba gives the raw predicted probabilities. Your software should have such a capability, and if it does not, you might want to use a different software. Consequently, I would not use any special kind of loss function with the soft labels to predict probabilities. Rather, I would model the original binary decisions as a multi-label problem and predict the probabilities from that model. One caveat is that neural networks tend to give predicted probabilities that are overconfident. Calibrating these predictions so they better reflect the reality of true event occurrence is possible but not always trivial. The multi-label aspect of this problem where event occurrence can be influenced by other events occurring further muddies the picture. (This may very well be an open problem.)
