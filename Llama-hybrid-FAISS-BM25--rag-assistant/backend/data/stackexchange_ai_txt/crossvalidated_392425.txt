[site]: crossvalidated
[post_id]: 392425
[parent_id]: 392087
[tags]: 
The 'transpose trick' is a general fact of linear algebra, so it always holds. But, transposing the data matrix before estimating the covariance matrix (as in the Matlab code you quoted) is not the proper way to use this fact to do PCA. This has to do with the centering issues you mentioned. What goes wrong Suppose the data matrix is $A$ , with $n$ observations on the rows and $p$ features on the columns. Let $A_c$ denote the centered data matrix, obtained by centering the columns of $A$ . The sample covariance matrix is $$C = \frac{1}{n} A_c^T A_c$$ But, if you transpose $A$ before computing the sample covariance matrix, you end up with: $$C_* = \frac{1}{p} A_{c*} A_{c*}^T$$ where $A_{c*}$ is the result of centering the rows of $A$ . Notice that $C$ and $C_*$ don't have the relationship needed to use the transpose trick, unless $A_c = A_{c*}$ (as happens when the row and column means are both zero, for example). And, in the latter case, the eigenvalues of $C_*$ will still differ from those of $C$ by a constant factor of $\frac{n}{p}$ . The following procedure should be used instead. How to do PCA using the transpose trick Given $n \times p$ data matrix $A$ (with $p > n$ , otherwise this is just wasted computation): Center the columns of $A$ to obtain the centered data matrix $A_c$ . Let $G = \frac{1}{n} A_c A_c^T$ . Note that $G$ and $C$ (above) do have the proper relationship to use the transpose trick. Let $V \Lambda V^T$ be the eigendecomposition of $G$ (with eigenvectors on the columns of $V$ and eigenvalues on the diagonal of $\Lambda$ ). Let $\tilde{V} = A_c^T V$ The diagonal of $\Lambda$ and the columns of $\tilde{V}$ are equal to the top $n$ eigenvalues and eigenvectors of the sample covariance matrix $C$ . Any remaining eigenvalues of $C$ are zero. It's a good idea to scale the columns of $\tilde{V}$ to have unit norm, as they won't generally be normalized to begin with.
