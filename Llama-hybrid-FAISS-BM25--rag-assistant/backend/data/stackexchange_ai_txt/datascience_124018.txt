[site]: datascience
[post_id]: 124018
[parent_id]: 
[tags]: 
Is Repeated K-Fold Cross Validation Enough to Evaluate a Machine Learning Model?

I am training models with a small dataset (around 800 observations) and I am using Repeated K-Fold cross validation to evaluate the models. Initially, i am using the same cross validation for hyperparameter-tuning, and then when i have found the optimal hyperparameters, i am training a new model with those parameters using Repeated K-Fold Cross Validation. Do i still need to use a seperate Test set to evaluate the model after all of this, or are the evaluations from the Repeated K-Fold Cross Validation sufficient? I am also worried that if i do need a seperate test set, it will be very small ( Thanks in advance.
