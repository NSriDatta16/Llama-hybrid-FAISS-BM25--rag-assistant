[site]: datascience
[post_id]: 52383
[parent_id]: 
[tags]: 
Understanding softmax layer in Deep Neural Network

In a DNN, once the logits vector is produced say $[y_0,y_1]$ where the number of neuron in the logits layer is 2, the condition holds where $y_0 >= 0$ and $y_1 . This vector is then passed into the softmax layer where it squeezes the value to be within the range of 0 to 1. However, how do i prove that $y_0$ will always be $>=0$ and $y_1$ will always be $ ? From the way i understand, both $y_0 + y_1 = 1$ hence $y_0 >= 0$ and $y_1 but i find that it doesn't really prove on why $y_0 >= 0$ and $y_1 . Could anyone assist me on my understanding.
