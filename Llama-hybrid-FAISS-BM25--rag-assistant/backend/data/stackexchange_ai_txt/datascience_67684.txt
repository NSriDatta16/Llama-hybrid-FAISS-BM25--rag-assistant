[site]: datascience
[post_id]: 67684
[parent_id]: 67641
[tags]: 
Notice that from any complete set, you can form an question-answer pair for your task: you can construct an incomplete set (the "question"), for which you know the desired complete set (the "answer"). In particular, given a complete set $S$ , you can randomly choose a subset $T$ of $S$ of a particular size, and call $T$ the incomplete set. In this way, from your dataset of complete sets, you can form a very large training set of input-output pairs $(T,S)$ . Next, given this training set, I suggest training any ML model to predict $S$ from $T$ . There are many possibilities for how you might instantiate the model. For instance, let's suppose the universe (all possible elements that could appear in a set) is not too large, say $m$ items. Then, you can encode a set using a one-hot encoding. Now, you can construct a neural network whose input is the one-hot encoding of $T$ and whose output is a $m$ -dimensional vector $(p_1,\dots,p_m)$ such that $p_1+\dots+p_m=1$ (this vector can be generated by a final softmax layer). You can now interpret $p_i$ as the probability that item $i$ should be included in the final set. In other words, you can form a final set by including item $i$ with probability $p_i$ and omitting it with probability $1-p_i$ (making an independent choice for each $i$ ). Now, out of the distribution on sets induced in this way, we want to restrict to those of size 40, i.e., condition on the event that the resulting set has size 40. Given a target set $S$ , it is possible to compute explicitly the probability of obtaining that particular set: it is the coefficient of $x^{40}$ in the polynomial $(p_1 x + 1-p_1) (p_2 x + 1-p_2) \cdots (p_m x + 1 - p_m)$ , which can be computed relatively efficiently. It is also a differentiable function of $p_1,\dots,p_m$ . Thus, you can define your loss function to be the negative log of this probability, and you can use stochastic gradient descent to train your network and find parameters that minimize this loss. As an optimization, you might force the output set to automatically include the input $T$ , and use softmax only over the elements not in $T$ .
