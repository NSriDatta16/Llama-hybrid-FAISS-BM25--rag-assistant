[site]: crossvalidated
[post_id]: 129773
[parent_id]: 129769
[tags]: 
In the case of the EM algorithm, the initial values can be set arbitrarily since the iterations are guaranteed to converge to the maximum: We have seen that both the E and the M steps of the EM algorithm are increasing the value of a well-defined bound on the log likelihood function and that the complete EM cycle will change the model parameters in such a way as to cause the log likelihood to increase (unless it is already at a maximum, in which case the parameters remain unchanged). There are several strategies to pick the initial values that will improve the overall performance of the algorithms: Note that the EM algorithm takes many more iterations to reach (approximate) convergence compared with the K-means algorithm, and that each cycle requires significantly more computation. It is therefore common to run the K-means algorithm in order to find a suitable initialization for a Gaussian mixture model that is subsequently adapted using EM. The covariance matrices can conveniently be initialized to the sample covariances of the clusters found by the K-means algorithm, and the mixing coefficients can be set to the fractions of data points assigned to the respective clusters. Bishop, C. M. (2006). Pattern recognition and machine learning (Vol. 1, p. 740). New York: springer. You can read on this in chapter 9 of the book. Using EM to estimate latent variables in the context of Gaussian Mixture Models with two components ($\Delta_i = 0$ and $\Delta_i = 1$), loglikelihood is given by (I'm sorry I'm in a hurry and can't type this myself) ESLII gives some advice on how to select the initial values: A good way to construct initial guesses for $\mu_1$ and $\mu_2$ is simply to choose two of the $y_i$ at random. Both $\sigma_1^2$ and $\sigma_2^2$ can be set equal to the overall sample variance. The mixing proportion $\hat{\pi}$ can be started at the value 0.5. Hastie, T., Tibshirani, R.,, Friedman, J. (2008). The elements of statistical learning: data mining, inference and prediction. Springer. I insist, you can pick any arbitrary initial values, but some will converge faster.
