[site]: datascience
[post_id]: 69913
[parent_id]: 69805
[tags]: 
The tf-explain package supports many interpretability methods. In particular, Grad CAM can "visualize how parts of the image affects neural network's output by looking into the activation maps": from tf_explain.callbacks.grad_cam import GradCAMCallback model = [...] callbacks = [ GradCAMCallback( validation_data=(x_val, y_val), class_index=0, output_dir=output_dir, ) ] model.fit(x_train, y_train, batch_size=2, epochs=2, callbacks=callbacks)
