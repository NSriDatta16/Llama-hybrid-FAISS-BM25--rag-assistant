[site]: crossvalidated
[post_id]: 286606
[parent_id]: 286576
[tags]: 
The answer by Dougal does a good job explaining the empirical distribution. This answer just highlights more the connection between when expectations are sums, and when they are integrals, addressing "explaining how[or when] a summation can be expressed as an expectation [rather vice versa]." Expectations of random variables/vectors are just weighted averages. If your random variable/vector, say $\mathbf{X}$, is discrete with probability mass function $p(\mathbf{x})$, then the average value of $h(\mathbf{X})$ is $$ E[h(\mathbf{X})] = \sum_\mathbf{x} h(\mathbf{x}) p(\mathbf{x}) \tag{1}. $$ If on the other hand $\mathbf{X}$ is continuous, then $$ E[h(\mathbf{X})] = \int h(\mathbf{x}) f(\mathbf{x}) d\mathbf{x}, $$ where $f(\mathbf{x})$ is the probability density function. In your case $\mathbf{x}$ is one particular value/row/observation chosen from your discrete dataset. This dataset, now that it is sitting in memory, is no longer random. Like Dougal explains, you can think about resampling one point/value/row/observation from your pre-existing dataset sitting in memory. Your transformation is $h(\mathbf{x}) = \log p_{\text{model}}(\mathbf{x};\theta)$. And your true probability mass function/weights is $\hat{p}_{\text{data}}(\mathbf{x}) = \frac{1}{m}$ (each row of your data has an equal weight). Plug these into (1) to get: $$ E[\log p_{\text{model}}(\mathbf{X};\theta)] = \sum_{\mathbf{x}} \frac{1}{m} \log p_{\text{model}}(\mathbf{x};\theta) . $$ This also explains why sometimes people refer to maximum likelihood estimators as minimum cross entropy estimators. This is different than the situation of classical statistics where they talk about data as being random, instead of the resampled values as being random. In the classical framework, once the data arrives, plugging your non-random datapoints into you pdf/pmf gives you a "likelihood." This new word is used to stress that this function is to be thought of as a function in the parameters . Classical statistics talks about maximizing the likelihood, which is computationally equivalent, but they draw the distinction about what is random and what is non-random differently.
