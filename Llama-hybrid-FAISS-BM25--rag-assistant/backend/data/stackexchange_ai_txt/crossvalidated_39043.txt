[site]: crossvalidated
[post_id]: 39043
[parent_id]: 39037
[tags]: 
You may have heard it said that neural networks are " universal function approximators ". In essence, the Cybenko theorem says that for any function mapping reals to reals, you can approximate it with a neural network with sigmoid activation functions. In fact, it turns out that neural networks allow you to compute any function which is computable by a Turing machine (ie anything you can write an algorithm to compute). Unfortunately, these proofs only say that for some finite configuration of neurons and weights, you can approximate any function. The theory is all nice and dandy, but your question seems to be more along the lines of how to actually encode the computation of some function into a set of neurons and weights. To illustrate, consider a simple example - the exclusive-or. The XOR takes two inputs, passes those inputs. When one and only one of the inputs are activated, then the output node is activated. With both or none of the inputs are activated, then the output node is not activated. Notice that the three hidden nodes do different things. The left most and right most nodes simply pass through the respect input nodes activations. The middle neuron takes the two inputs and somehow negates them if they are both on. This clever combining and recombining of inputs is essentially how work in a neural network is done. Obviously for more complex functions the combining and recombining must be done in more clever and complicated ways, but this is in essence what happens at a low level. The crazy thing is that this is really all you need to compute any computable function! Then again, turing machines also turn out to be deceptively simple... The problem is that we don't really have a way to magically generate the neural network which computes some arbitrary function. The proofs only tell us that there is some network out there that could do it. When we train our neural networks, we are simply trying to find a network which is pretty close. In the context of image recognition, you could imagine encoding patterns into the network. For example, to recognize the number '1', you could imagine a hidden nodes which expect a column of pixels to be mostly or all activated, with neighboring pixels to be off. This hidden node could be fairly good at recognizing a straight line in that particular column. Put enough of these together and pretty soon you've got a bunch of nodes that do it in enough places of your image that if I show the network a one, enough straight line hidden nodes will be activated, indicating a '1'. The problem of course become generalizing the network so it can recognize a varied set of inputs. Hopefully this helps you understand more or less the concepts of how a neural network can perform computations. However, you've hit upon a point which is rather important about neural networks: in general it is difficult at best to understand why the network spit out a particular output, especially when you consider that for something like image recognition, the networks are generally big enough that humans have a tough time comprehending each of the moving parts of the machine. Further complicating the matter is that in general most neural networks do not actually have a single hidden node for each little feature the network could learn about the data. Instead, detecting something like a straight line to classify the number '1' would take place in a non-centralized manner over many hidden nodes. Other algorithms, such as decision trees, are much nicer in this respect. If you are looking for more reading, I highly recommend reading through this tutorial over at ai junkie. It walks you through the basics of how a neural network works, and even gives a simple code example getting neural networks to drive a tank towards a goal. The tutorial does not however cover backpropagation, which is by far the most common way of training neural networks, and instead uses a simple genetic algorithm. Once he starts talking genetics, I guess you can stop reading...
