[site]: crossvalidated
[post_id]: 622950
[parent_id]: 622878
[tags]: 
Here are a few considerations, ordered via the thinking-out-loud principle. (i) Using k=2 the resulting data object can be thought of as a (two-dimensional) matrix. We have a huge number of tools to deal with matrices. We are used to two-ary operations (addition, multiplication, inner product ..) (ii) Using k=3 hugely increases the number of data points as you say, and there are no generalised tools that I'm aware of to deal with say a three-dimensional distance matrix. (iii) Generally k=2 implicitly encodes a lot of information, for example if f is a metric distance there are strong constraints on the three values f(x_i, x_j), f(x_i, x_k), f(x_j, x_k) . There is scope for enormous redundancy if you consider all triples g(x_i, x_j, x_k) given a three-ary function g . (iv) my hunch is if something could be gained the likelihood is that someone would have investigated it. I struggle to find ideas for using triplets (or beyond), where the redundancy mentioned above (iii) is probably my key intuition. Supplied with a three-ary function g(x_i, x_j, x_k) , the first thing that comes to my mind is to simplify it into a two-ary function f(x_i, x_j) by holding x_i and x_j constant and integrating/averaging g over varying x_k . I am not sure that your question whether k=2 is optimal is actually meaningful. I would first like to see at least one example of an actual three-ary function g that we can imagine to have some form of utility in clustering.
