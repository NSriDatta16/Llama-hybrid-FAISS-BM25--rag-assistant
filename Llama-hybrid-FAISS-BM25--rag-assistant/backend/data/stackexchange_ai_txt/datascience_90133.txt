[site]: datascience
[post_id]: 90133
[parent_id]: 89884
[tags]: 
It seems that the SeqSelfAttention layer is expecting all the time-steps. i.e. return_sequences=True Same is shown in the home page example. Link import tensorflow as tf, numpy as np from tensorflow import keras from tensorflow.keras.layers import Dense, Dropout,Bidirectional,Masking,LSTM from keras_self_attention import SeqSelfAttention X_train = np.random.rand(700, 50,34) y_train = np.random.choice([0, 1], 700) X_test = np.random.rand(100, 50, 34) y_test = np.random.choice([0, 1], 100) model = tf.keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2]))) model.add(keras.layers.Bidirectional(LSTM(20, dropout=0.25, recurrent_dropout=0.1, return_sequences=True))) model.add(SeqSelfAttention(attention_activation='sigmoid')) model.add(keras.layers.Flatten()) model.add(Dense(1, activation='sigmoid')) model.summary() model.compile(loss='binary_crossentropy') model.fit(X_train,y_train,epochs=2)
