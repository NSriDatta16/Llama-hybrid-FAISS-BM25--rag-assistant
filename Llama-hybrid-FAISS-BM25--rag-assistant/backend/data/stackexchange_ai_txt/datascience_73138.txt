[site]: datascience
[post_id]: 73138
[parent_id]: 
[tags]: 
Relu with not gradient vanishing function is possible?

I'm beginner in ML. In the ANN, relu has the gradient of 1 in x>0 how ever, i wonder in x= if activation function like y=x(for the all the x) has no gradient vanishing problem, why we dose not use this function in deep neural networks? Is there any side effect for y=x(for all x)? (maybe, the weight may go infinity in deep neural networks...... however, I think this problem is also being happen in ReLU. so it is not a problem(I think.))
