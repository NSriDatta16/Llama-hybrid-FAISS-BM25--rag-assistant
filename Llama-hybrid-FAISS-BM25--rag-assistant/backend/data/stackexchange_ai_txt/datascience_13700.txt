[site]: datascience
[post_id]: 13700
[parent_id]: 13682
[tags]: 
One intuitive answer as to why logistic function comes up is to look at Logistic Regression from the angle of generative model, which results in Linear Discriminant Analysis model. Basically, the idea is that instead of directly modeling the likelihood $p(y|x)$ like in logistic regression . You model the class-conditional $p(x|y)$ and $p(y)$, then derive the output $p(y)$ via Bayes' rule. $$ p(y|x) = \frac{p(x|y) p(y)}{p(x)} $$ It turns out that if you model the input by a Gaussian distribution or any distribution in the exponential family (with the same dispersion parameter for two classes), then your likelihood is a logistic function of $x$ $$ p(y = 1 | x) = \frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)}$$ where $$ p(x|y=1) = \mathcal{N(\mu_1, \Sigma)} $$ $$ p(x|y=1) = \mathcal{N(\mu_0, \Sigma)} $$ after some simplification $$ p(y=1|x) = \frac{1}{1 + exp(-w^Tx - b)} $$ where $ w = \Sigma^{-1}(\mu_0 - \mu_1) $ That explains why the logistic function turns up
