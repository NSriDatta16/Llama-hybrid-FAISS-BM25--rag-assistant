[site]: crossvalidated
[post_id]: 47817
[parent_id]: 14002
[tags]: 
PCA yields the EXACT same results as classical MDS if Euclidean distance is used. I'm quoting Cox & Cox (2001), p 43-44: There is a duality between a principals components analysis and PCO [principal coordinates analysis, aka classical MDS] where dissimilarities are given by Euclidean distance. The section in Cox & Cox explains it pretty clearly: Imagine you have $X$ = attributes of $n$ products by $p$ dimensions, mean centered PCA is attained by finding eigenvectors of the covariance matrix ~ $X'X$ (divided by n-1) -- call the eigenvectors $\xi$, and eigenvalues $\mu$. MDS is attained by first converting $X$ into distance matrix, here, Euclidean distance, i.e., $XX'$, then finding the eigenvectors -- call the eigenvectors $v$, and eigenvalues $\lambda$. p 43: "It is a well known result that the eigenvalues of $XX'$ are the same as those for $X'X$, together with an extra n-p zero eigenvalues." So, for $i Going back to definition of eigenvectors, consider the $i^{th}$ eigenvalues. $X'Xv_i = \lambda_i v_i$ Premultiply $v_i$ with $X'$, we get $(X'X)X'v_i = \lambda_i X'v_i$ We also have $X'X \xi_i = \mu_i \xi_i$. Since $\lambda_i = \mu_i$, we get that $\xi_i = X'v_i$ for $i
