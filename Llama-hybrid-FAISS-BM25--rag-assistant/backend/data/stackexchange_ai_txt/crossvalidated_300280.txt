[site]: crossvalidated
[post_id]: 300280
[parent_id]: 
[tags]: 
How to fit Anomaly Detection model to imbalanced time series data(Zero Inflected time series data)?

I'm doing a predictive modeling for predicting anomalies in the sensors data. For this I'm using the twitter AnomalyDetection package in R. We are getting ton of data from sensors for every day. For every 5 sec we get a one data point from sensor. we are averaging this data over 2 min. We have a one sensor which sends the data between 0(min value for sensor) and 12(max value for sensor). But most of the times sensor sends the zero's(~98%). We are building AnomalyDetection model on this data. What I think, this model trains on the most of the zeros's. When we used this model for new data points it gives an false alarm even if new data point lies between min and max of sensor value. For example : Descriptive statistics for this sensor data as below Because of the most of the data points are zero's summary statistics look like this. Below is the plot of the data by day. What I believe that if we give any value between min and max of sensor value to the model, model does not have to declare it anomaly but the trained model declaring it as an anomaly point. so it is false alarm. So how to train the model on less event data? Here event is point out of the control limit.
