[site]: datascience
[post_id]: 27277
[parent_id]: 
[tags]: 
Faster-RCNN how anchor work with slider in RPN layer?

I am trying to understand the whole Faster-RCNN, From https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work Then a sliding window is run spatially on these feature maps. The size of sliding window is n×n (here 3×3). For each sliding window, a set of 9 anchors are generated which all have the same center (xa,ya)(xa,ya) but with 3 different aspect ratios and 3 different scales as shown below. Note that all these coordinates are computed with respect to the original image. It is much more clear than other articles for my opion, but still hard to understand how the feature map generate. I saw another flow pics : The problem, I write below steps for example: If input is 600x1000x3 pic Through VGG16 convnet , layer 13 output feature map is 40x60x512 Use a 3x3 sliding window, generate 1x1x512 feature map ??? Here, how 3x3 sliding window use a set of 9 anchors ??? Sorry, I am really new to object detection and image proccessing. I only have a little understand about the steps, I known 9 anchor shapes (not the real anchor) are used to generate a lot of anchors(2400*9 in this case). I can only imagine that use 9 anchor shape to slide the original image to get the all IoU . I don't understand how to use 3x3 sliding window in conv feature map here. I know how anchors be selected, 2400*9 -> ignore cross-boundary -> 6000 -> apply NMS -> 2000 , in each minibatch, it randomly choose 512 anchors from 2000. What I can't understand is 3x3 slide with 9 anchor shape . Because from original paper, anchors with is 16, height from 11 to 273 . I don't think it use the 13 layer conv output feature map to calculate IoU . Anchor must be apply in original image, so what is the 3x3 sliding window doing ??
