[site]: crossvalidated
[post_id]: 341959
[parent_id]: 
[tags]: 
In MDPs with deterministic actions, should I use Q-learning or TD(0)?

Suppose in an Markov Decision Process (MDP), we have transition $(s, a, r, s', a', r', s'', ...)$, learning rate $\alpha$ and discount factor $\lambda$. The update formula of $TD(0)$: $V(s) \leftarrow (1-\alpha)V(s)+\alpha(r+\gamma V(s'))$ The update formula of Q-learning: $Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha (r + max_a' Q(s',a'))$ If in the MDP applying any action on any state will deterministically lead to another state, should I use Q-learning (off-policy) or $TD(0)$ (on-policy)? Furthermore, If we use function approximation for both $V(s)$ and $Q(s,a)$, I can imagine that $V(s)$ can directly input the feature representation of the state $s$, while $Q(s,a)$ can input the feature representation of the next state $s'$. If so, you will see the update formula Q-learning will only differ slightly with $TD(0)$: $Q(s') \leftarrow (1-\alpha)Q(s') + \alpha(r+\max_{s''} Q(s''))$. I see people have applied $TD(0)$ (or $TD(\lambda)$ in many board games with deterministic actions such as Go [1] and BackGammon [2]. Why not Q-learning? What is the difference between the two methods in such case? Reference [1] Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning of position evaluation in the game of Go. In Advances in Neural Information Processing Systems (pp. 817-824). [2] Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3), 58-68.
