[site]: crossvalidated
[post_id]: 362112
[parent_id]: 
[tags]: 
Machine Learning on Extremely Low Signal Data

I have terabytes of data with an extremely low signal to noise ratio, with the following characteristics: The relationship between the features and the response variable can change over time I'm trying to predict an event in the future, where the trigger for the event may happen between the time of prediction and when the event happened, so the best you can do is a probabilistic estimate There is therefore noise in the response variable I have thousands of features, most of which are probably irrelevant, but at times become relevant to the concept I'm trying to learn Many of the features are binary triggers, which can be highly indicative of an event happening in the future, but the feature is mostly sparse Adding more data does not necessarily help, as it just adds more noise This kind of data is different to the data on which machine learning typically does well, where The target concept is fixed ==> a cat always looks like a cat You're predicting something instantaneous ==> is there a cat in this photo vs might a cat appear some time in the future Has there been any decent research been published which addresses this general problem, or is it even an active field of research yet?
