[site]: crossvalidated
[post_id]: 509406
[parent_id]: 
[tags]: 
Skipgram model theory confusion

In the output layer of a skipgram model, there are $|\text{Context}|*|\text{Vocab}|$ values. And for each context word, the values are basically the dot product of the input word representation and all output word representations. Now, we just compute the softmax over these values and try to maximise the corresponding product of context probabilities. But shouldn't cosine similarity be a better metric than dot product for this? (Cosine similarity is normalised dot product). So why do people argue that dot product should be high for the relevant context words, where in the reality, it should be the cosine similarity which should be high. Also, suppose we don't use any approximation techniques for the model, and compute all the softmax values accurately. So isn't the second weight matrix as good as the first one in this case? (Normally, I have read that for obtaining the word embeddings, the first weight matrix is used, but why can't output one be used?)
