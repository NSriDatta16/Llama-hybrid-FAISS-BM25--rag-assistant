[site]: crossvalidated
[post_id]: 275122
[parent_id]: 275117
[tags]: 
It looks like you trained and tested on the same set (the 300 examples you chose). This is generally not recommended, and the results don't mean much. Make sure to set aside about 10% of the data, for testing. Then, you can train on your train set, and test on the test set, and get a more accurate evaluation of your error/accuracy. Do bear in mind, that if your test set has the same distribution as your data (1 positive to 12 negatives), a "stupid" classifier which always classifies 'no' will get 92.5% accuracy. Then, 79% accuracy would be bad. Look at other measures like roc-auc to better asses your classifier, in imbalanced cases. EDIT: As mentioned in the comments, roc-auc might not be the best option. AUC-PR could be better in highly skewed datasets (such as yours). I also like f1-macro, which is an unweighted average of both labels' f1-scores.
