[site]: crossvalidated
[post_id]: 642263
[parent_id]: 642254
[tags]: 
When you add Gaussian noise, the smallest and largest observations are no longer sufficient for and MLEs of $a,b$ . Instead consider the likelihood which, in the usual way, is given by the product of the density of each $\mathbf{y}_i$ , in this case given by \begin{align} f(\mathbf{y})&=\int_a^b f(\mathbf{y}|z)f(z)dz \\&=\frac1{2\pi\sigma_v^2}\int_a^b e^{-\frac1{2\sigma_v^2}[(y_1-z)^2+y_2^2]}\frac1{b-a}dz \\&=\frac1{\sqrt{2\pi}\sigma_u}e^{-\frac{y_2^2}{2\sigma_u^2}}\frac{\Phi(\frac{y_1-a}{\sigma_u})-\Phi(\frac{y_1-b}{\sigma_u})}{b-a}, \end{align} where $\Phi$ is the standard normal cdf. The resulting likelihood (or its log) should be straightforward to maximise numerically with respect to $a, b, \sigma_u^2$ . I would expect the MLEs of $a$ and $b$ to be biased, however, just like in the case of uniform observations without Gaussian noise, so some form of bias correction may be needed. An alternative is Bayesian inference, perhaps using an improper uniform prior $\pi(\theta_1)\propto 1$ on $\theta_1=\frac{a+b}2$ and an independent scale prior $\pi(\sigma_u)\propto \frac1{\sigma_u}$ on $\sigma_u$ . It would be tempting to use an independent, improper scale prior also on $\theta_2=b-a$ but I believe this would make the posterior improper (as in Hobert (1996) ) as the likelihood tends to a positive limit as $\theta_2 \rightarrow 0$ . A sensible way around this would be to assume that $\sigma_u$ and $\theta_2$ instead are of similar orders of magnitude, e.g. by letting $\pi(\theta_2|\sigma_u)$ be lognormal with a suitable variance. The argument for the conditional informative prior $\pi(\theta_2|\sigma_u)$ also applies if $\sigma_u$ is known. Thus, completely "objective" Bayesian inference does not seem feasible for this model.
