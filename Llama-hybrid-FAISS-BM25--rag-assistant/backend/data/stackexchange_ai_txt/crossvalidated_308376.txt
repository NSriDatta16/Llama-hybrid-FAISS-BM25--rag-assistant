[site]: crossvalidated
[post_id]: 308376
[parent_id]: 
[tags]: 
Best Practice for choosing hyperparameters for stacking/majority voting

My goal is to get the best hyperparameters for the classifiers I used in Stacking/Majority Voting. This is the steps that I did (with very small Titanic dataset): I use GridSearchCV to tune each classifiers (logreg, Random Forest, XGB, etc) I use the best performing classifier, with the best hyperparameters from step 1. I always use logreg as the 2nd classifier when Stacking My questions are : Is it considered "best practice" to use the best hyperparameter of each classifier for Stacking/Majority Voting? I found out that its possible to do GridSearchCV when Stacking (with mlxtend ), so the chosen hyperparameters is the best for Stacking, not the best for each classifier (as opposed to the 1st point). However, is it worthed the effort? I believed the computational cost will be huge because there will be a lot of hyperparameters combination for GridSearchCV to try. Or do I missing something important here? Please kindly give me your practical advice My current Stacking yields the best result, but only differ +- 1% from standalone advanced classifier (Random Forest/XGBoost)
