[site]: datascience
[post_id]: 112997
[parent_id]: 112988
[tags]: 
At first, I would have recommended a next-word generation with a model like GPT-2, but it seems more like a mask-filling topic with models like BERT. BERT is actually great for finding the most probable fields according to input with specific positions. Hence, after training a BERT model from scratch with plenty of addresses like this, you can ask to fill masks: import torch from transformers import BertTokenizer, BertModel,BertForMaskedLM tokenizer = BertTokenizer.from_pretrained('my_model') #Field 1: address, Field 2: city, field 3: state input_txt = "Hartford Avenue [MASK] [MASK] " inputs = tokenizer(input_txt, return_tensors='pt') Note that BERT requires a continuous text field to get the correlations between words and you can extract those words from the fields, and after the Bert predictions, collect the predicted words to copy them to the correct fields, if possible as proposals.
