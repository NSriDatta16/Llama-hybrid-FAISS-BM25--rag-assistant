[site]: crossvalidated
[post_id]: 345198
[parent_id]: 345188
[tags]: 
I'll try to answer this in three parts to address: 1) Maximum Likelihood, 2) The role of ML in neural networks, 3) Optimizers (e.g., Gradient Descent) I think its best to view Maximum Likelihood as a Principal, rather than an estimator. MLE is simply a manifestation of using the ML principal in a specific problem (example estimating loading vectors in factor analysis). From this top-down perspective, a neural network falls in the same category as any other modeling tool (like factor analysis). Now, in order to find the parameters of the neural network, you need to optimize a loss function. In deriving said loss function (aka objective), you can use the Maximum Likelihood Principal, which maximizes the likelihood of the data given the neural network parameters. When the objective is formulated for neural networks, it cannot be solved using a simple closed form (or semi-closed form) set of estimating equations (see Godambe 1991 ). That's where gradient descent (or other optimization tools) come into picture.
