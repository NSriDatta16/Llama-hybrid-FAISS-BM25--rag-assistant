[site]: crossvalidated
[post_id]: 392985
[parent_id]: 392898
[tags]: 
Looking at it more closely, In policy gradients, we subtract something called a 'baseline', which helps reduce the variance of the estimator. Since you are using the discounted reward, subtracting the mean says at every step, if I got less than the average, penalize that action, otherwise encourage it. The scaling might just result in another equivalent step size. Generally one simple baseline is mean of rewards obtained so far throughout the game, but I guess these guys are considering only this batch for some reason. As for your 3rd point, what you are actually estimating is the discounted sum over $r_i$ as you said. The recursive equation helps you compute the discounted reward starting from $i^{th}$ timestep till the end of the episode. We only have access to the per step rewards, so if you want the discounted sum for every timestep $i$ till end of the episode, we do this recursive estimation. A naive way to do that would be with $n$ for loops, each going from index $i$ till $n$
