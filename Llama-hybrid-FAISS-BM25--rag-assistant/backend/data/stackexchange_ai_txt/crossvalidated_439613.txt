[site]: crossvalidated
[post_id]: 439613
[parent_id]: 
[tags]: 
Reference: Data-Dependent Early Stopping Criterion for Deep Learning?

In the context of non-parametric regression, this paper provides an data-dependent rule for optimal early stopping, when learning an unknown function $f^{\star}$ lying in some RKHS . Here, one stops the gradient descent procedure which minimizing the loss-function $$ \frac1{n} \sum_{i=1}^n \left( f^{\star}(x_i) - f(x_i|w) \right) , $$ where $f(\cdot|w)$ also belongs to the same RKHS with reproducing kernel $K$ and therefore by the representor theorem admits the convenient form $$ f(\cdot)=\sum_{i=1}^n w_i K(\cdot,x_i) . $$ Is there an analogous data-dependent early stopping rule for feed-forward neural networks with fixed depth $d>1$ and fixed width $W>1$ ?
