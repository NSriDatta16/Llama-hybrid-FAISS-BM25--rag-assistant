[site]: crossvalidated
[post_id]: 237938
[parent_id]: 
[tags]: 
Optimization metric in Generative Adversarial Networks

The paper on GANs contains the following excerpt: Early in learning, when $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\log(1 − D(G(z)))$ saturates. Rather than training $G$ to minimize $\log(1 − D(G(z)))$ we can train $G$ to maximize $\log D(G(z))$. This objective function results in the same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning. I was hoping for some clarification on why using the new metric fixes the saturation problem. Doesn't the exact same problem still exist? $G$ is poor so $D$ can still reject samples with a high confidence because they are clearly different from the training data.
