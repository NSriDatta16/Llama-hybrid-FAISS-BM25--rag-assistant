[site]: crossvalidated
[post_id]: 424901
[parent_id]: 
[tags]: 
How to cut features with large amount of 0 values from high dimensional data?

I have genomic data (miRNA) that is high dimensional: $198$ samples and $1584$ features. Index miRNA1 miRNA2 .... miRNA1500 Type 1 48421.52 24242.14 .... 0 Tumor 2 2757.96 28965.2 .... 0 Healthy 3 4300.34 52565.07 .... 6981.41 Healthy ... ... .... ... 198 23854.73 24722.28 .... 0 Tumor $58.5\%$ of these features have more than $90\%$ of values being a $0$ . At the beginning I just wanted to cut all of those so that when I put the remaining ones into SVM, LASSO, Random forest or another model that can perform feature selection it will be less computationally expensive. However, I browsed some of these features and it turns out that even though they are present in only around $10$ samples from $200$ , they seem pretty informative since the proportions are for example $9$ samples classified as Tumor and $1$ as Healthy which can indicate that although most of samples have a $0$ value, if the value is present then it might be an indication for Tumor. In the end I only want to retain max $20$ features, so these $0$ -valued features will probably turn out to not score top20 anyway and I can just cut them. However there might be some hidden information, for example for every $0$ value in miRNA200, miRNA201 must have a non- $0$ value if a person is healthy and such information would be lost. In short : What are the approaches for cutting out such features that are present in small amount of samples? quick edit: What about features that only have $1$ or $2$ non- $0$ values? Can we just cut them? What would be a threshold of non- $0$ values to decide which features can be cut automatically and which not? edit2: The data is most likely not missing completely at random, therefore removing anything could introduce some bias. However I assume this bias would be of marginal importance in comparison to bias introduced by further operations (proper features selection techniques)?
