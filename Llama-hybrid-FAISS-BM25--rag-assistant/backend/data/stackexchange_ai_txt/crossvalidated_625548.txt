[site]: crossvalidated
[post_id]: 625548
[parent_id]: 
[tags]: 
Linear Regression with Learned Feature Selection?

I have a question related to linear regression that stems from my research. Suppose I have two real matrices: Design matrix $X \in \mathbb{R}^{N \times D}$ Target matrix $Y \in \mathbb{R}^{N \times E}$ For concreteness, this data might arise from students taking an exam with many questions. We have $N$ students with $D$ covariates (age, family income, parental education, etc.) and the exam has $E$ questions. What people in my field do is first take an average over the $E$ columns of $Y$ : $$ \bar{Y} := Y w,$$ where $w := \frac{1}{E}\mathbb{1} \in \mathbb{R}^E$ , and then fit an ordinary least squares model with $\beta \in \mathbb{R}^D$ : $$\beta^* = \arg\min_{\beta} \; (X \beta - \bar{Y})^T (X \beta - \bar{Y})$$ However, this approach assumes that each of the exam questions matters equally. I'm wondering if we can relax this assumption to improve generalization by learning feature selection of the targets in the following manner: Suppose we add constraints like (i) $||w||_1 = 1$ and (ii) $w_e \geq 0$ and instead minimize the following loss function: $$\beta^*, w^* = \arg \min_{\beta, w} \; (X \beta - Y w)^T (X \beta - Y w) \quad \text{ s.t. } \quad ||w ||_1 = 1, 0 \leq w_e$$ My questions are: Does this alternative least squares formulation have a solution? If so, what is that solution? Is the solution unique? Assuming a solution exists, what properties will this solution have? Is it possible to show that that this second formulation will have lower test squared error than the first formulation?
