[site]: crossvalidated
[post_id]: 328059
[parent_id]: 
[tags]: 
t-test vs Mannâ€“Whitney U test strongly differ in p-value

I have a time series that is quite erratic, having lots of small increases and then large drops. I want to see if the mean is statistically significantly different to zero. Using a t-test, I get a p-value of around 0.6, but using a wilcoxon ranksum test I get a p value of 0.0001. How is this possible? I think the p-value of the t-test makes much more sense as I have only around 500 sample values, most of then +2 and some of them -200 so that the cumulative return of all of them is around 274. It should not be statistically significantly different to zero. Why does the ranksum test say otherwise? Why does it differ so much from the t-test? df['returns'].fillna(0).std() 23.348037669415067 df['returns'].fillna(0).var() 545.1308630124249 df['returns'].fillna(0).mean() 0.5494399999999998 df['returns'].fillna(0).min() -274.0 df['returns'].fillna(0).max() 93.04 df['returns'].fillna(0).median() 4.0 last value of cumsum: 274.72 stat, p_stat = ranksums(df['returns'].fillna(0).values, np.zeros(len(df['returns'].values))) p_stat 2.9026104353437864e-32 stat, p_stat = ttest_ind(df['returns'].fillna(0).values, np.zeros(len(df['returns'].values))) p_stat 0.598862760915118 The p_stat of the t-test is 0.6, but the p_stat of the ranksums is 2.9e-32. How can that be? For t-test I'm using scipy.stats.ttest_in d and for ranksums I use scipy.stats.ranksums .
