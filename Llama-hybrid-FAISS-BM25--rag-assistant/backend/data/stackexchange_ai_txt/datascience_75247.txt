[site]: datascience
[post_id]: 75247
[parent_id]: 75240
[tags]: 
The cost function is the judge for your model. It judges how well your model perfoms. By choosing a loss function you choose which properties of your model outputs the loss function will judge. Mathematical convenience usually is desired for the loss function to be applicable The MSE will punish outputs that are further away from the desired value more severely than those that are closer because of the quadratic. Therefore, the outputs that are the furthest away from the desired value impact the cost function and thus the optimization greatly. If your dataset has many outliers, then these can influence your cost function a lot. In this case you could use techniques like dimensionality reduction or just choose another loss function like the $L_1$ loss, which is more robust to outliers. Furthermore, if you choose MSE you implicitly assume that the noise of your data is guassianly distributed, since the MSE loss will minimize the crossentropy between the empirical distribution of your outputs and the gaussian distribution. See https://stats.stackexchange.com/questions/288451/why-is-mean-squared-error-the-cross-entropy-between-the-empirical-distribution-a The logistic regression cost function will judge the outputs of your model by other characteristics. The logistic regression intends to classify your outputs correctly into two classes. The output of your model lies between $[0, 1]$ the respective classes. Once outputs lie on the correct side of the decision boundary their impact on the lossfunction decrease rapidly with the distance to the decision boundary. Look at the logistic curve and observe where points are closest to the classes ( $0$ or $1$ ) and then check out where the loss function is the steepest for each class. So the cost function of the logistic regression will punish those outputs severly that are missclassified, and those, although a bit less, that are correctly classified but lie very close to the decision boundary. As a result it is a decent loss function for binary classification. There are probably a few further implicit assumptions with logistic regression, however I have not dug into that in the past to be able to say what they are. Since one usually assumes that the data is i.i.d (identically, idependently distributed: each datapoint is to be treated equally), the loss for each output is summed up. In summary: Loss functions are chosen to judge the desired properties of the model outputs, and usually exhibit favorable mathematical qualities for the optimization. By choosing the loss function you can alter the criteria your optimization should follow and often times thus introduce prior knowledge.
