[site]: datascience
[post_id]: 37469
[parent_id]: 
[tags]: 
Tensorflow dense layers worse than keras sequential

I try to train an agent on the inverse-pendulum (similar to cart-pole) problem, which is a benchmark of reinforcement learning. I use neural-fitted-Q-iteration algorithm which uses a multi-layer neural network to evaluate the Q function. I use Keras.Sequential and tf.layers.dense to build the neural network repectively, and leave all other things to be the same. However, Keras gives me a good results and tensorflow does not. In fact, tensorflow doesn't work at all with its loss being increasing and the agent learns nothing from the training. Here I present the code for Keras as follows def build_model(): model = Sequential() model.add(Dense(5, input_dim=3)) model.add(Activation('sigmoid')) model.add(Dense(5)) model.add(Activation('sigmoid')) model.add(Dense(1)) model.add(Activation('sigmoid')) adam = Adam(lr=1E-3) model.compile(loss='mean_squared_error', optimizer=adam) return model and the tensorflow version is class NFQ_fit(object): """ neural network approximator for NFQ iteration """ def __init__(self, sess, N_feature, learning_rate=1E-3, batch_size=100): self.sess = sess self.N_feature = N_feature self.learning_rate = learning_rate self.batch_size = batch_size # DNN structure self.inputs = tf.placeholder(tf.float32, [None, N_feature], 'inputs') self.labels = tf.placeholder(tf.float32, [None, 1], 'labels') self.l1 = tf.layers.dense(inputs=self.inputs, units=5, activation=tf.sigmoid, use_bias=True, kernel_initializer=tf.truncated_normal_initializer(0.0, 1E-2), bias_initializer=tf.constant_initializer(0.0), kernel_regularizer=tf.contrib.layers.l2_regularizer(1E-4), name='hidden-layer-1') self.l2 = tf.layers.dense(inputs=self.l1, units=5, activation=tf.sigmoid, use_bias=True, kernel_initializer=tf.truncated_normal_initializer(0.0, 1E-2), bias_initializer=tf.constant_initializer(0.0), kernel_regularizer=tf.contrib.layers.l2_regularizer(1E-4), name='hidden-layer-2') self.outputs = tf.layers.dense(inputs=self.l2, units=1, activation=tf.sigmoid, use_bias=True, kernel_initializer=tf.truncated_normal_initializer(0.0, 1E-2), bias_initializer=tf.constant_initializer(0.0), kernel_regularizer=tf.contrib.layers.l2_regularizer(1E-4), name='outputs') # optimization # self.mean_loss = tf.losses.mean_squared_error(self.labels, self.outputs) self.mean_loss = tf.reduce_mean(tf.square(self.labels-self.outputs)) self.regularization_loss = tf.losses.get_regularization_loss() self.loss = self.mean_loss # + self.regularization_loss self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss) The two models are the same. Both of them has two hidden layers with the same dimension. I expect that the problems may come from the kernel initialization but I don't know how to fix it.
