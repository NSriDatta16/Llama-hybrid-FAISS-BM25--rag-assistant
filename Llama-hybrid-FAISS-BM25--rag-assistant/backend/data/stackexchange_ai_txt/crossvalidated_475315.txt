[site]: crossvalidated
[post_id]: 475315
[parent_id]: 474767
[tags]: 
Scoring rules assess the quality of a probabilistic forecast; i.e. a prediction with some uncertainty measure associated to it. This could be something simple like a mean and standard deviation, or it could be a full probability distribution (or something in between!). The idea behind a (proper) scoring rule is to encourage 'honest' probabilistic predictions. Suppose I am estimating an unknown parameter $\theta$ by some probability distribution $P(\hat{\theta})$ , and suppose we are using a positively oriented score (bigger is better). I will increase my score if The mean implied by $P(\hat{\theta})$ is close to $\theta$ and the uncertainty is relatively small The mean implied by $P(\hat{\theta})$ is far from $\theta$ but my uncertainty is relatively large If I get small uncertainty with large error, I will have poor score. Likewise, an accurate but uncertain forecast will be penalised. Essentially, I am trying to create a well-calibrated forecast. I am embracing uncertainty, and trying to identify an appropriate amount of uncertainty in my predictions.
