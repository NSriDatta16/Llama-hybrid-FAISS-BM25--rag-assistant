[site]: crossvalidated
[post_id]: 462813
[parent_id]: 462806
[tags]: 
The loss function should be either: $$ E = \sum_i^{N_{samples}} \text{cross_entropy}(x_i,y_i) + \lambda \sum_j^{N_{params}} w_j^2 $$ Or the averaged version: $$ \bar E = \frac{1}{N_{samples}} E \\ = \frac{1}{N_{samples}} \sum_i^{N_{samples}} \text{cross_entropy}(x_i,y_i) + \frac{1}{N_{samples}} \lambda \sum_j^{N_{params}} w_j^2 $$ If you only average over the cross entropy part, then your model won't be able to learn more from data when sample size is increased. If you only average over the L2 part, then your model won't increase the penalty when the number of parameters is increased.
