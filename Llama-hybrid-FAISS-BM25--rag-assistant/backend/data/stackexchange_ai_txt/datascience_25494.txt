[site]: datascience
[post_id]: 25494
[parent_id]: 
[tags]: 
What network is called high-capacity network? Why?

In this paper deep_image_prior : To demonstrate the power of this parametrization, we consider inverse tasks such as denoising, super-resolution and inpainting. These can be expressed as energy minimization problems of the type where E(x; x0) is a task-dependent data term, x0 the noisy/low-resolution/occluded image, and R(x) a regularizer. The choice of data term E(x; x0) is dictated by the application and will be discussed later. The choice of regularizer, which usually captures a generic prior on natural images, is more difficult and is the subject of much research. As a simple example, R(x) may be the Total Variation (TV) of the image, which encourages solutions to contain uniform regions. In this work, we replace the regularizer R(x) with the implicit prior captured by the neural network, as follows: Then I see : A parametrization with high noise impedance. One may wonder why a high-capacity network fθ can be used as a prior at all. Here, I wonder why they call fθ high-capacity network ? What is the definition? Google just lead me to many non-relevant pages.
