[site]: crossvalidated
[post_id]: 437769
[parent_id]: 309642
[tags]: 
In the paper Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning , Yarin Gal and Zoubin Ghahramani argue the following In classification, predictive probabilities obtained at the end of the pipeline (the softmax output ) are often erroneously interpreted as model confidence. A model can be uncertain in its predictions even with a high softmax output (fig. 1). Passing a point estimate of a function (solid line 1a) through a softmax (solid line 1b) results in extrapolations with unjustified high confidence for points far from the training data. $x^*$ for example would be classified as class 1 with probability 1. Here's figure 1. So, if we interpret the outputs of the softmax as model uncertainty or confidence, the model is highly confident for point $x^*$ , even though no training data was observed in that region, but this can be misleading, because the true function, in that region, could be completely different than the learned one (the solid black line).
