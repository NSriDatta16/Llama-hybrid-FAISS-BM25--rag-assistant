[site]: crossvalidated
[post_id]: 548509
[parent_id]: 548471
[tags]: 
The Fisher information matrix tells you how much information there is in each observed value about your parameters. If your observations are independent, then the information in $n$ samples is $n$ times the Fisher information matrix. The inverse of the Fisher information matrix is a lower bound on the covariance of your (unbiased) estimate (Cramer-Rao bound). So if you know how accurately you want to measure your parameters, you can invert that and divide by the elements of the Fisher Information to get a rough estimate for $n$ . If your estimators are not efficient you may need more. There's an R package mle.tools for calculating Fisher information - I've not looked to see if it handles the generalised Pareto distribution, but if not, it should at least give you a starting point for some references. Or if you have the log-likelihood, the hessian() function in package numDeriv may help. As a rule, extreme value distributions are not necessarily any harder to estimate. It depends instead on how varying the parameters changes the shape of the distribution. If varying the parameters affects the tails but leave the central part virtually unchanged, then you need data from the tails to get a good estimate. But if the parameters change the shape of the central part as well, the bulk of the information comes from here. If you're interested, you can investigate that by considering $f(x,p) \log(f(x,p+\epsilon)/f(x,p))$ where $f(x,p)$ is the pdf at $x$ for parameter $p$ , subject to a small perturbation $\epsilon$ . The term $\log(f(x,p+\epsilon)/f(x,p))$ is proportional to the information in the observation of an outcome $x$ about the parameter $p$ , and multiplying by $f(x,p)$ gives you the average per observation. It tells you where in the distribution the information about that parameter is typically coming from.
