[site]: crossvalidated
[post_id]: 474437
[parent_id]: 
[tags]: 
Attention Mechanisms and Alignment Models in Machine Translation

From the paper that introduced attention mechanisms ( Bahdanau et al 2014: Neural Machine Translation by Jointly Learning to Align and Translate ), it seems that the translating part is the regular RNN/LSTM encoder-decoder and the aligning part is the actual attention mechanism (another smaller MLP), used to align words in the input language sentence into the target sentence. Is that interpretation correct? the so-called attention mechanism is the alignment model? In that case, the attention mechanism is used to attend to certain input words in the source sentence during each iterative prediction of words for the target sentence?
