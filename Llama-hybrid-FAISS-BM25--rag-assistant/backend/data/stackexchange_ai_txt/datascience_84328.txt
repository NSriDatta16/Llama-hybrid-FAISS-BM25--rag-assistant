[site]: datascience
[post_id]: 84328
[parent_id]: 84326
[tags]: 
If you use a linear activation function on your perceptron you are essentially creating a linear regression where the weights connecting to your perceptron are analogous to the the coefficients on your linear regression model. The only difference in this case is you would normally fit your perceptron using backpropagation, but for the linear regression model you would likely use OLS. In either case - you will end up with a series of coefficients multiplying on your input variables in a linear fashion with some bias term. This of course is the most basic use of a perceptron. The advantage of the perceptron is you can use non-linear activation functions to fit non-linearities in your data, and your ability to stack them together to form a larger neural network.
