[site]: datascience
[post_id]: 112207
[parent_id]: 
[tags]: 
Reinforcement learning using univalent and multivalent heterogeneous features

Problem introduction I have a game in which human players play levels (just like the famous casual game candy crush) where each level has its own properties and its own difficulty. In said game, the only variable that I, myself, have control over is the level's difficulty. I also have means to empirically measure some metrics that reflect the engagement of a player based on the findings of ' Quantifying human engagement '. Problem formulation Based on the data that I collect from the players, I am able to reproduce the above-cited paper's findings by empirically computing a player's engagement from my dataset as follows : $$ \bar{t_a}(n) = \frac{\bar{t_p^{emp}}(n)}{p_c(n)} = \text{Reward} $$ Where $\bar{t_p^{emp}}(n)$ is just the average number of attempts needed by players that passed level $n$ to pass it and $p_c(n)$ is the churn probability defined by the total number of players that abandoned at level $n$ divided by the total number of players that played level $n$ . The set of actions that reflect the difficulty of a level which I have control over is represented as : $$ \text{Action} \in \{A,B,C\} $$ where each of the three options $A$ , $B$ or $C$ represent a degree of difficulty. I also have the option of making difficulty actions randomly at first in order to learn a player's preferences in real time and its impact on the above-defined reward. Based on my findings so far, the actions have an impact on the reward. Data collected I collect data that represent features for thousands of players, each of which is given a unique $id$ , in the following form : Univalent features (one value per row, can be numerical or categorical) Multivalent features (short time series have a fixed length of $k$ elements in the past) An example of my data looks like this : ID X1 X2 X3 Reward Action 1 [1,2,3,1,..] [True, False, ...] 0.33 0.33 A 1 [5,1,2,3,..] [True, True, ...] 0.6 0.5 B 2 [0.5,1,..] [False, False,...] 0.6 0.5 C I can have different numbers of observations for different $id$ 's (so for example, $12$ observations for $id_1$ and $50$ for $id_2$ ) But each row can be a standalone learning instance that contain the full historical data for the same $id$ of course. Modeling approach I would like to find a modeling approach that returns one of the three actions available in order to maximize the reward (human engagement) based on what has been given in the past. This makes me think that it is a reinforcement learning problem. However, I have yet to find any resources or tutorials on how to preprocess such data nor did I find any models that handle such cases. The only paper that I came across that discusses this problem is deep learning for youtube recommendation where the authors talk about univalent and multivalent features (so tabular data and time series data mixed together, same as my case) then they use an embedding layer to transform the feature space into a single format input for a deep neural network. However, the authors do not share any code to reproduce their preprocessing techniques. Any thoughts? Thanks!
