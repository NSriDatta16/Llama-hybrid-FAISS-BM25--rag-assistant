[site]: datascience
[post_id]: 74269
[parent_id]: 74061
[tags]: 
I can't say for sure but I think the issue here is you're not subtracting the mean of the rewards. The idea is that actions with above average reward are positive after mean normalization, while actions with below average reward are negative after mean normalization. Your update step is -log(P(action))*reward , which you then minimize with your optimizer. P(action) therefore log(P(action)) , -log(P(action))>0 If reward>0 , -log(P(action))*reward>0 . Minimizing this value is the same maximizing log(P(action))*reward , which is maximized when P(action)=1 . Conversely, if reward , -log(P(action))*reward . This has the opposite effect, where P(action) is driven to 0. The important part is the different sign on above average/below average rewards causes actions associated with good rewards to have their probability increased, while actions associated with bad rewards have their probability decreased.
