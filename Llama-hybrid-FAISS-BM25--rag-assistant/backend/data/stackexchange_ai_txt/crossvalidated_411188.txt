[site]: crossvalidated
[post_id]: 411188
[parent_id]: 411165
[tags]: 
You're right that there may be an issue with trying to model so few observations with such a high number of variables. Read the Power and Sample Size section here . Essentially the problem you're running into is that there are so many variables that it's difficult to determine per instance of good or bad picture what is responsible for the good or bad rating, and what is just random noise. So you've basically got to decide at this point if you want to increase your sample size, decrease your features, or more likely, do both! One way to decrease features would be further resolution reduction, so instead of 75×75 you could do 25×25, for example, but this might be too little resolution for you to capture the things about images you care about. Another strategy would be to give up on color and analyze images in black and white, getting rid of that third dimension on your variables. Even with both of these strategies, you're looking at several hundred or even several thousand variables, so you'd still probably want to sit down and rate more pictures. Finally, it's possible that your rating of pictures is somewhat arbitrary, in which case there is definitely an upper limit to the accuracy we can get with any of these models looking at the picture alone. For example, we'd need info on your mood, time of day, pictures you looked at earlier, to really precisely know what makes you think a picture is good or bad. Nonetheless, when you've got an issue of limited data with many features, random forests can help you out! I was able to squeeze out a few more percentage points of accuracy using the following code, and I bet it could be optimized more, like defining a max_depth to stop over-fitting. import sklearn.ensemble as ske #RANDOM FORESTS rfmodel = ske.RandomForestClassifier(n_estimators = 200, bootstrap = True, verbose = True) #Run model to assess accuracy rf_modelfit = rfmodel.fit(train_x, train_y) accuracy = rf_modelfit.score(test_x, test_y) It's also worth noting that you should compare your results to a naive baseline. In your case, your split makes it so that 45.288% of photos are good and 54.712% of photos are bad. In my tests with your logistic model, I get about 53.57% accuracy, which is worse accuracy than if we just classified every photo as bad, so the logistic model is basically a guess machine in its current state. The random forests model got 58.63% accuracy, so it's a slight improvement over the baseline!
