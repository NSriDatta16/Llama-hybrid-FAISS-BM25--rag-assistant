[site]: crossvalidated
[post_id]: 340351
[parent_id]: 280049
[tags]: 
If we have a classifier which, for a given x, tells us the probability that $y=1$ and the probability that $y=-1$ , using a function $f(x)$ such that the probability that $y=1$ is given by $\frac{1}{1+e^{-2f(x)}}$ and the probability that $y=-1$ is thus given by $1-\frac{1}{1+e^{-2f(x)}}=\frac{e^{-2f(x)}}{1+e^{-2f(x)}}$ , then for a specific value of x, the expected negative log likelihood is given by: $P(y=1|x)\cdot - \ln \left(\frac{1}{1+e^{-2f(x)}}\right) + P(y=-1|x)\cdot - \ln\left(\frac{e^{-2f(x)}}{1+e^{-2f(x)}}\right)$ A bit of rearranging and and noting that $P(y=1|x)+P(y=-1|x)=1$ yields: $\ln(1+e^{-2f(x)})+2\cdot P(y=-1|x)f(x)$ To find the $f(x)$ which maximises this, we differentiate wrt f(x) as were it a constant, and find: $-2\frac{e^{-2f(x)}}{1+e^{-2f(x)}}+2P(y=-1|x)=-2\frac{e^{-2f(x)}}{1+e^{-2f(x)}}+2(1-P(y=1|x))$ which equals zero when $f(x)=\frac{1}{2}\ln\left[\frac{P(y=1|x)}{1-P(y=1|X)}\right]=\frac{1}{2}\ln\left[\frac{P(y=1|x)}{P(y=-1|X)}\right]$ This is the same $f(x)$ which minimises the expected exponential loss for any given x. My (possibly flawed) understanding of this (in my view quite confusing) paragraph in ESL is that if we are using a function $f(x) \in \mathbb{R}$ as a classifier for a variable $y \in \{-1,1\}$ , we could proceed simply by minimising exponential loss and the fact that $\frac{1}{2}\ln\left[\frac{P(y=1|x)}{P(y=-1|X)}\right]$ minimises the expected loss for any given x suggests that one can interpret the output of $f(x)$ to mean that $\hat{y}=1$ if $f(x)>0$ and $\hat{y}=-1$ if $f(x) . This seems like a fairly sensible way to interpret $f(x)$ and thus maybe exponential loss is a sensible/meaningful type of loss. Alternately, we could interpret $f(x)$ straight off the bat and somewhat arbitrarily claim that its meaning is that the probability that y=1 is given by $\frac{1}{1+e^{-2f(x)}}$ . Then, if we seek to find $f(x)$ so as to maximise the expected log-likelihood, we find the expected $f(x)$ is the same as the one which minimises the exponential loss condition. Just because the two interpretations of $f(x)$ and their corresponding loss functions lead to the same expected $f(x)$ given an underlying conditional distribution $P(y|x)$ does not mean that one would expect these two losses to yield the same classifiers given a real data set (i.e. a sample from said distribution)
