[site]: crossvalidated
[post_id]: 337632
[parent_id]: 
[tags]: 
What is the best option for columns with huge missing values number?

I am dealing with my clients dataset.I have inspected data and concluded that some columns have extremely high value of NaNs.Next step is feature importance with random forests.This is the output of my python code for l in newl: print (l, pd.isna(dfs[l]).sum()) output BNUM_OUT 488 BNUM_IN 488 CALL_OUT_CNT 401 CALL_OUT_SMS 401 CUC_LN_CNT 4880 CUC_COMP_LN_CNT 4880 PROD_CNT_ADO 2825 PROD_CNT_MACRO 2825 REV_BUN_ADO 4908 REV_BUN_MAC 3977 REV_OUT 508 USAGE_GPRS_NAT 401 USAGE_OUT_INT_DUR 401 USAGE_OUT_OFFNET_DUR 401 USAGE_OUT_ONNET_DUR 401 LNE_DAYS_OUT 4 TOPUP_AMT 2263 TOPUP_CNT 1909 I have 5000 rows.How to choose which columns should I drop?70 % and more?How is this choice going to influence next random forest step?
