[site]: crossvalidated
[post_id]: 609712
[parent_id]: 609609
[tags]: 
Not an entire solution, but some thoughts I have about this situation. (Repeated) cross validation has (at least) two different sources of variance: variance uncertainty due to case (I'll use n for number of statistically independent cases), and variance due to model instability. Looking at more surrogate models ( n_repeats * k ) will reduce the part of the variance uncertainty on the final estimate that is due to model instability. But the total number of tested cases stays the same ( n ) after the first complete run. That part of the variance uncertainty can only be reduced by more cases, more surrogate models cannot possibly help. There's a further consideration: cross validation estimates for generalization error are often used as approximation of the generalization error of the model trained on all cases. This is the case when the task at hand is building a model on the data set at hand for application/production use. As opposed to comparing the performance of training algorithms for the given type of data (in that case, there's the problem that only part of the relevant variance components can be assessed by cross validation experiments - see Y. Bengio, Y. Grandvalet, No Unbiased Estimator of the Variance of K-Fold Cross-Validation, J. Mach. Learn. Res. 5 (2004) 1089â€“1105.) For the production-use scenario, we say that the variation we observe between our estimate and any single surrogate model could serve as approximation for the variance due to instability in the training between the average performance at n and the single model we then train on the full data set. That means, while we can say that relevant variance uncertainty due to the finite number of tested cases does down with $\frac{1}{n}$ , no such reduction can be claimed for the model-instability part of the variance.
