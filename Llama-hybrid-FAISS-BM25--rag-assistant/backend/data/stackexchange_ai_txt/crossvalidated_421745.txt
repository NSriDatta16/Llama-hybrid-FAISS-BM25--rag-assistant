[site]: crossvalidated
[post_id]: 421745
[parent_id]: 
[tags]: 
Bayesian predictions from posterior parameter distributions

I have two physical models $f(\theta)$ and $g(\theta)$ (not probability distributions) parameterized on the same set of parameters $\theta$ . I also have data $y$ with measurement noise $\epsilon$ which can be used to obtain posterior estimates $p(\theta | y)$ through one of the models: \begin{align} y &= f(\theta) + \epsilon \\ \text{where}\ \epsilon &\sim \text{Normal}(0, \sigma) \end{align} so a Gaussian likelihood function can be defined: \begin{equation} p(y | \theta) \sim \exp\left(\frac{-(y - f(\theta))^2}{2\sigma^2} \right) \end{equation} Questions: Most of what I've read seems to be concerned with constructing posterior predictive distributions for new $y$ (denoting as $y_{\text{new}}$ ), as in $p(y_{\text{new}}|y) = \int p(y_{\text{new}}|\theta) p(\theta|y)d\theta$ . But don't I just want the parameter-weighted distribution $f(\theta)p(\theta|y)$ without the measurement noise $\epsilon$ for the best estimate of $f(\theta)$ ? Letting $z=g(\theta)$ , is it necessary to specify a probability density function $p(z|\theta)$ so that new predictions of $z$ are defined as the marginal distribution $p(z|y) = \int p(z|\theta) p(\theta|y)d\theta$ (e.g., Robert, The Bayesian Choice , 2007)? Can I not just construct a probability distribution for the second physical model from $g(\theta)p(\theta|y)$ ? For instance using MCMC I could construct the probability distribution from the sequence $\{g(\theta_1), g(\theta_2), \ldots, g(\theta_t)\}$ .
