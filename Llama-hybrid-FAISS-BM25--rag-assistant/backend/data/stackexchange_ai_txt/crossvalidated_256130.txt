[site]: crossvalidated
[post_id]: 256130
[parent_id]: 
[tags]: 
How to simplify/optimize model if dataset size is small?

Let's say we have a dataset of 250 samples and we have engineered 30 attributes based on the domain knowledge. We perform a 10-fold CV and estimate the performance of our process of creating a model. Now as you know, it's likely that some of the 30 attributes are redundant and getting rid of some of them should both simplify the model and also better the performance. Now let's say I play with the attribute set manually and find some of them that if removed the performance gets better. However, we know that because this manual attribute removal (which is a kind of a feature/model selection) was based on the same CV split, the performance boost might be biased. On the other hand, if we try a nested CV, because of the small dataset size, it's going to be a waste of data and thus a damage to overall performance estimate. So how model simplifying/selection should be performed on this small dataset? These are the scenarios that I can think of: Doing a nested CV, which is a waste of data because of small dataset size Do nothing besides the initial feature engineering because we don't have the means (enough data) to improve our model. perform automatic feature selection on the same CV, which will result in biased performance estimation. perform manual feature selection (based on domain knowledge, and plotting the data) and evaluate the goodness of that based on the same CV, again the performance estimation is going to be biased. perform manual feature selection (based on domain knowledge, and plotting the data) without running CV throughout the way and when we're done, run a CV and don't revert our decision even if the performance dropped, because if we do revert the performance estimation is going to be biased.
