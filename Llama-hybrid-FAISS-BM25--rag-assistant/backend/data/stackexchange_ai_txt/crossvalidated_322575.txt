[site]: crossvalidated
[post_id]: 322575
[parent_id]: 
[tags]: 
Avoiding burn-ins in MC-EM

In Monte Carlo - EM, we use a Monte Carlo sampler in the E-step to approximate the posterior distribution of the latent variables. The algorithm goes iterates through E-step: $Z_1,...Z_m \sim p(Z | X, \theta)$ M-step: $argmax_{\theta} \quad \mathbb{E}_{p(Z | X, \theta)}[\ln p(X,Z) | \theta)] \approx \frac{1}{M}\sum_m \ln p(X,Z^{(m)} | \theta)$ If we use an MCMC method for the E-step, then we need to do some burn-in each time we go through this E-step. That means a lot of burn-in sequences (one for each time we update our parameter $\theta$ in the M-step). Hence my question: is there some case (or some method) where we can avoid so many burn-ins? I would be tempted of updating the parameter $\theta$ after each MCMC sample, as if it was another random variable that I maximize instead of sample. But I am aware that updating $\theta$ changes the distribution. However, maybe once the updates of $\theta$ are small enough then I can skip or reduce the number of burn-in samples since the distribution barely changes. Is there some reference that can shed some light over this?
