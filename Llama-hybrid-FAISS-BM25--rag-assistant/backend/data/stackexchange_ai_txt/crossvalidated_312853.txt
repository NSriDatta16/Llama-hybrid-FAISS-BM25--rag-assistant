[site]: crossvalidated
[post_id]: 312853
[parent_id]: 
[tags]: 
Neural network cost function- unnecessary terms?

My questions stem from a bit of confusion (and search for clarification/confirmation) about the form of a particular cost function for NNs. It's given in several sources online, but is often attributed to Andrew Ng's ML course. In particular, it proposes a cost function for a "standard" neural network as $$J_1(\theta) = -\frac 1m \sum_{i=1}^m\sum_{k=1}^K\begin{bmatrix}y_k^{(i)}\log((h_{\theta}(x^{(i)}))_k) + (1-y_k^{(i)})\log(1-(h_{\theta}(x^{(i)}))_k)\end{bmatrix}$$ where $y_k^{(i)}$ is the $k$th component of the target vector for the $i$th training example and $h(x^{(i)})_k$ is the $k$th component of the predicted output vector for the $i$th training example. Question 1: The "coefficients" of $y_k^{(i)}$ and $(1-y_k^{(i)})$ strongly suggest (to me) that this cost function is only applicable to classification networks with $K$ classes where $y$ is one-hot encoded (e.g. not a regression network with output $y \in \mathbb{R}^K$). However, I don't see any mention of this, or any mention of something like a softmax transformation of the output layer. Is my intuition accurate? Question 2: My primary question is regarding the summation over $k$. In the context of classification networks, I know that cross-entropy is a popular cost function and would give a similar, but simpler expression: $$J_2(\theta) = -\frac 1m \sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log((h_{\theta}(x^{(i)}))_k). $$ Comparing to $J_2(\theta)$, in $J_1(\theta)$ there are obviously extra terms which seem "unnecessary" to me. That is, they are not "incorrect" since they still reflect a discrepancy between the predicted and true output. If that is the case, why use $J_1(\theta)$? Is there some motivation to use the more "complicated" form $J_1(\theta)$? As a simple concrete example, consider the case of $K=2$ (with $m=1$, input $x$, output $y=(0,1)^T$). [I recognize that using two output nodes to perform binary classification using a NN is unnecessary since only one is needed, but I'm just taking this to compare to basic logistic regression.] $$ J_1(\theta) = -\log[1-(h_{\theta}(x))_1] - \log[(h_{\theta}(x))_2] $$ In logistic regression, the log-likelihood would just be just the second term (and that agrees with $J_2(\theta)$). Ultimately, it looks to me like $J_1(\theta)$ is accounting for both correctly classifying to class 2 AND also accounting for correctly "rejecting" the incorrect classification to class 1. More generally (for $K>2$ classes), it seems like $J_1(\theta)$ is "double-counting" but in the end the optimization of either $J_1(\theta)$ or $J_2(\theta)$ leads to the same set of learned parameters. Does that interpretation make sense? Thanks!
