[site]: crossvalidated
[post_id]: 482905
[parent_id]: 
[tags]: 
is my model overfitting? validation loss decreased in tandem with training loss by a constant gap

I have a single hidden layer forward neural network build in Keras to solve a regression problem. They are time-series panel data from countries, with 2-3 countries contributing to the majority of the dataset. In total, there are around 14,000 records. I used Keras' validation_split when training the model, which is set at 0.3 , and shuffle is set to True . It's interesting that the validation loss ( mean squared error ) is constantly lower than the training set, and the two losses seem to move in tandem by a constant gap. It seems like most of the time we should expect validation loss to be higher than the training loss. Is it possible that this pattern is caused by the unique composition of the validation set (i.e. turned out to be much "easier" than training set)? If that's the case, is there any way I can prove it? P.S. I have already checked that the train/test label is correct.
