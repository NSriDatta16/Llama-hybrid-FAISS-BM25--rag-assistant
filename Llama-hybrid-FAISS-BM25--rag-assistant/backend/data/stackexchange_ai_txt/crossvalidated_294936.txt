[site]: crossvalidated
[post_id]: 294936
[parent_id]: 
[tags]: 
Determining sample size with biased data

I'm new to this forum but I've read a lot of interesting topics to help me in my research, so I'm hopefull someone can help me with this question! My goal: provide management with a sample size. could be based on product category or variant level. This sample size has to provide statistical power to be 'certain' that the faulty bikes are found. so for example: how many bikes do I have to inspect in the 'blue' category when I know that 6,2% is defect? I'm doing a research (internship) for a company to determine a sample size for a daily inspection. To illustrate my question I've constructed a fictitional table with some data, see attached picture. For clarity, my research is not about bikes but I can not publicly share the real product names. therefor some things might seem unlogical. As you can see, produced bikes is left blank. I'm currently working on getting this data but is taking pretty long so I'm thinking of ways to work without it. There are several product categories (example blue bikes) and within this category several variants exists. In fact you can think of the same product, but supplied by another supplier (for physical reasons, variants can not and are not mixed) This variant get produced/delivered on different dates (mostly daily) and this can be seen as a production batch. This controlled bikes number is biased. This is because of 2 reasons: 1) The data is simply faulty (people only register a bike as controlled because it was faulty, but don't register controlled bikes that happened to be ok). This is a problem I can not really control (except deleting the really obvious errors) 2) Some bikes are clearly faulty and therefor get a inspection, while others clearly are good to go and thus won't get an inspection. For capacity/money reasons this is obviously good, but I'm struggling with it in my research. What I've done so far is the following: Aggregate all production dates per variant because if I wouldn't do this, n is often very small (>15 controlled bikes is rare, so the fictitous data is not representative) Analysed the aggegrated data and concluded that the variants are exponentially distributed (using a K-S test) with E(X)=average of the percentage faulty bikes within this variant. Now I'm kinda stuck. What I've found is the following: Using the program GPower I've determined that with an average rate of 6,2% faulty bikes within a category, a preferred power of 99% and significance of 95%, I'll need a sample of 4089. This number is what I'm not sure about. In reality, in my dataset there were 26374 inspections within this category (with unknown total bikes, but estimated at least half a million) and thus 1636 faulty bikes. So, is this company already inspecting way more than necessary or am I doing some things wrong? I think the latter.. I hope it is clear what my question is about. I want to give management a number of bikes they need to inspect so they can confidentally say the faulty bikes are found, considering the now known E(X) are probably biased.
