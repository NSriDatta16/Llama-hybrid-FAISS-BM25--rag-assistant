[site]: crossvalidated
[post_id]: 383698
[parent_id]: 340984
[tags]: 
The conclusion of the interviewer is silly, and it is an example of the Gambler's fallacy . Both classical and Bayesian methods lead to the conclusions that are broadly the opposite of his conclusion. Whenever you take draws at random from a distribution this leads you to a series of independent and identical distributed (IID) random variables. In this case the observed data gives information on the location of the distribution, and so it tends to be the case that the most likely future values are at or near the past values. Whenever I hear Dr Phil say in his charming Southern-drawl, "past behaviour is the best predictor of future behaviour" , I always imagine that he is talking about sequences of IID random variables. To analyse this particular case, let's start by framing it clearly. Given that the interviewer did not specify a particular normal distribution (e.g., standard normal), his reference to "a normal distribution" is a reference to a distributional family with unknown mean and variance. Since he refers to random draws, this means that the values are IID random variables from a normal distribution, and so the observable data sequence is $X_,X_2,X_3,... \sim \text{IID N}(\mu, \sigma^2)$ . This interview question is essentially just asking you to make a prediction about $X_2$ , given that you only observe that $X_1 . Classical approach: Under classical analysis, with a single data point, we can estimate the mean of the distribution but not its variance. The estimated mean parameter with a single data point is: $$\hat{\mu} = \bar{X}_1 = X_1.$$ Noting that the mode of the underlying distribution is $\text{Mode } \text{N}(\mu, \sigma^2) \equiv \max_{x \in \mathbb{R}} \text{N}(x|\mu,\sigma^2) = \mu$ , this means that the estimated mode is: $$\widehat{\text{Mode}} = \hat{\mu} = X_1 Since we are dealing with IID data, the estimated mode of the underlying distribution is the best (point-based) prediction for the next data point. Hence, we conclude that the best prediction for the next data point is negative. Bayesian approach: Under Bayesian analysis, we specify a prior distribution for the parameters and derive the predictive distribution of the new data point. In this case we can proceed conditional on $\sigma$ since it will not affect our conclusion. If we use the standard "non-informative" prior $\pi(\mu) \propto 1$ (which is improper), this gives the posterior distribution: $$\pi_1(\mu|x_1, \sigma) \propto \text{N}(x_1|\mu, \sigma^2) \pi(\mu) \propto \text{N}(\mu|x_1, \sigma^2).$$ The posterior mode is $\text{Mode } \pi_1 \equiv \max_{\mu \in \mathbb{R}} \pi(\mu| x_1, \sigma) = x_1 . The posterior mode is the best (point-based) prediction for the next data point. Hence, we conclude that the best prediction for the next data point is negative.
