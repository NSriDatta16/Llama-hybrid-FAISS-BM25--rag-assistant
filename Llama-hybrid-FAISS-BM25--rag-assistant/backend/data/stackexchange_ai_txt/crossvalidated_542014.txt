[site]: crossvalidated
[post_id]: 542014
[parent_id]: 541765
[tags]: 
Suppose that the model does not involve any parameter other than $\boldsymbol{\mu}$ . You can regard $\mu$ as a random quantity in a Bayesian approach. Then, if the prior on $\boldsymbol{\mu}$ is Gaussian, the posterior is also Gaussian and is given by the last step of the Kalman Filter. The filtered mean $\widehat{\boldsymbol{\mu}}_{T\vert T}$ for the last time $T$ gives the posterior mean which also is the posterior mode, and the posterior variance is the filtered variance. Moreover, you can use a diffuse (improper) prior for $\boldsymbol{\mu}$ ; provided that $\boldsymbol{\mu}$ can be identified, the posterior will be proper, and the mode will be the value of $\mu$ that maximises the likelihood. Note that since $\boldsymbol{\mu}$ does not depend on $t$ , a better notation would be something like $\widehat{\boldsymbol{\mu}}_{\bullet \vert T}$ . Things are more complex in the case where the model involves more parameters forming a vector $\boldsymbol{\theta}$ . The Bayesian approach would then lead to regard $\boldsymbol{\theta}$ as random as well. With mild conditions the full conditional $p(\boldsymbol{\mu} \vert \mathcal{Y}, \, \boldsymbol{\theta})$ will be Gaussian where $\mathcal{Y}$ is the whole series. This can be of some help. In a frequentist approach, you can maximise the profile-likelihood that depends only of $\boldsymbol{\theta}$ . Indeed, the value of $\boldsymbol{\mu}$ which maximises the likelihood for a given $\boldsymbol{\theta}$ is given by the Kalman filter as above. Remind however that you have to use the diffuse prior. In the second case, the estimate of $\boldsymbol{\mu}$ is provided by the Kalman filter at a very small cost. Note that depending on the dimensions of the system, it can be better to use an alternative state vector $\mathbf{x}_t^\star := \mathbf{x}_t - \boldsymbol{\mu}$ along with $\boldsymbol{\nu} := \mathbf{F} \boldsymbol{\mu}$ and the state space model \begin{align*} \mathbf{x}_t^\star &= \mathbf{G} \mathbf{x}^\star_{t-1} + \mathbf{n}_t\\ \mathbf{y}_t &= \mathbf{F} \mathbf{x}^\star_ t + \boldsymbol{\nu} + \mathbf{e}_t. \end{align*} The augmented state form would then take $\boldsymbol{\nu}$ as augmented part with a block diagonal transition matrix. The motivation is that $\boldsymbol{\nu}$ can be of smaller length than $\boldsymbol{\mu}$ , especially when the observation is a scalar $y_t$ . Note also that as in in this question linked in the comment by @Cam.Davidson.Pilon, we can cope similarly with the case where $\boldsymbol{\nu}$ is replaced by $\mathbf{z}_t^\top \boldsymbol{\beta}$ for a vector $\mathbf{z}_t$ of known covariates. Then we would put the vector $\boldsymbol{\beta}$ of unknown regression coefficients in the augmented state.
