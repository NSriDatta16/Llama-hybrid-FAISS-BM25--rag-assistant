[site]: datascience
[post_id]: 22704
[parent_id]: 22702
[tags]: 
In general, if you want to generate data in X, or X, Y pairs, then you should start by training a generative model - as opposed to a discriminative model, which is what most NN classifiers are. There are many types of generative model. Variational Autoencoders (VAE)and Generative Adversarial Networks (GAN) have been demonstrated recently with interesting results on images - although both take a lot of training data and time, and are limited to relatively small image dimensions (e.g. 128x128). There are many sub-types of these two designs, including a combined VAEGAN which attempts to combine strengths of both. With a trained CNN, you can generate data - sort of. What you can do is start with some arbitrary value of X and Y, then use back propagation to calculate gradients of a cost function. But instead of using the gradients to update weights, you back propagate all the way to the input, and use the gradients at the input to alter X, repeating the process multiple times. This is essentially how Deep Dream and Style Transfer work (although in general these don't use a Y value, but selected activation values within layers). There is a major caveat to this approach - your generated X will not be sampled evenly from any distribution of X that the network has been trained with. Instead you will generate a "super stimulus" X for the given Y. You mention RNNs. One way these can be used to generate X is by sampling from their output and feeding this back into the input . For text sequences this tends to generate grammatically correct nonsense. I am not sure if this would be considered a strictly generative model, since it is not clear to me whether the input X is being sampled evenly. It is likely that you could use the approach to generate images too, although you would have to take care defining what the sequence is (just a sequence of pixels line-by-line will probably not produce any recognisable image).
