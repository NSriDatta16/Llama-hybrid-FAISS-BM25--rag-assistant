[site]: datascience
[post_id]: 108144
[parent_id]: 
[tags]: 
Word Embedding Dimensions Reduction

In my NLP task, I use Glove to get each word embedding, Glove gives 50 float numbers as an embedding for every word in my sentence, my corpus is large, and the resulted model is also large to fit my machine, I'm looking for a way to reduce each word embedding from 50 numbers to something less, maybe one floating number is possible, is there an approach to do that? I was thinking about an aggregation approach,like taking the average of the 50 numbers, would that work?
