[site]: crossvalidated
[post_id]: 202221
[parent_id]: 
[tags]: 
For linear classifiers, do larger coefficients imply more important features?

I'm a software engineer working on machine learning. From my understanding, linear regression (such as OLS) and linear classification (such as logistic regression and SVM) make a prediction based on an inner product between trained coefficients $\vec{w}$ and feature variables $\vec{x}$: $$ \hat{y} = f(\vec{w} \cdot \vec{x}) = f(\sum_{i} w_i x_i) $$ My question is: After the model has been trained (that is, after the coefficients $w_i$ have be computed), is it the case that the coefficients will be larger for feature variables that are more important for the model to predict more accurately? In other words, I am asking whether the relative magnitudes of the coefficients can be used for feature selection by just ordering the variables by coefficient value and then selecting the features with the highest coefficients? If this approach is valid, then why is it not mentioned for feature selection (along with wrapper and filter methods, etc.). The reason I ask this is because I came across a discussion on L1 vs. L2 regularization . There is a blurb that says: Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that "the other 90 predictors are useless in predicting the target values". Reading between the lines, I would guess that if a coefficient is close to 0, then the feature variable with that coefficient must have little predictive power. EDIT : I am also applying z-scaling to my numeric variables.
