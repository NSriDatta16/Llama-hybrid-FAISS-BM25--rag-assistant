[site]: crossvalidated
[post_id]: 417749
[parent_id]: 417163
[tags]: 
There is nothing special but there are some observations to be done. It is true that it is a linear combination and applying the activation function gives a regression. However this is not a linear regression unless we have an identity as activation. An indeed using identity we will get a linear regression in the end. If we use logistic sigmoid, then the NN will be a logistic regression. There are three arguments in favor of current setup. Historical: it mimics the neuron behavior. Computation: there are fast vectorized operations to help you carry a lot of operations in short time. Mathematical: devising error propagation through gradients is much easier. And maybe another one: the NN could be highly nonlinear enough to approximate a lot of surfaces, what would bring to the table a different approach to give enough advantages over the classical setup. However I am not saying that it would not be successful, but that it is hard to overcome the advantages of current setup
