[site]: datascience
[post_id]: 117959
[parent_id]: 117949
[tags]: 
The Transformer has the inherent ability to generate variable-length sequences, you don't need to do anything special. The output of the Transformer decoder is always the same length as the input of the decoder. When using a Transformer at training time, we pass the whole target sequence (shifted one position to the right due to the addition of the "beginning of sequence" token at the beginning and the removal of the "end of sequence" token), so the output of the decoder has the same length as the target sequence. When using a Transformer at inference time, we start by providing as input only the "beginning of sequence" token, and we obtain an output of length 1. We create a new input, attaching the previous output to the input of the previous iteration, and repeat the process. We do this until it generates the "end of sequence" token. In all cases, the output of the decoder is the same length as the input of the decoder. Note that you need EOS tokens to mark the end of the sequence, both for the source and target sequences. Both during training and inference. When preparing the input data for training, you remove the EOS token from the input target sequence, and prepend a BOS token to it.
