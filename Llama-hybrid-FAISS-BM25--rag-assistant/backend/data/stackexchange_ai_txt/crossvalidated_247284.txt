[site]: crossvalidated
[post_id]: 247284
[parent_id]: 247280
[tags]: 
Notice that the regression model is $$ y_i = \beta_0 \,\mathrm{cons}_i + \varepsilon_i = \beta_0 + \varepsilon_i $$ writing it differently, $$ E(Y) = E(\beta_0 + \varepsilon_i) = \underbrace{\beta_0 + E(\varepsilon_i) }_{\text{since } \beta_0 \text{ is constant}} = \underbrace{\beta_0 + 0}_{\text{since } E(\varepsilon_i) = 0} $$ so $E(Y) = \beta_0 = \bar Y$ (see Why is expectation the same as the arithmetic mean? , and Why do residuals in linear regression always sum to zero when an intercept is included? and Why the sum of residuals equals 0 when we do a sample regression by OLS? ). Saying it differently, we are looking for some $\beta_0$ value that minimizes squared errors (since OLS minimizes squared errors), and the value that minimizes squared error is arithmetic mean (see Intuition on why the average minimizes the euclidean distance ).
