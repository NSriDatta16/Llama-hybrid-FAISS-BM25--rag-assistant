[site]: datascience
[post_id]: 73199
[parent_id]: 73189
[tags]: 
BERT's embeddings are 3 things : Token embeddings Segment embeddings Position embeddings I guess your question is about token embeddings. Token embeddings is a vector, where each token is encoded as a vocabulary ID. Example : # !pip install transformers from transformers import BertTokenizer t = BertTokenizer.from_pretrained("bert-base-uncased") text = "My dog is cute." text2 = "He likes playing" t1 = t.tokenize(text) t2 = t.tokenize(text2) print(t1, t2) tt = t.encode(t1, t2) print(tt) ['my', 'dog', 'is', 'cute', '.'] ['he', 'likes', 'playing'] [101, 2026, 3899, 2003, 10140, 1012, 102, 2002, 7777, 2652, 102]
