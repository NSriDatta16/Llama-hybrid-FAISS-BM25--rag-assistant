[site]: crossvalidated
[post_id]: 549481
[parent_id]: 
[tags]: 
probabilistic regression in existing packages

I know that typical machine-learning packages (such as scikit-learn or TensorFlow) contain plenty of functions for probabilistic classification: given a training set of pairs $(x_i,y_i)$ , where $y_i$ is the label of the $i$ th sample $x_i$ , and a test sample $x$ , we can compute the probability distribution for its label $y$ . For example, in scikit-learn, this can be done using Neural Nets, Support Vector Machine, Decision Trees, Random Forest, Gradient Boosting, Naive Bayes, Logistic Regression, and probably many more methods. But what about probabilistic regression, where the labels are real numbers, and so the task can be stated as computing a cdf for the label of the test sample $x$ ? I know scikit-learn has Gaussian Processes implemented, with various kernels. Is it all? Are there functions in common packages that produce non-Gaussian predictive distributions? I understand that standard regression functions can be adapted to probabilistic regression: for example, we can first fit a real-valued function $\mu(x)$ predicting the label $y$ given $x$ and then fit $\sigma^2(x)$ predicting $(y-\mu(x))^2$ given $x$ . This will give us a predictive distribution, $N(\mu(x),\sigma(x))$ , for the label of $x$ . This could be adapted to non-Gaussian predictive distributions, but it looks awkward. And in any case, I am looking for off-the-shelf solutions.
