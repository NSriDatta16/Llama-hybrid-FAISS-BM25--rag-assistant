[site]: crossvalidated
[post_id]: 371955
[parent_id]: 371937
[tags]: 
I think that depends on your objective and data. I will assume that your task is machine learning since you use the “feature” term: 156 features seem to have a high cardinality proportional to 625 samples. If your data is not a time-series sequence, you can try to use bootstrapping or Monte-Carlo simulations ; neuroscience researchers sometimes extract data from real human and animals manually, creating those can consume so much time and can be costly so that they sometimes end up with small amount of samples as you. Moreover, if you get computationally stuck, you may look up for jack-knifing your samples. But without knowing the data, using those methods can be dangerous, try to describe your features at least statistically and to plot them to observe what kind of distributions you have. Know your data. However, if you have a time-series sequence; you may look up for another approach which I do not have the knowledge of. Direct bootstrapping of a time-series sequence may be unsafe, you may need a clever approach. As last note: In any case, I’d suggest you to still try your dataset for your task. If most of your features are useless or highly correlated with each other; your number of actual useful features can be actually 20, 25 etc. Moreover, the patterns among the features can be so explicit for machine learning algorithms such that 625 samples can be surprisingly enough for modeling or some other analysis. Still, know your data.
