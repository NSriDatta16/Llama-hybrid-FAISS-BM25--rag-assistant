[site]: crossvalidated
[post_id]: 288770
[parent_id]: 
[tags]: 
Meaning of batch sizes for RNNs/LSTMs and reasons for padding

I've got a two conceptual questions about RNNs, particularly LSTMs, which I just can't figure out on my own or with the tutorials I find on the internet. I would really appreciate if you could help me with the following: If I understand correctly, the states learned within a LSTM are only relevant for one sequence. So, for the next sequence the states are being "relearned" due to $s_{t}=f(Ux_{t} + Ws_{t-1})$ with x being the input at timestep t , s being the state at timestep t and U and W being the matrices that are learned. Is there any good reason why you should use larger batch sizes than 1 with RNNs/LSTMs especially? I know the differences between stoachastic gradient descent, batch gradient descent and Mini-batch gradient descent, but not why the latter two should be preferred over the first one in RNNs/LSTMs. Why do you need the same sequence lengths within a batch, i.e. why is padding needed? The states are calculated for each sequence separately, so I don't see a reason for this. Does the backprop through time need the same number of states for each sequence, when it's being executed after a batch?
