[site]: crossvalidated
[post_id]: 291684
[parent_id]: 
[tags]: 
Ergodicity of MCMC in a hierarchical model

Many of the Bayesian hierarchical models that I am studying use a Markov chain as the model. These hierarchical models use different MCMC techniques to sample low-level and high-level parameters. My question is: does using different MCMC techniques for different parameters affect the ergodicity/convergence of the Markov chain in any way? For example, in Rasmussen 2000's infinite Gaussian mixture model : The variance of the Gaussian components are given an inverse Gamma prior, $p(\sigma^{-2}) \sim \mathcal{G}(\beta, w^-1)$. Within this, the $\beta$ hyper itself is given an inverse Gamma prior $\mathcal{G}(1,1)$, and the conditional posterior of $\beta$ is sampled using ARS. In his subsequent paper on infinite mixtures of Gaussian Process experts , he also employs a similar hierarchical model, where he learns the kernel parameters using Hamiltonian Monte Carlo. The indicator/latent variables are sampled using Gibbs sampling. The Dirichlet process concentration parameter $\alpha$ is sampled using ARS. Is there some sort of ergodicity guarantee for such models based on, say, the reversibility of the individual techniques used in each step in the chain, or is the proof a bit more subtle?
