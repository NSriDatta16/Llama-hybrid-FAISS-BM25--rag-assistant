[site]: datascience
[post_id]: 87413
[parent_id]: 
[tags]: 
Concatenation of CNN and LSTM to model time of a series of images

I have collected a dataset consisting of around 30'000 heat maps of 80 users. The heat maps represent typing behavior on a keyboard and are just images with a resolution of 39 x 39 x 3 (3 color channels). Each user had to fill-in self-reports about the actual pleasure (emotion). This is the ground truth. For each user I have between 30 and 700 such labels. Each label (self-report) has an associated heat map (which I have created over 30 minutes before the self-report). I have now built a CNN which I have used to predcit the ground truth (emotion) based on the heat maps. This works well but does not incorporate the time series. I see two possibilities to incorporate time: Instead of creating one heat map over 30 minutes, I could create 3 heat maps where each heat maps takes data from 10 minutes. These 3 heat maps could then be stacked ( 39 x 39 x 3 x 3 ) and used in a CNN with 3D convolutions. The time series of self-reports could also be exploited by a combination of CNN and LSTM. I could process each heat map by a CNN and feed the flattened output then into a LSTM to predict the self-reports of each user. Here, also ConvLSTMs could be used but this has the disadvantage that I cannot incorporate additional context data (such as the daytime). When concatenating a CNN and a LSTM, I can append context data to the flattened output of the CNN. Are there any other (better) possibilities to exploit time? Regarding the second option, I also see the problem that the series of self-reports ranges from 30 to 700 for the users. How can I handle this in a LSTM? Should I just pad with zeros if a sequence is smaller than 700? But this would result in a lot of zero padding if a user only had 30 self-reports... Last but not least, the timings of the self-reports are not equally spaced. How can I incorporate the time information (either absolute time or relative time to previous self-report) in such a model? As mentioned above, I think one possibility is to append this information to the flattened output of the CNN but I'm not sure if that is the best way to go.
