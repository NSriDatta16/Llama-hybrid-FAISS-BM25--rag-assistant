[site]: crossvalidated
[post_id]: 142960
[parent_id]: 
[tags]: 
Two broad categories of dimensionality reduction: with and without an explicit mapping function

I think there are two very broad categories of dimensionality reduction (DR) techniques: We can compute an analytic form of mapping from the training data, say $x\mapsto y: y=f(x)$ , where $f(\cdot)$ can be linear or nonlinear. In this case, given a new data point, we just plug it in as $x$ and compute its low-dimensional position. Such examples include PCA (unsupervised) and LDA (supervised); We don't really have a mapping function as before. The mapping is data-dependent in some sense. In this case, when a new data point comes, we have to mix it together with the training data and re-do DR. Such examples include Isomap and t-SNE. Am I right about this? If so, what are the names of the two categories? Bonus Question: For category 1 techniques, one can easily incorporate it into classification algorithms, or specifically $k$ -NN. What if I want to use the distance computed by Category 2 techniques in my $k$ -NN classifier? Must I do mix-and-reproject every time a new data point comes in? That would be computationally impossible...
