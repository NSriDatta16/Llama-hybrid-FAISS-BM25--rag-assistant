[site]: crossvalidated
[post_id]: 273458
[parent_id]: 
[tags]: 
Why Sens and Spec for my testing set are worse than in my fit model?

I am using R caret::train for creating my model with imbalance data: 0 1 table 30,991 6,179 percentage 83% 17% therefore I am using DMwR::SMOTE (I get better result preparing the data first rather than using smote internally in caret::train ) to generate a 60:40 proportion from my binary decision variable. After executing train function using Random Forest algorithm I get the following tuning parameters for my model: > best_ind then mtry threshold ROC Sens Spec Dist ROCSD SensSD SpecSD DistSD 13 5 0.6289474 0.8943797 0.8347721 0.7573713 0.2937601 0.0060855 0.009429729 0.01417039 0.01269218 Which is for me a good result in terms of Sensitivity (0.83) and Specificity (0.75). Now when I try to test my model using a testing set (I made a partition from my original data set, before smote) with the predict function: predicted I get: Confusion Matrix and Statistics Reference Prediction NO YES NO 2199 284 YES 900 333 Accuracy : 0.6814 95% CI : (0.6661, 0.6963) No Information Rate : 0.834 P-Value [Acc > NIR] : 1 Kappa : 0.1781 Mcnemar's Test P-Value : In both cases the parameters (sensitivity and specificity) are worse (especially for the second one: 0.53), than in fit model and now the auc is 57.78% and in the fit model was: 89% . Because the model is predicting using new information (testing set), it is reasonable to expect to have a lower performance than in the fit model. What surprises me is the magnitude of such difference. For example when I set sampling argument from trainControl to one of its values: smote , down , up . The difference between the spec , sens , from fit model and testing it is not that high. Why for this case is such big difference? Is it something we can explain? The variable importance of my model (sorted by importance) is the following: > caret::varImp(fit$finalModel, type=2) var importance 1 svc1 1554.34084 2 RealLengthOfStay 819.57894 3 dx1 784.11788 4 dischargediagnosiscode 766.96813 5 dx3 764.68220 6 admittingdiagnosiscode 762.31901 7 dx2 761.70058 8 physicalzipcode 679.44161 9 QNXTReferToId 638.57844 10 QNXTReferFromId 631.69746 11 QNXTPCPId 630.73276 12 AvgIncome 572.72575 13 Disposition 557.17779 14 AgeGroup 555.83880 15 IsReadmit 490.89471 16 RateCode 414.70836 17 referralservicecode 312.88713 18 svc2 297.14956 19 PCPChanges 205.11664 20 AbnormalFlag 162.65556 21 Acuity 131.38040 22 Risk 120.39911 23 RatePercentage 117.41145 24 gender 101.00251 25 LateToFill 97.58323 26 svc3 96.29999 27 iscasemanagement 92.42275 28 PrimaryLanguage 86.07432 29 QNXTProgramId 47.94606 30 IsPriority 23.93185 Based on the above situation I have the following questions: Is this kind of result reasonable, if so what would be the explanation? How much deviation can be reasonable on average or based on your experience? Is the imbalance nature of my data the possible reason of this different result or on contrary the cause would be more related to the quality of my predictor variables (and I need to find better ones), etc. Thanks in advance,
