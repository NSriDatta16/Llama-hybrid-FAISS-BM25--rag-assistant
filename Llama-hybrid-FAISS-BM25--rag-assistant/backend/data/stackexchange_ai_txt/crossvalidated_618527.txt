[site]: crossvalidated
[post_id]: 618527
[parent_id]: 616120
[tags]: 
A feed-forward network has a fixed number of layers with a fixed number of neurons in them. This means the input of the network always has to have the same feature dimensionality . 1 , 2 It is not possible to effectively train a feed-forward neural network that has inputs of different types/sizes. The standard solution to this problem would be either padding (which you said you do not want) or truncation (which loses information). An approach worth looking into would be embedding , which means translating any input into an arbitrary feature space of fixed size (e.g. representing each input as a 64-dimensional vector). But of course, the problem of variable input sizes remains the same and embedding would introduce an additional loss of information. So this might not be helpful for you at all. To my knowledge, it is not feasible to train a neural network on variable input sizes. However, the padding approach keeps all the information of the input samples for only slightly higher computational cost. I suggest you give this a try, even with highly varying input sizes. Also check out this question
