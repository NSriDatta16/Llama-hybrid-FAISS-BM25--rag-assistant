[site]: datascience
[post_id]: 101827
[parent_id]: 101783
[tags]: 
First of all, I would not consider each letter as a token of your input sequence, think of the words as a whole as your tokens. Regarding the problem of predicting the next token (word) given some input sequence, the accepted architecture nowadays is sequence-to-sequence with encoder-decoder , where you encode your input sequence (source sentence) into one or several vectors, which is then fed into the decoder (together with the former outputs). If you try to predict the next token with a usual step-by-step LSTM based only on former input tokens, without any context of the whole sentence, it might be not possible to predict something reasonable when having not enough words yet (think of a translation machine trying to predict a 2nd or 3rd word based only on the first or 2 first words), where each output token N is based on the input tokens 0...N + the N-1 output tokens predicted by that step: but in a proper sequence-to-sequence approach, you better encode your whole input sequence into a single representation, which is fed into the decoder (which can have a GRU, lSTM...): but there is still one problem with this sencond approach, which is dealing with the whole input sequence and, for very long input sentences, it might be difficult for a RNN to retain all the neccessary info, loosing then some context. Here is where attention based transformer models comes in to play: where each token is encoded via attention mechanism, giving words representations a context meaning. The decoder of the transformer model uses neural attention to identify tokens of the encoded source sentence which are closely related to the target token to predict. A great source of info about this all is the second edition of the book Deep learning with python by Fran√ßois Chollet .
