[site]: crossvalidated
[post_id]: 592538
[parent_id]: 
[tags]: 
Sklearn performs better than Pyspark

I have created 2 scripts for training a simple classification decision tree with cross-validation. The problem is that sklearn is performing better in time and accuracy than a spark. I have the exact same configuration so I don't really get how Spark gets 0.43 acc but sklearn gets 0.89 acc. Also, I have 4 nodes, 16Go Ram. The first one with pySpark : # PySpark validation croisée distribuée from pyspark.ml.feature import StandardScaler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline sc.stop() sc = SparkContext("local[*]", "Decision Tree Classifier") spark = SparkSession(sc) df = spark.read.csv("creditcard.csv", inferSchema=True, header=True).toDF("Time", "V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "V11", "V12", "V13", "V14", "V15", "V16", "V17", "V18", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "V27", "V28", "Amount", "Class") df = df.drop("Time") start = time.time() #on assemble les features dans un vecteur df = VectorAssembler(inputCols=df.drop("Class").columns ,outputCol="features").transform(df) # split comme avec sklearn split = int(0.7*df.count())-1 train = df.head(split) test = df.tail(df.count()-split) training = spark.createDataFrame(data=train, schema = df.columns) testing = spark.createDataFrame(data=test, schema = df.columns) tree = DecisionTreeClassifier(labelCol="Class", featuresCol="features") pipeline = Pipeline(stages=[tree]) # on tune le parametre maxDepth, j'ai essayé avec un seul paramètre car sklearn etait plus rapide que spark paramGrid = ParamGridBuilder().addGrid(tree.maxDepth , [10,30]).addGrid(tree.minInstancesPerNode, [1, 3]).build() crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(labelCol='Class'), numFolds=3) cvModel = crossval.fit(training) #entrainement avec validation croisée prediction = cvModel.transform(testing) auprc = sklearn.metrics.average_precision_score(np.array(testing.select("Class").collect()), np.array(prediction.select("prediction").collect())) print("AUPRC : " + str(auprc)+", time_exec : "+str(time.time()-start)) The second one with Sklearn : # Scikit learn validation croisée from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV from sklearn import tree start = time.time() df = pd.read_csv("creditcard.csv") df = df.drop("Time", axis=1) df = pd.concat([df, df]) #split comme avec spark split = int(0.7*len(df))-1 train = df.iloc[:split,:] test = df.iloc[split:,:] train_x = train.copy().drop("Class", axis=1) train_y = train["Class"].copy() test_x = test.copy().drop("Class", axis=1) test_y = test["Class"].copy() clf = GridSearchCV(tree.DecisionTreeClassifier(), {'max_depth':[10, 30], 'min_samples_leaf':[1,3]}, n_jobs=3) clf.fit(train_x, train_y) pred = clf.predict(test_x) auprc = sklearn.metrics.average_precision_score(test_y, pred) print("AUPRC : " + str(auprc)+", time_exec : "+str(time.time()-start))
