[site]: crossvalidated
[post_id]: 307218
[parent_id]: 307210
[tags]: 
Machine learning algorithms - including neural networks - can learn to approximate arbitrary functions, but only in the interval where there is enough density of training data. Statistics-based machine learning algorithms work best when they are performing interpolation - predicting values that are close to or in-between the training examples. Outside of your training data, you are hoping for extrapolation. But there is no easy way to achieve that. A neural network never learns a function analytically, only approximately via statistics - this is true for nearly all supervised learning ML techniques. The more advanced algorithms can get arbitrarily close to a chosen function given enough examples (and free parameters in the model), but will still only do so in the range of supplied training data. How the network (or other ML) behaves outside the range of your training data will depend on its architecture including the activation functions used. The only way to have a machine learning algorithm predict a function analytically, is to build something into the assumptions of the model. For instance (and perhaps trivially), you could create features that equalled various $\sin$ functions of your input e.g $\text{sin}(x), \text{sin}(2x+\pi/4)$. The network - or even simpler, a linear regression - would learn to associate the most predictive value which is the closest $\sin$ function.
