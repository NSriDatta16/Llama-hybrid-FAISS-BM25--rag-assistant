[site]: crossvalidated
[post_id]: 562009
[parent_id]: 562002
[tags]: 
Good news: You don't need to know Bayesian statistics for these derivations. Just a bit of probability and a bit of calculus. Let's look at $L_s$ first. Moving from the first line to the second is an application of Jensen's inequality , which rears its head often in ML. $L_s$ is a lower bound on the log-likelihood in this latent variable model. Moving from the second to the third is done by marginalizing out $S$ , using the law of total probability. It may be helpful to work in the opposite direction, from line 3 to 2 to 1, for a clearer story. We start at the log-likelihood of the model. Then we introduce a latent variable into the model. Then we lower-bound the new likelihood. Now for its partial derivative with respect to the parameters $W$ . First, recall the sum rule for derivatives. This lets you push the derivative operator into the summation. $$\frac{\partial L_s}{\partial W} = \sum_s \frac{\partial}{\partial W} p(s \mid \mathbf{a}) \log p(\mathbf{y} \mid s, \mathbf{a})$$ Second, recall the product rule . It lets us turn each summand into this: $$p(s \mid \mathbf{a}) \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{\partial p(s \mid \mathbf{a})}{\partial W}$$ This is close to what the paper presents, but not quite there. The last step we need is the log-derivative trick , which comes in on the last line belowâ€”after the factoring. \begin{align} \text{summand} &= p(s \mid \mathbf{a}) \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{\partial p(s \mid \mathbf{a})}{\partial W} \\ &= p(s \mid \mathbf{a}) \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{p(s \mid \mathbf{a})}{p(s \mid \mathbf{a})} \frac{\partial p(s \mid \mathbf{a})}{\partial W} \\ &= p(s \mid \mathbf{a}) \left[ \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{1}{p(s \mid \mathbf{a})} \frac{\partial p(s \mid \mathbf{a})}{\partial W}\right] \\ &= p(s \mid \mathbf{a}) \left[ \frac{\partial \log p(\mathbf{y} \mid s, \mathbf{a})}{\partial W} + \log p(\mathbf{y} \mid s, \mathbf{a}) \frac{\partial \log p(s \mid \mathbf{a})}{\partial W}\right] \\ \end{align} And with that, we've recreated the formula from the paper. P.S. The $p(y| s,a)$ is intentionally missing from equation (7), instead replaced by the $\frac{1}{N}$ . It's a Monte Carlo estimate of the gradient, so we replace the true probability distribution with the empirical distribution.
