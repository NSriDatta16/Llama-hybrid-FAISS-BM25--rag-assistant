[site]: crossvalidated
[post_id]: 608742
[parent_id]: 608411
[tags]: 
Yes, it is exactly as you say. LMs (and machine translation models, too) start with a randomly initialized embedding matrix, which is learned via standard backpropagation. It is typically not implemented via multiplying one-hot vectors, which would waste memory. All deep learning frameworks have embedding layers that do an index-based lookup.
