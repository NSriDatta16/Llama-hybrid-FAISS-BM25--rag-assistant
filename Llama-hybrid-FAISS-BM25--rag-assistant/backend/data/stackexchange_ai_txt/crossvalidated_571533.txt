[site]: crossvalidated
[post_id]: 571533
[parent_id]: 571532
[tags]: 
You're mixing things up here. CTC is a loss function for neural networks used for sequence-to-sequence tasks (e.g. audio to text) RNN, CNN, transformers, ... describe the architecture/type of a model Language models are a post-processing step, applied after the model computed an output for the input You can use the CTC loss with CNN-only models, RNN-only models, and all other sorts of models too as long as you have this sequential nature of the data. So I don't see a reason why not to use them for transformers too. Language models are a post-processing step and are optional. Often they can fix small errors, e.g. when the models predicts "Hella", a language model might be able to correct that and make a "Hello" out of it. But don't expect too much from them.
