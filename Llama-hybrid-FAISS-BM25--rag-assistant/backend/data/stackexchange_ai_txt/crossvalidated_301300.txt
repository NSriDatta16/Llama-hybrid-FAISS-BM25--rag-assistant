[site]: crossvalidated
[post_id]: 301300
[parent_id]: 
[tags]: 
Calculating Range based on Mean, Standard Deviation and Varying Sample Size

I have recently began studying statistics with my learning material being a book on "Basic Statistics for the Behavioral Sciences by Kenneth D. Hopkins and Gene V Glass (1978)", and so far I have understood concepts from the measures of central tendencies (i.e. the mean, median and mode) as well as the range to standard deviations. But when trying to tackle the exercises I have come across difficulties trying to understand the solutions. One problem asks to estimate the separate ranges of three samples of 10, 100 and 1000 individuals involving height with a mean of 63.5 inches and a standard deviation of 2.5 inches. The distribution is normal. The answers were stated as follows using the equation ~ range = E*(standard deviation): For n = 10, range = 3.1(2.5) = 7.75 For n = 100, range = 5(2.5) = 12.5 For n = 1000, range = 6.5(2.5) = 16.25 The issue I have is that I do not understand how the expected value, E, was calculated or why this equation works as a method for estimating the range. I would be much obliged if someone could explain this to me.
