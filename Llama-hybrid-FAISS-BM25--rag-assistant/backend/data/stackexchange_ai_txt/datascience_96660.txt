[site]: datascience
[post_id]: 96660
[parent_id]: 
[tags]: 
How to use flatten with SeqSelfAttention

I want to use SeqSelfAttention , but in final layer the dimension need to be reduced. However, adding Flatten gives following error : ValueError: The last dimension of the inputs to Dense should be defined. Found None. Model summary: Model: "sequential_3" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 200, 300) 5379900 _________________________________________________________________ bidirectional_2 (Bidirection (None, 200, 40) 51360 _________________________________________________________________ seq_self_attention_1 (SeqSel (None, None, 40) 2625 _________________________________________________________________ dropout_3 (Dropout) (None, None, 40) 0 _________________________________________________________________ dense_5 (Dense) (None, None, 5) 205 _________________________________________________________________ dense_6 (Dense) (None, None, 1) 6 ================================================================= Total params: 5,434,096 Trainable params: 54,196 Non-trainable params: 5,379,900 ______________________________________________________________ Need to add Flatten between dense_5 and dense_6 .
