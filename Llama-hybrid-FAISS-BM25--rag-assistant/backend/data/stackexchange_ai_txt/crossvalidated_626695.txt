[site]: crossvalidated
[post_id]: 626695
[parent_id]: 626122
[tags]: 
In fact there is an assumption of normality for the residuals, not for the marginal distribution of the dependent variable, as mentioned already in another answer. Now having an assumption in statistics in general means that the method is based on some theory that requires the model assumption to hold. The theory will give distributions for statistics of interest, i.e., behind tests and confidence intervals, and potentially optimality of the method in some sense. If the model assumption doesn't hold, it means that reality is different from what is assumed in the theory. This is actually always the case (nothing in reality is perfectly normally distributed, or independent for that matter). Actually it does not mean that a method will do something bad or misleading. We have a positive result if the model assumptions hold, but we normally don't have negative results if the model assumptions do not hold. Saying that the model assumptions have to be fulfilled is misleading, as in reality this is never the case, and nothing could be done. What is important is that the model assumptions are not violated in a way that will mislead the interpretation of the results . So it is important to know what kind of deviations from the model assumptions are dangerous. Extreme outliers will affect the effect estimation, and even more the estimation of the standard error, so they will often decrease the power of tests and increase the size of confidence intervals substantially. Strong skewness of the residual distribution is a problem as we normally want to estimate a center of a distribution, and under skewness it is ambiguous what the center is. The mean may not be the most efficient way to summarise such a distribution (note that in principle also regression estimates a mean, namely a conditional mean for every combination of explanatory variables). Problems get worse if in different places residual distributions have different degrees of skewness. Mild skewness at the same level at all places is rather harmless. Further possible issues exist with dependence of observations and potentially heteroscedasticity. The former is more serious, the latter may be harmless but isn't always, and sometimes it is a hint that the model can be improved or transformations are helpful. Do I really need to transform the DV to normality? As normality of the DV is not needed, see above, the answer is no. However you might wonder whether you should transform in case the residuals are not normal. A problem here is that standard methods do not only assume normality, they also assume that they are run on the raw data without having manipulated them in allow for nicer analyses . This means that if you transform the data based on not looking normal enough (or on not being significant or whatever, including removing outliers), this in itself will violate the model assumptions! (I have called this "misspecification paradox", see this paper (to appear in Journal of Data Science, Statistics, and Visualisation) ). So there are good reasons to not change the data too easily (or to do it before looking at the data on the basis of subject matter knowledge). Ultimately I'd say that model diagnostics are fine, because transforming the data or switching to nonparametric methods may do less harm than running an analysis with critical violations of its assumptions, but there is a trade-off really, and I recommend transforming the data and other data dependent actions only if it is crystal clear that these will help because model assumptions are strongly and critically violated. Outlier are best dealt with using robust methods by the way. You could run bootstrap if you want to have more valid (but not 100% valid) inference also in the non-normal case, however note that bootstrap is not necessarily robust in the sense that outliers may affect the bootstrap, and they actually also affect the estimator based on which you will do inference, be it classical or bootstrap. The bootstrap itself may require robustification.
