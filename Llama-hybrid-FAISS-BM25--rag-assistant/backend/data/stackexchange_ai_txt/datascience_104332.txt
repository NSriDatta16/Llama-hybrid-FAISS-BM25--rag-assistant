[site]: datascience
[post_id]: 104332
[parent_id]: 
[tags]: 
How to get word embedding in CBOW?

I find clear explanations for skip-gram model. We take the output weight matrix, multiply it with the one-hot vector of the word we want to get the embedding. How does it work in case of CBOW? I know we have to take one of the input weights (Wvnx) but which one?
