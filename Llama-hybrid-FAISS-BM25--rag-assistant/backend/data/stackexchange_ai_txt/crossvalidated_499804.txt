[site]: crossvalidated
[post_id]: 499804
[parent_id]: 499800
[tags]: 
This is a "trick" that applies more broadly than Bayesian statistics. It occurs whenever you know that a probability density function is proportionate to some known functional form. For example, suppose you have some non-negative function $g$ and you have a density function given by: $$p(\theta|x) \propto g(\theta).$$ In this case, the norming axiom of probability means that there is only one possible solution for the density function --- it is the density that is proportionate to stipulated function and integrates to one: $$p(\theta|x) = \frac{g(\theta)}{\int g(\theta) d\theta}.$$ Because there is a unique density function that is proportionate to any non-negative function $g$ , this means that you can find densities just by "recognising" their proportionate forms (i.e., what they look like without their constant of integration). Here are a few of the continuous distributions: $$\begin{matrix} \text{Normal density} & \text{N}(\theta|\mu, \sigma^2) \propto \exp \Big( -\frac{1}{2} (\frac{\theta - \mu}{\sigma})^2 \Big) & \text{ for } \theta \in \mathbb{R}, \\[6pt] \text{Student T density} & \quad \text{St}(\theta|\nu) \propto \Big( 1 + \frac{\theta^2}{\nu} \Big)^{-(\nu-1)/2} \ & \text{ for } \theta \in \mathbb{R}, \\[12pt] \text{Gamma density} & \text{Ga}(\theta|\alpha, \lambda) \propto \theta^{\alpha-1} \exp (- \theta/\lambda) \quad & \text{ for } \theta \geqslant 0, \\[16pt] \text{Weibull density} & \quad \text{Wei}(\theta|\alpha, \beta) \propto \theta^{k-1} \exp (- (\theta/\lambda)^k) \quad \ \ & \text{ for } \theta \geqslant 0, \\[16pt] \text{Beta density} & \quad \text{Be}(\theta|\alpha, \beta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1} \quad \quad \ & \quad \ \ \ \text{ for } 0 \leqslant \theta \leqslant 1. \\[12pt] \end{matrix}$$ In the context of Bayesian statistics, it is common to use proportionality to simplify the derivation of posterior densities, using the kind of working shown in your question. In this context we generally ignore constant multiplicative terms and just find what function the posterior is proportionate to. We then either "recognise" this as a standard distributional form, or if it is not a standard form we determine the constant of proportionality by integrating the function over its full range. (Sometimes there is no closed-form expression for the constant of integration, in which case we use numerical methods to estimate this constant, or we generate values from the posterior using MCMC methods.) By ignoring multiplicative constants in our working, and then bringing them back in in the last step, we greatly simplify Bayesian analysis. Note that the above result is a general probability rule that applies more broadly than Bayesian statistics, but it is particularly helpful in this context because of the standard result that the posterior is proportionate to the product of the likelihood and prior.
