[site]: datascience
[post_id]: 64805
[parent_id]: 64796
[tags]: 
Linear regression is a parametric model: it assumes the target variable can be expressed as a linear combination of the independent variables (plus error). Gradient boosted trees are nonparametric: they will approximate any* function. Xgboost deprecated the objective reg:linear precisely because of this confusion . It has been replaced by reg:squarederror , and has always meant minimizing the squared error, just as in linear regression. So xgboost will generally fit training data much better than linear regression, but that also means it is prone to overfitting, and it is less easily interpreted. Either one may end up being better, depending on your data and your needs.
