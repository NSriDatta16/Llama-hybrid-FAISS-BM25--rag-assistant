[site]: datascience
[post_id]: 16797
[parent_id]: 
[tags]: 
What does the notation mAP@[.5:.95] mean?

For detection, a common way to determine if one object proposal was right is Intersection over Union (IoU, IU). This takes the set $A$ of proposed object pixels and the set of true object pixels $B$ and calculates: $$IoU(A, B) = \frac{A \cap B}{A \cup B}$$ Commonly, IoU > 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the True Positive ($TP(c)$): a proposal was made for class $c$ and there actually was an object of class $c$ False Positive ($FP(c)$): a proposal was made for class $c$, but there is no object of class $c$ Average Precision for class $c$: $\frac{\#TP(c)}{\#TP(c) + \#FP(c)}$ The mAP (mean average precision) = $\frac{1}{|classes|}\sum_{c \in classes} \frac{\#TP(c)}{\#TP(c) + \#FP(c)}$ If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where $p \in (0, 1)$ is the IoU. But what does mAP@[.5:.95] (as found in this paper ) mean?
