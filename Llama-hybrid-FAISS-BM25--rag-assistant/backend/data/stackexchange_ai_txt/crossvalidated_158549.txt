[site]: crossvalidated
[post_id]: 158549
[parent_id]: 
[tags]: 
Why are activation functions needed in neural networks?

Why are activation functions needed in neural networks? I know that it is to capture "non-linearities", but I have never been able to find a proper down-to-earth explanation. In particular, I am curious about the ReLU function as it is quite effective, but seems like such a simple function. How does that capture non-linearities?
