[site]: crossvalidated
[post_id]: 404354
[parent_id]: 339897
[tags]: 
Can someone give examples of each under different modes (e.g. what is the MLE in a Discrete Naive Bayes or in Logistic Regression), also how they are related to the loss functions? When we deal with machine learning algorithms we are: 1) specifying a probabilistic model that has parameters. For example the parameters in logistic regression and naive bayes in this answer . 2) learning the value of those parameters from data(sometimes maybe from some experts). Normally there are two methods: Maximum Likelihood Estimation(MLE) and Maximum A Prosteriori(MAP). And the key point of MLE is that after training the learned parameters can make the observed data the most likely: $\theta_{ML}=\arg \max E_{x\sim \hat p_{data}}\log p_{model}(x; \theta)$ . Source: Deep Learning Book 5.5 . For an example you can see the 4.2 of this tutorial . To get the parameters that can make the observed data most likely we need to get the likelihood function and to optimize the value of it by tuning the parameters. $L(\theta)=\prod_{i=1}^n f(X_i|\theta)$ . Other references: Stanford CS109 Parameter Estimation
