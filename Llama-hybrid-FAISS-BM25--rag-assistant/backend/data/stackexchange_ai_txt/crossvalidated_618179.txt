[site]: crossvalidated
[post_id]: 618179
[parent_id]: 
[tags]: 
Re-training the entire time series after cross-validation?

In his book, Hands-on Time Series analysis with R, the author Rami Krispin says ' Typically, once we have trained and tested the model using the training and testing partitions, we will retrain the model with all of the data (or at least the most recent observation in the chronological order) '. My question is this: In time series cross-validation methods such as expanding and sliding window, the most recent observations fall within the test set, of course due to the chronological order. Intuitively, the most recent observations can be the most influential predictors, although this is not always true. But, for the cases where the most recent observations are predictive, aren’t we missing the information from the most recent observations by not using them for training? If so, what are your thoughts on measuring the model performance using one of the time-series cross validation method first but then re-training the entire data for the final model, as Rami suggest? But, when using the entire data for training, there is the danger of overfitting and, no validation. Also, let’s say I put aside the last 10% of the time series as a test set for out of sample predictions. Now, the remaining 90% is the total train-validation set. When using a cross-validation method, the validation set (say another 10%) must be also the most recent, chronologically. At maximum, the remaining 80% is all I have for my model training and parameter tuning. After the cross-validation step, I have now a single chosen model with determined hyperparameters. Next, I retrain the entire 90% with this model and have the adjusted new parameters (based on the 90%), but still the model type itself was selected using the first 80% of the data. For example, if I am looking at a 10 year of historical data, my model is selected based on the first 8 years and that makes me wonder as well. Any thoughts? Thanks.
