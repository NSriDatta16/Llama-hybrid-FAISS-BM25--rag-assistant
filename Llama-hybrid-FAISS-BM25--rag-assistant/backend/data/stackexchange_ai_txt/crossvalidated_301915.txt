[site]: crossvalidated
[post_id]: 301915
[parent_id]: 301911
[tags]: 
1) That's called a kernel matrix. Whether you need to calculate it depends on the method, for example in Kernel PCA you need to do this, and SVMs with kernels don't need this. 2) The point is to use nonlinear $\phi$ so that decision functions that depend on it are nonlinear, but you don't have to actually compute $\phi$, which possibly even embeds your examples into infinite-dimensional space (as RBF kernel) 3) Yes. Note that $\varphi(x)$'s components are polynomials in $x$'s entries. That's nonlinearity. For pictorial examples I made these sometimes ago (look for Kernel PCA transform of circles). You can see that, with RBF kernel's $\varphi$, data that is not linearly separable gets mapped to a space in which it is (since it's PCA is linearly separable). Kernelized SVMs depend on this fact, and they do something similar in the image of $\varphi$.
