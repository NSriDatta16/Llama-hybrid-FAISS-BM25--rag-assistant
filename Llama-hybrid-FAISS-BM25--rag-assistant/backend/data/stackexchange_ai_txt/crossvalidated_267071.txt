[site]: crossvalidated
[post_id]: 267071
[parent_id]: 
[tags]: 
Confusion about conversion from index to matrix notation when trace involved

This is in a similar vein to a recent question I asked, however I feel that it is sufficiently different to warrant another question. Again, this is from Andrew Ng's excellent freely available machine learning class. This question asks for the conversion from index notation to matrix vector notation of the cost function of the multivariate least squares given as: $$ \begin{align} J(\theta) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^p \Big( (\theta^T x^{(i)})_j - y^{(i)}_j \Big)^2 \end{align} $$ where $\theta \in \mathbb{R}^{n \times p}$, $X \in \mathbb{R}^{m \times n}$, $Y \in \mathbb{R}^{m \times p}$. Rewriting this as: $$ \begin{align} J(\theta) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^p \Big( u^{(i)}_j - y^{(i)}_j \Big)^2 \end{align} $$ where $u^{(i)}_j = (\theta^T x^{(i)})_j$, I can then convert the inner sum to a dot product: $$ \begin{align} J(\theta) = \frac{1}{2} \sum_{i=1}^m \big( u^{(i)} - y^{(i)} \big)^T \big( u^{(i)} - y^{(i)} \big) \end{align} $$ From here it seems that the next step would be to conclude that this is equivalent to: $$ \begin{align} J(\theta) &= \frac{1}{2} tr \Big( \big( U - Y \big)^T \big( U - Y \big) \Big) \\ &= \frac{1}{2} tr \Big( \big( X \theta - Y\big)^T \big( X \theta - Y \big) \Big) \end{align} $$ However I am missing the leap from reducing the outer summation. Again, I appreciate all the help.
