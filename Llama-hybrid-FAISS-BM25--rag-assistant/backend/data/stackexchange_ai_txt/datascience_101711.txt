[site]: datascience
[post_id]: 101711
[parent_id]: 101707
[tags]: 
The transformer architecture is not different from other architectures, in the sense that it has layers, trainable parameters and is trained with gradient descent techniques. Therefore, it can be subject to federated learning. However, Transformer models are normally very large in comparison with other architectures like LSTMs, and pose problems in the federated setup, specifically slow and unstable convergence. You may have a look at variants of the Transformer specially made for the federated setup, e.g.: Federated Learning with Dynamic Transformer for Text to Speech (INTERSPEECH'21)
