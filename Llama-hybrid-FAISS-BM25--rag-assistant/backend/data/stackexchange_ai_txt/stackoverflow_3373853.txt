[site]: stackoverflow
[post_id]: 3373853
[parent_id]: 3372327
[tags]: 
If you can settle for approximate probabilities and want something scalable, Gibbs sampling might work. The basic idea is pretty simple: start with the all-rows explanation and repeat the following to sample a bunch of explanations. Choose a random row. Flip a coin. If the coin came up heads, add the row to the explanation (do nothing if it's already there). If the coin came up tails, attempt to remove the row from the explanation. If the result is not an explanation, put the row back. In the limit, the fraction of the samples containing a given row converges to its true value. There are some practical implementations under the keywords "Bayesian inference using Gibbs sampling" (you have a uniform prior and observe that for each column, the disjunction of the rows incident with it is true). Since I'm not an expert in this stuff, though, I can't advise you as to the hazards of rolling your own.
