[site]: crossvalidated
[post_id]: 197631
[parent_id]: 197630
[tags]: 
Imputing missing data with the average is a simple solution that is deeply flawed for the kind of obvious reason that it creates an abnormal spike in the frequency of the PDF at that average value. There are many approaches to imputation. These include various flavors of sorting (cold deck, hot deck) to find a "nearest neighbor" or closest value across a wide range of cross-classified information as well as regression-based, multiple imputation. For sophistication as well as the fact that it does the least damage to the observed marginals, model-based methods are to be preferred. For a good review, see Paul Allison's Sage book, Missing Data , which has the advantage of including an applied focus. http://www.amazon.com/Missing-Quantitative-Applications-Social-Sciences/dp/0761916725/ref=sr_1_1?ie=UTF8&qid=1455986179&sr=8-1&keywords=paul+allison+missing But the canonical reference is probably Little and Rubin's Statistical Analysis With Missing Data which develops a typology of missingness -- Missing at Random, Missing Completely at Random, etc. -- as well as a protocol for making imputation work. http://www.amazon.com/Statistical-Analysis-Missing-Probability-Statistics/dp/0471802549/ref=sr_1_1?ie=UTF8&qid=1455986237&sr=8-1&keywords=roderick+little+imputation
