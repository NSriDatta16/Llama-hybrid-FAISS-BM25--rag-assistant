[site]: crossvalidated
[post_id]: 237246
[parent_id]: 
[tags]: 
Machine learning or statistical models that account for time evolution and underlying system changes

I wonder if there are some algorithms that can account for underlying system dynamics over time. One possible situation can be the following: in a ticket reporting data, a data point arrives when a problem is reported, and a ticket log is created (it contains information on ticket characteristics). These data points are not i.i.d - I can assume they are i.i.d. in a short time frame, but in the long run, system changes (for example, a previous problem got fixed, system got upgraded, etc., which are not quantified or recorded), so it is not appropriate to assume one same distribution for earlier data and current data because they are generated by users living under different system conditions (and there is no information on how often the system changes, plus the change is generally continuous and even different for different aspects). They are probably still independent since tickets are generated by different users, but not quite identically as time evolves. It seems to me most time series algorithms still assume i.i.d - please correct me if I am wrong - so I am not sure if I can directly make use of them. In addition, I am targeting at a classification problem. One naive approach I have is to keep updating the model, for example, every $q$ days based on only the most recent information (e.g. within last $p$ days). I am not sure if this is a valid one, but one problem is there are many tuning parameters here such as $p$ and $q$. While this can possibly be done heuristically, I wonder if this can be modeled analytically. It will be great if I can be pointed to existing algorithms, if any, that already aim to address this kind of scenario, literatures that talk about this or even keywords to search for. Thanks!
