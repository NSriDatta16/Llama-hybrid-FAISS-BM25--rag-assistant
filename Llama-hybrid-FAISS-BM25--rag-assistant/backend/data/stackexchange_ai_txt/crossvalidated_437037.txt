[site]: crossvalidated
[post_id]: 437037
[parent_id]: 324992
[tags]: 
The embedding layer is just a projection from discrete and sparse 1-hot-vector into a continuous and dense latent space. It is a matrix of size ( n , m ) where n is your vocabulary size and m is your desired latent space dimensions. Only in practice, there's no need to actually do the matrix multiplication, and instead you can save on computation by using the index. So in practice, it is a layer that maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). You could train it to create a Word2Vec embedding by using Skip-Gram or CBOW. Or you can train it on your specific problem to get an embedding suited for your specific task at hand. You could also load pre-trained embeddings (like Word2Vec, GloVe etc.) and then continue training on your specific problem (a form of transfer learning).
