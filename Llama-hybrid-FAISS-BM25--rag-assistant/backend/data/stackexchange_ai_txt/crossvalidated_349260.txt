[site]: crossvalidated
[post_id]: 349260
[parent_id]: 
[tags]: 
Regression on a small dataset

I have a small dataset of 23 y-values and 10 features of X (i.e., (23,10) matrix). I standardized the input data, imputed few missing values with means (around 5 values) and did linear regression, random forests, NN-MLP and SVR with scikit. SVR & NN-MLP seem to be overfit, and linear regression and random forests are giving adj. r-squared values of 0.42 and 0.58. I checked for collinearity and most of the variables except one seem uncorrelated. I compared the beta coefficients from linear regression and when I choose the top six contributors random forests gives a better adj. r-squared of 0.77. When I use leave one out (LOO) cross validation with maximizing neg_mean_absolute_error the CV score is bad (came out to be -1.3). My question is whether CV is valid in my case? As I searched through the forum I see two answers in which one says to use LOO and in the other they talk about boot strapping and feature selection methods. Is there any thumb-rule on sample size and number of features? Another concern about picking dominant variables: condition no., pairwise correlations and variance inflation factors, slopes of individual fits, didn't yield much information. Any other proper way to do sensitivity analysis? I want to come up with correlations of X_i with y and it is hard to get any more data soon, if my entire approach is wrong can you please suggest a way?
