[site]: crossvalidated
[post_id]: 418463
[parent_id]: 
[tags]: 
When and how to re-evaluate deployed models

There are a lot questions about how to train a model (family) or how to tune hyperparmeter. But there are surprisingly few question about how to monitor or evaluate a model already in production (exceptions are a similar question without any answer here or one in the Data Science StackExchange). Background: I have deployed a forecasting model (a regularized linear model) which generate forecasts multiple times a day. I thoroughly tested them against alternatives (e.g. time series models or LSTMs) and re-train my model automatically once a month. I monitor the predictive performance regularly and to observe whether the accuracy goes down. My questions: Am I doing everything I am supposed to do or do I miss anything? Since I am lazy, I do not take action as long as the predictive performance does not detoriate (the model and the regularization parameter will be re-trained each month based on an enriched dataset but I do not perform for example a complete feature engineering process). Is it a best practice to also regularly look at alternative models which might have been worse in the initial model selection process but which might become better over time (since the datasize continously increases, one can assume that LSTMs might outperform my model at some point)? Is there any rule of thumb when to perform again a complete re-evaluation of my models in production against alternatives? Any hint to a good reference is very welcome.
