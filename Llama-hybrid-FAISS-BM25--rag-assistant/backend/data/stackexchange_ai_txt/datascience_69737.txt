[site]: datascience
[post_id]: 69737
[parent_id]: 
[tags]: 
Fastest way for 1 vs all lookup on embeddings

I have a dataset with about 1 000 000 texts where I have computed their sentence embeddings with a language model and stored them in a numpy array. I wish to compare a new unseen text to all the 1 000 000 pre-computed embeddings and perform cosine similarity to retrieve the most semantic similar document in the corpus. What is the most efficient to perform this 1-vs-all comparison? I would thankful for any pointers and feedback!
