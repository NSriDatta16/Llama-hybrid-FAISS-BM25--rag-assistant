[site]: datascience
[post_id]: 104482
[parent_id]: 103983
[tags]: 
As you are looking for information from reputed resources, Tutorial why produces different results: gives reasoning why simple ML algorithm give better performance and more stable compared to neural network. Paper on industrial recognition tasks: for small amounts of training data, classical classifiers provided better performance to not pre-trained neural networks Paper on Heart Failure: compares performance of deep learning versus logistic regression Paper on landslide susceptibility assessment: proposes DLNN model had a higher performance than the four benchmark models; Multi Layer Preceptron Neural Network, a Support Vector Machine, a C4.5-Decision Tree model and a Random Forest model RESOURCE 1 - Tutorial : This study different results each time in machine learning , compares simpler ML algorithms(linear regression and logistic regression) with neural networks and explains why results vary Algorithm's sensitivity to specific data High Variance : Algorithm is more sensitive to the specific data used during training. Low Variance : Algorithm is less sensitive to the specific data used during training. It is said that simpler algorithms like linear regression and logistic regression have a lower variance than other types of algorithms. Considering in your observation, LSTM having high standard deviation shows its more sensitive than classic machine learning model(logistic regression) To ensure low variance : Change hyperparameter, change size of training dataset and change to simpler algorithms. Nature of algorithm Deterministic machine learning algorithms : That means, when the algorithm is given the same dataset, it learns the same model every time. An example is a linear regression or logistic regression algorithm. Stochastic algorithms(not deterministic) : Their behaviour incorporates elements of randomness. An example of an algorithm that uses randomness during learning is a neural network. It uses randomness in two ways: Random initial weights (model coefficients) and Random shuffle of samples each epoch . Neural networks (deep learning) are a stochastic machine learning algorithm. The random initial weights allow the model to try learning from a different starting point in the search space each algorithm run and allow the learning algorithm to “break symmetry” during learning. The random shuffle of examples during training ensures that each gradient estimate and weight update is slightly different. Solution: Controlling the randomness used by algorithms ensuring each time algorithm is run it gets the same randomness Evaluation Procedure The two most common evaluation procedures are a train-test split and k-fold cross-validation. These model evaluation procedures are stochastic, small decisions made in the process involve randomness. Observation Order The order that the observations are exposed to the model affects internal decisions. Some algorithms are especially susceptible to this, like neural networks RESOURCE 2 - Research paper on industrial recognition tasks Comparison of the performance of innovative deep learning and classical methods of machine learning to solve industrial recognition tasks Comparisons were made using the recognition rates achieved with five real data sets from industrial applications. The results showed that not pre-trained neural networks produce worse results than classical classifiers with the given small amounts of data for training. Deep neural networks require an extremely large amount of data for training in order to develop a good generalization capability and thus deliver good results Even if many training objects are available, deep neural networks remain susceptible to overfitting. Due to the large amount of data to be processed, the training also takes up a lot of time and can take days or even weeks 10. Very high computing power is required for the effective use of these methods 22. For this reason, such applications should be processed by the Graphics Processing Unit (GPU) instead of the Central Processing Unit (CPU) for saving time 10. In addition, many parameters have to be set and optimized, e.g. initial weights, activation function, learning rate, batch size 23. According to Bengio 21, the quality of the results depends on the initial values.Since neural networks are black boxes, the decision-making process is not user-comprehensible. RESOURCE 3 - Research paper on Heart Failure Neural networks Vs Logistic regression, 30 days all-cause readmission prediction The question of deep learning versus logistic regression for readmission prediction for Heart Failure, shows that logistic regression with regularization matches the best neural network performance. RESOURCE 4 - Research paper on landslide susceptibility assessment Comparing the prediction performance of a Deep Learning Neural Network model with conventional machine learning models in landslide susceptibility assessment The learning ability of the DLNN model has been evaluated and compared with a Multi Layer Preceptron Neural Network, a Support Vector Machine, a C4.5-Decision Tree model and a Random Forest model using the training dataset, whereas the predictive performance of each model has been evaluated and compared using the validation datasets. In order to evaluate their learning and predictive capacity of each model the classification accuracy, the sensitivity, the specificity and the area under the success and predictive rate curves (AUC) were calculated. Results showed that the proposed DLNN model had a higher performance than the four benchmark models. Although DLNN has been used seldom in landslide susceptibility assessments, the study highlights that the usage of deep learning approach could be considered as a satisfactory alternative approach for landslide susceptibility mapping. References : Different results each time in machine learning How to reduce model variance Randomness in machine learning Comparison of the performance of innovative deep learning and classical methods of machine learning to solve industrial recognition tasks Comparing the prediction performance of a Deep Learning Neural Network model with conventional machine learning models in landslide susceptibility assessment Neural networks versus Logistic regression for 30 days all-cause readmission prediction
