[site]: crossvalidated
[post_id]: 478558
[parent_id]: 476424
[tags]: 
ARIMA!!! - a marvel of theoretical rigor and mathematical elegance that is almost useless for any realistic business time series. Ok, that is an exaggeration: ARIMA and similar models like GARCH are occasionally useful. But ARIMA is not nearly as general purpose a model as most people seem to think it is. Most competent Data Scientists and ML Engineers who are generalists (in the sense that they don't specialize in time series forecasting or econometrics), as well as MBA types and people with solid general statistics backgrounds, will default to ARIMA as the baseline model for a time series forecasting problem. Most of the time they end up sticking with it. When they do evaluate it against other models, it is usually against more exotic entities like Deep Learning Models, XGBoost, etc... On the other hand, most time series specialists, supply chain analysts, experienced demand forecasting analysts, etc...stay away from ARIMA. The accepted baseline model and the one that is still very hard to beat is Holt-Winters, or Triple Exponential Smoothing. See for example "Why the damped trend works" by E S Gardner Jr & E McKenzie . Beyond academic forecasting, many enterprise grade forecasting solutions in the demand forecasting and the supply chain space still use some variation of Holt-Winters. This isn't corporate inertia or bad design, it is simply the case that Holt-Winters or Damped Holt-Winters is still the best overall approach in terms of robustness and average overall accuracy. A brief history lesson: Some history might be useful here: Exponential Smoothing models, Simple ES, Holt's model, and Holt-Winters, were developed in the 50s. They proved to be very useful and pragmatic, but were completely "ad-hoc". They had no underlying statistical theory or first principles - they were more of a case of: How can we extrapolate time series into the future? Moving averages are a good first step, but we need to make the moving average more responsive to recent observations. Why don't we just add an $\alpha$ parameter that gives more importance to recent observation? - This was how simple exponential smoothing was invented. Holt and Holt-Winters were simply the same idea, but with the trend and seasonality split up and then estimated with their own weighted moving average models (hence the additional $\beta$ and $\gamma$ parameters). In fact, in the original formulations of ES, the parameters $\alpha$ , $\beta$ , and $\gamma$ were chosen manually based on their gut feeling and domain knowledge. Even today, I occasionally have to respond to requests of the type "The sales for this particular product division are highly reactive, can you please override the automated model selection process and set $\alpha$ to 0.95 for us" (Ahhh - thinking to myself - why don't y'all set it to a naive forecast then??? But I am an engineer, so I can't say things like that to a business person). Anyway, ARIMA, which was proposed in the 1970s, was in some ways a direct response to Exponential Smoothing models. While engineers loved ES models, statisticians were horrified by them. They yearned for a model that had at least some theoretical justification to it. And that is exactly what Box and Jenkins did when they came up with ARIMA models. Instead of the ad-hoc pragmatism of ES models, the ARIMA approach was built from the ground up using sound first principles and highly rigorous theoretical considerations. And ARIMA models are indeed very elegant and theoretically compelling. Even if you don't ever deploy a single ARIMA model to production in your whole life, I still highly recommend that anyone interested in time series forecasting dedicate some time to fully grasping the theory behind how ARIMA works, because it will give a very good understanding of how time series behave in general. But ARIMA never did well empirically, see here . Hyndman writes (and quotes others): Many of the discussants seem to have been enamoured with ARIMA models. “It is amazing to me, however, that after all this exercise in identifying models, transforming and so on, that the autoregressive moving averages come out so badly. I wonder whether it might be partly due to the authors not using the backwards forecasting approach to obtain the initial errors”. — W.G. Gilchrist “I find it hard to believe that Box-Jenkins, if properly applied, can actually be worse than so many of the simple methods”. — Chris Chatfield At times, the discussion degenerated to questioning the competency of the authors: “Why do empirical studies sometimes give different answers? It may depend on the selected sample of time series, but I suspect it is more likely to depend on the skill of the analyst … these authors are more at home with simple procedures than with Box-Jenkins”. — Chris Chatfield When ARIMA performs well, it does so only because the models selected are equivalent to Exponential Smoothing models (there is some overlap between the ARIMA family and the ES family for $ARIMA(p,d,q)$ with low values of $p$ , $d$ , and $q$ - see here and here for details). I recall once working with a very smart business forecaster who had a strong statistics background and who was unhappy that our production system was using exponential smoothing, and wanted us to shift to ARIMA instead. So him and I worked together to test some ARIMA models. He shared with me that in his previous jobs, there was some informal wisdom around the fact that ARIMA models should never have values of $p$ , $d$ , or $q$ higher than 2. Ironically, this meant that the ARIMA models we were testing were all identical to or very close to ES models. It is not my colleague's fault though that he missed this irony. Most introductory graduate and MBA level material on time series modeling focus significantly or entirely on ARIMA and imply (even if they don't explicitly say so) that it is the end all be all of statistical forecasting. This is likely a holdover from the mind set that Hyndman referred to in the 70s, of academic forecasting experts being "enamored" with ARIMA. Additionally, the general framework that unifies ARIMA and ES models is a relatively recent development and isn't always covered in introductory texts, and is also significantly more involved mathematically than the basic formulations of both ARIMA and ES models (I have to confess I haven't completely wrapped my head around it yet myself). Ok, why does ARIMA perform so poorly? Several reasons, listed in no particular order of importance: ARIMA requires polynomial trends: Differencing is used to remove the trend from a time series in order to make it mean stationary, so that autoregressive models are applicable. See this previous post for details . Consider a time series $$Y(t)=L(t)+T(t)$$ with $L$ the level and $T$ the trend (most of what I am saying is applicable to seasonal time series as well, but for simplicity's sake I will stick to the case trend only). Removing the trend amounts to applying a transformation that will map $T(t)$ to a constant $T=c$ . Intuitively, the differencing component of ARIMA is the discrete time equivalent of differentiation. That is, for a discrete time series $Y$ that has an equivalent continuous time series $Y_c$ , setting $d = 1$ ( $Y_n'= Y_n - Y_{n-1}$ ) is equivalent to calculating $$\frac{dY_c}{dt}$$ and setting $d=2$ is equivalent to $$\frac{d^2Y_c}{dt^2}$$ etc...now consider what type of continuous curves can be transformed into constants by successive differentiation? Only polynomials of the form $T(t)=a_nt^n+a_{n-1}t^{n-1}...+a_1t+a_0$ (only? It's been a while since I studied calculus...) - note that a linear trend is the special case where $T(t)=a_1t+a_0$ . For all other curves, no number of successive differentiations will lead to a constant value (consider and exponential curve or a sine wave, etc...). Same thing for discrete time differencing: it only transfroms the series into a mean stationary one if the trend is polynomial. But how many real world time series will have a higher order ( $n>2$ ) polynomial trend? Very few if any at all. Hence selecting an order $d>2$ is a recipe for overfitting (and manually selected ARIMA models do indeed overfit often). And for lower order trends, $d=0,1,2$ , you're in exponential smoothing territory (again, see the equivalence table here ). ARIMA models assume a very specific data generating process : Data generating process generally refers to the "true" model that describes our data if we were able to observe it directly without errors or noise. For example an $ARIMA(2,0,0)$ model can be written as $$Y_t = a_1Y_{t-1}+a_2Y_{t-2}+c+ \epsilon_t$$ with $\epsilon_t$ modeling the errors and noise and the true model being $$\hat{Y}_t = a_1\hat{Y}_{t-1}+a_2\hat{Y}_{t-2}+c$$ but very few business time series have such a "true model", e.g why would a sales demand signal or a DC capacity time series ever have a DGP that corresponds to $$\hat{Y}_t = a_1\hat{Y}_{t-1}+a_2\hat{Y}_{t-2}+c??$$ If we look a little bit deeper into the structure of ARIMA models, we realize that they are in fact very complex models. An ARIMA model first removes the trend and the seasonality, and then looks at the residuals and tries to model them as a linear regression against passed values (hence "auto"-regression) - this will only work if the residuals do indeed have some complex underlying deterministic process. But many (most) business time series barely have enough signal in them to properly capture the trend and the seasonality, let alone remove them and then find additional autoregressive structure in the residuals. Most univariate business time series data is either too noisy or too sparse for that. That is why Holt-Winters, and more recently Facebook Prophet are so popular: They do away with looking for any complex pattern in the residuals and just model them as a moving average or don't bother modeling them at all (in Prophet's case), and focus mainly on capturing the dynamics of the seasonality and the trend. In short, ARIMA models are actually pretty complex, and complexity often leads to overfitting. Sometimes autoregressive processes are justified. But because of stationarity requirements, ARIMA AR processes are very weird and counter intuitive : Let's try to look at what types of processes correspond in fact to an auto-regressive process - i.e. what time series would actually have an underlying DGP that corresponds to an $AR(p)$ model. This is possible for example with a cell population growth model, where each cell reproduces by dividing into to 2, and hence the population $P(t_n)$ could reasonably be approximated by $P_n = 2P_{n-1}+\epsilon_t$ . Because here $a=2$ ( $>1$ ), the process is not stationary and can't be modeled using ARIMA. Nor are most "natural" $AR(p)$ models that have a true model of the form $$\hat{Y}_t = a_1\hat{Y}_{t-1}+a_2\hat{Y}_{t-2}...+a_p\hat{Y}_{t-p}+c$$ This is because of the stationarity requirement: In order for the mean $c$ to remain constant, there are very stringent requirements on the values of $a_1,a_2,...,a_p$ (see this previous post ) to insure that $\hat{Y}_t$ never strays too far from the mean. Basically, $a_1,a_2,...,a_p$ have to sort of cancel each other out $$\sum_{j=1}^pa_j otherwise the model is not stationary (this is what all that stuff about unit roots and Z-transforms is about). This implication leads to very weird DGPs if we were to consider them as "true models" of a business time series: e.g. we have a sales time series or an electricity load time series, etc...what type of causal relationships would have to occur in order to insure that $$\sum_{j=1}^pa_j e.g. what type of economic or social process could ever lead to a situation where the detrended sales for 3 weeks ago are always equal to negative the sum of the sales from 2 weeks ago and the sales from last week? Such a process would be outlandish to say the least. To recap: While there are real world processes that can correspond to an autoregressive model, they are almost never stationary (if anyone can think of a counter example - that is a naturally occurring stationary AR(p) process, please share, I've been searching for one for a while) . A stationary AR(p) process behaves in weird and counter intuitive ways (more or less oscillating around the mean) that make them very hard to fit to business time series data in a naturally explainable way. Hyndman mentions this (using stronger words than mine) in the aforementioned paper: This reveals a view commonly held (even today) that there is some single model that describes the data generating process, and that the job of a forecaster is to find it. This seems patently absurd to me — real data come from processes that are much more complicated, non-linear and non-stationary than any model we might dream up — and George Box himself famously dismissed it saying, “All models are wrong but some are useful”. But what about the 'good' ARIMA tools? At this point would point out to some modern tools and packages that use ARIMA and perform very well on most reasonable time series (not too noisy or too sparse), such as auto.arima() from the R Forecast package or BigQuery ARIMA. These tools in fact rely on sophisticated model selection procedures which do a pretty good job of ensuring that the $p,d,q$ orders selected are optimal (BigQuery ARIMA also uses far more sophisticated seasonality and trend modeling than the standard ARIMA and SARIMA models do). In other words, they are not your grandparent's ARIMA (nor the one taught in most introductory graduate texts...) and will usually generate models with low $p,d,q$ values anyway (after proper pre-processing of course). In fact now that I think of it, I don't recall ever using auto.arima() on a work related time series and getting $p,d,q > 1$ , although I did get a value of $q=3$ once using auto.arima() on the Air Passengers time series. Conclusion Learn traditional ARIMA models in and out, but don't use them. Stick to state space models (ES incredibly sophisticated descendants) or use modern automated ARIMA model packages (which are very similar to state space models under the hood anyway).
