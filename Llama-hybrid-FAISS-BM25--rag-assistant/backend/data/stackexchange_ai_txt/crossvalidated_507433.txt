[site]: crossvalidated
[post_id]: 507433
[parent_id]: 
[tags]: 
Is it like the concept of dummy trap doesn't exist is machine learning apart from Logistic Regression & Multiple Linear Regression?

While performing MLR & Logistic Regression model summary analysis I have seen the problem of perfect multicolinearity if we use one hot encoding without dropping a single feature. Is it necessary to drop a single feature along with one hot encoding for all types of algorithms ? I got an intuition that apart from MLR & Logistic Regression Algorithms, the concept of dummy trap doesn't exist for other algorithms like SVM, Decision Tree, Random Forest, KNN, Kmeans, AdaBoost, GradientBoost (Classification & Regression), XGBoost (Classification & Regression). I need proper & specific guidance about this.
