[site]: datascience
[post_id]: 8189
[parent_id]: 8188
[tags]: 
Consider the following two sentences: My awesome girlfriend bought me a delicious popsicle at the store. 0--1-------0----------0------0--0-1---------0--------0--0---0-----:2:11 My awesome girlfriend, Joyce, drove to the grocery store to buy me a delicious Dole popsicle. 0---1------0-----------0------0-----0--0---0-------0-----0--0---0--0-1---------0----0-------:2:16 See how you can sum the values and get 2 for both, but average the sentiments and get 2/11 vs. 2/16 . There are cases where people have seen better results measuring the total number of good sentiment words rather than letting zero value words cloud the picture. In this case, I would argue that the sentences have the same sentiment and averaging artificially pulls down the sentiment of the second sentence. In certain methods like TFIDF , the word vectors are normalized against are relevant corpus, so this may change things. In other methods, the stop words may have already been removed, so this has less effect. In twitter examples, users are limited to 140 characters, so one might consider this to be an inherent denominator which makes averaging really averages of averages. Its very dependent on the specific application and methodology. Without a more specific reference, it is difficult to be more specific in my answer. Hope this helps!
