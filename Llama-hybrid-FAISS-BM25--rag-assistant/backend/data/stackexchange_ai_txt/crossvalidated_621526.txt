[site]: crossvalidated
[post_id]: 621526
[parent_id]: 621152
[tags]: 
I think I may see part of the misunderstanding here: "Intuitively, in the real-world we do not know what is the relationship between label and data. So why would bother creating a relationship between them?" because a key goal of the analysis may be to understand or quantify the relationship between them, rather than just to make good predictions. We want to understand the data generating process. When we use a linear model (they are used in machine learning as well), it isn't because we think that the real world data follow an exactly linear law, just that it is a reasonable approximation that we can understand. This is not unreasonable, in science and engineering we often use Taylor series expansions of non-linear relationships, and a linear model is just a first order Taylor series approximation. If that approximation is good then the model can help us understand the data. All models are approximations - hence "all models are wrong, but some are useful". However, as you point out, statisticians also use non-linear and more opaque non-parametric methods, where again there is little or no assumption made about the true form of the relationship between inputs and outputs. If you are looking for a clear distinction between machine learning models and statistical models, you won't find one, because it doesn't exist. Almost all of machine learning is a branch of statistics. If you want a machine learning book that emphasises this, I can recommend the works of Chris Bishop and Kevin Murphy,
