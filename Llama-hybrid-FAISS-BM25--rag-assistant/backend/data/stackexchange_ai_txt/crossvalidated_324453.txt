[site]: crossvalidated
[post_id]: 324453
[parent_id]: 323570
[tags]: 
A larger training set almost inevitably implies greater variability of examples. Thus, pulling random batches during training will yield, on average, a higher entropy (across batches, not within). And so, the function you try to approximate gets more complex and you need to train longer in order to converge. Imagine, you want to solve a standard classification problem such as recognizing animals from images. Let's say you dataset comprises $10^9$ images and imagine this set of images $1cm$ high stack. The amount of discernibly different images is in the range of $10^{40}$ with the stack being $9.98 \times 10^{25} km$, which extends the diameter of the universe which is around $8.85 \times 10^{23} km$. (I saw this calculation at ICCV 2017; for details see slide no. 3 here ). Now, imagine you train on this vast dataset ($m:=10^{40}$). Even if it would be computationally feasible, at some point it would not further help you to classify your animal images correctly. That means, while training you would realize that after having seen only $n
