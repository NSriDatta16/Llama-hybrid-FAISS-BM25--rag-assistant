[site]: datascience
[post_id]: 103066
[parent_id]: 
[tags]: 
Understanding Sklearns learning_curve

I have been using sklearns learning_curve , and there are a few questions I have that are not answered by the documentation (see also here and here ), as well as questions that are raised by the function about sklearn more generally Here are some learning curves from my models of a data set And the code that produced them: train_sizes, train_scores, valid_scores =learning_curve(linear_regression_model,rescaled_X_train,Y_train) axes[0,0].plot(train_sizes,train_scores) axes[0,1].plot(train_sizes,valid_scores) train_sizes, train_scores, valid_scores =learning_curve(random_forest_model, rescaled_X_train,Y_train) axes[1,0].plot(train_sizes,train_scores) axes[1,1].plot(train_sizes,valid_scores) The documentation makes it seem like, the line learning_curve(linear_regression_model, rescaled_X_train, Y_train) fits the model rather than simply showing how the models fitting process previously behaved? a. If it is fitting the model again â€“ how do you pass hyperparameters (for example gamma for a SVM or maximum tree depth) and determine the cost function that is being used? b. If not, this seems very strange. I would have assumed that a linear regressor was by default just fit by least squares rather than something involving k-fold validation, as it appears to be if I am viewing the above graphs correctly. Is this how sklearn normally fits regressors? is the y- axis on these graphs accuracy score?
