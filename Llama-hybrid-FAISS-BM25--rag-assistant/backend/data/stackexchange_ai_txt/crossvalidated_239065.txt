[site]: crossvalidated
[post_id]: 239065
[parent_id]: 
[tags]: 
Beginner working with generative models

I'm getting a set of 20-dimensional feature vectors. I'm getting a 100 vectors per second. These feature vectors are occuring in a sequence, where one feature vector depends on the one that came before it. Feature vector: $(x_{1}, x_{2}, ... , x_{20})$, where $x_{i} \in \mathbb{R}$ For every such vector, I have a 2-dimensional "label" vector $(y_{1},y_{2})$, where $1 \leq y_{i} \leq 0$. For a concrete example, let's assume that we're working with self-driving cars and each dimension of the feature vector represents some attribute of the car's control. Feature 1 could be steering angle, feature 2 could be the car's speed, etc. while the "labels" could refer to the $(\Delta x, \Delta y)$ increment by which the car has traveled since the last vector was measured. I probably would have around 100,000 of these vectors. My goal was to design a neural network which, once trained on the data-set, would take in a 2-dimensional vector $(y_{1},y_{2})$ and generate an appropriate 20-dimensional feature vector of observations. Being a newbie, I had the vague idea of using a recurrent neural network to create a generative model from having learned the vectors--adapting the approach of Alex Graves' paper .The thought of using variational autoencoders or generative adversarial networks did occur to me, but I felt that it may be overkill at this point. However, is this the best approach to solve this problem? Could someone recommend a better approach, if it isn't?
