[site]: crossvalidated
[post_id]: 429263
[parent_id]: 429226
[tags]: 
Forward (or any other stepwise) selection is a bad method for selecting variables . Just don't. Using regularization (e.g. ridge regression - $\ell_2$ norm, lasso - $\ell_1$ norm, using dropout in neural networks) is a method that does variable selection for you and works. If you have just 120 samples, then neural network is a poor choice of algorithm . You need something simpler, and more robust, like linear regression. It is not strange at all that with such small sample your variable selection leaves you with a single variable, or just few of them. Your sample is small, so with more complicated model you would be easily overfitting. So this is a perfectly reasonable result. If, as I understand, you compared a number of models, and the one with single variable had relatively best performance, while the results are "awful", then maybe you don't have enough data to obtain better results? $R^2 = 1 - \tfrac{MSE}{\mathrm{Var}(Y)}$ , so unless you change something about the target variable (e.g. its distribution, by resampling the data) then they tell you the same thing, $R^2$ is just "normalized" MSE. Also be aware that $R^2$ is pretty misleading measure of error for non-linear models. The whole interpretation of "variance explained" does not apply in cases other then linear regression, and it can get values below zero, or above one, so it basically does not sum to $100\%$ , what makes the units less usable.
