[site]: crossvalidated
[post_id]: 583697
[parent_id]: 
[tags]: 
The effect of over-parameterization on local minima

While reading some papers about over-parameterization in deep learning models, I also read that "over-parametrization is a simple method to introduce additional dimensionality and help make the local minimal to be a saddle point so the optimizer would be less likely stuck at local minimal and can find the global minimal." My question is that how does over-parameterization and more dimensions help to make the local minima into a saddle point? I searched a lot but didn't find any proof or example that illustrates this. Thanks in advance.
