[site]: crossvalidated
[post_id]: 292142
[parent_id]: 291569
[tags]: 
Treating your model as a hierarchical panel data model is a good idea. On the other hand, taking the ratio of two time periods as your DV does not make a lot of sense to me, particularly with only 3 time periods. For my purposes, a much more sensible approach would be to take the natural log of county population as your DV with X as stated, including a 1 period lag of population. There is a lot of literature on panel data models recommending this approach as an appropriate, normalizing control for level differences in population by cross-section. I can think of many reasons motivating this statement, not least of which is that any resulting predictions will be expressed in a meaningful unit of analysis, something a ratio approach does not guarantee. The canonical, "go-to" reference for panel data models is Wooldridge's Econometric Analysis of Cross Section and Panel Data . His approach is almost entirely theoretical. This is a weakness for any applied researcher due to the many non-orthodox anomalies that can and will pop up in real (not theoretical), empirical information. Another excellent review of this methodology is Lee Cooper's Market Share Analysis (available for free download here ... http://www.anderson.ucla.edu/faculty/lee.cooper/MCI_Book/BOOKI2010.pdf ). Ignore the "market share" part, it's simply a very good and applied introduction to the topic that goes much deeper into practical considerations than Wooldridge. Yet another useful paper wrt hierarchical models incorporating fixed and random effects is Judith Singer's Using SAS PROC MIXED to Fit Multilevel Models, Hierarchical Models, and Individual Growth Models (available here ... https://www.ida.liu.se/~732G34/info/singer.pdf ). Again, ignore the SAS part as it, too, provides an excellent overview and discussion. Finally, for a more Bayesian treatment of this field, check out Gelman and Hill's book, Data Analysis Using Regression and Multilevel/Hierarchical Models . It, too, has many excellent insights. In chapter 13, Gelman discusses Bayesian approaches to estimating effects with n=1. These involve examining the posterior distributions produced by the model. To answer your questions, I'm not sure what a "sequential exogeneity violation" is but it sounds like it might be concerned with autocorrelation. If so, with only 3 periods, only two of which are used by the model, it's a moot point. Next, a dummy variable for year as well as a factor (a design matrix consisting of 0s and 1s) for county is a good idea -- these effects need to be included and controlled for. A similar factor for cities should be treated gingerly (if at all) as I can easily imagine issues concerned with overfitting and/or linear combinations that zero out effects between city and county popping up. With 5,000 municipalities and over 3,000 counties in the USA, you may face challenges with inverting the cross-products matrix if it becomes too large. I can think of two approaches to resolving this challenge. Note that both are approximating solutions: 1) A Bayesian approach would be to assume a prior distribution and simulate the data using some variant of hierarchical MCMC sampling, e.g., Gibbs, monte carlo, hamiltonian, etc. A method for dealing with categorical information containing huge numbers of levels was introduced in this paper by Ainslie, Massively Categorical Variables: Revealing the Information in Zip Codes (available here ... http://bear.warrington.ufl.edu/centers/MKS/abstracts/vol22/no1/aab6f24ee7_abstract.pdf ). As the authors of this paper demonstrate in the case of their model, modeling a single, massively categorical variable (zip code) was at least as informative, if not more informative, than using a suite of geo-demographic factors. 2) The lack of invertibility of the cross-products matrix using closed form, frequentist approaches to modeling appeared insoluble for years. Today however, with the advent of iterative, divide and conquer algorithms such as the bootstrap, jacknife, random forests and their many machine learning spin offs, frequentists can now leverage approximating solutions that don't require Bayesian assumptions. This paper by Wang, et al, A Survey of Statistical Methods and Computing for Big Data (available here ... http://arxiv.org/pdf/1502.07989.pdf ) goes into some depth about these "divide and conquer"-type algorithms.
