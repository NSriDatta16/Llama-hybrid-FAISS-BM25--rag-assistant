[site]: crossvalidated
[post_id]: 372998
[parent_id]: 372832
[tags]: 
If you wish to be efficient, you don't want to waste any calls to your URN (uniform random number generator). There's a standard way to do this, assuming you need the capability of generating long sequences of independent random numbers, rather than just a single random number. Rather than discussing this abstractly, let's consider a running example in which a uniform random number generator $U(2)$ with $n=2$ possible values $\{0,1\}$ is used to generate $m=3$ uniformly random numbers in $\{0,1,2\},$ a random variable I will call $U(3).$ Noticing that a sequence $u_0, u_1$ of two independent values from $U(2)$ has four possible equiprobable outcomes, we may apply rejection sampling . Specifically, consider this sequence $(u_0, u_1)$ as the binary string $u_1u_0,$ which can represent the natural numbers $0=00_2,$ $1=01_2,$ $2=10_2,$ and $3=11_2.$ Let's call this representation $X(\mathbf u) = X(u_0,u_1).$ Convert it to a ternary representation $v_1v_0$ where the $v_i$ are among the digits $\{0,1,2\}.$ The conversion goes like this: $$\eqalign{ (u_0,u_1)& =(0,0) \to 00_2 = 0 \to 00_3 \to (0,0)=(v_0,v_1)\\ (u_0,u_1)& =(1,0) \to 01_2 = 1 \to 01_3 \to (1,0)=(v_0,v_1)\\ (u_0,u_1)& =(0,1) \to 10_2 = 2 \to 02_3 \to (2,0)=(v_0,v_1)\\ (u_0,u_1)& =(1,1) \to 11_2 = 3 \to 10_3 \to (0,1)=(v_0,v_1) }$$ Because the four outcomes are equiprobable, $v_0$ has equal chances of being $0,1,$ or $2$ conditional on $X(u_0,u_1) \lt 3.$ In the fourth case (on the last line) we will reject the value and just try again, recursively, until finally $X(u_0,u_1)\lt 3.$ This is sure to happen eventually, because the chance that all results are rejected after $n$ trials is just $1/4^n,$ which rapidly approaches zero. Indeed, the expected number of trials is $1 + 1/4 + 1/4^2 + \cdots = 1/(1-1/4) = 4/3.$ We can do better by generating more than one random number at a time. For instance, consider drawing five independent values from $U(2).$ There are $2^5=32$ possibilities, all represented by five-digit binary strings. $3^3=27$ of the values of those strings are in the range (from $0$ through $26$ ) of the numbers that are represented by three-digit ternary strings. Thus, at an average cost of $32/27$ trials, we can generate $(u_0,u_1,u_2,u_3,u_4),$ convert the number $(u_4u_3u_2u_1u_0)_2$ to a ternary representation of at most four digits $(v_3v_2v_1v_0)_3,$ and return a sequence $(v_0,v_1,v_2)$ of three independent realizations of $U(3).$ Let's compare the two approaches. In the first algorithm, each trial requires two realizations of a $U(2)$ and the expected number of trials to obtain a single realization of a $U(3)$ is $4/3.$ Thus, the expected number of realizations of a $U(2)$ in order to generate a single $U(3)$ is $2\times 4/3 = 8/3 \approx 2.67.$ In the second algorithm, each trial requires five realizations of a $U(2)$ and the expected number of trials to obtain three realizations of a $U(3)$ is $2^5/3^3=32/27.$ Thus, the expected number of realizations of a $U(2)$ for each realization of a $U(3)$ that will be output is $32/27 \times 5 / 3= 160/81 \approx 1.98.$ The second therefore is $(2.67 - 1.98)/1.98 = 35\%$ more efficient than the first in terms of calls to a random number generator. (In terms of arithmetic it is less efficient due to the work required to convert from binary to ternary.) How low can we go? That is, can we achieve near-infinite efficiency? If you don't care about the computational work, consider what happens as you continue to increase the number of realizations of a $U(2)$ from $2$ to $5$ to some large number $k.$ Evidently the number of ternary digits you can generate in any iteration of the rejection sampling procedure is the largest integer $d$ for which $3^d \le 2^k.$ (Equality cannot hold, because $3$ and $2$ have different prime factors.) A formula for $d$ uses the floor function $\lfloor \cdot \rfloor:$ $$d = \lfloor \frac{\log 2^k}{\log 3}\rfloor = \lfloor k \log_3 2 \rfloor.$$ As before, each trial requires $k$ realizations of a $U(2)$ and the expected number of trials to obtain three realizations of a $U(3)$ is $2^k/3^d.$ Thus, the expected number of realizations of a $U(2)$ for each realization of a $U(3)$ that will be output is $2^k/3^d \times k / d.$ We can bound this expectation from the fact that $d \le k \log_3 2.$ Therefore $$\frac{k 2^k}{d 3^d} \ge \frac{k 2^k}{(k \log_3 2) 3^{k \log_3 2}} = \log_2 3.$$ This shows we can never do better (on average) than using $\log_2 3 \approx 1.585$ calls of a $U(2)$ to generate a $U(3);$ and our ability to reach this lower bound depends on finding powers $k$ and $d$ for which $3^d$ is less than $2^k$ but very close to it. The theory of continued fractions shows that such pairs exist and one can always find a pair to achieve an expectation as close as desired to $\log_n m$ . The only circumstance in which this expectation can actually equal the lower bound is when $n$ and $m$ are powers of a common base. It should be evident this entire analysis goes through with any pair of natural numbers $n\gt 1$ and $m\gt 1$ instead of $2$ and $3.$ In the example of the question, $n=5$ and $m=7$ . One efficient choice is to generate $k=133$ values from a $U(5)$ and combine them into $d=110$ values. This is only $0.516\%$ worse than the theoretical lower bound. Finally, we never assumed $m \gt n.$ For instance, $k=2$ draws from a $U(3)$ will generate $d=3$ realizations of a $U(2)$ at an average cost of $9/8$ trials. That is, each draw from a $U(2)$ requires $3/4$ of a $U(3)$ (on average) in this case. The lower bound is $\log_3 2\approx 0.631.$ To demonstrate, here is a Mathematica implementation in the form of a generate function. Its arguments are the number of values to return, count , and the values of $n$ , $m,$ and $k.$ It computes $d$ and repeatedly generates sequences of $d$ uniform integers from $0$ to $m-1$ until at least count have been created, then returns the first count of them. generate[count_, n_, m_, k_] := Module[{x, d = Floor[k Log[m, n]]}, Flatten[ Table[IntegerDigits[x = m^d; While[x >= m^d, x = FromDigits[RandomInteger[n - 1, k], n]]; x, m, d], {i, 1, Ceiling[count/d]}]][[1 ;; count]]] It can generate millions of such variates per second, as in x = generate[7*10^5, 5, 7, 133]; This example is expected to produce $10^5$ values of each integer $0,1,\ldots, 6,$ up to sampling variation. Here are the counts: Tally[x] {{3, 100237}, {5, 100079}, {1, 99949}, {2, 100259}, {4, 100187}, {0, 99839}, {6, 99450}} They don't differ significantly from what one might expect $(\chi^2(6)=4.955,\, p=0.55)$ .
