[site]: crossvalidated
[post_id]: 621362
[parent_id]: 621355
[tags]: 
I suspect class imbalance can be an issue for multi-class problems. Say you have a three class problem, with two common classes and one rare class. The cost function may be minimised more by using hidden units to model features of the distribution of the common classes that doesn't actually affect the decision boundary than it is by modelling the rare class in a way that does affect the decision boundary. This is simply because the cost function is usually the sum of terms for each training pattern, and a lot of them will belong to the two common classes. Basically class imbalance may result in resource allocation failing to assign sufficient neurons to the minority class. This is sort of the problem with proper scoring rules. If your application requires a decision, a proper scoring rule may reward a model that performs slightly better in an area of high data density away from the decision boundary, rather than a model that performs better at the decision boundary. This isn't to say that proper scoring rules are a bad idea, just that they are not a panacea, even if they are best practice for the majority of applications. I'm not sure the batch issue is a substantial problem as different batches are used in each epoch, and some of those by random sampling will have a slightly more than average number of minority patterns.
