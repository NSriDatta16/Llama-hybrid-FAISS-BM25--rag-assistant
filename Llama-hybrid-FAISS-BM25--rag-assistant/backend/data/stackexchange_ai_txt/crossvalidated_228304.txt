[site]: crossvalidated
[post_id]: 228304
[parent_id]: 
[tags]: 
What is meant by "joint modeling" in the case of multi-lingual word embeddings?

I've encountered this term in a few short papers on bilingual or multilingual embeddings, but I still don't understand what this means in a concrete sense (e.g. how is a word embedding model trained, what data is fed to it?) Here is an example of the types of papers I've seen this terminology in: http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf . It's not clear to me whether the model is simply trained on all text (in both languages) at once, and this is what "joint modeling" means, or if something fancier is happening. I understand the use of "joint embedding" has also been applied to multi-modal learning (e.g. a model that - typically neural net - that represents both text and image data). If there is overlap about what "joint modeling" means in the case of word embeddings and multi-modal NN models, it would be helpful to understand the parallels there as well.
