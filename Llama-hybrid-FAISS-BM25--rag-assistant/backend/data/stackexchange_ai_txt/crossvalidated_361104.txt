[site]: crossvalidated
[post_id]: 361104
[parent_id]: 361066
[tags]: 
Another purpose of using linear layers is to reduce dimensionality (and the number of parameters). For example the Skip-gram and CBOW model for word embeddings . The training task is to predict context words of a given word $p(w_o|w_i)$. A naive way is to count the occurrences of context words for each word and put them in a matrix $M$, then the probability is just $p(w_o|w_i) = f(M_{w_i})_{w_o}$, where $f$ is a normalization function. The problems is often the number of words $n$ is huge and we can't afford an $n$ by $n$ matrix. So we can first use an $n$ by $d$ matrix $A$ to reduce the dimension (to say 128) and use another $d$ by $n$ matrix $B$ to turn it back, then number of parameters can be reduced to $2*d*n$. It's kind of like matrix decomposition in the sense that we use $BA$ to approximate $M$. The model can be implemented as two linear layers followed by a normalization function and can be trained using the cross-entropy loss $E[y_n\log f(BAw_i)]$, where for the Skip-gram model, $w_i$ is a one-hot vector, for the CBOW model, $w_i$ is a BOW vector.
