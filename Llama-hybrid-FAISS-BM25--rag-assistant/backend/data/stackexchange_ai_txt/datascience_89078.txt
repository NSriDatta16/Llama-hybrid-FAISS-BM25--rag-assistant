[site]: datascience
[post_id]: 89078
[parent_id]: 89076
[tags]: 
Sure, if you have a large and good quality in-domain dataset, the results may certainly be better than with the generic pretrained BERT. This has already been done before: BioBERT is a BERT model pretrained on biomedical texts: [...] a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Of course, other factors may be taken into account in the decision to pretrain such a model, e.g. computational budget.
