[site]: datascience
[post_id]: 28883
[parent_id]: 
[tags]: 
Why adding combinations of features would increase performance of linear SVM?

I have a dataset of ~5000 elements represented by vectors composed by ~30 binary values (0 or 1) on which I am performing binary classification with SVM with linear kernel (I use the Scikit learn lib). For curiosity, I tried to add an extra feature that consists in a AND between two others (remember that all my features are boolean). The result was that the performance of the SVM improved. I was surprised by this improvement because the AND operation is equivalent to a multiplication, therefore I would expect that my SVM, as every linear classifier, was somehow naturally already taking into account mutiplications between features. What is wrong with my theoretic understanding of SVM ?
