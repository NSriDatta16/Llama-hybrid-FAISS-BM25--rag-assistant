[site]: datascience
[post_id]: 108286
[parent_id]: 108190
[tags]: 
No, this trick is only relevant to the vanilla sequence-to-sequence models. It is exactly as you say. The decoder gets the information from the encoder via the attention mechanism, so there is no way the long distance between source and target words could block the capacity of the RNN state. Also, the encoder is typically bi-directional, which means there are two RNNs each of them processing the source sentence in a different direction. As a result, each encoder state contains information about the complete source sentence, one half in the left-side context, the other in the right-side context. With today's Transformer models, it does not make any sense because Transformers treat the input as an unordered set and the only way it knows about the token order is the position embeddings (if the position embeddings are correct, you can permute the input tokens randomly).
