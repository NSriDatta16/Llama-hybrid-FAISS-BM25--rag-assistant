[site]: crossvalidated
[post_id]: 345370
[parent_id]: 
[tags]: 
What is the target value for 2nd to last layer in neural network

In a neural network implementing gradient descent, the following happens to the last layer and the second to the last layer. The cost function (average over all training examples of the sum of the squares of the differences between the predicted value and the target value) is calculated. Then the delta of the cost function is divided by the delta of any particular weight and this gives us the gradient. This is where I'm stuck. I have the following two questions: Minimizing error and traversing down the curve in gradient descent is what we're after and this is obvious once we have this gradient vector. But what exactly do we do with the vector? For instance, if a particular weight has a value 0.5 and the gradient vector has a value of -0.1 for that weight, what do we do to that weight, just make it -0.4? When computing the gradient to begin with, it seems intuitive for the last layer and the second to layer because the target is known. But what exactly is the "target" for the 2nd to last layer?
