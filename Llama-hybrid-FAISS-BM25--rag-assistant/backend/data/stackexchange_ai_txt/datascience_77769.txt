[site]: datascience
[post_id]: 77769
[parent_id]: 
[tags]: 
How to assess nested cross validation results in comparison to non-nested results?

I have a nonlinear regression model scoring genes from scores between 0 to 1 as to whether they are likely to cause disease. Training data is ~700 gene samples by 53 features. Currently I get results with xgboost which look like: r2 Nested CV Average: 0.807 MSE Nested CV Average: -0.016 Non-nested Results: XGBR Train r2: 0.949 Test r2: 0.805 XGBR Train MSE: 0.002 Test MSE: 0.018 r2: 0.895 Predicted r2: 0.871 Should I be concerned that the non-nested training result is overfitting in comparison to the test result, or should I only rely on my 5-fold nested cross-validation to determine overfitting is minimised? For reference, the XGBoost model I use is tuned like this: xgbr = xgboost.XGBRegressor(random_state=seed, objective='reg:squarederror') xgbr_params = { 'max_depth': (1, 10), 'learning_rate': (0.01, 0.5), 'n_estimators': (20, 50), 'reg_alpha': (1, 10), 'reg_lambda': (1, 10), 'gamma': (0, 0.5), 'min_child_weight': (1, 5), 'subsample': (0.1, 1), 'colsample_bytree': (0.1, 1)} #Best parameter output: xgbr = xgboost.XGBRegressor(random_state=seed, subsample=0.8258568992489053, min_child_weight=1, n_estimators=50, gamma=0.0, objective='reg:squarederror', colsample_bytree=1.0, learning_rate= 0.3987519903467713, max_depth=4, reg_alpha=1, reg_lambda=10)
