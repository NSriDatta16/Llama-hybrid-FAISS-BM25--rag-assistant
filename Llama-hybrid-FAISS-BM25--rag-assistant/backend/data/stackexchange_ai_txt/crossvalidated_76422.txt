[site]: crossvalidated
[post_id]: 76422
[parent_id]: 76420
[tags]: 
This is possible using some approaches and is certainly a valid approach. I am not sure if random forests can do this, though. Generating artificial data means making extra assumptions, don't do that if you don't have to. One technique you may want to look into is so-called one-class SVM. It does exactly what you are looking for: it tries to build a model which accepts the training points and would reject points from other distributions. Some references regarding one-class SVM: Sch√∂lkopf, Bernhard, et al. "Estimating the support of a high-dimensional distribution." Neural computation 13.7 (2001): 1443-1471. This paper introduced the approach. Tax, David MJ, and Robert PW Duin. "Support vector data description." Machine learning 54.1 (2004): 45-66. A different way to do the same thing, probably more intuitive. Both of these approaches have been shown to be equivalent. The first estimates a hyperplane which separates all the training data from the origin in feature space with maximal distance. The second estimates a hypersphere with minimal radius in feature space containing the training instances. One-class SVM is available in many SVM packages, including libsvm , scikit-learn (Python) and kernlab (R).
