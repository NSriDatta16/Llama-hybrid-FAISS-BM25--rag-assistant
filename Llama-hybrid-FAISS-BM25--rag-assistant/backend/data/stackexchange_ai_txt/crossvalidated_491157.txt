[site]: crossvalidated
[post_id]: 491157
[parent_id]: 174553
[tags]: 
I would argue that this is an ill posed problem. Same as for many other machine learning algorithms , in neural networks it is hard to say what exactly would we count as a "parameter" when penalizing AIC. The point of AIC is to penalize the log-likelihood by the complexity of the model. In case of simple models, like linear, or logistic regression this is simple, as the number of regression parameters determines the complexity of the model. For simple feed-forward neural network this would also be the case, but consider that you can increase complexity of a neural network without increasing the number of parameters: you can use skip-connections, max-pooling, masking, weight normalization, etc., they all have no parameters. Moreover, what would you say about dropout, it "turns-off" parameters that are available for the network, so maybe somehow we should discount the number of parameters when using it? In case of complicated machine learning algorithms, the number of parameters is much less useful as a measure of model complexity. To complicate it even more, in neural networks it was observed that bias-variance trade-off seems not to apply . The rationale behind using AIC is that "simpler" model is better, because it is more explainable and less prone to overfit. If, as it appears, neural networks do not have to be more prone to overfitting with increasing number of parameters, than it is disputable if penalizing for it makes sense.
