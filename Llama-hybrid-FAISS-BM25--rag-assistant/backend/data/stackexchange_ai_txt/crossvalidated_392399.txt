[site]: crossvalidated
[post_id]: 392399
[parent_id]: 391248
[tags]: 
We essentially examine the conditions under which the Law of Large Numbers holds for the sum $$\frac 1n \sum_{i=1}^nx_{ki}u_i,\;\;\; E(u_i)=0$$ for every $k=1,...K$ regressor, and we assume also a finite variance for $u_i$ . Now, when the $x_{ki}$ 's are non-stochastic then they are just a sequence of real numbers, and we might as well re-write the sum as $$\frac 1n \sum_{i=1}^na_{ki}u_i,\;\;\; E(u_i)=0$$ I write it like this to stress that the only source of randomness here is the $u_i$ 's, and so that, what we are looking at is the average of independent but non-identically distributed random variables $z_{ki}=a_{ki}u_i$ . This is the "Chebyshev's" Law of Large Numbers and requires that the variance of each random variable is finite. This means that we need $$\text{Var}(z_{ki}) = a_{ki}^2 \text{Var}(u_i) Markov generalized this LLN to possibly non-independent random variables, where we require that $$\text{Var}\left(\frac 1n \sum_{i=1}^na_{ki}u_i\right) \to 0$$ Under independence of $u_i$ 's, covariances are zero and we have $$\text{Var}\left(\frac 1n \sum_{i=1}^na_{ki}u_i\right) = \text{Var}(u_i)\cdot \frac 1{n^2}\sum_{i=1}^na_{ki}^2$$ and so the sufficient condition we need is $$\frac 1{n^2}\sum_{i=1}^na_{ki}^2 \to 0$$ or to revert back to original notation $$\frac 1{n^2}\sum_{i=1}^nx_{ki}^2 \to 0$$
