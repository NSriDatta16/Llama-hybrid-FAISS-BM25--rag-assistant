[site]: crossvalidated
[post_id]: 255888
[parent_id]: 254892
[tags]: 
To check model adequacy, hierarchical Bayesian models are usually evaluated exactly like in DHARMa - you simulated from the fitted model and calculate the quantile residuals. The only difference to DHARMa is that you vary parameters as well while doing the simulations. The approach is explained in many textbooks. Keywords are "posterior predictive simulations", "posterior model checks", or "Bayesian p-values". A few notes As for mixed models, the question arises how many levels of the hierarchy your re-simulate. Especially state-space population models look great when re-simulating only the final observation model, but if you re-simulate the process model as well, it often looks very very different -> good reason to be skeptical when people only show the final fit of the SSMs, this can look great even if the model is horribly wrong. The distributional theory for these residuals is not quite clear for complex models - for GLMMs, all simulations I did so far showed that simulated quantile residuals are with good approximation flat, but this may not be so for any complex model structure. In doubt, run a test on a properly specified model. DHARMa has a function to input posterior predictive simulations (createDHARMa), so that you can use all tools of DHARMa together with hierarchical Bayesian models as well However, all this is more a substitute for standard residual diagnostics, not R2, which is a measure of how well you explain the target variable. For R2, you can simply calculated R2 or RMSE over the posterior predictive distributions, whatever you like best, however, with all the caveats that arise for mixed models, i.e. you have to consider what you mean by "explain", does a random effect "explain" the data or not - this is again particularly apparent in state-space models, where you can get your fitted states very close to the data, even if the model has basically no clue why the data moves this way.
