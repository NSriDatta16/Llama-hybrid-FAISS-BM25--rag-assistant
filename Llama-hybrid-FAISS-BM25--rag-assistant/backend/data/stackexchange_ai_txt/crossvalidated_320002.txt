[site]: crossvalidated
[post_id]: 320002
[parent_id]: 320000
[tags]: 
Consider the diagram of a simple feed-forward neural network on page 170: Section 6.2.2.2 on page 178 is describing using this network with a sigmoid activation to predict the parameter of a Bernoulli distribution given some samples $x_0, x_1, ...$. Forward propagation (prediction) in this network will look like: $h = W^Tx + b_0$ $z = w^Th + b_1$ $\hat{y} = sigmoid(z)$ If the model prediction $\hat{y}$ is close to $1$ then $z$, the input to $sigmoid$ was very large, and we are in the nearly-flat region of the sigmoid where the gradient is small therefore where it is hard to learn via gradient-descent i.e. "saturation". But if we ever get to this point then we must already have the right answer, so it is not a problem. This is because we will never start with an initial guess of $\hat{y}$ near $0$, because our data will be standardized and our random initial weights will be small, so z could never blow up to the saturation region of an incorrect prediction on the first training iteration. Only as we perform gradient updates via backpropagation $z$ will head in the right direction towards $\infty$ until $sigmoid(z)$ is close to the correct prediction and the output error is sufficiently small. So at any time we will either be outside of the saturation region or we will have gotten there by converging to the correct prediction. A symmetric argument holds for $\hat{y}=0$, where $z$ must be very negative.
