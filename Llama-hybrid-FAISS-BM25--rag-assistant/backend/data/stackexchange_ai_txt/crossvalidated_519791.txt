[site]: crossvalidated
[post_id]: 519791
[parent_id]: 
[tags]: 
For BERT-based models, do we absolutely have to include the [CLS] and [SEP] special tokens in the input data?

The thought just occurred to me while I was processing data. If we're using the [CLS] token for classification, then it would obviously make sense to include it, but if we're not using that token do we have to include it? For example, if I'm performing a task where I just need the positions of specific tokens excluding the [CLS] and [SEP] tokens, then it seems to me that including the two tokens would be unnecessary. I've heard the opinion that "it'd be convenient to include them" since it would ensure that the pretraining and fine-tuning settings would be the same, but I'm not exactly convinced by that argument and was wondering if anyone more knowledgeable may have something to comment. Thanks.
