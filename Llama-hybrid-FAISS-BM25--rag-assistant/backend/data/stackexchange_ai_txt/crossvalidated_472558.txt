[site]: crossvalidated
[post_id]: 472558
[parent_id]: 472416
[tags]: 
I'm assuming that you have in mind that the number of "knots" (pieces of the piecewise linear function) are known, but their locations are not. Here are two ideas. Decision trees Vanilla decision trees (trivially) form piecewise (axis-aligned) decision boundaries, but I don't think that's what you had in mind. "Multivariate Decision Trees" form piecewise linear decision boundaries, which I'd guess is more what you're looking for. (Figure 1 from that paper below) Solid line - decision boundary of vanilla decision tree dashed line - decision boundary of multivariate tree Neural nets I know, I know, deep neural nets these days aren't usually interpretable, but very small, shallow architectures can be interpretable. If you have in mind that the knot locations are learnable, then I think it's a nice framework to work in. Your example can be solved with the composition of two (sets) of logistic regressions ( an ANN, with one hidden layer having two neurons ) These two hidden layers implement these two decision boundaries. These have the effect of mapping your red points to the origin, and blue points to one of $(0,1), (1,0),(1,1)$ . The last "layer" just has to separate the origin from everything else and would not even need to be learned. Edit: of course just because a network can learn this, doesn't mean it will.
