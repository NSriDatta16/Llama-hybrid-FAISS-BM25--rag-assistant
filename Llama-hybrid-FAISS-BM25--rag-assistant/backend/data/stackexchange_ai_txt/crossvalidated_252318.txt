[site]: crossvalidated
[post_id]: 252318
[parent_id]: 
[tags]: 
Using different classifiers for feature selection and prediction

It is a common way to use filter methods for feature selection, and then use other classifiers for prediction with reduce features. But is it also possible to use embedded or wrapper methods for feature selection, and using other classifiers for prediction? For example, feature importance ranking can be acquired by using random forest(embedded) or recursive feature elimination(wrapper). After choosing some high-rank features from them, I want to use other classifiers such as AdaBoost or SVM(using kernel) for the prediction. Although this approach does not seem to be intuitive, I think it works similar to filter methods. Is it appropriate approach? And if any, I would like to know pros and cons of this approach.
