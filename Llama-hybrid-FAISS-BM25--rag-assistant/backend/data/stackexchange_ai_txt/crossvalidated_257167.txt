[site]: crossvalidated
[post_id]: 257167
[parent_id]: 
[tags]: 
Bayesian approach systematically overestimates sigma (SD)

I am quite stunned: I did some test on generated data and found out that model estimates from JAGS of the standard deviation of input data are systematically overestimated by certain and fixed constant 1.20247, for N = 9! This is the result of more than 10,000 models (100,000 iterations each): Here is the model: model { intcept ~ dnorm(0, 0.01) sigma ~ dunif(0, 10) for (i in 1:N) { Y_exp[i] The above plot plots the sigma parameter against the unbiased sample standard deviation ( sd(Y) on the generated data in R, N-1 in denominator). The regression slope is 1.20247, which means that the sigma is overestimated by this constant! Not only in this simple model, but also the residual sigma in linear regression is overestimated by approx the same constant (I tested this in a different test) . And since sd(Y) is the best unbiased predictor of the distribution SD, the bayesian approach seriously overestimates the parameter! I tried to change the prior for sigma (or variance) to: sigma ~ dunif(0, 5) - no change half cauchy prior ( from Gelman, pg 529 ) - no change dgamma prior - the slope is smaller, but it is still a difference! It is possible that the slope will change with N. I only tested with N = 9. I haven't tested WinBUGS/OpenBUGS or other MCMC tools, I guess the result would be the same, since this is probably some statistical artifact (??) Why is this happening? The full code to reproduce this can be downloaded here . MAIN.R is the main code to run.
