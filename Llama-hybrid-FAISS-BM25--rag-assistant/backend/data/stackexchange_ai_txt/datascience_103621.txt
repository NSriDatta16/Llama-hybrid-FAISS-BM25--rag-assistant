[site]: datascience
[post_id]: 103621
[parent_id]: 103579
[tags]: 
tldr: in LDA the "generation part" is used to describe the design of the model, not for practical applications. LDA is a Bayesian generative model. The distinction between generative and discriminative models is addressed for example in this question or if you want more detail in this one . As the name suggests, generative models can generate data. But in the case of LDA and other topic modelling models, it's never actually used for the task of generation because it would produce meaningless texts. In fact the generative aspect is more about the design of the model: the idea is to represent a model as a statistical generation process, i.e. by drawing some values out of a distribution represented by the parameters of the model. This is why these models are always described through a "generative story", which represents the design of the model. Note that this design involves a lot of assumptions and simplifications, this is why the practical ability to generate is not really useful in LDA. In practice LDA is almost exclusively used to obtain a probabilistic clustering of a set of documents based on topics, with different topics represented as different distributions over the vocabulary. This is done by estimating the parameters of the model from the corpus, and this process is usually done with Gibbs sampling. Note that Gibbs sampling is a general estimation method, it's not specific to LDA. In case this helps, I often explain the differences between the different tasks in computer science terms: For the generation task: the input is a model parameters, the output is a set of documents (potentially infinite). For the estimation task: the input is a (finite) set of documents, the output is the model parameters. For the "application" task: the input is a model parameters and some input document $d$ the output is the posterior probabilities $p(t|d)$ for every topic $t$ .
