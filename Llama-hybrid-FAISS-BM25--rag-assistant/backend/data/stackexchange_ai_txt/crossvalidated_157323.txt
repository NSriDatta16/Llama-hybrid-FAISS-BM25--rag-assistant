[site]: crossvalidated
[post_id]: 157323
[parent_id]: 157309
[tags]: 
It all depends on the situation. What is the number of features, and how big is your dataset? Do the labels influence each other? You could calculate the cross correlation matrix between the labels to find out the latter. You could use regularized linear regression. It has the nice property that if you train it for one set of y's, you can immediately use the result to do the same for another set of y's, so training your classifier for multiple labels becomes very cheap. If you have a small number of dimensions and a large number of training objects, you might want to use a kernelised classifier (in this case you might be able to find a decision boundary that is more descriptive than just a linear decision boundary). In this case you can use ridge / kernel regression (= kernelised regularized linear regression). The performance of regularized linear regression (or ridge regression) is comparable to that of SVM's. SVM's are considered state of the art classifiers. If you don't have to worry about training times, I would just train multiple SVM's, one SVM for each label - it will only take longer. Depending on the number of features compared with the number of dimensions, you could look at multiple kernels. Finally, the labels might influence each other as well, so the methods above might be suboptimal, because they do not take this into account. To improve on this naive baseline, check this wikipedia page: https://en.wikipedia.org/wiki/Multi-label_classification
