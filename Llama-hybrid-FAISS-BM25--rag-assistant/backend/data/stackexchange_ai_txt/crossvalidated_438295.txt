[site]: crossvalidated
[post_id]: 438295
[parent_id]: 
[tags]: 
Identifiability of a probability given a set of conditional independence statements and distributions

I am seeking help for finding papers demonstrating the identifiability of a probability given a set of conditional independence statements and a set of probability distributions. More specifically, I am interested in a setting where independence statements can be represented as a directed acyclic graph (DAG also called Bayesian network). Consider a DAG, $W\to X\to Y\to Z$ . This DAG encodes CI statements, e.g., $Y \perp W \mid X$ . Given the following distributions $P(W,Y)$ , $P(X,Z)$ , and $P(X,Y)$ , can one infer $P(Z|W)$ ? The answer is no. The fact that $P(Z|W)$ is not determined by the three distributions can be demonstrated by creating two different (positive) joint distributions $P_1(W,X,Y,Z)$ and $P_2(W,X,Y,Z)$ such that they agree on $P(W,Y)$ , $P(X,Z)$ , and $P(X,Y)$ but disagree on $P(Z|W)$ , that is, $P_1(z|w) \neq P_2(z|w)$ for some $z$ and $w$ . I can create such $P_1$ and $P_2$ for this specific example. Now consider that we have a very complicated DAG with 10, 100, or 1000 variables. How can we (algorithmically) figure out whether the probability of interest is identifiable given an arbitrary collection of joint and conditional distributions? If not, how can one prove the non-identifiability? Are there any papers presenting a constructive method for demonstrating non-identifiability? What branches of statistics study this type of problem?
