[site]: crossvalidated
[post_id]: 260590
[parent_id]: 
[tags]: 
Is there prior work on "swiveling" convolution tensors I could study?

I am new to the field of neural networks. I started reading up on the field after AlphaGo's victories over Lee Sedol 9p in March 2016 inspired me to take another crack at programming a Go AI. I have read through http://neuralnetworksanddeeplearning.com/ and some other sources, so I get the basic concepts while not being familiar with the all the field's jargon or literature. This is making it hard to research what are probably basic questions since I do not know how to "put" them for a search engine to help me. In https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf , Silver et al wrote that they handled assymetries by rotating/reflecting (I call it "swiveling" for lack of a better word) the board into its eight positions and averaging the DCNN's results on all its variants. This seems strange to me, since I expect that the features learned by assymetrical convolution tensors would still be stuck on the board's orientation. If we have two assymetrical features A and B that can activate a third assymetrical feature C if they are next to each other, but the orientation of these features on the board is such that A is found in one orientation and B is found in another, then the DCNN may not discover C even if a human might. Does it not make more sense to borrow the idea of max pooling by swiveling the convolution tensors - not the input - into eight orientations, then returning the max result of those eight? It seems that all that would be needed to support backpropagation is saving which orientation was used in addition to the activation and output values so that one can associate the errors to the appropriate weights. I doubt that I am the first person to think of this, so I expect that there is prior work on the subject. What are "swiveling" convolution tensors called in the literature? What findings are there on its performance (accuracy and speed)? Is there a reason that such an idea was not used in AlphaGo? Also, as a bonus question, is there a free, open-source API I could use to try to program these myself without having to teach myself GPU programming?
