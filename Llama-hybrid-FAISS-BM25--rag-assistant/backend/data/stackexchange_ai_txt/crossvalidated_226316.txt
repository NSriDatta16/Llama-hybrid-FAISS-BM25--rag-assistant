[site]: crossvalidated
[post_id]: 226316
[parent_id]: 
[tags]: 
Difference in the error covariance matrix for correlated and autocorrelated errors

There is a big difference between the covariance matrix of correlated errors vs that of autocorrelated errors, but I have not found any formal explanation or notation. I would expect the covariance matrix describing variance between errors of different regressed variables is a $n$ x $n$ matrix with $n$ being the number of variables in the model. I often see this matrix written as $\Sigma$, with $\Sigma_{ij}$ = cov $(\epsilon_i , \epsilon_j)$. In the case of autocorrelated erros from for example time series data, the covariance matrix should describe variance of error terms in different time points, but refering to the same variable. This would be a $n$ x $n$ matrix with $n$ time points. I would say I often see this matrix notated as $\Omega$ , with $\Omega_{ij}^k$ = cov $(\epsilon^k(t_i) , \epsilon^k(t_j) )$. Describing autocorrelation of regression errors of variable $k$. This would mean, for multiple regression models with $L$ variables, we would have one $\Sigma$ and $L$ x $\Omega$ covariance matrices. Is this correct? Or am I confusing different terms for different situations? And most importantly: For applying generalized least squares estimation with (auto)-correlated errors. Which error covariance matrix is the one being estimated?
