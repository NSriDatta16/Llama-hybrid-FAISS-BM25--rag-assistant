[site]: crossvalidated
[post_id]: 285287
[parent_id]: 285283
[tags]: 
Starting from $x_t-\hat x_t$ is not how we build the statistical theory around this model. (This is also characteristic to a much broader class of statistical models than just the AR(1).) The AR(1) model for a time series process $x_t$ specifies that $$ x_t=\varphi x_{t-1} + \varepsilon_t $$ with $\varepsilon_t \sim i.i.d.(0,\sigma^2)$ and $\varepsilon_t$ being uncorrelated with $x_{t-1}$. Note that the distribution of $x_t$ is not specified in this definition. Given the definition, we can derive properties of $x_t$ as a process; properties of estimators of $\varphi$ and $\sigma^2$; and properties of some test statistics based on the latter estimators, e.g. a test that $H_0\colon\ \varphi=0$. To derive the maximum likelihood estimator, we need to know or assume the distribution of $\varepsilon_t$. To derive the conditional least squares estimator we do not need a distributional assumption on $\varepsilon_t$. Whatever the estimator (maximum likelihood, conditional least squares, ...), we need to know or assume the distribution of $\varepsilon_t$ to derive finite-sample properties of the estimator of $\varphi$ and test statistics based on the estimators. In any of these cases, the distribution of $x_t$ is irrelevant for the statistical theory around the model. In the derivations of the maximum likelihood estimator or the test statistics, the distribution of $x_t$ either plays no role or if it does, it cancels out in the final result. So it is simply irrelevant. P.S. Note that there is a difference between the error (or shock, or innovation) $\varepsilon_t$ and the model residual $\hat\varepsilon_t:=x_t-\hat x_t$. We make assumptions on the former, not the latter.
