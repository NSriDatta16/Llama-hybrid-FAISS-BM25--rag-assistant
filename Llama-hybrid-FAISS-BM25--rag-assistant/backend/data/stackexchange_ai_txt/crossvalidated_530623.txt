[site]: crossvalidated
[post_id]: 530623
[parent_id]: 530611
[tags]: 
One good idea could be to look up recent work for Phase 3 registration trials (which get a lot of scrutiny by regulators such as the US FDA and get planned with a lot of attention to detail by the pharmaceutical companies running the studies) that used the particular endpoint (or very similar ones). A very simple model could be a linear regression model for the numeric post-treatment score with treatment as a factor in the model and the numeric baseline score as a covariate. The biggest difficulties with this could be: Some assumptions for linear models might be so badly violated that it matters (many small deviations don't really matter). E.g. the regression residuals might be very non-normal or the errors might be larger in the middle of the Likert scale than at the ends etc. Ideally, one knows from previous studies whether a linear regression approach is okay (it's always a bit problematic to test assumptions in the study one is analyzing) and it might even be that people have done simulation studies with realistic settings for the exact outcome scale you are interested in. Even if the model fits quite well, what do you do about those that do not have a score at the end. This is a huge topic in itself and almost independent of the choice of model (see e.g. this guidance ). Options include that we assume people are similar to other people with similar covariate values (including that the treatment continued for them even though we did not get to see the final outcome - or perhaps we want to know what would have hypothetically happened if they had finished their treatment) and we could then e.g. do multiple imputation (i.e. you impute, say, 100 datasets and then analyze each, then you aggregate the results - this is nicely handled by standard software). If it's that those that stop treatment would then be like similar people in the control group, then approaches like "jump-to-reference" could be an option. An alternative is to model the actual ordinal outcome. Frank Harrell has recently talked a lot about this (e.g. here ) and has cited some papers on this (such as this one and there's also this ). The main advantage is that it attempts to model the actual data generating process and allows adjustment for the baseline score. You can have a look around the data methods forum that Frank Harrell champions and the discussions tagged as "ordinal" on there. That might be a really good place to post a question on how to implement such an ordinal analysis in case you do not find all your questions answered, yet. Point (2) regarding the simple linear model of course still does not go away and needs to be dealt with. Further alternatives that are less popular include Rank-based methods, but in their basic version they do not make it easy to adjust for the pre-test score. Forming the difference of pre- vs. post is also not that satisfactory, because a change of +2 on a 10 points scale is possible from a baseline of 5, but not from a baseline of 9 (and even when it's possible may mean very different things), an ordinal model can reflect that a lot better. Simple dichotomization (aka "responder analysis") of the outcome (e.g. >5 vs.
