[site]: crossvalidated
[post_id]: 315841
[parent_id]: 
[tags]: 
Why maximizing the lower bound of variational evidence maximizes the probability of observing data

In vaiational bayesian inference we attempt to find a proxy function to best estimate the intractable posterior $P(z|X)$. We define best as the probability distribution that minimizes the KL divergence. The final equation reached is: $$log(P(X)) = KL(q_{\lambda}(z|X) || p(z|X)) + ELBO(\lambda)$$ where $log(P(X))$ is the log probability of observing the data(evidence) $ELBO$ is the Evidence lower bound $\lambda$ is the parameter over which the entire family of $q$ is taken. Why does maximizing $ ELBO(\lambda)$ maximizes the $log(P(X))$ as is claimed by prof. Ali Ghosdi here at 32:50 It would occur to me that maximizing $ ELBO(\lambda)$ minimizes the $KL(q_{\lambda}(z|X) || p(z|X))$ and $log(P(X))$ is constant
