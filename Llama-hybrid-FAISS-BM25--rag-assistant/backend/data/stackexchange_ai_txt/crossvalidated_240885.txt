[site]: crossvalidated
[post_id]: 240885
[parent_id]: 
[tags]: 
Maximum likelihood estimation for data too big to fit in memory?

I have a data set that is many GB in size, containing hundreds of millions of rows. I would like to fit a probability model to that data (specifically a random-utility discrete choice model ), but it is too big to fit in memory all at once. This is what I would call "medium-size data" (fits on disk, too big for local memory) as opposed to "big" (too big to fit on one disk) or "small" (fits in local memory). The problem is that, if the data is too big to fit in memory, then the likelihood function itself will also be too big to fit in memory. What techniques or algorithms can be used to work around this problem, on a single machine? I'm aware of a recent result from Google [1] that describes how MCMC can be distributed across machines, but I'm not sure it can be applied specifically to my "one machine, not enough RAM" situation. 1: Scott, Steven L., et al. (2016). Bayes and big data: the consensus Monte Carlo algorithm. International Journal of Management Science and Engineering Management , 11 (2). Available at http://www.tandfonline.com/doi/full/10.1080/17509653.2016.1142191 (gated, published version), or http://research.google.com/pubs/pub41849.html (free, draft)
