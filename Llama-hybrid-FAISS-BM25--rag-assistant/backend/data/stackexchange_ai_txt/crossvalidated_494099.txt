[site]: crossvalidated
[post_id]: 494099
[parent_id]: 272301
[tags]: 
Short answer: you are assuming that cross-validation is necessary. Once you realize that it is only a crude approximation of Bayesian optimization, you can use a better approximation, such as sequential model-based optimization with Optuna . Choosing which hyper-parameters to use can be based on a score of a fitted model evaluated on data which were not involved in fitting (then re-fitting a new model and seeing if you get an improvement to the score). See my sample code on Github for an example of searching for optimal hyper-parameters with explicitly Bayesian tools and methods. You do not need to perform cross-validation, but you always need to apply the theory of validation. Kaggle is doing one type of validation for you when you submit the predictions and they score according to the "true" labels. If you are using machine learning to fit your model, you can also perform validation within the training loop (actually you always do, at the end of each training epoch). In this case, to avoid confusion, it might help to call the other type of validation "testing" instead.
