[site]: datascience
[post_id]: 112288
[parent_id]: 
[tags]: 
evaluation of gradient for the subset of parameters using backpropagation

Consider simple feed-forward neural network with few layers. I would like to evaluate only gradients of one particular layer, denoted by X. This should be performed repetitively, while parameters of this layers are being always updated. In the same time, the parameters of other layer are remaining intact. Is there any algorithm to evaluate these gradients of layer X in efficient manner?
