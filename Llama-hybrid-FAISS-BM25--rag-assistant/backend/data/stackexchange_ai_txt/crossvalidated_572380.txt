[site]: crossvalidated
[post_id]: 572380
[parent_id]: 572369
[tags]: 
The answer to your first question follows from the fact that the Kullback-Leibler divergence is, under mild conditions, invariant under transformations. This is straightforward and is shown in the section "Properties" of the Wikipedia site that you have referred to . The answer to your second question can be found in Papamakarios, George, et al. "Normalizing flows for probabilistic modeling and inference." Journal of Machine Learning Research 22.57 (2021): 1-64. in sections 2.3.1 and 2.3.2. What it says is basically this: If you can sample from $P_X$ , you should minimize the KL-divergence: $$ D_{KL}[P_X \| P_{F(U)}] = -\mathbb{E}_{P_X}[\log P_{F(U)}] - H_{P_X}, $$ where $H_{P_X}$ is the entropy of $P_X$ . For the optimization task of finding the parameters of $F$ and $P_U$ , the entropy is irrelevant, since it is constant. You can approximate the expectation $\mathbb{E}_{P_X}[P_{F(U)}]$ if you have samples of $P_X$ , so this is appropriate for your second scenario. For your first scenario, you should switch the arguments of the KL-divergence, i.e. you should minimize: $$ D_{KL}[P_{F(U)} \| P_X] = \mathbb{E}_{P_U}[\log P_U] - \mathbb{E}_{P_U}[\log P_{F^{-1}(X)}] $$ and since $P_U$ is considered to be easy to use, your only problem is to evaluate $P_{F^{-1}(X)}$ , but that is considered doable in your first scenario.
