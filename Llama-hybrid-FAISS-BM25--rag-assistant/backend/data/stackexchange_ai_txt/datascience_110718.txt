[site]: datascience
[post_id]: 110718
[parent_id]: 
[tags]: 
Sum vs mean of word-embeddings for sentence similarity

So, say I have the following sentences ["The dog says woof", "a king leads the country", "an apple is red"] I can embed each word using an N dimensional vector, and represent each sentence as either the sum or mean of all the words in the sentence (e.g Word2Vec ). When we represent the words as vectors we can do something like vector(king)-vector(man)+vector(woman) = vector(queen) which then combines the different "meanings" of each vector and create a new, where the mean would place us in somewhat "the middle of all words". Are there any difference between using the sum/mean when we want to compare similarity of sentences, or does it simply depend on the data, the task etc. of which performs better?
