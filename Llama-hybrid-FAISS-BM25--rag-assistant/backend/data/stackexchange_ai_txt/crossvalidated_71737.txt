[site]: crossvalidated
[post_id]: 71737
[parent_id]: 71455
[tags]: 
Since you are computing $r^2$ for each patient $i$ and you want the following properties: So patients with more data points will likely have a worse metric, if nothing is done. Series with more measurements should be higher weighted My concern is that those with 3 and those with 6 should be treated in a way that does not favor one way or another Series with lesser points should also be included judiciously. A simple heurisic comes to mind to measure linearity across the whole dataset: Let the number of data points for each patient be $n_i$ and the total number of patients be $K$. Since you are already computing $r_i^2$ for each patient $i$, how about getting a weighted average: $$\frac{\sum_{i=1}^{K}n_i r_i^2}{\sum_{i=1}^{K}n_i}$$ This has the property that it will give more importance to series with higher number of observations. Instead of $n_i$ as the coefficients in the numerator and denominator above, you can use any other function of $n_i$ to address your concerns I quoted above. Another thought: To counter the issue of ireegularly sampled observations,do the following. Fit $n_i-1$ dimensional polynomials ( link1 , link2 ) to each of the time series observations you have (as an extension of the divided differences idea by @Glen_b). Given $n_i$ algorithm outputs $\{V(t_k)\}$ at $n_i$ times $\{t_k\}$, this is just $$ V^{\textrm{poly. approx}}(t)=\sum_{k=1}^{n_i}V(t_k)\cdot\prod_{1\leq j\leq n,j\neq k}\frac{t-t_k}{t_k-t_j}. $$ Then, evaluate each of these polynomials at regular time locations for each patient. And then fit the lines and proceed as above.
