[site]: crossvalidated
[post_id]: 324370
[parent_id]: 
[tags]: 
References on number of features to use in Random Forest Regression

The default number of features $m$ used when making splits in a random forest regression is $m=p$ in Python's sklearn, where $p$ is the number of predictors in the regression problem (see the sklearn docs : If auto , then max_features = n_features ). In R's randomForest package, the default number of features used when making splits in a regression problem is $m=p\,/\,3$ (see the randomForest docs , and specifically the mtry argument to randomForest ). This point is (briefly) discussed at https://github.com/scikit-learn/scikit-learn/issues/7254 , where an sklearn contributor says that $m=p$ is recommended. I've seen the $m=p\,/\,3$ recommendation in several places (e.g. https://stackoverflow.com/questions/23939750/understanding-max-features-parameter-in-randomforestregressor and https://web.stanford.edu/~hastie/Papers/ESLII.pdf ) My general understanding is that $m$ should always be tuned in random forest regression problems, and that the optimal $m$ could vary depending on the setting. Are there any references that discuss $m$ specifically in the context of regression (as opposed to classification)? Is there any well-founded reason to prefer either R's or sklearn's default value for $m$ (or should the answer be "don't use the default, always tune," so that the default doesn't matter)?
