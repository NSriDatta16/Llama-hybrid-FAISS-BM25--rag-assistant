[site]: crossvalidated
[post_id]: 531575
[parent_id]: 531572
[tags]: 
I have never come across such a thing in literature, but it is a very interesting idea. Firstly, I'd like to point out that normalised confusion matrices exist (I know this isn't what you are asking for but it will illustrate a point I'm going to make, so just bare with me); for these types of confusion matrix there is some form of normalisation such that rows or columns sum to 1, the matrix has a norm of 1, or individual elements are normalised relative to the total number of samples. This means, of course, that a confusion matrix can contain entries which are in the range $[0,1]$ instead of the typical confusion matrix where entries are in range $[0, NumSamples]$ , it encapsulates the same relationships as an un-normalized confusion matrix but simply with the values scaled. My idea would be instead of creating a normalised matrix that contains TP/TN/FP/FN as entries you instead construct a matrix of the One-vs-One scores for different classes using a metric such as Average Precision which takes into account how thresholding affects prediction. Of course, this matrix would be symmetric as Dog-vs-Cat has the same AP as Cat-vs-Dog, but it would give an idea of prediction confidence based on the probabilistic scores rather than the hard predictions. AP would be my first choice, but this method would be relevant to any metric which use prediction scores (and would even work for metrics that use hard predictions too).
