[site]: crossvalidated
[post_id]: 533261
[parent_id]: 531560
[tags]: 
In your standard GAN, the discriminator loss is only an indication of how well the discriminator is doing. However in a WGAN, the discriminator loss is proportional to the negative Wasserstein distance between the generator distribution and the true distribution. However, this is only true if the discriminator is Lipschitz (among other conditions). But how can we control the Lipschitzness of a neural network? As it turns out, two ways are Clip the weights of the network. It shouldn't be hard to see that this makes each layer lipschitz, and therefore the entire network is lipschitz. Use a loss function to discourage the gradient from being too far from 1. This doesn't strictly constrain the network to be lipschitz, but empirically, it's a good enough approximation. Since your standard GAN, unlike WGAN, is not trying to minimize Wasserstein distance, there's no need for these tricks. However, constraining a similar measure -- the "spectral norm" of a neural network, has been found to improve standard GAN performance. For a linear layer, the spectral norm and the lipschitzness are the same. However, from what I've heard and seen, spectral normalization doesn't work particularly well in WGANs.
