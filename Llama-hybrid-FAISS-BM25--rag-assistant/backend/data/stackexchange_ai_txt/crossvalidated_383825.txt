[site]: crossvalidated
[post_id]: 383825
[parent_id]: 383813
[tags]: 
I am not aware of any convention to use more stringent alpha levels (and shoot for higher beta) in reproducibility studies, but this may certainly be specific to the field. The drawback of a lower alpha and higher beta is, of course, that you will need more data, i.e., a larger sample size and more resources to conduct your study. A statistical reviewer might run his own calculations and argue that the resources you are applying for can be reduced by, say, 75% by assuming "standard" alpha and beta values. So I would suggest that you need to argue why the more stringent values are useful. Is there a danger for this to become an underpowered study? And of course, in a Bayesian sense, the fact that you are reproducing a previously reported effect means that our priors are already shifted in favor for there being an effect. Which would argue for more lenient alpha and beta levels, not more stringent ones. I personally believe that judicious resampling is a much better approach to power analysis than using canned formulas or online sample size calculators. It allows you to tailor your power analysis to your specific study and to the specific data you have. I would say that using a "traditional" canned formula should require justification, not the other way around. Then again, I likely won't be reviewing your grant application. I would recommend that you keep your resampling approach and justify it in a short sentence as per my paragraph above. For bonus points, look whether there are any established power calculations for your specific problem, taking the effect size from your pilot data - these calculations will likely enough yield a roughly similar result to your resampling. If nothing else, reporting both should favorably impress any reviewers with your thoroughness. Finally, do not be wedded to the effect size you observe in your pilot data. (It's already better than taking one reported in the literature, which would certainly suffer from publication bias.) Rather, use an effect size that is clinically relevant - whether that is larger or smaller than what you see in your pilot data. The best guidance for choosing an effect size I have come across is to use an effect size that one would be sorry to miss because of underpowering the study.
