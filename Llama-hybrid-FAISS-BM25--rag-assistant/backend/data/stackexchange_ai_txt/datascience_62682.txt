[site]: datascience
[post_id]: 62682
[parent_id]: 62657
[tags]: 
I would approach such a problem with a method a bit similar to record linkage : trying to match every product description with the most relevant HTS description. The traditional approach would be to use textual similarity measures such as cosine TF-IDF, but many variants can also be considered, e.g. with embeddings or other ways to take semantic similarity into account. At the end the code corresponding to the most similar HTS description is predicted. Initially this could be done with vectors of words (unigrams), but it's true that comparing vectors of $n$ -grams is likely to be more precise. However $n$ -grams don't work the way you describe: in a case like your example you would have for instance $n=2$ and extract all the sequences of 2 consecutive words. This "bag of $n$ -grams" is what the vector represents. Combining different lengths of $n$ -grams is possible but not in the same representation: for example you could measure cosine similarity over 2-grams vectors only, then measure cosine similarity of 3-grams vectors only, and take the mean of the two scores (or even build a regression model using different similarity scores as features). In case it helps, here is an example of computing a very simple similarity score with bigrams (one could certainly find better examples online).
