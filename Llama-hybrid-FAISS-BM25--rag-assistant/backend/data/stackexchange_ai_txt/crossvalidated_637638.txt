[site]: crossvalidated
[post_id]: 637638
[parent_id]: 637636
[tags]: 
Transformers are often used for sequence/text classification. The transformer used for such tasks differs in architecture to generative models such as ChatGPT. While ChatGPT uses an Encoder and a Decoder, the classification models (often) only use an Encoder. In many cases a [CLS] token, as a classification token will be introduced in the beginning of each sequence. And the embedding of that token will then be used to classify the sequence. You can read more on the actual implementation here. https://huggingface.co/docs/transformers/tasks/sequence_classification
