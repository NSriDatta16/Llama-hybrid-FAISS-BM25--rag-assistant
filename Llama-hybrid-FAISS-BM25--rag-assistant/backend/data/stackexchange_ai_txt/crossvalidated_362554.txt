[site]: crossvalidated
[post_id]: 362554
[parent_id]: 
[tags]: 
At a loss regarding feature selection vs coefficient estimation. Can you ever re-do the latter after the former?

I'm looking at a binary classification problem where p>>>n (9,000 gene expression variables for 290 patients who either have or don't have disease). I hypothesized that it would be easy to find "significant" genes (variables) that help classify patient disease simply because there are so many genes in the dataset. Basically I expected to find "significant" variables which really are just noise. I used an elastic net logistic regression in caret using 5 repeats of 10-fold cv, leaving 30% of the data for testing. I only used 1000 genes, and selected the top 10 of those genes according to variable importance. To my surprise, the model with 10 genes did very well on the test set (90% sensitivity and specificity). Could that indicate that I actually found some genes that help determine disease status? However, the model performed horribly on a fresh data set obtained from a different study (but with the same type of diseased and control patients). However again, to my surprise, if I used these same 10 genes and re-estimated their coefficients using 70% of the new dataset, they again performed very well on the test set. I'm not sure what to think. Are these genes helpful or not? It seems like it's cheating to re-calculate coefficients every time you get a new dataset, but it's also surprising that they work well on the test sets when I do that. Googling these genes separately also pulls up one or two papers per gene where it is included in a huge list of genes that are upregulated or downregulated in the presence of this disease. There is no follow up discussion or analysis of those genes though, so perhaps the authors of those papers realized later that they found nothing of importance.
