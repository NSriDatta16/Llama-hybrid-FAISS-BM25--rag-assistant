[site]: datascience
[post_id]: 107960
[parent_id]: 
[tags]: 
why sign flip to indicate loss in hyperopt?

I am using the hyperopt to find best hyperparameters for Random forest. My objective is to get the parameters which returns the best f1-score as my dataset is imbalanced. So, I defined the below objective function based on a tutorial online space = { "n_estimators": hp.choice("n_estimators", [100, 200, 300, 400,500,600]), "max_depth": hp.quniform("max_depth", 1, 15,1), "criterion": hp.choice("criterion", ["gini", "entropy"]), "max_features": hp.quniform("max_features", 1, 14,1) } def hyperparameter_tuning(params): clf = RandomForestClassifier(**params,n_jobs=-1) f1score = cross_val_score(clf, ord_train_t, y_train,scoring="f1").mean() return {"loss": -f1score, "status": STATUS_OK} Am I doing this right? If I remove the loss keyword from return statement it throws key error . Why is it so? I just wish to name the key as f1score but it threw error. Once I changed it back to loss , it started working fine. I get output like below 88%|████████████████████████████████████████▍ | 88/100 [29:42 Does it mean my model returns an f1-score of 86%? Shouldn't the loss be 100-86 = 14? btw, why is it that we should sign-flip the f1score to indicate loss?
