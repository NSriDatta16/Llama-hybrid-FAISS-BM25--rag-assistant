[site]: crossvalidated
[post_id]: 418939
[parent_id]: 
[tags]: 
Neural networks: why don't we use a multi-dimensional learning rate

I've searched a bit on the internet a have found the answer nowhere so I decided to post here. When confronted to an optimization problem, we know that the sanity of the problem can be characterized by the condition number, which is the ratio of the highest eigenvalue of the hessian over the smallest one. I am puzzled by the fact that we use a 1D learning rate in neural networks for instance, where we have highly multidimensional problems. Why couldn't we go to the eigenvector's of the hessian basis and from there use a different learning rate for each component (which are then independent) ? It seems to me that it would solve sanity problems, but since I can't be the first who thought of this and since this is mentioned nowhere, my thinking is probably wrong.
