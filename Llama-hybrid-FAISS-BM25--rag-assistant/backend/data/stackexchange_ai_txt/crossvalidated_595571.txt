[site]: crossvalidated
[post_id]: 595571
[parent_id]: 595515
[tags]: 
Depending on the assumptions one is willing to make, the problem can be stated mathematically as follows. Case I Let $X_1,\ldots,X_n$ an i.i.d. sample with $X_i\sim N(\mu, \sigma^2)$ , with $\mu, \sigma^2$ both unknown finite parameters. Furthermore, let $X = n^{-1}\sum_{i=1}^n X_i$ and $S^2 = (n-1)^{-1}\sum_{i=1}^n (X_i-\bar X)^2$ be respectively the sample average and the sample variance and let us be interested in testing $H_0:\mu=\mu_0$ against $H_1:\mu>\mu_0$ . It can be proved that $$ \frac{\sqrt{n}(\bar {X}-\mu)}{\sigma^2}\sim N(0,1), $$ $$ \frac{(n-1)S^2}{\sigma^2}\sim \chi_{n-1}^2, $$ and that $\bar X$ and $S^2$ are independent. Thus $$ T_n = \frac{\frac{\sqrt{n}(\bar {X}-\mu)}{\sigma^2}}{\sqrt{\frac{\frac{(n-1)S^2}{\sigma^2}}{n-1}}} = \frac{\sqrt{n}(\bar {X}-\mu)}{\sqrt{S^2}}\sim t_{n-1}, $$ is the usual $t$ statistic, which follows a $t$ -Student distribution with $n-1$ degrees for freedom. A test of size $\alpha$ has rejection $$R_{\alpha} = \{X_1,\ldots,X_n: T_n \geq t_{n-1,1-\alpha}\}.$$ If we denote by $T_{n}^{obs}$ , the observed $t$ statistics, the $p$ -value (see here ) is given by $$ \sup_{\mu\leq \mu_{0}} P_\theta(T_n \geq T_{n}^{obs}) = P_{\mu_0}(t_{n-1}\geq T_{n}^{obs}). $$ Case II If $\sigma^2$ is known then there is no need to estimate it and the statistic to be used is $$ Z_n = \frac{\sqrt{n}(\bar X-\mu)}{\sigma}\sim N(0,1). ,$$ The test of size $\alpha$ is to reject $H_0$ if $Z_n\geq z_{1-\alpha}$ and the $p$ -value is $$ \sup_{\mu\leq \mu_{0}} P_\theta(Z_n \geq Z_{n}^{obs}) = P_{\mu_0}(Z\geq Z_{n}^{obs}). $$ Case III Lastly, if we don't wish to assume the normality of the sample but only that the sample is iid from a population with mean $\mu$ and variance $\sigma^2$ , both unknown, then we can use the statistic $$ Z_n^{s} = \frac{\sqrt{n}(\bar X-\mu)}{S^2}, $$ which by CLT converges to $N(0,1)$ ; the test and the $p$ -value are computed as in Case II. Case IV Again, we do not make the normality assumption but only that the sample is iid from a population with mean $\mu$ and variance $\sigma^2$ , with $\sigma^2$ known. The test statistic is as in Case II, and its distribution $N(0,1)$ holds only in the limit, for large $n$ . Example. Your example seems to follow in Case IV, with $n=36$ , $\bar x = 71$ , $\mu_0=67$ and $\sigma^2 = 7^2$ . Thus the observed test statistic is $$ \frac{\sqrt{36}(71-67)}{7}=3.428571 $$ and $p$ -value = $P(Z\geq 3.428571) = 0.000303$ . Nevertheless, to have less conservative results, in Case III and Case IV, some statisticians prefer to use the $t_{n-1}$ distribution in place of the limiting $N(0,1)$ distribution.
