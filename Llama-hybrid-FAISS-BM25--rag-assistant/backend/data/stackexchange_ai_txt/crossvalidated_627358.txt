[site]: crossvalidated
[post_id]: 627358
[parent_id]: 627341
[tags]: 
The TL;DR is that whitening isn't essential but it does simplify the task. Specifically, whitening the data reduces the number of parameters under estimation. " Independent Component Analysis: Algorithms and Applications " by Aapo Hyvärinen and Erkki Oja. Neural Networks, 13(4-5):411-430, 2000. Here we see that whitening reduces the number of parameters to be estimated. Instead of having to estimate the $n^ 2$ parameters that are the elements of the original matrix $A$ , we only need to estimate the new, orthogonal mixing matrix $\tilde{A}$ . An orthogonal matrix contains $n(n − 1)/2$ degrees of freedom. For example, in two dimensions, an orthogonal transformation is determined by a single angle parameter. In larger dimensions, an orthogonal matrix contains only about half of the number of parameters of an arbitrary matrix. Thus one can say that whitening solves half of the problem of ICA. Because whitening is a very simple and standard procedure, much simpler than any ICA algorithms, it is a good idea to reduce the complexity of the problem this way. The geometric intuition is that whitening doesn't remove any geometric information. It just rotates the data. Correlations only make sense in a specific coordinate system; if we rotate the coordinates, we can remove correlation. The second row of this diagram taken from Wikipedia contains several lines, but the center one has 0 correlation, even though it's just a rotation of the other lines in that row.
