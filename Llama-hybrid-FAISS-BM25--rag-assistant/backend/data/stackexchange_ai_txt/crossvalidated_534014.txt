[site]: crossvalidated
[post_id]: 534014
[parent_id]: 534010
[tags]: 
The purpose of both Bayesian networks and Markov networks is to represent conditional independencies , although each of them have slightly different ways of doing so. In a Bayesian network, conditional independencies can be understood using the Markov condition . It states that for each node in a Bayesian network, the random variable in this node is conditionally independent of its non-descendants given its parents. To better understand this, consider the Flu random variable in the Bayesian network in your question. Its descendants are Muscle-Pain and Congestion. This means that Season and Hayfever form its set of non-descendants. However, Season is also Flu's parent, and so we don't include it in Flu's set of non-descendants. Therefore, $$ \text{Flu} \perp \text{Hayfever} \ \mid \ \text{Season} $$ which reads as "Flu is independent of Hayfever given Season". Now suppose that we are interested in computing $p(\text{Flu})$ . Using the law of total probability, $$ p(\text{Flu}) = \sum_{\text{Season},\text{Hayfever},\\ \text{Congestion},\\ \text{Muscle-Pain}} p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) $$ Using Bayes' rule $$ p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) = \\ p(\text{Muscle-Pain} \mid \text{Season},\text{Hayfever},\text{Congestion},\text{Flu}) \times \\ p(\text{Congestion} \mid \text{Hayfever},\text{Season},\text{Flu}) \times \\ p(\text{Flu} \mid \text{Season},\text{Hayfever}) \times \\ p(\text{Hayfever} \mid \text{Season}) \times \\ p(\text{Season}) $$ However, we know that Flu is independent of Hayfever given Season, and so the term $$ p(\text{Flu} \mid \text{Season},\text{Hayfever}) $$ becomes $$ p(\text{Flu} \mid \text{Season}) $$ We can then use the Markov condition to deduce other conditional independencies from the Bayesian network. These are $$ \text{Season} \perp \emptyset \ \mid \ \emptyset \\ \text{Hayfever} \perp \text{Flu},\text{Muscle-Pain} \ \mid \ \text{Season} \\ \text{Muscle-Pain} \perp \text{Hayfever},\text{Congestion},\text{Season} \ \mid \ \text{Flu} \\ \text{Congestion} \perp \text{Season},\text{Muscle-Pain} \ \mid \ \text{Flu},\text{Hayfever} \\ $$ where $\emptyset$ indicates no parents or no non-descendants. We can use these to simplify the expression for $$ p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) $$ even further, such that $$ p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) = \\ p(\text{Muscle-Pain} \mid \text{Flu}) \times \\ p(\text{Congestion} \mid \text{Hayfever},\text{Flu}) \times \\ p(\text{Flu} \mid \text{Season}) \times \\ p(\text{Hayfever} \mid \text{Season}) \times \\ p(\text{Season}) $$ What did we gain from this simplification? More generally, what did we gain from representing these conditional independencies using a Bayesian network? Notice that if we want to compute $$ p(\text{Flu}) = \sum_{\text{Season},\text{Hayfever},\\ \text{Congestion},\\ \text{Muscle-Pain}} p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) $$ which actually consists of 4 summations: $$ p(\text{Flu}) = \sum_{\text{Season}} \sum_{\text{Hayfever}} \sum_{\text{Congestion}} \sum_{\text{Muscle-Pain}} p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) $$ Let us substitute $$ p(\text{Flu},\text{Season},\text{Hayfever},\text{Congestion},\text{Muscle-Pain}) = \\ p(\text{Muscle-Pain} \mid \text{Flu}) \times \\ p(\text{Congestion} \mid \text{Hayfever},\text{Flu}) \times \\ p(\text{Flu} \mid \text{Season}) \times \\ p(\text{Hayfever} \mid \text{Season}) \times \\ p(\text{Season}) $$ into this summation, such that $$ p(\text{Flu}) = \sum_{\text{Season}} \sum_{\text{Hayfever}} \sum_{\text{Congestion}} \sum_{\text{Muscle-Pain}} \\ p(\text{Muscle-Pain} \mid \text{Flu}) \times \\ p(\text{Congestion} \mid \text{Hayfever},\text{Flu}) \times \\ p(\text{Flu} \mid \text{Season}) \times \\ p(\text{Hayfever} \mid \text{Season}) \times \\ p(\text{Season}) $$ Notice that we can re-arrange and distribute the summations such that $$ p(\text{Flu}) = \\ \left[\sum_{\text{Muscle-Pain}} p(\text{Muscle-Pain} \mid \text{Flu}) \\ \left[\sum_{\text{Congestion}} \sum_{\text{Hayfever}} p(\text{Hayfever} \mid \text{Season}) \cdot p(\text{Congestion} \mid \text{Hayfever},\text{Flu}) \\ \left[\sum_{\text{Season}} p(\text{Season}) \cdot p(\text{Flu} \mid \text{Season})\right]\right]\right] $$ This a bit messy, so let us clean it up by letting: \begin{align} \phi_1(\text{Flu}) &= \sum_{\text{Season}} p(\text{Season}) \cdot p(\text{Flu} \mid \text{Season}) \\ \phi_2(\text{Season},\text{Flu}) &= \phi_1(\text{Flu}) \cdot \sum_{\text{Congestion}} \sum_{\text{Hayfever}} p(\text{Hayfever} \mid \text{Season}) \cdot p(\text{Congestion} \mid \text{Hayfever},\text{Flu}) \\ \phi_3(\text{Season},\text{Flu}) &= \phi_2(\text{Season},\text{Flu}) \cdot \sum_{\text{Muscle-Pain}} p(\text{Muscle-Pain} \mid \text{Flu}) \\ p(\text{Flu}) &= \phi_3(\text{Season},\text{Flu}) \end{align} So, computing $p(\text{Flu})$ now involves a sequence of recursive calculations. Additionally, we have drastically reduced the number of additions and multiplications that we need to perform by Making use of the conditional independencies induced by the Bayesian network Re-arranging and distributing the summations. This two-step process is encapsulated by the Belief Propagation algorithm. To summarize: Why are probabilistic graphical models useful? Because they allow us to model conditional independencies, which in turn allow us to compute probabilities very efficiently. As for your second question Can we, or how can we use continuous distributions at different nodes? Yes. This is done by replacing summations with integrations in the process above. However, the process of re-arranging integrations is a bit more nuanced. See Fubini's theorem for details.
