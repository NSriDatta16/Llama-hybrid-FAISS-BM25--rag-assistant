[site]: datascience
[post_id]: 120469
[parent_id]: 
[tags]: 
Binary transformer classification model predicts everything as same value

I'm training a binary classifier using a transformer on structured numerical data (so the order of the columns in my spreadsheet matters). I have adapted the keras text classification model for IMDb reviews ( https://keras.io/examples/nlp/text_classification_with_transformer/ ) but my model is predicting everything as the same class. After some analysis of the outputs I've figured out that all the predictions are why does this occur? I have tried experimenting with the hyperparameters but this has not fixed my problem. My dataset is small (around 500 samples) but balanced, and for each sample contains 846 sequential real numbers. Any help or modifications to my code will be very appreciated! import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import pandas as pd from keras import metrics class TransformerBlock(layers.Layer): def __init__(self, embed_dim, num_heads, ff_dim, rate=0.000001): super().__init__() self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),] ) self.layernorm1 = layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = layers.LayerNormalization(epsilon=1e-6) self.dropout1 = layers.Dropout(rate) self.dropout2 = layers.Dropout(rate) def call(self, inputs, training): attn_output = self.att(inputs, inputs) attn_output = self.dropout1(attn_output, training=training) out1 = self.layernorm1(inputs + attn_output) ffn_output = self.ffn(out1) ffn_output = self.dropout2(ffn_output, training=training) return self.layernorm2(out1 + ffn_output) class TokenAndPositionEmbedding(layers.Layer): def __init__(self, maxlen, vocab_size, embed_dim): super().__init__() self.token_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) def call(self, x): maxlen = tf.shape(x)[-1] positions = tf.range(start=0, limit=maxlen, delta=1) positions = self.pos_emb(positions) x = self.token_emb(x) return x + positions whole = pd.read_excel('bigset.xlsx', header=None) wholedata = np.array((whole.drop(columns = [0]).values).tolist()) wholelabel = np.array((whole[0].values).tolist()) (x_train, y_train), (x_val, y_val) = (wholedata[0:500], wholelabel[0:500]), (wholedata[501:581], wholelabel[501:581]) # Define input shape and model hyperparameters maxlen = x_train.shape[1] # Maximum sequence length embed_dim = maxlen # Embedding size for each vector num_heads = 2 # Number of attention heads ff_dim = 4 # Hidden layer size in feed forward network inside transformer # Define transformer model inputs = layers.Input(shape=(maxlen,)) embedding_layer = TokenAndPositionEmbedding(maxlen, maxlen, embed_dim) x = embedding_layer(inputs) transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) x = transformer_block(x) x = layers.GlobalAveragePooling1D()(x) x = layers.Dropout(0.1)(x) x = layers.Dense(20, activation="sigmoid")(x) x = layers.Dropout(0.1)(x) outputs = layers.Dense(1, activation="sigmoid")(x) model = keras.Model(inputs=inputs, outputs=outputs) model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"]) model.fit( x_train, y_train, batch_size=10, epochs=300, validation_data=(x_val, y_val) ) predictions = model.predict(x_val) print(predictions) conf_matrix = tf.math.confusion_matrix(labels=y_val, predictions=predictions) print(conf_matrix) ```
