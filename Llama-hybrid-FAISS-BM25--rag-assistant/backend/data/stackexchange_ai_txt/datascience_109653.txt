[site]: datascience
[post_id]: 109653
[parent_id]: 109645
[tags]: 
You can create a sort of encoder-decoder network with two different inputs. latent_dim = 16 # First branch of the net is an lstm which finds an embedding for the var1 var_1_inputs = tf.keras.Input(shape=(window_len_1, n_1_features), name='var_1_inputs') # Encoding var_1 encoder = tf.keras.layers.LSTM(latent_dim, return_state=True, name = 'Encoder') encoder_outputs, state_h, state_c = encoder(var_1_inputs) # Apply the encoder object to var_1_inputs. var_2_inputs = tf.keras.Input(shape=(window_len_2, n_2_features), name='var_2_inputs') # Combining future inputs with recurrent branch output decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, name = 'Decoder') x = decoder_lstm(var_2_inputs, initial_state=[state_h, state_c]) x = tf.keras.layers.Dense(16, activation='relu')(x) x = tf.keras.layers.Dense(16, activation='relu')(x) output = tf.keras.layers.Dense(1, activation='relu')(x) model = tf.keras.models.Model(inputs=[var_1_inputs,var_2_inputs], outputs=output) optimizer = tf.keras.optimizers.Adam() loss = tf.keras.losses.Huber() model.compile(loss=loss, optimizer=optimizer, metrics=["mae"]) model.summary() Here you are, of course I inserted random numbers for layer, latent dimensions, etc. You can also have different features to input with var_1 and var_2 and these have to passed as arrays.
