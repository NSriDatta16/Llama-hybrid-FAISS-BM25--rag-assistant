[site]: datascience
[post_id]: 69279
[parent_id]: 69025
[tags]: 
After 4 days of hard continuous computing the results suggest that indeed the tol=0.01 parameter seems to be the best one. This of course depends on the topology of the data and other factors but if the literature suggests that plus my own findings as well this might hint that it might be a universal value applicable for perhaps all shapes of data. There is a very low correlation between the tolerance parameter and the output result with a sample size of >1000. The average value barely changes as we increase the tol parameter from 1e-10 to 1e-0 (0.9 being the max value I tried). However the computing time drastically lowers itself from around 7 minutes to 5 minutes per run. Since there is no change to the output value but only in the duration of the calculation it is rational to use 0.01 as a tolerance parameter. However above 0.01 the values actually start to increase, which means that 0.01 is as good as it gets. As for premature terminations of the optimizer, this should not be a problem since this can still be calibrated with the recombination parameter (CR= crossover probability), by setting it to a low value for a delayed convergence and a high one for an earlier one. This in it's turn also has to be optimized. One thing is worth noting is that using a Latin Hypercube as init values is not good. I have found much higher results with random sampling, much better coverage, and if there is a problem with clustering of the values, then just increase the number of runs. There is no way the differential evolution optimizer will find the best value in 1 run, so it has to be run multiple times either way. Unfortunately I can't provide the data, but if anyone else has the time to verify these findings on their own data that would be good too.
