[site]: crossvalidated
[post_id]: 302887
[parent_id]: 
[tags]: 
Bootstrapping standard errors

For research purposes I'm trying to predict the transfer prices of football players. I've implemented several models of which one is Lasso. Now, besides prediction, I would like to measure actual effects of certain variables, for which I've implemented Post-Lasso estimation (Least squares on the non-zero coefficients chosen by Lasso). I've read on several sites, and for one here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4207812/ that the standard errors are wrong, because they do not take into account the selection procedure which I've used before the least squares estimation. Furthermore, I am dealing with heteroskedasticity and therefore I cannot use the 'normal' standard errors returned by OLS. Now I was wondering whether or not the bootstrap method: Bagging [1] works to deal with this problem. I would sample with replacements multiple times, run least squares on all these samples and calculate the standard errors over all the coefficients. My question: Would this work and why (not)? Thanks a lot! [1] Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140.
