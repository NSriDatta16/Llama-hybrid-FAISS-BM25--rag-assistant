[site]: crossvalidated
[post_id]: 337666
[parent_id]: 225916
[tags]: 
One way to do this a little more adaptively is to specify the penalty instead of the number of nonzeroes you want. (Set sparse='penalty' .) Then you can specify the same parameter value across all components, while still allowing them to have different levels of sparsity. Ordinarily, one might select the penalty parameter via cross-validation, but spca cannot deal with missing values, so it's hard to hold out a set of entries. You could hack together something similar, though: Train the SPCA, holding out one case. You'll obtain L and R such that $X_{-1} \approx LR$. (Suppose $X_{-1}$ has cases as rows, and you left out row 1.) Using half the variables (columns), fit a regression model to predict $X_{1}$ from $R$. You'll obtain $L_1$ such that $L_1R \approx X_{1}$. Measure the predictive accuracy of the regression model using the other half of the variables. You should probably hold out more than 1 row at a time. I'll look for references if anybody's still looking at this page -- drop me a comment.
