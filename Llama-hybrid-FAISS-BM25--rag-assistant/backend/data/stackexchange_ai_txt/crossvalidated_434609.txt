[site]: crossvalidated
[post_id]: 434609
[parent_id]: 
[tags]: 
Taking derivative for RNN back propogation

I am trying to understand the derivation of backpropagation for recurrent neural networks (RNNs) from this source: https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf I am stuck in understanding the below-given equation: $\frac{\partial}{V_{ij}}(V_{lm}s_{m}) = \delta _{il} \delta _{jm}s_{m}$ I dont understand where these two Kronecker deltas come from.
