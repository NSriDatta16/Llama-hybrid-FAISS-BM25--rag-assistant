[site]: datascience
[post_id]: 97151
[parent_id]: 97102
[tags]: 
How is it possible to use DeepQ along with a tabular Q representation? The whole purpose of Q-learning with action-value function approximation (= DeepQ) is to overcome the limitations of a tabular approach. In any case, you need to consider that, in a sense, game theory + RL $\approx$ multi-agent RL (MARL). The game that you are trying to tackle is (most likely) a zero-sum game as it is a competitive game among 2 or more players. If you try to use one agent vs another opponent agent (how did you model the opponents?) you treat the opponent as part of the environment and that will lead to a non-stationary policy. This problem is quite critical as most of the times, except if opponent is easily exploitable and has very limited strategies, it will lead to conditions that won't allow your training agent to converge. Then you have to deal with the problem of partial observability as players cannot see each other's hands. In other words, treating a MARL problem as RL is not a good idea. Here is Hanabi a cooperative multi-agent card game. There are lots of MARL "solutions" (there is no actual solution just better performance) for this game out there. This might give you a starting point. Before starting, consider that MARL contains quite advanced material and if you are not skilled with RL (and understand the math behind) it won't actually help you. Whatever algorithm you find it won't be an easy plug-n-play solution.
