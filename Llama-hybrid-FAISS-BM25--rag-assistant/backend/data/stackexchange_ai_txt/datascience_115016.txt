[site]: datascience
[post_id]: 115016
[parent_id]: 115013
[tags]: 
Your understanding is mostly correct, but don't forget that you can not only take into account the previous tokens, you can also consider: p(this word | next words) p(this word | N previous word,N next words) etc. Often a combination of these probabilities offers optimal results. how is this machine learning I wonder? Well, ML is very often a deterministic calculation, it doesn't have to be a complex approximation problem: decision trees, Naive Bayes, linear regression... do I simply only compare the posterior probabilities of the few known prepositions and find the argmax as the prediction? Yes, that would be the idea. Of course if you use multiple models as suggested above there must be some kind of combination additionally.
