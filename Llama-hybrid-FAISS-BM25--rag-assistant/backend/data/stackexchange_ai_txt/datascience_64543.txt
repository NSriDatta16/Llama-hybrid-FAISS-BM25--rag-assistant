[site]: datascience
[post_id]: 64543
[parent_id]: 
[tags]: 
Carlification of the MSE loss sum symbol

So I have a question regarding the MSE loss on the application of a Neural Network. Loss function: $\text{MSE} = \frac{1}{2} \sum_{i=1}^{n} (Y_i - \hat{Y_i}) ^ 2$ I am wondering for what the $\sum_{i=1}^{n}$ stands. Do I sum over the loss of all training examples for each output node in my Neural Network? Or do I use a single training example and sum over all Neural network output nodes? Or do I both and sum over all training examples and over all output nodes? I want to use the MSE loss later than for updating my weights in the Neural Network. What would I do for that?
