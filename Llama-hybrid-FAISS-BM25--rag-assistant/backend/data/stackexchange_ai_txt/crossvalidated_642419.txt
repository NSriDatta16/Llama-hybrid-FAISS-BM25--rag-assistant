[site]: crossvalidated
[post_id]: 642419
[parent_id]: 156711
[tags]: 
Your question might relate to 'independent filtering' of data that is performed by commonly used bioinformatics pipelines before any FDR adjustment takes place. The typical situation this is applied is when there are a very large number of outcomes to be tested (say expression levels of different genes) and a small number of predictors. The aim of this, as you suggest, is to reduce the total number of hypotheses to be tested. The software will remove outcomes where there is insufficient data on that outcome for a positive signal to be detected. It does not use any information from the predictors in this process. A criteria for gene expression might be that for a gene to be carried forward into FDR adjustment and interpretation phase, a certain number of samples in the dataset must have non-zero expression levels, or that the average expression level (or variation in expression level) should exceed a certain threshold. If all decided a priori I don't see a problem with this approach. The approach outlined in the link below is to tweak the threshold to obtain the maximum number of 'significant' results from the remaining dataset. This feels like it could be problematic, I haven't explored the arguments for why it might or might not be. See here for more information with respect to gene expression analysis with Deseq2. https://uclouvain-cbio.github.io/WSBIM2122/sec-rnaseq.html#independent-filtering
