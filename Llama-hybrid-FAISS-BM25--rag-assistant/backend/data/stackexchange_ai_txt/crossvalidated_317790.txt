[site]: crossvalidated
[post_id]: 317790
[parent_id]: 160070
[tags]: 
TDNNs is a simple way to represent a mapping between past and present values. The delays in the TDNN remain constant throughout the training procedure and are estimated before by using trial and error along with some heuristics. It, however, may be the case that these fixed delays do not capture the actual temporal locations of the time dependencies. On the other hand, the "memory" feature of the RNN structures can capture this information by learning these dependencies. The problem with RNNs is that they are impractical to use when trained with traditional techniques (e.g Backpropagation through time) for learning long term dependencies. This problem arises from the so-called "vanishing/exploding" gradient which basically means that as we propagate the error signals backwards through the networks structure they tend to vanish or explode. More advanced recurrent structures (eg LSTM) have properties that mitigate this problem and can learn long term dependencies and are particularly suited for learning sequential data.
