[site]: datascience
[post_id]: 36836
[parent_id]: 
[tags]: 
In RNNs why do networks always use the last output vs the last input?

All the descriptions of RNNs introduce some equation like: $\ h_t = f_W(h_{t-1},x_t)$ and I'm wondering why we don't just go straight to the "source", ie the last input like : $\ h_t = f_W(x_{t-1},x_t)$ The only thing I can maybe think of is maybe the RNN can store some sort of "state" in ht-1 But then, along the same vein, what would happen in an LSTM architecture if we used previous inputs vs previous outputs? It seems to me that the presence of the memory functionality should be able to avoid this potential problem (if it even is one)
