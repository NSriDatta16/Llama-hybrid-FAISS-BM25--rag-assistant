[site]: crossvalidated
[post_id]: 215641
[parent_id]: 
[tags]: 
Why does SVM needs to keep support vectors?

I am reading the book Artificial Intelligence a Modern Approach and I have trouble understanding why the SVM needs to keep support vectors. From the book: SVMs are a nonparametric method -- they retain training examples an potentially need to store them all. On the other hand, in practice they often end up retaining only a small fraction of the number of examples . And then: A final important property is that the weights associated with each data point are zero except for the support vectors -- the points closest to the separator. Because there are usually many fewer support vectors than examples SVMs gain some of the advantages of parametric models. Source: Artificial Intelligence a Modern Approach p746 As the SVMs separator is defined by a hyperplane w.x + b = 0 we only need to know w and b to make predictions. Why should it keep all the support vectors?
