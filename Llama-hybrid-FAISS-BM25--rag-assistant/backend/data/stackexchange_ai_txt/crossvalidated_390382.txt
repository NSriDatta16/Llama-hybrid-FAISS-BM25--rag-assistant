[site]: crossvalidated
[post_id]: 390382
[parent_id]: 
[tags]: 
Is it necessary to constrain the size of the neighborhood in LLE to be less than the space dimensionality?

The wikipedia entry on Locally Linear Embedding (LLE) says that LLE can be broken into stages, the first of which is to learn a barycentric linear model of the data with its $k$ -nearest neighbors: $$ E(W) = \sum_i |{\mathbf{X}_i - \sum_j {\mathbf{W}_{ij}\mathbf{X}_j}|}^\mathsf{2}$$ Where $j$ ranges over $x_i$ 's $k$ -nearest neighbors. However, if the $x_i$ have $d$ dimensions and $k > d$ , then if data are sampled randomly, they will almost surely be linearly independent, and thus the loss function above can always be made zero (since we can write $x_i$ exactly as a linear combination of $d$ other vectors in the space) and the $W_{ij}$ are not uniquely determined. This seems like a caveat for LLE. Specifically, if $k>d$ , can LLE be expected to fail to give reasonable results? What if $k>>d$ ? If not, then why is this intuition incorrect?
