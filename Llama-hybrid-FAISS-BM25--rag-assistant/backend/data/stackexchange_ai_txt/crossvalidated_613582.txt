[site]: crossvalidated
[post_id]: 613582
[parent_id]: 
[tags]: 
What is the correct pipeline to work with Link prediction using GNNs

Trying to understand GNNs better, I copied the code from this blog post from PyG documentation. I copied and pasted the code without modifications, which works as described in the post. It is a link prediction on the MovieLens dataset. After the training loop with a reasonable accuracy score, I want to use the trained Model to predict new links in an unseen dataset. To do it, I re-run the previous code with the extra following line: torch.save(model.state_dict(), "my_path.pth") My second step then is to create a new script to use the trained model to predict new links: The script is something like copying the first 40 lines of the previous script (to load movies and users again) and trying to predict the links: checkpoint = torch.load("my_path.pth", map_location=torch.device('cpu')) model = Model(hidden_channels=64) model = model.to(device) model.load_state_dict(checkpoint) batch_size = 32 # Set the batch size for the DataLoader shuffle = False # Do not shuffle the data during iteration num_workers = 0 # Set the number of worker threads for loading the data (0 for no parallelization) new_dataloader = LinkNeighborLoader( data, # Sample 30 neighbors for each node for 2 iterations num_neighbors=[20, 10], # Use a batch size of 128 for sampling training nodes batch_size=batch_size, edge_label_index=(("user", "rates", "movie"), (data["user", "rates", "movie"].edge_index)), ) model.eval() preds, probs, pred_labels = [], [], [] for _, batch in enumerate(new_dataloader): batch.to(device) pred = model(batch) preds.append(pred) prob = torch.sigmoid(pred) probs.append(prob) pred_labels.append((prob > 0.9).long()) print(len(pred_labels)) print(len(probs)) Instead of creating a new synthetic dataset, I tried using the data I used for training to check if the predictions would work properly. I purposely deleted some information about users and movies to have a different dataset size. However, it raises the error size mismatch for movie_emb.weight: copying a param with shape torch.Size([9742, 64]) from checkpoint, the shape in the current model is torch.Size([9216, 64]). It makes sense because I used the number of nodes of my graph in the model definition (e.g., line self.user_emb = torch.nn.Embedding(data["user"].num_nodes, hidden_channels) ), but now I'm wondering if it's mandatory always to use the same number of nodes for trained GNNs. Is this assumption correct? If so, is the standard pipeline for Link predictions always performing training with the knowledge links and then predicting the new ones?
