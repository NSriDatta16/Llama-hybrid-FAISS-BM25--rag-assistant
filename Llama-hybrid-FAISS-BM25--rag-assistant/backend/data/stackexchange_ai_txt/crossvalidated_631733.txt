[site]: crossvalidated
[post_id]: 631733
[parent_id]: 631685
[tags]: 
I think you are hitting undetermined behaviour here as XGBoost is not designed to have y be a pandas.DataFrame . I suspect that the sklearn API casts this into some numpy array but after than all bets are off. It appears it does something like one a classification task per column. Strictly speaking XGBoost does not support ordinal regression, we can roughly approximate it to some degree by using multi-class classification as surrogate. i.e. having mlogloss (multi-class log-loss) as our evaluation metric and our objective as multi:softprob so we predict the probability of each data-point belonging to each class. Then we can directly change the predicted probabilities to ordinal values. The change itself can be done using two main options, without or with thresholding: A. We directly pick the class/label with the highest predicted probability (something akin to np.argmax(predicted_probs, axis=1) in Python) B. We apply thresholds to our predicted probability ranges to map them to ordinal values. (something akin to : np.argmax(predicted_probs > thresholds, axis=1) in Python). The above being said, if we are dealing with multiple levels (e.g. 10+) and the spacing between is expected roughly equal, it might make sense to simply treat this as a regression problem directly. Similarly, if we are really aching about it, we can code the proportional odds loss (aka cumulative log-odds loss) as our objective manually and use that directly. Just remember we need the loss, the gradient and the Hessian.
