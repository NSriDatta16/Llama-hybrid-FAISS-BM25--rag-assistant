[site]: datascience
[post_id]: 116611
[parent_id]: 
[tags]: 
Not able to understand Transfer Learning with Vgg16

So, I have to work with Vgg16 in my semester group project, and was following this to do transfer learning. I don't understand CNN much, but am learning currently. The very first problem was that Vgg16 has 16 layers, whereas the base_model.summary() had 26 when initialised with VGGFace(include_top=True and 19 when VGGFace(include_top=False . Looks like the 16 layers are those with weight. Now, tutorial uses include_top=False and did x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(1024, activation='relu')(x) x = Dense(1024, activation='relu')(x) x = Dense(512, activation='relu')(x) preds = Dense(NO_CLASSES, activation='softmax')(x) model = Model(base_model.input, preds) As much as I understood, we first took output layer of base_model and it added 5 layers to that 1 GlobalAveragePooling2d, 4 Dense layers. My question is why did it modify the Vgg16 layer structure. Why do we need to repace last 7 layers with 5 different layers. Couldn't we set the same 7 layers as trainable or just add identical layers . What is the actual advantage of this replacement. Before replacement After replacement 'global_average_pooling2d_11', 'dense_42', 'dense_43', 'dense_44', 'dense_45'
