[site]: datascience
[post_id]: 105092
[parent_id]: 
[tags]: 
KL divergence vs cross entropy

I have a binary Image classification problem and use a deep learning model for classification problems. In general, we use the cross-entropy loss for this but I would like to use kl-divergence as a loss function. Can you use it or not? If not why can't I use it as I get probabilities at the output softmax layer of the model Any kind of reference is helpful Thanks in Advance
