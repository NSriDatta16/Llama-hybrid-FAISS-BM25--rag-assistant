[site]: datascience
[post_id]: 114245
[parent_id]: 
[tags]: 
What could cause pre-trained Opus-MT models have wildly varying inference time when being used with transformers library?

I have been testing pre-trained Opus-MT models ported to transformers library for python implementation. Specifically, I am using opus-mt-en-fr for English to French translation. And the tokenizer and translation model is loaded via MarianTokenizer and MarianMTModels--similar to code examples shown here on huggingface. Strangely, for the same pre-trained model translating the same English input on an identical machine, I have observed anywhere between 80+ ms and (whopping) 4 s per translation (example input = "kiwi strawberry"). Wonder if anyone has observed similar behaviours, and what could cause such a wide variation?
