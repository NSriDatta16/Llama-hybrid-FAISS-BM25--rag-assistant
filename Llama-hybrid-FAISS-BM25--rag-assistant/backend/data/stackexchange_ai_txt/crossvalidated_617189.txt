[site]: crossvalidated
[post_id]: 617189
[parent_id]: 
[tags]: 
What is the meaning of "SGD scales the gradient uniformly in all directions"?

I'm really newbie about neural network and optimization. When I read the references, I found this journal Wang et al 2018 . The journal stated: One disadvantage of SGD is that it scales the gradient uniformly in all directions; this can be particularly detrimental for ill-scaled problems. This also makes the process of tuning the learning rate Î± circumstantially laborious What does it mean by "it scales the gradient uniformly in all directions"?
