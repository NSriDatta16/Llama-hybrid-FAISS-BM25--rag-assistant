[site]: crossvalidated
[post_id]: 540718
[parent_id]: 540341
[tags]: 
I'd look at the residuals for determining the effectiveness of a model. Plots of residuals versus other quantities are used to find failures of assumptions. The most common plot, especially useful in simple regression, is the plot of residuals versus the fitted values. A null plot would indicate no failure of assumptions. Curvature might indicate the fitted mean function is incorrect. Residuals that seem to increase or decrease in average magnitude with the fitted values might indicate nonconstant residual variance. A few relatively large residuals may be indicative of outliers, cases for which the model is somehow inappropriate. Assumptions of a linear model are: Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y. Independence: The residuals are independent. In particular, there is no correlation between consecutive residuals. Homoscedasticity: The residuals have constant variance at every level of x. Normality: The residuals of the model are normally distributed. If one or more of these assumptions are violated, then the results of our linear regression may be unreliable or even misleading. Using the given data, I build a simple linear regression model given below. # required libraries library(caret) library(ggplot2) library(magrittr) # split the train dataset into train and test set set.seed(2021) index % predict(df_test) data.frame( R2 = R2(predictions, df_test $satisfied), RMSE = RMSE(predictions, df_test$ satisfied), MAE = MAE(predictions, df_test $satisfied)) R2 RMSE MAE 1 0.4513587 0.9956456 0.8224509 # Residuals = Observed - Predicted # compute residuals residualVals satisfied - predictions df.1 Discussion To understand the strength/weakness of a model, relying on a single metric is problematic. Visualization of model fit, particularly residual plots in the context of linear regression model, are critical to understanding whether the model is fit for purpose. When the outcome is a number, the most common method for characterizing a modelâ€™s predictive capabilities is to use the root mean squared error (RMSE). This metric is a function of the model residuals, which are the observed values minus the model predictions. The mean squared error (MSE) is calculated by squaring the residuals and summing them. The RMSE is then calculated by taking the square root of the MSE so that it is in the same units as the original data. The value is usually interpreted as either how far (on average) the residuals are from zero or as the average distance between the observed values and the model predictions. Another common metric is the coefficient of determination, commonly written as R2. This value can be interpreted as the proportion of the information in the data that is explained by the model. Thus, an R2 value of 0.45 in the above model, implies that the model can explain less than half of the variation in the outcome/dependent/response variable, satisfied variation. Simply put, the above model is not good. It should be noted that R2 is a measure of correlation and not accuracy.
