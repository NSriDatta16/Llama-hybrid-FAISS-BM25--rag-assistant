[site]: crossvalidated
[post_id]: 239333
[parent_id]: 
[tags]: 
Why do we care about unbiasedness?

The OLS estimator $\hat{\beta}$ of $\beta$ is unbiased. Now, $\hat{\beta}$ is a function of the random sample (y,$x_1$,...$x_j$) and is therefore a random variable itself. When we compute $\mathbb{E}(\hat{\beta}|X)$ we are basically fixing $X$ to be the data we have and we take an expectation over the $y$ variable. Unbiasedness tells us that if we compute all possible values of $\hat{\beta}$ by runnning and OLS regression for every possible $y$, then we would find that $\mathbb{E}(\hat{\beta}-\beta|X)=0$ by averaging over the $\hat{\beta}$'s, that is, the average over all those $\hat{\beta}$'s is the true population parameter (conditional on $X$). However, in practice we have only one $y$ which will allow to compute an OLS estimate $\hat{\beta}$ which differs almost surely from $\beta$ - since $\hat{\beta}$ is random, equality would be an event of pure luck. But that being the case, why do we care so much that the coefficient is unbiased? Is it because it allows for an interpretation of the coefficients as the marginal effect on the mean of the sampled random dependent variable (or the marginal effect of $X$ on $y$ when $y$ is drawn from a typical random sample in the sense that the sample mean equals the population mean) ? I have also read coefficients being interpreted as the average marginal effect of a variable on the dependent variable. However, I would read it not as the average/mean marginal effect on the dep. variable but rather as the marginal effect on the mean of the dep.variable conditional on $X$. Which interpretation is correct?
