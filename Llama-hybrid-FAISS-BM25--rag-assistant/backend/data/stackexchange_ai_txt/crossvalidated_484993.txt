[site]: crossvalidated
[post_id]: 484993
[parent_id]: 
[tags]: 
Observational study comparing 2 products with different (but overlapping) feature coverage

I have two software products $A$ and $B$ which form treatment $X$ . $B$ is a new version of $A$ and in development. So $B$ does not have all the features that $A$ has. However, $B$ is designed to make the user experience better, so $B$ should show better engagement in the subset of features of $A$ . My goal is to quantify how much better $B$ is compared to $A$ , regardless of the differences in features. I have framed this as mediation problem. $X \rightarrow Y$ represents direct effect of $X$ on $Y$ which represents improvement in user exp. $M$ represents features. Part of $Y_A$ is caused by its vast set of features that is lacking in $B$ . Ideally, $X \rightarrow M \rightarrow Y$ should capture this difference in $Y$ due to differences in features. Is this right formulation? In summary, I am trying to decompose total $Y_a-Y_b$ into 2: that caused by how much better $B$ is than $A$ and that caused by difference in features in $B$ vs $A$ . One intuition I'm lacking is this: can I simply filter the sessions to only include the common features, $m$ and estimate $y_a-y_b$ ? I guess its represented by this: $$P(Y|X=a, M=m) - P(Y|X=b, M=m)$$ I have read in many papers that conditioning on post-treatment variable can lead to bias. can someone add some intuition to this?
