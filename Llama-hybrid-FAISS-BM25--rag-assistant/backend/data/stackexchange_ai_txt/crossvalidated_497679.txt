[site]: crossvalidated
[post_id]: 497679
[parent_id]: 
[tags]: 
Why the Gaussian Distribution in ML/DL?

So whenever I learn about a method, or a technique to combat some disadvantage present in said method, some mention of transforming data into the Gaussian Distribution exists. My focus here is mainly on Deep Learning. For example, when it comes to C.N.Ns, standardizing our input data by subtracting the mean of your input examples by the std of the your input example, ( which has the effect of transforming the input feature distributions into a unit gaussian ) allows us to not only properly scale our input features but also make it resemble the Gaussian distributions ... (for reasons that are not apparent to me and which I intend to find out in the answers to this question) Another example is a technique called batch normalization introduced to combat the effect of saturated neurons by being able to place the input value to subsequent layers in an "active" regime of the activation function as to continuously stimulate backprop & learning. But the question remains.. why Unit Gaussian ? Is it because it describes most phenomena out there to the degree that we just simply assume that the each feature (i.e. random variable) in our network should follow the normal distribution because that's the way of life? Or again, is there some underlying theory behind this? EDIT1 : Subtracting sample mean and dividing by std does not transform an existing distribution into a Gaussian D. but only applies some scaling and shifting characteristics to the distribution without changing its inherent shape.
