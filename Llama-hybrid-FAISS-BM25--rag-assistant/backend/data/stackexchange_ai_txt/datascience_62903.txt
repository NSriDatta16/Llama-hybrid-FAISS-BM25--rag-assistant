[site]: datascience
[post_id]: 62903
[parent_id]: 61491
[tags]: 
First think of it like this. The most naive way you would encode words such that you can put it to neural network model is one hot encoding. If this is the case you will notice that your encoding vector size will grow linearly with your vocabulary size and on top of that it is sparse(without proper handling it is inefficient). Second, your encoding doesn't have any "meaning" and you are basically asking the network to do the heavy lifting on figuring out meanings behind which could work but not ideal. So how can we help our network? Humans have prior knowledge of how languages works and we try to instill them in the form of embeddings. Based on this we helped our neural network by giving a simpler task of "connecting the dots". Some other features that we want is we can have a measure semantic similarity. Which I believe is mentioned by Shubam. To wrap up we solved 2 problems. First, in some sense we are doing dimensionality reduction. Second, we are able to project words in our vocabulary in a more meaningful way. You can apply similar ideas for character, but probably make less sense.
