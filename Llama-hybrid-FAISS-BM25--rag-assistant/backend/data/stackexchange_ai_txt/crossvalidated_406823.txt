[site]: crossvalidated
[post_id]: 406823
[parent_id]: 
[tags]: 
Is it better to use activation (ReLu) layer after Global Average pooling layer?

I have seen most have used non-linearity -> GAP -> Dense. But in some have used following order; GAP -> non-linearity -> Dense. Which is better? Is it good to use activation layer after GAP? (Here, GAP means Global Average Pooling layer)
