[site]: crossvalidated
[post_id]: 212506
[parent_id]: 212505
[tags]: 
Use a gradient descent optimizer. This is a very good overview. Regarding the code, have a look at this tutorial . This and this are some examples. Personally, I suggest to use either ADAM or RMSprop. There are still some hyperparameters to set, but there are some "standard" ones that work 99% of the time. For ADAM you can look at its paper and for RMSprop at this slides . EDIT Ok, you already use a gradient optimizer. Then you can perform some hyperparameters optimization to select the best learning rate. Recently, an automated approach has been proposed . Also, there is a lot of promising work by Frank Hutter regarding automated hyperparameters tuning. More in general, have a look at the AutoML Challenge , where you can also find source code by the teams. In this challenge, the goal is to automate machine learning, including hyperparameters tuning. Finally, this paper by LeCun and this very recent tutorial by DeepMin (check Chapter 8) give some insights that might be useful for your question. Anyway, keep in mind that (especially for easy problems), it's normal that the learning rate doesn't affect much the learning when using a gradient descent optimizer. Usually, these optimizers are very reliable and work with different parameters.
