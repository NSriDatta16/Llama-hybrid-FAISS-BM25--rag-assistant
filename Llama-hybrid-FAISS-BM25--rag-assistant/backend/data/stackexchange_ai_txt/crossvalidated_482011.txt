[site]: crossvalidated
[post_id]: 482011
[parent_id]: 481998
[tags]: 
This is an interpretation issue: there are multiple ways to interpret the statement, and they given different results We know from the original question that taking one of each $p\in\{0.1,0.3,0.6,0.7,0.8\}$ gives $\mathrm{var}[Y]=0.91 We might also mean that $p$ is a random variable, and want to average over its distribution > r var(r) [1] 1.250052 So far, the claim isn't looking very good. In fact, de Finetti's theorem tells us that 2 has to give 1.25 as the answer: the distribution of exchangeable binary variables is iid Bernoulli conditional on the mean of $p$ . But we're not done yet. Suppose we took more than one observation with each $p$ The one-of-each approach by simulation > r var(r) [1] 9.049306 The random- $p$ approach, by simulation > r var(r) [1] 43.29736 In this case $\bar p=0.5$ and the constant- $p$ formula gives $50\bar p(1-\bar p)=12.5$ So, the one-of-each variance is smaller than $50\bar p(1-\bar p)=12.5$ and the random- $P$ variance is larger. That's the general phenomenon the reference was talking about. Varying $p$ gives you overdispersion, but only if you take more than one observation from each $p$ . There's no such thing as overdispersed exchangeable binary data. We can do something analytic, to finish off. Suppose $p$ is random with mean $p_0$ and variance $\tau^2$ , and the conditional distribution of $Y|p$ is Binomial(m,p). The conditional variance decomposition says $$\mathrm{var}[Y] = E[\mathrm{var}[Y|p]]+\mathrm{var}[E[Y|p]]$$ which comes to $$E[mp(1-p)]+\mathrm{var}[mp]=E[mp(1-p)]+m^2\mathrm{var}[p]$$ Now $$E[mp(1-p)]=E[mp]-E[mp^2] = mp_0-mp_0^2-m\tau^2$$ so $$E[mp(1-p)]+\mathrm{var}[mp]= mp_0-mp_0^2-m\tau^2+m^2\tau^2$$ If (and only if) $m=m^2$ this simplifies to $\mathrm{var}[Y]=mp_0(1-p_0)$ . For $m>1$ it is larger. On the other than, the variance of $Y$ conditional on $p$ is always smaller than $mp_0(1-p_0)$ , which fits with approach 1.
