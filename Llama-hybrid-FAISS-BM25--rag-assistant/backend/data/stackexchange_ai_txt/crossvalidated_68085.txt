[site]: crossvalidated
[post_id]: 68085
[parent_id]: 58765
[tags]: 
One helpful distinction that might add a little to the answers above: Andrew Ng gives a heuristic for what it means to be a non-parametric model in Lecture 1 from the course materials for Stanford's CS-229 course on machine learning. There Ng says (pp. 14-15): Locally weighted linear regression is the first example we’re seeing of a non-parametric algorithm. The (unweighted) linear regression algorithm that we saw earlier is known as a parametric learning algorithm, because it has a fixed, finite number of parameters (the $\theta_{i}$’s), which are fit to the data. Once we’ve fit the $\theta_{i}$’s and stored them away, we no longer need to keep the training data around to make future predictions. In contrast, to make predictions using locally weighted linear regression, we need to keep the entire training set around. The term “non-parametric” (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis $h$ grows linearly with the size of the training set. I think this is a useful contrasting way to think about it because it infuses the notion of complexity directly. Non-parametric models are not inherently less-complex, because they may require keeping much more of the training data around. It just means that you're not reducing your use of the training data by compressing it down into a finitely parameterized calculation. For efficiency or unbiasedness or a host of other properties, you may want to parameterize. But there may be performance gains if you can afford to forgo parameterizing and keep lots of the data around.
