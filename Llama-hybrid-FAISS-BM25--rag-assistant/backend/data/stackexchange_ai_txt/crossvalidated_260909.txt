[site]: crossvalidated
[post_id]: 260909
[parent_id]: 
[tags]: 
Abundance of inputs, scarcity of targets

I'm working with biomedical data and the available data sets comprise 500,000 cell measurements. Each data point refers to the measurement of an individual cell and the features (10 in number) pertain to different antibody counts. Given some of these measurements, a patient is diagnosed positive or negative for a particular disease. In other words the mapping of the data is 500000:1. Now, I have 10 example cases of positive and another 10 of negative diagnosis. In all, that gives me 20 target values and 10 million input examples. I was planning to train a recurrent neural network to make predictions on the target data after sequential processing of new examples. Obviously there's an abundance of input data, but are the actual samples (10 positive and 10 negative) enough to train the network? My intuition is that 10 million examples in total should be enough for the network to encounter enough patterns in the data to be able to discriminate between cases (although I'm making this assumption based on logic alone and no domain knowledge whatsoever), however the many-to-one mapping maybe obliterates that advantage (?) Does anyone have any further insights? EDIT: To clarify, the datasets consist of 500k 10 dimensional measurements which map to 1 label. The goal is for the network to correctly learn to map a new set of 500k measurements to the correct label.
