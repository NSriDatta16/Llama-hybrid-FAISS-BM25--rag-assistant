[site]: crossvalidated
[post_id]: 234122
[parent_id]: 
[tags]: 
Expected activations in a CNN hidden layers

I'm playing around with Caffe and I'm doing some experiments on different convolutional neural networks. I think that I built also a good theoretical knowledge on the CNN but I want to dig a bit deeper on how they behave internally. In particular I'm interested in the expected activation level of the neurons in the hidden layers of the network. I expect it to be related to the score function used, the input function, the non-linearities used and the number/size of the layers. I'm not entirely if this is the case and how to get this estimations, do you know if there are some papers analyzing it? For example I see that with AlexNet provided in Caffe, the activations in the lower levels are quite large (up 500). This is expected because there's no normalization of the input. The activations in the last layers are way lower. This is different for the lenet network that is analyzing the MNIST dataset where the input is normalized. It will be interesting to know if such values can be at least estimated only by using the input function and the score function used.
