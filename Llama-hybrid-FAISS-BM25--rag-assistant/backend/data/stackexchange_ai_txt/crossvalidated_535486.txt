[site]: crossvalidated
[post_id]: 535486
[parent_id]: 535315
[tags]: 
We have a very sensitive optics system¹, where our signal changes slowly over time. However, there is a background noise of 60hz caused by nearby AC current. We sample 25 points over a second, and recover data like: time signal 0.011 1546 0.044 1615 0.081 1772 0.115 1795 ... 0.802 1792 0.838 1498 0.873 1507 Plotted, it looks like: Note that the periodic behaviour above is aliasing from the 60hz signal. We could just take the sample average of all points to get a estimate of the signal, but this isn't efficient: consider what happens when the samples start and end both “high”, or both “low” - this biases the average up, or down, respectively. Instead, we directly model the system using the sinusoidal regression: $$f(t, A, B, C) = C + A \sin(120\pi t + B) $$ Like the links in the question, we expand this as: $$f(t, A', B', C) = C + A' \sin(120\pi t) + B'\cos(120\pi t) $$ which turns our problem into a linear regression problem. Solving using least squares and converting back to the original parameterization gives: C = 1676.40 B = 1.46 A = 200.74 Plotting the fitted model vs the observed: We use the value of C , 1676.4, as our final estimate. Note that this is different from the sample average of 1654.0. ¹ We are measuring low light signals that bounce of cells in our bioreactors .
