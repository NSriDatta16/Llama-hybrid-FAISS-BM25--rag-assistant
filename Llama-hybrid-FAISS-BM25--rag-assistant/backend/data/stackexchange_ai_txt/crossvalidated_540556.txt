[site]: crossvalidated
[post_id]: 540556
[parent_id]: 
[tags]: 
Sensible data set structure for fraud prediction

I am fairly new to machine learning and am currently working on the following problem. I am trying to compare several machine learning algorithms (ANN, SVM, random forests) regarding their efficacy to predict fraud based on the development of financial data over a 3-year timespan through a comparison with non-fraudulent companies. My dataset consists of rows which each contain a companies and several financial figures for one year, times three per company (2 years before fraud and year of fraud). During a quick test run I started doubting whether my data set is structured sensibly or even my approach makes sense. Despite my research I could not find out whether there is a better way to do it, so I decided to ask for your advice. For each fraudulent company in my dataset, I have each of the 3 years marked as "fraudulent". However, the fraud is ultimately happening in the last year. This leads me to 2 beliefs: The algorthms train on each row separately and do not consider the 3 years per company jointly. Each row marked as fraudulent is seen as a separate fraud instance as oppossed to every company. I've uploaded an excerpt from the training dataset here: https://github.com/GabdeFort/Predict-FFS-Machine-Learning I am using RapidMiner.
