[site]: crossvalidated
[post_id]: 245050
[parent_id]: 245049
[tags]: 
From the name we know Both Adaboost and Gradient Boosting are similar because they are both boosting models , where we add models one by one sequentially on each iteration and increase the model "variance" and reduce the "bias". My answers to this post may be helpful for you to see "boosting step by step". How does linear base leaner works in boosting? And how it works in xgboost library? The difference between this two is adaboost is using classification trees and re-weight on mis-classified examples. But gradient boosting is using regression trees to fit the log odds.
