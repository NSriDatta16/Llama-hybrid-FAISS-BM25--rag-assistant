[site]: crossvalidated
[post_id]: 430196
[parent_id]: 430192
[tags]: 
100% accuracy can clearly be achieved (also on the validation or unknown data) for some problem settings, but I guess those are rare cases. At the latest when the influence of the random noise "blurs" the borders of the data enough, the accuracy most likely will go down on unknown data, while it still may be up on the training data due to overfitting. An example of a case where 100% accuracy is possible are one of the first experiments with neural nets, where researchers built AND/OR/XOR gates using neural nets. Back in these days I think, they didn't train the nets for these operations, but you surely could do that. The result will be 100% accuracy. But this is a very special case. In fact, you probably would train the neural net with all the inputs it could ever see. Maybe you could even skip some of the inputs in your training set and it would still reconstruct the logical operation but that is quite limited. So in this case you don't really have a split between training and validation data because you train on the whole space of possible inputs. Of course such settings are not the typical settings for the application of machine learning. Why would you bother to train a ML model if you already labelled / can label the whole space of possible input by another method?
