[site]: crossvalidated
[post_id]: 636389
[parent_id]: 
[tags]: 
How do Incorrect Statistical Assumptions affect Estimation?

As a learning example, I am trying to see how adversely statistical analysis is impacted when the distribution of errors is incorrectly specified. Here are some specific situations I thought about: Continuous Random Variable: Suppose I observe $n$ points from a non-iid process such as a Random Walk, Brownian Motion or AR (Autoregressive Process). Imagine I naively and incorrectly assume that these $n$ points have indeed come from a i.i.d random variable (I want to estimate the mean and variance of this data). I think that a Random Walk/Brownian Motion can be transformed into iid Normal (Donsker Theorem), therefore if we knew the data was a Random Walk/Brownian Motion, we could estimate the parameters of the Random Walk/Brownian Motion after transformation.... however, if we incorrectly persisted in parameter estimation WITHOUT transforming the Random Walk/Brownian Motion - I wonder how bad the parameter estimates will be? Discrete Random Variable (basically a Markov Chain): Suppose I flip a coin $n$ times, where the probability of getting Heads on the current flip depends on the previous flip. If the previous flip was Heads, there is a $p_1$ probability of Heads in the next flip (and 1- $p_1$ probability of Tails in the next flip). If the previous flip was Tails, there is a $p_2$ probability of Tails in the next flip (and a 1 - $p_2$ probability of Heads in the next flip). Imagine I believe that this coin is i.i.d (I want to estimate the general probability of getting a heads and its variance) Discrete Random Variable (basically a Hidden Markov Model, EM/latent variable situation): Suppose now there are two coins. For Coin1, there is a $p_1$ probability of Heads if the previous flip was Heads and $p_2$ probability of Tails if the previous flip was Tails. For Coin2, there is a $p_3$ probability of Heads if the previous flip was Heads and $p_4$ probability of Tails if the previous flip was Tails. If Coin1 was chosen there is a $p_5$ probability I pick Coin1 again for the next flip (a 1- $p_5$ probability I pick Coin2) - and if Coin2 was chosen there is a $p_6$ probability I pick Coin2 again (and a 1 - $p_6$ probability I pick Coin1). I play this game $n$ times and have a sequence of length $n$ comprised of heads and tails, but I don't know which coins generated each individual flip. Imagine I believe that there is only a single coin and this coin is also i.i.d. (Since I only believe there is 1 coin, I want to estimate the general probability of getting a heads and its variance) In these types of situations (assuming I stubbornly persist with my incorrect beliefs about these problems), I am interested in knowing: How costly will my incorrect knowledge of the situation effect statistical inferences made on the parameter estimates? (ex: $p_1$ vs $\hat{p_1}$ , Var( $p_1$ ) vs Var( $\hat{p_1}$ ) .... compare incorrect estimates to actual parameter value from the correct data generating process) What parts of the statistical inference will be incorrect? For example, perhaps the mean estimates might be almost correct but the variance estimates will be significantly incorrect ,perhaps the parameter estimates will no longer have asymptotic normal distributions, parameter estimates will require more samples to converge to true values or simply will not converge to their true value in probability (consistency), etc. Which statistical theorems will no longer apply in these situations? For example, maybe Central Limit Theorem or Law of Large Numbers will be less relevant in these situations, etc. Currently, I am trying to design some statistical simulations (using R Studio) that simulate these kinds of situations and then I can see on a case-by-case basis how costly my incorrect knowledge of the situation will prove to be ... how badly my inferences will be affected. But I was wondering if there some mathematical/statistical way to analyze these situations in abstraction? PS: (I spent the whole day making notes on how incorrectly persisting with standard OLS in the presence of correlated errors/non-constant variances affect parameter estimation ... I can post these notes if someone is interested in seeing them)
