[site]: crossvalidated
[post_id]: 25616
[parent_id]: 25567
[tags]: 
Gradient descent is usually a bad optimization algorithm. Try one of the following: stochastic gradient descent Levenberg-Marquardt conjugate gradient These algorithms are usually faster and achieve better results E. g. the "two spirals" problem can be solved by gradient descent in >10,000 iterations and by Levenberg-Marquardt in I think there is an advice about the training set size in the neural network FAQ but I can't find it at the moment. Usually it should be number of inputs * n, where n sufficiently large (OK, this probably won't help you).
