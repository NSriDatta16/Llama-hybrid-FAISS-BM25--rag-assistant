[site]: crossvalidated
[post_id]: 411593
[parent_id]: 411591
[tags]: 
As you point out an accuracy of 0.5 (on average) is the lowest you could get by random labeling. A very low F1 score and an accuracy of 0.5 are therefore not logically inconsistent. I would recommend rephrasing the question title. Your question is "why does the classifier predict everything as negative?". If the class imbalance does not exist (no error in your pre-processing), that is indeed surprising. At first glance, your confusion matrix is indicative of a highly imbalanced data set whereby the classifier labels virtually all observations as negative. You mention you manually down or up sample to ensure equal class distribution beforehand. My guess is that there is something wrong with this pre-processing. For instance do you apply this sampling equally to train/validation and test sets? (Mind you, In real-life you do not have knowledge or control of the test set). Why not try with a simple classifier first and what output you get. There should be no reason a classifier is biased towards a particular label.
