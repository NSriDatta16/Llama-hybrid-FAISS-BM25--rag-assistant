[site]: crossvalidated
[post_id]: 420185
[parent_id]: 393445
[tags]: 
I'm "answering" this question because I stumbled upon it while looking for the same question. The first question: why is $$\Delta = \frac{l}{2(l-1)}$$ (your $l$ , usually $p$ )? The idea is that $\Delta$ is the actual jump step when your inputs are in $[0, 1]$ . However, it is easier to think of index-jumping in the array of possible inputs, where the jump is $l/2$ , which is identical to the above definition of $\Delta$ . When you jump half the array forward or backward (and the array has an even number of steps $l$ ) there are always exactly two pairs of indices that can be jumped to or from. So in your example, if you number the indices starting with 0: $\{0, 1, 2, \dots , 19\}$ . From 0 you always jump to 10, from 1 to 11, ... and the same is true for jumps back (from 10 to 0, from 11 to 1, ...). This ensures that (given a randomly chosen start point) each element of the input array is chosen with the same probability, therefore giving you uniform coverage of the input space. There is a nice graphical explanation of this in Saltelli et al. (2008, p. 117ff). I cannot answer question 2. Saltelli et al (2008, p. 120) argue no , because in a model of $y = x_1 + x_2$ where $x_1 \in [0, 1]$ and $x_2 \in [0, 10]$ the second factor seems more important. However, if $\Delta$ is normalized (different for each $x_i$ ), both factors would be equally important. This points towards not scaling and seems sensible to me. Monari and Strachan (2017) argue for scaling: The sensitivity indexes employed in the Morris method can be dependent on the variation ranges applied. In partic ular, leaving the variations applied to each parameter with its own units can produce misleading results since the calculated elementary affects will have in turn different units, making it impossible to compare them. To avoid this issue model input samples have been scaled and centred so that each of them has mean equal to zero and standard deviation equal to one. So do Sin and Gernaey (2009): This refinement consists of scaling the elementary effects to obtain a non-dimensional measure. This is needed for two particular reasons: (1) to prevent any misleading conclusions about the importance of the factors in complicated models and (2) to reliably compare the results of the analysis on different model outputs. This necessity arises especially when a model contains outputs and factors with large differences in their values/units, for example one or more orders of magnitude. This also sounds logical to me. So which is better, scaling or not scaling? Note that scaling the input factors $q_i$ or not makes no difference, since they only appear as arguments to the output function $f$ and therefore have no influence on the actual elementary effects. No scaling means $\Delta = \frac{l}{2(l-1)}$ , scaling means having $\Delta_i$ and using them depending on which $q_i$ is currently changed. I have no idea. Scaling $f$ (like Sin and Gernaey 2009) is an additional issue and in my opinion can only increase comparability if one is looking at multiple outputs $f$ . Saltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola (2008); Global sensitivity analysis: The primer, John Wiley & Sons, doi:10.1002/9780470725184 Monari, F. and Strachan, P. (2017); Characterization of an airflow network model by sensitivity analysis: parameter screening, fixing, prioritizing and mapping; Journal of Building Performance Simulation 10, p. 17 - 36, doi: 10.1080/19401493.2015.1110621 Sin, GÃ¼rkan and Krist V. Gernaey (2009); Improving the Morris method for sensitivity analysis by scaling the elementary effects, Computer Aided Chemical Engineering 26, p. 925 - 930, doi: 10.1016/S1570-7946(09)70154-3
