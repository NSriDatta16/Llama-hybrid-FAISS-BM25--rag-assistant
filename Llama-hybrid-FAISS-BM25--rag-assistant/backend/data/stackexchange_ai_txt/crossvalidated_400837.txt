[site]: crossvalidated
[post_id]: 400837
[parent_id]: 
[tags]: 
How could I go about implementing k-fold cross validation giving my circumstance?

The way I understand k-fold cross validation is that a given dataset or a training subset of the dataset is divided into k equal sets called folds. Then the training should be performed iteratively through each fold, where the remainder of the folds are combined forming a union on which training is performed. Lastly, the performance metric is then averaged across by k. In my case however, I have static folder structure setup as such: positive-negative | +--test: | +--negative:77 | +--positive:77 +--train: | +--negative:154 | +--positive:154 +--valid: | +--negative:26 | +--positive:26 And I simply use batches for test, train and valid directories to retrieve the samples using ImageDataGenerator when fitting the model. Since my GPU is fairly old with only 3GB VRAM, I only load in batch sizes of 2 for each of the directories. Given this scenario, how viable is K-fold cross validation in my scenario, and what are the approaches? Additionally, across all my images I have a high imbalance of labelling where there is clearly a lot more labels of one class over the other: immunonegative: 4,659 immunopositive: 1,117 To offset this, I essentially only selected as many negative samples as there is positive samples, but I can't help but feel that a lot of my data is going to waste, and I'm wonderin if there could be strategies I could employ to prevent data wastage?
