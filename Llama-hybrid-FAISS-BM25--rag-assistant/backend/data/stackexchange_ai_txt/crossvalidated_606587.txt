[site]: crossvalidated
[post_id]: 606587
[parent_id]: 605657
[tags]: 
MCMC is a computational technique for implementing Bayesian inference. Normally, the difference between MLE and Bayesian inference is that Bayesian inference incorporates prior information. In this case, however, you appear to have omitted steps from MLE and Bayes so that both they collapse down to the same thing. First, you have not actually computed Fisher information but rather observed information for the MLE estimate. Computing the Hessian of the log-likelihood at the MLE gives observed information. Fisher information requires the expected value of the Hessian over the distribution of $y$ . Secondly, and more importantly, you don't mention any prior information for the Bayes MCMC approach. By ignoring the prior, you are effectively assuming a uniform improper prior for $\theta$ , in which case Bayes and MLE become virtually the same thing. The agreement between the two approaches becomes more complete because you are using observed rather than Fisher information for the MLE, and observed information is what is used in Bayesian inference. So the two covariance matrices you estimate at the end are presumably not just multiples of one another but exactly the same, modulo MCMC convergence.
