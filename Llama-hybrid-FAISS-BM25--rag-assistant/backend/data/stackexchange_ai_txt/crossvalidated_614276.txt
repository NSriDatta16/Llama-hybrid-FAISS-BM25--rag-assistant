[site]: crossvalidated
[post_id]: 614276
[parent_id]: 614268
[tags]: 
Random forest is one way (things like gradient boosted decision trees - e.g. XGBoost/LightGBM are other ways - which tend to have often have slightly better prediction performance) of building trees that in combination ("as an ensemble") can be much more complex than a single tree without overfitting. By "without overfitting" I don't mean that they cannot overfit, but rather than it turns out to be easier to regularize the process of building an ensemble of trees (by tuning various hyperparameters of the process) so that they don't overfit (as assessed by e.g. cross-validation). Whether you want these more complex models is mostly a function of what's your goal. Often, you will need the more complex ones, if you primarily want to optimize performance and interpretation is not the main goal (if interpretation is very important various interpretation tools exist, but also there's research into reasonably performant ways of building much simpler models such as CORELS ). I.e. whether you always want to do random forest vs. just a decision tree really depends on what you want to achieve and the context. Regarding training and test (and validation set), you usually want (at least) three things: to train your model based on data: for this you need a training set to choose hyperparameters of your model: you usually need to do this on data that's not your training set (otherwise you tend to overfit), one solution is a separate "validation set" (or cross-validation to be more efficient) - once you've chosen your hyperparameters, you can re-train on the combined training + validation set with the chosen hyperparameters to know how well your model is working: if you use the data you trained on or on which you choose the hyperparameters (aka the validation set), then you are potentially misleading yourself (and others, often very, very badly - I've seen flawed evaluations on the training data that suggested near perfect performance when the model was just producing useless garbage), so if you want to know how well your model works you need new data to evaluate it on For the last two points, the considerations outlined here apply. I.e. you don't just need any data or to thoughtlessly randomly split your data, you really need to think about this. If you don't have factors in the data, this does not change too much except that it makes your life easier. Purely numeric data are easier to deal with for many algorithms and a lot of effort (e.g. target encoding, creating embeddings etc.) goes into how to represent categorical features for models that don't deal with them so well.
