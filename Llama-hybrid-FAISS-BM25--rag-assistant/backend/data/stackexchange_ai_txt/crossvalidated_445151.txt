[site]: crossvalidated
[post_id]: 445151
[parent_id]: 445145
[tags]: 
In this general setting, what would be the best approach to calculate corr with null values? Input or discard? I don't know the size of these datasets, but they are generally "big". I'm assuming that you have only 2 variables and you want to compute the correlation between them. Imputing the missing values, if you have no knowledge of why they are missing, and/or you do not have other variables that can predict what the missing values could be (ie. multiple imputation), or if the data are a time series (so that an interpolated value would make sense), then the other option is a single value imputation such as the mean or median. This is a bad idea as it can dramatically change the distribution. It would be better to discard the corresponding observation from the other variable and delete the row (ie. only use complete cases). If the data are missing completely at random (MCAR) then this will lead to unbiased results. However If they are missing at random (MAR) or missing not at random (MNAR) then the results will be biased. Presumably you will not know which missingness mechanism is applicable so you will not know whether the results are biased or not. But all you can do is work with the data you have What is the maximum proportion of null data so that the correlation still is an important information value? Thanks! There is no maximum, but the more missingness you have, the less reliable will be the results.
