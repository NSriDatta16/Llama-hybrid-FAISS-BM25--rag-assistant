[site]: crossvalidated
[post_id]: 130431
[parent_id]: 130400
[tags]: 
I'm not a deep learning expert by any means, but my guess is that the PCA serves two functions: computational improvements if the input dimensionality is significantly reduced, and a kind of preconditioning for the optimization problem. Although a normal autoencoder setup certainly can learn the linear relationships, it may make the learning process easier if that step is useful and initialized with it. Broadly speaking, it should be about equivalent to pretraining the first layer of the autoencoder with the principal components (if you don't drop much in the PCA). Preprocessing with an autoencoder is often used to derive features to use in some other classifier or whatever. Compared to pretraining, plugging preprocessor autoencoder features into a neural net means that the final classifier can't adapt the learned features for its particular learning problem. Depending on your problem, this might hurt the performance of the final classifier somewhat. But it means that you can reuse the same learned features for multiple final classification/regression/whatever tasks, which saves a lot of training time in trying to adapt the features, and might save a substantial amount of test time if you're running a suite of learning methods on data and so can reuse the autoencoder outputs for all of them. In semi-supervised settings it might also give you better results to avoid overfitting the feature extractor to your limited labeled data.
