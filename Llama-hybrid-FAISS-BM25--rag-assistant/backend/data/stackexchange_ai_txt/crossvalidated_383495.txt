[site]: crossvalidated
[post_id]: 383495
[parent_id]: 383489
[tags]: 
As a general rule, the more parameters model have (any model), the more patterns it can learn, but also, the more likely it is to overfit. So if you ask about training set accuracy, the more would be always better, however you would need to learn it proportionally longer. With test set accuracy, adding more neurons (making the network "wider") or layers (making it "deeper") would be improving accuracy, but at some point the test set accuracy would start dropping, as the model would start overfitting to training set. The relation would however be less straightforward with the modern, complicated architectures (pooling layers, skip connections, batch normalizations, etc.) and heavy usage of regularization that is common in deep learning, so this isn't exactly linear.
