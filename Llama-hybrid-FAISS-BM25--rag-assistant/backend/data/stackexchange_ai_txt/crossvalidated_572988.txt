[site]: crossvalidated
[post_id]: 572988
[parent_id]: 572985
[tags]: 
I see that you've edited to state that you're tuning n_estimators and max_depth . For random forest, the most important parameter to tune is max_features (in python's sklearn) or mtry (in R's randomForest ), the number of variables to consider for finding the best split. Moreover, you don't have to tune the number of trees in random forest; see: Do we have to tune the number of trees in a random forest? For xgboost, the most important tuning parameter is the learning rate. Finally, it's not clear how you're choosing hyperparameters. You should choose the best hyperparameters according to a holdout set, not the data used to train the model. See: How bad is hyperparameter tuning outside cross-validation? Step-by-step explanation of K-fold cross-validation with grid search to optimise hyperparameters What is the difference between test set and validation set? Is it possible to get at least 60% accuracy after tuning hyper-parameters (e.g. by method like GridSearchCV)? The only way to know this is to do an experiment: tune the hyper-parameters and see. Some problems are harder than others. For instance, the features you have may not be very predictive of the outcome, so achieving the desired accuracy may not be possible.
