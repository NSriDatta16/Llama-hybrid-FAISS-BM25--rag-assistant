[site]: datascience
[post_id]: 126342
[parent_id]: 
[tags]: 
How does ReLU function make it possible to let the CNN learn more complex features in input data?

In many descriptions of a CNN i often read that at the end of the Convolutional layer, a ReLU function is needed, for two reasons: first it solves many problems about the vanishing gradient problem, second it enables the network to learn more complex feature in the data. What i cannot figure out is, how is it possible to understand such an improvement by just looking at the form of the ReLU function. I removes all negative values and this translates in getting rid of things such as smooth transitions of grey in an image. But starting from this i do not see how this should lead to the capability of detecting more complex features in an input image.
