[site]: crossvalidated
[post_id]: 219886
[parent_id]: 218193
[tags]: 
There are many factors that can affect the result, beyond the fact that the network is an autoencoder. If you're happy with the way the Matlab network performs, it should be possible to replicate in python by making sure all of these factors are the same. Here are some: Network architecture (how many layers, units per layer, connectivity pattern) Activation function for each layer Loss function Update rule and all associated hyperparameters (e.g. if using stochastic gradient descent, then learning rate should be the same, as well as any procedure for changing the learning rate over time; if using a batch optimization algorithm like conjugate gradient or BFGS, then parameters of the optimization solver should match). Procedure for determining when to stop training (e.g. number of training epochs, early stopping or other convergence criteria). Minibatch size Procedure for presenting training examples (e.g. if any shuffling procedure is used, or selective repetition of particular examples) Weight initialization procedure Data preprocessing (e.g. centering/scaling/whitening method) Regularization method and all associated hyperparameters (e.g. $l_1$ / $l_2$ penalties, fraction of dropped units if using dropout, noise method and noise parameters if using denoising autoencoders, etc.) Procedure for selecting hyperparameters (e.g. random vs. grid search, scaling/bounds of grid search, size of training vs. validation set, number of cross validation folds) Some other things to consider: Matlab and/or Keras might make some of these choices by default or under the hood, so you'd have to check carefully to make sure they're the same. If using stochastic regularization methods like dropout or denoising autoencoders, the noise/dropout should only occur during training, and be turned off when running the network When checking that the Matlab/python networks are equivalent, use the same dataset The loss function is generally not convex (and the training procedure may get trapped at saddle points). So, it's likely that the final network will differ across training runs (and might not converge to a good solution on some runs). So, identical results can't be expected. But, error on the validation set and qualitative aspects like presence/absence of the ball should be similar. It might be a good idea to compare a number of training runs in both Matlab and python to account for run-to-run variability. If you've already trained the Matlab network and just need to use it, another possible approach would be to 'port' the trained network to keras/python. This would involve constructing the exact same network (including all equations and parameters, but the factors involving initialization/training would be taken out of the picture). If everything is duplicated, it should produce exactly the same results. It could also make sense to do this as a test, even if you want to perform further training with the python network. The reason is that it isolates 'runtime' factors from training factors. If the python network produces different results despite having duplicated the network architecture and parameters, then something on this level is still unaccounted for. If results are the same, then you can move on to duplicating and testing initialization/training factors. Another option could be continue to run your network in Matlab but port the rest of the code to python. To call Matlab from python, use the Matlab engine API for python or python-matlab-bridge . To call python from Matlab, use Matlab's Python API . Here's something else to consider, along different lines. If your camera is fixed and the ball is the only thing that moves, then there's no reason to try to represent the constant background image. You could take a static image of the background (possibly averaging several frames to reduce noise), then subtract it from every frame of the video. The only thing that should remain is the ball, plus a small amount of noise (which could be thresholded out). Use these preprocessed images as input to the autoencoder. A better method might be to use a weighted loss function for each frame. Each pixel would have a weight that decreases its contribution to the error if it's similar to the corresponding pixel in the background image. The hope with either of these methods is that they might focus the autoencoder's efforts on the ball and not the background. Of course, there might not even be a need for a neural network in this case. Given such preprocessed images, you could use a simple centroid finding algorithm to obtain the ball coordinates and use these as input to your reinforcement learning algorithm.
