[site]: crossvalidated
[post_id]: 438851
[parent_id]: 
[tags]: 
Time spans in coherence computation

Currently I'm writing my master thesis and I'm having troubles with coherence definition. My question is similar to Clear steps to calculate coherence between two time series Mathematically it's defined as $$C_{xy}(\omega) = \displaystyle{\frac{|S_{xy}(\omega)|^2}{S_{xx}(\omega) S_{yy}(\omega)}} $$ where $S_{xy}$ is the cross-spectrum between $X$ and $Y$ and $S_{xx} \text{ and } S_{yy}$ the corresponding power spectra. However, when doing the math using the periodogram estimation $$\hat{S}_{xy}(\omega) = \frac{1}{N} x(\omega) y^{*}(\omega)$$ (being $x(\omega)$ the DFT of $X$ and $y^{*}(\omega)$ the conjugated DFT of $Y$ ) everything cancels out and $C_{xy}$ turn to be $1$ for all $\omega$ . This is where my question arises: I'm using seewave::coh function from R to compute coherence, which calls spec.pgram function in this part of the source code: Y I get that spans argument allows to avoid the "always 1 problem", but I don't get the "how". I would really appreciate if some of you could explain $\text{how}$ this works in the definition given above. Thank you so much.
