[site]: datascience
[post_id]: 36431
[parent_id]: 
[tags]: 
Fast and/or memory efficient classification solutions

I'm working on a classification problem with a very large dataset (a little under 1 billion obs) and around 25 predictors. I'm doing this analysis in R on a VM with 128GB of memory, but am still hitting memory issues when training certain types of models, not to mention having to wait for quite a while for runs to complete. I'm primarily using logistic regression and random forests, and keeping my training dataset to 10% of the overall sample. What are some solutions (packages, platforms, techniques) I could use to address these memory and/or speed issues? I'm fairly comfortable with Python, so solutions do not have to be R-specific.
