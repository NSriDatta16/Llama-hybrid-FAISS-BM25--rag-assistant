[site]: datascience
[post_id]: 97880
[parent_id]: 97869
[tags]: 
As you well mentioned, tree-based models are not sensitive to feature scaling, but on the contrary it might help with the convergency of finding the minimum in the optimization on boosted models I replicated your code and I found pretty much the same metrics in both scaling and no scaling versions of the model. from sklearn.datasets import load_boston from sklearn.tree import DecisionTreeRegressor from sklearn.pipeline import Pipeline from sklearn.compose import make_column_transformer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_absolute_error boston= load_boston() X, y = pd.DataFrame(boston.data, columns=boston.feature_names), pd.DataFrame(boston.target) train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 69) scale_list = ['CRIM', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT'] preprocessor = make_column_transformer((StandardScaler(),scale_list), remainder="passthrough") smodel = Pipeline([("scaler",preprocessor), ("model", DecisionTreeRegressor(random_state=42))]).fit(train_x, train_y) tmodel = Pipeline([("model", DecisionTreeRegressor(random_state=42))]).fit(train_x, train_y) print(f"metric with scale features: {mean_absolute_error(test_y, smodel.predict(test_x))}\nmetric with no scale features:{mean_absolute_error(test_y, tmodel.predict(test_x))}") metric with scale features: 2.6941176470588237 metric with no scale features:2.662745098039216 I dare to think that something is wrong with the way you are applying the scaling. I recommend you to check it or use pipelines. Also I was unsure if you used Ensemble or a simple DT, but in both cases remember to set the random_state in order to have reproducible results. Hope it helps!
