[site]: crossvalidated
[post_id]: 531247
[parent_id]: 313857
[tags]: 
The idea is that a kernel function is essentially a similarity measure and that two points that are similar to eachother will be close to eachother in feature space. For example, condider three points $x_{i}$ , $x_{j}$ and $x_{k}$ . For the RBF kernel $k(x,x^\prime)$ = $\exp (-\frac{\left \| x-x^\prime \right \|^2}{2\sigma})$ , let $k(x_{i},x_{j})=0.9$ and $k(x_{i},x_{k})=0.1$ . You can interpret this as $x_{i}$ and $x_{j}$ being more similar than $x_{i}$ and $x_{k}$ and therefore $\phi(x_{i})$ and $\phi(x_{j})$ will be closer to eachother in feature space than $\phi(x_{i})$ and $\phi(x_{k})$ . Additionally, note that a kernel function is just an inner product in some feature space, hence $k(x_{i},x_{j}) = \langle\phi(x_{i}),\phi(x_{j})\rangle$ . Hence, we measure the similarity of two points in feature space by calculating their inner product. Note that in feature space the inner product of every point with the origin is 0, $\langle\phi(x),0\rangle=0$ . Hence there is low similarity between the points in our training set and the origin. Therefore, the one class svm is trying to separate similar points from disimilar points, where the origin represents disimilar points. Therefore, the one class SVM (as defined by Scholkopf) tries to find a hyperplane in feature space that maximizes the distances from the origin (disimilar points) and minimizes the distance to the training data (similar points). When transforming the hyperplane back into the input space this will be a non-linear decision boundary, which essentially acts as a demarcation of a percentile of the probability distribution. For example the decision boundary could contain 95% of your data. Anything outside of this boundary would then be seen as unlikely to have come from the probability distribution generating the training data and therefore be classified as an outlier. Additionally, note that the SVDD, which finds the smallest enclosing hypersphere that encloses all datapoints and may make more intuitive sense, is equivalent to the one Class SVM as defined by Scholkopf for translation invariant kernels. In other words, if you use the RBF kernel, the idea of separating the training points from the origin in feature space is equivalent to the idea of finding the smallest enclosing hypersphere.
