[site]: datascience
[post_id]: 112574
[parent_id]: 112556
[tags]: 
The main difference between RNN-based text generation and BERT is the attention mechanism based on transformers. This attention mechanism is very important to add context between words and explains why the results are better than RNN in many applications. However, in terms of text generation, GPT-2 is more adapted than BERT because it uses a masked self-attention mechanism. The mask self-attention model is trained by guessing the next word or token, contrary to BERT which is trained on whole phrases. That allows the definition of the next sequence of words in terms of probability, thanks to the previous ones. Here is an article that explains how GTP2 works: https://jalammar.github.io/illustrated-gpt2/ The model is available here: https://huggingface.co/gpt2
