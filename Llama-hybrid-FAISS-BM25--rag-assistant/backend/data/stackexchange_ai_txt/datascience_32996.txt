[site]: datascience
[post_id]: 32996
[parent_id]: 32980
[tags]: 
Count of epochs is also a hyper-parameter. However, if you meant to ask what to choose to work upon, whether increasing epochs or some other methods like feature engineering , then below is my answer. Increasing number of epochs is often attributed as hammer stroke to train the model where you yourself often don't have to think much about the data and deepNet does it for you generally. However, it comes at a cost of computational complexity, risk of overfitting, etc. Instead, it is always advisable to work as much on tasks like feature engineering, tuning different other hyper-parameters while training. In fact, a data scientist is supposed to work on these things preferably and not just blindingly utilising the black-box concept of deepNet. However, even if you choose to increase the count of epochs, you may utilise the technique called early stopping . It basically says to stop the training at a certain epoch if the validation loss does not improve which you can use without much thinking about the impacts of increasing the count of epochs because if the validation loss does not improve, it will stop there.
