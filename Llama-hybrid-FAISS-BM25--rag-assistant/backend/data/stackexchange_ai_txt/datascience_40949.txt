[site]: datascience
[post_id]: 40949
[parent_id]: 40944
[tags]: 
This is a common situation and can be caused by a number of things, such as: Too much variance in the model (e.g. did the hyperparameter tuning identify low values for eta, alpha, lambda, gamma, subsampling/column sampling/row sampling, minimum child weights and/or high values for depth and number of leaves?) Having a small dataset with high variance. XGBoost is prone to overfitting and this is exacerbated when there isn't much data for it. Not shuffling the data so that the validation set has lower variance than (or differs in distribution from) the training and test sets. train_test_split shuffles by default, so this shouldn't be a problem if you've used it as indicated in the question for generating the validation set also. Stopping training early is a perfectly valid and common method of increasing bias in a model and doing so based on a validation set is acceptable. It is preferable for the training to stop based on the test set. To achieve this you could address the potential issues above. Without knowing more about the data and how the validation set was generated, the most reliable advice I can give is to alter the hyperparameters to increase model bias.
