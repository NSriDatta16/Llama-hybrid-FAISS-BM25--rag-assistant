[site]: datascience
[post_id]: 124108
[parent_id]: 123931
[tags]: 
For tabular data simple boosted ml models are still a good choice, when compared to state of the art Fusion Transformers (2023), see https://arxiv.org/abs/2106.11959v3 Therefore, lalbel propagation like in https://scikit-learn.org/stable/modules/semi_supervised.html still is a method worth trying. If you deal with big clean data of a certain modality, then you might go with pretraining (like suggested by Noe with the masked language loss etc...) with unlabelled data and then adding a task head for whatever task you want so solve. Training the task head will still require you to have enough data.
