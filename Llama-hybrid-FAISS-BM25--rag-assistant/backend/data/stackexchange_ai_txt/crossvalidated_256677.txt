[site]: crossvalidated
[post_id]: 256677
[parent_id]: 
[tags]: 
How do output-target values align when training a recurrent neural network?

I'm trying to wrap my head around some (crucial) details on how recurrent neural networks work and currently I'm having trouble understanding how the inputs align with the outputs when optimizing an RNN. Suppose a dataset of observations $D = \{x^{(i)}, y^{(i)}\},$ with $i = 1,..., N$ Vectors $x$ and $y$ are aligned, in the sense that feature values "correspond" to the class values. If we treat the $x$'s as elements of a sequence and define a number of time steps for the network to include in its computation of the output then how do the network's outputs align with the ground truth? E.g. if we include the last 4 elements in a sequence for the network's prediction how does that prediction align with the ground truth for the optimization? Surely we cannot compare $\hat{y}^{(1)}$ with $y^{(1)}$, but rather $\hat{y}^{(1)}$ with $y^{(4)}$, correct? ($\hat{y}$ being the network's output/prediction)
