[site]: datascience
[post_id]: 121523
[parent_id]: 121514
[tags]: 
$\mathcal{H}(x)$ is the function you want to approximate with your Neural Network. Typically, you train a model $\mathcal{H}_\theta(x)$ that directly tries to learn that function by minimizing some loss $\mathcal{L}(h(x), \mathcal{H_\theta}(x)), h(x) \sim \mathcal{H(x)}$ , but in this paper the authors show that it is easier to optimize the residual $\mathcal{F}(x) := \mathcal{H}(x) - x$ , i.e every layer learns only a small part of $\mathcal{H}(x)$ , so that the composition of all layers provide a better approximation.
