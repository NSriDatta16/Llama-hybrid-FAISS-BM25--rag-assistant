[site]: crossvalidated
[post_id]: 489258
[parent_id]: 
[tags]: 
Controlling for confounding variables with multiple regression - isn't correlation a problem?

From the Wikipedia definition - "a confounder (also confounding variable, confounding factor, or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association." So to my understanding, a confounder would usually be correlated with the main independent variable, as it influences it. One way to control for a confounder would be to add it to the multiple regression model. But in the context of machine learning it is said that having correlated features in the model should be avoided. In particular, it was answered in the following question: https://datascience.stackexchange.com/questions/36404/when-to-remove-correlated-variables "But if are concerned about interpretability then it might make sense to remove one of the variables, even if the correlation is mild. This is particularly true for linear models. One of the assumptions of the linear regression is lack of perfect multicollinearity in the predictors. If A is correlated with B, then you cannot interpret the coefficients of neither A nor B. To see why, imagine the extreme case when A=B (perfect correlation). Then, the model y=100 A+50 B is the same as the model y=5 A+10 B or y=-2000 A+4000 B. There are multiple equilibra in the possible solutions to the least square minimzation problem therefore you cannot "trust" neither." So to my understanding, if the confounder we add to the multiple regression model is correlated(which to my understanding is usually the case) with the independent variable, we will not be able to interpret the coefficients appropriately, so how could we actually understand the relationship between the main independent variable and the dependent variable?
