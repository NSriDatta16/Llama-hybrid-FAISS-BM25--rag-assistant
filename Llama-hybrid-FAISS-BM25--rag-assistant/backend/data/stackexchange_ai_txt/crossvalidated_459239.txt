[site]: crossvalidated
[post_id]: 459239
[parent_id]: 
[tags]: 
Custom metrics for multiclass classification when class errors have different weights

I have a multiclass classification problem (eg. the target variable is made by 4 different outcomes: Product A, Product B, Product C and NO Product). Not all the errors are equal: for example, if the true label is "Product A" and the prediction is "NO Product" it is not a big problem, while if the true label is "Product C" the impact of the error is much bigger. Basically, I have to insert this information into the loss function of the algorithm (I am currently using Xg-Boost, Random Forest, etc). I know that it's possible, for example using scikit-learn to train algorithms using the parameter class_weight in a way that a specific class error is more important. Is there the possibility to say, for example, that a miss-classification of product C in Product B is more problematic w.r.t a miss-classification of Product C to Product B? I am basically looking at something that penalize the model for making cross predictions. Any idea? Thank you!
