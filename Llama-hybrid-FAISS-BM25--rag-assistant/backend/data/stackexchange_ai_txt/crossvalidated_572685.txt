[site]: crossvalidated
[post_id]: 572685
[parent_id]: 538577
[tags]: 
Here is a non-technical answer: In each lot (subset of data) the authors of that paper average of the clipped gradients (scaled to be length $\leq C$ ) from that lot. The clipping allows us to know that the sensitivity (max change by arbitrarily rewriting one data point in the original set) of taking an average is $C$ . use the Gaussian mechanism with epsilon (parameter given by user), sensitivity $C$ (parameter given by user and ensured in previous bullet) to add random noise to the average. the math proof seems involved, but the basic idea is that the Gaussian mechanism as used provides epsilon-delta-privacy, and composition theorems mean you can add them up over the iterations.
