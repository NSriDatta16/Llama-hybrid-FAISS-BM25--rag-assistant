[site]: crossvalidated
[post_id]: 41125
[parent_id]: 41071
[tags]: 
Summary The crux of the matter is that $F$ may exhibit local interactions, for certain restricted sets of $(x,y)$ , but if these have zero chance of happening, $F$ effectively has no interaction. There is no unique criterion for interaction to exist, but this reasoning shows that any criterion must be related to the chance that $F$ exhibits local interactions. Friedman's criterion actually is one measure of the amount of local interaction, and it clearly is designed to be zero or positive. It works because its expectation will be positive exactly when there is some amount of local interaction somewhere. Context and definitions: why second mixed partials are involved This definition focuses on two variables, so without any loss of generality we can explore the setting where $\mathbf{x}=(x,y)$ and $F$ is a continuously twice-differentiable function of $x$ and $y$ defined on some open convex set. The intuitive meaning of "interaction" is that the variation of $F$ with $x$ depends on the value of $y$ (at least for some values of $y$ ) or, symmetrically, that the variation of $F$ with $y$ depends on the value of $x$ . Variations are naturally expressed in terms of the first partial derivatives of $F$ : for there to be no interaction, the $x$ partial, $F_x(x,y)$ , must not depend on $y$ , and the $y$ partial, $F_y(x,y)$ , must not depend on $x$ . A lack of dependence on a variable implies the derivative with respect to that variable is zero, whence we conclude that necessarily the second mixed partials, $F_{xy}(x,y)$ and $F_{yx}(x,y)$ , must be zero for there to be no interaction. (Because $F$ has continuous second derivatives, this condition is also a sufficient one: these mixed partial derivatives are then everywhere equal to each other and their vanishing implies independence.) An interaction is therefore detected by finding any point $(x_0,y_0)$ at which $F_{xy}(x_0,y_0)\ne 0$ . Because this second derivative is a continuous function, we may infer that $F_{xy}(x,y)$ remains nonzero in some neighborhood of $(x_0,y_0)$ . What happens when the variables are random Now suppose that $x$ and $y$ are values of random variables $X$ and $Y$ , respectively, so that (by virtue of its continuity, which makes it measurable), $F_{xy}(X,Y)$ itself becomes a random variable. If the chances are negligible that $(X,Y)$ lies in any region where a dependence exists in $F$ , then as far as we are concerned, the interaction of $F$ within that region is of no consequence. Accordingly, to detect an effective interaction, we need to focus on the behavior of $F_{xy}$ in regions only where $(X,Y)$ has some nonzero chance. To this end, let $I_F$ be the indicator that $F_{xy}$ is nonzero; that is, $I_F(x,y) = 1$ exactly when $F_{xy}(x,y)\ne 0$ and otherwise $I(x,y)=0$ . The chance that $F$ exhibits an interaction, then, equals the expectation of $I_F$ . That is, interaction is detected by the criterion $\mathbb{E}_{X,Y}[I_F(X,Y)] \gt 0$ . Deriving Friedman's criterion This much we can work out from the definitions and basic principles of probability. It is now just a short step to recognize that Friedman's criterion is tantamount to the one we derived: continuity implies $F_{xy}$ is finite, whence the expectation of $F_{xy}^2$ can be positive if and only if $F_{xy}$ has a chance of being nonzero. It may be more convenient to use the former criterion $\mathbb{E}_{X,Y}(F_{xy}(X,Y)^2)\gt 0$ instead of the indicator criterion, because $F_{xy}^2$ has nice mathematical and computational properties: it is a continuous function of $(x,y)$ , unlike $I_F$ , which jumps between $0$ and $1$ , and it is at least as differentiable as $F_{xy}$ is. This suggests that Friedman's criterion may lead to computationally more tractable algorithms than the indicator criterion when applied to estimates of $X$ and $Y$ made from finite datasets. Application If the "training data" $\{(x_i,y_i)\}$ are $n$ independent realizations of $(X,Y)$ , then expectations may be estimated by averages over the data. The two estimates are $$\widehat{\mathbb{E}}_{X,Y}(I_F(X,Y)) = \frac{1}{n}\sum_i I_F(x_i,y_i) = \#\{i:F_{xy}(x_i,y_i) \ne 0\} / n$$ (the relative frequency of data where the second mixed partial of $F$ is nonzero) and $$\widehat{\mathbb{E}}_{X,Y}(F_{xy}(X,Y)^2) = \frac{1}{n}\sum_i F_{xy}(x_i,y_i)^2.$$ The right hand side reduces to $1/n$ times the sum of $F_{xy}(x_i,y_i)^2$ for the data where the second mixed partial of $F$ is nonzero, because (obviously) all the other terms are zero. Thus, in the first instance we are summing a bunch of 1's and in the second instance we are summing a bunch of nonzero squares of derivatives. Either way, each sum is positive if and only if the other one is. In the case $F(x,y) = \beta_0 + \beta_1 x + \beta_2 y + \beta_{1,2} x y$ , indeed $F_{xy}(x,y) = \beta_{1,2}$ , a constant. This function has an interaction everywhere. When $\beta_{1,2}$ is nonzero, the mean indicator is $1$ and the mean value of $F_{xy}(X,Y)^2$ is $\beta_{1,2}^2$ . Because $F_{xy}$ is constant in this case, it does not matter what the distribution of $(X,Y)$ is. This justifies the standard terminology in which, although the $x_i$ and $y_i$ are fixed (not random), $\beta_{1,2} x y$ is called an "interaction term" and $\beta_{1,2}$ is used to measure the strength of that interaction.
