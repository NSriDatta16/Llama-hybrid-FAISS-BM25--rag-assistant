[site]: datascience
[post_id]: 32785
[parent_id]: 32740
[tags]: 
It is very simple. Just add a Dense layer (Keras-wise) with one unit after your LSTM network, with sigmoid activation. If you don't use Keras, Dense layer is simply a fully-connected neuron with one unit (in your case), which you can easily implement in the deep learning framework of your preference. Then you can train your network to directly classify the input sequence using soft-labeling, so that it outputs a probability from 0 to 1 that a sequence is anomalous. Needless to say, you can have an arbitrary number of inputs, not just one. Bottom line : use a fully-connected layer with one unit (neuron) and sigmoid activation after the LSTM output
