[site]: crossvalidated
[post_id]: 196799
[parent_id]: 94130
[tags]: 
It lies in the nature of the algorithm. Let us assume that we have two meaningful features $X_1$ and $X_2$ that are strongly correlated. From the paper http://arxiv.org/abs/1106.5112 (The All Relevant Feature Selection using Random Forest, Miron B. Kursa, Witold R. Rudnicki) we can take a short description of the boruta algorithm: "To deal with this problem, we developed an algorithm which provides criteria for selection of important attributes. The algorithm arises from the spirit of random forest â€“ we cope with problems by adding more randomness to the system. The essential idea is very simple: we make a randomised copy of the system, merge the copy with the original and build the classifier for this extended system. To asses importance of the variable in the original system we compare it with that of the randomised variables. Only variables for whose importance is higher than that of the randomised variables are considered important." Essentially the Boruta algorithm trains a random forest on the set of original and randomized features. This random forest during training, as every random forest, only sees a subset of all features at every node. Hence sometimes it will not have a choice between $X_1$ and $X_2$ when picking the variable for the current node and it cannot prefer one of the two variables $X_1$ and $X_2$ over the other. This is is the reason why Boruta can not classify one of the variables $X_1$ and $X_2$ as unimportant. One would have to modify the underlying random forest algorithm to always see $X_1$ and $X_2$ with their random shadow variables $\hat X_1$ and $\hat X_2$ at every node. Then the random forest could often for example select the variables $X_1$ and $\hat X_2$ which would result in Boruta selecting variable $X_1$ and rejecting $X_2$. (Here $X_2$ is rejected because $\hat X_2$ has a higher importance than $X_2$)
