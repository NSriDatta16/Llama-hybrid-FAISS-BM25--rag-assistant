[site]: crossvalidated
[post_id]: 406561
[parent_id]: 
[tags]: 
Custom TF 2.0 training loop performing considerably worse than keras fit_generator - can't understand why

In trying to better understand tensorflow 2.0, I am trying to write a custom training loop to replicate the work of the keras fit_generator function. In my head, I have replicated the steps fit_generator takes to train my network, but this is clearly not the case as the network trains significantly better using fit_generator as opposed to my training loop (my loop: MAE ~3.0, keras fit_generator: MAE ~2.0). Below is my class: class CRNN(tf.keras.Model): def __init__(self): super(CRNN, self).__init__() # Consider LocallyConnected1D self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=50, strides=1, padding='same', activation=None, kernel_initializer='he_uniform', name='conv1a') self.pool1 = tf.keras.layers.MaxPool1D(pool_size=100, strides=None, name='pool1') self.gru1 = tf.keras.layers.GRU(units=32, name='gru1') self.dense1 = tf.keras.layers.Dense(units=16, activation=None, name='dense1') self.output1 = tf.keras.layers.Dense(units=1, activation='relu', name='output1') self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1) self.mae = tf.keras.losses.MeanAbsoluteError() self.optimizer = tf.keras.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True) def call(self, inputs): x = self.conv1(inputs) x = self.lrelu(x) x = self.pool1(x) x = self.gru1(x) x = self.dense1(x) x = self.lrelu(x) return self.output1(x) def train_step(self, sample, label): with tf.GradientTape() as tape: predictions = self.call(sample) loss = self.mae(label, predictions) gradients = tape.gradient(loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) self.train_loss(loss) def eval_once(self, sample, label): predictions = self.call(sample) loss = self.mae(label, predictions) self.eval_loss(loss) def train(self, num_epochs): self.train_loss = tf.keras.metrics.Mean(name='train_loss') self.eval_loss = tf.keras.metrics.Mean(name='eval_loss') self.store_gradients = np.empty((num_epochs, )) for epoch in range(num_epochs): start_time = time.time() self.train_loss.reset_states() self.eval_loss.reset_states() for samples, labels in train_gen: self.train_step(samples, labels) train_gen.on_epoch_end() for samples, labels in val_gen: self.eval_once(samples, labels) print('Epoch: {0}, Time: {1:.2f}, Train Loss: {2:.2f}, Test Loss: {3:.2f}'.format(epoch + 1, time.time() - start_time, self.train_loss.result(), self.eval_loss.result())) I am using two custom generators (both are tf.keras.util.Sequences), one for the training data and one for the validation data, but they are used for both training strategies, so I don't feel like they are the issue. In order to implement my custom training loop, I run: tf.keras.backend.clear_session() model = CRNN() model.train(20) In order to run keras fit_generator, I run: model2 = CRNN() model2.compile(optimizer=model2.optimizer, loss=model2.mae) history2 = model2.fit_generator(generator=train_gen, validation_data=val_gen, epochs=20, workers=1, use_multiprocessing=False, verbose=2, callbacks=[]) I feel like I must be missing something extremely obvious here, can anyone help me out? Please let me know if I neglected to give any other important/relevant information. Thanks so much! Edit: I have also tested this model on a single input in order to verify that it is capable of over fitting. Both are able to over fit to a single data point.
