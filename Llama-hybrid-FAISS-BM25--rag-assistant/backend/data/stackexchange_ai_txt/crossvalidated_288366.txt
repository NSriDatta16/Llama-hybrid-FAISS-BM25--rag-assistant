[site]: crossvalidated
[post_id]: 288366
[parent_id]: 
[tags]: 
Is splitting the data into test and training sets purely a "stats" thing?

I am a physics student studying machine learning / data science, so I don't mean for this question to start any conflicts :) However, a big part of any physics undergraduate program is to do labs/experiments, which means a lot of data processing and statistical analysis. However, I notice a sharp difference between the way physicists deal with data and the way my data science / statistical learning books deal with data. The key difference is that when trying to perform regressions to data obtained from physics experiments, the regression algorithms are applied to the WHOLE dataset, there is absolutely no splitting into training and test sets. In the physics world, the R^2 or some type of pseudo-R^2 is calculated for the model based on the whole data set. In the stats world, the data is almost always split up into 80-20, 70-30, etc... and then the model is evaluated against the test dataset. There are also some major physics experiments (ATLAS, BICEP2, etc...) that never do this data splitting, so I'm wondering why there is a such a staunch difference between the way physicists/experimentalists do statistics and the way data scientists do statistics.
