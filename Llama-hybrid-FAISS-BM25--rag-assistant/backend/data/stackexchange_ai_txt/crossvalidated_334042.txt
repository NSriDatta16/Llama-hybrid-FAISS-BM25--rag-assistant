[site]: crossvalidated
[post_id]: 334042
[parent_id]: 
[tags]: 
How to visualise a tiny neural network as a function

Say you have the simplest possible neural network with 1 input, 1 output and 1 hidden variable as depicted below. In this case, the activation function is logistic. I assume between x and y, the logistic function here is $\frac{1}{1+exp(-(ax+b))}$. Beyond that, I don't know how they combine between layers. I had assumed that z = logistic(cy+d) where y=logistic(ax+b), but from plotting this manually against the neural network output, it appears to not be the case. Furthermore, I have no idea how the input from multiple nodes gets transformed into the output of a single node. For the neural network below: How do the functions describing the individual neurons combine? Does z = logistic(ey1+fy2+g), or logistic(ey1) + logistic(fy2) + g or something else? Using the coefficients a,b,c,d,e,f,g, how would you write the output z as a function of x? This has been bugging me for a while and I would really appreciate some clarification on this. I understand that writing the formula for a neural network of any appreciable size would be both intractable and unhelpful, I just want to get an understanding of the underlying maths.
