[site]: stackoverflow
[post_id]: 4656549
[parent_id]: 4654552
[tags]: 
Simply put: that advice on optimization is misleading. Not necessarily wrong, but incomplete. It appears your source was CodeProject . He states he's mostly talking about optimization for ARM. First, it's highly processor-dependent how char and short are handled. Depending on the architecture, conversions may be zero or minimal cost, depending on when and how they occur -- at load time, the type of operation, what instructions can run in parallel and in effect may be free, depending on the rest of the code - for example on the TI DSP c64 architecture, which can run 8 ops per cycle. Typically the most-efficient use will be the "native" integer size, but it also depends on where the data comes from - it may be more efficient to load, modify and store back char/short data than to load and convert to int, modify, and store back as char/short. Or it may not - it depends on the architecture and the operations being performed. The compiler often has a better look at whether to do this for you or not. Second, in many, many architectures char and short are as fast as int, especially if the calculation avoids implicit conversions to int. Note: this is easy to mess up in C, like "x = y + 1" - that forces conversion up to int (assuming x & y are char or short), but the good thing is that almost all compilers are smart enough to optimize-away the conversion for you. Many other cases of having a local be char/short will cause the compiler to optimize-away any conversions depending on how it's used later. This is helped by the fact that in typical processors, the overflow/wrap-around of a char/short is the same result as calculating it as an int and converting on store (or by simply addressing that register as char/short in a later operation - getting the conversion for 'free'). In their example: int wordinc (int a) { return a + 1; } short shortinc (short a) { return a + 1; } char charinc (char a) { return a + 1; } In many architectures/compilers, these will run equally fast in practice. Third, in some architectures char/short are faster than int. Embedded architectures with a natural size of 8 or 16 bits (admittedly not the sort of development you're thinking of nowadays) is an example. Fourth, though not a big issue generally in modern ram-heavy, huge-cache processor environments, keeping local stack storage size down (assuming the compiler doesn't hoist it to a register) may help improve the efficiency of cache accesses, especially level-1 caches. On the other side, IF the compiler isn't smart enough to hide it from you, local char/shorts passed as arguments to other functions (especially not file-local 'static' functions) may entail up-conversions to int . Again, per above, the compiler may well be smart enough to hide the conversion. I do agree with this statement at the start of the site you quote: Although a number of guidelines are available for C code optimization, there is no substitute for having a thorough knowledge of the compiler and machine for which you are programming.
