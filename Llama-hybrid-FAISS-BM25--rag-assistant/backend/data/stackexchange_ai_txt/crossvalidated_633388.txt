[site]: crossvalidated
[post_id]: 633388
[parent_id]: 599919
[tags]: 
It is not true that in a VAE, every data point "has its own latent variable". The same latent variables describe every data point. What is different per data point is our beliefs about the values of these variables. For instance, say we have a VAE that performs inference on a generative model of images. An image is a vector $x$ (e.g. a list of the RGB-values of its pixels) and assumed to be generated from a set of latent causes $z$ (also a vector). The generative model is given by $p(x,z)=p(x|z)p(z)$ , and the true posterior would be $p(z|x)=\frac{p(x|z)p(z)}{p(x)}$ , but this is difficult to compute in practice and so we approximate it using a variational distribution $q(z|x)$ , which we choose to have a simple form that will allow us to work with it easily. You can find the parameters of $q(z|x)$ (e.g., the means and (co-)variances, if it's Gaussian) in various ways. One way is to do iterative optimization, like the coordinate ascent scheme you described. Another way is to train a neural network to output the parameters of $q(z|x)$ , by running the image $x$ through the network. This is called amortized inference , because we use the same neural network (with the same neural network weights) for every image. Instead of a (possibly lengthy) optimization procedure, we use a machine that takes in the image and gives us the answer. Or at least, something close to the optimal answer, as VAEs aren't guaranteed to produce the optimal parameters of $q(z|x)$ - they are just trained to do a good job which hopefully also generalizes to new data (as with any neural net). The important point is: you get a different $q(z|x)$ for any given image, but these are beliefs about the same set of latent variables, since every image is assumed to be caused by the same causal variables (e.g. the positions, colors, shapes etc. of objects in a scene). For instance, every image might have a handwritten digit in it, but the number might be different, it might be in a different position in the image, it might have a different stroke width, a different slant, etc.. How does the mean-field assumption enter into this? It is essentially orthogonal to this whole story. As with any variational inference problem, you can choose $q(z|x)$ to have whatever form you like. However, the typical situation with VAEs is that the variational beliefs over the latent variables are chosen to be independent. That is, $q(z|x)=\prod_i{q(z_i|x)}$ . And that is precisely the mean-field assumption.
