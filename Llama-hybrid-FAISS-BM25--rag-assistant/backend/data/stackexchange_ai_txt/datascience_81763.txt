[site]: datascience
[post_id]: 81763
[parent_id]: 81740
[tags]: 
I would go for this: data -> tokenize -> tfidf* -> neural net But in tfidf vectorizer, you could actually regularize the number of terms used, say for example restricting the minimum number of occurrences of a term and/or defining the max_number of features so that you only keep the ones that have the highest importance according to Tfidf. If you want to reduce the number of features via some decomposition technique PCA won't be adequate since the term-frequency matrix is sparse, so you could, for example, using NMF (Non-negative matrix factorization instead) So: data -> tokenize -> tfidf->NMF -> neural net This time the regularization on tfidf is not necessary since you have an additional step. In the end, metrics on CV will guide you on what the best strategy is
