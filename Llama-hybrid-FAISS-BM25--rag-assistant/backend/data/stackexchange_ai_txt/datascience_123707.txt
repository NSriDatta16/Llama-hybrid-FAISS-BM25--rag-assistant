[site]: datascience
[post_id]: 123707
[parent_id]: 
[tags]: 
heavy underfitting of keras LSTM regression

I moved the question from stackoverflow to here. I used keras LSTM to do the standard regression project of time series prediction. But I see a very low (almost 0) R-squared score on the training data (so does testing data, but let's focus on the training first), which is regard as a heavy underfitting I think. Usually we say neural network is very easy to be overfitted even the label is un-predictable, we can still have a high score on training. But I have tried many cases like large number of layers, units, the training scores are still low. Is it possible because of the structure, for example, what's the difference between model.add(LSTM(64)) and model.add(Dense(64))? df_train, df_test = train_test_split(df, train_size=0.9, shuffle=False) array_X_train = df_train[cols_X].values[::50] array_X_test = df_test[cols_X].values[::50] array_Y_train = df_train[label].values[::50] array_Y_test = df_test[label].values[::50] model = Sequential() model.add(LSTM(64, activation='tanh',return_sequences=True, input_shape=(array_X_train.shape[1], array_X_train.shape[2]))) model.add(LSTM(256, activation='tanh', return_sequences=True)) model.add(LSTM(256, activation='tanh', return_sequences=True)) model.add(LSTM(64, activation='linear')) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer="adam", loss="mse", metrics=['mse']) model.fit(array_X_train, array_Y_train, epochs=2, batch_size=28, verbose=2, workers=100, use_multiprocessing=True, callbacks=EarlyStopping(monitor='loss')) # R-squared are calculated by r2_score(array_Y_train, array_Y_train_pred), which is not showed here Above is my structure of LSTM. For the epochs=2 , I tried a bigger number and the result is similar. Therefore I use 2 to save time. For continuous label, I use MinMaxScaler() normalization to be consistent with the range of sigmoid and for features, I use StandardScaler() normalization. Here at each time point, we have 7 features and I use 100 time window to predict the next time, namely the shape of a sample is (100*7) . I have 3000000 samples with split ratio 9:1. Moreover I select 1 training sample every 50 to reduce the overlap between closed samples. Therefore the final number of training samples is about 60000, i.e. the shape of input X is (60000*100*7) If you need more information, pls tell me. Edit: As advised, I also tried linear activation for the last layer: model.add(Dense(1, activation='linear')) and back to StandardScaler() to have the whole real values. However, the training score is still very low. And I saw the predict values focus nearby zero.
