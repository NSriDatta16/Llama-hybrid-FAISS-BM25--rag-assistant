[site]: datascience
[post_id]: 97659
[parent_id]: 78055
[tags]: 
Essentially the target vectors $\mathbf{w}$ and the context vectors $\mathbf{c}$ learn the same thing (they jointly factorise a matrix of word co-occurrence statistics and are two sides of the same coin). You can use either equivalently. However, a better result is given by taking their average... This is because the embeddings are trained so that $\mathbf{w}^\top\mathbf{c}$ approximates a useful statistic. If the $\mathbf{c}$ 's are discarded and only $\mathbf{w}$ 's used then $\mathbf{w}^\top\mathbf{w}$ gets back the encoded statistic but with an error. Using $\mathbf{w}^\top\mathbf{c}$ is obviously preferable as it gets back the statistic with minimal error, but requires keeping both sets of embeddings (twice the memory). Under some simplifying assumptions, using their average $\mathbf{a}=\tfrac{\mathbf{w}+\mathbf{c}}{2}$ can be seen to half the error, so not as good as keeping both (e.g. concatenating), but better than just using either $\mathbf{w}$ or $\mathbf{c}$ . More detail is given in this paper
