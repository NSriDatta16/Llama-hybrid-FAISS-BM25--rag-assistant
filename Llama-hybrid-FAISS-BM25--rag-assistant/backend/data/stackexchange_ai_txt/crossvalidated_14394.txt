[site]: crossvalidated
[post_id]: 14394
[parent_id]: 12041
[tags]: 
Usually the by-product of optimization algorithms is hessian of optimisation function. If optimisation function is the log-likelihood then the hessian returned will be the estimate of the information matrix. Here is the relevant formula from wikipedia page : $$\mathcal{I}(\theta)=-E\frac{\partial^2 \log f(X)}{\partial \theta \partial \theta '}=E\frac{\partial \log f(X)}{\partial \theta}\frac{\partial \log f(X)}{\partial \theta'}$$ The log-likelihood of the sample supplied to the optimisation function is of the form: $$l(\theta)=\frac{1}{n}\sum_{i=1}^n\log f(X_i,\theta)$$ Hence the hessian of this function estimated on the optimal point is: $$\frac{\partial^2 l(\theta)}{\partial\theta\partial\theta'}=\frac{1}{n}\sum_{i=1}^n\frac{\partial^2\log f(X_i,\theta)}{\partial \theta\partial\theta'},$$ so we estimate the expectation with a sample average, which will tend to the true expectation due to law of large numbers. If you look at the arima code as @Chase suggested, you will see that this is exactly the way R calculates the variance matrix of the coefficients (as the inverse of hessian, which is the information matrix). If the optimisation algorithm does not return the hessian, you can exploit this formula for information matrix: $$ \mathcal{I}(\theta)=E\frac{\partial \log f(X)}{\partial \theta}\frac{\partial \log f(X)}{\partial \theta'} $$ The estimate using the gradients will be then $$\frac{1}{n}\sum_{i=1}^n\frac{\partial \log f(X_i)}{\partial \theta}\left(\frac{\partial \log f(X_i)}{\partial \theta}\right)'.$$ But for this you will need somehow to estimate the gradient at each point $X_i$. This can be done either numerically or analytically. Note that since the differentiation is done with respect to $\theta$, with $X_i$ as constant it might not be that hard.
