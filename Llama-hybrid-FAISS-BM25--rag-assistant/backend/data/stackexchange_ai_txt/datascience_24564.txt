[site]: datascience
[post_id]: 24564
[parent_id]: 
[tags]: 
Log loss and expected aggregates

For my current use case, I have a high number of (noisy) samples that I do binary classification on. I use a neural network to approximate this with a sigmoid layer as output and log loss as loss function. My question is about the loss function, I understand that we use log loss to penalize probabilities that are far from the truth because of the log likelihood. The problem (or maybe it is not a problem) is that I want to aggregate my results to get the expected amount of positive samples in big groups. If we look at it from that perspective, looking at it as a mean as opposed to a probability also makes sense. In that case, having 2x 0.4 and 1x 0.7 is the same mistake as 1x 0. and 2x 0.75. Does it still make sense to use the log loss or might it be better to use L2 or L1 regression instead?
