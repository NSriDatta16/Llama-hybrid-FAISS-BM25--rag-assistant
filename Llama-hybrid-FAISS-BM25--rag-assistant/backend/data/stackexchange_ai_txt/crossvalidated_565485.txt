[site]: crossvalidated
[post_id]: 565485
[parent_id]: 565467
[tags]: 
Don't do that --- it is very bad The goal in statistical inference and prediction (including in machine learning problems) is to make a realistic inference that includes recognition of the uncertainty that comes with having a limited amount of data . If you understate the inherent level of uncertainty in the inference then that is bad. What you are proposing to do is to generate synthetic values that will overwhelm your actual data and then use this to make your inferences --- if you were to do that then all that will happen is that you will "infer" back to the assumed structure that went in to producing your synthetic data, which will generally be very different to the probabilistic structure of the actual data generating process. $^\dagger$ What you are proposing is a species of logical fallacy. I have not heard a name for it before, but I call it the "inferential compounding fallacy". It occures when you take a predicted outcome from a model (or simulated values from an assumed synthetic model) and then feed the outcome back in to your analysis as if it were observed data. The effect of feeding in unbiased predictions is that you systematically underestimate the uncertainty in your inferences, and the effect of feeding in biased predictions (or synthetic data) is that you systematically bias your inference and underestimate the uncertainty. $^\dagger$ If we were capable of guessing the probabilistic structure of data generating processes a priori this make the disciplines of statistics and machine learning largely moot.
