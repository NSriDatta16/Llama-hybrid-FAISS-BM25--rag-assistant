[site]: stackoverflow
[post_id]: 4603944
[parent_id]: 
[tags]: 
What's the benefit of accepting floating point inaccuracy in c#

I've had this problem on my mind the last few days, and I'm struggling to phrase my question. However, I think I've nailed what I want to know. Why does c# accept the inaccuracy by using floating points to store data? And what's the benefit of using it over other methods? For example, Math.Pow(Math.Sqrt(2),2) is not exact in c#. There are programming languages that can calculate it exactly (for example, Mathematica). One argument I could think of is that calculating it exactly is a lot slower then just coping with the inaccuracy, but Mathematica & Matlab are used to calculate gigantic scientific problems, so I find it hard to believe those languages are really significantly slower than c#. So why is it then? PS: I'm sorry for spamming you with these questions, you've all been really helpful
