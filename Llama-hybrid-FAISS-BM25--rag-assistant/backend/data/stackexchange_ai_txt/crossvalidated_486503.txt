[site]: crossvalidated
[post_id]: 486503
[parent_id]: 485717
[tags]: 
Yes, your understanding is mostly correct; XGBoost actually uses coordinate descent to fit the base-learn GLMs and not Newton's method directly but conceptually you are correct. We will ultimately fit a GLM. I gave more details on the matter in the thread: Difference in regression coefficients of sklearn's LinearRegression and XGBRegressor . In general, this equivalence is not unexpected. In the end of the day, the linear combination of linear models is still a linear model; our model is of form $y \approx X \beta$ . Ultimately, each boosting iteration updates the final estimate by some amount $\alpha$ , our learning rate. Making a large number of updates (i.e. boosting iterations) will result to having a XGBoost learner equivalent to GLM (with associated $L_1$ and $L_2$ regularisation).
