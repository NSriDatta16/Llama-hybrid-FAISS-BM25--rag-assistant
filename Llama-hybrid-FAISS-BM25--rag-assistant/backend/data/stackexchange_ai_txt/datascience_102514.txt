[site]: datascience
[post_id]: 102514
[parent_id]: 
[tags]: 
Is it possible to fine-tune a (Spanish RoBERTa) model for a different task?

I'm doing sentiment analysis of Spanish tweets. After reviewing some of the recent literature, I've seen that there's been a most recent effort to train a RoBERTa model exclusively on Spanish text. It seems to perform better than the current state-of-the-art model for Spanish language modeling so far, BETO . The initial BETO model has been trained with the Whole Word Masking technique for a variety of tasks, but does not include text classification . However, I've come across a model that used the BETO model to create a model that is able to do text classification - more precisely, sentiment analysis. It seems therefore possible to have a baseline model that cannot perform a certain task and fine-tune (or is it train?) for a different task. QUESTION: Would it, therefore, be possible to take the afore-mentioned RoBERTa model - which in its initial form cannot perform text classification - and fine-tune it to a different task (sentiment analysis in tweets)? If so, how would I go about this? Can someone name any helpful resources? Note: I have a labelled dataset with Spanish Tweets that I could use for fine-tuning.
