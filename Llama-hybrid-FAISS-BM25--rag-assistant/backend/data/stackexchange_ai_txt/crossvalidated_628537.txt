[site]: crossvalidated
[post_id]: 628537
[parent_id]: 
[tags]: 
Should I create an ensemble by averaging deep models' weights and biases?

When I train deep models with cosine annealing learning-rate scheduling and warm restarts , I get models that achieve completely different scores on my validation set, after each training cycle. There seems to be no relationship between the scores on the validation set and the actual number of training cycles run since the very beginning, i.e., there seems to be no overfitting or underfitting from cycle to cycle, as from cycle to cycle, the validation scores go either up or down, inconsistently. It looks like each training cycle produces completely different models as if I would train the network from scratch many times with different random seeds (that impact the training set shuffling and thus the batches). My question is whether creating a final model with the mean weights and biases of all learned models is a good approach. Is there literature that supports or opposes this approach with experiments?
