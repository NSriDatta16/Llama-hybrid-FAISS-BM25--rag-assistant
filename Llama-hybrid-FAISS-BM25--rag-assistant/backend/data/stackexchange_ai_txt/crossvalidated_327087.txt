[site]: crossvalidated
[post_id]: 327087
[parent_id]: 327081
[tags]: 
Let's start with something simpler, a decision tree , since random forests are forests of independent decision trees that are averaged. Decision tree is build by splitting your data into subsets conditionally on the features used. The splits are done by choosing the features, one at a time, and then choosing a split based on the values of the feature. In both cases we make our choices based on some loss function that is minimized. In regression case we minimize the variance , what is equivalent to minimizing squared loss. In classification case, we use entropy, or Gini impurity as a criterion. In the end your data gets packed into a number of subgroups and to make predictions, in classification case you predict the most frequent value within the subgroup, and in regression case you predict the mean of the subgroup. Obviously, if you calculate the mean of the binary values, you'd get the fraction, i.e. empirical probability. So basically in both cases you can calculate probabilities the same way, this problem reduces only to the criteria that is used for building the tree: mean squared error vs entropy (or Gini impurity). If you choose mean squared error, then you'd be minimizing the same loss as linear regression, while choosing entropy, leads to minimizing the same loss function as logistic regression .
