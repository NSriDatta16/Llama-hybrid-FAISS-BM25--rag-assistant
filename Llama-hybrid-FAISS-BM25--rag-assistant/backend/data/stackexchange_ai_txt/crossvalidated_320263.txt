[site]: crossvalidated
[post_id]: 320263
[parent_id]: 320252
[tags]: 
First of all you are in the territory of problems some very smart people have developed on before, which makes it almost impossible to invent something better than the rest of the community without an intense study of the existing literature. Trust me I have tried. In order to get good estimates of confidence you need to go into ensemble models. If you want to have a conference on a opinion ask many experts. here there are two main families, which both give very good scores. Random Forest - take the average of multiple deeply trained decision trees. Typically the first benchmark machine learning model. Gradient Boosted Decision Trees - Train new decision trees on the residual of the previous. Thereby putting higher emphasis on previously badly modeled observations. Very often the best performing model on the data science competition site kaggle.com. So my advice would be use Random Forest or a gradient booster and trust the scores. The gradient booster is properly the closes model to your idea.
