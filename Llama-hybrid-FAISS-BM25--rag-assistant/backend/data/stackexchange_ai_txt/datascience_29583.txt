[site]: datascience
[post_id]: 29583
[parent_id]: 
[tags]: 
How do Dynamic Memory Network scale to large inputs?

How do Dynamic Memory Network, for example from the paper Ask Me Anything: Dynamic Memory Networks for Natural Language Processing , scale to large inputs? The paper states the following: In these cases, the input may be a sentence, a long story, a movie review, a news article, or several Wikipedia articles. Did anyone actually implement this network architecture to read an arbitrarily large corpus of text (say 250 Wikipedia articles)? The only example I could find online use rather small inputs (a few lines of text). What is the bottleneck of using this type of network with large inputs? One solution I see is to encode paragraphs instead of sentences.
