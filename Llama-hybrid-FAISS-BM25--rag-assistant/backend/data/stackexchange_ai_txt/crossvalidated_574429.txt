[site]: crossvalidated
[post_id]: 574429
[parent_id]: 
[tags]: 
Recovering samples from a density estimation with an additional prior on the samples. Used for Gibbs sampling

Abstract Idea : Given a noisy measured density ( $d_j$ at position $p_j$ ) and a density model, sample from the model parameters under the following stochastic model: Stochastic Model : Prior for model parameters $\theta$ : $p(\theta)$ $x_1 \in \mathbb{R}, x_2 \in \mathbb{R}, \dots , x_L \in \mathbb{R}$ , $L \ggg 1 $ , i.i.d samples from $p(x_i|\theta)$ . Sample density estimates $\{d_j\}_{j=1}^N$ at position $\{p_j\}_{j=1}^N$ from $\mathcal{N}(d_j | \frac{1}{L}\sum_{i=1}^L k(x_i-p_j), \sigma_0^2) = p(d_j|\{x_i\}_{i=1}^L)$ , where $k$ is some kernel (such as a gaussian). Thus the joint probability density is: $$ p(\{d_j\}_{j=1}^N,\{x_i\}_{i=1}^L,\theta) = \left(\prod_{j=1}^N p(d_j|\{x_i\}_{i=1}^L)\right)\left(\prod_{i=1}^L p(x_i|\theta)\right) p(\theta) $$ Solution Sketch: Task: Sample $\theta \, \mid \, \{d_j\}_{j=1}^N$ using a MCMC algorithm (Gibbs sampler), for this we need to sample from the conditional distributions: Sampling $\theta^{t+1}$ from $p(\theta^{t+1} | \{x^t_i\}_{i=1}^L)$ is in principle extremely hard, but well studied and large, fast libraries (implemented in C) are available for exactly this problem. I'm stuck here: Sampling from $p(\{x^{t+1}_i\}_{i=1}^L | \{d^t_j\}_{j=1}^N, \theta^t)$ , I feel like this problem should be well studied in the literature, as essentially what I want to recover are the most likely samples under some observed density and some prior information on the underlying density. However, I can't seem to find this in the literature. Any pointers to papers/keywords that deal with this problem would be super helpful! My current ideas are mostly restricted to assume $L \ggg 1$ such that $p(\{x^{t+1}_i\}_{i=1}^L | \{d^t_j\}_{j=1}^N, \theta^t)$ can be approximated by an factorized distribution $\prod_i q(x^{t+1}_i)$ , but I'm kinda stuck here.
