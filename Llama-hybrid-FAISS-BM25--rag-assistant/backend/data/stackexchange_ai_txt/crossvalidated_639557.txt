[site]: crossvalidated
[post_id]: 639557
[parent_id]: 
[tags]: 
Does the Bias of the Model Only Depend on Model Class?

In machine learning with statistical approach, does the bias of the model solely depend on the selection of the model class without considering the training data? There is a claim regarding the bias-variance decomposition in a technical report (Settles Burr, Active Learning Literature Survey, 2009) saying: The second term is the bias, which represents the error due to the model class itself, e.g., if a linear model is used to learn a function that is only approximately linear. This component of the overall error is invariant given a fixed model class. The third term is the model’s variance, which is the remaining component of the learner’s mean squared error with respect to the true regression function. Minimizing the variance, then, is guaranteed to minimize the future generalization error of the model ( since the learner itself can do nothing about the noise or bias components ) I'm wondering why this is true or not. And why does the variance depend on the training data instead? I search online for a while but there is no answer for such doubt. I also consult with GPT, well, its answer resonate with the claim in the paper, and saying " no formal proof can be given as it is conceptual understanding of what bias represents in the bias-variance decomposition and how it relates to the choice of model class " (by GPT).
