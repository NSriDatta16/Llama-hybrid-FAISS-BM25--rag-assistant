[site]: crossvalidated
[post_id]: 429537
[parent_id]: 429335
[tags]: 
We must assume these "standard deviations" are actually standard errors of the mean (which is likely), for otherwise they are worthless without counts of the raw data used to compute each standard deviation. (If these are truly standard deviations but all those counts are known to be the same, then you don't need to know the common count because the weights, as described below, depend only on the relative counts, and you may proceed as if they are standard errors.) These standard errors (SEs) do not vary a great amount (the square of the largest is only about twice the square of the smallest), so it's likely they won't make a difference: you can use standard Analysis of Variance (ANOVA). ANOVA is a good approach. It is Ordinary Least Squares (OLS) regression in disguise. This insight permits you to use weighted OLS to handle the information conveyed by the SEs. The proper way to weight the observations is proportional to the reciprocal variance: that is, to the reciprocal squared SEs: see https://stats.stackexchange.com/a/246449/919 and https://stats.stackexchange.com/a/12255/919 for the mathematics behind this. In R , for instance, after creating a data frame X with Value , SE , and Group fields, you could use its OLS procedure lm : fit.w Using group $B$ as the reference, the output for your data is Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.17973 0.05081 42.902 1.34e-13 *** GroupA 0.16855 0.07227 2.332 0.0397 * GroupC 0.16913 0.07295 2.318 0.0407 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1006 on 11 degrees of freedom Multiple R-squared: 0.3976, Adjusted R-squared: 0.2881 F-statistic: 3.63 on 2 and 11 DF, p-value: 0.06157 Pay no attention to the individual p-values (in the right hand column). They are deceptively low because you plan to make three related tests, not one; and these p-values are computed only for the case where each of the coefficients they test is the only one you are interested in. (By the way, the unweighted fit produces slightly lower p-values, but not enough to make us analyze the effects of the weights in more detail.) The first result to examine is the p-value for the overall regression F-statistic, reported at the very end as $0.06157.$ That's weak evidence of any difference. Many people would stop here and treat the groups as having equal means. But if you can accept such weak evidence (for instance, your threshold might be $10\%$ rather than $5\%$ ), you would proceed (as usual) to make post hoc comparisons. You need to account for planning on three tests. For instance, you could use Tukey's HSD to perform the three comparisons among the groups. TukeyHSD(aov(fit.w)) diff lwr upr p adj B-A -0.1685526460 -0.35444973 0.01734443 0.0764764 C-A 0.0005789762 -0.19659465 0.19775261 0.9999653 C-B 0.1691316222 -0.02804201 0.36630525 0.0951872 Again, you would begin your review by consulting the "adjusted p-values" in the rightmost column. Two of them are less than $0.10,$ but none are less than $0.05.$ If your threshold for statistical significance is $0.10,$ for instance, then you would conclude there are differences between groups $A$ and $B$ (first line, $p=0.076$ ) and groups $C$ and $B$ (third line, $p=0.095$ ), but not between groups $C$ and $A$ (which exhibit very nearly the same weighted mean).
