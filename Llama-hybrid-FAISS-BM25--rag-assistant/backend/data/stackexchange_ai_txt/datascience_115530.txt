[site]: datascience
[post_id]: 115530
[parent_id]: 
[tags]: 
Sigmoid Activation Function (Output layer) Alternative

I have a Convolutional-VAE architecture where the target images are in the range [0, 1], their pixel values. To synthesize/reconstruct images in this scale, I am using a sigmoid activation function in the output/last layer. However, a problem with sigmoid activation function is that it is almost linear in the middle and then saturates towards the ends. This means that you need a large input to have a small/negligible output in these saturated zones. As a result, extreme values are usually not predicted by the network. For example, for a given target image, if the max value = 0.998, the max predicted value = 0.688, and therefore a good amount of difference is present. A similar situation can be seen for min values between true and predicted values. Is there a way to avoid this sigmoid bias for the output layer? I am also assuming that because of this, the reconstruction loss also increases.
