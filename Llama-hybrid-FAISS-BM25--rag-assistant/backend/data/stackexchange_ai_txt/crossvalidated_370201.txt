[site]: crossvalidated
[post_id]: 370201
[parent_id]: 370178
[tags]: 
The fit produces estimates $\hat\beta = (\hat\beta_0, \hat\beta_1, \hat\beta_2)$ which, because they are functions of the random response $Y,$ are random variables. These random variables have a covariance matrix. Least-squares and maximum likelihood procedures routinely make an estimate of this covariance matrix available: let's call it $\hat V.$ OLS also reports "degrees of freedom" for $\hat V.$ Given that you selected $x$ independently of any reaction to the data or the regression results, you can treat it as constant. Thus, $\hat\beta_1 + 2\hat\beta_2 x$ is a random variable whose properties are determined by the joint distribution of $(\hat\beta_1, \hat\beta_2).$ We readily compute that this is an unbiased estimator of $\beta_1 + 2x\beta_2$ because $$\mathbb{E}(\hat\beta_1 + 2\hat\beta_2 x) = \mathbb{E}(\hat\beta_1) + 2x\mathbb{E}(\hat\beta_2) = \beta_1 + 2x\beta_2.$$ Its variance equals $$\operatorname{Var}(\hat\beta_1 + 2\hat\beta_2 x) = \operatorname{Var}(\hat\beta_1) + (2x)^2 \operatorname{Var}(\hat\beta_2) + 2x \operatorname{Cov}(\hat\beta_1, \hat\beta_2).$$ The three variance-covariance terms are estimated from the corresponding entries in $\hat V;$ namely, $\operatorname{Var}(\hat\beta_1) \approx \hat V_{\hat\beta_1,\hat\beta_2}$ and so on. Thus you can test hypotheses about $\beta_1 + 2x\beta_2$ (or indeed about any constant linear combination of the parameters) exactly as you would test hypotheses about the parameters individually; namely, compare the difference between the estimate and its hypothesized value (under the null) to its standard deviation: For least squares procedures, the test statistic for a comparison to $0$ is $$t = \frac{\hat\beta_1 + 2x\hat\beta_2 - 0}{\sqrt{\hat V_{\hat\beta_1,\hat\beta_1} + (2x)^2 \hat V_{\hat\beta_2,\hat\beta_2} + 2x \hat V_{\hat\beta_1,\hat\beta_2}}}$$ and it ought to approximately have a Student t distribution with the same degrees of freedom associated with either of $\hat\beta_1$ or $\hat\beta_2$ (which is routinely reported in software output). For maximum likelihood procedures, the same statistic is suggestively called $Z$ and it ought to approximately have a standard Normal distribution. Compute the p-value accordingly. To illustrate this procedure (and demonstrate it works), the R code below simulates from this model, carries out OLS fit and the foregoing calculations for a range of possible $x$ (although in the application it is valid to use these p-values to make decisions only about a single $x$ ), and plots the results. At the left the fit is the dark curve, in contrast to the true underlying model shown in gray. At the right a p-value of $0.05$ is plotted as a horizontal dashed line for reference. Note the zoomed horizontal ( $x$ ) scale at the right. Observe, too, how $\beta_1 + 2x\beta_2 = 0\beta_0 + \beta_1 + 2x\beta_2$ and its estimate are represented in this vector-oriented code as the array of coefficients $(0, 1, 2x)$ (stored in the contrast variable). This is possible because the software always organizes the rows and columns $\hat V$ in this order. n $df.residual # # Compute p-values for a range of possible x.0. # x.0 y
