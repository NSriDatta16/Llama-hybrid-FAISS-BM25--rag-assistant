[site]: crossvalidated
[post_id]: 312588
[parent_id]: 311903
[tags]: 
I think the simplest way to answer this is that it is easy to optimize via coordinate ascent because you only have box constraints (this is because you are solving for the unbiased version of SVM which doesn't have the constraint $\sum_i \alpha_i y_i = 0$). Thus, you can optimize for each $\alpha_i$ separately and not worry about any constraint violation (i.e., optimize for $\alpha_i \in [0,1]$). If you had the biased version, then for example SMO by Platt could be used which chooses a pair for which to optimize at each stage (and maintains the bias constraint throughout). $\alpha$ can be any value between 0 and 1, in particular for vectors on the margins (technically, if $\alpha_i$ is between 0 and 1, then $x_i$ must be on the margin). You can also solve logistic regression using coordinate descent; from an optimization perspective there are additional questions around whether there are closed-form solutions to solving for a single variable at a time, or whether optimization needs to be done to solve or approximately solve. There are numerous methods which solve using for example dual coordinate descent. Here is one such method, which also discusses solving via updates to a single variable.
