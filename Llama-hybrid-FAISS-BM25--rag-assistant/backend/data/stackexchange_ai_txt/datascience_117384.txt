[site]: datascience
[post_id]: 117384
[parent_id]: 117373
[tags]: 
It depends on the data volume you have. As far as I know, there are 2 cases to have good NLP models: Either you have plenty of data (>10 GB as a raw order of magnitude) so that you can build accurate models, even if there are special characters. Either you don't have a lot of data (~1GB or less) and you have to simplify it as much as possible, and even improve it (for instance, replace ; by ,). In other words, you compensate the quantity with quality. Keep in mind that data complexity is correlated with data quantity. The more the data is complex, the more data you need. In conclusion, if you have a lot of data, you should keep the accents as they are necessary to make differences between words, and some words in french are different with or without accents (ex: t√¢che, tache, etc.), but any model would differentiate them according to their context (cf. attention mechanism). If you don't have a lot of data, removing accents would be better, because it would reduce the vocabulary corpus, and hence improve the learning. Note: There are very good NLP spell checkers available to recover the correct spelling with accents.
