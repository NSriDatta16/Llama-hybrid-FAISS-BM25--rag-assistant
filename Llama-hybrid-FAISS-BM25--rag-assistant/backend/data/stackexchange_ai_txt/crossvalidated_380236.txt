[site]: crossvalidated
[post_id]: 380236
[parent_id]: 
[tags]: 
Adjustment for binary classification with differing proportions

My data had a different proportion of 1 (20%) and 0 (80%). I found here that we can use upsampling to get a good sensitivity. The caret package in R allows upsampling. In the link , they tried a logistic regression (see model sim_glm ) to do upsampling and specified repeats = 2 to repeat the 10-fold cross-validation 2 times. I'm not sure how it works. Does it upsample observations with $Y=1$ and create a bootstrap sample "before" doing the logistic regression during each fold of cross-validation? Why do we need repeats here? I also wonder how this would work for a random forest model. Will it create a bootstrapped upsample for every tree and then do a 10-fold cross-validation to choose the best ROC? Or it upsamples first and then creates bootstrap samples to feed the trees?
