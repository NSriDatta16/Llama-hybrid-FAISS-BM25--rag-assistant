[site]: datascience
[post_id]: 31649
[parent_id]: 31555
[tags]: 
There are a few approaches you might take to establish how and whether accuracy can be improved. Some options sketched out here in brief are to compare network results to a baseline estimator, diagnose misclassifications, apply dimensionality reduction, and network architecture troubleshooting. Without knowing much about the data, I think you could try to establish whether it is even possible to increase the accuracy of your network by comparing network performance with a baseline estimator like a decision tree or support vector machine. The bonus of using a decision tree is that an algorithm such as ID3 uses information gain to make the splits and this might give you some intuition about your data and whether it's possible to improve accuracy before sinking time into it. If you haven't already, you could do some exploratory analysis to understand how noisy the data/classes are. It may also be useful to take a closer look at the errors your network is making. A confusion matrix or classification report can help you to diagnose whether your network is struggling with (for example) one particular class or a specific area of the decision boundary. If your classes are unbalanced, you could look at re-weighting your training examples and approaches such as SMOTE etc that Benji mentions. With 70 features, the network may benefit from some dimensionality reduction. If you apply PCA to the data you can establish how many components you need to explain a reasonable amount of variance in the data, or if it is more complicated. It is possible that your network may respond better if taking the transformed data as input rather than the raw data (on that topic, have you pre-processed your data?). Finally, a broader architecture search might yield improved results - I'm assuming you may have done this already, but have you tried increasing the units in each layer or cutting out that second hidden layer? You might also try using different activation functions or a different learning rate schedule. Bengio's ' Practical recommendations for gradient-based training of deep architectures ' is worth a skim (the article is aimed mainly at deeper networks, but elements of sections 3.1, 3.2 and 4 are relevant). This answer has only touched briefly on each areas but I'm hoping there are some solid leads you can chase up.
