[site]: crossvalidated
[post_id]: 251826
[parent_id]: 200500
[tags]: 
In this thread, there is already a good amount of illuminating discussion on this subject. But let me ask you: "Alternatives to what exactly?" The damning thing about p-values is that they're forced to live between two worlds: decision theoretic inference and distribution free statistics. If you are looking for an alternative to "p I'll point out that the way we conduct modern testing in no way agrees with the theory and perspectives of Fisher and Neyman-Pearson who both contributed greatly to modern methods. Fisher's original suggestion was that scientists should qualitatively compare the $p$-value to the power of the study and draw conclusions there . I still think this is an adequate approach, which leaves the question of scientific applicability of the findings in the hands of those content experts. Now, the error we find in modern applications is in no way a fault of statistics as a science. Also at play is fishing, extrapolation, and exaggeration. Indeed, if (say) a cardiologist should lie and claim that a drug which lowers average blood pressure 0.1mmHg is "clinically significant" no statistics will ever protect us from that kind of dishonesty. We need an end to decision theoretic statistical inference. We should endeavor to think beyond the hypothesis. The growing gap between the clinical utility and hypothesis driven investigation compromises scientific integrity. The "significant" study is extremely suggestive but rarely promises any clinically meaningful findings. This is evident if we inspect the attributes of hypothesis driven inference: The null hypothesis stated is contrived, does not agree with current knowledge, and defies reason or expectation. Hypotheses may be tangential to the point the author is trying to mak. Statistics rarely align with much of the ensuing discussion in articles, with authors making far reaching claims that, for instance, their observational study has implications for public policy and outreach. Hypotheses tend to be incomplete in the sense that they do not adequately define the population of interest, and tend lead to overgeneralization To me, the alternative there is a meta-analytic approach, at least a qualitative one. All results should be rigorous vetted against other "similar" findings and differences described very carefully, especially inclusion/exclusion criteria, units or scales used for exposures/outcomes, as well as effect sizes and uncertainty intervals (which are best summarized with 95% CIs). We also need to conduct independent confirmatory trials. Many people are swayed by one seemingly significant trial, but without replication we cannot trust that the study was done ethically. Many have made scientific careers out of falsification of evidence.
