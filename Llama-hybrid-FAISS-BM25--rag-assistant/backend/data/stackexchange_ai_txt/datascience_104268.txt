[site]: datascience
[post_id]: 104268
[parent_id]: 104261
[tags]: 
XGBoost will not learn "interactions" on its own. Feature generation is often used to enhance the explanatory power of $X$ . Often $x_n - x_k$ or $x_n / x_k$ are checked and used. There are also tools for feature generation, e.g. "Featuretools" for Python. One thing you can do to find out what kind of interactions have the most explanatory power, you can fit trees with only few splits (three or so) on all the possible interactions (one interaction after another, so one shallow model per interaction) and check the prediction (e.g. MSE, MAE) for each case, such as: $$ y(x_1-x_2), y(x_1/x_2), ..., y(x_1-x_n), y(x_1/x_n),$$ $$ y(x_2-x_1), y(x_2/x_1), ..., y(x_2-x_n), y(x_2/x_n),$$ $$...$$ You could keep only those interactions which have "high" explanatory power so to avoid having a massive amount of features in the model.
