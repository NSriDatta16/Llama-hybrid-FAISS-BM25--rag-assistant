[site]: datascience
[post_id]: 117445
[parent_id]: 38613
[tags]: 
K-fold cross-validation "consists of splitting the available data into K partitions (typically K = 4 or 5), instantiating K identical models, and training each one on K – 1 partitions while evaluating on the remaining partition" [Deep Learning with Python by François Chollet]. On the other hand, flow_from_directory takes the path to a directory & generates batches of augmented data [source: TensorFlow documentation] Since the whole dataset is not accessible at the same time when using flow_from_directory , the required conditions for the K-fold cross-validation method are not met. So, K-fold cross-validation and flow_from_directory are incompatible. That being said, there might be other variants of K-fold cross-validation applying the same concept to each batch (as large as possible), but I have not heard of such an extension. Why should it not be a problem in the first place? Scenario A | small dataset: K-fold cross-validation is beneficial when one has so few data points that the validation set would end up being very small. In such cases, the whole dataset can be loaded off the disk and be divided into multiple partitions, e.g., using tf.data.Dataset.from_tensor_slices . Scenario B | large dataset: In such cases, it may not be feasible to load the whole dataset at the same time (due to insufficient memory). Generating batches of data can resolve this issue. The K-fold cross-validation may not be even advantageous here due to having a large amount of data points (unless there are many classes with a low number of data points leading to a large-size dataset, and simultaneously an insufficient number of data points per class).
