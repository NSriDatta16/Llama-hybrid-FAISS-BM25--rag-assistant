[site]: crossvalidated
[post_id]: 358837
[parent_id]: 305863
[tags]: 
Make the identity of the agent one of the features, and train on all data. Probably train on a mini-batch of eg 128 agents at a time: run through the time-series from start to finish for those 128 agents, then select a new mini-batch of agents. For each mini-batch, run a slice of say 50 timesteps, then backprop. Keep the end states from that slice, and run the next 50 timesteps, starting from those end states. Rinse and repeat till you get to the end of the timesteps, for that mini-batch of ~128 agents. By putting the identity of each agent as one of the features, you make it possible for the network to: learn from all of your data, thereby maximizing the utilization of your data, and learn the unique characteristics of each agent, so that it won't just average all of the agents together when you predict the future for a specific agent, make sure to use their corresponding agent id features, and the network will adjust the predictions accordingly. Edit: Alpo Jose wrote: ok, do I need to use one-hot encoding to make identity of agent? Ooo, that's true. There are 20,000 of them. That's kind of a lot. I think that what you might want to do is 'embed' them. Have a lookup layer, that takes in an agent id (expressed as an integer, an index), and outputs a high-dimensional vector, like something like a 50-300 length vector, probably a vector whose length approximately matches the hidden size of your LSTM. Mathematically, a lookup table, also called an 'embedding layer', is equivalent to making the agent ids into one-hot vectors, then passing through a linear (fully-connected) layer. However, the requirements in memory are much reduced for the embedding layer. In terms of what the embedding layer will learn, as you train it, the embedding layer will form some kind of latent representation of each agent. The latent representation won't likely be readable/interpretable in any way, but will allow the model to learn things like 'ok this agent, 1524, is relatively effective, but not on weekends; whereas 1526 is great every day; etc ....'. The latent dimensions of the embedding vector might actually mean stuff, but no-one ever tries to figure out what they mean (I think that would be hard/impossible). However, the high-dimensional per-agent embeddings allow the model to learn something about the behavior of each agent, and model this in the time-series predictions.
