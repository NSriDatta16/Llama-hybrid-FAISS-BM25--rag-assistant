[site]: crossvalidated
[post_id]: 482964
[parent_id]: 482905
[tags]: 
You are right to be suspicious here - I would not go on until you know what actually happens. I'm afraid we won't be able to tell you what exactly causes this, but we may be able set you on the track of possible mechanisms that can cause such results. One possibile explanation: According to the documentation of Model.fit() , validation_split will reserve the last cases (rows) for the validation set. ( shuffle shuffles the training data, but as I read the documentation that is after the validation set is split off). If your data set exhibits some internal order, this can result in particularly easy-to-predict last cases. In other words, the validation set is not representative. In that case, the model can still be overfitting (despite you seeing training error > validation set error). You say the data has structure, such as time series (unless all time points of a time series are the features of a single case/row) and different countries. I'd treat these factors as potentially important confounders can add correlation between rows (clusters). Until you've shown that no such clustering occurs, you basically have only two options: Either decided to specify that the model should not be used outside the calibration range of e.g. the countries you have good data for, or do your train/validation/test splits so that you predict unknown countries (as well as e.g. unknown time series)
