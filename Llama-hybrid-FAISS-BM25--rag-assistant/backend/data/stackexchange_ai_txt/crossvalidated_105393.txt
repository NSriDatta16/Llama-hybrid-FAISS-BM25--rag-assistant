[site]: crossvalidated
[post_id]: 105393
[parent_id]: 
[tags]: 
Comparing regression outputs for different response variables

I have inherited some legacy code designed to solve the following problem - we are given a set of observable binary variables $Y_1, \dots , Y_N$ that we believe are related to a fixed set of input variables $X_1 ,\dots , X_M$ (N.B. high dimension, $M \sim 100$). For a given input we want to predict which event $Y_i$ is the most likely. The current program tries to predict this as follows - given training data, we build a model for each $Y_i$ seperately (using logistic regression + feature selection). Then given an input we look at all of the logit scores and choose the highest. This doesn't work because some of the logistic regression classifiers can be overtrained, which leads to extreme logit scores which often dominate the other scores. Analysis reveals that the logistic function may not be a good choice either. I have two questions : is this way of finding the most likely event sensible / salvagable? I would rather not have to rebuild this whole thing from scratch if possible. if so, is there a more sensible way to compare the scores from two classifiers in such a way that it is more fair? My thoughts are that we could compare some sort of Bayesian posteriori to get a fair evaluation for our state of knowledge, but I feel like if I were to do this I would have to lose the logistic regression model, and I am not sure what I should replace it with.
