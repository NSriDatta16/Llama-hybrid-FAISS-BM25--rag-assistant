[site]: crossvalidated
[post_id]: 2188
[parent_id]: 490
[tags]: 
I have a slight preference for Random Forests by Leo Breiman & Adele Cutleer for several reasons: it allows to cope with categorical and continuous predictors, as well as unbalanced class sample size; as an ensemble/embedded method, cross-validation is embedded and allows to estimate a generalization error; it is relatively insensible to its tuning parameters (% of variables selected for growing a tree, # of trees built); it provides an original measure of variable importance and is able to uncover complex interactions between variables (although this may lead to hard to read results). Some authors argued that it performed as well as penalized SVM or Gradient Boosting Machines (see, e.g. Cutler et al., 2009, for the latter point). A complete coverage of its applications or advantages may be off the topic, so I suggest the Elements of Statistical Learning from Hastie et al. (chap. 15) and Sayes et al. (2007) for further readings. Last but not least, it has a nice implementation in R, with the randomForest package. Other R packages also extend or use it, e.g. party and caret . References: Cutler, A., Cutler, D.R., and Stevens, J.R. (2009). Tree-Based Methods, in High-Dimensional Data Analysis in Cancer Research , Li, X. and Xu, R. (eds.), pp. 83-101, Springer. Saeys, Y., Inza, I., and Larra√±aga, P. (2007). A review of feature selection techniques in bioinformatics. Bioinformatics , 23(19) : 2507-2517.
