[site]: crossvalidated
[post_id]: 273190
[parent_id]: 273189
[tags]: 
Weight decay specifies regularization in the neural network. During training, a regularization term is added to the network's loss to compute the backpropagation gradient. The weight decay value determines how dominant this regularization term will be in the gradient computation. As a rule of thumb, the more training examples you have, the weaker this term should be. The more parameters you have the higher this term should be. So, Weight decay is a regularization term that penalizes big weights. When the weight decay coefficient is big, the penalty for big weights is also big, when it is small weights can freely grow. So, now if you go back to reading the answer which you linked in your question, it would make complete sense now.
