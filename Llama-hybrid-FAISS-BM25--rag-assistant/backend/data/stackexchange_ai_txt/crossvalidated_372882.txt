[site]: crossvalidated
[post_id]: 372882
[parent_id]: 348959
[tags]: 
It does basicly the same. It penalizes the weights upon training depending on your choice of the LightGBM L2-regularization parameter 'lambda_l2' , aiming to avoid any of the weights booming up to a level that can cause overfitting, suppressing the variance of the model. Regularization term again is simply the sum of the Frobenius norm of weights over all samples multiplied by the regularization parameter lambda and divided by the number of samples. You add this to the cost function of the machine learning algorithm that you work on just like linear regression. If you want to have a mathematical landscape specificly for LightGBM, you can refer to the LightGBM paper published at Conference on Neural Information Processing Systems (NIPS) 2017 to see the cost function which Microsoft Team have designed for the boosting algorithm: https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf Just imagine or express manually that you add the regularization term to the cost function just like it is done on the cost function of the linear regression.
