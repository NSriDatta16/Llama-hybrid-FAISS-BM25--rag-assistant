[site]: crossvalidated
[post_id]: 389091
[parent_id]: 
[tags]: 
How to understand "multimodal" RNNs for image captioning?

This paper Deep Visual-Semantic Alignments for Generating Image Descriptions on image captioning proposed a Multimodal Recurrent Neural Network architecture. From my understanding, the multimodal RNN is essentially a language model conditioned on a given image vector. Somehow I'm confused what "multimodal" refers to in that paper, how do I interpret this? Does it mean that since it is conditioned on an image vector, the language model becomes multimodal?
