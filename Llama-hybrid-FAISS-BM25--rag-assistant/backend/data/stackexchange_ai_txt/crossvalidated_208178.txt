[site]: crossvalidated
[post_id]: 208178
[parent_id]: 
[tags]: 
Reinforcement Learning in context of trajectory planning

Are there RL methods that allow for planning under variable objectives? Suppose I have a robot and want to find a policy $\pi$ that takes in the robot's current state $s_t$ and a goal state $s_{\text{goal}}$, and computes the control sequence $u$ to realize the goal state. $u \sim \pi(\cdot|s_t, s_\text{goal})$ I'm aware of methods like iLQG and AICO, which are fast enough to work in real-time but they require robust dynamics models. Would policy gradients work here, or is the potential sample space too large? It seems like changing the goal specifies an entirely new MDP.
