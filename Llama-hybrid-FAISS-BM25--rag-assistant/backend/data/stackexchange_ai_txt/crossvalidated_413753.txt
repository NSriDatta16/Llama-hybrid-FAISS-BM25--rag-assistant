[site]: crossvalidated
[post_id]: 413753
[parent_id]: 
[tags]: 
Using bagging and random forests together

I was looking at a kernel implementation (for text classification) and the following piece of code got me a little bit confused (I removed part of the features - in order to keep it light - as most of them are similar - e.g. number of negative, positive, neutral words): pipeline_ = Pipeline([ ('fu', FeatureUnion([ ('tfdif_features', Pipeline([ ('cv', CountVectorizer()), ('tfidf', TfidfTransformer()), ('tfidf_', Wrapper(RandomForestClassifier())), ])), ('nb_pos_features', Pipeline([ ('nb_pos', NumberSelector('posWords') ), ('nb_pos_', Wrapper(RandomForestClassifier())), ])), ])), ('xgb', XGBClassifier()), ]) At first I thought that it looks like bagging, as for each feature a base model is created, followed by boosting. But isn't the Random Forest in this case a Decision Tree? Why using Random Forest and not something else (e.g. Logistic regression)? Any clarification will be greatly appreciated.
