[site]: crossvalidated
[post_id]: 283261
[parent_id]: 
[tags]: 
Why does overlapped pooling help reduce overfitting in conv nets?

In the seminal paper on ImageNet classification with deep conv nets by Krizhevsky et al., 2012 , the authors talk about overlapped pooling in convolutional neural networks, in Section 3.4. Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z Ã— z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit. What is the intuition that overlapped pooling helps reduce over-fitting in conv-nets?
