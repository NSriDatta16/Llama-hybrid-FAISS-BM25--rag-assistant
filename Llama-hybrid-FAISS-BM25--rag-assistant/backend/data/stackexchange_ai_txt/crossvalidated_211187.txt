[site]: crossvalidated
[post_id]: 211187
[parent_id]: 211178
[tags]: 
rudra you are totally correct. That's why you need a non linear classifier : effectively to implement multiple separate templates to approximate translation invariance (see convolutional NNs). In addition you typically look at smaller features (the receptive field in CNN vocabulary) to deal with both translation rotation and occlusion effects. linear classifiers such as SVMs are used with 'bag of words' with eg sift feature vectors ( again splitting the picture into eg quadrants and calculating the feature vectors in those quadrants)
