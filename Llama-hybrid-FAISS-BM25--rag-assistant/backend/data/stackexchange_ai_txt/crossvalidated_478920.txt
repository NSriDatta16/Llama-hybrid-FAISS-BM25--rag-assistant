[site]: crossvalidated
[post_id]: 478920
[parent_id]: 476275
[tags]: 
Bias in a model can be due to many factors. From what I understand, you maybe talking about sampling bias. It Happens when the training data doesnâ€™t accurately represent the environment the program is expected to run into. It is not possible to train a model on the entire universe of data, rather data is assumed to be sampled from the population. There is science behind sampling the training data that is both large enough and representative enough to mitigate sample bias and must come from the same population density. In your case, the data seems to be from different population densities which contradicts the assumption of machine learning theory i.e training data should be sampled from the same population. and hence will never generalise. The machine learning theory is based on the assumption that there exists a true function g(x) (which we don't need to know) and our goal is to come up with a hypothesis f(x) which approximates g(x) on the sampled dataset. Bias and variance in error decomposition both are dependent on your choice of hypothesis f(x) and the true function g(x) of the population. Bias occurs when your hypothesis is far from the true function. Even if you have representative data and not using the right hypothesis, you will have high bias. For ex: if you are using linear hypothesis to model a non-linear function. Variance occurs when you have large pool of hypothesis(high model complexity) which makes it difficult to navigate to the true function resulting in higher variance. For ex: using a polynomial regression to model a linear relationship. For a deeper understanding check out: https://www.youtube.com/watch?v=L_0efNkdGMc This is one of the best resource for machine learning theory.
