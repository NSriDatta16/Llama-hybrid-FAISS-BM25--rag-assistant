[site]: datascience
[post_id]: 39882
[parent_id]: 
[tags]: 
How can I perform backpropagation directly in matrix form?

I had made a neural network library a few months ago, and I wasn't too familiar with matrices. So, instead of performing matrix dot products (between weights and inputs, then adding a bias matrix), I simply looped through each array element (for example, I looped through every weight), and as one would expect, it's slow. I recently rewrote the library and employed matrix dot products (with numpy) for forward propagation, and it is way faster than the original library. Original Forward Propagation Code: x = 0 while x Newer Forward Propagation Code: ip = np.asarray(ip) ip = np.reshape(ip, (len(ip), 1)) for x in range(len(self.struct) - 1): if x == 0: food = np.dot(self.weights[x], ip) else: food = np.dot(self.weights[x], food) if backprop: self.logz[x] = np.add(food, self.biases[x]) np.reshape(self.logz[x], (1, len(self.logz[x]))) food = activation(np.add(food, self.biases[x]), self.actfunc[x]) if backprop == True: return food else: return np.reshape(food, (1, len(food))).tolist()[0] Now that forward propagation works comparatively well, I'm trying to attempt the same for backpropagation. I need to do this because the backpropagation code takes an unacceptably long time to generate gradients for networks that have many neurons in their layers. Here's my current backpropagation code: netop = self.feedForward(ip, True) output = np.asarray(output) output = np.reshape(output, (len(output), 1)) delzdela = [None] * (len(self.struct) - 1) weightgrad = [None] * (len(self.struct) - 1) biasgrad = [None] * (len(self.struct) - 1) for x in range(len(self.weights)): # Loop to calculate gradients of neurons of hidden layers weightgrad[x] = np.zeros((self.struct[x + 1], self.struct[x]), dtype=np.float64) biasgrad[x] = np.zeros((self.struct[x + 1], 1), dtype=np.float64) for x in range(len(self.struct) - 1): if x == 0: delzdela[-(x + 1)] = np.dot(2, np.subtract(netop, output)) else: delzdela[-(x + 1)] = np.zeros((self.struct[-(x + 1)], 1)) for y in range(self.struct[-(x + 1)]): for z in range(self.struct[-x]): delzdela[-(x + 1)][y] += derivative(self.logz[-x][z], self.actfunc[-x]) * delzdela[-x][z] * self.weights[-x][z][y] for x in range(len(self.struct) - 1): # Calculating gradients of weights for y in range(self.struct[x]): for z in range(self.struct[x + 1]): if x == 0: weightgrad[x][z][y] = ip[y] * derivative(self.logz[x][z], self.actfunc[x]) * delzdela[x][z] else: weightgrad[x][z][y] = activation(self.logz[x - 1][z], self.actfunc[x - 1]) * derivative(self.logz[x][z], self.actfunc[x]) * delzdela[x][z] for x in range(len(self.struct) - 1): # Calculating gradients of biases for y in range(self.struct[x + 1]): biasgrad[x][y] = derivative(self.logz[x][y], self.actfunc[x]) * delzdela[x][y] return [weightgrad, biasgrad] This code takes a training example ( ip ) and its label ( output ) and returns the gradient for this single training example. As you can see, I am looping through each index and dimension of the arrays, which is very slow. My question is how I can directly use matrix operations to perform backpropagation, instead of looping through every index and performing operations on individual numbers.
