[site]: crossvalidated
[post_id]: 207820
[parent_id]: 207766
[tags]: 
What you are discussing sounds more like nearest neighbor classification to me, i.e. assign every point to the nearest "training" object. Essentially, you are describing classification, not clustering. You appear to be doing only "half an iteration". A full iteration would also update the cluster centroids. The task of clustering is to discover structure in the data - in the case of k-means the structure are the centroids. In other algorithms it is usually the set of points that makes up the cluster. Now to the "helipad" example. K-means does not minimize average distances . It minimizes the average squared distance. For a purely cost-driven approach that is worse, centroid-linkage is maybe better. For the helipad emergency it's better to have fewer extreme distance; but one may want to optimize the maximum instead. That is easier to formuate with hierarchial clustering, e.g. using minimax linkage. Furthermore, k-means may be placing the centers in the ocean, or in a lake... (this does not happen with minimax) if you had to connect all point with a point-to-point radio network, single-linkage is meaningful, because it is the minimum spanning tree. And Ward linkage optimizes squared deviations, just like k-means. This shows a key benefit of hierarchical clustering: it's flexibility. It allows both different objectives (linkages) and arbitrary distance functions (e.g. haversine distance). K-means does neither.
