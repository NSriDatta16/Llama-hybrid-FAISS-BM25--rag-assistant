[site]: crossvalidated
[post_id]: 431254
[parent_id]: 
[tags]: 
Enforcing constraints on weight matrices using ReLU activation

In the paper ' A Deep Non-Negative Matrix Factorization Neural Network ' by Flunner and Hunter, proof of Theorem 1 says that "The ReLu Activation function is a standard approximation of a non-negative constraint", though no proof of such an argument is given, but I can't find such a claim anywhere, neither the experiments that I have run, shows confirmatory results. So, is the above argument in the paper true?
