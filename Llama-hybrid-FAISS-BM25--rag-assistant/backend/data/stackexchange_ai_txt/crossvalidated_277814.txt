[site]: crossvalidated
[post_id]: 277814
[parent_id]: 
[tags]: 
Using *EDIT* non-informative priors to account for perfect separation in logistic regression?

In generating logistic regressions for treatment-survival data, perfect separation is a problem in a few of my data sets. I've decided to use a Bayesian approach to account for the perfect separation Sources: (Gelman 2008) and this question . This approach also allows me to easily make predictions from the regression using a developed function in R. My understanding of this Bayesian approach is limited. I can set the prior df to an arbitrary value, increasing it until I don't have a problem fitting the model due to perfect separation, but I don't understand what this regularization is effectively doing. From my understanding, using informative priors assumes I know something about the data and I'm including it as a starting point for model fitting. A weakly informative prior doesn't supply any controversial information but can pull data away from incorrect estimations. I think the latter is more aligned with my proposed method ( Source ). I've read this but I do not have any prior information on which to base prior distributions or df. I know this is an oversimplified understanding which leads to my question: What information am I providing to the model when I arbitrarily set the prior df (in R prior.df) at a level that allows for the model to converge?
