[site]: crossvalidated
[post_id]: 438399
[parent_id]: 304822
[tags]: 
It depends on your requirements and your data. What do you mean by similarity? Do you mean similarity by word distribution or categorical similarity? I thought you have some labeled data but I am not sure what kind of labels you have. Let's say you have lots of documents from different categories and we can treat all pairs from the same categories as positive pairs and from different categories as negative pairs, then we can be sure that the distance between different categories will be enlarged and hence it can help for document similarity search. You mentioned your encoder, two layers of CNN, but not its structure. Most probably your model suffers from the high bias problem because it performs worse even than TFIDF. I would recommend you check the training loss to see if it really gets stuck by training cases. If so you can use the now popular BERT and use its pretrained weights( transfer learning for regularization ). A two layer CNN model would be expressive enough for the task with your size of data depending on the filters and kernel size you use. You can use reduce the regularization by reduce the dropout rate and etc to make the bias smaller. Or maybe the variance would be large and try to fix that. The safe way is to train your model using two loss functions(multi-task learning) and interleave the two training process by setting a condition according to each other's loss. That's you combine the original loss(task) and the triplet loss to make sure that you can make some gains from the triplet loss. You can design the strategy.
