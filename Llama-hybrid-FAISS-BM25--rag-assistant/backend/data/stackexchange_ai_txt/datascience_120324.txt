[site]: datascience
[post_id]: 120324
[parent_id]: 120224
[tags]: 
It might be useful to frame this problem using common terminology. Hashing is mapping an object, a string in this case, to an integer. What you want are collisions (i.e., similar objects are mapped to the same integer bucket). The goal is to pick a hashing function that does that based on the number of shared letters in the string. A scalable implementation of this idea is MinHash and locality-sensitive hashing (LSH). Here is a rough version using Python's datasketch library: from datasketch import MinHash, MinHashLSH str1 = 'some random string one' str2 = 'some rzndom string one' str3 = 'some rndom string one' str4 = 'a very different string' strings = [str1, str2, str3, str4] # Hash each string, letter-by-letter hashes = [] for s in strings: m = MinHash(num_perm=128) for c in s: m.update(c.encode('utf8')) hashes.append(m) # Create LSH storage lsh = MinHashLSH(threshold=0.8, num_perm=128) for n, hash_value in enumerate(hashes, 1): lsh.insert(f"str{n}", hash_value) # Test that the queries for the hash values return expected neighbors hash_str1 = hashes[0] assert set(lsh.query(hash_str1)) == set(['str1', 'str2', 'str3']) hash_str4 = hashes[3] assert set(lsh.query(hash_str4)) == set(['str4'])
