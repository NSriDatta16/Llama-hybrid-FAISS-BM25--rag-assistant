[site]: datascience
[post_id]: 99579
[parent_id]: 
[tags]: 
Predictions for classes on which the DNN was not trained yet - is that possible?

my data is of multi-class, multi-label type, and I plan to have 100 output classes in total. My input X to the model is audio data, my y is a one-hot encoded numpy array with 100 columns showing a 1 to indicate the respective class (e.g. y = [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 ... ] model = Sequential() model.add(...) # more layers ... CNN ... model.add(...) # more layers ... LSTM ... model.add(Dense(512, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu')) model.add(Dense(100, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam') At the moment, I only have audio files belonging to only 12 (of the planned 100) classes. Which means that 88 columns of y are not assigned any 1 currently. Then after training with the current 12-classes data (NSize ~ 16000), I run model.predict(...) and get probabilities for almost all of the 100 columns, some of them quite high percentages. Is this possible that a model outputs quite high prediction probabilities for classes which it never received as input? Any suggestions to fix that? (I can almost 100% exclude an error on the one-hot encoding of y ) Kind regards, ziggyler
