[site]: datascience
[post_id]: 18813
[parent_id]: 18778
[tags]: 
The question 'is this a good model?' is essentially a business question. There is always a precision/recall tradeoff and you decide based on your business goal what model to choose. Data science can help you with the question on how to compare classifiers. Let us start, that each (or nearly each) classifier can predict not only the outcome (Class2 or Class3), but also the probability of this outcome. Your confusion matrix and all your metrics are suitable for making business decision, but, from data science point of view, have two problems 1) they do not take into account the agreement by chance 2) they are based on the threshold probability 0.5, which is not necessarily the optimal probability for deciding about the class. To deal with (1), have a look at Cohen's kappa To deal with (2), use metrics based on probability. The simpler one is Area Under the ROC Curve , see also here . Even finer, but not so straightforward to interpret is Logarithmic Loss Is recall depend on training data? How do I improve recall for class 2 along with accuracy? Recall depends on the quality of your model (and thus also on the quality of your training data) and on the chosen probability threshold. By decreasing probability threshold, you improve recall but worsen precision. Which tradeoff to choose is again a business decision.
