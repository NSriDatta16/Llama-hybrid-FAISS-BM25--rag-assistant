[site]: crossvalidated
[post_id]: 631761
[parent_id]: 
[tags]: 
Understanding of Gamma distribution as precision prior in Bayesian inference for Gaussian

Christopher M. Bishop in his book "Pattern Recognition and Machine Learning" nicely explains where does Student t-distribution $St(x|\mu,\lambda,\upsilon)$ originate into. In Chapter 2, it is derived as infinite mixture of Gaussians having the same mean but different precisions ( $\lambda=\sigma^{-2}$ ), which are following $Gamma(\lambda|a,b)$ distribution. The latter is introduced as a conjugate prior for the precision of the Gaussian variable with known mean. The Student's degrees of freedom $\upsilon$ is connected to the shape parameter $a$ as $\upsilon=2a$ . As I understand, we can look at this picture from an intuitive perspective. One could draw samples from Gaussians with different precisions which follow the Gamma distribution. The result would converge to the Student's distribution.(see https://www.sumsar.net/blog/2013/12/t-as-a-mixture-of-normals/ for the demonstration). Now, it is interesting to think about $\upsilon=2a$ . Essentially, $a$ reflects the number of points in prior observations. Bishop elaborates on this by calculating the posterior distribution of precision based on likelyhood $p(X_N|\lambda)$ of $N$ observed data points. $p(\lambda|X_N) = p(X_N|\lambda)*Gamma(\lambda|a,b)$ and it is also $Gamma(\lambda|a_N,b_N)$ where $a_N = a + N/2$ and $b_N = b + N/2 * \sigma^2_{ML}$ . So $a_N$ and $b_N$ are $N$ dependent. From sequential learning perspective, this posterior can be taken as a prior with $N$ prior observations. Now let's go back to the sampling from the Gaussians with different precisions to construct the total Student distribution. Intuitively, the larger the number of prior observations $N$ , the smaller should be the uncertainty of precision choice. However, plotting $Gamma(\lambda|a_N,b_N)$ one may see that the curve gets wider and is pushed towards larger $\lambda$ . How do I interpret this? Naively, it seems that larger $a_{N}$ and $b_{N}$ lead to wider prior distribution, increasing $\tau$ uncertainty, the opposite from the intuitive expectations?
