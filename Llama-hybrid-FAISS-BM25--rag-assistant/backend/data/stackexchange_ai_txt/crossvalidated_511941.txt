[site]: crossvalidated
[post_id]: 511941
[parent_id]: 
[tags]: 
Neural network overfitting from more samples

I am fitting a neural network model for a text classification task, the issue is that as I introduce more samples from the dataset I start to see more overfitting. This charts show train/val accuracy scores (same occurs with other metrics), the two train runs you can see improving, but the two validation runs differ, in the validation run which shows less overfitting I am giving the model the same small sample from the dataset around 3200 examples, which I repeat on each epoch, in the run which shows more severe overfitting, I am giving the model new samples each epoch. So to summarise, when showing the model more of the full dataset the overfitting becomes worse, which is the opposite of what I would expect. My question would be; under which conditions would this occur, and what would be some methods to alleviate this issue? Thanks in advance! EDIT: Just adding some information from the discussion in the comments: the classes are imbalanced (80/20), but I am sampling them to ensure an even split. This is happening with all metrics; BCE, Recall, AUC...
