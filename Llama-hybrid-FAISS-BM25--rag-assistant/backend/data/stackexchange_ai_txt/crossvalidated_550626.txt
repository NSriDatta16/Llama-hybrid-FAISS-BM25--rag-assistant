[site]: crossvalidated
[post_id]: 550626
[parent_id]: 550619
[tags]: 
Interpretation of the Mean When we say that the average value spent on meals was 50 USD - it means that if we take the total amount spent on fast foods and equally divide the sum among all the people who made the purchase - each person would get 50 USD. However, this number hides a lot of information. We can get an average of 50 USD in a lot of different situations. One extreme is when everyone spends exactly 50 USD. Another extreme is when half the people spend 0 USD and another half spend 100 USD. And there are infinite number of situations in between that would give us a mean value of 50. Average Deviation Hence, we are interested in the variability of those amounts. One intuitive way to quantify how much variability there is is to calculate the average deviation from that mean value. So when we know the mean value, for each person we can calculate the difference between their spent amount and the mean value, and get the average of that: $$MAD = \frac{\sum_i | x_i - \bar{x} |}{n}$$ This is "Mean Absolute Deviation" (MAD). It answers the question: among the customers - what is the average difference between their purchase and the average? We can check what this score would be in the two extreme scenarios. If every purchase was equal to 50 USD then the average would be 50, and the MAD would be 0. And if half of the purchases were 0 and another half 100 then the mean would be 50 and the MAD would be 50. Standard Deviation Standard deviation is a variant of the MAD, but it's harder to interpret. Note that when we look for averages differences from the mean in MAD calculation - we take the absolute value. We want to get rid of the sign, because otherwise roughly half the deviations will be negative and half - positive, and so they would cancel out. Standard deviation, instead of taking the absolute value, uses the square, which, just like absolute value, transforms negative numbers into positive. And then transforms-back by taking the square root: $$SD = \sqrt{\frac{\sum_i ( x_i - \bar{x} )^2}{n}}$$ The idea is the same. It is harder to interpret, but it has some nice properties. The reasons why standard deviation is used more often are discussed here: Why square the difference instead of taking the absolute value in standard deviation? Questions Does SD show that, on average, people spend between USD 43 and USD 57 on meals? No. What would "on average people spend between X and Y" mean, exactly? Average is a point estimate, not a range. If the amount spent followed a normal distribution that we could derive that around 68% of customers spent between 43 and 57 USD. However, dollar amounts certainly do not follow normal distributions (i.e. they don't have negative values). Or, on average, they spend USD 50 per purchase but their purchase amount has a relatively large standard deviation of USD 7? This is correct. But does this answer your question? It restates that the average was 50, and SD was 7. And only adds some external interpretation that 7 is relatively large. Or, on average, they spend USD 50 per purchase but my estimate has a relatively large standard deviation of USD 7? No, there is a separate measure for that, called Standard Error.
