[site]: crossvalidated
[post_id]: 325151
[parent_id]: 324857
[tags]: 
I came up with one possible interpretation of these symbols that seem to be consistent with all the material I've seen so far in these courses. As pointed out in Dennis Soemers' answer, Sutton's book define $S_t$ as: state at time $t$, typically due, stochastically, to $S_{t−1}$ and $A_{t−1}$ The "typically due" led me to think that perhaps $S_t$ doesn't refer to the same thing in all contexts. Maybe, $S_t$ in one context is actually a random variable, with a distribution that depends on $S_{t-1}$ and $A_{t-1}$. Then, in another context (or in the same context but for other $S_i$, $i\neq j$), it is a deterministic quantity, already specified for us. Which leads me to interpret $S_t$ as simply a placeholder symbol. With this I mean that the symbol $S_t$ doesn't mean much without a context. The context then specifies if that state is supposed to be random, with a distribution conditioned on the state and action at the previous time step, or if it is a deterministic value. Furthermore, given a context, it's not necessary for all $S_i$ to share the same nature, some can be random and some can be deterministic (although the case where more than one state in the chain is deterministic is probably meaningless$^{[1] }$). Hence, when we write things like $$ Q_\pi(s,a)=\mathbb E_\pi[G_t | S_t=s, A_t=a] ~,$$ we mean that $S_t$ and $A_t$ are not random (not even random variables that we have observed their values). Instead, they are simply deterministic values given by us. However, the subscript $\pi$ in the expectation is then the context that defines all the $S_i$ and $A_i$ for $i>t$ to be random, with distributions that depend on the previous state and action, or the policy $\pi$ and the previous state, respectively. Whether or not the subscript $\pi$ also determines the nature of all the $S_i$, $A_i$ for $i This previous equation is in some sense asking: "What is the expected value of $G_t$, in the context that we start at state $s$, take the action $a$, and from that point onward we sample states and returns from the MDP model we defined somewhere else, and sample actions from our policy $\pi$. I think it's important to repeat that in that equation $S_t$ and $A_t$ are not (at least according to what I could come up as a sensible interpretation of this notation) random variables! They are not random variables that we have somehow observed their values. Otherwise, this equation would suffer from the problem I described earlier: if policy $\pi$ never chooses action $a$, or never reaches state $s$, then we would be conditioning on impossible events. Instead, they're deterministic quantities. I would even go as far as advocate towards using the notation: $$ Q_\pi(s,a)=\mathbb E_\pi[G_t] \quad,\quad S_t=s \text{ and } A_t=a$$ instead, to clear up any possible confusion that these variables might be random. [1]: Note that this is fundamentally different from the case where these states are all random, but we have observed the value of them.
