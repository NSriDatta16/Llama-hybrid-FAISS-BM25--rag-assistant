[site]: crossvalidated
[post_id]: 365224
[parent_id]: 
[tags]: 
Cross Validation and Confidence Interval of the True Error

I'm interested in the relation between Cross Validation and the True Error Estimation of a Classifier (Chapter 5 - Machine Learning - Mitchell). Suppose we have 150 examples, I decide to use a 100 repeated 5-fold Cross Validation to understand the behavior of my classifier. At this point I have $100 \times 5$ results and I can use the mean and std dev of the error rates to estimate the variance of the model performance. $$\text{mean(Error Rate)} \pm z \times \text{std(Error Rate)}$$ I could estimate Confidence Interval of the True Error (that I would obtain on the unseen data) using the the average Error rate: $$\text{mean(Error Rate)} \pm z \times \sqrt{ \frac{mean(errorRate) \times (1 - mean(errorRate) )}{ N}}$$ Two questions: Is this approach correct? Is correct to set $N=150$ in the second equation or I should use the average number of Test Data used as Test Set in each fold of CV ($N = 30$)?
