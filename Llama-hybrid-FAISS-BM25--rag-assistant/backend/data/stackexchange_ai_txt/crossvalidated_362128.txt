[site]: crossvalidated
[post_id]: 362128
[parent_id]: 
[tags]: 
Time-series prediction with RNNs: What to expect from the learning process?

When training an RNN for time series prediction, what can one expect to see visually as the model learns? In particular, are plateaus a normal indication that the model is underfitting or do they represent a more fundamental problem with the algorithm? In my case, I am training a Keras RNN to predict a (partly noisy) time series using 30000 training examples (namely, sliding windows). The result looked like this on the training set and like this on the testing set To me, the above looks promising: The prediction on the training set is predictably tighter than that on the testing set, even if the latter still manages to capture the overall trend. However, if I only train on 300 training samples , the training set looks like this and the testing looks like . I would expect that because I only trained on 300 samples instead of 300000, the performance would be worse. This is indeed the case. But, how about the plateaus? Can I conclude that they arise as a direct consequence of the fact that there aren't enough training samples? I.e., that the RNN, for lack of data, just picks the previous value as the predicted value on certain segments of the test sets? PS: I noticed similar plateaus when I drastically decrease the number of hidden layers. Once again, it seems to make sense since that implies the model is heavily biased. It's just that the plateaus show up at seemingly random segments of the test set---and I can't really make sense of that.
