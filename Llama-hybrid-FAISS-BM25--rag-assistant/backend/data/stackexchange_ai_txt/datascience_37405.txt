[site]: datascience
[post_id]: 37405
[parent_id]: 28253
[tags]: 
The entire philosophy of Distributed Word Representations makes use of the fact that a word is understood by the context it has . When we say context , we mean the words that come in the neighborhood of a particular word . Now context in natural language is a tricky thing . As an example . the words open and create are two words that are not so similar semantically . But think of the scenario of a Bank related corpus , you will frequently see statements like How do I open a new account & How do I create a new account In the context of such a document , open and create become similar words. That is why context specific word vectors are needed. Now , when it comes to which is the better option between the two Use pre-trained vectors Use custom vectors , it depends on how much data do you have for your custom use case . If you have enough data then it's always safe/better to go for a custom vectorization as it will be very specific to the context the corpus has . But in all other cases , you can use the pre trained embeddings. These generalize quite well on a variety of docs.
