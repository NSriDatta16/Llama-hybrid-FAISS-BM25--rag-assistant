[site]: crossvalidated
[post_id]: 411919
[parent_id]: 411736
[tags]: 
It is only an efficiency issue. In theory, the attention mechanism can work with arbitrarily long sequences. The reason is that batches must be padded to the same length. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few. By this sentence they mean they want to avoid batches like this: A B C D E F G H I K L M N O P Q _ _ _ _ _ _ _ _ _ _ _ _ R S T U _ _ _ _ _ _ _ _ _ _ V W _ _ _ _ _ _ _ _ _ _ _ _ Because of one long sequence, most of the memory is wasted for padding and not used from weights update. A common strategy to avoid this problem (not included in the tutorial) is bucketing , i.e., having batches with an approximately constant number of words, but a different number of sequences in each batch, so the memory is used efficiently.
