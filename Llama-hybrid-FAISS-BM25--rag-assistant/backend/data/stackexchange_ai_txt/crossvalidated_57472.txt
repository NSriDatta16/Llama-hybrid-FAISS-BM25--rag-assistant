[site]: crossvalidated
[post_id]: 57472
[parent_id]: 57470
[tags]: 
Nuisance parameters are typically introduced to account for extra variation in the model. Typically, the amount of variation in the data accounted for by your parameters of interest is compared to the amount of unaccounted for variation (Residual Error). By reducing the amount of unaccounted for variation, you become better able to detect effects of your parameters of interest. As an example, the F-statistic is essentially the amount of variation explained by each of your parameters on average divided by the amount of variation a total junk parameter would explain by chance. This junk variation is derived by dividing the Residual Error by the unused degrees of freedom in the model. As you can see, by making the Residual Error smaller, you reduce the size of the denominator, which serves to increase the size of the F-statistic. This increases the chance that you will find an effect of your parameters of interest. To generate a real-world example: I'm administering painful heat to people at different temperatures in the context of different treatments for pain-relief and I'm measuring how much pain people report feeling. In this case, I would want to include the temperature of my stimulations as a nuisance covariate when examining the effects of the analgesics. This isn't because I'm wondering if higher temperatures would lead to higher pain - I know they would. That's exactly why I would like to account for that variation when I'm trying to compare the effects of the different treatments, thus giving me a better chance of detecting differences due to treatment.
