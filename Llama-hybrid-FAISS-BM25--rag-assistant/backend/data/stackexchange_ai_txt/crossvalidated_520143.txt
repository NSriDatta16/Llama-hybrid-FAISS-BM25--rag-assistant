[site]: crossvalidated
[post_id]: 520143
[parent_id]: 
[tags]: 
Estimating ability with IRT when item parameters are known using R package mirt

I have a paper which has utilised IRT on a revised scale of paranoia (R-GPTS). The scale comprises 10 items and there are 5 response options: In the above highlighted responses, this scores 3 / 40. The item parameters are for the persecution subscale: I am trying to get an ability estimate but I can tell that I'm missing something. This is the IRT model the authors used: A two-parameter graded response model (GRM) was fitted to the items (Samejima, 1969). These are the packages they used: All analyses were conducted in R, version 3.5 (R Core Team, 2013). Packages used included ‘psych’ >(Revelle, 2018), ‘mirt’ (Chalmers, 2012), ‘pROC’ (Robin et al., 2011), and ‘optimumCutpoints’ (Lopez-Raton et al., 2014). In Estimating ability using IRT when the model parameters are known there is a response by philchalmers who wrote mirt , the very package used in the study. Taking his code I have come up with the following: #generates set of random response patterns for 10 items dat $value[sv$ name == 'a'] $value[sv$ name == 'b1'] $value[sv$ name == 'b2'] $value[sv$ name == 'b3'] $value[sv$ name == 'b4'] Then, I use fscores() for ability estimate: # response pattern #1 fscores(mod,response.pattern=c(1,0,1,0,0,0,0,0,1,0)) # response #2 fscores(mod,response.pattern=c(4,0,4,4,3,3,4,4,4,4)) # response #3 fscores(mod,response.pattern=c(4,0,4,4,4,4,4,4,4,4)) The output I get is as follows: > # response pattern #1 > fscores(mod,response.pattern=c(1,0,1,0,0,0,0,0,1,0)) Item1 Item2 Item3 Item4 Item5 Item6 Item7 Item8 Item9 Item10 F1 SE_F1 [1,] 1 0 1 0 0 0 0 0 1 0 -2.078653 0.5700438 > > # response #2 > fscores(mod,response.pattern=c(4,0,4,4,3,3,4,4,4,4)) Item1 Item2 Item3 Item4 Item5 Item6 Item7 Item8 Item9 Item10 F1 SE_F1 [1,] 4 0 4 4 3 3 4 4 4 4 1.856076 0.5946364 > > # response #3 > fscores(mod,response.pattern=c(4,0,4,4,4,4,4,4,4,4)) Item1 Item2 Item3 Item4 Item5 Item6 Item7 Item8 Item9 Item10 F1 SE_F1 [1,] 4 0 4 4 4 4 4 4 4 4 2.352869 0.63262 Now, although the #2 and #3 look about right, the whole problem is that I really don't think #1 should have $\theta$ = -2.0. In their study the authors report a mean score of about 2.3 I should be getting closer to $\theta$ = 0. However, the strangest thing is that in abovementioned post there is a suggestion to use catR . Using this package I'm getting ability estimates that I think are correct. Here is my code: library(catR) pars These are the estimates I'm getting: > #estimate for response pattern #1 > thetaEst(pars, c(1,0,1,0,0,0,0,0,1,0), model = "GRM", method = "EAP") [1] 0.08901428 > > #estimate for response pattern #2 > thetaEst(pars, c(4,0,4,4,3,3,4,4,4,4), model = "GRM", method = "EAP") [1] 1.907622 > > #estimate for response pattern #3 > thetaEst(pars, c(4,0,4,4,4,4,4,4,4,4), model = "GRM", method = "EAP") [1] 2.253911 I'm getting $\theta$ = 0.09 which looks about right to me. So can anyone tell my what it is that I'm doing wrong with the original mirt code?
