[site]: crossvalidated
[post_id]: 412591
[parent_id]: 252652
[tags]: 
I can think of a couple reasons: one theoretical, the other practical. The theoretical reason is because we want to use maximum likelihood estimation (MLE.) This not only gives us a principled way to pose the model fitting procedure as an optimization problem, there are also theoretical results which only work when we use MLE, such as Wilk's theorem which gives rise to the likelihood ratio test and the analysis of deviance , or Wald's test which gives us a way to test the significance of parameters in a generalized linear model. All of these tools go away if we don't use maximum likelihood estimation. However, there is also another very practical answer: the squared error loss function is not always convex for GLMs . For example, it is easy to prove this true for logistic regression. You can even see the loss function start to curve downward near 0 and 1: So even if we wanted to use MSE as an atheoretical loss function this non-convexity could cause problems during fitting!
