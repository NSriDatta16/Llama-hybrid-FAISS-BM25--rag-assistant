[site]: crossvalidated
[post_id]: 313452
[parent_id]: 
[tags]: 
Autocorrelation and heteroskedasticity in time series data

I have several time series of two variables over the course of one year (approx. 2.5k observations). I hypothesize one variable ( x ) acts as a potential predictor for the other variable ( y ). I looked for the period in which y is best described by x ("best" as in strongest Pearson), took the observations from just that optimal period, calculated the simple linear regression model and its parameters and predicted y upon all x . Hence, I have two y 's over the same time series of one year - the observed and predicted, the latter being the result of the regression model calculated from the optimal period. Now, I detected autocorrelation and heteroskedasticity in the data from the optimal period. For one example time series, see below the regression diagnostic plots and statistical test results inside them. Top left plot: raw data in a scatterplot; top right plot: residuals vs indepedent varible (DW = Durbin Watson test and BG = Breusch-Godfrey test for autocorrelation); middle left: residuals vs fitted plot (BP = Breusch-Pagan test for heteroskedasticity); middle right: normal Q-Q plot (W = Shapiro-Wilk test and A = Anderson-Darling test for normality in residuals); bottom left: scale-location plot, bottom right: residuals vs leverage plot to detect outliers. From the tests and visual inspection I can infer that there is autocorrelation and heteroskedasticity present in my data. I am somewhat stuck on how to proceed from here on. Especially, I would be glad about help on the following: Autocorrelation is common in time series data, which does not mean that I do not have to correct for it, I guess? Normality in residuals is not THAT important, especially for larger sample sizes. Sample sizes for my optimal periods have 240 observations as a minimum. Enough to put this issue off? For pure prediction purposes, which is what I am interested in mainly, I heard that regression disgnostics and their treatment do not offer much improvement for the predicted values, but are important for t - and F -statistics. Should I even worry about rectifying any issues revealed by the diagnostics if it is the forecasted y I am after? Box-Cox transformation could help rectifying heteroskedasticity and the Cochraneâ€“Orcutt, Hildreth-Lu, or First Differences Procedures can tackle autocorrelation, at least in theory. How can I know which problem to address first? Does remedying heteroskedasticity improve or worsen the autocorrelation situation, and vice versa ? Is there a procedure that can alleviate both problems in one go (i.e. Newey-West estimator)? The scatterplot indicates that the relationship is not very well described by a linear regression model and may potentially better fit a gls model in the first place. However, I would like to consistently apply a linear model to all my time series, even though they might not very well explain the distribution of the data. That in itself, would be an interesting outcome. I am using the R environment for my analyses.
