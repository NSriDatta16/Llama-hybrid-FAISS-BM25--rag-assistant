[site]: crossvalidated
[post_id]: 541545
[parent_id]: 541544
[tags]: 
You are correct. The Gaussian process is a distribution over functions . As with other Bayesian methods, you start with a prior and combine it with data (observed outcome) through likelihood to get a posterior. The posterior can be used to make predictions and can be used as a prior for further analysis ( Bayesian updating ). The posterior distribution in Bayesian optimization is used for making guesses about the next move either by using it to find an optimal move given some definition of optimality or by sampling from it. In Bayesian optimization you optimize the function approximated by Gaussian process (or another model) rather than optimizing the function directly as in many other forms of optimization. If you want to learn more, I recommend the book by Carl Rasmussen and Christopher Williams , the recorded talks by Carl Rasmussen , Neil Lawrence , and Michael Osborne , the blog posts by Katherine Bailey and Martin Krasser , including an additional one on Bayesian optimization, and the visual guide by Johen GÃ¶rtler et al . When I first learned about Gaussian processes I found the notation and language a little bit confusing and implementing the stuff and playing around with the code helped a lot. After you feel that you understand it, I recommend re-reading the book by Rasmussen and Williams once more.
