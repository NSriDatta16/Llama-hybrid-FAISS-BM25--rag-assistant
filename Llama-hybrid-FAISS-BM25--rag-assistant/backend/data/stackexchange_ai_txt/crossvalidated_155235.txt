[site]: crossvalidated
[post_id]: 155235
[parent_id]: 155177
[tags]: 
In general, the intuition of multiple layers is to create higher-level abstractions and capture more non-linearities between the data. A meaningful explanation (at least for humans), like the one you described for image processing, is usually achieved with convolutional networks. Now, in sequence modeling, we want to create such abstractions, and at the same time enforce their correlation with the previous inputs. So, the lower-level abstractions to feedback to the next lower-level abstractions, and so forth and so on with the higher ones. Often the input and output vectors have the same length, where in such cases we just manage to capture more non-linearities between the data (and maybe take advantage of the memory cells in LSTM units). In the case of NPL, it varies for character level modeling and word level modeling, and I am not sure that you could say something like that. Similar intuition as yours could be achieved combining convolutions with RNNs, a great example is the work of Kalchbrenner et al. . Finally, a very good blog post, by A. Karpathy , describing character level processing with RNNs might be of your interest.
