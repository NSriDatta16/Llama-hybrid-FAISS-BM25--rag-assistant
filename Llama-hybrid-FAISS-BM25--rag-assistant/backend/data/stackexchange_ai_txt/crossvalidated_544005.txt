[site]: crossvalidated
[post_id]: 544005
[parent_id]: 
[tags]: 
Does time increase linearly relative to data size in a machine learning workflow?

Let's say I am working with a dataset first containing 100 rows of data. I want to follow the normal process of doing some feature engineering, then training a model on this data. Running the whole script in bulk will take X minutes. Now let's say I run the same script on the same data, this time containing 1000 rows though. Let's say this takes Y time. Would it be safe to assume that Y=10X? Or would the time grow in some other way relative to the size of the data?
