[site]: datascience
[post_id]: 96284
[parent_id]: 96247
[tags]: 
General remark: your two new models give very different results in terms of precision and recall, I find this a bit surprising. I would probably try different learning methods (e.g. decision trees, SVM) in order to investigate if this is really due to the features or not. a) Absolutely, the threshold should be specific to the model and features, it would be suboptimal to keep the same value. b) Typically one would check whether there is a statistically significant difference in the performance between the models. I'm not sure which significance test is appropriate here. c) Then you should evaluate your models with $F_{\beta}$ -score instead of just F1, with $\beta$ higher than 1 in order to favour recall. I'd say at least 2, it depends how much more costly is a FN than a FP error. Note that the threshold should also be selected based on this measure. d) I would certainly try different methods, because it's always possible that another method would perform better. I always recommend decision trees because they are robust and interpretable. e) I'm not sure but there might be a confusion here: class_weight=balanced means that you give a higher importance to the minority class than what it really represents in the data, in other words the learning algorithm will work as if the two classes have the same number of instances. So as far as I know there would be no point in resampling additionally to using the weights. However you could provide manual weights, for instance 0.1,0.9 if you want to favour detecting the minority class even more (that would increase recall). By the way I would suggest to also try without using weights at all: it's unlikely to be good in terms of recall but that it could be useful just to know the precision/recall in this case. f) For tuning hyper-parameters the important point is to use a validation set different from the final test set. For every "method" (set of features) independently: Use grid search to determine the best parameters for the method, evaluating on the validation set only. Pick the best parameters. At this stage if you want you can train the final model using both training set+validation set (but only with the selected parameters). Then you can apply the 3 final models on the unseen test set and compare their performance. Essentially the parameter tuning is part of the training process (it's a kind of "meta-training"), so there's no data leakage as long as you don't use the test set for determining the best parameters.
