[site]: crossvalidated
[post_id]: 224663
[parent_id]: 224655
[tags]: 
The approach is reasonable as long as the bag-of-features representation properly captures your desired concept of similarity/distance between images. For example, it has the property that local features could be spatially re-arranged within an image and the representation wouldn't change. That could be good or bad, depending on your perspective. Similar reasoning applies to the choice of distance metric. There is no absolute best; it depends entirely on what you want 'similar' and 'distant' to mean in the context of your application. For example, cosine distance measures the relative angle between vectors, but is invariant to magnitude. This would mean that distance between images is determined by the relative frequency of each feature, but not the absolute count. This is a popular choice for measuring distance between bag-of-word models of text documents, because relative word frequencies can better capture the meaning of text documents (e.g. a longer document might contain more occurrences of each word, but this doesn't affect the meaning). Whether cosine distance makes sense for your image retrieval system depends on whether you want your system to have that property. Your goal is to return the image in your database that's closest to the query image. This is called nearest neighbor search. It isn't necessary to compute the distance between the query and every single image in your database; more efficient procedures exist. The general idea is that distances between images in the database can be exploited, along with properties of the distance metric. If you know that the query image has a particular distance to some database image, you can infer that other database images can't possibly be the nearest neighbor. Therefore, many unnecessary distance computations can be eliminated and the search can be performed in sublinear time. You mentioned locality sensitive hashing, so it sounds like you're aware of this issue. Another popular class of methods uses tree data structures (e.g. KD-trees, ball trees, cover trees) to accelerate the search. Searching for "nearest neighbor search" will return many papers on the topic, and ready-to-use code is available for a number of methods.
