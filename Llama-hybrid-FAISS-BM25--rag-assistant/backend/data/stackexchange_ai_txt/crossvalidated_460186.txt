[site]: crossvalidated
[post_id]: 460186
[parent_id]: 460185
[tags]: 
Strictly speaking Decision Trees (DT) are non-parametric estimators so there is not directly analogy of a response distribution for a single tree. A DT learns a discontinuous piecewise constant model; it will do a recursive partitioning of our sample space. If we can integrate that sample space/support out we can get a "mean" but that is not always trivial especially if our sample space is not closed (e.g. "above than 1.9m tall"). That said, we can use splitting criteria that correspond to the expected distribution of our response variable but this is not defining a closed form distribution; i.e. we can define a (marginal) likelihood at a particular node, and we can then use the product of the marginal likelihoods over all split nodes as the overall marginal likelihood value for a tree but that's particular for that tree. This is well understood under the framework of Generalised Regression Trees Chaudhuri et al. (1995) and Split Selection Methods for Classification Trees Loh & Shih (1997) where we employ parametric models to obtain splits in inner (and terminal) nodes. Zeileis et al. (2008) Model-based recursive partitioning gives an overview how this is working on a single tree and the function partykit::mob implementing it. Just to be clear here: This is not a CART implementation/CART does not assume any likelihood for the data; continuous or discrete data are fine for it. Now regarding the distribution across a whole forest: The work on Bayesian CART (Denison et al. 1998 - A Bayesian CART Algorithm can offers a proper framework where the number of splitting nodes, the node positions, etc. is approximated stochastically as to set up a probability distribution over the space of possible trees. This can incorporate the work of Zeileis et al. but I don't think anyone has actually done it within the context of a forest of GLM trees. There is some work on likelihood based approaches for Random forests; e.g. Quadrianto and Ghahramani (2009) A Very Simple Safe-Bayesian Random Forest but I have not seen it catching up a lot. In general, Bayesian additive regression trees (BARTs - Chipman et al. (2008)) sit squarely on the notion that we use a likelihood (and a prior) to grow an ensemble of trees. They are much more closely related to boosting than simple bagging like RFs are but the principles are very similar.
