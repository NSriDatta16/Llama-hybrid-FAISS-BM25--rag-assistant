[site]: crossvalidated
[post_id]: 226930
[parent_id]: 226923
[tags]: 
ReLU is the max function(x,0) with input x e.g. matrix from a convolved image. ReLU then sets all negative values in the matrix x to zero and all other values are kept constant. ReLU is computed after the convolution and is a nonlinear activation function like tanh or sigmoid. Softmax is a classifier at the end of the neural network. That is logistic regression to normalize outputs to values between 0 and 1. (Alternative here is a SVM classifier). CNN Forward Pass e.g.: input->conv->ReLU->Pool->conv->ReLU->Pool->FC->softmax
