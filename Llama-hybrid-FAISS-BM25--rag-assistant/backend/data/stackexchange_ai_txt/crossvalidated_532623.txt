[site]: crossvalidated
[post_id]: 532623
[parent_id]: 
[tags]: 
How should evaluate a Testing set using a pattern learned with PCA?

I tried to search for an answer about the evaluation and interpretation of a pattern learned with PCA, on data from a Testing Set, but I found no answer. Let me explain my situation: My Data Mining task is to implement an instance of the KDD process in order to learn an accurate classifier of Windows applications. I have a single "dataset.csv" file on which I do the initial split into Training Set and Testing Set (the entire learning & validation phases will focus exclusively on the Training Set). My choice is to learn a classifier with the Gaussian Naive Bayes algorithm, so I decide to apply the transformation of my Training Set through the PCA, in order to ensure independence between the attributes (so, for example, on my Training Set with 50 features, I decide to transform it into a new Training Set with 10 Principal Components and learn & build from it my pattern with the Gaussian Naive Bayes and identifying the best configuration with a GridSearchCV). Once learned my pattern, I have to perform the final evaluation on the whole Testing Set splitted at the start. Here's my problem: The pattern was learned on a Training Set composed of 10 columns (i.e. 10 Principal Components), but my Testing Set is composed of 50 features (like my Training Set), and this conflicts with the learned pattern because the number of columns is different and I cannot make predictions. What should I do? EDIT To be correct, I specify the small details of my problem so that you can give me as complete an answer as possible. I will describe very quickly what I did: I split the starting dataset into Training Set and Testing Set using sklearn.model_selection.train_test_split with test_size=0.3 (i.e. 30%). After splitting, I performed (on the Training Set) Data Cleaning (replacing any missing values with statistical.mode for each column) and Data Scaling using sklearn.preprocessing.MinMaxScaler . After this little PreProcessing, I performed the PCA on the entire Training Set's indipendent variables, thus obtaining the new Training Set composed of 10 columns (i.e. the 10 Principal Components) and then I used sklearn.model_selection.GridSearchCV to learn the best configuration of sklearn.naive_bayes.GaussianNB (which is the estimator parameter), for the different var_smoothing values that I insert in a list and associate with the param_grid parameter, and setting cv=5 parameter for K-Fold Cross Validation used by GridSearchCV and finally fit on the entire Training Set. This is how I learn my model, which now I have to test on the Testing Set. I would like slightly more clarity on what should be done from this point on, so my doubt is: Would it be correct to perform the PCA exclusively on the Testing Set, calculating the same number of Principal Components and then evaluating?
