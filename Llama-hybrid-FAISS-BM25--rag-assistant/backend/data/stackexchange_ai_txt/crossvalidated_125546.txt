[site]: crossvalidated
[post_id]: 125546
[parent_id]: 
[tags]: 
VW multiclass classification

I am new to vw and trying to do a multiclass text classification with 18 classes. features are unigram, bigram and trigrams. Total features are around 1.4 million Total training examples 35 million This is how my training data looks like 11 |UniGram 19334:0.250 1597:0.250 104:0.250 682:0.250 |BiGram 708757:1.000 966191:1.000 |TriGram 11 |UniGram 1878:0.500 17678:0.500 |BiGram |TriGram 11 |UniGram 341:1.000 |BiGram |TriGram 11 |UniGram 275:0.250 287:0.250 479:0.250 249:0.250 |BiGram 709552:1.000 1017093:1.000 |TriGram 11 |UniGram 6981:0.500 985:0.500 |BiGram |TriGram 11 |UniGram 860:0.167 6253:0.167 24:0.167 571:0.167 163227:0.167 |BiGram 1082564:0.667 1006767:0.667 949487:0.667 |TriGram 11 |UniGram 870:0.250 13824:0.250 82:0.250 208:0.250 |BiGram |TriGram 11 |UniGram 3925:0.500 390:0.500 |BiGram |TriGram 11 |UniGram 1704:0.500 1185:0.500 |BiGram |TriGram 11 |UniGram 2612:0.500 3004:0.500 |BiGram |TriGram 13 |UniGram 218720:0.500 231:0.500 |BiGram |TriGram 13 |UniGram 231:0.333 |BiGram |TriGram 13 |UniGram 231:0.500 |BiGram |TriGram 13 |UniGram 29169:0.333 3198:0.333 119436:0.333 |BiGram 741003:1.333 |TriGram 13 |UniGram 245835:0.250 231:0.250 |BiGram |TriGram 13 |UniGram 2184:0.333 231:0.333 27:0.333 |BiGram |TriGram 13 |UniGram 218320:0.333 231:0.333 8:0.333 |BiGram |TriGram 13 |UniGram 8:0.250 52746:0.250 231:0.250 130367:0.250 |BiGram 741369:1.000 |TriGram 1320863:2.250 13 |UniGram 130367:0.500 231:0.500 |BiGram |TriGram 13 |UniGram 3198:0.500 1036:0.500 |BiGram |TriGram I have some doubts regarding my training I trained using vw.exe --oaa 18 -b 28 -d vwTrainingData.vw --readable_model m.model.txt -f m.vw how do i interpret the output generated during training ? l_regressor = m.vw Num weight bits = 28 learning rate = 0.5 initial_t = 0 power_t = 0.5 using no cache Reading datafile = vwTrainingData.vw num sources = 1 average since example example current current current loss last counter weight label predict features 1.000000 1.000000 1 1.0 11 1 7 0.500000 0.000000 2 2.0 11 11 3 0.250000 0.000000 4 4.0 11 11 7 0.125000 0.000000 8 8.0 11 11 3 0.062500 0.000000 16 16.0 11 11 10 0.031250 0.000000 32 32.0 11 11 3 0.015625 0.000000 64 64.0 11 11 3 0.007812 0.000000 128 128.0 11 11 3 0.003906 0.000000 256 256.0 11 11 4 0.001953 0.000000 512 512.0 11 11 3 0.000977 0.000000 1024 1024.0 11 11 3 0.000488 0.000000 2048 2048.0 11 11 1 0.000244 0.000000 4096 4096.0 11 11 3 0.000122 0.000000 8192 8192.0 11 11 3 0.000061 0.000000 16384 16384.0 11 11 9 0.000031 0.000000 32768 32768.0 11 11 1 0.000015 0.000000 65536 65536.0 11 11 4 0.000008 0.000000 131072 131072.0 11 11 3 0.000004 0.000000 262144 262144.0 11 11 3 0.000002 0.000000 524288 524288.0 11 11 3 0.000001 0.000000 1048576 1048576.0 11 11 1 0.000000 0.000000 2097152 2097152.0 11 11 1 0.000000 0.000000 4194304 4194304.0 11 11 3 0.000003 0.000005 8388608 8388608.0 3 3 1 0.000004 0.000005 16777216 16777216.0 12 12 4 0.000005 0.000007 33554432 33554432.0 6 6 2 finished run number of examples per pass = 35766253 passes used = 1 weighted example sum = 3.57663e+07 weighted label sum = 0 average loss = 5.78758e-06 best constant = 0 total feature number = 143645637 When i read the readable model file i see Version 7.7.0 Min label:-1.000000 Max label:1.000000 bits:28 0 pairs: 0 triples: rank:0 lda:0 0 ngram: 0 skip: options: --oaa 18 Why the min label and max labels are -1 and 1 ? Is there something that i am missing ? When i predict it always predicts one class. I feel that i am doing something completely wrong not sure what.
