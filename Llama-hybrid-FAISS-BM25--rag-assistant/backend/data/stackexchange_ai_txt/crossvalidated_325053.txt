[site]: crossvalidated
[post_id]: 325053
[parent_id]: 
[tags]: 
Why word2vec maximizes the cosine similarity between semantically similar words

I have an understanding into the technicals of word2vec. What I don't understand is: Why semantically similar words should have high cosine similarity. From what I know, goodness of a particular embedding is seen in shallow tasks such as word analogy. I am unable to grasp the relationship between maximizing cosine similarity and good word embeddings.
