[site]: crossvalidated
[post_id]: 613789
[parent_id]: 
[tags]: 
Performance estimation bias in machine learning

Suppose that we have a dataset $D$ that we split on train and test, denoted as $D_{train}$ and $D_{test}$ , respectively. We want to construct a predictive model and also estimate its performance. Furthermore, assume that we can choose among many configurations $C=\{c_i\}_{k=1}^K$ . A configuration is just a (possibly meta) function that given data as input, produces a model as output . That is, is not just the classifier or the regressor but all the steps such as feature selection, imputation etc. Of course a classifier or a feature selector has also hyperparameters. As such each configuration is a unique instantiation of all these imputers, selectors, classifiers etc. For example a configuration might look like this: $$ imputer (\text{mode = mean}) \longrightarrow selector (\text{Lasso}, \lambda = 10) \longrightarrow classifier (\text{Random Forest}, n_{trees}=100)$$ Lets say that we train each of these configurations on the train set and we find the configuration with the greatest performance $c^*$ , as measured by the performance metric $P$ on the test set. Where bias is introduced? Note that each of these configurations have a "true" performance $P_{c_k}$ when evaluated on the whole population. In our setting we are just calculating the sample performance $\hat{P}_{c_k}$ for each configuration. Of course this estimation is unbiased and we can report it. Note that the performance $P_{c_k}$ refers to the performance of the model produced by the configuration $c_k$ . What is biased (upward) and we can't report it is the performance of the meta-algorithm that selects among many configurations the best. We can prove this using the Jensen's inequality: $$ E[\max(\hat{P}_{c_1}, \ldots, \hat{P}_{c_K})] \geq \max (E[\hat{P}_{c_1}], \ldots, E[\hat{P}_{c_K}) = \max(P_{c_1}, \ldots P_{c_K}) $$ Comparing algorithms in different datasets I was reading this paper and the authors state the following (1st paragraph on 4th page): More specifically, consider a typical publication where a new algorithm is introduced and its performance (after tuning the hyperparameters) is compared against numerous other alternatives from the literature (again, after tuning their hyperparameters), on several datasets. The comparison aims to comparatively evaluate the methods. However, the reported performances of the best method on each dataset suffer from the same problem of multiple inductions and are on average optimistically estimated. Can someone explain where the bias is introduced in this comparison?
