[site]: crossvalidated
[post_id]: 471504
[parent_id]: 
[tags]: 
Bayesian expectation maximization

For a concrete example, let $\{(Y_i, Z_i):i = 1, 2, \dots, n\}$ denote the pair of observed (continuous) variables and latent (discrete) variables, and $\theta \in \mathbb{R}^d$ be some $d$ -dimensional vector of parameters that govern $Y_i,Z_i$ 's distribution. Under the expectation maximization framework, we iteratively maximize the expected log likelihood under our posterior for $Z$ . That is $$ \theta^{(t)} = \text{argmax}_{\theta \in \mathbb{R}^d} \mathbb{E}_{Z \mid Y, \theta^{(t-1)}} [\text{log}f(Y,Z; \theta) ] $$ Under the Bayesian framework (although point estimates aren't very Bayesian...) where $\theta$ has some prior distribution, how does our framework change? The bundle of comments and questions to follow is secondary, but hopefully sheds light onto why I am confused. My intuition suggests we replace the log likelihood with the log joint distribution $\text{log}p(Y,Z,\theta)$ . However, do we still take the expectation under $Z$ ? This seems arbitrary now that we consider $\theta$ as a latent variable; why should we take the expectation under $Z$ and not $\theta$ ? -- I should take a step back and consider again the purpose of expectation maximization. We want to estimate the maximum likelihood or determine the posterior mode. It is ambiguous to me whether the latent variable $Z$ is introduced to make inference amenable or is an inherent component of the postulated model. Either way, we include $Z$ in the model, but are still interested in the point estimate. Supposedly, conditioning on $Z$ gives us a more manageable distribution to work with, but we don't know what $Z$ 's realizations are, thus we aim to maximize the expected likelihood (or joint density in the Bayesian case). OK, so maybe I should just take the expectation under $Z$ and replace the log likelihood function with the joint distribution of $Y,Z, \theta$ .
