[site]: crossvalidated
[post_id]: 500440
[parent_id]: 500401
[tags]: 
The video you linked focuses on probability distributions where the hypothesis is binary; i.e. $H$ is a random variable with two possible values of "true" or "false". For Variational Autoencoders, we're typically interested in much richer random variables. For example, VAEs are commonly used for images. In this setting, $x$ is a random variable over matrices of pixel values, and it does not make sense to talk about it being "true" or "false". Similarly, $z$ , often called the code , is a random variable over vectors, not "true" or "false". $p(z)$ is a probability density over possible code vectors $z$ : i.e., $p(z) \geq 0$ for all $z$ , and $\int p(z) dz = 1$ . Similarly, $p(x|z)$ is a probability density over possible matrices of pixel values $x$ for a particular $z$ : $p(x|z) \geq 0$ for all $x$ , and $\int p(x | z) dx = 1$ . $p(x, z)$ is then a joint density over both images and codes: $p(x, z)$ will be larger for $x$ and $z$ that are more likely, smaller (but not negative) for $x$ and $z$ that are less likely, and $\int \int p(x, z) dx dz = 1$ . Note that $p(z)$ is not the probability that $z$ takes a particular value. For continuous random variables, we can get the probability that the variable falls in a range of possible values, but the probability that it takes any single value is zero.
