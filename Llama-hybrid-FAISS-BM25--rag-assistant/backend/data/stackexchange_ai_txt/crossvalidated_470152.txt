[site]: crossvalidated
[post_id]: 470152
[parent_id]: 
[tags]: 
choosing a baseline algorithm using a nested cross validation

I am trying to work on a project for school and I am currently trying to select the best model (models) for the classification task. I have a few questions regarding this: What does a baseline model mean and how can I pick/select one. Do I need to select a baseline even though I am going to compare several algorithms and conduct hyperparameter tuning later using cross-validation? If it is necessary Does it make sense to use nested cross-validation just for the sake of choosing a baseline/ comparing different algorithms with the default parameters? just like in the code below? # Split-out test dataset X = features_ds y = labels_ds X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True) # Model selection #evaluation - baselines models = [] models.append(('LDA', LinearDiscriminantAnalysis())) models.append(('KNN', KNeighborsClassifier())) models.append(('CART', DecisionTreeClassifier())) models.append(('NB', GaussianNB())) models.append(('SVC', SVC())) models.append(('AdaBoost', AdaBoostClassifier())) models.append(('RF', RandomForestClassifier())) models.append(('GBClassifier', GradientBoostingClassifier())) models.append(('lightgbm', LGBMClassifier())) models.append(('XGBoost', XGBClassifier())) models.append(('ExtraTreesClassifier', ExtraTreesClassifier())) # evaluate each model in turn (spot checking algorithms (of baselines)) results = [] names = [] seed=7 num_folds=10 for name, model in models: kfold = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True) cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy') results.append(cv_results) names.append(name) print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())) # Compare Algorithms pyplot.boxplot(results, labels=names) ```
