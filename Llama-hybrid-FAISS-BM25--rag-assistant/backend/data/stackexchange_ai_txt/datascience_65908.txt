[site]: datascience
[post_id]: 65908
[parent_id]: 
[tags]: 
Are calibrated probabilities always more reliable?

EDIT: Based on the answer below, I have updated the question and added more detail. I have applied Dirichlet calibration to my fast-bert sentiment classification model, and I am struggling to really understand why/ if it is actually more reliable. Application: def dirichlet_scaling(train, y_train, test): logreg = LogisticRegression(multi_class='multinomial', solver ='newton-cg') logreg.fit(np.log(train), y_train) return logreg.predict_proba(np.log(test)) train --> validation set, size ~ 11000 test --> test set, size ~ 7000 These are the uncalibrated probability reliability diagrams: The classwise diagrams below are based on this recent article https://arxiv.org/abs/1910.12656 . Evaluation metrics: Confusion-Matrix: After applying Dirichlet scaling, as per the article, I get these classwise-reliabilities: And overall confidence calibration: Dirichlet Evaluation Metrics: Dirichlet Calibrated Confusion Matrix After calibration, the minority class - mixed sentiment - is less accurately predicted. The sample below is of mixed samples with high uncalibrated probabilities (> .9). These cases are all undoubtedly mixed (even the incorrect label). However, after calibration they are less confidently predicted. My main question is: The estimator over-estimates the minority class and under-estimates the majority class, which balances the class imbalance - calibrating the probabilities seems to reverse this to a certain extent. After calibration the class imbalance appears more pronounced in the predictions. I may very well be wrong, but the miscalibration in the classwise reliability diagrams appears to be the classifier incorporating the class distributions (class priors). Is it not usually the case that when imbalanced datasets are artificially balanced, the class priors are reintroduced to balance the probabilities? Does this technique also reduce calibration? Is it possible the classifier was already well calibrated? And that the over/under-estimation seen in the classwise-reliability diagrams is perhaps not always indicative of miscalibration if overall confidence is well calibrated?
