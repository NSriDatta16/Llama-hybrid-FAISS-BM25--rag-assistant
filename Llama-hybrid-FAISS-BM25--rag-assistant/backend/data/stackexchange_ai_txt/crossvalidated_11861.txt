[site]: crossvalidated
[post_id]: 11861
[parent_id]: 11850
[tags]: 
Cross validation would probably be good here. To do this you split your data set into 2 parts. You use the first part to fit both models, and then use the fitted model to predict the second part. This can be justified as an approximation to a fully Bayesian approach to model selection. We have the likelihood of a model $M_{i}$ $$p(d_{1}d_{2}...d_{N}|M_{i}I)=p(d_{1}|M_{i}I)\times p(d_{2}|d_{1}M_{i}I)\times p(d_{3}|d_{1}d_{2}M_{i}I)\times..$$ $$..\times p(d_{N}|d_{1}d_{2}...d_{N-1}M_{i}I)$$ Which can be seen heuristically as sequence of predictions, and then of learning from mistakes. You predict the first data point with no training. Then you predict the second data point after learning about the model with the first one. Then you predict the 3rd data point after using the first two to learn about the model, and so on. Now if you have a sufficiently large data set, then the parameters of the model will become well determined beyond a certain amount of data, and we will have, for some value $k$: $$p(d_{k+2}|d_{1}....d_{k}d_{k+1}M_{i}I)\approx p(d_{k+2}|d_{1}....d_{k}M_{i}I)$$ The model can't "learn" any more about the parameters, and is basically just predicting based on the first $k$ observations. So I would choose $k$ (the size of the first group) to be large enough so that you can accurately fit the model, $20$-$30$ data points per parameter is probably enough. You also want to choose $k$ large enough so that the dependence in the $d_{k+1}...d_{N}$ which is being ignored does not make the approximation useless. Then I would simply evaluate the likelihoods of each prediction, and take their ratio, interpreted as a likelihood ratio. If the ratio is about $1$, then neither model is particularly better than the other. If it is far away from $1$ then this indicates one of the models is outperforming the other. a ratio of under 5 is weak, 10 is strong, 20 very strong, and 100, decisive (corresponding reciprocal for small numbers).
