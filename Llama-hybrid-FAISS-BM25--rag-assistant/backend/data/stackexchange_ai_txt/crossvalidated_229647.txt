[site]: crossvalidated
[post_id]: 229647
[parent_id]: 228601
[tags]: 
Ok, thanks to the excellent @Mur1lo answer I now have a better understanding and would like to make my own attempt at making this abstract concept as concrete as I can. Suppose we have a sample of 5 coin draw results. We assume that they are sampled from a population with Bernoulli distribution with the true parameter $\pi_0$. When we look at a specific coin draw with result $x_3=1$, we can calculate the log-likelihood this patient was sampled from a Bernoulli distribution with all kind of parameter values, e.g. $\pi = 0.2$ or $\pi=0.9$ and so on. so, log-likelihood is a function estimating the likelihood of $x_3$ for each possible value of $\pi$. $$ LL(\pi|x_3) = x_3ln(\pi) + (1-x_3)ln(1-\pi) $$ Which simply means that if $x_3=1$ the likelihood for that was $\pi$ and if it's 0 the likelihood for that is $1-\pi$. If we assume independence between the coin draws, then we have an 'average' function representing the log-likelihood of the entire sample of n=5 coin draws. $$ LL(\pi|X) = \sum{x_i}ln(\pi) + (n-\sum(x_i))ln(1-\pi) $$ We want to find the maximum of $LL(\pi|X)$ - the mle = $\pi_{mle}$. The score function $u(\pi)$ is a vector of the derivatives w.r.t each parameter of the log-likelihood. Luckily in our case, it's a simple scalar as there's only one parameter. Under some conditions, it will help us find $\pi_{mle}$, since in that point the score function would be $u(\pi_{mle}) = 0$. We can calculate the observation score function for a single observation (coin draw): $$ u(\pi|x_3) = \frac{x_3}{\pi} - \frac{1-x_3}{1-\pi} $$ and the sample score function of n=5 patients: $$ u(\pi|X) = \frac{\sum{x_i}}{\pi} - \frac{n-\sum{x_i}}{1-\pi} $$ when we set this latest function to 0, we get $\pi_{mle}$. BUT, the specific 5 draws sample has nothing to do with the expectancy of the score function ! The expectancy is the value of the observation score function for every possible value of x, multiplied by the probability of that value, which is the density function! In our case, x can take only 2 values: 0 and 1. And the density function is as we assumed is a Bernoulli with parameter $\pi_0$: $$ E(u(\pi|x_i)) = \sum_x (\frac{x}{\pi} - \frac{1-x}{1-\pi}) \pi_0^x(1-\pi_0)^{1-x} = \frac{\pi_0}{\pi} - \frac{1-\pi_0}{1-\pi}$$ and its clear that it zeros out when evaluated at the true parameter $\pi_0$. The intuitive interpretation is: For each value of $\pi$, what's the mean rate of change in likelihood? The information matrix is the variance of the likelihood - how sensitive will our solution be to different data? (see this answer ). $$I(\pi|x_i) = var(u(\pi|x_i)) = var(\frac{x_i}{\pi} - \frac{1-x_i}{1-\pi}) = var(\frac{x_i-\pi}{\pi(1-\pi)}) = \frac{var(x_i)}{\pi^2(1-\pi)^2} = \frac{\pi_0(1-\pi_0)}{\pi^2(1-\pi)^2}$$ and when evaluated at the true parameter $\pi_0$ it simplifies to: $$I(\pi_0|x_i) = \frac{1}{\pi_0(1-\pi_0)}$$ (see washington edu notes for more details). Amazingly, there's another way of measuring how sensitive the likelihood would be in a certain $\pi$! that's the expectancy of the curvature = Hessian = second derivative. The steeper our likelihood, the more accurate we'll be. See details in mark reid's blog
