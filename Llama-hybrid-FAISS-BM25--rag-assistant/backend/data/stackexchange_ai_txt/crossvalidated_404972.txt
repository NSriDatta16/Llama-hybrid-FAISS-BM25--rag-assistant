[site]: crossvalidated
[post_id]: 404972
[parent_id]: 404932
[tags]: 
A hierarchical Bayes structure is a special form of prior structure in that, if $$\theta|\eta\sim\pi_1(\theta|\eta)\quad\text{and}\quad\eta\sim\pi_2(\eta)$$ then $$\theta\sim\int \pi_1(\theta|\eta)\pi_2(\eta)\text{d}\eta\tag{1}$$ In this (mathematical) sense, the prior does not "learn" from the data in that it remains a prior . (And "adaptively" sounds redundant in this setting.) Obviously, the hyperparameter $\eta$ also enjoys a posterior distribution $\pi_2(\eta|x^\text{obs})$ , which exploits the information brought on $\eta$ by the observation $x^\text{obs}$ through $\theta$ . But the big difference with an "empirical Bayes" non-Bayesian approach is that the latter uses a plug-in $\pi_1(\theta|\hat\eta(x^ \text{obs}))$ instead of (1)
