[site]: crossvalidated
[post_id]: 220068
[parent_id]: 138229
[tags]: 
Intuitively, I think of a multilayer perceptron as computing a nonlinear transformation on my input features, and then feeding these transformed variables into a logistic regression. The multinomial (that is, N > 2 possible labels) case may make this more clear. In traditional logistic regression, for a given data point, you want to compute a "score", $\beta_i X$, for each class, $i$. And the way you convert these to probabilities is just by taking the score for the given class over the sum of scores for all classes, $\frac{\beta_i X}{\sum_j \beta_j X}$. So a class with a large score has a larger share of the combined score and so a higher probability. If forced to predict a single class, you choose the class with the largest probability (which is also the largest score). I don't know about you, but in my modeling courses and research, I tried all kinds of sensible and stupid transformations of the input features to improve their significance and overall model prediction. Squaring things, taking logs, combining two into a rate, etc. I had no shame, but I had limited patience. A multilayer perceptron is like a graduate student with way too much time on her hands. Through the gradient descent training and sigmoid activations, it's going to compute arbitrary nonlinear combinations of your original input variables. In the final layer of the perceptron, these variables effectively become the $X$ in the above equation, and your gradient descent also computes an associated final $\beta_i$. The MLP framework is just an abstraction of this.
