[site]: crossvalidated
[post_id]: 88627
[parent_id]: 88615
[tags]: 
Think of the importance sampling as defining a new $f^*(x_i) = f(x_i)w_i/\sum w_i$. Then, when you do your Bayesian bootstrap, as in your formula for $\hat{\theta}_j$, do it with respect to $f^*$ instead of the original $f$. The concept behind this is that the importance sampling "corrects" for a misspecified probability of observing the $x_i$ (misspecified because it isn't $1/n$.) This correction should be the same regardless of the weights generated for a particular bootstrap sample, because it's due to the underlying probability model. So you shouldn't combine the correction weights with the bootstrap weights to form new weights (your first approach). On the other hand, your second approach adds randomness by sampling according to the importance sampling weights rather than doing the importance sampling calculation, thus adding randomness to your results and causing the resulting distribution to be too wide. Your third approach adds randomness for the same reason, but to a different degree, and therefore has the same effect.
