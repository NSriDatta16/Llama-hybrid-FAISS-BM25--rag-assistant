[site]: crossvalidated
[post_id]: 596548
[parent_id]: 
[tags]: 
Within group feature selection for PCA

Let $\mathbf{p} \in R^D$ be the first eigenvector obtained by PCA on a dataset $\mathbf{X} \in R^{N \times D}$ . $$ \mathbf{X}\mathbf{p} $$ is the first principal component of my dataset. The relative importance of each feature in $\mathbf{X}$ can be interpreted in light of the values in $\mathbf{p}$ . With sparse PCA, I can increase interpretability by reducing the number of features used in that first principal component. Now, lets say the set of features $\mathcal{D}$ (such that $|\mathcal{D}| = D$ ) is partionned in groups $\mathcal{D} = \mathcal{D}_1 \cup \ldots \cup \mathcal{D}_T $ such that all feature in $\mathcal{D}$ belongs to one and only one group $\mathcal{D}_i$ . My main question is: Is there an alternative to sparce PCA that would induce sparsity at the group level such that few variable are selected in each group of feature . I don't want to select a whole group and put the others to zero. Ideally, I would like one feature selected in each group. Example: input features are: {1, 2, 3, 4, 5} groups are: {1, 2} and {3, 4, 5} the desired algorithm returns a non-zero coefficients in the first eigenvector for features 1 and 4 I have considered a couple of ideas already. Are they bad ideas? Considered options: Make sparse PCA. Select, in each group, the feature with the highest coefficient. Then, either: Keep only those coefficients. (idea 1) Run PCA again on the selected features. (idea 2) Note: My final goal is PCA so I need feature selection for PCA and not the reverse. I am working on time series
