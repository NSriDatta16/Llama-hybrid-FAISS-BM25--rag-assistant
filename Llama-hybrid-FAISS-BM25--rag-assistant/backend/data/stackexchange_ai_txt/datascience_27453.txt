[site]: datascience
[post_id]: 27453
[parent_id]: 27449
[tags]: 
Here's my take on your questions. Yes, you can zero-pad vectors. However, I would strongly recommend you use an LSTM as part of your encoder if you're using sentences as input. An LSTM can take variable-length input - that's part of its charm! The loss function depends entirely on your goal. It sounds like you're trying to create a sequence-to-sequence VAE. I would go about this by putting a softmax function on your output layer, to generate the next most probable word in the sentence. So, I would use categorical_crossentropy as the loss function, and I would use softmax instead of sigmoid , because you're choosing the next most likely word from a set vocabulary (this is not a binary issue but rather a categorical issue). Hope that helps!
