[site]: datascience
[post_id]: 69910
[parent_id]: 54232
[tags]: 
I think there are a few misconceptions in your statements. Please take into account the following BERT does not provide word-level representation. It provides sub-words embeddings and sentence representations. For some words, there may be a single subword while, for others, the word may be decomposed in multiple subwords. The representations of subwords cannot be combined into word representations in any meaningful way. ELMO does provide word-level representations. Word embeddings don't reflect meaning but co-occurrence statistics within the context window. This means that two antonyms will probably have similar representations, as they can appear in similar contexts within short context windows. Distances over continuous representation spaces do not mix well with untangled concepts, like meaning, e.g. should two antonyms be closer than two unrelated words? That being said... If you want a model that does word sense disambiguation (WSD), you should train a model on a WSD dataset if there is available data. If you have no data, you could try a nearest neighbor approach like in this article , where the authors specifically show their approach with "bank" for different contextualized word embeddings (see figure below). You should also have a look at this other article , where they also study the geometry of BERT representations in relation to WSD.
