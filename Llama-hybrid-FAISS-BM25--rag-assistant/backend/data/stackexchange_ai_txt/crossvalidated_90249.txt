[site]: crossvalidated
[post_id]: 90249
[parent_id]: 90231
[tags]: 
It isn't that the other answers are incorrect, per se, it is just that I think a more clear pedagogically oriented answer can be provided. There are two commonly used Tukey's test in this type of situation. Tukey's HSD and Tukey's LSD. The LSD is less conservative than the HSD, similar to a full combinations of t-tests being performed in parallel. As you might imagine based on this description the LSD does not control for familywise error rate; the HSD does. So a lot hinges on which Tukey's test you are using. If you are using LSD, current tradition dictates you probably shouldn't be. If you are using HSD, we can keep going down this road. In regards to your first question: "Is this relevant given that ANOVA has already established that there is no significant interaction effect between Species and Density?"... Most 'ANOVA tradition' dictates that you not use either post-hoc unless a statistically significant effect has been found. An exception to this 'rule' is that tradition allows that if you had a good reason to compare groups at the outset before you looked at your data, you could simply do a t-test between those groups without controlling for familywise error rate. Also, if there was a good reason to compare one to many, there is a post hoc specifically for that approach (the name escapes me at the moment). However, there is no denying that eyeballing your data you are seeing that the DV for Species A at a density of 0 is noticeably larger than the other combinations. So, you should mediate on this a little further. Do the DV means look different enough to you to be remarkable? What if you consider the degree of error in your measurements (e.g. (don't quote me I'm talking off the cuff) but, I think that sqrt MSE will give you an idea of what the ANOVA shrunken error)? Regardless, traditional approaches to ANOVA will not let you make any claims about this cell of the design versus the others or declare that they are significant; nevertheless, some fields are okay with you speculating based on non-significant effects. Your advisor, peers, and previous journal articles will give you an idea of how much rope you have in making your claims and observations. For my part, I believe that now that you've looked at the data and are making decisions about how to proceed that you are on shaky ground in terms of having p-values that are directly meaningful. Ideally, you'd run the study again, this time with an a priori intent to check Species A at Density 0 against the other cells of your design. An alternative approach would be to use less familiar statistical methods that don't require you to plan your analyses ahead of time (e.g. Bayesian). But those beliefs of mine are a matter of opinion, so allow me to drift back to the facts. Let's go on to your second question: "Why would there be significant differences in Tukeys if ANOVA says there is no interaction effect in the first place?" This is one that even some seasoned scientists don't understand. Here is my shot at an explanation... the observed deviance between two cells that are maximally different relative to the grand mean will be on average higher than the observed deviance for four cells, three of which are very similar, relative to the grand mean. In short, the more cells you have that are similar in an interaction, the less likely you are to observe an statistically interaction even if a few cells /do/ reflect an interaction. I suspect your primary problem is that you used Tukey's LSD. Otherwise, the problem may be as jgstat suggested This is unlikely to be your primary problem here because you have a pretty simple 2x2 design. I think your problem is probably that you used Tukey's LSD and so the differences between two cells happened to pop-out but that by-and-large the cells are mostly similar.
