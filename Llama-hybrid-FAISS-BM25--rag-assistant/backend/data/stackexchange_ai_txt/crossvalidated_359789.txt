[site]: crossvalidated
[post_id]: 359789
[parent_id]: 
[tags]: 
How to incorporate confusion matrix (on validation data) into final classification probability

I have a machine learning classifier that I've trained to distinguish between two classes, and have calculated the confusion matrix on a validation set. I'd like to incorporate those priors into the prediction probability the classifier gives on a new sample, and I just don't know Bayesian statistics well enough to pull it off. I could use some help. Let's say I have two classes $A$ and $B$. I had unequal numbers of each for validation. The confusion matrix from my validation set is prediction/actual A B total A 80 20 100 B 7 70 77 In this case, I just round the classification probability to get the class, so $p =0.5$ is class $B$. Not knowing anything else, I assume that its equally likely for class $A$ or class $B$ to occur in nature. Now suppose the classifier tells me that the probability that a new datum comes from class $A$ is $p=95$%. I'm not sure what probability I want. For example, if I'm just looking for $P({\rm it\ is\ class\ }A|p)$, the naive setup for Bayes Thm would be $P({\rm it\ is\ class\ }A|p) = \frac{P(p|{\rm it\ is\ class\ }A)P({\rm it\ is\ class\ }A)}{P(p|{\rm it\ is\ class\ }A)P({\rm it\ is\ class\ }A)+P(p|{\rm it\ is\ not\ class\ }A)P({\rm it\ is\ not\ class\ }A)}$ Or do I want something more complicated, like $P({\rm it\ is\ class\ }A|p,p{\rm\ is\ a\ True\ Positive})$. Or, is this not even a Bayesian problem in the first place? Plus, in either case, I just can't wrap my head around what numbers to put where. Like I said, I could use some help.
