[site]: datascience
[post_id]: 23520
[parent_id]: 17710
[tags]: 
Feature selection: XGBoost does the feature selection up to a level. In my experience, I always do feature selection by a round of xgboost with parameters different than what I use for the final model. I typically use low numbers for row and feature sampling, and trees that are not deep and only keep the features that enter to the model. Then fine tune with another model. This prevented overfitting for me when the number of features was very high. Feature generation: XGBoost (classification, booster=gbtree) uses tree based methods. This means that the model would have hard time on picking relations such as ab , a/b and a+b for features a and b . I usually add the interaction between features by hand or select the right ones with some heuristics. Depending on the application, this can really boost the performance.
