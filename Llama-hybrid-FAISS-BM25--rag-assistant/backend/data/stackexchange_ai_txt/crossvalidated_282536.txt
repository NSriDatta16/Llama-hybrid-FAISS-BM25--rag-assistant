[site]: crossvalidated
[post_id]: 282536
[parent_id]: 282524
[tags]: 
You can try LOOCV on the training set using a logistic regression model with an l1 penalty, or an SVM with an l1 penalty. Because you have N What I might do is LOOCV (leave one out cross validation) on your training set only and then use the testing set in a predictive context, to assess the model you created in the cross validation phase. scikit-learn has readily available tools for what I just described. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html for the model, you can set it to use "l1" as the penalty http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html for the classification metric http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html for the cross validation. Here's a possible set up: from sklearn import linear_model from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import LeaveOneOut from sklearn.metrics import roc_auc_score loocv = LeaveOneOut() clf = Pipeline([ ('scale', StandardScaler()), ('logistic_regression', linear_model.LogisticRegressionCV( penalty = 'l1', solver = 'liblinear', cv = loocv) )]) clf.fit(training_data, training_response) prediction = clf.predict(testing_dat) roc_auc = roc_auc_score(testing_response, prediction) In addition you can get information on which features were included in the model from : columns = clf.named_steps['logistic_regression'].coef_ columns_chosen = zip(data_columns, columns) and look at the nonzero pairs. Another possible approach is to combine all of your data and use LOOCV. Another line of attack could be using extra trees and or random forest. You could generate a variable of random data that you know has nothing to do with your response variable. Include that in your model and look at the feature importance rankings from the random forest fitting within your training data only. Discard any features whose importance was less than or equal to that of the random column. Use the selected features to predict on your testing set.
