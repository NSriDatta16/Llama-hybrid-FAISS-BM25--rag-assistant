[site]: crossvalidated
[post_id]: 163442
[parent_id]: 163388
[tags]: 
For a regression problem with $k$ variables (w/o intercept) you do OLS as $$\min_{\beta} (y - X \beta)' (y - X \beta)$$ In regularized regression with $L^p$ penalty you do $$\min_{\beta} (y - X \beta)' (y - X \beta) + \lambda \sum_{i=1}^k |\beta_i|^p $$ We can equivalently do (note the sign changes) $$\max_{\beta} -(y - X \beta)' (y - X \beta) - \lambda \sum_{i=1}^k |\beta_i|^p $$ This directly relates to the Bayesian principle of $$posterior \propto likelihood \times prior$$ or equivalently (under regularity conditions) $$log(posterior) \sim log(likelihood) + log(penalty)$$ Now it is not hard to see which exponential family distribution corresponds to which penalty type.
