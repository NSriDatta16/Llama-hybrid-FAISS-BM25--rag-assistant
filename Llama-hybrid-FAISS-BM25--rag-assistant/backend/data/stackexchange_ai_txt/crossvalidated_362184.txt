[site]: crossvalidated
[post_id]: 362184
[parent_id]: 362153
[tags]: 
I'm confused how comparing two mean values is different from doing a t-test. Can't I just see the two means and conclude if there's a difference? When you're "comparing two mean values" you are generally looking at two sample means. Of course these are going to be different (almost always). But the question people are usually interested in is not whether the specific arithmetic means in the observed samples were different. Naturally they expect that this will be the case. What is usually of interest is to use the samples to infer something about the populations from which the samples were drawn. For example, it may be of interest to know whether the sample means are further apart than we should reasonably see if the population means were in fact the same. How does ttest makes sure if the 2 means are statistically different? As an example, if you want to test whether a particular drug lowers blood pressure better than a placebo would, you might compare the average change (before minus after) for the treatment and placebo groups. Under typical assumptions, if the drug has no more effect than a placebo then we can work out what the distribution of a (standardized) difference in sample mean blood pressure change should look like. If the drug really has an effect we would expect the drop in blood pressure to tend to be larger than we might reasonably see if it did nothing more than the placebo. If the difference is large enough that it'd be highly surprising to see one (a mean blood pressure drop between treatment and placebo) at least that big if there were no effect, you're left with two possibilities (if your assumptions hold) -- the two population mean changes (blood pressure decreases) don't differ but a surprising outcome occurred anyway (akin to tossing at least 7 heads in 8 tosses of a fair coin, say), or the drug had some effect (above that of a placebo) and there's no need to invoke a minor miracle to explain such a large sample difference. If the difference in means (treatment vs placebo) is small, we will not find the "no effect" explanation hard to maintain (a difference at least that large could readily occur). As the difference becomes very large this explanation becomes more and more untenable. We have to decide at what point we will move from the first kind of explanation to the second. The t-test simply formalizes this process; we compute a standardized difference between the two mean drops in blood pressure (the t-statistic), and if a difference at least that large would be extremely rare (if the drug was no additional help above the placebo) we conclude that the only tenable explanation is that the population means do differ (that the drug has a bigger effect than could be accounted for by it being no better than placebo). Our significance level gives us the formal cut-off point (i.e. how rare is too rare to continue to accept that the 'no difference' explanation is believable). All the standard null-hypothesis significance tests (all the tests you're likely to learn about any time soon) use the same logic. What changes is the specific hypothesis, the test statistic and its distribution if the null hypothesis is true. (I have not fussed over the one-or-two tailed distinction here in the interest of brevity. For say a drug for blood pressure you'd want to also pick up the possibility that its effect on blood pressure is worse that placebo. You may also want to compare to a standard treatment. In practice there are pretty strict protocols for how such drug tests should be conducted which I have ignored.)
