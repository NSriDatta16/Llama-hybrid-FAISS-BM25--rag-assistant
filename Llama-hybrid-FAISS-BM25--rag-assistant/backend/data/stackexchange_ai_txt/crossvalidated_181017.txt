[site]: crossvalidated
[post_id]: 181017
[parent_id]: 97077
[tags]: 
Using only one feature for classification seems arbitrary. The way I see it, there are two possibilities. (i) increase number of features (ii) consider permutation tests. For (i), I can direct you to Fast Feature Selection . Also, I implemented the procedure for MATLAB and LIBSVM for binary classification. Code is found at github . For (ii), permutation tests require multiple runs (e.g. 100, 1000) of classifier evaluation where labels are randomly permuted. In the end accuracies are sorted by runs to obtain the significance level at p=0.05. Assume you did 100 permutation runs, then you obtain the significance level at position 5 or 95 depending on the ordering.
