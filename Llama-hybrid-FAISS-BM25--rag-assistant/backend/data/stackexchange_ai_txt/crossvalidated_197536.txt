[site]: crossvalidated
[post_id]: 197536
[parent_id]: 197529
[tags]: 
Not necessarily related, but still might be interesting if you're trying to think of machine learning in the broadest way possible. The "no free lunch theorem" states that overall, all machine learning models may be worse than random, as the relation between input and output may be pathologically complicated. You can read about it here . I think this is very closely related to your question, since the most important part of machine learning is feature selection & generation (and indeed most state of the art machine learning solutions stand out in their ability to "correctly" select the right feature representation). So my 2 cents on your question, which is vague as others have pointed out. A "better" machine learning model is only better in the sense that it makes implicit assumptions about the probability distribution of potential connections between input and output. Feature selection is part of the machine learning process. That is, when you select a certain feature and call it "perfect", you are per definition wrong in calling it that - it's a happy chance that this feature tends to perform well in the way realistic machine learning problems tend to behave. It's not "a priori" better than other feature generation methods. Hope that helped a bit.
