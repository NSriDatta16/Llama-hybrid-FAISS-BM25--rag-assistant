[site]: datascience
[post_id]: 106975
[parent_id]: 106956
[tags]: 
The main difference it that BERT includes attention mechanisms, whereas Doc2Vec doesn't. Attention mechanisms are functions to detect context between words, i.e. learning from words positions using attention weights. This gives a better result than classic embedding approaches like Doc2Vec, thanks to a contextual approach of data. On the other hand, BERT can handle out of vocabulary words because it uses subwords (example: "sub" + "word" + "s") instead of complete words (ex: "subwords"), which gives more meaningful information about the data.
