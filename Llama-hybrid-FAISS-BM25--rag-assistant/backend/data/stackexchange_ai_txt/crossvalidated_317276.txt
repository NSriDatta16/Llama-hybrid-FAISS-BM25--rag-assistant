[site]: crossvalidated
[post_id]: 317276
[parent_id]: 
[tags]: 
Combining errors from two groups of measurements

Suppose I have two ways of measuring some quantity, $a$ and $b$, where each method has its own intrinsic error, $\sigma_a$ and $\sigma_b$. I make repeated measurements with each method and produce two samples of data, each having a mean $\hat{\mu}_i$ and sample standard deviation $\hat{\sigma}_i$, $i \in \{a, b\}$. Each sample also has a standard error of the mean, estimated as $\sigma_i$ \ $\sqrt{n_i}$, where $n_i$ is the number of measurements with method $i$. What is the correct way to represent the underlying distribution based on these two sets of measurements? The mean should be some weighted average of the sample means, with weights given by a combination of $\sigma_a$, $\sigma_b$, $\hat{\sigma_a}$, $\hat{\sigma_b}$, $n_a$ and $n_b$. The standard deviation of the underlying distribution should be a function of those same quantities, I expect. In my first attempt, I found the mean and standard error of the mean for the two samples, and then combined them by multiplying the Gaussian distributions: $N(\mu_{true}, \sigma_{true}^2) \propto N(\mu_{a}, \frac{\sigma_{a}^2}{n_a}) \times N(\mu_{b}, \frac{\sigma_{b}^2}{n_b})$ (This was inspired from Kalman filter theory, in which one of the Gaussians would be coming from the process model rather than a measurement) What bothers me about this approach is that the sample standard deviations don't appear - if one measurement method is much less accurate (appearing as a much wider range of sample values), then I'd expect 1) the weight when computing the true mean to be less, and 2) the overall confidence in the true mean to be less. If I had to guess, I'd say the problem lies in my understanding of the relationship between the intrinsic error $\sigma_i$ and the sample standard deviation $\hat{\sigma_i}$
