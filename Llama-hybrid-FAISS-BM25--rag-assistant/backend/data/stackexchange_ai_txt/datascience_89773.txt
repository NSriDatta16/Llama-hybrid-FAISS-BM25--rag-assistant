[site]: datascience
[post_id]: 89773
[parent_id]: 89755
[tags]: 
So it´s a classification problem with a grid-search, without cross-validation. Yes, don´t use cv in time series data. There is an option, in which you can use cv, when you slowly start with less data and put more and more data during the process. But it´s complex. For the grid-search are 2 opportunities. Either use GridSearchCV and define cv as none, or you use ParameterGrid(). For my interest I used this method: https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning/44682305#44682305 in which is GridSearchCV defined as none. import pandas as pd test = pd.DataFrame({"id":[1,2,3,4,5,6,7,8,9], "age":[20,30,32,40,55,32,20,41,38], "gender":[0,1,0,1,0,0,1,1,0], "m1":[12.4, 30,9.4,14,19,20,34,31,16], 'm2':[34,36,22,16,22,27,42,65,13], 'label':[0,0,1,1,0,1,1,1,0]}) test.head() from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score X = test.drop('label', axis=1) y = test.label X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) rf_params = {'n_estimators': [100, 200], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 50], 'min_samples_split': [2, 20]} cv=[(slice(None), slice(None))] rf_clf = GridSearchCV(RandomForestClassifier(random_state=42),rf_params, n_jobs=-1, verbose=2, cv=cv) rf_clf.fit(X_train, y_train) #best parameters of model print(rf_clf.best_params_) #make predictions rf_pred = rf_clf.predict(X_test) print('Accuracy', accuracy_score(rf_pred, y_test)) The GridSearchCV part shows me: GridSearchCV(cv=[(slice(None, None, None), slice(None, None, None))], estimator=RandomForestClassifier(random_state=42), n_jobs=-1, param_grid={'max_depth': [10, 50], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 20], 'n_estimators': [100, 200]}, verbose=2) So this method works. Here I used random forest, because in my own experience, random forest is in most cases very good. In big datasets, the SVC takes too much time. PS: Before I forget, I changed the gender into numbers. You can use one-hot encoding for that or catboost, which can do this automatically. But with catboost you get different results in comparison with rf or other algorithms. So I prefer to transfer gender into numbers.
