[site]: crossvalidated
[post_id]: 400785
[parent_id]: 
[tags]: 
In Bayesian inference, why are some terms dropped from the posterior predictive?

In Kevin Murphy's Conjugate Bayesian analysis of the Gaussian distribution , he writes that the posterior predictive distribution is $$ p(x \mid D) = \int p(x \mid \theta) p(\theta \mid D) d \theta $$ where $D$ is the data on which the model is fit and $x$ is unseen data. What I don't understand is why the dependence on $D$ disappears in the first term in the integral. Using basic rules of probability, I would have expected: $$ \begin{align} p(a) &= \int p(a \mid c) p(c) dc \\ p(a \mid b) &= \int p(a \mid c, b) p(c \mid b) dc \\ &\downarrow \\ p(x \mid D) &= \int \overbrace{p(x \mid \theta, D)}^{\star} p(\theta \mid D) d \theta \end{align} $$ Question: Why does the dependence on $D$ in term $\star$ disappear? For what it's worth, I've seen this kind of formulation (dropping variables in conditionals) other places. For example, in Ryan Adam's Bayesian Online Changepoint Detection , he writes the posterior predictive as $$ p(x_{t+1} \mid r_t) = \int p(x_{t+1} \mid \theta) p(\theta \mid r_{t}, x_{t}) d \theta $$ where again, since $D = \{x_t, r_t\}$ , I would have expected $$ p(x_{t+1} \mid x_t, r_t) = \int p(x_{t+1} \mid \theta, x_t, r_t) p(\theta \mid r_{t}, x_{t}) d \theta $$
