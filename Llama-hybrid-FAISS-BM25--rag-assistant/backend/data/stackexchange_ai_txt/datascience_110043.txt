[site]: datascience
[post_id]: 110043
[parent_id]: 110006
[tags]: 
word2vec is an algorithm to train word embeddings: given a raw text, it calculates a word vector for every word in the vocabulary. These vectors can be used in other applications, thus they form a pretrained model. It's important to understand that the model (embeddings) depends a lot on the data they are trained on. Some simple applications can simply use a general pretrained model, but some specific applications (for example specific to a technical domain) require the embeddings to be trained on some custom data.
