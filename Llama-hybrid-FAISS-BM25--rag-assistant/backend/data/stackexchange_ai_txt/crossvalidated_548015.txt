[site]: crossvalidated
[post_id]: 548015
[parent_id]: 
[tags]: 
The length of spectral density is longer than the data using spectrum() in R

I'm using spectrum(method = "pgram") in R to calculate the spectral density in my time series. spectrum() returns the spectral density for each frequency(from 1/n, 2/n to 1/2, n is the time series length), that means I should get a series of spectral density with the length = n/2, and the first cycle should be n. However, when I use the R built-in data "sunspot.month", I found the length of spectral density gets longer than the data when the data has some certain lengths. Here is the code: ## Total data length = 3177 ## length(sunspot.month) ## results are expected ## sunspot_spec = spectrum(sunspot.month[1:400],plot = F) length(sunspot_spec $freq) 1/sunspot_spec$ freq[1] ## returned length is 864, longer cycle ## sunspot_spec = spectrum(sunspot.month[1:1660],plot = F) length(sunspot_spec $freq) 1/sunspot_spec$ freq[1] ## returned length is 1600, longer cycle ## sunspot_spec = spectrum(sunspot.month[1:3150],plot = F) length(sunspot_spec $freq) 1/sunspot_spec$ freq[1] I didn't use any smoothing in the periodogram. I'm not sure what R is doing and why longer cycle is used. I'm new to signal processing. Any help/comments/inputs is appreciated. Thanks!
