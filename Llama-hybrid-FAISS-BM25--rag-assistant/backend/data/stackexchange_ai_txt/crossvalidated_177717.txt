[site]: crossvalidated
[post_id]: 177717
[parent_id]: 141529
[tags]: 
The typical negative log$_2$ probability $P(x_i)$ of a data vector $x_i$ refers to the average of $-$log$_2P(x)$, namely the entropy, which is given by $$\text{Entropy}[P(x)]= -\sum_i P(x_i)\text{log}_2P(x_i),$$ where the sum is over all the possible data vectors. Basically, the expression tells you how many bits of information you get when you draw a typical sample out of the data set. Entropy also represents the minimum number of bits one can use to compress the data set without loss of information. If the probability distribution $P(x)$ is not known in advance, one can estimate it by analyzing the statistics of the data. Principle component analysis (PCA) can be used to evaluate how redundant a data set is, and thus get you closer to the real value of the entropy, but it has its limits. PCA only yields the most compact linear representation for the data. In one of his reviews on the topic, Yoshua Bengio mentions that it's always better to have an overcomplete set of parameters than an undercomplete set. When in doubt, use slightly more units! See http://arxiv.org/pdf/1206.5533v2.pdf , section 3.2; it's short and also somewhat vague, but that is just an indication of how neural networks depend so much on the application that there is always plenty of experimenting to do to reach the optimal strategy. To find a more compressed nonlinear representation of the data, someone suggests using Projection Pursuit ( http://cseweb.ucsd.edu/~dasgupta/254-deep/nakul.pdf ), but I have never used it. It's maybe worth looking into.
