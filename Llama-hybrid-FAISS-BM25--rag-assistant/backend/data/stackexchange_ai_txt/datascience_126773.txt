[site]: datascience
[post_id]: 126773
[parent_id]: 
[tags]: 
Unsupervised Log Anomaly Detection

I am thinking about using the variational autoencoder model for anomaly detection . I have an Android Logs dataset. As the logs generated are a representative of time series type of data I thought about using the VAEs with an architecture involving LSTMs in the encoder and decoders. As far as what I know about the basics of the working of the autoencoder models , given a sequence size say n , during learning/training phase n-1 entries are used to generate a representation/prediction of the nth entry (log in this case) and then reconstruction error is minimized by the model. How could I approach including the preprocessing steps and any specific architecture for the VAE model that would result in higher accuracy ? I was also thinking about converting the logs into word embeddings using Word2Vec and then using them as input for the VAE model.
