[site]: datascience
[post_id]: 40143
[parent_id]: 13405
[tags]: 
Affine transformation is of the form, $$ g(\vec(v) = Av+b $$ where, $A$ is the matrix representing a linear transformation and $b$ is a vector. In other words, affine transformation is the combination of linear transformation with translation. Linear transformation always carry vector $b$ = 0 in the source space to 0 in target space. E.g $ y=3x + 4$ , in school we called it linear equation, but it is not speaking strictly about linear transformation, because it has translation (+4), and linear transformation don't do that. so, every linear transformation is affine (just set b to the zero vector). However, not every affine transformation is linear. Now, in context of machine learning, linear regression attempts to fit a line on to data in an optimal way, line being defined as , $ y=mx+b$ . As explained its not actually a linear function its an affine function. And probably should be renamed. Its good to get the terminologies right. Similarly, in a single layer of neural network is often expressed mathematically as: $$y(\vec{x})=W\vec{x}+\vec{b} $$ $W$ is the weight matrix and $\vec{b}$ is the bias vector. This function is also usually referred to as linear although it's actually affine.
