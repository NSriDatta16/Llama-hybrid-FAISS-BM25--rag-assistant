[site]: crossvalidated
[post_id]: 310294
[parent_id]: 
[tags]: 
Why Logistic and Linear Regressions P-value results are so different?

I have a data: A B C D Successes Trials Rate 0 0 0 0 19 19000 0,100000 0 0 1 1 21 19000 0,110526 0 1 0 1 17 19000 0,089474 0 1 1 0 21 19000 0,110526 1 0 0 1 15 19000 0,078947 1 0 1 0 22 19000 0,115789 1 1 0 0 17 19000 0,089474 1 1 1 1 21 19000 0,110526 So if I run Linear Regression where outputs are Number of Successes or Success Rate, I get results with C variable as significant. However, if I run Logistic Regression the variable C is not significant. **Regression Analysis: Successes versus C** Method Categorical predictor coding (1; 0) Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 1 36,125 36,125 24,77 0,003 C 1 36,125 36,125 24,77 0,003 Error 6 8,750 1,458 Total 7 44,875 Model Summary S R-sq R-sq(adj) R-sq(pred) 1,20761 80,50% 77,25% 65,34% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 17,000 0,604 28,15 0,000 C 1 4,250 0,854 4,98 0,003 1,00 Regression Equation Successes = 17,000 + 0,0 C_0 + 4,250 C_1 **Regression Analysis: Rate versus C** Method Categorical predictor coding (1; 0) Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 1 0,001001 0,001001 24,77 0,003 C 1 0,001001 0,001001 24,77 0,003 Error 6 0,000242 0,000040 Total 7 0,001243 Model Summary S R-sq R-sq(adj) R-sq(pred) 0,0063560 80,50% 77,25% 65,33% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 0,08947 0,00318 28,15 0,000 C 1 0,02237 0,00449 4,98 0,003 1,00 Regression Equation Rate = 0,08947 + 0,0 C_0 + 0,02237 C_1 **Binary Logistic Regression: Successes versus C** Method Link function Logit Categorical predictor coding (1; 0) Rows used 8 Response Information Event Variable Value Count Name Successes Event 153 Event Non-event 151847 Trials Total 152000 Deviance Table Source DF Adj Dev Adj Mean Chi-Square P-Value Regression 1 1,8947 1,89470 1,89 0,169 C 1 1,8947 1,89470 1,89 0,169 Error 6 0,5072 0,08453 Total 7 2,4019 Model Summary Deviance Deviance R-Sq R-Sq(adj) AIC 78,88% 37,25% 2419,72 Coefficients Term Coef SE Coef VIF Constant -7,018 0,121 C 1 0,223 0,163 1,00 Odds Ratios for Categorical Predictors Level A Level B Odds Ratio 95% CI C 1 0 1,2503 (0,9088; 1,7201) Odds ratio for level A relative to level B Regression Equation P(Event) = exp(Y')/(1 + exp(Y')) Y' = -7,018 + 0,0 C_0 + 0,223 C_1 Goodness-of-Fit Tests Test DF Chi-Square P-Value Deviance 6 0,51 0,998 Pearson 6 0,51 0,998 Hosmer-Lemeshow 0 0,00 * As I understood one of the reason that Linear Regression and ANOVA could be used for binomial output is described in the chapter "DOE with Categorical Inputs and Outputs" (p. 399 Introduction to Engineering Statistics and Lean Sigma by Theodore Allen): " In general, none of the design of experiments and regression methods in this and previous chapters are appropriate if the response is categorical, e.g., conforming or non-conforming to specifications. Logistic regression and neural nets described in the next chapter are relevant when outputs are categorical. However, if each experimental run is effectively a batch of “b” successes or failures, then the fraction non-conforming can be treated as a continous response. Moreover, if the batch size and true fraction non-conforming satisfies the following, then it is reasonable to expect that the residuals in regression will be normally distributed: b × p0 > 5 and b × (1 – p0) > 5. (15.16) This is the condition such that binomial distributed random probabilities can be approximated using the “normal approximation” or normal probability distribution functions."
