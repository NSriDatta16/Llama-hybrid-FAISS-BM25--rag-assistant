[site]: crossvalidated
[post_id]: 588217
[parent_id]: 587902
[tags]: 
Whether or not you consider this from a Bayesian perspective, the suggestion by Camille Gontier (+1) to use simulations is probably your most useful approach. The risk at one extreme is having unrealistically low variance due to making multiple measurements over a geographically restricted area with just a few sites, leading to potential pseudoreplication and poor applicability of your model to new sites. The risk at the other extreme is sampling over many sites with too few replicates in each, so that the associations of interest might be dwarfed by variability among sites (like baseline abundances of species) that is poorly controlled for in your data. Simulating data based on the literature and your understanding of the subject matter forces you to consider the various sources of variability within and between sites and to decide on a range of realistic estimates. Trying to model those simulated data forces you to think carefully about your modeling strategy and to evaluate the implications of those types of variability for the best combination of site numbers and squares within sites, consistent with your budget. My guess is that you should maximize the number of sites provided that you can still have a few observations (squares) within each site. For example, if you treat the sites as random effects (clusters) in a mixed model, you typically want many clusters . As Robert Long put it : "In general, the number of clusters is more important than the number of observations per cluster." But the specifics of your situation might indicate otherwise. Informed simulations should point the way.
