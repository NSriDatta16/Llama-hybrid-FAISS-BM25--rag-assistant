[site]: crossvalidated
[post_id]: 614158
[parent_id]: 
[tags]: 
RMSE of Training data is lower compared to test dataset

First of all, this is not a case of Overfitting. The task is to forecast Temp using univariate Single Step forecasting. I have trained the LSTM model with on jena climate dataset(Dataset https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip .) Following are steps Extract Temp as a series Divide in train(60000), test(5035) and Val(5000) data Convert one-dimensional time series to a supervised model using lag features For example: Sequence 1 to 8 is converted with a window of 5 as below [[[1], [2], [3], [4], [5]]] [6] [[[2], [3], [4], [5], [6]]] [7] [[[3], [4], [5], [6], [7]]] [8] Code for it is... def df_to_X_y(df, window_size=5): df_as_np = df.to_numpy() X = [] y = [] for i in range(len(df_as_np)-window_size): row = [[a] for a in df_as_np[i:i+window_size]] X.append(row) label = df_as_np[i+window_size] y.append(label) return np.array(X), np.array(y) Data is fed to LSTM mode by following the config model1 = Sequential() model1.add(InputLayer((24, 1))) model1.add(LSTM(64)) model1.add(Dense(8, 'relu')) model1.add(Dense(1, 'linear')) model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError(),MSE_matrics()]) history1 = model1.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=10, callbacks=[cp1]) print(model1.evaluate(X_train1, y_train1)) print(model1.evaluate(X_val1, y_val1)) print(model1.evaluate(X_test1, y_test1)) This is going to print RMSE or SSE or any other but train error remains less compared to test error. Experiment 2: I tried different configs for train, test validation split like, train(5000), Val(30000), test(5000) Now in this cal train errors are less compared to the test. Question is: are these errors increasing with the number of samples? if so how we can report train-test-val error and confirm that model is doing well?
