[site]: stackoverflow
[post_id]: 2597635
[parent_id]: 2597407
[tags]: 
I have a "items" table with 1 million rows and a "users" table with 20,000 rows. That is, independently of whether you JOIN or denormalize, you would still transfer roughly 1M/20k = 50 times more User information over the wire than strictly necessary. Encoding, transferring and then decoding data adds load. I'm considering adding a username column to the items table and removing the join. Why are you then, in your original JOIN , also bringing over all this other (potentially voluminous) information (such as User.profileurl , User.homepage etc.) if all you need is the user name? Remember, for User columns, you are transferring on average 50 copies of each bit of information. Have you considered drastically trimming down the columns you are SELECT ing from in the JOIN (both from the User as well as the Item tables?) I'm asking stackoverflow because benchmarks aren't telling me too much. Both queries finish very quickly. Regardless, I'm wondering if removing the join would lighten load on the database to any significant degree. In a first phase, removing columns you do not intend to use can reduce load, as less data has to be encoded, transferred (from server to client application) then decoded. In a second phase , let me start with a question of my own: do you really need all million rows in one shot? If you do not, e.g. if you are user-interface driven and you paginate them (using OFFSET ... LIMIT ... ), then you will not necessarily care about the 50x User information duplication (unless the LIMIT gets into the tens of thousands.) Otherwise, you may want to measure the advantage of eliminating the 50x duplication by first SELECT ing only User.id and User.username into application memory (20k pairs, into a hash-table/map), then SELECT ing only Item rows (1M iterations) everytime resolving, at the application level, Item.user_id against the hash-table/map. Of course, always use EXPLAIN to ensure that the proper indices exist and are being used when an index should be used, and run ANALYZE TABLE after any of your tables grow from under a few hundred rows to thousands or millions.
