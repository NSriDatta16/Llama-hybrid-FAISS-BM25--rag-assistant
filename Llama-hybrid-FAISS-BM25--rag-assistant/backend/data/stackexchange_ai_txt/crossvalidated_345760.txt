[site]: crossvalidated
[post_id]: 345760
[parent_id]: 345745
[tags]: 
The general procedure is to compute an information theoretic criterion (e.g. AIC), and use it to compute weights which can then be used to average model predictions, or sometimes model parameters. A great book on the subject is Burnham & Anderson, 2002. Model Selection and Multimodel Inference . doi: 10.1007/b97636 . In brief: compute the AIC values and transform them into differences with respect to the best model (the one with smaller AIC)${\Delta _m} = {\rm{AI}}{{\rm{C}}_m} - \min {\rm{AIC}}$. These can be used to compute an estimate of the relative likelihood of the model $m$ given the data $\mathcal{L} \left( {m|{\rm{data}}} \right) \propto \exp \left( { - \frac{1}{2}{\Delta _m}} \right)$. To obtain the Akaike weight $w_m$ of the model $m$ these relative likelihoods need to be normalized so as to sum to 1 (divided by their sum) $${w_m} = \frac{{\exp \left( { - \frac{1}{2}{\Delta _m}} \right)}}{{\mathop \sum \limits_{k = 1}^K \exp \left( { - \frac{1}{2}{\Delta _k}} \right)}}$$ the model predictions can then be combined by multiplying them according to the relative weight $w_m$. I am not very familiar with negative binomial models, however I tried to make an example using the example in the help page of the function glm.nb # fit models library(MASS) quine.nb1
