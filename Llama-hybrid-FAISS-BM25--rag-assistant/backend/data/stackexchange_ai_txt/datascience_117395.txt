[site]: datascience
[post_id]: 117395
[parent_id]: 
[tags]: 
What does the random seed influence in transformers?

I've been doing some experiments with setting different seeds on transformer training, and wanted to understand why I see so much variance. Am I correct in thinking the random seed only influences two things when transformer training: The random sorting of the training data; Which connections get removed by dropout. Oh, and the initial value of the randomly-initialized weights. If so, that implies there is no stochastic element at all when using the model for inference? Maybe my question would be better phrased as: what functions use randomness, so I can search a codebase for them, and confirm where they are used? (I'm mainly looking at PyTorch implementations such as fairseq and huggingface; but I am assuming tensorflow implementations handle random numbers the same way.)
