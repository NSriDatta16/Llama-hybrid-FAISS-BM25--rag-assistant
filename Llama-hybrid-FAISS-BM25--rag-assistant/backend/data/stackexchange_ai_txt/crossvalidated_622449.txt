[site]: crossvalidated
[post_id]: 622449
[parent_id]: 22501
[tags]: 
I think it is more intuitive to think of $A^\top A$ as a bilinear form than as a linear map. Say we have a linear transformation $\mathbb{R}^n \to \mathbb{R}^m$ with matrix $A$ (with respect to the standard basis $\vec{e}_1, \vec{e}_2, \dots \vec{e}_n$ ). For an example we can visualize, picture $\mathbb{R}^2 \to \mathbb{R}^3$ given by $$ A = \begin{bmatrix} 1 & 1\\ 1 & 2\\ 1 & 0 \end{bmatrix} $$ Call the columns $$ \vec{v}_1 = \begin{bmatrix}1 \\ 1 \\ 1 \end{bmatrix} \,\,\textrm{ and }\,\, \vec{v}_2 = \begin{bmatrix}1 \\ 2 \\ 0 \end{bmatrix} $$ We visualize this linear transformation as a way to "stretch" $\mathbb{R}^2$ onto the plane $H = \textrm{span}(\vec{v}_1, \vec{v}_2)$ by sending $\vec{e}_1 \mapsto \vec{v}_1$ and $\vec{e}_2 \mapsto \vec{v}_2$ . $A^\top A$ is a gadget which lets us compute the dot product of the images of vectors from $\mathbb{R}^2$ in the plane $H$ . The computation is simple. Let $\vec{v}, \vec{w} \in \mathbb{R}^2$ be two vectors in the domain of the transformation. $$ \begin{align*} (A \vec{v}) \cdot (A \vec{w}) & = (A\vec{w})^\top (A \vec{v})\\ &= \vec{w}^\top A^\top A \vec{v} \end{align*} $$ This allow us to interpret many of the uses of $A^\top A$ in linear algebra and statistics in a nice geometric way. For instance, let's try to understand how to project a vector $\vec{y}$ into the plane $H$ , and how this relates to OLS linear regression. Let the projected vector be called $\textrm{proj}_H(\vec{y})$ . We certainly want $$ \vec{y} = \textrm{proj}_H(\vec{y}) + \vec{y}^\perp $$ for some vector $\vec{y}$ which is perpendicular to $\vec{H}$ . We also want $\textrm{proj}_H(\vec{y})$ to be in $H$ , which is to say, it should be in the image of $A$ . Say we have $\textrm{proj}_H(\vec{y}) = A \vec{\beta}$ (here I am thinking of $\beta$ as the vector of coefficients of $\vec{v}_1$ and $\vec{v}_2$ ). So we have $$ \vec{y} = A\vec{\beta} + \vec{y}^\perp $$ Taking $A^\top$ of both sides we have $$ A^\top \vec{y} = A^\top A \vec{\beta} + \vec{0} $$ since $A^\top$ dots all of the columns of $A$ with $\vec{y}^\perp$ , and these should all be zero. We can directly solve this to get $$ \beta = (A^\top A)^{-1} (A^\top \vec{y}) $$ which is the "normal equation". So the actual projection is then $\textrm{proj}_H(\vec{y}) = A \vec{\beta} = A (A^\top A)^{-1}(A^\top \vec{y})$ However, I think it is also instructive to back up a step and think about what this means from a geometric perspective. Starting with $A^\top \vec{y} = A^\top A \vec{\beta}$ we can dot both sides with $\alpha$ where $\alpha$ is some other potential weighting of the columns. Then we have $$ \begin{align*} \vec{\alpha}^\top A^\top A \vec{\beta} &= \vec{\alpha}^\top A^\top \vec{y}\\ (A \vec{\alpha})\cdot (A \vec{\beta}) & = (A \vec{\alpha}) \cdot \vec{y} \end{align*} $$ This is saying that, of all the vectors in $H$ , $A \vec{\beta}$ is the vector whose dot product with any other vector $\vec{w} = A \vec{\alpha} \in H$ is exactly the same as the dot product of $\vec{y}$ with $\vec{w}$ . This is a nice rephrasing of what it means to find the "best approximation" of $\vec{y}$ by a vector in $H$ , but which avoids talking about any vectors "outside" of $H$ other than $\vec{y}$ . How does this relate to OLS linear regression? Say we have explanatory variables $x_1, x_2, ..., x_k$ . We have $N$ observations which pair $(x_{1i}, x_{2i}, ..., x_{ki})$ with $y_i$ . We want to find $\beta_i$ of the form $y = \beta_0 + \sum_{i = 1}^N \beta_i x_i = $ which fits this data while minimilizing MSE. We can reframe this as attempting to minimize $$ |\vec{y} - X\beta|^2 $$ where $\vec{y} \in \mathbb{R}^N$ is the vector with coordinates $y_i$ and $X$ is the matrix whose first column is all $1$ , and whose subsequent columns have coordinates $x_{ij}$ . In other words, we are attempting to project $\vec{y}$ into the hyperplane which is the span of $X$ ! This is exactly the problem we solved above. Another intuition coming from this interpretation: Given a matrix $A: \mathbb{R}^n \to \mathbb{R}^m$ , consider the Rayleigh quotient map $R: S^{n-1} \to \mathbb{R}$ given by $$ R(\vec{u}) = \vec{u}^\top A^\top A \vec{u} = |(A \vec{u})|^2 $$ The last expression gives this map a direct geometric interpretation: we take a unit vector, apply $A$ and square its length. When we apply $A$ to the unit sphere we obtain an ellipsoid in the span of the columns of $A$ . So $R(\vec{u})$ is keeping track of the squared norm of this point on an ellipsoid. The image of critical points of this map represent the vertexes of this ellipsoid. Imagine $\vec{v} \in \mathbb{S}^{n-1}$ . Because of the geometry of the sphere, every tangent vector $\vec{w}$ is perpendicular to $\vec{v}$ . Notice that $A \vec{w}$ will still be a tangent vector to the ellipse at $A \vec{v}$ . It is geometrically obvious to me (think about an ellipse!) that $A(\vec{v}) \perp A(\vec{w})$ is zero for all $\vec{w} \in \vec{v}^\perp$ if and only $A\vec{v}$ is a vertex of the ellipsoid. In other words, the tangent plane to an ellipsoid is only perpendicular to a radial vector at the vertexes. So the critical points $\vec{v}$ of $R$ satisfy $$ A(\vec{v}) \cdot A(\vec{w}) = 0 \textrm{ for all } \vec{w} \in \vec{v}^\perp $$ This allows us to easily see that the critical points of $R$ are eigenvectors of $A^\perp A$ . Let $\vec{v}$ be a critical point of $R$ . Write $A^\perp A (\vec{v})$ in the form $\lambda \vec{v} + \vec{w}$ for $\vec{w} \in \vec{v}^\perp$ . Dotting both sides with $\vec{w}$ gives us: $$ \begin{align*} \vec{w}^\top A^\top A \vec{v} &= \vec{w}^\top (\lambda \vec{v} + \vec{w})\\ (A \vec{w}) \cdot (A\vec{v}) &= |\vec{w}|^2 \\ |\vec{w}|^2 &= 0 \end{align*} $$ So $\vec{v}$ is an eigenvector of $A^\top A$ . This is equivalent to the usual derivation with Lagrange Multipliers. The only difference is that I replaced the differential calculus with the geometric fact that the radial vector of an ellipsoid is perpendicular to the tangent plane only at the vertexes. So we can interpret the eigenvectors of $A^\top A$ as being the critical points of the map $R$ which has a very natural geometric interpretation. For instance, the interpretation of the eigenvector $\vec{v}_1 $ with the greatest eigenvalue $\lambda_1$ is " $A(\vec{v}_1)$ is the vector on the ellipsoid $\textrm{Image}(S^{m-1})$ which is furthest from the origin". This is the first principle component in PCA. The other principle components are the other axes of this ellipsoid! How does this relate to statistics? Using the same notation as above let $\vec{X}$ as our design matrix and $\vec{y}$ be our observed outcome vector. Let the orthonormal eigenvectors of $X^\top X$ be $\vec{v}_i$ with eigenvalues $\lambda_i$ with this sequence of eigenvalues decreasing in value. Then we know that we want $$ X^\top \vec{y} = X^\top X \vec{\beta} $$ Writing $\vec{\beta}$ in the basis of eigenvectors we have $$ X^\top \vec{y} = \sum_{i=1}^{k+1} \alpha_i \vec{v}_i $$ If we eliminate all of the terms from this sum with "small" eigenvalues $|\alpha_i| \leq \epsilon$ then the new $\vec{\beta}'$ will live in the lower dimensional subspace generated by only the "large" eigenvectors, but as we can see $X^\top X \vec{\beta}'$ will still be "close" to $X^\top \vec{y}$ . In this sense, we could consider projecting $\vec{y}$ onto first just the largest eigenvector, then the first two, then the first three, etc. As we do so the projection will approach the true regression, but we have diminishing returns with smaller eigenvalues. It is interesting that the highest eigenvalue terms are carrying "most of the information" about the regression. Note: To train your geometric intuition a bit more, consider the following three ellipsoids: The image of $S^2$ under: The identity transformation A stretch by a factor of $3$ in the $x$ direction only. A stretch by a factor of $3$ in the $x$ direction, and $2$ in the $y$ direction. In (3) we have eigenvalues $3$ , $2$ , $1$ in the directions $e_1$ , $e_2$ , $e_3$ and these are our only choices. In (2) we have eigenvalues $3$ and $1$ . Our eigenspace for $\lambda = 1$ is 2 dimensional and is spanned by $e_2$ and $e_3$ . In (1) we have eigenvalue $1$ and the eigenspace is the entire domain! For a "generic map" $A$ we should expect that the ellipsoid $\textrm{Im}(A)$ will be an ellipsoid whose axis lengths are all distinct, so $A^\top A$ will usually have a diagonalization. However it is possible (though extremely rare for real data) that two of the axes could have the same length leading to higher dimensional eigenspaces. This would give us some choice in the selection of an orthogonal basis for this eigenspace.
