[site]: crossvalidated
[post_id]: 511916
[parent_id]: 
[tags]: 
Is consecutive training of the same NN with different loss/cost functions a valid technique?

I am training a regression CNN using Python 3.8 and TensorFlow 2+ with a 92 Softmax output. At first I was using Mean Squared Error as my network's loss function, which was good at predicting large numbers (e.g any value above 0.2) but it was very bad at predicting small values (e.g. 0.04, etc.). To fix this I tried using Mean Squared Logarithmic Error which did indeed fix it but also messed up the predictions of large values. So my question is: Can I train the CNN using MSLE and then recompile it and train it again using MSE? My logic is that through each training, the loss function would adjust the neurons responsible for large and small values predictions, Is that correct? For your reference, below is my code, but I do not imagine you will need it to answer the question. BSize = 450 l_r = 8e-4 epchs = 100 drop = 0.1 bn_momentum = 0.8 cost = 'mse' opt = Adam(learning_rate=l_r)#, amsgrad=True) # opt = SGD(learning_rate=l_r, momentum=0.89, nesterov=True) # opt = AdaBound(learning_rate=l_r, amsgrad=True) start = timeit.default_timer() model = Sequential() '''The variable nmbr_of_LAC is from another code ''' model.add(Conv1D(filters=64, kernel_size=4, activation='relu', input_shape=(nmbr_of_LAC, 1))) model.add(Conv1D(filters=96, kernel_size=4, activation='relu')) model.add(AveragePooling1D(pool_size=2, strides=2)) model.add(Conv1D(filters=256, kernel_size=3, activation='relu')) model.add(Conv1D(filters=512, kernel_size=3, activation='relu')) model.add(MaxPooling1D(pool_size=3, strides=2, padding='same')) model.add(BatchNormalization(momentum=bn_momentum, trainable=False)) model.add(Flatten()) model.add(Dense(1600, activation='relu', use_bias=False)) model.add(Dense(576, activation='relu', use_bias=False)) model.add(Dense(92, activation='softmax', use_bias=False)) model.summary() # Compiling & Fitting reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.1, patience=4, min_lr=1e-7, min_delta=0.4e-4, verbose=1) early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, min_delta=0.2e-4, verbose=1) model.compile(loss=cost, optimizer=opt , metrics=['mse', 'msle']) model.fit(x=x_train[:], y=y_train[:], validation_data=(x_val,y_val), epochs=epchs, batch_size=BSize, verbose=1, callbacks=[early_stopping, reduce_lr])
