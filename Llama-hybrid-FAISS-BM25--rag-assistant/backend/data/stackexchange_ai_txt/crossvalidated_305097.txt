[site]: crossvalidated
[post_id]: 305097
[parent_id]: 305085
[tags]: 
First, $R^2$ is not a measure of effect size but rather of in-sample model fit. When people ask about effect size, they commonly want average marginal effects. In OLS regression, this is simple: $$ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon $$ $$ \frac{\partial y}{\partial x_1} = \beta_1 $$ Which is to say the the effect of a one-unit change in $x_1$ on $y$ is equal to $\beta_1$. This is less simple in a nonlinear model like a gamma GLM. Say you're using a log-link: $$ E[y] = e^\left(\beta_0 + \beta_1x_1 + \beta_2x_2\right) $$ By the chain rule: $$ \frac{\partial y}{\partial x_1} = \beta_1 e^\left(\beta_0 + \beta_1x_1 + \beta_2x_2\right) $$ This just shows that you need to specify values of $x_1$ and $x_2$ in order to compute a number. The simplest thing to do is to specify two values each for $x_1$ and $x_2$, generally holding one or the other constant, and take the difference in their predictions to be the measure of effect size. For example, say that x1 and x2 are both binary. The average marginal effect of x1 is computed (in pseudocode because IDK anything about SPSS): y1 = predict(model, x1 = 0, x2 = mean(x2)) y2 = predict(model, x1 = 1, x2 = mean(x2)) AME = y2 - y1 Obviously you could choose another value for x2 than the mean, and that choice should depend on your context. But the more imporant point is that nonlinear models force you to make some choice, which is part of the reason that many researchers prefer linear models, even when their assumptions are violated.
