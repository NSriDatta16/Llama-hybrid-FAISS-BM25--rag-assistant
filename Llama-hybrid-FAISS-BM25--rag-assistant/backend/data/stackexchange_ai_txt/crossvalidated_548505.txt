[site]: crossvalidated
[post_id]: 548505
[parent_id]: 
[tags]: 
Look ahead bias in the skip connection of the Transformer-decoder/GPT2 architecture

How come the residual connection on the attention module in Decoder-Transformers/GPT2 does not cause a look ahead bias? This is my current understanding: GPT is similar to the decoder side of the Transformer architecture. The masked self attention module and the Residual connection are the same as in the Transformer. However, the second self-attention is not present, and the input data is passed to it. Attention on the decoder side uses a mask to avoid look ahead bias, i.e. input data attending any future data. A Residual connection on an input x returns x + F(x) instead of F(x) , where x is the input sequence. Residual connections are needed to stabilize back propagation. In this case, F(x) is masked attention, but x is not. This leads to my question: Why doesn't the Residual connection cause a look ahead bias? Maybe because masking the attention is sufficient, for some reason?
