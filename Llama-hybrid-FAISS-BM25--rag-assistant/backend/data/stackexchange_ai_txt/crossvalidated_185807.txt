[site]: crossvalidated
[post_id]: 185807
[parent_id]: 185800
[tags]: 
I would recommend moving on one more page, to page 251 of ISLR, and using ridge regression as illustrated there instead to get your "best" model. If you repeat a search for the "best" variable-selection model on multiple bootstrap samples of the same data set, even with identical criteria for the "information criterion," you will almost certainly get a surprisingly large collection of "best" models. Try it on your data and see. So which one is really "best"? Will any generalize well? How will you correct for the bias introduced by looking for the "best" predictors in a particular data sample? Unless you have a compelling need to minimize the number of predictor variables, ridge regression will give you an appropriately penalized model with a penalty chosen by cross-validation. Unlike variable-selection approaches, ridge regression handles collinearity among predictor variables well; it is likely to generalize well and the ridge coefficients should vary relatively little among bootstrap samples. If your interest is in prediction then the retention of all variables by ridge regression is likely to provide better performance than any variable-selection model. The glmnet package used in the exercise starting on page 251 includes facilities for ridge regression on generalized linear models, including logistic regression if you use the setting family = "binomial" . If you have a compelling need to cut down on predictor variables you can tweak the glmnet parameters to perform LASSO or the hybrid elastic net. The choice of penalty (and thus the number of variables) can be determined by cross-validation. As with any variable-elimination technique, however, there will still be some arbitrary selection among individual collinear predictors in models developed those ways.
