[site]: datascience
[post_id]: 28948
[parent_id]: 
[tags]: 
Why use SOM for clustering?

Suppose I have a dataset with $M$ observations of dimension $N$. I've seen many people use Kohonen networks (or self-organizing maps, SOMs) to do this task. In this approach, the neurons have weights of the same dimension of the observations, $N$. The idea is to train the network so that each node $w_i$ becomes a centroid of a cluster, and thus identify the clusters with the neurons of the network. As we don't know the number of clusters a priori, one should establish a large number of neurons, $W$ such that $W What I don't understand is why one would do this. I could just say that I have $M$ possible centroids (each datapoint) and then use the discriminating criteria to check which ones are the actual centroids. In this approach there are $M$ centroid candidates and in the SOM one, $W$. But in both cases the dimension is $N$. Why is then using a SOM favorable? I believe I would want to have more data when performing clustering, so I think that having $M$ samples instead of $W$ would be better. Also, the dimensionality of the problem stays the same, so no advantage there. On top of it, it would be much faster to do it without the SOM, as training it may take a while, but the output (i.e. the values of the neuron weights) would just be pretty similar to a subset of the original dataset, which could be obtained just by identifying possible centroids from the $M$ observations. Why is it then that SOMs are used for clustering?
