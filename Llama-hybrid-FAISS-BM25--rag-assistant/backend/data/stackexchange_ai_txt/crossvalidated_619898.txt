[site]: crossvalidated
[post_id]: 619898
[parent_id]: 403150
[tags]: 
If you calculate $R^2$ by using the sum of the squared residuals (differences between the predicted probabilities and the $0/1$ category labels), then your $R^2$ is just a function of the Brier score (basically just the name given to square loss when the outcomes are categorical and predictions are probabilities). I dispute the other answer and say that a monotonic transformation of the Brier score (which is what you seem to be doing by taking Efron's pseudo $R^2$ ) is a perfectly reasonable way to assess performance. Brier score can be decomposed into measures of discrimination and calibration. The discrimination means the ability for the model to separate the predictions for the two labels, and this is related to what the ROCAUC measures. However, ROCAUC does not consider calibration, that is, if a predicted probability of $p$ corresponds to the event really happening with probability $p$ . After all, if the events that you predict to happen with $0.8$ probability keep happening with $0.6$ probability, there is a sense in which that prediction of $0.8$ is not telling the truth. If you are getting high scores when it comes to discrimination on its own by having a high ROCAUC yet scoring poorly on an overall measure of model performance that considers both discrimination and calibration, that signals to me that the the model is good at discriminating between the categories but has poor calibration. This is kind of a known issue for many machine learning models. Fortunately, it is possible to calibrate the predictions, and if you only apply monotonic transformations to the original predictions, the ability for the entire pipeline to discriminate between categories will not be affected (where the pipeline is the original model plus a monotonic calibration step). Python's sklearn has a calibration tutorial in its documentation. Also, note the issues with the $R^2$ implementation in sklearn and how that implementation is disputed in a recent publication by Hawinkel, Waegeman & Maere (2023). (One of those authors appears to be a member on here , too!) REFERENCE Hawinkel, Stijn, Willem Waegeman, and Steven Maere. "Out-of-sample $R^2$ : estimation and inference." The American Statistician just-accepted (2023): 1-16.
