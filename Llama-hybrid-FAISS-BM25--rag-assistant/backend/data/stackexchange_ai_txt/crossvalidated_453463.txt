[site]: crossvalidated
[post_id]: 453463
[parent_id]: 453387
[tags]: 
I am a Bayesian, but I find these kinds of criticisms against "frequentists" to be overstated and unfair. Both Bayesians and classical statisticians accept all the same mathematical results to be true, so there is really no dispute here about the properties of the various estimators. Even if you are a Bayesian, it is clearly true that the sample mean is no longer an unbiased estimator (the very concept of "bias" being one that conditions on the unknown parameter). So first of all, the frequentist is correct that the sample mean is not an unbiased estimator (and any sensible Bayesian would have to agree with this given the assumed distributions). Secondly, if a frequentist actually encountered this situation, they would almost certainly update their estimator to reflect the censoring mechanism in the data. It is entirely possible for the frequentist to use an estimator that is unbiased, and which reduces down to the sample mean in the special case where there is no censored data. Indeed, most standard frequentist estimators would have this property. So, although the sample mean is indeed a biased estimator in this case, the frequentist could use an alternative estimator that is unbiased, and which happens to give the same estimate as the sample mean for this particular data. Therefore, as a practical matter, the frequentist can happily accept the estimate from the sample mean is the correct estimate from this data. In other words, there is absolutely no reason that the Bayesian needs to "come to the rescue" --- the frequentist will be able to accomodate the changed information perfectly adequately. More detail: Suppose you have $m$ non-censored data points $x_1,...,x_m$ and $n-m$ censored data points, which are known to be somewhere above the cut-off $\mu_* = 100$ . Given the underlying normal distribution for the pre-censored data values, the log-likelihood function for the data is: $$\ell_\mathbb{x}(\mu) = \sum_{i=1}^m \ln \phi (x_i-\mu) + (n-m) \ln (1 - \Phi(\mu_*-\mu)).$$ Since $\ln \phi (x_i-\mu) = - \tfrac{1}{2}(x_i-\mu)^2+\text{const}$ , differentiating gives the score function: $$\frac{d \ell_\mathbb{x}}{d \mu}(\mu) = m (\bar{x}_m - \mu) + (n-m) \cdot \frac{\phi(\mu_*-\mu)}{1 - \Phi(\mu_*-\mu)}.$$ so the MLE is the value $\hat{\mu}$ that solves: $$\bar{x}_m = \hat{\mu} + \frac{n-m}{m} \cdot \frac{\phi(\mu_*-\hat{\mu})}{1 - \Phi(\mu_*-\hat{\mu})}.$$ The MLE will generally be a biased estimator, but it should have other reasonable frequentist properties, and so it would probably be considered a reasonable estimator in this case. (Even if the frequentist is looking for an improvement, like a "bias corrected" scaled version of the MLE, it is likely to be an other estimator that is asymptotically equivalent to the MLE.) In the case where there is no censored data we have $m=n$ , so the MLE reduces to $\hat{\mu} = \bar{x}_m$ . So in this case, if the frequentist used the MLE, they will come to the same estimate for non-censored data as if they were using the sample mean. (Note here that there is a difference between an estimator (which is a function) and an estimate (which is just one or a few output values from that function).
