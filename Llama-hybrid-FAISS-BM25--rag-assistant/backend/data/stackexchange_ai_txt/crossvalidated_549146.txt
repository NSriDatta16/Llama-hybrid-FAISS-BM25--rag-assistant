[site]: crossvalidated
[post_id]: 549146
[parent_id]: 548972
[tags]: 
OP here, just wanted add some supplementary material and demonstrate the following: a comparison between Frequentist Regression and Bayesian Regression using R #cool trick to directly bring this data into R my_data Frequentist Regression : This is how a Frequentist Regression Model (i.e. a Regression Model where the parameters are estimated using Ordinary Least Squares (OLS) - what we all learn in school). First, fit the regression model: #fit regression model model_1 Next, view the results: #view results summary(model_1) Call: lm(formula = age ~ weight + height, data = my_data) Residuals: Min 1Q Median 3Q Max -6.2369 -1.8688 0.3864 2.1065 5.6170 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -525.2843 369.9144 -1.420 0.181 weight 0.1875 0.1238 1.515 0.156 height 0.6871 1.0859 0.633 0.539 Residual standard error: 3.796 on 12 degrees of freedom Multiple R-squared: 0.1954, Adjusted R-squared: 0.06135 F-statistic: 1.457 on 2 and 12 DF, p-value: 0.2712 Optional : Visualize Results library(scatterplot3d) s3d $weight, my_data$ height,my_data$age, pch = 19, type = c("p"), color = "darkgrey", main = "Regression Plane", grid = TRUE, box = FALSE, mar = c(2.5, 2.5, 2, 1.5), angle = 55) # regression plane s3d$plane3d(model_1, draw_polygon = TRUE, draw_lines = TRUE, polygon_args = list(col = rgb(.1, .2, .7, .5))) # overlay positive residuals wh 0 s3d $points3d(my_data$ height, my_data $weight, my_data$ age, pch = 19) 2) Bayesian Regression: Now, we try to fit a Bayesian Regression Model to the same data: #load library library(rstanarm) library(see) library(bayestestR) library(performance) First, we specify priors on the Height and Weight variables (I picked a normal distribution for both of them - in my original question, we would have decided on these priors by using the research done on giraffes by other biologists): #specify priors my_prior Next, we run the Bayesian Regression Model #run bayesian regression model model_2 Now, we can view the results: summary(model_2) Model Info: function: stan_glm family: gaussian [identity] formula: age ~ . algorithm: sampling sample: 4000 (posterior sample size) priors: see help('prior_summary') observations: 15 predictors: 3 Estimates: mean sd 10% 50% 90% (Intercept) -9000290.7 3116.3 -9004290.9 -9000230.6 -8996293.9 weight 2999.7 1.0 2998.4 2999.7 3001.1 height 17.0 2.0 14.4 17.0 19.6 sigma 3207.5 65.0 3124.2 3207.2 3291.0 Fit Diagnostics: mean sd 10% 50% 90% mean_PPD 55.5 824.4 -1002.3 66.1 1107.1 Look at the model performance: #model performance performance(model_2) # Indices of model performance ELPD | ELPD_SE | LOOIC | LOOIC_SE | WAIC | R2 | R2 (adj.) | RMSE | Sigma ---------------------------------------------------------------------------------------------- -574.459 | 154.366 | 1148.918 | 308.733 | 1160.324 | 0.983 | -1.000 | 23876.735 | 3207.163 > se se (Intercept) weight height 3116.342642 1.038384 2.040471 Optional: Visualize Results #MCMC Trace x #Posterior Distributions plot_title #confidence ellipse bayesplot::color_scheme_set("green") plot(model_2, "scatter", pars = c("height", "weight"), size = 3, alpha = 0.5) + ggplot2::stat_ellipse(level = 0.9) References : https://rpubs.com/Qsheep/BayesianLinearRegression https://www.theoj.org/joss-papers/joss.01541/10.21105.joss.01541.pdf https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html#default-priors-and-scale-adjustments https://mc-stan.org/rstanarm/reference/plot.stanreg.html Note: I am still learning about Bayesian Regression - please feel to correct any mistakes that I might have made (e.g. It seems like the Bayesian Regression Model is performing far worse than the Linear Regression Model due to my choice of priors? When I run the Bayesian Regression Model with the default priors ("weakly informative priors"), e.g. model_2 - the results of the Bayesian Linear Regression are comparable with the Linear Regression Model. I must be doing something wrong?). Thank you!
