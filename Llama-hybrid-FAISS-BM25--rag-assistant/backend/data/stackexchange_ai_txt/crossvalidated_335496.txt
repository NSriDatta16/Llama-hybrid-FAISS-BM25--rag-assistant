[site]: crossvalidated
[post_id]: 335496
[parent_id]: 72697
[tags]: 
I'll try to go over the intuition behind generating the posterior predictive distribution step-by-step. Let $y$ be a vector of observed data that come from a probability distribution $p(y|\theta)$ and let $ \tilde y$ be a vector of future (or out-of-sample) values we want to predict. We assume that $ \tilde y$ comes from the same distribution as $ y$. It might be tempting to use our best estimate of $\theta$---such as the MLE or MAP estimate---to obtain information about this distribution. However, doing so would inevitably ignore our uncertainty about $ \theta$. Thus, the appropriate way to procede is to average over the posterior distribution of $ \theta$, namely $ p(\theta|y)$. Notice also that $ \tilde y$ is independent of $ y$ given $ \theta$, as it is assumed to be an independent sample drawn from the same distribution as $ y$. Thus, $ \displaystyle p(\tilde y| \theta, y) = \frac{p(\tilde y, y|\theta )p(\theta)}{p(\theta, y)} = \frac{p(\tilde y|\theta )p(y |\theta) p(\theta)}{p(y| \theta)p(\theta)} = p(\tilde y |\theta).$ The posterior predictive distribution of $ \tilde y$ is thus, $ \displaystyle p(\tilde y|y ) = \int_\Theta p(\tilde y | \theta,y) p(\theta | y) d\theta = \int_\Theta p(\tilde y | \theta) p(\theta | y) d\theta $ where $ \Theta$ is the support of $ \theta$. Now, how do we obtain the samples from $ p(\tilde y|y)$? The method you describe is sometimes called the method of composition , which works as follows: for s = 1,2,...,S do draw $\theta^{(s)}$ from $ p(\theta|y)$ draw $\tilde y^{(s)}$ from $ p(\tilde y|\theta^{(s)})$ where, in most situations, we have already the draws from $ p(\theta|y)$, so that only the second step is required. The reason why this works is quite simple: First note that $ p(\tilde y, \theta | y) = p(\tilde y| \theta, y)p(\theta | y)$. Thus, sampling a parameter vector $\theta^{(s)}$ from $ p(\theta|y)$ and, then, using this vector to sample $ \tilde y^{(s)}$ from $ p(\tilde y | \theta^{(s)}) = p(\tilde y | \theta^{(s)}, y)$ yields samples from the joint distribution $ p(\tilde y, \theta|y)$. It follows that, the sampled values $\tilde y^{(s)}, s=1,2,...,S$ are samples from the marginal distribution, $p(\tilde y|y)$.
