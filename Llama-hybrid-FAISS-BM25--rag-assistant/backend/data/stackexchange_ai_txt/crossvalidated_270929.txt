[site]: crossvalidated
[post_id]: 270929
[parent_id]: 270914
[tags]: 
The main use of variable importance, at least for me, is to diagnose problems with the data. For instance, if one variable is by far the most predictive, it is quite likely that "label leakage" is occurring, where inadvertently information contained in the label also appears in that feature. It sounds silly, but when you have thousands of features this often happens accidently. It's important to understand why that feature might be so predictive, before going foward. I would not remove features that have low importance, although I come from a machine learning background. Feature pruning is more common in statistical circles. Variable importance is also useful to convince others that your sophisticated ML/stats algorithm is doing something reasonable. It reveals some of the inner workings of an otherwise blackbox model.
