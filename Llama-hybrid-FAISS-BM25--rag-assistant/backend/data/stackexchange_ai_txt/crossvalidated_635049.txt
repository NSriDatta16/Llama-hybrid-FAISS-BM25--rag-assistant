[site]: crossvalidated
[post_id]: 635049
[parent_id]: 359245
[tags]: 
Well, I tried it. Yes it is slower to compute than other functions (e.g. tanh, arctan, softsign), in gfortran anyway. And I found other drawbacks: the mean squared error was usually worse than other functions after same number of epochs. And it could not tolerate as high a learning rate as others before going unstable. I was only doing simple classifier with 1 or 2 hidden layers. I did wonder like you whether the slow decay of the derivative would help. Maybe it does in deep learning (many layers)?
