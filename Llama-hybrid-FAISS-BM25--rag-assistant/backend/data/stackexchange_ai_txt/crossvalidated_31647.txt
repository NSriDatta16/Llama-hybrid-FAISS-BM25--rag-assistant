[site]: crossvalidated
[post_id]: 31647
[parent_id]: 31624
[tags]: 
Now, after repeating the steps 10 times, I will have 10 different optimized models. yes. Cross validation (like other resampling based validation methods) implicitly assumes that these models are at least equivalent in their predictions, so you are allowed to average/pool all those test results. Usually there is a second, stronger assumption: that those 10 "surrogate models" are equvalent to the model built on all 100 cases: To predict for an unknown dataset (200 points), should I use the model which gave me minimum error OR should I do step 2 once again on the full data (run grid.py on full data) and use it as model for prediction of unknowns? Usually the latter is done (second assumption). However, personally I would not do a grid optimization on the whole data again (though one can argue about that) but instead use cost and γ parameters that turned out to be a good choice from the 10 optimizations you did already (see below). However, there are also so-called aggregated models (e.g. random forest aggregates decision trees), where all 10 models are used to obtain 10 predictions for each new sample, and then an aggregated prediction (e.g. majority vote for classification, average for regression) is used. Note that you validate those models by iterating the whole cross validation procedure with new random splits. Here's a link to a recent question what such iterations are good for: Variance estimates in k-fold cross-validation Also I would like to know, is the procedure same for other machine-learning methods (like ANN, Random Forest, etc.) Yes, it can be applied very generally. As you optimize each of the surrogate models, I recommend to look a bit closer into those results: are the optimal cost and γ parameters stable (= equal or similar for all models)? The difference between the error reported by the grid optimization and the test error you observe for the 10% unknown data is also important: if the difference is large, the models are likely to be overfit - particularly if the optimization reports very small error rates.
