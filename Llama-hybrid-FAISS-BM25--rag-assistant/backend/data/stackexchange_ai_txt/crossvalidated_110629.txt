[site]: crossvalidated
[post_id]: 110629
[parent_id]: 110616
[tags]: 
Donbeo, a couple of pointers: When training a random forest model, you need to optimize the tuning parameter mtry , which is the number of features randomly selected for each tree. Use five- or ten-fold cross-validation for this. The reason why mtry could influence out-of-sample prediction error is that when you grow larger trees, the trees are going to be more correlated with one another. When training a random forest model, you should also grow a large enough forest. Perhaps 100 trees is not a big enough forest. Try growing a bigger forest in addition to optimize mtry . You need not worry about the size of the forest leading to over-fitting. Actually, the bigger the forest, the better (although there are diminishing returns).
