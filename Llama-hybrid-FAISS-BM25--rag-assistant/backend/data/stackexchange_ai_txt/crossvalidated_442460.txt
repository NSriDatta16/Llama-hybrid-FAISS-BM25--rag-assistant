[site]: crossvalidated
[post_id]: 442460
[parent_id]: 
[tags]: 
Can someone verify if the following Bayesian Information Criterion (BIC) model selection algorithm is correct for Gaussian mixture models?

I am trying to find an automated way of picking the number of clusters $K \in \mathbb{N}$ for unsupervised learning scenarios, specifically for GMM. I was suggested to use something called the "Bayesian Information Criterion" (BIC) for GMM The recommended BIC algorithm states, Let $k = 1$ , (start with one cluster) Run EM algorithm for GMM and obtain mean, variance and mixing coefficients that maximizes the log-likehood, which we refer to as {mean, variance, mixing coefficients} Plug these values into the "magical" formula $$BIC(k, N) = -2 \times \text{log-likelihood function} (\text{mean, variance, mixing coefficients}) + \log(N) \times k (2d+1)$$ where $N$ is the number of data points, $d$ is the dimension of the data $k \leftarrow k + 1$ and repeat (step 1 - 4) until $k$ is very large Pick the $k$ that gives the minimum BIC value. I am not sure if this is correct and would like a second pair of eye to verify. In particular, I am not seeing anything that indicates how I should pick my initial values. I am also not sure how long I should run this algorithm for. Note: a similar formulation for the BIC is given in this question The bayesian information criterion (BIC) Under the Gaussian model
