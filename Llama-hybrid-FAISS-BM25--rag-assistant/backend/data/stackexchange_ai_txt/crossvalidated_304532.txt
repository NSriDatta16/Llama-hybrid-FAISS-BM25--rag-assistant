[site]: crossvalidated
[post_id]: 304532
[parent_id]: 
[tags]: 
A3C implementation using Tensorflow

I'm trying to implement my own version of the Asynchronous Advantage Actor-Critic method, but it fails to learn the Breakout game. My code was mostly inspired by Arthur Juliani's and OpenAI Gym's A3C versions. The method works well for a simple Doom environment (the one used in Arthur Juliani's code) and for Pong, but it fails in Breakout, since it stops learning when it achieves a mean score of 40-50. My code is located in my GitHub repository . I have already adapted my network to resemble the architecture used by OpenAI Gym's A3C version, which is: 4 convolutional layers with the same specs, those being: 32 filters, 3x3 kernels, 2x2 strides, with padding (padding='same'). The output of the last convolutional layer is then flattened and fed to a LSTM layer with an output size of 256. The initial states C and H of the LSTM layer are given as an input. The output of the LSTM layer is then separated into two streams: a fully connected layer with an output size equals to the number of actions (policy) and another fully connected layer with only one output (value function) (more details in Network.py of my code); The loss function used is just as is informed in the original A3C paper. Basically, the policy loss is the log_softmax of the linear policy times the advantage function. The value loss is the square of the difference between the value function and the discounted rewards. The total loss accounts for the value loss, policy loss, and the entropy. The gradients are clipped to 40 (more details in Network.py of my code); There is only one global network and several worker networks (one network for each worker). Only the global network is updated. This update is done with respect to the local gradients of each worker network. Therefore, each worker simulate the environment for BATCH_SIZE iterations, saving the state, value function, chosen action, reward received, and the LSTM state. After BATCH_SIZE (I used BATCH_SIZE = 20) iterations, each worker pass this data to the network, calculate the discounted rewards, the advantage function, the total loss, and the local gradients. It then updates the global network with those gradients. Finally, the worker's local network is synchronized with the global network (local_net = global_net). All workers does that asynchronously (for more details in this step, check the work and train methods of the Worker class inside the Worker.py); The LSTM states C and H are reset between episodes. It is also important to note that the current states C and H are kept locally by each worker; To apply the gradients to the global network, I used the Adamoptimizer with learning rate = 1e-4. I have already tried different configurations for the network (by trying several different convolutional layers configurations, including different activation functions), other optimizers (RMSPropOptimizer and AdadeltaOptimizer) with different parameters configurations, and different values to BATCH_SIZE. But it always ends up failing in the Breakout environment. I usually waited for about 4-5 days to see if any improvements occurred, but it maintained a mean score of 40. Here is an image showing the score obtained by each thread in relation to the number of local episodes. The x-axis shows how many episodes each thread has experienced. Therefore, I would like to know if anyone have obtained success in training an agent in the Breakout game using the A3C with a LSTM layer. If so, what are the parameters used? Any help would be appreciated!
