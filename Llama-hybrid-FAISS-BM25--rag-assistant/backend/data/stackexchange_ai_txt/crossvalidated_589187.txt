[site]: crossvalidated
[post_id]: 589187
[parent_id]: 588994
[tags]: 
I think CV is pretty directly possible for mixed models (not also that confidence interval estimation via bootstrap is done a lot for mixed models): I'd actually say that with a mixed model you have the decided advantage that you spell out the data structure you consider. This structure is what you also need to correctly set up the CV splitting routine in order to get independent splits. Here's a start: I use random factors to describe sources of variance of which an application prediction is expected to have a new, never-before encountered level. Thus, when simulating application prediction conditions during cross validation, we want the test set to be independent in all such random factors . When random factors are nested , splitting into independent subsets at the outermost factor will automatically lead to independence also for all random factors nested within. When random factors are crossed , independence at both (all) factors at the same time means that we get 3 subsets of data: training data comprising a subset of the levels of the crossed random factors, test data comprising a disjoint subset of the levels of all the crossed factors. And there will be a third subset now that has for some factors "training levels" but for other factors "test levels". This part of the data can neither enter the training of the particular surrogate model, nor the test since it would break independence. Traditional cross validation concepts do not cover this. Extensions are possible (I'm working on this), but I'm not yet aware of a standard implementation I can point you to. This is possibly because with a few crossed random factors, the available training subset soon becomes rather small compared to the full data (if you do 5-fold for 3 crossed random factors, the training subsets are only about half of the full data). At some point, it becomes more economic to instead organize a validation study that collects independent test data. fixed factors are exploited for prediction, and thus no independence in splitting is required. In fact, one often goes a step further and stratifies them. The classification example linked by OP is IMHO wrong in their splitting: they formulate a (1 | patient_id) random factor, but omit to account for this in their splitting routine. (I have not yet read the Colby paper)
