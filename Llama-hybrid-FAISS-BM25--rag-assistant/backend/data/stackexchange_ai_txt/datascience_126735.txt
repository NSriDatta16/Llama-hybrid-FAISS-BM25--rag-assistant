[site]: datascience
[post_id]: 126735
[parent_id]: 
[tags]: 
Homebrew NNet Troubles

I have been trying to develop my own homebrew scalable neural network. I was hoping someone could Identify what aspect of my training process is most likely failing. I believe it may be due to either the learning rate causing odd overshooting, the way I am streaming the data into the network, or incorrect implementation of RMSprop. I have been trying to test the robustness of my program using the double spiral test, and when this gave me problems I tried to have the network learn a simple hyperbolic shape which also somehow failed. I am using RMSprop for all my optimization and for the spiral problem I was using -LogLikelihood as a cost. I have been experiencing the same problem of a fluctuating loss function (the training loss), with the only commonality across all my loss graphs is a decrease followed by a increase in the early epochs I have only solved even simpler problems than these using this network which was without using RMSprop I am using 2 to 4 layers of ReLu and TanH activations with ~15 neurons each and all give me similar graphs regardless of the data it is trying to match and regardless of the order of the network. While performing the spiral test I would always end up with dead ReLu neurons regardless of my initial LR(alpha). If you are wondering how I partitioned my saved values I stored: -Output of each layer -Error with respect to the output of each layer -weights and biases -learning rate per each weight -velocity of the learning rate per each weight The following is my code for my RMSprop implementation: def Optimizer(self, type, weightError, InitalLR, Rho, Epsilon, Beta, layer): if type == "RMSProp": ### the weight error is a matrix equivilent in size to the weight matrix ### the velocity and LR are also in this same shape and update per each weight self.Velocity[layer] = Beta*self.Velocity[layer] + (1-Beta)*np.power(weightError, 2) self.LR[layer] += InitalLR/(Epsilon + np.sqrt(self.Velocity[layer])) The following is my code for my weightUpdates: def UpdateWeights(self, type, InitalLR, Rho, Epsilon, Beta): for i in range(len(self.weightTensor)): # the following calculates dCost/dLayerOutput * dLayerOutput/dLinearSummation Output_and_Activation_Error = np.multiply(self.outputErrors[i+1], self.ActivationFunctions(deriv=True, type=self.LayerID[i+1][2], input=(self.outputMatrix[i+1]))) # Now calculates dCost/dLayerOutput * dLayerOutput/dLinearSummation * (dLinearSummation/dweight) = dCost/dweight WeightERROR = (np.reshape(self.outputMatrix[i], (len(self.outputMatrix[i]), 1)) @ np.reshape(Output_and_Activation_Error, (len(Output_and_Activation_Error), 1)).T) # Takes this dCost/dweight and plugs it into the optimizer which updates the learning rate saved in the class self.Optimizer(type, WeightERROR, InitalLR, Rho, Epsilon, Beta, i) # update self.weightTensor[i] -= np.multiply(self.LR[i], WeightERROR) Some guidance on how to interpret loss graphs would be useful as well. Thank you.
