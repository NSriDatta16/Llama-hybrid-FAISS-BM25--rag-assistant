[site]: crossvalidated
[post_id]: 144181
[parent_id]: 144178
[tags]: 
Actually, the answer to that question (5.a.v) is listed as False: False. The variance in test accuracy will decrease as we increase the size of the test set. So you were right to be confused (though I'm not sure where you thought you saw a True). When the size of the test set increases, the test accuracy becomes an average of more iid terms, and so its variance decreases linearly in the number of test points. More specifically: The test accuracy is $t = \frac1n \sum_{i=1}^n a_i$, where $a_i$ is 1 if we got the $i$th test example correct and 0 if we got it wrong. In the setup for this problem, we assume that the elements of the test set are independent and identically distributed samples from a distribution, independent also of the learned model. So the $a_i$ are also iid. The variance of an iid sum, using standard properties of the variance, is $$\newcommand\Var{\mathrm{Var}} \Var\left[ \frac1n \sum_{i=1}^n a_i \right] = \frac1{n^2} \Var\left[ \sum_{i=1}^n a_i \right] = \frac1{n^2} \sum_{i=1}^n \Var\left[ a_i \right] = \frac1n \Var[a] .$$ So the variance of the test accuracy decreases as the test set size increases.
