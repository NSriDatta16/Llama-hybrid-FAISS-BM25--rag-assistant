[site]: datascience
[post_id]: 88025
[parent_id]: 73255
[tags]: 
reward_threshold (float) â€“ Gym environment argument, the reward threshold before the task is considered solved [1] Just from that one sentence definition, it sounds like a total reward that e.g. an agent must earn, before the task is complete, and so ends. If this were for example a task for the cartpole agent to stay upright/vertical, it might be formulated as the number of frames, so 1 frame = 1 point reward and if reward_threshold = 200 , the agent must balance the pole for 200 frames to succeed. Have a look at the example of cartpole on the OpenAI Gym website : while True: candidate_model = model.symmetric_mutate() rewards = [run_one_episode(env, candidate_model, False) for _ in range(5)] reward = np.mean(rewards) if reward >= env.spec.reward_threshold: print "Reached reward threshold!" rewards2 = [run_one_episode(env, candidate_model, False) for _ in range(env.spec.trials)] if np.mean(rewards2) >= env.spec.reward_threshold: break else: print "Oops, guess it was a fluke" So the agent runs for 5 episodes ( for _ in range(5) ) and each episode returns an award. We compute the mean reward over 5 episodes ( reward = np.mean(rewards) ) and then introduce the desired control flow, based on that result. In this case, they use the default reward_threshold from the environment ( env.spec.reward_threshold ), print a success message, and do a final check to see if the agent really has learnt something consistent or not, by checking once again on a new set of episode ( for _ in range(env.spec.trials) ).
