[site]: crossvalidated
[post_id]: 457640
[parent_id]: 
[tags]: 
Very basic understanding of machine learning from statistics point of view

I don't know if this is the right place, but I have a general question about machine learning from a statistics point of view. I know bits and pieces, but I don't get the overall picture. Add to that the fact that my math skills are pretty limited and I only have a basic grasp of statistics. Here's what I think I know: Our data comes from some unknown distribution $P(X,Y)$ . Let's take the Iris data set. Each of the 4 features follows the same unknown distribution (i.i.d. assumption), even though in reality this may not be true. So we have four unknown distribution $A,B,C,D$ . This means, for the Iris data set, our unknown distribution would be $P((A,B,C,D), Y)$ , where $A,B,C,D$ are combined as $X$ by multiplying them. We can do this, because they are i.i.d. And $Y$ also follows some unknown distribution. So if I have the following training instance: $(1,2,3,4) : 1$ it means it's a concrete realisation of the random variables $A,B,C,D$ and $Y$ . in this case, the instance has the class label 1. If we knew the distributions, we could just calculate the total probability by multiplying the individual probabilities. Some machine learning algorithms try to estimate $P(X,Y)$ from data, by using MLE or MAP. $P(X,Y) = P(Y|X)P(X)$ or $P(X|Y)P(Y)$ . MLE estimates $P(Y|X)$ (estimating $P(X)$ is not necessary, I guess) and MAP estimates $P(X|Y)P(Y)$ . Algorithms that use MLE or do something completely different, without probabilities (like SVM) are discriminative, the others are generative. Generative algorithms always use the Bayes Theorem. Now the notation here really confuses me: The way I understand it, MLE estimates $P(D;\theta)$ , where $\theta$ is some unknown but fixed value. In our case, $\theta$ is our machine learning model, the hypothesis. Usually, we would write $P(D|\theta)$ , but since $\theta$ is not a random variable, we use the other notation. So if I use logistic regression or a decision tree, MLE would estimate $P(\mathrm{Data};\mathrm{SomeConcreteLogisticRegressionFunction})$ or $P(\mathrm{Data};\mathrm{SomeConcreteDecisionTree})$ What confuses me most, I think, is the notation: MLE estimates $P(Y|X)$ which makes sense: I want to know the most probable label. In this notation $P(D|\theta)$ , D would be the label and $\theta$ is our training data? For logistic regression, I have also seen $P(Y|X;\theta)$ , which I think means: Probability of the label, given our training data and our learning model. Or is MLE only used for getting a value for $\theta$ from the data ( $P(D|\theta)$ ) and $P(Y|X;\theta)$ is something different? EDIT So as per the comment, I guess this makes sense: $P(D|\theta)$ or $P(D;\theta)$ means I estimate $\theta$ from the data set. in our case, the data set is X. So $P(D|\theta)$ is $P(X|\theta)$ . How does the $Y$ come into the notation?
