[site]: crossvalidated
[post_id]: 459467
[parent_id]: 
[tags]: 
How to calculate the ratio of observations to model parameters in a VAR(p) model

In machine learning, it's pretty common to use the general rule of thumb that one should have at least 10X as many observations as there are model parameters to avoid overfitting. As discussed here and here , it is clear that the cumulative number of parameters in a VAR model grows quadratically with the number of variables one is trying to predict. However, it seems to me that when considering overfitting--one should really just look at the individual equations of a VAR model. That is, as I understand it, VARs are really a composition of multiple models--one model/equation for each of the $k$ variables you are trying to predict. Ignoring the y-intercept, each of those models should have a parameter growth rate of $pk$ for lag order $p$ and $k$ time series. When trying to calculate the ratio of observations to model parameters, shouldn't one compare the number of observations from a single variable to $pk$ rather than to $pk^2$ ? Example: Fitting a VAR $(2)$ with $k=3$ , how many observations must be included to ensure that the VAR model has at least 10X as many observations as model parameters? Since $kp=6$ , you should ensure that each variable in the model has at least 60 time-points that can be predicted. Assuming the 10X rule is a good rule, including this many time-points per variable should help avoid overfitting.
