[site]: crossvalidated
[post_id]: 633501
[parent_id]: 633476
[tags]: 
Let us complicate things a bit and take a look at another regression, the logistic regression, in an ML model that predicts a binary class. You might say that this is quite far-fetched, but you may get an intuition as well from this as this can show that your question is not just a problem of the linear regression, but can show up elsewhere in new guises, as discussed on meta at If someone asks "What is the intuition behind the idea that for linear regression, ...", does the answer have to be only about the linear regression? . Linear regression OLS, from Ordinary least squares : In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable. MLE as a tool to minimize the loss function of an ML model of a logistic regression loss function of a 0/1 classification, from logistic regression : In machine learning applications where logistic regression is used for binary classification, the MLE minimises the cross-entropy loss function. MLE, from Maximum likelihood estimation : The maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability (or probability density, in the continuous case). If the parameter consists of a number of components, then we define their separate maximum likelihood estimators, as the corresponding component of the MLE of the complete parameter. Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable Y being 0 or 1 given experimental data. Wrap up Wrapping this up, OLS is like MLE, the latter is just to make the best prediction so that the model's loss function hits its lowest value. You have to find the parameters that predict the best separation from one class to the other, and you will have a problem when rare features are needed to hit the rare observations (rare features crosses), since then, you can drop all of the other features and the model overfits since the MLE can become infinite (unconstrained). In a linear regression, you just need to hit the best curve through the observations. The weakness of the logistic regression is also its strength In other words, the logistic regression makes a 0/1 decision from an asymptotic probabity function and therefore can fully hit any needed prediction as long as you give it enough parameters. Yet training with rare features does not always mean that it is well-trained for other observations yet to come. This weakness of the logistic regression to overfit a model can be fought with regularization (sort of punishing), but it is better, if you have less features that do not set up these rare feature crosses and still predict rare observations with the right class. The weakness of the logistic regression is then also a strength! If you have unbalanced data, for example if you predict rare diseases of just 1 in a 10000, the logistic regression tries to overfit such rare features on rare observations, but this it also means the strength that it does not forget about these rare observations since it can gain up to "infinite points" for the MLE. If you manage to have so few features that you do not build up rare feature crosses but still have many enough that the maximization of the MLE is rewarded well enough for a rare disease, you do not need to unbalance your data. See Case-control sampling : Logistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome. See for example Rare Feature Selection in High Dimensions : It is common in modern prediction problems for many predictor variables to be counts of rarely occurring events. This leads to design matrices in which many columns are highly sparse. Thus, the weakness is also a strength of the logistic regression. Example disease: 1 out of 10000 features: 10 -> leads to a too broad prediction, the disease observations are not classified since the power of the rest is too big. features: 20 -> leads to a healthy balance between broad and narrow "multi-dimensional feature-observation clouds". features: 100 -> leads to some rare feature crosses, you overfit (~ sort of overtrain) the model only on the given disease observations and risk missing some disease observations of the future if the disease still comes also from some broad features (that might mirror some yet unknown rare feature or that might just be needed as well). The first and the last bullets should be avoided in a good model unless you begin exploring the data and want to put the mainstream against the rare features, but you will gladly see that in the loss function that helps you find the best outcome with regularization and/or a new run with a lower or higher in dimensionality (less or more features). Linking back to your question: avoid overfitting with feature reduction If you know what you are doing, this is good to go. But if you just guess the number of features, try avoiding rare feature crosses, which are more likely the more features you add. Linking back to your question on the linear regression, if you add as many features as you have observations in a logistic regression of an ML model, you will get a matrix of some highly sparse columns, which is an overfit of the model so that it can tell you all and up to 100 % about the given observations but that cannot do so for the unknown future. See Cross Validated Why is logistic regression particularly prone to overfitting in high dimensions? , for example the highest voted answer: Perfect separation is more likely with more dimensions ... the model tries to predict as close to 0 and 1 as possible, by predicting values of μ that are as low and high as possible. To do this, it must set the regression weights, β as large as possible. (but then leads to overfitting if β can become very large for rare feature crosses) As a result, regularisation becomes more important when you have many predictors. And here from my own answer - and I warn you that I am still not sure whether I understood this right (and by the way, this whole answer is on shaky grounds since I am not a professional): In the usual case and in small dimensionality, a constrained MLE exists for each observation, it is calculated over a given number of observations that face a smaller number of features - thus it needs to be calculated by using constraints. With higher dimensionality, rare feature crosses arise where an unconstrained MLE exists, because parameters and observations become 1:1 cases then: one unique feature (~ parameter) = one isolated class assignment (~ observation). In these cases, those observations that are not mapped to just one feature lose their impact and need to be recovered by regularisation. (Which would then tell us why we need to regularize such ML models during training to avoid overfitting.) PS: Mind that "more features" does not just mean "new features of whatever", like "intelligence" and "weight" and "foot size", but it can also just mean higher dimensionalities of the same, already given features, like x1², x1³ and so forth from an x1="intelligence". Those higher dimensions will all have the role as stand-alone features even if they are mathematically correlated since they can strongly change the model accuracy.
