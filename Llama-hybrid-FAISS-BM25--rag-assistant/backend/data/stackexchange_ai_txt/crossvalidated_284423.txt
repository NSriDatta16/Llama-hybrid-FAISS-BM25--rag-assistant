[site]: crossvalidated
[post_id]: 284423
[parent_id]: 
[tags]: 
How to regularize when facing exchangeable vectors in deep learning?

I am seeking to estimate a regularization method to estimate the conditional probability $p(y|\mathbf{X})$ where $\mathbf{X}\in\mathbb{R^{n\times m}}$ and where the probabilities are invariant by permutation column-wise on $\mathbf{X}$. Formally, let $\mathbf{X}=(X_1, .., X_n)$ let $\pi: [1 .. n] \to [1 .. n]$ be a permutation, let $\mathbf{X}_\pi=(X_{\pi(1)}, .., X_{\pi(n)})$ We have $p(y|\mathbf{X})=p(y|\mathbf{X}_\pi)$ and $p(\mathbf{X})=p(\mathbf{X}_\pi)$. The exchangeability of the vectors is a strong prior, and I am seeking a regularization method (maybe akin in spirit to covolutional networks) that structurally reflect this property of the problem. A data augmentation scheme can be used, generating many extra examples by applying random column permutations on known examples; however, this feels like an extremely wasteful approach in terms of calculations. Does anyone know how to regularize $p$ estimation in such a situation?
