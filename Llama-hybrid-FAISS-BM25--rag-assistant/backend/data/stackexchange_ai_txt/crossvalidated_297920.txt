[site]: crossvalidated
[post_id]: 297920
[parent_id]: 
[tags]: 
predicting N probabilities for each pixel in keras

I Am developing a classification based-model to predict 12 probability for each pixel in the image , I have built the architecture , but I am not sure whether I am right or not , I am a newbie in deep learning . EDIT 1 : I was adviced to use the fully convolutional approach , which means never using the Fully connected layers , I did try that , that gave me a better saturated predicted channels , but they were never the best , here is my architecture , Any Help ?? Edit 1 Architecture : def Build_Classification_Model(): Kernel_Size_Val=4 import keras from keras.models import Model from keras.layers import Flatten, Dense, Input, Reshape,concatenate,BatchNormalization,Dropout,Activation,Conv2D,MaxPool2D Input_Img=Input(shape=(Img_Size,Img_Size,1),name='Main_Input') #The First Conv Layer + BatchNormalization X=Conv2D(filters=4,kernel_size=Kernel_Size_Val,activation='relu',name='First_Conv_0',padding='same')(Input_Img) X=Conv2D(filters=4,kernel_size=Kernel_Size_Val,activation='relu',name='First_Conv_1',padding='same')(X) X=Conv2D(filters=8,kernel_size=Kernel_Size_Val,activation='relu',name='Second_Con_0',padding='same')(X) X=Conv2D(filters=8,kernel_size=Kernel_Size_Val,activation='relu',name='Second_Conv_1',padding='same')(X) X=Conv2D(filters=16,kernel_size=Kernel_Size_Val,activation='relu',name='Third_Conv_0',padding='same')(X) X=Conv2D(filters=16,kernel_size=Kernel_Size_Val,activation='relu',name='Third_Conv_1',padding='same')(X) X=Conv2D(filters=32,kernel_size=Kernel_Size_Val,activation='relu',name='Fourth_Conv_0',padding='same')(X) X=Conv2D(filters=32,kernel_size=Kernel_Size_Val,activation='relu',name='Fourth_Conv_1',padding='same')(X) X=Conv2D(filters=64,kernel_size=Kernel_Size_Val,activation='relu',name='Fifth_Conv_0',padding='same')(X) X=Conv2D(filters=64,kernel_size=Kernel_Size_Val,activation='relu',name='Fifth_Conv_1',padding='same')(X) U=Conv2D(filters=Num_Bins+1,kernel_size=Kernel_Size_Val,activation='softmax',padding='same')(X) V=Conv2D(filters=Num_Bins+1,kernel_size=Kernel_Size_Val,activation='softmax',padding='same')(X) X=concatenate([Input_Img,U,V]) MyModel=Model(Input_Img,X) MyModel.compile(optimizer='adam',loss=keras.losses.categorical_crossentropy,metrics=['accuracy']) print(MyModel.summary()) return MyModel The following is the function for my baseline architecture : def Build_Classificion_U_Model(): import keras from keras.models import Model from keras.layers import Flatten, Dense, Input, Reshape,concatenate,BatchNormalization,Permute,Dropout from keras.layers import Conv2D from keras import regularizers Input_Img=Input(shape=(Img_Size,Img_Size,1),name='Main_Input') #The First Conv Layer + BatchNormalization X=Conv2D(filters=8,kernel_size=5,activation='relu',name='Conv1')(Input_Img) #The Second Conv Layer + BatchNormalization X=Conv2D(filters=16,kernel_size=5,activation='relu',name='Conv2')(X) #The Third Conv Layer + BatchNormalization X=Conv2D(filters=32,kernel_size=5,activation='relu',name='Convfsf3d')(X) X=Flatten()(X) X=Dense(units=256,activation='relu')(X) X=Dense(units=49152,activation='relu')(X) X=Reshape(target_shape=(Img_Size,Img_Size,12))(X) X=Conv2D(filters=12,kernel_size=1,activation='softmax')(X) X=concatenate(inputs=[Input_Img,X]) MyModel=Model(Input_Img,X) MyModel.compile(optimizer='adam',loss=keras.losses.categorical_crossentropy,metrics=['accuracy']) print(MyModel.summary()) return MyModel I will explain my architecture , the input is about a 64 * 64 * 1 graysale image , followed by many convolution layers and then it is flattened then there it is followed by many FC Layers but the final one is 49152 to be reshaped back to ( 64 * 64* 12 ) 12 here represents the probabilities and then concatenated with the input layer , to be compared with the output but it give me so bad results , why ?
