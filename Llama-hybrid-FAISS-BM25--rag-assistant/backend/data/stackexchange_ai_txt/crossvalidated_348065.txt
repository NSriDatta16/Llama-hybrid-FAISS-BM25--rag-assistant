[site]: crossvalidated
[post_id]: 348065
[parent_id]: 
[tags]: 
Multiple values in a variable for linear regression

Say I have a data set that I am trying to perform a linear least squares regression on. Suppose that the end goal is to predict y from x. The training data set I am working with has the form y: (0.500,0.500,0.500,.300,.300,.300,.100,.100,.100) x: (15.6, 15.2, 15.9, 11.2, 10.9, 11.0, 5.6, 5.3, 6.0) The important thing to note here is that what I am trying to predict (y) has multiple, different (x) values in the training data set. Suppose that the data comes from an experiment. Say (y) is some density and (x) is some reading on a machine. Say the data comes from measuring some standard known densities, collecting a bunch of readings, and the end goal is to fit a model so that when unknown density is read, and the machine gives a value (x), we can use the model to predict the actual density. How should regression be done here? My understanding is that there are dependencies in the data here, so a standard regression model to predict y given x cannot simply be fit and applied. Do I need to invert the model? Take averages first? What is the correct thing to do? I've been told to invert the model, but I am not sure I understand what the justification is. Assume that the intended solution is a linear regression model.
