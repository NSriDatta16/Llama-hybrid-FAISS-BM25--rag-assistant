[site]: crossvalidated
[post_id]: 252480
[parent_id]: 
[tags]: 
Learning ranking model leveraging multiple noisy pairwise constraints

I am trying to come up with a probabilistic model that (1) learns a pairwise ranking function (2) and leverage multiple noisy pairwise constraints. My problem setup has two parts: (1) There are $N$ objects: $O_i, i=1,\ldots,N$. Each object $O_i$ has an associated observed real-valued vector $X_i \in \cal R^d$. The primary goal is to learn a function such that $f(x_i) > f(x_j)$ if the object $O_i$ ranks higher than $O_j$. A regression seems to be a good fit. (2) There are $K$ processes, $k=1,\ldots,K$. The $k$th process generates a noisy matrix $Z^k_{ij} \in \{0,1\}$, where $i,j=1,\ldots,N$. All matrices have the same size and are fully populated. The value $Z^k_{ij}=1$ if the process prefers object $O_i$ over object $O_j$. The secondary goal is to infer a matrix $Z^*_{ij}$ with latent variables. It is safe to assume that every two processes are independent when generating data. It makes sense to attribute $k$th process a hidden variable $0 It is not too difficult to implement part (1) separately. The challenge is to build a single model that leverages both sources of information: (1) object features and (2) multiple pairwise constraints as generated by independent processes (experts). In other words, the processes can be considered as experts. I am struggling creating a single probabilistic model that puts all pieces in one story. Any help would be greatly appreciated.
