[site]: crossvalidated
[post_id]: 202803
[parent_id]: 175654
[tags]: 
The basic ideas are not that difficult: First model: You just multiply the respective coefficients with the new data points and see whether the sum is bigger than the negative intercept (then am is 1 ) Second model: You first bin the numerical variables into distinct intervals (with cut() ) and then run the logistic regression again (dummy variables will be created automatically) . For new data points you check into which interval they would fall and add the resulting coefficients (if the interval is not present you assign 0 ). You again check whether the sum is bigger than the negative intercept (see first model above). You can scale all the coefficients and the intercept by multiplying with a factor (e.g. it is quite popular to take 20/ln(2) ) As an example consider the following case where we want to build a toy scoring model for predicting am from the mtcars dataset: library(OneR) # for bin and eval_model function mtcars_bin Obviously this model can be further improved by cutting the intervals differently, e.g. combining the ones which give comparable scores, rounding etc., but the general principle stays the same.
