[site]: crossvalidated
[post_id]: 294383
[parent_id]: 294297
[tags]: 
It's likely that in the paper, that $\pi_i$ is supplied by the agent during active learning. When used for optimal control, for example, it is common to use an $\epsilon$-greedy policy for behaviour and deterministic greedy policy for the learning target. For offline learning, it is still common to work with a known policy, such as simple equiprobable action selection. Again in this case the behaviour policy is known by design, it is not constructed afterwards from the data. As the main objective for importance sampling is to weight trajectories from observation to target policy, you may suffer from sampling bias if use the same observations to estimate behaviour policy and weight sample returns. You might be able to estimate an effective behaviour policy from your data, but that would probably cause problems with variance and maybe also bias, especially if it is not essentially true that the history is from a single policy. Off policy learning already suffers from larger variance than on policy. I have not ever tried estimating a behaviour policy from data like this, so I cannot say for sure whether it would not work, just I have concerns. However, you may have to give up on the idea of importance sampling. That does not prevent you using an off-policy approach. You could for example use single step Q-learning, with experience replay sampling from your history. This will learn from the state transitions and rewards that you have stored, and will not need any data from the behaviour policy. What you may not be able to do is any multiple step algorithms such as Q($\lambda$), because these need longer trajectories.
