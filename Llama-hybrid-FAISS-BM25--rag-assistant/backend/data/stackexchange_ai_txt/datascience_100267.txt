[site]: datascience
[post_id]: 100267
[parent_id]: 100262
[tags]: 
RNNs can use the hidden state to store relevant history in its hidden state and use it to make a prediction of what word can follow. This makes RNNs potentially very good language models, i.e., models of distributions over sentences. CBOW only considers a window of few surrounding words regardless of their ordering, which is, however, crucial information for language modeling. CBOW is, therefore, a pretty bad language model, but language modeling is not what the model was designed for. Although it is a pretty poor language model, it provides a similarly good training signal for word embeddings. Because it is much faster than RNN, the embeddings can be trained on much larger data. Also, the model design allows further optimization for large vocabulary sized.
