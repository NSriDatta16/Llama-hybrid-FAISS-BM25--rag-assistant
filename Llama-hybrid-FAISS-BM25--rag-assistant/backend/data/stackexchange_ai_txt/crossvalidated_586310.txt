[site]: crossvalidated
[post_id]: 586310
[parent_id]: 586298
[tags]: 
If you do cross-validation (CV), you have one model per fold. E.g. if you did 2-times-repeated 7-fold CV, you have 14 models. Sometimes it is acceptable to have that many models and to just average their results. However, you reduce runtime (just one model), make it easier to use model interpretability tools and may also get better performance, if you refit on all the data. Whether you will is not always totally clear, you on the one hand have more data (ought to improve things), but by training multiple times on different data, you are getting some (potentially useful) variety and are to some extent doing seed averaging (you can of course train "a final time" on all data multiple times with different random number seeds to get that same effect). From what I've heard from top-Kagglers it's more likely that retraining on all data is the best choice. What are reasons not to? Training cost is probably not the main issue, since you could afford to do CV, so just one more fit presumably does not matter so much. However, sometimes you do early stopping based on CV. Generally, you'd want to define when you stop based on CV (e.g. identify a fixed number of epochs/trees/whatever that seems to work across folds), but if your training approach is so fragile that this is not possible and you need a validation set to decide when to stop (arguably, you ought to stabilize things to prevent this situation), that could be an argument for avoiding re-training.
