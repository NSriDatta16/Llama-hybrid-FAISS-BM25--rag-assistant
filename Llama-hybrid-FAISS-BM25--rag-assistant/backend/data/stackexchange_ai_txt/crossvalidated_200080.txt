[site]: crossvalidated
[post_id]: 200080
[parent_id]: 200052
[tags]: 
I won't have enough space in the comments to give my thoughts on Vinces answer, which is certainly very good but lacking some additional insight. Can answers be merged? Dimensionality reduction would not make the data more separable during training: you are giving away information. However, this information drain might be useful for the generalization of the learned model, i.e. its performance on unseen data will increase. You do not always need more samples to catch patterns in a high(er)-dimensional space although in general this might actually be the case. It also depends on the correlation of the features. If you just double a feature vector, i.e. two feature vectors will be the same, the increase in dimensionality will obviously not have significant consequences other than probably upsetting your regularization scheme of the learner. It is quite important to see how much the data correlates, the rank of the feature matrix can be a very good inside. Vince makes a very good point when he says that dimensionality reduction can fasten your algorithm during training and testing. However, keep in mind that the reduction itself also has a computational complexity, so performing PCA and adding an SVM might not give you much of a speedup (and can be absorbed into one weight vector I think). However, if you keep the dimensionality reduction simple enough, then this will really work in your favor. A good example of that are HOG features in object detection ( http://cs.brown.edu/~pff/papers/lsvm-pami.pdf ). In the paper PCA is performed on the HOG feature vector. A closer look at the eigenvectors reveals that a simple dimensionlity reduction, which just sums up different groups of features, will be enough and it indeed is. This gave a 2 to 3 times speedup. As for the kernel trick: the function that `lifts' your data into a higher dimensional space does not necessarily have to map into euclidean space but into a Reproducing Kernel Hilbert Space (RKHS). This gives you a bit more freedom in designing such functions. The Gram matrix of the kernel function, i.e. the dot-product in higher-dimensional space for all combinations of training examples can vaguely be seen as a similarity/dissimilarity matrix. However, I would not really push this thought too far. For an RBF kernel this is certainly true but a polynomial kernel is very hard to interpret in this way. Actually additional components are added to your feature vector when using kernels, i.e. the dimensionality increases. It does so implicitly in the RKHS. For many kernels, e.g. polynomial kernels, the dimensionality increases massively. For the RBF kernel it is even infinite under some conditions. Doing both (PCA + kernel-based learning) is called kernel pca and essentially fuses both methods. Keep in mind though: just as a dimensionlity reduction may speed up your algorithms, kernel functions will almost always slow them down due to its increased computational complexity at test time c.f. a linear classifier. This can partially be circumvented by explicit feature mapping, i.e. actually executing the mapping function from original to high-dimensional space and keeping an eye on the dimensionality. See, for example, this paper: http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf This trick is quite nice as it gives you an intuition of the higher dimensional space while not making it too large in the sense of having too many components. Once your feature vectors are explicitely mapped into the higher-dimensional space, you may apply a linear classifier. There is certainly more out there (e.g. low-rank expansion) but I think these two answers should give you a good and somewhat detailed overview. Vinces answer should be upvoted and chosen as best answer if you are happy with the information we gave you.
