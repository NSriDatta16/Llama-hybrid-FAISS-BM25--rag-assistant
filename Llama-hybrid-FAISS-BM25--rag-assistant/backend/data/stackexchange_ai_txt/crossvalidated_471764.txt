[site]: crossvalidated
[post_id]: 471764
[parent_id]: 471685
[tags]: 
There seem to be two things going on here. First, with large data sets normality tests tend to fail. Real data seldom have exactly normal distributions. This page goes into extensive discussion. That doesn't mean normality tests are unimportant, but they have to be interpreted in terms of whether the deviations from normality are sufficiently large to invalidate your model for its intended uses . For example, normality of residuals isn't needed for least-squares regression to give the best linear unbiased estimates of regression coefficients. At most, normality of residuals is assumed by standard parametric tests of coefficient statistical significance. Even without strict normality of residuals the central limit theorem can still come to the rescue. Second, you should rethink your approach toward evaluating which interactions to include. You have compared a large set of pairwise interactions individually against the model without interactions. With 21 separate tests for particular interaction pairs, you have (with a p all of the individually identified interaction terms together into a model without regard to how much that combination of interaction terms is really helping. As your predictors are likely to be correlated I suspect that there is a lot of redundancy among the included interactions. How to proceed depends on how you intend to use your model. The model without interactions might give adequate performance for some applications. If the interactions themselves are of interest, you seem to have enough data to include all 28 possible pairwise interactions plus the 8 primary predictors in a single model. You would have nearly 30 times as many data points as the number of parameter values that model would estimate, typically enough to avoid overfitting. Then, depending on the purpose of your modeling, you could use that entire model, keep all interactions but with a penalized approach like ridge regression (to get a model potentially less likely to overfit and thus more likely to work for predicting new cases), or pull back carefully from the full model to remove the least useful interactions while all the remaining interactions are still taken into account . A final warning: the names of your variables suggest that these data might be coming from a time series . If that's the case there are inherent problems with non-independence among observations that require different approaches than simple linear regression.
