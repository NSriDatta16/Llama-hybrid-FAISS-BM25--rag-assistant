[site]: crossvalidated
[post_id]: 466035
[parent_id]: 465787
[tags]: 
This is a legitimate question but, on a general basis, getting the normalising constant does not help. For instance, since $$\int_0^\infty \dfrac{x^{\mu-1}+x^{\nu-1}}{(1+x)^{\mu+\nu}}\,\text{d}x=2 B(\mu,\nu) = \dfrac{2\Gamma(\mu+\nu)}{\Gamma(\mu)\Gamma(\nu)}\qquad\mu,\nu>0$$ the probability density $$p(x)=\dfrac{\Gamma(\mu)\Gamma(\nu)}{2\Gamma(\mu+\nu)}\,\dfrac{x^{\mu-1}+x^{\nu-1}}{(1+x)^{\mu+\nu}}\qquad 0 enjoys a known normalising constant. But this does not help me in simulating it, the more because the Gamma function $\Gamma(\cdot)$ is itself only available through numerical approximations the associated cdf has no closed-form expression, unless one considers the incomplete Gamma function to be "known". The only case I can think of where the normalising constant helps is when considering the inverse cdf approach: $X=F_X^{-1}(U)$ is a realisation from the distribution with cdf $$F_X:x \mapsto \int_{-\infty}^x p_X(x)\,\text{d}x$$ However if $p_X$ is known up to a normalising constant and $F_X(x)$ is available for all $x$ 's, then one can derive this normalising constant from $F_X(\infty)=1$ . Otherwise, most simulation methods manage very well without the constant (and often produce an estimate of the constant as a by-product). For instance, accept-reject algorithms work well without the constant, which can be estimated from the acceptance rate; importance sampling methods, incl. sequential Monte Carlo methods, operate by weighting and resampling, which does not require normalising constants (unless unbiasedness is required); MCMC algorithms such as Metropolis-Hastings, Gibbs sampling, Langevin algorithms, HMC, all work without the normalising constant.
