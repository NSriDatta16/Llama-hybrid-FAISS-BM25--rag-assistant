[site]: crossvalidated
[post_id]: 540846
[parent_id]: 446093
[tags]: 
is it OK to use y_test for both validation (which is used for early stopping) or is this a recipe for overfitting? It not a recipe for over-fitting unless the test set is small, but it is a recipe for an optimistically biased performance estimate, so I would strongly advise against it and use something like nested cross-validation, or at least partitioning the data into a training set, a validation set (for optimising the hyper-parameters) and a test set (for performance evaluation). For an explanation/demonstration of why this is a problem, see the paper I wrote with Mrs Marsupial: Gavin C. Cawley, Nicola L. C. Talbot, On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation, Journal of Machine Learning Research, 11(70):2079âˆ’2107, 2010. ( www ) why should we use y_train as a validation set as well? isn't it wasteful since the objective always improves on the training set? Using cross-validation on the training set to get a model selection criterion for hyper-parameter optimisation is a sensible approach. For some machine learning methods (e.g. SVM) you can get an estimate (or bound) on the leave-one-out cross-validation error almost for free as a by-product of the training algorithm, which is very handy for efficient hyper-parameter optmisaion. Just don't use that estimate for performance evaluation as well as hyper-parameter optimisation as it will be optimistically biased.
