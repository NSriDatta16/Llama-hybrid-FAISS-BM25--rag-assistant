[site]: crossvalidated
[post_id]: 339870
[parent_id]: 
[tags]: 
Case where transfer learning performs better than finetuning?

I'm working on a computer vision classification subject using CNN. I use a pretrained model as basis of my model. I read in Stanford CS 231n class that for large dataset with domain specific, best practice was to fine-tune (retrain all layers). I would like to know if fine-tune (with a right learning rate) always perform better than transfer learning or if there it is possible to erase some usefull features and overfit the dataset/domain.
