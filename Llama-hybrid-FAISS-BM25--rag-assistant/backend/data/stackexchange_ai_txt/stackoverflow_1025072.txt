[site]: stackoverflow
[post_id]: 1025072
[parent_id]: 
[tags]: 
OpenGL L-System Rendering

I have gotten my basic L-System working and I decided to try and optimize the rendering of the application. Previously I was looping through the whole string of the L-System with a switch case and drawing. Better yet I will show you what I was doing: for(unsigned int stringLoop = 0; stringLoop I removed all of this because I was literally resolving the tree every single frame, I changed this loop so that it would save all of the verticies in a std vector. for(unsigned int stringLoop = 0; stringLoop Now I changed my render loop so I just read straight from the vector array. DesignPatterns::Facades::OpenGLFacade _graphics = DesignPatterns::Facades::openGLFacade::Instance(); _graphics.Begin(OGLFlags::LINE_STRIP); for(unsigned int i = 0; i Now my problem is that when I am using a vector array and I use Line Stip, I get extra artifact. The first image is the original render that is the unoptimized one, then the second is the newer render which runs faster, and the thirds is a render of a dragon curve which uses no push and pops like the first two are using (I am pretty sure the push and pop is where the problems are coming in). Is the problem with my logic here of storing verticies, or is it because I am using a line strip? I would just use lines, but it doesn't really work at all, it ends up looking more of a line_stipple. Also sorry about the length of this post. alt text http://img197.imageshack.us/img197/8030/bettera.jpg alt text http://img23.imageshack.us/img23/3924/buggyrender.jpg alt text http://img8.imageshack.us/img8/627/dragoncurve.jpg
