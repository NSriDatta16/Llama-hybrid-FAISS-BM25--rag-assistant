[site]: crossvalidated
[post_id]: 172014
[parent_id]: 171957
[tags]: 
A few things that might help: The distribution of $z$ by itself isn't likely to be important, but the distribution of your data given values of $b_i$ will be. That may sound like a pedantic difference but it's a big one. For example, say you've chosen to use a gaussian likelihood and you have 10 observations of $z$ for which $b_i = 0, \forall i\in[1, n]$ (i.e. all the predictors are 0) and then 10 other observations for which $b_i = 1, \forall i\in[1, n]$. Then, as long as the distribution of $z$ around the predicted mean in the first 10 cases (which would be 0) is roughly gaussian and the and distribution of $z$ around the second 10 with a predicted mean equal to the sum of the coefficients is also roughly gaussian, you'd have a good model (even though the distributions of the $z$ values across all 20 observations is something weird -- a mixture of two gaussians). Even in this simple case just looking at the $z$ values themselves wouldn't tell you anything useful. You'd be better off picking one of several likelihoods based on some theoretical reasoning and then comparing the fit of them all to choose the best one. 2. With regards to both choosing a likelihood and a form for the model, you'll want to know how to do one of two things (or possibly both): Model Selection - There are many ways to compare models to one another for the sake of choosing the "best" one, but using an information criteria like WAIC is a good choice. Bayes factors or other information criteria like AIC are other possibilities, but WAIC generally has better properties. Regardless though, these things are always just approximations of how much better one model is likely to be at making predictions so you could also just test that directly through cross-validation too. Model Averaging - Rather than choose a single model, you can also provide some inference using all many of them. This is possible for the sake of making predictions as well as determining covariate effects, though it takes a good bit of computational legwork. The first option is certainly more common (you could characterize it as "trial-and-error" in a sense) so I'd start there if you don't think a simple linear model with nothing more than main effects is good enough (there's no way to know a priori how complicated the model should be). Not knowing anything more about your data or problem (but at least assuming the outcome is continuous), I'll take a somewhat wild guess and say that either a gaussian or gamma likelihood will give you about as reasonable of a model as you're likely to get with more pedestrian methods. Failing that you could also punt on picking a likelihood and try Approximate Bayesian Computing instead. There are "likelihood-free" ways to draw inference like this, though they seem to be used much less frequently.
