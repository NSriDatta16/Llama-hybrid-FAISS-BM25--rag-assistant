[site]: datascience
[post_id]: 91141
[parent_id]: 
[tags]: 
Is it good practice to use SMOTE when you have a data set that has imbalanced classes when using BERT model for text classification?

I had a question related to SMOTE. If you have a data set that is imbalanced, is it correct to use SMOTE when you are using BERT? I believe I read somewhere that you do not need to do this since BERT take this into account, but I'm unable to find the article where I read that. Either from your own research or experience, would you say that oversampling using SMOTE (or some other algorithm) is useful when classifying using a BERT model? Or would it be redundant/unnecessary?
