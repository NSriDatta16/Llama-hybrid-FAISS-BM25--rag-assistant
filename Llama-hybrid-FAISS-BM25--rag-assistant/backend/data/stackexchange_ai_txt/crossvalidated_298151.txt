[site]: crossvalidated
[post_id]: 298151
[parent_id]: 
[tags]: 
Making sense of the results of machine learning feature selection

Credits: I recently used vowpal wabbit ( https://github.com/JohnLangford/vowpal_wabbit/wiki ) and Ariel F's weight loss github ( https://github.com/arielf/weight-loss ) along with Atulya's data scrapper ( https://github.com/atulya2109/Stats-Royale-Python ) to run machine learning on 54,000+ games of Clash Royale (a card dueling game like Magic the Gathering). Results Red (negative) means stronger card characters in Clash Royale because they beat the opponent and made his score go down. https://codepen.io/TylerL-uxai/full/ayVEbm/ Where I'm Confused I don't understand why a card like tornado4 (seen in 1896 games) can rank most powerful, when tornado5 (seen in 3400 games) is considered slightly weak. Just using tornado levels 4, 5, and 6 as examples, shouldn't they all be closer together? Why would level 5 tornado correlate more with losing than winning, when the weaker tornado level 4 is considered the most powerful card? Did I do something wrong collecting data? I've been working 2 weeks on this and feel like the data is finally reliable... but apparently it's not? Data Mining Rules Games by pretty expert players with a rating of at least 3000 (the players on average were about 3500 to 4500) Assigned "weight" to the players rather than using trophy ranking. I found that one game that dropped 500 trophies would throw off the data, so each game is equal with a +/- 25 point win/lose No draws or 2v2 games Tried to make sure there were no duplicates Only games within the last 48 hours
