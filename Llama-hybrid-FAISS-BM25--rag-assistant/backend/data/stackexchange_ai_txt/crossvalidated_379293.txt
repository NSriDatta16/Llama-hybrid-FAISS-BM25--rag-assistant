[site]: crossvalidated
[post_id]: 379293
[parent_id]: 
[tags]: 
VAE with mixture of gaussian prior

I try to understand this paper where they try to use a mixture of Gaussian as a prior, instead of the standard gaussian. There are several things unclear to me though: They say that they set $\pi_k = \frac{1}{K}$ and draw $z$ from Cat( $\pi$ ). But later in equation 5 they parameterize $p_\beta(z_k = 1|x, w)$ by a neural network, and also condition it on the inputs. How does that fit together? Also in equation 5 they calculate a KL divergence. What is this KL divergence explicitly though? How to optimize it? They don't write how to optimize the $z$ -prior term in equation 4. Is there a closed form solution similar to the $w$ -prior term? Or how would one do that?
