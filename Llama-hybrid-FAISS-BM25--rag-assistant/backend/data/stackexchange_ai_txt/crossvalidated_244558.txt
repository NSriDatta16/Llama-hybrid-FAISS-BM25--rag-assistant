[site]: crossvalidated
[post_id]: 244558
[parent_id]: 
[tags]: 
Estimating probability with uncertainty with small sample sizes

This is my first question here, so I hope it's appropriate for this site. I have a kind of poor-man's "machine learning" algorithm. A program takes an input text, randomly applies a variety of changes to the wording based on a custom thesaurus. It replaces individual words and short phrases. A human reviews the changes, and possibly reverts some of the changes that don't make sense in the given text. The data for each substitution that was either accepted or reverted is saved to a database. Now I want to use that data to give a probability between 0 and 1 that a given change should be applied based on the past history of how often it was accepted or reverted. The idea is that changes that are known to be problematic are made less frequently when the algorithm is randomly selecting changes to make to the input text. Currently I have the formula: P = (accept_count+0.1)/(accept_count+revert_count+0.1) But it's very arbitrary and I feel it can be improved upon by applying some basic knowledge from statistics - I just don't know where to even start. I can see two problems with the current formula. A change that has no data has P=1, a change that's been accepted 10 times and never reverted also has P=1, but we can be much more confident about it. So a change with no data should have a P below 1 and increase towards 1 as the accept count rises. A pure ratio formula like this is suboptimal. It discards information on how large the actual accept/revert counts are. 500/1000 shouldn't be the same as 5/10. The larger those counts, the larger our sample, and the more confident we are about P. Somehow that should be reflected.
