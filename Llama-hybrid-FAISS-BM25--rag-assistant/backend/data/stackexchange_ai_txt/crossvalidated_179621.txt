[site]: crossvalidated
[post_id]: 179621
[parent_id]: 154795
[tags]: 
The predictor variables don't need to be normally distributed. Instead, the noise term $\epsilon$ is normally distributed: $$y=w^{T}x+\epsilon$$ If we build the model using maximum likelihood estimates, then it's just linear regression without regularization. If we assume the weights have a Gaussian prior and use MAP estimates, then it will become ridge regression. But in either case, predictor variables need not be normally distributed. As for when to use ridge regression, in most cases ridge regression outperforms other regularization methods (say L1-norm). You can refer to the following FAQ page by the LIBSVM author: https://www.csie.ntu.edu.tw/~cjlin/liblinear/FAQ.html#l1_regularized_classification Therefore I will say try ridge regression first. You may also want to try L1-regularization (LASSO) or elastic nets when: You know some of the features you are including in your model might be zero (i.e., you know the some coefficients in the "true model" are zero) Your features do not highly correlate with each other You want to perform feature selection but don't want to use wrapper/filter approaches The above reasons are exactly the nature of L1-regularization. The LASSO yields sparse output, which can be viewed as built-in feature selection. However it only selects one feature from a bunch of highly correlated features.
