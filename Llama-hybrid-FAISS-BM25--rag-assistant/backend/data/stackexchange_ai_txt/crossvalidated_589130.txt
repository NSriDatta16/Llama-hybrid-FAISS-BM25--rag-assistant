[site]: crossvalidated
[post_id]: 589130
[parent_id]: 399789
[tags]: 
I have worked with this problem a lot, and I cannot find a satisfactory answer, but here are a few ways that I've attacked it. (1) If $M=k$ , and if you know that a valid probability distribution can be created, then this will be the solution: $$ \begin{equation} \begin{bmatrix} 1 & ... & 1\\ 0 & ... & M\\ (0-mean)^2 & ... & (M-mean)^2\\ ...& ... & ... \end{bmatrix} P_{k} = \begin{bmatrix} 1 \\ mean \\ variance \\ .. \end{bmatrix} \end{equation} $$ (2) If $M>k$ , then choose a subset of $[0,...,M]$ that is size $k$ that you know can be a domain for a valid probability distribution, and repeat (1). For both (1) and (2), knowing you have domain that can produce a valid distribution isn't an easy task. Yamada and Primbs (in Construction of Multinomial Lattice Random Walks for Optimal Hedges) wrote some formulas to verify whether the domains are valid for moments up to kurtosis, but the formulas get pretty nasty for orders higher than that. (3) As others have pointed out, this isn't a linear optimization problem: but you can turn it into a quadratic one. I use the Goldfarb-Idnani algorithm to minimize $pAp^T$ , where $A$ is the identity, then add the constraints that $p_i\ge0$ , $\sum_{i=0} ^{M} p_i = 1$ , $\sum_{i=0} ^{M} ip_i = mean$ , &etc. Goldfarb-Idnani doesn't require a guess that satisfies the constraints (which is your problem to begin with). It also minimizes entropy, which makes for a really nice looking solution. The downside is that it takes considerably longer than (1) or (2). (4) Donghui Chen and Robert J.Plemmons, Nonnegativity Constraints in Numerical Analysis. Construct a matrix like in (1), but where $M>k$ , then solve with their algorithm. Much faster than (3), but the distribution it gives will not be minimizing entropy. Slower than (1), but can work much better in practice.
