[site]: crossvalidated
[post_id]: 132529
[parent_id]: 131138
[tags]: 
The reason is that the VC dimension for Gaussian kernels is infinite, and thus, given the correct values for the parameters (sigma), they can classify an arbitrarily large number of samples correctly. RBFs work well because they ensure that the matrix $K(x_{i},x_{j})$ is full rank. The idea is that $K(x_{i},x_{i}) > 0$, and off-diagonal terms can be made arbitrarily small by decreasing the value of $\sigma$. Notice that the kernel corresponds to a dot product in the feature space. In this feature space, the dimension is infinite (by considering the series expansion of the exponential). One could thus see this as projecting those points in different dimensions so that you can separate them. Consider by contrast, the case of linear kernels, which can only shatter four points on the plane. You may take a look at this paper , though it's very technical. One of the standard books on SVMs should make this concept more accessible.
