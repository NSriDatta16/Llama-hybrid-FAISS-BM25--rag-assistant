[site]: crossvalidated
[post_id]: 585744
[parent_id]: 585701
[tags]: 
I'd normally prefer PCA for low-d (usually 2-d) data visualisation, as orthogonal components are more intuitive in this respect. I also suspect that PCA will usually work better for dimension reduction of the x-space in linear regression, because collinearity is unhelpful in regression, although I normally use neither for this task (I have seen data in which Principal Components Regression did a pretty good job, but even there something else such as Partial Least Squares could be found that is slightly better using cross-validation for comparison; I don't think NMF would've been any good in these situations though).
