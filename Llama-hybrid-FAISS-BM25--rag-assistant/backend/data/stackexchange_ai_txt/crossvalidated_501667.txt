[site]: crossvalidated
[post_id]: 501667
[parent_id]: 501633
[tags]: 
If you reduce the learning rate, you slow down how fast the gradient descent algorithm traverses the loss function. You can think of this as meaning smaller, more localised 'steps'. A higher learning rate means larger steps, and hence faster traversal. A downside is that by taking larger steps, you can potentially 'overshoot' the optimal solution - this likely explains the higher RMSE. You could consider changing your batch size, using a different optimiser and changing your neural network structure (layering and size of your convolutions). You have a lot of different levers to pull.
