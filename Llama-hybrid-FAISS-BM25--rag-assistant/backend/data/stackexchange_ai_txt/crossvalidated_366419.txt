[site]: crossvalidated
[post_id]: 366419
[parent_id]: 365938
[tags]: 
UPDATE See this second post for McDonald's feedback on my answer where the notion of risk consistency is related to stability. 1) Uniqueness vs Stability Your question is difficult to answer because it mentions two very different topics: uniqueness and stability . Intuitively, a solution is unique if given a fixed data set, the algorithm always produces the same results. Martin's answer cover's this point in great detail. Stability on the other hand can be intuitively understood as one for which the prediction does not change much when the training data is modified slightly. Stability applies to your question because Lasso feature selection is (often) performed via Cross Validation, hence the Lasso algorithm is performed on different folds of data and may yield different results each time. Stability and the No Free Lunch Theorem Using the definition from here if we define Uniform stability as: An algorithm has uniform stability $\beta$ with respect to the loss function $V$ if the following holds: $$\forall S \in Z^m \ \ \forall i \in \{ 1,...,m\}, \ \ \sup | > V(f_s,z) - V(f_{S^{|i},z}) |\ \ \leq \beta$$ Considered as a function of $m$, the term $\beta$ can be written as $\beta_m$. We say the algorithm is stable when $\beta_m$ decreases as $\frac{1}{m}$. then the "No Free Lunch Theorem, Xu and Caramis (2012)" states that If an algorithm is sparse , in the sense that it identifies redundant features, then that algorithm is not stable (and the uniform stability bound $\beta$ does not go to zero). [...] If an algorithm is stable, then there is no hope that it will be sparse. (pages 3 and 4) For instance, $L_2$ regularized regression is stable and does not identify redundant features, while $L_1$ regularized regression (Lasso) is unstable. An attempt at answering your question I think 'lasso favors a sparse solution' is not an answer to why use lasso for feature selection I disagree, the reason Lasso is used for feature selection is that it yields a sparse solution and can be shown to have the IRF property, i.e. Identifies Redundant Features. What is the most crucial reason that causes this instability The No Free Lunch Theorem Going further This is not to say that the combination of Cross Validation and Lasso doesn't work... in fact it has been shown experimentally (and with much supporting theory) to work very well under various conditions. The main keywords here are consistency , risk, oracle inequalities etc.. The following slides and paper by McDonald and Homrighausen (2013) describe some conditions under which Lasso feature selection works well: slides and paper: "The lasso, persistence, and cross-validation, McDonald and Homrighausen (2013)" . Tibshirani himself also posted an great set of notes on sparcity , linear regression The various conditions for consistency and their impact on Lasso is an active topic of research and is definitely not a trivial question. I can point you towards some research papers which are relevant: Video lectures on the No free lunch theorem, by Xu H.M. Bøvelstad et all, A comparison of feature selection approaches for gene selection, (2007) The lasso, persistence, and cross-validation, McDonald and Homrighausen (2013) Huang and Bowick, Summary and discussion of: “Stability Selection” Lim and Yu, Estimation Stability with Cross Validation, (2015) A talk by Peter Buhlmann: Stability Selection for High-Dimensional Data, (2008) and the accompanying paper Wang, Nan et all, Random Lasso, (2011) Stackexchange post: Model stability when dealing with large $p$, small $n$ problem Roberts, Nowakm Stabilizing the lasso against cross-validation variability, (2014) which argue that "percentile-lasso, can result in large reductions in both model-selection instability and model-selection error, compared to the lasso" An awesome set of notes by Tibshirani and Wasserman on sparcity , linear regression
