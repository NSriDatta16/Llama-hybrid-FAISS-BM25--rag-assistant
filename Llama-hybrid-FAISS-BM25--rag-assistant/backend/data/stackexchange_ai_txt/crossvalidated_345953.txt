[site]: crossvalidated
[post_id]: 345953
[parent_id]: 345737
[tags]: 
I would say that at a high level, the inductive bias of DNNs (deep neural networks) is powerful but slightly too loose or not opinionated enough. By that I mean that DNNs capture a lot of surface statistics about what is going on, but fail to get to the deeper causal/compositional high level structure. (You could view convolutions as a poor's man inductive bias specification). In addition, it is believed in the machine learning community that the best way to generalize (making good inferences/predictions with little data) is to find the shortest program that gave rise to the data. But program induction/synthesis is hard and we have no good way of doing it efficiently. So instead we rely on a close approximation which is circuit search, and we know how do that with backpropagation. Here , Ilya Sutskever gives an overview of that idea. To illustrate the difference in generalization power of models represented as actual programs vs deep learning models, I'll show the one in this paper: Simulation as an engine of physical scene understanding . (A) The IPE [intuitive physics engine] model takes inputs (e.g., perception, language, memory, imagery, etc.) that instantiate a distribution over scenes (1), then simulates the effects of physics on the distribution (2), and then aggregates the results for output to other sensorimotor and cognitive faculties (3) (B) Exp. 1 (Will it fall?) tower stimuli. The tower with the red border is actually delicately balanced, and the other two are the same height, but the blue-bordered one is judged much less likely to fall by the model and people. (C) Probabilistic IPE model (x axis) vs. human judgment averages (y axis) in Exp. 1. See Fig. S3 for correlations for other values of σ and ϕ. Each point represents one tower (with SEM), and the three colored circles correspond to the three towers in B. (D) Ground truth (nonprobabilistic) vs. human judgments (Exp. 1). Because it does not represent uncertainty, it cannot capture people’s judgments for a number of our stimuli, such as the red-bordered tower in B. (Note that these cases may be rare in natural scenes, where configurations tend to be more clearly stable or unstable and the IPE would be expected to correlate better with ground truth than it does on our stimuli.) My point here is that the fit in C is really good, because the model captures the right biases about how humans make physical judgments. This is in big part because it models actual physics (remember that it is a actual physics engine) and can deal with uncertainty. Now the obvious question is: can you do that with deep learning? This is what Lerer et al did in this work: Learning Physical Intuition of Block Towers by Example Their model: Their model is actually pretty good on the task at hand (predicting the number of falling blocks, and even their falling direction) But it suffers two major drawbacks: It needs a huge amount of data to train properly In generalizes only in shallow ways: you can transfer to more realistic looking images, add or remove 1 or 2 blocks. But anything beyond that, and the performance goes down catastrophically: add 3 or 4 blocks, change the prediction task... There was a comparison study done by Tenenbaum's lab about these two approaches: A Comparative Evaluation of Approximate Probabilistic Simulation and Deep Neural Networks as Accounts of Human Physical Scene Understanding . Quoting the discussion section: The performance of CNNs decreases as there are fewer training data. Although AlexNet (not pretrained) performs better with 200,000 training images, it also suffers more from the lack of data, while pretrained AlexNet is able to learn better from a small amount of training images. For our task, both models require around 1,000 images for their performance to be comparable to the IPE model and humans. CNNs also have limited generalization ability across even small scene variations, such as changing the number of blocks. In contrast, IPE models naturally generalize and capture the ways that human judgment accuracy decreases with the number of blocks in a stack. Taken together, these results point to something fundamental about human cognition that neural networks (or at least CNNs) are not currently capturing: the existence of a mental model of the world’s causal processes. Causal mental models can be simulated to predict what will happen in qualitatively novel situations, and they do not require vast and diverse training data to generalize broadly, but they are inherently subject to certain kinds of errors (e.g., propagation of uncertainty due to state and dynamics noise) just in virtue of operating by simulation. Back to the point I want to make: while neural networks are powerful models, they seem to lack the ability to represent causal, compositional and complex structure. And they make up for that by requiring lots of training data. And back to your question: I would venture that the broad inductive bias and the fact that neural networks do not model causality/compositionality is why they need so much training data. Regularization is not a great fix because of the way they generalize. A better fix would be to change their bias, as is currently being tried by Hinton with capsules for modelling whole/part geometry, or interaction networks for modelling relations.
