[site]: crossvalidated
[post_id]: 274778
[parent_id]: 258721
[tags]: 
What is a gradient? It is a derivative of a function at a certain point. Basically, gives the slope of the line at that point. How to calculate this slope geometrically(just consider a 2 D graph and any continuous function)? You draw a tangent at that point crossing x-axis and a perpendicular to the x-axis from that point. It will form a triangle and now calculating slope is easy. Also if this tangent is parallel to x-axis the gradient is 0 and if it is parallel to y-axis the gradient is infinity . Why use ascent or descent? If I have a function which is convex then at the bottom the gradient or derivative is 0. Similarly, if we have a concave function at the top gradient or derivative is 0. Why are we interested in 0? This is because it helps us find either the lowest(convex) or highest(concave) value of the function Now our machine learning has a cost function and they can either be concave or convex . If it is convex we use Gradient Descent and if it is concave we use we use Gradient Ascent . Now there are two cost functions for logistic regression . When we use the convex one we use gradient descent and when we use the concave one we use gradient ascent. Also, note that if I add a minus before a convex function it becomes concave and vice versa.
