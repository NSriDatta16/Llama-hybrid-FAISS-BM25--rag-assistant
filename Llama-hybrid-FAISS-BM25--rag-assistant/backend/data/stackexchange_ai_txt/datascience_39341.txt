[site]: datascience
[post_id]: 39341
[parent_id]: 39334
[tags]: 
EDIT after comments Also are arguments of Embedding layer OK? Yes. But You need to pass return_sequences = True in the first LSTM layer so that it will pass sequences to the next LSTM layer. From the docs return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. How are both these adjusted? From the docs . input_dim: Size of the vocabulary, i.e. maximum integer index + 1. This determines the largest integer in the input data. The largest integer in the input should be no larger than the vocabulary size. This should be the output_dim: Dimension of the dense embedding. int >= 0 input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed). I am posting code below for dummy data. The size of vocabulary has been taken 100. from keras.models import Sequential from keras.layers import Embedding, LSTM, Dense import numpy as np import pandas as pd from sklearn.model_selection import train_test_split input_array = np.random.randint(100, size=(250, 2000)) input_y = np.random.randint(7, size = (250)) Y_dumy = pd.get_dummies(input_y) X_train, X_test, Y_train, Y_test = train_test_split(input_array, Y_dumy, test_size=0.1, random_state=0) model = Sequential() model.add(Embedding(input_dim = 100, output_dim = 64, input_length=2000)) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences =True)) model.output_shape #Output shape should be: #model.output_shape = (None, 2000, 64) #3D tensor with shape: (batch_size, sequence_length, output_dim) model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(Y_dumy.shape[1], activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy']) model.fit(X_train, Y_train, epochs=50, verbose=True, validation_data= (X_test, Y_test))` Where is the error coming from and how can it be solved? I believe the error is coming because of absence of input_length. For similar errors please have a look at this post . After the comments The error is coming because of return_sequences =False in the first LSTM layer.
