[site]: crossvalidated
[post_id]: 490113
[parent_id]: 490109
[tags]: 
No, in general you will obtain a different line wih ordinary least squares if you interchange x and y . You can easily check this with your formula by interchanging x and y and comparing it to $1/\beta$ . The reaaon for this descrepancy is that ordinary least squares is not about fitting a line through points, but about prediction and thus assumes a specific role of the variables: x is "predictor", y is "response". If your problem is actually about fitting a line through points, you should consider "orthogonal least squares", which is a symmetric approach and has (for straight lines) two equivalent solutions: the right singular vector $\vec{v}_1$ corresponding to the largest singular value $s_1\geq\ldots\geq s_n$ in the singular value decomposition (SVD) $Q=USV^T$ of the matrix built from the centered data points $$Q^T = (\vec{q}_1,\ldots,\vec{q}_n) \quad\mbox{ with }\quad \vec{q}_i = \vec{x}_i - \vec{a}$$ the eigenvector corresponding to the largest eigenvalue of $Q^TQ$ . $Q^TQ$ is identical to the scatter matrix, or $(n-1)$ times the covariance matrix of the data points $\vec{x}_1,\ldots,\vec{x}_n$ . Thus, this vector is simply the principal component obtained from a principal component analysis (PCA) Note that orthogonal least squares also yields a reasonable result when the points happen to fall on (or around) a vertical line. Reference: H. Späth: "Orthogonal least squares fitting with linear manifolds." Numerische Mathematik 48 (1986), pp. 441–445.
