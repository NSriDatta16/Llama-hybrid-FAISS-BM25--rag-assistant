[site]: crossvalidated
[post_id]: 369133
[parent_id]: 368996
[tags]: 
Your description is more or less correct. First thing to note though is that dropout is usually applied on an entire layer, rather than a neural network. Of course, you could apply dropout to all layers in your neural network. Let's assume we are dealing with a fully connected network. If dropout is applied during training with a factor of 0.5, that means that only half of all activation functions will deliver an output. The idea is that by training a different mix of units during each step, you get a more robust model that is able to deal well with noise/variability. (You can also compare it to the effect of ensemble networks.) Now during testing however, we would like to use the full knowledge of the network, and thus not drop any units. Doing so would mean that our network is no longer properly scaled: the expected value during testing no longer matches the expected value during training. A simple way to fix this is indeed to scale all the activation outputs of the layer by the value of dropout (p). When p is 0.5, during testing, this means we have double the amount of outputs at the layer in question, so we rescale all those outputs by multiplying them with 0.5.
