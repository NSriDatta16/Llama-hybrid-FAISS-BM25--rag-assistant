[site]: crossvalidated
[post_id]: 244945
[parent_id]: 244917
[tags]: 
Disclaimer: I have never worked with this distribution before. This answer is based on this wikipedia article and my interpretation of it. The Dirichlet distribution is a multivariate probability distribution with similar properties to the Beta distribution. The PDF is defined as follows: $$\{x_1, \dots, x_K\} \sim\frac{1}{B(\boldsymbol{\alpha})}\prod_{i=1}^Kx_i^{\alpha_i - 1}$$ with $K \geq 2$, $x_i \in (0,1)$ and $\sum_{i=1}^Kx_i = 1$. If we look at the closely related Beta distribution: $$\{x_1, x_2 (=1-x_1)\} \sim \frac{1}{B(\alpha,\beta)}x_1^{\alpha-1}x_2^{\beta-1}$$ we can see that these two distributions are the same if $K=2$. So let's base our interpretation on that first and then generalise to $K>2$. In Bayesian statistics, the Beta distribution is used as a conjugate prior for binomial parameters (See Beta distribution ). The prior can be defined as some prior knowledge on $\alpha$ and $\beta$ (or in line with the Dirichlet distribution $\alpha_1$ and $\alpha_2$). If some binomial trial then has $A$ successes and $B$ failures, the posterior distribution is then as follows: $\alpha_{1,pos} = \alpha_1 + A$ and $\alpha_{2,pos}=\alpha_2 + B$. (I won't work this out, as this is probably one of the first things you learn with Bayesian statistics). So the Beta distribution then represents some posterior distribution on $x_1$ and $x_2 (=1-x_1)$, which can be interpreted as the probability of successes and failures respectively in a Binomial distribution. And the more data ($A$ and $B$) you have, the narrower this posterior distribution will be. Now we know how the distribution works for $K=2$, we can generalise it to work for a multinomial distribution instead of a binomial. Which means that instead of two possible outcomes (success or failure), we will allow for $K$ outcomes (see why it generalises to Beta/Binom if $K=2$?). Each of these $K$ outcomes will have a probability $x_i$, which sums to 1 as probabilities do. $\alpha_i$ then takes a similar role to the $\alpha_1$ and $\alpha_2$ in the Beta distribution as a prior for $x_i$ and gets updated in a similar fashion. So now to get to your questions: How do the alphas affect the distribution? The distribution is bounded by the restrictions $x_i \in (0,1)$ and $\sum_{i=1}^Kx_i = 1$. The $\alpha_i$ determine which parts of the $K$-dimensional space get the most mass. You can see this in this image (not embedding it here because I don't own the picture). The more data there is in the posterior (using that interpretation) the higher the $\sum_{i=1}^K\alpha_i$, so the more certain you are of the value of $x_i$, or the probabilities for each of the outcomes. This means that the density will be more concentrated. How are the alphas being normalized? The normalisation of the distribution (making sure the integral equals 1) goes through the term $B(\boldsymbol{\alpha})$: $$B(\boldsymbol{\alpha}) = \frac{\prod_{i=1}^K\Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^K\alpha_i)}$$ Again if we look at the case $K=2$ we can see that the normalising factor is the same as in the Beta distribution, which used the following: $$B(\alpha_1, \alpha_2) = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1+\alpha_2)}$$ This extends to $$B(\boldsymbol{\alpha}) = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\dots\Gamma(\alpha_K)}{\Gamma(\alpha_1+\alpha_2+\dots+\alpha_K)}$$ What happens when the alphas are not integers? The interpretation doesn't change for $\alpha_i>1$, but as you can see in the image I linked before , if $\alpha_i
