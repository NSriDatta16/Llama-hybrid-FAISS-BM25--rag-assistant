[site]: crossvalidated
[post_id]: 74173
[parent_id]: 74164
[tags]: 
Your first sentence is not necessarily correct. First off, an increase in numbers of parameters does indeed increase the degrees of freedom and the standard errors of point estimates (hence their degree of generalization). An example of this is the nested classes of Exponential and Weibull models. It's not universally agreed upon that "model complexity" necessarily means the parameter space, though, but it is a good place to start for discussion. Semiparametric and nonparametric inference make overfitting a nonissue by generalizing the likelihood function into a new type of function where such extraneous parameters are ancillary. The only caveat is that the statistician has to correctly identify such models. Examples of such extended likelihoods are conditional likelihood (in mixed modeling), partial likelihood (in Cox models), pseudo likelihood (forgetting some applications for that...), profile likelihood, quasilikelihood, (and the list goes on). The parameter spaces for such likelihoods are seen as projections of high (possibly infinte) dimensional (compact) parameter spaces. It's only in fully parametric inference where every causal relationship needs to be specified, such as the correlation structure for teeth within a mouth, or the correlation between failure times in a prospective cohort among denominators of individuals counted more than once. Many of these likelihoods are overly complex or intractable hence inference about them is non-existent or otherwise not popular. Modeling processes is a fully parametric endeavor. You must be able to simulate data from an estimated data generating mechanism. SP/NP often cannot achieve this. Neither can the produce fitted effects nor can they claim to simulate realizations from any data generating process. SP/NP focuses on the point estimation of a specific parameter and efficiently calculates estimates and standard errors for that parameter cancelling out all other parameters in the data generating process through either conditioning, estimating them as nuisance parameters, or some other process. SP/NP inference examples are the log-rank test (NP), the plain vanilla asymptotic t-test without normality assumptions (NP), conditional logistic regression (SP), generalized estimating equations (GEE), and Cox proportional hazards models (SP). Examples where semi-parametric inference breaks down is in the case of missing at random data (as opposed to missing completely at random data), where the value of some observed outcome or covariate depends on the things which we deemed to be ancillary (such as informative censoring in Cox models). A fully likelihood based survival analysis would require separate models (and their correlation) for survival and censoring outcomes.
