[site]: crossvalidated
[post_id]: 435791
[parent_id]: 435778
[tags]: 
Linear regression, random forest and SVR are supervised learning models. Unlike unsupervised learning models, they take labelled in-sample (training) data $(\mathbf{x},\mathbf{y})$ as their input for building a prediction model that hopefully can make reliable forecasts when confronted with unseen out-of-sample (test) data, where $\mathbf{y}$ contains the real-valued labels for datapoints $\mathbf{x}$ . Each of these three have been considered for forecasting in many articles, sometimes in comparison to one another to see which does the task with lowest (out-of-sample) prediction error, so there are two important considerations when selecting which to use: (1) is the machine learning algorithm known to overfit or underfit training data? Overfitting means that the prediction model is fit too tightly to the training data's signal and noise, that it does not generalize well to test data that has a different signal-to-noise ratio. (2) how far ahead is the predictor known to make reliable forecasts? Linear regression is considered the best, linear, unbiased estimator according to the Gauss Markov Theorem, however, it has been proven to be a high variance supervised learning model because regularized regression models such as ridge regression, lasso regression and elastic net regression can achieve even lower variance than OLS. Random forest forecasts tend to be the runner-up to SVR in terms of accuracy, but they also suffer from overfitting like OLS and are least interpretable. SVR is known to capture non-linearity and so can forecast in the short-term very well, as long as you are using the appropriate kernel for your task, but the absence of studies done for forecast horizons longer than 3- or even 1-day suggests it should not be used for long-horizon forecasting . Exponential smoothing , or exponentially weighted moving average (EWMA), where high (low) weights are given to very recent (long ago) observations that decay exponentially, are more realistic than equal-weighting schemes, but for some data, this exponential decay might be too fast. Financial data, for example, tends to possess a slow rate of decay in their autocorrelation functions, which gave rise to the use of the much slower hyperbolic decay rate, based on the power rule. For more control over the rate of decay, there is also fractionally integrated GARCH (FIGARCH).
