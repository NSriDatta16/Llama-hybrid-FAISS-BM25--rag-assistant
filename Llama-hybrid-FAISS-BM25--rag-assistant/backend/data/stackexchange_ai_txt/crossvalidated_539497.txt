[site]: crossvalidated
[post_id]: 539497
[parent_id]: 
[tags]: 
Concordance Scoring Best & Best Alternative Results

I have a model that outputs two ordered results from a large set: Let's call them the "best" and "next" choices. I set of judges rate this output, and can tell me various things, including whether the model is completely correct (i.e., best really is best, and next really is next best), or it can tell me that they are backwards, or it can tell me that one or the other shouldn't be there at all. (I'm not worried about what the judge says should be there instead; for the moment I'm just looking for a statistic, not a training set.) I don't think that a simple IRR (% agreement or Kappa) is applicable here, neither can I see how to use a Pearson RC, Kemeny-Snell distance or Edmond-Mason $\tau_{x}$ , because of the possibility that the judges are acting more like oracles than judges, and can disagree with one or both, or the ranking. I supposed that I could theoretically consider that the model and judges/oracles are ranking the same very large set, and then just picking off the top two, but I don't see where that gets me if they disagree on those two. [Extended in response to commentary awhug] Let's say that both the program and judges are ranking a set of 100 options by giving integer ranks from 1 to 10. These may be non-unique, and esp. when an option is considered "wrong", that is, should not be ranked at all, they can give it a very low rank, say 999. (I'm not stuck on these choices if any of them seem inappropriate, esp. the choice of 999 as the "wrong" score. Might be better to just leave them out, but that would give me unbalanced vectors, which isn't allowed for some tests, such as Kt.) In reality, although the program actually ranks all 100 options, the judges probably do not. Generally we'll get only a top (1), or the top two (1,1 or 1,2), sometimes 3. We could probably ask them to rate the top 5 from the program (without revealing the program's rankings, of course), so, for example, we say that A,...,E are the options that should be ranked, and we can ask them to give each a rank from 1 to 5, or indicate that any of them is "wrong" (in which case, herebelow, as above, I'll use 9). So, for example, the program output hereafter will always be (1,2,3,4) for (A,...,J) [I've reduced to a space of 10 options for brevity, but in reality there are hundreds.] That is, it decided that there were 4 appropriate options from the 10, and that every other option was wrong. That is, a complete ranking would be: Pa1 (Note that it dosen't matter the order of the program's rankings. We'll assume that this is on (A,...), and just remap the judges to that.) Now, each judge reports a partial and overlappnig ranking that (as just explained) we will map to the (A,...,J). (You'll see what the small 'a' is for below.) Here are some examples of what judges could report. Note that they are asked to name options, not rank a given list, so they might leave some out, but, by assumption, they would not ever name an option that is not on the list. A real world example might be: "Name some of your favorite countries to visit, in order from favroite to least favorite." No judge would name a country that does not exist, however, they might say, for example, "I love France best, then India, but I hate Belgium." (I'm (obviously) going to use the first letters here as A...J, so I=India, and F=France, etc. This is just a happy coincidence of this example. The real problem isn't vitisting countries, but the real options have long confusing names of chemicals, and don't conveiently map to A..J, but the task and data are more-or-less the same, except that the list is hundreds long.) So, anyway, the above statement might be represented by this judgement vector: Je1 Here the zeros are un-mentioned options. Alternatively, and perhaps better, I could just call them 99s: Je2 Note that I can't just leave the left out (or hated) ones out altogether unless the list was actually a dictionary, like: Je3 Or, possibly, where leave-outs are the same as hates: Je4 I'm happy with any of those, if they make more sense, but for the below I'm going to just use the Je2 format. So, given the program's output, as above: Pa1 Let's say we get the following judgements: Ja1 and so on. Each of JaX can be passed indpedently, with Pa1, to, for example R's Kendall library: > library("Kendall") > Pa1 Ja1 Kendall(Pa1,Ja1) tau = 1, 2-sided pvalue =0.00084901 > Ja4 Kendall(Pa1,Ja4) tau = 0.112, 2-sided pvalue =0.8014 So, my first problem is just how does one aggregate Kendall scores? That is, suppose I had the four scores above: > Kendall(Pa1,Ja1) tau = 1, 2-sided pvalue =0.00084901 > Kendall(Pa1,Ja2) tau = 0.933, 2-sided pvalue =0.0018951 > Kendall(Pa1,Ja3) tau = 0.576, 2-sided pvalue =0.079696 > Kendall(Pa1,Ja4) tau = 0.112, 2-sided pvalue =0.8014 I could simply report someting like the mean and deviation of the tau: > mean(c(1,0.933,0.576,0.112)) [1] 0.65525 sd(c(1,0.933,0.576,0.112)) [1] 0.4071849 But what about the p-values? I'm thinking of treat this like a meta-analysis, combining the p-values using one of the several methods used in meta-analysis (e.g., summing them). But this seems like a painfully indirect way of building a statistic for something that seems pretty simple. Moreover, the meta-analysis methods I'm familiar with don't take into account the fact that the judges disagree with one another a lot, whereas you might get something more like this: Jb1 Here the agreement between judges is much greater. As could be eaily demonstrated by inter-rater reliabilities, perhaps even using Kt's for that as well. Here are other cases where you would intuitively want much different results: Jc1 In JcX the judges agree with one another but uniformly disagree (slightly) with the program. In JdX they also agree with one another, but completely disagree with the program. And in JeX the judges are all over the place, and also, obviously disagree with the program. Intuitively, Je is a harder problem than the other two, so perhaps the fact that the program disagrees with the judges isn't so bad. Okay, that should be enough for the nonce. Thanks for your attention and advice!
