[site]: crossvalidated
[post_id]: 262535
[parent_id]: 
[tags]: 
Bayesian posterior: is multiplying likelihood by prior (rather than simulation) an acceptable approach?

Ken Rice has a helpful introductory set of slides available online called 'Bayesian Statistics (a very brief introduction)'. http://faculty.washington.edu/kenrice/BayesIntroClassEpi515kmr2016.pdf On slide 23 he gives this formulation, which comes directly from Bayes theorem: Posterior ∝ Likelihood × Prior However, within a section on 'when priors don't matter (much)', on slide 33 he describes a method whereby you multiply the likelihood function by the prior to get the posterior. But he describes this as "semi-Bayesian". (On slide 35, I think he's referring to the same thing when he mentions an "approximate Bayes" approach, and describing "full Bayes" as better.) My question is: in what sense is taking a prior expressed as a functional form and multiplying it by a likelihood function only semi-Bayesian? Is it just that the (normal) likelihood he presents is only an approximation to the real likelihood function? Or is it because the multiplication he presents is only approximate? Or is there something more fundamentally 'semi' about this type of Bayesianism? More generally, the focus of texts on Bayesian inference seems to be on simulation (especially MCMC) approaches. Is this because it is 'wrong' to get your posterior distribution from multiplying a prior distribution by the likelihood function generated by some new data? Or is it because the analytical route is not often available to you?
