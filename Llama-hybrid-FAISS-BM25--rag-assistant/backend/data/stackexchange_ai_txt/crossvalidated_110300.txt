[site]: crossvalidated
[post_id]: 110300
[parent_id]: 109617
[tags]: 
To recapitulate what we have already discussed in the comments above. First, I see absolutely no problem in applying PCA to a time series or panel data. @NickCox said the same in the comments to the question you linked to. The paper that OP of that other question brought up and you linked in your question, does not contradict that (and I agree with you, it does not look very relevant). Second, PCA will reduce your $4\cdot182=728$ features to any number you want (e.g. to two, then you can plot all the data as a scatter plot and look at it), but it does not take class information into account at all (by classes I mean that subjects exhibiting and non-exhibiting something of interest form two classes). So you can end up with a largely overlapping classes in 2D, but it is very well possible that classes are actually well separated, just not in the first two PCs. The only thing PCA cares about is the overall amount of variance of the projection. Therefore, if class separation is what you are interested in, you might want to apply e.g. LDA (linear discriminant analysis) as another dimensionality reduction technique. It can also reduce the dimensionality (e.g. to 2D), but it looks for projections that achieve maximal class separation. Both PCA and LDA are linear methods, so if you know how to interpret PCA results, you know how to interpret LDA results as well. One caveat is that LDA can overfit if the number of features is high, but the number of subjects is not high enough ( see e.g. my answer here ). One sign of overfitting would be that LDA results in projections having tiny variance. If you are familiar with PCA math, you know that PCA is taking leading eigenvectors of covariance matrix $\boldsymbol \Sigma$ as projection axes. LDA takes leading eigenvectors of $\boldsymbol \Sigma_W^{-1} \boldsymbol \Sigma_B$ as projection axes, where $\boldsymbol \Sigma_W$ and $\boldsymbol \Sigma_B$ are within- and between-class covariance matrices. In case of overfitting, you can regularize LDA and force it to look for directions with large variance (somewhat like PCA!) by taking $(1-\lambda)\boldsymbol \Sigma_W + \lambda \mathbf{I}$ instead of $\boldsymbol \Sigma_W$. When $\lambda=0$, you get LDA. When $\lambda =1$, you get PCA on class means. Everything in between is regularized LDA.
