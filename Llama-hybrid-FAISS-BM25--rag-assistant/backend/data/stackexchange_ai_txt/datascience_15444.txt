[site]: datascience
[post_id]: 15444
[parent_id]: 
[tags]: 
Backpropagation Through Time (BPTT) of LSTM

I am currently trying to understand the BPTT for Long Short Term Memory (LSTM) in TensorFlow. I get that the parameter num_steps is used for the range that the RNN is rolled out and the Error backpropagated. I got a general question of how this works. For reference a repitition of the formulas. I'm referring to: Formulas LSTM ( https://arxiv.org/abs/1506.00019 ) Question: What paths are backpropagated that many steps? The constant error carousel is created by the formula 5, and the derivate for backpropagation ($s(t)\rightarrow s(t-1)$) is $1$ for all timesteps. This is why LSTMs capture long range dependencies. I get confused with the dependencies of $g(t)$, $i(t)$, $f(t)$ and $o(t)$ of $h(t-1)$. In words: The current gates do not just depend on the input, but also on the last hidden state. Doesn't this dependency lead to the exploding/vanishing gradients problem again? If I backpropagate along these connections I get gradients that are not one. Peephole connections essentially lead to the same problem. Thanks for your help!
