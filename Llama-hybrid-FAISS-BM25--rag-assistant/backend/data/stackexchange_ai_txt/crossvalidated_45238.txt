[site]: crossvalidated
[post_id]: 45238
[parent_id]: 45235
[tags]: 
The essential requirement is to chose a proposal distribution that is not 0 or too small in areas where the target distribution could have significant mass. The idea is that you're approximating one distribution with a another, and to the extent that they're similar it will take fewer samples to obtain a good approximation. Let $p(z)$ be some intractable target distribution and $q(z)$ be some convenient distribution that your favorite language has a built in sampler for. You can visualize your goal as trying to use $N$ samples from $q(z)$ to make a histogram that looks like $p(z)$. So, obviously you would like your samples to concentrate in regions where $p(z)$ is large. But if $q(z)$ is very small in such regions, then you're unlikely to obtain very many samples there. And if $q(z)=0$ in such regions then you will never obtain samples there. Here's the relevant excerpt from Bishop's Pattern Recognition and Machine Learning, page 534 "If, as is often the case, $p(z)f(z)$ is strongly varying and has a significant proportion of its mass concentrated over relatively small regions of z space, then the set of importance weights {rl} may be dominated by a few weights having large values, with the remaining weights being relatively insignificant. Thus the effective sample size can be much smaller than the apparent sample size L. The problem is even more severe if none of the samples falls in the regions where $p(z)f(z)$ is large. In that case, the apparent variances of $r_l$ and $rlf(z(l))$ may be small even though the estimate of the expectation may be severely wrong. Hence a major drawback of the importance sampling method is the potential to produce results that are arbitrarily in error and with no diagnostic indication. This also highlights a key requirement for the sampling distribution $q(z)$, namely that it should not be small or zero in regions where $p(z)$ may be significant."
