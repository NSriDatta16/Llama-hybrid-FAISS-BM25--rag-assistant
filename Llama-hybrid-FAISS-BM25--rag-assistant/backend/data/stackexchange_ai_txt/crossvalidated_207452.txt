[site]: crossvalidated
[post_id]: 207452
[parent_id]: 207450
[tags]: 
Local minima are not really as great a problem with neural nets as is often suggested. Some of the local minima are due to the symmetry of the network (i.e. you can permute the hidden neurons and leave the function of the network unchanged. All that is necessary is to find a good local minima, rather than the global minima. As it happens aggressively optimising a very flexible model, such as a neural network, is likely to be a recipe for overfitting the data, so using e.g. simulated annealing to find the global minima of the training criterion is likely to give a neural network with worse generalisation performance than one trained by gradient descent that ends up in a local minima. If these heuristic optimisation methods are used, then I would advise including a regularisation term to limit the complexity of the model. ... or alternatively use e.g. a kernel method or a radial basis function model, which is likely to be less trouble.
