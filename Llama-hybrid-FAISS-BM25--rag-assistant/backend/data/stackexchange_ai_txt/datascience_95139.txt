[site]: datascience
[post_id]: 95139
[parent_id]: 95061
[tags]: 
In general, when we discuss this topic, it is around the idea of Stochastic GD, Mini-batch GD, Batch GD . The idea of averaging is to move the gradient towards an average of the Batch. So, "N" refers to the batch_size in general. On aggregation at the Output layer, it is more of a design implementation I believe. Check this example for Keras import tensorflow as tf, numpy as np from tensorflow.python.keras.layers import Input, Dense from tensorflow.python.keras.models import Model, Sequential import keras.backend as K model = Sequential() model.add(Dense(3, activation="relu", input_shape=(2,), kernel_initializer=tf.keras.initializers.Ones())) model.add(Dense(2, activation="relu", kernel_initializer=tf.keras.initializers.Ones())) x = tf.constant([[2.,2.],[4.,4.],[5.,5.]]) y =tf.constant([[10.,12.], [20.,20.], [25.,25.]]) # This is Keras standard definition i.e. reduce to mean on last axis def mse_loss(y_true, y_pred): return K.mean(K.square(y_pred - y_true), axis=-1) model.compile(loss=mse_loss) from keras.callbacks import LambdaCallback def batchOutput(batch, logs): print("Finished batch: " + str(batch), end=''); print("--->",logs) batchLogCallback = LambdaCallback(on_batch_end=batchOutput) model.fit(x,y, batch_size=1, epochs=1,callbacks=[batchLogCallback], verbose=0, shuffle=False) Finished batch: 0---> {'loss': 2.0} Finished batch: 1---> {'loss': 8.509464263916016} Finished batch: 2---> {'loss': 12.874275207519531} You can manually calculate the loss as the input and the weights have been taken as simple integers. The final loss [4, 0] is averaged to 2. def mse_loss(y_true, y_pred): return K.sum(K.square(y_pred - y_true), axis=-1) If you use the above function output will be, Finished batch: 0---> {'loss': 4.0} Finished batch: 1---> {'loss': 17.01892852783203} Finished batch: 2---> {'loss': 25.748550415039062}
