[site]: datascience
[post_id]: 107333
[parent_id]: 107208
[tags]: 
There have been success modeling card games with reinforcement learning (RL) using Deep Q-learning Network (DQN) with experience replay. Experience replay buffer is a large collection of tuples: (state (s), action (a), reward (r), next state (sâ€²). An RL agent can learn which combinations lead to the highest reward. The representations are stored in a deep learning network which can learn very large state-space representations. This deep learning representation can replace an explicit card count. One option is to adapt an OpenAI environment. There is already an OpenAI environment for Blackjack . After your card game is encoded in an environment, an agent can be trained with a DQN. A good starting point is Stable Baselines3 (SB3) , a set of reliable implementations of reinforcement learning algorithms in PyTorch, which as a DQN implementation .
