[site]: crossvalidated
[post_id]: 99921
[parent_id]: 99914
[tags]: 
First of all, I find "accuracy" sometimes a bit misleading, as it refers to distinct things: The term accuracy in general for evaluating systems or methods (I'm analytical chemist) refers to the bias of predictions, i.e. it answers the question how good predictions are on average. As you know, there are lots of different performance measures that answer different aspects of performance for classifiers. One of them happens to be called accuracy as well. If your paper is not for a machine learning/classification audience, I'recommend to make this distinction very clear. Even for this more specific meaning of accuracy I'd be very explicit of what I call accuracy as again several ways of dealing with class imbalance may occur. Typically, class imbalance is ignored, leading to the well-known $\frac{TP+TN}{all~cases}$ calculation. However, you may also use the average of sensitivity and specificity, which amounts to controlling class imbalance by weighting your average. The F $_1$ -score is often introduced as harmonic mean of precision and recall (or positive predictive value and sensitivity). For your question I think it helpful to spell this out a bit further and simplify it: $F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall} = \frac{2 \frac{TP}{all~P} \frac{TP}{all T}}{\frac{TP}{all~P} + \frac{TP}{all T}} = \frac{2 \frac{TP^2}{all~P \cdot all T}}{\frac{TP \cdot all~T}{all~P \cdot all T} + \frac{TP \cdot all~P}{all~P \cdot all T}} = \frac{2~TP^2}{TP \cdot all~T + TP \cdot all~P} = \frac{2~TP}{all~T + all~P} $ The last expression is not a fraction of anything that I can think of as a certain group of test cases. In particular, a (heavy) overlap between the TRUE and the POSITIVE cases is expected. This would keep me from expressing an F-score as percentage as that kind of implies a proportion of cases. Actually, I think I'd warn the reader that F-score does not have such an interpretation. What makes me particularly uneasy combining these measure is that when we look at sensitivity = precision and specificity on the one hand, and positive and negative predictive values (PPV = recall) at the other hand, they are conditional probabilities, where sensitivity/precision and specificity are conditioned on the true class membership and the predictive values and recall are conditioned on the predicted class membership. To get the latter from the former, we can use Bayes rule and the prior probabilities of the classes. So F-score(s) average (by harmonic mean, but my point would stay the same with arithmetic or geometric mean) together figures of merit that are so-to-speak before and after the Bayes rule. I.e., we produce a metric that is neither completely here nor there.
