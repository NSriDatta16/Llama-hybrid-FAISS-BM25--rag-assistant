[site]: datascience
[post_id]: 124014
[parent_id]: 124013
[tags]: 
This type of problem normally is set up as many-to-many, where the input is a piece of time series and the expected output is the same time series shifted one position to the left so that, for each time step, you predict the next value: To understand what "many" means in this context take this into account: an LSTM always receives a sequence as input and generates a sequence as output; however, the designer can decide to "ignore" parts of the output of an LSTM. To ignore part of the LSTM output, you simply don't take it into account in the training loss. For instance, in the many-to-one setup, you ignore all the output timesteps except the last one. The "many" has nothing to do with timeslots or multivariate timeseries. Take into account, nevertheless, that the inputs and outputs at each time step are vectors. You can, of course, have vectors with dimensionality 1 as inputs/outputs, i.e. scalars.
