[site]: crossvalidated
[post_id]: 359909
[parent_id]: 
[tags]: 
Is accuracy an improper scoring rule in a binary classification setting?

I have recently been learning about proper scoring rules for probabilistic classifiers. Several threads on this website have made a point of emphasizing that accuracy is an improper scoring rule and should not be used to evaluate the quality of predictions generated by a probabilistic model such as logistic regression. However, quite a few academic papers I have read have given misclassification loss as an example of a (non-strict) proper scoring rule in a binary classification setting. The clearest explanation I could find was in this paper , at bottom of page 7. To the best of my understanding, minimizing misclassification loss is equivalent to maximizing accuracy, and the equations in the paper make sense intuitively. For example: using the notation of the paper, if the true conditional probability (given some feature vector x ) of the class of interest is η = 0.7, any forecast q > 0.5 would have an an expected loss R (η| q ) = 0.7(0) + 0.3(1) = 0.3, and any q $\leq$ 0.5 would have an expected loss of 0.7. The loss function would therefore be minimized at q = η = 0.7 and consequently proper; the generalization to the entire range of true conditional probabilities and forecasts seems straightforward enough from there. Assuming the above calculations and statements are correct, the drawbacks of a non-unique minimum and all predictions above 0.5 sharing the same minimum expected loss are obvious. I still see no reason to use accuracy over the traditional alternatives such as log score, Brier score, etc. However, is it correct to say that accuracy is a proper scoring rule when evaluating probabilistic models in a binary setting, or am I making a mistake - either in my understanding of misclassification loss, or in equating it with accuracy?
