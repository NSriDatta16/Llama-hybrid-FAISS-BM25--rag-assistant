[site]: crossvalidated
[post_id]: 617042
[parent_id]: 
[tags]: 
Intuition for chain rule in Bayesian approach for prediction?

I am having trouble understanding the following steps in the Bayesian approach to predicting the probability of tossing a head H given some data D \begin{aligned} p(H \mid D) & =\int_0^1 p(H, w \mid D) d w \\ & =\int_0^1 p(H \mid w, D) p(w \mid D) d w \end{aligned} I understand the first step is the sum rule and the second is the chain rule for probability. I am thinking I should be looking at the integrand as $P((H\cap w) \ \mid D)$ but am having trouble visualising this. What would be the probability of heads? [Update] Here is the slide from my lecture hand out on Bayesian approach to prediction. There is also information on Wikipedia
