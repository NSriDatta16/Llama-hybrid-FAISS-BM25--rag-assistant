[site]: crossvalidated
[post_id]: 207646
[parent_id]: 207632
[tags]: 
Are not all (or nearly) all approaches in some sense trying to deduce something that generalizes and thus predicts what will happen? There is not so much of a distinction in this respect and it is not an entirely different goal. The sentence In these situations, it is not always necessary to think about samples and populations, or to think about a model that expresses a scientific idea. seems wrong. These things are still very important. However, I would agree that the emphasis in these areas is more on prediction rather than e.g. hypothesis testing (which you might say was about proving/disproving scientific ideas). The bit about finding an algorithm (equation less often) that makes reasonably good predictions is a key bit. Predictions are not always evaluated on different data (see cross-validation). It may also be worthwhile to mention that some different terminology (e.g. "learning" instead of "fitting") has developed in these areas due to their historical origin, but that many of the same ideas and issues apply. In fact, not commenting on some of the issues in prediction would seem like a major omission. E.g. in our first Phase 2 trial in 20 patients our drug has a huge efficacy, can we expect the same efficacy in Phase 3? Or we test 10 doses in a dose finding study and simply pick the best one in terms of the point estimate, should we expect to see the same efficacy in Phase 3? The overall trial failed to show that the drug works, but we looked at 20 subgroups and decided that in one of them the drug works. How likely is it that a new trial would show this? These questions involve many of the same issues as one gets in machine learning - the more na√Øve things I describe above (which are sort of cases of over-fitting on your training data) are avoided to some extent by the more reliable machine learning approaches.
