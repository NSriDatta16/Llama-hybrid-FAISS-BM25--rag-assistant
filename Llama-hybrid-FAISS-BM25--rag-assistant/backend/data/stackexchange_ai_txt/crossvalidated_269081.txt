[site]: crossvalidated
[post_id]: 269081
[parent_id]: 
[tags]: 
Distinguish 2-way and 3-way interactions in binary data, independent of reference category

$\newcommand{\P}{\mathbb{P}}$I have three variables $X_a, X_b, X_c \in \{0, 1\}$ and I define a joint distribution: $$ \P(X_a, X_b, X_c) \propto \exp \left \{ \theta_{abc} \mathbb{I}_{\{X_a = 0 \wedge X_b = 0 \wedge X_c = 0\}} \right \} $$ from which we get the following conditional probabilities of $X_a$ $$ \P(X_a = 1 | X_b, X_c) \propto \exp \left \{ \theta_{bc} \mathbb{I}_{\{X_b = 0 \wedge X_c = 0\}} \right \} $$ and $$ \P(X_a = 2 | X_b, X_c) \propto \exp \left \{ 0 \right \}. $$ where $\theta_{bc} = 1$ and $\mathbb{I}_{\{X_b = 0 \wedge X_c = 0\}}$ is the indicator function for the event $\{X_b = 0 \wedge X_c = 0\}$. So the distribution is only parameterized by a three-way interaction between the variables $\{X_a, X_b, X_c\}$. My goal is to recover this 3-way interaction with logistic regression. However, as I show in the below example, this turns our to be problematic, because depending on the choice of reference categories for the predictors $X_b$ and $X_c$, the 3-way interaction is either represented in the intercept or (as desired) in a parameter for the 3-way interaction. Here is a reproducible example: # Sample Data using Gibbs Sampling set.seed(1) n Sanity check: > table(data[,1], data[,2], data[,3]) , , = 0 0 1 0 555 179 1 210 218 , , = 1 0 1 0 204 210 1 222 202 As desired, the configuration $\{0, 0, 0\}$ has a higher relative frequency than all other configurations, which have (roughly) the same relative frequency. Now we fit a logistic regression to model $$ \log \frac{\P(X_a = 1 | X_b, X_c)}{\P(X_a = 0 | X_b, X_c)} $$ We do this using glm() , which takes the smaller category (0) as 'reference' category and hence uses the indicator function for the higher category (1) as a predictor. We get: mod Now what should this model predict? For the case $X_b = 0, X_c = 0$ it should predict $\P(X_a = 0 | X_b, X_c) > \P(X_a = 1 | X_b, X_c)$ or equivalently $$\log \frac{\P(X_a = 1 | X_b, X_c)}{\P(X_a = 0 | X_b, X_c)} And the model does that: For $X_b = 0, X_c = 0$ we get: Xb And for $X_b = 0, X_c = 1$ we get: Xb So far so good. My problem now is that I cannot identify the presence of 2-way and/or 3-way interactions of the model parameters. Basically the 3-way interaction is absorbed in the intercept, and the other parameters are chosen such that they predict smaller equal probabilities for all other configurations. If I used the indicator function for the event 0 instead of the event 1 in the logistic regression, I would not have this problem: data_rec So I get one parameter capturing the 3-way interaction, and all other parameters are zero (for large $n$). Of course, if the 3-way interaction would give a higher probability for the configuration $\{1, 1, 1\}$ I have again the same problem and actually the reference category of the previous model (1) would have led to interpretable parameters. So the problem I have is that whether I can identify the presence of 2-way or 3-way interactions depends on the dummy coding of my predictors, which is undesirable. I guess this is a problem probably many other people already had so I am wondering whether there are any elegant solutions to this? My final goal is to recover a model of the form of the joint distribution above with $p$ variables that can include 2-way and 3-way interactions between those variables using a series of $p$ logistic regressions.
