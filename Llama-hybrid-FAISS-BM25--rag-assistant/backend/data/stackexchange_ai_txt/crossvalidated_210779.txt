[site]: crossvalidated
[post_id]: 210779
[parent_id]: 210748
[tags]: 
Multiple regression can "handle" multicollinearity; it's just that there will be vary large standard errors for coefficients of multicollinear variables. So you might not need PCA at all. That said, there's also no need to omit categorical variables from the PCA. A categorical variable with k levels is transformed into (k-1) dummy variables, each coded 0/1, as is done silently by statistical linear regression software. These dummy variables can be standardized to 0 mean and unit variance just like the continuous variables for PCA. Whether that's useful for your application is another question, but it can be done. I don't see a problem with separating out the categorical variables as you propose. You could use cross-validation to choose the number of principal components to include. But if you have multicollinearity that includes the categorical variables, then you have not accomplished what you set out to do in the first place. All this leads me to suggest ridge regression for your application. It handles multicollinear variables together like PCA, because it effectively is a principal-components regression with differential weighting of components instead of all-or-none selection. As a result, the coefficients will tend to be more consistent among different samples from the same population than with standard multiple regression, while controlling for overfitting. Whichever approach you choose, however, you need to work with variables that together are linearly related to your outcome variable. If that requires transformations, splines, or whatever then that should be done at the beginning. Note that PCA also involves linear relations among variables; transforming components after PCA would seem to be very hard to describe to others and seems at first thought to be prone to problems.
