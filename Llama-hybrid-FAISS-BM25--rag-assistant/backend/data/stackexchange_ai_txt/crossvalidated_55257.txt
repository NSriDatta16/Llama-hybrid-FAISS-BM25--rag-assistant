[site]: crossvalidated
[post_id]: 55257
[parent_id]: 54849
[tags]: 
From the linked abstract, it appears that "explaining away" is discussing a learning mechanism, a common way that humans reason, not a formal method of logic or probability. It's a human-like way of reasoning that's not formally correct, just as inductive reasoning is not formally correct (as opposed to deductive reasoning). So I think the formal logic and probability answers are very good, but not applicable. (Note that the abstract is in a Machine Intelligence context.) Your giants example is very good for this. We believe that earthquakes or giants can cause the ground to shake. But we also believe that giants do not exist -- or are extremely unlikely to exist. The ground shakes. We will not investigate whether a giant is walking around, but rather we'll inquire as to whether an earthquake happened. Hearing that an earthquake did in fact happen, we are even more convinced that earthquakes are an adequate explanation of shaking ground and that giants are even more certain not to exist or are at least even more highly unlikely to exist. We would only accept that a giant caused the ground to shake only if: 1) we actually witnessed the giant and were willing to believe that we were not being fooled and that our previous assumption that giants were highly-unlikely or impossible was wrong, or 2) we could totally eliminate the possibility of an earthquake and also eliminate all possibilities D, E, F, G, ... that we previously had not thought of but that now seem more likely than a giant. In the giant case, it makes sense. This learning mechanism (an explanation we find likely becomes even more likely and causes other explanations to become less likely, each time that explanation works) is reasonable in general, but will burn us, too. For example, the ideas that the earth orbits the sun, or that ulcers are caused by bacteria had a hard time gaining traction because of "explaining away", which in this case we'd call confirmation bias. The fact that the abstract is in a Machine Intelligence setting also makes me thing this is discussing a learning mechanism commonly used by humans (and other animals, I imagine) that could benefit learning systems even though it can also be highly flawed. The AI community tried formal systems for years without getting closer to human-like intelligence and I believe that pragmatics has won out over formalism and "explaining away" is something that we do and thus that AI needs to do.
