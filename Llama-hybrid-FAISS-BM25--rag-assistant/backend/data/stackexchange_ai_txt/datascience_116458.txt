[site]: datascience
[post_id]: 116458
[parent_id]: 37673
[tags]: 
(,) Q ( s , a ) returns a value of entire total reward that's expected ultimately, after we pick an action a . () V ( s ) is the same, just with an extra reward from current state s as well. I don't see why a value of − Q − V would be useful. On the other hand, − V − Q would be useful because it would tell us the reward we would get on +1 s t+ 1 if we took the action a . I think OP has a bit of misunderstanding here. Neil has given a very accurate answer, and I will just try to augment it with some math-less explanation, in case any one else has the same query. We know that every state has a Value attached to it. We further know that every action we take at a state would have a certain value attached to it (which we call the Q value). So we have state-value V(s) and state-action value Q(s,a). Given a policy Π(a|s), the Value of a state can be computed by summing ( over all possible actions for that state ) the probability of taking an action multiplied with the Q-value of that action. So if a state has just 2 actions with equal probability, then the state value is just the average of the 2 Q-values associated with each action. (,) Q ( s , a ) returns a value of entire total reward that's expected ultimately, after we pick an action a . () V ( s ) is the same, just with an extra reward from current state s as well. I think this is where the confusion on the -ve advantage creeps in. As outlined earlier, V(s) is not Q(s,a) + reward from current state. I had a great deal of confusion w.r.t the Rt+1 nomenclature outlined in Sutton and most other books (though a few have used Rt). Coming back to OPs question, we now see that V(s) need not be greater than Q(s,a). Maybe it is more for certain actions emanating from that state. Maybe it is less for certain actions emanating from that state. Now let us talk of the advantage function and why that nomenclature is critical. We know that the calculation of state value V(s) takes into account all possible actions and their respective q-values. Say this value is 100. Let us say there are 5 actions emanating from this state. a1 & a4 have q-values in excess of 100 and a2,a3,a5 have q values lesser than 100. So we are at an advantage if we take actions a1 & a4 and the quantum of the advantage is given by the difference between the q-value for that action and V(s). If we want to pick optimal actions, it makes sense to calculate the advantage each action has and pick the ones having advantage > 0. This simply gives the advantage this particular action has over the default policy. Hence Q(s,a)-V(s) is a good definition of the advantage function.
