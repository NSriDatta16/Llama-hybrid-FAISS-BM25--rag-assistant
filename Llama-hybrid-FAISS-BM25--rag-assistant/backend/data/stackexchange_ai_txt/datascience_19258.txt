[site]: datascience
[post_id]: 19258
[parent_id]: 19254
[tags]: 
I think that before anything else it's important to explore the data to identify features that aren't bringing anything worthwhile (v. low variance, duplicates, collinearity) and to see if you've got any issues with missing values/outliers in your numeric data. With that said, I think you'd want to keep your numeric variables as such (initially at least), but how many do you have? You may wish to normalise them so that the scale of one variable doesn't dominate (eg. is income included?). Or if you have time, try both? Look at the distribution and see what bins might be appropriate to discriminate by: https://en.wikipedia.org/wiki/Normalization_(statistics) Ordinal data is tricky; in your case you can't have 1.5 beds, but that's what you might end up with if you treat them as numeric. I would initially run with one hot encoding, and when you get some results you can look at the impact of your features to decide if you want to keep them that way: https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science Without knowing the full range of your grade values or how granular/continuous your prediction needs to be, you could easily do either - I'd say think about what the ideal outcome would be and build towards that. If you've got time try both, try different bins for a multi-class classifier; but if you have a massive range of target outputs then regression is probably preferable. Finally - start with something simple and interpretable, and build from there. If you run with classification, can a decision tree deal with your dataset, and how does it do/where does it fall down? You might get lucky, but if not you can try an SVM or random forest. Given the number of variables you might want to consider some dimensionality reduction first; you might get some good results from clustering. Hope this helps!
