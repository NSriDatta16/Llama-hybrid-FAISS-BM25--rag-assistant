[site]: crossvalidated
[post_id]: 12902
[parent_id]: 
[tags]: 
Comparison of time series sets

I have three sets of time-series data I am looking to compare. They have been taken on 3 separate periods of about 12 days. They are the average, maximum and minimum of head counts taken in a college library during finals weeks. I have had to do mean, max and min because the hourly head counts were not continuous (see Regular data gaps in a time series ). Now the data set looks like this. There is one data point (average, max or min) per evening, for 12 evenings. There are 3 semesters the data was taken for, in only the 12-day periods of concern. So for example, Spring 2010, Fall 2010, and May 2011 each have a set of the 12 points. Here's an example chart: I have overlaid the semesters because I want to see how the patterns change from semester to semester. However, as I have been told in the linked thread , it's not a good idea to slap the semesters tail-to-head since there is no data in between. The question is then: What mathematical technique can I use to compare the pattern of attendance for each semester? Is there anything special to time-series that I must do, or can I simply take the percent differences? My goal is to say that library usage over these days is going up or down; I am just not sure what technique(s) I should use to show it.
