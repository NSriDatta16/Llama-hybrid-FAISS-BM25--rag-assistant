[site]: datascience
[post_id]: 25540
[parent_id]: 25531
[tags]: 
If you feed the output of the LSTM directly into a softmax, you probably won't get good results. If you use a softmax layer after a tanh layer, bad stuff happens. As you say, the confidence will never get near 100%. For instance, if there are two classes, you can never get above about 88% confidence. If there are $k$ classes, you can never get confidence above $e/(e + (k-1)/e)) = e^2/(e^2 + k-1)$. So, rather than directly feeding the output of the LSTM directly into softmax, you can instead use the output of the LSTM as the input to one or more (fully-connected) layers of neural network.
