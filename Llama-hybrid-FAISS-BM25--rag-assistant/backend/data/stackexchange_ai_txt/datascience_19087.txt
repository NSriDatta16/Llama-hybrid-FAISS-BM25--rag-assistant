[site]: datascience
[post_id]: 19087
[parent_id]: 
[tags]: 
Classification with frequency feature vector produces poor results

I'm classifying data into 2 classes, by Logistic Regression from python scikit-learn. I'm trying different types of feature vectors: Binary Frequency TF-IDF In Binary and TF-IDF feature vectors, I get great results. However, in Frequency feature vectors, I get poor results unless I multiply the values by some factor (for example 10, or 100). The multiplication improves the results significantly (the higher the factor, the better). Is this a normal behavior or is it more likely that I'm doing something wrong? I suspected that the values are too small and my array rounded some of them down to zero because its type was merely float (unspecified bits number). But I tried changing it to np.float64 , and the results didn't change. What other factors can be causing this? Edit: Learning Curves: Binary | TF-IDF | Frequency X 10 (from left to right): Normal frequency (3 attempts): Edit 2: (data and code) Frequency Data: X axis: a dictionary of 1K words , Y axis: 1K samples - 500 each class Labels (1K binary labels): note that it's a transpose vector. so X here is the samples Code for frequency: #loop... vec[i] = np.divide(sample_vals, len(sample.split())) Code for frequency X 10: #loop... vec[i] = np.multiply(np.divide(sample_vals, len(sample.split())), 10) Classification code: #in `X` I have the data, and in `y` I have the labels scores = [clf.fit(X[train], y[train]).score(X[test], y[test]) for train, test in kfold.split(X)]
