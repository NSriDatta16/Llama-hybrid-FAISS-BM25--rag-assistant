[site]: crossvalidated
[post_id]: 577845
[parent_id]: 
[tags]: 
How to choose a parameter grid for a model's hyperparameter tuning?

How do people decide on the ranges for hyperparameters to tune? For example, I am tuning an xgboost model, I've been following a guide on kaggle to set the ranges of each hyperparameter to then do a bayesian optimisation gridsearch. E.g. this guide lists the typical values for max_depth of xgboost as 3-10 - how is this range decided on as typical? I chose to read about the hyperparameter and set: xgb_parameters = { 'max_depth': (1, 4), ... } I chose 1-4 as I read a large depth is more computationally expensive and an overfitting risk, but I have no reason for choosing 4 to be the end of my range specifically. Is there a resource or paper I should be referring to to understand the ranges of hyperparameters for any models I'm interested in? Or is it generally set by trial and error depending on your prediction problem? Or am I worrying too much about needing exact reasoning for ranges and as long as I have a reason for my general range its acceptable?
