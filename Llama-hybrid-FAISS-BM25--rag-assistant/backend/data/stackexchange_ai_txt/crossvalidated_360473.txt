[site]: crossvalidated
[post_id]: 360473
[parent_id]: 314567
[tags]: 
When working on "feature importance" generally it is helpful to remember that in most cases a regularisation approach is often a good alternative. It will automatically "select the most important features" for the problem at hand. Now, if we do not want to follow the notion for regularisation (usually within the context of regression), random forest classifiers and the notion of permutation tests naturally lend a solution to feature importance of group of variables. This has actually been asked before here: " Relative importance of a set of predictors in a random forests classification in R " a few years back. More rigorous approaches like Gregorutti et al.'s : " Grouped variable importance with random forests and application to multivariate functional data analysis ". Chakraborty & Pal's Selecting Useful Groups of Features in a Connectionist Framework looks into this task within the context of an Multi-Layer Perceptron. Going back to the Gregorutti et al. paper their methodology is directly applicable to any kind of classification/regression algorithm. In short, we use a randomly permuted version in each out-of-bags sample that is used during training. Having stated the above, while permutation tests are ultimately a heuristic, what has been solved accurately in the past is the penalisation of dummy variables within the context of regularised regression. The answer to that question is Group-LASSO , Group-LARS and Group-Garotte . Seminal papers in that work are Yuan and Lin's: " Model selection and estimation in regression with grouped variables " (2006) and Meier et al.'s: " The group lasso for logistic regression " (2008). This methodology allows us to work in situation where: " each factor may have several levels and can be expressed through a group of dummy variables " (Y&L 2006). The effect is such that " the group lasso encourages sparsity at the factor level. " (Y&L 2006). Without going to excessive details the basic idea is that the standard $l_1$ penalty is replaced by the norm of positive definite matrices $K_{j}$, $j = \{1, \dots, J\}$ where $J$ is the number of groups we examine. CV has a few good threads regarding Group-Lasso here , here and here if you want to pursue this further. [Because we mention Python specifically: I have not used the Python's pyglmnet package but it appears to include grouped lasso regularisation .] All in all, in does not make sense to simply "add up" variable importance from individual dummy variables because it would not capture association between them as well as lead to potentially meaningless results. That said, both group-penalised methods as well as permutation variable importance methods give a coherent and (especially in the case of permutation importance procedures) generally applicable framework to do so. Finally to state the obvious: do not bin continuous data . It is bad practice, there is an excellent thread on this matter here (and here ). The fact that we observe spurious results after the discretization of continuous variable, like age , is not surprising. Frank Harrell has also written extensivel on problems caused by categorizing continuous variables .
