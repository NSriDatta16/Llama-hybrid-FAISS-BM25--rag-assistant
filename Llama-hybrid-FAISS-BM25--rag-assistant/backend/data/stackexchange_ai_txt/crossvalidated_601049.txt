[site]: crossvalidated
[post_id]: 601049
[parent_id]: 
[tags]: 
Do attention models change the sequences attended to for every novel sentence? Or do they have a general idea of where to look?

I've learned about many different types of attention models. But the one thing that I never understood was how they knew where to focus on/attend to in the sentence. What I'm wondering is if they can pick out the necessary information even for a novel sentence (that is, something not from the training set). Or if they simply learn to focus on general parts of a sentence, which may or may not correspond to the words that need to be attended to.
