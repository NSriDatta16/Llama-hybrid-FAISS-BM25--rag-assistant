[site]: crossvalidated
[post_id]: 309082
[parent_id]: 
[tags]: 
Risks of disproportionate numbers of training examples for different classes?

fairly new to machine learning so please be gentle! I was wondering what the risks might be if I have significantly more examples for one class than the other when training a 2-D perceptron; for example, if I have 500 examples of C1 versus 1000 examples of C2 , assuming that they are drawn from the same random distribution. In other words, if I have significantly more examples of one class than the other, even though they're from the same distribution and have equal probability of being drawn, doesn't this break the assumption that they're identically drawn and distributed? Are there any other risks I should be aware of?
