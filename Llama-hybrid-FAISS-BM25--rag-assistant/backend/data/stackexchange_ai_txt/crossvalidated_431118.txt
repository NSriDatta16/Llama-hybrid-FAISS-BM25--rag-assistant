[site]: crossvalidated
[post_id]: 431118
[parent_id]: 347265
[tags]: 
The https://xgboost.readthedocs.io/en/latest/tutorials/model.html [documentation of XGBoost][1] provides a great introduction to the boosted trees algorithm. Perhaps, I can help out answering your first question : I think you state two false alternatives. The algorithm uses the entire training set (leaving aside the fact that you can sample observations in every iteration). Plus, every tree is fitted on the residuals of the previous trees. Concerning the latter: In the gradient descent method, we try to reach the global minimum of an objective function (loss function) by walking through the solution space along the gradient. You can consider every added tree, a further step towads the greatest descending slope direction. Since the gradient of the loss functions logloss or mean square error happen to be the difference between prediction and observation , you can interpret every tree to be fitted to the previous residuals. However, I'm not sure if you can use this interpretation for every loss function (beside from logloss and mean square error).
