[site]: crossvalidated
[post_id]: 551126
[parent_id]: 421935
[tags]: 
This is an add up of what is K and V and why the author use different parameter to represent K and V. Short answer is technically K and V can be different and there is a case where people use different values for K and V. K and V can be different! Example Offered! What are K and V? Are they the same? The short answer is that they can be the same, but technically they do not need to be the same. Briefly introduce K, V, Q but highly recommend the previous answers : In the Attention is all you need paper, this Q, K, V are first introduced. In that paper, generally(which means not self attention), the Q is the decoder embedding vector(the side we want), K is the encoder embedding vector(the side we are given), V is also the encoder embedding vector. And this attention mechanism is all about trying to find the relationship(weights) between the Q with all those Ks, then we can use these weights(freshly computed for each Q) to compute a new vector using Vs(which should related with Ks). If this is self attention: Q, V, K can even come from the same side -- eg. compute the relationship among the features in the encoding side between each other.(Why not show strong relation between itself? Projection.) Case where they are the same : here in the Attention is all you need paper, they are the same before projection. Also in this transformer code tutorial , V and K is also the same before projection. Case where K and V is not the same : In the paper End-to-End Object Detection Appendix A.1 Single head(this part is an introduction for multi head attention, you do not have to read the paper to figure out what this is about), they offer an intro to multi-head attention that is used in the Attention is All You Need papar, here they add some positional info to the K but not to the V in equation (7), which makes the K and the V here are not the same. Hope this helps. Edit: As recommended by @alelom, I put my very shallow and informal understand of K, Q, V here. For me, informally, the Key, Value and Query are all features/embeddings . Though it actually depends on the implementation but commonly, Query is feature/embedding from the output side(eg. target language in translation). Key is feature/embedding from the input side(eg. source language in translation), and for Value , basing on what I read by far, it should certainly relate to / be derived from Key since the parameter in front of it is computed basing on relationship between K and Q, but it can be a feature that is based on K but being added some external information or being removed some information from the source(like some feature that is special for source but not helpful for the target)... What I have read(very limited, and I cannot recall the complete list since it is already a year ago, but all these are the ones that I found helpful and impressive, and basically it is just a summary of what I referred above...): Neural Machine Translation By Jointly Learning To Align And Translate. Attention Is All You Need. and a tensorflow tutorial of transformer: https://www.tensorflow.org/text/tutorials/nmt_with_attention . End-to-end object detection with Transformers, and its code: https://github.com/facebookresearch/detr . lil'log: https://lilianweng.github.io/posts/2018-06-24-attention/
