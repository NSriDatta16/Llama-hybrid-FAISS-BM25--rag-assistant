[site]: crossvalidated
[post_id]: 349096
[parent_id]: 
[tags]: 
Cross-entropy for comparing images

Suppose we have two greyscale images which are flattened to 1d arrays: $y=(y_1, y_2, \ldots, y_n)$ and $\hat{y} = (\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_n)$ with pixel values in $[0,1]$. How exactly do we use cross-entropy to compare these images? The definition of cross entropy leads me to believe that we should compute $$-\sum_{i} y_i \log \hat{y}_i,$$ but in the machine learning context I usually see loss functions using "binary" cross entropy, which I believe is $$ -\sum_i y_i \log \hat{y}_i - \sum_i (1-y_i) \log (1-\hat{y}_i).$$ Can someone please clarify this for me?
