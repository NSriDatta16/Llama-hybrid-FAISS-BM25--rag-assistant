[site]: crossvalidated
[post_id]: 286255
[parent_id]: 274720
[tags]: 
A 1 hot word vector in a large dictionary or a co-occurrence matrix in a large corpus are both usually very sparse matrices. This is a problem because a one hot vector, [0 0 1 0 .. 0] vector cannot compute relations to another one hot vector, i.e. dot product. So this would cause two words such as hotel and motel to have no mathematical relations with one another if all of the word vectors were one hot vectors. A co-occurrence matrix will solve this problem of relations, because hotel and motel will have similar co-occurrence results in a large corpus. But this matrix is too large in size, and computationally is inefficient to store in memory. For a corpus with a dictionary of 100,000 unique tokens, this is a (100,000 x 100,000) matrix which is 10 billion numbers. Using something like word2vec or glove, you solve both of these problems. Word2vec uses a feed forward neural network with a single hidden layer. We will end up using the weights from the hidden layer as the word vector representations. The length of vectors is a hyperparameter, (100, 300, etc.). This way words such as hotel and motel can do vector operations to find the similarity or closeness to another vector. Also, this reduces the size of each word vector from 100,000 from the example above to 300 or 100, depending on the dimension of the word vector set.
