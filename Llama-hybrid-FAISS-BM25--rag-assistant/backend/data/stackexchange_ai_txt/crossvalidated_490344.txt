[site]: crossvalidated
[post_id]: 490344
[parent_id]: 
[tags]: 
Linear model with partially time-varying coefficients

Suppose we have a linear model with time-varying coefficients $$ y_i = x_i' \beta_{t_i} + \epsilon_i, \; i = 1, 2, \cdots, n $$ where the design points are $t_i = \frac{i}{n}$ , and $\beta(t): [0,1] \rightarrow \mathbb{R}^p$ is a component-wise Lipshitz function on $[0,1]$ . A standard way to estimate $\beta$ is by local kernel regression of Nadaraya-Watson type, a simplified version of which goes as follows (see, for example, Cai (2007) Journal of Econometrics ). Let $K(s)$ be a symmetric continuous kernel function and $K_{h, t} (s) = \frac{1}{h} K(\frac{s-t}{h})$ . Local kernel regression means that, at a given $t \in [0,1]$ , one estimates $\beta(t)$ by fitting the working linear model $$ K_{h, t} (t_i) y_i = K_{h, t} (t_i) x_i' \beta(t) + u_i, \; i = 1, 2, \cdots, n. \quad(*) $$ With weakly dependent data $(x_i, y_i)$ , for a given bandwidth $h$ it can be shown that $$ \hat{\beta}(t) - \beta(t) = O_p(h) + O_p(\frac{1}{\sqrt{nh}}), $$ a "bias + variance" type of expression. Therefore the optimal bandwidth is $O(n^{-\frac13})$ and with this optimal bandwidth selection, $\hat{\beta}(t)$ converges to $\beta(t)$ at $n^{\frac13}$ -rate pointwise and in $L^2[0,1]$ . ( Cai (2007) obtained $n^{\frac25}$ -rate under the assumption that $\beta$ is twice-differentiable.) Question Suppose some of the components of $\beta$ are time-invariant, say $\beta = (\beta_1', \beta_2'(t))'$ and $\beta_1$ is a constant vector of parameters. Now I am going to make the following proposition, which is fairly naive and whose conclusion must be wrong. Since $\beta_1$ is time-invariant, it can be estimated consistently by fitting the working model $(*)$ at multiple points, say $m$ points, then average the $m$ resulting estimates. If $m$ is constant, the resulting averaged estimate would have variance reduced by factor of $\frac{1}{m^2}$ but rate of convergence to true parameter is still $n^{\frac13}$ . Now let $m$ grow with $n$ , say $m = n^{\alpha}$ , then the resulting rate is $n^{ 2 \alpha + \frac13 }$ . In particular, if $\alpha > \frac{1}{12}$ , then the resulting rate is faster than $n^{\frac12}$ . However, this cannot be right , as $n^{\frac12}$ is the optimal parametric rate (there is no unit-root data involved). (The parametric $n^{\frac12}$ -rate has been obtained for the time-invariant parameters in this setting by Zhang and Wu (2012) Annals of Statistics , under more restrictive conditions.) What's wrong with the above argument, and what would be the correct rate from the above procedure? (One possible reason is joint vs. separate convergence---one must choose $h$ and $m$ simultaneously, not take the limit in $h$ and then $m$ . This is not very precise. Further clarification would be nice.)
