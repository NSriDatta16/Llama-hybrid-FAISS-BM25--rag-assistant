[site]: crossvalidated
[post_id]: 588972
[parent_id]: 588876
[tags]: 
For part 1, yes you should exclude one of the dummies per group. To be more specific, for every set of dummy variables that split the data into mutually exclusive and collectively exhaustive categories, you should omit one of the dummy variables to avoid the “dummy variable trap”. The “dummy variable trap” refers to the perfect multicolinearity that arises when one of your independent variables can be perfectly predicted by the other variables in your regression. For example, if $j1$ and $j2$ are both equal to zero, then you already know for a fact that $j3$ will be equal to 1. More formally, take your first example regression equation. $$y = \beta_1 n_1 + \beta_2 n_2 + \beta_3 i_1 + \beta_4 i_2 + \beta_5 i_3 + \beta_6 j_1 + \beta_7 j_2 + \beta_8 j_3 + \beta_9 + \epsilon$$ (Note that I’ve added on an $\epsilon$ and a constant term $\beta_9$ .) Note also that by the definition of a dummy variable, since for any individual observation, only one of the $j$ variables can be equal to 1, you can write $$j_3 = 1 - j_2 - j_1$$ Therefore, $j_3$ is completely determined by the values of $j_1$ and $j_2$ . Now, substitute this back into the original equation: $$y = \beta_1 n_1 + \beta_2 n_2 + \beta_3 i_1 + \beta_4 i_2 + \beta_5 i_3 + \beta_6 j_1 + \beta_7 j_2 + \beta_8 (1-j_2-j_1) + \beta_9 + \epsilon$$ After some algebra, you could rewrite the equation as: $$y = \beta_1 n_1 + \beta_2 n_2 + \beta_3 i_1 + \beta_4 i_2 + \beta_5 i_3 + (\beta_6-\beta_8) j_1 + ( \beta_7-\beta_8) j_2 + (\beta_9+\beta_8)+ \epsilon$$ Thus, you can rewrite the whole regression equation without $j_3$ . So instead of including all of them, which will make your estimates of $\beta_6$ , $\beta_7$ , and $\beta_9$ incorrect. Instead, you omit the group and interpret the coefficients on $j_1$ and $j_2$ as the changes in the outcome relative to the omitted group . So in your example, if $j_3$ is omitted, then $\beta_6$ represents the average increase in price that is associated with a house being blue, rather than “other”. Note that this interpretation would not be possible if you included $j_3$ as a regressor. In that case, you would be comparing blue houses to some category of house that doesn’t exist, and so your coefficients would be essentially meaningless. This same logic applies to the $i$ variables. As for part 2, the $i$ and $j$ variables are distinct sets of dummy variables in the example you’ve given. That’s because the color of the house should not be perfectly correlated with its location (unless you have a really unusual dataset). So you can’t uniquely determine the location of a house from its color, and vice versa. In general, a “set of dummy variables” should represent a set of categories that are both: Mutually exclusive : no more than one of the variables can ever be equal to one for any observation Collectively exhaustive : one of the variables is equal to one for every observation So the color example is perfect. A color can be either blue, red, or “other”. But it can’t be both blue and red, or blue and other. Thus, these categories are mutually exclusive. Further, since “other” captures everything that is not blue or red, every house must fall into one of these categories. So, the $j$ variables are also collectively exhaustive, and so one of them should be omitted. Hope this helps.
