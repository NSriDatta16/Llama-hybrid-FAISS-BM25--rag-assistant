[site]: datascience
[post_id]: 120913
[parent_id]: 
[tags]: 
Benchmarking LLMs on technical questions

There are several existing benchmark sets to evaluate the performance of large language models on natural-language comprehension tasks, such as CoQA, LAMBDA, HELLASWAG, LogiQA. I'm interested in benchmarking large language models on technical tasks such as writing code, where questions might consist of something like 'Write a Python program to print the first ten prime numbers, one per line' and the output would be considered correct if feeding it to the Python interpreter does indeed produce the first ten prime numbers. Are there any existing benchmark sets of that nature yet?
