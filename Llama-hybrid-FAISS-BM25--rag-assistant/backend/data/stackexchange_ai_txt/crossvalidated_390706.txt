[site]: crossvalidated
[post_id]: 390706
[parent_id]: 390674
[tags]: 
We often de-trend the time-series before modeling other aspects of it. Depending on your needs, you can use differencing , i.e. model $y'_t = y_t - y_{t-m}$ , or subtract the trend, e.g. using linear regression $y_t = \beta_0 + T_t + \varepsilon_t$ ( $\beta_0$ is intercept and $T_t = \beta_1 t$ is the parameter for time), next subtract the estimated trend $\tilde{y}_t = y_t - (\beta_0 + T_t) $ and then model the $\tilde{y}_t$ using another algorithm. Notice that second approach is a direct consequence of assuming additive trend, e.g. $y_t = T_t + f(X_t) + \varepsilon_t$ . † When subtracting the trend $T_t$ , it does not have to be linear trend, it can be smooth function (e.g. LOESS ), piecewise-linear , moving average , but also averaging linear trends in overlapping windows, or something else. Beyond simplicity, when modeling de-trended series you also easily visualize the series to gain better insights (what do you see when plotting de-trended series?) or notice problems with the assumed trend (e.g. "jumps" at the borders of windows, when using piece-wise trend components). The drawback of using two-stage approach, is that when estimating trend and other components using a single model, you would be also correcting for changes related to the other components, rather then ignoring them as with estimating the trend independently. I don't see how using the slope parameters as features would have any added effect over de-trending the series. If you did so, your network would still need to learn how to use this information, so why not use model that learns the trend by itself (by adding time as a feature)? † - like in in STL decomposition , $y_t = T_t + S_t + \varepsilon_t$ , where $T_t$ is trend and $S_t$ is seasonality and $\varepsilon_t$ is random noise, or in Prophet, $y_t = T_t + S_t + H_t + \varepsilon_t$ , where $T_t$ is piecewice-liear trend, $S_t$ is seasonality and $H_t$ are holiday effects ( Taylor and Letham, 2017 ).
