[site]: crossvalidated
[post_id]: 210172
[parent_id]: 210157
[tags]: 
This result is indeed normal and this is a classic problem in machine learning. It is called unbalanced problem. Those are problems with the number of samples from one class significantly higher than the number of sample from the other class. Thus what is happening is that the optimization simply assign all to 1's to 0's. At high level it is simple to understand. When your model tries to assign some 1's in the training phase, it immediately incur an error on some of the 0's. This error is higher than assigning all samples to 0's. Now if you compute it, the accuracy is indeed really high almost 100% since you have a strongly unbalanced problem. There are 2 main solutions to this problems that I know of: Training multiple models - each of those will have only a sample of the 0's and all the 1's and then average. The number of 0's and 1's samples should be about the same. Using weight. Weight each samples with the inverse of the number of time that class appear (1/97000 for 0, and 1/2300 for 1's)
