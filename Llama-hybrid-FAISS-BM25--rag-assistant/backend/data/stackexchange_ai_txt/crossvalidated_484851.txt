[site]: crossvalidated
[post_id]: 484851
[parent_id]: 484838
[tags]: 
There are several reasons for using non-informative priors. These include: (a) Actually not having any useful prior information or strong personal opinion upon which to base an informative prior. (b) Trying to show skeptics that a Bayesian approach is reasonable because a non-informative prior gives a result numerically similar to a traditional (frequentist) approach when little or no prior information is provided, while allowing for use of prior information when it exists. (c) A sensitivity analysis (often ad hoc ) to see how much influence a strong prior has had on the results of a Bayesian analysis. (d) Because a Bayesian approach is simpler than a traditional one, so that one wants to use the Bayesian approach even when no useful prior information is available. (e) In a continuing investigation, to get onto a Bayesian track initially without prior information, but hoping to use results of phase $i-1$ of the investigation as prior information for phase $i,$ and so on as the investigation moves iteratively from one phase to the next. As an example of (d), it is now widely recognized that Wald confidence intervals of the type $\hat p \pm 1.96\sqrt{\frac{\hat p(1-\hat p)}{n}}$ have such bad coverage probabilities as to be essentially useless for $n$ smaller than $n=100$ or so. There are several other styles of CIs that are better, some of which are quite messy computationally. However, the Jeffries CI [based on a Bayesian procedure with the non-informative prior distribution $\mathsf{Beta}(.5,.5)]$ works quite well in frequentist situations and is easy to compute in R. Specifically, if there are 43 Successes in 63 binomial trials, then a 95% interval estimate for $p$ is $(0.561,\,0.787)$ . [Compared with Wald $(0.567,\, 0.797)$ and Agresti-Coull $(0.561,\, 0.787)].$ qbeta(c(.025,.975), 43.5, 20.5) [1] 0.5614128 0.7873252 Perhaps an example of (b) is in this Q&A , where a traditional method fails only in occasional difficult cases, whereas a Bayesian method with a Gibbs Sampler provides a common method for all cases. However, you have to be careful about "noninformative" priors because as the situation changes what you imagined to be noninformative, can be more informative than you supposed in a not-so-similar situation. (See the note by @Xi'an.)
