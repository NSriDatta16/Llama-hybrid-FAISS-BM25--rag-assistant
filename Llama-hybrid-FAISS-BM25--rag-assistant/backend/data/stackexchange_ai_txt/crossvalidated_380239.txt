[site]: crossvalidated
[post_id]: 380239
[parent_id]: 319349
[tags]: 
In my opinion, the following might be one -but not the only- reason for the relatively low popularity of Deep Learning in regression problems: Much of the success of modern Neural Networks comes from their ability to exploit the compositional nature of the world . This is, in perception problems such as image or audio analysis, features have an order (spatial or temporal) and local patterns aggregate to form higher level concepts and objects (e.g, a picture of a car is made of wheels and other parts, which are made of lower level visual features, which are made of basic shapes like edges, circles and lines, etc.). Modern Neural Networks such as Convolutional Neural Networks take advantage of this by learning increasingly abstract features in the deeper layers. These ideas are explained in more detail in these slides by Hinton et al. [1] Conversely, classical regression problems consist of a number of non-ordered features, and the target value can be predicted fairly well with a shallow linear/nonlinear model of the input features. In some sense, this compositional property present in problems such as image classification or speech recognition is not present in problems such as " Predict the income of an individual based on their sex, age, nationality, academic degree, family size... ". This might explain why some of the regression problems where Deep Learning is more popular are those based on images (e.g., Age prediction based on face photography). Of course, these are only some intuitive ideas, and a more formal analysis of this problem is certainly a nice research topic. Deep Learning is certainly a field where more theoretical guarantees and insights are needed. [1] https://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf
