[site]: crossvalidated
[post_id]: 566236
[parent_id]: 522501
[tags]: 
Many models output something other than a hard classification. While particular software implementations might make it easy or difficult to access these, you get the full probability of class membership from models such as logistic regressions, neural networks, and random forests. I would argue that, if you do not know the cost of making a mistake, you have no business making a hard classification, and all you should be doing is estimating the probability. Consequently, the goal should be to obtain the best probability predictions you can. Two typical metrics for this go by the names of log-loss and Brier score . Both of these are perfectly compatible with class imbalance, as they will (correctly) use the imbalance to inform their probability predictions. In other words, you need overwhelming evidence of a minority class, because the majority class is so much more likely. In technical terminology, the posterior probability $P(\textrm{category}\vert \textrm{data})$ depends on the prior probability $P(\textrm{category}).$ A machine learning model that outputs a probability value is predicting the posterior probability. Using Bayes' theorem, we can see that this value depends on the prior probability (the class membership proportion) $$ P(\textrm{category}\vert \textrm{data}) = \frac{ P(\textrm{data}\vert \textrm{category}) P(\textrm{category}) }{ P(\textrm{data}) }. $$
