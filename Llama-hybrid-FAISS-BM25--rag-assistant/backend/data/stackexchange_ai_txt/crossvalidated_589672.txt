[site]: crossvalidated
[post_id]: 589672
[parent_id]: 589655
[tags]: 
You can't use cross-entropy loss with a linear model. Notice that it calculates $\log(\hat y)$ and $\log(1 - \hat y)$ , while $\hat y$ can be negative or bigger than one, so logs would be undefined ( NaN in practice) and it simply won't work. For it to work, you would either need to adapt the loss function so that for $\hat y and $\hat y > 1$ it returns something like $\infty$ , but then it's not the elegant cross-entropy loss that we know anymore. Another solution might be to transform $\hat y$ so that it is constrained, for example by truncating it (again, not nice) or using something like a logistic function...
