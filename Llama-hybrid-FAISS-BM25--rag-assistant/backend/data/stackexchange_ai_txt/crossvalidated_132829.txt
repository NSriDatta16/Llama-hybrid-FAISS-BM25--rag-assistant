[site]: crossvalidated
[post_id]: 132829
[parent_id]: 
[tags]: 
Solution to exercice 2.2a.16 of "Robust Statistics: The Approach Based on Influence Functions"

On page 180 of Robust Statistics: The Approach Based on Influence Functions one finds the following question: 16: Show that for location-invariant estimators always $\varepsilon^*\leq\frac{1}{2}$. Find the corresponding upper bound on the finite-sample breakdown point $\varepsilon^*_n$, both in the case where $n$ is odd or $n$ is even. The second part (after the period) is actually trivial (given the first) but I can't find a way to prove the first part (sentence) of the question. In the section of the book pertaining to this question one finds (p98): Definition 2: The finite-sample breakdown point $\varepsilon^*_n$ of an estimator $T_n$ at the sample $(x_l,\ldots, x_n)$ is given by: $$\varepsilon^*_n(T_n;x_i,\ldots,x_n):=\frac{1}{n}\max\{m:\max_{i_1,\ldots,i_m}\sup_{y_1,\ldots,y_m}\;|T_n(z_1,\ldots,z_n)| where the sample $(z_1,\ldots,z_n)$ is obtained by replacing $m$ data points $x_{i_1},\ldots,x_{i_m}$ by arbitrary values $y_1,\ldots,y_m.$ The formal definition of $\varepsilon^*$ itself runs for almost a page, but can be thought of as $$\varepsilon^*=\underset{n\rightarrow\infty}{\lim}\varepsilon^*_n$$ Although not defined explicitly, one can guess that location-invariant means that $T_n$ must satisfy $$T_n(x_1,\ldots,x_n)= T_n(x_1+c,\ldots,x_n+c), \text{ for all } c\in \Bbb{R}$$ I (try to) answer whuber's question in the comment below. The book defines estimator $T_n$ is several pages, starting at p82, I try to reproduce the main parts (I think it will answer whuber's question): Suppose we have one-dimensional observations $(X_1,\ldots,X_n)$ which are independent and identically distributed (i.i.d.). The observations belong to some sample space $\mathcal{H}$, which is a subset of the real line $\mathbb{R}$ (often $\mathcal{H}$ simply equals $\mathbb{R}$ itself, so the observations may take on any value). A parametric model consists of a family of probability distributions $F_\theta$, on the sample space, where the unknown parameter $\theta$ belongs to some parameter space $\Theta$ ... We identify the sample $(X_1,\ldots,X_n)$ with its empirical distribution $G_n$, ignoring the sequence of the observations (as is almost always done). Formally, $G_n$, is given by $(1/n)\sum_{i=1}^n\Delta_{x_i}$ where $\Delta_{X}$, is the point mass 1 in $X$. As estimators of $\theta$, we consider real-valued statistics $T_n=T_n(X_1,\ldots,X_n)=T_n(G_n)$. In a broader sense, an estimator can be viewed as a sequence of statistics $\{T_n,n\geq 1\}$ , one for each possible sample size $n$. Ideally, the observations are i.i.d. according to a member of the parametric model $\{F_\theta;\theta\in\Theta\}$ , but the class $\mathcal{F}(\mathcal{H})$ of all possible probability distributions on $\mathcal{H}$ is much larger. We consider estimators which are functionals [i.e., $T_n(G_n)=T(G_n)$ for all $n$ and $G_n$] or can asymptotically be replaced by functionals. This means that we assume that there exists a functional $T:\mbox{domain}(T)\rightarrow\mathbb{R}$ [where the domain of $T$ is the set of all distributions $\mathcal{F}(\mathcal{H})$ for which $T$ is defined] such that $$T_n(X_1,\ldots,X_n)\underset{n\rightarrow\infty}{\rightarrow}T(G)$$ in probability when the observations are i.i.d. according to the true distribution $G$ in $\mbox{domain}(T)$. We say that $T(G)$ is the asymptotic value of $\{T_n;n\geq 1\}$ at $G$. ... In this chapter, we always assume that the functionals under study are Fisher consistent (Kallianpur and Rao, 1955): $$T(F_\theta)=\theta\;\mbox{ for all } \theta\in\Theta$$ which means that at the model the estimator $\{T_n;n\geq 1\}$ asymptotically measures the right quantity. The notion of Fisher consistency is more suitable and elegant for functionals than the usual consistency or asymptotic unbiasedness.
