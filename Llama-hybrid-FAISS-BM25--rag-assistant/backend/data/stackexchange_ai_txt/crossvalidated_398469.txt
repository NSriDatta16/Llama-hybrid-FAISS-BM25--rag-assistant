[site]: crossvalidated
[post_id]: 398469
[parent_id]: 
[tags]: 
Is KL Divergence loss appropriate for generative model?

I have coordinates data like: x_train = [7.6291 112.74 43.232 96.636 61.033 87.311 91.55 115.28 121.22 136.48 119.52 80.53 172.08 77.987 199.21 94.94 228.03 110.2 117.83 104.26 174.62 103.42 211.92 109.35 204.29 122.91 114.44 125.46 168.69 124.61 194.97 134.78 173.77 141.56 104.26 144.11 125.46 166.99 143.26 185.64 165.3 205.14] Dimension is (1,42), total train samples are over (300000,42). I want to use Variational Autoencoder to train this dataset. In here, I am not sure KL divergence loss is appropriate for like this data. Because, when I train without KL loss (only mse loss), model is fine. But when I train with (KL + mse loss), loss did not decrease and not changing. That is why I want to know KL loss has a limitation on data or something?
