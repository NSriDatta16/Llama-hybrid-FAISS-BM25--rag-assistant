[site]: crossvalidated
[post_id]: 424000
[parent_id]: 
[tags]: 
How does ReLU deal with negative inputs?

I'm replicating this paper for my PhD, which says that they are using deep learning to predict stock returns. So the inputs are (mostly) continuous variables that can be negative and positive. Outputs are stock returns which can be [-1, inf) . The paper is rather vague in their methodology but mentioned that they are using ReLU for their hidden layers and linear activation for output. So far I haven't had any luck getting anywhere near reasonable results which led me to question every parameter in my network. So my question is, how does ReLU work when expected outputs can be negative?
