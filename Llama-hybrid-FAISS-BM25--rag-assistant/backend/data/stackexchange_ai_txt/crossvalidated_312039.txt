[site]: crossvalidated
[post_id]: 312039
[parent_id]: 312033
[tags]: 
I don't know the section of Kruschke you're thinking of, but I suspect the general point, is this. (I'll take some liberties with the math for the sake of intuition). Assume that the complete posterior is $P(\theta_1, \theta_2, \theta_3 \mid X)$ from which you have $N$ samples, arranged as an N x 3 matrix, S . Repeatedly choosing an element randomly from S[,1] , another one randomly from S[,2] , and then subtracting them (which is the sample equivalent of drawing from $P(\theta_1 \mid X) $ and then from $P(\theta_2 \mid X)$ and subtracting them), will generate $N$ new quantities whose distribution will not in general converge to $P(\theta_2 - \theta_1 \mid X)$. However, repeatly sampling a row randomly, say the $k$th, from S[,1:2] and then subtracting S[k,2] from S[k,1] , will generate $N$ quantities whose distribution will converge to $P(\theta_2 - \theta_1 \mid X)$, because this process is its sample analogue. Kruschke altered the covariance to show a familiar form of the kind of dependency between $\theta_1$ and $\theta_2$ that will be preserved in samples from the second procedure but not in samples from the first. In short, everything you want is available from a set of samples from the joint posterior, but if you only know the distribution of its marginals then all bets are off. For a classical analogy, recall that group-specific confidence intervals may overlap while the test of a group difference (or equivalently, an interval for that difference) may still reject.
