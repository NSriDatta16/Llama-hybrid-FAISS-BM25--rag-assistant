[site]: crossvalidated
[post_id]: 36177
[parent_id]: 36134
[tags]: 
Such an estimator does not exist. The intuition is that the median can stay fixed while we freely shift probability density around on both sides of it, so that any estimator whose average value is the median for one distribution will have a different average for the altered distribution, making it biased. The following exposition gives a little more rigor to this intuition. We focus on distributions $F$ having unique medians $m$, so that by definition $F(m) \ge 1/2$ and $F(x) \lt 1/2$ for all $x \lt m$. Fix a sample size $n \ge 1$ and suppose that $t: [0,1]^n \to [0,1]$ estimates $m$. (It will suffice that $t$ only be bounded, but usually one doesn't seriously consider estimators that produce obviously impossible values.) We make no assumptions about $t$; it does not even have to be continuous anywhere. The meaning of $t$ being unbiased (for this fixed sample size) is that $$E_F[t(X_1, \ldots, X_n)] = m$$ for any iid sample with $X_i \sim F$. An "unbiased estimator" $t$ is one with this property for all such $F$. Suppose an unbiased estimator exists. We will derive a contradiction by applying it to a particularly simple set of distributions. Consider distributions $F = F_{x,y,m, \varepsilon}$ having these properties: $0 \le x \lt y \le 1$; $0 \lt \varepsilon \lt (y-x)/4$; $x + \varepsilon \lt m \lt y - \varepsilon$; $\Pr(X = x) = \Pr(X = y) = (1-\varepsilon)/2$; $\Pr(m-\varepsilon \le X \le m+\varepsilon) = \varepsilon$; and $F$ is uniform on $[m-\varepsilon, m+\varepsilon]$. These distributions place probability $(1-\varepsilon)/2$ at each of $x$ and $y$ and a tiny amount of probability symmetrically placed around $m$ between $x$ and $y$. This makes $m$ the unique median of $F$. (If you are concerned that this is not a continuous distribution, then convolve it with a very narrow Gaussian and truncate the result to $[0,1]$: the argument will not change.) Now, for any putative median estimator $t$, an easy estimate shows that $E[t(X_1, X_2, \ldots, X_n)]$ is strictly within $\varepsilon$ of the average of the $2^n$ values $t(x_1, x_2, \ldots, x_n)$ where the $x_i$ vary over all possible combinations of $x$ and $y$. However, we can vary $m$ between $x + \varepsilon$ and $y - \varepsilon$, a change of at least $\varepsilon$ (by virtue of conditions 2 and 3). Thus there exists an $m$, and whence a corresponding distribution $F_{x,y,m,\varepsilon}$, for which this expectation does not equal the median, QED.
