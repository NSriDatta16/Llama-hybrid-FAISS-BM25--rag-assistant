[site]: crossvalidated
[post_id]: 90951
[parent_id]: 90902
[tags]: 
In simple cases I think the answer is: the grand mean (over all test cases and all folds) has the same variance for $k$-fold and LOO validation. Simple means here: models are stable, so each of the $k$ or $n$ surrogate models yields the same predicion for the same sample (thought experiment: test surrogate models with large independent test set). If the models are not stable, the situation gets more complex: each of the surrogate models has its own performance, so you have additional variance. In that case, all bets are open whether LOO or $k$-fold has more additional variance*. But you can iterate the $k$-fold CV and taking the grand mean over all test cases and all $i \times k$ surrogate models can mitigate that additional variance. There is no such possibility for LOO: the $n$ surrogate models are all possible surrogate models. The large variance is usually due to two factors: small sample size (if you weren't in a small sample size situation, you'd not be worried about variance ;-) ). High-variance type of error measure. All proportion-of-test-cases-type of classification errors are subject to high variance. This is a basic property of estimating fractions by counting cases. Regression-type errors like MSE have a much more benign behaviour in this respect. For classification errors, there's a number of papers that looks at the properties of different resampling validation schemes in which you also see variances, e.g.: Kohavi, R.: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Mellish, C. S. (ed.) Artificial Intelligence Proceedings 14$^th$ International Joint Conference, 20 -- 25. August 1995, Montréal, Québec, Canada, Morgan Kaufmann, USA, , 1137 - 1145 (1995). We observed very similar behaviour for vibrational spectroscopic data: Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G.: Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005). (I guess similar papers may exist for regression errors as well, but I'm not aware of them) * one may expect LOO to have less variance because the surrogate models are trained with more cases, but at least for certain types of classification models, LOO doesn't behave very well.
