[site]: crossvalidated
[post_id]: 158860
[parent_id]: 
[tags]: 
how to understand this neighborhood components analysis model?

I am reading an article with title "neighborhood components analysis" lately. http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf . This article is trying to introduce a linear transformation of the input space, such that in the transformed space, the traditional machine learning method KNN (k-nearest-neighbors) can perform very well. The author use a stochastic scheme in the transformed space, which define the parameter $p_{ij}$ as the probability of node $j$ choose node $i$ as its neighbor and inherit its class label from node $j$. Then, the objective function which is to be maximized is the expected number of nodes correctly classified under this probability definition. This is a very interesting article, although I did not finish reading all of it yet. Now, my questions are as follows. (1): Why does the author define the probability $p_{ij}$ in the way as in the article? Is there any specific requirement? If so, what kind of requirements? (2): How to differentiate the function $f$ with respect to the matrix $A$, if we already know that $f(A)=\sum_{i}\sum_{j\in C_i}p_{ij}$? (3): Why minimizing the objective function $f(A)$ is equivalent to minimizing the $L_{1}$ norm between the true class distribution and the stochastic class distribution induced by $p_{ij}$ via the matrix $A$? (4): What is "low dimensional linear embedding of labeled data?" How to understand it? I hope these are not trial questions. Many thanks for your time and attention.
