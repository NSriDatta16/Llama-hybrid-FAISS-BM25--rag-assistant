[site]: crossvalidated
[post_id]: 576674
[parent_id]: 576565
[tags]: 
Including an offset term in a Cox model is not a good idea. You might "already know" a predictor's "effect on the hazard rate," but that knowledge is almost always imprecise. What you "know" about that predictor might come from a model that didn't include the other predictors in the new model, and thus the "known" coefficient might itself suffer from omitted-variable bias. Forcing a particular coefficient on that predictor in the new model will then bias the coefficients for all the other predictors in the model. A better approach would be to include the predictor you "already know" in the new model without imposing the offset. Then show that its coefficient agrees with what you "already know." That helps validate both the prior and the current model. What you are trying to do with coxphf() is somewhat inconsistent logically. That function penalizes all regression coefficients, while an offset() term requires that the corresponding coefficient equal 1 exactly. I suppose that a similar program could be written to maintain the offset coefficient at 1 and penalize all the others, but I'm not aware of such for a Firth penalization. There are other readily available penalization methods that allow for such distinctions between penalized and unpenalized coefficients. A ridge() term in a coxph() model provides for a specified penalty for a subset of predictors, leaving the others unpenalized. The tools in the glmnet package allow for differential weighting of penalties among predictors and can thus specify predictors whose coefficients aren't penalized at all. Instead of an offset for the predictor in question, you could use a Bayesian survival model in which you specify an appropriately narrow prior for its coefficient. That has the advantage of a coherent theoretical basis while forcing you to think about just how sure you are about the coefficient's value.
