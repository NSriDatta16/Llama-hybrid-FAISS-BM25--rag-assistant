[site]: crossvalidated
[post_id]: 608985
[parent_id]: 
[tags]: 
How to compare different forecasting models (TS, ML, DL) using the same and replicable validation method

I am working on a research problem in which I compare an ARIMA, a lightGBM and an LSTM model across different hierarchical levels in a sales forecast setting (similar to the M5 forecasting competition scenario). Before I elaborate further on my setting I want to be upfront with my question: How can I use the same validation technique (preferably something like k=5 kfold validation using sci-kit learn time series split) across all models (TS, ML and DL) to make the same comparison using Python? My forecasting horizon is 10 days. In the end, I want to have a cross-validation score and a test score for the last 10 days. I test different hypotheses using different external data as well as clustering methods. However, I find it very confusing to align the validation technique across all models to make them perfectly comparable and replicable. I use autoARIMA to tune the ARIMA model. As far as I understood it minimises AIC and BIC. I use sci-kit learn's time series split (k=5) to tune lightGBM minimising MAE and finally I use a similar handwritten technique for LSTM with a 30-day look-back period and 10 days forecasts minimising MAE as well. How would you approach such a problem? How would you structure the code and scripts? Is there a library out there that I can use for all models or do I have to split the data manually and compare models using a for loop? Thank you for any help!
