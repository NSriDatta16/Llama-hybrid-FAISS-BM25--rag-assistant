[site]: datascience
[post_id]: 49267
[parent_id]: 49266
[tags]: 
The validation set's purpose is to track your level of overfitting while you are experimenting with your network. When we try something we want to know how well it works on data it hasn't seen before. By looking on the score on the training set we can't tell what is good from what is overfitting. If you instead use your test set to evaluate the results of all your experiments, then your test set stops being a reliable final evaluation since you have included it in your decisions. The test set is only for final evaluations. To fill this need we set aside a part of our data as a validation set. A dataset that we do not directly train on, but instead the use to check results of training different layer/unit configurations, tuning hyperparameters, feature engineering and all the other things we possibly want to try. Early stopping is just one of the possible uses. Regarding your comments about hyperparameters: You don't use the validation set directly when you are tuning hyperparameters (with grid search or any other method). You still use the training set, but then you use the validation set to test what gave the best result.
