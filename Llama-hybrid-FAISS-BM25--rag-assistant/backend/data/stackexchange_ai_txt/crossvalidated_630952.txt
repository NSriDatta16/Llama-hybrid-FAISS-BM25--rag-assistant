[site]: crossvalidated
[post_id]: 630952
[parent_id]: 630615
[tags]: 
Is the AIC values for that model + nested models reliable to use for model selection? Because PQL does not give a true likelihood function, AIC is not recommended for models computed using PQL. Because the AIC is based on the model's likelihood, its application in this situation can be misleading. The AIC values you obtained may not be reliable for model selection due to the restrictions of utilising AIC with PQL models. They may not represent the trade-off between model fit and complexity accurately. If the QAIC should be used instead, how to calculate QAIC for mblogit model? When dealing with overdispersed count data or models where the likelihood is approximated, QAIC is frequently recommended. It corrects the AIC for the lack of fit that can arise when using quasi-likelihood approaches. This doesn't answer your question completely, but the message you received indicates that there is a problem with the scale parameter (c-hat). c-hat is a measure of how well QAIC adapts for overdispersion. c-hat less than one indicates that there is no overdispersion, which is unusual for quasi-likelihood models. The NA result could be due to estimation difficulties or other computational limitations within the MuMIn::QAIC function. Is there any other model selection approaches for this issue? Yes! When comparing nested models, likelihood ratio tests (LRTs) may be used. This, however, is contingent on the availability of a reasonable likelihood estimate. Some modified information criteria, such as the BIC (Bayesian Information Criterion) or AICc (AIC with finite sample size correction), may be more appropriate, albeit these have drawbacks as well. Cross Validation is a robust method for model selection that does not rely on information criterion if it is practicable. Bayesian Methods: Another option is to use a Bayesian model comparison approach (such as Bayes factors), which require a Bayesian framework and can be computationally expensive.
