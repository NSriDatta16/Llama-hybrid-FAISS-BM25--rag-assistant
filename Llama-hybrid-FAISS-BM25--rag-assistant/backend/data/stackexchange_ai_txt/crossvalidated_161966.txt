[site]: crossvalidated
[post_id]: 161966
[parent_id]: 
[tags]: 
Variance of average due to correlation between auto-correlated time series

I want to calculate the average value of $n_i$ time series each of length $n_t$, i.e. an average of $n_i \times n_t$ values, together with a measure of uncertainty. To be more concrete, I have $n_i\approx 6$ temperature measurements at different places for $n_t\approx 100$ different times. Each of the time series is auto-correlated but the different time series are also correlated. How can I estimate the variance of the average? My approach so far was to treat each time series separately. Let $x^i_t$ be the measurement of time series $i$ at time $t$. Then I estimated the variance $\mathrm{Var}(\bar x^i)$ of the mean over time, $\bar x^i$, for a fixed $i$ to be $$ \mathrm{Var}(\bar x^i) = \frac{\mathrm{Var}(x^i_t)}{n_t} \frac{n_t-1}{n_t/\gamma_i-1} $$ with $\gamma_i$ derived from the auto-correlation $\rho^i_k$ of $x^i_t$ : $$ \gamma_i = 1+ 2\sum_{k=1}^{k_\mathrm{max}}(1-\frac{k}{n_t})\rho^i_k \, . $$ While the linked article uses $k_\mathrm{max}=n_t-1$, I found it more useful and also convincing to sum up only the first values of the auto-correlation that are significantly different from $0$. (Please correct me if I am wrong here.) How do I combine the variances of the single time series to the uncertainty of the total average $$ \bar{\bar x} = \frac{1}{n_i}\sum_i \bar x^i $$ taking into account the correlation between the time series as for the general case proposed here ? Using $$ \mathrm{Var}(\bar{\bar x}) = \frac{1}{n^2} \sum_i \mathrm{Var}(\bar x^i) $$ ignores this correlation.
