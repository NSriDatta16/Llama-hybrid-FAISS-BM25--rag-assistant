[site]: crossvalidated
[post_id]: 440620
[parent_id]: 440550
[tags]: 
No, this is not really feasible. While I can't really prove that it's not possible, I can give an example which demonstrates why I think it's a bad idea. Suppose you decide to train a neural network to invert a matrix. Your network has an input size of $n^2$ , and output size of $n^2$ , and the hidden layer probably needs at least $n^2$ neurons as well (else UAT doesn't hold). So already, you have weight matrices of size $n^2$ by $n^2$ , and the time complexity for a forward pass is up to $n^4$ , which is already greater than $n^3$ needed for matrix inversion.
