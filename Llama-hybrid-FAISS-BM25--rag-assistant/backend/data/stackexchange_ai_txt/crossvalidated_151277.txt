[site]: crossvalidated
[post_id]: 151277
[parent_id]: 150946
[tags]: 
Is there a principled way to estimate the variance of the classifiers using the testing distribution? Yes, and contrary to your intuition it is actually easy to do this by cross valiation. The idea is that iterated/repeated cross validation (or out-of-bag if you prefer to resample with replacement) allow you to compare the performance for slightly different "surrogate" models for the same test case , thus separating variance due to model instability ( training) from variance due to the finite number of test cases (testing). see e.g. Beleites, C. & Salzer, R.: Assessing and improving the stability of chemometric models in small sample size situations, Anal Bioanal Chem, 390, 1261-1271 (2008). DOI: 10.1007/s00216-007-1818-6 As @RyanBressler points out, there's the Bengio paper about cross validation fundamentally underestimating variance of the models. This underestimation occurs with respect to the assumption that resampling is a good approximation to a new independent sample (which it obviously isn't). This is important if you want to compare the general performance of some type of classifier for some type of data, but not in applied scenarios where we talk about the performance of a classifier trained from the given data. Note also that the separation of this "applied" test variance into instability and testing variance uses a very different view on the resampling: here the surrogate models are treated as approximations or slightly perturbed versions of a model trained on the whole given training data - which should be a much better approximation. the performance that method A and method B each achieve on the test set are virtually identical. However, I fear this is simply due to the variance of the two methods, and perhaps method A would be better on average if I could sample more training and testing sets. This is quite possible. I'd suggest that you check which of the 2 sources of variance (instability, i.e. training and testing uncertainty) is the larger source of variance and focus on reducing this. I think Sample size calculation for ROC/AUC analysis discusses the effects of finite test sample size on your AUC estimate. However, for performance comparison of two classifiers on the same data I'd suggest to use a paired test like McNemar's: for finding out whether (or which) classifier is better, you can concentrate on correct classifications by one classifier that are wrongly predicted by the other. These numbers are fractions of test cases for which the binomial distribution lets you calculate variance.
