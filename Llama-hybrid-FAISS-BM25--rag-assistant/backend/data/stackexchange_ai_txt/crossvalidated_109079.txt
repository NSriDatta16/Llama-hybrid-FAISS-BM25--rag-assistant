[site]: crossvalidated
[post_id]: 109079
[parent_id]: 
[tags]: 
Confusion between caret randomForest predict() results and reported model performance

This question seems related, but the consensus was that the issue had to do scaling the data, which I do prior to training, so I don't think that's the issue: Issue on prediction with FinalModel of RandomForest in R using the CARET package I've uploaded a sample data set, and here is how I generated my model: library(randomForest) library(caret) library(ggplot2) data Here's what caret() reports as the model performance: > model Random Forest 516 samples 11 predictors 5 classes: '0', '0.5', '1', '1.5', '2' No pre-processing Resampling: Repeated Train/Test Splits Estimated (25 reps, 0.9%) Summary of sample sizes: 468, 468, 468, 468, 468, 468, ... Resampling results across tuning parameters: mtry Accuracy Kappa Accuracy SD Kappa SD 2 0.747 0.663 0.0643 0.0853 6 0.76 0.68 0.0507 0.068 11 0.758 0.678 0.0574 0.0763 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 6. In my "real" model, I have a training/hold-out set, and creating two sets of plots showing model predictions for the training/hold-out sets vs. the corresponding true observations. That's when I noticed something that seemed odd to me. # data set of model predictions on training data vs. actual observations results And here's a plot confirming 100% accuracy on the training set: p But if I look at the saved tuning predictions, subsetting only those where mtry = 6 (what caret reports as the final model), I don't get anywhere near that performance: model_resamples And the same sort of plot: p Is this just a case of over-fitting where holding out 10% of the data can create a 25% decrease in performance, yet the final model trained with the same parameters but all rows can yield 100% performance? It seemed unlikely, but that's the only thing coming to mind at the moment. I just want to make sure there's nothing wrong in my training or predicting methods where I'm creating a problem where there shouldn't be one. Note: I created the tables/plots prior to adding set.seed() in the model training code above. The exact table and plot may differ slightly, but in re-running, they general result is the same (perfect re-prediction vs. ~77% reported by the model). It didn't seem to warrant re-doing the results/plots above, so I left them.
