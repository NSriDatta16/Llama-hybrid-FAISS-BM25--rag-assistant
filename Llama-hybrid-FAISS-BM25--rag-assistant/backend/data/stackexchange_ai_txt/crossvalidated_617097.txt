[site]: crossvalidated
[post_id]: 617097
[parent_id]: 617093
[tags]: 
As I wrote yesterday , there are issues with data augmentation. If you have a small sample size, you put yourself at similar risk of overfitting as you would be fitting a complex model, and if you have a large sample size where that is not such a concern, then I question the need to synthesize artificial data. With just $45$ observations, you lack the sample size to do sophisticated modeling like neural networks (unless you just want to learn the mechanics of writing neural network code). Unless this overflow follows a simple pattern, consistently strong performance is unlikely. My suggestion is to work with a simple model like a linear regression on a few features, perhaps three to five features, following a rule-of-thumb for using one feature per $10$ - $15$ observations. This is unlikely to achieve the kind of performance that you would get from sophisticated modeling on a large data set, but this is probably all your data will allow.
