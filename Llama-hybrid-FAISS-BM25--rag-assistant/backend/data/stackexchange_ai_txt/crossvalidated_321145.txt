[site]: crossvalidated
[post_id]: 321145
[parent_id]: 
[tags]: 
Show that $(\mathbf{x}, \boldsymbol\Theta\mathbf{x}+\boldsymbol\eta)$ is jointly normal

This is from Theodoridis' Machine Learning , exercise 3.16. Suppose $\mathbf{x}$ is a vector of jointly normal random variables with covariance matrix $\boldsymbol\Sigma_x$. Let $$\mathbf{y} = \boldsymbol\Theta\mathbf{x}+\boldsymbol\eta$$ where $\boldsymbol\Theta$ is a $k \times l$ matrix of [I assume, known or fixed] parameters and $\boldsymbol\eta$ is normal with mean $\mathbf{0}$ and covariance matrix $\boldsymbol\Sigma_\eta$, independent of $\mathbf{x}$. Show that $\mathbf{y}$ and $\mathbf{x}$ are jointly Gaussian with covariance matrix $$\boldsymbol\Sigma = \begin{bmatrix} \boldsymbol\Theta\boldsymbol\Sigma_x\boldsymbol\Theta^{T}+\boldsymbol\Sigma_\eta & \boldsymbol\Theta\boldsymbol\Sigma_x \\ \boldsymbol\Sigma_x\boldsymbol\Theta^{T} & \boldsymbol\Sigma_x \end{bmatrix}\text{.}$$ It is clear that $\mathbf{y} \sim \mathcal{N}_k(\boldsymbol\Theta\boldsymbol\mu_x,\boldsymbol\Theta\boldsymbol\Sigma_x\boldsymbol\Theta^{T}+\boldsymbol\Sigma_\eta)$. This explains the top-left diagonal of $\boldsymbol\Sigma$. The $\boldsymbol\Theta\boldsymbol\Sigma_x$, as well as the $\boldsymbol\Sigma_x\boldsymbol\Theta^{T}$, are easy to explain as well. However, I'm not sure how to show that $\mathbf{y}$ and $\mathbf{x}$ are jointly Gaussian. In particular, I considered looking at the distribution of $\mathbf{y} \mid \mathbf{x}$, but using this assumes bivariate normality of $(\mathbf{x}, \mathbf{y})$ to start.
