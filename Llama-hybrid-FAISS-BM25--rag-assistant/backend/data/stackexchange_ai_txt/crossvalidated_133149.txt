[site]: crossvalidated
[post_id]: 133149
[parent_id]: 
[tags]: 
Why is variance (instead of standard deviation) the default measure of information content in principal components?

The information content of principal components is almost always expressed as a variance (e.g., in scree plots or in statements like "the first three PCs contain 95% of the total data variance"). The intent of this usage is to describe how much variation/information is contained in the PCs. it seems to me that variance can be a misleading measure of information contained in PCs, because it is a squared metric of variation that emphasizes large deviations from the mean over small ones. This can grossly underemphasize the importance of information contained in lower-eigenvalue PCs. The standard deviation of PCs would seem to be a much more direct, meaningful and balanced metric of the information they contain. I am very clear on the rationale for the use of variance in statistics more generally, i.e. it is much more mathematically convenient than standard deviation. However, I'm wondering if there is a specific rationale for why variance is used a measure of variation in PCs instead of standard deviation. Are there any good references for this dilemma? Update to clarify: I should be clear that I am not asking about why variance is used in the derivation of the principal components, but rather why it is used as a default descriptor of variation in the PCs when reporting results of the PCA. Many people seem to use "variance" and "variation" as synonymous in this context, but isn't standard deviation a measure of variation, and variance a squared measure of variation? A PC that contains 95% of the data variance might contain only 80% of the variation in the data as measured in standard deviations: isn't the latter a better descriptor?
