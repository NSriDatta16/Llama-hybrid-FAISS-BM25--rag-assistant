[site]: crossvalidated
[post_id]: 314025
[parent_id]: 
[tags]: 
NARX architecture of neural networks

I’ve a question related to the NARX architecture of neural nets: What is the usage of the tapped delay line in this architecture? My problem is, that I can’t figure out the difference of this compared to a basic multilayer perceptron. Let’s say I have a set of data $S$ which represents a time series. I always want to predict the $k_{th}$ values of this series by training the network on subsets of $S$ from $j$ to $k-1$. So, the amount of tapped delay lines of the network is equal to the size of values of the subsets of $S$. On predictions of the $k_{th}$ value, I backpropage the error between the expected and the predicted value to adjust the weights. For the next prediction, I start with the expected kth value to be the last in the subset. So what I actually do is to use a sliding window over my training data where the expected, the target value is always feed back into the net with when the sliding window moves. Can’t I just use a MLP for this? Is it not just a matter of the training procedure?
