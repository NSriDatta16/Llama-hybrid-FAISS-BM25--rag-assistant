[site]: crossvalidated
[post_id]: 466765
[parent_id]: 
[tags]: 
Agent randomly achieves high rewards but doesn't change its policy

I am currently training a deep reinforcement learning model in a continuous environment using Ray. The environment I am using was coded up in OpenAI Gym using baselines by another person who's research I am trying to replicate in Ray so that it may be parallelized. The model converges about 5% of the time in baselines after thousands of episodes though I have not been able to replicate this. My problem is that the AI stops changing its policy very quickly and the mean reward then stays roughly the same for the rest of the run. This is independent of every parameter I've tried to vary: Learning Algorithm: I've used PPO, A3C, and DDPG Learning Rate: I've done hyperparameter sweeps from 1e-6 up to 10 Episode Length: I've used complete_episodes as well as truncated_episodes in batch_mode . I've varied rollout_fragment_length and made sure train_batch_size was always equal to rollout_fragment_length times the number of workers entropy_coeff in A3C kl_target , kl_coeff , and sgd_minibatch_size in PPO Network Architecture: I've tried both wide and deep networks. The default Ray network is 2x256 layers. I've tried changing it to 4x2048 and 10x64. The baselines network that succeeded was 8x64. Changing activation function: ReLU returns NaNs but tanh lets the program run Implementing L2 loss to prevent connection weights from exploding. This kept tanh activation from returning NaNs after a certain amount of time (the higher the learning rate the faster it got to the point it returned NaNs) Normalization: I normalized the state that is returned to the agent from 0 to 1. Actions are not normalized because they both go from -6 to 6. Scaling the reward: Ive had rewards in the range of 1e-5 to 1e5. This had no effect on the agent's behavior. Changing reward functions Making the job easier (more lenient conditions) I am running this in a docker image and have verified that this occurs on another machine so it's not a hardware issue. Here is a list of things I believe are red flags that could be interrelated: I was originally getting NaNs (Ray returned NaN actions) when using ReLU but switching my activation function to tanh. The KL divergence coefficient cur_kl_coeff in PPO goes to zero almost immediately when I include L2 loss. From what I understand, this means that the model is not making meaningful weight changes after the first few iterations. However, when the loss is just policy loss, cur_kl_coeff varies normally. Loss doesn't substantially change on average and doesn't ever converge Reward doesn't change substantially reward_mean always converges to roughly the same value. This occurs even when reward_mean starts above average or initially increases to above average due to random weight initialization. Note: This is not an issue of the agent not finding a better path or a poor reward function not valuing good actions. Zooming out shows why mean reward peaked previously. The agent performed phenomenally but was never able to repeat anywhere near its success. And, I cannot stress this enough, this is not an issue of me not letting the agent run for long enough. I have observed this same behavior across dozens of runs and cumulatively tens of thousands of iterations. I understand that it can take models a long time to converge, but the models consistently make zero progress. tldr; My model doesn't change it's policy in any meaningful way despite getting very high rewards on certain runs.
