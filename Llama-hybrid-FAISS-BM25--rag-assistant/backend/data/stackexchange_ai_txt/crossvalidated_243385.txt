[site]: crossvalidated
[post_id]: 243385
[parent_id]: 
[tags]: 
Metric for determining how different a given string is from elements in a larger body of text?

I am trying to remove names from a large body of text to reduce the noise in my dataset. I have tried several approaches: Python's nltk.pos_tag function and a more naïve approach in which strings with the first letter capitalized and the remaining letters lowercase and not containing any numbers or punctuation are marked as possible names. The pos_tag implementation is very slow, and only performs marginally better than the more naïve approach. At this point, I am dissatisfied with the results. I know this word removal process will never be perfect, but it could certainly be better. What kind of algorithms/approaches exist to determine a given string's dissimilarity from the remaining strings in a body of text? Specifically, I would like to measure how dissimilar, for example, strings like "Heather" or "Samantha" are from a larger body of text like "Hello, Heather and Samantha, would you both like to attend my party? We would really enjoy your company..." Would this be a good use case for something like Levenshtein distance? I'm just trying to find a more accurate and quicker approach to finding names in text. Edit: I have tried Named-Entity Recognition (NER), too. While it's reasonably accurate, this implementation is too slow for my liking. Edit 2: For those coming here in the future, I used a filtering approach like @Zach suggested below. I had 8m+ words, but I only kept words that began with an uppercase letter, had all other letters lowercase and did not contain any numbers or punctuation. This trimmed my corpus down to about 5,000 words. I checked the remaining values against an English-language (Oxford English Dictionary) dictionary set of words totaling 200k+, keeping only those not in the dictionary. That got my list down to 2,000. From there, using nltk.pos_tag and filtering out words with parts-of-speech not equal to 'NNP' did the job. The code in Python: def extract_POS(text, pos): ''' Returns any part-of-speech (POS) passed as pos from a given string. This is useful for Named-Entity Recognition (NER) and extract certain types of words from text. Parameters: - text: iterable, e.g. str or list - pos: str (e.g. "NNP") ''' entities = [] if type(text) in {set, list, tuple}: text = " ".join(string for string in text) for chunk in pos_tag(word_tokenize(text)): if type(chunk) is tuple and pos in chunk[1]: entities.append(chunk[0]) return entities def remainder_lower(string): ''' Useful for finding names in text. Parameters: - string: str Example: string1 = 'HELLO' string2 = 'Hello' print(remainder_lower(string1)) # False print(remainder_lower(string2)) # True ''' if not string[0].isupper(): return False for letter in string[1:]: if letter.isupper(): return False return True possible_names = {(word, word.lower()) for row in df['corpus'] for word in set(str(row).split()) \ if remainder_lower(word) and not any(letter in punctuation or letter.isdigit() \ for letter in word)} possible_names = {proper for proper, lower in possible_names if lower not in dictionary_words} pos_to_add = extract_POS(possible_names, 'NNP')
