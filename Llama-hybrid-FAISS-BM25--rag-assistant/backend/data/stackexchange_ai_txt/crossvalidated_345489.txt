[site]: crossvalidated
[post_id]: 345489
[parent_id]: 342462
[tags]: 
The link provided in @itdxer's comment is great. Based on this link, I am writing this answer. Hyperparameter optimization is neural networks is a tedious job as it contains many set of parameters. The possible approaches for finding the optimal parameters are: Hand tuning (Trial and Error) - @Sycorax's comment provides an example of hand tuning. Here, based on trial and error experiments and experience of the user, parameters are chosen. Grid Search - Here a grid is created based on parameter values. And then all possible parameter combinations is tried and and the best one is selected. Random Search - Here, instead of trying all possible combinations as in Grid Search, only randomly selected subset of the parameters is tried and the best is chosen. Bayesian Optimization (Gausian Proces) - Gaussian Process uses a set of previously evaluated parameters and resulting accuracy to make an assumption about unobserved parameters. Acquisition Function using this information suggest the next set of parameters. ( Do not understand much, taken from this link ) Tree-structured Parzen Estimators (TPE) - Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. ( Do not understand much, taken from this link ). Now as per this link The Bayesian Optimization and TPE algorithms show great improvement over the classic hyperparameter optimization methods. They allow to learn from the training history and give better and better estimations for the next set of parameters. Now the good thing is that there is a Python library called hyperopt for doing these. More details in the below pages: http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html https://jaberg.github.io/hyperopt/ . https://github.com/jaberg/hyperopt/wiki . https://github.com/wenyangfu/hyperparam-search-guides/blob/master/hyperopt-guide.md https://www.youtube.com/watch?v=Mp1xnPfE4PY
