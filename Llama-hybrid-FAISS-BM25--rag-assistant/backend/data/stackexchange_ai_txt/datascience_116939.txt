[site]: datascience
[post_id]: 116939
[parent_id]: 104607
[tags]: 
You're essentially trying to compare the two revolutionary neural/attention-based MT models: Bahdanau 2015 and Vaswani 2017. A first difference is that their attention weights formula is slightly different: Bahdanau uses a feed-forward network to essentially try to learn a similarity function. Vaswani uses a scaled dot product. A second difference is that during encoder-decoder attention, Bahdanau compares one decoder vector to all encoder vectors. Vaswani, on the other hand, compares all decoder vectors to all encoder vectors. A third difference: as Jindrich mentioned, transformers are faster to train because the encoder is parallel, not sequential. Apart from time, they also need fewer computations (about 100 times) than the state-of-the-art NMT models at the time the Vaswani paper was published (see table 2 on page 8) to obtain a competitive translation quality (BLEU). I don't know how this compares to the Bahdanau architecture, since Vaswani doesn't include it. A fourth important difference between the two architectures is how they treat long-range dependencies . It is often said that Bahdanau solved Seq2Seq's information bottleneck problem by exposing all the encoder's hidden states to the decoder instead of only the final one. However, there are actually still two information bottlenecks present in Bahdanau, namely within the encoder and decoder respectively. Information from words at one end of the sentence has a hard time propagating all the way through to the other end. All of that information is crammed into the LSTM's hidden state, which becomes less and less sharp as it ripples through. In a transformer, the attention between two words is unaffected by the distance between them -- ceteris paribus, e.g. disregarding positional encoding. The dot product is the same dot product. One final difference between the two, based on the above: Bahdanau essentially pre-processes word embeddings with a BiLSTM, performing attention over what that spits out. Vaswani performs attention over the embeddings directly, merely adding a modulation to the individual words (a pre-processing step where the words don't interact yet) to discern whether the same word is in a different position.
