[site]: crossvalidated
[post_id]: 18134
[parent_id]: 18116
[tags]: 
There are IMHO no formal differences that distinguish machine learning and statistics at the fundamental level of fitting models to data. There may be cultural differences in the choice of models, the objectives of fitting models to data, and to some extend the interpretations. In the typical examples I can think of we always have a collection of models $M_i$ for $i \in I$ for some index set $I$ , and for each $i$ an unknown component $\theta_i$ (the parameters, may be infinite dimensional) of the model $M_i$ . Fitting $M_i$ to data is almost always a mathematical optimization problem consisting of finding the optimal choice of the unknown component $\theta_i$ to make $M_i$ fit the data as measured by some favorite function. The selection among the models $M_i$ is less standard, and there is a range of techniques available. If the objective of the model fitting is purely predictive, the model selection is done with an attempt to get good predictive performance, whereas if the primary objective is to interpret the resulting models, more easily interpretable models may be selected over other models even if their predictive power is expected to be worse. What could be called old school statistical model selection is based on statistical tests perhaps combined with step-wise selection strategies, whereas machine learning model selection typically focuses on the expected generalization error, which is often estimated using cross-validation. Current developments in and understandings of model selection do, however, seem to converge towards a more common ground, see, for instance, Model Selection and Model Averaging . Inferring causality from models The crux of the matter is how we can interpret a model? If the data obtained are from a carefully designed experiment and the model is adequate it is plausible that we can interpret the effect of a change of a variable in the model as a causal effect, and if we repeat the experiment and intervene on this particular variable we can expect to observe the estimated effect. If, however, the data are observational, we can not expect that estimated effects in the model correspond to observable intervention effects. This will require additional assumptions irrespectively of whether the model is a "machine learning model" or a "classical statistical model". It may be that people trained in using classical statistical models with a focus on univariate parameter estimates and effect size interpretations are of the impression that a causal interpretation is more valid in this framework than in a machine learning framework. I would say it is not. The area of causal inference in statistics does not really remove the problem, but it does make the assumptions upon which causal conclusions rest explicit. They are referred to as untestable assumptions . The paper Causal inference in statistics: An overview by Judea Pearl is a good paper to read. A major contribution from causal inference is the collection of methods for the estimation of causal effects under assumptions where there actually are unobserved confounders, which is otherwise a major concern. See Section 3.3 in the Pearl paper above. A more advanced example can be found in the paper Marginal Structural Models and Causal Inference in Epidemiology . It is a subject matter question whether the untestable assumptions hold. They are precisely untestable because we can not test them using the data. To justify the assumptions other arguments are required. As an example of where machine learning and causal inference meets, the ideas of targeted maximum-likelihood estimation as presented in Targeted Maximum Likelihood Learning by Mark van der Laan and Daniel Rubin typically exploit machine learning techniques for non-parametric estimation followed by the "targeting" towards a parameter of interest. The latter could very well be a parameter with a causal interpretation. The idea in Super Learner is to rely heavily on machine learning techniques for estimation of parameters of interest. It is an important point by Mark van der Laan (personal communication) that classical, simple and "interpretable" statistical models are often wrong, which leads to biased estimators and too optimistic assessment of the uncertainty of the estimates.
