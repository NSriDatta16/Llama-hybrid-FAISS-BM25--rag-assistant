[site]: crossvalidated
[post_id]: 96633
[parent_id]: 
[tags]: 
Way of measuring students' performance

Using their previous results and the results form an aptitude test, students are given a target score. This score is a decimal number between $0$ and $120$. After their final exams they are graded on the same scale (although they receive a letter corresponding to intervals). I would like to devise a statistical test to compare their target score with their actual score. My idea is to use the $\chi^2$-test of significance, where the target scores are the expected frequencies $E_i$, and their actual scores are the observed frequencies $O_i$. Say that I have $20$ students, then $$X^2 = \sum_{i=1}^{20} \frac{(O_i-E_i)^2}{E_i}$$ My null hypothesis, $H_0$, would be that there is no difference in teaching and learning in my college than at the average college. (The target scores come from a team of government statisticians performing statistical analysis on all past outcomes, for all institutions.) Clearly, if $O_i=E_i$ for all $1 \le i \le 20$ then $X^2=0$, and we can have $0\%$ confidence in rejecting the null hypothesis $H_0$. Let's say that we wanted to be $95\%$ confident that we could reject the null hypothesis, then we would find $\chi_{20}^2(5\%) = 31.41$ and require that $X^2 \ge 31.41$. I get the degrees of freedom to be the same as the number of students because there are no constraints. The total final scores of the candidates, i.e. $\sum O_i$ isn't fixed. Does this model seem like it would work, or is it a miss-application of the test? Can you recommend any other models or any refinements to the current one?
