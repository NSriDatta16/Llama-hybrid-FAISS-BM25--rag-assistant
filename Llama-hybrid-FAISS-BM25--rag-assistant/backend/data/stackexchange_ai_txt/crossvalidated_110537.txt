[site]: crossvalidated
[post_id]: 110537
[parent_id]: 90902
[tags]: 
From An Introduction to Statistical Learning When we perform LOOCV, we are in effect averaging the outputs of $n$ fitted models, each of which is trained on an almost identical set of observations; therefore, these ouputs are highly (positively) correlated with each other. In contrast, when we perform $k$ -fold CV with $k , we are averaging the outputs of $k$ fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from $k$ -fold CV. To summarize, there is a bias-variance trade-off associated with the choice of $k$ in $k$ -fold cross-validation. Typically, given these considerations, one performs $k$ -fold cross-validation with $k=5$ or $k=10$ , as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.
