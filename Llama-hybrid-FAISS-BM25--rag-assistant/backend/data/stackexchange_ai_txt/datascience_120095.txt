[site]: datascience
[post_id]: 120095
[parent_id]: 120087
[tags]: 
Welcome to the data science sector. Your three points seem to relate to different aspects. I will try to address all three: 1. Feature Importance To explain the effect, I would go the other way and start with a model trained on few features that are mainly uncorrelated (just for the sake of explaination). One our features will get the highest feature importance (let's call it A). To simplify things, this roughly means that the feature is used in a lot of important nodes of a lot of trees of our random forest. Now assume we add another feature (B) that is very strongly correlated with feature A. If we now train another random forest, it does make a huge difference whether feature A or feature B is used in a node. So approximately half of the time A is used and half of the time B is used. This means, A only appears in half of the nodes compared to the first model. Which would give A half of the feature importance it got before (again, we simplify things. Depending on the definition of feature importance, the effect might differ, but the general concept stays the same) Following the above explanation, one can see, that removing some features will affect the other feature's importance differently, depending on how strongly they were correlated with the removed features. This might already explain your observation. Conclusion: Feature importance measures only the importance for one concrete model. Not the value of the information behind the feature. 2. Accuracy It looks like you have some sort of imbalanced data. I suggest to look for the frequency of occurances of values in SYMPT. I assume there are just two values 0 and 1 and 71% of all values are labeled 1. In that case, a model that always says 1 would get an accuracy of 71%. So in that case, if you models would not lear much, all would get 71% accuracy. In such a case of implanaced data, one better uses othere metrics to evaluate the model, such as roc-auc, precision and recall (look a both), true- and false-positive-rate. 3. Adding Column AA It looks like there is a trend in your data. If the ID-Column is a valuable information, then the data might be sorted in some way. If you are not sure how the data is sorted, it would be the best to ignore the ID and to shuffle the data. Otherwise, you need to analyze the data to understand the ordering. Note : In scikit-learn you need to explicitly set set shuffle=True for cross validation (see for example https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html ). I hope this helps you a bit to understand what is going on with your models
