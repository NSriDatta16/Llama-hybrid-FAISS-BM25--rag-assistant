[site]: datascience
[post_id]: 84704
[parent_id]: 67396
[tags]: 
Lookback: I am not sure what you refer to. First thing that comes to mind is clip which is a hyperparameter controlling for vanishing/exploding gradients. Mini batching: There is a tradeoff between computational speed and speed of convergence. In short, *it has been observed that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize.*Hence, you need to search over the ideal size for your case. See an excellent discussion here LSTM units: otherwise called latent dimension of each LSTM cell, it controls the size of your hidden and cell states. The larger the value of this the "bigger" the memory of your model in terms of sequential dependencies. This will be softly depended to the size of your embedding. Epochs: if you are not familiar with the notion of stochastic, mini-batching and batch training I suggest you familiarise your self with it before moving any further. here or here . In essence the number of epochs will define the number of times your model will see the entirety of your dataset.
