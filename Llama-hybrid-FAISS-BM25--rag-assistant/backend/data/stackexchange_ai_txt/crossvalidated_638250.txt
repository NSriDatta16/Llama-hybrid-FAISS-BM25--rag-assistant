[site]: crossvalidated
[post_id]: 638250
[parent_id]: 
[tags]: 
Problems in understanding Word2vec architectures

I have probably a very simple question, but I did not find any clear resource on the web. First let's consider the Skip-gram model, in which we try to predict a context word given the target word. In this case, the input layer is a one-hot-vector of size N (size of the vocabulary); the output layer, is made up of k different vectors (where k is the size of our window)? so for each word, we will have 3 distinct probabilities of appearing in the context of the target word in the 3 different positions? If we consider the CBOW model and k=3, in this case we will have 3 input one-hot-vectors. For a given word i , the word-embedding is given by averaging the 3 vectors made up of the weights connecting the neuron i to all the neurons of the Hidden Layer?
