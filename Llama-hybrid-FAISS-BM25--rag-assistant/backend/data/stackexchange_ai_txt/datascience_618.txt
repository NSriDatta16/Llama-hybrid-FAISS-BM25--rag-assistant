[site]: datascience
[post_id]: 618
[parent_id]: 595
[tags]: 
Although adesantos has already given a good answer, I would like to add a little background information. The name for the problem you are looking at is "imputation". As adesantos already said, one of the possibilities is to fit a distribution. For example, you could fit a multivariate Gaussian to the data. You will get the mean only from the samples you know and you calculate the covariances only from the samples you know. You can then use standard MVG results to impute the missing data linearly. This is probably the simplest probabilistic method of imputation and it is already quite involved. If you are a neural networks, a recently proposed method that can do so are deep latent gaussian models by Rezende et al. However, understand the method will require a lot of neural net knowledge, quite some variational Bayes knowledge about Markov chains. Another method, which I have hear to work well is to train a generative stochastic network (Bengio et al). This is done by training a denoising auto encoder on the data you have (neglecting missing values in the reconstruction loss). Say you have a reconstruction function f and a input x. Then you will reconstruct it via x' = f(x). You then reset the values of x' with those you know from x. (I.e. you only keep the values that were missing before reconstruction.) If you do so many times, you are guaranteed to sample from the distribution given the values you know. But in either case, these methods require quite some knowledge about statistics and neural nets.
