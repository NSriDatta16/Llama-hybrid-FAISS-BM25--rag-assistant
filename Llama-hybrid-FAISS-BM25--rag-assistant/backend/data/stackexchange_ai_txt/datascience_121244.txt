[site]: datascience
[post_id]: 121244
[parent_id]: 121228
[tags]: 
I’ll answer the last question first :) Aggregation is typically done by “pooling” and this can output the either the mean of the inputs, the max, or the min. These are done with pooling layers, for example From the tensorflow documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D “Downsamples the input representation by taking the maximum value over a spatial window of size pool_size. The window is shifted by strides. The resulting output, when using the "valid" padding option, has a shape of: output_shape = (input_shape - pool_size + 1) / strides) To answer the first question, convolution is usually thought of as sliding a window of some kernel size along the input, multiplying the kernel by the input, and then pooling to create the output. The stride is the amount that the window slides, the kernel dimension is how big your kernel is ( typically much smaller than input dimension ), and the input dimension is how long a single instance of the training data is. In the 1d example diagram above, the kernel is dimension is 3, input dimension is 5, and the output of the max pooling is 1x3. So to clarify your second paragraph, the data and kernel size aren’t usually the same, all inputs are convoluted by the same kernel (if you only have one), and the size of the output will depend on your input size, kernel size, stride, and pooling. Typically you’ll want to have more than one filter btw.
