[site]: crossvalidated
[post_id]: 68579
[parent_id]: 68562
[tags]: 
This will always happen for large-enough values of $\lambda$ (the regularization coefficient). (If your predictors aren't very good, this will be a very small value.) Are you providing it explicitly? The linked documentation says for the parameter Lambda : Vector of nonnegative Lambda values. See Definitions. – If you do not supply Lambda , lasso calculates the largest value of Lambda that gives a nonnull model. In this case, LambdaRatio gives the ratio of the smallest to the largest value of the sequence, and NumLambda gives the length of the vector. – If you supply Lambda , lasso ignores LambdaRatio and NumLambda . Default: Geometric sequence of NumLambda values, the largest just sufficient to produce B = 0 So, if you're passing an explicit value for Lambda , it's probably too large. Otherwise, you'd expect a sequence of results for different values of Lambda , only the largest of which would return all zeros. If that's not what you're getting, then can you post the exact function call and maybe your X and Y to further debug? Update: Based on your comment, I see the problem: you're including the answer as a predictor! When I run your code, I get a 13 x 39 matrix B , where B(1:end-1, :) is all 0 and B(end, :) starts close to 1 and decreases as the regularization increases down to 0. This is just saying that the best fit is to take the answer, and scale it down a bit due to the regularization. :) Instead, take the answer out of your predictors: [B, FitInfo] = lasso(X(:, 1:end-1), y); will work, and give you predictions that make more sense. Update again: Okay, now that we've figured out you're not actually including the answer as a predictor, time to explain what's going on a little more. :) The LASSO results depend on a regularization parameter, called $\lambda$. The thing that it minimizes is $ \tfrac{1}{2} \| X b - y \|_2^2 + \lambda \| b \|_1$, which is half the mean-squared error in the predictions with linear weightings $b$, plus the sum of the absolute values of the elements in $b$. In a Bayesian setting, you can think of this as a model where $y \approx \sum_i b_i x_i$ (with Gaussian noise), but with a ( Laplace ) prior on the elements $b_i$ which results in many of them coming out as zero. The strength of that prior is set by $\lambda$. In other words, for high values of $\lambda$, LASSO will always give you a zero vector; for $\lambda \to 0$, as I mentioned before, it becomes the standard least squares fit. Since you often don't have any reason to favor a particular value of $\lambda$, lasso gives you a few different ones by default. In particular, it gives you the smallest value of $\lambda$ such that $b = 0$, and then various smaller values of $\lambda$ backing off in a geometric series. These are stacked up into a matrix B , and for these default values of the parameters it gives you 39 different values of $\lambda$. So, B(:, 1) is a result fairly close to least-squares; B(:, 2) is a result a little closer to having some zeros, and so on, up to B(:, end-1) which will have only very few (in this case, one) nonzero elements, and then B(:, end) is always all zeroes. Does that help?
