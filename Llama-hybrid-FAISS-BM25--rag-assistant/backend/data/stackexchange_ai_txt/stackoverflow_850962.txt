[site]: stackoverflow
[post_id]: 850962
[parent_id]: 845058
[tags]: 
I believe that a memory mapped file will be the fastest solution. I tried four functions: the function posted by the OP ( opcount ); a simple iteration over the lines in the file ( simplecount ); readline with a memory-mapped filed (mmap) ( mapcount ); and the buffer read solution offered by Mykola Kharechko ( bufcount ). I ran each function five times, and calculated the average run-time for a 1.2 million-line text file. Windows XP , Python 2.5, 2 GB RAM, 2 GHz AMD processor Here are my results: mapcount : 0.465599966049 simplecount : 0.756399965286 bufcount : 0.546800041199 opcount : 0.718600034714 Numbers for Python 2.6: mapcount : 0.471799945831 simplecount : 0.634400033951 bufcount : 0.468800067902 opcount : 0.602999973297 So the buffer read strategy seems to be the fastest for Windows/Python 2.6 Here is the code: from __future__ import with_statement import time import mmap import random from collections import defaultdict def mapcount(filename): with open(filename, "r+") as f: buf = mmap.mmap(f.fileno(), 0) lines = 0 readline = buf.readline while readline(): lines += 1 return lines def simplecount(filename): lines = 0 for line in open(filename): lines += 1 return lines def bufcount(filename): f = open(filename) lines = 0 buf_size = 1024 * 1024 read_f = f.read # loop optimization buf = read_f(buf_size) while buf: lines += buf.count('\n') buf = read_f(buf_size) return lines def opcount(fname): with open(fname) as f: for i, l in enumerate(f): pass return i + 1 counts = defaultdict(list) for i in range(5): for func in [mapcount, simplecount, bufcount, opcount]: start_time = time.time() assert func("big_file.txt") == 1209138 counts[func].append(time.time() - start_time) for key, vals in counts.items(): print key.__name__, ":", sum(vals) / float(len(vals))
