[site]: crossvalidated
[post_id]: 618346
[parent_id]: 617940
[tags]: 
The method recommended here is based on 'classic' Shapley value decomposition that, as has been noted frequently in the literature, is not scalable as it involves 'removing columns/features' and thus refits $2^f$ different models (where $f$ is the number of features). This method does not assume independence of featrues however and, in fact, is very useful for ascribing importance in conditions where features are correlated (this is often noted in the literature on 'Dominance Analysis' - on which the {domir} package used below is based). Shapley values, using the column dropping approach to Shapely values is also quite flexible as it is easy to drop multiple columns simultaneously. As an example, consider this random forest model implemented in R with the mtcars data from the {datasets} package. > set.seed(25) > randomForest::randomForest(mpg ~ ., data = mtcars) |> + predict() |> + yardstick::rsq_vec(mtcars$mpg, estimate = _) [1] 0.8489851 I set the seed to ensure the reproducibility of this result specifically, especially as the model is refit in runs below. It's standard importance metrics are also computed. > set.seed(25) > randomForest::randomForest(mpg ~ ., data = mtcars, importance = TRUE) |> + randomForest::importance() %IncMSE IncNodePurity cyl 10.902547 165.83137 disp 12.746522 239.73487 hp 12.443743 193.31249 drat 5.389437 58.19412 wt 13.574631 242.07634 qsec 2.536330 28.38492 vs 4.417597 35.49211 am 3.717252 14.87757 gear 3.698947 19.99795 carb 6.280669 39.68566 These metrics suggest wt and disp are most important (in that order). The alternative Shapley/dominance-based approach is computed below. > domir::domir( + terms(mpg ~ ., data = mtcars) |> formula(), + \(fml) { + set.seed(25) + randomForest::randomForest(fml, data = mtcars) |> + predict() |> + yardstick::rsq_vec(mtcars$mpg, estimate = _) + }, + .cdl = FALSE, .cpt = FALSE + ) Overall Value: 0.8489851 General Dominance Values: General Dominance Standardized Ranks cyl 0.12199217 0.14369177 4 disp 0.17183115 0.20239595 1 hp 0.14207064 0.16734174 3 drat 0.02690479 0.03169053 10 wt 0.15773333 0.18579046 2 qsec 0.03852428 0.04537686 9 vs 0.05023545 0.05917118 6 am 0.04421375 0.05207835 7 gear 0.03942196 0.04643421 8 carb 0.05605759 0.06602895 5 This approach suggests also that disp and hp are important but in that order and, if interested, the user can ask for .cdl = TRUE to see the progression of feature collinearity affects how each feature explains variance in mpg . Note that this version of importance results in an exact decomposition of the $R^2$ into components attributable to each feature. This is only possible when random elements in the model are held constant (i.e., the random forests have the same random seed). Using the .set argument, a user can also group features which includes and drops them all jointly. > domir::domir( + terms(mpg ~ ., data = mtcars) |> formula(), + \(fml) { + set.seed(25) + randomForest::randomForest(fml, data = mtcars) |> + predict() |> + yardstick::rsq_vec(mtcars$mpg, estimate = _) + }, + .set = + list(`grp 1` = ~ cyl + disp + wt, `grp 2` = ~ carb + hp, `grp 3` = ~ gear + am), + .cdl = FALSE, .cpt = FALSE + ) Overall Value: 0.8489851 General Dominance Values: General Dominance Standardized Ranks drat 0.05185996 0.06108465 6 qsec 0.05776773 0.06804328 5 vs 0.08047303 0.09478733 4 grp 1 0.33222563 0.39132092 1 grp 2 0.22981961 0.27069922 2 grp 3 0.09683914 0.11406459 3 Here, many of the most strongly correlated features are in grp 1 which results in the most important set of features. Note that grouping the features in this way affects how all the features' importance are computed as it affects the number of sub-models that are estimated and incorporated into the computations. Again, this approach is much less scale-able than alternative implementations that sample the sub-model space or impose assumptions. That said, the grouping of features can reduce the sub-model space substantially. Moreover, features of little interest can be put into the .all argument and lumped into a component of the $R^2$ that does not contribute at all to the sub-model space.
