[site]: crossvalidated
[post_id]: 301707
[parent_id]: 301694
[tags]: 
I found an important statement from Francisco S. Melo of the Institute for Systems and Robotics, Instituto Superior TÃ©cnico, Lisboa, PORTUGAL, which might help you: "Theorem 1.[...] the Q-learning algorithm, given by the update rule [math formula] converges w.p.1 to the optimal Q-function as long as [...] all state-action pairs be visited infinitely often ."[1] So, if you aim to have the optimal Q-function and thus to obtain the optimal policy, then the answer should be: yes, you need for each state all possible actions according to the above quote. However, I am not sure, if RL is the right tool here, because recall that in RL actions should bring you from one state to another state. I am not sure, if an action is taken on one account (I understood that your state is formed by a tuple (acc_id, some account attributes list) correct?), how would you go from that state to another? E.g. lets assume initial state is (id:0, {age:20, income:1000}) -> (do some action) -> Transition to state (id:2, {age:35, income:2000}). What about reading on recommendation systems, since your problem seems to be closely related to it, at least this is my impression? See [2]. It seems that you want to recommend an action based on some attributes of an account. Hope that helps a bit :). [1] http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf [2]Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions
