[site]: crossvalidated
[post_id]: 22649
[parent_id]: 22645
[tags]: 
You can't use standard cross-validation techniques on time series because the observations aren't independent. This may be the source of your problems. If you are randomly sampling to get your five folds for cross-validation, your CV statistics will be mistakenly low due to correlations across the folds. If your test set on which you calculate the final model is somehow more independent from those five folds (for example, maybe you split training/cv vs. test by time?) you will see poor performance on the test set. For instructions on properly handling time series cross-validation, see Rob Hyndman's overview of cross-validation (scroll down for time series coverage) and his example of time series cross-validation using R . Rob describes using a rolling origin for time series cross-validation that seems like it might work for you. Why are you seeing different results for your stratified vs. non-stratified cross-validations? In the unstratified case, it sounds like you are creating training and validation sets that are less correlated than in the stratified case, since you are splitting by time when doing unstratified cross-validation. This might lead to higher cross-validation error because of less correlation across the sets. In the stratified case, you say you have "various examples from different times mixed together." In this case, the training vs validation sets have more correlation, leading to lower training error, representative of overfitting rather than useful predictive power.
