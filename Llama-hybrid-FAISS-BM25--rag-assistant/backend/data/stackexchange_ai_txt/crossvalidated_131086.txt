[site]: crossvalidated
[post_id]: 131086
[parent_id]: 
[tags]: 
Using simulation to estimate type-I error in Bayesian Tests

When doing a bayesian test, it is possible to estimate a "type-I error" of the test procedure by generating data from the null-hypothesis and running the bayesian test procedure several times. While the use of Type-I errors does not make sense in a pure bayesian framework, this mixed methodology seems to work OK and is even recommended by FDA when using bayesian analysis in clinical trials. Are there any practical negative consequences of using this mixed methodology? E.g. cases where the estimated type-I error will be wrong? I'm thinking of this for instance in the context of Bayesian A/B-testing, where we have two different Bernoulli random variables, $A$ and $B$, and wish to find out whether $p_A > p_B$ or $p_A p_B | D) > 0.95$ or $P(p_A 0.95$, and then concluding whether $p_A > p_B$ or $p_A p_B | D)$.
