[site]: crossvalidated
[post_id]: 29623
[parent_id]: 29614
[tags]: 
Mind that in both tests, you test a completely different hypothesis with different assumptions. The results are not comparable, and that is a far too common mistake. In absolute risk you test whether the (average) difference in proportion differs significantly from zero. The underlying hypothesis in the standard test for this assumes that the differences in proportion are normally distributed. This might hold for small proportions, but not for large. Technically you calculate the following conditional probability : $$P( p_1 - p_2 = 0 | X )$$ with $p_1$ and $p_2$ the two proportions, and $X$ your explanatory variable. This is equivalent to testing the slope $b$ of the following model : $$p = a + b\cdot X + \epsilon$$ where you assume that $\epsilon \sim N(0,\sigma)$ . In relative risk you do something completely different. You test the odds of having a positive outcome based on the explanatory variable $X$ . So you calculate $$P\left( \log\left(\frac{p_1}{p_2}\right) = 0 | X \right)$$ which is equivalent to testing the slope in the following logistic model: $$\log\left(\frac{p}{1-p}\right) = a + b\cdot X + \epsilon$$ with $\log(\frac{p}{1-p})$ being the log of the odds. Note that this hypothesis is formulated in terms of the odds, and not proportions! So the assumptions of the model are also formulated in terms of the odds (or more exactly, the log of the odds). You're testing a different hypothesis. The reason why this makes a difference is given in Peter Flom's answer: a small difference in absolute risks can lead to a big value for the odds. So in your case it means that the proportion of people getting the disease don't differ substantially, but the odds of being in one group is significantly larger than the odds of being in the other group. That is perfectly sensible.
