[site]: crossvalidated
[post_id]: 398237
[parent_id]: 298409
[tags]: 
Based on basics of data science taught during my masters and my understanding of working in the finance modeling: First you collect your data for a specific vintage, say 2017 data. You divide this 70:30 or 80:20 into two sets: Train and Test 1. Train set - on which model is trained (maybe you're comparing models Random forest model performs better than Logistic) 2. Test set - on which model is selected (you see how the model performs on test set. You realize RF model is overfitted, i.e. Performance in test drops much more than Train, whereas Logistic is robust. You perform checks to make RF model more stable by going back to train and making finetuning parameters. or maybe You decide to go with logistic even though RF performed better on Train) i.e. Your model will be finally selected with the help of Test. 3. Model Validation - Final check, nothing should be used from this to change in the training. This could be a stability test for ex. Vintage 2018. This is called out-of-time validation. If this cannot be procured and you know it before hand, try to maybe remove two month of data e.g. Feb&August (this should be done carefully since certain months have different trends like December) as Validation and split rest into train: test 70:30. Else you can split the whole data randomly into 60:25:15 or any other combination. Usually the issue is loss of information when all three are coming from same dataset, so try to have enough observations.
