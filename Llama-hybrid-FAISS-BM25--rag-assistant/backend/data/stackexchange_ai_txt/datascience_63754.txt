[site]: datascience
[post_id]: 63754
[parent_id]: 63746
[tags]: 
I don't think there will be a definitive answer, but I suspect that you'll get better results using the averaging method rather than the padding method. One big problem with the padding method is that it's sensitive to word order. For example, the sentences "Gibbons are one type of ape" and "One type of ape is the Gibbon" look very different if we do a word-by-word comparison. "Gibbons" is very different than "One", "are" is very different than "type", etc. This problem will be offset slightly because RoBERTa embeddings are context-sensitive, but you get the idea. Another big problem is that the padding can dominate the similarity comparison for sentences with different lengths. Suppose we want to compare the following two sentences which have very similar meaning: Sentence1: "Very furry" would be an apt description for most mammalian species. Sentence2: Mammals have hair. We would add 8 padding tokens to sentence2 in order to give it the same length as sentence1. But then the similarity computation is dominated by the padding tokens and not by the actual content of the sentence. To me, the averaging approach seems superior because it avoids these two problems. Of course it also has drawbacks. The "sentence embeddings" obtained by averaging the word embeddings will be noisy because each word is given equal weight. You could try running a keyword detector over the sentences you wish to compare. Then compare the means of the top-k keywords instead of the whole sentence.
