[site]: crossvalidated
[post_id]: 354442
[parent_id]: 316523
[tags]: 
Linear, single-layer FFNs are non-identified The question as since been edited to exclude this case; I retain it here because understanding the linear case is a simple example of the phenomenon of interest. Consider a feedforward neural network with 1 hidden layer and all linear activations. The task is a simple OLS regression task. So we have the model $\hat{y}=X A B$ and the objective is $$ \min_{A,B} \frac{1}{2}|| y - X A B ||_2^2 $$ for some choice of $A, B$ of appropriate shape. $A$ is the input-to-hidden weights, and $B$ is the hidden-to-output weights. Clearly the elements of the weight matrices are not identifiable in general, since there are any number of possible configurations for which two pairs of matrices $A,B$ have the same product. Nonlinear, single-layer FFNs are still non-identified Building up from the linear, single-layer FFN, we can also observe non-identifiability in the nonlinear , single-layer FFN. As an example, adding a $\tanh$ nonlinearity to any of the linear activations creates a nonlinear network. This network is still non-identified, because for any loss value, a permutation of the weights of two (or more) neurons at one layer, and their corresponding neurons at the next layer, will likewise result in the same loss value. In general, neural networks are non-identified We can use the same reasoning to show that neural networks are non-identified in all but very particular parameterizations. For example, there is no particular reason that convolutional filters must occur in any particular order. Nor is it required that convolutional filters have any particular sign, since subsequent weights could have the opposite sign to "reverse" that choice. Likewise, the units in an RNN can be permuted to obtain the same loss. See also: Can we use MLE to estimate Neural Network weights?
