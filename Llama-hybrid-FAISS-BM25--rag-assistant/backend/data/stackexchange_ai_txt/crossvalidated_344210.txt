[site]: crossvalidated
[post_id]: 344210
[parent_id]: 
[tags]: 
How to prove subadditivity of joint entropy?

I've been kind of stuck on this exercise for the last hours. Heres everything I know and everything I'm allowed to use to prove it: Let $X= \{w_1, \ldots, w_n \}$ and $Y = \{v_1, \ldots, v_n \}$ be two discrete random variables with $\text{wlog } |X| \leq |Y|$ . Let the joint entropy be defined as: $$ H(X,Y) = - \sum_{i=0}^n \sum_{j=0}^m p_{i,j} \log(p_{i,j}) $$ I dont know if $X $ and $Y$ are dependent or independent. All I know is that $p_{i,j} = Pr[X=w_i \land Y=v_j]$ Lemma of Gibbs is all I have; I don't know if I'm allowed to use conditional probabilities. The assignment was to prove the subadditivity of the joint entropy of $X$ and $Y$ : $$H(X,Y) \leq H(X) + H(Y) $$ Full solutions discouraged, please post hints, I believe I'd learn more that way.
