[site]: crossvalidated
[post_id]: 242794
[parent_id]: 242772
[tags]: 
I'm going to assume that your 1000 explanatory variables are highly collinear, as is often the case with omics data (or so I hear). This is to say that many of your variables are highly correlated with one another. When you have this situation, linear models (including logistic regression) have very unstable (variable) parameters . One solution is regularization . This will force your coefficients to shrink, and only the most important ones will remain. L1 regularization can penalize coefficients down to zero. The downside is that your coefficient estimates are biased, and you won't be able to interpret the partial effects as such, like you would in an un-regularized model. These methods are good for variable selection. Another approach might be to fit a random forest , and then look at the variable importances. These will tell you how much your predictive model suffers if a given variable is omitted, but they won't tell you what the marginal effect of that variable is on the outcome. Furthermore, they don't assume, like L1-regularized linear models do, that the effect of the X's on y is a (transformed) linear function. A textbook to get you started is here .
