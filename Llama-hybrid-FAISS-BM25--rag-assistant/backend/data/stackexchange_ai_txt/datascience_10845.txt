[site]: datascience
[post_id]: 10845
[parent_id]: 10706
[tags]: 
I think the following synthetic example explains what is going on: from sklearn.base import clone from sklearn.svm import SVC import numpy as np from sklearn.datasets import make_blobs X, y = make_blobs(1000, centers=[[1, 1], [-1, -1], [1, -1]], cluster_std=0.8) models = [ ('normal', SVC()), ('balanced', SVC(class_weight='balanced')), ] import matplotlib.pyplot as plot plot.ioff() for i, (name, m) in enumerate(models): for k in np.unique(y): m = clone(m).fit(X, (y == k).astype(int)) xx, yy = np.meshgrid(np.arange(-3, 3, 0.1), np.arange(-3, 3, 0.1)) z = m.predict(np.c_[xx.ravel(), yy.ravel()]) z = z.reshape(xx.shape) colors = ['blue', 'green', 'red'] plot.contour(xx, yy, z, colors=colors[k]) plot.scatter(X[:, 0], X[:, 1], c=y) plot.title(name) plot.show() The balanced SVM is clearly superior when considering each model individually, at least as measured by the F1 score, which punishes false positives and false negatives equally. But One-vs-Rest uses maximum score, which in this case is given by the distance to the decision hyperplane. It is not yet clear why this is worse in some cases, but clearly it is possible for one model to beat another for individual classes, but to lose for One-vs-Rest: how well the relative distances are modeled is what counts in One-vs-Rest.
