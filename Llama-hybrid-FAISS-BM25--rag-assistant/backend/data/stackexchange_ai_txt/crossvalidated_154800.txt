[site]: crossvalidated
[post_id]: 154800
[parent_id]: 
[tags]: 
When does it makes sense to use Cross Validation?

My understanding is that cross validation is about using different chunks of the training data to train the model and average out the error estimation so that the variance is less. For example, in Leave One Out Cross Validation, we use 1 row for test n-1 rows for training. This is slow and we prefer 10 fold cross validation where the data is divided into 10 chunks where each chunk serves as validation data while rest will be used for training. Where I don't get the concept is when should I be using Cross Validation ? I am working on Kaggle's Bike Sharing Demand data and I have used Linear regression, Random Forest as well as Generalized Boost models. Out of those, so far Random Forest gives me lowest Root Mean Squared Log Error. Now, in this scenario, will it make sense to do 10 fold cross validation for each model ?
