[site]: stackoverflow
[post_id]: 4784898
[parent_id]: 4780318
[tags]: 
A data structure that can efficiently compute and cache all those values is going to be quite complex. I'm not certain that any database system is able to do this without iterating over subsets. Intersection is a notoriously hard operation, and CouchDB doesn't really have anything available to handle intersection properly. As you correctly identified, emitting all permutations ( subsets , to be precise) is going to be a memory hog because it's still going to multiply your items by a huge factor (2 n key-value pairs for n sets). You can reduce this by collapsing prefixes together (the CouchDB key structure lets you retrieve the values for ["A"] and ["A","B"] when you emit for ["A","B","C"] using the group level option) but only by a factor of 2 (2 n-1 key-value pairs for n sets). So, if your items have on average three associated sets, you're going to be fine (4 key-value pairs instead of 3), but four associated sets is heavier (8 instead of 4) and five is starting to get annoying (16 instead of 5). This also makes items with many associated sets vulnerable to performance issues (a 10-set item would create more than 500 key-value pairs). A middle ground approach would be to emit keys up to four sets in length (it merely doubles the required memory), and run some application-side processing when a deeper intersection is required (grab all items without reduction, run the reduction within the application). With some luck, the number of concerned items will be smaller - if it isn't, you can always use the maximum set size to sacrifice more memory for more performance. An opposite approach would be to have the application update 2 n totals when every document is inserted/updated (by fetching all "totals" documents that match a subset of the current item). Those totals would be stored in a different database and would be queried by key. This approach is better if you can afford to be updating totals on-the-fly (or your architecture lets you update them by listening to the updates in the main database), as it makes the queries lightning-fast.
