[site]: crossvalidated
[post_id]: 593380
[parent_id]: 518167
[tags]: 
Regression trees (recursive partitioning) make these main assumptions: Sample size is huge (say > 100,000, depending on the number of candidate features) so that the tree structure has some stability Relationships between continuous predictors and outcome are piecewise flat (as stated earler) Assumption 2. is never seen in nature so you know already that regression trees have poor fit to the data, especially if there are any strong continuous predictors. Assumption 1 is something you already know. The failure of individual trees to be competitive at prediction and explanation is why random forests, bagging, and boosting exist. Here is a paper on sample sizes needed for trees: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-137
