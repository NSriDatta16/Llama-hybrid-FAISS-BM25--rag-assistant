[site]: crossvalidated
[post_id]: 351982
[parent_id]: 351963
[tags]: 
As others already noticed, to answer the question "how good" is your recommender, you would need some kind of benchmark. How did you recommend the products before? If you had some kind of recommender system, use it as your benchmark. If you didn't, there are several possible choices for benchmarks: randomly recommended products, recommending the same most popular product (especially if the distribution of product purchases is very skew), the last product bought by each user as a recommendation. Each of the scenarios can serve as a benchmark of a naive recommender system, if you can do better then those scenarios, then your recommender is worth considering. Notice that such tests can be partly done with dry run: simply make your recommendations based on data until month $t$ and assign "recommendation" scores for products in month $t+1$ (in the past), so to check what did the users buy at $t+1$ time. This doesn't account for your marketing efforts (you didn't send or show them the recommendations!), so this wouldn't give you final answer, but it is a good starting point. If I were you, I'd also think of using different metric for assessing the recommender performance. The two alternative choices for measuring performance of recommender systems with implicit ratings (users did not state preferences explicitly), are Mean Percentage Ranking (MPR) and Mean Reciprocal Rank (MRR) . The first metric is defined in terms of percentile ranks, where $rank_{ui}$ is rank of $i$-th product for $u$-th user, where the ranks range from $0\%$ (most preferred) to $100\%$ (least preferred), and in terms of indicator function $d_{ui}$ that is equal to $1$ is $i$-th product was bought (clicked etc.) by $u$-th user and $0$ otherwise, $$ MPR = \frac{\sum_{u,i} d_{ui} \times rank_{ui}}{\sum_{u,i} d_{ui}} $$ as you can see, the measure is simply mean of the percentile ranks that were bought by the users. $MPR=0\%$ means that your recommender is perfect, $MPR=50\%$ is the score that you'd see for recommending random products and $MPR=100\%$ is the score you'd achieve if all your recommendations were wrong. This score has very clear interpretation, so it may be useful. Moreover, notice that if you have $k$ products, then $MPR \times k$ is the average position in ranking of the products that were bought, so if $MPR \times k = 3$ and you recommend $4$ product for each user, then you should be happy with the results. The second measure is defined in terms of ranks $rank_{ui}$, where $1$ is the most recommended product and $k$ is least recommended product, $$ MRR = \frac{1}{n} \sum_u \min_{i \in \{ i \,\mid\, purchase_{ui} > 0\}}(\,rank_{ui}\,)^{-1} $$ The score is calculated by taking average of the lowest ranks of the product that was bought by users . The higher score the better, where $MRR=1$ means perfect matching. This score has is defined in terms of harmonic mean , so $\tfrac{1}{MRR}$ gives you the position of the product with lowest rank that was bought. I find those two metrics more intuitive and easier to interpret the precision at $k$ and would recommend trying them. I wouldn't agree with the advice that you could compare your recommender to Amazon, Netflix, Spotify etc. This generally doesn't make much sense unless you have similar product and users (I guess you don't). If you have different products and different users, then there is no reason to expect similar performance scores. Notice that even if you had same products and users, then still your platform may differ, you may have different marketing strategies and tools, different prices, availability etc., so lots of factors may influence your performance.
