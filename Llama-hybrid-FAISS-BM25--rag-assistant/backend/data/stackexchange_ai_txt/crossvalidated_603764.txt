[site]: crossvalidated
[post_id]: 603764
[parent_id]: 603535
[tags]: 
In the tutorial you refer to , the only reason is simplicity. However, in machine translation practice, there is typically no reason to use different maximum lengths. Transformers often use learned position embeddings (instead of the sinusoidal position encoding in the original Transformer paper ). In that case, limiting the source or target side more would mean less training signal for the position embeddings. Also, due to byte-pair encoding and similar algorithms used for input text segmentation, the length of the source and target sentences are typically similar, even for languages that typically use a different number of words to express the same thing.
