[site]: datascience
[post_id]: 68223
[parent_id]: 68220
[tags]: 
These matrices are not learned parameters but are a result of previous (yet parameterized) computations. In self-attentive layers, are all three of them the same, they are the outputs of the previous layers. In encoder-decoder attention, the queries are decoder states from the previous layer, keys and values and the encoder states. In Equation 1 of the Attention is all you need paper, these are just parameters that come from outside: It just says, what do you do, if get some queries, keys, and values from somewhere outside. This also the case of the unnumbered equation on top of page 5. Here, you only project the keys, queries, and values for the heads of multiple attentions. Here $W_i^Q$ , $W_i^K$ , and $W_i^V$ are learnable parameters and they learned by the standard back-propagation algorithm. Note that although keys and values (and queries in the self-attention) are equal only at the input of the $\text{Multihead}$ function. The $\text{Attention}$ function already gets the projected states.
