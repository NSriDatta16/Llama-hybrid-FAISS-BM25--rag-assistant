[site]: datascience
[post_id]: 112388
[parent_id]: 112385
[tags]: 
Consider the following image of a simple neural network. Note that the network uses a linear activation function and that there are no bias terms (this makes the intuition easier). Each path from the input to the output is as follows $$ f(x) = (x*w1)*w4 = (x*0.5)*0.5 $$ $$ f(x) = (x*w2)*w5 = (x*0.5)*0.5 $$ $$ f(x) = (x*w3)*w6 = (x*0.5)*0.5 $$ When you perform gradient descent, the change in the weights will always be the same as each path is identical.
