[site]: crossvalidated
[post_id]: 568253
[parent_id]: 568238
[tags]: 
Squared error used for classification problems is called Brier score and same as log-loss is a strictly proper scoring rule , i.e. it leads to producing well-calibrated probabilities . It is perfectly fine to use squared error as a loss function for classification. This issue was studied by Hui and Belkin (2020) , who conclude: We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy. You may notice in Section 5 of the paper some technical considerations that the authors found to improve training. Check also the why sum of squared errors for logistic regression not used and instead maximum likelihood estimation is used to fit the model? and What is happening here, when I use squared loss in logistic regression setting? threads.
