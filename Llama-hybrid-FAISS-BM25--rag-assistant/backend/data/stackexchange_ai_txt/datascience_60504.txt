[site]: datascience
[post_id]: 60504
[parent_id]: 60287
[tags]: 
Ans 1: get_dummies() or (label encoder + one-hot encoder) would do the trick. Ans 2: Scaling categorical dummy data does not make sense. It also loses out on interpretability. Ans 3: Logistic regression might tend to overfit since you only have 180 observations. KNN might perform well with a small number of observations but it doesn't handle categorical variables well. Random Forest or Extreme Random Forest might be your best bet. However, there's no "one fits all" concept in ML. You'll have to try a variety of algorithms and see which fits your dataset best. Though there are Linear SVM and other algorithms, I'd suggest that you try simpler ones as otherwise it might be an overkill. Good luck!
