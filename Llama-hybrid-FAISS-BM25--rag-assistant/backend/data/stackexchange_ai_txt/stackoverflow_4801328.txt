[site]: stackoverflow
[post_id]: 4801328
[parent_id]: 4801284
[tags]: 
I suspect the problem is that you have so much data stored in string format, which is really wasteful for your use case, that you're running out of real memory and thrashing swap. 128 GB should be enough to avoid this... :) Since you've indicated in comments that you need to store additional information anyway, a separate class which references a parent string would be my choice. I ran a short test using chr21.fa from chromFa.zip from hg18; the file is about 48MB and just under 1M lines. I only have 1GB of memory here, so I simply discard the objects afterwards. This test thus won't show problems with fragmentation, cache, or related, but I think it should be a good starting point for measuring parsing throughput: import mmap import os import time import sys class Subseq(object): __slots__ = ("parent", "offset", "length") def __init__(self, parent, offset, length): self.parent = parent self.offset = offset self.length = length # these are discussed in comments: def __str__(self): return self.parent[self.offset:self.offset + self.length] def __hash__(self): return hash(str(self)) def __getitem__(self, index): # doesn't currently handle slicing assert 0 = size: yield buffer[:size] buffer = buffer[1:] def parse_os_read(file, size=8): file.readline() # skip header file_size = os.fstat(file.fileno()).st_size whole = os.read(file.fileno(), file_size).replace("\n", "").upper() for offset in xrange(0, len(whole) - size + 1): yield whole[offset:offset+size] def parse_mmap(file, size=8): file.readline() # skip past the header buffer = "" for line in file: buffer += line if len(buffer) >= size: for start in xrange(0, len(buffer) - size + 1): yield buffer[start:start + size].upper() buffer = buffer[-(len(buffer) - size + 1):] for start in xrange(0, len(buffer) - size + 1): yield buffer[start:start + size] def length(x): return sum(1 for _ in x) def duration(secs): return "%dm %ds" % divmod(secs, 60) def main(argv): tests = [parse, parse_sep_str, parse_tuple, parse_plain_str, parse_orig, parse_os_read] n = 0 for fn in tests: n += 1 with open(argv[1]) as f: start = time.time() length(fn(f)) end = time.time() print "%d %-20s %s" % (n, fn.__name__, duration(end - start)) fn = parse_mmap n += 1 with open(argv[1]) as f: f = mmap.mmap(f.fileno(), 0, mmap.MAP_PRIVATE, mmap.PROT_READ) start = time.time() length(fn(f)) end = time.time() print "%d %-20s %s" % (n, fn.__name__, duration(end - start)) if __name__ == "__main__": sys.exit(main(sys.argv)) 1 parse 1m 42s 2 parse_sep_str 1m 42s 3 parse_tuple 0m 29s 4 parse_plain_str 0m 36s 5 parse_orig 0m 45s 6 parse_os_read 0m 34s 7 parse_mmap 0m 37s The first four are my code, while orig is yours and the last two are from other answers here. User-defined objects are much more costly to create and collect than tuples or plain strings! This shouldn't be that surprising, but I had not realized it would make this much of a difference (compare #1 and #3, which really only differ in a user-defined class vs tuple). You said you want to store additional information, like offset, with the string anyway (as in the parse and parse_sep_str cases), so you might consider implementing that type in a C extension module. Look at Cython and related if you don't want to write C directly. Case #1 and #2 being identical is expected: by pointing to a parent string, I was trying to save memory rather than processing time, but this test doesn't measure that.
