[site]: crossvalidated
[post_id]: 364254
[parent_id]: 23143
[tags]: 
I agree with the previous answer but here are my reservations. It is advisable to remove duplicates while segregating samples for training and testing for specific classifiers such as Decision Trees. Say, 20% of your data belonged to a particular class and $\frac{1}{4}^{th}$ of those seeped into testing, then algorithms such as Decision Trees will create gateways to that class with the duplicate samples. This could provide misleading results on the test set because essentially there is a very specific gateway to the correct output. When you deploy that classifier to completely new data, it could perform astonishingly poor if there are no samples similar to the above said 20% samples. Argument : One may argue that this situation points to a flawed dataset but I think this is true to real life applications. Removing duplicates for Neural Networks, Bayesian models etc is not acceptable.
