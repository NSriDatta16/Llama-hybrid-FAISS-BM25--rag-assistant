[site]: crossvalidated
[post_id]: 641277
[parent_id]: 
[tags]: 
Smoothness of a neural network (specifically second-order)

If we use ReLU activations, then the function which our neural network represents is piecewise linear. It is not smooth and the first derivative doesn't exist everywhere. However, if we use sigmoid or tanh activations, then the function represented by the neural network is smooth. The first derivative exists everywhere and is continuous. I am interested to know whether we can say anything about the second derivative of a neural network with sigmoid/tanh activations. Are there any guarantees we can make about whether or not it exists everywhere? Or is this something which doesn't have a simple answer?
