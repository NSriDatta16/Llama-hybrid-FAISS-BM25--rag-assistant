[site]: crossvalidated
[post_id]: 79206
[parent_id]: 79202
[tags]: 
The key difference is in the training criterion. A least squares training criterion is often used for regression as this gives (penalised) maximum likelihood estimation of the model parameters assuming Gaussian noise corrupting the response (target) variable. For classification problems it is common to use a cross-entropy training criterion, to give maximum likelihood estimation assuming a Bernoilli or multinomial loss. Either way, the model outputs can be interpreted as estimate of the probability of class membership, but it is common to use logistic or softmax activation functions in the output layer so the outputs are constrained to lie between 0 and 1 and to sum to 1. If you use the tanh function, you can just remap these onto probabilities by adding one and dividing by two (but it is otherwise the same). tanh is a good choice for hidden layer activation functions. The difference between scaled conjugate gradients and Levenberg-Marquardt are likely to be fairly minor in terms of generalisation performance. I would strongly recommend the NETLAB toolbox for MATLAB over MATLABs own neural network toolbox. It is probably a good idea to investigate Bayesian regularisation to avoid over-fitting (Chris Bishop's book is well worth reading and most of it is covered in the NETLAB toolbox).
