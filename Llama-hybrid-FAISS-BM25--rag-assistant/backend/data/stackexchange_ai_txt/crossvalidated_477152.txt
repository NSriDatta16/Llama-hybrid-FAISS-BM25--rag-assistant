[site]: crossvalidated
[post_id]: 477152
[parent_id]: 
[tags]: 
Using cross-entropy for regression problems

I usually see a discussion of the following loss functions in the context of the following types of problems: Cross entropy loss (KL divergence) for classification problems MSE for regression problems However, my understanding (see here ) is that doing MLE estimation is equivalent to optimizing the negative log likelihood (NLL) which is equivalent to optimizing KL and thus the cross entropy. So: Why isn't KL or CE used also for regression problems? What's the relationship between CE and MSE for regresion? Are they one and the same loss under some circumstances? If different, what's the benefit of using MSE for regression instead? Related questions: Does the cross-entropy cost make sense in the context of regression? Do neural networks learn a function or a probability density function? How to construct a cross-entropy loss for general regression targets?
