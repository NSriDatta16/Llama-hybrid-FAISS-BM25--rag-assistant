[site]: crossvalidated
[post_id]: 541695
[parent_id]: 539798
[tags]: 
I would say the main reason is smoothness. When an outlier leaves the moving window of the simple average, it may lead to a big move in the parameter. EWMA produces smoother parameters in this case because rather than dropping off, the outlier's influence gradually declines. In case of batch norm I don't see the reason why the last batch should have the higher influence than previous ones. It is uncommon to feed less relevant batches first into the queue. You would expect that batches are random, and their influence shouldn't be different. The memory and computational complexity argument for fixed moving window size is not relevant here either. Both are negligible compared to all other stuff you store in RAM and calculate in each batch. These are just drops in bucket. If instead of the moving window you'd like to use entire training sample then this argument becomes completely irrelevant due to recurrent formulas used for computation of means and variance.
