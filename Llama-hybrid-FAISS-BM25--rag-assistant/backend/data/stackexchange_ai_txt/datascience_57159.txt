[site]: datascience
[post_id]: 57159
[parent_id]: 
[tags]: 
Do pseudo r squared metrics make sense for classifiers that aren't logistic regression?

I'm working with some domain scientists that are used to using logistic regression to predict a binary value. One of the ways they evaluate their logistic regression model is through the Nagelkerke $r^2$ value, which attempts to measure how much of the variance of the dataset is explained by the model. They're now trying to compare the performance of their logistic regression model to some machine learning models (random forest, for example), and they'd like to calculate the $r^2$ value for the machine learning models. Is this possible? Is there a good reason this wouldn't make sense to do? The Nagelkerke $r^2$ value is a rescaled version of the Cox and Snell $r^2$ , which is calculated using the likelihood of a fitted logistic regression model vs. the likelihood of a null logistic regression model, according to wikipedia. $r^2_{CS} = 1 - (\frac{L_0}{L_M})^{2/n}$ where $L_0$ is the likelihood of the null model and $L_M$ is the likelihood of the fitted model. In my mind, I can calculate likelihood (or some proxy) for machine learning models like a random forest, so I should be able to compare it to a null logistic regression model, right? I can at least get an estimation of the probability of each class out of a random forest model, which I should be able to use to calculate something like the likelihood. Is that a bad idea, though? I was surprised to not find much on this with a bit of googling.
