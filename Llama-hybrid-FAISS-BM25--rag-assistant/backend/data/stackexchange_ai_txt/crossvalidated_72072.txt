[site]: crossvalidated
[post_id]: 72072
[parent_id]: 72068
[tags]: 
This is a really interesting question; I'm assuming that you are using a maximum likelihood framework, where the likelihood of of a particular word $w_i$ appearing in a document having label $L$ is simply given by $p(L|w_i) = \frac{count(L,w_i)}{count(w_i)}$. Now, part of the reason that this problem arises is because of the use of this maximum likelihood framework, where the likelihood that a document has label $L$ can only be non-zero when all of the words in the constituent document have appeared at least once in documents with label $L$ in our observations before. In that sense, your derivation is correct; when working with a maximum likelihood framework, it is entirely reasonable to assign a document zero probability of having label $L$ even if only one of its words has never had label L before. That said, this is obviously quite unsatisfactory for our purposes. One, on a practical level, it is rather evidently not true. And two, it rather seriously affects our ability to score and classify documents based on the evidence. I'll talk about each one briefly. The first objection - that is, that on a practical level it doesn't really work as we've formulated - is entirely the result of the fact that the maximum likelihood approach is a frequentist one. In a Bayesian treatment, these probabilities might be very small, but would presumably be non-zero as the result of the inclusion of some reasonable prior. Despite that caveat, can still use this framework for prediction, bringing us to number two. One approach is to do what you mentioned; that is, to entirely discard the probabilistic framework and simply use the observed counts as scoring features, allowing ranking of particular labels for a given document. In this formulation, simply label a given document $D$ via the procedure: $label(D)= arg max_{l \in L}[\sum_{w_i \in D} P(l|w_i)]$ This is nicely computationally tractable, of course, and uses the counts we've already gathered. Note, by the way, that despite the use of the observed MLE probabilities for scoring, the quantity yielded by this is NOT normalized, and thus not a probability distribution. However, this approach does have shortcomings - for example, it does not deal with the fact that the probability distribution over the labels in $L$ is likely not uniform. There are a number of ways to treat this; a commonly used approach which would be useful here is the Naive Bayes Classifier , which nicely adds estimates over the label observations. Lastly, given the pairs that you have, and the number of irrelevant features that are appearing in each document (ie, words that have no real use in classification, like "the"), you might look into linear models such as Winnow , which is extendable to multiple classes and is designed for dealing with large numbers of irrelevant features. I hope that helps!
