[site]: crossvalidated
[post_id]: 358164
[parent_id]: 357811
[tags]: 
In a 2014-2017 paper published in Bayesian Analysis, Peter Gr√ºnwald and Thijs van Ommen show that Bayesian inference can be inconsistent under misspecification in simple linear regression problems and propose this powering-down by a Learning rate $\eta$ to recover consistency, that is, to achieve convergence to the model within the assumed family that is closest (in the Kullback-Leibler metric) to the true model. Here, misspecification is understood as heteroskedasticity, that is, dependence of the variance on the regressors. There are more precise results in the literature, as for instance the rate at which the pseudo-posterior concentrates is minimax optimal, i.e. no algorithm can dobetter in general . Choosing the power $\eta$ is operated by minimising the log-loss we expect to obtain, according to the $\eta$-generalized posterior, if we actually sample from this pseudo-posterior. Here is a figure reproduced from this paper that shows the improvement brought by this technique compared with regular Bayes, when the assumed model is wrong, as the sample size grows. Consistency happens. This notion is connected with a larger literature on PAC-Bayesian techniques (Seeger (2002), Catoni (2007), Bissiri et al. (2016)).
