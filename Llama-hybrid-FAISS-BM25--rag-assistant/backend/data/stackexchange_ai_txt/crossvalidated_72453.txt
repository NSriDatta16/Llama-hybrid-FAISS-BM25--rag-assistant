[site]: crossvalidated
[post_id]: 72453
[parent_id]: 
[tags]: 
How to compare two groups with multiple measurements for each individual with R?

I have a problem like the following: 1) There are six measurements for each individual with large within-subject variance 2) There are two groups (Treatment and Control) 3) Each group consists of 5 individuals 4) I want to perform a significance test comparing the two groups to know if the group means are different from one another. The data looks like this: And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. This ignores within-subject variability : n.simulations And here is code for plots: #Plots par(mfrow=c(2,2)) boxplot(GroupA[,2]~GroupA[,1], col="Red", main="Group A", ylim=c(.9*min(out[,2]),1.1*max(out[,2])), xlab="Subject", ylab="Value") stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T) #abline(h=mean(GroupA[,2]), lty=2, lwd=3) for(i in 1:length(unique(GroupA[,1]))){ m Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data. So what is the correct way to analyze this data? Bonus: The example above is a simplification. For the actual data: 1) The within-subject variance is positively correlated with the mean. 2) Values can only be multiples of two. 3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 4) Number of Subjects in each group are not necessarily equal. Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the "correct" p-values. EDIT: Ok, here is what actual data looks like. There is also three groups rather than two: dput() of data: structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list( NULL, c("Group", "Subject", "Value"))) EDIT 2: In response to Henrik's answer: So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those "stars of authority" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM. #Get Invidual Means summary=NULL for(i in unique(dat[,2])){ sub EDIT 3: I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane: #Get Subject Means means
