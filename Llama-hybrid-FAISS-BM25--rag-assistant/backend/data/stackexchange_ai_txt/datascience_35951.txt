[site]: datascience
[post_id]: 35951
[parent_id]: 
[tags]: 
Why do we need activation function (like ReLU) after an affine layer?

In Convolutional Neural Networks, assume the input and the output of the affine layer are $x$ and $y$, respectively. This affine operation $y = W^{\top} x + b$ has already add non-linearity to the system given that $b \neq 0$. Why do we still need a function like ReLU to add non-linearity to the system?
