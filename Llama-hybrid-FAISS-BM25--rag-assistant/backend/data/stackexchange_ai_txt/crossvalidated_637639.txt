[site]: crossvalidated
[post_id]: 637639
[parent_id]: 
[tags]: 
Measuring features effect and importance in Partial Least Square (PLS) regression

Context: it is possible to assess features importance and effect for a model using model-independent scoring techniques such as Partial Dependence (PD) profile, Acculumated Local Effect (ALE) profile, H-Statistic and measure of increase in loss after feature permutation. PLS is an algorithm that allow to start from a dataset with correlated features and transform it in a space of lower dimension trying to reduce correlation between the "new" dimensions. My goal is to compare feature importance and effect of a PLS, a LASSO and a Random Forest model using a common technique. Problem: the scoring techniques rely on the fact that features are not correlated (except for the ALE); they are based on ceteris paribus , i.e. changing the value of a feature while keeping the others constant for a given individual. I am worried to make a mistake using them for the PLS. Question: how to use partial dependence & co. techniques for the PLS algorithm? Remarks: For other reasons, I had process through iterative feature eliminations for the LASSO and the Random Forest models so the question does not apply to them here. I would like to avoid using the ALE profile. If I could use the partial dependence and related measures like the H-Statistic and Individual Conditional Expectations (ICE) profile (also named Ceteris Paribus profile), it would make the identification of interactions much easier. Source: Explanatory Model Analysis: https://ema.drwhy.ai/ Interpretable Machine Learning: https://christophm.github.io/interpretable-ml-book/ Disclaimer: question crossposted on Data Science ( https://datascience.stackexchange.com/q/126560/158251 ) as I do not know which community is the most relevant for it.
