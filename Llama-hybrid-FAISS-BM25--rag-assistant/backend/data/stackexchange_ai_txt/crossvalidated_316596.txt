[site]: crossvalidated
[post_id]: 316596
[parent_id]: 
[tags]: 
Understanding Proximal Policy Optimization

I'm a beginner in Reinforcement Learning, and have been learning about Policy Optimization methods like Proximal Policy Optimization and Trust Region Policy Optimization. I had a couple questions about PPO: Am I correct in my understanding that the surrogate objective is clipped since the surrogate loss is only a local approximation of the expected return of the policy? If so, why is the minimum taken of the clipped surrogate objective and the surrogate objective itself? Doesn't this allow for large updates to the policy, the very issue PPO and TRPO is trying to prevent? Papers/lectures on these algorithms emphasize the surrogate objective being a pessimistic/lower bound on the actual expected return of the policy. Why should this be the case? Rather than having the function itself be a lower bound of the expected return of the policy, shouldn't steps be taken to ensure that the gradient of the surrogate objective is a pessimistic bound of the gradient of the expected return of the policy? Wouldn't this be the way to solve the problem of taking too large steps to optimize the policy? Thanks for any help!
