[site]: crossvalidated
[post_id]: 405352
[parent_id]: 
[tags]: 
Bayesian approach: ignoring the denominator leads to the conditional density equaling the joint density?

I know there are a lot of questions here about ignoring the denominator in a Bayesian approach, but I don't think mine is a duplicate of any of them. I am reading the book "Pattern recognition and machine learning" by Cristopher Bishop. Imagine we have a set of N observations of a (single) variable, which we collect in a vector $\mathbf{x} \in \mathcal{R}^N$ . We would like to find the mean $\mu$ of the probbility density function that generated that data, using a Bayesian approach. Thus, we first need to find the posterior probability $p(\mu|\mathbf{x})$ We can write: $p(\mu|\mathbf{x}) = p(\mathbf{x}|\mu) \cdot \dfrac{p(\mu)}{p(\mathbf{x})}$ Now as the book says, we can ignore the denominator because it is just a normalizing factor $p(\mu|\mathbf{x}) \propto p(\mathbf{x}|\mu) \cdot p(\mu) = p(\mathbf{x}, \mu) $ where the last equation follows from the product rule , or the defition of conditional density for $p(\mathbf{x}|\mu)$ if you want. So we are approximating a conditional distribution with a joint distribution? How is that even possible? For one, $p(\mu|\mathbf{x})$ whould be a function of $\mathbf{x}$ , while $p(\mathbf{x}, \mu) $ whould be a function of both $\mathbf{x}$ and $\mu$ , right?
