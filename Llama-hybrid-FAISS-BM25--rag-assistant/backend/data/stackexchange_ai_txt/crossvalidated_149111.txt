[site]: crossvalidated
[post_id]: 149111
[parent_id]: 149110
[tags]: 
The following is based on simple cross sections, for time series and panels it is somewhat different. In the population, and therefore in the sample, the model can be written as: \begin{align} \newcommand{\Var}{\rm Var} \newcommand{\Cov}{\rm Cov} Y &= \beta_0 + \beta_1 x_1 + … + \beta_k x_k + u \\ &= X\beta + u \end{align} This is the linearity assumption, which is sometimes misunderstood. The model should be linear in the parameters - namely the $\beta_k$. You are free to do whatever you want with the $x_i$ themselves. Logs, squares etc. If this is not the case, then the model cannot be estimated by OLS - you need some other nonlinear estimator. A random sample (for cross sections) This is needed for inference, and sample properties. It is somewhat irrelevant for the pure mechanics of OLS. No perfect Collinearity This means that there can be no perfect relationship between the $x_i$. This is the assumption that ensures that $(X’X)$ is nonsingular, such that $(X’X)^{-1}$ exists. Zero conditional mean: $E(u|X) = 0$. This means that you have properly specified the model such that: there are no omitted variables, and the functional form you estimated is correct relative to the (unknown) population model. This is always the problematic assumption with OLS, since there is no way to ever know if it is actually valid or not. The variance of the errors term is constant, conditional on the all $X_i$: $\Var(u|X)=\sigma^2$ Again this means nothing for the mechanics of OLS, but it ensure that the usual standard errors are valid. Normality; the errors term u is independent of the $X_i$, and follows $u \sim N(0,\sigma^2)$. Again this is irrelevant for the mechanics of OLS, but ensures that the sampling distribution of the $\beta_k$ is normal, $\hat{\beta_k} \sim N(\beta_k , \Var(\hat{\beta_k}))$. Now for the implications. Under 1 - 6 (the classical linear model assumptions) OLS is BLUE (best linear unbiased estimator), best in the sense of lowest variance. It is also efficient amongst all linear estimators, as well as all estimators that uses some function of the x. More importantly under 1 - 6, OLS is also the minimum variance unbiased estimator. That means that amongst all unbiased estimators (not just the linear) OLS has the smallest variance. OLS is also consistent. Under 1 - 5 (the Gauss-Markov assumptions) OLS is BLUE and efficient (as described above). Under 1 - 4, OLS is unbiased, and consistent. Actually OLS is also consistent, under a weaker assumption than $(4)$ namely that: $(1)\ E(u) = 0$ and $(2)\ \Cov(x_j , u) = 0$. The difference from assumptions 4 is that, under this assumption, you do not need to nail the functional relationship perfectly.
