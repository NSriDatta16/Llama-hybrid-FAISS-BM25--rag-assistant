[site]: crossvalidated
[post_id]: 230875
[parent_id]: 5591
[tags]: 
Define probability . I mean it. Before we progress any further, we need to settle on terms. An intuitive definition of probability is a measure of uncertainty. We are uncertain whether the next coin toss will come up heads or tails. That is uncertainty in the data $D$. We are also uncertain whether the coin is fair or not. That is uncertainty about the model $M$... or you can call uncertainty about the state of the world. To arrive at the conditional distribution $P(M|D)$, you need to have the joint distribution $P(M,D)$ -- i.e., the knowledge of the whole population of coins in circulation, how many of them are forged, and how forged coins behave (which may depend on the way the coins are spun and caught in the air). In the particular example of coins, this is at least conceptually possible -- the government figures are available on the coins that are supposed to be fair (28$\cdot$10 9 per year), or at least those with stable characteristics. As far as forged coins go, the scale of production of less than a million is probably not worth talking about, so $10^6/28\cdot10^9$ may be a probability that the coin you got from a cashier's register is unfair. Then you need to come up with a model of how the unfair coin works... and obtain the joint distribution, and condition on the data. In the practical world problems with say medical conditions and the way they work, you may not be able to come up with none of these components of the joint distribution, and can't condition. Bayesian modeling provides a way to simplify the models and come up with these joints $P(M,D)$. But the devil is in the details. If you say that the fair coin is the one with $p=0.5$, and then go ahead and specify a traditional Beta prior, and get the Beta conjugate posterior, then... surprise, surprise! $P(p=0.5)=0$ for either of these continuous distributions, no matter if your prior is $B(0.5,0.5)$ or $B(1000,1000)$. So you'd have to incorporate a point mass at $0.5$, give it a prior mass ($28\cdot10^9/(28\cdot10^9 + 10^6)$, say), and see if your data moves the posterior away from that point mass. This is a more complicated calculation that involve Metropolis-Hastings sampling rather than the more traditional Gibbs sampling. Besides the difficulties in talking about what exactly the right models are, Bayesian methods have limited ways of dealing with model misspecification. If you don't like Gaussian errors, or you don't believe in independence of coin tosses (your hand gets tired after the first 10,000 or so tosses, so you don't toss it as high as the first 1,000 or so times, whch may affect the probabilities), all that you can do in Bayesian world is to build a more complicated model -- stick breaking priors for normal mixtures, splines in probabilities over time, whatever. But there is no direct analogue to Huber sandwich standard errors that explicitly acknowledge that the model may be misspecified, and are prepared to account for that. Going back to my first paragraph -- again, define probability. The formal definition is the trio $ $. $\Omega$ is the space of possible outcomes (combinations of models and data). $\mathcal F$ is the $\sigma$-algebra of what can be measured on that space. $P$ is the probability measure / density attached to subsets $A\subset \Omega$, $A\in\mathcal F$ -- which have to be measureable for the mathematics of probability to work. In finite dimensions, most reasonable sets are measurable -- see Borel sets , I am not going to bore you with details. With the more interesting infinite spaces (those of curves and trajectories, for instance), things get hairy very quickly. If you have a random process $X_t, t\in[0,1]$ on a unit interval in time, then the set $\{ X_t > 0, t\in[0,0.5]\}$ is not measurable, despite its apparent simplicity. (Sets like $\{ X_t > 0, t\in\{t_1, t_2, \ldots, t_k\}\}$ are measurable for finite $k$, and in fact generate the required $\sigma$-algebra. But that is not enough, apparently.) So probabilities in large dimensions may get tricky even at the level of definitions, let alone computations.
