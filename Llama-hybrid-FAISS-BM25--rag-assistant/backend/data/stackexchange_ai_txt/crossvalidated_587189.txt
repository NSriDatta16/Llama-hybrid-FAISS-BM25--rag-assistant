[site]: crossvalidated
[post_id]: 587189
[parent_id]: 122668
[tags]: 
Clumpiness detection Ripley's L function , as noted and nicely illustrated by @whuber, will generate an indicator of "clumpiness" (i.e. amount of clustering) in the distribution for a given target distance (or normalized distance). The same is true for the discrepancy metric outlined in Martin Roberts' answer, which is spiritually similar to Ripley's L function but computes the maximum difference from uniformity observed within a given interval, instead of a proportion for a given distance. A nice practical example of using Ripley's family of functions to detect "clumps" in a 2D scenario is given in this article by Kiskowski et al. Issues In both Ripley's L function and discrepancy , some target distance/interval needs to be specified. If you need a summary statistic for "clumpiness" (i.e. how even/uneven your distribution is independently of a specific distance/interval), you can examine Ripley's L function, as proposed by @whuber, and derive different statistics, but it is not obvious how to go about it. Side note Given a distance value $d$ , the Ripley's K function $K(d)$ returns the fraction of observed distances below $d$ , among all the pairwise distances in your data. Then Ripley's L is defined as $L(d)=K(d)-K_u(d)$ , where $K_u(d)$ is the value of K(d) for a perfectly uniform distribution. In @whuber’s reply $K_u(d)=1 - (1-d)^2$ . This is true for the continuous case, but only asymptotically true for the discrete case. The exact solution for the discrete case would be $\frac{M^2-(M-d)(M-d+1)-M}{M^2-M}$ , where $M$ is the number of distinct observable positions. Note that here we are assuming that for the discrete case the range of positions is not normalized to $[0,1]$ and can be described as $[0,M)$ . So, $M$ could also represent the resolution with which we observe positions. Here is a visual proof of the formula for the discrete case with $10$ distinct positions ( $M=10$ ) and $d=3$ . There are $M^2=100$ pairwise distances. Those below $d=3$ are colored in blue. The number of white cells is $(M-d)(M-d+1)=56$ . Therefore, the number of blue cells is $M^2-(M-d)(M-d+1)$ =44. Note that it’s common practice to avoid considering the distance of an element with itself, so the $0$ -valued cells on the diagonal are not going to be counted, giving us a number of distances below $d$ of $M^2-(M-d)(M-d+1)-M$ out of a total of $M^2-M$ . Binning-based approaches Alternative metrics can be devised using a binning approach. By defining regularly spaced bins over the range of observations (or, if known, the domain of the sampled distribution), one can define the distribution of data points across the bins (either as absolute numbers or relative frequencies). The entropy of this distribution can then be computed as follows: $-\sum_{i=1}^{b}f(x_i)·log_2(f(x_i))$ where $b$ is the number of bins and $f(x_i)$ the relative frequency of observations in bin $i$ (i.e. counts in bin $i$ over total counts). In other words, we are splitting our range into bins and then asking how uniform (high entropy) or non-uniform (low entropy) our counts of observations in each bin are. Using the same binning approach, other measures of inequality, like the Gini coefficient , can be used. A practical example of this approach, using both entropy and Gini coefficient, can be found in this article by Mascolo et al. Issues Binning-based approaches directly generate a summary statistic (the overall positional entropy , or Gini coefficient). They do not require that we specify a distance/interval at which we want to evaluate "clumpiness", but they require that we define a bin size, which will affect the counts and the resulting statistic. Therefore, bin size indirectly defines the scale at which one wants to detect clustering, similarly to the choice of $d$ for Ripley’s functions. Binning also introduces artifacts by arbitrarily chunking the sequence at regular intervals, which could potentially result in missing a clump that sits across two bins. A simple way to make results less dependent on the binning frame is to use two sets of bins with an offset of half the bin size, as described here by Mascolo et al. If you are interested in the statistical significance of the observed unevenness of your distribution, keep in mind that this method (as the others) is sensitive to the number of datapoints. Getting low entropy values by chance is easier for small samples. As the sample size increases, the expected value of the entropy approaches the theoretical maximum $-\sum_{i=1}^{b}1/b·log_2(1/b)=-log_2(1/b)=log_2(b)$ where $b$ is the number of bins. Therefore, remember to control for sample size when comparing experiments. So far there’s no exact solution to the problem of finding the expected value of entropy, given the sample size and the number of bins. Interval variance If you want to obtain a summary statistic that indicates the evenness/unevenness of a one-dimensional distribution, as outlined in the question, and that does not require you to specify a given distance/interval/bin size at which to analyze clumpiness, an easy solution is to use the variance of the intervals . That is, we take our observations, sort them, and compute the distance among each consecutive observation (i.e. the length of the interval between two consecutive data points). Given a sample $x_1,\ldots,x_N\in{X}$ of size $N$ for a given distribution, we define the interval series $i_{x_2-x_1},\ldots,i_{x_{N}-x_{N-1}}\in{I}$ , of length $N-1$ . Intuitively, for the "even" distribution $X$ introduced by @Ketan in their question, all the intervals are exactly the same and their variance is therefore $0$ . Conversely, for the "uneven" distribution $Y$ , interval sizes are either small (within clumps) or large (across clumps), leading to high variance. Shown below are five $N=64$ samples from a uniform distribution in $[-5,+5]$ [top], and five $N=64$ samples for a "clumped" distribution in the same domain (with four uniformly distributed clumps centered at $-4 [\pm0.5]$ , $-2 [\pm0.75]$ , $1.5 [\pm0.25]$ and $3.75 [\pm0.5]$ ) [bottom]. The corresponding interval histograms are shown below. Notice that samples from clumped distributions (5 rightmost bars in each bin) have many more small intervals (from within clumps) than the uniform samples (5 leftmost bars in each bin). The samples from clumped distributions also have several (inter-clump) large interval values that are completely absent in the uniform samples. The corresponding interval variances are: Uniform distribution samples: $0.025\pm0.005$ Clumped distribution samples: $0.137\pm0.009$ which recapitulate our intuition that "clumped" distributions should have larger interval variance. A practical example of this measure is illustrated in this article by Philip and Freeland. Issues A finite random sample from a uniform distribution will not be perfectly even , regardless of the measure of evenness/unevenness/clumpiness chosen. As sample size increases, the expected value of the interval variance approaches 0, just like the Gini coefficient and the value of the Ripley’s L function, while entropy reaches its theoretical maximum $log(b)$ . But if the sample size is not large enough, the expected value can be far from that. When comparing results to an ideal perfectly uniform sample, you should be aware that these metrics are biased. With the binning-based approaches one can increase bin size, while with Ripley’s functions one can increase the value of $d$ . But there is no way to control for that with the variance of the intervals. Keep that in mind if you plan to apply it to small samples, which are very prone to picking up noise in the form of natural variance among the intervals of uniformly distributed points.
