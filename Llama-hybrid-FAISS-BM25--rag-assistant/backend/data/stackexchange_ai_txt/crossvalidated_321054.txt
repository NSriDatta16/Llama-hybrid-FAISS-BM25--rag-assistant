[site]: crossvalidated
[post_id]: 321054
[parent_id]: 
[tags]: 
What are "residual connections" in RNNs?

In Google's paper Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation , it is stated Our LSTM RNNs have $8$ layers, with residual connections between layers ... What are residual connections ? Why residual connections between layers? Ideally, I am looking for a simple and intuitive explanation first, possibly accompanied by schematic representations. The details can, of course, be found in the original papers, but I thought this question(s) would be beneficial to the community.
