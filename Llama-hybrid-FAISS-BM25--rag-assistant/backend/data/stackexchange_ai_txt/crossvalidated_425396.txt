[site]: crossvalidated
[post_id]: 425396
[parent_id]: 425389
[tags]: 
Then for joint PDF why do we use the product of individual PDFs [...] assuming noise is i.i.d.? What is the purpose of product in laymen terms? Shouldn't it be summation and averaging of individual PDFs? Let's not mix estimators to their associated PDFs. An estimator $\hat{h}$ for $h$ takes this shape according to the intuitive averaging approach: $$\hat{h} = \frac{1}{n}\sum_{k=1}^ny[k] = h + \frac{1}{n}\sum_{k=1}^nv[k]$$ Is this a good estimator? It depends on the noise. If we assume that different measurements of it are independent of each other, and all have zero-average, then we're reasonably sure that the estimator will not fail us. Then we also assume that the measurements are also identically distributed because it makes sense and makes calculations much easier. So the i.i.d. guarantees us that the above estimator is unbiased (i.e. its expected value is $h$ ). Anyway, the estimator $\hat{h}$ is another random variable because it depends on the average of the different $v[k]$ that we get (for each sample of $n$ measures we will generally get different results). What is the PDF of this new random variable? This is where the rules of composition for PDFs kick in. The different $v[k]$ are i.i.d., so we can have the joint PDF as the product of the respective $PDF$ s (because of independence, see @Fr1's answer about this) and this product is quite easy to calculate (because they're all identical). Anyway, this joint distribution is not the distribution of $\hat{h}$ , because it still depends on each measurement individually considered a random variable itself. We have to take into account that $\hat{h}$ is the average of all of them, and do a few integrations for this. We end up with this formula: $$ f_{\hat{h}}(x) = \frac{1}{\sqrt{2 \pi \sigma_{a}^2}} e^\frac{(x - h)^2}{2 \sigma_{a}^2} $$ i.e. this new variable resulting from averaging the samples is still following a normal distribution, with mean value $h$ (it's unbiased ) and a different variance according to the following formula (this is where the identical assumption makes calculations easier): $$\sigma_{a}^2 = \frac{\sigma^2}{n}$$ which means that the more measurements we take, the less the variance for this estimator will be. Hence, the estimator is both unbiased and in a sense good , because it gets better as we take more measurements. (Arguably, it's not that good, in the sense that the standard deviation decreases like $\sqrt{n}$ , which means that to half the standard deviation you have to take four times measurements). So, to recap: we make the assumption of independence so that the average is a good estimator of the parameter $h$ ; averaging independent variables means multiplying their respective PDFs to obtain the joint distribution of all measurements, which is an $n$ -dimensional function; integrating this $n$ -dimensional function under the constraint of taking an average of all variables yields the PDF of the estimator; adding the "identically distributed" hypothesis further simplifies calculations and makes results easier to interpret.
