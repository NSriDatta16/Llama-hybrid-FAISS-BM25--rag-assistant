[site]: crossvalidated
[post_id]: 488271
[parent_id]: 
[tags]: 
Ranking Prediction Intervals - Multiple Comparisons?

I fit a model that tries to match the personality of sales reps to customers based on demographics. It's a hierarchical bayesian model that predicts the probability of conversion with sales rep[i] given the customers demographic. I.e. Are some sales reps good at talking to older / younger customers? $$ P(\text{Conversion w/ Sales Rep[i]} | \text{Customer's Demographic}) = \alpha[\text{sales rep}[i]] + \beta[\text{sales rep}[i]] X $$ I then take an incoming customer, and make a prediction for the probability of conversion of that customer with each and every agent and rank the prediction intervals - the best agent to then match this customer with is the one that has the highest probability of conversion credible interval. Is this reasonable to do or does it violate the multiple comparisons problem since I begin comparing 100 different prediction credible intervals? Is it ok because its bayesian? If 1 is reasonable, would the same approach still hold true If I used a frequentist model and used confidence intervals instead? 3). What's a good way to assess the model? It seems like I don't care as much about predictive capability, but more so that the coverage of the credible intervals are plausible - is there a good way to validate that for logistic models such as this?
