[site]: crossvalidated
[post_id]: 446550
[parent_id]: 446537
[tags]: 
Fitting 10K GLMs will be slow. A 2x2 Chi-squared test for each comparison of two event counts will be faster and will work well enough provided your event counts don't get too extreme, e.g. your event counts range from [5, 495] for your sample size of 500. You can work with the event counts directly so you won't need to create the vectors of zeroes and ones. For info on executing the ChiSquare tests, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html There are other possible ways to speed things up more. For example, grouping identical event count comparisons together so you only calculate the p-value for each comparison once; or predetermining the smallest event count y needed to achieve p = x. However, those tricks are leveraging the fixed sample sizes of your set up, and you may want to make those more flexible later on. I suspect simply changing to the ChiSquare test may make your code fast enough for your needs. In response to comment below, here are some more thoughts on speeding up the code further. For a fixed n=500 for each of the two groups, you could leverage clusters of (y-x) in checking for p = 495, no y > x exists that gives p I can see creating a dictionary of the (x, y) for all x in [0, 500] as keys. I suspect that would run faster than even a cleverly reduced set of if statements. Below is the R code I used to get the (x, y). I used a continuity correction in the ChiSquare test. You might get a few different y when not using the continuity correction, but those differences are small and it is debatable as to which should be preferred. n 0.05 & y 0.05){ y
