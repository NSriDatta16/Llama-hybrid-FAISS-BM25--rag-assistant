[site]: datascience
[post_id]: 118764
[parent_id]: 108594
[tags]: 
Yes, it is possible but the performance of your second model will strongly depend on the performance of the first one, as well as how relevant the low-dimensional representation learned by that first model is to solve the problem you built that second model for. You can use that first/pre-trained model up to that embedding layer, and add more layers to create that second model, which you would train with the weights already initialized with, hopefully, already useful values. That is called fine-tuning. Here is an example from Kaggle. You could also consider freezing the weights from the first model (transfer learning) to reduce the number of parameters to learn and train your second model. This also is equivalent to creating input features from the embedding layer of the first model.
