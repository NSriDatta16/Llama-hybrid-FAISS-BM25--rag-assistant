[site]: crossvalidated
[post_id]: 452136
[parent_id]: 408307
[tags]: 
I see that the question is pretty old, but since there is no answer, let me give you my opinion. First of all, as you said Q-Learning is off-policy and TD hence deadly triad if used with function approximation. If you really wanted to write it anyway; I am confused about your explanation on the bottom part. I believe off-policy has not much difference than SARSA, except that now the agent won't be acting towards the policy we are using, but acting according to the trajectory on behavior policy. So the difference is not about updating weights before A' or not. If I understood it correctly.
