[site]: crossvalidated
[post_id]: 625227
[parent_id]: 332179
[tags]: 
I want to add another interesting paper relating to this question, where the authors propose a cyclical annealing scheme for the KLD term to improve the training of a VAE for natural language processing tasks. The basic idea is to repeat the process of increasing the weighting term $\beta$ multiple times starting from zero. The authors argue that this warm re-start allows one to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles. Furthermore, they show that this strategy can improve reconstruction loss compared to a monotonically increasing $\beta$ only at the beginning of the training. $\beta$ schedules" /> Paper: Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. 2019. Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 240â€“250, Minneapolis, Minnesota. Association for Computational Linguistics. DOI: 10.18653/v1/N19-1021 Code: github.com/haofuml/cyclical_annealing
