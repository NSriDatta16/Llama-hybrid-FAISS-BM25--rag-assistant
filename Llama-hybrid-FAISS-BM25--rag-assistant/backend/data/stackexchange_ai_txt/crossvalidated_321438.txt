[site]: crossvalidated
[post_id]: 321438
[parent_id]: 321307
[tags]: 
Edit Rounding error is not the issue. The issue is inherent bias in the likelihood with small sample size. I have accepted JimB's answer. Although, his likelihood and mine are identical and return identical estimates when the same constraints are applied. Earlier answer Yesterday I answered my own question. That answer is retained further below. I suggested the reason the estimated $N$ of $79.496$ was different from the true $N$ of $80$ was because of rounding error in the betas. I no longer believe that because if I remove the betas from the likelihood I still get the incorrect answer of $79.496$, as shown with the following R code. Either my likelihood is wrong or there is some fundamental bias in a model of die rolls. I am baffled as to why I cannot accurately estimate the true $N$ of $80$. # generate some data N Here is my answer from yesterday: I think I have figured out why my search algorithm was returning $N$ = 79.5 instead of the true value of $N$ = 80. I think it was due solely to rounding error in the betas used in the multinomial logistic link. Originally I obtained the betas using closed-form equations I derived here: https://math.stackexchange.com/questions/2587203/invert-multinomial-logit-link-with-three-unknown Then I estimated betas via multinomial logistic regression: N These multinomial logistic regression betas differ from my closed-form betas in the fifth decimal place, a difference which initially I thought was trivial. However, that difference turns out not to be trivial, as shown below. Next I compared the counts obtained using my closed-form betas against the true counts, and I compared the counts obtained using the multinomial logistic regression betas against the true counts. My closed-form betas returned counts closer to the true counts. The difference in counts obtained using the two sets of betas are small, but again not trivial: # These betas are from my closed-form equations beta.x = 0.6931472 beta.y = 1.098612 beta.z = -0.4054651 # check counts in each class using my closed form betas (N * exp(beta.x) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z))) - nA (N * exp(beta.y) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z))) - nB (N * exp(beta.z) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z))) - nC (N * (1 - exp(beta.x) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z)) - exp(beta.y) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z)) - exp(beta.z) / (1 + exp(beta.x) + exp(beta.y) + exp(beta.z)))) - nD # These betas are obtained using multinomial logistic regression mr.beta.x = 0.6931183 mr.beta.y = 1.0985900 mr.beta.z = -0.4054899 # check counts in each class using multinomial logistic regression betas (N * exp(beta.x) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z))) - nA (N * exp(beta.y) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z))) - nB (N * exp(beta.z) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z))) - nC (N * (1 - exp(beta.x) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z)) - exp(beta.y) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z)) - exp(beta.z) / (1 + exp(mr.beta.x) + exp(mr.beta.y) + exp(mr.beta.z)))) - nD If I use the multinomial logistic regression betas in my search algorithm the estimated $N$ is 82: # These betas are from multinomial logistic regression beta.x = 0.6931183 beta.y = 1.0985900 beta.y = -0.4054899 N.trial The betas obtained via my closed-form equations allowed the algorithm to return an estimate of $N$ within 0.504 of the true value, while the betas obtained from multinomial logistic regression resulted in an estimate of $N$ that differed from the true value by 2. This is only one data set. Maybe if I try other data sets the beta returned by the two different methods will not differ as much. I guess it makes sense that my closed form equations performed better because there is no real estimation going on to obtain them. I feel a little bad that rounding or estimation error in the fifth decimal place of the betas can have such a fairly big impact on the estimate on $N$. I do not know how to improve on the estimate. However, I guess a difference of 0.504 from the true value of $N$ is not real bad.
