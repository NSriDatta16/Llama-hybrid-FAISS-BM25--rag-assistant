[site]: crossvalidated
[post_id]: 374783
[parent_id]: 
[tags]: 
Bayesian statistical conclusions: we implicitly condition on the known values of any covariates, $x$

My Bayesian data analysis textbook says the following: Bayesian statistical conclusions about a parameter $\theta$ , or unobserved data $\tilde{y}$ , are made in terms of probability statements. These probability statements are conditional on the observed value of $y$ , and in our notation are written simply as $p(\theta|y)$ or $p(\tilde{y}|y)$ . We also implicitly condition on the known values of any covariates, $x$ . Page 6, Bayesian Data Analysis, Third Edition, by Gelman et al. I'm wondering what is meant by this last part: We also implicitly condition on the known values of any covariates, $x$ . I'm aware that $x$ are the explanatory variables (also known as covariates or predictors), but how do we "implicitly condition on them"? What is meant by this? I would greatly appreciate it if people could please take the time to explain this.
