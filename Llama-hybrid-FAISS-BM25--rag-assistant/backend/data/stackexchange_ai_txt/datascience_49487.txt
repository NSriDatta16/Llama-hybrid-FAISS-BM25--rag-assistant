[site]: datascience
[post_id]: 49487
[parent_id]: 
[tags]: 
Why is my MLP with 2 features is doing worse than MLP with 1 feature where the one feature is a combination of feature1*feature2?

I have programmed a MLP for a dataset (~500 rows) containing the length (L) and width (W) of an organism and the output of biomass (the organisms weight in pounds, B). mlp = MLPRegressor((5, 5), max_iter=1000) I have trained the model with features # Model 1 # Input = Feature 1: Length, Feature 2: Width. Output = Biomass df = {'length': [60.1, 59.2, 59.4, 58.5], 'width': [15.4, 16.2, 14.9, 15.7], 'weight': [8.34, 7,65, 7.89, 7.14]} # Model 2 # Input = Feature 1: Length * Width^2. Output = Biomass df = {'length*height^2': [60.1, 59.2, 59.4, 58.5], 'weight': [14253.31, 15536.44, 13187.39, 14419.66]} The overall accuracy of my model with one feature is over 95%, however the accuracy with the features separated is about 85%. My understanding of an MLP is that Model 1 should do better than model 2 as it will basically find the best combination of length and height to biomass, However my 1 feature model is doing significantly better. I have also tried standardizing the dataset with a scaler with no luck. scaler = StandardScaler()
