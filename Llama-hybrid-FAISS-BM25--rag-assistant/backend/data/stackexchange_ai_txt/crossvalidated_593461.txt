[site]: crossvalidated
[post_id]: 593461
[parent_id]: 593458
[tags]: 
This is clearly an open and active research question. To my knowledge it is relevant in a few different contexts: 1/ In a supervised learning setting, it can be extremely tedious (for humans) to label the training data. In that case it is preferable to label the samples that "matter the most". 2/ In Scientific Machine Learning, ground truth data is usually generated by computationally expensive ODE or PDE solvers, which limits the number of available samples. 3/ In sensor-based ML, such sensors (cameras, satellites, thermometers, etc...) can be expensive/difficult to monitor, which limit the number of available sensors. You may take a look at a few recent papers: Increasing Data Diversity with Iterative Sampling to Improve Performance Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning Gaussian processes for autonomous data acquisition at large-scale synchrotron and neutron facilities Exploring Representativeness and Informativeness for Active Learning To my knowledge, the problem of "crafting" small but representative datasets is very popular in the domain of Active Learning as well as data-centric AI .
