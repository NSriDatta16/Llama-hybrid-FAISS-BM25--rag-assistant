[site]: crossvalidated
[post_id]: 372848
[parent_id]: 
[tags]: 
How to prevent overfitting with regression using ranger (randomforest)

I use caret to train the model (on Boston dataset from the mlbench package). Here is the code set.seed(2) ind=sample(nrow(Boston),trunc(0.7*nrow(Boston))) train=Boston[ind,] test=Boston[-ind,] # Fit lm model using 5 x 5-fold CV: model model When printing the model, it seems I have a good model Random Forest 354 samples 13 predictor No pre-processing Resampling: Cross-Validated (5 fold, repeated 5 times) Summary of sample sizes: 282, 282, 285, 283, 284, 283, ... Resampling results across tuning parameters: mtry splitrule RMSE Rsquared MAE 2 variance 4.172443 0.8113023 2.702026 2 extratrees 4.574969 0.7819608 2.946490 7 variance 3.744418 0.8324785 2.475156 7 extratrees 3.812538 0.8342013 2.478945 13 variance 3.821406 0.8214275 2.517686 13 extratrees 3.795269 0.8282988 2.465104 Tuning parameter 'min.node.size' was held constant at a value of 5 RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 7, splitrule = variance and min.node.size = 5. When plotting the model, I get However when I calculate sqrt(mean((predict(model)-train $medv)^2)) # 1.487133 sqrt(mean((predict(model,newdata=test)-test$ medv)^2)) # 2.648461 I would like to know what I did wrong, and what do I have to do in order to improve the model. Thank you
