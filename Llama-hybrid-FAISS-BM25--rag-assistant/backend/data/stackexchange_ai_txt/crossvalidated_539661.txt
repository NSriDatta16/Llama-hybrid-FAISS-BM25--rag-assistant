[site]: crossvalidated
[post_id]: 539661
[parent_id]: 
[tags]: 
How to evaluate ensemble models using independent data

I am creating species distribution models using the popular R package biomod2 . I am using biomod2 functionality to create ensemble models. Ensemble models combine individual models based on their weight of evidence (statistical performance). Traditionally, model averaging or multi-model inference was performed using likelihood techniques (i.e., AIC). More recently, techniques have been developed to average models that are not likelihood based, such as Random Forests. This is accomplished by using cross-validation or split sample validation to assess statistical performance and then combine models using measures of statistical performance like ROC, kappa, TSS, etc. My question is how can individual models be assessed for statistical accuracy using cross-validation or split sample, then be combined into an ensemble weighted based on performance, and then the ensemble model be assessed for accuracy using cross-validation or split sample again with independent evaluation data? The issue here is that when you create different partitions of calibration and evaluation data to assess the individual models, there ends up not being any independent data left to evaluate the ensemble model. The only way that I can think that this would work is to use cross-validation and ensemble within runs. Each run of cross-validation would have the same partition of data for all models (i.e., if you were doing 5-fold cross validation on 3 models, in Run 1 the three models would use the same 20% of data as evaluation and 80% as calibration, in Run 2 the three models would use the same 20% of data as evaluation and 80% as calibration, etc.). That way, all three models in each run can be combined, and the resulting ensemble model could be assessed using the same 20% of evaluation data that was used for the individual models. The issue with this arrangement is that it results in 5 separate models, one for each run of the cross-validation. That isn't a very satisfying result. Any recommendations for best practices on creating ensemble models and then evaluating them? I realize people are probably going to say a dataset that is truly independent in space and time that is saved for assessment of the final ensemble model. Any alternatives to this for people with data that isn't independent in space and time and small dataset that aren't big enough to have a portion left out for a final evaluation?
