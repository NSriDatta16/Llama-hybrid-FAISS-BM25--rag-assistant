[site]: crossvalidated
[post_id]: 252987
[parent_id]: 233850
[tags]: 
I agree with all @amoeba 's comments. But would add a couple of things. The reason PCA is used on images is because it works as' feature selection ' : objects span multiple pixels, so correlated changes over multiple pixels are indicative of objects. Throwing away un correlated Pixel changes is getting rid of' noise' which could lead to bad generalisation. In general nns (using gradient descent) do better with sphered inputs (ie un correlated normalised inputs), this is because then your error surface will be more symmetrical and so the single learning rate works better (you need a small learning rate in highly curved directions and a large learning rate in shallow directions). Did you normalise your inputs (good) as well or just decorrelate? Did you use l2 regularisation? This has a similar effect to PCA regularisation. For correlated inputs it emphasises small similar weights (ie averaging out the noise, penalising large weights so single pixels cannot have a disproportionate effect on classification) (see eg elements of statistical learning book), so perhaps you saw no benefit to PCA because the l2 regularisation was already effective. Or perhaps the nn was too small to overfit. Lastly did you reoptimise the parameters... I would expect you would need different learning rate and other parameters after changing to first k principal components.
