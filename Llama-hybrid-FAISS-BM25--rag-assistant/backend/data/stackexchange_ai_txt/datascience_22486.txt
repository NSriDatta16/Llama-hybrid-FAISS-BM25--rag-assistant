[site]: datascience
[post_id]: 22486
[parent_id]: 
[tags]: 
Batch normalization was introduced in the 2015 paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift It is conceptually an extension of the general advice to scale input values to neural networks (typically to mean 0, standard deviation 1). Batch normalization applies the same idea to re-scale neural network activations on a per-layer basis, and this can result in improved training times.
