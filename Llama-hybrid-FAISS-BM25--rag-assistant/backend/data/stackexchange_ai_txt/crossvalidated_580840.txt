[site]: crossvalidated
[post_id]: 580840
[parent_id]: 
[tags]: 
How to best bucket data to optimize performance of models?

I'm training multiple models to predict churn for customers belonging to online stores. Each model contains customer-level features and data related to n-number of stores. However, we've paid barely any attention when it comes to grouping stores together for the model training. We have roughly 300 different stores each containing roughly 100k - 10M customers. What's the best way to find the optimal grouping of stores for each model training such that I maximize the f1 score performance? Do I need to subsample and iterate through each combination (note: the data set is imbalanced)? Is it some EDA into SHAP values of each store for its existing model? Or is this something suited to be tackled by like a clustering algorithm (kNN)?
