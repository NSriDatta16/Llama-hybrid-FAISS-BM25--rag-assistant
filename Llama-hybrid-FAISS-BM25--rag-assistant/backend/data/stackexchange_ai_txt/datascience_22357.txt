[site]: datascience
[post_id]: 22357
[parent_id]: 22348
[tags]: 
"One solution to this problem is to use ML to discover not only the mapping from representation to output but also the representation to itself. This approach is called representation learning." Mapping the representation to output is called "Hetero association." Mapping the representation to itself is called "Auto association." Both approaches are about classification but in the first case you are associating a representation (or object, or item, or vector) to a label (for a category different from the original represented object, item, or vector). In the second case you are associating a representation (or object, or item, or vector) to a label (for the original represented object, item, or vector). By representation, they mean an image, object, item, vector, etc.. So an auto associative encoder can take an image and output the category for the original image, while a hetero associative encoder can take an image an output a learned association for that image (for example, the word "dog"). How would autoassociative learning be useful? Spell checkers are a good example. Given a small lexicon of words ("dog, hog, hat, heat") each word forming its own category, we can create an auto encoder that when presented with each word, featurizes the word, and returns the original category / word. Let's featurize the words using trigrams. So we split each word into its feature set and let that feature set represent the orginal word as follows: dog --> {' d', ' do', 'dog', 'og ', 'g '} --> dog hog --> {' h', ' ho', 'hog', 'og ', 'g '} --> hog hat --> {' h', ' ha', 'hat', 'at ', 't '} --> hat heat --> {' h', ' he', 'hea', 'eat', 'at ', 't '} --> heat Now we present a new word, featurize it, and see where it falls: hot --> {' h', ' ho', 'hot', 'ot ', 't '} --> ??? "hot" matches 0% of the features in "dog". "hot" matches 40% of the features in "hog". "hot" matches 40% of the features in "hat". "hot" matches 33% of the features in "heat". hot would be classified as either "hog" or "hat" if the match threshold were 40% or below. If we had a higher match threshold than 40% then there would be no match and hence no proper classification for the word hot. The auto encoding of each of the lexicon entries (dog, hog, hat, heat) allowed us to take the original representation of each word and split it into features, which can be combined to point back to the original representation. dog --> {' d', ' do', 'dog', 'og ', 'g '} --> dog A hetero encoding could take each of the lexicon entries and map it to a different category. dog --> animal hog --> animal hat --> thing heat --> thing a simple hash table could suffice as the hetero encoder in this example. Neural networks take an input vector of features and output the category either in an autoassociative or heterassociative manner.
