[site]: datascience
[post_id]: 71412
[parent_id]: 71391
[tags]: 
Control charts are widely used still (whether people know it or not). However it is limited to cases where it is possible to define meaningful and stable targets and bounds for a metric. The classical example is in manufacturing, when the intent is to produce items which has a certain property at a certain value. Like each cereal box produced should be 510 grams, 500 grams of net contents and 10 grams for the box. Being outside for example +-5% of this would be considered an anomaly. However when there are no such clear metric, or targets, more advanced tools can be useful. For example consider you run a web service which serves webpages to users. Different time of day have different usage patterns. Different users make different kinds of request. Different kinds of requests have different impact on the system resources and thus the ability for your system to serve other (unrelated) requests. System performance will vary quite a lot, but a lot of it due to factors outside your control. And often it is not individual metrics which are the clue to "something" is wrong, but combinations of them. This makes it hard define single-value metrics, and to set control-chart targets and limits. But an autoencoder can learn time-based patterns, the user-dependent patterns, request-type dependent patterns, and their relationships. And give an overall score for how anomalous the overall situation is.
