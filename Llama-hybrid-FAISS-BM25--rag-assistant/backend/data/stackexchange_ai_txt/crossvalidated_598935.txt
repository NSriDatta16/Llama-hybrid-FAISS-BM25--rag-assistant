[site]: crossvalidated
[post_id]: 598935
[parent_id]: 598924
[tags]: 
Even with data augmentation 90% of 2482 does not magically become 46554. Data augmentation increases the value of each image, maybe even so that one is worth a few images, but it's still worth less than 46554 completely unrelated images would be worth. The kind of model you have could certainly be overfitting in your situation. However, I would also look at the training loss and validation loss, not just accuracy, because accuracy uses very little information per image (i.e. just 1 or 0, while whatever loss function you are using will take into account that predicting 0.9 for a 1 outcome is better than predicting 0.51). I.e. accuracy is quite a noisy thing to look at that can jump around in a misleading way due to the very small amount of information it captures. Other ways of combatting overfitting besides data augmentation include various forms of regularization such as dropout or weight decay. Other promising ideas could include transfer learning from other types of images or unsupervised learning on a large amount of unlabeled images (if you have such images), even if you want to use a smaller CNN than those that are available pre-trained you could pre-train yours on ImageNet or distill a larger CNN trained on ImageNet into your smaller one.
