[site]: crossvalidated
[post_id]: 205592
[parent_id]: 
[tags]: 
Is resampling more accurate than block average for statistical analysis of data (cross-post)?

this is a cross question I asked on the computer science community before I was advised to cross-post it here. I'm working in laboratories where molecular dynamics data are almost always analysed usign block average as stated in the famous Allen and Tildesley book. We divide the datas in blocks of size $M$ on which we compute our interesting quantities. We consider that for large enough blocks, the fluctuations converge so: $$ s=\lim_{M \rightarrow \inf}\frac{M\sigma^2( _M)}{\sigma^2(X)}$$ Where $s$ is called the statisical inefficiency and, when converged for M large enough, is an estimation of the spacing between uncorrelated records. You can then evaluate your statistical error: $$\sigma^2( _{run})=\frac{s}{N}\sigma^2(X)$$ for a run of N records. Then I stumbled accross this lecture notes that starts its chapter saying explicitly how block averaging is a naive and mediocre method to evaluate statistical error. Resampling methods seems to be more accurate and easier to implement. I then have two questions: 1) How are resampling methods an "upgrade" of block averaging? The author states that it's due to the potential large variation of value between blocks but on simple systems which converge really fast to equilibrium, I still have an order of magnitude of difference between bootstrapping and block average. Can the starting values be sufficient to impact the statistical error evaluation? 2) Why is block average so in use in the molecular dynamics community? I suspect that this reference books did propagate this methods but it was published at a time where resampling became of common use. I'm not sure that any of my colleagues ever asked this question and my knowledge of statistical analysis is to low for me to really defend my view.
