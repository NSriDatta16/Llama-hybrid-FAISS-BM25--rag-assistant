[site]: crossvalidated
[post_id]: 539216
[parent_id]: 442773
[tags]: 
For counts presumably you'd want to look at something like Poisson loss or - if you want mean-absolute-error or mean-squared-error of square-root transformed counts or... If it's more about the locations, then you want some kind of definition of what determines whether you have detected something at a certain location (e.g. with bounding boxes you can use accuracy, recall, precision etc. at some IOU level, but you still need to determine whether you want to calculate first a metric per image and then average so that a single image with many objects does not get too much weight). How many images you then need depends a bit on how many images have how many objects on them and what kind of mistakes you expect your algorithm to make. However, one easy experiment to get a feel for this is to make up some hypothetical performance numbers for a certain number of images (e.g. for 30 images make up the 30 performances per image for each of your algorithms). This might look like this: Image Metric for algorithm 1 Metric for algorithm 2 Metric for algorithm 3 1 1.0 0.0 1.0 2 0.5 0.5 1.0 3 0.75 0.25 0.75 ... ... ... ... 30 0.9 0.1 0.8 You can then repeatedly (let's say 1000 times) draw 30 rows from this table with replacement. For each of the 1000 repetitions, you calculate the difference between algorithms for your performance metric of interest. Then you have a look at how much that varies (i.e. the bootstrap distribution) and you have an idea of how sure you could be with that many annotated images. Of course, if you have a bit of a budget, there are online services where you can get people you pay annotate images for you. Or if you have a bunch of friends/colleagues or just more time yourself, then there are good (and free) annotations tools that speed up the process and partially automate it (e.g. you might only have to quickly draw a bounding box with your mouse for each object and hit a key for next image, everything else done automatically).
