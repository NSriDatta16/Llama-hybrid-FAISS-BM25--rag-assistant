[site]: crossvalidated
[post_id]: 606197
[parent_id]: 
[tags]: 
Self-supervised Target Definition in the Original Neural Language Model by Bengio et al (2003)

I understand how later neural language models (such as those used in the Word2Vec papers) framed the language modelling problem in a self-supervised way by learning to predict the next word (or any context word) in a sequence of words. However, when reading Bengio et al's original Neural LM paper ( a neural probabilistic language model ) it isn't very clear to me how they built the objective function for the optimizer. From the paper: Does this mean that the authors had to build the target by manually counting the empirical conditional probability for every "row" in the training set? For instance, say the network is processing the string "foo bar baz" right now: The target for this self-supervision instance would need to be the number of times "baz" followed "foo bar" divided by all occurrences of "foo bar" in the validation set?
