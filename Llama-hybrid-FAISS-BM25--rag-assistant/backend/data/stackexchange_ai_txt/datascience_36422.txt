[site]: datascience
[post_id]: 36422
[parent_id]: 36418
[tags]: 
How Adding a dropout layer to a neural network has a few functions. First of all, here is some simple code that implements it: In [1]: import numpy as np In [2]: weights = np.random.randint(0, 100, (8, 8)) # example weights In [3]: weights Out[3]: array([[10, 48, 9, 87, 23, 18, 60, 83], [29, 48, 88, 43, 34, 56, 52, 82], [35, 0, 70, 3, 8, 88, 6, 15], [39, 16, 59, 91, 33, 13, 53, 73], [20, 56, 70, 35, 16, 12, 80, 6], [21, 17, 23, 21, 72, 93, 58, 56], [26, 86, 38, 90, 91, 87, 65, 0], [67, 6, 10, 94, 19, 25, 49, 61]]) In [4]: dropout_mask = np.random.randint(0, 2, (8, 8)) # either 0 or 1 In [5]: dropout_mask Out[5]: array([[1, 1, 0, 0, 0, 1, 0, 1], [1, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 1, 0], [0, 0, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 1, 1, 1, 0, 0, 0, 1], [0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 0, 1, 0, 1, 1, 1]]) # we make a copy here just to use helpful variable names In [6]: masked_weights = weights.copy() # where the mask is "on" (==1), we set those weights to 0 In [7]: masked_weights[dropout_mask == 1] = 0 In [8]: masked_weights Out[8]: array([[ 0, 0, 9, 87, 23, 0, 60, 0], [ 0, 48, 0, 0, 34, 56, 52, 82], [ 0, 0, 70, 0, 8, 0, 0, 15], [39, 16, 59, 91, 0, 13, 0, 73], [20, 0, 70, 35, 16, 12, 0, 6], [21, 0, 0, 0, 72, 93, 58, 0], [26, 0, 0, 90, 0, 0, 0, 0], [ 0, 6, 10, 0, 19, 0, 0, 0]]) So we have selected roughly half of weights and turn them off for a pass through the network. EDIT : Droput randomly drops neurons on each pass in training, as shown above, but during test time (a.k.a. inference), the dropout layers are deactivated by default. This means that all neurons are available and are used. There is still, however, the issue of scaling - so while all neurons are active during inference, their outputs are scaled to reflect the layers overall output such that the expected overall sum remains unchanged. This can be performed a few different ways - see comments on this answer for related links. There are of course nice implementations of dropout in all deep learning frameworks (PyTorch, Tensorflow, CNTK, and so on). As is discussed and shown here . Intuition One intuitive explanation, is that is ensures each neuron increases its robustness and manages to contribute to the network in its own right. What I mean by this, is that dropout prevent chain or clusters of neurons being strongly dependent on one another. It helps focus the neurons to be able to deliver some useful information in as many different scenarios as possible, e.g. when it only receives signal from a subset of the neurons in the preceding layer. It can be helpful to think of dropout in terms of regularisation . We add regularisation terms to models in order to penalise them again certain behaviour. An example would be in linear regression via the Bayesian Information Criterion , whereby we increase a model's error if it uses more and more covariates. We also choose the form of error/regularisation when computing values in backpropagation e.g. we select the sum of squared errors (the $L_2$ loss), or just the raw error (the $L_1$ loss). Dropout, on the other hand, applies a change the network itself! We are constricting it by simply removing access to a random selection of neurons during one single pass through the network. Maths There actually is not a lot of maths involved, with the original paper explaining the concept in no more than four lines: The magic really happens in that second line, with the element-wise multiplication of a Bernoulli distributed random variable $\textbf{r}$ and the vector outputs of the preceding layer, $\textbf{y}$. This performs exactly the same operation as IN [7] in my code example above. I believe the reason is that it was inspired by the intuitive idea, easy to implement and finally proven to work exceptionally well in practice. The above equations can then be applied to various problems - I would suggest reading the paper for some examples. For further understanding of dropout (and everything around neural network concepts!), I heartily recommend reading Michael Nielsen's book . Exaplantions of dropout with excellently helpful diagrams can be found in Chapter 3 .
