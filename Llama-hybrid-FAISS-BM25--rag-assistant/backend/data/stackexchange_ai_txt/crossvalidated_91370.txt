[site]: crossvalidated
[post_id]: 91370
[parent_id]: 90609
[tags]: 
Ignoring the probabilities of a false positive for the moment, I would look at it like this: If you run the experiment twice an get the same result, you have no idea whether there was two true positive results or two false positive results in a row. If you run the experiment twice and get two different results, then you do not know which is the true positive and which was the false positive result. In either case you should then run a third experiment, just to be certain. This maybe fine for experiments that are relatively inexpensive, but where the cost is potentially high (like losing customers) you really need to consider the benefit. Looking at the probabilities, the first time you run the experiment, there is a 1/20 chance of a false positive. The second time you run the experiment there is still a 1/20 chance of a false positive (think of it as rolling a die where each roll has a 1/6 chance of obtaining a certain number). There is only a 1/400 chance of having two false positives in a row. The real issue is to have a well defined hypothesis with stringent procedures, and to have a sample size, level of error, and confidence interval you can live with or afford. Repetition of the experiment should be left to exploring customers over time changes made by the organisation changes made by the competition rather than second guessing results. Although explaining this to managers is easier said than done.
