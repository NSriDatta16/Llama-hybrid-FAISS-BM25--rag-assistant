[site]: datascience
[post_id]: 23082
[parent_id]: 
[tags]: 
What is Extreme Learning Machine? Why tuning of weights is not required?

The wikipedia says: Extreme learning machines are feedforward neural network for classification, regression, clustering, sparse approximation, compression and feature learning with a single layer or multi layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. How is it possible to assign values to links without actually seeing the Training set ? Till now from my research to understand this, I've made a lead, but not satisfactory because I can't anything from this below paragraph. For any nonconstant piecewise continuous function that is used as the activation function, if the parameters of the hidden neurons are tuned, then the function can make the SLFNs approximate any target continuous function f (x). Then, according to any continuous distribution probability, the function sequence {h subscript i (x)} where i goes from 1 to L can be randomly generated, and it has the universal approximation capability, which means that $$\lim_{L→ \infty} ||L i=1 βi hi (x) − f (x)|| = 0$$ holds with probability of one with appropriate output weights $β$. SOURCE: A Parallel Multiclassification Algorithm for Big Data Using an Extreme Learning Machine, IEEE transactions on neural networks 2017 Can anyone prove/explain this with an example please? Also any extra knowledge on extreme learning machine is very welcomed.
