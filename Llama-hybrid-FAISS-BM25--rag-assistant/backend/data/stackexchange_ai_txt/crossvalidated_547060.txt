[site]: crossvalidated
[post_id]: 547060
[parent_id]: 547052
[tags]: 
Measurements do not always show ideal behavior and the presumed underlying distribution that is used for the maximum likelihood estimate (MLE) is often not the distribution of the measurements. For instance, in the graph above the distribution of the measurements is a mixture of two Gaussian distributions 25% with $\sigma = 10$ and 75% with $\sigma = 1$ . (so for whatever reason the distribution is not ideal, either it is because the population is not ideal or it is because the measurements are not perfect) This component with large variation will increase the sampling variance (and inaccuracy/error) of the estimator a lot. Instead of using the MLE (which in a simple case, like estimating the mean of the population, boils down to the average/mean value of the sample), one could use a statistic that filters a few of the extreme values from the sample. This would greatly reduce the variation of the statistic. Then this alternative statistic is more robust to outliers, more robust to the small component of the distribution that has extreme values, and increases the variance if they are not 'taken care of'. Example with the above distribution. Let's consider a sample of size 10 and we compute the MLE as the mean of the sample and the alternative by considering only the mean of the middle 6 values. Let's see how do these differ in distribution/error: ### function to compute estimate in two different ways get_sample = function() { ### geneare data n = 10 sigma = 10^rbinom(n,1,0.25) ### mixture distribution 0.25 ### part sigma = 10 and 0.75 part sigma = 1 x = rnorm(n,0,sigma) ### compute estimates est1 = mean(x) est2 = mean(x[order(x)][3:8]) ### use only values 3 to 8 ### (deleting outer 20%) return(c(est1,est2)) } ### compute the estimates set.seed(1) x The MLE is often an estimator that has the lowest variance or performs sufficiently if the ideal conditions are true. But when the assumed distribution (on which this statement of low variance is based) is only slightly perturbed (but with values of large magnitude) then this can already result in a large variance for the MLE. Note 1: It also depends on what sort of MLE you have. For instance, when we estimate the mean of a distribution and the distribution is a Gaussian distribution then the MLE is the mean of the sample, and as you see in the example above, the mean is not very robust against small perturbations. But when the distribution is a Laplace distribution then the MLE is the median of the sample, and this will be more robust against small perturbations. Note 2: In the example above we simply excluded the bottom and top 20% from the sample. But robust estimators are not that simple. It is a complex and large field. For instance, what if we have only had positive outliers, then our discarding of the bottom part makes the estimate biased? And how much should we discard? There are many considerations that go into constructing a robust estimator (and sometimes it is a bit art instead of science, but the example shows the idea of why it generally works).
