[site]: datascience
[post_id]: 26666
[parent_id]: 26640
[tags]: 
You check for hints of overfitting by using a training set and a test set (or a training, validation and test set). As others have mentioned, you can either split the data into training and test sets, or use cross-fold validation to get a more accurate assessment of your classifier's performance. Since your dataset is small, splitting your data into training and test sets isn't recommended. Use cross validation. This can be done using either the cross_validate or cross_val_score function; the latter providing multiple metrics for evaluation. In addition to test scores the latter also provides fit times and score times . Using your example; from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris from sklearn import svm from sklearn.metrics import accuracy_score iris = load_iris() X = iris.data[:, :5] # we only take the first two features. y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=51) svm_model = svm.SVC(kernel='linear', C=1, gamma='auto') svm_model.fit(X_train,y_train) predictions = svm_model.predict(X_test) accuracy_score(predictions, y_test) raw accuracy: 0.96666666666666667 Using the cross_val_score function, and printing the mean score and 95% confidence interval of the score estimate: from sklearn.model_selection import cross_val_score scores = cross_val_score(svm_model, iris.data, iris.target, cv=5) print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2)) Accuracy: 0.98 (+/- 0.03) Of course the iris dataset is a toy example . On larger real-world datasets you are likely to see your test error be higher than your training error, with cross-validation providing a lower accuracy than the raw number. So I wouldn't use the iris dataset to showcase overfitting. Choose a larger, messier dataset, and then you can start working towards reducing the bias and variance of the model (the "causes" of overfitting). Then you can start exploring tell-tale signs of whether it's a bias problem or a variance problem. See here: https://www.quora.com/How-many-training-samples-are-needed-to-get-a-reliable-model-in-ML/answer/Sean-McClure-3?srid=zGgv
