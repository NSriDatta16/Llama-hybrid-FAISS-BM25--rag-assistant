[site]: crossvalidated
[post_id]: 328492
[parent_id]: 
[tags]: 
The actual role of second-order optimization as oppose to first-order optimizations

I do not fully understand how second-order optimization approaches help machine learning algorithms, like multilayer perceptron, to achieve the global minimum error. As you know, Stochastic Gradient Descent is in the first-order optimization family as it helps to optimize the error function by going downhill toward the global minimum. On the other hand, the second-order optimization like L-BFGS has been also an alternative approach which relies on finding the second-order derivative of the target function. I learned in calculus that second derivative tells you convexity/concavity of the function. Hence, how does that help for machine learning? In other word, does it mean that if I know if the function is concave then I can tell where the global minimum error is?
