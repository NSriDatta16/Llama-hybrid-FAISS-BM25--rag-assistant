[site]: crossvalidated
[post_id]: 48279
[parent_id]: 
[tags]: 
Anomaly prediction confidence for frequentist vs bayesian parameter inference

I am comparing the behavior of some implementations of Bayesian and frequentist approaches to parametric anomaly detection and currently trying to figure out the differences when the sample set is really small. The Bayesian case looks fairly simple - it is possible to estimate the probability of a given item being from the same distribution as the data obtained and cut it at certain threshold. However, the frequentist case is not so obvious to me. I have a feeling that the frequentist approach would be to refuse to answer whether an item is anomalous or not until we get enough data to reject a null-hypothesis that it is a normal item. For example, how would a frequentist statistician approach the following problem: Given the data set [0, 0.5, -0.5] and knowing that the data comes from the normal distribution, is it possible to estimate whether any of the following elements [10, 5, 2] are anomalous with the significance level of 0.05 using the frequentist approach? What happens step by step when the training data set grows bigger, from 3 say to 100 items - at which point will it be possible to evaluate the above given test items?
