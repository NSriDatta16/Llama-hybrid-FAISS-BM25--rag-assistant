[site]: datascience
[post_id]: 12198
[parent_id]: 12195
[tags]: 
Recurrent neural networks (RNNs) can work with series as input or output or both. Even a simple one-layer RNN is effectively "deep" because it has to solve similar problems as multi-layer non-recursive networks. That is because backpropagation logic in a RNN has to account for delay between input and target, which is solved by backpropagation through time - essentially adding a layer to the network for every time step of delay between first input and last output. RNN architecture has become more sophisticated in recent years by using "gating" techniques such as Gated Recurrent Units (GRU) or Long Short Term Memory (LSTM). These have multiple trainable params - 3 or 4 - per neuron, and the schematics are more complicated than feed-forward networks. They have been demonstrated as very effective in practice , so this extra complexity does seem to pay off. Although you can research and implement RNNs yourself in a library like Theano or Tensor Flow, several neural network libraries already implement RNN architectures (e.g. Keras , torch-rnn )
