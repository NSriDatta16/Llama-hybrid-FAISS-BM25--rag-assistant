[site]: crossvalidated
[post_id]: 561768
[parent_id]: 561735
[tags]: 
The significance level of the t-test is somewhat robust against mild to moderate deviations from normality, but we must take care not to claim too much. By being overly general, the second recommendation (re $n>30$ ) is potentially dangerous. Many posts on site discuss this or similar "sample size $30$ " rules (I encourage you to search out some ) but let me give an explicit, simple example that shows how easily we can strike a problem, since it seems this particular claim can be found even in high-level courses. Angela and Bart are planning to use a pair of ordinary dice in a game, but because they're taking different roles in the game they have different concerns about possible bias. Chaina is an impartial person who will supervise the game for them. Before the game, the players want to be confident that the dice are not badly biased against them, so they ask Chaina to roll the pair of dice 100 times to help them check the dice are not going to place either of them at a severe disadvantage. The outcome "double-six" (which is an important roll in the game) is of particular concern for them both; one benefits from it occurring, and the other from it not occurring. Consequently Chaina rolls the two six-sided dice and records "1" every time they come up double-six, for each of 100 attempts. If the dice are fair and rolled properly, we should have to a good approximation a series of 100 independent Bernoulli trials (0/1 values), where the probability of 1 on each trial is 1/36. After extensive investigation and copious application of slide rules, abacuses, calculators and counting on fingers and toes, I eventually managed to convince myself that the resulting sample of 0's and 1's meets the second condition in your post. That is, the sample size of 100 appears to exceed 30, by at least several more toes than I had in my possession. As an additional check, I asked my six year old neighbor for his thoughts on the matter, and he gladly lent his expert opinion at no charge, stating that 100 was indeed "lots" larger than 30, which more than made up for his unwillingness to contribute additional hands and feet to the task of computing relative sample sizes. Consequently, according to this rule, we can proceed with our hypothesis tests with complete confidence. Angela wishes to conduct a test of whether the mean - i.e. the proportion of recorded 1's (representing having got "double six") in 100 attempts is too low -- her alternative hypothesis -- compared to that for a pair of fair dice (100/36) at the $1\%$ level. She does a (one tailed, one sample) t-test which she believes should be fine, per your information. Bart wishes to conduct a test of whether the mean - the proportion of 1's in 100 attempts is too high compared to that for a pair of fair dice (100/36) at the $5\%$ level. He does a (one tailed, one sample) t-test, which he feels assured is fine, again per your information. Chaina also wants to check the dice are fair. She decides to follow the recommendations of a number of researchers in recent years, such as Benjamin et al. (2018)[1] and Ioannidis (2018)[2] to do a two-tailed one-sample t-test at the 0.005 ( $\frac12\%$ ) level. If the dice are fair (i.e. $H_0$ is true), what's the rejection rate (i.e. the true significance level, $\alpha$ ) for each of these three tests? Try a simulation and see for yourself. Would any of them be satisfied with the rule you quote? If our protagonists were to use R(*) to check the properties of their test, they will see approximately $6\%$ rejection rate on the $1\%$ test, roughly $0.07\%$ on the $5\%$ test and approximately $6\%$ on the two-tailed test that was to be conducted at $\frac12\%$ . How is that adequate? Yet this is a pretty simple, "everyday" sort of situation. If the "30" rule only applies sometimes , shouldn't they tell us when it does work? We seem to be missing important guidance here. Clearly a more sophisticated rule is required; this one will not do. Further, the common claim that the t-test is very robust obviously needs modification. It's not like we have some bizarre edge-case here. It's clearly not level-robust in this example --- and we haven't yet touched on its power behaviour, which is even more easily impacted. * I made a point of this because it matters how your software treats a sample of all one value in the one sample t-test, which will happen occasionally. I agree with R's treatment of this situation; e.g. it gives a p-value of $0$ when the t-statistic has a non-zero numerator and a zero denominator in a two tailed test (where the hypothesized value is infinitely-many sample standard deviations from the sample mean) Any potential software differences are in any case easily avoided by considering an analogous continuous distribution (which also avoids some other potential arguments). Now our response is no longer a set of 0/1 outcomes but a continuous bimodal distribution with two very narrow peaks (i.e. where the spread around each mode is much smaller than the distance between the modes), with the lower-outcome mode having most of the probability (about 35/36 of it) and the higher-outcome peak having the remaining probability (1/36). Imagine, perhaps, that we're recording (say) energy of individual photons where the energy is quantized (so we measure an extremely narrow range around a specific value for each of two values) and we're seeing one of the energies about 35 times as frequently as the other. Or imagine any other outcome that is bimodal with very narrow peaks and one much more frequent; other example circumstances that could fit this situation are not difficult to come up with. With such a setup we will see similar results to those I present above, just from a simple continuous bimodal distribution. Some example R code: pp = replicate(100000, t.test(rbinom(100, 1, 1/36), mu=1/36, alternative="less")$p.value) mean(pp To conduct the corresponding continuous data versions of the tests you could add a random (scaled and centered) symmetric beta value to each observation, with center 0 and range of something like (say) $10^{-4}$ times the distance between the modes. [1] Benjamin, D.J., Berger, J., Johannesson, M., Nosek, B., Wagenmakers, E., Berk, R., et al., "Redefine statistical significance", Nat. Hum. Behav. , 2018, no. 2, pp. 6–10. [2] Ioannidis JPA., "The proposal to lower P-value thresholds to .005", JAMA. 2018; 319:1429–30.
