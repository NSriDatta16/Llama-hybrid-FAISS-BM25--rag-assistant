[site]: datascience
[post_id]: 53394
[parent_id]: 53385
[tags]: 
Accuracy on the training data basically doesn't count. I don't quite want to say to ignore it, because a train/test accuracy of 100/70 seems different to me than a train/test accuracy of 71/70, but you're mostly not interested in performance on the training data. Using a test set mimics a real application of machine learning. Think about Siri or Alexa. The goal is to predict speech that it hasn't heard. There's no way to know how it will perform on such speech, so the next-best approach is to use some data where you know the answer but hide it from your model. After you train the model, assess how it performs on data where it has not seen the answer. If the model is accurate, then that's a good sign about its ability to perform on real speech recognition tasks. Training data is like the practice questions or homework problems, while test data is the exam.
