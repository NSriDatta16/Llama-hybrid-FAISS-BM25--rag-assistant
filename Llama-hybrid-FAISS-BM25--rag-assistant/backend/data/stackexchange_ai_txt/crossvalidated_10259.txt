[site]: crossvalidated
[post_id]: 10259
[parent_id]: 10049
[tags]: 
If you are using tree-based methods, you can play around with the splitting criterion. For example, at each step, choose the split that gives the highest weighted accuracy (the average of the two classes' accuracies). This can be used as the basis for a random forest too, which should give you a good classifier. I once used a similar process to boost precision while sacrificing recall. It worked very well (better than thresholding the scores from the classification algorithm which were very noisy anyway).
