[site]: crossvalidated
[post_id]: 192680
[parent_id]: 192662
[tags]: 
Typically, given a 50% dropout (standard), the weights are multiplied by 0.5 when predicting new instances. The gradient is always computed as the average over minibatches, without scaling. That said, dropout networks use a far higher learning rate (10 to 100 times the usual values), so you can think of that as a form of scaling. You can read all about it in the paper that introduced dropout .
