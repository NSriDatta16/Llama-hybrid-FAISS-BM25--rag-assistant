[site]: datascience
[post_id]: 86719
[parent_id]: 86597
[tags]: 
One option is a model comparison approach. Start with simpler models then progressively trying more complex models. Along the way, checking if the additional complexity is resulting in increased predictive ability. If you take a simplifying representation with a bag-of-words model , you can fit a Naive Bayes classifier. If you make the Markov assumption , you'll only need to look at the last element in the sequence. Then it can be modeled as a simple Probabilistic Graphical Model (PGM). Then start relaxing the Markov assumption by looking back at progressively more time steps. Recurrent Neural Network (RNN) might work if you have thousands of labeled data points and the relationship is very complex. Long Short Term Memory (LSTM) does not make much sense because the sequences are so short. LSTMs work better RNNs when the relevant information could possible be many time steps back. My hypothesis is that a Naive Bayes classifier will be a good-enough model because there are only 4 features and the sequences are so short they can be ignored.
