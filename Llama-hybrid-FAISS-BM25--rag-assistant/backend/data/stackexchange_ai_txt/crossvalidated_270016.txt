[site]: crossvalidated
[post_id]: 270016
[parent_id]: 
[tags]: 
When creating an SVM, if the training set is too large, can we use partial set of landmarks?

When creating an SVM, if the training set is too large, can we use partial set of landmarks? For example, if the training set is 1,000,000, it may be too computationally hard to use SVM, since there would now be 1,000,000 new features for the SVM. Could we do something like use a random 1% of the features? We could use a random 1% of the training set instead to get 10,000 features, but why not just use a random 1% for the landmarks and train with all 1,000,000 training samples?
