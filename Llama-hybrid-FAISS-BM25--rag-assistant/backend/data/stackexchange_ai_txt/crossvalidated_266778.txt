[site]: crossvalidated
[post_id]: 266778
[parent_id]: 266749
[tags]: 
It means that your chain most likely did not converge. By this I mean you should be wary of the entire chain, not just worry about the dimensions with low effective number of samples. Solutions Burn-in (discarding early part of the chain) - see also this question Sometimes a low effective number of samples is just because the chain started in a low-probability region, and found the basin of convergence (the high probability region, or typical set) only later on. I do not recommend that now you just play with burn-in, since with only one chain it's hard to tell what's going on. Instead, you might want to run a few chains in parallel, as opposed to a single long chain (see MacKay's book, Chapter 29 ). With multiple chains it is usually easier to spot lack of convergence, although there are different schools of thought here on the number of chains (I usually do 3 or 4). Regarding burn-in, there are also several opinions. Some people, such as Geyer , say that it's pointless, as long as you are sure that you start in a high probability region: Any point you don't mind having in a sample is a good starting point. However, this is easier said than done in 150 dimensions. State-of-the-art statistical packages burn-in as much as 50% of the chain (see Stan ), but one of the reasons for such a long burn-in is that it is also used to adapt some of the sampler parameters. In your case, I definitely recommend that you initialize your sampler from a good guess (e.g., somewhere around the mode is better than nothing, although the mode itself is likely not in the typical set), and do some burn-in since in high dimensions it's hard to know where the probability mass resides (not the same as the probability density). In my opinion, it's better to burn-in more than less (at worst, you are simply throwing away some effective samples, whereas if you do not burn-in you might be keeping samples that are not representative of the target distribution). Sample more Simple enough, start where your MCMC chain ended (or from some other good guess, as mentioned above), discard the previous chain, and take more samples overall. Better tune your sampler I don't know which MCMC method you are using, but most samplers have several tunable hyper-parameters (e.g., jump length(s) for Metropolis-Hastings with Gaussian proposal, mass matrix for Hamiltonian Monte Carlo, etc.). Performance of most MCMC samplers heavily depends on the choice of these parameters, so you might have a look at your sampled distribution and figure out whether you can improve them. Change sampler Sometimes, the sampler you are using might be not the best choice. For example, if your problem deals with continuous variables with a smooth, differentiable target distribution, you probably want to use some variant of Hamiltonian Monte Carlo (state of the art is NUTS , as implemented in Stan ). Change parameterization As suggested by @Bj√∂rn in the comments, you might want to try a different parameterization of your model. Simple reparameterizations amount to a linear or nonlinear transformation of your variables; the former, for example, to reduce correlation between variables, and the latter, for example, to get rid of long, fat tails in the posterior which may be hard to sample from. More complex reparameterizations include the addition of auxiliary variables that might help mixing (e.g., by bridging different regions of the posterior), but this becomes extremely model-dependent.
