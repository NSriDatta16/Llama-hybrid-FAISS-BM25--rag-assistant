[site]: crossvalidated
[post_id]: 366207
[parent_id]: 366165
[tags]: 
Potentially yes , the adversary that has access to the entire model can learn pretty much about your data. Recall what random forests are: ensembles of decision trees trained on random subsets of data (usually both: subsets of rows and columns). Using the example from this tutorial , decision tree splits the data into branches, conditionally on the training data. The example below shows like the data is split based on the sex, age and the number of siblings & spouses abord, using the Titanic dataset . First split is based on sex and we learn from it that 36% of the dataset are females who survived the Titanic disaster. Next split shows that there is 61% of males older then 9.5 years who died during the disaster etc. You can clearly see that the decision tree has memorized many of the characteristics of the dataset. The information is partial because we calculate only some conditionals (e.g. from the tree above we learned nothing about age of the males) and with continuous predictors we bin them into two categories at each split. Moreover, since while constructing random forest you took random samples from the data, the numbers won't be exact (the error depends on the fraction you subsample). On another hand, since you usually use large number of such trees, taken together they give you more precise picture. As the trees were trained on random subsets, each tree can use different variables and different splits, together giving you more detailed view on the underlying data (single tree can make a split on age > 9.5 , but multiple trees could make many different splits etc.). If you have many trees and many splits, then also the contentious variables will eventually get spitted into many different bins. The answer on if data aggregated in such shattered was as it is done by random forest can be used to de-anonymize the underlying data, would depend on many things. First, it would depend on the data used for training the model: are the variables detailed enough to recognize the individual participants? If you recorded only some very broad demographic details, then possibly this may not be the case, but combinations of those features, or features with very skewed distributions (rare features) could be used for de-anonymization. Second, this would depend to great extent on the characteristics of the model: how big were the subsampling fractions?; how many trees did you use?; did you enforce the minimum number of cases per branch to be a relatively large value?; did you use any pruning? etc. Basically, the mode "tuned" the trees would be, the more noise you introduce and the more anonymized your data will be (but I guess, this would be very hard to measure and quantify!). If you use large fractions of the data for splits, many trees, allow for small branches and use no pruning, then the random forest may memorize quite detailed picture of your training data.
