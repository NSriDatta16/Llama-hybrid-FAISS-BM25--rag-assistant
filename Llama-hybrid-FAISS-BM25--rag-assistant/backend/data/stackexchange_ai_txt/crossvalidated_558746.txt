[site]: crossvalidated
[post_id]: 558746
[parent_id]: 
[tags]: 
Does feature selection always benefit the performance of a ML model?

In my ML pipeline, I normally perform feature selection, by performing a few of the tests mentioned below, the ones relevant to the model. I tend to drop features with negative outlier to the rest of the features, but I wonder whether more data is better, and leave the feature selection to the algorithm to dechiper which features to prioritise. Is identifying and removing extremely multicollinear regression variables always in benefit of the performance of such a model? Is identifying features in a classification model which do no depend on the label set via a Chi-squares test for independence always in benefit of the performance of such a model? The features_importances function in sklearn measures the average gain of purity by splits of a given variable. In the cases where the importance level is extremely low, it is always advisable to drop such features.
