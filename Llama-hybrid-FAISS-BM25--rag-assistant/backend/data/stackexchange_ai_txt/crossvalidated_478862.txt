[site]: crossvalidated
[post_id]: 478862
[parent_id]: 
[tags]: 
Do we update a priori distribution somehow?

I'm trying to understand Bayesian statistics. Recently I asked here whether we estimate paramteres of a priori distribution in bayesian statistics. I was responded that we typically don't estimate them unless we're using Empirical Bayes and because we're going to "update" a priori distribution anyway. In wikipedia I've read Conjugate priors are especially useful for sequential estimation, where the posterior of the current measurement is used as the prior in the next measurement. In sequential estimation, unless a conjugate prior is used, the posterior distribution typically becomes more complex with each added measurement, and the Bayes estimator cannot usually be calculated without resorting to numerical methods. I thought that maybe we assume some a priori distribution, get our observations, calculate a posteriori distribution, treat it as our a priori distribution and we repeat this procedure untill convergence. Unfortunately I've realised that this doesn't make sense since for example for Poisson-Gamma with a priori with parameters $\gamma,\ \beta$ the a posteriori is again a gamma distribution with parameters $$ \gamma'=\gamma+\sum_{j=1}^n X_j$$ $$\beta'=\beta+n$$ and such parameters connot be "convergent". So: (a) why we don't need to bother ourselves with the exact form of a priori distribution in pure bayesian statistics? (b) how do we "update" a priori distribution? (c) what exactly the sequential estimation means?
