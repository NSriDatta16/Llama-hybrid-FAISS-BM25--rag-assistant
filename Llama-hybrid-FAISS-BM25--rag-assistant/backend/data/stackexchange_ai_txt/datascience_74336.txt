[site]: datascience
[post_id]: 74336
[parent_id]: 74316
[tags]: 
It is relatively common when learning basics of RL (as opposed to Deep RL with neural networks etc), to consider a single discrete state variable. For instance many grid worlds, maze solvers etc just enumerate the positions. For practical learners, the variable is effectively one-hot encoded, but it is still a single variable. The number of state features is not relevant to whether RL is applicable or not. Assuming you have correctly formulated your problem as a Markov Decision Process (MDP), then the important things to consider are: Do the state variables capture enough data about the current state so that the problem has the Markov property (that predictions of future rewards and state can be made using only the current state and action choices)? Do the state variables need to be pre-processed before being used as features for your learning algorithm? Is your state space, once turned into features, small and suitable for learning with a simple tabular method? If the state space is not small, does a simple transformation (such as using a discrete tiling) make the problem simple, possibly linear, or is it complex? Simple problems can be adjusted to work with tabular approaches or linear approximators, complex ones will need approaches from Deep RL, such as DQN or A3C.
