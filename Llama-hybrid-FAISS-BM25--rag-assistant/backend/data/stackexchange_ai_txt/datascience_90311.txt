[site]: datascience
[post_id]: 90311
[parent_id]: 90310
[tags]: 
The most often used method of optimising neural networks is a process called (stochastic) gradient descent. You provide the network the inputs and expected output, during training the model outputs get compared to the expected output. The difference between the two is what is called the error or loss. Based on how wrong or right the network is, you can calculate how should adjust the parameters/internal weights of the model to lower the error of the model. A more in-depth explanation of gradient descent (and neural networks in general) can be found on this site .
