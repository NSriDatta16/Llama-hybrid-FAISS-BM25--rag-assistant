[site]: crossvalidated
[post_id]: 461351
[parent_id]: 
[tags]: 
What is use of XGboost objective function/what's best objective function/best w as in the docs

I am studying how XGboost works but I got completely confused and lost at the end. I am aware how the GBM works and boosting works. I even completely understand the xgboost docs (atleast I thought) to some point. https://xgboost.readthedocs.io/en/latest/tutorials/model.html#additive-training How/where is the objective given above in terms of Gradient, Hessian and the regularization be used in the actual tree building ? I thought initially the objective is node split criteria to compute gain (as in docs last section), but looks like split criteria is gini, entropy, variance (as in case of usual decision trees) Say if we are building a regression or classification model(which fit regression trees as well - at log odds level), we use the below steps. 1. first constant is initialized as prediction and we compute the gradient of loss (given by mse, log loss) w.r.t predicted function. 2. Then we try to fit a tree to the -grad(loss) w.r.t previous prediction. The splits in the tree are made based on gini, entropy, variance criteria. 3. the final leaf values are mean of the observations target values in the leaves. Now, in these steps, 1.) where does the objective function of the XGboost (G,H, regularized) fit ? Is it the same as the loss function in step 1 above (using any arbitrary loss with also including regularization ?). 2.) If that's case what do the authors mean by 'best objective function' in 'the structure score section' of docs. 3.) And how/why is it used in the gain computation ? 4.) Also, as in the docs, what is the best 'w' (in the structure score section) and why do we need to compute the best value, doesn't it have only a single value for observation. I thought w is just the mean of target value of all observations in the leaves. where each target value is just the - grad of loss Thank you so much for any responses and pardon my ignorance.
