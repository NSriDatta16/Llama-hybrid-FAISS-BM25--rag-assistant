[site]: crossvalidated
[post_id]: 309433
[parent_id]: 
[tags]: 
Is it problematic if most of the weights or biases in a hidden layer are the same sign?

I'm trying to diagnose overfitting in my multi-layer perceptron by looking at the weights, biases and gradients in each layer. I'm noticing that in the neural network that is overfitting, the weights in one of the hidden layers all have the same sign, and the majority of the biases in all the hidden layers have the same sign. In the network that isn't overfitting, the biases are centred on zero, with rougly equal numbers of positive and negative values. I'm using xavier initialization for both.
