[site]: crossvalidated
[post_id]: 287249
[parent_id]: 287242
[tags]: 
In the generative adversarial networks, Generator is used for generating 'fake' samples that look very likely to 'real' samples, and Discriminator is to distinguish the differences between 'fake' and 'real' samples. The training process is to train Generator and Discriminator iteratively. Discriminator finds differences between 'fake' and 'real' samples; Generator tries to fool Discriminator by generating most similar samples. When Discriminator cannot find the differences, Generator can successfully generate 'real' samples. So in your problem, when training Generator , the Discriminator is fixed and then plays a role as loss function for Generator . This loss function is called adversarial loss . Formally, I use the notation in the original paper of GAN , the loss function of GAN is given as follows: $$ \min_{G} \max_{D} V(D,G)=\mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))] $$ Training Discriminator when Generator is fixed, $$ \max_{D} V(D,G^*)=\mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G^*(z)))] $$ where $G^*$ means $G$ is fixed. We can see that optimizing $D$ is to maximzing the difference between 'real' sample $x$ and 'fake' sample $G^*(z)$. Training Generator when Discriminator is fixed, $$ \min_{G} V(D^*,G)=\mathbb{E}_{x\sim p_{data}}[\log D^*(x)] + \mathbb{E}_{z\sim p_{z}(z)}[\log(1-D^*(G(z)))] $$ where the first term on right hand can be ignored because of constant term for $G$. Above is the loss function for training Generator . The loss function value is computed from fixed Discriminator with 'real' samples $x$ and 'fake' samples $G(z)$. Hope that's clear to you.
