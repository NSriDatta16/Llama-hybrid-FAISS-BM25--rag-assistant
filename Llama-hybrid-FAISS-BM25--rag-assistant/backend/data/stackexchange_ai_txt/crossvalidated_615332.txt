[site]: crossvalidated
[post_id]: 615332
[parent_id]: 
[tags]: 
Likelihood being driven to zero because of large number of model-observation misfits

I am performing a Bayesian calibration of a computer model and wondering if I am setting up my likelihood correctly. I have model output generated using Monte Carlo sampling of prior distributions of model parameters. I then assign a score ("likelihood") to each simulation in my ensemble using model-observation misfit: \begin{equation} s_j = \exp \left[ -\frac{1}{ 2\sigma^2 } \sum_i{ \left(f_i^j - z_i\right)^2 } \right] \end{equation} where $f$ is the modeled quantity, $z$ is the observed quantity, $\sigma^2$ is the misfit variance, $j$ is the index of the ensemble members, and $i$ is the index of the observations. The assumptions here are that the misfits are i.i.d. and normally distributed. I then go on to normalize the scores, $s_j$ , to obtain a set of weights that are used to scale my prior distributions and obtain posteriors. My confusion stems from the fact that, if there are many misfits that are used in the calculation of $s_j$ (i.e., there are many $i$ 's in the above equation), the score, $s_j$ , that's calculated for every ensemble member will be driven to zero. In general, for any value of $\sigma$ , if there is a large enough sample of misfits, all of the $s_j$ 's will be, essentially, zero. What am I missing here? Am I violating some underlying assumption in how I'm constructing my likelihood and calculating $s_j$ ? As a little side test, I made a Python script that generates random samples of the misfit, $\left(f_i^j - z_i\right)$ , as i.i.d. samples from a normal distribution and then calculates $s_j$ . With this setup, where I have 100 misfits, $s_j$ is already really small (1e-19). In my real case, I have even more misfits and the $s_j$ 's are even smaller. mu = 0. sigma = 50. n = 100 r = np.random.normal(loc=mu, scale=sigma, size=(n,)) s_j = np.exp( (-1. / (2. * sigma**2)) * np.nansum( r**2 ) )
