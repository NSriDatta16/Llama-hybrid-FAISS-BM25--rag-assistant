[site]: crossvalidated
[post_id]: 542055
[parent_id]: 542054
[tags]: 
Whether you need a softmax layer to train a neural network in PyTorch will depend on what loss function you use. If you use the torch.nn.CrossEntropyLoss , then the softmax is computed as part of the loss. From the link: The loss can be described as: $$ \text{loss}(x,class) = −\log\left(\frac{\exp⁡(x[class])}{\sum_j \exp(x[j])}\right) $$ This loss is just the concatenation of a torch.nn.LogSoftmax followed by the torch.nn.NLLLoss loss. From the documentation of torch.nn.CrossEntropyLoss : This criterion combines LogSoftmax and NLLLoss in one single class. and from the documentation of torch.nn.NLLLoss : Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. It seems that the developers of these pretrained models had the torch.nn.CrossEntropyLoss in mind when they were creating them.
