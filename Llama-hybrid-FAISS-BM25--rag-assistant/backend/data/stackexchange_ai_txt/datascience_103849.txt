[site]: datascience
[post_id]: 103849
[parent_id]: 103836
[tags]: 
Without any information about the data it's hard to answer. First, you should be able to estimate whether the merge should indeed produce 18 billion rows: is it the expected result or is it a mistake? You can investigate this by using a subset of your real data in one or both of the dataframes, and see how fast the resulting dataframe grows. If this merge operation is a full cartesian product with two huge dataframes, then it could be the normal result and it's a problem of design. Obviously there's a physical limit on the amount of memory, at some point it's not surprising that you hit this limit with this kind of size. In this case a solution could be to implement a specific code with a more efficient data structure, or using disk storage instead of memory storage.
