[site]: datascience
[post_id]: 49006
[parent_id]: 
[tags]: 
Given a single discrete data set, how should I divide it into training data and test data?

I have a dataset in libSVM format consisting of 6000 entries, each with 5 indices, and each index has a binary value 1 or 2. Each of the 6000 entries has a label of 1 or 0, and I am trying to use various machine learning algorithms to determine the correct label (0 or 1) given a particular set of 5 indices/values. For example, consider the following dataset (the real one is 6000 lines): 0 101:1 102:1 103:0 104:1 105:1 0 101:0 102:1 103:0 104:1 105:1 0 101:0 102:1 103:1 104:1 105:1 1 101:1 102:1 103:1 104:1 105:1 1 101:0 102:1 103:0 104:0 105:1 1 101:1 102:1 103:1 104:0 105:0 1 101:0 102:1 103:0 104:0 105:0 For an algorithm that predicts binary classification, like xgboost , conceptually, how do I first use my dataset to train the model, and then apply the model to the data? I ask because xgboost asks for two files, a data training set and a data test set. It seems to me that the algorithm should just require a single full set of data, use all of the data to train and build a model, and then apply that model to the original data set and determine if the labels are being assigned "0 or 1" accurately. Any help in understanding this concept is much appreciated.
