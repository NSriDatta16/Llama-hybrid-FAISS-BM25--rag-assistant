[site]: crossvalidated
[post_id]: 448997
[parent_id]: 448987
[tags]: 
You seem to be talking about posterior predictive distribution , i.e. the a posteriori distribution of the data. You don't see $\theta$ , because it is marginalized over possible parameter values. The distribution of the data given some particular parameter value is the likelihood function $P(x|i) = \phi_i(x)$ . Regarding your comments, I guess the other thing that you may mean is Bayesian updating. Given some data we update a prior $$ p(\theta | x) \propto p(x | \theta)\,p(\theta) $$ next, knowing this you can use the posterior as a prior to be updated with new data $\tilde{x}$ , $$ p(\theta|x,\tilde{x}) \propto p(\tilde{x}|\theta,x) \, p(\theta|x) $$ By the chain rule , this can done in one step. So you just plug-in the posterior estimates given $x$ as a prior for the likelihood for $\tilde{x}$ , this may be what you are asking about.
