[site]: crossvalidated
[post_id]: 619174
[parent_id]: 619020
[tags]: 
In hard-margin SVM, "If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible" . The first thing to do is plot the points to see whether linear separability holds. The colored regions show the convex hulls of the two sets of points. Clearly we don't have to consider any points within their interiors: they cannot serve as support points. Thus, we can ignore $(2,2)$ altogether. Because the linear separability is visually evident, we can easily draw some line that separates these classes. The line $y=4$ works, for instance, as shown. Parallel to that line are the lines of the form $y=h$ for numbers $h.$ Any value of $h$ between $3$ and $5$ will also separate the classes. The line $y=4$ evidently lies at a distance from class 1 of $4 - 3 = 1$ and a distance from class -1 of $5 - 4 = 1.$ Those distances are found by dropping straight down from the vertex $(4,5)$ to the line or moving straight up from the vertex $(3,3)$ to the same line. This enables us to connect these vertices of the classes with the path $$(4,5)\to (4,4)\to (3.5,4)\to (3,4)\to (3,3)\tag{*}$$ where the middle edge $(4,4)\to (3.5,4)\to (3,4)$ moves along the line itself. On the other hand, the dotted segment between $(3,3)$ and $(4,5)$ follows the straight path $$(4,5)\to(3.5,4)\to(3,3)$$ This path consists of the hypotenuses of two right triangles. Two of their edges are the (vertical) segments projecting the support points to the separating line. Consequently the length of the dotted line is at least as great as the distance between $(4,5)$ and $(3,3)$ perpendicular to the separating line. On the other hand, if we were to construct a perpendicular at any point along the dashed segment, the path $(*)$ would coincide with the dotted line. The previous inequality, valid for all separating lines, reduces to an equality in this special case. You can exploit these ideas to prove generally that the optimal hard-margin hyperplane bisects any line segment that realizes the shortest distance between two linearly separable subsets of $\mathbb R^n.$ The support vectors can be read off the figure. The distance from this hyperplane to either class is half the distance between the classes. As an exercise, redo this analysis replacing the point $(3,3)$ in class 1 with the point $(5,3).$ As a hint, a line segment realizing the shortest distance now goes from $(5,3)$ to $(5,5).$ (Why did I write "a" line segment? Redo the problem once more upon replacing $(3,3)$ by the pair of points $(3,3), (7,3).$ You should find many line segments that realize the shortest distances between the classes.) An hour or so spent studying convex analysis -- definitions of convexity, of convex hulls, characterizations and properties of convex sets, and the proofs of equivalence of those characterizations -- will have an immediate payoff for the intuition as well as your understanding of SVM (and of many fundamental ideas of optimization, too).
