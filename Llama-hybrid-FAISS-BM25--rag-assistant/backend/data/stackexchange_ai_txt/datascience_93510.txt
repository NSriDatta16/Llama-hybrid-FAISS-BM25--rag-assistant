[site]: datascience
[post_id]: 93510
[parent_id]: 
[tags]: 
Different training method for encoder-decoder model

Trying to learn the encoder-decoder model for some NLP problems. I am referring to this Keras tutorial . During the model training phase, this tutorial just uses the following: model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs) I understand this logic. But the confusion is in some other tutorials for EXACTLY THE SAME PROBLEM. For example, in Tensorflow's documentation for NMT with Attention the training_phase is very different where they use custom training loops with a custom train step and calling the step for every batch manually. The question is are these 2 different training methods which should be used in particular cases OR its the same training method with 2 different forms of implementation?
