[site]: crossvalidated
[post_id]: 486424
[parent_id]: 
[tags]: 
Deriving posterior update equation in a Variational Bayes inference

I'm reading a paper ( He, et al. 2010 ) that has used variational Bayesian inference to solve an inverse problem. I have difficulties deriving the relations for updating the variational approximations used in this problem. Problem statement: Suppose observed data $\boldsymbol{v}$ is yield by $$ \boldsymbol{v}=\boldsymbol{\Phi} \boldsymbol{\theta}+\boldsymbol{\epsilon} $$ where $\boldsymbol{\Phi}\in\mathbb{R}^{n\times N}$ is a random projection matrix, $\boldsymbol{\epsilon}\in\mathbb{R}^{n}$ , and $\boldsymbol{\theta}\in\mathbb{R}^{N}$ . Given noisy observation $\boldsymbol{v}\in\mathbb{R}^{n}$ , we would like to recover the sparse signal $\boldsymbol{\theta}$ . Reconstruction method: The noisy observations can be modeled as $$ \boldsymbol{v}\sim p(\boldsymbol{v} \mid \boldsymbol{\theta}, \alpha_{0}) = \mathcal{N}\left(\boldsymbol{\Phi} \boldsymbol{\theta}, \alpha_{0}^{-1} \boldsymbol{I}\right) $$ with $$ \alpha_{0} \sim \operatorname{Gamma}\left(a_{0}, b_{0}\right) $$ The spareness of $\boldsymbol{\theta}$ is modeled by $$ \boldsymbol{\theta}=\boldsymbol{w} \odot \boldsymbol{z}, \quad\text{with } w_{k, s, i} \sim \mathcal{N}\left(0, \alpha_{s}^{-1}\right), \quad z_{k, s, i} \sim \operatorname{Bernoulli}\left(\pi_{k, s, i}\right) $$ where $\odot$ is Hadamard product, with $$ \pi_{k, s, i}=\left\{\begin{array}{ll}\pi^{s}, & s=0,1 \\ \pi^{s 0}, & 2 \leq s \leq L, z_{p a(k, s, i)}=0 \\ \pi^{s 1}, & 2 \leq s \leq L, z_{p a(k, s, i)}=1\end{array}\right. $$ \begin{aligned} \alpha_{s} & \sim \operatorname{Gamma}\left(c_{0}, d_{0}\right), \quad 0 \leq s \leq L \\ \pi^{s} & \sim \operatorname{Beta}\left(e_{0}^{s}, f_{0}^{s}\right), s=0,1 \\ \pi^{s 0} & \sim \operatorname{Beta}\left(e_{0}^{s 0}, f_{0}^{s 0}\right) \\ \pi^{s 1} & \sim \operatorname{Beta}\left(e_{0}^{s 1}, f_{0}^{s 1}\right), 2 \leq s \leq L \end{aligned} We use a set of distributions $q(\Theta)$ , with $\Theta=\left\{\boldsymbol{w}, \boldsymbol{z}, \alpha_{0},\left\{\alpha_{s}\right\}_{s=0: L}, \pi^{0}, \pi^{1},\left\{\pi^{s 0}, \pi^{s 1}\right\}_{s=2: L}\right\}$ , to approximate the posterior $p(\Theta|\boldsymbol{v})$ . To this end, we derive a lower-bound $\mathcal{L}$ and maximize it until the model converges. My question: I'm having a hard time to understand the update equations for the posterior distributions. The paper present them as follows $$ q\left(z_{k, s, i}\right)=\operatorname{Beronulli}\left(z_{k, s, i} \mid p_{k, s, i}\right) $$ $$ q\left(w_{k, s, i}\right)=\mathcal{N}\left(w_{k, s, i} \mid \mu_{k, s, i}, \sigma_{k, s, i}^{2}\right) $$ $$ q\left(\alpha_{0}\right)=\operatorname{Gamma}\left(\alpha_{0} \mid a, b\right) $$ $$ q\left(\alpha_{s}\right)=\operatorname{Gamma}\left(\alpha_{s} \mid c_{s}, d_{s}\right) $$ $$ q(\boldsymbol{\pi})=\prod_{s=0}^{1} \operatorname{Beta}\left(\pi^{s} \mid e^{s}, f^{s}\right) \prod_{s=2}^{L}\left[\operatorname{Beta}\left(\pi^{s 0} \mid e^{s 0}, f^{s 0}\right) \operatorname{Beta}\left(\pi^{s 1} \mid e^{s 1}, f^{s 1}\right)\right] $$ My understanding so far is that we had previously defined a set of prior distributions to model the problem. We choose a set of variational approximate posteriors $q$ and iteratively maximize a lower-bound on the log-likelihood (although, I thought it should be the marginal log-likelihood instead, but seems the paper is just stating log-likelihood) to update the $q$ and shape it into the posterior $p$ . We choose $q$ similar to the prior model, with it's own parameters and update the parameters. For instance, paper states that $$ \sigma_{k, s, i}^{2}=\left(\left\langle\alpha_{s}\right\rangle+\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}^{2}\right\rangle \mathbf{\Phi}_{(j)}^{T} \mathbf{\Phi}_{(j)}\right)^{-1} $$ and $$ \mu_{k, s, i}=\sigma_{k, s, i}^{2}\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}\right\rangle \boldsymbol{\Phi}_{(j)}^{T}\left(\boldsymbol{v}-\sum_{l=1 \atop l \neq j}^{N} \boldsymbol{\Phi}_{(l)}\left\langle z_{(l)}\right\rangle\left\langle w_{(l)}\right\rangle\right) $$ I'm having a hard time seeing where the update equations are coming from. My Attempt I have used two approaches, but none got close to the relation above. One was to find a lower-bound by writing down the KL-distance between $q$ and $p$ . The other is using Mean field approximation. Below I have detailed my attempts Mean field approximation: First, I'm not sure if this is the approach that the paper has taken, as it specifically states that it founds a lower bound. Nonetheless, here goes my attempt: We write the joint probability of the variables as $$ p\left(v, w, z, \alpha_{0}, \alpha_{s},\pi\right) = p\left(v \mid w, z, \alpha_{0}\right) p\left(\omega \mid \alpha_{s}\right) p(z \mid \pi) p\left(\alpha_{0}\right) p\left(\alpha_{S}\right) p(\pi) $$ \begin{align} \ln q^*(w) &= \mathbb{E}_{z, \alpha_{0}, \alpha_{s},\pi}[\ln p(v, w, z, \alpha_{0}, \alpha_{s},\pi)] + C\\ &=\mathbb{E}_{z, \alpha_{0}, \alpha_{s},\pi}[\ln p\left(v \mid w, z, \alpha_{0}\right) p\left(\omega \mid \alpha_{s}\right) p(z \mid \pi) p\left(\alpha_{0}\right) p\left(\alpha_{S}\right) p(\pi)] + C\\ &=\mathbb{E}_{z, \alpha_{0}, \alpha_{s},\pi}[\ln p\left(v \mid w, z, \alpha_{0}\right) p\left(\omega \mid \alpha_{s}\right)] + C_2\\ &=\mathbb{E}_{z, \alpha_{0}}[\ln p\left(v \mid w, z, \alpha_{0}\right)]+\mathbb{E}_{\alpha_{s}}[\ln p\left(\omega \mid \alpha_{s}\right)] + C_2 \end{align} The way I understand it, this will lead to the probability density function for the normal distribution $ q\left(w_{k, s, i}\right)=\mathcal{N}\left(w_{k, s, i} \mid \mu_{k, s, i}, \sigma_{k, s, i}^{2}\right)$ and that way we find the update relations for the hyperparameters $\mu_{k, s, i}$ and $\sigma_{k, s, i}^{2}$ . \begin{align} \mathbb{E}_{z, \alpha_{0}}[\ln p\left(v \mid w, z, \alpha_{0}\right)]+\mathbb{E}_{\alpha_{s}}[ \ln p\left(\omega \mid \alpha_{s}\right)] =& \mathbb{E}_{z, \alpha_{0}}[\ln \mathcal{N}\left(\mathbf{\Phi} \boldsymbol{\theta}, \alpha_{0}^{-1} \boldsymbol{I}\right)]+\mathbb{E}_{\alpha_{s}}[\ln \mathcal{N}\left(0, \alpha_{s}^{-1}\right)]\\ =& \mathbb{E}_{z, \alpha_{0}}[\ln \mathcal{N}\left(\mathbf{\Phi} \boldsymbol{\theta}, \alpha_{0}^{-1} \boldsymbol{I}\right)]+\mathbb{E}_{\alpha_{s}}[\ln (\frac{1}{\sqrt{2\pi\alpha_s^{-1}}}\exp(\frac{-w^2}{2\alpha_s^{-1}}))]\\ =&\mathbb{E}_{z, \alpha_{0}}[\dots]+\mathbb{E}_{\alpha_{s}}[\ln(\sqrt{\alpha_s})-\ln(\sqrt{2\pi})-\frac{w^2}{2\alpha_s^{-1}}]\\ =&\mathbb{E}_{z, \alpha_{0}}[\ln((2 \pi)^{-\frac{n}{2}} \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})^{-\frac{1}{2}}\\ &\exp(-\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})))]\\ &+\mathbb{E}_{\alpha_{s}}[\frac12\ln(\alpha_s-2\pi)-\frac{w^2}{2\alpha_s^{-1}}]\\ =&\mathbb{E}_{z, \alpha_{0}}[-\frac{n}{2}\ln(2 \pi)-\frac{1}{2}\ln( \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I}))\\ &-\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})]\\ &+\mathbb{E}_{\alpha_{s}}[\frac12\ln(\alpha_s-2\pi)-\frac{w^2}{2\alpha_s^{-1}}]\\ =&-\mathbb{E}_{z, \alpha_{0}}[\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})]-\mathbb{E}_{\alpha_{s}}[\frac{w^2}{2\alpha_s^{-1}}]\\ &+\mathbb{E}_{\alpha_{s}}[\frac12\ln(\alpha_s-2\pi)]+\mathbb{E}_{z, \alpha_{0}}[-\frac{n}{2}\ln(2 \pi)-\frac{1}{2}\ln( \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I}))]\\ &+C\\ =&-\mathbb{E}_{z, \alpha_{0}}[\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})]-\mathbb{E}_{\alpha_{s}}[\frac{w^2}{2\alpha_s^{-1}}]+C_2 \end{align} So I expect this derivation to be equivalent with plugging $\sigma_{k, s, i}^{2}$ and $\mu_{k, s, i}$ into $q\left(w_{k, s, i}\right)$ . So I expect to get from the derivation above to \begin{align*} q\left(w_{k, s, i}\right)=&\frac{1}{\sqrt{\left(\left\langle\alpha_{s}\right\rangle+\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}^{2}\right\rangle \mathbf{\Phi}_{(j)}^{T} \mathbf{\Phi}_{(j)}\right)^{-1}2 \pi}}\\ &\exp(-\frac{1}{2}\left(\frac{w_{k, s, i}-\left(\left\langle\alpha_{s}\right\rangle+\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}^{2}\right\rangle \mathbf{\Phi}_{(j)}^{T} \mathbf{\Phi}_{(j)}\right)^{-1}\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}\right\rangle \boldsymbol{\Phi}_{(j)}^{T}\left(\boldsymbol{v}-\sum_{l=1 \atop l \neq j}^{N} \boldsymbol{\Phi}_{(l)}\left\langle z_{(l)}\right\rangle\left\langle w_{(l)}\right\rangle\right)}{\sqrt{\left(\left\langle\alpha_{s}\right\rangle+\left\langle\alpha_{0}\right\rangle\left\langle z_{k, s, i}^{2}\right\rangle \mathbf{\Phi}_{(j)}^{T} \mathbf{\Phi}_{(j)}\right)^{-1}}}\right)^{2}) \end{align*} But I can't quite make the connection. My 2nd Attempt Based on the answer from @Microhaus, I've tried to find a lower-bound for $\sigma$ $$ L = \mathbb{E}_{q}[\ln p(\Theta, \mathbf{v})] - \mathbb{E}_q[\ln q(\boldsymbol{\pi}, \alpha_0, \left\{ \alpha_s \right\}_{s=0:L}, \mathbf{w} , \mathbf{z})] $$ Denoting those parts of the ELBO in which $\sigma_{k, s, i}$ appears as $L_{[\sigma_{k, s, i}]}$ , we have: \begin{align} L_{[\sigma_{k, s, i}]} =& \mathbb{E}_q[\ln(p(\mathbf{v} | \alpha_0, \mathbf{w}, \mathbf{z}])\\ &+ \mathbb{E}_q \left[\sum^L_{s=0} \sum^{I(s)}_{s=1} \sum^K_{k=1} \ln p\left(w_{k, s, i} \mid \alpha_{s}\right) \right]\\ &- \mathbb{E}_q \left[\sum^L_{s=0} \sum^{I(s)}_{s=1} \sum^K_{k=1} \ln q\left(w_{k, s, i} \mid \mu_{k, s, i}, \sigma_{k, s, i}^{2}\right) \right] \end{align} For the $1$ st expectation, we have: \begin{align} \mathbb{E}_q[\ln(p(\mathbf{v} | \alpha_0, \mathbf{w}, \mathbf{z}])=& \mathbb{E}_q[\ln((2 \pi)^{-\frac{n}{2}} \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})^{-\frac{1}{2}}\\ &\exp(-\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})))]\\ =& \mathbb{E}_q[-\frac{n}{2}\ln(2 \pi)-\frac{1}{2}\ln \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})\\ &-\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})]\\ =& \mathbb{E}_q[-\frac{1}{2}\ln \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})-\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})] \end{align} For the $2$ nd expectation, we have: \begin{align} \mathbb{E}_q \left[\ln p\left(w_{k, s, i} \mid \alpha_{s}\right) \right]=&\mathbb{E}_q \left[\ln \sqrt{\frac{\alpha_s}{ 2 \pi}} \exp(-\frac{1}{2}\alpha_s w^2_{k, s, i}) \right]\\ =&\mathbb{E}_q \left[\frac{1}{2}\ln\alpha_s -\frac{1}{2}\ln 2 \pi -\frac{1}{2}\alpha_s w^2_{k, s, i} \right]\\ =&\mathbb{E}_q \left[\frac{1}{2}\ln\alpha_s -\frac{1}{2}\alpha_s w^2_{k, s, i} \right] + C \end{align} For the $3$ rd expectation, we have: \begin{align} \mathbb{E}_q \left[\ln q\left(w_{k, s, i} \mid \mu_{k, s, i}, \sigma_{k, s, i}^{2}\right) \right]=&\mathbb{E}_q \left[\ln \frac{1}{\sigma_{k, s, i} \sqrt{2 \pi}} \exp(-\frac{1}{2}\left(\frac{w_{k, s, i}-\mu_{k, s, i}}{\sigma_{k, s, i}}\right)^{2}) \right]\\ =&\mathbb{E}_q \left[-\ln\sigma_{k, s, i} -\frac{1}{2}\ln 2\pi -\frac{1}{2}\left(\frac{w_{k, s, i}-\mu_{k, s, i}}{\sigma_{k, s, i}}\right)^{2} \right]\\ =&\mathbb{E}_q \left[-\ln\sigma_{k, s, i} -\frac{1}{2}\left(\frac{w_{k, s, i}-\mu_{k, s, i}}{\sigma_{k, s, i}}\right)^{2} \right] + C \end{align} By summing the expressions we find \begin{align} L_{[\sigma_{k, s, i}]} =& \mathbb{E}_q\left[-\frac{1}{2}\ln \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})\right]-\mathbb{E}_q\left[\frac{1}{2}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})\right]\\ &+ \mathbb{E}_q \left[\frac{1}{2}\ln\alpha_s\right] -\mathbb{E}_q \left[\frac{1}{2}\alpha_s w^2_{k, s, i} \right] + \mathbb{E}_q \left[-\ln\sigma_{k, s, i} \right]\\ &-\mathbb{E}_q \left[\frac{1}{2}\left(\frac{w_{k, s, i}-\mu_{k, s, i}}{\sigma_{k, s, i}}\right)^{2} \right] + C\\ =& -\frac{1}{2}\mathbb{E}_q\left[\ln \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})\right]-\frac{1}{2}\mathbb{E}_q\left[(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})\right]\\ &+ \frac{1}{2} \langle\ln\alpha_s\rangle -\frac{1}{2}\mathbb{E}_{q(\alpha_s)} \left[\alpha_s\right]\cdot \mathbb{E}_{q(w_{k, s, i}|\mu_{k, s, i}, \sigma^2_{k, s, i})} \left[(w_{k, s, i}|\mu_{k, s, i}, \sigma^2_{k, s, i})^2 \right] - \langle\ln\sigma_{k, s, i} \rangle\\ &-\frac{1}{2}\mathbb{E}_q \left[\left(\frac{w_{k, s, i}-\mu_{k, s, i}}{\sigma_{k, s, i}}\right)^{2} \right] + C\\ =& -\frac{1}{2}\mathbb{E}_q\left[\ln \operatorname{det}(\alpha_{0}^{-1} \boldsymbol{I})\right]-\frac{1}{2}\mathbb{E}_q\left[(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})^{\top} (\alpha_{0}^{-1} \boldsymbol{I})^{-1}(\boldsymbol{v}-\mathbf{\Phi} \boldsymbol{\theta})\right]\\ &+ \frac{1}{2} \langle\ln\alpha_s\rangle -\frac{1}{2} \langle\alpha_s\rangle\cdot \langle w_{k, s, i}^2 \rangle - \langle\ln\sigma_{k, s, i} \rangle\\ &-\frac{1}{2}\mathbb{E}_q \left[\frac{w^2_{k, s, i}+\mu^2_{k, s, i}-2 w_{k, s, i}\mu_{k, s, i}}{\sigma^2_{k, s, i}} \right] + C \end{align} We find the partial derivative of the lower bond and solve $\frac{\partial}{\partial \sigma_{k,s,i}}L_{[\sigma_{k,s,i}]}=0$ to find the update relation for $\sigma_{k,s,i}$ .
