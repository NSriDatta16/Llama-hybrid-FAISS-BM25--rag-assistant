[site]: stackoverflow
[post_id]: 4844672
[parent_id]: 4844165
[tags]: 
From http://aggregate.org/MAGIC/#Average%20of%20Integers : (low & high) + ((low ^ high) / 2) is an overflow-proof average of two unsigned integers. Now, this trick only works on unsigned integers. But because ((a+x) + (b+x))/2 = (a+b)/2 + x , you can fudge it as follows, if you have unsigned integers with the same bit size as your signed integers: unsigned int u_low = low + MAX_INT + 1; unsigned int u_high = high + MAX_INT + 1; unsigned int u_avg = (u_low & u_high) + (u_low ^ u_high)/2; int avg = u_avg - MAX_INT - 1; UPDATE : On further thought, this will work even if you don't have signed integers. Signed and unsigned integers are equivalent over addition, subtraction, and bitwise operations. So all we need to worry about is making sure that divide acts like an unsigned divide, which we can do by using a shift and masking out the uppermost bit. low += MAX_INT + 1; high += MAX_INT + 1; avg = (low & high) + (((low ^ high) >> 1) & MAX_INT); avg -= MAX_INT + 1; (Note that if you're using Java, you can use an unsigned shift, ... >>> 1 , instead of (... >> 1) & MAX_INT .) HOWEVER, there's an alternative I stumbled upon that's even simpler, and I haven't yet figured out how it works. There's no need to adjust the numbers by MAX_INT or use unsigned variables or anything. It's simply: avg = (low & high) + ((low ^ high) >> 1); Tested with all combinations of 16-bit signed integers low and high in the range -32768..32767, but not yet proven outright (by me anyway).
