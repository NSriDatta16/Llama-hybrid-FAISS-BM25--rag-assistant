[site]: datascience
[post_id]: 101754
[parent_id]: 
[tags]: 
leave one pair out cross validation

I am trying to train and validate my datasets which contains 17 datasets. I have divided them as 15 for training and 2 for validation. In the process, I train on 15 datasets and use the generated model to predict the results on the remaining 2 datasets. This process is called leave out validation in my understanding. Irrespective of the classifier I use (SVM, optimizable SVM, knn, optimizable KNN), I always get a very high training accuracy closer to 90-100%. The validation accuracy is comparatively poorer closer to 50-60 %. The datasets in the validation set will be a part of training in some runs. In this case, I can not understand if they are doing so good in the training why the validation results are so bad.?
