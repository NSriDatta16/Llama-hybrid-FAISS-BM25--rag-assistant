[site]: crossvalidated
[post_id]: 131007
[parent_id]: 
[tags]: 
Integrating length for input-space feature PC projections in kernel PCA

I read a paper detailing the algebraic process of kernel PCA. I have question though: the paper details the projection of new points onto the new eigenvectors in the feature space, but what if I want eigenvectors themselves? I am performing an analysis that compares eigenvector magnitudes in a Gaussian inner product space. The closest I found in the paper was the definition of the eigenvectors as a hypothetical linear combination of data vectors in the kernel space: $$\mathbf{V} = \sum_{i = 1}^l \alpha_i \Phi\left(\mathbf{x}_i\right).$$ From the paper, it seems that you derive all such $\alpha$ with the relation $$\ell\lambda\mathbf{\alpha}=K\mathbf{\alpha}$$ (where $\mathbf{\alpha}$ is supposed to look bold as a vector). But even after you obtain $\alpha_i$, you can't compute the eigenvectors in the feature space from the previous formula because, in the case of a Gaussian kernel that operates on $\|\mathbf{x} - \mathbf{y}\|$, there are infinitely many dimensions in the the "hidden" range of $\Phi$. How can I compute $\mathbf{V}$, the eigenvectors in the $\mathbf{\Phi}$? Edit 0 I understand that we can't formalize eigenvectors in a infinite-dimensional space implied by a Gaussian-norm inner product kernel. What I actually want is the length of the eigenvector. In the linear case, that would be the semimajor axes of the imaginary ellipse. I want to be able to have, for a "classification" PCA case as this one , a graph like the one on the left showing a curved (for lack of a better word) eigenvector projected down to the input space. I'd prefer a parametric form over which I may compute an arclength integral. Edit 1 Oops. I meant principal component , not eigenvector.
