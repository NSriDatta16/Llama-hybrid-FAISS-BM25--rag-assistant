[site]: datascience
[post_id]: 121220
[parent_id]: 
[tags]: 
SB3-PPO and Keras-PPO on CartPole env: performances can be improved and catastrophic forgetting limited?

I am trying to experiment different hyperparameters and neural network structures in order to stabilize the learning process of the CartPole environment and possibly reduce the occurrence and gravity of catastrophic forgetting events with PPO. I have tested both Stable Baseline3 and Keras PPO the code structure could be found here https://keras.io/examples/rl/ppo_cartpole/ In a previous post ( https://www.reddit.com/r/reinforcementlearning/comments/12x90xy/cartpole_with_stable_baseline3_strange_behaviour/ ), I wrote about playing CartPole and increasing the max number of steps (around 40k). The learning curve I got was ... not a curve, but a flat line around 10 steps and with some sporadic peaks. Being a beginner I tried to ask for a possible explanation but not yet get any answer from the community. Now I'd like to share some other attempts keeping the max number of steps equal to the default number of 500. The first question I would like to ask to all the experts here is what could be the reasons for a so different curves between the two implementations of PPO algorithm; I tried to keep the same hyperparameters in the two runs. The second question could be if these are the learning curves that we could expect using PPO on cartpole. I gave a glance around and found an old discussion on this topic here https://www.reddit.com/r/reinforcementlearning/comments/ogwct6/how_to_deal_with_catastrophic_forgetting/?onetap_auto=true So I tried to implement the two main suggestions: to use Mish activation function instead of ReLu and to use reward normalization. I was able to activate the latest change just on SB3-PPO and not on Keras-PPO. Here are the results. What I can note is a slight improvement in the catastrophic forgetting events in Keras-PPO using Mish activation function, but I think it should be tested several times to have statistical confirmation of this. Instead, I note a global performance worsening for SB3 PPO implementing bot Mish and rewards normalization. So the third question I'd like to ask is if this makes sense and if you note any mistake I could have done. Here is the notebook of the two experiments: Keras PPO: https://www.kaggle.com/code/federicobari/keras-ppo-cart-pole-with-perf-charts SB3 PPO: https://www.kaggle.com/code/federicobari/ppo-stable-baseline3
