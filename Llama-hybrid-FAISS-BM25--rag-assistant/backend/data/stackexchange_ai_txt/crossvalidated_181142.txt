[site]: crossvalidated
[post_id]: 181142
[parent_id]: 181127
[tags]: 
There are some conditions we might need. For example, we need to know Are there any conditions on the activation function $\sigma(x)$? It seems like you have in mind $\sigma(x) = (1 + e^{-x})^{-1}$, but the activation function you are using is not explicitly stated. Are there any conditions on the space $R$ where the $x$'s live? In your question you set $0 If we assume that (1) $R$ is (contained in) a compact set and (2) $\sigma(x)$ is non-constant, continuous, and bounded on $R$ then it follows that there exists a neural network with activation function $\sigma(x)$ which approximates any continuous function $f(x)$ on $R$. So, in particular, we can approximate the function $f(x) = x$. This is the universal approximation theorem . If we assume that $R = (-\infty, \infty)$ then we need $\sigma(x)$ to be unbounded. To see why, note that if $|\sigma(x)| \le K$ and $f(x)$ is a neural network, then $f(x)$ is also bounded. If $f(x)$ is bounded then it cannot approximate the function $x \mapsto x$ for $x > \sup_x |f(x)|$. So, we need $\sigma(x)$ to be unbounded to have any shot. There are a few activation functions which are unbounded; for example $\sigma(x) = \max\{0, x\}$ gives us $x = \sigma(x) - \sigma(-x)$. And $\sigma(x) = x$ also works. Another interesting possibility, which might be used in practice, is $\sigma(x) = \log(1 + e^x)$ for which $\sigma(x) \approx x$ for large $\max\{0, x\}$. So you could get $\sigma(x) \approx x$ for large $x$ by looking at $\log(1 + e^{x}) - \log(1 + e^{-x})$. The problem would be making the approximation work simultaneously for large and small $x$ using only these units, which may not be possible.
