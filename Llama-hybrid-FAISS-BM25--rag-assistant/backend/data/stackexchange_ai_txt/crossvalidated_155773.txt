[site]: crossvalidated
[post_id]: 155773
[parent_id]: 154621
[tags]: 
My suggestion is independent of the software used. We need to clarify whether the outliers are outliers in Y, the dependent variable, or outliers in the predictor, X. Outliers in the predictor, X, are easily handled with the vast numbers of transformations available that would reshape the PDF (probability density function) of X. While I agree with Eric Farng that deleting outliers in Y is not recommended, I disagree with him that they should be deleted only after "careful consideration." In my opinion, one should never delete outliers since they contain important and useful information, that is, unless one can determine that these values are somehow "bad," illegal or fraudulent, etc. The alternatives to outlier deletion in Y are to leverage modeling methods that are robust to outliers. Why am I opposed to a priori outlier deletions in Y? Let me use an example: after "careful consideration" (Eric Farng's wording, however one chooses to define this) a first set of outliers is deleted. Does this mean that you are done with deleting outliers? Probably not, since a second analysis would almost certainly reveal a new set of outliers relative to the new mean and standard deviation. What does one do with this new information? And how many passes of the data are required to completely scrub it of outliers? Clearly, this is a potentially endless process of outlier deletion that makes little or no sense. Most importantly and even before one gets into the almost philosophical question of whether or not to delete outliers, it should be noted that MARS is one of the robust, non-parametric alternatives that exploits nonlinear, quantile functions of the relationship between X and Y. From a purely applied and practical point of view this means that MARS is highly robust, almost immune, to the presence of outliers in Y: by definition, deleting outliers in Y is unnecessary, even moot, when leveraging MARS.
