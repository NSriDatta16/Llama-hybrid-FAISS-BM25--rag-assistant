[site]: datascience
[post_id]: 19112
[parent_id]: 
[tags]: 
Do neural networks have explainability like decision trees do?

In Decision Trees, we can understand the output of the tree structure and we can also visualize how the Decision Tree makes decisions. So decision trees have explainability (their output can be explained easily.) Do we have explainability in Neural Networks like with Decision Trees?
