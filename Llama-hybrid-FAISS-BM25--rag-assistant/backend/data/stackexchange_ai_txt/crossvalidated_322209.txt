[site]: crossvalidated
[post_id]: 322209
[parent_id]: 322188
[tags]: 
Simultaneous modelling of mean and variance using double generalized linear models The emphasis of gamlss is obviously on generalized additive models (GAMs). The general of idea of simultaneously modelling the mean and variance using generalized linear models (GLMs) rather than GAMs has been around much longer, and you might find the dglm package (Double GLMs) a simpler entry point for this sort of analysis. I came up with the idea of double generalized linear models as part of my 1985 PhD thesis. In the normal case, the first publication with the same idea is Aitkin (1987). For your example data we have: > set.seed(1839) > x y library(dglm) > fit summary(fit) Call: dglm(formula = y ~ x, dformula = ~x) Mean Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.3273 1.2319 -0.266 0.791 x -0.0336 0.0467 -0.719 0.472 (Dispersion Parameters for gaussian family estimated as below ) Scaled Null Deviance: 1001 on 999 degrees of freedom Scaled Residual Deviance: 1000 on 998 degrees of freedom Dispersion Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 4.9319 0.08996 54.8 0.00e+00 x 0.0508 0.00157 32.4 7.99e-230 (Dispersion parameter for Gamma family taken to be 2 ) Scaled Null Deviance: 2082 on 999 degrees of freedom Scaled Residual Deviance: 1419 on 998 degrees of freedom Minus Twice the Log-Likelihood: 10296 Number of Alternating Iterations: 6 You can see from the summary results that there is no significant trend for the means but a very highly significant increasing trend for the variances. The dispersion coefficients are exactly twice the sigma coefficients returned by gamlss (in Stefan's answer) because dglm is modeling the variance rather than the standard deviation. We can do even better by using $\log x$ as the predictor for the variance. You simulated the data to have sd($y_i$)$=\sigma_i=x_i$ so, on the log-scale, the true variance model is $\log\sigma^2_i=2\log x_i$. We can fit this by: > fit summary(fit) Call: dglm(formula = y ~ x, dformula = ~log(x)) Mean Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.0189 0.0410 -0.461 0.645 x -0.0346 0.0323 -1.069 0.285 (Dispersion Parameters for gaussian family estimated as below ) Scaled Null Deviance: 1001 on 999 degrees of freedom Scaled Residual Deviance: 1000 on 998 degrees of freedom Dispersion Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.0843 0.1694 0.498 0.619 log(x) 1.9850 0.0453 43.806 0.000 (Dispersion parameter for Gamma family taken to be 2 ) Scaled Null Deviance: 2068 on 999 degrees of freedom Scaled Residual Deviance: 1188 on 998 degrees of freedom Minus Twice the Log-Likelihood: 10079 Number of Alternating Iterations: 4 You can see that the variance trend coefficient is estimated to be 1.985 when the true value is 2. The variance intercept is estimated to be 0.0843 when the true value is 0. How it works The basic idea is quite simple. We first fit a linear regression to the data: > fit.m Then we extract the squared deviance residuals (aka unit deviances ) from the fit and fit a log-linear chisquared glm to them: > d fit.d Then we take the fitted values from the variance (dispersion) model and use them as weights for the mean model: > w fit.m The dglm() function simply iterates this process to convergence. It can be shown that this computes MLEs of all the parameters. To examine the mean-model fit, you should use: > summary(fit.m, dispersion=1) The dispersion should be set to 1 because the variances have already been incorporated into the weights. To examine the variance model, you should use > summary(fit.d, dispersion=2) The dispersion should be set to 2 because the unit deviances follow scaled chisquared distributions on 1 df, and chisquared on 1 df has squared coefficient of variation equal to 2. The above example is for normal data, but the dglm() function works for any GLM family. REML The dglm package has the additional ability to use a REML method to return approximately unbiased variance estimators, unlike ML estimation which will return variance estimators that are systematically too small. For this data it makes little difference, because the number of mean parameters (2) is small compared to the number of observations (1000): > fit summary(fit) Call: dglm(formula = y ~ x, dformula = ~x, method = "reml") Mean Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.3297 1.2353 -0.267 0.790 x -0.0335 0.0467 -0.716 0.474 (Dispersion Parameters for gaussian family estimated as below ) Scaled Null Deviance: 998 on 999 degrees of freedom Scaled Residual Deviance: 997 on 998 degrees of freedom Dispersion Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 4.9428 0.09029 54.7 0.00e+00 x 0.0507 0.00157 32.2 3.96e-227 (Dispersion parameter for Gamma family taken to be 2 ) Scaled Null Deviance: 2082 on 999 degrees of freedom Scaled Residual Deviance: 1419 on 998 degrees of freedom Minus Twice the Log-Likelihood: 10296 Number of Alternating Iterations: 7 References Aitkin, M. (1987). Modelling variance heterogeneity in normal regression using GLIM. Applied Statistics 36(3), 332-339. Smyth, G. K. (1989). Generalized linear models with varying dispersion. J. R. Statist. Soc. B , 51, 47â€“60. Smyth, G. K., and Verbyla, A. P. (1999). Adjusted likelihood methods for modelling dispersion in generalized linear models. Environmetrics , 10, 696-709. http://www.statsci.org/smyth/pubs/Ties98-Preprint.pdf Smyth, G. K., and Verbyla, A. P. (1999). Double generalized linear models: approximate REML and diagnostics. In Statistical Modelling: Proceedings of the 14th International Workshop on Statistical Modelling , Graz, Austria, July 19-23, 1999, H. Friedl, A. Berghold, G. Kauermann (eds.), Technical University, Graz, Austria, pages 66-80. http://www.statsci.org/smyth/pubs/iwsm99-Preprint.pdf Smyth, GK, and Verbyla, AP (2009). Leverage adjustments for dispersion modelling in generalized nonlinear models. Australian and New Zealand Journal of Statistics 51, 433-448.
