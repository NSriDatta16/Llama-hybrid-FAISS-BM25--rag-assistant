[site]: datascience
[post_id]: 16910
[parent_id]: 
[tags]: 
Skip gram Word2Vec model, neural network implementation

I have referred and used materials from these - cs224n stanford . Following is the algorithm provided in the stanford course notes for training a skip gram model. Here |V| is the vocab size, matrix V is hidden layer matrix that would select a particular input word embedding for center word. I am assuming that hidden layer matrix V is of dimensions - |V| x d. (d - feature lengths), and input one hot vector of length |V| x 1. The result of hidden layer then would be of (1 x d) dimensions. Problem - In step 4, it is said to generate 2m score vectors. To me it seems the dimension of the score vectors would be 1 x 1 each, as they would be a product of result from the hidden layer of dimension (1 x d), and output word vector matrix U, of dimension |V| x d. The overall result should be of |V| x 1, And each word score will be of 1 x 1 dimension (column vector). Then how can we compare each of them to one hot vectors as asked in step 6 (in doing so, won't it make probability of a particular word ideally 1, and hence all other word probabilities should be zero)? Also, what are the actual output one hot vectors? And is the probability of each word in the 2m window equal to all other words there?
