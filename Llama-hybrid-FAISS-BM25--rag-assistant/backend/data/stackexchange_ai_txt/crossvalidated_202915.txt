[site]: crossvalidated
[post_id]: 202915
[parent_id]: 202879
[tags]: 
Some of the things I encounter: Treating significance level and CI coverage probabilities as interchangeable, so that people end up doing things like speaking of "95% significance". [What's worse is when people who make such errors point to their lecture notes -- or even textbook -- as support for this; in other words the mistake is not theirs, but is being compounded a hundredfold or many-thousands-fold, and worse, even if they understand it correctly, they may actually have to repeat the error anyway, to pass the subject.] There's also a common tendency to think that "significance" somehow exists outside a specific hypothesis/question (leading to questions like "are my data significant" without any clear notion of what question is to be addressed). [A related issue is the "what test should I use for these data?" as if it were the data - rather than the question to be answered - that's the driver of choice of analysis. (While the "design" of the study can impact the specific tests used, the question of interest is more important -- for example, if you have three groups available but your question of interest only relates to a comparison of two of them, the fact that you have three doesn't force you to do a one-way type analysis rather than a straight comparison of the two groups of interest ... as long as your choice of analysis doesn't derive from what the data show. Ideally you plan your questions and analyses before you have data, rather than throwing analysis at data and see what sticks, which it seems post-hoc analysis questions - including "what test should I use for these data?" - tend to lead to.) An occasional tendency to refer to the complement of p-value as some sort of "confidence in", or "probability of" the alternative. "nonparametric data"; another one unfortunately found in a couple of books (and, sadly, in an article that purports to correct a common error) this one comes up so often that it's in my short list of automatically generated comments (which begins "Data are neither parametric nor nonparametric; those are adjectives that apply to models or techniques...") (thanks Nick Cox for reminding me of this particular bugbear) Usually what is intended is "non-normal data" but parametric doesn't imply normal, and having approximate normality doesn't imply we need parametric procedures. Similarly, non-normality doesn't imply we need non-parametric procedures. Occasionally, what is intended is "ordinal data" or "nominal data" but in neither case does that imply that finite-parametric models are inappropriate. A common tendency to misunderstand the meaning of "linear" in "linear model" in a way that would be inconsistent with the use of the term "linear" in "generalized linear model". This is partly the fault of the way we use terminology. conflating the mean-minus-median kind of skewness with third moment skewness, and conflating a zero in either (or even both) with symmetry. Both errors are frequently found in basic texts widely used in some particular application areas. [There's a related error of treating zero skewness and zero excess kurtosis as implying normality] this one is so common it's becoming hard to call it an error any more (due in part to the efforts of a particular program) -- calling excess kurtosis simply "kurtosis"; a mistake pretty much guaranteed to lead to communication problems.
