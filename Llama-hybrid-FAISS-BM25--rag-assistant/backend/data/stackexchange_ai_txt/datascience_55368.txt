[site]: datascience
[post_id]: 55368
[parent_id]: 
[tags]: 
Measuring the similarity between a numeric data matrix and one or more categorical variables?

Given a numeric data matrix $A$ of size $n \times p$ , which each row represents an observation along $p$ variables, and a second categorical data matrix $M$ of size $n \times z$ , where each row represents some categorical description of the data in matrix $A$ (i.e. metadata), how can we measure the correspondence between the variables in $M$ and the data in $A$ ? The goal is to determine which of the variables in $M$ best reflects the relationship of the data points in matrix $A$ , where each variable in $M$ is some imperfect / estimated feature of the data matrix matrix observations. So if one were to imagine clustering the data in matrix $A$ , the goal is to find which of the variables in $M$ best "fits" the clusters such that the levels of the variable separate well across the clusters in $A$ . One more consideration: the variables in $M$ may contain different numbers of levels, e.g. $M_1$ may take on one of three values while $M_2$ may take on one of ten values, and so on. A couple approaches I've considered are: Multinomial logistic regression Clustering the data in $A$ and measuring the similarity between the data partitioning and each variable in $M$ . For example, if we pretend that the first four columns in the iris dataset (Sepal Length, Petal Width, etc.) are our numeric data matrix, and the last column (Species) is one of the categorical variables we are interested in comparing the data to, we could do something like: Multinomial Logistic regression If I train a logistic regression model using only a single variable from $X$ (or, something like PC1 from a PCA-projected version of the data), then I can compute a pseudo- $R^2$ and get something reasonable like ( R example): null (In the actual analysis, this process would be repeated for each of the categorical variables in $M$ using a for-loop..) However, when I attempt to train a model using the full data (or a large enough number of principle components from the PCA-projected version of the data), then the model seems to overfit the data, resulting in a pseudo- $R^2$ value of "1": mod On the other hand, by first clustering the numeric data, the problem then becomes one of measuring similarity between two categorical variables with differing levels, although this is also not ideal since we are losing a lot of information in the process of going from a numeric data matrix with $p$ columns, to vector of cluster assignments. Finally, I could perhaps try and "brute force" the problem by clustering the data into some number of clusters and then computing ratio of variable levels within each cluster, compared with the expected ratio of levels if they were randomly distributed across clusters, however, this seems like overkill. Does anyone know if there is a better way to approach this problem?
