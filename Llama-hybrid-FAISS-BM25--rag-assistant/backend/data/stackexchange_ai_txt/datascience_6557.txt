[site]: datascience
[post_id]: 6557
[parent_id]: 6510
[tags]: 
By default random forest picks up 2/3rd data for training and rest for testing for regression and almost 70% data for training and rest for testing during classification.By principle since it randomizes the variable selection during each tree split it's not prone to overfit unlike other models.However if you want to use CV using nfolds in sklearn you can still use the concept of hold out set such as oob_score(out of bag)=True which shows model performance with or without using CV. So in a nutshell using oob_score=True with or without nfolds can itself tell whether using CV is good for your data.Generally if your target is following a certain distribution and you don't have much observation data with you then using CV will not give much improvement.
