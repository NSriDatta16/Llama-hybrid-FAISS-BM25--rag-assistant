[site]: datascience
[post_id]: 121865
[parent_id]: 
[tags]: 
Dueling DQN with varying number of actions

I have an RL problem, where the number of actions depends on the state. Furthermore, each action-value computation requires action information in the form of a high-dimensional, continuous vector in addition to the state. It is not feasible to input all of these contextual vectors into the Q-network at once (i.e. embed them as part of the state), and emit q-values for the maximally possible number of actions, mainly due to the strongly fluctuating amounts of available actions per state and the dimensionality of the contextual vectors. For regular DQN, I have solved this by inputing each contextual vector along with the state into the Q-network one-by-one. The Q-network emits just a single value, the q-value. This works fine and performs well. However, I am stuck on using the same approach for Dueling DQN. I have managed to implement a working solution, but it performs much worse than DQN. My dueling architecture emits the state value $v$ and the advantage $a$ , given the state and contextual vector as input. I then use the target network (without gradient calculation) to do the same for all other actions/contextual vectors. Using the obtained state values and advantage values, I compute the average of both, and subtract both from the sum $v + a$ . The final q-value is thus $q = v + a - a_{mean} - v_{mean}$ . Clearly, there is a difference to the vanilla dueling architecture, because I have no way of computing a pure state value, since I must input the contextual vector as well. Does anyone have experience with such a scenario? I have yet to find any literature or information on this topic.
