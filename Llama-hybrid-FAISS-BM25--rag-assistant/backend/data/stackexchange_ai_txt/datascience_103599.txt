[site]: datascience
[post_id]: 103599
[parent_id]: 94923
[tags]: 
If you do not want to use filter or wrapper feature selection methods, then you can use tree based algorithms to find the feature importance of all the features. You can use Random Forest , LighGBM , XGBoost , CatBoost for this purpose. CatBoost is an interesting one as it can work with categorical features. Also you can use L1 or L2 regularization to select the best features. Read up on Ridge and Lasso algorithms. A word of caution though. Feature selection methods are to be taken with a pinch of sale. Never solely rely on feature selection methods. The best feature selection method IMO is filtering out features based on domain knowledge.
