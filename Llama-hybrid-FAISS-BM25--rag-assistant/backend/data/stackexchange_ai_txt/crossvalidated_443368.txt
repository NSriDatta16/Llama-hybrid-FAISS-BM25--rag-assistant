[site]: crossvalidated
[post_id]: 443368
[parent_id]: 443351
[tags]: 
Logistic Regression as its name suggests is a regression technique: it estimates class membership probability whereas SVM on its own is only a classifier. Such a probability estimate is more informativte than the SVM's distance to the class boundary. libsvm can calculate such probabilities by actually fitting a logistic regression to the distance from class boundary. They use different loss functions: binomial loss for logistic regression vs. hinge loss for SVM. In consequence, SVM puts even more emphasis on cases at the class boundaries than logistic regression (which in turn puts more emphasis on cases close to the class boundary than LDA). In fact, it ignores all cases that are not directly adjacent to the class boundaries. Cases somewhat further from the class boundary have more influence on logistic regression (but diminishing with distance from boundary). This makes SVM (but not LR) a sparse model. In high dimensional spaces (lots of input features), points tend to be "at the outside", i.e. many data points will be adjacent to some class boundary in some direction. In consequence, many (possibly all) points may become support vectors, and the SVM isn't sparse any more. This is typically a cumbersome situation from a computational point of view (and is often a bad sign terms of overfitting). SVM maximize an existing (clear) margin between the classes, dealing with not perfectly separable classes is a "standard add-on". For logistic regression it's the other way round: while it naturally deals with not perfectly separable situations, perfect separation needs some "add-on" (regularization) Kernels are not really a difference, since they can be used not only with SVM but also with logistic regression (and many other models)
