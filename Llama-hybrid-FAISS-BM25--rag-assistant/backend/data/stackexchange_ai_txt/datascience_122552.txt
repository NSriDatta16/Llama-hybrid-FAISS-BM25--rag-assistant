[site]: datascience
[post_id]: 122552
[parent_id]: 
[tags]: 
Which language model to use for this use case? [Finetuning on custom dataset]

An example from my train data is as follows: "Input": " User: Why is my peach tree not producing fruit? AI: Lack of fruit can be due to poor pollination, insufficient chill hours, or nutrient imbalances. User: How to ensure enough chill hours? ", "Output": "User: How can I ensure my peach tree gets enough chill hours to produce fruit?" Basically, the Output is the last question in the Input, modified to be more clear, adds the contextual information and resolves coreferences so when it stands alone it has all the information that the question in the Input meant. I need to train a model so that given the above Input a similar Output as shown above is generated. The model should be lightweight ( I currently don't have enough training examples, have 300 or so, using GPT to generate more such examples. I have tried finetuning the current data on seq2seq models like "t5-small" and "facebook\bart" from hugging face but they give incorrect output. Nowhere close to how I want the output to be. I know that with more training data the model would perform better but I just wanted to clarify my approach before proceeding. Am I going on about it the right way, i.e., making the model understand it needs to make the last question of the input more clear. Which specific model to choose for this task, how to know what would perform better than the other beforehand for my task? How much training data should I have to generate (an estimate would be helpful). Any suggestions?
