[site]: crossvalidated
[post_id]: 638421
[parent_id]: 
[tags]: 
Explaining the approximation to the variational free energy in Bayes by Backprop

In the paper Weight Uncertainty in Neural Networks, Blundell et al., 2015 , the authors approximate the exact cost (variational free energy) $$ \mathcal F(\mathcal D, \theta) = \mathrm{KL}[q(\mathbf w \vert \theta) \mathrel{\|} P(\mathbf w)] - \mathbb E_{q(\mathbf w \vert \theta)}[\log P(\mathcal D \vert \mathbf w)] \tag{1} $$ with the approximation $$ \mathcal F(\mathcal D, \theta) \approx \sum_{i=1}^n \log q(\mathbf w^{(i)} \vert \theta) - P(\mathbf w^{(i)}) - \log P(\mathcal D \vert \mathbf w^{(i)}) \tag{2}, $$ "where $\mathbf w^{(i)}$ denotes the $i$ th Monte Carlo sample drawn from the variational posterior $q(\mathbf w^{(i)} \vert \theta)$ ." This appears to be a (simple) Monte Carlo estimate but why isn't there included a $\frac 1n$ normalizing term for the expectation estimate, i.e. why isn't the correct approximation: $$ \mathcal F(\mathcal D, \theta) \approx \frac 1n \sum_{i=1}^n \log q(\mathbf w^{(i)} \vert \theta) - P(\mathbf w^{(i)}) - \log P(\mathcal D \vert \mathbf w^{(i)}) \tag{2}\;\;? $$
