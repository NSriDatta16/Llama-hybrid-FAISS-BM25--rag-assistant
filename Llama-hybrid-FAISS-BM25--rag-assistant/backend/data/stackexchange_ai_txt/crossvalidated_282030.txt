[site]: crossvalidated
[post_id]: 282030
[parent_id]: 
[tags]: 
What can possibly go wrong in a Generative Adversarial Network?

Lately, after reading about GANs , I started experimenting with the MNIST dataset, and the result we acceptable. Here are some details about the networks I used: Discriminator: 784 inputs $\rightarrow$ 256 ReLUs $\rightarrow$ 1 Sigmoids Generator: 100 $\sim \mathcal{U}(-1,1)$ $\rightarrow$ 256 ReLUs $\rightarrow$ 784 Sigmoids Using 20k examples of the MNIST dataset. Doing mini-batch (size = 100) gradient descent (using Tensorflow's AdamOptimizer ). I trained the discriminator equally as the generator. So I wanted to experiment more. I download a tiny dataset of 600 butterflies pictures, downscaled them to 64x64 and put them into grayscale, they look like this: Given such tiny dataset, I expected the GAN to overfit the data and to generate images that are excessively close to what's in the dataset. I went for a topology similar to the one I used with MNIST: Discriminator: 4096 inputs $\rightarrow$ 1024 ReLUs $\rightarrow$ 1 Sigmoids Generator: 100 $\sim \mathcal{U}(-1,1)$ $\rightarrow$ 1024 ReLUs $\rightarrow$ 4096 Sigmoids I also tried with hidden layers of size 512 . With minib-batches of 10 example, I tried training the discriminator: equally, twice as much, and three times as much as the generator. Even after hours of training, the network still fails at generating butterfly images. Here are the usual results: What is possibly going wrong: Is it the topology? Is it the dataset size? Why didn't it overfit to the dataset? Tried different training synchronization ratios, are there any other things to consider about training synchronization? Activation functions?
