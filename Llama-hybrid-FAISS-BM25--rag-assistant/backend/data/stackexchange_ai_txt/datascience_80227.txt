[site]: datascience
[post_id]: 80227
[parent_id]: 
[tags]: 
Should I keep common stop-words when preprocessing for word embedding?

If I want to construct a word embedding by predicting a target word given context words, is it better to remove stop words or keep them? the quick brown fox jumped over the lazy dog or quick brown fox jumped lazy dog As a human, I feel like keeping the stop words makes it easier to understand even though they are superfluous. So what about for a Neural Network?
