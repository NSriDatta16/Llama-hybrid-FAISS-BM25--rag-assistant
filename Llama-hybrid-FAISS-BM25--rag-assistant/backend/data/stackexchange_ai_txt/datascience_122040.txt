[site]: datascience
[post_id]: 122040
[parent_id]: 
[tags]: 
Why are normal distributions so important in deep learning?

I am currently reading on normalization/standardization techniques as well as batch normalization in deep learning and I don't really understand why normal distributions are so important inside deep learning models. If it's true that neural networks can approximate any function, then if we have two input features of the same scale but different distributions, why would this hinder the learning process. My current belief is that it isn't so much that normal distributions are important per se, but the individual features that are input into a neural network should have the same distributions for the fastest convergence and standard normal distributions are mathematically the easiest to work with. I also don't understand what the impact of putting a batch normalization layer before and activation layer is. Wouldn't the BN layer normalize everything and then the activation layer denormalize every by putting all the values into its own distribution. My questions are: -Why are normal distributions so important for neural networks(I understand that same scale input features is important for better convergence but why distribution) -Why do we place BN before activations, wouldn't the activations just change the distributions anyways?
