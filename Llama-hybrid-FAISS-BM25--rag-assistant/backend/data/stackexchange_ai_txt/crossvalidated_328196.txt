[site]: crossvalidated
[post_id]: 328196
[parent_id]: 327965
[tags]: 
The following framework can be used to address the question. There are four possible combinations of $r_i$ and $s_i$: \begin{equation} (r_i,s_i) \in \Omega = \{(0,0),\,(0,1),\,(1,0),\,(1,1)\} \end{equation} We can express the joint distribution as \begin{equation} p(r_i = j, s_i = k|\theta) = \theta_{jk} \qquad \text{for $(j,k) \in \Omega$} . \end{equation} where $\theta = (\theta_{00},\theta_{01},\theta_{10},\theta_{11})$ and \begin{equation} \theta_{00} + \theta_{01} + \theta_{10} + \theta_{11} = 1 . \end{equation} With this setup, the conditional distribution is \begin{equation} p(r_i=j|s_i=k) = \frac{p(r_i=j,s_i=k)}{p(s_i=k)} = \frac{\theta_{jk}}{\theta_{0k} + \theta_{1k}} . \end{equation} However, we do not know the value of $\theta$. Now suppose we have the following observations: $y_{1:n} = (y_1, \ldots, y_n)$ where $y_i = (r_i, s_i) \in \Omega$ and $p(y_i=(j,k)|\theta) = \theta_{jk}$. Using the observations, the likelihood is given by \begin{equation} p(y_{1:n}|\theta) = \prod_{i=1}^n p(y_i|\theta) = \theta_{00}^{c_{00}}\, \theta_{01}^{c_{01}}\, \theta_{10}^{c_{10}}\, \theta_{11}^{c_{11}} \end{equation} where $c_{jk}$ is the number of times $y_i = (j,k)$ occurs in $y_{1:n}$. Note $c_{00} + c_{01} + c_{10} + c_{11} = n$. Let the prior for $\theta$ be Dirichlet: \begin{equation} p(\theta) = \textsf{Dirichlet}(\theta|\alpha) \propto \theta_{00}^{\alpha_{00}-1}\, \theta_{01}^{\alpha_{01}-1}\, \theta_{10}^{\alpha_{10}-1}\, \theta_{11}^{\alpha_{11}-1}, \end{equation} where $\alpha = (\alpha_{00},\alpha_{01},\alpha_{10},\alpha_{11})$. Then the posterior distribution is \begin{equation} p(\theta|y_{1:n}) = \textsf{Dirichlet}(\theta|\alpha+c) \propto \theta_{00}^{\alpha_{00}+c_{00}-1}\, \theta_{01}^{\alpha_{01}+c_{01}-1}\, \theta_{10}^{\alpha_{10}+c_{10}-1}\, \theta_{11}^{\alpha_{11}+c_{11}-1} . \end{equation} The mean of this distribution is characterized by \begin{equation} E[\theta_{jk}|y_{1:n}] = \frac{\alpha_{jk}+c_{jk}}{A+ n} , \end{equation} where $A= \alpha_{00}+\alpha_{01}+\alpha_{10}+\alpha_{11}$. We can approximate the conditional distribution using posterior expectations: \begin{equation} \theta_{j|k} \approx \frac{E[\theta_{jk}|y_{1:n}]}{E[\theta_{0k}|y_{1:n}]+E[\theta_{1k}|y_{1:n}]} = \frac{\alpha_{jk} + c_{jk}}{\alpha_{0k}+c_{1k} + \alpha_{0k} + c_{1k}} . \end{equation} The fully Bayesian approach would involve integrating out the posterior uncertainty in $\theta_{j|k}$ directly, obtaining a $\textsf{Multinomial-Dirichlet}$ distribution.
