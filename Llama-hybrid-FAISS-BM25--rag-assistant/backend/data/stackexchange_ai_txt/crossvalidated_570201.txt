[site]: crossvalidated
[post_id]: 570201
[parent_id]: 517155
[tags]: 
I may be able to offer some reasoning why the LM is calculated as $nR^2$ and has a $\chi^2$ distribution, although it is inherently somewhat technical. The easiest setting to get the general idea of what is happening is using an LM test for variable relevancy. The main theorem used in the derivation is the Frisch-Waugh theorem for partitioned regression and the rest is relatively intuitive given some foundational knowledge of multivariate calculus and statistics. Precise test derivations will be similar, mutatis mutandis. Suppose we have a model $$y=X_1\beta_1+X_2\beta_2 +\epsilon$$ where $y$ and $\epsilon$ are $n$ x $1$ , $X_1$ is $n$ x $k_1$ , $X_2$ is $n$ x $k_2$ , $\beta_1$ is $k_1$ x $1$ , and $\beta_2$ is $k_2$ x $1$ , and we wish to test the hypotheses $$H_0:\beta_2 =0$$ $$H_1:\beta_2 \neq 0$$ Then we can estimate the model under the null, i.e. $y=X_1 \beta_R +\epsilon_R$ , by minimising the sum of squares of the full model whilst imposing the condition that $\beta_2=0$ i.e. we solve $$\min_{b}\{e'e\} \quad \text{subject to} \quad b_2=0$$ To do this we can utilise Lagrange multipliers. We set up the Lagrangian as $$\mathscr{L}(b_1,b_2,\lambda)=e'e +\lambda'b_2$$ This can be expanded as $$\mathscr{L}(b_1,b_2,\lambda)=(y-X_1 b_1-X_2 b_2)'(y-X_1 b_1-X_2 b_2) +\lambda'b_2$$ We then obtain the FOCs: $$\begin{array} ((1) & 0=-2X_1'(y-X_1 b_1-X_2 b_2) \\ (2) & 0=-2X_2'(y-X_1 b_1-X_2 b_2)+\lambda \\ (3) & 0=2b_2 \end{array}$$ We may then obtain from this system the two equations $$X_1'(y-X_1 b_1)=X_1' e_R=0$$ and $$\lambda = X_2'(y-X_1 b_1)=X_2' e_R$$ Now, if we define $M_1=I_n = X_1(X_1'X_1)^{-1}X_1'$ as the annihilator matrix for $X_1$ then by the Frisch-Waugh-Theorem $e_R=M_1 \epsilon$ , where $\epsilon$ is from the original model, i.e. from $y=X_1\beta_1+X_2\beta_2 +\epsilon$ . Thus, $$\lambda = X_2'e_R=X_2'M_1\epsilon \sim N(0,\sigma^2 X_2'M_1X_2)$$ since we assume $\epsilon \sim N(0,\sigma^2 I_n)$ or we may appeal to the Mann-Wald central limit theorem in large samples to obtain a similar asymptotic distribution which will also be normal, albeit with slightly different variance. Since $rank(X)=k$ , $X_2$ has full column rank, as does $M_1$ . Thus, $\sigma^2 X_2'M_1X_2 = (\sigma M_1 X_2)'(\sigma M_1 X_2)$ is positive definite, and therefore has an inverse root matrix. So, $$(\sigma^2 X_2'M_1X_2)^{-\frac{1}{2}}\lambda \sim N(0,I_{k_2})$$ . Turning this into a quadratic form we obtain a $\chi^2$ distribution with $k_2$ degrees of freedom: $$\lambda'(\sigma^2 X_2'M_1X_2)^{-1}\lambda \sim \chi^2 (k_2)$$ We still have the issue that there is a $\sigma^2$ here so we may use a consistent estimator of $\sigma^2$ in its stead to get a usable test statistic. For the precise form of the LM of $nR^2$ we use $\hat{\sigma}^2=\frac{e_R'e_R}{n}$ which is biased but consistent, other estimators for $\sigma^2$ are also usable so long as they are consistent. So our LM statistic is $$\text{LM}=\frac{\lambda'(X_2'M_1X_2)^{-1}\lambda}{e_R'e_R/n}$$ . Now if we run the model $$y=X_1 \beta_R + \epsilon_R$$ and save the residuals $e_R$ we can run the auxiliary regression $$e_R = X\gamma + \eta$$ where $X=[X_1 \ X_2 ]$ . The TSS of the auxiliary regression are $$\text{TSS}=\sum(e_{R,i}-\bar{e}_R)^2=\sum e_{R,i}^2=e_R'e_R$$ where $\bar{e}_R$ is the mean of the restricted residuals and since a constant is assumed to be included in the model, this is zero. Further, if we consider the predictions of $e_R$ in the auxiliary model then the average of these will also be zero by the same reasoning. So $$\text{ESS} = \hat{e}_R'\hat{e}_R=(X\hat{\gamma})'(X\hat{\gamma})$$ the usual OLS estimator for $\gamma$ is $\hat{\gamma}=(X'X)^{-1}X'e_R$ and so $$\text{ESS} = (X(X'X)^{-1}X'e_R)'(X(X'X)^{-1}X'e_R) = e_R'X(X'X)^{-1}X'e_R$$ . Now, by the orthogonality condition between $X_1$ and $e_R$ , $X_1'e_R=0$ so $$X'e_R=\begin{bmatrix} X_1' \\ X_2' \end{bmatrix} e_R =\begin{bmatrix} 0 \\ X_2' e_R \end{bmatrix}$$ further, by Frisch-Waugh $$(X'X)^{-1} = \begin{bmatrix} \dots & \dots \\ \dots & (X_2'M_1X_2)^{-1} \end{bmatrix}$$ so $$\text{ESS} = \begin{bmatrix} 0 & e_R'X_2 \end{bmatrix} \begin{bmatrix} \dots & \dots \\ \dots & (X_2'M_1X_2)^{-1} \end{bmatrix} \begin{bmatrix} 0 \\ X_2' e_R \end{bmatrix}$$ $$\text{ESS} = e_R'X_2(X_2'M_1X_2)^{-1}X_2'e_R$$ thus for the auxiliary regression $$R^2 = \frac{\text{ESS}}{\text{TSS}}=\frac{e_R'X_2(X_2'M_1X_2)^{-1}X_2'e_R}{e_R'e_R}$$ and so $$\text{LM}=n\left[\frac{e_R'X_2(X_2'M_1X_2)^{-1}X_2'e_R}{e_R'e_R}\right]=n R^2\sim \chi^2 (k_2)$$
