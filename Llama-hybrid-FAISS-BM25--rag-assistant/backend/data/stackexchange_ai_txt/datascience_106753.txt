[site]: datascience
[post_id]: 106753
[parent_id]: 106713
[tags]: 
+1 to Craig for the answer to the actual question. But I want to address two other remarks from your post. ...the class probability of sklearn random forest does not seem correct. Because for sklearn random forest, the sizes of classes of the training set determines the class probabilities of a single tree and the class probabilities of the random forest. So, removing instances from or adding instances to the training set would change the class probabilities of the random forest which does not seem correct. The majority vote is not affected by the sizes of the classes of the training set. Removing/adding enough instances to the training set will tip the balance in some leaves, resulting in a change in the class probabilities of the random forest as well. So the sklearn method is just "more continuous" than the weka method. These two methods are sometimes referred to as "soft" and "hard" voting, e.g. both are available in the VotingClassifier class of sklearn. I suspect most of the time the final results are similar, although do note that in a highly imbalanced setting, the hard votes of each tree may never come up with the minority class, so soft voting may be preferable as a way to come up with unbiased probabilities. Another main difference is that sklearn random forest cannot be applied to discrete features. The discrete features must be transformed to numeric features by one-hot encoding or ordinal encoding before applying sklearn random forest. Weka random forest can be applied to both numeric features and discrete features directly. There are actually two layers to this difference. The first is the historical development of two main tree algorithms, CART and the Quinlan family (ID3 then C4.5 and C5.0). Quinlan family trees split categorical features by creating one child per category, whereas CART always produces binary trees. But, there are implementations of CART that produce binary splits of categorical data without encoding; the second layer of the difference is that sklearn does not implement that in random forests. They do have an implementation for that categorical splitting in their HistGradientBoosting classes, which presumably at some point will get ported over to a random forest setting as well.
