[site]: crossvalidated
[post_id]: 612256
[parent_id]: 
[tags]: 
Bound for $L_2$-error of random forest estimate

I am struggling to understand a certain inequality based on the regression $L_2$ -error of a regression function estimate. The setting is that of random forests for regression. Let $\Theta = \{ \Theta_{1}, \dots, \Theta_{M} \}$ be the (iid) random variables that capture the randomness that goes into contructing the individual trees. Let $m(x) = \mathbb{E}\left[ Y ~|~ X=x \right]$ be the (true, unknown) regression function that we want to estimate with the random forst. Assume that trees in the forest are fully grown, i.e. each cell in a tree contains exactly one of the points subsampled/bootstrapped for construction of the tree. Consequently, we can write the regression function estimate of the forest as $m_{n}(X) = \sum_{i=1}^n W_{ni}(X)Y_{i}$ where $W_{ni}(x) = \mathbb{E}_{\Theta}\left[\mathbb{1}_{x_{i}\in A_{n}(x, \Theta_{j})}\right]$ and $A_n(x, \Theta)$ is the cell of $x$ in a tree generated via $\Theta$ . Now, the inequality in question is the following. It's from the proof of Theorem 2 in Scornet2015 . $$ \mathbb{E}\left[m_{n}(X) - m(X) \right]^2 \leq 2 \mathbb{E}\left[ \sum_{i=1}^n W_{ni}(X)(Y_{i}-m(X_{i})) \right]^2 + 2 \mathbb{E}\left[ \sum_{i=1}^n W_{ni}(X)(m(X_{i})-m(X)) \right]^2 $$ My first question is: Why is that? I have tried applying the basic textbook error decompositions but am not getting anywhere. My second question is: In the publication, the authors refer to the first term as the "estimation error" and the second as the "approximation error". This does not quite fit with my current understanding of these terms: Estimation error: Error of selected function as compared to best possible choice from hypothesis class Approximation error: Error of best possible from hypothesis class as compared to true regression function Getting an intuition on the second question is probably more important to me.
