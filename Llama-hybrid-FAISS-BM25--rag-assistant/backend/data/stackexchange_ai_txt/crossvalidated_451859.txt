[site]: crossvalidated
[post_id]: 451859
[parent_id]: 
[tags]: 
Improve F1-score for multiclass text classification with highly imbalanced dataset

I am trying to classify clients' complaints with a dataset of 180k complaints. I have 132 classes like this: Counter({'DIAG_000_NODIAG': 66291, 'FORWARD': 29126, 'DIAG_087': 22843, 'DIAG_049': 17668, 'DIAG_148': 10508, 'DIAG_011': 6739, 'DIAG_069': 3529, 'DIAG_209': 3263, 'DIAG_204': 2915, 'DIAG_206': 2613, 'DIAG_207': 1573, ... }) So, highly imbalanced. When I apply Multinomial Naive Bayes classifier with Bag of Words which usually performs well for text classification I get an average F1-score of ~66% (if I try TFIDF I get the same). To try to solve this problem I calculated the cumulative frequency of classes and put all the classes above 85% (this was where the elbow was in the bar plot) in another class called 'other'. After this I was left with 7 classes: Besides that I applied SMOTE to get a balanced dataset. After this I applied Multinomial NB once again and the average F1-score was pretty much the same. I am using a train-test split of 70-30 What can I do to improve this classification measure?
