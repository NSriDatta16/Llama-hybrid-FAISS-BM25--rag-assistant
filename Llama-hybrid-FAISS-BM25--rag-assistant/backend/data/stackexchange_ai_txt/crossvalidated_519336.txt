[site]: crossvalidated
[post_id]: 519336
[parent_id]: 519219
[tags]: 
Let's define "stats perspective" as running a model in R and adding or deleting new variables based on their statistical significance or degree to which they change AIC. First of all, "adding or removing variables based on their significance" is not a good practice . As the name suggests, significance testing is about testing a hypothesis, it is not a tool for optimizing anything, and by using it for the variable selection you assume some kind of optimization problem. Second, ask yourself what exactly does a "statistically significant parameter" means. We can do many different statistical tests, but the most common ones, the out-of-the-box statical tests for regression models test the hypothesis that the parameter differs from zero. If a parameter is equal to zero in the regression model, it has no effect on the results. Let's start with the fact that not every machine learning model has parameters (e.g. $k$ -NN, or decision tree don't). But even if it has parameters, why would you care if they are zero or not? If the model makes correct predictions, it can have as many zero parameters as you want, worst case they would make it computationally slower than in the case of a lighter model. More than this, sometimes we intentionally drag the parameters towards zero, like with $L_1$ regularization, as means of feature selection. In such a case we want as many of the parameters to be zeros, as long as they do not worsen the results.
