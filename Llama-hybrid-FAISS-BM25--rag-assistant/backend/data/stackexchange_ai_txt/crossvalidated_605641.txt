[site]: crossvalidated
[post_id]: 605641
[parent_id]: 
[tags]: 
Why isn't (symmetric) log(1+x) used as neural network activation function?

Specifically, I mean $$ f(x)= \begin{cases} -\log(1-x) & x \le 0 \\ \space \space \space \log(1+x) & x \gt 0 \\ \end{cases} $$ Which is red in the plot: It behaves similarly to widely used $\tanh(x)$ (blue) except it avoids saturation/vanishing gradients since it has no horizontal asymptotes. It's also less computationally expensive. Is there some issue with it I'm missing?
