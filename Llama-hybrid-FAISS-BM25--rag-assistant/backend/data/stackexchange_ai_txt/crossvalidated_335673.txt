[site]: crossvalidated
[post_id]: 335673
[parent_id]: 335648
[tags]: 
Yes. You can use the following code to convince yourself. N 0)+1. # Make sure that the data can be separated plot(data[,1],data[,2], col = labels, xlab = 'x1', ylab = 'x2') require(Rtsne) model This is what you would observe when P is 3 (only one irrelevant attribute with respect to the linear separation, we are close to reproducing the linear separation). And for P is 15, the tSNE cannot reproduce the linear separation. What happened ? This is simple. The tSNE method relies on pairwise distances between points to produce clusters and is therefore totally unaware of any possible linear separability of your data. If your points are "close" to each other, on different sides of a "border", a tSNE will consider that they belong to a same cluster. This was exactly the point of the simulations above. When the number of dimensions is large, points look close to each other, no matter the side of the border they belong to. This is what tSNE fails to capture here. On the other hand, when the number of irrelevant dimension is low, close points had "no other choice" but to be on the same side of the border. Side note. Even though you may have a nice performance with a neural network, it may not mean that your data is linearly separable (unless there is only one unit in your neural, hum, network). Indeed neural networks can recognize non linear boundaries. If you want to test how "linearly separable" a data set is, you should use linear Support Vector Machines or regressions.
