[site]: crossvalidated
[post_id]: 614102
[parent_id]: 
[tags]: 
Proper conclusions from learning curves

I have a machine learning problem that I solve via nonlinear regression. I have 80 samples totally and try to understand is it useful to gather more data. For this, I plot learning curves in the following way: I randomly split the data into 60/20 train/validation samples I implement the training on different number of training samples: 20,23,26...,60. For all iterations, I test on the same validation dataset (20 samples). So in the end, I get Validation Error vs. Number of training samples Repeat steps 1-3 with different random seeds for the train/validation split 5 times Finally, for each number of training samples, the error is averaged over the 5 splits Now, I need to understand two things: Standard deviation over different splits for the validation is much higher then for the training. Does it mean, that the number of validation samples is not enough? Or maybe from split to split the data varies too much? Do I still have the potential to decrease the validation error and improve the model performance, if I gather more data? The plots don't look saturated to me.
