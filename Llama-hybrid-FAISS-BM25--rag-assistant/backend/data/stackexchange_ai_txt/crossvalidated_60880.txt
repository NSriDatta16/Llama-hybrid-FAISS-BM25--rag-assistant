[site]: crossvalidated
[post_id]: 60880
[parent_id]: 
[tags]: 
Does the normal approximation get better as a density becomes more peaked?

I have a sequence of densities $f_n(x_n)$, of random variables $X_n$, with means $\mu = 0$ and variances decreasing with $n$: $$ \sigma^{2}(X_n) = \frac{\sigma^2}{n}. $$ I am approximating $f_n(X)$ using a normal approximation $N(0, \frac{\sigma^2}{n})$. I would like to check if the normal approximation is getting better as $n$ increases. My intuition was that, given that $f_n(x_n)$ gets more and more peaked as the variance decreases, it becomes more "quadratic-like" so that the normal approximation should become more accurate (is some sense). I tried to verify this in several ways, by using several criterions (relative error, KL distance etc..) to check if the approximation gets better, but I have failed so far. My suspicion is that this is probably wrong, so maybe somebody can point out to me why the approximation doesn't improve. Thanks Note: $X_n$ are not sample averages, so the CLT doesn't apply.
