[site]: crossvalidated
[post_id]: 283909
[parent_id]: 
[tags]: 
(Deep learning) classification confidence

I have a model that is trained on n classes and has more than 95% accuracy on the test set. The model is going to receive a mixture of images that are either from one of the n classes or from unknown classes. My guess / expectation was that in the latter case, the distribution of values on the last layer (softmax) would be more or less uniform. Then to spot an alien image, it would be enough to threshold the prediction layer. Unfortunately that is not the case, and the model seem to express a very high level of confidence every time. Even worse (using keras): >>> max(model.predict(np.random.rand(1, 299, 299, 3))[0]) 1.0 So basically the model is 100% sure that random noise is one of the class. How to detect the images that do not belong to one of the classes that the model was trained on? Note: everything has been normalized and in Keras predict returns probability, not class numbers.
