[site]: crossvalidated
[post_id]: 131289
[parent_id]: 106075
[tags]: 
This is usually referred to as class imbalance or skewed data not bias. For a random forest you can use roughly balanced bagging to resample the data used to grow each tree during the bagging process. You can also look into using a weighted or cost sensitive criteria for tree growth like weighted gini or entropy. Note that weights should be tuned using a grid search or hyperparameter optimization as it is difficult to guess good ones. IE weighting the majority class more then the minority class may produced the best balanced error somewhat counterintuitively. Finally heilinger distance decision trees have recently been proposed as less sensitive to this sort of things. I wrote a random forest implementation that includes a bunch of different methods for imbalanced data .
