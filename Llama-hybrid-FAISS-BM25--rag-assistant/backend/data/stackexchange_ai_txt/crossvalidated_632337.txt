[site]: crossvalidated
[post_id]: 632337
[parent_id]: 353819
[tags]: 
The study mentioned by PinkBanter has too many flaws in my opinion. Comparing Apples to Oranges: GloVe Wikipedia 100 dim vector, 400K words, GloVe is a completely different method than Word2Vec Word2Vec Google News 300 dim vector, 3 million words Domain-Specific Word2Vec 60 dim vector, 2 million words Domain-Specific Word2Vec 100 dim vector, 100K words Questionable tests: One of the 3 downstream applications used no machine learning, just some sort of simple SQL-like query None of the machine learning applications used deep learning. Who does significant high-quality NLP anymore without deep learning? Two of the ML-based downstream applications focused on general terms so domain-specific terms had little value Finally, the paper's conclusion has two seemingly contradictory statements First, the word embeddings trained on EHR and MedLit can capture the semantics of medical terms better and find semantically relevant medical terms closer to human expertsâ€™ judgments than those trained on GloVe and Google News. Finally, the word embeddings trained on biomedical domain corpora do not necessarily have better performance than those trained on general domain corpora for any downstream biomedical NLP task. And that last statement extrapolates that the article's simple (and in my opinion greatly flawed tasks) is a valid proxy for ANY biomedical task. ANY ??? Because so many people just read the short summary, they will probably get the wrong conclusion than if the authors were more restrained and had written "any downstream biomedical NLP task in this paper ", which also would not have been true but at least far less hyperbolic.
