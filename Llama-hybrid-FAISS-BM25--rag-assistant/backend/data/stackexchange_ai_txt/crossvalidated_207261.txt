[site]: crossvalidated
[post_id]: 207261
[parent_id]: 207171
[tags]: 
This problem sounds like a great candidate for the encoder/decoder type of models. You can encode the image with a CNN and have the decoder output the card names via a RNN (LSTM/GRU). You will stop when the decoder outputs a 'stop' symbol. The RNN will end up producing a list of card names. This is similar to the automatic image captioning which combines a CNN encoder with an LSTM language decoder. You can also add an attention mechanism on your decoder to select part of the CNN output to decode. This should give better results than a straight forward encode/decode. Finally, you can take the CTC approach used in speech transcription where spectrograms are lined up with letters. Your spectrograms will be slices of the final layer in your CNN and your letter would be the card names. If you have boundary data of the cards the problem becomes easier as you don't have to learn a reader mechanism. Adding this data is expensive though and in my opinion not in the spirit of end-to-end deep-learning.
