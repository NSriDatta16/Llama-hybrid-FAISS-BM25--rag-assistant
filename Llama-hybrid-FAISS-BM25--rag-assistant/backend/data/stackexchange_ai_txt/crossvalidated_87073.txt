[site]: crossvalidated
[post_id]: 87073
[parent_id]: 86917
[tags]: 
UPDATE There is probably an numerical error with one class nu-svm in LibSVM. At optimum, some training instances should satisfy w'*x - rho = 0 . However, numerically they may be slightly smaller than zero Then they are wrongly counted as training errors. Since nu is an upper bound on the ratio of training points on the wrong side of the hyperplane, numerical issues occur in calculating the first case because some training points satisfying y*(w'*x + b) - rho = 0 become negative. This issue does not occur for nu-SVC for two-class classification. The authors added this issue to their FAQ . -----------OLD ANSWER BELOW------------------------------------------------------------------------- Thanks for @cbeleites's note. I investigated the influence of both $\gamma$ and $\nu$ in one class SVM. I used 5-fold cross validation (but not the '-v 5' option in libsvm) bu shuffling the data 100 times and then average the accuracy (still use the proportion classified correctly). The result images show the training accuracy, testing accuracy, and generalization error (the difference between the former two) with different combinations of $\gamma$ and $\nu$. Cbeleites is correct that $\gamma$ itself is not sufficient to determine the variance of the model. The underfitting is very clearly shown in subfigure(3), but it seems like there is only a slightly overfitting around the middle part ($\nu \approx 0.1$, $\gamma \approx 5$, I didn't locate in the exact coordinate). And there is the "longish optimum" as Cbeleites mentioned in the comment. Basically large $\gamma$ and $\nu$ might cause the underfitting but the overfitting dependence on the coefficient is not that evident. I used the logarithm of $\gamma$ and $\nu$ to show the smaller value region more clearly below.
