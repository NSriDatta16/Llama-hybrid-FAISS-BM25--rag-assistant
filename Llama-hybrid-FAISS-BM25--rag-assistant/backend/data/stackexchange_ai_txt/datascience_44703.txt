[site]: datascience
[post_id]: 44703
[parent_id]: 
[tags]: 
How does Gradient Descent and Backpropagation work together?

Please forgive me as I am new to this. I have attached a diagram trying to model my understanding of neural network and Back-propagation? From videos on Coursera and resources online I formed the following understanding of how neural network works: Input is given, which gets weight assigned to it using a probability distribution. The activation functions use the weights to provide the predicted value. The cost or loss functions calculate the error of the prediction between the actual class and the predicted value. The optimization functions such as gradient descent use the results of the cost function to minimize the error. If the above is correct then I am struggling to understand the connection between Gradient Descent and Backpropagation? Here is an image of my understanding so far:
