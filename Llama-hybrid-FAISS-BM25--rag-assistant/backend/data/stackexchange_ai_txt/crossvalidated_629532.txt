[site]: crossvalidated
[post_id]: 629532
[parent_id]: 389762
[tags]: 
Yes, the m-out-of-n bootstrap will most likely (or: almost surely ;-) work in your case because it is known to be more robust than the usual bootstrap and thus works under much weaker conditions: Bickel, Götze, van Zwet: "Resampling fewer than n observations: gains, losses, and remedies for losses." Statistica Sinica 7 (1997), 1-31 Actually, your particular example is given as a case when the traditional (n-out-of-n) bootstrap fails in the above paper. Another advantage of the m-out-of-n bootstrap is that it can be used without replacement and thus does not introduce a considerable amount of ties into continuous distributions for which ties are impossible. A problem with the m-out-of-n bootstrap, however, is that it requires knowledge of a rescaling factor $\tau_n$ which is necessary for constructing the confidence intervals. According to P. Bertail, D. N. Politis, J. P. Romano: "On subsampling estimators with unknown rate of convergence,” Journal of the American Statistical Association, vol. 94, no. 446, pp. 569–579, 1999, $\tau_n$ must be chosen such that such that $\tau_n^2 \operatorname{Var}(T)$ converges to some constant V, so you can simulate some data for your statistic with varying sample size $n$ and have a look at the convergence rate of the variance. (Remark: Bertail et al. also suggested a method to estimate $\tau_n$ by another bootstrap, but when I tried it, I found this method to be grossly inaccurate, so it should be taken with a grain of salt). The destillery package mentioned by @laiguanasupersonica supports $m in the function booter() , but, alas, it has a bug which prevents the choice replace=FALSE . Moreover, the function destillery::ci.booted() reports the confidence intervals as character strings , which makes it awkward to use, and it does not provide support for introducing a scaling factor $\tau_n$ . It is thus easiest to implement it yourself as described in section 5 of the following article (for ready-to-run R code, see listing 1): Dalitz, Arning, Goebbels: "A Simple Bias Reduction for Chatterjee's Correlation." https://arxiv.org/abs/2312.15496 (2023) Another inconvenience of the m-out-of-n bootstrap is that it has a parameter $m$ that must be chosen, which will depend on $n$ . For asymptotic convergence of the (rescaled!) bootstrap distribution to the true distribution, it must be of the form $m(n)\to\infty$ and $m(n)/n\to 0$ as $n\to\infty$ (proven by Politis & Romano, 1994), but this still leaves plenty of of options. The sample code from listing 1 in the above paper uses $m=2\sqrt{n}$ , because it worked best for the particular statistic of interest. Swanepool derived from a Monte Carlo study for a different estimator the rule of thumb $m=2n/3$ , but this was based on very weak grounds: Only two simulations with $n=15$ and $n=30$ , and these simulations already showed a tendency that the coverage probability for the choice $m=2n/3$ falls below the nominal level as $n$ increases. This should be expected, because the choice $m=2n/3$ violates the condition $m/n\to 0$ . Swanepoel: "A note on proving that the (modified) bootstrap works." Communications in Statistics-Theory and Methods 15.11 (1986): 3193-3203. The simplest method to estimate $m$ from the data seems to be the method by Götze & Račkauskas, but this requires an additional bootstrap for minimizing the distance between two empirical cumulative distribution functions for the choices $m$ and $m/2$ : Götze, Račkauskas: "Adaptive choice of bootstrap sample sizes." Lecture Notes-Monograph Series (2001): 286-309.
