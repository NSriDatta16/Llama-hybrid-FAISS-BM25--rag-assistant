[site]: datascience
[post_id]: 107712
[parent_id]: 107708
[tags]: 
The technique you are talking about is known as Stacking. Stacking falls under an umbrella known as ensemble methods which also includes Bagging and Boosting. First stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners. Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms. the idea of stacking is to learn several different weak learners and combine them by training a meta-model to output predictions based on the multiple predictions returned by these weak models. So, we need to define two things in order to build our stacking model: the L learners we want to fit and the meta-model that combines them. For example, for a classification problem, we can choose as weak learners a KNN classifier, a logistic regression and a SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it. So, assume that we want to fit a stacking ensemble composed of L weak learners. Then we have to follow the steps thereafter: split the training data in two folds choose L weak learners and fit them to data of the first fold for each of the L weak learners, make predictions for observations in the second fold fit the meta-model on the second fold, using predictions made by the weak learners as inputs Flow diagram for stacking model : For more details on ensemble methods and stacking please refer to this link
