[site]: crossvalidated
[post_id]: 372349
[parent_id]: 
[tags]: 
Conditional posteriors - Is this the correct way to obtain them?

Forgive the novice question but I'm new to Bayesian inference... Suppose $y \sim p(y|\mu, \sigma, \theta)$ and we want to sample from the posterior using Gibbs sampling. That is, we need the conditional posteriors of the form $p(\mu | y, \sigma, \theta)$ etc (for simplicity I will denote all distributions by $p$ though I know this is incorrect/poor notation) We have the likelihood which is $$L(y| \mu, \sigma, \theta) = p(y | \mu, \sigma, \theta)$$ and suppose we also have the priors such as $p (\mu)$ . Am I correct to say that the conditional posterior of, say, $\mu$ is obtained by (assuming independent priors): $p(\mu | y,\sigma,\theta) = \frac{p(\mu ,y,\sigma,\theta)}{p(y,\sigma,\theta)}= \frac{p(y | \mu,\sigma,\theta)p(\mu,\sigma,\theta)}{p(y,\sigma,\theta)} =\frac{p(y | \mu,\sigma,\theta)p(\mu)p(\sigma,\theta)}{p(y,\sigma,\theta)} \propto p(y | \mu,\sigma,\theta)p(\mu)$ Hence if $\mu$ has conjugate prior, we need only consider the likelihood and prior of $\mu$ ?
