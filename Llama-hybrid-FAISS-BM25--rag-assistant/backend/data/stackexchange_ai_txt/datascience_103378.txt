[site]: datascience
[post_id]: 103378
[parent_id]: 103339
[tags]: 
I've attempted to create a procedure for this which splits the data into two partitions, but I would appreciate feedback as to whether my implementation is correct from numpy.core.defchararray import count import pandas as pd import numpy as np import numpy as np from math import floor, log2 from sklearn.decomposition import PCA import matplotlib.pyplot as plot def print_full(x): pd.set_option('display.max_rows', len(x)) print(x) pd.reset_option('display.max_rows') def main(): s = pd.read_csv('A1-dm.csv') print("******************************************************") print("Entropy Discretization STARTED") s = entropy_discretization(s) print("Entropy Discretization COMPLETED") # This method discretizes attribute A1 # If the information gain is 0, i.e the number of # distinct class is 1 or # If min f/ max f = threshold] print("s2 after spitting") print(s2) print("******************") print("******************") print("calculating maxf") print(f" maxf {maxf(s['Class'])}") print("******************") print("******************") print("calculating minf") print(f" maxf {minf(s['Class'])}") print("******************") print(f"Checking condition a if {s1.nunique()['Class']} == {1}") if (s1.nunique()['Class'] == 1): break print(f"Checking condition b {maxf(s1['Class'])}/{minf(s1['Class'])} maxInformationGain): maxInformationGain = I[f'informationGain_{i}'] maxThreshold = I[f'threshold_{i}'] print(f'maxThreshold: {maxThreshold}, maxInformationGain: {maxInformationGain}') partitions = [s1,s2] s = pd.concat(partitions) # Step 6: keep the partitions of S based on the value of threshold_i return s #maxPartition(maxInformationGain,maxThreshold,s,s1,s2) def maxf(s): return s.max() def minf(s): return s.min() def uniqueValue(s): # are records in s the same? return true if s.nunique()['Class'] == 1: return False # otherwise false else: return True def maxPartition(maxInformationGain,maxThreshold,s,s1,s2): print(f'informationGain: {maxInformationGain}, threshold: {maxThreshold}') merged_partitions = pd.merge(s1,s2) merged_partitions = pd.merge(merged_partitions,s) print("Best Partition") print("***************") print(merged_partitions) print("***************") return merged_partitions def information_gain(s1, s2, s): # calculate cardinality for s1 cardinalityS1 = len(pd.Index(s1['Class']).value_counts()) print(f'The Cardinality of s1 is: {cardinalityS1}') # calculate cardinality for s2 cardinalityS2 = len(pd.Index(s2['Class']).value_counts()) print(f'The Cardinality of s2 is: {cardinalityS2}') # calculate cardinality of s cardinalityS = len(pd.Index(s['Class']).value_counts()) print(f'The Cardinality of s is: {cardinalityS}') # calculate informationGain informationGain = (cardinalityS1/cardinalityS) * entropy(s1) + (cardinalityS2/cardinalityS) * entropy(s2) print(f'The total informationGain is: {informationGain}') return informationGain def entropy(s): print("calculating the entropy for s") print("*****************************") print(s) print("*****************************") # initialize ent ent = 0 # calculate the number of classes in s numberOfClasses = s['Class'].nunique() print(f'Number of classes for dataset: {numberOfClasses}') value_counts = s['Class'].value_counts() p = [] for i in range(0,numberOfClasses): n = s['Class'].count() # calculate the frequency of class_i in S1 print(f'p{i} {value_counts.iloc[i]}/{n}') f = value_counts.iloc[i] pi = f/n p.append(pi) print(p) for pi in p: ent += -pi*log2(pi) return ent
