[site]: datascience
[post_id]: 110498
[parent_id]: 98002
[tags]: 
Here is a paragraph from Speech and Language Processing Ch 9 . to make effective use of these scores, weâ€™ll normalize them with a softmax to create a vector of weights, $\alpha_{ij}$ , that indicates the proportional relevance of each input to the input element $i$ that is the current focus of attention. My intuition; the same reason for normalizing between layers. Speeds up and stabilizes the learning process by keeping the output (input for next layer) consistent with in a range.
