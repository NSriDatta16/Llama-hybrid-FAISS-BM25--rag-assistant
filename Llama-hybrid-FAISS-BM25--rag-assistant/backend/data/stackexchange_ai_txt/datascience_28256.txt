[site]: datascience
[post_id]: 28256
[parent_id]: 24739
[tags]: 
Honestly, this does not sound like a machine learning problem. I can think of two approaches: If you have an analytic model for $dX/dt$, you can integrate it over $t$ to obtain a model for $X$. Then find the parameters that minimize squared distance to the data (plus some regularization perhaps). A grid search might be sufficient. Use standard methods of interpolating a function, such as spline interpolation . The advantage of this is that you get a smooth closed-form solution. Please also keep in mind that the derivative for a finite set of points $X(t_1), X(t_2), ..., X(t_n)$ does not exist. There are infinitely many functions that pass through all points (and almost all of them are not differentiable). In other words: You have to make additional assumptions on the function that you are looking for. With the methods above, you make these assumptions explicitly, which I think is better than burying them deep inside a machine learning algorithm.
