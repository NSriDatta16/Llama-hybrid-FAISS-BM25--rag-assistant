[site]: stackoverflow
[post_id]: 1884377
[parent_id]: 1884323
[tags]: 
tuple(x for sublist in base_lists for x in sublist) Edit : note that, with base_lists so short, the genexp (with unlimited memory available) is slow. Consider the following file tu.py : base_lists = [[a, b] for a in range(1, 3) for b in range(1, 6)] def genexp(): return tuple(x for sublist in base_lists for x in sublist) def listcomp(): return tuple([x for sublist in base_lists for x in sublist]) def withsum(): return tuple(sum(base_lists,[])) import itertools as it def withit(): return tuple(it.chain(*base_lists)) Now: $ python -mtimeit -s'import tu' 'tu.genexp()' 100000 loops, best of 3: 7.86 usec per loop $ python -mtimeit -s'import tu' 'tu.withsum()' 100000 loops, best of 3: 5.79 usec per loop $ python -mtimeit -s'import tu' 'tu.withit()' 100000 loops, best of 3: 5.17 usec per loop $ python -mtimeit -s'import tu' 'tu.listcomp()' 100000 loops, best of 3: 5.33 usec per loop When lists are longer (i.e., when performance really matters) things are a bit different. E.g., putting a 100 * on the RHS defining base_lists : $ python -mtimeit -s'import tu' 'tu.genexp()' 1000 loops, best of 3: 408 usec per loop $ python -mtimeit -s'import tu' 'tu.withsum()' 100 loops, best of 3: 5.07 msec per loop $ python -mtimeit -s'import tu' 'tu.withit()' 10000 loops, best of 3: 148 usec per loop $ python -mtimeit -s'import tu' 'tu.listcomp()' 1000 loops, best of 3: 278 usec per loop so for long lists only withsum is a performance disaster -- the others are in the same ballpark, although clearly itertools has the edge, and list comprehensions (when abundant memory is available, as it always will be in microbenchmarks;-) are faster than genexps. Using 1000 * , genexp slows down by about 10 times (wrt the 100 * ), withit and listcomp by about 12 times, and withsum by about 180 times (withsum is O(N squared) , plus it's starting to suffer from serious heap fragmentation at that size).
