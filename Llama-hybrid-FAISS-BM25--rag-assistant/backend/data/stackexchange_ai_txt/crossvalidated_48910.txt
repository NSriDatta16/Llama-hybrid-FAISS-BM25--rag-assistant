[site]: crossvalidated
[post_id]: 48910
[parent_id]: 
[tags]: 
What is the name of (and alternatives to) this Bayesian point-estimate?

Assume that we have a Markovian environment that generates at every time step an event $A$ with probability $p^*$ and an event $B$ otherwise. Now suppose you are a Bayesian agent that wants to learn $p^*$ from the observations you've made so far. If we let our hypothesis class be $\{H_x\}_{0 \leq x \leq 1}$ and our initial prior be the uniform distribution ($f_0(x) = 1$), then according to Bayes rule we will update at each time step as: $$ f_{t+1} = \begin{cases} \frac{x f_t(x)}{\int_0^1 x f_t(x) dx} & \text{if we saw A} \\ \frac{(1 - x) f_t(x)}{\int_0^1 (1 - x) f_t(x) dx} & \text{if we saw B} \end{cases} $$ The best estimate (at time $t$) $p_t$ for $p^*$ will then be the mean of $f_t$. I can show that tracking $p_t$ reduces to counting, in the sense that if by time $t$ you saw $a$ many $A$ events and $b = t - a$ many $B$ events then $$p_t = \frac{a + 1}{(a + 1) + (b + 1)}$$ This seems like a basic result I would have seen in an introductory statistics course (which I unfortunately do not have). What is this called? If an agent was using the above procedure to estimate $p^*$ what kind of inference, regression, or estimation would they be doing? Further, to make the answer not just a one-liner. What are some alternatives? In particular, what kind of assumptions would I need to make to have $p_t = \frac{a}{a + b}$? I.e. how can I eliminate the uniformity bias caused by my choice of prior?
