[site]: stackoverflow
[post_id]: 4467832
[parent_id]: 
[tags]: 
How does variable bit rate video compression average out per frame?

Over what time frame does variable bit rate use for averaging? For example, say I want to encode 60 seconds of 640 x 280 25 fps video at 2000 kilobits per second. Does the codec look at the first second (25 frames) of video, determine how to compress those 25 frames into 2000 kilobits, then move onto the next second of video (25 frames)? Or does it analyse the whole video (maybe the first 10 seconds are pure black) and calculate that it can use more than 2000 kilobits for the last 50 seconds, but still maintain a 2000 kilobit average over the entire video? Or is it based on the key frame interval of the particular codec. If I had the keyframe interval set to 250 (10 seconds of video), would the codec assign 20,000 kilobits over that 10 second period? I'm sure it's actually different for all different codecs, but I figure there must be a best practice (or at least a term I can Google).
