[site]: crossvalidated
[post_id]: 373055
[parent_id]: 
[tags]: 
Gradient descent optimization

I am trying to understand gradient descent optimization in ML(machine learning) algorithms. I understand that there's a cost functionâ€”where the aim is to minimize the error $\hat y-y$ . In a scenario where weights $w_1, w_2$ are being optimized to give the minimum error, and partial derivatives are being used, does it change both $w_1$ and $w_2$ in each step or is it a combination (e.g., in few iterations only $w_1$ is changed and when $w_1$ isn't reducing the error any more, the derivative starts with $w_2$ )? The application could be a linear regression model, a logistic regression model, or boosting algorithms.
