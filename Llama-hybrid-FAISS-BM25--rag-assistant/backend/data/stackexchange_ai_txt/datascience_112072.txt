[site]: datascience
[post_id]: 112072
[parent_id]: 
[tags]: 
What are the inputs of encoder and decoder layers of transformer architecture?

In the paper (attention is all you need), it says "embeddings" are the input of the encoding layer. As I know embeddings are the numerical representation of words which is (for example) the output of bert model. In the other hand, In BERT paper says the input of BERT is tokenized sentence . Since encoder part of the transformer is BERT, In transformer architecture( this ) is the input of the encoder "tokenized sentence" or "embedded sentence"? In short: what is the input of the encoder and decoder layer in transformers? Please provide an example. Thanks
