[site]: crossvalidated
[post_id]: 171460
[parent_id]: 
[tags]: 
Why fitting does not find the true best point

I fitting an expression of the form: $s(t)=\frac{1}{1+\exp[a+\sum_k b_k x_k(t)]}$ Where $a$ and $b_k$ are fitting parameters and $x_k(t)$ an input time series. I have $k=1,...,N$ different types of input variables and I'm trying to find what is the combination among the $N$ time series that best fit the observed data. If I only use the first $k=1$ time series I have a better fitting result (better RMS), that if I use $k=1$ and $k=2$ together. So, my question is: why does the fitting Method (NMinimize of NonlinearModelFit function from Mathematica 10), when $x_1$ and $x_2$ are included at the same time, does not find the $b_1$ solution equal to the fitting process where only $x_1$ is present, together with $b_2=0.0$ result?
