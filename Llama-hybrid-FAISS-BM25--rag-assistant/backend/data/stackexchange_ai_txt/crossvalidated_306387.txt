[site]: crossvalidated
[post_id]: 306387
[parent_id]: 306267
[tags]: 
It is unfortunate that Empirical Risk Minimization is a topic where the Internet is full of incorrect information including https://en.wikipedia.org/wiki/Mean_squared_error , where Wikipedia changes its mind whether it is an estimate or a population parameter within the same article. First of all MSE is a population metric, therefore it is incorrect to talk about in-sample or out-of-sample MSE. Let's assume we have $N$ $(y,X)$ pairs for training where $y$ is a sacalar and $X$ is a vector containing predictors. Let's also assume that $\hat{y}_N(X)$ is a predictor found using a fitting procedure using those $N$ samples, then $MSE$ of prediction of fitted model is: $MSE(\hat{Y}) = E[(\hat{y}(X)-y)^2] = \int (\hat{y}(X)-y)^2 p(y,X) dydX$ On the other hand we can have an estimate of MSE. There are two flavors of this: the in sample and out-of-sample estimate. Please take note of the hat I am putting over $MSE$ below. $\hat{MSE_{in}} = \frac{1}{N}\sum_i^N ((\hat{y}(X_i)-y_i)^2$ Let's assume we have another $M$ test samples we haven't used during the fitting procedure: $\hat{MSE_{out}} = \frac{1}{M}\sum_i^M ((\hat{y}(X_i)-y_i)^2$ Now what can we say about the expected values of these estimators? It is easy to see that $E[\hat{MSE_{out}}] = MSE$ simply push the expectation inside the summation and use that fact that $\hat{y}$ is independent of the test samples. Also we use the $IID$ assumption over the samples. Same cannot be concluded for in sample estimate because $\hat{y}$ is not independent of the training samples (obviously as we used the training samples to find $\hat{y}$), hence: $E[\hat{MSE_{in}}] \ne MSE$ In fact it is downward biased ( https://web.stanford.edu/~hastie/Papers/ESLII.pdf , section 7.4): $E[\hat{MSE_{out}}] = E[\hat{MSE_{in}}] + \frac{2}{N}\sum_1^N Cov(y_i,\hat{y_i})$ This is the source of the nasty overfitting problem. Having all that cleared now let's look at how these estimates change according to number of predictors and number of samples used in the fitting process The difference between the red and blue lines illustrates the bias between the in-sample and out-of-sample estimates of MSE. We also added a new concept: "MSE of proposed model". This is not the MSE of the fit but the asymptotic MSE as if we had infinite training samples. As the number of training samples increase both in-sample estimate and out-of-sample estimate converge to the flat line given that the training procedure is consistent!!! ( http://bengio.abracadoudou.com/lectures/theory.pdf , slide 10) Now let's examine the effect of adding predictors. For nested models adding predictors always decreases (at least doesn't increase) $E[\hat{MSE_{in}}]$ given that the fitting procedure reached the global minimum. For nonlinear models optimization usually gets stuck in local minimum, and this may appear to have increased the $\hat{MSE_{in}}$, but this is not an effect of the added predictor, but a side-effect of the fitting procedure. For non-nested cases it can go either way. I believe the original question was towards a nested linear model. What happens to $\hat{MSE_{out}}$ is more interesting and deserves a demonstration. It is also more important as it provides an unbiased estimate of $MSE$ and is also a key objective in Cross Validation procedures. Suppose that we have data generated by a polynomial of degree 4 with additive Gaussian noise. Below is a bunch of samples to illustrate how it looks like. Now I will plot $\hat{MSE_{out}}$ for fitted polynomials from degree 2 to degree 6. vertical axis is logarithm of error and horizontal axis is again number of samples used in fitting procedure. I always used 1000 test samples to estimate $\hat{MSE_{out}}$. The correct model is degree 4 and plotted in red color. magenta is degree 2, green degree 3, yellow degree 5, and blue degree 6. As you can see when the number of training samples is less than 10, the best $MSE$ is achived with a polynomial degree 2, after that polynomial degree 3 takes over, and when we reach 15, degree 4 dominates and from that point on it becomes unbeatable. This is a known phenomenon where true model degree may not give the best prediction ability on small samples. When will a less true model predict better than a truer model? Another interesting observation is that even though degree 2 and degree 3 do a better job at small samples, as number of training samples increase not only they are dominated by degree 4 (the true model), but also degree 5 and 6. This is natural as degree 5 and degree 6 subsume degree 4.
