[site]: crossvalidated
[post_id]: 507796
[parent_id]: 
[tags]: 
Why the decision boundaries are linear in an input space?

In the section 4.2.1 in "Pattern Recognition and Machine Learning, Bishop", the author considered a 2-class problem and assumed class-conditional densities $p(x | C_k)$ are Gaussian, and all have the same covariance matrix. The posterior pdf of class $C_1$ is expressed below, $$ p(C_1 | x) = \sigma(w^T x + w_0) \\ \sigma(a) = \frac{1}{1 + \exp(a)} \\ w = \Sigma^{-1} (\mu_1 - \mu_2) \\ w_0 = -0.5 \mu_1^T \Sigma^{-1} \mu_1 + 0.5 \mu_1^T \Sigma^{-1} \mu_2 + \ln \frac{p(C_1)}{p(C_2)} $$ $\Sigma$ and $\mu_i$ are the covariance matrix and the mean corresponding to class $C_i$ This is the remark I am confused, The resulting decision boundaries correspond to surfaces along which the posterior probabilities $p(C_k|x)$ are constant and so will be given by linear functions of x, and therefore the decision boundaries are linear in input space. Because there are only two classes, I interpret the remark as, the resulting decision boundaries correspond to surfaces along which the posterior probabilities $p(C_1|x) = p(C_2|x) = 0.5$ . How can I verify the decision boundaries are linear? and what are the expressions of the decision boundaries?
