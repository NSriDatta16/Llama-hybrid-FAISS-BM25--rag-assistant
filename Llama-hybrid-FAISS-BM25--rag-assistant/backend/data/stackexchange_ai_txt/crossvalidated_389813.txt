[site]: crossvalidated
[post_id]: 389813
[parent_id]: 
[tags]: 
Hypothesis testing in non-parametric regression

Say I have two processes/time series, $X = (X_{t_{1}},X_{t_{2}},\dots , X_{t_{n}})$ and $Y = (Y_{t_{1}},Y_{t_{2}},\dots , Y_{t_{n}})$ observed at times $t_i$ for $i=1,2,\dots, n$ where $0 . I'm interested in the regression model $$ Y_{t_{i}} = \alpha(t_i) + \beta(t_i)X_{t_i} + \epsilon_{t_i}. $$ Specifically I am interested in the sample paths of $\alpha(t)$ and $\beta(t)$ . I don't really want to assume any structure for the errors so I decided to fit a non-parametric local linear regression model to give the sample paths as the arg min of the following equation $$ (\hat{\alpha}(t) , \hat{\beta}(t)) = \arg \min_{\alpha,\beta} \sum_{i=1}^{n} K \left( \frac{t-t_i}{h}\right) \left( Y_{t_i} - (\alpha + \beta X_{t_{i}}) \right)^2. $$ Where $K(\cdot)$ is some kernel function and $h$ is the bandwidth. My Question: Is there any principled way to test the hypothesis that $\alpha(t) = 0$ for some intervals $I \subset [0,T]$ ? Any advice most appreciated!
