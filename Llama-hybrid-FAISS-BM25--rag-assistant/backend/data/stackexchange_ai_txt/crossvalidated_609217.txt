[site]: crossvalidated
[post_id]: 609217
[parent_id]: 
[tags]: 
Some doubts about the logic of PAC learning

I am having some doubts related to the PAC learning. I understood the main idea, but with the explanation found in the book "Understanding Machine Learning" I couldn't understand some ideas. The author starts by defining some sets, being H the possible models, h the selected model, D the true distribution of the data in the removed environment, Ldf being the loss in the test, Ls loss in training, hs being the best model according to the result of Ls. I understand that the set Dm will show the distribution of the collected data, but how would this inequality work? (I think Dm(M) will contain the distribution of several data-sets that do not match the environment (since the model overfit these data), that is, how does it compare the distribution between the two sets? And it keeps using D, which is my main difficulty in understanding. So if someone could explain the questions about D made above, giving a brief example of its form and how it arrived at the final result I would appreciate it.
