[site]: crossvalidated
[post_id]: 341469
[parent_id]: 121886
[tags]: 
This issue seems actually overlooked in many machine learning courses / resources. I ended up writing an article about scaling on my blog. In short, there are "monotonic transformation" invariant learning methods (decision trees and everything that derives from them), translation invariant learning methods (kNN, SVM with RBF kernel), and the others. Obviously, the monotonic transformation invariant learning methods are translation invariant. With the first class, you do not need to do any centering / scaling. With the translation invariant algorithms, centering is useless. Now, for the other methods, it really depends on the data. Usually, it may be worth trying with scaling (especially if variables have different orders of magnitude). In a general case, I would recommend trying various preprocessings of the data : without scaling, scaling dividing by the standard deviation, scaling dividing by the sum of absolute values of your data (which would make it lie on a simplex). One of them will perform better than the others, but I cannot say which one until I have tried.
