[site]: crossvalidated
[post_id]: 578527
[parent_id]: 423856
[tags]: 
First note that a fully connected neural network usually has more than one activation functions (the activation function in hidden layers is often different from that used in the output layer). Any function that is continuous can be used as an activation function, including linear function g(z)=z, which is often used in an output layer. Activation functions in hidden layers are usually nonlinear, e.g. relu, which is a piecewise linear function that introduces the most simple nonlinearity. Other often used activation functions include sigmoid, tanh, softmax, etc.
