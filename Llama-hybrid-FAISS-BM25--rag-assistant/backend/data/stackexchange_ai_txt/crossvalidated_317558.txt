[site]: crossvalidated
[post_id]: 317558
[parent_id]: 
[tags]: 
Can random forest based feature selection method be used for multiple regression in machine learning

I would like to have a good feature selection method for a continuous response variable, over around 100 predictors. I would like to keep my model type as linear multiple regression model, rather than tree-based model. My current method: I could calculate the (linear) correlation between each of my predictor and response, and select a subset of predictors with "strong" correlations for the final multiple regression. The prediction performance of the selected predictors will be then determined in this final multiple regression model. However, feature selection in this way is subjective, and I am afraid of missing "important" features. I would like to apply a more objective and complete way of feature selection, such as "all-relevant" feature selection in Boruta or "variable of importance" in random forest. However, as I understand, both methods are based on tree-based random forest, which are not linear regression. My questions are: Is my current method a proper way to handle my research purpose? Can random forest based feature selection handle feature selection purpose for multiple linear regression model? Are there any other feature selection methods recommended?
