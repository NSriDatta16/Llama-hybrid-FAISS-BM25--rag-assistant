[site]: crossvalidated
[post_id]: 66421
[parent_id]: 
[tags]: 
Why is it difficult to incorporate uncertainty in random effects when making predictions from mixed models?

There are several threads on R-sig-ME about obtaining confidence intervals for predictions using lme4 and nlme in R. For example here and here in 2010, including some commentary by Dougals Bates, one of the authors of both packages. I hesitate to quote him verbatim, for fear of them being taken out of context, but anyway, one comment he makes is "You are combining parameters and random variables in your predictions and I'm not sure what it would mean to assess the variability of those predictions. A Bayesian may be able to make sense of it but I can't get my head around it." https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q1/003447.html I know that the Bayesian glmm package MCMCglmm can produce credible intervals for predictions. Lately, the development version of lme4 on github has been given a predict method, but it is accompanied by the following comment: " @note There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters; we recommend \code{\link{bootMer}} for this task." https://github.com/lme4/lme4/blob/master/R/predict.R So, why is it difficult to incorporate uncertainty in random effects when making predictions from mixed models in a frequentist setting ?
