[site]: crossvalidated
[post_id]: 186754
[parent_id]: 186749
[tags]: 
Your hypothesis concerns whether a particular intervention improved some performance. As stated, we need not create a prediction model necessarily. You are simply trying to estimate a conditional mean difference in the outcome under the two settings: one where the intervention is applied, and the other where it isn't. This may indeed be as simple as a plain old t-test. Controlling for other, independent determinants of the outcome, however, will increase precision (and, therefore, the power to detect if the intervention was effective). If repeated measurements were drawn over time, then you are fitting a basic pre-post type of model using historical trends to infer the typical "intervention free" outcomes. If you implemented the intervention in an A/B setting, then there are groups of individuals representing either scenario. In any case, there may be things influencing the observed outcome having little to do with the intervention, such as autocorrelation or secular trends. Controlling for these in a regression model allows you to make true "apples-to-apples" comparisons. So, depending on the nature of the "influencing" factors, you can always use adjustment in a multivariate regression model. Alternately, if there are 100s of such predictors, controlling for all of them simultaneously is impossible if the sample size is modest (perhaps, under 1000) due to multicollinearity, loss of efficiency, and other considerations. In this case, a sensible approach is using propensity score adjustment. This is a hybrid of prediction and inference. You use these 100s of predictors in a binary regression model to predict who is likely to have received the intervention and who is likely to not have, create a propensity score, and adjust for it in an analysis including the binary intervention indicator. Typically, these are still very highly collinear, so loss of power is still a consideration. The advantage of propensity score adjustment is that you may develope such scores using the machine learning techniques of feature selection to create a parsimonious model when predictors outnumber observations.
