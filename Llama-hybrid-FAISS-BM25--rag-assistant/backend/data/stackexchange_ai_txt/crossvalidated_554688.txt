[site]: crossvalidated
[post_id]: 554688
[parent_id]: 554492
[tags]: 
first author here. Yes $\tau$ can be sampled arbitrarily as long as we have adequate coverage. A sensible choice is to follow a policy $\pi$ , with a higher temperature to encourage exploration, or, as in the paper, take a random action with probability $\ll 1$ . In the paper the policy is constructed from the log-flows as softmax(logits=log_F) (which achieves $p(x)\propto R(x)$ ). In the new Foundations paper , we propose some ways to explicitly parameterize the policy. Perhaps I am misunderstanding the question, but in the flow matching objective of (11) & (12) we do sum over all the parents and all the children of each state in the trajectory (all the states $s$ such that $T(s,a)=s'$ and $T(s',a)=s$ , which I assume you mean here instead of $F$ --we used $F$ for flows and $T$ for the transition). We also expect this to work without having to do a learning step on all states. The idea is that we hope to learn an assignment of the flow that generalizes well, so we can not only do well on the training set, but also an unseen test set. That being said, the point that there are unlimited solutions here holds, but it's possible to constrain things such that there is a unique solution. In the Foundations paper, section 5.2 explains that a "backwards policy" $P_B$ can be chosen freely and that $P_B$ and $R$ together completely and uniquely specify the flow $F$ .
