[site]: crossvalidated
[post_id]: 619707
[parent_id]: 
[tags]: 
Generating "surrogate data" to calculate error on estimators

We have a dataset in the form of a time series $Y_n$ . We assume it follows an underlying parametric distribution $f(n,\beta)$ , $\beta$ being the parameters. From the observed dataset, we get an estimator $\tilde{\beta}$ for the parameters (say from an MLE). I am interested in the uncertainty of this estimator. Of course, there could be a variety of methods, but I am interested in the use of surrogate data as done in this paper: https://www.sciencedirect.com/science/article/pii/S0048733315001699 Basically, the idea is simple: we generate a bunch of artificial data using the distribution $f$ with parameters $\tilde{\beta}$ (we do a Monte Carlo simulation for those who like complicated words to describe simple things). Those generated data will differ from the original: we then apply our MLE to each of these new surrogate data to obtain new estimations. We obtain this way a distribution of estimated parameters, which can be summarized with whatever suitable metric (e.g. std dev) Sounds great to me, makes intuitive sense. The only problem is, I can't really find documentation to back up the validity of this method. When searching, I can't get out of a loophole because it seems that all I can find when looking for surrogate data tests are specific tests for the linearity of a system. I could not find a reference that uses surrogate data in this way to calculate uncertainties on estimators. Question: Is this surrogate data procedure valid for calculating errors on estimators, and could you point me to sources that discuss/prove it?
