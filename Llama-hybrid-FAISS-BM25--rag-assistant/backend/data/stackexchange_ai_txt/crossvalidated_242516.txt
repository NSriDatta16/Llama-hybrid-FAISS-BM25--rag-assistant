[site]: crossvalidated
[post_id]: 242516
[parent_id]: 
[tags]: 
How can one efficiently analyze results from a bibliographic search

I have the following situation. Using scopus ( https://www.scopus.com ) I was looking for scientific articles pertaning to a concrete domain. I obtained 5.000 results. Each entry contains a lot of information about the article (around 40 different features) like country of the authors, research domain, year, number of citations, funding achieved, etc. I wanted them to test some web tool we have developed. So we emailed all of them (since each scopus records contains email for the author of each article) and we observed the following: a) only around 1.000 visited the website b) only around 100 registered on the website c) only 20 used the tool d) only 5 contacted us showing real interest on the tool Here, we would like to analyze data and understand which are the main features, between all those who finally contacted us. We guess that if we find some pattern, then and next time, we will try to find articles of the people pertaining to that pattern. Not so clear is for us how could we achieve this. One approach we guess is to statistically analyze data. Here I have always worked with numeric fields but not text, so I do not know what would be the best approach. Or perhaps we could also train a machine learning method, so once trained, we can input it other articles so that it will try to predict the success chances. So my question here is, what do you think is the best approach to follow. PS: If somebody can help with this, we are working on a scientific publication, he/she could join as coauthor
