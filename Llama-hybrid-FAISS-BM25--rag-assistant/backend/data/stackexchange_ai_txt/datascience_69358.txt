[site]: datascience
[post_id]: 69358
[parent_id]: 
[tags]: 
Why does vanilla transformer has fixed-length input?

I know that in the math on which the transformer is based there is no restriction on the length of input. But I still canâ€™t understand why we should fix it in the frameworks (PyTorch). Because of this problem Transformer-XL has been created. Can you explain to me where this problem is hiding, please?
