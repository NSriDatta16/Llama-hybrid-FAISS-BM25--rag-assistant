[site]: crossvalidated
[post_id]: 529960
[parent_id]: 529926
[tags]: 
While it is true that nowadays it is fairly easy to gather large piles of data, this doesn't mean that it is good data. The large datasets are usually gathered by scraping the resources freely available on Internet, for example, for textual data those may be Reddit post, news articles, Wikipedia entries, for images those may be all kinds of images posted by people, for videos those could be things posted on YouTube. Notice that there are many potential problems with such data. First, it is unlabelled. To label it someone needs to do it. Most commonly, this is done by Amazon Mechanical Turk workers that are paid very little amounts of money for the task so aren't really motivated to do it correctly, neither have any internal motivation for tagging random images. Also, you have no guarantees that the labelers have proper knowledge for tagging (e.g. they are asked to label wild animals, or car brands, that they are not familiar with). You can do it yourself, but you would need a lot of time, and this as well doesn't guarantee that there won't be human errors. You could do the labeling automatically, but then your "clever" machine learning algorithm would learn from the labels provided by a "dumb" heuristic, if the heuristic worked, would you need the more complicated algorithm learn to imitate it..? Second, this data is biased. Most of the textual datasets are limited to English languages. Most of the image datasets with photos of humans depict white-skinned individuals. Most of the datasets with pictures of architecture show cities from the US or Europe. Those aren't really representative, unless you are building a machine learning model that would be used only by the white, English-speaking men living in the US. There was recently a nice preprint on this topic Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks by Northcutt et al.
