[site]: crossvalidated
[post_id]: 507044
[parent_id]: 507028
[tags]: 
In Bayesian model choice , the family of models under comparison is $$\mathfrak F=\left\{p(\cdot|\theta_m,m)\,; m=1,\ldots,M,\,\theta_m\in\Theta_m \right\}$$ and the parameter of a member of this family is the pair $(m,\theta)$ . I index $\theta_m$ by $m$ to stress that the parameter is model-dependent. Given a prior $\pi(\cdot)$ on the pair $(m,\theta)$ , the posterior weight $p(m|D)$ is the marginal in $m$ of the joint posterior $\pi(m,\theta_m|D)$ . The most likely model associated with $p(m|D)$ is a Bayesian decision procedure associated this posterior and a $0-1$ loss function, also known as MAP estimator, but only a Bayesian decision procedure rather than the outcome of a Bayesian analysis . Concerning the worries about over-fitting, the evidence is responding to a larger parameter space by penalising a more poor fit thanks to the integration over this larger parameter space. This is connected with the intuition that distributions in larger dimensions are more concentrated, hence are more penalised when integrated out. The ratio of evidences known as Bayes factors are consistent tools for selecting a model and the first order approximation of the evidence known as BIC, Schwarz' criterion or the "Bayesian" information criterion , $\text{BIC}_m \approx âˆ’2\log p(D|m)$ , is consistent as well. This property of the (Bayesian) evidence to automatically penalise the more complex models is often called Ockham's razor , even though I personally dislike the term as non-quantitative . Some Bayesian papers promote a more balanced perspective, in comparison.
