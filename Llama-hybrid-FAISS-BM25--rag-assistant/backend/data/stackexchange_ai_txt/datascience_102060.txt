[site]: datascience
[post_id]: 102060
[parent_id]: 
[tags]: 
How create a representative small subset from a huge dataset, for local development?

​ I have a time series problem and the dataset I'm using is rather huge. Around 100GB. For local development I'm trying to subset this into a very small batch around 50MB, just to make sure unit tests and some very streamlined "analytic" tests pass, my code is not a mess, and my model is actually trying to do something meaningful with this data. I know that I cannot create a very good "representative" small subset which can totally mimick the original, but I want to make sure I find many of my model's base flaws with this data before training it on that huge dataset. Maybe having multiple different sized batches for different scopes of tests is an option too, I don't have any preferences. ​ What is the best strategy to create this subset? I think for a data that is not sequential, unlike mine, random downsampling of the datapoints might be a good thing, but I don't know what is a good practice in time Series data. Should I just choose a small frame of time as the new dataset? What about casuality? How to sample according to class imbalance? These are the first questions that come to my mind. But feel free to expand on even more questions. Edit: What I am working on is this dataset . The dataset is quite large, and I want to effectively choose a subset from it. The task is to detect seizures. One option is to the number of subjects, I think. But I am open to all options that you might suggest!
