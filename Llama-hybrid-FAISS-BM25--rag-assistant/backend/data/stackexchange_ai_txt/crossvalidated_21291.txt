[site]: crossvalidated
[post_id]: 21291
[parent_id]: 
[tags]: 
SVM, variable interaction and training data fit

I have 2 general/more theoretical question. 1) I'm curious how SVMs handle variable interactions when building predictive models. E.g., if I have two features f1 and f2 and the target depends on f1, f2, and say f1 * f2 (or some function h(f1, f2)), does SVM fit (not just on OOS but even on training data) improve when including f1, f2 and h(f1, f2) in the features over just including f1 and f2? Does the SVM algorithm deal with feature interactions? It seems like it would with how the SVM tries to create hyperplanes in higher dimensional space, but not sure so wanted to ask. 2) When fitting a SVM on training data, given enough features and finding the optimal parameters (via brute force search or whatever), will a SVM always trivially fit the training data? Not sure if I worded that right, but basically, if there is enough variance/noise in the features, will a SVM always fit the training data 100%? Conversely, if the SVM does not fit the training data 100%, does that mean there is some information (or other features) which affect the target variable that are not captured in the data? Thanks Small clarification. I'm referring to kernel SVMs specifically
