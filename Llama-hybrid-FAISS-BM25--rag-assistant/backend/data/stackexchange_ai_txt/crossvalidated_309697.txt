[site]: crossvalidated
[post_id]: 309697
[parent_id]: 48766
[tags]: 
I started digging through the code for the boot package and found the function cv.glm() at https://github.com/cran/boot/blob/5b1e0fea4d1ab1716f2226d673e981d669495b75/R/bootfuns.q#L825 , as well as going through Introduction to Statistical Learning by James et al. I haven't gotten to the $K$-fold CV section yet, but here's my understanding... The first component of delta is the average mean-squared error that you obtain from doing $K$-fold CV. The second component of delta is the average mean-squared error that you obtain from doing $K$-fold CV, but with a bias correction. How this is achieved is, initially, the residual sum of squares (RSS) is computed based on the GLM predicted values and the actual response values for the entire data set. As you're going through the $K$ folds, you generate a training model, and then you compute the RSS between the entire data set of $y$-values (not just the training set) and the predicted values from the training model. These resulting RSS values are then subtracted from the initial RSS. After you're done going through your $K$ folds, you will have subtracted $K$ values from the initial RSS. This is the second component of delta . I'm hoping this is right, as this is how I'm interpreting the code. Here is the code snippet, for your reference. Thankfully, it appears that this code is mostly self-contained. sample0 n) || (K
