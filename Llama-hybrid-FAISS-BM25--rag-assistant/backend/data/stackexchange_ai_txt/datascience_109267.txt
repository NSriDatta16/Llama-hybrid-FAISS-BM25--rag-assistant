[site]: datascience
[post_id]: 109267
[parent_id]: 
[tags]: 
Oversampling in imbalanced classification producing perfect classification

I have a 40 to 1 imbalance for my binary classification problem. I proceeded to solve the imbalance by oversampling and I generated synthetic samples to oversample my minority class. After having almost equal number of samples in both classes, my classification AUC ROC is 0.99 which is way better than I expected. I feel like my machine learning model is just memorizing samples that are too similar and not very generalizable. The analogy I think of is my initial majority class is like flat city and my minority class with the augmentation is a tall tower so it is very easy for my model to identify it. Is it common achieve perfect classification after data augmentation? How do I avoid the model memorizing data? Thanks
