[site]: crossvalidated
[post_id]: 441949
[parent_id]: 388620
[tags]: 
I'm working on something like this now. Let's say the researcher wants a full covariance latent variable $Z ~ N(mu, \Sigma)$ . Like the multiplication trick from VAE, we can have a matrix multiplication trick. Still sample from a high dimensional unit Gaussian, $e \sim N(0, I)$ . There is this property of linear transformations of high D Gaussians: https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations#hid2 . If we want the transformed covariance to be $\Sigma$ , we have to hit $e$ with the matrix square root of the proposal covariance $\Sigma^{1/2}e \sim N(0, \Sigma)$ . Someone above pointed out that $\Sigma$ must be symmetric positive definite. This can be modeled as a sum of rank 1 matrices, formed as the outer product of each component vector. This is a sum of quadratic forms, so we know it will at least be positive semidefinite. To get positive definite, we have to choose a sufficient number of component vectors. By CLT (> 30 components), the off diagonal entries will be zero mean Gaussian, while the main diagonal remains positive. The number of components needed in practice might depend on other factors. I tried this with MNIST and got decent results . There might be a way to enforce PD by parametrizing $\Sigma$ with Inverse Wishart prior, but that also requires one of the parameters to be a symmetric PD matrix.
