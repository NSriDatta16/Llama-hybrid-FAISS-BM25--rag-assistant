[site]: datascience
[post_id]: 108284
[parent_id]: 108277
[tags]: 
I see one possible solution using xgboost (it supports scikit-learn API). Here the documentation explains how to create custom objectives . Assume that we have classification and regression objectives. If they are twice differentiable, we can implement their gradient and hessian functions and combine them into an overall objective: def gradient_clf(predt, dtrain): """Returns f'(x)""" def gradient_reg(predt, dtrain): """Returns g'(x)""" def gradient(predt, dtrain): return gradient_clf(predt, dtrain) + gradient_reg(predt, dtrain) def hessian_clf(predt, dtrain): """Returns f''(x)""" def hessian_reg(predt, dtrain): """Returns g''(x)""" def hessian(predt, dtrain): return hessian_clf(predt, dtrain) + hessian_reg(predt, dtrain) def custom_objective(predt, dtrain): return gradient(predt, dtrain), hessian(predt, dtrain) Then we should pass custom_objective to XGBRegressor 's objective parameter to train the model with our objective. During prediction, for the classification task we either manually choose a threshold for or calibrate obtained confidence scores, and for the regression task we keep the scores.
