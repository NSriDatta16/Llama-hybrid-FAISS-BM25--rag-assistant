[site]: crossvalidated
[post_id]: 561001
[parent_id]: 
[tags]: 
What is the benefit of regression with student-t residuals over OLS regression?

Sometimes I see advice to fit regressions with student-t residuals rather than using OLS (which is equivalent to assuming normally distributed residuals) if the distribution of the residuals is heavy-tailed. However, since the OLS estimator is BLUE (by Gauss-Markov), it should have lower variance (and therefore MSE) than a regression that assumes student-t residuals fit via maximum likelihood. This is true even if the residuals truly are t-distributed. In a simple simulation (see bottom for R code), OLS and t-regression are essentially equivalent with respect to out-of-sample MSE and both achieve correct coverage for CIs for the $\beta$ coefficient even though the true residuals follow a student-t distribution. If OLS performs equally for standard tasks such as prediction (as judged by MSE) or inference (as judged by coverage of CIs for model parameters), what are the advantages of fitting a regression assuming student-t errors? Shouldn't I benefit by estimating parameters assuming a correctly specified likelihood rather than a misspecified likelihood? Or is my simulation misleading? An answer to this post suggests that prediction intervals will be wrong if one fails to use t-distributed errors. Is that really the only benefit? Here's the simulation code: library(hett) # for fitting t-based regressions get_regression_function $loc.fit$ coefficients design_mat $x intercept x)) design_mat $loc.summary$ coefficients) t_pt_est $y t_res y # MSE -- test set ssr_ols[i]
