[site]: crossvalidated
[post_id]: 350863
[parent_id]: 350860
[tags]: 
Not only is it probably a bad idea to single impute the all-missing case with the overall mean, it's also a bad idea to simply take the average among the non-missing cases. Using the data to drive an empirical imputation procedure is only valid when the data are ignorably-missing: that means that missingness doesn't depend on the actual value. Knowing nothing of your problem, I can't help you figure that out. But an example where it matters is, say, self-reported pain scale. If the patient can't actually tell you how much pain they're in because they're in too much pain, then that's not really missing (quite the opposite, in fact). You can't test or determine non-ignorable missingness. When missingness is ignorable, a minimally complete approach to imputation is multiple imputation. If the data are not ignorably missing, you can impute with "worst case scenario" (based on empirical maximums), or use probability models for responses and expectation maximization to model a range of possible responses. EDIT: The data are structured such that there is 1 row per user, and 1 column per item. The value represents the days between when an item came available and when the user buys that item. Missing values means the user has not yet ordered that item. My recommendation is to use a survival model with right censoring for items not-yet ordered. The average is not a valid metric for right censored data.
