[site]: crossvalidated
[post_id]: 262114
[parent_id]: 
[tags]: 
Does it make sense to log-transform the dependent when using Gradient Boosted Trees?

When we are modeling non-negative data with linear regression we often log-transform the dependent, thereby ensuring that all the predictions made by the model will be positive as well. Does it make sense to do the same when using Gradient Boosted Trees (GBTs)? For Random Forests the output is an average of the training labels in the "winning" leaf of each tree, thereby ensuring that the prediction will be non-negative. Is it safe to make the same assumption for GBTs? I haven't been able to follow the derivation from the original paper (section 4.3).
