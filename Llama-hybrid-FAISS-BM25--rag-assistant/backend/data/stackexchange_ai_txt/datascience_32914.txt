[site]: datascience
[post_id]: 32914
[parent_id]: 32859
[tags]: 
Yes. That should be fine. What you suggest makes sense and should not bias the results. The reasons you give are good ones. For any reasonable classifier, if the value of an attribute is always zero in the training set, that should cause the attribute to be essentially ignored. There is a simple test to let you confirm this. You can try, for each document in the validation set, zeroing out the entries in the feature vector that correspond to words that were not present in any document in the training set, and see if that changes the classification. If it doesn't, then you know that your method has had no effect and hasn't introduced any bias. Here is another alternative, if you prefer: Conceptually, in an ideal world the vocabulary for each fold would only include words in the training set. As a matter of implementation, it's certainly possible to implement that in a more efficient way than re-generating the vocabulary and re-generating the feature vectors for each fold. As long as your implementation generates the same vectors, it doesn't matter how it obtains them. Thus, you could have an implementation that works like the following: Generate a "superset vocabulary" that includes all of the words found anywhere in the dataset (not just the training set). Generate a sparse feature vector for each document, based on the "superset vocabulary". I suggest that you represent these in an efficient way, e.g., as a Python dictionary that maps from word to count. For each fold: Split into training and validation sets. Generate a subset vocabulary, containing only the words in the training set. Probably there is a small set $S$ of words that are in the superset vocabulary but not the subset vocabulary, so I suggest storing that set $S$. For each document in the training set and validation set, generate a derived feature vector for the document, using only the subset vocabulary. I suggest generating this by starting from the feature vector for the superset vocabulary, then removing the words in $S$. This should be more efficient than regenerating the feature vector from scratch. Now train and validate these derived feature vectors. This is equivalent to the "conceptually ideal" approach, but will run faster. That said, I think your proposal is the pragmatic one, and in practice should yield equivalent results, and be even faster still.
