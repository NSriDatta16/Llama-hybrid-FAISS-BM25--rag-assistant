[site]: crossvalidated
[post_id]: 487233
[parent_id]: 487129
[tags]: 
The explanation provided in your book is correct. Let's see now your sub-questions: ( Preliminary point ) Gradient Boosting (GB) Machines are iterative ensemble learners. Within the GB framework each new base learner tries to compensate for the deficiencies of the existing base-learners in the ensemble. This contrasts how other ensemble methods work where the predictions of the base learners are averaged together (random forests being the obvious example). Your book correctly emphasises the high learning rate $\alpha$ to show that the first learner matters a lot as the fit it provides is not strongly regularised. If $\alpha$ was low it would be plausible that even missing one of first learners it would not affect the overall fit that much. The overall effect of the first learners being the most impactful and the later learners making significantly diminishing contributions is well-known. A standard way to ameliorate that effect is/was actually using low $\alpha$ values; another prominent technique is DART trees where we "drop trees" at random in order of prevent over-specialisation of the base-learners (see Rashmi & Gilad-Bachrach (2015) DART: Dropouts meet Multiple Additive Regression Trees for more details). Your book correctly emphasises the training set performance to show you that the generalisability of model performance is not the main issue here. Usually when we evaluate a model's performance we have either a separate test set or some validation schema that is performed to emulate how well our model would perform in unseen data. Training set performance is how good we fit our known data. Training set performance is not indicative how generalisable our model is but it is indicative of how well we fit our known response. By focusing on the training set, the writer tries to remove any worries that performance is due to not fitting unseen data adequately.
