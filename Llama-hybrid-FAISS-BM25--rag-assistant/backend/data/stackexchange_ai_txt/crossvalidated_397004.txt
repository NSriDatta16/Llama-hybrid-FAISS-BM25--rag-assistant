[site]: crossvalidated
[post_id]: 397004
[parent_id]: 
[tags]: 
How to compute gain statistic for the multinomial Naive Bayes classifier from Jurafsky and Martin (2018)

I'm trying to figure out how to compute the gain statistic G(w) following the fitting of the multinomial Naive Bayes model . This statistic is described on p17 of the new edition of Jurafsky and Martin, Speech and Language Processing , Chapter 4 "Naive Bayes and Sentiment Classification" . I can compute everything here except the P(c_i|\bar{w}). In the text, they state that \bar{w} means that a document does not contain the word w What if the word exists in every document, as in the example of the word "Chinese" below? In the multinomial variant of Naives Bayes, word likelihoods are based on (smooth) counts, not occurrence or non-occurrence. So how, in very specific terms, is P(c_i|\bar{w}) computed? For illustration, I show a worked example from Manning, Raghavan, & Sch√ºtze (2008). An Introduction to Information Retrieval . Cambridge: Cambridge University Press, Chapter 13 Table 13.1. . This uses the implementation of the multinomial Naive Bayes classifier in my R package quanteda , shown for concrete illustration (this is not an R question however!). library("quanteda") ## Package version: 1.4.1 ## Example from 13.1 of _An Introduction to Information Retrieval_ ## https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf corp Any help is greatly appreciated.
