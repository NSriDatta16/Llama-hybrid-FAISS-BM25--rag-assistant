[site]: datascience
[post_id]: 112190
[parent_id]: 112189
[tags]: 
Recent models using transformers like Lambda have less than 100 trillion parameters, and it answers much better than most humans (if not all, as it has a huge volume of knowledge). I mean that the human brain and the artificial brain are not comparable, and it doesn't mean that having 100 trillion parameters in an artificial brain would be equivalent to a human one. Then, the correlation between parameters and FLOPs is quite linear indeed. I have created a table in an article to have a rough estimation order of magnitude because very little information exists on this topic. Source: https://medium.com/p/1cd2225fd0f2
