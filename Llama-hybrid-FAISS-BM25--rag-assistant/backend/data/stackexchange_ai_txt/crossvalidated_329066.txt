[site]: crossvalidated
[post_id]: 329066
[parent_id]: 
[tags]: 
Boosting A Logistic Regression Model

Adaboost is an ensemble method that combines many weak learners to form a strong one. All of the examples of adaboost that i have read use decision stumps/trees as weak learners. Can i use different weak learners in adaboost? For example, how to implement adaboost (generally boosting) to boost a logistic regression model? One main difference of classification trees and logistic regression is that the former outputs classes (-1,1) while the logistic regression outputs probs. One idea is to choose the best feature X from a set of features and pick up a threshold (0.5?) to convert the probs to classes and then use a weighted logistic regression to find the next feature etc. But i imagine that there exists a general algorithm to boost different weak learners different than decision stumps that outputs probabilities. I believed that Logitboost is the answer to my question but i tried to read the "Additive Logistic Regression" paper and got stuck in the middle.
