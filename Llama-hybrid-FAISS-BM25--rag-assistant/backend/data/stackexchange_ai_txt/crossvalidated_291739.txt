[site]: crossvalidated
[post_id]: 291739
[parent_id]: 291680
[tags]: 
The reason we use dot products is because lots of things are lines. One way of seeing it is that the use of dot product in a neural network originally came from the idea of using dot product in linear regression. The most frequently used definition of a line is $y = ax+b$. But this is the same as saying $b = y-ax$, which is the same as saying $b = (y,x) \cdot (1,-a)$. So mathematically, a line is expressed with a dot product between the coordinate axes $y,x$ and some other vector. And lines are useful for linear regression. And you can view neural networks as a linear model with a nonlinear activation tacked on top.
