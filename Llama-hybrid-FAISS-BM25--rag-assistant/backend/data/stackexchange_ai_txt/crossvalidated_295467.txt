[site]: crossvalidated
[post_id]: 295467
[parent_id]: 294559
[tags]: 
Important References I would strongly suggest you to check the Bayesian Network or probabilistic graphical model literature, which can answer your question perfectly. If you have limited time, this page by Kevin Murphy, A Brief Introduction to Graphical Models and Bayesian Networks is a good start. The page gives basic ideas of inference and learning from data. In section Temporal models, it gives different forms of HMM. Here are some examples. My answers From the way you described your question, I assume you were thinking to represent HMM in "state diagram" . where each node represents a state, and the links represent transition probabilities. I am going to use a different notation here: we use graphical model to represent Hidden Markov Model as in following figure. In your example, this model represent we have $N$ data points on both hidden state $X$ ("temperature"), and observed data $Y$ ("ring sizes"). The arrow in the graph represents the conditional dependencies. $P(X_i|X_{i-1})$ is the "state transition", $2 \times 2$ matrix. $P(Y_i|X_i)$ is the "emission probability", $2 \times 3$ matrix. Note, this diagram gives us the conditional dependency assumptions, i.e., it gives us the join distribution as follows $$ \begin{align*} P(\mathbf X,\mathbf Y) & =\left( P(X_1)\prod_{i=1}^{N-1} P(X_{i+1}|X_{i}) \right) \left( \prod_{i=1}^N P(Y_i|X_{i}) \right)\\ \end{align*} $$ And the joint distribution is parameterized by $2+4+6=12$ parameters, which are $P(X_1)$ a $1\times 2$ matrix, $P(X_i|X_{i-1})$ a $2\times 2$ matrix, $P(Y_i|X_i)$ a $2\times 3$ matrix. When there are more than one "factors" generated by hidden stages, we can change the diagram into Note, the formula of joint distribution will be changed into $$ \begin{align*} P(\mathbf X,\mathbf Y, \mathbf Z) & =\left( P(X_1)\prod_{i=1}^{N-1} P(X_{i+1}|X_{i}) \right) \left( \prod_{i=1}^N P(Y_i|X_{i}) \right)\left( \prod_{i=1}^N P(Z_i|X_{i}) \right)\\ \end{align*} $$ Note, now the model has more parameters in $P(Z_i|X_i)$. In addition, when there are more than one "factors" in hidden state to affect observations, we can use this diagram to represent. (the given model is only ONE example, one can edit it to reflect different dependency assumptions. For example, adding links on $Z_i$.) The are not classical HMM but a general directed model. Different names, e.g., Auto regressive HMM, Input-output HMM Coupled HMM Factorial HMM etc., of the model can be found in Murphy's tutorial page mentioned earlier. For general directed probabilistic graphical model, we still can learn the model from data, and once we have the model, we can run "inference" to predict. The book Bayesian Network in R is a good start.
