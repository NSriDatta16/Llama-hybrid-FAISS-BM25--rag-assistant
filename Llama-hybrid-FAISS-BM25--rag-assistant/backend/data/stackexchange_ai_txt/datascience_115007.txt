[site]: datascience
[post_id]: 115007
[parent_id]: 114994
[tags]: 
TLDR: If you do need to do feature selection then yes you would use R^2. Look up forward and backward feature selection. Another good option is to use L1 regularization. I would probably try L1 first if you're able to run the model with all the features. I am guessing you wont want to do L1 regularization or backward selection because you are trying to avoid running the regression with all variables? If that is the case I would recommend using forward selection and just stop at whatever num variables you want your model to end up having. It is better to do forward selection than simply finding R^2 for every X with the outcome. It sounds like your question is about how to interpret your model. However when I read the comments, it seems like you are asking about how to reduce the number of variables for the model. Based on assuming that you are trying to ask about using less variables. The next question I would ask is what is the goal for reducing the number of attributes? If it is for regularization (to prevent overfitting) then the person that commented about variable selection being unstable is correct. You can do cross validation with L1 or L2 regularization, or both (elastic net). Furthermore, if you do L1 regularization, you can also use this as another way to do feature selection because it will assign coefficients to the variables with 0 or 1. If you are trying to reduce the number of variables because you want to reduce computational complexity to reduce training time or use less memory then you can do feature selection or dimensionality reduction. If you do feature selection you are picking the features that have the best predictive power. There are different ways to do feature selection (ex: forward selection, backward selection). You can also do dimensionality reduction (ex: PCA) which will create "new features" AKA principal components out of the original features which are linear combinations of the original features. These principal components do not have anything to do with the outcome variable (unlike feature selection and regularization). The top principal components are the combos of the original features that provide the most information.
