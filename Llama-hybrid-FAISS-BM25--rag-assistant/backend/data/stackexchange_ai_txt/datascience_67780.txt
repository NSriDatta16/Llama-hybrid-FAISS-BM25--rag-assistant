[site]: datascience
[post_id]: 67780
[parent_id]: 
[tags]: 
GAN loss suddenly explodes and model breaks

Almost every time I've tried to train a DCGAN using keras I find that the loss suddenly skyrockets and the model completely stops improving. I find this happens regardless of what combination of loss functions, learning rates, label smoothing, or any other "GAN hack" I employ (and I've tried a lot, including cloning and running other people's models). Here is my most recent example, from a DCGAN that uses linear activation and MSE loss (metrics are D loss, D accuracy, G loss, G accuracy): Epoch 26: 04224/04334; D=0.147 (79.7%); G=0.646 (14.4%); Epoch 27: 04224/04334; D=0.139 (80.6%); G=0.644 (15.2%); Epoch 28: 04224/04334; D=0.151 (79.0%); G=0.646 (14.2%); Epoch 29: 04224/04334; D=199583.580 (75.8%); G=328619.291 (18.6%); Epoch 30: 04224/04334; D=262175.355 (31.6%); G=14210.427 (77.9%); Epoch 31: 04224/04334; D=948.686 (49.5%); G=135.858 (46.9%); Here are the corresponding image aggregates: I understand that GANs typically fail to converge, but haven't been able to find anyone else with this specific issue (massive model-breaking spike). Any idea what could be causing it?
