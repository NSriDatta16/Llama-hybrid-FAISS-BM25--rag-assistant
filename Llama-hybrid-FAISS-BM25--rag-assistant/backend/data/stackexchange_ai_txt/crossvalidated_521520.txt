[site]: crossvalidated
[post_id]: 521520
[parent_id]: 
[tags]: 
Is looking at the correlation table between predictors an insufficient test for multicollinearity?

In some of these data science analyses that I've seen posted on the web, e.g., https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155 , they "test" for multicollinearity by looking at the correlation table between regressors, so if you see a pair of regressors with a large (in magnitude) correlation coefficient, you drop one of them because it'd otherwise introduce multicollinearity. But this doesn't seem sufficient. This only allows you to see whether a regressor is (or approximately) a linear combination of one other variable, but this doesn't show whether a regressor is linear combination of 2 or more variables. I think a better approach is VIF, but I rarely ever see this in these analyses, so I'm wondering if I'm misunderstanding something here, or most analyses don't require that level of detail? My second question is, say you find a pair of predictors, by looking at the correlation matrix, that have high correlation (e.g., > 0.95). I think it's a sound approach to simply drop one of them, but does it matter which one you drop? I would think it doesn't since you can just write one as a linear combination of the other, and it therefore doesn't matter which one you keep or drop.
