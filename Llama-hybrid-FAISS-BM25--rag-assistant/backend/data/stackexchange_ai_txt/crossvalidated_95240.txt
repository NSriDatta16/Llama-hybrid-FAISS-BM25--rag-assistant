[site]: crossvalidated
[post_id]: 95240
[parent_id]: 95144
[tags]: 
If you follow the math, adding extra training examples implies to recalculate the way you compute the likelihood. Instead of summing over dimensions, you also sum over training examples. If you train one model after the other, there is no guarantee that the EM is going to coverage for every training example, and you are going to end up with bad estimates. Here is a paper that does that for the Kalman Filter (which is an HMM with Gaussian Probabilities), it can give you a taste of how to modify your code so you can support more examples. http://ntp-0.cs.ucl.ac.uk/staff/S.Prince/4C75/WellingKalmanFilter.pdf He also has a lecture on HMM, but the logic is pretty straightforward.
