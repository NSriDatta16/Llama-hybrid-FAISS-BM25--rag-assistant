[site]: crossvalidated
[post_id]: 569132
[parent_id]: 
[tags]: 
Multiple hypothesis testing as explained by Kruschke

I'm reading John Kruschke's Doing Bayesian Data Analysis , and in Chapter 11, he discusses multiple hypothesis testing. He mentions the following: Suppose we want to the hypothesis of fairness for a coin, and we have a second coin in the same experiment that we are also testing for fairness. In real biological research, for example, this might correspond to testing whether the male/female ratio of babies differ from 50/50 in each of two species of animals. We want to monitor the false alarm rate overall, so we have to consider the probability of a false alarm from either one. Thus, the p-value for the first coin is the probability that a proportion, equal to or more extreme than its actual proportion, could arise by chance from either coin This seems a bit weird to me, since the outcomes from both coins are independent. This may be a silly question, but doesn't this type of reasoning imply that for any experiment you do, you have to consider the possibility of false alarms from (a potentially infinite pool) of other experiments that are completely unrelated, for which false alarms could arise?
