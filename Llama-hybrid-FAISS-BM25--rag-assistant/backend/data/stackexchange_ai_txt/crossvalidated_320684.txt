[site]: crossvalidated
[post_id]: 320684
[parent_id]: 320670
[tags]: 
See the last paragraph in the first column of page 5: Speaker verification and identification trials were constructed by randomly picking one anchor positive sample (AP) and 99 anchor negative samples (AN) for each anchor utterance. Then, we computed the cosine similarity between the anchor sample and each of the non-anchor samples. So, it sounds like they run multiple trials (one per test utterance). In each trial they take one of the test utterances (let's call it $x_t$) and create 100 pairs $(x_t, x)$ where $x$ is a randomly chosen utterance. However, they make sure that out of the 100 pairs one of them is positive (i.e. both $x_t$ and $x$ are from the same speaker) and the rest are negative (i.e. $x_t$ and $x$ are from different speakers). You can assign a value of 0 or 1 to each trial by computing the cosine similarity between the two utterances in each pair (i.e. $\frac{x_t \cdot x}{||x_t|| \times ||x||}$) and checking weather the positive pair got the highest similarity score among all the 100 pairs (1) or not (0). Finally, you can run multiple trials to compute the accuracy. In the first column of page 5 in the paper they specify how many trials they ran for each dataset. If you run $N$ trials and $M$ of them evaluate to 1 then the accuracy of your method is $M/N$.
