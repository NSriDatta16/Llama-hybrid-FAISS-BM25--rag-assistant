[site]: crossvalidated
[post_id]: 142787
[parent_id]: 
[tags]: 
Balancing Per-Class Accuracy of Multiclass Classifier

Suppose I have a multi-class classifier like Naive Bayes, k-Nearest Neighbors, Decision Trees, Random Forest, etc. The classifier maps a feature vector to (let's say) 3 classes: A, B, or C. My problem is that the accuracy I get from this classifier is drastically different per-class, for example: A: 93% B: 68% C: 27% I would like to achieve equal or similar per-class accuracy, even if it results in lower average accuracy, like: A: 53% B: 51% C: 56% At the very least, I would like to boost the accuracy on classes that have below-random accuracy. Most people working on multi-class problems seem to report average accuracy, not minimum per-class accuracy or sensitivity. This paper uses neural nets to address this problem. My question: Are there systematic ways of adjusting either (1) the input to the classifier, (2) the parameters of the classifier, or (3) the output of a multi-class classifier in order to balance its per-class accuracy? Note: I'm working with Python's scikit-learn module.
