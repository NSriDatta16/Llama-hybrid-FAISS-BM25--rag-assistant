[site]: datascience
[post_id]: 85991
[parent_id]: 27687
[tags]: 
It is not the number of many articles that matter but the total number of words. Enough "meaningful/good" is an empirical question that depends on the dataset. One way to test the results of a newly trained model is the Google analogy test set which compares a new model's predicted word to established embedding benchmarks. As far as minimum number examples needed for each unique token in vocabulary, the general consensus is there should be at least 40 examples per token. If there are fewer than 40 examples for a token, the vector estimates can be unstable and the token should be dropped from training.
