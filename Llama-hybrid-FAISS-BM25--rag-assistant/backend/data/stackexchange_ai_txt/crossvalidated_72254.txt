[site]: crossvalidated
[post_id]: 72254
[parent_id]: 72226
[tags]: 
Let's unpack the formula. Ignoring the controversial factor of $1/N$, it is a sum of expressions of the form $$\frac{(T_{exp}-T_{fit})^2}{\sigma^2_{exp}}.$$ These are squares of ratios. The numerators, $T_{exp}-T_{fit},$ are the residuals : they (additively) compare the observed values (presumably $T_{exp}$) to the fitted values $T_{fit}$. This makes sense when the hypothesized explanation for any differences between the fit and the observations is some random, uncorrelated additive variation. The denominators, $\sigma_{exp},$ are estimates of the expected amount of variation (based on the fit). In some circumstances they vary from point to point; in other situations they are taken as constant. No matter: the point is that the ratio $$\frac{T_{exp}-T_{fit}}{\sigma_{exp}}$$ re-expresses each residual as a multiple of the expected amount of variation. (Notice that it is unitless.) Intuitively, then, these ratios ought to be around $\pm 1$ in size, although we would hope many of them would be closer to $0$. If we make a number of statistical assumptions, have sufficient amounts of data, perform least-squares or maximum likelihood fitting, and invoke the Central Limit Theorem, we may justify considering these ratios as being approximately, sort of, perhaps, Normally distributed. The sum, then--without the offending factor of $1/N$ that would turn it into a mere average-- looks like the sum of squares of uncorrelated standard Normal variables. This is the definition of $\chi^2$. That is, when all these assumptions hold, we may interpret the sum $$\sum_i\frac{(T_{exp}(i)-T_{fit}(i))^2}{\sigma^2_{exp}(i)} = \sum_i\left(\frac{T_{exp}(i)-T_{fit}(i)}{\sigma_{exp}(i)}\right)^2$$ (where the index $i$ is used to enumerate the observations) as if it were a single random draw from a $\chi^2$ distribution. Actually that's not quite right, because these residuals are correlated. They are connected by virtue of the common underlying fit, which typically relies on estimates of a small number of parameters, say $p$ of them. Because of this, the sum-- qua random variable--actually behaves more like a sum of $N-p$ squared standard Normal variates. This value, $N-p$, is the number of "degrees of freedom." Were we to divide the sum by $N$, we would obtain a scaled $\chi^2$ distribution. There's nothing wrong about this, but it isn't too helpful, either: tables and software compute values of $\chi^2$ distributions, not values divided by $N$. After all, $N$ isn't even directly relevant: only $N-p$ matters in calculating $\chi^2$. Leaving out the $1/N$ factor therefore makes the calculation depend only on one number, the degrees of freedom, rather than a pair of numbers. That--the second version in the question--is the one to use.
