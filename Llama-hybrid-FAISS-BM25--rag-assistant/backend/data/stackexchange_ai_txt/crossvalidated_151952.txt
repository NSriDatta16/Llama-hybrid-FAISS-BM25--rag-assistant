[site]: crossvalidated
[post_id]: 151952
[parent_id]: 151837
[tags]: 
It is true. In fact, it suffices only to assume that $n-r(n)$ diverges. There's a meta-theorem lurking here, so allow me at the outset to be vague both about the form of convergence and even about what is converging. Suppose we are given a countable sequence $(X_i)=X_1, X_2, \ldots, X_n, \ldots$ of mathematical objects with the following minimal properties: They can be averaged . Specifically, for $0\le m \lt n$ let $$S_{m,n} = (X_{m+1} + X_{m+2} + \cdots + X_n) / (n-m)$$ be a "partial average" between $m$ and $n$. Their averages converge: $S_n \to \mu$ as $n\to \infty$. They are "self-similar" in the sense that any property of the sequence also holds for any tail sequence $(Y_i)$ where $Y_i = X_{m+i}$ and $m \ge 0$. (In particular, this means that if the tail averages of the $X_i$ converge to $\mu$ at a certain rate, then the tail averages of the $Y_i$ also converge to $\mu$ at the same rate. ) Let $\mathbb{N} = \{1, 2, 3, \ldots\}$ be the non-zero natural numbers and suppose $r: \mathbb{N}\to \mathbb{N}\cup\{0\}$ for which $r(n) \lt n$ for all $n$. The question asks about the convergence of $S_{r(n), n}$. Let's see what might be needed to make that happen. About the only effective way to analyze these partial averages is to break up the sums: $$\eqalign{ n S_n &= n(X_1 + \cdots + X_n)/n \\ &= (X_1 + \cdots + X_r(n)) + (X_{r(n)+1} + \cdots + x_n) \\ &= r(n) S_{r(n)} + (n-r(n)) S_{r(n), n}. }$$ Upon rearranging we may solve for the tail average: $$S_{r(n), n} = \frac{n S_n - r(n)S_{r(n)}}{n - r(n)}.$$ This can give us trouble if the denominator remains small. Suppose, though, that $n - r(n) \to \infty$: that is, for any $N\in\mathbb{N}$, there exists $n_0$ such that $n-r(n) \gt N$ for all $n\ge n_0$. It's easy to see that in this circumstance the right hand side will also converge to $\mu$, because $S_n\to \mu$. $S_{r(n^\prime)} \to \mu$ for any subsequence $n^\prime$ for which $r(n^\prime)\to\infty$. $\frac{r(n^\prime)}{n^\prime-r(n^\prime)}S_{r(n^\prime)}\to 0$ and $\frac{n^\prime - r(n^\prime)}{n^\prime-r(n^\prime)}S_{n^\prime}\to \mu$ for any subsequence $n^\prime$ for which $r(n^\prime)$ remains bounded. Conversely, if $n-r(n)$ has a bounded infinite subsequence $n^\prime$, then all bets are off: $S_{r(n^\prime), n^\prime}$ is a sequence of tail averages involving a bounded number of the $X_i$ and the assumptions (1) - (3) do not allow us to infer anything about them. Almost any sequence that does not exactly equal its limit after a finite number of elements would serve as a counterexample. It is therefore apparent that the proper condition to invoke (because it is both necessary and sufficient for convergence of the tail averages) is the assumption that $n-r(n)$ diverges: this will suffice to guarantee the convergence of $S_{r(n), n}$ to $\mu$. This is a weaker condition than $\lim\inf \frac{r(n)}{n} \gt 0$ (which clearly implies the divergence of $r(n)$). Consider, for instance, the sequence $r(n) = \lfloor \sqrt{n}\rfloor$. For this sequence, $r(n)$ diverges but $r(n)/n$ converges to zero. Providing the detailed steps for a sequence of iid random variables and almost sure convergence is now a purely mechanical exercise, which I leave to the reader (because this is a self-study question).
