[site]: datascience
[post_id]: 126406
[parent_id]: 
[tags]: 
GAN/DC-GAN isn't converging

I've been trying to train a vanilla GAN(for MNIST) for a few days, and nothing works. I've tried a lot of different layers, hyperparameters, and more, but every time the discriminator's loss decreases(slowly), the generator's loss increases(pretty fast), and the outcome is garbage. After that, I've tried taking an existing GAN code from GitHub to check if it'll work, and again, nothing worked. Then, I thought that maybe the vanilla architecture is just really not stable, so I've implemented the DC-GAN, with the same settings that's on the PyTorch tutorial, and again, didn't work. note: Both of the architectures that I'm using are taken from tutorials/Git repos/something else, and there, it converged Here's my(pretty messy) GAN code: import torch import matplotlib.pyplot as plt import numpy as np from torch import nn as nn from torch import functional as F from torch.utils.data import Dataset, DataLoader from torchvision import datasets from torchvision import transforms class Discriminator(nn.Module): def __init__(self,in_dims) -> None: super(Discriminator,self).__init__() self.in_dims = in_dims self.flatten = nn.Flatten(start_dim=1,end_dim=-1) self.fc1 = nn.Linear(np.prod(in_dims), 256) self.relu1 = nn.LeakyReLU(0.1) self.fc2 = nn.Linear(256,128) self.relu2 = nn.LeakyReLU(0.2, inplace=True) self.fc3 = nn.Linear(128,1) self.sigmoid = nn.Sigmoid() def forward(self,x): x = self.flatten(x) x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return self.sigmoid(x) class Generator(nn.Module): def __init__(self,input_dim,output_dim=784) -> None: super(Generator,self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.bn1 = nn.BatchNorm1d(128) self.relu1 = nn.LeakyReLU(0.1, True) self.fc2 = nn.Linear(128, 256) self.bn2 = nn.BatchNorm1d(256) self.relu2 = nn.LeakyReLU(0.2, True) self.fc3 = nn.Linear(256, 512, bias=False) self.bn3 = nn.BatchNorm1d(512) self.relu3 = nn.LeakyReLU(0.2, True) self.fc4 = nn.Linear(512, 1024, bias=True) self.fc4 = nn.Linear(512, 1024, bias=False) self.bn4 = nn.BatchNorm1d(1024) self.relu4 = nn.LeakyReLU(0.2, True) self.fc5 = nn.Linear(1024, 28 * 28, bias=True) self.tanh = nn.Tanh() self.reshape = torch.reshape def forward(self,x): x = self.fc1(x) x = self.bn1(x) x = self.relu1(x) x = self.fc2(x) x = self.bn2(x) x = self.relu2(x) x = self.fc3(x) x = self.bn3(x) x = self.relu3(x) x = self.fc4(x) x = self.bn4(x) x = self.relu4(x) x = self.fc5(x) x = self.tanh(x) return self.reshape(x, (-1,28,28)) def view_batch_grid(batch, labels, figsize=(8,8), cols=3, rows=3, label_dict={0 : "fake", 1: "real"}): figure = plt.figure(figsize=figsize) for i in range(1, rows * cols + 1): sample_idx = torch.randint(len(batch), size=(1,)).item() img, label = batch[sample_idx], labels[sample_idx] figure.add_subplot(rows, cols, i) plt.title(label_dict[int(label)]) plt.axis("off") plt.imshow(img.squeeze().detach().numpy(), cmap="gray") plt.show() dataset = datasets.MNIST( root="dataset", train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.1307],[0.3081])]) ) dataloader = DataLoader(dataset, batch_size=64, shuffle=True) device = torch.device("cuda") discriminator = Discriminator(784).to(device) generator = Generator(100).to(device) criterion = nn.BCELoss() lr1 = 3e-4 lr2 = 3e-4 epochs = 25 latent_dim = 100 disc_optim = torch.optim.SGD(discriminator.parameters(), lr=lr1) gen_optim = torch.optim.Adam(generator.parameters(), lr=lr2) gen_loss_hist = [] disc_loss_hist = [] for epoch in range(epochs): for iter, real_batch in enumerate(dataloader): z_samples = torch.randn(dataloader.batch_size,latent_dim).to(device) fake_batch = generator(z_samples) real_pred = discriminator(real_batch[0].squeeze().to(device)) fake_pred = discriminator(fake_batch.detach()) disc_real_loss = criterion(real_pred, torch.ones_like(real_pred)) disc_fake_loss = criterion(fake_pred, torch.zeros_like(fake_pred)) disc_loss = (disc_real_loss + disc_fake_loss) / 2 discriminator.zero_grad() disc_loss.backward() disc_optim.step() disc_loss_hist.append(disc_loss.item()) fake_pred = discriminator(fake_batch) gen_loss = criterion(fake_pred, torch.ones_like(fake_pred)) generator.zero_grad() gen_loss.backward() gen_optim.step() gen_loss_hist.append(gen_loss.item()) if iter % 100 == 0: print(f'Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}, Epoch: {epoch}') plt.plot(gen_loss_hist, label="Generator Loss") plt.plot(disc_loss_hist, label="Discriminator Loss") plt.legend() plt.show() z_samples = torch.randn(dataloader.batch_size,latent_dim).to(device) fake = generator(z_samples).to(torch.device("cpu")) view_batch_grid(fake, labels=torch.zeros(len(fake))) And here is a plot of the losses(after 25 epochs), and results: Here is the DC-GAN: import torch import matplotlib.pyplot as plt import numpy as np from torch import nn as nn from torch import functional as F from torch.utils.data import Dataset, DataLoader from torchvision import datasets from torchvision import transforms class Generator(nn.Module): def __init__(self,nz=100,ngf=28,nc=1): super(Generator,self).__init__() self.main = nn.Sequential( nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(inplace=True), nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(inplace=True), nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(inplace=True), nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(inplace=True), nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False), nn.Tanh() ) def forward(self,x): return self.main(x) class Discriminator(nn.Module): def __init__(self,nc=1, ndf=64) -> None: #Gets a (1,28,28) image super(Discriminator,self).__init__() self.main = nn.Sequential( nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(negative_slope=0.2, inplace=True), nn.Conv2d(ndf, ndf * 2, 4, 2, 1), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(negative_slope=0.2, inplace=True), nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(negative_slope=0.2, inplace=True), nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=False), nn.Sigmoid() ) def forward(self,x): return self.main(x) def view_batch_grid(batch, labels, figsize=(8,8), cols=3, rows=3, label_dict={0 : "fake", 1: "real"}): figure = plt.figure(figsize=figsize) for i in range(1, rows * cols + 1): sample_idx = torch.randint(len(batch), size=(1,)).item() img, label = batch[sample_idx], labels[sample_idx] figure.add_subplot(rows, cols, i) plt.title(label_dict[int(label)]) plt.axis("off") plt.imshow(img.squeeze().detach().numpy(), cmap="gray") plt.show() dataset = datasets.MNIST( root="dataset", train=True, download=True, transform=transforms.Compose([ transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]) ]) ) dataloader = DataLoader(dataset, batch_size=64, shuffle=True) device = torch.device("cuda") discriminator = Discriminator().to(device) generator = Generator().to(device) criterion = nn.BCELoss() epochs = 15 latent_dim = 100 disc_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5,0.999)) gen_optim = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5,0.999)) gen_loss_hist = [] disc_loss_hist = [] for epoch in range(epochs): for iter, real_batch in enumerate(dataloader): z_samples = torch.randn(dataloader.batch_size,latent_dim,1,1).to(device) fake_batch = generator(z_samples) real_pred = discriminator(real_batch[0].to(device)).reshape(-1) disc_real_loss = criterion(real_pred, torch.ones_like(real_pred)) fake_pred = discriminator(fake_batch.detach()).reshape(-1) disc_fake_loss = criterion(fake_pred, torch.zeros_like(fake_pred)) disc_loss = (disc_real_loss + disc_fake_loss) / 2 discriminator.zero_grad() disc_loss.backward() disc_optim.step() disc_loss_hist.append(disc_loss.item()) fake_pred = discriminator(fake_batch).reshape(-1) gen_loss = criterion(fake_pred, torch.ones_like(fake_pred)) generator.zero_grad() gen_loss.backward() gen_optim.step() gen_loss_hist.append(gen_loss.item()) if iter % 100 == 0: print(f'Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}, Epoch: {epoch}') plt.plot(gen_loss_hist, label="Generator Loss") plt.plot(disc_loss_hist, label="Discriminator Loss") plt.legend() plt.show() z_samples = torch.randn(dataloader.batch_size,latent_dim,1,1).to(device) fake = generator(z_samples).to(torch.device("cpu")) view_batch_grid(fake, labels=torch.zeros(len(fake))) And here's the plot of the losses, and the results:
