[site]: datascience
[post_id]: 121983
[parent_id]: 121982
[tags]: 
The number of input channels to a convolutional layer is given by the output of its previous layer. In this case, it is given by the output of the first 1D convolution (because ReLU is an element-wise operation so it does not change the dimensionality of its input), which outputs 48 channels according to the description you provided. The 1D convolution has a small matrix, the "kernel", which is shifted over the input matrix along a given dimension. An individual kernel's dimensions are width $\times$ input channels. The kernel is multiplied element-wise with the overlapping part of the input, and the result is added into a single element in the output. Then, we shift the kernel stride positions and do the same over the whole length of the input. You do the same with as many different kernels as the defined number of output channels. Actually, all kernels of a 1D convolutional layer are usually grouped into a single tensor of dimensionality width $\times$ input channel $\times$ output channels. stride defines the jump size of the shifts, so it determines the length of the output of the convolution: the higher the stride the shorter the output. kernel_size determines the width of the kernel, so it may or may not affect the length of the output, depending on the padding value because, without padding, the kernel can only be applied where there is enough room to overlap completely with the input and not outside of it (which would be allowed if we had padding); given that padding is 0 by default, the higher kernel_size , the shorter the length of the output of the convolution. To help understand, maybe you can check the following animation. It is meant for word embeddings input, but it's the same for any other signal type. In the animation, kernel_size is 3, input channels is 4, stride is 0, and they do have full padding. Here we only have one kernel, so there is one output channel. Having stride > 0 would mean that the shift will have jumps instead of being continuous; as you can see, it is not related to the number of channels. Should they don't have padding, the kernel could only be computed for the positions with ful overlap with the input, leading to shorted output length; again, not related to the number of channels.
