[site]: crossvalidated
[post_id]: 322789
[parent_id]: 
[tags]: 
Compute $\pi(H_0|x)$ with Jeffreys prior for a family $N(\theta,1)$

Given a random sample $x = (x_1,\ldots,x_n)$ taken from a family $\{N(x|\theta,1):\theta \in \mathbb{R}\}$. And consider the hypothesis test: $H_0: \theta = 0 $ vs $H_1: \theta \in \mathbb{R}$ (this is what my classnotes say although I usually understand it as $\theta \in \mathbb{R} \setminus \{0\}$). I'm asked to compute $\pi(H_0|x)$ given that $\pi(H_0) = \pi(H_1) = \frac{1}{2}$ and that the prior is the Jeffreys distribution. My approach The Jeffreys prior for a $N(x|\theta,1)$ a normal with known variance and unknown mean is an improper prior $\pi^J(\mu) = c$ with $c > 0$. By Bayes' rule, $\pi(H_0|x) = \frac{\pi(H_0) m(x|H_0)}{\pi(H_0) m(x|H_0) + \pi(H_1) m(x|H_1)} = \frac{0.5 m(x|H_0)}{0.5 m(x|H_0) + 0.5 m(x|H_1)}$ where: $m(x|H_0) = f(x|\theta_0) = N(x|0,1)$ $m(x|H_1) = \int_{\mathbb{R} \setminus \{0\}} f(x|\theta_1,H_1) \pi(\theta_1|H_1) d\theta_1 = \sqrt{2\pi}c$. After these considerations: $\pi(H_0|x) = \frac{0.5e^{-x^2/2}}{0.5e^{-x^2/2} + 0.5 c \sqrt{2\pi}} = \frac{1}{1 + c \sqrt{2\pi}e^{x^2/2}}$ My question Is this a valid deduction of the posterior? Is this a valid posterior? References Some of my approach comes from " The Bayesian Choice " by C. P. Robert around section 5.2.5.
