[site]: crossvalidated
[post_id]: 397008
[parent_id]: 395197
[tags]: 
Short answer: The main reason for overfitting is using a complex model when you have a small training set. The main reason for underfitting is using a model that is too simple and can not perform well on the training set. Main reason for overfitting? Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. -Deep Learning book, Goodfellow et al. The goal of machine learning is to train a model on the training set in hopes that it will perform just as well on the testing data. But does getting good performance on the training set always translate to a good performance on the testing set? It will not, because your training data is limited . If you have limited data, your model might find some patterns that work for that limited training set, but those patterns do not generalize to other cases (i.e. test set). This can be solved by either: A- Providing a larger training set to the model to reduce the chance of having arbitrary patterns in the training set. B- Using a simpler model so that the model will not be able to find those arbitrary patterns in the training set. A more complex model will be able to find more complicated patterns, so you need more data to make sure your training set is large enough to not contain arbitrary patterns. (e.g. Imagine you want to teach a model to detect ships from trucks and you have 10 images of each. If most of the ships in your images are in the water, your model might learn to classify any picture with a blue background as a ship instead of learning how a ship looks. Now, if you had 10,000 images of ships and trucks, your training set is more likely to contain ships and trucks in a variety of backgrounds and your model can no longer just rely on the blue background.) Main reason for underfitting? Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Models with low capacity may struggle to fit the training set. -Deep Learning book, Goodfellow et al. Underfitting occurs when your model is just not good enough to learn the training set, meaning your model is too simple. Whenever we start solving a problem, we want a model which is at least able to get a good performance on the training set, and then we start to think of reducing overfitting. Generally, the solution to underfitting is pretty straight forward: Use a more complex model.
