[site]: crossvalidated
[post_id]: 584531
[parent_id]: 405814
[tags]: 
This a good question, and one that Iâ€™ve encountered a number of times at work. The short answer is it cannot hurt to scale even when input variables are all on same scale (in your case 0-100). When input variables to your model have smaller values, in the range of 0-1, training speed can increase and potential problems of getting stuck in local optimas (in case of neural networks) can be avoided. You may also have identical scales for your input variables but with various means and standard deviations, and so is not standardized. You were right to test scaling vs not as to inform any differences. As you revealed, scaling provided more explanation of variance. Ultimately in PCA you attempting to draw out components that most explain variance in your data. If scaling helps you to identify that, and something like Factor Analysis can be used to confirm it, then scaling is beneficial for your use case.
