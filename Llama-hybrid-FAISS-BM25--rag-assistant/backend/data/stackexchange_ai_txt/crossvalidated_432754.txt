[site]: crossvalidated
[post_id]: 432754
[parent_id]: 
[tags]: 
Why Log loss, AUC and precision & recall change differently when class imbalance problem is tackled?

I have a dataset and I'm working on a binary classification task with it. It has a class imbalance problem where False class versus True class is 10: 1. If I train a neural network on it directly without tackling the imbalance problem, I got the following result: test LogLoss 0.2025 test AUC 0.8578 precision recall f1-score support False 0.94 0.99 0.96 2294923 True 0.84 0.30 0.44 224278 accuracy 0.93 2519201 macro avg 0.89 0.65 0.70 2519201 weighted avg 0.93 0.93 0.92 2519201 After I added class weight to the classifier training process, i.e., class_weight={0: 1., 1: 10.}, I retrained the model and got the following result: test LogLoss 0.4166 test AUC 0.8646 precision recall f1-score support False 0.97 0.81 0.88 2294923 True 0.28 0.74 0.41 224278 accuracy 0.81 2519201 macro avg 0.62 0.78 0.65 2519201 weighted avg 0.91 0.81 0.84 2519201 It seems the log loss is worse but the AUC is better. The True class's precision is worse but recall is better. How do you explain these changes in metrics, why some are better and some worse? Based on the result,should I use class weight in the training?
