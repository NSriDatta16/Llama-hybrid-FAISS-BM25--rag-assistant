[site]: crossvalidated
[post_id]: 135264
[parent_id]: 8630
[tags]: 
Let's say I choose some linear combination of these variables -- e.g. $A+2B+5C$, could I work out how much variance in the data this describes? This question can be understood in two different ways, leading to two different answers. A linear combination corresponds to a vector, which in your example is $[1, 2, 5, 0, 0, 0]$. This vector, in turn, defines an axis in the 6D space of the original variables. What you are asking is, how much variance does projection on this axis "describe"? The answer is given via the notion of "reconstruction" of original data from this projection, and measuring the reconstruction error (see Wikipedia on Fraction of variance unexplained ). Turns out, this reconstruction can be reasonably done in two different ways, yielding two different answers. Approach #1 Let $\newcommand{\S}{\boldsymbol \Sigma} \newcommand{\w}{\mathbf w} \newcommand{\v}{\mathbf v}\newcommand{\X}{\mathbf X} \X$ be the centered dataset ($n$ rows correspond to samples, $d$ columns correspond to variables), let $\S$ be its covariance matrix, and let $\w$ be a unit vector from $\mathbb R^d$. The total variance of the dataset is the sum of all $d$ variances, i.e. the trace of the covariance matrix: $T = \mathrm{tr}(\S)$. The question is: what proportion of $T$ does $\w$ describe? The two answers given by @todddeluca and @probabilityislogic are both equivalent to the following: compute projection $\X \w$, compute its variance and divide by $T$: $$R^2_\mathrm{first} = \frac{\mathrm{Var}(\X \w)}{T} = \frac{\w^\top \S \w}{\mathrm{tr}(\S)}.$$ This might not be immediately obvious, because e.g. @probabilityislogic suggests to consider the reconstruction $\X \w \w^\top$ and then to compute $$\frac{\|\X\|^2 - \|\X-\X \w \w^\top\|^2}{\|\X\|^2},$$ but with a little algebra this can be shown to be an equivalent expression. Approach #2 Okay. Now consider a following example: $\X$ is a $d=2$ dataset with covariance matrix $$\S = \left(\begin{array}{c}1&0.99\\0.99&1\end{array}\right)$$ and $\mathbf w = (\begin{array}{}1&0\end{array})^\top$ is simply an $x$ vector: The total variance is $T=2$. The variance of the projection onto $\w$ (shown in red dots) is equal to $1$. So according to the above logic, the explained variance is equal to $1/2$. And in some sense it is: red dots ("reconstruction") are far away from the corresponding blue dots, so a lot of the variance is "lost". On the other hand, the two variables have $0.99$ correlation and so are almost identical; saying that one of them describes only $50\%$ of the total variance is weird, because each of them contains "almost all the information" about the second one. We can formalize it as follows: given projection $\X\w$, find a best possible reconstruction $\X\w\v^\top$ with $\v$ not necessarily the same as $\w$, and then compute the reconstruction error and plug it into the expression for the proportion of explained variance: $$R^2_\mathrm{second}=\frac{\|\X\|^2 - \|\X-\X \w \v^\top\|^2}{\|\X\|^2},$$ where $\v$ is chosen such that $\|\X-\X \w \v^\top\|^2$ is minimal (i.e. $R^2$ is maximal). This is exactly equivalent to computing $R^2$ of multivariate regression predicting original dataset $\X$ from the $1$-dimensional projection $\X\w$. It is a matter of straightforward algebra to use regression solution for $\v$ to find that the whole expression simplifies to $$R^2_\mathrm{second}=\frac{\|\S \w\|^2}{\w^\top \S \w \cdot \mathrm{tr}(\S)}.$$ In the example above this is equal to $0.9901$, which seems reasonable. Note that if (and only if) $\w$ is one of the eigenvectors of $\S$, i.e. one of the principal axes, with eigenvalue $\lambda$ (so that $\S \w = \lambda \w$), then both approaches to compute $R^2$ coincide and reduce to the familiar PCA expression $$R^2_\mathrm{PCA} = R^2_\mathrm{first} = R^2_\mathrm{second} = \lambda/\mathrm{tr}(\S) = \lambda/\sum \lambda_i.$$ PS. See my answer here for an application of the derived formula to the special case of $\w$ being one of the basis vectors: Variance of the data explained by a single variable . Appendix. Derivation of the formula for $R^2_\mathrm{second}$ Finding $\v$ minimizing the reconstruction $\|\X-\X \w \v^\top\|^2$ is a regression problem (with $\X \w$ as univariate predictor and $\X$ as multivariate response). Its solution is given by $$\v^\top = \left((\X \w)^\top (\X \w)\right)^{-1}(\X \w)^\top \X = (\w^\top \S \w)^{-1} \w^\top \S.$$ Next, the $R^2$ formula can be simplified as $$R^2=\frac{\|\X\|^2 - \|\X-\X \w \v^\top\|^2}{\|\X\|^2} = \frac{\|\X \w \v^\top\|^2}{\|\X\|^2}$$ due to the Pythagoras theorem, because the hat matrix in regression is an orthogonal projection (but it is also easy to show directly). Plugging now the equation for $\v$, we obtain for the numerator: $$\|\X \w \v^\top\|^2 = \mathrm{tr}\left(\X \w \v^\top (\X \w \v^\top)^\top\right) = \mathrm{tr}(\X\w\w^\top\S\S\w\w^\top\X^\top)/(\w^\top\S\w)^2=\mathrm{tr}(\w^\top\S\S\w)/(\w^\top\S\w) = \|\S\w\|^2 / (\w^\top\S\w).$$ The denominator is equal to $\|\X\|^2 = \mathrm{tr}(\S)$ resulting in the formula given above.
