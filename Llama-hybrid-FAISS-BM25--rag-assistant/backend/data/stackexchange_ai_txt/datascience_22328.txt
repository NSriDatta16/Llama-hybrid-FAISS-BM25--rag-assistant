[site]: datascience
[post_id]: 22328
[parent_id]: 22314
[tags]: 
so what steps should I take when creating a network based around strings for the first time? Neural networks work with numerical data. They also work best with relatively small floating point numbers, centred around zero. You can be less strict about that part, but you will often see the approach in neural networks of calculating the mean and standard deviation from the training data, for each feature, then converting all the features by doing x = (x - means) / stds (you want to store these scaling factors you used along with the network data, because you will want to re-used the same values when you use the network to make predictions later). So what do you do if the input data is not already in this form? You prepare it in your code, just before using it to train or predict. It is a very common structure to see in machine learning scripts: raw_features, raw_labels = load_from_disk( some_data_source ) all_features = convert_features( raw_features ) all_labels = convert_labels( raw_labels ) train_X, test_X, train_y, test_y = split_data( all_features, all_labels ) model = build_model( .... various model params ....) model.fit( train_X, train_y ) test_predictions = model.predict( test_X ) report_accuracy( test_predictions, test_Y ) The above is rough pseudo code, so typically all the functions above have different names, or are multiple lines that do the same thing that you might not bother to encapsulate into a re-usable method if you are writing a quick script. The part I have shown that splits the features might be built in to the training function, and it is also common that the training process can use the test data to help monitor progress. If the loading and conversion takes a long time, you might do it in a separate script and save the resulting NumPy array in a separate file to load it quicker next time. So the part you are concerned about is how you might build a convert_features section of your code from the starting strings. The answer is to use whatever values you can extract from your strings that might be relevant. The string length might be a simple start i.e. len( text ) - but you can also look into any other measure you can figure out (e.g. number of vowels, which uncommon bi-grams are in the word). Deciding which features to try and testing between them is feature engineering , and this often involves some creativity. The important thing is that the features must all be numeric. For a neural network, you should also try and make them relatively small and/or convert them to have mean 0, standard deviation 1 before going to next stage. When you use the network to make predictions later, you have to repeat most of the pipeline: model = load_model( model_file_or_identifier ) raw_features, raw_labels = fetch_data( some_data_source ) X = convert_features( raw_features ) y = convert_labels( raw_labels ) predictions = model.predict( X ) report_predictions( X, y )
