[site]: crossvalidated
[post_id]: 601542
[parent_id]: 347087
[tags]: 
How you resolve ties will change your answers, by small amounts and I've had it be enough to change whether a covariate is statistically significant enough to include in the model under border line cases. For that reason, you want to square up your method of resolving ties in your first few iterations of your model ... not at the end. In most cases you don't have infinitely small time slices available. In the real world your known time frames might be a day or even a week. The fewer discrete time points and the more data you have, the more likely it is that you'll have multiple data instances for a given point in time (i.e. "ties"). The "discrete" computations in various platforms will process an average that is usable and quick if you don't have too much data. Discrete assumes all events at a given time point were actually simultaneous and uses a complex averaging algorithm to estimate that point in time. SAS has an additional, similar method called "exact" which is similar to "discrete" except that it allows for the fact that some events are actually a little earlier than others at a given time period such as a day. So, Discrete and Exact are two approaches to approximating averages based on the literal probabilities from simultanaeous data. I believe the "Breslow" method is a common default in some software and in situations where you care about accuracy, a lot of data ties can throw it off. The Efron method provides a very good model for balancing multiple data inputs from the same time point and it runs quicker on large sets than probability calcs such as the "discrete" method. There are other methods. If your boss is not technical and you're just an IT guy wanting to be able to say "I did some ML and got an answer from the machine" you probably don't care about this. The machine will always give you some answer and you'll be able to print a chart. If you're a statistician or maybe a trained data scientist working in Pharma or modeling accidents or some other field where you need the absolute truth for making split-hair decisions in the face of chaos, you want to get this bit of fine-tuning right. And - honestly - Cox Models are already a little fragile, since the rely on the PH assumption being pretty much true across the full range of data. Also, according to Alison, it can sometimes make a noticible difference in results. All software will have some kind of default setting for ties -- intentionally or otherwise -- just so it doesn't lock up. The "Breswell" method, which is the default in some packages such as the SciKitLearn library in Python and also in SAS, isn't the best for estimating when there are lots of ties. As Paul Alison says in "Survival Analysis Using SAS - A Practical Guide": "Breslow's approximation ... works well when ties are relatively few. But when data are heavily tied, the approximation can be quite poor." I like the "Efron" method (and that may be the default in some R libraries) but Klien and Moeschberger have a small data example in "Survival Analysis ... Techniques.." where Efron results were closer to Breslow than Discrete which isn't great. The recent case where I saw improvements from Efron had about 450 records with about 35% of the timepoints being "tied" with 2 or more observations. If it doesn't slow your computer down too much the "discrete" method can be a good fallback when there are lots of ties assuming your software is coded accurately. There are others smarter than I in this field. My knowledge is based on a graduate statistics course I took in survival analysis and some follow-up analysis using those tools. Good luck!
