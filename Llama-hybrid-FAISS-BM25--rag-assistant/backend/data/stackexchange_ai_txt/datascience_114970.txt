[site]: datascience
[post_id]: 114970
[parent_id]: 114945
[tags]: 
First, SHAP values are not directed translated as probabilities, they are marginal contributions for model's output. As explained in this post , we can't interpret SHAP values from raw predictions. Also, if you check shap.TreeExplainer model_output : "raw", "probability", "log_loss", or model method name What output of the model should be explained. If "raw" then we explain the raw output of the trees, which varies by model. LGBMClassifier applies a transformation in model's output , so raw predictions are not translated as probabilities as well. Therefore, it is important to consider model's output in order to interpret SHAP values correctly. Finally, when you calculate feature importance, you calculate the average contribution for all instances in dataset, so values are not summing to 1 necessarily, because you have negative and positive contributions, and your average output is not 1 (predicting a single class).
