[site]: crossvalidated
[post_id]: 521729
[parent_id]: 521609
[tags]: 
There are quite a few algorithms. The simplest is (I believe I have the name right) Wild Binary Regression segmentation. Here our goal is to identify a place to split our time series into 2 and fit a regression for each with connectivity constraints. Then we get the residuals and fit on those and combine the two models (this is gradient boosting) then get residuals and fit again and so on and so forth until some criteria, either a number of changepoints or some criteria which typically takes into account the number of points like maybe the AICC with k replaced with the number of changepoints found. I believe that is the algorithm although I approach it from a pure gradient boosting perspective in this bit of messy code: https://github.com/tblume1992/LazyProphet I do have a much cleaner implementation on pip but there is no documentation for it yet.
