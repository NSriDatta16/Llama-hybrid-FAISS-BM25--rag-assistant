[site]: crossvalidated
[post_id]: 108946
[parent_id]: 108928
[tags]: 
I think it is the reverse. SVM specially with a Gaussian kernel have a much higher probability of overfitting the data than LDA which is LINEAR on the data space!. SVM with a Gaussian Kernel will map the data into a infinite dimension space, and in that space the data is linearly separable. If one uses a very high C, the cost of errors (placing data in the wrong side of the margin) will be unbearable, and the SVM will find the hyperplane that separates the classes (since there is one - they are linearly separable!). That plane (linear on the infinite dimension space) will be a very convoluted manifold in the data space, that separates the classes exactly - the very definition of overfitting!! For SVM with a linear kernel, I dont know, but if the data is generated by two Gaussians of the same shape (same covariance matrix) the LDA is the optimal solution, which would mean the least overfitting. A Linear SVM can at most match the LDA! For other cases, I dont know.
