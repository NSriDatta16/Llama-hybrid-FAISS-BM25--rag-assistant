[site]: crossvalidated
[post_id]: 413868
[parent_id]: 
[tags]: 
trend stationary with external regressors

Suppose I have two trend - stationary time series with strong correlation. In the case where there are no regressors, if a time series is trend-stationary, it becomes stationary by subtracting a deterministic trend. Meaning, I fit a line to the data $Y=f(t)$ and subtract that line from the data. Then I fit the residuals with Arima. In the case where there is an independent regressor... do I subtract the best fit line from both $Y = f_1(t),\, X = f_2(t)$ , or do I subtract lines : $Y = f_1(X, t), \, X = f_2(Y, t)$ , where $X$ is the regressor, $Y$ is the dependent time series variable, and $t$ is time. I think the answer is to subtract a best fit line which only considers time, because stationary means the covariance and expectations are constant functions of time... but I am looking for a second opinion from you geniuses. I worry because I don't want to lose the correlation between regressor and dependent variable by subtracting best fit trend lines from the data.
