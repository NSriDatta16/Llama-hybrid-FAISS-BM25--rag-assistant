[site]: datascience
[post_id]: 94844
[parent_id]: 
[tags]: 
XGBoost - Imputing Vs keeping NaN

What is the benefit of imputing numerical or categorical features when using DT methods such as XGBoost that can handle missing values? This question is mainly for when the values are missing not at random. An example of missing not at random categorical feature Class 1: User has a red car Class 2: User has a blue car Class 3: User has no car (missing value) In this case is it better to treat this feature as a binary 0/1 with NaN missing value, or as multi-label feature: 0,1 and -999 for missing? The same question applies for a numerical feature that indicates age of user's car. Here, missing values indicate the user has no car. Is it better to keep missing values as NaN, or to impute these values? If imputing is better, should I impute using the median value and add an interaction feature for when the values are missing?
