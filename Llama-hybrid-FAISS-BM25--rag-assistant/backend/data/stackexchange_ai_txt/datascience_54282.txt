[site]: datascience
[post_id]: 54282
[parent_id]: 54281
[tags]: 
The reason is that your data is in a way that the algorithm does not make any mistake on it in the current feature space. it is an easy problem that the algorithm does not need to ignore wrong points due to the easy data which is provided. If it occurs that your data is hard to be classified, the margin then will try to ignore some data points which may lead to narrow margins. It is worth mentioning that even though it performs the same, it won't be as a simple perceptron, at least in most cases. Consider that SVM somehow considers the geometrical position of data points while a simple perceptron always tries to reduce the cost function. You can take a look at the pictures which are provided here .
