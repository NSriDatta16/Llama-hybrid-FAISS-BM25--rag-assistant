[site]: crossvalidated
[post_id]: 600807
[parent_id]: 
[tags]: 
Do BERT word embeddings change after training, depending on context?

Before answering "yes, of course", let me clarify what I mean: After BERT has been trained, and I want to use the pretrained embeddings for some other NLP task, can I once-off extract all the word-level embeddings from BERT for all the words in my dictionary, and then have a set of static key-value word-embedding pairs, from where I retrieve the embedding for let's say "bank", or will the embeddings for "bank" change depending on whether the sentence is "Trees grow on the river bank", or "I deposited money at the bank" ? And if the latter is the case, how do I practically use the BERT embeddings for another NLP task, do I need to run every input sentence through BERT before passing it into my own model? Essentially - do embeddings stay the same for each word / token after the model has been trained, or are they dynamically adjusted by the model weights, based on the context? Also, I actually don't need to know specifically with regards to BERT, that's just an example, but rather in the general case of transformer-based embeddings, i.e. ESM, GPT3, etc etc.
