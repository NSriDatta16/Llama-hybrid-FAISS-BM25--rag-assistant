[site]: crossvalidated
[post_id]: 266562
[parent_id]: 266560
[tags]: 
You can simply carry forward the sum of the connection weights and the partial derivatives of the error w.r.t the target as the result of an activation function, and it will be unbounded. This is actually using what is called a "linear" activation function, vs. use of a logistic, tanh, exp, RBF activation function on the input-side and the output-sides. Recall, if you expand the range of the activation output in any way, e.g. via a multiplicative or power process, the range and standard deviation of the activation function outputs will increase, and lengthen the learning process. There are some basic rules for ANNs. First, correlation between input features will cause the ANN to waste time learning the correlation, and this is why PCA and decorrelation is so important before clamping inputs to the input nodes. ANNs also like the input features to preferably be in the range [-1,1] and not even standard normally distributed using Z-scores, or even normalization or percentiles. (Percentiles will work though). You can do anything you want at the activation functions, however for a classification problem, you almost have to use the softmax function on the output side. If you are using an ANN for function approximation (continuously-scaled output value(s)), then I like to start with a linear function and then ramp up to logistic, tanh if linear doesn't reduce error at a greater rate.
