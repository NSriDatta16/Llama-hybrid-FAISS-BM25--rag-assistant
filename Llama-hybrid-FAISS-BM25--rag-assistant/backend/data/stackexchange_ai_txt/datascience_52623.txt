[site]: datascience
[post_id]: 52623
[parent_id]: 52616
[tags]: 
One approach could be to simply use standard time series models, like ETS/TBATS/ARIMA for each department. These methods will, with proper care, take into consideration specific trends you might see (as well as seasonality) albeit predicting when a trend will fall/how long it will last is an incredibly difficult task. This wouldn't be an invalid approach, but the problem here is that each time series will be fit in isolation and you seemingly have large amounts of similar time series along with valuable exogenous data, much of it that does not depend on time. The approach above would mean fitting a potentially large amount of time series models which is largely impractical and tedious (in my opinion) and in the case of ARIMA you would not be able to include variables that don't vary with time. There are multivariate extensions of ARIMA methods that could help, but if you have many different time series convergence is often a problem. This is where I would suggest you look into regression algorithms (statistical or ML) with time (in some fashion) as a covariate. Your dataset would basically be expanded so that one row is the observed response variable at any specific month + year, for any given department. Your time covariates could be two columns; one with the year, another with the month, or simply a continuous variable like 2019 + 1/12, 2019 + 2/12, etc. You would then use "department" as a categorical variable, along with perhaps lagged values of the response (to perhaps incorporate current trend information) or perhaps variables like "average health service production in last k month(s) by department" or "median health service production in past year over all departments = over all the data", etc. The choice of how many months to take, along with the level at which you aggregate to (over a department? over all departments? over specific departments only?) will be determined based on what you think is reasonable. If you have time related events, you could model this with indicator variables, and also perhaps use variables like "months since this time related event has started/ended", etc. to perhaps capture lagged effects. Seasonality, if it exists, could be easily captured with indicator variables/Fourier series transforms. To obtain more trend features, you might also run another regression algorithm using the past couple months of data (from the time period for that specific observation/row of data) to get the overall current trend (at that point in time). Non-time related continuous variables (like "how much our patients have hip surgery in our hospitals versus other hospitals") is a simple join to the expanded dataset by department. In many of these feature engineering ideas, you will end up with NA values at the start of the series which implies giving up maybe a few months of data at the beginning (I think this shouldn't be too big of a deal if you have time series' that are 120 units long). Regardless, I'd recommend using time series cross validation, using the most recent data as testing indices, and walking forward until you run out of data. In your case, each test set would be comprised of 12 months worth of data, and you would add one month of data at a time to the training set (moving the test sets time period one month ahead). I hope this helps.
