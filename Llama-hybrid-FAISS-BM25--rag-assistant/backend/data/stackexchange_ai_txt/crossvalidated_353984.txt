[site]: crossvalidated
[post_id]: 353984
[parent_id]: 
[tags]: 
Is there any theoretical problem with averaging regression coefficients to build a model?

I want to build a regression model that is an average of multiple OLS models, each based on a subset of the full data. The idea behind this is based on this paper . I create k folds and build k OLS models, each on data without one of the folds. I then average the regression coefficients to get the final model. This strikes me as similar to something like random forest regression, in which multiple regression trees are built and averaged. However, performance of the averaged OLS model seems worse than simply building one OLS model on the entire data. My question is: is there a theoretical reason why averaging multiple OLS models is wrong or undesirable? Can we expect averaging multiple OLS models to reduce overfitting? Below is an R example. #Load and prepare data library(MASS) data(Boston) trn
