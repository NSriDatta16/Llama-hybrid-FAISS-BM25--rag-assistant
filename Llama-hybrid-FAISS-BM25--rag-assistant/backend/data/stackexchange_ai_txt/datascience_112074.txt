[site]: datascience
[post_id]: 112074
[parent_id]: 
[tags]: 
MAML applied to unknown tasks in Meta-RL paper

I have been reviewing a paper on Meta-RL applied to Non-Stationary (NS) environment ( Paper on arxiv ), which assume that in a certain context of interest NS may be modeled as a switching environment behaviour, i.e. a set of stationary MDPs whose dynamics are stationary, also called tasks . In particular, in their context, authors assume their objective is to infer a label $y_t$ , assumed to be the next state of an RL setting, based on the knowledge of an input $x_t$ , the current state. The next state depends on a probability distribution $p(Y_t|X_t, T_t)$ , where $T_t$ is the task. They assume that tasks distribution is not known in advance, and that task boundaries are not known (i.e., it may happen that $\exists t_i, t_j: t_i \neq t_j \wedge T_{t_i} \neq T_{t_j}$ ). In order to model the NS, authors proposes to learn a initialisation of prior parametrization of the probability $p$ , i.e. finding $\theta^*$ which can be used as parameters of $p_\theta(y_t|x_t, T_t)$ . In order to learn this prior, they proposes to use Model Agnostic Meta Learning ( Paper of MAML ) in the following way: In particular, I have not understood how they deal with the fact that tasks are non known in advance. In fact, MAML requires that tasks are accessible and known, in order to sample trajectories from it: but in the paper I mentioned this hypotesis is not applicable! Am I misinterpeting their statement? Maybe they are assuming, as an initial hypotesis, that each task $T_t$ lasts for 2 time-steps and thus $\mathcal{D}_{T_t}^{tr} = (x_t, y_t)$ and $\mathcal{D}_{T_t}^{val} = (x_{t+1}, y_{t+1})$ ?
