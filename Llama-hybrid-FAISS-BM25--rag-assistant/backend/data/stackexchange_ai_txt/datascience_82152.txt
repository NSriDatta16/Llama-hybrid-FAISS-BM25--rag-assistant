[site]: datascience
[post_id]: 82152
[parent_id]: 
[tags]: 
Why KL Divergence instead of Cross-entropy in VAE

I understand how KL divergence provides us with a measure of how one probability distribution is different from a second, reference probability distribution. But why are they particularly used (instead of cross-entropy) in VAE (which is generative)?
