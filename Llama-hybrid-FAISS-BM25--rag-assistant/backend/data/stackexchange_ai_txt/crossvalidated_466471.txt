[site]: crossvalidated
[post_id]: 466471
[parent_id]: 
[tags]: 
is Sigmoid activation function better than Leaky Relu?

I recently implemented a neural network from scratch in python with one hidden layer on the iris dataset for classification. Initially i applied Leaky relu for hidden layer acrivations and got an accuracy around 40% then i switched to sigmoid and found that accuracy improved drastically to 93% accuracy. How is that possible? I learned from Andrew ng lectures that Leaky relu is a best option.
