[site]: crossvalidated
[post_id]: 107761
[parent_id]: 
[tags]: 
Question about Neural Network Training

What is the benefit of optimizing our neural network error function using back prop rather than just using gradient or stochastic gradient descent directly on the error function? How come we don't just take the partial derivative of the error function with respect to every weight and do gradient or stochastic gradient descent based on that? Instead of doing back propagation to come up with these artificial error values for hidden nodes and then for each hidden node multiplying its error times the various partial derivatives of weights associated to its incoming edges, as if that hidden node was actually an output node.
