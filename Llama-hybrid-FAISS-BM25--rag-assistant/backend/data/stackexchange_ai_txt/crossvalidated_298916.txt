[site]: crossvalidated
[post_id]: 298916
[parent_id]: 297380
[tags]: 
There is a lot in this question. Lets go over what you've wrote one by one. The solutions that fit training data are infinite. We don't have precise mathematical equation that is satisfied by only a single one and that we can say generalizes best. The fact that there are infinite many solutions comes from learning problem being an ill-posed problem so there cannot be a single one that generalizes best. Also, by no free lunch theorem whichever method we use cannot guarantee that it is the best across all learning problems. Simply speaking we don't know which generalizes best. This statement is not really true. There are theorems on empirical risk minimization by Vapnik & Chervonenkis that connect the number of samples, VC dimension of the learning method and the generalization error. Note, that this only applies for a given dataset. So given a dataset and a learning procedure we know the bounds on generalization. Note that, for different datasets there are no and cannot be single best learning procedure due to no free lunch theorem. Optimizing weights is not a convex problem, so we never know we end up with a global or a local minimum. So why not just dump the neural networks and instead search for a better ML model? Here there are few things that you need to keep in mind. Optimizing non-convex problem is not as easy as convex one; that is true. However, the class of learning methods that are convex is limited (linear regression, SVMs) and in practice, they perform worse than the class of non-convex (boosting, CNNs) on a variety of problems. So the crucial part is that in practice neural nets work best. Although there are a number of very important elements that make neural nets work well: They can be applied on very large datasets due to stochastic gradient descent. Unlike SVMs, inference with deep nets does not depend on the dataset. This makes neural nets efficient at test time. With neural nets it is possible to directly control their learning capacity (think of number of parameters) simply by adding more layers or making them bigger. This is crucial since for different datasets you might want bigger or smaller models. Something that we understand, and something that is consistent with a set of mathematical equations? Linear and SVM do not have this mathematical drawbacks and are fully consistent with a a set of mathematical equations. Why not just think on same lines (need not be linear though) and come up with a new ML model better than Linear and SVM and neural networks and deep learning? Dumping things that work because of not understanding them is not a great research direction. Making an effort in understanding them is, on the other hand, great research direction. Also, I disagree that neural networks are inconsistent with mathematical equations. They are quite consistent. We know how to optimize them and perform inference.
