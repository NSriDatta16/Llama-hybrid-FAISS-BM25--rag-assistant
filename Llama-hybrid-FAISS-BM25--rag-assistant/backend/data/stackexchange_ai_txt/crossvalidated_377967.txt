[site]: crossvalidated
[post_id]: 377967
[parent_id]: 377966
[tags]: 
Cross-entropy with one-hot encoding implies that the target vector is all $0$ , except for one $1$ . So all of the zero entries are ignored and only the entry with $1$ is used for updates. You can see this directly from the loss, since $0 \times \log(\text{something positive})=0$ , implying that only the predicted probability associated with the label influences the value of the loss. This works because the neural network prediction is a probability vector over mutually-exclusive outcomes, so by definition, the prediction vector must (1) have non-negative elements and (2) the elements must sum to 1. This means that making one part of the vector larger must shrink the sum of the remaining components by the same amount. Usually for the case of one-hot labels, one uses the softmax activation function. Mathematically, softmax has asymptotes at 0 and 1, so singularities do not occur. As a matter of floating point arithmetic, overflow can occasionally result in $\log(1)$ or $\log(0)$ . Usually these are avoided by rearranging the equations and working on a different scale, such as the logits (e.g. https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits ) A related question and more detailed calculus can be found in Backpropagation with Softmax / Cross Entropy
