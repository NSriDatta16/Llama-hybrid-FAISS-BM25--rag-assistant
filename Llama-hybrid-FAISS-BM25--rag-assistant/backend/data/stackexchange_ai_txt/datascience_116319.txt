[site]: datascience
[post_id]: 116319
[parent_id]: 116317
[tags]: 
Running multiple times should indeed have no effect (if you used a random seed). Although sklearn documentation for logistic regression mentions that it is possible to have some variation due to randomization not controlled by a seed. It has more to do with how many test samples you have than the "simplicity" (the amount of features). If you have 20 test samples then the possibility of the two algorithms predicting the same things is not negligible. If you have a million test samples, and both algorithms predict the exact same results, it is much more probable that you made a mistake along the process than for the algorithms to make the same prediction for each of the test samples. If your classes are heavily imbalanced (like, 95:05), it is possible that both algorithms made the naive assumption that everything belongs to the big class (this is the extreme case, but heavy bias towards the majority class could explain this in a degree). If this is the case, try and optimize your problem using a metric better suited for this, such as F1 score. Note that I assumed that "same results" means "both algorithms made the same prediction for each test sample". The same accuracy score follows from this.
