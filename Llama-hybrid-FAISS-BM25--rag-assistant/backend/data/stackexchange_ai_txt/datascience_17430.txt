[site]: datascience
[post_id]: 17430
[parent_id]: 
[tags]: 
Document similarity: Vector embedding versus BoW performance?

I have a collection of documents, where each document is rapidly growing with time. The task is to find similar documents at any fixed time. I have two potential approaches: A vector embedding (word2vec, GloVe or fasttext), averaging over word vectors in a document, and using cosine similarity. tf-idf or its variations such as BM25. Will one of these yield a significantly better result? Has someone done a quantitative comparison of tf-idf versus averaging word2vec for document similarity? Is there another approach, that allows to dynamically refine the document's vectors as more text is added?
