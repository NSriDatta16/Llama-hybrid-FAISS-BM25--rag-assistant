[site]: crossvalidated
[post_id]: 420505
[parent_id]: 420502
[tags]: 
In a couple of words (the shorter the better!) when you add a variable to a model, if the added variable adds some explanatory power, then the addition increases the model fit (i.e. the capacity of the model as a whole of predicting the dependent variable in the sample where the model is estimated). However, bear in mind that adding more variables also entails a higher risk of overfitting (i.e. building a model with a high fit within the sample over in which it is estimated and a degraded prediction performance when used on other samples). So over time some specification criteria have been introduced such that they balance the number of parameters to be estimated against the model fit, so that the addition of variables (and therefore parameters to be estimated) may be discouraged when the resulting increase in the mode fit is not high enough compared to the parameter penalization. With regard to your question "More generally, what is it meant when the model "explains more variation" in the dependent variable -- certainly this is not equivalent to "this explains the variable" more?" in basic models like regression, the more variance of the dependent variable is explained by the model, the less is explained by residuals, the better the model is because (to use your words) “it explains the dependent variable more”
