[site]: crossvalidated
[post_id]: 237774
[parent_id]: 237763
[tags]: 
I mean more of a variance reduction, i.e. predictable result. I increased the number of trees, but it didn't help. Due to the central limit theorem , and due to the fact Random Forests predictions are obtained through averaging, increasing the number of trees should help. The default in R is 500L , set this as high as you can support (I've often put it to 5000L , depending on the data). The randomness in Random Forests come from both attribute bagging and bootstrap aggregating. You may also try to reduce the randomness either of those add. Last, depending on how many features and how many samples you have, it might simply due to the data, and no amount of hyperparameter tinkering will solve it. You mentioned in a comment: I've tried quite a few, between 6 and 400. I use a regression-based feature selector that keeps all features with p-value below 0.05 And, as I said, I find feature selection mostly useless with Random Forests. The reasons are simple: you risk overfitting by doing that selection and Random Forests are good with large number of features. Let the forest decide which features are worthy, unless you have orders of magnitude more features than samples, then do some small reduction, just enough to remove noise features.
