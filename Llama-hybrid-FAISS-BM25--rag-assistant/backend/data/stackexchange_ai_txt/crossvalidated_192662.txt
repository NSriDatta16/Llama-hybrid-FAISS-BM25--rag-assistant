[site]: crossvalidated
[post_id]: 192662
[parent_id]: 
[tags]: 
When implementing dropout in neural networks with SGD, how does one calculate the gradient?

Specifically, I know that in SGD one sums all the gradients for weights/biases for each minibatch and divides by the mini batch size, would one do the same thing for dropout networks? Or would they divide by the number of networks in the mini-batch for which the weight/bias is not dropped out?
