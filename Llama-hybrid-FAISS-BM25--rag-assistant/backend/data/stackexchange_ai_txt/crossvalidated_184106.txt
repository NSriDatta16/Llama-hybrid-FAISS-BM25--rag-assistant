[site]: crossvalidated
[post_id]: 184106
[parent_id]: 182734
[tags]: 
NN: one hidden layer is enough but can have multiple layers nevertheless, left to right ordering (model: feed forward NN) trained only in supervised way (backpropagation) when multiple layers are used, train all the layers at the same time (same algorithm: backpropagation), more layers makes it difficult to use as errors become too small hard to understand what is learned at each layer DNN: multiple layers are required, undirected edges (model: restricted boltzman machine) first trained in an unsupervised way, where the networks learns relevant features by learning to reproduce its input, then trained in a supervised way that fines tune the features in order to classify train the layers one by one from input to output layer (algorithm: contrastive divergence) each layer clearly contains features of increasing abstraction The move to DNN is due to three independant breakthroughs which happened in 2006. Regarding theorems on NN, the one the question alludes to is: universal approximation theorem or Cybenko theorem: a feed-forward neural network with a single hidden layer can approximate any continuous function. However in practice it may require much more neurons if a single hidden layer is used.
