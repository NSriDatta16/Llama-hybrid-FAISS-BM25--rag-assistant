[site]: datascience
[post_id]: 122705
[parent_id]: 122701
[tags]: 
The way you suggested indeed results in some loss of data, but this is not necessarily a problem. You don't need the data itself, but rather their patterns, and at some cases the patterns are easier to emerge if you modify your data. This is called feature engineering, the process of creating descriptive numerical features out of the existing data, and it is used both in situations like yours where the data do not have the required structure (another example is trying to fit an entire time series into a single row) and simply to create more features in order to improve accuracy. If you calculate enough statistics it will probably still be able to learn enough. Don't just stay at mean and std. For example, the maximum age of the children as a feature would provide a pretty good way for the model to learn a lower boundary, significantly shortening the search space. Also the number of children itself (more children points to older parent). Apart from simple statistics, you can also try with ratios (for example, the ratio of children with a small difference in age to the ratio of "outlier" children). Another way would be even to train a separate model (something like an autoencoder) that takes the ages of children and returns a singular value that you can use as input to the final model, or even use it along with the rest of the statistics. If you dumb this idea down, even simple one-hot encoding (where the columns would be age_1st_child , age_2nd_child would work with algorithms that can handle NaN values. Your imagination is the limit, and I am sure you will be able to find a lot of stuff about feature engineering online.
