[site]: crossvalidated
[post_id]: 73706
[parent_id]: 
[tags]: 
What is the best way to analyze data if your experimental design changes while running the experiment?

I imagine this is a somewhat common situation in practice. I am thinking mainly in terms of preclinical drug trials. 1) During the course of the study a new technique is learned or some time/money is freed up to be able to use a new technique as a secondary outcome. 2) Some key piece of equipment breaks, the lab member with the skill to perform a technique leaves, or a planned technique is looking like it is yielding inconsistent results for unknown reasons so that outcome is dropped for financial reasons. 3) When time comes to analyze data it comes to the knowledge of the researcher that the planned methods are inadequate/inappropriate and they wish to use a different approach. 4) Experiment 1 (e.g. test drug A) does not look like it will pan out, so a financial decision is made to stop that experiment and instead test drug B. Alternatively some other part of the experiment may be modified such as cell culture or animal strain. 5) Study is stopped prematurely or additional funds are freed up to increase sample size. I can think of more, but you get the idea. Are p values of any use under these circumstances? Should corrections for multiple comparisons be made in the case of #4? At what point does deviation from the plan invalidate the hypothesis testing procedure? The best way to me would be to simply plot the data and describe it in the hopes it may be useful for someone. However this behaviour is discouraged in favor of performing significance/hypothesis tests and the data may not be published if these are not included. EDIT: I was thinking about it and the actual best thing to do is to take the data and come up with a model to explain it that makes a precise prediction that can then be tested. It seems like this should always be the best thing to do, though.
