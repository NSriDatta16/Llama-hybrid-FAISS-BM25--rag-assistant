[site]: crossvalidated
[post_id]: 455957
[parent_id]: 
[tags]: 
Bayesian updating with normal but incomplete signals

Suppose I want to update my beliefs on a realization of vector $\theta = (\theta_{1},...,\theta_{d}) \in \mathbb{R}^{d}$ and for every one of these $\theta_{j}$ , in each period I may receive signal $y_{jt} = \theta_{j} + \epsilon_{jt}$ . $\theta$ is normally distributed with mean $\mu_{\theta} \in \mathbb{R}^{d}$ and covariance matrix $\Sigma_{\theta} \in \mathbb{R}^{d\times d}$ , the later allowing for positive correlation between the $\theta_{j}$ 's. Further, $\epsilon_{jt}$ is an iid normal shock with mean 0 and variance $\sigma_{\epsilon}^{2}$ . Because of these assumptions, $y_{t} \sim \mathbb{N}(\mu_{\theta},\Sigma_{\theta} + \sigma_{\epsilon}^{2} \mathbb{I}_{d})$ . If we observed all signals $y_{t} = (y_{1t},...,y_{dt}) \in \mathbb{R}^{d}$ , then characterizing the mean and covariance of the posterior normal distribution is straightforward, assuming that prior $\pi(\theta)$ is exactly the distribution of $\theta$ . My question is the following: assume that for some reason we do not observe all signals, only a subset of $y_{t}$ , is there a way to have an analytic expression for the posterior mean and covariance matrix as a function of the signals we do observe? For example, assume that $d=3$ and you only observe signals $y_{1t}$ and $y_{3t}$ . To derive $\pi(\theta|y_{1t},y_{3t})$ , you would need to compute the likelihood $L(y_{1t},y_{3t} | \theta)$ , which is going to be the marginal joint distribution of $y_{1t}$ and $y_{3t}$ . However, I am having trouble deriving this likelihood for an arbitrary combination of signals we do observe. If you assume that indicator vector $d_{t} \in \mathbb{R}^{d}$ is such that if $d_{jt} = 1$ then we observe signal $y_{jt}$ , is there a way to characterize the posterior mean and variance as a function of this vector $d_{t}$ ? Even if you do not know the answer, I would appreciate if you could point me to literature that might be useful. Thanks!
