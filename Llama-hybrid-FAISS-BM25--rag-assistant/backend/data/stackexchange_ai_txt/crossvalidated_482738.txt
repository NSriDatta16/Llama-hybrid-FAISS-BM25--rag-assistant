[site]: crossvalidated
[post_id]: 482738
[parent_id]: 
[tags]: 
Seasonal term not significant after differencing

I am working with a time series (weekly frequency), $\{y_t\}$ . I have 250 points in total. The time series is not stationary according to KPSS test (it is stationary according to ADF) at 5%. In any case, an expert in the field told me that the data should have annual seasonality ( $\sim 50$ lags), and indeed this is what the periodogram suggests. So to confirm this intuition I fitted a model of deterministic seasonality, of the form $ y_t = A \cos(\omega t) + B \sin(\omega t) + \epsilon_ t $ using statsmodels' SARIMAX ( https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html ) with the proper angular frequency and I got that $A \ne 0$ significantly (p-value $= 0.01$ ). So far so good, everything is consistent. Now, in the future I will include other (non-stationary) variables, so I wanted to check what happens if I take 1st differences. By doing SARIMAX(0,1,0) with the cosine as exogenous variable I therefore expected to get the same value for $A$ as before (I find the SARIMAX documentation above a bit confusing in the sense that I believe that SARIMAX with $d=1$ does not fit an ARIMA model on the residue, but an ARMA model on the differenced residue- in particular, the exogenous variables do get differenced as well.) However, even though $A$ is does not change much after differencing (24 vs 22, the previous value), the p-value is now 0.39, so suddenly the seasonality term is not significant. I cannot understand this result. How can a 1st difference remove the seasonal pattern? Or is it that the significance test for $A$ is not reliable in the differenced case? How could I proceed, any ideas? Thank you very much.
