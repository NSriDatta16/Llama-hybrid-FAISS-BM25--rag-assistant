[site]: crossvalidated
[post_id]: 404496
[parent_id]: 404476
[tags]: 
It's clear that your models are suffering from the imbalance in your data, which is a thing you'll need to fix. Now, on to your questions: Any other feature engineering techniques i can do to improve predicting class 0? [have tried different things on text like TFIDF, Hashing Trick, selectKBest, SVD(), and maxAbsScaler() on all features] These are all valid preprocessing steps, but no feature engineering step can help you fix your real problem (i.e. class imbalance). They help in dealing with other issues such as high-dimensionslity, overfitting, etc. Any other algorithms i should try? [have only tried random forest classifier] Tree-based algorithms are usually the most suited in dealing with imbalanced data. You could try some of the popular tree-boosting algorithms that are very popular these days (e.g. XGBoost, LightGBM, Catboost) Is low recall a big deal? Depending on what you're aiming for... What strikes me as important isn't the value of recall that is low but its difference to that of class 0. A dropoff from 98% to 66% is a massive difference and should be dealt with. Mostly have been just "plugging and playing" ... anything obvious i am missing? would applying over-sampling help? If so, how can that be done in python / sklearn? Yes over-sampling is the first thing you should do! This can be done in Python through imbalanced-learn , which offers a large variety of under and over-samplers. This will play well, as long as you swap sklearn.pipeline.Pipeline with imblearn.pipeline.Pipeline . Just note that this step will have to be done after converting your text to vectors (i.e. after CountVectorizer or TFIDF).
