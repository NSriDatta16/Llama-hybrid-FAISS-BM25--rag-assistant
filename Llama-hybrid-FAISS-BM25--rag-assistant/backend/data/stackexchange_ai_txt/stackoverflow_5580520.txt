[site]: stackoverflow
[post_id]: 5580520
[parent_id]: 5579837
[tags]: 
The answer to your query is complicated. It will be even more complicated if your tables contain fields that are being written to, if your data is cold or too large to be kept in memory. It is also dependent on the storage engine, storage engine version and a few other things. Solid results you can get only by benchmarking. I may contribute a useful anecdote, though. In another job, we had a user database that held all data for a single user in one row. User data was about 1-2K per user, in total, and we had 25 million user records. The database has been reading and writing data in pages of 8K each (MySQL InnoDB would be 16K pages, btw). That means we have about 4-6 user records per database page, and about 5 million pages of data. The user record contained fields that stored the time of the users last login. Between 7 and 9 in the morning we would see about 8 million unique users logging in, so we would be about 8 million pages being dirtied and in need of a writeback. Essentially we would be writing back the entire user table back to disk twice or thrice, each day. We introduced an artificial 1:1 relationship which resembles your user and user_details pages: We had a user and a user_lastlogin table. The user_lastlogin records were very narrow, consisting essentially only of a user id and the (three different) last login times of a user (depending on service being used). Because user_lastlogin is very narrow we now have almost thousand records per page, and only 25.000 pages in the table. With 8 million unique logins, we now need to write back a lot less data (because checkpoints and page writebacks in a database are delayed). Disk I/O load sank by orders of magnitude. Lesson learned: It can be very useful to separate static from volatile data (We considered the password static data, as we had about 15.000 password changes per day, whereas we had 8 million last_login updates per day).
