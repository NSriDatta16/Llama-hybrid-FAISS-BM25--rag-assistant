[site]: crossvalidated
[post_id]: 564983
[parent_id]: 563865
[tags]: 
Upfront, a few words about how to find the approach that might be most beneficial to you: First point: : In general , if you fit a predictor (aka "model") for your labels, you can presume that, in most of the cases, the independent variables (aka properties, features, ...) that are used by your predictor are also those that your labels depend on, otherwise they wouldn't be contained in the predictor. Indeed, many fitting algorithms, like e.g. lm() in R, even give you, for each fitted parameter, a p-value for the significance of this parameter. And parameters are often linked to independent variables, like e.g. in linear models: $$ y = p_1 x_1 + p_2 x_2 + \ldots + p_m x_m + b, $$ where $y$ is the value you want to predict, the $x_i$ are your independent variables, and $p_i$ is the parameter "linked" to $x_i$ . Then the significance of the fitted parameter can be understood as the significance of the belonging independent variable. Having said that, there are lots of more complex models out there, like e.g. deep neural networks, GBMs, or random forest, where it is pretty much impossible to "see" the relevance of an independent variable just by looking at the model. Second point: The problem of finding the most important variables, i.e. those that are most relevant for predicting, has been addressed by many papers and there are lots of approaches to tackle this. Ideally, you would try each of them and then pick the best for your situation, but that is not doable, since there are too many of them. For a shortcut, I would suggest, rather than using independence tests like Chi2 and Kendal's Tau, to have a look at random forest or mutual information based methods, which also have ready to use implementations, e.g. here and here . Third point: Kendall's tau test is only working if both of the variables you want to compare are ordinal , i.e. can be ordered. So, unless you have an ordering for your three labels A, B, and C, you cannot use Kendall's tau test. Note that, if you choose an ordering for your labels, it does have to make sense: E.g., if you choose A Now, let's get to answering your two questions: Question One: Yes, selection bias can alter your results of independence tests or other methods of detecting feature importance. Question Two: There are methods for detecting whether and how much selection bias is influencing your results, at least to a certain extent, and how you could recover from that. If you want to pursue this road, you might want to start with this paper: Bareinboim, Elias, Jin Tian, and Judea Pearl. "Recovering from selection bias in causal and statistical inference." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 28. No. 1. 2014. I could not find any pertinent implementations. The paper you mention, "Huang, Jiayuan, et al. "Correcting sample selection bias by unlabeled data.", is concerned with one particular type of selection bias, which is arguably the most common one, namely "covariate shift". Basically, covariate shift means, that the selection process creates a bias on the choice of your samples , but that it does not change the probability of the label given the sample . I don't know the details of your situation, but from what you describe, I would think that covariate shift is the selection bias you experience. Unfortunately, I couldn't find an implementation for this method either. So what should you do? Given the bleak outlook concerning usable implementations for the methods described above, I would first try to find a way to remove the selection bias from the experiments. If this is not possible, see whether you can figure out yourself how the selection mechanism is biasing your data. Often, expert knowledge and common sense can get you quite far. Third, use the results as they are and report them always with the additional information about the selection bias. This still contains information which might be sufficient for some purposes.
