[site]: crossvalidated
[post_id]: 318346
[parent_id]: 254548
[tags]: 
I suggest you to read this paper: Large Scale Distributed Deep Networks As far as I know, this approach is common in industry. As you know, SGD is an iterative and serial (not parallel) method with updates to the model vector $x$ of the form $$x_{k+1} = x_{k} - \mathop{\nabla}\limits{f(x_k;z_i)} $$ For SGD every iteration depends on the previous iteration. Most schemes learn local models independently and communicate to update the global model. The algorithm differ in how the update is performed. There are several algorithm, that solve the problem of applying SGD on large data sets. HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent CYCLADES: Conflict-free Asynchronous Machine Learning Parallel Stochastic Gradient Descent with Sound Combiners
