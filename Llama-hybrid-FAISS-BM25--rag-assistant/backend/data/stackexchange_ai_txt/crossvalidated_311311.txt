[site]: crossvalidated
[post_id]: 311311
[parent_id]: 311242
[tags]: 
Essentially any model where the kernel of the posterior distribution in log units could be expressed in the Stan language (by incrementing the target keyword), and you can add any prior assumptions you want on the parameters. Getting the Stan MCMC algorithm to sample sufficiently efficiently is another matter, but you have a good chance of that in this situation. Your Stan program might look something like data { int C; // number of colors int S; // number of shapes int T; // number of time periods vector[T] r_m; // market returns vector[C * S * T] r; // stacked returns } parameters { vector[C] beta_c; vector[S] beta_s; real sigma[C,S]; } model { vector[C * S * T] mu; // conditional mean vector[C * S * T] sigma_flat; // error sd int pos = 1; for (c in 1:C) for (s in 1:S) { mu[pos:(pos + T - 1)] = beta_c[c] * beta_s[s] * r_m; sigma_flat[pos:(pos + T - 1)] = rep_vector(sigma[c,s], T); pos = pos + T; } target += normal_lpdf(r | mu, sigma_flat); // log-likelihood // illustrative priors target += normal_lpdf(beta_c | 0, 1); target += normal_lpdf(beta_s | 0, 1); target += exponential_lpdf(to_vector(sigma) | 1); } But you will have to modify it if your panels are unbalanced, if you have better priors, or if you want to relax the assumption that beta_c and beta_s are independent a priori .
