[site]: datascience
[post_id]: 123856
[parent_id]: 122232
[tags]: 
Bag of words(BOW) is a term generally used for the assumption where one doesn"t care about the order of the words. Words are usually treated as a set instead of a sequence . So BOW is not really a statastical embedding technique. There are lot of stastical embedding techniques like multi hot vectorization, TF-IDF vectorization which treat documents as a set of terms hence make BOW assumption. Order of term doesn"t matter here just their presence or absence. CBOW/skipgram are techniques used in word2vec to generate pseudo-corpus(input/output examples) out of real corpus(text). Goal of word2vec is to generate good embeddings for the words. For that we need to leverage the neighbourhood of the words. Words with similar neighbourhood ends up having similar embeddings(word vectors). To do this we frame the problem as a neighbourhood/context prediction problem. While training itself for that purpose the model ends up generating good embeddings as a side-effect. In skipgram problem is to find context word(falling in n radius) given a center word. In cbow we frame the problem as: Predict center word given preceding and succeeding n words where n is window radius. So for a sentence: "John loves icecream made in Italy." skipgram will create input/output training examples as follows: icecream -> John icecream -> loves icecream -> made icecream -> in cbow will create training input/outputs as follows: {John, loves, made, in} -> icecream As seen above for same example skipgram generates more training examples. That"s the reason it is preferred over cbow in case of small data corpus. However when data is huge cbow is prefered as it generate semantically crisp embedding of words with large overlapping neighbourhoods.
