[site]: crossvalidated
[post_id]: 134772
[parent_id]: 
[tags]: 
Estimating conditional variance y|x

I am building a predictor for $y = f(x)$ using training samples ${(x_i, y_i)}$ (assume) drawn i.i.d from some distribution $p(x,y)$, by optimising the empirical L2-loss: $f(x) = argmin_f \; \sum_i ||f(x_i)-y_i||_2^2$. (Assume $f$ is suitably parameterised, say linear regression or neural networks, etc.) It's known that the minimiser $f = E[y|x]$. Now, I'd like to get a confidence estimate of my prediction $f(x)$, using say the variance of the prediction. So, I thought of this: Generate a new dataset ${(x_i,y_i^2)}$. Find $g(x) = argmin_g \; \sum_i ||g(x_i) - y_i^2||_2^2$. We know that the minimiser $g = E[y^2|x]$. Compute the conditional variance estimate as ${\rm var}(y|x) = g(x)-f(x)^2$ and use this as an estimate of the uncertainty in the prediction $f(x)$. Is the above a theoretically sound way of estimating the confidence in the prediction?
