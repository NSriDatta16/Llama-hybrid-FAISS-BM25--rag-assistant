[site]: crossvalidated
[post_id]: 299770
[parent_id]: 
[tags]: 
Universal approximation theorem - representing versus learning functions

In Goodfellow et al.'s Deep Learning , the authors write, "the universal approximation theorem means that regardless of what we are trying to learn, we know that a large MLP will be able to represent this function. We are not guaranteed, however, that the training algorithm will be able to learn that function" (193). What is the difference between "representing" and "learning" a function? For example, if the function is a polynomial of order $n$, does the theorem state that a neural network can learn polynomials, but the parameters of the specific polynomial of interest may not be learnable based on the number of neurons / number of data points?
