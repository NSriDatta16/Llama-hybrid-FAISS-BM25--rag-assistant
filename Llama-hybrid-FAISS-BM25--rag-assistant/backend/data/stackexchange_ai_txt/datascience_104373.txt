[site]: datascience
[post_id]: 104373
[parent_id]: 104171
[tags]: 
Keras provides a good example how to load pretrained word embeddings and train a model on it. The tricky part is to load the pretrained embeddings, but this is well explained in the code and can be adopted easily. Also note that you need to load the embeddings in the embedding layer, which must be "frozen" (should not be trainable). This can be achieved by seting trainable=False : from tensorflow.keras.layers import Embedding embedding_layer = Embedding( num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False, ) There are a number of useful resources provided by Keras related to natural language processing (NLP), including examples on semantic similarity (BERT), NER transformers, and sequence-to-sequence learning.
