[site]: crossvalidated
[post_id]: 617476
[parent_id]: 575314
[tags]: 
When evaluating a machine learning (or other statistical model) against multiple evaluation metrics, is there a standardized way to choose the "best" model? NO It depends on what you value from your predictions. In your example, if you value a high $F_1$ score over a high dice score, you might be inclined to go with the model with the highest $F_1$ score. If you value a high dice score over a high $F_1$ score, you might be inclined to go with the model with the highest dice score. If you value both, you might be inclined to go with a model that never achieves the highest of either but always performs well, as opposed to the top-performing model in terms of $F_1$ that has a terrible dice score (or the top-performing model in terms of dice score that has a terrible $F_1$ score). If you want to make this quantitative, you can use an equation of multiple measures of model performance that accounts for how you value each measure of performance. Economics refers to such an equation as a utility function , and different users can have different utility functions. In many regards, the $F_1$ score can be seen as a utility function of the precision and recall, so you are already used to working with this notion of combining measures of performance in a way that gives one real number as an output. This is not even a matter of using proper scoring rules. Brier score and log loss can disagree about which model is best. If this happens, it becomes a matter of what you value for your problem. Our Stephan Kolassa has argued in favor of the log loss in answers/comments on here, while our Dikran Marsupial has written about why Brier score might be preferred.
