[site]: datascience
[post_id]: 13189
[parent_id]: 13188
[tags]: 
The definition of a Language Model (LM) is a probability distribution over sequences of words. The simple illustration of an LM is predicting the next word given the previous word(s). For example, if I have a language model and some initial word(s): I set my initial word to My My model predicts there is a high probability that name appears after My . By setting the initial words to My name , my model predicts there is a high probability that is appears after My name . So it's like: My -> My name -> My name is -> My name is Tom , and so on. You can think of the autocompletion on your smartphone keyboard. In fact, LM is the heart of autocompletions. So, LSTM-LM is simply using an LSTM (and softmax function) to predict the next word given your previous words. By the way, Language Model is not limited to LSTM, other RNNs (GRU), or other structured models. In fact, you can also use feedforward networks with context/sliding/rolling window to predict the next word given your initial words.
