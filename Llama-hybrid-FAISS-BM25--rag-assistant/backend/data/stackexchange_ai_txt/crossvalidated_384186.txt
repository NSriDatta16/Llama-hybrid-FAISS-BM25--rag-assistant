[site]: crossvalidated
[post_id]: 384186
[parent_id]: 
[tags]: 
Random Forests performs far less better than Naive Bayes

In one of my Machine Learning courses I have to find the best predictor for this dataset and its binary target "Caesarian". First of all, I tried to improve the datas : there are few features. I did One-Hot-Encoding on : Delivery Time Blood Pressure Heart Problem (already done in the database as binary) I normalized my datas for the Age and Delivery Number columns. Then I did k-fold with Random Forests, Naive Bayes, GLM and some other, I tried several k for the k-fold, $k\in \{1,2,3,6,8,16\}$ , and I was very surprise by the very poor performances on prediction for the Naive bayes (more than 30% error on average for the k-fold, no matter what k), and most of all, I was extremely surprise that RandomForest performs a 40% mistake on average, no matter what $k$ and with different values for the number of trees, from tens to thousands, and also different values for the number of variables ramdomly sampled as candidates at each split. I tried Leave-One-Out to compute the error, and it's the same, the Naive Bayes model's quite bad, and still surprisingly, the Random Forest's worst than Naive Bayes. Is there something I missed out ? I can understand that Naive Bayes can have a bad classification performance as it really depends on the datas and it's a really weak classifier, but what about RandomForest ? So two questions now : Random Forest aren't suppose to perform better ? What can I do to improve them or at least understand why it's so bad ? Here are the boxplots of the MSE I got for a 6-fold : Note : I didn't tried to optimize Bagging yet
