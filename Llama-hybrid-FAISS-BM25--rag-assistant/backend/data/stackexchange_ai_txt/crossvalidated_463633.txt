[site]: crossvalidated
[post_id]: 463633
[parent_id]: 393288
[tags]: 
Language modelling is the most widely used pre-training/self-supervised task. Word embeddings are naturally transferred to be used in some downstream task. For vision, rotations and denoising is popular. There is also in-painting, though it doesn't work as well at the moment. For reinforcement learning, there is structure-from-motion , and self-play. For vision-and-language tasks, there are now many joint-modality networks whose secret sauce is in the pre-training tasks that they do. For instance, LXMERT and VisualBERT Recently there has been an incredible proliferation of self-supervision tasks; this is an active and exciting area of research!
