[site]: crossvalidated
[post_id]: 51469
[parent_id]: 51440
[tags]: 
You might find the following article helpful. Various techniques are outlined to get probability estimates for the outputs of SVM in Milgram . In combining the probability estimates a weighted or unweighted sum of probabilities, naive Bayes or various other techniques can be used. See Chapter 5 for a comprehensive study of fusing classifier outputs. Kittler argues theoretically that the sum rule (adding up the probabilities of various classifiers and choosing the class with the highest probability) is optimal. I don't know what type of improvement in accuracy you can expect from only two support vector machines. The argument behind ensemble is that the probability of a correct collective decision approach 1 if the number of classifiers in the ensemble approach infinity. Using only two classifiers, will either agree on the decision or disagree on the decision. I would think that the ensemble wont be any better than the best single classifier?
