[site]: crossvalidated
[post_id]: 402686
[parent_id]: 
[tags]: 
Word perplexity on a subword language model

Let's have corpora $X = x_1...x_N$ in which every word can be represented using subwords (from a fixed size vocabulary of subwords) $x_i = x_{i,0}...x_{i,M(x_i)}$ where $M(x_i)$ is number of subwords to which the word is divided. For a word language model we would calculate perplexity using formula: $\exp\left(\frac{{\sum_{i=1}^N \log \left(\dfrac{1}{q(x_i)}\right)}}{N}\right)$ where $q(x_i)$ is probability of a word from language model. My questions are: Is it valid to calculate word perplexity on a subword language model, where $q'(x_i) $ would equal to $\prod_{j=1}^{M(x_i)}r(x_{i,j})$ and $r(x_{i,j})$ is probability from subword language model. The whole formula would look like this: $\exp\left(\frac{{\sum_{i=1}^N \log \left(\dfrac{1}{\prod_{j=1}^{M(x_i)}r(x_{i,j})}\right)}}{N}\right) = \exp\left(\frac{{\sum_{i=1}^N \sum_{j=1}^{M(x_i)}\log \left(\dfrac{1}{r(x_{i,j})}\right)}}{N}\right)$ If not is there other way to calculate word perplexity on subword model and do two language models with different vocabulary can be even compared? The probability is actually conditional probability $q(x_i|x_{0...i-1})$ , does it change something in that manner?
