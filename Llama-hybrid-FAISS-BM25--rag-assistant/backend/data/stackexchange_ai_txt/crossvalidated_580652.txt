[site]: crossvalidated
[post_id]: 580652
[parent_id]: 
[tags]: 
Cosine Similarity of the word embeddings after UMAP dimensionality reduction

I want to calculate similarities of the word embeddings. As a basis I took SpaCy german corpus: nlp = spacy.load("de_core_news_lg") . I have approximately 200 thousands words with 300 embedding dimensions. I needed to cluster all my words into clusters(other task) and after some experiments i figured out, that with normalized embedding clusterization works better. For normalization I have used 2-nd norm: self.data.apply(lambda x: x['embedding']/np.linalg.norm(x['embedding'],ord = 2), axis=1) , where x['embedding'] is an 300-dim numpy array. My experiments showed, that clusterization works best with 50 dimension. That's why i have used UMAP clusterization. For clustering I have used hdbscan algorithm. After that I want to compute cosine_similarity between elements. But my results are weird. For example cosine_similarity between 'Python' and 'Java' is 0.0 . But for totally different(by meaning) words similarity is close to 1(for example: 'Football' and 'Phone'). Is there any reason for such behaviour of cosine_similarity measurement? Euclidean distance works as expected(Between Python and Java close to 0 and Football and Phone is large). If I don't use UMAP, cosine_similarity words as expected. Cosine Similarity implementation: cos_sim = round(dot(emb1, emb2)/(norm(emb1)*norm(emb2)),3) Euclidean Distance implementation: eucl_dist = round(np.linalg.norm(emb1 - emb2),3)
