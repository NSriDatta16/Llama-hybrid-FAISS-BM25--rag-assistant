[site]: crossvalidated
[post_id]: 411136
[parent_id]: 215757
[tags]: 
In a blog post , Davis King suggests treating the task as a regression problem. With respect to the validation data, we have a series of estimates of out-of-sample error. We would like to continue training as long as the validation error is decreasing. On the other hand, if the validation error has a low probability of decreasing, then we can infer that there is not much benefit to continued training. His blog post is a more elaborate than this idea, but this is the basic premise. A sharp corner that I have observed is that the optimizer may not make much progress for several epochs before finally finding a good path forward and making more significant progress once again. I don't know that there's an ideal way to mitigate this issue in the neural network setting, other than some trial-and-error.
