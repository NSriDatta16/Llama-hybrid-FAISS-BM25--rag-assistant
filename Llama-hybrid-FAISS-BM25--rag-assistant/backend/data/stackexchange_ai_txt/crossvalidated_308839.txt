[site]: crossvalidated
[post_id]: 308839
[parent_id]: 308790
[tags]: 
While we might analyze the many different formulae that we find in statistics, and see that second moments have a special place... ...maybe a more special place in statistics than in physics (Which also occasionally uses square terms for simplicity as well, for instance 'radius of gyration' $r_g^2$ . And also a term like ' moment of inertia' is neither an entirely simplified term and contains it's origin moment just like statistical terms contain their origin square . On top of this, physicists like simplicity like $\hslash = \frac{h}{2 \pi}$ , while statisticians, well)... Yet, the reasons for these uses of square terms (e.g. $\left(\frac{x}{\sigma} \right)^2$ which easily becomes seen as a containing a "constant" $\sigma^2$ instead of $\sigma$ , when you take it out of the brackets) may be more easily found in historical reasons . $\mathbf{h^2}$ and $\mathbf{R^2}$ Via the answer of Nick Cox on this earlier CV question Who is the creator or inventor of coefficient of determination (R-squared)? we see that the history had a great influence on this term. And this is not just for $R^2$ , the term $h^2$ is "invented" by the same person. Just see an article search on google: https://scholar.google.com/scholar?q="degree+of+determination"&as_ylo=1918&as_yhi=1924 You see that Sewall Wright did a great deal on the first descriptions of the concept of 'degree of determination'. He expressed it both $R^2$ and $h^2$ in terms of the square of something else 1) coefficients of correlation $R$ , and 2) heredity or an equivalent correlation coefficient $h$ (see an earlier source than mentioned by Nick Cox: Wright 1920 ). In an article like Mordecai Ezekiel 1929 Meaning and Significance of Correlation Coefficients you see that for a considerable time people where using all kind of expressions with the correlation coefficient (in the specific example article: $r^2$ , $r$ , $\sqrt{1-r^2}$ , $1-\sqrt{1-r^2}$ ) aside from $r^2$ , which made the explicit notation of $r^2$ important ( physics does not provide this freedom of choice, where we need to consider what kind of moment, first, second, third, or function thereof, or something else like the median, is best to describe a certain distribution or situation ). In the wonderfull overview from Wright 1934 " the method of path coefficients " he suggests "The squared path coefficient may accordingly be called a coefficient of determination. such coefficients were used before the term path coefficient was applied to the square root." although people remained using the squared definition. Probably this 'method of path coefficients' was not much liked, because who is teaching/learning this nowadays and what other statistics guru has been using these definitions? In this overview from Wright in 1934, you also find a reference to a 1918 article in which he uses squares of correlation coefficients but not yet a term related to 'determination'. $\mathbf{\sigma^2}$ This term is very often not used as such. And instead it is used without the square on the left hand side of the equation $\sigma = \sqrt{E\left[(X-\mu)^2\right]}$ or replaced by the term 'variance'. A typical expression is $Var(X)$ . Another existing expression is $\mu_2$ (widely used in older texts). The subscript denotes the order of the moment. So $\mu_1=\mu$ (or better $\mu^\prime_1=\mu$ ) is the first raw moment or the mean, the subscript 2 means second moment (variance in case of the central second moment), the subscript 3 means third moment, .... , etc (A problem with this symbol $\mu_2$ is that it is unclear around which point the moment, e.g. central or raw, is defined, even if $\mu^\prime$ vs $\mu$ exists to differentiate between raw and central. The symbol $\mu$ for mean has actually the same problem, although it has become very standard such that the ambiguity is not so relevant in most cases) Well, this large text under this item explains a bit why $\sigma^2$ may have just be easier for many scientists and statisticians. Still also like the $h^2$ and $R^2$ there is a historic origin. Interesting reads: Pearson 1894 Contributions to the Mathematical Theory of Evolution in which, at some point, the standard deviation is actually written as $\sigma = \sqrt{\mu_2}$ Airy 1861 (who uses a letter $c$ in place of $\sigma$ and the description error of mean square , but also compares with different, non squared, concepts mean error and probable error ) Fisher examines in 1920 the difference between $\sigma_1$ and $\sigma_2$ the unknown $\sigma$ estimated by either the first central moment 'mean error' or second central moment 'mean squared error'. According to Wikipedia (Okt 19 2017) , Fisher first used the term 'variance'. "It is therefore desirable in analyzing the causes of variability to deal with the square of the standard deviation as the measure of variability. We shall term this quantity the Variance" If you read the article you see that he often puts variance on the left hand side of the equation, and denotes it with a letter $V$ . The use of a letter $V$ is actually still common nowadays in works on mathematical statistics. In this article he uses often $\sigma^2$ , but that is for simplicity. Imagine Fermat's theorem written with a term like $c = \sqrt[n]{a^n+b^n}$ instead of $c^n = a^n+b^n$ . In this way, simplicity in equations, the use of $\sigma^2$ becomes strengthened. Note that replacing $\sigma^2$ by $V$ is not always useful. Sometimes one wants to indicate that the calculation is about $\sigma^2$ . For instance the equation 1 in the 1918 article $\sigma^2 = \sum a^2$ is more clear than $V = \sum a^2$ , if the $\sigma$ , what it is about, is written explicitly in the equation. Earlier than Fisher, there is mention of 'variability' : 1916 James Johstone ( THE MATHEMATICAL THEORY OF ORGANIC VARIABILITY ) describes a concept of variability in relation with the Gaussian distribution. In relation to 'deviation squared' or 'squared deviation' you will find several earlier sources. One interesting reference among early uses of 'squared deviation' is Francis Ysidro Edgeworth (1917) who speaks, in a footnote, of 'fluctuation' in place of $\sigma^2$ .
