[site]: crossvalidated
[post_id]: 605333
[parent_id]: 
[tags]: 
LSTM accuracy not increasing more than 68%

I am a newbie in the field of deep learning. I am trying to use LSTM with time series data with more than 300,000 datapoints. The target variable is a class with 7 unique values. I used one-hot encoding to covert it into numerical values. The final dataset looks as follows. 0 1 2 3 4 5 6 datetime 1970-01-09 03:32:31 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1970-01-09 03:40:59 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1970-01-09 04:14:28 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1970-01-09 13:07:18 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1970-01-09 15:44:40 0.0 0.0 0.0 0.0 0.0 0.0 1.0 My question is that after looking at multiple examples, I came up with the following LSTM but my training accuracy is not increasing more than 68%. Do you have any recommendations about what changes can help? I have tried changing the look_back value, epochs, batch size, number of hidden nodes, layers, etc. The accuracy changes only slightly with these modifications. X_train, y_train = preprocess_LSTM(train, look_back) X_train = np.reshape(X_train, (len(X_train), look_back, outputs)) model = Sequential() model.add(LSTM(200, input_shape=(input_nodes, outputs))) model.add(Dense(100, activation='relu')) model.add(Dense(outputs, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network X_train = np.asarray(X_train) y_train = np.asarray(y_train) model.fit(X_train, y_train, epochs=5, batch_size=128) # evaluate model _, accuracy = model.evaluate(X_train, y_train, batch_size=128) print (accuracy)
