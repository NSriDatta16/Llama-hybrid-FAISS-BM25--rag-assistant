[site]: crossvalidated
[post_id]: 459547
[parent_id]: 
[tags]: 
What is the relationship (if any) between the number of input features and underfitting/ overfitting in ML/DL models?

Firstly below are a few points on this topic based on my understanding: Overfitting occurs when a particular model is too complex for the given data. This results in the model memorizing the data leading to poor generalization. This results in high variance each time a different subset is used for training. Underfitting could happen when the data is not rich enough to indicate a good relationship with the predicted variable. Any useful deviation is observed as irrelevant by the model. On the other hand, if the model is too simple (example linear in case of non-linear setup) the model may fail to capture the data dynamics. The effect is a high bias in prediction. As known from the bias-variance tradeoff concept, optimizing the model to reduce variance makes it perform worse in terms of bias. Now as stated in my question, how does the number of features in input data affect these aspects? It is true that increasing the number of samples can in general help prediction (From a probability point of view this is justifiable, as more data helps with better representation of the original distribution). However, what can one say about number of data features? Specific to Deep Learning DL can automatically extract useful features. What would happen if one performs feature selection beforehand? Feature transformation (Eg. PCA) can obtain relevant features but can also result in far reduced number of features. In such a case, will the reduced feature cause underfitting when using DL for prediction? Are there any theoretical upper and lower bounds on the number of features? Appreciate any relevant points, literature references, conceptual or mathematical notions on this topic. In my research, I have not seen any direct references to this problem. I am currently working on some empirical evaluation, but looking for some theoretical background to support the ideas! Thank you.
