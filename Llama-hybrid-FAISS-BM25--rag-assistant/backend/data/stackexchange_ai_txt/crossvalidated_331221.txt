[site]: crossvalidated
[post_id]: 331221
[parent_id]: 
[tags]: 
XGBoost vs Gradient Boosting Machines

For a classification problem (assume that the loss function is the negative binomial likelihood), the gradient boosting (GBM) algorithm computes the residuals (negative gradient) and then fit them by using a regression tree with mean square error (MSE) as the splitting criterion. How is that different from the XGBoost algorithm? Does XGBoost utilize regression trees to fit the negative gradient? Is the only difference between GBM and XGBoost the regularization terms or does XGBoost use another split criterion to determine the regions of the regression tree?
