[site]: datascience
[post_id]: 44501
[parent_id]: 
[tags]: 
Comparison between approaches for timeseries anomaly detection

After various days of research, I could take a global picture of the existing methods to perform anomaly detection on time series, namely: Forecasting with Deep Learning. Eg. RADM or LSTM model Forecasting without Deep Learning. Eg. Seasonal ARIMA + Kalman Filters Denoising fixed-length windows with autoencoders (Deep Learning approach). Eg. MAD-GAN, CNN/LSTM autoencoders, variational autoencoders, etc. Denoising without deep-learning. Eg. Applying filters such as Kalman or Hodrick Prescott, and test if the deviation of the predicted with the original timeseries is under a threshold. Maybe there are even more methods that are not classificable into this screenshot. My question is, depending in the types of datasets that can occur, which approach suits better the need of developing a timeseries anomaly system which: Detects anomalies in an univariate way, but allows for multivariate posterior integration. Is suitable for online data-streaming and run-time adaptation/learning Is capable of learning data distributions holding more than one seasonality. [Plus] Might allow for semi-supervised improvement at any step. Note: take into account timeseries that can have different trends; none, one or more seasonalities; cualitative/step/continous timeseries; with more or less missing data, etc.
