[site]: crossvalidated
[post_id]: 32525
[parent_id]: 31249
[tags]: 
It is all data-dependent: added example features can span the range between good-features and pure-noise. Let's assume some grounded in reality (supposedly useful) & randomly selected data. By adding more data (rows or examples, not columns or features) your chances of over-fitting decrease rather than increase. All other things being equal, the two paragraph summary goes like this: Adding more examples, adds diversity. It decreases (improves) the generalization error because your model becomes more general by virtue of being trained on more examples. Adding more input features, or columns to a fixed number of examples may work both ways. Good features can help up to a certain point. But once overdone, they increase over-fitting. More features may be either irrelevant or redundant and there's more opportunity to complicate the model in order to fit the fixed-number of examples at hand. There are some simplistic criteria to compare quality of models. Take a look for example at AIC or at BIC . They both show that adding more data always makes models better, while adding parameter complexity beyond the optimum, reduces model quality. If you have the luxury of a large or unlimited number of examples, there's a practical way to optimize the modeling process: watch the testing (out-of-sample) error rate and make sure to stop training once the out-of-sample error rates bottom out. While doing that, also watch the training error rate. If the smoothed/moving-average rate fails to keep converging, you may have non-stationary data, or bad/irrelevant features or representation (e.g. the NN layout) to begin with.
