[site]: crossvalidated
[post_id]: 389643
[parent_id]: 389608
[tags]: 
I am answering myself. Still not with a proof, but with a better solution. Assume first a slightly different setup, where instead of a single number $y_t$ , we get a sample average $\bar{y}_t$ from $n_t$ samples. Clearly, if $n_t$ is large, we would like to trust it a lot and unlearn much of the previous samples. if afterwards, $n_{t+1}$ is small, we would not want to unlearn the good sample quickly. The correct way to handle this is to have two variables: one which is an exponentially decaying sum of the number of samples $N_{t+1}= (1-c) N_t + n_t$ Note that when $n_t=1$ for all $t$ , this will converge to $N=1/c$ . The update of the mean $\mu_{t+1}$ is just the correctly weighted average of two means with different number of samples: $\mu_{t+1}=(1-\frac {n_t}{N_{t+1}}) \mu_t + \frac {n_t}{N_{t+1}} \bar{y}_t$ The same now holds trivially when replacing $n_t$ with $w_t$ . I have no doubt about the $\mu_{t+1}$ update and it also makes sense with importance sampling. I am not sure about the $N_t$ , but I think it is probably hard to come up with a perfect justification for any type of exponential moving average.
