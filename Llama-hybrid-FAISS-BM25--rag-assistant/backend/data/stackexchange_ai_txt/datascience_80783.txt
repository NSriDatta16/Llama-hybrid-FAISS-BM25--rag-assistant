[site]: datascience
[post_id]: 80783
[parent_id]: 80595
[tags]: 
Since many of your questions were answered already, I may only share my personal experience with your last question: 7) Is it a good idea to use BERT embeddings to get features for documents that can be clustered in order to find similar groups of documents? Or is there some other way that is better? I think what a good idea would be is to start with simpler approaches. Especially when dealing with long documents relying on vectorisers such as tf-idf may lead to better results while having the advantages of less complexity and usually more interpretability. I just finished a cluster exercise for longer documents and went through a similar thought process and experimentations. Eventually, I obtained the best results with tf-idf features. The pipeline I used consisted of: Process data (stop-word removal, lemmatising, etc) Fit tf-idf vectorizer (alternatively you may try also doc2vec ). Run some sort of dimension reduction algorithm (PCA in my case). (K-means) clustering - evaluate optimal number of clusters. If you are eager to use BERT with long documents in your down-stream task you may look at these two main approaches: Truncation methods head-only (first 512 tokens) tail-only (last 512 tokens head+tail Depending on your domain, for example if each document is concluded with an executive summary, tail-only may improve results. Hierarchical methods mean pooling max pooling As stated here Truncation methods applies to the input of the BERT model (the Tokens), while the Hierarchical methods applies to the ouputs of the Bert model (the embbeding).
