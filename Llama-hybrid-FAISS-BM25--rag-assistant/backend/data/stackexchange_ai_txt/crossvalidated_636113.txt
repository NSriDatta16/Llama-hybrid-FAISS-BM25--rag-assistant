[site]: crossvalidated
[post_id]: 636113
[parent_id]: 7207
[tags]: 
Perhaps one helpful example to think about is information retrieval. Say you have 1,000,000 documents. 100 of them are relevant to a search query (i.e. positives), while the rest are irrelevant (i.e. negatives). The important thing is that the "baseline" of retrieving the documents in a random order, with the 100 positive documents uniformly distributed through all 1,000,000 documents, is a very poor baseline which should be easily beaten by any search system. Typically, most of the irrelevant documents are obviously irrelevant and easily classified as such. Let's say a basic search system ranks all 100 positive documents uniformly within the first 1000 documents, while a good search system ranks all 100 positive documents uniformly within the first 200 documents. Now we see the problem with the ROC curve: for any search system, a recall (i.e. true positive rate) of 1 is reached for a very small false negative rate (before even 1% of negatives are misclassified as positive), and so the ROC curve (which plots recall against the false negative rate) almost immediately shoots up to 1. However, the precision-recall (PR) curves will look very distinct. In this simple example, they will be flat lines of constant precision, but for the basic search system the precision will be 10%, while the good search system will have a precision of 50%. Note that the x-axis for ROC effectively corresponds to all of the (very many) negative documents, while for PR the x-axis just corresponds to the positive documents. The lack of focus of ROC on the rare and interesting positive cases is what makes it less useful in this scenario. In summary: in situations where the positive class is rare and we expect any decent classifier to rank most positives above almost all of the negatives , then PR is better at showing differences in performance than ROC.
