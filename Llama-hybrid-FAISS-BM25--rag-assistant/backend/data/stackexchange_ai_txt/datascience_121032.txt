[site]: datascience
[post_id]: 121032
[parent_id]: 14142
[tags]: 
There are a few errors in this implementation: In your u_dist you are sampling from a normal distribution. You should be sampling from a uniform distribution . You are sampling also from a standard normal which is centered around 0, while you should be sampling in the entire space of the data (so to use min and max on your data to find the edges). In the u_dist you are taking the 2nd closest neighbor, instead of the 1st. You should exponentiate the distances to the dimension of your data. In addition, the implementation could be improved: You are using for loops instead of vector operations. You are mixing numpy arrays with python lists, which is IMO not very elegant. The results are not reproducible, as they depend on the internal seed of numpy/python. Here is how your code would look like after the changes: import numpy as np from sklearn.neighbors import NearestNeighbors def hopkins(X, portion=0.1, seed=247): # X: numpy array of shape (n_samples, n_features) n = X.shape[0] d = X.shape[1] m = int(portion * n) np.random.seed(seed) nbrs = NearestNeighbors(n_neighbors=1).fit(X) # u_dist rand_X = np.random.uniform(X.min(axis=0), X.max(axis=0), size=(m,d)) u_dist = nbrs.kneighbors(rand_X, return_distance=True)[0] # w_dist idx = np.random.choice(n, size=m, replace=False) w_dist = nbrs.kneighbors(X[idx,:], 2, return_distance=True)[0][:,1] U = (u_dist**d).sum() W = (w_dist**d).sum() H = U / (U + W) return H Clustered Data For clustered data (and here clustered means - not uniformly spread across the space; so even a sample from a normal distribution is considered extremely clustered) you would get something like this: c = 2 # how much clusters are far from each other means = c*torch.tensor([[0.,0], [1.,1], [1,-1], [-1,1], [-1,-1]]) # draw from 5 clusters cov = torch.tensor([[1.,0.],[0,3]]) # 2d covariance matrix # draw the samples n = 500 # actual samples are x5 this samples = D.MultivariateNormal(means, cov).sample((n,)) # plot the clusters X = samples.reshape(-1,2).numpy() y = np.tile(range(1,6), n) plt.scatter(X[:,0], X[:,1], c=y, s=5) # calculate the hopkins statistic hopkins(X) > 0.8983462887476471 Uniform For uniform data you should get something around 0.5: X = D.Uniform(torch.tensor([-3.,-3]), torch.tensor([3,3.])).sample((5*n,)) plt.scatter(X[:,0], X[:,1], s=5) hopkins(X.numpy()) >0.47382462294058925 Grid For a grid you should get a very low statistic, even around 0: x = np.linspace(-3, 3, 50) y = np.linspace(-3, 3, 50) xv, yv = np.meshgrid(x, y) X = np.c_[xv.reshape(-1,1), yv.reshape(-1,1)] plt.scatter(X[:,0], X[:,1], s=5) hopkins(X) > 0.13711754341114704
