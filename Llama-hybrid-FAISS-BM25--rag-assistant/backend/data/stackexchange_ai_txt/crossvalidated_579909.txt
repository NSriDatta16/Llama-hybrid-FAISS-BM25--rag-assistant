[site]: crossvalidated
[post_id]: 579909
[parent_id]: 
[tags]: 
Why does data get so tangled up in high dimension?

When I look at textbooks on classification and machine learning, many of the examples focus on data that is often twisted up such as to avoid linear separation. I have an example picture below. The common description of the classification problem is that data can be twisted in this manner, and hence kernel methods or random forests are better at dealing with nonlinear decision boundaries. Of course, there is no a priori reason that datapoints should be nicely separated and linearly separable. But I was trying to understand how the data usually gets so twisted up? First, the usual claim is that in high dimension, meaning perhaps dimensions greater than 10 or so, the distance between points becomes so big, that all points are essentially the same distance from each other. If that is the case, then I would imagine that data should not be twisted, because all the points are far from both similar points and dissimilar points. At the same time, there is this common manifold assumption, where data generally lie on a lower dimensional manifold. But again, how would this manifold assumption reconcile with the distance issues in high dimension? As we add dimensions, points seem to become more equally distant from each other. So I am just trying to understand how the geometry of high dimensional data evolves, given the contrast between the high dimensional distance issues versus the manifold assumption?
