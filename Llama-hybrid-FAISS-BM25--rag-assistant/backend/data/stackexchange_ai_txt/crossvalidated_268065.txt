[site]: crossvalidated
[post_id]: 268065
[parent_id]: 
[tags]: 
Modelling decaying seasonal variation with SARIMA

I have a time series on quarterly data with seasonality which I am trying to fit a SARIMA model to. The seasonal variation seems to be decreasing with time. I am wondering what the best way to model this with a SARIMA model is. Here is the series decomposed: My confusion stems from when it is appropriate to take the first seasonal difference. In the literature, taking the first seasonal difference seems to be the correct way to handle seasonality. But if I do that, I model that the seasonal difference is constant over time, which it doesn't seem to be. If I then try to model without taking the first seasonal difference, all reasonable models violate the assumption of the AR constants being between -1 and 1. Here are the four quarters plotted separately:
