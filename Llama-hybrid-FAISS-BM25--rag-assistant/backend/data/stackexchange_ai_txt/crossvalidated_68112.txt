[site]: crossvalidated
[post_id]: 68112
[parent_id]: 68101
[tags]: 
In general, a higher value of the likelihood (or log-likelihood) is preferred. The relationship to the value of 0 is irrelevant. However, there are a number of caveats: if you add more parameters (predictors) to the model, the likelihood can only go up, so selecting the maximum value would always give the model with the most variables. There are multiple ways around it via various information criteria, that penalize the model by its number of parameters. Look up the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). I don't know exactly what the "likelihood ratio chi-squared" is, but it could be the value of the likelihood-ratio test statistic compared to the null model (not the other competing models). It is unlikely to be the Wald or the score test statistic. However, if the competing models are nested (one is a special case of the other) you can use the log-likelihood values to compute the likelihood-ratio test statistics of one model versus the other. $$ \mathrm{LRT} = -2 \cdot( \log L(M_1) - \log L(M_2)) \sim \chi^2_{p_2-p_1}$$ under $H_0$ , where $p_1$ and $p_2$ are the number of parameters There are no "typical" values for the value of the log-likelihood, just as there are no typical values for the sum of all the y values in your data. They are only meaningful when comparing different fits to the same set of values for the outcome.
