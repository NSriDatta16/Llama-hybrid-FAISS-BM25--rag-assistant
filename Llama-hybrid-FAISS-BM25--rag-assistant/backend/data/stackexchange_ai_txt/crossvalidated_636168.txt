[site]: crossvalidated
[post_id]: 636168
[parent_id]: 636104
[tags]: 
Let's formalize the approach discussed in your answer. You're going to sample $n$ users $(i=1,2,\ldots, n)$ . A sampled user is a bad actor with probability $p$ , which is the overall proportion of bad actors (the thing you want to know). Let's model this with a Bernoulli RV, where $x_i=1$ for a bad actor and $x_i=0$ otherwise. $$x_i \sim \text{Bern}(p), i=1,2,\ldots, n$$ Now, for user $i$ , suppose you sample $m_i$ clips for that user. If we're dealing with a bad actor, then a bad clip will be generated with probability $\theta_1$ . If the user is a good actor, then they generate bad clips with probability $\theta_0$ (which may be zero). $$y_{ij} |x_i \sim \text{Bern}\left(\theta_{x_i}\right), j=1,2,\ldots, m_i$$ Finally, let's write out the log-likelihood function which will be useful for inference. $$\ell(p, \theta_1, \theta_2 | {\bf x}, {\bf y}) = \sum_{i=1}^n\sum_{j=1}^{m_i}\left\{y_{ij}\log \theta_{x_i} + (1-y_{ij})\log (1-\theta_{x_i}) + x_i \log p + (1-x_i)\log(1-p)\right\}$$ Identifiability Without making some strong assumptions about the probabilities $p$ , $\theta_0$ , and $\theta_1$ , you are going to have a tough time distinguishing between large $p$ and large $\theta_1$ . For example, $$T = \sum_{i=1}^n\sum_{j=1}^{m_i} y_{ij}$$ is the total number of bad clips in your sample and \begin{aligned} E(T) &= \sum_{i=1}^n\sum_{j=1}^{m_i} E(y_{ij}) \\ &= \sum_{i=1}^n\sum_{j=1}^{m_i} p \theta_1 + (1-p)\theta_0. \end{aligned} So if $\theta_0=0$ (for instance) then it is hard to tell whether $p$ or $\theta_1$ is driving the bad clips. Bayesian Inference Let us specify the following prior distributions \begin{aligned} p &\sim \text{Beta}(1, 1) \\ \theta_0 &\sim \text{Beta}(1, 100) \\ \theta_1 &\sim \text{Beta}(4, 16). \end{aligned} In these choices, we are specifying (i) a reasonably uninformative prior for $p$ , (ii) a belief that good-actors rarely produce bad clips, (iii) a belief that bad-actors generate bad clips between 3.7% and 46.8% of the time (with high probability, e.g., 0.99). Now we can apply a standard Bayesian inference algorithm, such as adaptive MCMC. R Code Here's some R code, complete with synthetic data to try this out. I am using the MHadaptive R package for MCMC. For the code to work well, I am transforming the variables from a $(0, 1)$ scale to a $(-\infty, \infty)$ scale. This change of variables requires the addition of a Jacobian in the log posterior evaluation. set.seed(123124908) # True values n $trace)/(1+exp(mcmc$ trace)) + rnorm(length(param), 0, 1e-9) # Make plot par(mfrow=c(1,3)) hist(param[,1], main="posterior for p", xlim=range(c(p, param[,1]))) abline(v=p, lwd=2, col='orange', lty=3) hist(param[,2], main="posterior for theta0", xlim=range(c(theta[1], param[,2]))) abline(v=theta[1], lwd=2, col='orange', lty=3) hist(param[,3], main="posterior for theta1", xlim=range(c(theta[2], param[,3]))) abline(v=theta[2], lwd=2, col='orange', lty=3) Results How did we do? We did... alright? It looks like we correctly captured $\theta_1$ but we failed to capture the true value for $\theta_0$ or $p$ . This shows the importance of having accurate priors! Although with more data, perhaps the situation will be improved.
