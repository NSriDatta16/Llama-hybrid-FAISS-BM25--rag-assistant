[site]: datascience
[post_id]: 28956
[parent_id]: 
[tags]: 
Unevenly stretched sequences with LSTM/GRU

I am looking for a right NN architecture(probably, based on LSTM/GRU) for the classification problem I faced. I have an alphabet of events {A, B, C, D, ..., N} and sequences of these events for each training example. It is obviously that Embedding + LSTM/GRU gives me some baseline model, but my problem is a little bit more complex. I also have timestamps attached to each event, so the same sequence in terms of order my represent different scenarios in reality for example {B, C, C, A } and {B, C, C, A }. I tried to replace embedded vectors representing events with just events' timestamps(or duration from previous event) in the LSTM/GRU layer input and it gave me also some model with 75%-80% quality from the baseline model(and about ~6% of examples were correctly classified which were not correctly classified in baseline model). Now I am looking for an NN architecture to combine these two datasets(sequence of events and sequence of timestamps). I have already tried the following approaches: Simply concatenate timestamp information into embedding vectors(with and without training), for example if embedding size = 32, we would have 33 dimensional vector for LSTM input - 97%-100% quality from the baseline model NN with 2 inputs for event sequence and for timestamps sequence with concatenation before LSTM - ~90% quality from the baseline model NN with 2 inputs for event sequence and for timestamp sequence with 2 LSTMs and concatenation after - ~95% quality from the baseline model Thanks for any comments or recommendations.
