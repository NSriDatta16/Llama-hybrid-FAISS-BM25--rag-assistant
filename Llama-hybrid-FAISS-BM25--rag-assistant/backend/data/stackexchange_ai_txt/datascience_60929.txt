[site]: datascience
[post_id]: 60929
[parent_id]: 60917
[tags]: 
Here are two approaches that use the idea of havng your prediction be a distribution over your continuous variables rather than a single value. "Easy" but computationally expensive Independently train K regressors. You now have K samples from a distribution, and you can Big downside - you have to train lots of networks, and run lots of networks at inference time. For some details "Confidence and prediction intervals for neural network ensembles" Harder Have one network explicitly predict a parametrized (e.g. Gaussian/ mixture of gaussians) distribution. For details see "Mixture density network" , for example So in the case of a single Gaussian, the network would output a mean and standard deviation, and so you have an estimate of the uncertainty. A possible danger here is that you might need to be clever about how to train the variance parameter. You could imagine a case where a network does a good job on the means but is either over- or under- confident in its predictions.
