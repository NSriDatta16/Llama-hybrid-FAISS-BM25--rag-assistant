[site]: crossvalidated
[post_id]: 109456
[parent_id]: 
[tags]: 
About cross-validation for machine learning

Assume I have 1000 samples of data. I split the data randomly into training and test sets of size 800 and 200, respectively. Now, I train a classifier using the training set, and then evaluate the performance of the classifier using the test set. Assume I am not satisfied with the results. So, I change some parameters, and do this again. Is the approach wrong? Should I have split the 800 samples into another two sets, fine-tune model, and only then try it on the test set? What if my sample is too small for two splits? The data is split randomly, so I assume this would not be completely wrong?
