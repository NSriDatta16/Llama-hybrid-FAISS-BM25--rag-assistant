[site]: crossvalidated
[post_id]: 576609
[parent_id]: 
[tags]: 
How XGBoost chooses between two features that gives the same information?

I have 10 variables in a dataset ( X1 , X2 , .., X10 ) plus the binary target variable ( Y ). When I train a model with xgboost.sklearn.XGBClassifier() and then plot the feature importances with xgboost.plot_importance(my_model, importance_type='gain) , I get that X1 variable is the most important (~70%) for this first model. However, if I drop the X1 variable, train again, I get the X2 variable is the most important (~70%) in this second model with no significant differences in the final performance of both models. So, I compute the linear correlation between those two hoping it's high and I just get -22% (and this -22% makes business sense, so no problem with that) My question What criteria XGBoost uses to choose X1 over X2 when are both variables present? (And it gives a feature importance to X1 near 70% and to X2 near 0%), does this suggest that XGBoost already gets all the information of X2 using X1? If yes, how?
