[site]: datascience
[post_id]: 88486
[parent_id]: 88336
[tags]: 
Simplest approach First a very fast and perhaps practical approach: just remove them without replacing them! From your bar chart, it seems you have a lot of transactions - several hundred thousand. Removing a few hundred (I can't even see a bar for the > $600 transactions) and not replacing them wouldn't mean the remaining data is unusable. Replacing those transaction e.g. using the mean of the distribution, is basically trying to make sure they don't have any significant effect on the model; so what is the point in replacing them? Looking at your frequency chart, I would be interested to know what appears in the final large peak. Could you just remove those transaction? You could give this a go and move onto the modelling, see if you get sensible results - if not, come back to investigate further. Back to statistical methods Using 3 standard deviations isn't a bad approach - assuming your data is normally distributed, it means you only remove 0.3% of the data . The issue is perhaps that your distribution of transaction amounts does not look normally distributed - it looks more like a beta distribution (the orange line): You could therefore try your approach of removing data outside the distribution, by computing the parameters of the beta distribution , and using those instead of the normal mean/std. To do this, check out scipy.stats.beta , and be aware that you should probably normalise your transactions to the range [0, 1.0] (see Scikit-Learn's StandardScaler ). Here is a thread on understanding the output of your beta distribution. Impute If you simple treat your outliers as missing data, there are some nice ideas and explanations about filling missing gaps in your data (a.k.a. data imputation ) in this well-known book: Elements of Statistical Learning (pdf) $^1$ - see section 9.6. $^1$ Authors: Trevor Hastie, Robert Tibshirani, Jerome Friedman
