[site]: crossvalidated
[post_id]: 97694
[parent_id]: 97508
[tags]: 
In the AR(1) model $y_{t}=\theta y_{t-1}+ϵ_t$, with, say $y_0 = \tilde y$, the OLS estimator is $$\hat \theta_{OLS} = \frac {\sum_{t=1}^Ty_{t-1}y_t}{\sum_{t=1}^Ty_{t-1}^2} = \frac {\sum_{t=1}^Ty_{t-1}(\theta y_{t-1}+ϵ_t)}{\sum_{t=1}y_{t-1}^2} = \theta + \frac {\sum_{t=1}^Ty_{t-1}ϵ_t}{\sum_{t=1}^Ty_{t-1}^2}$$ So $$E[\hat \theta_{OLS}]-\theta = E\left(\frac {\tilde yϵ_1}{\sum_{t=1}^Ty_{t-1}^2}+\frac {y_1ϵ_2}{\sum_{t=1}^Ty_{t-1}^2}...+\frac {y_{T-1}ϵ_T}{\sum_{t=1}^Ty_{t-1}^2}\right)$$ The usual way of proving/examining unbiasedness when regressors are stochastic, is to use the Law of Iterated Expectations and calculate the expected value of the estimator conditional on the regressors: $$E[\hat \theta_{OLS}]-\theta = E\left[E\left(\frac {\tilde yϵ_1}{\sum_{t=1}^Ty_{t-1}^2}+\frac {y_1ϵ_2}{\sum_{t=1}^Ty_{t-1}^2}...+\frac {y_{T-1}ϵ_T}{\sum_{t=1}^Ty_{t-1}^2}\mid \{y_0, y_1,...,y_{T-1}\}\right)\right]$$ All terms are included in the conditioning set (are "measurable" with respect to it) except the last one so we have $$E[\hat \theta_{OLS}]-\theta = E\left[\frac {\tilde yϵ_1}{\sum_{t=1}^Ty_{t-1}^2}+\frac {y_1ϵ_2}{\sum_{t=1}^Ty_{t-1}^2}...+\frac {y_{T-1}E\left(ϵ_T\mid \{y_0, y_1,...,y_{T-1}\}\right)}{\sum_{t=1}^Ty_{t-1}^2}\right]$$ $$E[\hat \theta_{OLS}]-\theta = E\left[\frac {\tilde yϵ_1}{\sum_{t=1}^Ty_{t-1}^2}+\frac {y_1ϵ_2}{\sum_{t=1}^Ty_{t-1}^2}...+\frac {y_{T-2}ϵ_{T-1}}{\sum_{t=1}^Ty_{t-1}^2}\right] + 0$$ so we are back to unconditional expectations. But $$E\left[\frac {\tilde yϵ_1}{\sum_{t=1}^Ty_{t-1}^2}\right] \neq \frac {E\left[\tilde yϵ_1\right]}{E\left[\sum_{t=1}^Ty_{t-1}^2\right]}=0$$ because $\epsilon_1$ is included also in the denominator , and the expected value cannot be applied separately. Likewise for the other expected values. So the bias is non-zero. Matters get worse when a constant term is included, since then the sample averages of the dependent variable enter also the formula for the OLS estimator, strengthening the correlation that leads to the existence of bias. If in the above expressions, instead of $y_{t-1}$ we write $x_t$ for some regressor that is assumed independent of all disturbances i.e. not only the concurrent one, then we can see why unbiasedness holds -and this is the assumption (stict exogeneity) that makes it happen, nothing less than that. From these expressions also one can see that assuming only that $x_t$ is independent of $\epsilon_t$, but possibly dependent of past disturbances, is not enough to make the estimator unbiased.
