[site]: crossvalidated
[post_id]: 598116
[parent_id]: 
[tags]: 
Covariance matrix of linear estimator of basis

I am reading Elements of Statistical Learning, specifically section 5.2.2, an example on the South African Heart Disease dataset. The idea is to model the logit of the conditional probability of the output as linear in the basis representation. Specifically, we have $$\text{logit}(P(Y|X))=\theta_0+\sum_{i=1}^p h_i(X_i)^T\theta_i$$ where there are $p$ features and $h_i(X_i)$ are the basis functions to be used for feature $i$ , and $\theta_i$ are the corresponding vectors of coefficients. Then, we can model the whole thing as lienar via $$\text{logit}(P(Y|X))=h(X)^T\theta$$ where $\theta$ is the stacked $\theta_i$ 's, and $h(X)$ is the stacked $h_i(X_i)$ 's. More succinctly, if we take an $N\times df$ matrix $\mathbf{H}$ as the basis values (here $N$ for number of data and $df$ represents the total number of degrees of freedom, i.e. unique basis functions), we can apply linear regression analysis with $\mathbf{H}$ as the design matrix. The book then states that we estimate the covariance of $\theta$ via $$\hat{\mathbf{\Sigma}}=(\mathbf{H}^T\mathbf{W}\mathbf{H})^{-1}$$ where $\mathbf{W}$ is the "diagonal weight matrix from the logistic regression". Why is the $\mathbf{W}$ here? I'm confused as to how the weigh matrix from the logistic regression comes into play here.
