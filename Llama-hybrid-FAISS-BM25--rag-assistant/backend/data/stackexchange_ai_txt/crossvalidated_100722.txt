[site]: crossvalidated
[post_id]: 100722
[parent_id]: 100679
[tags]: 
I understand that the $x's$ are also random, in which case what follows is to be understood "conditional on the $x$'s". There are conceivably four "cases" here ($f_{\gamma}(\gamma)$ is the probability density function of $\gamma$), $$ \begin{align} & \prod_{i=1}^{n}P_i\left(\lambda[x,E(\gamma)]\right) = \prod_{i=1}^{n}P_i\left[\lambda\left(x,\int\gamma f_{\gamma}(\gamma)\mathrm{d}\gamma\right)\right] &\qquad [1]\\ & \prod_{i=1}^{n}P_i(E[\lambda(x,\gamma)]) = \prod_{i=1}^{n}P_i\left[\int f_{\gamma}(\gamma)\lambda(x,\gamma)\mathrm{d}\gamma\right] &\qquad [2]\\ & \prod_{i=1}^{n}E\left(P_i[\lambda(x,\gamma)]\right)=\prod_{i=1}^{n}\left(\int f_{\gamma}(\gamma)P_i[\lambda(x,\gamma)]\mathrm{d}\gamma\right) &\qquad [3] \\ & E\left[\prod_{i=1}^{n}P_i[\lambda(x,\gamma)]\right] = \int f_{\gamma}(\gamma)\left(\prod_{i=1}^{n}P_i[\lambda(x,\gamma)]\right)\mathrm{d}\gamma &\qquad [4] \end{align}$$ Since the above are conditional on the $x'$s, the expected value is taken with respect to $\gamma$ only. Moving from $[1]$ to $[4]$, what we are doing is gradually increasing the extent to which we are allowing the uncertainty in $\gamma$ to interact with the structure . In $[1]$, we "sweep away" this uncertainty by using, instead of $\gamma$, a centrality measure for it, $E(\gamma)$, and then calculate all consecutive interactions (trough the function $\lambda ()$, through the $P_i$'s, and then through $\prod$), using just this centrality measure. In $[2]$, we allow the uncertainty in $\gamma$ to interact with the function $\lambda()$, by permitting $\lambda ()$ to take the various values it takes for each possible value of $\gamma$, and then averaging. But after that, the uncertainty stops affecting the expression, since it has been averaged out. In $[3]$, we allow the uncertainty in $\gamma$ to interact directly with the composite function $P_i\circ \lambda$: for each possible value of $\gamma$ we calculate the magnitude $P_i[x,\lambda(\gamma)]$, and then averaging over it. In other words, each expression is affected by a different random quantity. Depending on the actual functional forms employed, these three expressions in general give different results. Only if the affine property is preserved, they are equivalent: If, say, $\lambda(x,\gamma) = a_1 + h(x)\gamma$ and $P_i = a_2+b_1\lambda$, then , conditional on the $x$'s, the three expressions will give identical results because of the linearity property of the expected value operator. But if non-affine forms are involved the equality breaks. Why? Because with non-linearity, each possible value of $\gamma$ will affect differently "itself" and $\lambda$ ($[1]$ against $[2]$), and also it will affect differently $\lambda$ and $P_i \circ \lambda$ ($[2]$ against $[3]$). If, say, $\lambda(x,\gamma) = x+ \gamma^2$, $\lambda$ is essentially not a function of $\gamma$ but a function of a non-linear function of $\gamma$. In other words, non-linearity "implies" another function composition. In this example, if nevertheless $P_i$ is an affine function of $\lambda$, then while $[1]$ is not in general equal to $[2]$, still, $[2]$ is equal to $[3]$. The same reasoning applies to $[4]$: in it, the uncertainty in $\gamma$ is allowed to interact withe the whole product, before averaging it out. You should not be confused with the case where "the $P_i$'s are independent". The question is "independent with respect to what?" Presumably, with respect to the $x's$. But they are not independent with respect to $\gamma$ -and here our issue is the uncertainty in $\gamma$. If the $P_i$'s are independent with respect to the $x$'s (and you indicate that this indeed holds), then $[4]$ is the average (always with respect to $\gamma$) joint probability, while $[3]$ is the product of the average marginal probabilities. $[4]$ acknowledges fully the uncertainty in $\gamma$, while $[3]$ gives you the joint probability when the uncertainty in $\gamma$ has collapsed in its expected value.
