[site]: crossvalidated
[post_id]: 41429
[parent_id]: 41420
[tags]: 
I mostly concur with @PeterFlom's answer. In my opinion, you should not average your data (you are basically throwing away 2/3 of your information, why would you want to do that?), but you should definitely account for the fact that measurements on the same patient will tend to be closer together than measurements on different patients. In such a situation, I usually recommend mixed linear models, which are a simple instance of the multi-level models @PeterFlom recommends. Specifically, you would use a generalized linear mixed model. The link function would be logistic, as in "ordinary" logistic regression. However, the functional form would include multiple observations on each participant, modeled by a random effect, just as in "ordinary" linear mixed models, $y∼F(Xβ+Zγ)$. In R, you can fit this by glmer() in the lme4 package, using the binomial family. For prediction, you could use a single measurement. Whether or not a mixed model predicts better than a non-mixed model in a particular setting is hard to say, of course. What the mixed model does is account for intra-person variability. If you just average the three original data points, you lose all the variability between measurements, so you will be too optimistic about your ability to predict from a single new observation. If, on the other hand, you simply throw in all observations without taking the grouping into account, you will again be too optimistic, as all standard errors will shrink. Think of what would happen if you started with a single observation per participant, say 100 data points... and then simply copied each observation 100 times. You would end up with 10,000 "observations" and far smaller standard errors than with the original data, although you didn't enter any new information. In addition, mixed models allow modeling other grouping factors, like the location, its specific demographics, its staff, diagnostician characteristics etc. So they are a lot more general than averaging.
