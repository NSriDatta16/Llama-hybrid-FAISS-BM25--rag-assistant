[site]: crossvalidated
[post_id]: 252012
[parent_id]: 251982
[tags]: 
I always viewed the regularizer separately from the loss. Most machine learning problems come in the form of "Regularizer + Empirical Risk", where Empirical Risk means the arithmetic mean of the sum of the loss of every training sample. What you mean by "spread out across all observations" probably is that when you take the stochastic gradient of a single sample with respect to the weights, then you have to also consider the regularizer which does not get "spread out"/averaged. Compare gradient of regularized GD: $\nabla_w~\lambda~Regularizer(w) + \nabla_w n^{-1}\sum_{i=1}^{n}loss_i (w) $ to regularized SGD (only one element of the sum is considered): $\nabla_w~\lambda~Regularizer(w) + \nabla_w loss_i (w) $ Short: In my opinion it makes sense to separate the terms "regularization" and "cost" (which I named "Empirical Risk" for the full data and "loss" for one sample)
