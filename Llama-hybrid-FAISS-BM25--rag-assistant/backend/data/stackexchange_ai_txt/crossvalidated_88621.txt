[site]: crossvalidated
[post_id]: 88621
[parent_id]: 88535
[tags]: 
Everytime I cross-validate, the equation of the classifier built will change. Usually, there is a series of assumptions associated with the cross validation: First of all, you assume that each of the many surrogate models is equivalent (at least in its predicting power) to the model trained on the whole data set. Noticing that this assumption is violated (the pessimistic bias of cross validation is a symptom of this violation), you can make the weaker assumption that at least the surrogate models are equvaltent to each other. This allows you to pool the results of their testing. If even that assumption breaks down, you have to face the fact that your models (or their predictions) are unstable . So first of all, as long as 1. or at least 2. is met at the level of stable support vectors, there is no practical problem with your reporting. Different support vector sets can lead to models that basically give the same predictions. As long as that is the case, there is still no trouble: you have different support vectors that essentially describe the same decision boundary. The only situation where these changes become problematic is when the predictions become unstable. But e.g. with iterated $k$-fold cross validation you can measure whether there is any problem of unstable predictions. (See e.g. Confidence interval for cross-validated classification accuracy ) If predictions are stable, you are fine with reporting the model trained on the whole data set. If there is, there are 3 possibilities. You can report the validation results including the observed instability (validation means demonstrating that your model does its job, and stable predictions are probably part of that). And you can add to your professional experience that you overfitted the model, and go for a more restricted one. (Strictly speaking, that will require its own independent test set. But you may get away here for practical reasons with cross validating again if you just step once towards a less complex model. If you do that, I'd recommend you make the reader aware that another validation should have been done, but that was impossible for practical reasons. And report the full "history" of developing the model.) Slightly esoteric for SVM, but you can aggregate the surrogate models into an ensemble classifier.
