[site]: crossvalidated
[post_id]: 204624
[parent_id]: 204613
[tags]: 
It is a good observation. If you want to optimize some objective function, using another one as a proxy might not be the best idea. If function $f$ and function $g$ measure kind of the same thing, minimizing $f$ will reduce $g$, but it is not a given that it will minimize it. In some cases, it is acceptable to use the standard cost functions for growing a tree. If the function you want to minimize is related to the function for growing the tree, it might still be a good idea to use the Gini or Entropy cost function, as they are very fast to compute. This will allow you to grow more trees in the same time, which may lead to a better model once average. However, if your cost function is measuring something else (or something opposite!) than the Gini or Entropy cost function, your model will not have much meaning in the context you are evaluating it in. In such cases, adapting the cost function might help. Existing options After a quick look, I don't think Matlab , Scikit-learn nor R support custom cost functions for single tree growing or Random Forest. However, it is possible to add class weight. By putting more or less weight on the positive class, you might be able to manipulate the precision and recall of the grown tree. If you are already familiar with Random Forest in one of those environments, it might be the easiest thing to try first. It just becomes one additional hyperparameter. I think that the reason to why tree growing is so limited in term of cost functions comes down to efficiency. The number of split points to be evaluated is huge, and the function needs to be evaluated quickly. But there is an effort going on to be able to minimize arbitrary cost function using simple techniques, known as Gradient Boosting (Wikipedia) . Gradient Boosting allows the optimization of any cost function, and it fits trees sequentially by selecting the tree that best corrects the error of the current model, according to your cost function. One thing you should be careful about, is that you should probably consider the True Positives and other measures with regard to a soft assignment , using the probability that the class is a positive instead of the $0,1$ approach, in order to smooth the function. Boosting seems to be used with good results in the Learning to rank problem (Wikipedia) , and I think this may be due in part to the ability to adapt to the cost functions used in this framework. Most environment propose gradient boosting, Scikit-Learn , R , XGBoost . Another option would be to implement trees and random forests using the F-score as a split criterion, but you might suffer through a lot of technical details needed to get your implementation to par with existing systems.
