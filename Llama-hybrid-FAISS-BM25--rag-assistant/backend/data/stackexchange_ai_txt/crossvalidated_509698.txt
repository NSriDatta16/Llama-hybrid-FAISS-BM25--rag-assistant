[site]: crossvalidated
[post_id]: 509698
[parent_id]: 
[tags]: 
Is there a name for this: dealing with class imbalance by learning to rank?

I've seen the following approach used in link prediction settings, and I think it exists in many other areas as well. Say we have a very imbalanced binary classification dataset, with imbalance in the order of 10 000:1. Imagine that you have a neural network with a sigmoid output to predict Pos/Neg. You can then remove the sigmoid activation, and feed it N examples, one of which is positive, giving you N linear output values. If you apply a softmax over these N values, you get a new kind of learning problem where the task is to predict which out of N examples is the positive one. After training, the output of the network is still higher for more positive, so when you stick the sigmoid back on, it becomes a regular classifier again, which may be better at detecting positives. Like I said, I've seen this used in settings that could be cast as a highly imbalanced classification problem, but I can't find any references to this method applied to straight classification problenm with high imbalance, and tested against other approaches like weighted cross entropy or SMOTE. Is there a general name for this type of approach, and has it been evaluated against other methods?
