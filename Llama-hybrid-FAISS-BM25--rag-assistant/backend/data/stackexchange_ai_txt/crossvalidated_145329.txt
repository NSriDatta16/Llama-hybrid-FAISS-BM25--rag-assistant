[site]: crossvalidated
[post_id]: 145329
[parent_id]: 145323
[tags]: 
Essentially, the advantage of a second-derivative method like Newton's method is that it has the quality of quadratic termination. This means that it can minimize a quadratic function in a finite number of steps. A method like gradient descent depends heavily on the learning rate, which can cause optimization to either converge slowly because it's bouncing around the optimum, or to diverge entirely. Stable learning rates can be found... but involve computing the hessian. Even when using a stable learning rate, you can have problems like oscillation around the optimum, i.e. you won't always take a "direct" or "efficient" path towards the minimum. So it can take many iterations to terminate, even if you're relatively close to it. BFGS and Newton's method can converge more quickly even though the computational effort of each step is more expensive. To your request for examples: Suppose you have the objective function $$ F(x)=\frac{1}{2}x^TAx+d^Tx+c $$ The gradient is $$ \nabla F(x)=Ax+d $$ and putting it into the steepest descent form with constant learning rate $$ x_{k+1}= x_k-\alpha(Ax_k+d) = (I-\alpha A)x_k-\alpha d. $$ This will be stable if the magnitudes of the eigenvalues of $I-\alpha A$ are less than 1. We can use this property to show that a stable learning rate satisfies $$\alpha where $\lambda_{max}$ is the largest eigenvalue of $A$ . Choosing a learning rate which is too large will overshoot the minimum, and the optimization will diverge. The steepest descent algorithm's convergence rate is limited by the largest eigenvalue and the routine will converge most quickly in the direction of its corresponding eigenvector. Likewise, it will converge most slowly in directions of the eigenvector of the smallest eigenvalue. When there is a large disparity between large and small eigenvalues for $A$ , gradient descent will be slow. Any $A$ with this property will converge slowly using gradient descent. In the specific context of neural networks, the book Neural Network Design has quite a bit of information on numerical optimization methods. The above discussion is a condensation of section 9-7.
