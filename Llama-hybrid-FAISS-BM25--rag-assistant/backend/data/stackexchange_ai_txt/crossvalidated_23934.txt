[site]: crossvalidated
[post_id]: 23934
[parent_id]: 23932
[tags]: 
The first formula is the correct one, of course. I am not sure I can see what one could gain by multiplying the densities together, keeping the same variable as the argument of both, as the result will not be a density except in some very special cases. An apparent multiplication of densities does occur in Bayesian inference where the likelihood $p(x|\theta)$ is combined with the prior $\pi(\theta)$ as $\pi(x|\theta)\pi(\theta)$. However, the former is a conditional density, and the resulting density is a joint density of the data $x$ and the parameters $\theta$. To follow through the derivations for the normal case with the normal conjugate prior, see http://www.eisber.net/StatWiki/index.php/Mathematische_Statistik_-_%C3%9Cbung_Erg%C3%A4nzungsaufgabe_2_Beispiel_2 . It omits the scaling factors in front of the density and concentrates on the kernels; I would encourage you to follow this through with a complete derivation of all the densities involved.
