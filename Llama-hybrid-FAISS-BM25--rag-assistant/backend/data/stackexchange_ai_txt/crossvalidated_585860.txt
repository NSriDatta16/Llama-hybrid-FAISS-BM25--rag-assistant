[site]: crossvalidated
[post_id]: 585860
[parent_id]: 
[tags]: 
Stacked RNN to overcome vanishing gradient

Let me clarify that with vanishing gradient i don't mean the gradient to become zero (like for DNN with multiple sigmoidal layers) but learning long term dependencies.. So, LSTM are well known that they overcome the "long term dependency", and that stacking RNN also really helps and it's become a sort of industry standard, however I've found no paper on why this happens (in case I missed some, please link them), so I want to share my interpretation, and see if it's plausible or not. Given a dataset and a model with a single LSTM layer, we can check it's memory capacity to see what's its ability to learn a long term relation on that dataset, let's say that is "good" (whatever that means) for $N$ steps, thus $x^t$ is "decently reconstructable" until time $t+N$ . Thus we can see it sort of like this: where the red means "not decently reconstructable"... however, if we stack another LSTM, not the "information at time $t+N$ can be "forwarded" for other $N$ steps, something like this: However this ways of reasoning has the implication that the second LSTM has double the memory capacity, which is pretty hard to assume, so the solution would be to have residual connection from each stacked LSTM layer to the output layer, but that's another topic... So the question, is this "way of thinking" somewhat correct/founded or am I just making this up?
