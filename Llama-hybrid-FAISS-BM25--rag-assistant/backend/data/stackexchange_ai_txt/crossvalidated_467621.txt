[site]: crossvalidated
[post_id]: 467621
[parent_id]: 
[tags]: 
Simple algorithm to detect change point in time series

Apologies if this question has already been asked, but a lot of similar questions are regarding R or complex algorithms that I don't want. I have numerous 2-dimensional time series, eacch plotting the average success percentage of a task. Occasionally, some certain task which previously worked at a high rate (eg. 60-90%) starts to fail often (due to some unforeseen event), so the average success rate drops significantly (eg. 0-30%). I want a simple algorithm for a program to detect and notify me if this drop/change point occurs. I don't need to locate when, I just need an alert to see which one of these plots detected a change point (aka the task is starting to fail). I've seen a lot on CUSUM or other methods that require preset thresholds, but I can't preset a specific threshold, since some of these plots (or task) start at a higher percentage, and starts to fail more than other plots. What are some simple algorithms to detect a change-point/drop in a time series? Or what are some ways to detect a significant change in the average success rate (mean) plotted by time?
