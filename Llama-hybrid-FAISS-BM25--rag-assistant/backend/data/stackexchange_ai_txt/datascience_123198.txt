[site]: datascience
[post_id]: 123198
[parent_id]: 118522
[tags]: 
Does GPT-3 remember data from prompts used to fine tune it? Very unlikely, according the LIMA: Less Is More for Alignment paper: These results strongly suggest that almost all knowledge in large language models is learned during pretraining , and only limited instruction tuning data is necessary to teach models to produce high quality output. Also note that labels in prompts don't seem to matter much, see Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? . Sewon Min , Xinxi Lyu , Ari Holtzman , Mikel Artetxe , Mike Lewis , Hannaneh Hajishirzi , Luke Zettlemoyer . EMNLP 2022.
