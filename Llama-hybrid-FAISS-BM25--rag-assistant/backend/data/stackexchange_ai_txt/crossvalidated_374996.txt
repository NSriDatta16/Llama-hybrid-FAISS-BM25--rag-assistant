[site]: crossvalidated
[post_id]: 374996
[parent_id]: 
[tags]: 
Proving $\mathcal{H}_{Singleton}$ is PAC-learnable

I'm referring to Section 3.5, ex. 2 in Understanding machine learning . To my understanding, given $\varepsilon, \delta$ , I need to find minimum sample size $n$ s.t. $$P[e_P(ERM(S_n) > \varepsilon] Where $S_n$ is a sample of size $n$ . and $ERM$ is an algorithm that given the sample return an hypothesis with minimum empirical error. I tried to count the number of hypotheses that are "bad", in a way that their true error is more then $\varepsilon$ , and to show that the the probability that the $ERM$ algorithm will choose one of those is less than $\delta$ , but that wasn't successful. I also tried doing the opposite - count all possible hypotheses that the $ERM$ algorithm can output and show that the probability that any of them has true error which is larger then $\varepsilon$ is less then $\delta$ . That wasn't successful either. Is there a way proving it without using VC-dimension-arguments?
