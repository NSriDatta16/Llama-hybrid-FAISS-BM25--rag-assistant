[site]: crossvalidated
[post_id]: 642292
[parent_id]: 
[tags]: 
Machine Learning Experiment: should training parameters be fixed for a valid comparisons across model?

I am training an autoencoder three times, each time on a different dataset. The three datasets all have the same number of features, but have vastly different sizes. Assuming one of the datasets is A with length n, then dataset B includes dataset A and other n number of samples. Dataset C includes dataset B and other n number of samples. I would like to measure how the performance of the model changes as the number of samples increase. However, the "optimal" training parameters depend on the data. Given an arbitrary set of hyperparameters that is fixed, a model A (trained on dataset A) might appear to be perform better than model B (trained on B), when in reality, had the training parameters been adjusted for model B, it would have outperformed model A. On the other hand, finding out the "optimal set" of training parameters for each dataset is overwhelming and difficult.
