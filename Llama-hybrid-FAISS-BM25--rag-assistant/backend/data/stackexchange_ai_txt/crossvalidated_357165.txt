[site]: crossvalidated
[post_id]: 357165
[parent_id]: 357145
[tags]: 
We seek to minimize the cost on the whole training dataset (for example, consider the regression loss, classification loss is the same, as well as many other losses) $$ C = \frac{1}{2n} \sum_{i=1}^n \|y_i - a^L(x_i)\|^2 + \frac{\lambda}{2n} \sum_{w} w^2 $$ Note that this is equivalent to $$ C = \frac{1}{n} \sum_{i=1}^n \left[ \frac{1}{2} \|y_i - a^L(x_i)\|^2 + \frac{\lambda}{2n} \sum_{w} w^2 \right] = \mathbb{E}_{i \sim U[1, \dots, n]} \left[ \frac{1}{2} \|y_i - a^L(x_i)\|^2 + \frac{\lambda}{2n} \sum_{w} w^2 \right] $$ That is, $C$ is an expectation over random index of a training sample, $i$, uniformly distributed from 1 to n. Stochastic Gradient Descent is used to optimize objectives of this kind when we only have the gradient of the expression under the expectation evaluated at and averaged over for some batch (however small) of indices $i$. That is, we "approximate" the true gradient with $$ \tilde g = \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial w} \left[ \frac{1}{2} \|y_i - a^L(x_i)\|^2 + \frac{\lambda}{2n} \sum_{w} w^2 \right] = \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial w} \left[ \frac{1}{2} \|y_i - a^L(x_i)\|^2 \right] + \frac{\lambda}{n} w $$ Now, to your questions: No, batch size $m$ does not affect the strength of regularization, as it does not depend of your training data. You can think of it as of an average of $m$ identical samples with a weight of $\lambda/2n$.
