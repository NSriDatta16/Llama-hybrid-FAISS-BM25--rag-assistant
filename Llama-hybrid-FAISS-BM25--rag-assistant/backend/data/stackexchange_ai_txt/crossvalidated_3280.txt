[site]: crossvalidated
[post_id]: 3280
[parent_id]: 3276
[tags]: 
Ridge regression (Hoerl and Kennard, 1988) was initially developed to overcome singularities when inverting $X^tX$ (by adding $\lambda$ to its diagonal elements). Thus, the regularization in this case consists in working with a vc matrix $(X^tX-\lambda I)^{-1}$. This L2 penalization leads to "better" predictions than with usual OLS by optimizing the compromise between bias and variance (shrinkage), but it suffers from considering all coefficients in the model. The regression coefficients are found to be $$ \hat\beta=\underset{\beta}{\operatorname{argmin}}\|Y-X\beta\|^2 + \lambda\|\beta\|^2 $$ with $\vert\vert\beta\vert\vert^2 = \sum_{j=1}^p\beta_j^2$ (L2-norm). From a bayesian perspective, you can consider that the $\beta$'s must be small and plug them into a prior distribution. The likelihood $\ell (y,X,\hat\beta,\sigma^2)$ can thus be weighted by the prior probability for $\hat\beta$ (assumed i.i.d. with zero mean and variance $\tau^2$), and the posterior is found to be $$ f(\beta|y,X,\sigma^2,\tau^2)=(y-\hat\beta^tX)^t(y-\hat\beta^tX)+\frac{\sigma^2}{\tau^2}\hat\beta^t\hat\beta $$ where $\sigma^2$ is the variance of your $y$'s. It follows that this density is the opposite of the residual sum of squares that is to be minimized in the Ridge framework, after setting $\lambda=\sigma^2/\tau^2$. The bayesian estimator for $\hat\beta$ is thus the same as the OLS one when considering the Ridge loss function with a prior variance $\tau^2$. More details can be found in The Elements of Statistical Learning from Hastie, Tibshirani, and Friedman (ยง3.4.3, p.60 in the 1st ed.). The second edition is also available for free.
