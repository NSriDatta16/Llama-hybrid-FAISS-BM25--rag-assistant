[site]: crossvalidated
[post_id]: 185504
[parent_id]: 185450
[tags]: 
You have matched pairs. That is, you (ultimately) have a correct / wrong classification for each sample. (You will need to get this status for each sample to validly assess the results.) As a result of this fact ( among others ), Fisher's exact test is not appropriate for your data. Instead, you should format your table so that each count indexes the status of a given matched pair. In other words, you would enumerate the number of pairs for which both classifications were correct, only $h_0$ was correct, only $h_1$ was correct, and where both classifications were wrong. In the end, the sum of the counts in the table will be half of your current table. For example, your table might end up looking like this (totally made up): tab = as.table(matrix(c(400, 150, 50, 100), nrow=2, byrow=2)) rownames(tab) You can assess if the heuristics differ in predictive accuracy using McNemar's test. (I have explained McNemar's test here and here .) mcnemar.test(tab) # McNemar's Chi-squared test with continuity correction # # data: tab # McNemar's chi-squared = 49.005, df = 1, p-value = 2.553e-12 Let me note that there may be a better way to evaluate these heuristics. Many models (e.g., logistic regression) will output a continuous value, which people then dichotomize to generate the predicted classifications. If your case is like this, you would do better to compare the outputted continuous values, rather than the classifications. Two possibilities would be to compare the area under the Receiver Operating Characteristic (ROC) curve (see here and here , e.g.), or to compare a proper score (such as the Brier score ).
