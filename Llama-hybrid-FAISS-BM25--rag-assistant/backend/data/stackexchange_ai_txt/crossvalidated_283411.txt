[site]: crossvalidated
[post_id]: 283411
[parent_id]: 272411
[tags]: 
There are already some good answers here explaining the general point, I'll just add two more points: The overfitting of the empirical risk is especially prominent in cases of a small training set. When the data don't contain enough information to learn the underlying pattern, more regularization is needed to fill in the gap. In the specific case of the Deep learning the case is not so clear. Especially with very large nets, it is virtually impossible to find the global minimizer of the loss function, which likely corresponds to the heavily overfitted case. See AISTATS 2015 paper by Choromanska et al. for details [1]. Moreover, there are various interesting works studying the memorization behavior of the deep nets, such as [2] and [3]. Their implication is that even though deep nets have certainly the capacity to overfit terribly, in practice they generalize well even when trained with the unregularized empirical risk. [1] https://arxiv.org/abs/1412.0233 [2] https://openreview.net/forum?id=rJv6ZgHYg&noteId=rJv6ZgHYg [3] https://arxiv.org/abs/1611.03530
