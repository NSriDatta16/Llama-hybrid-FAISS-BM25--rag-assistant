[site]: crossvalidated
[post_id]: 533750
[parent_id]: 
[tags]: 
Typo in Equation 7.122 in Pattern Recognition and Machine Learning

In Pattern Recognition and Machine Learning the author calls Equation 7.122 in Ch 7.2.3 the log likelihood when it is actually the likelihood. I couldn't find this listed in any errata online. I've outlined why below but beware it is kind of long and blatantly obvious so I guess I'm more seeking to document this than asking for clarification as I couldn't find this in any errata or on Google. You have a $K$ class classification problem with targets $t_{nk} \in \{0,1\}$ with a $1$ -of- $K$ encoding scheme (only one of the $K$ targets $t_{nk}$ is one and the rest are zero for given $n$ ) where the class probabilities are given by $y_k(x)=Softmax(w_k^T x)$ . The probability of class $k$ for training data point $x_n$ is given by $y_{nk}=y_k(x_n)$ i.e we have; $$p(t_{nk}|x_n)=y_{nk}=y_k(x_n)$$ Define a vector of all the target values, $t_{nk}$ , for observation $x_n$ ; $$\textbf{t}_n=\begin{bmatrix} t_{n1} & t_{n2} & ... & t_{nK}\\ \end{bmatrix}^T$$ then $$p(\textbf{t}_n|x_n)=\prod_{k=1}^K y_{nk}^{t_{nk}}$$ since the $t_{nk}$ have a $1$ -of- $K$ encoding. Then $T$ is a matrix whose $n^{th}$ row is $\textbf{t}_n^T$ . i.e. the $(n,k)^{th}$ element of $T$ is $T_{nk}=t_{nk}$ . Note I omitted the dependence on the weight vectors previously. Then the likelihood is given by; $$p(T|w_1,...,w_K)=\prod_{n=1}^N p(\textbf{t}_n|x_n)=\prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_{nk}}$$ which is the same as 7.122 and is the likelihood not the log likelihood.
