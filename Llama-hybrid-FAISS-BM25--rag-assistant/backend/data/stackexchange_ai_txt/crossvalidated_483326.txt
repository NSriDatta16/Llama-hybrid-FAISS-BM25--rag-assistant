[site]: crossvalidated
[post_id]: 483326
[parent_id]: 
[tags]: 
Should I add a new difference(z = x1-x2) feature into model?

Recently, I am thinking about this question:should I add new features based on raw features' differences? Setting Suppose I have 50k data and 20 features and it's a regression task. In data science practice, we usually add new features based on raw features. However, I don't know when we should add a new feature z (z = x1 - x2) into our model . My Throughts Here is my understanding:since feature is going to be dumped in models, so whether a feature works fine depends on both feature and model. Let's take linear regression as an example: head(mtcars) fit1 = lm(mpg~ cyl+disp +hp +vs, data = mtcars) summary(fit1)$adj.r.squared data_add = cbind(mtcars,'c1' = mtcars $disp - mtcars$ hp) fit2 = lm(mpg~ cyl+disp + hp +vs + c1, data = data_add) summary(fit2)$adj.r.squared summary(fit2) add_noise $disp - mtcars$ hp + rnorm(nrow(mtcars),0,1)) fit3 = lm(mpg~ cyl+disp + hp +vs + c1, data = data_add_noise) res = c(res,summary(fit3)$adj.r.squared) } return(mean(res)) } add_noise(10000) Outputs: > summary(fit1) $adj.r.squared [1] 0.7359967 > summary(fit2)$ adj.r.squared [1] 0.7359967 > add_noise(10000) [1] 0.7359121 In linear regression, if we put z = x1-x2 into our model, we will get a singular design matrix, which means R won't use z to fit coefficients. In other words, new feature z won't give any boost to our model performance. If we use z = x1- x2 + rnorm(n=1,mean = 0,sd = 1) into our model, we will decrease our model performance since we introduce additional noise into our model. However, if we use lgbm/xgboost/rf models, since tree model split nodes based on infomation gain/infomation gain ratio/gini impurity, our new feature z = x1 - x2 may helps with our model performance. Summary Whether we should add our new difference feature(z = x1- x2) into our model depends on model we use. I will be very grateful to see any other ideas!
