[site]: datascience
[post_id]: 34299
[parent_id]: 34297
[tags]: 
In general, it is very hard to prove a negative. So, concluding that a result cannot be somehow improved with some unspecified technique tends to be almost impossible. Often the best one can do is to follow state-of-the-art good practices and try out several sensible approaches. Then it is at least unlikely that the results can be easily improved. For example, using p-value cut-offs from univariate regressions to eliminate variables and using stepwise model selection is known to be a poor practice, so I would assume that one could improve over your current results. For example, without knowing more details of your data, I would expect that elastic net without pre-screening variables with the hyperparameters selected using cross-validation (within the training set) would perform better on a hold-out test set in many cases. Additionally, if you primarily care about predictive performance then linear regression is unlikely to achieve the best predictive performance. Some form of boosted tree or similar approach will likely be substantially better, variables may tend to be non-linearly related to the outcome. With some subject matter knowledge/good feature engineering you may also achieve pretty good results with penalized (i.e. ridge/elastic net or similar) linear regression - but perhaps the outcome needs to be transformed (e.g. to the log-scale), perhaps some predictors need transforming (e.g. also to the log-scale) and perhaps we can expect certain interactions to be potentially important. It is also not clear how you are evaluating performance (cross-validation? a hold-out set?). Evaluating model performance on data that directly entered the modeling is likely to be overoptimistic, particularly when you have many potential predictors and presumably not a lot of data. You mentioned a split, which I speculated might be a train-test split, but then it seemed like you kept changing the split to find good performance metrics. If you do that, you can no longer trust the resulting performance metrics.
