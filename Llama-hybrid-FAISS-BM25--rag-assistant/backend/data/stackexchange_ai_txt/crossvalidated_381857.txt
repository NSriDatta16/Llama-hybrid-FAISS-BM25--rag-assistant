[site]: crossvalidated
[post_id]: 381857
[parent_id]: 
[tags]: 
Correct tensor sizes for a pixel based classification of multidimensional satellite imagery with neural network

I've successfully worked through a few samples that take an entire image and apply one class to it. I want to classify each pixel in an image. I keep getting hung up when attempting to calculate loss. I have a single 10 band satellite image tiled into 128x128 images. The last band is the classification so 9, 128, 128 are inputs and 1, 128, 128 are labels. I'm using a batch of 16 and 8 classes that fall between 0 and 255. criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) Simple test model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Linear(147456, 500) self.fc2 = nn.Linear(500, 8) #Not sure 8 is correct def forward(self, x): x = x.view(-1, 147456) x = F.relu(self.conv1(x)) return self.fc2(x) Here is the loop in my train method for idx, data in enumerate(tqdm(train_loader, desc="training")): inputs = Variable(data['sat_img'].cuda()) labels = Variable(data['map_img'].cuda()) # zero the parameter gradients optimizer.zero_grad() # forward outputs = model(inputs) print('inputs', inputs.shape) print('labels', labels.shape) print('outputs', outputs.shape) loss = criterion(outputs, labels.long().view(-1)) #CRASH HERE The loop above crashes when I call loss. The print out of shapes is: inputs: torch.Size([16, 9, 128, 128]) labels: torch.Size([16, 128, 128]) outputs: torch.Size([16, 8]) Error: Expected input batch_size (16) to match target batch_size (262144). Question: CrossEntropyLoss expects a tensor of size C which are the weights for each of my 8 classes. Currently the shape of that is 16 batches, 8 classes. It also expects a batch size of correct answers. But I have 128x128 answers and I can't use a 2D array. Should I use a different criterion? Thanks
