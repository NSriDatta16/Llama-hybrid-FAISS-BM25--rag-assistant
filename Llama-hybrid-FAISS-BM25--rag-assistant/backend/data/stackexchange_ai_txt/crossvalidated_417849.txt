[site]: crossvalidated
[post_id]: 417849
[parent_id]: 417835
[tags]: 
The importance of features is a data-based thing. It does not directly depend on the model you prepared. However, some "importance" can be skipped or ignored by the model due to his nature. If your xgboost model recognized some features as important you should expect this feature would be important for other models. However if the nature of your DNN model is much different from xgboost, i.e. its convolution or recurrent, attention-based, you should NOT assume that feature that was not important for the xgb would not be important for DNN. You can expect that the transfer of feature importance between models will be more accurate if your models are similar in the analytical way, with a strong emphasis on the dynamic or static nature of model dichotomy and topology-based dichotomy.
