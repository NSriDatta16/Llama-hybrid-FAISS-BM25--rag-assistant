[site]: crossvalidated
[post_id]: 442964
[parent_id]: 
[tags]: 
Best Linear Predictor Assumptions

Chapter 6 of Plane Answers to Complex Questions, 4th Edition motivates linear regression through the lens of best prediction. Unfortunately, I'm confused about some assumptions in deriving the best linear predictor. It's not too hard to show $\hat y(x):=\mathbb E[y|x]$ is the best predictor with respect to the mean squared error, but since this is hard to compute in general, the author restricts attention to a linear approximation of $\hat y(x)$ . The linear approximation only requires knowledge of means, variances, and covariances of $x$ and $y$ . Let $V_{xx}:=\text{cov}(x)$ and $V_{xy}:=\text{cov}(x,y)$ . The author starts, Let $\beta_*$ be a solution to $V_{xx}\beta=V_{xy}$ , then we will show that the function $$ \mu_y+(x-\mu_x)^\top\beta_* $$ is the best linear predictor of $y$ based on $x$ . Why is the initial line "Let $\beta_*$ be a solution to $V_{xx}\beta=V_{xy}$ " required? This implies $\mathscr C(V_{xy})\subseteq\mathscr C(V_{xx}),$ but I don't see what this provides.
