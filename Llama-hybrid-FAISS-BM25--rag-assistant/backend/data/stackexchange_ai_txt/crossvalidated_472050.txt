[site]: crossvalidated
[post_id]: 472050
[parent_id]: 472032
[tags]: 
Here is an example of how you might use the bootstrap to estimate confidence intervals. import numpy as np import matplotlib.pyplot as plt from sklearn.utils import resample as bootstrap from scipy.optimize import curve_fit %matplotlib inline def f(x,A,B): return np.exp(-A*x) - np.exp(-B*x) #real data x = np.linspace(0,10,25) y = np.exp(-0.5*x) - np.exp(-0.75*x) + np.random.normal(0, 0.012, size = x.size) #confidence intervals nboot = 100 bspreds = np.zeros((nboot, y.size)) for i in range(nboot): xb,yb = bootstrap(x,y) params, cov = curve_fit(f,xb,yb, bounds = ((0, 0), (np.inf, np.inf))) bspreds[i] = f(x,*params) params, cov = curve_fit(f,x,y, bounds = ((0, 0), (np.inf, np.inf))) fig, ax = plt.subplots(dpi = 120) plt.plot(x, bspreds.T, color = 'C0', alpha = 0.05) plt.plot(x,f(x,*params), color = 'C0') plt.plot(x, f(x, 0.5, 0.75), color = 'red') The lines in light blue are the bootstrap curves, the dark blue is the fit from the real data, and the red is the actual curve (noiseless). You can take the pointwise quantiles to get interval estimates. Prediction is a little more nuanced. In order to get a prediction interval, you need some sort of assumption about how the data gave rise. In my example, I used least squares, which is equivalent to assuming that the data have Gaussian noise. That isn't always the case, and so some thought really needs to go into this question. IN any case, once you decide on a likelihood (the thing that generates the data), then you can do something like a parametric bootstrap. ypred = f(x,*params) noise = np.std(y - ypred) predictions = np.array([np.random.normal(ypred,noise) for j in range(10_000)]) u,l = np.quantile(predictions, [0.025, 0.975], axis = 0) fig, ax = plt.subplots(dpi = 120) plt.plot(x, ypred, color = 'red') plt.plot(x, u, 'C0--') plt.plot(x, l, 'C0--') There is of course the Bayesian method, which makes both of these appraoches very easy and natural. I'm going to omit that approach for now because it necessitates priors. So what is happening here? What are these two techniques doing? Confidence intervals are meant to convey uncertainty in the parameters . That is very different than uncertainty in the outcome (which we will get to in a moment). If I was able to resample my data from whatever phenomenon that generated it, I could naturally estimate the variability in the parameters. After all, functions of random variables are themselves random, so there is no reason to believe I would estimate the same set of parameters even though I am using data from the same phenomenon. For obvious reasons, we can't collect data in this fashion, so we do the next best thing. We resample the data we have, with replacement, and refit our models. The variation that arises in the parameters from this technique is meant to approximate the variation I would see were I to resample. That variation is passed through the non-linearities, resulting in what you see. Each line corresponds to a resampled dataset. We can conclude from this that the true noiseless outcome is somewhere within that band of light blue (I'm being very fast and loose here). Prediction intervals on the other hand reflect uncertainty in the outcome. Given that I know the true mean outcome, where would I probably see my next observation? That requires knowledge of the data generating process (as I alluded to). In the example I show, I've fixed the mean outcome as if it were truly known. We don't actually know the true outcome, so a better prediction interval would come from bootstrapping the mean outcome and then performing another bootstrap on top of that in order to generate predictions from the model. Then, take quantiles as I have done. If I'm not mistaken, the bootstrap has a very deep connection to the Bayesian approach, but I'm not prepared to speak about that at this time. If you have further questions, I'd be happy to answer them.
