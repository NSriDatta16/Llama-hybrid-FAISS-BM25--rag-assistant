[site]: crossvalidated
[post_id]: 567567
[parent_id]: 
[tags]: 
White noise in time series models

In virtually every time series model out there (e.g. AR(1)), there is always the existence of a white noise term $\varepsilon_{t}$ like $y_{t} = \delta + \phi y_{t-1} + \varepsilon_{t}$ . I don't understand why its almost obligatory to stick in a 'white noise' term after every single time series model ever. For instance, if we fit an AR(1) model and try to forecast the next period, the conditional mean $E[y_{t+1}|y_{t}]$ gets rid of the white noise altogether, and for a sufficiently long forecast horizon we converge to the long-run mean $\frac{\delta}{1-\phi}$ . I know I'm missing something quite fundamental here, but now it just seems like the sole purpose of $\varepsilon_{t}$ existing is for us to eventually cancel it out with an expectation operation. Why do we even care about including it? My personal take on this is: We have $y_{t}$ being the actual process that we don't know with certainty (hence there is noise $\varepsilon_{t}$ ), and the best we can do is to model the 'non-noise' portion as e.g. an AR(1), giving us $\hat{y_{t}} = \delta + \phi y_{t-1}$ , and the reason for using white noise in particular, is because it is simply a convenient choice (i.e. it is an i.i.d process and hence adding it to the model does not violate the time invariant requirement for all time series models) Is this a correct explanation?
