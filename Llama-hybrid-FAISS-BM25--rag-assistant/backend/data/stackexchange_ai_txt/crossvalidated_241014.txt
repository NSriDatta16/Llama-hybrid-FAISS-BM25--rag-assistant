[site]: crossvalidated
[post_id]: 241014
[parent_id]: 
[tags]: 
Gated Softmax Classification

Exploring the concept of Gated Classification , I came across this page (and paper): http://www.cs.toronto.edu/~rfm/gatedsoftmax/index.html It states that- "An equivalent view of the model is a log-bilinear classifier (or bilinear logistic regression), whose hiddens may be viewed "style" variables that capture within-class variability." I am trying to understand this view of Gated Classification. As a simple motivating example, I have two sets of data which are being used to learn a classification rule. Two different models $m_k, k=2$ are fitted, let's say using a logistic classifier, each having its own (and different) set of inputs. I want to apply the learnt decision rule (estimated model parameters) to an out of sample population. I can test each model separately, which will give me different proability distributions (probability of observing $y=[0,1]$) $p_k$ for each of the two models. Now one model may work better over some population than the other. And this distinction is not apparent, which I wish to explore by feeding the probability distributions, to another classifier of the form: $$ ln (\frac{E(y)}{1-E(y)}) = \beta_0 + \beta_1*l_1*p_1 + \beta_2*l_2*p_2$$ where $l_k$ will be a set of categorical variables that capture the linear separation between the probability distributions. From what I understand so far, the "bilinear" effect is captured by the interaction of $l_k$ with $p_k$. Yet, I am not certain of my interpretation of this "log-bilinear classifier" in this simple example.
