[site]: crossvalidated
[post_id]: 330979
[parent_id]: 330491
[tags]: 
Is it correct to say that in a deterministic environment, planning in Tabular Dyna-Q is a form of experience replay ? I would say that it's not entirely correct to say this, only because the terms "Experience Replay" and "Dyna-Q" are well-understood as referring to specific implementations. It is true that in the specific situation you describe ( tabular RL in deterministic environments), they end up doing similar things. However, they still do these similar things using different implementations, which may create subtle differences in practice. For example, the two ideas probably have different memory requirements. For this reason, I don't think it's ever correct to use one term when the other is meant, even though they are very close to each other in this situation. The following is a quote from the Conclusion of "Reinforcement Learning for Robots Using Neural Networks" (1993), Long-Ji Lin's dissertation. This is one of the first sources of Experience Replay. Throughout the entire document, Experience Replay and Dyna are consistently treated as different ideas, but indeed with many similarities: This dissertation proposed a technique called experience replay. This technique in effect takes advantage of a model, but does not have the difficult problem of building a model, because the model is simply the collection of past experiences. So the important distinction really isn't in what they accomplish, but how they do it. Once you move beyond the setting you described ( Function Approximation instead of tabular, and/or nondeterministic instead of deterministic), you'll see more apparant differences.
