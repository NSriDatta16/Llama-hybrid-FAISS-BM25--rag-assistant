[site]: crossvalidated
[post_id]: 109385
[parent_id]: 109382
[tags]: 
A lot of validation measures are usually 'averaged' over the whole partition to allow for direct comparison on other data sets. For example, mean square error for continuous prediction is $$ \text{MSE} = \frac{1}{N} \sum_{i=1}^N (\text{prediction}_i - \text{actual}_i)^2. $$ Other examples are MAE, area under the ROC curve, Brier score, R squared (generalized or otherwise). In this way, if the difference in size between partitions is small (say, within a few) then you shouldn't be worried about any kind of imbalance. In your case for 10-fold CV, (after shuffling the data to ensure random assignment) I would take $\lfloor 2901/10 \rfloor = 290$-sized partitions and give the last data point to any of the partitions. It won't matter which. You can always make your validation measure size-independent by dividing by the partition size. For example, if you calculate logarithmic scoring in total, then just divide the total score by the partition size.
