[site]: crossvalidated
[post_id]: 451586
[parent_id]: 
[tags]: 
How to properly upscale images using deconvolutions

I have a dataset that has 32x32x3 images. However, I want to use models that were developed for 224x224x3 images, e.g. resnet. A common theme I see is that people resize 32x32x3 to 224x224x3, but this means a CNN is scanning large segments of identical and redundantly colored space. Conceptually, this is not ideal. One idea is to put a few layers at the beginning of the network, allowing the network to learn a mapping from 32x32x3 to 224x224x3. I have read a little bit about deconvolutions and think that might be the answer. What is a reasonable set of deconvolution layers to go from 32x32x3 to 224x224x3?
