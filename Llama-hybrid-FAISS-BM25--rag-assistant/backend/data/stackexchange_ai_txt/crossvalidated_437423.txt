[site]: crossvalidated
[post_id]: 437423
[parent_id]: 434553
[tags]: 
This problem has often been discussed; one keyword is comparision of "centroid-rotation" vs. "pc-rotation", another one is "parcels" vs. "item-factor-analysis" (or so, I may not be up-to-date). The computation of the items-mean is here in principle equivalent with determining the "centroid", and if multiple means are taken from multiple subsets of items, then this is in principle "parceling". When items are strongly anticorrelated, then the centroid (=mean) item the positive- and negative values in the data extinguish each other and the data of the centroid have small variance and are thus not well configured to determine the "main-direction" of the factor. Opposite to this, PCA handles anticorrelated data in the same way as correlated data, and in such cases should be the better option. Here is a small protocol with random data based on 4 items, 2 relevant factors and 4 itemspecific errors. All items load strongly on the first factor, but the second pair with negative loading. This should spoil the "mean"/"centroid"-solution construction table "ULAD" for 4 items from 2 "relevant" factors and 4 "itemspecific" factors The factor-loadings are unstandardized, so the variance in the items is <>1 See the anti-correlated loadings on the first random factor "f 1" between pairs of items: ULAD | f 1 | f 2 | err 1 | err 2 | err 3 | err 4 ------------------------------------------------------------ it 1 | 1.0000 | 0.1000 | 0.3000 | 0.0000 | 0.0000 | 0.0000 it 2 | 1.0000 | 0.3000 | 0.0000 | 0.4000 | 0.0000 | 0.0000 it 3 | -1.0000 | 0.2000 | 0.0000 | 0.0000 | 0.1000 | 0.0000 it 4 | -1.0000 | -0.2000 | 0.0000 | 0.0000 | 0.0000 | 0.4000 [22] data = ulad*ufac // generate "empirical" data from random vectors("ufac") // We have n=1000 cases from random-generator with // normal distribution in datamatrix "UFAC" The data-items get their mean-vector appended as 5'th data-vector: [23] data = {data,meansp(data)} // append to data-matrix the columnwise means // as new data-vector Show covariances: [24] cov=data *' /n // operator *' makes dotproduct with transpose. COV | it 1 | it 2 | it 3 | it 4 | mean ------------------------------------------------------ it 1 | 1.1341 | 1.0229 | -1.0097 | -1.0299 | 0.0293 it 2 | 1.0229 | 1.2059 | -0.9346 | -1.0391 | 0.0638 it 3 | -1.0097 | -0.9346 | 1.0742 | 0.9615 | 0.0228 it 4 | -1.0299 | -1.0391 | 0.9615 | 1.1754 | 0.0170 ------------------------------------------------------ mean | 0.0293 | 0.0638 | 0.0228 | 0.0170 | 0.0332 Note the small variance in the "means"-vector! Show correlations: [30] R=covtocorr(cov) R | it 1 | it 2 | it 3 | it 4 | mean ------------------------------------------------------ it 1 | 1.0000 | 0.8747 | -0.9148 | -0.8920 | 0.1511 it 2 | 0.8747 | 1.0000 | -0.8211 | -0.8728 | 0.3187 it 3 | -0.9148 | -0.8211 | 1.0000 | 0.8556 | 0.1209 it 4 | -0.8920 | -0.8728 | 0.8556 | 1.0000 | 0.0858 ------------------------------------------------------ mean | 0.1511 | 0.3187 | 0.1209 | 0.0858 | 1.0000 We do solution for principal components. The program allows, to do the rotation taking respect only for selected subsets of items [35] lad=cholesky(R) [36] pc=rot(lad,"pca",1..4) // rotate to pc only using items 1..4 for criterion PC | pc1 | pc2 | pc3 | pc4 | pc5 ------------------------------------------------------ it 1 | 0.9685 | -0.1018 | -0.0434 | 0.2231 | 0.0000 it 2 | 0.9379 | 0.2918 | -0.1801 | -0.0528 | 0.0000 it 3 | -0.9445 | 0.2881 | 0.0656 | 0.1437 | 0.0000 it 4 | -0.9521 | -0.1019 | -0.2866 | 0.0323 | 0.0000 ------------------------------------------------------ mean | 0.0689 | 0.5488 | -0.6674 | 0.4986 | 0.0000 We see, that the mean-vector is completely uncorrelated to the first principal component of the first four items. This is, because their main loadings neutralize for the centroid. Let's look at it from the view, whether the "means"-vector capture the principal component, when taken as leading item: [149] pca=rot(lad,"pca",5) // use "means"-vector alone for first pc [150] pca=rot(pca,"pca",1..4,2..5) // use it1-it4 for 2'nd,3'rd,4'th,5'th pc PC | pc1 | pc2 | pc3 | pc4 | pc5 ------------------------------------------------------ it 1 | 0.1511 | -0.9606 | 0.1497 | -0.1791 | 0.0000 it 2 | 0.3187 | -0.9179 | -0.1555 | 0.1781 | 0.0000 it 3 | 0.1209 | 0.9552 | -0.2437 | -0.1167 | 0.0000 it 4 | 0.0858 | 0.9601 | 0.2435 | 0.1071 | 0.0000 ------------------------------------------------------ mean | 1.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 The centroid, which we find indicated by "mean", does in no way collect the main direction or the main variance in the model. The with the mean uncorrelated second pc (first pc of the remaining variance of the items 1-4) has much more weight and represents the factor loadings much more reliable, with which the model was created. Now we do the same analysis, but we create the four items as parallel (and positively) correlated to the first random factor: ULAD | f 1 | f 2 | err 1 | err 2 | err 3 | err 4 ------------------------------------------------------------ it 1 | 1.0000 | 0.1000 | 0.3000 | 0.0000 | 0.0000 | 0.0000 it 2 | 1.0000 | 0.3000 | 0.0000 | 0.4000 | 0.0000 | 0.0000 it 3 | 0.6000 | 0.6000 | 0.0000 | 0.0000 | 0.1000 | 0.0000 it 4 | 0.5000 | -0.8000 | 0.0000 | 0.0000 | 0.0000 | 0.4000 Generate data with the same random-vectors as before. Append "means"-vector as before. Show covariances: [80] cov=data *' /n COV | it 1 | it 2 | it 3 | it 4 | mean ------------------------------------------------------ it 1 | 1.1341 | 1.0229 | 0.6504 | 0.4800 | 0.8218 it 2 | 1.0229 | 1.2059 | 0.7447 | 0.2962 | 0.8174 it 3 | 0.6504 | 0.7447 | 0.6977 | -0.1504 | 0.4856 it 4 | 0.4800 | 0.2962 | -0.1504 | 1.0837 | 0.4274 ------------------------------------------------------ mean | 0.8218 | 0.8174 | 0.4856 | 0.4274 | 0.6380 See the better variance in "means"-vector! Show correlations: [85] R=covtocorr(cov) R | it 1 | it 2 | it 3 | it 4 | mean ------------------------------------------------------ it 1 | 1.0000 | 0.8747 | 0.7312 | 0.4330 | 0.9661 it 2 | 0.8747 | 1.0000 | 0.8119 | 0.2591 | 0.9319 it 3 | 0.7312 | 0.8119 | 1.0000 | -0.1730 | 0.7278 it 4 | 0.4330 | 0.2591 | -0.1730 | 1.0000 | 0.5139 ------------------------------------------------------ mean | 0.9661 | 0.9319 | 0.7278 | 0.5139 | 1.0000 Show PC-solution: [90] lad=cholesky(cor) [91] pc=rot(lad,"pca",1..4) PC | pc 1 | pc 2 | pc 3 | pc 4 | pc 5 ------------------------------------------------------ it 1 | 0.9578 | -0.1621 | 0.1929 | -0.1383 | 0.0000 it 2 | 0.9638 | 0.0357 | -0.2590 | -0.0526 | 0.0000 it 3 | 0.8547 | 0.4828 | 0.0771 | 0.1743 | 0.0000 it 4 | 0.3089 | -0.9446 | -0.0035 | 0.1108 | 0.0000 ------------------------------------------------------ mean | 0.9746 | -0.2233 | -0.0057 | 0.0175 | 0.0000 Here we see, that - as hoped - the "means"-vector indeed roughly captures the "PC-1" factor, meaning the first principal component. Again we change perspective: [178] pca=rot(lad,"pca",5) \\ first pc from "means"-vector only [179] pca=rot(pca,"pca",1..4,2..5) \\ remaining pc's from "items"-residuals \\ only PC | pc1 | pc2 | pc3 | pc4 | pc5 ------------------------------------------------------ it 1 | 0.9661 | 0.0570 | 0.1991 | -0.1539 | 0.0000 it 2 | 0.9319 | 0.2500 | -0.2537 | -0.0690 | 0.0000 it 3 | 0.7278 | 0.6608 | 0.0792 | 0.1653 | 0.0000 it 4 | 0.5139 | -0.8523 | 0.0004 | 0.0976 | 0.0000 ------------------------------------------------------ mean | 1.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 We see, that the factor which is detected by the "mean"-vector, captures well the first pc of the "item"s. Things are a bit more complicated when more items and more relevant principal components (or factors, in case one does a factor-analysis and not simply PCA) are in the game and subsets of items get "parceled" and are replaced by their respective "means"-vector. About the details with the problems of the "parceling" you must read elsewhere, I think there's a lot of discussion and study online around.
