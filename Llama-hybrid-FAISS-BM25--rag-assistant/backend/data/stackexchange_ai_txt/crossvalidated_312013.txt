[site]: crossvalidated
[post_id]: 312013
[parent_id]: 
[tags]: 
Which likelihood and prior corresponds to which loss and regularization?

In Bayesian linear regression having the objective $$\min_w \underbrace{\sum_{i=1}^N (w^Tx_i - y_i)^2}_{\text{log-likelihood}} + \underbrace{\lambda ~ w^Tw}_{\text{log prior distribution}}$$ can be seen this as the maximization of the log posterior distribution. The log posterior distribution then falls apart into two parts: A part corresponding to the log likelihood and a part corresponding to the log of the prior distribution. In the above case we made the assumptions (1) That the model is given by $y = w^Tx + \epsilon$ with $\epsilon$ being white noise, leading to the specific form of the log-likelihood (squared error). (2) That the prior is given by a 0-centered Gaussian, leading to the specific form of the regularization term ($L_2$ regularization) My Question is: Does anyone know anymore corresponding pairs like this (likelihood = loss function or prior = regularization)? Is there maybe some list of well known connections somewhere? And more generally: Given an objective loss and regularization function, when can I find a fitting likelihood and prior distribution, s.t. the maximization of the posterior distribution corresponds to my objective.
