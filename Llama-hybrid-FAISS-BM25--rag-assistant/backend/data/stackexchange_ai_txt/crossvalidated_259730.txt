[site]: crossvalidated
[post_id]: 259730
[parent_id]: 
[tags]: 
Derivation of Restricted Boltzmann Machine Conditional Probability

I was reading Goodfellow, Bengio, and Courville's Deep Learning and in Chapter 20 there is a derivation of the conditional probability $P(h_j=1|\boldsymbol{v})$, the conditional probability of the $j^{th}$ hidden unit being on given a data vector $\boldsymbol{v}$: Note that $Z'$ is a normalizing constant, and $\tilde{P}(E)$ is the unnormalized probability of event $E$. The derivation of 20.7 through 20.11 is clear to me, but for equation 20.13, we assume the inside of our product, $\exp{\{c_j h_j + \boldsymbol{v}^\top \boldsymbol{W}_{:,j} \boldsymbol{h}_j\}}$, is exactly equal to our (unnormalized) unit probability $\tilde{P}(h_j|\boldsymbol{v})$. (This is described in the paragraph between 20.11 and 20.12, and we evaluate this $\tilde{P}(h_j|\boldsymbol{v})$ for $h_j = 0$ and $h_j = 1$ to get equation 20.13.) My question is, why can we make this assumption? The argument the paper seems to make is that because $P(\boldsymbol{h}|\boldsymbol{v})$ takes on the form $\frac{1}{Z'} \prod_{j=1}^{n_h} f(h_j)$, we can safely assume $f(h_j)$ must be our probability for each unit, $\tilde{P}(h_j|\boldsymbol{v})$. I understand that $P(\boldsymbol{h}|\boldsymbol{v})$ must equal $\frac{1}{Z'} \prod_{j=1}^{n_h} \tilde{P}(h_j|\boldsymbol{v})$, but if we are given the equation $P(\boldsymbol{h}|\boldsymbol{v}) = \frac{1}{Z'} \prod_{j=1}^{n_h} f(h_j)$, couldn't there be several functions $f$ that fit this criterion? $f$ wouldn't necessarily have to be $\tilde{P}(h_j|\boldsymbol{v})$, would it?
