[site]: crossvalidated
[post_id]: 576179
[parent_id]: 576145
[tags]: 
A good model selection question. Because Grid Search would already return the optimum hyperparamaters without users having to adjust the hyperparameters again. A grid type search for finding out best performing hyper-parameters qualifies as tuning, either values are selected on the grid or with an auxiliary algorithm, i.e., a Bayesian optimisation or random search methods. that makes the final left out test set redundant. No. Because we still need to select the "best" hyper-parameters on the grid and having a separate validation set for this purpose will reduce overtraining, i.e, in the sense that we shouldn't rely on the same dataset for both model parameters and hyper-parameters. Even for using grid-search, it is generally recommended to split training-validation sets and use the test for measuring the learning performance. A great paper discussing the importance of keeping validation set could guide here us a bit better from Manchester & Liverpool teams: On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning Yun Xu & Royston Goodacre Journal of Analysis and Testing volume 2, pages 249â€“262 (2018) doi Figure 1 from the paper:
