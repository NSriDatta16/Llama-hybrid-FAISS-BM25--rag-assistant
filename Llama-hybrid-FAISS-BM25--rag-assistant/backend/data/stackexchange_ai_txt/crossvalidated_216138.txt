[site]: crossvalidated
[post_id]: 216138
[parent_id]: 
[tags]: 
How is the linear SVM's cost function derived from the Lagrangian (for the primal form)?

This is the cost function that seems to be used for training linear SVM classifiers: $J(\mathbf{w}, b) = \dfrac{1}{2} \mathbf{w}^T \cdot \mathbf{w} \quad + \quad C {\displaystyle \sum\limits_{i=1}^{m}max\left(0, 1 - y_i (\mathbf{w}^T \cdot \mathbf{x}_i + b)\right)}$ I can see why it would work, but I am wondering how it was derived from the linear SVM constrained optimization problem (primal form)? $\underset{\mathbf{w}, b}{\operatorname{minimize}}\quad{\frac{1}{2}\mathbf{w}^t \cdot \mathbf{w}} \\ \text{subject to} \quad t^{(i)}(\mathbf{w}^t \cdot \mathbf{x}^{(i)} + b) \ge 1 \quad \text{for } i = 1, 2, \cdots, m$ I suppose it comes from the generalized Lagrangian? $\mathcal{L}(\mathbf{w}, b, \mathbf{\alpha}) = \frac{1}{2}\mathbf{w}^t \cdot \mathbf{w} - \sum\limits_{i=1}^{m}{\alpha^{(i)} \left(t^{(i)}(\mathbf{w}^t \cdot \mathbf{x}^{(i)} + b) - 1\right)} \\ \text{with}\quad \alpha^{(i)} \ge 0 \quad \text{for }i = 1, 2, \cdots, m$ But I don't see how you go from this equation to the hinge loss function $max(0, 1-t)$. Any idea?
