[site]: datascience
[post_id]: 87453
[parent_id]: 77341
[tags]: 
There is not much theoretical work on the topic. The number of dimensions for an embedding is an empirical question depending on the corpus and the task. The " GloVe: Global Vectors for Word Representation " paper by Pennington showed that there are large gains in accuracy by adding dimensions up to couple of hundred then diminishing returns. The most common number of dimensions is either 128 or 256. 128 and 256 are both powers of 2 which can speed up training time.
