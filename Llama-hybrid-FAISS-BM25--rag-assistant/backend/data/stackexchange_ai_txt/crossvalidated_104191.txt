[site]: crossvalidated
[post_id]: 104191
[parent_id]: 
[tags]: 
Logistic Regression doesn't converge if I use a particular baseline

Apologies if the question is too broad, then please close the thread. Or maybe this belongs in StackOverflow? I observed something strange and wonder if someone more educated can shed some light on the issue. I don't think it's necessary to go into a ton of detail (i.e., providing code and data). I'm running a logistic regression in R using glm. I have some dataset where about 90% of my dependendent variables = 1. There are also 15 independent variables. About 10 of the variables are factors with 3 levels: "Yes", "No", "No Data" (not NA, to prevent exclusion). Overwhelmingly most of the data (80%+) for these factor variables is "Yes", but the default baseline for those independent variables was "No Data." With this configuration, the regression converges. Also, if I use "No" as the baseline, the regression converges. If I change the baseline for those variables to "Yes," the regression no longer converges. Any particular reason for this? Is this some property of logistic regression or a result of R code? Is it because my dependent variable has low variance or because the independent variables? I'm curious because ideally we would like faster convergance, and if I knew some properties of logistic regression that prevent convergance, then I could build better regressions and be more confident about the results. For example, I had a similar dataset, and made the same changes (releveling). This regression happened to converge, but it took much more iterations, and some of the results were actually different than before I made the change. So, it seems like important knowledge. Thanks!
