[site]: crossvalidated
[post_id]: 518051
[parent_id]: 
[tags]: 
Show that classification tables do not always correlate with goodness of fit for logistic regression

Background I am reading the textbook Applied Logistic Regression by David Hosmer, specifically chapter 4, which discusses logistic regression model assesment of fit. Hosmer gives an interesting example of how classification tables may not imply goodness of fit for a logistic regression model. Hosmer introduces univariate logistic regression as follows: Given a dichotomous random variable $Y\in\{0,1\}$ , we model the expected value of $Y$ given $X$ using a logistic function. $$E(Y|X) = \pi(x) = \frac{1}{1+e^{-(\beta_0 + \beta_1X)}}$$ and he goes on to define the logit and derive the likelihood function, but I don't think those are needed for the problem below. Problem Suppose that $P(Y=1) = \theta_1$ and $X \sim N(0,1)$ in the group with $Y=0$ and $X \sim N(\mu,1)$ in the group with $Y=1$ . (1) It can be shown that the slope coefficient for the logistic regression model is $\beta_1 = \mu$ , and the intercept is $$ \beta_0 = \ln\left[\frac{\theta_1}{1-\theta_1}\right] - \frac{\mu^2}{2} $$ (2) Then, the probability of misclassification (PMC), may be shown to be $$ \mathrm{PMC} = \theta_1 \Phi\left[\frac{1}{\beta_1}\ln\left(\frac{1-\theta_1}{\theta_1}\right)-\frac{\beta_1}{2}\right] + (1-\theta_1)\Phi\left[\frac{1}{\beta_1}\ln\left(\frac{\theta_1}{1-\theta_1}\right)-\frac{\beta_1}{2}\right] $$ where $\Phi$ is the standard normal CDF. This implies that the misclassification rate depends on the slope of the model rather than the goodness of fit. Discussion Hosmer says that (1) is easy to show, and is shown in an old textbook named Discriminant Analysis by Lachenbruch. I was unable to find a PDF of this textbook. I think to show (1), I should use conditional probability somehow, but I'm not sure. I assume that (2) will follow using similar techniques. How can I show (1) and (2)?
