[site]: crossvalidated
[post_id]: 185053
[parent_id]: 
[tags]: 
Normalizing a large set of ratios

I have a problem that has been causing headaches. To set up the problem, here's what I have. Bear with me, since I'm trying to clarify the problem as much as possible: A large set of personnel These personnel have performance evaluations that are based on a number of criteria with a numerical value of 1 to 5. The individual performance traits are averaged to get the "trait average" Every evaluator (i.e., boss) has his/her own "cumulative average" that is an average of all of the evaluations they have done. This allows us to see how well an employee is doing compared to the boss's historical average. This cumulative average changes over time as the evaluator writes more evaluations. The cumulative average is attached to the individual trait average at the time the report was written. I am trying to compare these personnel across several subunits of the organization who all have different bosses (evaluators) I have a complete dataset of all of their evaluations, and want to use a ratio of the trait average/cumulative average to compare performance. I realize that I need to normalize (or standardize) the data in order to fairly compare people, since the denominator shifts according to an evaluators grading criteria. The personnel have multiple evaluations that all need to be factored in. My question is how to best normalize. Do I need to normalize each individual evaluation trait average, each individual evaluation cumulative average, divide the two, then add the ratios for all the reports for that person and divide by the total number of evaluations for that individual to get an overall average of the ratios? This is my gut feeling, but I am uncertain. Any help would be appreciated!
