[site]: crossvalidated
[post_id]: 441237
[parent_id]: 441205
[tags]: 
Separation poses no problems for estimating a random forest model. The reason separation is a problem for logistic regression is that the likelihood increases as the absolute value of the coefficient increases. Random forests don't share this trait because the model isn't estimated by optimizing an objective function in this manner. All you have to do to train a random forest model is identify splits that improve the classification of the training data. If a split can improve the classification, do it.* If not, you've reached a terminal node. Indeed, if your data has separation, then whenever a perfectly separable feature is among the random set of features selected by the random forest, the model will choose one of the perfectly separable features to split on. This is because a perfectly separable feature always produces pure child nodes, so it maximizes the information gain criterion. * The caveat to this is that some regularization methods will reject splits unless the improvement is at least a certain size, or will reject splits if the child nodes contain too few training samples, or similar strategies for controlling model complexity.
