[site]: crossvalidated
[post_id]: 469194
[parent_id]: 
[tags]: 
Does gradient descent assume updates of one layer/parameter at a time?

I read the following in "Deep Learning", from Goodfellow et al (Chapter 8, page 313) : The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously. When we make the update, unexpected results can happen because many functions composed together are changed simultaneously, using updates that were computed under the assumption that the other functions remain constant. How is that possible? Isn't gradient descent based on the fact that a function decreases fastest if we move in the direction of the negative gradient , and across all weights precisely at the same time ?
