[site]: crossvalidated
[post_id]: 234480
[parent_id]: 210541
[tags]: 
There is a tutorial on scikit-learn, a paper on Facebook, and a topic on Quora, all discussing this approach. Basically, you tend to get slightly better performance, because you are combining predictors with different inherent biases. This approach also allows for online learning, since you can cache the computationally demanding gradient boosting part and update only the logistic regression part. One important thing to keep in mind is that you have to use different subsets of the data for training gradient boosting and for training logistic regression (see the scikit-learn link for an example). Whether the slight increase in performance is worth the added complexity depends on the particular application.
