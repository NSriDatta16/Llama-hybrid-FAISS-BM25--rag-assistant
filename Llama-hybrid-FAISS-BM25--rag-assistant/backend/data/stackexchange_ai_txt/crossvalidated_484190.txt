[site]: crossvalidated
[post_id]: 484190
[parent_id]: 428247
[tags]: 
I haven't tried this before but you could get a thesaurus dataset and use it to look for patterns in antonym pairs. Perhaps there are certain dimensions of the embeddings which correspond to magnitude (e.g. great vs good) while others correspond to semantics (e.g. dogs & cats are both pets). If so in theory you could make (some of) those magnitude dimensions negative & then find neighboring words to automatically find antonyms. It's not perfect but it might be the best option when working with existing embedding models.
