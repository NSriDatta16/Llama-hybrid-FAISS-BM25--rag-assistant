[site]: crossvalidated
[post_id]: 278623
[parent_id]: 
[tags]: 
How does a Generator (GAN) create samples similar to the data space from a vector of random numbers?

The adversarial loss function and training algorithm for GANs are more or less intuitive to me, but I feel I do not entirely understand the Generator's domain. Specifically, the following line from the original paper ( Generative Adversarial Nets, Goodfellow et al. 2014) To learn the generator's distribution $p_g$ over data $x$, we define a prior on input noise variable $p_z(z)$, then represent a mapping to data space as $G(z;\theta_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\theta_g$. Where this is the first step in the training algorithm: Sample minibatch of $m$ noise samples $[{z^{(1)},...,z^{(m)}}]$ from noise prior $p_g(z)$ I am not sure I understand probability distribution and random sampling enough to understand this part. Could someone explain what is going on? It seems like everyone skips over the very beginning of the GAN algorithm (given random vector Z, do stuff) and just talk about the adversarial loss. Does the generator sample from a probability distribution that was assigned in the beginning, or is it simply doing transformations with its weights $W$ on a vector of random numbers generated upon each iteration?
