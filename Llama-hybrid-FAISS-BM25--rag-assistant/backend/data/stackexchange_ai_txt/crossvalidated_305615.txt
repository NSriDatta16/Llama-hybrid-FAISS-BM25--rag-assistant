[site]: crossvalidated
[post_id]: 305615
[parent_id]: 305590
[tags]: 
For a frequentist model, the variance of the prediction magnifies in proportion to the square of the distance from the centroid of $X$. Your method of calculating prediction intervals for a Bayesian GLM uses empirical quantiles based on the fitted probability curve, but does not account for $X$'s leverage. A binomial frequentist GLM is no different from a GLM with identity link except that the variance is proportional to the mean. Note that any polynomial representation of logit probabilities leads to risk predictions that converge to 0 as $X\rightarrow -\infty$ and 1 as $X\rightarrow \infty$ or vice versa, depending on the sign of the highest polynomial order term . For frequentist prediction, the squared deviation (leverage) proportional increase in variance of predictions dominates this tendency. This is why the rate of convergence to prediction intervals approximately equal to [0, 1] is faster than the third order polynomial logit convergence to probabilities of 0 or 1 singularly. This is not so for Bayesian posterior fitted quantiles. There is no explicit use of squared deviation, so we rely simply on the proportion of dominating 0 or 1 tendencies to construct long term prediction intervals. This is made apparent by extrapolating very far out into the extremes of $X$. Using the code I supplied above we get: > x_pred_dom gibbs_preds prop.table(table(gibbs_preds)) gibbs_preds 0 1 0.97733585 0.02266415 > So 97.75% of the time, the third polynomial term was negative. This is verifiable from the Gibbs samples: > prop.table(table(gibbs_samps[, 4] Hence the predicted probability converges to 0 as $X$ goes to infinity. If we inspect the SEs of the Bayesian model, we find the estimate of the third polynomial term is -185.25 with se 108.81 meaning it is 1.70 SDs from 0, so using normal probability laws, it should fall below 0 95.5% of the time (not a terribly different prediction based on 10,000 iterations). Just another way of understanding this phenomenon. On the other hand, the frequentist fit blows up to 0,1 as expected: freq gives: > plogis(freq$fit + c(-1.96, 1.96) %o% freq$se.fit) [,1] [1,] 0 [2,] 1
