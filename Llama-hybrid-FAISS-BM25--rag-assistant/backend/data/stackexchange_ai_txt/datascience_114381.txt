[site]: datascience
[post_id]: 114381
[parent_id]: 
[tags]: 
Why use cosine similarity instead of scaling the vectors when calculating the similarity of vectors?

I'm watching a NLP video on Coursera. It's discussing how to calculate the similarity of two vectors. First it discusses calculating the Euclidean distance, then it discusses the cosine similarity. It says that cosine similarity makes more sense when the size of the corpora are different. That's effectively the same explanation as given here . I don't see why we can't scale the vectors depending on the size of the corpora, however. For example in the example from the linked question: User 1 bought 1x eggs, 1x flour and 1x sugar. User 2 bought 100x eggs, 100x flour and 100x sugar User 3 bought 1x eggs, 1x Vodka and 1x Red Bull Vector 1 and 2 clearly have different norms. We could normalize both of them to have length 1. Then the two vectors turn out to be identical and the Euclidean distance becomes 0, achieving results just as good as cosine similarity. Why is this not done?
