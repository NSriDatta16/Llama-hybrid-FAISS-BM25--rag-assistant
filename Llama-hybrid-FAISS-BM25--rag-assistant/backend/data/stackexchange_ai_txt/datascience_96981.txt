[site]: datascience
[post_id]: 96981
[parent_id]: 96978
[tags]: 
Hi and welcome to the community! Don't get confused between those. they are different ways of explaining the same concept. The point is that in such problems with very imbalanced class populations, you need to use an evaluation metric which considers the effect of fined detailed inspection i.e. TP, FP, TN and FN. Precision/Recall and AUC/ROC both use them. But What's the main difference between them? AUC/ROC give you a wonderful visual representation (of course along with a number) and Precision/Recall give you more comprehensive detailed numerical evaluation. So the first is good for comparing several models and second one is better for deep inspection of each of models (of course they are still used vice-versa but "less nicer"). Do not even hesitate to include both. Just enriches the evaluation section of your paper. Positive class is the main point of your paper however you also want to keep a track of performance on trivial class (normal points) so I suggest include both i.e. the classification_report that you posted is a great way of reporting results You should certainly report Macro Average! In imbalanced problems you actually use precision/recall or auc/roc to get rid of something and weighted average is exactly calculation that thing! That "thing" is affecting evaluation by size of big classes. Example: here you see that precisions is very good for normal point and very bad for anomalies. What Weighted Average is telling you? it says 0.91 which is very good. But how is the performance? it is 0.1 on detecting anomalies which is the point of your paper! right? So be careful ... imbalanced problems should be evaluated by Macro Average.
