[site]: crossvalidated
[post_id]: 362136
[parent_id]: 273486
[tags]: 
I am sorry to say but your implementation is nowhere close to the NiN paper implementation. I just wrote the Keras code which, to my best knowledge, closely approximates the method used in the paper. I get 0.54% Test Error on MNIST which is close enough to their 0.47% . Further hyperparameter tuning would improve my result. def NiNBlock(kernel, mlps, strides): def inner(x): l = Conv2D(mlps[0], kernel, strides=strides, padding='same')(x) l = Activation('relu')(l) for size in mlps[1:]: l = Conv2D(size, 1, strides=[1,1])(l) l = Activation('relu')(l) return l return inner def get_model(img_rows, img_cols): img = Input(shape=(img_rows, img_cols, 1)) l1 = NiNBlock(5, [192, 160, 96], [1,1])(img) l1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(l1) l1 = Dropout(0.7)(l1) l2 = NiNBlock(5, [192, 192, 192], [1,1])(l1) l2 = AveragePooling2D(pool_size=(3, 3), strides=(2, 2))(l2) l2 = Dropout(0.7)(l2) l3 = NiNBlock(3, [192, 192, 10], [1,1])(l2) l4 = GlobalAveragePooling2D()(l3) l4 = Activation('softmax')(l4) model = Model(inputs=img, outputs=l4) return model model = get_model(img_rows, img_cols) model.summary() model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy']) model_name = 'NiN_mnist' filepath = "{}.weights.best.hdf5".format(model_name) checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') callbacks_list = [checkpoint] model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val), callbacks=callbacks_list)
