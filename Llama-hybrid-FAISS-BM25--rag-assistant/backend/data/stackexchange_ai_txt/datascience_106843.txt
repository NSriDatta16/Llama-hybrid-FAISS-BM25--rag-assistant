[site]: datascience
[post_id]: 106843
[parent_id]: 
[tags]: 
Keras CNN low training and val acc

I don't what is wrong with my current network architecture maybe some of you can help. I have a dataset that is highly imbalanced so i have implemented a datagenerator which balances the image data so there is the same number of images for each class. I have implemented a small resnet (almost the same as in the keras documentation): inputs = keras.Input(shape=(224, 224, 3), name="img") x = layers.Conv2D(64, 3, activation="relu")(inputs) x = layers.Conv2D(128, 3, activation="relu")(x) block_1_output = layers.MaxPooling2D(3)(x) x = layers.Conv2D(64, 3, activation="relu", padding="same")(block_1_output) x = layers.Conv2D(128, 3, activation="relu", padding="same")(x) block_2_output = layers.add([x, block_1_output]) x = layers.Conv2D(64, 3, activation="relu", padding="same")(block_2_output) x = layers.Conv2D(128, 3, activation="relu", padding="same")(x) block_3_output = layers.add([x, block_2_output]) x = layers.Conv2D(64, 3, activation="relu")(block_3_output) x = layers.GlobalAveragePooling2D()(x) x = layers.Dense(256, activation="relu")(x) #x = layers.Dropout(0.5)(x) outputs = layers.Dense(3, activation='softmax')(x) model = keras.Model(inputs, outputs, name="toy_resnet") model.summary() ---------- Model: "toy_resnet"Model: "toy_resnet" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== img (InputLayer) [(None, 224, 224, 3) 0 __________________________________________________________________________________________________ conv2d (Conv2D) (None, 222, 222, 64) 1792 img[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 128 73856 conv2d[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 73, 73, 128) 0 conv2d_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 73, 73, 64) 73792 max_pooling2d[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 73, 73, 128) 73856 conv2d_2[0][0] __________________________________________________________________________________________________ add (Add) (None, 73, 73, 128) 0 conv2d_3[0][0] max_pooling2d[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 73, 73, 64) 73792 add[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 73, 73, 128) 73856 conv2d_4[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 73, 73, 128) 0 conv2d_5[0][0] add[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 71, 71, 64) 73792 add_1[0][0] __________________________________________________________________________________________________ global_average_pooling2d (Globa (None, 64) 0 conv2d_6[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 256) 16640 global_average_pooling2d[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 3) 771 dense[0][0] ================================================================================================== Total params: 462,147 Trainable params: 462,147 Non-trainable params: 0 I have trained it for 50 epochs and made several changes during my testing all with the same result where training accuracy and validation accuracy begins to flucturate after 20 epochs. Do i increase the depth, increase number of neurons or train for more epochs? or is this problem more related to the learning rate where my model gets stuck in a local minima? I have checked the data generator and the batches looks fine. i have used the following config for training: model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.CategoricalCrossentropy(), metrics=[keras.metrics.CategoricalAccuracy()]) def reduce_lr(): return ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001) history = model.fit( train_gen, epochs=epochs, validation_data=val_gen, callbacks=[checkpoint(), reduce_lr()] ) I haven't found any good articles on this particular problem since i wouldn't say my model is overfitting/underfitting yet... ---------------UPDATE----------------- Do i try until i hit 100% or increase dataset right away
