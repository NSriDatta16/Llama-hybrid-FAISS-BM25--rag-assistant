[site]: crossvalidated
[post_id]: 121072
[parent_id]: 
[tags]: 
How to measure when error stabilizes (convergence) on Random Forests (or, when do I stop training)

I'm doing an implementation of Random Forests. As I was the original paper (page 11) and this nice book on the subject (15.3.1, page 592), they mention that when the out-of-bags error stabilizes (when our solution converges) as more trees are being trained, the training can be stopped before actually getting to train all the trees. That's very nice, but I haven't found a way to do it. I was thinking of a naive approach, having a window of the last trees trained and calculating the variance or the stddev of the error. After one of these values goes below a threshold (also naively defined by... me) then training can stop. Is there a well-known approach to this problem?
