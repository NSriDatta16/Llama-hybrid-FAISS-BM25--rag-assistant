[site]: crossvalidated
[post_id]: 579598
[parent_id]: 578231
[tags]: 
A similar question (if not the same) is: If the predicted value of machine learning method is E(y | x), why bother with different cost functions for y | x? The theoretical mean of a distribution minimizes the squared error, but that does not mean that the sample mean is always the best estimator with regards the squared error loss function. The sample mean has a statistical variation. In an answer to the above question an example is givem that shows how the sample median is performing better than the sample mean when the errors are Laplace distributed. Below is a copy of the image: Another example question is: Why is the Median Less Sensitive to Extreme Values Compared to the Mean? The median can be a better estimator in the case of distributions with outliers. Related is also: Could a mismatch between loss functions used for fitting vs. tuning parameter selection be justified? The answer to that question explains how, when we wish to have an estimator that optimizes the mean squared error, then it doesn't mean that we need to use the squared error loss function in fitting/training the model. Asside from using different estimators like the median or a maximum likelihood estimators, there is also the concept of biased estimators that can improve the expectation of the loss. Examples are regularisation (Ridge regression, lasso regression), Bayesian estimators, shrinking.
