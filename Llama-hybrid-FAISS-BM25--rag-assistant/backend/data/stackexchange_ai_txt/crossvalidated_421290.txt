[site]: crossvalidated
[post_id]: 421290
[parent_id]: 
[tags]: 
Interpreting association rules

I'm using mlxtend to find association rules in a dataset of products that customer bought over the span of a month. You can see an example of how it is used here . The dataset has around 1000 types of items and around 100k customers. My problem with the result I obtain is that the leverage of the rules is really low. For item sets $A$ and $C$ a rule $A\to C$ has leverage defined as $\Pr[A\cup C] - \Pr[A]\Pr[C]$ (please note that the event $A\cup C$ means that the items from both set $A$ and set $C$ appear, we are not interested in the appearance of items from the intersection of the sets). Therefore, if the leverage is close to zero then the items seem to occur independently. All of the rules I have extracted have leverage lower than 0.05 and only a handful have leverage larger than 0.03. I'm at loss of how to interpret this result . Given the large number of rules I have extracted this feels like a random fluctuation and consequently all the customers seem to be buying items independently. Does that make sense? Please note that I am not a domain expert on the items or the customers so I cannot tell if this is normal or not. Does this mean that the rules extracted are not meaningful in the sense that they represent actionable insights (e.g. for marketing)? Edit: After giving this some thought I realized that the support of the rules can be really low. As leverage denotes absolute deviation from the events being independent I need another statistic to get a clearer picture. Something that captures the relative deviation would most likely achieve that but if the support is low then the relative fluctuations will be larger. So ideally I need a robust statistic that weighs low support against relative deviation to identify association rules that are "unusual" in some sense. Is there a canonical statistic for this task or is it a question of an ad hoc approach?
