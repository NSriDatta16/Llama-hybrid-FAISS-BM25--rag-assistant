[site]: crossvalidated
[post_id]: 611408
[parent_id]: 
[tags]: 
Gaussian Negative log likelihood loss vs MSE

I'm training a neural network on a regression problem. I wanted to compare between (1) Gaussian negative log likelihood (GNLL) loss (the output of the network is the mean and log variance) and (2) the MSE loss (the output of the network is the mean only). I noticed that when using MSE, the MSE loss gets close to zero. However, when I use GNLL loss, MSE decreases but stays large. Why is it difficult for the neural network to reduce MSE when using GNLL as a loss function?
