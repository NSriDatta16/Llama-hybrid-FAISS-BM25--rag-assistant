[site]: crossvalidated
[post_id]: 140650
[parent_id]: 140537
[tags]: 
Because RNN is trained by backpropagation through time, and therefore unfolded into feed forward net with multiple layers. When gradient is passed back through many time steps, it tends to grow or vanish, same way as it happens in deep feedforward nets
