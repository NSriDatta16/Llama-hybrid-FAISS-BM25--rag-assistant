[site]: datascience
[post_id]: 97629
[parent_id]: 97628
[tags]: 
Your understanding is not correct. You go from a $B \times M \times D$ tensor to a $B \times M \times V$ tensor (i.e. the logits). As you can see, in the final tensor we have M vectors of dimension V (one vector per token), not just a single vector. To obtain the $B \times M \times V$ , you just perform a matrix multiplication. This applies to Transformers, but also to most sequence generation models, like LSTMs.
