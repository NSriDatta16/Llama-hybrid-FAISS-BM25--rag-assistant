[site]: crossvalidated
[post_id]: 203271
[parent_id]: 
[tags]: 
Alpha on Katz Backoff using Simple Good-Turing

I'm building an n-gram language model to predict the next word, I've implemented a simple Good-Turing smoothing on all my probabilities and have calculated the P0(mass probability of unseen event). I want to know if the Alpha for the lower order Ngrams when using Katz Backoff is exactly this P0 or I should do some other calculation?
