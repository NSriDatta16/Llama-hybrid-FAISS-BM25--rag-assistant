[site]: datascience
[post_id]: 27687
[parent_id]: 
[tags]: 
How much text is enough to train a good embedding model?

I need to train a word2vec embedding model on Wikipedia articles using Gensim. Eventually, I will use the entire Wikipedia for that but for the moment, I'm doing some experimentation/optimization to improve the model quality and I was wondering how many articles would be enough to train a meaningful/good model? How many examples needed for each unique word in vocabulary?
