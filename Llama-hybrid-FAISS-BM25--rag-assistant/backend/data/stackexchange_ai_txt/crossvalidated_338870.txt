[site]: crossvalidated
[post_id]: 338870
[parent_id]: 
[tags]: 
Proving that the average of a log-likelihood ratio involving an ML estimator is positive

Consider a random variable X, with a probability density function $f_\theta(x)$, where $\theta \in \Theta$. Denote by $\hat{\theta}(X)$ the Maximum likelihood estimator of the parameter. We know that for any two parameters, $\theta$ and $\theta'$ we have \begin{equation} \mathbb{E}_\theta \bigg[\log \frac{f_\theta{(X)}}{f_{\theta'}(X)}\bigg] = D(f_\theta || f_{\theta'})>0 \end{equation} where $E_\theta$ is the expectation when $f_\theta(x)$ is the underlying distribution, and $D(f_\theta || f_{\theta'})$ the KL-divergence between $f_\theta$ and $f_{\theta'}$. I would like to know whether a similar inequality holds when $\theta$ is replaced by $\hat{\theta}$. Thus, my question is if the following inequality holds: \begin{equation} \mathbb{E}_\theta \bigg[\log \frac{f_{\hat{\theta}(Y)}{(X)}}{f_{\theta'}(X)}\bigg] >0, \end{equation} where in this expression $Y$ and $X$ are independent and identically distributed under $f_\theta$ (i.e., the ML estimate is evaluated at different data points than the pdf $f_\theta$). Intuitively this result should hold since on average the ML estimator will give an estimate close to $\theta$. However, I am not sure if any other assumptions are needed? I would really appreciate any help or hints towards proving this claim. Thanks!
