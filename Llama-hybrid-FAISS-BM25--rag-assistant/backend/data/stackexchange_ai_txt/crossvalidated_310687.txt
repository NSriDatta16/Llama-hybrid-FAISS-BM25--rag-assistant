[site]: crossvalidated
[post_id]: 310687
[parent_id]: 
[tags]: 
Proof that the expected MSE is smaller in training than in test

This is Exercise 2.9 (p. 40) of the classic book "The Elements of Statistical Learning", second edition, by Hastie, Tibshirani and Friedman. In the book, it's mentioned that this exercise was brought to the authors' attention by Ryan Tibshirani, from a homework assignment by Andrew Ng. Ex. 2.9. Consider a linear regression model with $p$ parameters, fit by least squares to a set of training data $(x_1,y_1),\dots,(x_N,y_N)$ drawn at random from a population. Let $\hat{\beta}$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1,\tilde{y}_1),\dots,(\tilde{x}_M,\tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{tr}(\beta)=\frac{1}{N}\sum_{i=1}^N(y_i-\beta^T x_i)^2$ and $R_{te}(\beta)=\frac{1}{M}\sum_{i=1}^M(\tilde{y}_i-\beta^T \tilde{x}_i)^2$ , prove that $$ \mathrm{E}[R_{tr}(\hat{\beta})]\leq\mathrm{E}[R_{te}(\hat{\beta})], $$ where the expectations are over all that is random in each expression.
