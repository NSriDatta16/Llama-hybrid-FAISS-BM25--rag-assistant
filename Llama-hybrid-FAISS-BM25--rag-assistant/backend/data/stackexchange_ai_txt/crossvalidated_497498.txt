[site]: crossvalidated
[post_id]: 497498
[parent_id]: 475208
[tags]: 
why does masking requires us to sample (take only 15%) of the words? I think this guarantees that only about 1/7 of the words are masked, which is just like the window size in word2vec. That is on average in BERT we use 7 words as context to predict one word. The more words we mask the smaller the "window size" and the smaller the context. Google did some experiments to try different corruption rates and here is the results: Source: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer I don't understand the mitigation they use - in 10% of the times they replace [mask] with a random word, and in 10% they replace it back to the original word. This answer of mine would be of some help.
