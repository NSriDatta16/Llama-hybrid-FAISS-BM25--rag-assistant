[site]: crossvalidated
[post_id]: 64301
[parent_id]: 64299
[tags]: 
I think this might help, although I'm still unsure myself on the relationship between kernel logistic regression and good old generalised additive models with locally weighted regression smooths. The NIPS paper suggests they're at least strongly related, if not the same. You can think of kernel logistic regression as fitting a weighted logistic regression for each data point $x_i$, based on its neighbours $x_j$. The weights are given by $K(||x_j - x_i||)$ with $K(*) \to 0$ as the distance between $x_j$ and $x_i$ increases. From this, you can use the standard IRLS algorithm to get the solution, by applying it in turn to each $x$. This is of course computationally inefficient, since you're applying an $O(N^2)$ algorithm to $N$ data points, making it $O(N^3)$ in all. This is the problem that the IVM is designed to solve.
