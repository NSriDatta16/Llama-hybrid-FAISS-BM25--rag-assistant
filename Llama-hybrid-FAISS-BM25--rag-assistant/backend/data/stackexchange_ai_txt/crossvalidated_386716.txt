[site]: crossvalidated
[post_id]: 386716
[parent_id]: 
[tags]: 
When does my autoencoder start to overfit?

I am working on anomaly detection using an autoencoder neural network with $1$ hidden layer. This is an unsupervised setting, as I do not have previous examples of anomalies. The input data has patterns but also varies a lot, hence, is partly stochastic in nature. For understanding purposes, I trained a (complete) autoencoder with dimensions input = $500$ , hidden = $500$ , output = $500$ and sigmoid functions in the hidden and output layer. My training data has dimension $X\in[0,1]^{5000\times500}$ (500 variables, 5000 samples). I used $3$ algorithms, with learning rate $0.01$ , mini batch size $64$ , and pretty much the standard algo-parameters in Keras/TensorFlow: standard stochastic gradient descent (SGD) advanced/extended SGD with Nesterov momentum $0.9$ and learning rate decay $10^{-8}$ Adam optimizer ( $\beta_1=0.9$ , $\beta_2=0.999$ , learning rate decay $0$ ) The image below shows the corresponding error curves. In my case both keep decreasing (except for Adam) so I would say "keep training". On the other hand I know intuitively that I should not train so long because there must be some overfitting going on. So how do I know when to stop training, how would you interpret the result below? Would I be right, to just take Adam and use 250 epochs (even though it has a wide bias between training/validation sets)?
