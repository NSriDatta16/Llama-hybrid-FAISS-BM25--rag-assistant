[site]: crossvalidated
[post_id]: 408671
[parent_id]: 208930
[tags]: 
They differ in their approaches to tree generation and selection of baselearners to retain for the predictive model: RuleFit first generates a boosted decision tree ensemble. That is: It sequentially grows trees on a pseudo response variable, where the pseudo response for each tree is corrected for the predictions of earlier trees. The amount of correction for earlier trees is controlled by the learning rate (.01, by default). For induction of each tree, a random (sub or bootstrap) sample of the training data is selected (same as random forest). After generating the boosted decision tree ensemble, all nodes from all trees in the decision tree ensemble are transformed into dummy coded rule variables (taking a value of 0 if the conditions of the rule do not apply, a value of 1 if they do). The final predictive model is selected through sparse regression on the rule variables and the original predictor variables. A random forest is a decision tree ensemble, in which The trees are not induced sequentially, i.e., the response variable is not corrected for predictions of earlier trees. For every split in every tree, a random subset of mtry predictor variables are selected as potential candidates for the split. For induction of each tree, a random (sub or bootstrap) sample of the training data is selected (same as in boosting and RuleFit). The final predictive model averages over the predictions of each tree in the ensemble. Btw, R package pre also supports fitting prediction rule ensembles using a random-forest-type approach.
