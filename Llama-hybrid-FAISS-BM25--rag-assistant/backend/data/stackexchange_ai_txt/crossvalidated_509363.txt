[site]: crossvalidated
[post_id]: 509363
[parent_id]: 298849
[tags]: 
You can construct a smoother version of max function using softmax function, as the expression in your book suggests. Consider the following formulation of a max function: $$\max(z_1,\dots,z_n)=\mathrm{argmax}(z)\times z^T$$ The function argmax returns a vector with 0s and 1s. Thus it produces a rough max function. Rough in sense that its first derivative wrt its arguments is discontinuous: it's either 0 or 1. Whenever $z_i=z_j$ the first derivative jumps between 0 and 1. By replacing argmax with what machine learning people call softmax you get a smooth version of max function too, as suggested in your book. Here's a couple of charts to demonstrate the point. The following is a surface of an ordinary $\max(x_1,x_2)$ function. Compare it to the version using the expression from your textbook $\mathrm{softmax}(x_1,x_2)^T\times (x_1,x_2)$ : Smoother version of max can be easier to deal with analytically.
