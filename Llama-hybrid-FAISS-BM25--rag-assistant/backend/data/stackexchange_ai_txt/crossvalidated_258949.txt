[site]: crossvalidated
[post_id]: 258949
[parent_id]: 258943
[tags]: 
PCA will give you features, but which will be a linear combinations of the original 100 features. As such, you will (potentially) need all of the 100 to create the principal components. This is not the proper tool if you cast your problem as a regression of $y$ based on $\mathbf{X}$. The maximization problem that PCA solves is $\max_{\mathbf{p}_1 \in \mathbb{R}^{100}} \mathsf{Var}(\mathbf{p}_1^\top\mathbf{X})$ subject to $\mathbf{p}_1^\top\mathbf{p}_1=1$, i.e. explain most of the variance of the design matrix. The other principal components $\mathbf{p}_j, j=2, \ldots$ are obtained by imposing the further constraint that $\mathbf{p}_i^\top\mathbf{p}_j=0$ for $i=1, \ldots, j$, so that the principal components are orthogonal. An alternative that will also give a regression based on linear combinations of $\mathbf{X}$-features is PLS, which aims at maximizing $\max_{\mathbf{w}}\mathsf{Cov}(\boldsymbol{y}, \mathbf{X}\mathbf{w})$ subject to $\|\mathbf{w}\|=1$ and orthogonality of the weights vectors. In principle, obtain the principal components, choose the number $K$ you want to keep (based on e.g. a scree plot diagnostic) and obtain your new regressors as $\{\mathbf{p}_i\mathbf{X}\}_{i=1}^K$. pca If you want to do variable selection in say a linear regression, you could use LASSO to shrink some coefficients in the regression to zero and only keep the features of interest.
