[site]: crossvalidated
[post_id]: 478398
[parent_id]: 
[tags]: 
How does AdamW weight_decay works for L2 regularization?

hello I have a question about weight regularization in Adam apparently the weight_decay in the AdamW function https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw-pytorch has the same impact as an L2 regularization my questions are: is that parameter the same as lambda that we have in the regularization term ? L2 regularization formula how does it exactly work? and what is its impact on the model complexity?
