[site]: crossvalidated
[post_id]: 489563
[parent_id]: 
[tags]: 
Best tests for significance of relative risk

Currently I have insurance claim loss data (say roughly N=10,000), with independent variables state (California, Washington, etc.), policy number (uniquely identifies a policyholder), type of car (sedan, SUV, etc.), and product type (each product has different claim loss structure), so something like the following: Car type State Product type Policy Number Loss Sedan CA A 918319 1000 SUV CA B 918319 400 Sedan WA C 931239 600 Truck CO A 138234 300 Also, the amount of data that exists for each car type is uneven; for instance, there exists N=3000 entries for sedan but only N=30 entries for utility vehicles. I'd like to test which car type has higher risk, i.e. which car type, generally speaking, has a higher average loss per policy number. I'm wondering if there are any issues with the following approach, or important points I need to consider: Right now my plan is to conduct t-tests by comparing the average loss per policy for each car type vs. the overall average, e.g. null hypothesis might be: average loss per policy for sedans is equal to the average loss per policy for all car types, and alternative is average loss per policy for sedans is higher than average loss per policy for all car types. For the t-test specifically for sedans, since claim losses are typically not normally distributed, to help satisfy the normality assumption, I plan to randomly assign each data entry with sedan as the car type to a random sample, so that each sedan sample has roughly 30 entries (to get each resultant sample closer to a normal distribution), and then apply the t-test on the resulting sampling distribution. Are there other approaches? For car types without a lot of entries, my guess is a non-parametric approach might be better, but I'm not sure what would be the best test.
