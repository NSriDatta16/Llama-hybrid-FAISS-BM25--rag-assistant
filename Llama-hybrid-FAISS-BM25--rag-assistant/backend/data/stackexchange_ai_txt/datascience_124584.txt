[site]: datascience
[post_id]: 124584
[parent_id]: 
[tags]: 
Why is 0.7, in general, the default value of temperature for LLMs?

I have recently read through a lot of documentation and articles about Large Language Models (LLMs), and I have come to the conclusion that 0.7 is, most of the time, the default value for the temperature parameter . See a few quick reference examples where the default value is either 0.7 or 0.75: https://platform.openai.com/docs/api-reference https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be https://rasa.com/docs/rasa/next/llms/llm-intent/ However, I am struggling to find any reference that would explain the rationale for using 0.7. I understand that lower values of the temperature result in more deterministic outputs and that higher values result in more random outputs. Nonetheless, why is it more recommended to select temperature=0.7 rather than temperature=0.6 or temperature=0.4 for instance? In contrast, in "GPT-4 Technical Report", a value of 0.6 is used as the "best-guess" by the authors. See https://arxiv.org/pdf/2303.08774.pdf , p.24. So my question would boil down to: - Is it purely empirical or are there either benchmarks, or mathematical equations, which would substantiate the approach of selecting a temperature close to 0.7? - If it is purely empirical, what were the empirical reasons leading to the adoption of values close to 0.7? (E.g., is it due to the default parameters used in a highly cited paper?, in a highly used library?, etc.) Thank you
