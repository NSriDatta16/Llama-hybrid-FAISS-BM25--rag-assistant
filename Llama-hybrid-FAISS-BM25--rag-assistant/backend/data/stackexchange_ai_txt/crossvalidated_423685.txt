[site]: crossvalidated
[post_id]: 423685
[parent_id]: 312118
[tags]: 
This sounds like a bad idea, though I can't exactly tell what the criterion for adding a predictor to the model was. How can a "predictor's odds ratio" be "changed" by 5% if it was just added to the model? 5% indeed was pulled out of you-know-where. The techniques Spatzle mentioned, forward/backward/best subset, are an improvement, because they base the variables chosen on a metric like AIC, which balances how well the model fits the data with the number of variables in the model (in an attempt to avoid overfitting ). However, these algorithms too are actually usually a bad idea! They are "greedy," so they often fail to optimize well, and more importantly they still tend to overfit. It's also hard to interpret p-values/significance from these models since the variables are adaptively selected. LASSO is a good place to start with variable selection. Ridge regression is less directly suited to variable selection. PCA does dimensionality reduction; variable selection is a specific type of dimensionality reduction that maintains variable definitions. PCA usually will spit out weird linear combinations of the original variables that will be hard to interpret. The fact is, in the sciences where model interpretability and inference may be more important than predictiveness (i.e. you don't want to just throw all the data in a Random Forest or Neural Net), nothing really beats a careful, scientific approach to the problem. Make sure the variable codings are clean and sensible, and deal with missing data appropriately. Then choose covariates, a model, and functional forms for the covariates based on your scientific knowledge, perhaps tweaking the model slightly after fitting to improve model diagnostics. Some of your coefficients won't be significant, which is fine, and better yet your p-values will be honest!
