[site]: crossvalidated
[post_id]: 420886
[parent_id]: 164436
[tags]: 
This is as much discussion as answer, and I doubt there is a single "answer". I'm halfway though a clustering class for R on DataCamp with Dmitriy Gorenshteyn. I'm realizing that I've wasted many hours of my life trying to do clustering by trial an error. To manage outliers it would be great to to omit / group the outliers into their own cluster so that just the meaningful ones remain, but this isn't possible since there's no similarity measure to unite outliers. So, I think it makes more sense to split by the height you want (rather than the k groups you expect), and just keep the clusters with high similarity measures / relatively high counts. With hierarchical clustering you'll end up with a lot of "clusters" that are really single observations, and can be considered outliers. It looks like DBSCAN is the real way to go (great suggestion by @stephan-kolassa). It makes so much more sense to me now to realize that often most observations may not belong in a cluster. That the distance metric (max, min, average) must impact on how outliers are included or excluded from clusters, but I'm not sure which measure would be better for managing outliers. Maybe I'll update my answer later, or someone else will provide a better answer in the future.
