[site]: crossvalidated
[post_id]: 374935
[parent_id]: 
[tags]: 
How to deal with really sparse time series data for a binary classification task using RNN or LSTM?

I have a binary classification prediction task and more often than not, the time series data is like really sparse. The number of zeroes in the time series data is almost always more than 99%. I believe this is causing my RNN and LSTM models to behave really weird and unstable. The validation scores are highly variable between multiple runs and there there are lot of ties when it comes to class probability predictions. The time series data is derived based on some dates. It somewhat looks like below: The above data is showing just 2 tenants. Tenant 9 had encountered two items on 2009-09-11 and 2009-09-26 . Each row in item_id is a tuple of length N where N is the number of distinct item_id for all tenants. So if N is 100, the tuple corresponding to 2009-09-11 for tenant 9 will have all zeroes except one location where it will be 1 for the item encountered. Now you can imagine how sparse this will be, especially after padding! In some cases its almost 100% sparse. Highly doubt this will be useful for any sorts of modeling. My models are very unstable even after training for large number of epochs and I notice there are ties on a range of 20 to 70% - meaning the predictions are inconclusive in distinguishing between the classes. Questions: Is the sparseness really the reason for models being so unstable and inconclusive? One way to deal with the sparseness is to make the data less granular by deriving time sequence using month and year or year instead of the actual date. Is this an approach worth trying?
