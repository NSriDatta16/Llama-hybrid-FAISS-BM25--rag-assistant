[site]: crossvalidated
[post_id]: 511993
[parent_id]: 511112
[tags]: 
This issue has been appreciated for some time. See Harrell on page 210 of Regression Modeling Strategies, 2nd edition : For a categorical predictor having $c$ levels, users of ridge regression often do not recognize that the amount of shrinkage and the predicted values from the fitted model depend on how the design matrix is coded. For example, one will get different predictions depending on which cell is chosen as the reference cell when constructing dummy variables. He then cites the approach used in 1994 by Verweij and Van Houwelingen , Penalized Likelihood in Cox Regression, Statistics in Medicine 13, 2427-2436. Their approach was to use a penalty function applied to all levels of an unordered categorical predictor. With $l(\beta)$ the partial log-likelihood at a vector of coefficient values $\beta$ , they defined the penalized partial log-likelihood at a weight factor $\lambda$ as: $$l^{\lambda}(\beta) = l(\beta) - \frac{1}{2} \lambda p(\beta)$$ where $p(\beta)$ is a penalty function. At a given value of $\lambda$ , coefficient estimates $b^{\lambda}$ are chosen to maximize this penalized partial likelihood. They define the penalty function for an unordered categorical covariate having $c$ levels as: $$p_0(\beta) = \sum_{j=1}^c \left( \beta_j - \bar \beta \right)^2$$ where $\bar \beta$ is the average of the individual regression coefficients for the categories. "This function penalizes $\beta_j$ 's that are further from the mean." It also removes the special treatment of a reference category $1$ taken to have $\beta_1=0$ . In illustrating their penalized partial likelihood approach in a model with a 49-level categorical covariate, they constrained the coefficients to sum to zero, and then optimized to choose $\lambda$ and generate the penalized coefficient estimates. Penalization must involve all levels of a multi-level categorical covariate in some way, as the OP and another answer indicate. One-hot encoding is one way to do that. This alternative shows a way to do so with dummy coding, in a way that seems to keep more emphasis on deviations of individual coefficient values from the mean of coefficients within the same covariate, rather than on their differences from coefficients of unrelated covariates.
