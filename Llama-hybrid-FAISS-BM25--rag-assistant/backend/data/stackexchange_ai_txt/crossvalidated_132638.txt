[site]: crossvalidated
[post_id]: 132638
[parent_id]: 74262
[tags]: 
I have an example that might be somewhat different from what you originally intended when you asked this question. The past year or two have given rise to an ongoing discussion in psychology over the cause of the lack of replicability of effects from randomized experiments. Versions of this debate have surfaced for many years, but the debate has become more strident since the publication of a paper showing that many practices that are standard in psychology in the formulation of hypotheses, collection of data, analysis of data, and reporting of results allow researchers to find results supporting even arbitrarily chosen hypotheses (in the original paper, the researchers used these practices to show that listening to "When I'm Sixty-Four" by the Beatles made people younger). The root of the problem, of course, is the pervasive incentive structures in psychology (and in other sciences) to obtain novel, positive, "publishable" results. These incentives encourage research scientists to adopt practices that, while not as obviously "wrong" as data fabrication, nonetheless lead to an increased rate of false positive results. These practices include: The collection of multiple, and highly similar, dependent variables. Only the dependent variable that produces the results most consistent with the original hypothesis is reported. During data collection, testing for significant results multiple times and stopping data collection when significance is obtained. During analysis, the inclusion of multiple covariates in the statistical model. In the final paper, only the combination of covariates that leads to results most consistent with the original hypothesis is reported. Dropping conditions that lead to results that are inconsistent with the original hyptoheses and failing to report these conditions in the paper. And so on. I would argue that the "lurking variable" in these cases is the incentive structure that rewards researchers for obtaining positive, "publishable" results. In fact, there have already been several high-profile results in psychology (many of which are in my specialty, social psychology) that have failed to replicate. These failures to replicate, many argue, cast doubt on entire subfields of psychology. Of course, the problem of incentive structures that encourage false positives is not unique to psychology; this is a problem that is endemic to all of science, and thus to all randomized controlled trials. References Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science , 17, 1359-1366. Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science , 7, 615-631. Yong, E. (2012). Bad copy. Nature , 485, 298-300. Abbott, A. (2013). Disputed results a fresh blow for social psychology. Nature , 497, 16.
