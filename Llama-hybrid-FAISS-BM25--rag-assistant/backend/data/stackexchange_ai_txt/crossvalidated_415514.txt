[site]: crossvalidated
[post_id]: 415514
[parent_id]: 415497
[tags]: 
It is a good idea to bootstrap or cross-validate (e.g., 100 repeats of 10-fold cross-validation) indexes that were not optimized. For example, I recommend optimizing on a gold standard such as log-likelihood, penalized log-likelihood, or in a Bayesian model log-likelihood + log-prior. You can report measures such as pseudo $R^2$ that are just transformations of the gold standard objective function, and in addition do resampling validation on helpful indexes such as the $c$ -index (concordance probability = AUROC), Brier score, and most of all, the full calibration curve. I do validation of smooth nonparametric calibration curves by bootstrapping 99 predicted values when using a probability model, i.e., to validate the absolute accuracy of predicted probabilities of 0.01, 0.02, ..., 0.99. Likewise you can show overfitting-corrected estimates of Brier score, calibration slope, mean squared error, and many other quantities. Details are in my RMS book and course notes .
