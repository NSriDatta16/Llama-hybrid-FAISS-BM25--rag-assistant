[site]: datascience
[post_id]: 94100
[parent_id]: 
[tags]: 
Negative log-likelihood not the same as cross-entropy?

The negative log-likelihood $$ \sum_{i=1}^{m}\log p_{model}(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}) $$ can be multiplied by $\frac{1}{m}$ after which the law of large numbers can be used to get $$ \frac{1}{m} \sum_{i=1}^{m}\log p_{model}(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}) \rightarrow E_{}(\log p_{model}(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta})) $$ as the sample size $m$ tends to infinity. This expectation is the "cross-entropy". Now here comes my question: The book I am reading(Deep Learning by Goodfellow et al) mentions several attractive properties of using the negative log-likelihood(like consistency). But meanwhile, it also also uses cross-entropy directly as the loss function of maximum likelihood estimators: This doesn't make sense to me - to talk about negative log-likelihood and cross-entropy as being identical. It would make sense for me to talk about NLL as an approximation of the cross-entropy. I mean, they give different results - so why use one over the other? This seems like a valid question when they do not give the same results and must thus also affects the performance. Like, I am only aware of neural networks that use cross-entropy and not ones that use NLL - how come? Maybe cross-entropy even holds other properties than negative log-likelihood?
