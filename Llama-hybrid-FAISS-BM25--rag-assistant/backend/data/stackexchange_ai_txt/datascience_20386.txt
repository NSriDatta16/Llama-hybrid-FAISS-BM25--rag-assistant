[site]: datascience
[post_id]: 20386
[parent_id]: 20367
[tags]: 
If you are talking about tabular data (not images, video, sounds etc) then yes. All of preparations you mentioned are applicable to Artificial neural networks. For instance, data normalization is very important since neurons activations are calculated applying some activation function on a weighted sum(linear combination) of it's inputs. A value of 0.01 and a value of 10000000 in a weighted sum will always have a large value if the weights are at the same scale. Theoretically the network could learn how to deal with such discrepancies on the input values but in a practical sense normalization can make learning faster. There is a classical paper on efficient backprop, with many tips and tricks, by Yann LeCun .
