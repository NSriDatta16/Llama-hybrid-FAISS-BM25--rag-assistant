[site]: datascience
[post_id]: 126643
[parent_id]: 103496
[tags]: 
You can first take the one-hot representation. You can assimilate this to a probability distribution of a categorical random variable, where one option has probability 1 so any samples we draw would be that value. Then, you would use the Gumbel-Softmax trick to add noise to the distribution and sample from it. This is an implementation taken from here : class Sample_Softmax(Model): def __init__(self, num_values=5, tau=1.0): super(Sample_Softmax, self).__init__() self.tau = tau self.arange = tf.range(start=0, limit=num_values, dtype=tf.float32) def call(self, x): # generate gumbel noise noise = np.random.gumbel(size=x.shape) noisy_x = x + noise # apply softmax temperature noisy_x = noisy_x / self.tau # compute softmax probs = K.exp(noisy_x) / K.sum(K.exp(noisy_x)) # dot product for the index of max array element index = tf.tensordot(probs, self.arange, axes=1) return index Another option would be to take the result of the softmax and multiply it directly by the embedding matrix to obtain an embedded vector representation that is a weighted average according to the obtained noisy probabilities. As an optional step, before applying the Gumbel noise, you may want to smooth the probability distribution with label smoothing: def smooth_labels(labels, factor=0.1): labels *= (1 - factor) labels += (factor / labels.shape[1]) return labels
