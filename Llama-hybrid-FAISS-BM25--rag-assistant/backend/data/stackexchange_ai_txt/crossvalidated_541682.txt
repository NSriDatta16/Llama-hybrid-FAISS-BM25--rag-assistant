[site]: crossvalidated
[post_id]: 541682
[parent_id]: 
[tags]: 
As model complexity increases, why does the training error increase?

Suppose I have 3 clusters of data. I train a machine learning model on the combined data set (i.e., 3 clusters of data combined), and the training error decreases as the complexity of the model increases. However, after a certain point, the error increases. I have a feeling this might be happening because the 3 clusters of data are very different from one another. So it may be "difficult" to fit a model on the "combined" data set when the data is heterogeneous. What might be an intuitive explanation of this phenomenon (i.e., training error increases as model complexity increases)?
