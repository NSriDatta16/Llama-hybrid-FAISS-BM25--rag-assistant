[site]: crossvalidated
[post_id]: 157796
[parent_id]: 
[tags]: 
Dig deeper on "Determine the Number of Clusters and Validate It"

Updates to this thread: Based on Anony-Mousse's comments on my current results, there is only one big cluster in my data set. However, I think it might still be possible to reveal the clusters if I dig deepr (e.g., get rid of attributes that might be useless or distracting). Any suggestions on how to better organize the data to help the clusters arise ? I want to classify the residential areas of a city into groups based on their social-economic characteristics, including housing unit density, population density, green space area, housing price, number of schools/health centers/day care centers, etc. So that we can understand how many different groups the residential areas can be divided into? And what are their unique characteristics? My data looks likes: I have 15 variables and their correlation is not strong, most around 0.1 to 0.2. I plan to do the k-mean clustering for this grouping task. To determine the number of clusters, I first did PCA analysis, and this is the cumulative proportion of components: It seemed 9 components would account for >80% of cumulative propotrion. But I think 9 clusters is too many for my project. So I used the NbClust package to determine the number of clusters. It reported 3 clusters is the best based on 26 criteria. My questions: 1) It seemed that any arbitrary data sets can be classified into groups, as long as we apply k-mean clustering. Is that possible some data sets naturally don't have any structure and cannot be divided into groups (e.g., uniform distributed in all feature space and don't have any pattern)? I am concerned about whether my data set really has any structure or not, since the cumulative proportion of components is so low. 2) Is the 3 clusters recommendation given by NbClust reasonable? How can I validate this? By the way, using plot(comp, col=k$clust, pch=16) command, the 3 components looks like:
