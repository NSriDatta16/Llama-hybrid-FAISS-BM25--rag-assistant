[site]: crossvalidated
[post_id]: 112935
[parent_id]: 63821
[tags]: 
In a Bayesian Belief Network (BBN), the joint probability can be decomposed. Assume the following. U = {X1, X2, X3, X4 }, U is a set of variables P(U) = P(X1, X2, X3, X4), P is the joint probability Using the chain rule, you can decompose the P as follows $P(U) = P(X1, X2, X3, X4) = P(X1)P(X2|X1)P(X3|X1,X2)P(X4|X1,X2,X3)$ Because a BBN satisfies the Markov condition, you can decompose P as follows. $P(U) = \prod_i P(X_i|pa(X_i))$ Let's just say the BBN structure, its directed acyclic graph (DAG), is indeed the following. X1 -> X2 -> X3 -> X4 Then, $P(U) = P(X1, X2, X3, X4) = P(X1)P(X2|X1)P(X3|X2)P(X4|X3)$ Do you see any efficiency of computing P by using the the Markov condition versus the Chain Rule? A more concrete example. Let's say all variables are binary and take on the values yes/no. Let's say you observe X1=yes, X2=yes, X3=no, and you want to predict the states of X4. How would you do this? If you did not have the structure (DAG), then you can do counts (as you stated in your post) to compute the conditional probabilities. $P(X4=yes | X1=yes, X2=yes, X3=no)$ and $P(X4=no | X1=yes, X2=yes, X3=no)$ But, if you do have the DAG, you know you can do the following. $P(X4=yes | X3=no)$ and $P(X4=no | X3=no)$ Which is faster to compute, even if you are just counting, with or without the DAG?
