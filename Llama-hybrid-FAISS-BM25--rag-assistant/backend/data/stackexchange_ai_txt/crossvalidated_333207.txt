[site]: crossvalidated
[post_id]: 333207
[parent_id]: 333148
[tags]: 
Based on what I've read about RL, I thought it would be a good idea to encode a pattern in the way the obstacles fall, so the agent can truly "learn" that pattern and make optimal moves. (1) Does this approach make sense in the context of RL (hardcoding a pattern in the falling objects)? It depends . The agent will necessarily base its decision on which action to take based on current game state. Whether or not you want to hard-code a pattern depends on what you allow the agent to observe before it makes its decision. If you hard-code a time-based pattern, then you will need to allow the agent to observe the time, as well as it's current location. If the hard-coded pattern is in a loop for a continuous problem, and you don't want to challenge the agent to figure out the loop length, you may want to tell the agent where it is in the loop (typically the time modulo the loop length). Or you could help the agent by providing game state based on multiple loop lengths and allow it to figure out which is best for it. If falling objects in the game "spawn" above certain positions, and the agent gets to observe them falling one or more time steps before they hit the ground, then you don't need a hard-coded pattern, or for the agentto know what time it is, because the agent can make close enough to optimal moves based on observations. So you could have a random pattern of objects falling. Occasionally by chance this may "trap" the agent in a way that a hardcoded pattern may not, depending on how dense your pattern of falling objects is, and how much notice the agent gets before they would strike it. (2) If it does, what are some algorithms I should be looking into to power the agent? It depends . Probably, as suggested in the comments, a Deep Q Learning approach would work well. Especially if the agent gets to see the falling objects before they strike. If the agent does not get to see falling objects in advance, and the state is literally just the current time and location of the agent, then you may get away with tabular Q-learning. The state space would be num_locations times num_steps. So provided the space where the agent could move and number of time steps not too high, you can keep the problem simple enough to solve quickly and exactly. It is worth IMO exploring a really simple environment - effectively a 3D maze for your game - before getting into solvers that use neural networks.
