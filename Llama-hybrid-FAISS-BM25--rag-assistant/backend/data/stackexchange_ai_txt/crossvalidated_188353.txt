[site]: crossvalidated
[post_id]: 188353
[parent_id]: 
[tags]: 
Single CV yields higer prediction error estimate compared to nested CV

I'm trying to calculate an unbiased prediction error estimate for a regression problem with random forest. Dataset dimension is ~25100x13. So, as a first step, I used a nested 10/10-fold CV loop which yielded an $R^2$ estimate of 0.87 ($\pm$ 0.01) (I thought this was really high). So I tried to perform the nested CV loop repeatedly (10 times), but consistently got an estimate really close to the original one. Additionally, I performed the same procedures on a randomly generated dataset of the same dimensions, to make sure this is not some kind of glitch in my code. This resulted in $R^2 Satisfied, I then proceeded to use a single 10-fold CV loop for model selection, which surprisingly resulted in $R^2$ = 0.78. I thought that nested CV removes the optimism, associated with performing hyperparameter selection on the same data, on which model evaluation is performed, but in my case the opposite seems to be true. So either nested CV is somehow allowing some optimism still, or a single 10-fold CV includes some pessimistic bias. I'm not really sure how to interpret these results. Any hints appreciated. [EDIT] Forgot to add that I'm using randomized search for hyperparameter optimization with 60 iterations. [EDIT2] Here's the code I'm using: Model evaluation outer_scores = {'best_param': [], 'r2': []} print 'Starting nested cross-validation...' outer = cross_validation.KFold(len(y), n_folds=10, shuffle=True) for fold, (train_index_outer, test_index_outer) in enumerate(outer): np.random.seed(None) X_train_outer, X_test_outer = X[train_index_outer], X[test_index_outer] y_train_outer, y_test_outer = y[train_index_outer], y[test_index_outer] print 'Cross-validating on fold %i' % fold rs = RandomizedSearchCV(RandomForestRegressor, params, n_iter=60, n_jobs=-2, verbose=1, iid=False, refit=True, cv=10, scoring='r2',) rs.fit(X_train_outer, y_train_outer) pred_outer = rs.predict(X_test_outer) r2 = r2_score(pred_outer, y_test_outer) print "INNER R2: %.3f" % r2 outer_scores['best_param'].append(rs.best_params_) outer_scores['r2'].append(r2) print np.mean(outer_scores['r2']) # this returns the high $R^2$ Model selection reg = RandomizedSearchCV(RandomForestRegressor, params, n_iter=60, n_jobs=-2, verbose=1, iid=False, refit=True, cv=10, scoring='r2', random_state=1) reg.fit(X, y) return reg.best_score_ # this returns lower $R^2$, than the code above
