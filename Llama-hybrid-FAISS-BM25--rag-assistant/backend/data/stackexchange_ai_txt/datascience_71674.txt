[site]: datascience
[post_id]: 71674
[parent_id]: 
[tags]: 
Oversampling for regression for data grouped in clusters

I am dealing with a regression problem in which I want to predict the upcoming value of a time-dependent variable by using the previous values of other variables (not including the output variable itself), and the relations between variables are expected to be complex, so I am using an LSTM Neural Network. The input data come from several different sources, and for each source, the data take values in some specific range. Let me give you a simplified example of what I mean: Let us have a variable $X$ that we want to use to predict the value of $y$ . Each $(X,y)$ -tuple can come from 4 different data sources. From the source $S_1$ we obtain data for which $X$ takes values mostly in the interval $[30,32]$ and $y$ takes values in the interval $[10,12]$ . From the source $S_2$ we obtain data for which $X$ takes values mostly in the interval $[40,42]$ and $y$ takes values in the interval $[14,16]$ . From the source $S_3$ we obtain data for which $X$ takes values mostly in the interval $[50,52]$ and $y$ takes values in the interval $[7,9]$ . From the source $S_4$ we obtain data for which $X$ takes values mostly in the interval $[60,62]$ and $y$ takes values in the interval $[11,13]$ . I am not using the source $S_i$ as a training variable. If I train a model over this data, it works fine, because it does the following: it learns that if $X$ is in $[30,32]$ , then it should return a value in between $[10,12]$ , if $X$ is in $[40,42]$ it should return a value in between $[14,16]$ , etc . That is, even if I am not saying to the model from which source the data come, it learns the source from the data and acts as if we had a different model for each source. Of course, if I remove one of the sources, train the model over the data coming from the other three and test it on the one I left out, the predictions are not good, because the model never learned how to predict values in that range. I know (or I strongly believe) that there are some relations in the variables that would allow building a decent model regardless of the source, but to use such a complex model as LSTM NN leads to overfitting for each source. Of course, I know that I could try with a simpler, more interpretable model, where the predictions for the input data that fall out of the clusters where nicely interpolated from the ones inside the clusters. But, assuming I want to keep the LSTM model, what options do I have? I was thinking about some kind of oversampling technique aimed at generating training data for the values of $X$ that fall outside the intervals. Is there a good way of tackling my problem?
