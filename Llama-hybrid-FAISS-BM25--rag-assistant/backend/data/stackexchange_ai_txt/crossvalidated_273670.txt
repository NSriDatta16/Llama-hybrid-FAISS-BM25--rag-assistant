[site]: crossvalidated
[post_id]: 273670
[parent_id]: 273484
[tags]: 
Neural language model or another model takes numbers as input and they do not process/work directly over the string (or text ) so you need to convert them into the required format. One common approach is using bag of words model that you can create by counting the number of times a word is coming into the sentence and making a one hot vector with dimension equals to whole vocabulary. Simply you can use CountVectorizer of sklearn in Python to do this all work for you. from sklearn.feature_extraction.text import CountVectorizer Another better approach is making vector of each words using word2vec or Glove. You can use pretrained vectors or you can train your own if your data size is large. You can generate word vectors of each word using spacy library that is beautiful and come with internal feature of generating vectors. To use it you can follow following syntax. from spacy.en import English parser = English() parser.vocab['word'].vector You can combine different word vectors into a fixed dimension for a sentence and then can get ready to apply the algorithms for getting outputs. Coming to your main question how to get output probabilities. You can start with basic recurrent neural network language learning model. You may need a deep learning library like Tensorflow / Keras. In Tensorflow let me tell you code then explain it cell = tf.contrib.rnn.GRUCell(state_size) rnn_outputs , final_state = tf.nn.dynamic_rnn(cell,rnn_inputs,seq_len,dtype=tf.float32) with tf.variable_scope('softmax'): W = tf.get_variable('weight',[state_size,num_classes],initializer=tf.constant_initializer(0.0)) b = tf.get_variable('bias', [num_classes],initializer = tf.constant_initializer(0.0)) logits = tf.matmul(final_state, W) + b preds = tf.nn.softmax(logits,name='poiu') First you have to define a cell there are many types like LSTM, GRU and other variants. These are actually used to store information and solve RNN training problems of vanishing and exploding gradients, which in turn makes it possible to learn long-term relationships among sentences. Next step is to find the output after your data has been passed through a neural network. After that you define a trainable variable that network uses to save the various learnt parameters that can be used for prediction and all. Final step is to pass all the learnt parameters through a softamx layer that is used to calculate what is the probability of each sentence belonging to a class. Note: After all this you have to use/define a loss function that is actually through which networks learn. loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels="your output")) You can search over internet for various resources related to this. https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words http://karpathy.github.io/2015/05/21/rnn-effectiveness/ http://r2rt.com/ # For tensorflow implementation guide
