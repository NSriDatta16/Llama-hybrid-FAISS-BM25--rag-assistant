[site]: crossvalidated
[post_id]: 423192
[parent_id]: 365195
[tags]: 
If I have understood the question as intended, you have in mind a setting in which you can obtain independent realizations of any random variable $X$ with any distribution $F$ (having finite variance $\sigma^2(F)$ ). The "game" is determined by functions $h$ and $\mathcal L$ to be described. It consists of the following steps and rules: Your opponent ("Nature") reveals $F.$ In response you produce a number $t(F),$ your "prediction." To evaluate the outcome of the game, the following calculations are performed: A sample of $n$ iid observations $\mathbf{X}=X_1, X_2, \ldots, X_n$ is drawn from $F.$ A predetermined function $h$ is applied to the sample, producing a number $h(\mathbf{X}),$ the "statistic." The "loss function" $\mathcal{L}$ compares your "prediction" $t(F)$ to the statistic $h(\mathbf{X}),$ producing a non-negative number $\mathcal{L}(t(F), h(\mathbf{X})).$ The outcome of the game is the expected loss (or "risk") $$R_{(\mathcal{L}, h)}(t, F) = E(\mathcal{L}(t(F), h(\mathbf{X}))).$$ Your objective is to respond to Nature's move by specifying some $t$ that minimizes the risk. For example, in the game with the function $h(X_1)=X_1$ and any loss of the form $\mathcal{L}(t, h) = \lambda(t-h)^2$ for some positive number $\lambda,$ your optimal move is to pick $t(F)$ to be the expectation of $F.$ The question before us is, Do there exist $\mathcal{L}$ and $h$ for which the optimal move is to pick $t(F)$ to be the variance $\sigma^2(F)$ ? This is readily answered by exhibiting the variance as an expectation. One way is to stipulate that $$h(X_1,X_2) = \frac{1}{2}(X_1-X_2)^2$$ and continue to use quadratic loss $$\mathcal{L}(t,h) = (t-h)^2.$$ Upon observing that $$E(h(\mathbf{X})) = \sigma^2(F),$$ the example allows us to conclude that this $h$ and this $\mathcal L$ answer the question about variance. What about the standard deviation $\sigma(F)$ ? Again, we need only exhibit this as the expectation of a sample statistic. However, that's not possible, because even when we limit $F$ to the family of Bernoulli $(p)$ distributions we can only obtain unbiased estimators of polynomial functions of $p,$ but $\sigma(F) = \sqrt{p(1-p)}$ is not a polynomial function on the domain $p\in (0,1).$ (See For the binomial distribution, why does no unbiased estimator exist for $1/p$? for the general argument about Binomial distributions, to which this question can be reduced after averaging $h$ over all permutations of the $X_i.$ )
