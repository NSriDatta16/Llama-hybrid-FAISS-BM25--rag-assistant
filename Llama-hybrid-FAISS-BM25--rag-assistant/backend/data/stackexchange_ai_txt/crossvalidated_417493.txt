[site]: crossvalidated
[post_id]: 417493
[parent_id]: 319041
[tags]: 
You are right that, when fitting a line through points, the orthogonal distance is the most natural loss function that can be applied to arbitrary lines (note that the y-distance becomes meaningless for lines perpendicular to the x-axis). This problem is known under a number of names, e.g. "orthogonal regression", or (the most used term, AFAIK) "Principal Component Analysis" (PCA). For a discussion of this problem in arbitrary dimenstions, see Späth: "Orthogonal least squares fitting with linear manifolds." Numerische Mathematik 48, pp. 441–445, 1986 As @aginensky already pointed out, the idea behind Linear Regression is not to fit a line through points, but to predict y-values for given x-values. That's why only the distance in y is used, which is the prediction accuracy. Reformulating the problem of fitting a curve $\vec{x}(t)$ through points $\vec{p}_i$ , $i=1\ldots N$ as a prediction problem makes things complicated, because the predictor $t$ is unknown and even to some degree arbitrary. For curves other than straight lines, this is still a problem that is subject to active research. One possible (incomplete) approach is described in the following article, which is incomplete because it does not provide a solution for finding an initial guess for the curve, but only how to iteratively improve such an initial guess: Wang, Pottmann, Liu: "Fitting B-spline curves to point clouds by curvature-based squared distance minimization." ACM Transactions on Graphics 25.2, pp. 214-238, 2006
