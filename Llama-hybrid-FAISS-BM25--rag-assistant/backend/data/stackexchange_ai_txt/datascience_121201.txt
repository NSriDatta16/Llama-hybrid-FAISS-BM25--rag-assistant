[site]: datascience
[post_id]: 121201
[parent_id]: 121200
[tags]: 
Note that: In your first and second GPT-2 links, the logic to feed data to the model is handled by the Trainer class , that's why they don't need to explicitly prepare the input and output data and give it to the model. In your third GPT-2 link , you can find the place where the expected output (i.e. labels) is passed to the model (which internally shifts them to meet the actual expectations of the Transformer decoder): outputs = model(input_tensor, labels=input_tensor) Each implementation is different, even for the same model. Looking for known structures in the code is usually effective, but sometimes it is not, and you need to actually dive into the code to understand what it does.
