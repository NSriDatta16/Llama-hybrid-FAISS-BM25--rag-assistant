[site]: crossvalidated
[post_id]: 603599
[parent_id]: 603590
[tags]: 
The absolute value function can be written as $$|a-b|=\text{ReLU}(a-b) + \text{ReLU}(b-a),$$ and has a minimum at 0 for $a = b$ . We can compose this with a sigmoid layer $$\sigma\left(\delta\left(\text{ReLU}(a-b) + \text{ReLU}(b-a)+\epsilon \right) \right),$$ and this is very close to what is desired for $\epsilon because this shifts the minimum below 0 and choosing $\delta means that a negative value maps to a number greater than 0.5, and a positive value to a number less than 0.5. Naturally, there will be "wrong answers" for $|a-b|$ that are close to $\epsilon$ . This is unavoidable with continuous functions (such as those used in neural networks). Changing the magnitude of $\epsilon$ controls this. A deficiency with this is that its outputs are not exactly 0 and exactly 1. You can't obtain these values exactly because the sigmoid function only obtains 0 and 1 in a limit of infinitely large or small values. It's probably also hard to train a neural network to find weights that work well, especially for representing $|a-b|$ .
