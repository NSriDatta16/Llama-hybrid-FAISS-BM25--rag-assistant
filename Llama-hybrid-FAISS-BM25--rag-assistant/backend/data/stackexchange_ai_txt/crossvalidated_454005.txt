[site]: crossvalidated
[post_id]: 454005
[parent_id]: 
[tags]: 
manifold representation in latent space of autoencoder for anomaly detection

Autoencoders are sometimes used for anomaly detection. Two methods used are loss difference (taking the reconstruction error and using that to determine if it is an anomaly, which I understand) and latent space clustering methods. In latent space clustering, you train the AE on the inlier data only, then train some second model on the latent representations of the outliers vs the latent representations of the standard data, which theoretically should be different and easy to classify, even with some linear model. My question for this is, why would this work, and is this a reliable technique? When encoding some signal (lets say we're working with images of peoples' faces, and the outlier pictures have some yellow dot in the background), the best encoding format would learn the variations of the features of just the inlier data - so for this example it would encode things like skin tone, eye location, etc. of just the regular faces. If we are learning just the variations in the inlier data, isn't there no real reason for the latent space of the outliers to be any different than the latent space of the inliers, since the varying factors of the outliers are not learned at all? Essentially what I can't wrap my head around is that the factor that makes a signal an outlier does not have to be a varying factor in the inlier group, and thus there doesn't really need to be a reason to learn it at all. This is actually the reason that loss difference methods work, where the AE learns the structure of the inlier data and fails to reconstruct some outlier properly, and when the reconstruction error spikes, one can properly conclude that there is an outlier. Even if the outlier factor has something to do with a learned inlier feature (example, large eyes taking up 50% of head in outlier group vs inliers being regular headshots), isn't there still no real reason the latent space should be any different? The sub-signal that makes the entire signal (lets say sub-image of an image) an outlier still just gets thrown through the standard calculation for an inlier signal, and can output as whatever - there is no reason the latent representation specifically has to be distinct from the inlier representation for this, especially when it hasn't trained on this trait before. It can just "fold" this over the inlier manifold and project as just a regular point just fine. How come I see these methods come around sometimes? Thanks for the help.
