[site]: crossvalidated
[post_id]: 229993
[parent_id]: 
[tags]: 
Use of super/sub sampling techniques in survival analysis?

I just have a quick question on using sampling techniques in survival analysis. I am currently working on a project where we are trying to predict which patients should get a specific treatment. I have done KM curves, and a Cox analysis, all seeming to confirm what we have previously shown and are trying to nail down. We are also trying to use a Multi-task Logistic Regression technique called patient specific survival prediction (PSSP: http://pssp.srv.ualberta.ca/ ) We currently only have a small dataset (~200 patients, with 50 events). We are currently running PSSP on the dataset with 13 features (recorded medical data). Due to the small number of events to censored data about 1:4, my classification results were not what I had expected. So I wanted to try some techniques to help balance the classes. I looked into techniques to either subsample the larger class or boost the rarer class with more examples can be used. I did this using a Synthetic Minority Oversampling Technique (SMOTE) and ENN algorithms implemented in python. Using these sampling algorithms improved my performance as I had predicted they would now that the classes were closer to balanced at the cutoff points we looked at. So the Question is: Is it feasible to use upsampling and downsampling techniques with survival data in order to balance the classes for prediction algorithms, or are we inherently biasing our classifier due to the synthetically generated patients with events. And If we are biasing the classifier, how can we tell? First time posting to a stack exchange site, please let me know. Thanks.
