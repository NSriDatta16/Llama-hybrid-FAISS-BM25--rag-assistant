[site]: crossvalidated
[post_id]: 618031
[parent_id]: 
[tags]: 
correct usage of nested CV

I am a beginner in Machine Learning and I want to compare prediction results of multiple regression models (such as Ridge, Lasso, ANN...) using Nested Cross-Validation on a time series dataset. I have searched and reviewed several online articles and come up with the following procedure (or algorithm Iâ€™d better say). The purpose of the procedure is to find best MSPE for all given regression models using nested CV. Assume that our time series data is loaded into dataSet . I call split function to divide data set into two parts: main Training set (~70%) and Test set (~30%) def split(self): trainRows = round(0.7 * self.dataSet.shape[0]) self.mTrainSet = self.edaData.loc[0:trainRows - 1, :] self.mTestSet = self.edaData.loc[trainRows:, :] pass then I call the following iterate function. I added a few comments to clarify each step. # def iterate(self) -> float: # defining outer (or parent) cross-validator (10 folds cv) tscv_ifold: TimeSeriesSplit = TimeSeriesSplit(n_splits= 10) if (self.outerCV == None) else self.outerCV # defining inner (or child) cross-validator (5 folds cv) tscv_jfold: TimeSeriesSplit = TimeSeriesSplit(n_splits= 5) if (self.innerCV == None) else self.innerCV # independent variable name yName = 'Price (USD)' # {Date} field must be removed because of {StandardScaler} and # regression calculation self.mTrainSet.drop(['Date'], axis= 1, inplace= True) self.mTestSet.drop(['Date'], axis= 1, inplace= True) # stores MSPE(s) as well as regression models' tuning parameters for outer iteration(s) outerResults: list[RegressionResult] = [] # iterating over parent folds for i, (iTrainIndices, iTestIndices) in enumerate(tscv_ifold.split(self.mTrainSet)): iTrainSet = self.mTrainSet.loc[iTrainIndices] iTestSet = self.mTrainSet.loc[iTestIndices] # standardaization scaler = StandardScaler().fit(iTrainSet) iStandardTrainSet: pa.DataFrame = pa.DataFrame(scaler.transform(iTrainSet), columns= iTrainSet.columns) iStandardTestSet: pa.DataFrame = pa.DataFrame(scaler.transform(iTestSet), columns= iTestSet.columns) # stores MSPE(s) as well as regression models' tuning parameters for inner iteration(s) innerResults: list[RegressionResult] = [] # defining inner (or child) cross-validator (5 folds cv) for j, (jTrainIndices, jTestIndices) in enumerate(tscv_jfold.split(iStandardTrainSet)): jTrainSet = iTrainSet.loc[jTrainIndices] jTestSet = iTrainSet.loc[jTestIndices] jyTrain: pa.Series = jTrainSet[yName] jxTrain: pa.DataFrame = jTrainSet.drop([yName], axis= 1) jyTest: pa.Series = jTestSet[yName] jxTest: pa.DataFrame = jTestSet.drop([yName], axis= 1) # running the inner regression and storing the result innerResults.append(self.regress(jxTrain, jyTrain, jxTest, jyTest, tscv_jfold)) # finding the least MSPE for current j iteration jx = min(range(len(innerResults)), key= lambda i: innerResults[i].mspe) jBestTuningArgs = innerResults[jx].args iyTrain: pa.Series = iStandardTrainSet[yName] ixTrain: pa.DataFrame = iStandardTrainSet.drop([yName], axis= 1) iyTest: pa.Series = iStandardTestSet[yName] ixTest: pa.DataFrame = iStandardTestSet.drop([yName], axis= 1) # calculation A # running the outer regression and storing the result regResultA = self.regress(ixTrain, iyTrain, ixTest, iyTest, tscv_ifold) # calculation B # running the outer regression with calculated best-tuning-args and storing the result regResultB = self.regressEx(ixTrain, iyTrain, ixTest, iyTest, jBestTuningArgs) # storing the regression with the best MSPE for current i iteration outerResults.append(regResultA if (regResultA.mspe regress and regressEx functions are as follow; # regression calculation def regress(self, xTrain: pa.DataFrame, yTrain: pa.Series, xTest: pa.DataFrame, yTest: pa.Series, validator: TimeSeriesSplit ) -> RegressionResult: if (validator != None): # alpha = lambda ** (-1) cv = RidgeCV(alphas= np.arange(0.5, 2, 0.01), cv= validator, scoring= 'neg_mean_squared_error') # providing x train set as well as y train set (or series) cv.fit(xTrain, yTrain) # alpha is known here using cross-validation model: Ridge = Ridge() if (validator == None) else Ridge(alpha= cv.alpha_) model = model.fit(xTrain, yTrain) guess = model.predict(xTest) error = mean_squared_error(yTest, guess) _alpha = cv.alpha_ if (validator != None) else None return RegressionResult(mspe= error, alpha= _alpha) and # extended regression calculation def regressEx(self, xTrain: pa.DataFrame, yTrain: pa.Series, xTest: pa.DataFrame, yTest: pa.Series, tuningArgs: dict ) -> RegressionResult: # alpha is known here using provided tuning parameters if ((tuningArgs != None) and (tuningArgs['alpha'] != None)): _alpha = tuningArgs['alpha'] else: _alpha = None model: Ridge = Ridge() if (_alpha == None) else Ridge(alpha= _alpha) model = model.fit(xTrain, yTrain) guess = model.predict(xTest) error = mean_squared_error(yTest, guess) return RegressionResult(mspe= error, alpha= _alpha) Would you please someone let me know if this procedure is correct?
