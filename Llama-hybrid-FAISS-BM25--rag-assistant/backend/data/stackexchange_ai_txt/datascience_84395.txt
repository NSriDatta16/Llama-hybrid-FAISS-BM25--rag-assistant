[site]: datascience
[post_id]: 84395
[parent_id]: 
[tags]: 
What is the next step after k fold CV?

I came across this video lecture https://www.youtube.com/watch?v=wjILv3-UGM8 on k fold cross validation (CV). The algorithm given in the video lecture is presented below: for k = 1:5 train on all except k get model $M_{\tilde{k}}$ calculate accuracy on $k$ as $A_k$ end Calculate final cross validation accuracy: $A = > \frac{1}{5}\sum_{k=1}^5 A_k$ This is quite clear to me. Here $M$ is I guess just a single type of ML algorithm. However at time stamp 6:35 the presenter raises the question that what do we do with all the 5 different models that were built? According to him, we either combine all the models and make decision based on that or take the best model out of the 5 . Is this statement true? In many sites including here ( https://stats.stackexchange.com/questions/310953/doubt-about-k-fold-crossvalidation?noredirect=1&lq=1 ; https://stats.stackexchange.com/questions/11602/training-on-the-full-dataset-after-cross-validation and https://stats.stackexchange.com/questions/11602/training-on-the-full-dataset-after-cross-validation ) and research papers I have understood that: -- for doing model training using k fold CV, we re-train on the entire dataset after the end of the CV loop and that is the final model. -- We do not select any model from inside the CV loop if the idea of doing CV training is to check the accuracy of the ML algorithm on the entire dataset. -- However, if we have multiple ML algorithms say random forest, neural network, SVM inside the CV loop then we select the algorithm with the highest accuracy. -- Another technique, nested cross-validation is used for hyperparameter tuning. Is my understanding correct?
