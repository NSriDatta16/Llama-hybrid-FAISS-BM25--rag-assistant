[site]: crossvalidated
[post_id]: 159647
[parent_id]: 
[tags]: 
Why the Brier Score's better when probabilities are estimated through PAVA instead of Platt Scaling?

I've been studying (and applying) SVMs for some time now, mostly through kernlab in R . kernlab allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002). I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm. I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004). Predictions were made on the training set. I set the Cost really low C=0.001 to try to get some misclassifications. The Brier Score is defined as: $$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$ Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$. require(isotone) require(kernlab) ##PAVA SET/VER data1 Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me. BRIER.PAVA [1] 0.09801759 BRIER.PLATT [1] 0.6710232 The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling. If you check PRED you will see all probabilites fall on the ~0.33 range, while on PROB more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low C . References: Zadrozny, B., and Elkan, C. "Transforming classifier scores into accurate multiclass probability estimates." Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002. T.-F. Wu, C.-J. Lin, and Weng, R.C. "Probability estimates for multi-class classification by pairwise coupling." The Journal of Machine Learning Research 5 (2004): 975-1005. EDIT: Also, if you check the AUC of the different probabilities, they are quite high. requires(caTools) AUC.PAVA And here's the result > colMeans(AUC.PAVA) setosa versicolor virginica 0.9988667 0.9988667 0.8455333 > colMeans(AUC.PLATT) setosa versicolor virginica 0.8913333 0.8626667 0.9656000 Looking at these AUC, I would say Platt Scaling is a really underconfident technique.
