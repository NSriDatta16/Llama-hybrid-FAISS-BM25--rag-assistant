[site]: datascience
[post_id]: 111743
[parent_id]: 
[tags]: 
How to deal with different amounts of data every day?

I am doing a time series prediction task. There are different amounts of news headlines every day, and the goal is a binary prediction task to predict next day's stock movement. The amount of headlines varies everyday. There might be 5 headlines, 6 headlines or more for one day. I am planning to embed each headline into a vector space of, for example, 300 dimensions. How shall I deal with it? As far as I know, neural networks require a fixed size of input. Should I pad my data? For example, there are at most 10 headlines everyday, so should I pad my data into size of [10, 300] for every day? PS: I don't want to compute the average of the embeddings because I want to know the impact of each news healine later.
