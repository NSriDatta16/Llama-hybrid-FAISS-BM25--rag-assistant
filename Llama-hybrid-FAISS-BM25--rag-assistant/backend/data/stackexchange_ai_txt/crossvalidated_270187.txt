[site]: crossvalidated
[post_id]: 270187
[parent_id]: 
[tags]: 
SVM: Why does the number of support vectors decrease when C is increased?

I am learning how to use libsvm through sklearn.svm in python. I read here about what happens and why when you change the C value as part of your model. My intuition from what I've learned would be that lower C values would use less support vectors to make a more general classification, while higher C values would use more support vectors to attempt to 'overfit' and account for all outliers. That is not the case. For an example where I looped through a set of C values like so: print(c) model = svm.SVC(kernel='linear', C=c) model.fit(Xtrain, ytrain) print("support vectors:", len(model.support_)) I got results: 1.0 support vectors: 1810 10.0 support vectors: 1750 100.0 support vectors: 1626 1000.0 support vectors: 1558 As you can see, as C goes up, the number of support vectors used in the model goes down. Why does this happen?
