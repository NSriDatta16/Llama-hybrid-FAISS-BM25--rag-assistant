[site]: crossvalidated
[post_id]: 569199
[parent_id]: 
[tags]: 
What is the use of expected value in machine learning models?

I see that we have a concept called expected value being used in machine learning (ML) models. For example, SHAP has a concept called Expected value. It means when all input features are 0, we can consider model to output the expected value (baseline value). Similarly, in linear regression, we have a term called intercept. Intercept also means the same, which is expected model output when all input features are zero. While, I understand that in real world we may not encounter this scenario of all input features being zero. Why do we consider this base performance? Is it because, we assume that all models will have some inherent power to predict output (on random). So, we have that base value? Like a student who doesn't prepare for exams, can still get some marks (>0). Is that the same understanding here?
