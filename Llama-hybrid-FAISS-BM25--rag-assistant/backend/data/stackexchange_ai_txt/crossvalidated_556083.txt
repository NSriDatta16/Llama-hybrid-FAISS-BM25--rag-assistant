[site]: crossvalidated
[post_id]: 556083
[parent_id]: 555999
[tags]: 
From a Bayesian perspective, you just want to compute $P(X > 10)$ given a series of measurements of $X$ . There may not exist a sample size for which $P(X>10) if $P(X>10)$ is in fact greater than 0.01! For example, you take 10,000 measurements of $X$ and they're all greater than 10. You'd have to have a pretty skewed prior to conclude $P(X>10) . In general, $X>10$ is a binary event, and computing $P(X>10)$ is typically performed via a Beta distribution . You can find an open source implementation of this exact functionality here , with an accompanying introduction and tutorial . While those are geared toward evaluating performance metrics, the same process applies to your problem. UPDATE : The pdf you cited is asking "Given a prior on $P(X>10)$ , what sample size should I expect to need to verify that $P(X > 10) with some higher degree of certainty (technically they're asking how many required to narrow margin of error of $P(X > 10)$ by some amount, but I think that's the best way to translate that source onto your problem). This effectively gives rise to a compound experiment: Sample a possible P(X > 10) from the prior. 1 Simulate hypothetical future samples until your posterior estimate of $P(X > 10)$ (prior combined with new synthetic samples) is less than 0.01. Record the number of synthetic samples you needed to reach that conclusion. Repeat steps 1 and 2 and build a histogram of how many synthetic samples you needed. From there, you could pick the most common number of samples needed, or the upper $\alpha$ quantile of your histogram of samples (i.e. $\alpha$ chance that this sample size will be sufficient to establish $P(X > 10) with some threshold of certainty). The pdf you linked seems to effectively describe ways to streamline that procedure combined with different ways to extract a sample size from that histogram. But I think the core concept is accurately outlined with the above experiment. That said, this may or may not be the optimal approach to your particular problem. If you have to set out funding for a large, expensive, time consuming study ahead of time, sure you'd want to try to figure out what sample size you'd need to verify a claim expected to be true to some greater degree of certainty than you have. However, if you can get feedback as you go, you could effectively just run one real experiment instead of a large $N$ synthetic ones from step 1 above. That is, just keep measuring parts until your posterior estimate (as described in my first answer) of $P(X > 10) falls below your chosen threshold of certainty. 1 In the paper, this would be a distribution implied from a previous study. In your example, it's your best idea of what the distribution of $P(X > 10)$ is. You'd typically use a beta distribution for this as described above. For example, if you recall having witnessed 10 working parts, and one nonfunctional one, you could use Beta(10, 1). See https://en.wikipedia.org/wiki/Additive_smoothing#Pseudocount for an overview of this heuristic. This distribution represents your uncertainty of $P(X > 10)$ , but since it's expressed as a probability distribution, you can sample it. This simulates possible real world values of $P(X > 10)$ based on your current understanding.
