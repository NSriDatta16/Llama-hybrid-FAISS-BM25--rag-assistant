[site]: crossvalidated
[post_id]: 40595
[parent_id]: 40570
[tags]: 
I would argue strongly that is should apply only at the interpretation level. Multiplicity implicitly involves the definition of an investigation by an investigator(s) (i.e. the study wise error rate to be controlled) and needs to accurately reflect the intentions that drove the process of generating inputs to the inference/decision. (This is a bit slippery and for instance Wittgenstein admitted late in his career that he regretting not realizing intentionality early in logic.) For instance, if someone intended to do all the comparisons but stopped with the first one because it was so good – this is a multiplicity to be dealt with. On the other hand if that comparison was credibly documented as the only one to be made – there isn’t. It should not matter if the data entry clerk who was taking a statistics course without permission ran all possible comparisons as an exercise. That sounds like your situation to me. ( This judgement can very slippery and thanks to user603, I can point to Jake's birthday as a good example http://www.johndcook.com/blog/2012/09/07/limits-of-statistics/ ) Something like this happened to an early colleague. They want to test A versus placebo but someone wanted them to includ B as well. They thought B was silly but being a nice guy included the B group. The result was A versus placebo was clearly significant but not after adjusting for B. They could never get the study published because of that. Also, Ed George had a nice talk at the joint meeting this summer where he was in effect arguing for an analysts' posterior for those who have access to the data versus a reported posterior for those who only find out about the study if it is selectively reported to them. Thinking about his talk afterwards and that slippery intentionality stuff possibly also applying to the analysts, the "Men in Black" movie seemed relevant or at least their use of the Neuralizer - http://en.wikipedia.org/wiki/Neuralizer It’s as if the Bayesian analyst knows they would have been neuralized as soon as they realized a given data set did not achieve some pre-set goal. So when they get a data set that does meet it, they realize they don't know how often they have been neuralized but they know the selection rule for avoiding being neuralized this time.
