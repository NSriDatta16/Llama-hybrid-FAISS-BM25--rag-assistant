[site]: crossvalidated
[post_id]: 350089
[parent_id]: 341739
[tags]: 
Regarding question (2), vanishing/exploding gradients happen in LSTMs too. In vanilla RNNs, the gradient is a term that depends on a factor exponentiated to $T$ ($T$ is the number of steps you perform backpropagation) [1]. This means that values greater than 1 explode and values less than 1 shrink very fast. On the other hand, gradients in LSTMs, do not have a term that is exponentiated to $T$ [2]. Therefore, the gradient still shrinks/explodes, but at a lower rate than vanilla RNNs. [1]: Pascanu, R., Mikolov, T., Bengio, Y., On the difficulty of training Recurrent Neural Networks, Feb. 2013 - https://arxiv.org/pdf/1211.5063.pdf [2]: Bayer, Justin Simon. Learning Sequence Representations. Diss. München, Technische Universität München, Diss., 2015, 2015 - mentioned in https://stats.stackexchange.com/a/263956/191233
