[site]: crossvalidated
[post_id]: 451047
[parent_id]: 450637
[tags]: 
It is often quite beneficial to pass sample weights to a training function or for scoring on a test data set; there is no point in passing them to a scoring function used for predictive purposes. The former can occur in several situations, for example: Each training record can represent one or more identical observations. In this case, we'd want the weight to equal the number of observations corresponding to the training record. Our test data records will presumably also have records that can represent one or more observations, and this should definitely be taken into account when calculating the out-of-sample performance statistics. Prior knowledge may tell us that the variability of the target variable given the features differs significantly across training records. Sometimes this is handled automatically, e.g., with a logistic regression, but sometimes not, e.g., with a least squares objective. In this case, we'd like the weights to be proportional to the inverse of the conditional variance of the target variable, or at least to a rough guesstimate thereof. (See "weighted least squares" for a proof of this.) One could perhaps argue the point, but in this case it seems to me the weights should also be applied to the test data set, otherwise our metrics may be heavily skewed by observations which we have little hope of predicting well, and our modeling effort will likely end up being similarly focused on those observations - even though we know, apriori, that we can't expect to predict them well. The sample may not be representative of the population which is the intended target of the modeling effort. In this case, we'd likely want to increase the weight associated with undersampled subsets of the population. This is done frequently in sample survey analysis, where it may be costly to construct an appropriate stratified sample but post-hoc adjustments are easy to do. Obviously, this reasoning carries over to scoring on a test data set as well. See Ben Reiniger's answer! When scoring takes place for predictive purposes, though, there is no point in weighting any more. The training has already happened. Each record to be scored is scored individually; the results are not combined (across records) into a performance statistic. If I gave the record a "weight" of, say, $2.5$ , what would the scoring algorithm do with it? It is just calculating the prediction based on the feature values in the record; the prediction will be the same regardless of whether the record represents, for example, $3$ observations or $1$ , or whether the error associated with the prediction is likely to be large or small.
