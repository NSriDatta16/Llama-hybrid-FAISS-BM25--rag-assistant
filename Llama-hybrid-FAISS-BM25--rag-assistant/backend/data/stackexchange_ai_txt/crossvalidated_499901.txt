[site]: crossvalidated
[post_id]: 499901
[parent_id]: 
[tags]: 
How much does upcasing benefit NLP?

I have been wokring on an NLP project at work where the available training data has all been preprocessed (upcased, some characters removed etc.). However, I have just been informed that the data I will receive to predict will not be preprocessed in the same manner (ridiculous I know, but operational constraints and the left hand not communicating with the right hand). I suppose my question is, how potentially dangerous is it to try and predict on data that was not processed in the same manner as the training data? I'm thinking very, but since the model has been evaluated on processed data as well, I really have no idea what will happen when we try to apply it to new data. I realize that this question is likely very data dependant and dependant on the preprocessing strategy, but if anyone knows of any literature that discusses this issue that would be great.
