[site]: crossvalidated
[post_id]: 612041
[parent_id]: 579134
[tags]: 
The logistic regression optimizes the log loss between the predicted probabilities and categories (coded as $0$ and $1$ ). The true observations are the $y_i$ , and the predictions are the $\hat y_i = \dfrac{1}{ 1 + \exp\left(-x_i^T\hat\beta\right) } $ , where $\hat\beta$ is the vector of estimated parameters for the regression model. $$ L(y,\hat y) = -\dfrac{1}{N}\overset{N}{\underset{i=1}{\sum}}\left[ y_i\log(\hat y_i) + (1 - y_i)\log(1 - \hat y_i) \right] $$ Notice that this is not the $AUC$ . That is, logistic regression parameter estimation does not seek out the smallest $AUC$ . If some parameter values decrease the $AUC$ yet also decreases the log-loss, those parameter values with the higher $AUC$ will be preferred. This can happen if some ability to discriminate between the categories is exchanged for better calibation of the predicted probabilities, as $AUC$ only cares about the ability for the model to distinguish between the two categories. Therefore... I'm confused as to how the AUC could go lower with the addition of more variables. Even if the new variables have zero predictive power, why wouldn't the coefficients just be set to zero and the AUC stay at ~.83? If you train by maximizing the $AUC$ , you will observe this, short of numerical issues that arise from doing this kind of strange optimization. However, the default of logistic regression is to find the parameters that optimize log-loss, not $AUC$ . Another answer mentions that logistic regression lacks a closed-form solution and must be solved numerically, putting some of the blame on the merely approximate "solution" given as the logistic regression parameters. While it is true that logistic regression lacks a closed-form solution, modern implementations of the numerical optimization are so good that it almost might as well.
