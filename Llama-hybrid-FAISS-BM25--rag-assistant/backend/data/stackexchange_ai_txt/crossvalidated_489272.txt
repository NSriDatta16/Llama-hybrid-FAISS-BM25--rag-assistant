[site]: crossvalidated
[post_id]: 489272
[parent_id]: 489226
[tags]: 
Assuming that you are refering to a fully connected neural network, the key issues to realize here are: $w^l\,\,\,\,\,\rightarrow$ is the matrix of weights that connect the neurons of the layer $l-1$ with the layer $l$ . The most spread notation is to define it as a $\text{J}\times \text{K}$ matrix, where $\text{J}$ is the number of neurons of the layer $l$ and $\text{K}$ Is the number of neurons of the layer $l-1$ . $a^{l-1}\rightarrow$ due to the previous notation of $w^l$ , the vector of activations of the layer $l-1$ Is a $\text{K-dimensional}$ vector. $b^l\,\,\,\,\,\,\rightarrow$ Also, due to the previous notation, the vector of biases of the layer $l$ is a $\text{J-dimensional}$ vector. $z^l\,\,\,\,\,\,\rightarrow$ Has to be a $\text{J-dimensional}$ vector. With this said, let's have a look at what you are proposing: $$ z^l = a^{l-1}w^l + b^l$$ The only situation that I can come up with where this expression can work is if we are calculating each neuron's $z^l_j$ individually, and we also would have to change the usual notation $\rightarrow$ the former $a^{l-1}$ would have to be tranposed, and we would have to work with the vector of weights of neuron $j$ rather than with the weight matrix of layer $l$ . Finally $b^l_j$ would have to be of course a scalar as it represents the bias of only the neuron $j$ . So to sum up , changing the usual notation is possible but it may become less efficient. As we have saw above, with the proposed equation, we can no longer work with matrices $\rightarrow$ the computation become more slower.
