[site]: crossvalidated
[post_id]: 561528
[parent_id]: 
[tags]: 
No Free Lunch Theorem and Random Search

Recently, I posted a question about a very interesting property of the Random Search Algorithm ( The "Amazing Hidden Power" of Random Search? ), in particular - in the context of function optimization, regardless of the number of dimensions your data set has, Random Search can return a result that has a 0.95 probability of being in the top 5% of all solutions, in only 60 iterations! However, many people provided very useful analysis and comments on this property of Random Search, indicating that in most real world problems, solutions within the top 5% are often insufficient - and we often want solutions in the top 1%. Unfortunately, the number of iterations required to reach a solution in the top 1% of all solution far exceeds (exponentially) 60 iterations. One of the users (@ Thomas Lumley) posted an (amazing) answer to this question that related Random Search to the (famous) "No Free Lunch Theorem": Regarding this (amazing) answer, I am trying to understand the "nature of the discrete function F", and as well why the proposition outlined is true (i.e. Over all Functions, Random Search will perform the best). First Part: Regarding the first part, here is how I understand the set-up: Suppose you have a class of functions (N number of such functions exist) where the input can be all strings of length 3 involving the first 3 letters of the (English) Alphabet "A, B, C" - and the output of this class of functions is a discrete number between 0-1000. From this set of functions, if you you take one of these functions (e.g."F1"), and using the Random Search algorithm, randomly evaluate F1 for "n" iterations - and finally select the iteration which gave you the largest value within these "n" iterations. Is the gist of the example? Part 2: How is it possible to prove that in the above example, for all functions "F1 to Fn" Random Search will definitely perform better than any other optimization algorithm? Informally, I can accept this fact - perhaps some of these functions might have completely random structures in which some optimization algorithm other than Random Search (e.g. Gradient Descent) might be at a disadvantage - but using math, how can we prove that in the above example, Random Search will provide the best possible results when averaged over all such functions? Can someone please comment on this? Thanks!
