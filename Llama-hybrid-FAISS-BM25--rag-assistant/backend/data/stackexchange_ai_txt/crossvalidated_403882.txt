[site]: crossvalidated
[post_id]: 403882
[parent_id]: 403840
[tags]: 
Side note; looks like you've got three extra columns of data... somewhere... oh ya may have got that on the last edit, aside from the slightly wonky spacing it's far more comprehensible. Recommendations based off information so far provided; consider leveraging Word-2-Vec for some of the encoding, eg. x3 through x9 , as it's a simple way of dealing with novel inputs; there'd still be the same number of columns, but the benefits may outweigh the costs of restructuring the network some. in regards to columns x1 and x2 , maybe take another glance at the paper linked to the other question I commented about. If I read it correctly they had some special sauce for feeding raw inputs into their network that maybe inspirational. Information that will likely help with better answers or updates to this one would be; what are you trying to model, eg. are you trying to predict, recognize/categorize, etc.? what's the behavior of the network currently and what are you looking to improve? what libraries, if any, are you using? Simply put, I've found no fool proof , "proper" or the way for massaging inputs, however, depending upon how you frame what you want modeled there maybe options that are an easy fit. Furthermore it may help in being flexible in what it is that you want modeled, try to frame things from a few different angles so to speak. Or for a more tangible example check this Q&A , in particular how the linked to researchers re-framed genetic data into a time-series problem; by being a bit flexible in mindset they where able to do some interesting things swiftly. Hopefully with a little more information someone can come along and do better at providing a direct answer with less hand waving . Updates Okay so with the information now provided from your comments it almost sounds like you may want to train a network to train variations of network parameters. There's many ways that could be modeled; Neuroevolution , Genetic Evolution , etc. and ways of mitigating mutation could even be similar to pruning (or what I think of as intentional brain-damage ) so that offspring networks don't get overly complex. What I'm getting at is that cluster size, scaling factors, and other parameters maybe something that can be sorted through the use of a network higher in the stack that tries fiddling with things for ya; essentially it would be taking the model that you're already happy with, tweak some-thing about it a bit, inbreed those that preformed best, brain damage the mid-age offspring, and repeat. Side note, if ya add more information (such as what I pointed out) to your question it'll definitely help those writing better answers than I.
