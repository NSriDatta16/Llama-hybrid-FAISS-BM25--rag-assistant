[site]: crossvalidated
[post_id]: 592077
[parent_id]: 
[tags]: 
Probability of having a disease when an imperfect test has yielded a negative result twice

I think the answer is already given by this post , but I wanted to check a couple of aspects. A person I know developed a 'cold' after an event with many people. Unfortunately we still have to be wary of Covid in such cases, so he took a first antigen test, which came up negative, and then another one the next day, which also came up negative. Assuming a disease prevalence of 20%, which is close to the positivity rate when and where this took place, and knowing that the antigen test has a sensitivity of 90.1% and specificity of 99.3%, my calculation is: $$P(\text{diseased} | \text{test negative twice}) = \frac {P(\text{diseased}) \cdot P(\text{test negative twice} | \text{diseased})} {P(\text{diseased}) \cdot P(\text{test negative twice} | \text{diseased}) + P(\text{healthy}) \cdot P(\text{test negative twice} | \text{healthy}) } =\\ = \frac {0.2 \cdot (1-0.901)^2} {0.2 \cdot (1-0.901)^2 + (1-0.2) \cdot 0.993^2} \approx 0.25\%$$ Assuming these calculations are formally correct, the doubts I have, on which I would be interested to have some feedback, are: is it correct to assume a prior probability of disease equal to the 'official' positivity rate? I am under the impression that the people who get tested and whose results are published are from a biased population, e.g. they have symptoms or have had risk contacts, so a person taken from a more general population may have a different prior(?) the post I linked above includes the binomial coefficient in the calculation of the probability of observing the actual test results given the true condition; in my example that coefficient is 1 because all tests have the same result; but even in general, for the Bayesian calculation the coefficient should be the same for all terms, so it should always cancel out between numerator and denominator; it should only be required to calculate the TPR/TNR-related powers; or am I missing something? using the binomial probability assumes that tests are independent, i.e. that the result of one test does not affect the results of the other tests; is that always a correct assumption? EDIT : additional question I forgot one important aspect, also mentioned (but not addressed very explicitly) by the other post: the uncertainty on the probability estimate. In my example 2 tests were run and were both negative. If we had got 1 positive and 1 negative: $$P(\text{diseased} | \text{test 1N 1P}) = \frac {P(\text{diseased}) \cdot P( \text{test 1N 1P} | \text{diseased})} {P(\text{diseased}) \cdot P( \text{test 1N 1P} | \text{diseased}) + P(\text{healthy}) \cdot P( \text{test 1N 1P} | \text{healthy}) } =\\ = \frac {0.2 \cdot (1-0.901) \cdot 0.901} {0.2 \cdot (1-0.901) \cdot 0.901 + (1-0.2) \cdot 0.993 \cdot (1-0.993)} \approx 76\%$$ The difference in point estimate is clearly very large between the two cases. And this for me is an issue in practice. I'll explain why. Imagine a case where one has run a single initial test and found it to be negative. By the same calculations as above, it's easy to show that the probability of having the disease is $\approx 2.4\%$ . If one then runs a second test and finds it to be negative again, as shown this makes the probability drop to $\approx 0.25\%$ . However, if the second test comes up positive, the probability of disease is now $\approx 76\%$ ! So OK, I get the binomial variance argument from the post, but I am not sure that is applicable in this scenario. From the point of view of a person taking multiple tests, this mad rollercoaster of probabilities going up and down by orders of magnitude surely doesn't look very conclusive or reassuring, does it? I mean, what is the value of sticking a smallish standard deviation on the current point estimate, when one knows that if the next test is positive, the probability is actually multiplied by 30...? Also, very pragmatically, one may take tests to make impactful decisions, like going to work in presence or not. How does one do that? You could say: repeat the test every 2 days until the probability of having the disease falls below, say, $5\%$ . In this case the person I mentioned might have stopped at the first negative test. But not doing the next test, one runs the risk to miss that the actual probability of disease is much larger. Are there any statistical guidelines for this kind of decisions?
