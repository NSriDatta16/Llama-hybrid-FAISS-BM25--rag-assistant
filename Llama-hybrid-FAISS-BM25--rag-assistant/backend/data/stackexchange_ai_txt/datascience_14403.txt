[site]: datascience
[post_id]: 14403
[parent_id]: 
[tags]: 
Why doesn't training RNNs use 100% of the GPU?

I wonder why training RNNs typically doesn't use 100% of the GPU. For example, if I run this RNN benchmark on a Maxwell Titan X on Ubuntu 14.04.4 LTS x64, the GPU utilization is below 90%: The benchmark was launched using the command: python rnn.py -n 'fastlstm' -l 1024 -s 30 -b 128 How can I diagnose what the bottleneck is?
