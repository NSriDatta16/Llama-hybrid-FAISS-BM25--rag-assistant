[site]: crossvalidated
[post_id]: 287489
[parent_id]: 286797
[tags]: 
My experience is that in general, time varying covariates is implemented & used with little thought and elegance (because the simplest method works ok!). The canonical method is to simply use regular regresion-fitting algos but letting data for each timestep and subject be a row in the dataset. You have to go pretty deep into an absurd frequentist rabbit hole to get this to fit into the rest of regression-puritan theory, and I dont personally grasp why this is legal in most cases but heck - it works. I think this is the reason why you can't find in depth discussions or implementations of it. Thinking about it spoils the fun! If you want to be fancy about it (I would) I'd add survival time (known at prediction time so ok feature) as a variable. The next is something i like to do but not sure about how common it is. I'd also weight each observation s.t: Rows contribute according to how long they would be used i.e time to next datapoint (s.t sum weights per subject proportional to survival time for that subject) . This counters high frequency time-clustered observations getting undue influence. Permits loss to be interpreted as 'time that we're correct' Rows for each subject contribute equally. (Weights per subject sum to 1) Counters that subjects with many observations gets undue influence.
