[site]: datascience
[post_id]: 104656
[parent_id]: 104629
[tags]: 
The arguments that you say are pretty much correct. The primary reason for using pre-trained embeddings is typically the lack of task-specific training data. In tasks that (at least for some languages) have vast amounts of training data, such as machine translation, the word embeddings are always trained jointly with the rest of the model. The mere fact that you are using GloVe indicates that your training data is not that large. In that case, it is very likely that at inference time, words can appear that were not in the training data, but GloVe still has a good representation for them and the rest of the model knows how to use it. GloVe has a really large vocabulary, it is much more likely that there will be GloVe words at inference time than unknown training words. You can also opt for a hybrid solution. If there are words in the training data, that are not in GloVe, but still are frequent enough, you can learn their embeddings jointly with the rest of the model while keeping the rest of the GloVe embeddings frozen. (But implementing this might be a little bit tedious.)
