[site]: datascience
[post_id]: 23838
[parent_id]: 23837
[tags]: 
are there "machine learning" ways to evaluate a pseudorandom number generator? No it is the wrong tool for the job. Statistical tests, like those you mention monobit, runs, poker test etc, are a way to evaluate a PRNG and search for bias or unwanted patterns which would establish that it had flaws. If you try to use ML on random data - e.g. map dependent x,y data to a label z where all are determined randomly - then one sign the data is random will be that the ML does poorly. However, this will not give any measure of quality of the randomness. Using a high variance model it will still be possible to fit the data, although cross validation and test sets will score badly. It might be way over my head, but could a generative adversarial network learn a truly pseudorandom generator that way? A GAN learns how to map a low dimensional space into a manifold (sub-shape) of a larger dimensional space, allowing you to sample from a population in the higher dimension. It cannot inject "noise" into that space other than using something provided already by a PRNG. The GAN itself cannot be the PRNG. The GAN sampling routine is typically driven by an external PRNG (i.e. not driven by the weights or processing of the GAN) Probably you could construct a PRNG from a RNN, although I suspect it would be hard to create one of any quality, and even if you could get something close to statistical randomness from it, the performance is likely to be poor compared to standards such as Mersenne Twister.
