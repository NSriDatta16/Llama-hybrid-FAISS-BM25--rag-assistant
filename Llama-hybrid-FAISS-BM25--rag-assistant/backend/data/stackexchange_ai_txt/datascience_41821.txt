[site]: datascience
[post_id]: 41821
[parent_id]: 
[tags]: 
Minimum numbers of support vectors

I'm trying to understand the concept of SVM. Consider linearly separable data $\{(x_i , y_i )\}_{i=1}^n , x_i \in \mathbb R^d , y_i \in \{âˆ’1, 1\}. \text{Let}\ \ \{x | w^T x + b = 0\}$ be the margin-maximizing hyperplane that separates the data. Intuitively I understand that since there is a classification problem with at least 2 classes, there should be at least two support vectors (one for each class). But is there any formal proof of that (the minimum number of support vectors is 2)? And could there be more than two support vectors? If so, can you give an example?
