[site]: crossvalidated
[post_id]: 55992
[parent_id]: 
[tags]: 
Convergence of batch gradient descent in logistic regression

I am not really sure about how it behaves when using batch gradient descent in logistic regression. As we do each iteration, $L(W)$ is getting bigger and bigger, it will jump across the largest point and $L(W)$ is going down. How do I know it without computing $L(W)$ but only knowing old $w$ vector and updated $w$ vector? If I use regularized logistic regression, will the weights become smaller and smaller or any other patterns?
