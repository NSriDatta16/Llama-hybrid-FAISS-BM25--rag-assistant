[site]: crossvalidated
[post_id]: 592742
[parent_id]: 
[tags]: 
VQ-VAE - why do we need to separate the codebook alignment loss and the commitment loss?

In VQ-VAE , we separate the codebook alignment loss $||sg(z_e(x))-e||^2$ and the commitment loss $||z_e(x)-sg(e)||^2$ where sg stands for the stop-gradient operator, and the loss is $||sg(z_e(x))-e||^2 + \beta ||z_e(x)-sg(e)||^2$ where $\beta$ is a hyper-parameter. I checked and at least for MNIST images reconstruction my model trained quite well if instead of the sum of these two I just used $||z_e(x)-e||^2$ . Why do we bother with the separation and the stop-gradient? just to be able to control the $\beta$ coefficient?
