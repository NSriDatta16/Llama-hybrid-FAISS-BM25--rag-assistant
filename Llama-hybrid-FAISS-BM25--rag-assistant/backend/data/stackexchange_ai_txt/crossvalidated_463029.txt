[site]: crossvalidated
[post_id]: 463029
[parent_id]: 463022
[tags]: 
McFadden's pseudo- $R^2$ is consistent with the log-likelihood model we optimise in logistic regression. The ordinary $R^2$ is consistent with the log-likelihood model for the linear regression. In linear regression, we maximise the log-likelihood: $$ - \sum_i (y_i - \beta x_i)^2 $$ Compare this with the definition of $R^2$ : $$ R^2 = 1 - \frac{\sum_i (y_i - \beta x_i)^2}{\sum_i (y_i - \bar y)^2} $$ In the numerator we have the likelihood of our model, and in the denominator the likelihood of the null-model. McFadden's pseudo- $R^2$ is constructed according to the exactly the same schema, just that the log-likelihoods are defined differently for logistic regression. In fact, one can use McFadden's pseudo- $R^2$ for any log-likelihood-based model, just by plugging in the corresponding log-likelihood function. Maybe "generalised $R^2$ " would be a more appropriate name. Homo/heteroskedasticity is not the actual issue here. In linear regression it is implied in the log-likelihood (actually each term in the sum can be thought of as divided by $2 \sigma^2$ , but that doesn't matter, since it doesn't change the vector $\beta$ ). But, in logistic regression, variance doesn't figure at all.* *Well, it does on a different level, but we have implicitly accepted that as soon as we agreed on using logistic regression in the first place.
