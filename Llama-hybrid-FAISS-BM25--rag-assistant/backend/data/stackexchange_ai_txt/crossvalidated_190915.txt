[site]: crossvalidated
[post_id]: 190915
[parent_id]: 190911
[tags]: 
I like randomForestSRC a lot. It has some really nice plots and diagnostics. There are a lot of choices of how to implement the algorithm. For example, looking at the help pages, rfsrc has a splitrule , where "The default rule is weighted mean-squared error splitting mse". How does randomForest do it? They each can control the size of trees, but do it in two different ways: one by specifying the maximum number of leaves, the other by specifying the maximum depth. There are dozens of such choices, some of which are exposed as parameters (the two I mentioned), but many which are not. So I can't tell you exactly why they produce different results, but a random forest is not a closed-form like, say, OLS, nor is it an optimization (MLE) procedure. It's more algorithmic in nature so there are no mathematical reasons that would force them to agree. Why do you ask? Are you simply looking for an explanation? Your question about forcing the same answer makes me think you're doing something like speed benchmarking and want to compare speeds for reaching exactly the same answer. Or something that might better be explicit. EDIT: OK, per your comment, I'd recommend changing your question to be about the paradox and documenting what you did and your results. My guess is, to the extent that an RV actually helps results, it's because adding an RV might dilute the effect of RF's preferring to split continuous or categorical variables with lots of levels. If so, trying RVs with randomForestSRC with split of zero (usual, deterministic) and non-zero (random splits) might illustrate this.
