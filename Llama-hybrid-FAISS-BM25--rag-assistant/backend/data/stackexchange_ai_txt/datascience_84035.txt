[site]: datascience
[post_id]: 84035
[parent_id]: 64764
[tags]: 
While reading this Nature paper:Explainable AI for Trees: From Local Explanations to Global Understanding . The section 2.7.4 "Local model monitoring reveals previously invisible problems with deployed machine learning models", says the following: Deploying machine learning models in practice is challenging because of the potential for input features to change after deployment. It is hard to detect when such changes occur, so many bugs in machine learning pipelines go undetected, even in core software at top tech companies [78]. We demonstrate that local model monitoring helps debug model deployments by decomposing the loss among the model’s input features and so identifying problematic features (if any) directly. This is a significant improvement over simply speculating about the cause of global model performance fluctuations Then they do 3 experiments with the Shapley values provided by the TreeExplainer We intentionally swapped the labels of operating rooms 6 and 13 two-thirds of the way through the dataset to mimic a typical feature pipeline bug. The overall loss of the model’s predictions gives no indication that a problem has occurred (Figure 5A), whereas the SHAP monitoring plot for room 6 feature clearly shows when the labeling error begins Figure 5C shows a spike in error for the general anesthesia feature shortly after the deployment window begins. This spike corresponds to a subset of procedures affected by a previously undiscovered temporary electronic medical record configuration problem (Methods 17). Figure 5D shows an example of feature drift over time, not of a processing error. During the training period and early in deployment, using the ‘atrial fibrillation’ feature lowers the loss; however, the feature becomes gradually less useful over time and ends up hurting the model. We found this drift was caused by significant changes in atrial fibrillation ablation procedure duration, driven by technology and staffing changes Current deployment practice is to monitor the overall loss of a model over time, and potentially statistics of input features. TreeExplainer enables us to instead directly allocate a model’s loss among individual features
