[site]: crossvalidated
[post_id]: 192616
[parent_id]: 189101
[tags]: 
I see no reason you couldn't formalize this as a discrete Markov decision process , a fundamental tool in reinforcement learning. MDPs are fully specified when one knows: $S:$ a set of states $A:$ a set of actions, sometimes indexed by state $T(s,a,s'):$ a transition function giving the probability of arriving in state $s'$ from state $s$ having taken action $a$ $R(s,a,s')$: a reward function $\gamma$: a discount factor What you call a "strategy" is the optimal policy , a function mapping states to actions, often written $\pi(s)$. Importantly, MDPs are memoryless: The probability of transitioning to a given state depends only on the current state and the actions available. This may seem at odds with the memory of your environment, but it's no obstacle, so long as your current state accounts for all relevant information of prior states. Put another way, the present must recall all relevant history. In your example, that means that each state must account for (1) up to 10 prior agent decisions (2) up to five prior reward criteria. You'd have a large state space, each state corresponding to the trails observed. (If I made no errors on the back of my envelope, you'd have just under 65,000 states.) As for how the environment decides the reward criterion based on trails of alternations or repeat decisions, it can be formalized as part of the transition function. Knowing all that, you could then find the optimal policy through planning algorithms like value iteration or policy iteration. For some useful resources, you might start with Michael Brown's dissertation . If you can program or are willing to learn, you might look at BURLAP , a reinforcement learning library for Java.
