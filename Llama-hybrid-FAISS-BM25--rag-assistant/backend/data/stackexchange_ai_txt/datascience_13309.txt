[site]: datascience
[post_id]: 13309
[parent_id]: 13306
[tags]: 
Gradient boosting learns multiple decision or regression trees after each other. The difference with random forests is that the trees correct each other. Each new tree is fitted on the residual produced by the predictions from the earlier trees. The explain method shows for each prediction (i.e. record) why a particular decision was made. This results in a set of decisions. Each tree makes several decisions. The decisions of all trees are stacked to get the set of rules that justifies a single decision. This information is specific for the gradient boosting model and the specific record. If you want to know more about the gradient boosting model, you may want to consider feature importance. Feature importance expresses how important the features are. You can use print model.get_feature_importance() for this.
