[site]: crossvalidated
[post_id]: 276384
[parent_id]: 262877
[tags]: 
This question certainly deserves an answer, so I will do my best and hope that others will improve this answer. I have been reading the paper by Raftery and Poole which introduced the technique. A variety of applications can be found by searching for "Bayesian melding" but most of them just seem to repeat the notation and content of Raftery and Poole in their methodology sections. The situation is that there is a parameter $\theta$, possibly a vector, another parameter $\phi$, possibly a vector, and a (deterministic) function $M$ such that $\phi = M(\theta)$. In the original example, $\theta=(P_0, MSY)$ where $P_0$ is the size of a whale population and $MSY$ is the maximum sustainable yield (number of whales that can be hunted) and $\phi = P_{1993}$ the population of whales $1993$ years after time zero. The function $M$ is given by a complicated difference equation, iterated $1993$ times. The researcher has some prior information about $\theta$, given by a probability distribution $q_1(\theta)$, and some prior information about $\phi$, given by a probability distribution $q_2(\phi)$. It is desried to make inferences about $\theta$ and $\phi$, given these prior distributions and the function $M$. Unfortunately, the fact that $\phi = M(\theta)$ completely determines the distribution of $\phi$, so there is no way to use the information about $\phi$ given by the distribution $q_2(\phi)$ to conclude anything about $\phi$. The simpler approach is to ignore the prior $q_2(\phi)$ and just make inferences about $\phi$ using the distribution $q_1^*(\phi)$ induced from the fact that $\phi = M(\theta)$ and $\theta \sim q_1(\theta)$. The Bayesian melding approach is to replace the prior on $\phi$ by $$q^{[\phi]}(\phi) \propto q_2(\phi)^{1/2}q_1^*(\phi)^{1/2}$$ which is then also used to get a distribution on $\theta$. The idea seems to be that this distribution somehow combines the prior knowledge about $\phi$ with the fact that $\phi= M(\theta)$, even though these two facts are mathematically incompatible unless $q_2(\phi) = q_1^*(\phi)$. The evidence given in Section 5 of the Raftery and Poole paper seems to be simulating some values of $P_0, MSY$ and $P_{1993}$ for the whales, then adding some noise to $P_{1993}$ and concluding that Bayesian melding gives a better estimate of $P_{1993}$ than the simpler approach, which is not very surprising, since it seems that the added noise is just literally sampled from $q_2(P_{1993})$ in this case. (However, I may well have misunderstood this, and the fact that Bayesian melding seems to be somehat popular suggests that there is more going on here than I have understood.) Edit: from the point of view of a statistician, the more natural thing to do would be to make the function $M$ stochastic in some way, for example by adding noise. After some further thought, it seems that the method can be thought of in this way. Assume that either $\phi = M(\theta)$ (with probability $\alpha$) or $\phi$ is independent of $\theta$ (with probability $1-\alpha$). Then it follows that the distribution of $\phi$ is a mixture $$\alpha q_1^*(\phi) + (1-\alpha) q_2(\phi)$$ and if $q_2(\phi)$ is approximately equal to $q_1^*(\phi)$, which ought to be the case unless the model $M$ is obviously wrong, then this is approximately equal to $$q_2(\phi)^{1-\alpha}q_1^*(\phi)^{\alpha}$$ because $(1+x)^\alpha \approx 1+\alpha x$ when $x \approx 0$.
