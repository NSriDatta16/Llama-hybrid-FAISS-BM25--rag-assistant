[site]: crossvalidated
[post_id]: 67033
[parent_id]: 67014
[tags]: 
You have 31 observations here, not 30. I am not a routine R user but it's clear that your logit model fitted with glm is completely nonsensical as a fit of a logistic curve. The logistic curve is not to be fitted using a glm with logit link. Such a model is usually for binary data, coded 0 or 1, although there is an extension to proportions, but your response variable is neither binary nor proportional. (There is a historical link in that Berkson borrowed the S-shape of the logistic as a suitable link function for modelling binary responses in what we call logit or logistic modelling but the software functions to be used in practice for logistic growth curves are quite different.) As you fitted a model with almost as many parameters as data points its good fit is not surprising. Look at the output: R has evidently treated your identifiers alphabetically, i.e. the ordering is 10, ..., 19, 2, 20, ..., 29, 3, 30, 31, 4, ..., 8. Manifestly block identifier is here a time measure and not categorical. The parameterisation makes no more sense than the model form. I'm surprised that R even accepted such data for a logit model, but as it's quite the wrong model I'll leave comment there. I concur with the pointer given by @tabSF that nonlinear least squares is the easiest way forward. I put your data into Stata and got a nonlinear least squares fit quite readily. There is some choice over parameterisation but here K is an asymptote, A tunes the speed of approach and T is the time at which the count reaches K/2. The numbers supplied on the command line are just guesses to help the iteration process. . nl (users = {K=20000}/(1 + exp(-{A}*(block-{T=15})))) (obs = 31) Iteration 0: residual SS = 2.48e+08 Iteration 1: residual SS = 1.96e+08 Iteration 2: residual SS = 4.49e+07 Iteration 3: residual SS = 4253893 Iteration 4: residual SS = 3998721 Iteration 5: residual SS = 3997420 Iteration 6: residual SS = 3997405 Iteration 7: residual SS = 3997404 Iteration 8: residual SS = 3997404 Source | SS df MS -------------+------------------------------ Number of obs = 31 Model | 2.6465e+09 3 882161243 R-squared = 0.9985 Residual | 3997404.48 28 142764.446 Adj R-squared = 0.9983 -------------+------------------------------ Root MSE = 377.8418 Total | 2.6505e+09 31 85499391.4 Res. dev. = 452.7564 ------------------------------------------------------------------------------ users | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- /K | 16316.17 259.5102 62.87 0.000 15784.58 16847.75 /A | .2569907 .0099346 25.87 0.000 .2366405 .2773408 /T | 17.8977 .2089318 85.66 0.000 17.46972 18.32568 ------------------------------------------------------------------------------ From the numeric output the fit looks spectacularly good but it is vital to plot the data and the fitted curve as well, which are not so encouraging. A sceptic would doubt that the data show clear evidence for levelling off. You should be able to do something similar in R. Don't be terribly surprised if you get small differences in parameter estimates. Nonlinear least squares is a dark art and small differences in algorithm have their consequences.
