[site]: crossvalidated
[post_id]: 591685
[parent_id]: 466336
[tags]: 
Early stopping is very similar to regularisation like Lasso or Ridge. It is similarly reducing the size of the parameters. Below is a sketch for the situation of ordinary least squares (OLS) regression. It shows the cost as function of parameters $\beta_1$ and $\beta_2$ which has a shape of ellipses around the optimal OLS solution. The sets of potential solutions for Lasso, Ridge and gradient descent are similarly creating a descending path towards the optimal OLS solution. In the case of LASSO one of the algorithms to solve the problem, least angle regression ( LARS ), is even literally following a path down towards the OLS solution. The reason that early stopping is mostly used with Neural Networks is probably practical. The optimisation is anyways using gradient descent. The reason that LASSO and Ridge regression are mostly used with linear regression is probably because these methods are more particularly punishing the inclusion of multiple regressor variables (especially LASSO prefers a sparse solution). This can relate to plausible prior assumptions that a solution with few parameters should be preferable (one does not know which regressors are important, but one may have the idea that only a few should be sufficient andany others are just rubbish).
