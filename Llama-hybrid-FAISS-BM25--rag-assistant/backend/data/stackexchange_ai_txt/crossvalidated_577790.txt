[site]: crossvalidated
[post_id]: 577790
[parent_id]: 
[tags]: 
High concurvity/ collinearity between time and temperature in GAM predicting deaths but low ACF. Does this matter?

I have problems with concurvity in my thesis research, and don't have access to a statistician who works with time series, GAMs or R. I'd be very grateful for any (partial) answer, however short. I'll gladly return the favour where I can! For really helpful input I'd be more than happy to offer co-authorship on publication. Deadlines are very close, and I'm heading towards having no results at all if I can't solve this concurvity issue :( I'm using GAMs to try to understand the relationship between deaths and heat-related variables (e.g. temperature and humidity), using daily time series over a 14-year period from a tropical, low-income country. My aim is to understand the relationship between these variables and deaths, rather than pure prediction performance. The GAMs include distributed lag models (set up as 7-column matrices, see code at bottom of post), since deaths may occur over several days following exposure. Simple GAMs with just time, lagged temperature and lagged precipitation (a potential confounder) show very high concurvity between lagged temperature and time, regardless of the many different ways I have tried to decompose time, such as te(year, doy) with a cyclic spline for doy and various choices for k or as s(time) with various k, or as te(year, month) + s(doy) , or as te(year, month) + s(monthday) to name a few, but concurvity with temperature never drops below 0.8 for at least one of the time terms. The autocorrelation functions (ACF) however, shows values close to zero, only just about breaching the 'significance line' in a few instances. It does show patterning though, although the regularity is difficult to define. 1) Should I be worried about the high concurvity, or can I ignore it given the mostly non-significant ACF? I've read dozens of heat-mortality modelling studies and none report on concurvity between weather variables and time (though one 2012 paper discussed autocorrelation). 2) If I cannot ignore it, what should I do to resolve it? I've seen a 2012 paper by Yang et al. that proposes including an autoregressive term based on a moving average. My mathematical knowledge is insufficient to follow their statistical argument. But in a distributed lag model wouldn't this adjust away part of the effect of the exposure (see directed acyclic graph below, in which D is death and E is exposure)? (For those interested in DAGs, distributed lag and autoregressive terms, see paper by Barnett et al (2017) I've also come across sequential regression . Would this be more or less appropriate? If appropriate, a pointer to an example would be really appreciated! Some examples of simplified GAMs and their model output are shown below. (They do not yet include further complicating terms such as specific humidity yet, which is not only highly concurved with time as well, but also shows high concurvity with temperature - which will be the next problem to figure out): conc38b Concurvity for the above model between (temp_max, lag) and (year, month, week, weekday) is 0.91: $worst para te(year,month,week,weekday) te(temp_max,lag) te(precip_daily_total,lag) para 1.000000e+00 1.125625e-29 0.3150073 0.6666348 te(year,month,week,weekday) 1.400648e-29 1.000000e+00 0.9060552 0.6652313 te(temp_max,lag) 3.152795e-01 8.998113e-01 1.0000000 0.5781015 te(precip_daily_total,lag) 6.666348e-01 6.652313e-01 0.5805159 1.0000000 Output from gam.check() : Method: REML Optimizer: outer newton full convergence after 16 iterations. Gradient range [-0.01467332,0.003096643] (score 8915.994 & scale 1). Hessian positive definite, eigenvalue range [5.048053e-05,26.50175]. Model rank = 544 / 544 Basis dimension (k) checking results. Low p-value (k-index Some output from summary(conc38b) : Family: Negative Binomial(16.731) Link function: log Formula: deaths ~ te(year, month, week, weekday, bs = c("cr", "cc", "cc", "cc")) + heap + te(temp_max, lag, k = c(10, 3)) + te(precip_daily_total, lag, k = c(10, 3)) Parametric coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.64899 0.01232 52.682 Below are the ACF plots (note limit y-axis = 0.1 for clarity of pattern). They show peaks at 5 and 15, and then there seems to be a recurring pattern at multiples of approx. 30 (suggesting month is not modelled adequately?). Not sure what would cause the spikes at 5 and 15. There is heaping of deaths on the 15th day of each month, to which deaths with unknown date were allocated. This heaping was modelled with categorical variable/ factor heap with 169 levels (0 for all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent heaping day over the 14-year period): I get an almost identical looking ACF when I decompose time into (year, month, monthday) as in model conc39 below, although concurvity between (temp_max, lag) and the time term has now dropped somewhat to 0.83: conc39 Method: REML Optimizer: outer newton full convergence after 14 iterations. Gradient range [-0.001578187,6.155096e-05] (score 8915.763 & scale 1). Hessian positive definite, eigenvalue range [1.894391e-05,26.99215]. Model rank = 323 / 323 Basis dimension (k) checking results. Low p-value (k-index Some output from summary(conc39) : Family: Negative Binomial(17.425) Link function: log Formula: deaths ~ te(year, month, monthday, bs = c("cr", "cc", "cr")) + heap + te(temp_max, lag, k = c(10, 4)) + te(precip_daily_total, lag, k = c(10, 4)) Parametric coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.65212 0.01192 54.730 $worst para te(year,month,monthday) te(temp_max,lag) te(precip_daily_total,lag) para 1.000000e+00 3.261007e-31 0.3313549 0.6666532 te(year,month,monthday) 3.060763e-31 1.000000e+00 0.8266086 0.5670777 te(temp_max,lag) 3.331014e-01 8.225942e-01 1.0000000 0.5840875 te(precip_daily_total,lag) 6.666532e-01 5.670777e-01 0.5939380 1.0000000 The default approach in time series studies of heat-mortality is to model time with fixed df, generally between 7-10 df per year of data. I am, however, apprehensive about this approach because a) mortality profiles vary with locality due to sociodemographic and environmental characteristics and b) the choice of df is based on higher income countries (where nearly all these studies have been done) with different mortality profiles and so may not be appropriate for tropical, low-income countries. Although the approach of fixing (high) df does remove more temporal patterns from the ACF (see model and output below), concurvity between time and lagged temperature has now risen to 0.99! Moreover, temperature (which has been a consistent, highly significant predictor in every model of the tens (hundreds?) I have run, has now turned non-significant. I am guessing this is because time is now a very wiggly function that not only models/ removes seasonal variation, but also some of the day-to-day variation that is needed for the temperature smooth : conc20a Output from gam.check(conc20a, rep = 1000) : Method: REML Optimizer: outer newton full convergence after 9 iterations. Gradient range [-0.0008983099,9.546022e-05] (score 8750.13 & scale 1). Hessian positive definite, eigenvalue range [0.0001420112,15.40832]. Model rank = 336 / 336 Basis dimension (k) checking results. Low p-value (k-index Output from concurvity(conc20a, full=FALSE)$worst : para s(time) te(temp_max,lag) te(precip_daily_total,lag) para 1.000000e+00 2.462064e-19 0.3165236 0.6666348 s(time) 2.462398e-19 1.000000e+00 0.9930674 0.6879284 te(temp_max,lag) 3.170844e-01 9.356384e-01 1.0000000 0.5788711 te(precip_daily_total,lag) 6.666348e-01 6.879284e-01 0.5788381 1.0000000 Some output from summary(conc20a) : Family: Negative Binomial(22.701) Link function: log Formula: deaths ~ s(time, k = 112, fx = TRUE) + heap + te(temp_max, lag, k = c(10, 3)) + te(precip_daily_total, lag, k = c(10, 3)) Parametric coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.62502 0.01116 56.020 ACF functions: The data look as follows: deaths_total date time year month week doy monthday weekday heap heap_bin temp_max precip_daily_total 466 5 2002-04-11 466 2002 4 14 101 11 4 0 0 40.15 1.35 467 0 2002-04-12 467 2002 4 14 102 12 5 0 0 38.45 5.40 468 5 2002-04-13 468 2002 4 14 103 13 6 0 0 34.65 5.13 469 2 2002-04-14 469 2002 4 15 104 14 0 0 0 37.39 0.09 470 35 2002-04-15 470 2002 4 15 105 15 1 16 1 34.50 6.80 471 4 2002-04-16 471 2002 4 15 106 16 2 0 0 36.49 0.12 where time is a counter of days over the 14-year period (running from 1-5113) and doy = day of year (running from 1-366) Data can be found on my GitHub site in the file data_cross_validated_post2.rds . A csv version is also available. This is my code: library(readr) library(mgcv) df $deaths_total,doy=df$ doy, year = df $year, month = df$ month, weekday = df $weekday, week = df$ week, monthday = df $monthday, time = df$ time, heap=df $heap, heap_bin = df$ heap_bin, precip_hourly_dailysum = df $precip_hourly_dailysum) dat$ temp_max $temp_max) dat$ temp_min $temp_min) dat$ temp_mean $temp_mean) dat$ wbgt_max $wbgt_max) dat$ wbgt_mean $wbgt_mean) dat$ wbgt_min $wbgt_min) dat$ temp_wb_nasa_max $temp_wb_nasa_max) dat$ sh_mean $sh_mean) dat$ solar_mean $solar_mean) dat$ wind2m_mean $wind2m_mean) dat$ sh_max $sh_max) dat$ solar_max $solar_max) dat$ wind2m_max $wind2m_max) dat$ temp_wb_nasa_mean $temp_wb_nasa_mean) dat$ precip_hourly_dailysum $precip_hourly_dailysum) dat$ precip_hourly $precip_hourly) dat$ precip_daily_total $precip_daily_total) dat$ temp $temp) dat$ sh $sh) dat$ rh $rh) dat$ solar $solar) dat$ wind2m Thank you if you've read this far!! :-))
