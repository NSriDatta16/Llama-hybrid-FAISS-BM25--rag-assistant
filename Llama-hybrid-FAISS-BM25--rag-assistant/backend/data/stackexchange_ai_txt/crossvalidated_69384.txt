[site]: crossvalidated
[post_id]: 69384
[parent_id]: 69267
[tags]: 
For small numbers of variables, this problem is easier than it looks: for each group of data you can systematically try all $m!$ permutations of the coefficients and retain the permutation that minimizes the sum of residual squares. To describe the algorithm in detail, let the data consist of $g$ pairs $X^{(j)}, Y^{(j)}$ where each $X^{(j)}$ is an $n_j$ by $m$ matrix whose columns are the independent values and each $Y^{(j)}$ is an $n_j$ column vector of dependent values. The model is $$E[Y^{(j)}] = X^{(j)}\sigma_j(\beta)$$ where $\beta$ is an $m$-vector of coefficients $\sigma_j$ are permutations of the coefficients of $\beta$ $1 \le j \le g$. Let $\mathfrak{S}_m$ denote the group of all permutations of the positions of an $m$-vector. To estimate the parameters $\beta$ and $\{\sigma_j\}$, define $$f_j(\beta) = \min_{\sigma \in \mathfrak{S}_m} ||Y^{(j)} - X^{(j)}\sigma(\beta)||^2.$$ Given $\beta$, this amounts to finding a permutation of $\beta$ which minimizes the residuals in group $j$. One way to find this permutation is to compute $||Y^{(j)} - X^{(j)}\sigma(\beta)||^2$ for each of the $m!$ permutations $\sigma$. This brute force approach is reasonable for small $m$. The estimate of $\beta$ minimizes the objective $F(\beta) = \sum_{j=1}^g f_j(\beta)$. Because $F$ is smooth almost everywhere, continuous, and quadratic in a neighborhood of is minimum, we may apply a gradient-based numerical solver to find the optimal value $ \widehat\beta$. This can be expected to converge quickly. Thus, the total effort is proportional (roughly) to $g m! \bar n$ where $\bar n = (n_1+\cdots+n_g)/g$ is the average group size. Having obtained an estimate of $\beta$--which involves estimating the $\sigma_j$ when evaluating the $f_j$--we may then proceed as usual to predict $Y$, compute residuals, perform all diagnostics, etc. A convenient way to implement this would be to permute the rows of the data $X^{(j)}$ once and for all according to the inverse of the estimated $\sigma_j$ and then perform a standard regression on the dataset obtained by combining all the $X^{(j)}, Y^{(j)}$ cases. The usual least-squares (and maximum likelihood) theory should hold approximately provided the groups are large and not too great in number. (When there is a large number of groups, some of the $\sigma_j$ are likely to be incorrectly estimated, leading to a reduction in the sum of squared residuals: in effect, we should expect to overfit the data.) As a moderately difficult example I synthesized $1979$ cases with $m=6$ variables (each generated independently from a standard Normal distribution) partitioned into $g=10$ groups averaging almost $200 = \bar n$ per group. I set $\beta=(1,-2,3,-4,5,-6)$ and added iid Normally distributed error $\varepsilon$ with standard deviation $\tau=12$. After $50$ evaluations of $F$, requiring $15$ seconds total, convergence was achieved at the (reasonable) estimate $\widehat\beta = (1.10, -2.15, 2.88, -4.50, 4.98, -6.41)$. The estimate of $\tau$ was $12.12$ (to be compared to the actual realized value of $12.15$). Evidently overfitting was not a problem in this example. Here is the R code used for the example. It is written in a moderately general way to allow flexible experimentation and even application to real data. # # Specify the problem. # set.seed(17) m
