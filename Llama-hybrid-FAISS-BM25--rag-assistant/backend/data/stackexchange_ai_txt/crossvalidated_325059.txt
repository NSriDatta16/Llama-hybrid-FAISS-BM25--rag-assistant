[site]: crossvalidated
[post_id]: 325059
[parent_id]: 
[tags]: 
How to update multiple arms in a multi-armed bandit problem?

I'm relatively new to reinforcement learning and have the following multi-armed bandit problem: Let's assume we have a bandit with $n$ arms. Each arms has a different reward distribution with support $[0,1]$. Further assume we follow a $\epsilon$ greedy policy, i.e. pick the arm with the highest expected reward or with a small probability $\epsilon$ pick an arm randomly. So far, so easy. Here comes the strange part: When one arm is played we do not only observe the reward $r$ of this arm, but also the reward of all arms with reward $r$. What I have done is to update all arms whose rewards can be observed. However, the algorithm will not converge to the arm with the highest reward. Why is that? Is there any way to leverage the reward information of all observed arms? More generally, is this actually a multi-armed bandit problem or something else? Thank you so much for your help!
