[site]: crossvalidated
[post_id]: 347378
[parent_id]: 
[tags]: 
Variational autoencoder: Why reconstruction term is same to square loss?

In variational autoencoder (see paper) , page 5, the loss function for neural networks is defined as: $L(\theta;\phi;x^{i})\backsimeq 0.5*\sum_{j=1}^J(1 + 2\log\sigma^i_j-(\mu^i)^2) - (\sigma^i)^2) + \frac{1}{L}\sum_{l=1}^L \log p_\theta(x^i|z^{i,l})$ While in the code, the second term $\frac{1}{L}\sum_{l=1}^L \log p_\theta(x^i|z^{i,l})$ is actually achieved by: binary_crossentropy(x, x_output) , where x and x_output is input and output of autoencoder respectively. My question is why are the losses of input and output is equivelant to $\frac{1}{L}\sum_{l=1}^L \log p_\theta(x^i|z^{i,l})$ ?
