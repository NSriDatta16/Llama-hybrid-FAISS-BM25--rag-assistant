[site]: crossvalidated
[post_id]: 183959
[parent_id]: 183873
[tags]: 
I just wanted to add to the other answers a bit about how, in some sense, there is a strong theoretical reason to prefer certain hierarchical clustering methods. A common assumption in cluster analysis is that the data are sampled from some underlying probability density $f$ that we don't have access to. But suppose we had access to it. How would we define the clusters of $f$? A very natural and intuitive approach is to say that the clusters of $f$ are the regions of high density. For example, consider the two-peaked density below: By drawing a line across the graph we induce a set of clusters. For instance, if we draw a line at $\lambda_1$, we get the two clusters shown. But if we draw the line at $\lambda_3$, we get a single cluster. To make this more precise, suppose we have an arbitrary $\lambda > 0$. What are the clusters of $f$ at level $\lambda$? They are the connected component of the superlevel set $\{x : f(x) \geq \lambda \}$. Now instead of picking an arbitrary $\lambda$ we might consider all $\lambda$, such that the set of "true" clusters of $f$ are all connected components of any superlevel set of $f$. The key is that this collection of clusters has hierarchical structure. Let me make that more precise. Suppose $f$ is supported on $\mathcal X$. Now let $C_1$ be a connected component of $\{ x : f(x) \geq \lambda_1 \}$, and $C_2$ be a connected component of $\{ x : f(x) \geq \lambda_2 \}$. In other words, $C_1$ is a cluster at level $\lambda_1$, and $C_2$ is a cluster at level $\lambda_2$. Then if $\lambda_2 hierarchy of clusters. We call this the cluster tree . So now I have some data sampled from a density. Can I cluster this data in a way that recovers the cluster tree? In particular, we'd like a method to be consistent in the sense that as we gather more and more data, our empirical estimate of the cluster tree grows closer and closer to the true cluster tree. Hartigan was the first to ask such questions, and in doing so he defined precisely what it would mean for a hierarchical clustering method to consistently estimate the cluster tree. His definition was as follows: Let $A$ and $B$ be true disjoint clusters of $f$ as defined above -- that is, they are connected components of some superlevel sets. Now draw a set of $n$ samples iid from $f$, and call this set $X_n$. We apply a hierarchical clustering method to the data $X_n$, and we get back a collection of empirical clusters. Let $A_n$ be the smallest empirical cluster containing all of $A \cap X_n$, and let $B_n$ be the smallest containing all of $B \cap X_n$. Then our clustering method is said to be Hartigan consistent if $\Pr(A_n \cap B_n) = \emptyset \to 1$ as $n \to \infty$ for any pair of disjoint clusters $A$ and $B$. Essentially, Hartigan consistency says that our clustering method should adequately separate regions of high density. Hartigan investigated whether single linkage clustering might be consistent, and found that it is not consistent in dimensions > 1. The problem of finding a general, consistent method for estimating the cluster tree was open until just a few years ago, when Chaudhuri and Dasgupta introduced robust single linkage , which is provably consistent. I'd suggest reading about their method, as it is quite elegant, in my opinion. So, to address your questions, there is a sense in which hierarchical cluster is the "right" thing to do when attempting to recover the structure of a density. However, note the scare-quotes around "right"... Ultimately density-based clustering methods tend to perform poorly in high dimensions due to the curse of dimensionality, and so even though a definition of clustering based on clusters being regions of high probability is quite clean and intuitive, it often is ignored in favor of methods which perform better in practice. That isn't to say robust single linkage isn't practical -- it actually works quite well on problems in lower dimensions. Lastly, I'll say that Hartigan consistency is in some sense not in accordance with our intuition of convergence. The problem is that Hartigan consistency allows a clustering method to greatly over-segment clusters such that an algorithm may be Hartigan consistent, yet produce clusterings which are very different than the true cluster tree. We have produced work this year on an alternative notion of convergence which addresses these issues. The work appeared in "Beyond Hartigan Consistency: Merge distortion metric for hierarchical clustering" in COLT 2015.
