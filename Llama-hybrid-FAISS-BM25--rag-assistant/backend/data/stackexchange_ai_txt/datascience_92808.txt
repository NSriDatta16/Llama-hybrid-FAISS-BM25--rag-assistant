[site]: datascience
[post_id]: 92808
[parent_id]: 
[tags]: 
differences in NN optimization algorithm between Matlab and Scikit-Learn

I'm trying to build a neural network to estimate the responses of a nonlinear system . My database contains 1000 load cases (inputs) and maximum responses (outputs) from numeric simulations. I'm using Python Scikit-Learn. I've run a gridsearch on the hyperparameters but the NN does not provide reliable results. I ended up with two hidden layers , 20 and 30 neurons, relu for activation function. Not anything like what I've seen on articles and tried myself with Matlab: good results with one hidden layer , 20 neurons, logistic for activation function. I wonder what could be so different between Matlab and Scikit-Learn to yield such a difference in the net architecture and its performance. Can it be the performance of optimization algorithms available in each?
