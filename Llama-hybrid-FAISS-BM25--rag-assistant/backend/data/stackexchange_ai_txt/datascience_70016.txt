[site]: datascience
[post_id]: 70016
[parent_id]: 
[tags]: 
Is the hyperbolic tangent function a solution to the weight clipping problem of WGAN?

Instead of clipping to the range $[-c,c]$ in WGAN (Wasserstein generative adversarial network), why not smoothly map into the range $[-c,c]$ by using $c\times \mathrm{tanh}(w)$ ? This would guarantee the Lipschizt constant is no greater than $c$ . The problem I am talking about is mentioned in https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html "Sadly, Wasserstein GAN is not perfect. Even the authors of the original WGAN paper mentioned that “Weight clipping is a clearly terrible way to enforce a Lipschitz constraint” (Oops!). WGAN still suffers from unstable training, slow convergence after weight clipping (when clipping window is too large), and vanishing gradients (when clipping window is too small). Some improvement, precisely replacing weight clipping with gradient penalty, has been discussed in Gulrajani et al. 2017. I will leave this to a future post." but even in that 'improvements' paper, I do not see my idea about using tanh implemented.
