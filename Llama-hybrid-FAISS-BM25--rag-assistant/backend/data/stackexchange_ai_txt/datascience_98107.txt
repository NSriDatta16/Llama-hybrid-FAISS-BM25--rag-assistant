[site]: datascience
[post_id]: 98107
[parent_id]: 97901
[tags]: 
The first source doesn't give a source for their claim (that performance decreases as you get more data), so I'd ignore it. As a rule of thumb, the more training data the harder it will be to overfit, and I think this applies to all ML algorithms. I.e. diminishing returns, but it shouldn't get worse. The second source , the image, is what we do tend to observe, at least in the fields of image and text processing. I think one explanation is that deep learning algorithms have been better able to take advantage of modern GPUs, and thus it has been possible to scale their capacity to the point where they are able to learn from huge data sets.
