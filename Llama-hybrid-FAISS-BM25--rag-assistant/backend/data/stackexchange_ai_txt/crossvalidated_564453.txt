[site]: crossvalidated
[post_id]: 564453
[parent_id]: 564451
[tags]: 
It really depends on exactly what kind of interpretability you need. I understand your feeling that XGBoost is overkill. However, it is the best thing you found, right? So, might as well use it? Unless it's causing issues because of computation constraints, I'd go ahead with XGBoost (if you're confident there's no overfitting going on). So, what kind of interpretability do you need? If you need some kind of broad measure of feature importance, then you can certainly get that with these complex methods! The only thing you can't really get with something like XGBoost (that you could easily get with logistic regression) is a super direct feature coefficient representing "If you increase this variable by 1, the prediction will go up/down by X". LIME and Permutation Feature Importance are great tools that, I think, will take care of your interpretability needs in most cases.
