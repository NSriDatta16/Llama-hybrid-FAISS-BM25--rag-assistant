[site]: crossvalidated
[post_id]: 621692
[parent_id]: 
[tags]: 
Develop a global model for multiple objects

First of all, i don't know if "global model" is the right name for this. The goal is to develop one model to predict the solar energy of PV-Systems in an energy management system for the next day. Whenever a new PV-System is adopted to the energy management system one "global" model will be trained from scratch everytime (I am aware of transfer learning, but this is the task). So there should be one model which is used for every PV-System. I have 10 datasets available for the modelling process. I want to use 5 for training, 2 for validation and 3 for testing. I am not quite sure how to handle these different datasets. The first idea that i had was to build a model on one dataset and test it on the other datasets, but i was told that this approach is not robust. I should develop a model based on multiple PV-systems. Now, i am struggling to find the right approach to include more than one PV-System in the modelling process. I thought about a "competition": Select 5 PV-Systems as training data Generate the same features for each dataset Split each PV-System into train and validation data Train different models (e.g. FFNN, RNN), do feature selection, optimize the hyperparameters, evaluate the performance on validation data Take the other two PV-systems which are preserved for validation and test each model from step 4. If the perfomance isn't satisfying go back to modelling Choose the model that performs best on the validation PV-Systems. That will be the model that is used as a global model for the energy management system Evaluate the choosen model on the test PV-systems Does this approach makes sense? Or is there an more elegant way to approach this? I couldn't find anything similar in the literature. Would be very thankful to some input for other ideas or confirmation
