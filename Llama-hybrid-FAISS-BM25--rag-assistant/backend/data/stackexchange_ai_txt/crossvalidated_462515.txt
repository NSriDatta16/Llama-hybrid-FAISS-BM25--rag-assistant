[site]: crossvalidated
[post_id]: 462515
[parent_id]: 
[tags]: 
Weights between the Last 2 layers keep getting negative

TL;DR weights between the last 2 layers keep getting negative to the point that the softmax(z) of the output layer can't divide by zero ( e^-750 ~= 0 thus deciding by 0) I am making a Neural Network from scratch and just when I thought I finished a problem appears and I can't seem to fix it. RuntimeWarning: invalid value encountered in true_divide appears and it seems to be caused by the softmax function in the output layer when I am calculating the derivative of the softmax as softmax(z)(1 - softmax (z)) as a part of the error of the output layer with the given formula: So then I started investigating the problem and it seems that the vector z(L) gets bigger and bigger in the negative direction after every iteration (mean value of the vector is around -744.5) so by the 19000 - 20000 iteration in the MNIST training of the 1st epoch, it spits out the above-mentioned error. So I investigated a bit more and by printing the average value of the weight matrix between the last 2 layers, I see that it is steadily increasing in the negative direction after every iteration (average weights of -14.9). The binary classifier that I ran before this test works like a charm and reducing the Learning Rate over a fewer number of epochs could converge; it still just postpones the problem. And now I am stuck, and I don't know why are my weights behaving like that. Any help would be much appreciated! Here are the formulas that I use for backpropagation along with the Python code: OUTPUT LAYER ERROR self.layers[-1].error = np.multiply(self.dcost(y, self.layers[-1].a), self.layers[-1].dactivation(self.layers[-1].z)) HIDDEN LAYER ERROR Weights between layer L and L+1 are associated with the layer L+1 for layer in reversed(self.layers[1:-1]): layer.error = (np.dot(np.transpose(self.layers[self.layers.index(layer)+1].weights), self.layers[self.layers.index(layer)+1].error)) * layer.dactivation(layer.z) CALCULATING GRADIENT for layer in self.layers[1:]: layer.gradients = np.dot(layer.error.reshape(-1, 1), self.layers[self.layers.index(layer)-1].a.reshape(1, -1)) UPDATING THE WEIGHTS layer.weights -= self.lr * layer.gradients
