[site]: crossvalidated
[post_id]: 499711
[parent_id]: 448799
[tags]: 
I would say that there are two main reasons for the use of sampling without replacement for most implementations of stochastic variants of Gradient Boosting. The first one is simply historic : In 1999 both Breiman and Friedman published papers that went in the similar direction of trying reduce both Bias and Variance for ensemble estimators. Breiman published Adaptive Bagging , which was more based on Random Forests and used sampling with replacement. Friedman (who had also authored the first paper on Gradient Boosting) published instead Stochastic Gradient Boosting , which used sampling without replacement. In general, the work of Friedman in this field has been much more successful, as his work on Gradient Boosting is at the base of all modern implementations, which therefore use sampling without replacement as the standard. The second reason is that sampling without replacement is more flexible : the goal of Stochastic Gradient Boosting is not only to introduce randomness, but also to make boosting iterations faster. Quoting from the original paper: Using $\tilde{N} = N$ introduces no randomness and causes Algorithm 2 (SGB, e.n.) to return the same result as Algorithm 1 (GB, e.n.). The smaller the fraction $f=\tilde{N}/N$ , the more samples used in successive iterations will differ, thereby introducing more overall randomness into the procedure. Using the value $f=1/2$ is roughly equivalent to drawing bootstrap samples at each iteration. Using $\tilde{N} = f \cdot N$ also reduces computation by a factor of $f$ . This means that we could indeed use sampling with replacement instead (also sampling a lower fraction $f$ ), but the computation factor would be much higher for an equivalent amount of "randomness" added to the iterations.
