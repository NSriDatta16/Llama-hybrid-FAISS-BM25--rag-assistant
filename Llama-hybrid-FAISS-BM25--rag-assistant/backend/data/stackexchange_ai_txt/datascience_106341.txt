[site]: datascience
[post_id]: 106341
[parent_id]: 
[tags]: 
How to improve language model ex: BERT on unseen text in training?

I am using pre-trained language model for binary classification. I fine-tune the model by training on data my downstream task. The results are good almost 98% F-measure. However, when I remove a specific similar sentence from the training data and add it to my test data, the classifier fails to predict the class of that sentence. For example, the sentiment analysis task "I love the movie more specifically the acting was great" I removed from training all sentences containing the words " more specifically" and surprisingly in the test set they were all misclassified, so the precision decreased by a huge amount. Any ideas on how can I further fine-tune/improve my model to work better on unseen text in training to avoid the problem I described above? (of course without feeding the model on sentences containing the words "more specifically" ) Note: I observed the same performance regardless of the language model in use (BERT, RoBERTa etc).
