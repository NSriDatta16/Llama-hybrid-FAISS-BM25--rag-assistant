[site]: crossvalidated
[post_id]: 600283
[parent_id]: 308468
[tags]: 
You tagged this question with the tag "Maximum Likelihood". In maximum likelihood estimation you explicitly maximize an objective function (namely the likelihood). It just so happens that for an observation that we assume to be drawn from a Gaussian random variable, the likelihood function usually takes a nice form after you take a logarithm. Then there is usually a leading negation, encouraging the entrepreneurial optimizer to switch away from maximizing the objective to minimizing the negative of objective, or roughly the "cost". For discrete maximum likelihood estimation the "cost" also has another meaningful name since it takes the same form as the euclidean distance in the observation space. (Note that this notion of distance is always there whether or not you're doing discrete parameter estimation, but it's a little less obvious than the discrete ML estimate which just boils down to picking the nearest valid point). Since there's no such thing as negative distance there is a seemingly strong preference for minimizing the cost and not maximizing the objective in these cases. You should feel comfortable swapping back and forth between minimizing a cost and maximizing an objective. There is a real reason that ML and MAP estimates specifically choose to maximize an objective function (pdf's are purely positive, and the highest values are quite interesting spots [the mode]), but the practical realization of an estimator is going to be several mathematical manipulations away from the textbook definition.
