[site]: crossvalidated
[post_id]: 478549
[parent_id]: 
[tags]: 
Linear Regression and correlation - Avgerages VS Raw Data

I faced the following problem: I took a raw-data with two variables: x - independent variable, ranges between 1-5. y - dependent variable, boolean variable 0, 1. Got the following density plot, and performed Linear Regression: percentages under each accumulation represent the % of dots in a specific accumulation out of all the dots in the same x value. Sum of percentage of a specific x value is 100%. The R - squared is about 0. However, if I calculate the average of each x value (which is the % between each accumulation where y = 1 actually) I got the following: R Squared about 1 !! It also affect Pearson correlation coefficient ofcourse. Can you help me to solve the discrepancy please ? By definition, linear regression describes the mean of each x value, so if the averages so fit linear line, why the R squared of the first plot is so low ?
