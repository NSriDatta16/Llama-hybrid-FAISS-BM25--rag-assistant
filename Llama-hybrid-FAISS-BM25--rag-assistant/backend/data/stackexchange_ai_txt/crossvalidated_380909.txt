[site]: crossvalidated
[post_id]: 380909
[parent_id]: 380194
[tags]: 
I think you have the right idea but you are somewhat thrown off by default values of the learning rate. All gradient boosting algorithms make the working assumption that they will be used iteratively so they define a learning rate/step size. This is fundamental to all boosting algorithms and it almost always a tunable parameter. Friedman et al. (2001) seminal boosting paper " Additive logistic regression: a statistical view of boosting " refer to it as a shrinkage factor $\nu$ and it effectively regularises the learning by not allowing the learners to over-fit the data (Rossett et al. 2004 " Boosting as a regularized path to a maximum margin classifier " explores this in some detail). Catboost, as well as XGBoost, refer to the learning rate as $\eta$ . As a general rule, learning rates are purposely set to low values such that the boosting procedure is able to learn the final function in a principled incremental way. The hard truth is that the smaller value $\eta$ help us to overcome overfitting at the expense of requiring more iterations during training. Having said the above, let's go back to Catboost in particular. If during the call to Catboost we do not specify the learning rate, Catboost tries to pick it automatically based on some internal heuristics that are a function of the dataset and the number of iterations. For this use, which in all fairness is an edge-case having only a single iteration, the learning rate is automatically choose to be 0.02999999933 (Catboost version 0.9.1.1 ). Based on your attached graph this learning rate in not aggressive enough. If we make a call like: estimator = CatBoostRegressor(n_estimators=1, max_depth=1, loss_function='RMSE', learning_rate=1 ) the plot of the "final fit", will indeed match your expectations and will look like: This model has a MSE of ~105.5; Catboost took a single but quite well-educated step! :)
