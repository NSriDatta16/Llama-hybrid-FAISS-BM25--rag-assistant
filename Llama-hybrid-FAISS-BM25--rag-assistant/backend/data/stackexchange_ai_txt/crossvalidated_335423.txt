[site]: crossvalidated
[post_id]: 335423
[parent_id]: 335396
[tags]: 
One-step Q-learning does not sample forward trajectories, it just takes the maximum value bootstrapped from the estimated action value. You will notice that in the examples of importance sampling in Monte Carlo control, that the weighting by importance sampling is applied after taking a step. So the weight is always 1.0 for the first step. This is then consistent with having no apparent importance sampling in one-step off-policy bootstrapping methods. In multi-step Q-learning, e.g. Q($\lambda$) you should notice that taking any action other than the greedy action will zero the weights of further samples from that trajectory that are used to adjust earlier state, action values. That is a form of importance sampling, since the probability of not taking the greedy action in the target policy is zero. In theory you could also multiply weights of future greedy actions by the inverse probability of selecting them in the behaviour policy - I have not tried that, but I suspect that it mainly just interacts with $\lambda$ and/or increases variance without much benefit. You would also need to track (and average) all the zero-ed adjustments and add them back in . . .
