[site]: crossvalidated
[post_id]: 369361
[parent_id]: 369322
[tags]: 
This article is a good place to start. " Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network " by Alex Sherstinsky Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well. This is a dense document with all of the equations your heart might desire. It would be difficult to reproduce all of the relevant materials here. Another presentation can be found in " A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation " by Gang Chen. We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit. Also, if you're unfamiliar with backpropagation , we have a number of threads on the topic. Regarding GRUs, I'm not aware of a similar paper. The promise of GRUs was supposedly that GRUs would provide comparable performance to LSTMs with a lower parameter count and fewer computations; results are mixed. For a comparison of LSTMs and GRUs, see Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. " Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling ."
