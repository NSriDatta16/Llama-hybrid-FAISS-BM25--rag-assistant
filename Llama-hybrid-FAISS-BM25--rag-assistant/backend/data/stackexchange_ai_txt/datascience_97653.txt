[site]: datascience
[post_id]: 97653
[parent_id]: 87093
[tags]: 
TL;DR: A theoretical/mathematical explanation for why word2vec/GloVe embeddings of analogies appear to form parallelograms, and so can be "solved" by adding/subtracting embeddings, is given here , as summarised in this blog . More explanation of w2v is given here . The dimensions of word2vec (or GloVe, etc) word embeddings are not directly interpretable, but capture correlations in word statistics, which reflect meaningful semantics (e.g. similarity), so some dimensions may happen to be interpretable. The embedding of a word is effectively a low-rank projection of the co-occurrence statistics of that word with all other words (like what you would get from PCA/SVD - but that would require an unweighted least square loss function). That projection in word2vec is probability weighted and non-linear, making it difficult to interpret what any dimension "means". Also, if the embedding matrix $W$ (all embeddings $w_i$ stacked together) is rotated by any rotation matrix $R$ , and $R^{-1}$ applied to the other embedding matrix $C$ , the transformed embeddings perform identically. So there isn't a unique solution, but an equivalence class of solutions, meaning the values in embeddings aren't necessarily meaningful in their own right, only when considered relative to each other. The theoretical explanation of analogies is too long to summarise here, but it boils down to word embeddings capturing log probabilities, so adding embeddings is equivalent to multiplying probabilities and so is meaningful. I gather it's bad form to include link explanations, but the two linked research papers should last in perpetuity.
