[site]: crossvalidated
[post_id]: 320701
[parent_id]: 
[tags]: 
How to use Keras pre-trained 'Embedding' layer?

guys! I've trained model in keras using Embedding on specific corpus of articles. I use this tutorial http://adventuresinmachinelearning.com/word2vec-keras-tutorial/ Now I want use it as layer in my LSTM. I found a lot of examples, how to load word2vec or glove, but no one discuss in connection with Keras. As I understand i should load and extract the embedding weights from the embedding layer and store it in a numpy array, ready for use in Keras. But how can i make it? I found some interesting thoughts here http://www.orbifold.net/default/2017/01/10/embedding-and-tokenizer-in-keras/ There is much confusion about whether the Embedding in Keras is like word2vec and how word2vec can be used together with Keras. I hope that the simple example above has made clear that the Embedding class does indeed map discrete labels (i.e. words) into a continuous vector space. It should be just as clear that this embedding does not in any way take the semantic similarity of the words into account. Check the source code if want to see it even more clearly. any clues?
