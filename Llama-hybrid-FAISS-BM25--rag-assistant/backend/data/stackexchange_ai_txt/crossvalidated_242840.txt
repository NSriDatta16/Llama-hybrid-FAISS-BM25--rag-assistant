[site]: crossvalidated
[post_id]: 242840
[parent_id]: 242834
[tags]: 
In SVM we have $\hat y_0 = \text{sgn}(\sum_{i=1}^n \alpha_i y_i K(x_i, x_0))$ so if we've already fit our SVM then multiplying $K$ by a positive constant $\gamma$ doesn't affect our decision boundary. More generally, we know that $K(u, v) = \langle\phi(u), \phi(v) \rangle_H$ where $H$ is our RKHS and $\phi$ is our feature map. If $\gamma > 0$ then we know that $K'(u, v) := \gamma K(u, v) = \langle \sqrt \gamma\phi(u), \sqrt \gamma\phi(v) \rangle_H$ so we are just using a scaled version of the same feature map $\phi$. Our Hilbert space $H$ is closed under scalar multiplication too so $K'$ and $K$ correspond to the exact same Hilbert space although our $n$ representers aren't the exact same. So intuitively we shouldn't be able to do anything different with $K'$ vs $K$. The vector normal to our hyperplane in $H$ is $$ w = \sum_i \alpha_i y_i \phi(x_i) $$ so if we replace $\phi(x_i)$ with $\sqrt \gamma \phi(x_i)$ we just get $w' = \sqrt \gamma w$, and this has the exact same nullspace and therefore corresponds to the exact same hyperplane. $ $ Update This will change the distance between points but by the same amount so that ultimately there is no difference. Our hyperparameters will also need to change but in a predictable way. Suppose we're using the linear kernel $K_L(u,v) = u^T v$. If $K'_L := \gamma K_L$ then all we've done is scale all the distances between our data points by the same amount (it's like using the inner product $\langle u, v \rangle_\gamma := u^T \gamma I v$: the outputted values are different but $\gamma I$ just contracts or expands every dimension equally so relatively nothing changes). So it is true that we will have changed the margin width, but not in a meaningful way since we'll have changed all other distances also.
