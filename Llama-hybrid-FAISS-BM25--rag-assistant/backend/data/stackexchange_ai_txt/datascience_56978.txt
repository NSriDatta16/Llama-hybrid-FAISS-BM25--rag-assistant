[site]: datascience
[post_id]: 56978
[parent_id]: 56975
[tags]: 
I want to build a data synchronization scheduler which keeps track of the amount of data sync after every scheduled triggers and auto-adjusts the next schedule. This doesn't immediately strike me as a "use machine learning" problem to be honest. If you just want the scheduler to schedule the next run to be after an amount of time that's determined by the number of records processed in the current batch then that's quite a simple and deterministic formula. You could have the default gap be 20 mins and then do something like: next gap = 10 * (10 / n) Where n is the number of records processed in the last run. That would mean in this case: So let at 10 AM Job 1 got executed and got 10 entries. At 10 AM job 2 got executed and got 100 entries. At 10 AM job 3 got executed and got 200 entries. The next Job 1 would be scheduled at 20 * (20 / 20) = 20 minutes, the next job 2 would be at 20 * (20 / 100) = 4 minutes and the next job 3 would be at 20 * (20 / 200) = 2 minutes. If you really wanted to use ML for it, I guess I'd suggest using a time series forecasting algorithm like ARIMA or Prophet to predict the number of samples each job will have to process over the next hour or something, and set the next run-time appropriately based on that.
