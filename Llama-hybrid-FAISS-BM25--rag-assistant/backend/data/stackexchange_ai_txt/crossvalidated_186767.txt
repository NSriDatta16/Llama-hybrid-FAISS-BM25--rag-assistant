[site]: crossvalidated
[post_id]: 186767
[parent_id]: 186756
[tags]: 
Scaling the features does make a difference in the solution of SVM because scaling features does not affect the regularization term but it affects the loss term. Training objective of an SVM can be written as follows: $$ O(w) = \frac{\lambda}{2} R(w) + \sum_{i=1}^n L_w(x_i, y_i) \tag{1} $$ where $R(w)$ is the regularizer (e.g. $R(w) = ||w||^2$) and $L_w(x_i, y_i)$ is the loss function (e.g. $L_w(x_i, y_i) = \max \left\{ 0, 1 - y_i w^T \phi(x_i) \right\}$ for binary linear SVM). SVM classifier is the $w^*$ that minimizes $(1)$; that is $w^* = \arg\min_w O(w)$. Multiplying the feature function $\phi(x_i)$ by a constant changes $w^* = \arg\min_w O(w)$ because $R$ is not affected by the scaling but $L_w$ is. If you scale up $L$ by a factor, then you have to scale up $\lambda$ by the same factor to get the same classifier. My favorite tutorials/papers about SVMs: Tutorial by Andrew Moore. Learning Structural SVM with Latent Variables , ICML 2009 paper, by J. Yu and T. Joachims. Tutorial from VLFeat, with cool tricks on how to train SVMs with Stochastic Gradient Descent.
