[site]: stackoverflow
[post_id]: 763762
[parent_id]: 763656
[tags]: 
Yes, those things are still relevant. I do a fair bit of that kind of optimization but, to be fair, I'm mostly writing code that has to do relatively complex things in about 10ms on an ARM9. If you're writing code that's running on more modern CPUs then the benefits won't be as great. If you don't care about portability, and you're doing a fair bit of maths, then you might also look at using whatever vector operations are available on your target platform - SSE on x86, Altivec on PPC. Compilers can't easily use these instructions without a lot of help, and the intrinsics are pretty easy to use these days. Another thing that's not mentioned in the document you linked to is pointer aliasing. You can sometimes get good speed improvements if your compiler has support for some kind of "restrict" keyword. Plus, of course, thinking about cache usage can be important. Reorganizing your code and data in a way that makes good use of the cache can result in dramatic speed increases compared to optimizing away the odd copy or unrolling a loop. As ever, though, the most important thing is to profile. Only optimize code that's actually slow, make sure your optimization actually makes it quicker, and look at the disassembly to see what optimizations the compiler is already doing for you before you try to improve it.
