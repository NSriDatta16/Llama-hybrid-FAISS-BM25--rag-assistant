[site]: crossvalidated
[post_id]: 400058
[parent_id]: 
[tags]: 
Adaboost Notation Confusion

The adaboost algorithm is as follows: $\mathbf{Input}$ : sequence of m examples $ $ with the labels $y_i \in Y = \{1,...,k\}$ weak learning algorithm WeakLearn integer T specifying the number of iterations $\mathbf{Initialize}$ : $D_i(i) = \frac{1}{m}$ for all i $\mathbf{Do}$ t = 1,2,...,T: 1. Call WeakLearn, providing it with distribution $D_t$ 2. Get back a hypothesis $h_t : X \rightarrow Y$ 3. Calculate the error of $\displaystyle h_t: \epsilon_t = \sum_{i:h_t(x_i)\neq y_i} D_i(i)$ If $\epsilon_t > 0.5$ , then set $T = t - 1$ and abort loop. 4. $\displaystyle \beta_t = \frac{\epsilon_t}{1-\epsilon_t}$ 5. Update distribution $D_t$ as $\displaystyle D_{t+1} = \frac{D_t(i)}{Z_t} \times \begin{cases} \beta_t, & \text{if $h_t(x_i) = y_i$} \\ 1, & \text{otherwise} \end{cases} $ where $Z_t$ is a normalization constant (chosen so that $D_{t+1}$ will be a distribution) My confusion is regarding two steps: What does the notation for the error, $\epsilon_t$ mean? Does it mean add all the weights if there is a missclassification? What does the statement If $\epsilon_t > 0.5$, then set T = t - 1 and abort loop. mean? Is it stating to abort the leap (as in break ) or do not count this as an iteration and continue to loop? Some details: I am actually randomly choosing from $D_i$ based on the sample weights. The idea to generate a weak SVM classifier.
