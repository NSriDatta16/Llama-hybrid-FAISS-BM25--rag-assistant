[site]: crossvalidated
[post_id]: 505562
[parent_id]: 
[tags]: 
KNN as a crude prototype of Gaussian Process Regression?

I've heard it said before that K-Means-Clustering is a prototypical method for Expectation-Maximization algorithm. Where KM Clustering returns a hard cluster assignment, EM returns soft assignments, namely the probability of each assignment. I put my imagination to work over the weekend extended this analogy to K-Nearest-Neighbor and Gaussian Process Regression. Both models use a linear combination of training points as the prediction of the output. KNN makes hard assignments through the average of the K Neighbors in consideration, whereas GP regression makes soft assignments by sampling a multivariate gaussian given the mean vector and covariance vector. In KNN, the prediction is a weighted average of inputs, whereas in GP Regression, we don't dictate the particular linear combination of training data points explicitly. Rather, by conditioning on training data, we constrain the model to sample functions that fit the training data reasonably well. This is probably the limit of the models' similarities. But if I'm on the right track, this will have given me a good mental schema to reason through GP regression as it's quite an abstract concept.
