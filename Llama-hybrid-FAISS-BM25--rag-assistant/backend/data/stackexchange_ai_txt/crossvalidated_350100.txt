[site]: crossvalidated
[post_id]: 350100
[parent_id]: 
[tags]: 
Is it cheating to track val_loss and then measure accuracy on that same val dataset?

So I'm training my neural network, and I've split my dataset into training and validation sets. I'm training the neural network on the training set, but I'm also tracking the validation set loss, and I'm actually using early stopping based on the validation loss to automatically stop training. But then I'm assessing my classifier's accuracy on the same validation set I used to determine the early stopping point. Is this a common practice, a totally unacceptable practice, or in sort of a grey area? Thanks.
