[site]: crossvalidated
[post_id]: 418286
[parent_id]: 325776
[tags]: 
Kurt Hornik's 1991 paper "Approximation Capabilities of Multilayer Feedforward Networks" proves that "standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to $L^P(\mu)$ performance criteria, for arbitrary finite input environment measures $\mu$ , provided only that sufficiently many hidden units are available." In other words, the hypothesis that the activation function is bounded and nonconstant is sufficient to approximate nearly any function given we can use as many hidden units as we like in the neural network. The paper should be available here: http://zmjones.com/static/statistical-learning/hornik-nn-1991.pdf
