[site]: datascience
[post_id]: 86551
[parent_id]: 86511
[tags]: 
If you have a list of words for every topic you can indeed try to directly measure the similarity of this list against a sentence, but it's likely that a sentence doesn't always contain one of the topic words so it might not work very well. A more advanced method would be to obtain a semantic representation for every topic (or topic word) from an external corpus. Any large corpus can be used for that, it doesn't have to be related to your input data. The traditional way to do that is to extract a context vector for every target word by counting the co-occurrences of the target in the corpus: for every occurrence of the target word, take for instance a window of 5 words to its left and 5 words to its right. Then count how many times every context word appears in the window of the target across the whole corpus. This way the final context vector contains the distribution of the context words, i.e. a representation of the meaning of the target word. Comparing this context vector against a sentence is likely to produce a more accurate semantic similarity score. There are many variants about the exact definition of the context vector: usually stop words are removed but one could also use TF-IDF and/or other kinds of normalization. The more modern version of this method is probably to use word embeddings, but I'm not knowledgeable enough about this.
