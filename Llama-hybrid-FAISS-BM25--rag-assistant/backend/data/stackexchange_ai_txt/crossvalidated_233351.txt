[site]: crossvalidated
[post_id]: 233351
[parent_id]: 233262
[tags]: 
One hot encoding and feature hashing are both forms of feature engineering where a data scientist is trying to represent categorical information (blood type, country, product ID, word) as an input vector. We might represent Afghanistan as [1,0,0,0], Belarus as [0,1,0,0], Canada as [0,0,1,0], and Denmark as [0,0,0,1]. We could make this vector large enough to hold a position for each country in the world. But what about words, where there are thousands, and some words that appear in your test set may not appear in your training set? A hash function maps data of arbitrary size to data of fixed size. You can use hash(string) mod n to return a number between 0 and n - 1, and then this is the index that you increment in the input vector. An example from https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way : to represent "the quick brown fox": h(the) mod 5 = 0 h(quick) mod 5 = 1 h(brown) mod 5 = 1 h(fox) mod 5 = 3 Once we have this we can simply construct our vector as: (1,2,0,1,0) Finally Also, do we represent hashed features in Sparse format ? With a sufficiently large vector, feature hashing will produce sparse vectors (where most of its values are zero).
