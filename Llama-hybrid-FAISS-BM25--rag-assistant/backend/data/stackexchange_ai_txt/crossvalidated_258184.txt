[site]: crossvalidated
[post_id]: 258184
[parent_id]: 
[tags]: 
Generating real-valued time series with RNN

I'm trying to adapt Andrej Karpathy's char-rnn model (which is also described in Alex Graves's Generating Sequences With Recurrent Neural Networks ) to real-valued time series. Unfortunately I'm having issues in training the network. The idea is to predict the next value of the series given the current value. I started with a network with a single input unit, followed by 2 LSTM layers of 100 units each, and a single output unit. I use MSE as the cost function. I was able to train the network on a simple sinusoidal signal, so that the model can generate the same signal. However, the network fails to learn more complex signals, such as the sum of three sinusoids at different frequencies. Observing the output of the network during training, I observed that the network sometimes just learn the identity function, i.e., the output is the same as the input sequence. Considering that MSE is not very discriminative when a low-frequency periodic sequence is shifted by one sample, maybe MSE is not the best cost function. Any suggestions on how to improve the model?
