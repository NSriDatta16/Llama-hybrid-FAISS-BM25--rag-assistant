[site]: crossvalidated
[post_id]: 477464
[parent_id]: 477446
[tags]: 
First, recognize that logistic regression does not impose a decision boundary. It provides probabilities of class memberships. What you show as a "decision boundary" is presumably based on a cutoff of 0.5 in predicted probability for converting the probabilities into class assignments. Other cutoffs can be better if false-positive and false-negative assignments have different costs. That's very important to remember as you are learning about this. Second, it doesn't look like a simple linear model based on $x_1$ and $x_2$ alone will do a good job of distinguishing these classes. You have 2 clusters of different classes around $x_1 = 0$ , distinguished by their $x_2$ values. You have 3 clusters around $x_2 = -0.2$ , with only the cluster also having $x_1 \approx 0$ in the blue class. In that case, even an interaction term between $x_1$ and $x_2$ wouldn't work to distinguish the 2 classes in the 3 lower clusters, as one lower red cluster would still be on the opposite side of the blue class from the other lower red cluster. You need a more complex model. As @Dave notes in a comment on another answer, a $x_1^2$ term might well help, providing a way to distinguish the 2 lower red clusters from the blue cluster. You might also consider approaches other than logistic regression. For example ISLR in Chapter 9 shows how choices of kernels in support vector machines can help distinguish classes that have non-linear boundaries, as yours do in this plot.
