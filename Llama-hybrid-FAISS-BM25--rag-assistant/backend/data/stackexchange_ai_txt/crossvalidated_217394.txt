[site]: crossvalidated
[post_id]: 217394
[parent_id]: 
[tags]: 
Sparsity estimators

I've recently started to read about sparsity estimation. Let's recall that the sparsity function is defined as $s(\tau)=f(Q(\tau))$, where $f$ is the population density and $Q$ is its quantile function. A particularly popular estimador was obtained by Siddiqui in 1960 [1] and studied in depth by Bloch and Gastwirth in 1968 [2]. This estimator allows the simple form: $$S_{mn}=\frac{n}{2m}(X_{[n\tau]+m:n}-X_{[n\tau]-m+1:n}),$$ where $X_{i:n}$ is the i-th order statistic of a sample $\{X_i\}$ of $f$. On the other hand, a natural and a priori naive approach to sparsity estimation would be to make a kernel density estimation of $f$, $\hat{f}$, evaluate it at the sample quantile $X_{[n\tau]+1:n}$ and then invert it. This is: $$s(\tau)\approx\frac{1}{\hat{f}(X_{[n\tau]+1:n})}.$$ Now, I would certainly expect Siddiqui's estimator to perform much better than this one. However, I have carried simulation studies to compare them and they have shown the opposite. So my question is: why has Siddiqui's idea received so many attention in the literature?. [1] http://nvlpubs.nist.gov/nistpubs/jres/64B/jresv64Bn3p145_A1b.pdf [2] https://projecteuclid.org/download/pdf_1/euclid.aoms/1177698342
