[site]: datascience
[post_id]: 13266
[parent_id]: 
[tags]: 
Understanding the math behind SVM

I was going through this Udacity video (part of the ML nano-degree), where the math behind maximizing the length between the data clouds of two classes is done. So, the line closest to the +1 class cloud would be: $w^T x_1 + b = 1$ And, the line closest to the -1 class cloud would be: $w^T x_2 + b = -1$ ($x_1$ and $x_2$ being points of both the lines respectively.) So, for getting the distance between both of them (the lines above), the professors subtract both of the equations. And then, while finding ($x_1-x_2$), they divide both the sides with the norm of w ($||w||$), which starts at 2:24 minutes. From then, I didn't understand the math in the video. Why did the professors divide the sides by the norm? What do the norm specify/imply? Can someone please explain the math from after 2:24?
