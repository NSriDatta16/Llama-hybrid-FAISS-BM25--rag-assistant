[site]: crossvalidated
[post_id]: 476151
[parent_id]: 476133
[tags]: 
Testing the strength of the relationship between features and labels sounds a lot like feature selection. Feature selection can be done without building a classifier (e,g, chi-squared tests to remove features which are independent of the label, or remove features with correlation coefficient magnitudes that are too small). These are called "filter-based" methods. You can also do feature selection by building a classifier (e.g. lasso , boruta ) to screen out features which don't improve the model. These are called "wrapper-based" methods. Unfortunately, comparing two or more auto-encoders won't be as simple as comparing the number of features that your selection method labels "useful." It's conceivable that one autoencoder gives 10 weak predictors while another one gives 1 very strong predictor. You wouldn't necessarily know which predictors and weak and which are strong without assessing their classification performance, and the Question has expressly forbidden that. There isn't a "canonical" way to do this, because feature selection is a hard, complex task. Beyond "wrapper" and "filter," feature selection methods can be contrasted in terms of computing resources consumed, runtime, suitability of their assumptions, and susceptibility to exclude relevant features or include irrelevant features. It's not possible to summarize all feature selection methods in an Answer; it would be challenging to do so in an academic article. The Question is profitably reframed in terms of feature selection because there are any number of feature selection methods available to choose from. The only limitation the Question places on methods is that they do not include classifiers, so wrapper-based methods like lasso and boruta are excluded. This is fine. There are lots more.
