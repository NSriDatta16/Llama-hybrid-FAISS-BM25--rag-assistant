[site]: crossvalidated
[post_id]: 586079
[parent_id]: 
[tags]: 
How to include condition in linear models?

I am measuring two related variables $Y_A$ and $Y_B$ , where $Y_A+Y_B=1$ , with different settings for $X$ . I know that each $y$ does not physically depend on $X$ , so it’s a constant, as I change $X$ . Since $Y$ is a constant, one might wonder why do I even bother to perform this experiment with different settings of $X$ . The reason is that the cost of measuring $Y$ , almost deterministically depends on $X$ . Since $Y$ is constant, I would rather measure it for cheap values of $X$ ( $x>1$ ). The problem is that I get what I pay for. Measuring $Y$ at different values of $X$ , gives me different results, that I attribute to the part of the measuring instrument error, named $Z$ . If my $Y$ would have been normally distributed around its average, I would have called it a day. But, by plotting $Y$ against $X$ , I am observing that $Y$ measurements follow a linear pattern as $X$ changes, and that there’s heteroskedasticity. I can’t say I’m surprised, given the instrument. Actually, I expect the instrument to give errorless measurements for $x=0$ , and for the errors to more or less increase. Since, there’s not enough money in the world to measure $y$ for $x=0$ , I’ll accept $y$ for $x=1$ to be good enough. I’ll even go further and define $y$ for $x=1$ as equal to $y$ for x=0, from which it follows that I accept the instrument to be errorless for $x=1$ as well. Who’s going to object it? Nobody can measure for $x . The problem is that even though there is enough money in the world to measure $y$ for $x=1$ , I don’t have it, but I do have money to measure $Y$ for $x>1$ , and my goal is to use these measurements to estimate $y$ for $x=1$ , which I defined as equal to $y$ for $x=0$ . So, $$ E(Y_A|X) := \alpha_A = ct $$ $$ E(Y_B|X) := \alpha_B = ct $$ $Z_A$ and $Z_B$ are the instrument errors for measurements of $Y_A$ , and $Y_B$ respectively. $$Z_A = Y_A - \alpha_A$$ $$Z_B = Y_B - \alpha_B$$ $Z_A$ and $Z_B$ must be $0$ for both $x=0$ and $x=1$ , by definition, so we can model it as a linear model of $X$ like this $$Z_A = \beta_AX +\epsilon_i$$ $$Z_B = \beta_BX +\epsilon_i$$ Putting the 2 together, I have $$Y_A=α_A+β_A X+ϵ_i$$ $$Y_B=α_B+β_B X+ϵ_i$$ And, $$E(Y_A│X)=α_A$$ $$E(Y_B│X)=α_B$$ And because of some normalization $$Y_A+Y_B=1$$ $$α_A+α_B=1$$ Which means that I can sum the initial linear models and obtain $$0=β_A X+β_B X+ϵ_i$$ And then the entire model looks like this, except that the last one doesn’t look proper. $$Y_A=α_A+β_A X+β_B 0+ϵ_i$$ $$Y_B=α_B+β_A 0+β_B X+ϵ_i$$ $$0=0+β_A X+β_B X+ϵ_i$$ Does it even make sense? Do I even have extra information? My main question is how to include this extra information. As an extra, it happens that I can also see in the data that the $Y_A$ and the $Y_B$ models are heteroskedastic and I plan to tackle that using $x_i^{-1}$ weights, or possibly $x_i^{\frac{-1}{2}}$ . I strongly suspect they are endogenous as well, but I don’t think I can do anything about it. How bad is that? Any commentary is highly appreciated, but answers that also suggest R code will be even higher appreciated.
