[site]: crossvalidated
[post_id]: 312494
[parent_id]: 312470
[tags]: 
The short answer is no, and the long answer is "it depends". If I observe the sum of two random variables, but I have no information on the individual random variables themselves, then the error model is unidentifiable. However, if I know something about the distributions of the two errors, then I can gather some information about them based on the observed sum. Here's an example: I tell you that two random numbers, $a$ and $b$, sum to 7, and ask you what your guess is for $a$ and $b$. You have no idea: the support for the answers is the entire real line! The probability you guess the correct answer is $0$. However, if I tell you that $a$ and $b$ come from two fair dice, then you have some prior information to work with. Your guesses are likely far more accurate. Motivated by our example, I recommend you take a look at Bayesian methods for measurement error and mixed effect models. Mixed effect models allow you to "soak up" excess variability into a random effect. Here are a few good places to start: Bayesian analysis of measurement error models using INLA (paper and R package) Linear Mixed Effect Models with Measurement Error (a master's thesis) RJ Carroll's book on Measurement Error (largely frequentist, but that's ok too) This entire Wikipedia page on Errors-in-Variables regression. Unfortunately, I'm not an expert in this part of statistics, so I can only point you in the direction I would go and the kind of questions I would then ask. I do hope that this information helps you ask the right questions to get a better-informed answer.
