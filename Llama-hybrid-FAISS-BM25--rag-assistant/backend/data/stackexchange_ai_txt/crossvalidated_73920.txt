[site]: crossvalidated
[post_id]: 73920
[parent_id]: 
[tags]: 
Multiple definitions of AdaBoost

The description of AdaBoost in Kevin Murphy's Machine Learning book (shown in a snapshot below) differs from the one in Wikipedia . I am trying to relate both definitions. Step by step, my questions are: What exactly is $\text{err}_m$ (step 4) supposed to capture below? Is this equivalent to the $\epsilon_t$ in Wikipedia's definition? Why isn't there a stopping rule in Kevin Murphy's method but there is one in Wikipedia's definition? There seems to be a typo in the parenthesis in the denominator, just in case - is it supposed to say the following?: $\text{err}_m = \frac{\sum_{i=1}^N w_{i,m} \mathbb{I}(\hat{y} \neq \phi(\mathbf{x_i)}}{\sum_{i=1}^N w_{i,m}} $ Most importantly, Wikipedia provides the following criteria for choosing the weak learner and for stopping: $h_{t} = \underset{h_{t} \in \mathcal{H}}{\operatorname{argmax}} \; \left\vert 0.5 - \epsilon_{t}\right\vert$ where $\epsilon_{t} = \sum_{i=1}^{m} D_{t}(i)I(y_i \ne h_{t}(x_{i}))$ $If \left\vert 0.5 - \epsilon_{t}\right\vert \leq \beta$, where $\beta$ is a previously chosen threshold, then stop. while Kevin's book defines the full algorithm as follows, and I don't see those two steps above in it:
