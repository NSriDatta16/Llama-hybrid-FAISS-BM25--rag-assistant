[site]: datascience
[post_id]: 22017
[parent_id]: 22004
[tags]: 
Fascinating problem, welcome to the site! In the second edition, this is covered in section 14.2.4 Unsupervised as Supervised Learning . The idea here is to design a classifier (that's supervised learning) between the desired but unknown distribution, and a known reference distribution (14.10), then to use this classifier along with the reference distribution to estimate the unknown distribution at any point (14.12). You can see that when the classifier deems a point to have come from $g$ ($\hat \mu \to 1$), the density $g_0 \hat \mu /(1-\hat \mu)$ increases. The converse is also true. This is what you would expect. Mass refers to the probability measure of the sample. The idea is to redress by difference in sample sizes by weighting the probability of each sample inversely by its size (or equivalently, the size of the other sample). That's why the density of the mixture is the average of the components. A mixture density is simply a distribution formed by a mixture or sum (of two distributions in this case). Finally, $\mu$ is commonly used in statistics to denote the mean. The conditional mean of $Y$ is simply the probability that $x$ is drawn from $g$ (because the $Y=0$ term does not contribute to the expectation), thus $$\mu(x) \equiv \mathbb E(Y|x) = \mathrm P(Y=1|x) = \frac{g}{g+g_0}(x)$$ This equation comes from Bayes' rule, after using $g = P(x|Y=1)$ and $g_0 = P(x|Y=0)$. This approach shows the power of using density ratios . In fact, there's a whole book devoted it.
