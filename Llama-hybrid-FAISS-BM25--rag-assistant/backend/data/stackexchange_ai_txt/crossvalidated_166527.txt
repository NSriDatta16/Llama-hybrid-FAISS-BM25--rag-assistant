[site]: crossvalidated
[post_id]: 166527
[parent_id]: 165062
[tags]: 
You have a training set $\{ (X_i, p_i) \}_{i=1}^n$, where the corresponding label $y_i \sim \mathrm{Bernoulli}(p_i)$. I'll assume that the $y_i$s are independent of one another. Call $\mathcal P$ the distribution of $\{ (X_i, y_i) \}_{i=1}^n$. Then one way forward is to consider drawing multiple datasets $\mathcal D_j = \{ (X_i, y_{ij}) \}_{i=1}^n$ from $\mathcal P$, training a forest on each $\mathcal D_j$, and predicting according to the most common prediction from the different classifiers. This approximates taking the expectation of the learned model under $\mathcal P$. In random forests, it turns out that we can basically approximate this more directly. Construct a weighted dataset $\{(X_i, 1, p_i)\}_{i=1}^n \cup \{ (X_i, 0, 1-p_i) \}_{i=1}^n$, i.e. you include each data point once with each label, weighted according to how likely you think it is that the data point has that label. Many random forest implementations support these weights, and use them to determine how likely you are to sample the data point into a bootstrap replication for a tree. For example, scikit-learn's RandomForestClassifier has a sample_weight argument to the fit method.
