[site]: crossvalidated
[post_id]: 543657
[parent_id]: 
[tags]: 
Vanishing gradient problem and choice of cost function

I am reading chapter 6.2.7 vanishing gradient problem in the book Ovidiu Calin - Deep Learning Architectures - A Mathematical Approach. On page 187 the author mentioned one of the causes, i.e. the size of the cost function gradient depends on the choice of the cost function. The author gave the table of 3 cost function examples: The author gave comments on each of the choice, stating that the last choice (binary classification NLL) has the least vanishing gradient effect, because it does not depend on the activation function. As I understand the choice of the cost function depends on the nature of the problem. For example in a regression problem the NLL is the quadratic cost plus some constant terms, whereas the 2nd choice of cost is the NLL in a multi-class classification problem. Therefore I would like to know, is the nature of the problem one of the underlying causes here for the vanishing gradient problem? Thanks in advance!
