[site]: crossvalidated
[post_id]: 613934
[parent_id]: 351536
[tags]: 
Undersampling at the data-collection phase can make sense , as you might be able to do more efficient data-collection. This is the topic of that King and Zeng paper discussed in the link. However, once you have the data, unless you have computational constraints (cannot fit a huge dataset into memory, modeling is too slow even if you can, etc), undersampling makes no sense to me. I have my qualms with oversampling, too, but undersampling discards precious data. Since it turns out that class imbalance is rarely a problem for proper statistical methods , with those rare exceptions being outside the usual sense in many machine learning circles about why imbalance is a problem, discarding precious data to fix a non-problem seems like the worst idea of all. Regarding oversampling, there probably is no need to do this. Proper statistical methods handle class imbalance just fine in most cases. However, at least you arenâ€™t wasting data solving a non-problem when you oversample.
