[site]: datascience
[post_id]: 47653
[parent_id]: 47449
[tags]: 
Your "fixed timestep" idea is actually very similar to a common technique called frame skipping . Instead of waiting a fixed amount of time, agents wait a fixed number of frames $k$ before choosing a new action. In the meantime, they repeat their most recently chosen action. Frame skipping was included as part of the Atari 2600 Arcade Learning Environment . It was also used in the foundational DQN paper . Common values of $k$ are 3, 4, and 5. The value chosen depended on the game being played, since different games had important events happen at different time resolutions. In these papers, frame skipping enabled training to happen roughly $k$ times faster. So this is definitely a valid technique to try. I actually think this would generally be less of a concern in the robotics application. Forward propagation usually happens much more quickly than real-world event timescales. As an example, Stanford famously applies RL to fly small helicopters , which requires considerable precision. Finally, if your forward propagation really is taking too long, you should consider a faster architecture. One approach would be just to make your neural net smaller. You might consider policy distillation for compressing a large, trained network into a smaller one. Also, make sure you're not using some ridiculously slow activation function like sigmoid or tanh. ReLU is the common choice if you don't need a bounded output for a given neuron. If you do, I recommend softsign . If your time bottleneck is actually in action selection , due to a large action space and using a value network, you should seriously consider switching to a policy-based method (e.g. actor critic). This would help because sampling from a distribution over actions would potentially be much faster than the $\max$ operation involved in value-based methods. You can read more about this in Section 13.7 of Sutton and Barto's RL book .
