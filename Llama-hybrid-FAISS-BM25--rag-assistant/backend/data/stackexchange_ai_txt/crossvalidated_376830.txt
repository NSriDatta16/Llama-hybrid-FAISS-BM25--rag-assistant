[site]: crossvalidated
[post_id]: 376830
[parent_id]: 184657
[tags]: 
First of all, what does policy (denoted by $\pi$ ) actually mean? Policy specifies an action $a$ , that is taken in a state $s$ (or more precisely, $\pi$ is a probability, that an action $a$ is taken in a state $s$ ). Second, what types of learning do we have? Evaluate $Q(s,a)$ function: predict sum of future discounted rewards, where $a$ is an action and $s$ is a state. Find $\pi$ (actually, $\pi(a|s)$ ), that yields a maximum reward. Back to the original question. On-policy and off-policy learning is only related to the first task: evaluating $Q(s,a)$ . The difference is this: In on-policy learning, the $Q(s,a)$ function is learned from actions that we took using our current policy $\pi(a|s)$ . In off-policy learning, the $Q(s,a)$ function is learned from taking different actions (for example, random actions). We don't even need a policy at all! This is the update function for the on-policy SARSA algorithm: $Q(s,a) \leftarrow Q(s,a)+\alpha(r+\gamma Q(s',a')-Q(s,a))$ , where $a'$ is the action, that was taken according to policy $\pi$ . Compare it with the update function for the off-policy Q-learning algorithm: $Q(s,a) \leftarrow Q(s,a)+\alpha(r+\gamma \max_{a'}Q(s',a')-Q(s,a))$ , where $a'$ are all actions, that were probed in state $s'$ .
