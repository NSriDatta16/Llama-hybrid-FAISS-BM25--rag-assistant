[site]: datascience
[post_id]: 66238
[parent_id]: 66216
[tags]: 
By passing a callable for parameter scoring , that uses the model's oob score directly and completely ignores the passed data, you should be able to make the GridSearchCV act the way you want it to. Just pass a single split for the cv parameter, as @jncranton suggests; you can even go further and make that single split use all the data for the training portion, and the testing portion won't even get used in the above setup. (Does sklearn perform a check to prevent passing cv=1 ?) I haven't had a chance to try this out yet: def oob_scorer(estimator, X, y): return estimator.oob_score_ model = GridSearchCV(estimator=RandomForest(...), param_grid={...}, scoring=oob_scorer, cv=PredefinedSplit([-1]*TRAIN_SET.shape[0]), ... ) scikit docs: Fixed split Custom scorer Related Qs: Scikitlearn grid search random forest using oob as metric? RandomForestClassifier OOB scoring method I'm not sure the hackiness of this approach is worth it; it wouldn't be terribly difficult to make the grid loop yourself, even with parallelization. EDIT: Yes, a cv-splitter with no test group fails. Hackier by the minute, but you can split off just a single test point, or add a dummy test set, or... Here's a working example. It does seem the oob_score is being used, and the test set has just a single sacrificial point: https://github.com/bmreiniger/datascience.stackexchange/blob/master/GridSearchNoCV_oob.ipynb
