[site]: stackoverflow
[post_id]: 3302059
[parent_id]: 3301983
[tags]: 
No, Big-O notation is not necessarily restricted to the worst-case. Typically you will see Big-O published for best-case, average-case, and worst-case. It is just that most people tend to focus on the worst-case. Except in the case of a hash table the worst-case rarely happens so using the average-case tends to be more useful. Yes, a good hash function reduces the probability of a collision. A bad hash function may cause the clustering effect (where different values hash to the exact same value or close to the same value). It is easy to demonstrate that HashSet can indeed become O(n) by implementing the GetHashCode function in such a manner that it returns the same value all of the time. In a nutshull, yes HashSet and Dictionary can be described as having O(1) runtime complexity because the emphasis is on the average-case scenario. By the way, Big-O can be used to analyze amortized complexity as well. Amortized complexity is how a sequence of separate (and sometimes even different) operations behave when grouped together as if they were one big operation. For example, a splay tree is said to have amortized O(log(n)) search, insert, and delete complexity even though the worst-case for each could O(n) and the best-case is O(1).
