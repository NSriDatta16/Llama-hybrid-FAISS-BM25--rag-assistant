[site]: crossvalidated
[post_id]: 183520
[parent_id]: 
[tags]: 
Advantage of latent SVM for part-based object detection

In the famous paper Object Detection with Discriminatively Trained Part Based Models , the authors use a Latent SVM approach to learn the detector of each part, because the localization of the parts in the training samples is unknown (the localization is the latent variable). However, in order to keep the SVM optimization convex, the latent variables of positive samples must be fixed during Latent SVM training and they are optimized iteratively using a coordinate descent approach. Here is my question : what is exactly the advantage of using a LatentSVM in this context? I understand that : The semi-convexity property is important because it leads to a convex optimization problem in step 2, even though the latent values for the negative examples are not fixed. A similar procedure that fixes latent values for all examples in each round would likely fail to yield good results. Suppose we let Z specify latent values for all examples in D. Since LD (β) effectively maximizes over negative latent values, LD (β) could be much larger than LD(β, Z), and we should not expect that minimizing LD (β, Z) would lead to a good model. Of course, it would be stupid to optimize LD(β,Z) over Z (latent position) for negative samples, because it would mean that we seek the simplest negative samples. However, i do not see why we need Z at all for negative samples. Why is a classical bootstraping (or data-mining as they call it) not enough?
