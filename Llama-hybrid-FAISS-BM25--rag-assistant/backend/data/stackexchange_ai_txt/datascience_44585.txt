[site]: datascience
[post_id]: 44585
[parent_id]: 44584
[tags]: 
Changing the batch size might change things, but I would doubt an increase in acciracy of 16% by change to batch size alone, given you have 30,000 images. I would recommend playing with the learning rate in conjunction with batch size or at least allowing your model to train for more epochs. You can use the following points to guide your trials. Changing the batch size to something smaller will generally: Make the learning curve more volatile, because each loss update is the average of a smaller set of samples, meaning the variation between batches will likely increase Make learning faster because the model will get bigger (but maybe more biased) updates for weights in each step Decrease overall model accuracy - assuming the model with larger batches were to be allowed to train for much longer, given the first two points above There has been a lot of research published on the effects of batch size, so maybe have a quick look through some papers ( like this one ) for the general outcomes. Also have a look at the answer to this question about the effects of batch size on DNNs.
