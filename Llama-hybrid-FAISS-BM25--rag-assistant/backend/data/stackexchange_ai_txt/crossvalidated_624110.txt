[site]: crossvalidated
[post_id]: 624110
[parent_id]: 624104
[tags]: 
Until you study Bayesian inference this will not make as much sense to you. But you started off incorrectly. With continuous data the probability of achieving any one value of a test statistics (or of a summary measure the test statistic is based on) is zero. So the p-value is the probability of getting a test statistic that is more extreme than the observed one if $H_0$ is true and the model is correct. To read more about the enormous difference between $\alpha$ and decision errors, read this . $\alpha$ isn't the probability of making a mistake. This is one of the most common interpretation errors and is at the root of a lot of unclear thinking by a lot of practitioners. The term "type I error rate" is a misnomer that started us off on the wrong foot a century ago. It's not a rate and is not an error probability. That's why I've moved to the term "type I assertion probability $\alpha$ in its stead. It's just an assertion trigger probability. No conditional probability that assumes $H_0$ is true can inform you about the veracity of $H_0$ . With Bayesian posterior probabilities you compute probabilities of any assertion you have and use direct reasoning. If you decide to act as if an effect is positive when P(effect > 0 | data, prior) = 0.98 you automatically get a decision error probability to carry along, which is simply 0.02.
