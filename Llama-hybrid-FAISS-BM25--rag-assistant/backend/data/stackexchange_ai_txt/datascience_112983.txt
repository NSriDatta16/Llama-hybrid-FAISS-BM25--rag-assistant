[site]: datascience
[post_id]: 112983
[parent_id]: 
[tags]: 
GNN Model - Analyzing Training Curve

Introduction. Actually, I am working on a Graph Neural Network (GNN) model to predict some graph-level float values. So, input=graph, output=float predicted value. I trained and evaluated the proposed model with the below structure and the training curve is illustrated. The problem is the validate curve fluctuates and it does not converge well to the training curve. Dataset Info. My dataset consists of ~20K graph samples such that each graph has its own size(in terms of #nodes, #edges), and each node in each graph has its own feature value. Note that, the size of features (X_vi) of all nods in all graphs is constant). Number of samples: 18570. Number of output: 1. Number of edge features: 2. Number of node features: 7. Num of training samples 14856 [ 80.0 % ]. Num of validation samples 1857 [ 10.0 % ]. ----------------------------------------------------------------- Total number of model parameters:1007701 Task type: regression | Evaluation metric: rmse self.args_gnn = 'gcn' self.args_drop_ratio = 0.5 #dropout ratio (default: 0.5) self.args_num_layer = 5 #number of GNN message passing layers (default: 5) self.args_emb_dim = 300 #dimensionality of hidden units in GNNs (default: 300) self.args_batch_size = 64 #input batch size for training (default: 32) self.args_epochs = 150 #number of epochs to train (default: 300) self.args_num_workers = 0 #number of workers (default: 0) self.args_lr = 0.001 #learning rate self.args_wdecay = 0.001 #weight decay self.args_pooling = 'sum' #pooling
