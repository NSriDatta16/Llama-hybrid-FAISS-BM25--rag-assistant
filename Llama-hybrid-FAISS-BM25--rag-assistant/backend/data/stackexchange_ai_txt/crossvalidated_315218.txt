[site]: crossvalidated
[post_id]: 315218
[parent_id]: 172842
[tags]: 
Theoretically, Random Forest is ideal as it is commonly assumed and described by Breiman and Cuttler . In practice, it is very good but far from ideal. Therefore, these questions are very valid. RF are not handling outliers as ideally as it is widely assumed. They are susceptible to even a single outlier with extreme values as was shown in How are Random Forests not sensitive to outliers? , and also there are couple of papers about how heteroscedasticity affects the RF predictions. In real-life data, you may have a lot (1-2%) of such outliers caused by typos (for human-inputted data like 3200 instead of 32.00), jumps of electrical current due to induction or simply due to unexpected exposures (for IoT), heteroscedasticity, etc. These "outliers" end up in many leaves of the decision trees, pulling predictions to higher values. In case of unbalanced data where large number of target_value = 0, RF tends to underestimate predictions significantly. Log-transformations can improve accuracy, especially in case of very skewed data (with very long tails). See for example "Forecasting Bike Sharing Demand" by Jayant Malani et al. ( pdf ), and this kaggle submission . RF tends assigning higher importance to variables that have larger range of values (both categorical and continuous). For example, see this blog post: Are categorical variables getting lost in your random forests? So, data preprocessing is very important even in the case of Random Forest. I hope, this answer frames the validity of the questions and the links will provide some answers with starting points.
