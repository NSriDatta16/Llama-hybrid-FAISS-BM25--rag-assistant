[site]: crossvalidated
[post_id]: 225550
[parent_id]: 
[tags]: 
Should I optimize neural networks that are part of ensemble of neural networks?

I'm creating ensemble of neural networks for a simple binary classification task. Every neural network is generated and trained a bit differently (number of hidden layers, number of neurons per layer, learning rate etc.). My question is, should I train these weaker networks and optimize each of them as much as possible or should I just use them as soon they reach "good enough" classification? My "good enough" classification is around 80% since I can get it pretty fast without too much hassle of parameter optimization, but I can also reach 92-93% with some parameter tuning and it takes me 15-30 minutes to do it. What did you discover works better in general?
