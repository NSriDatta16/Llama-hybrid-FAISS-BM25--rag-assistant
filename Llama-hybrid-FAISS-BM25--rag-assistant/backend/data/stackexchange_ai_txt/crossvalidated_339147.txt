[site]: crossvalidated
[post_id]: 339147
[parent_id]: 339141
[tags]: 
The issue here is that the average of CIs are simply not “efficient” (not the appropriate use of this word from a statistical perspective, but reasonable in an informal sense for this context). If you take the average of the boundaries of the CIs, you will end up with a new interval that has about the same length as the intervals used to find the averages. Thus, you end up with a new interval that is (1) better centered on the population mean ( i.e. , it has higher probability that it “captures” the mean), and (2) is much larger than it would need to be to capture the mean 95% of the time (upon hypothetical replication). However, as you suggest in your query, if you aggregate your data into one larger data set, then you obtain a much narrower interval. So, at the heart of this question is what is more important: ¿confidence or precision? If you are willing to sacrifice precision for confidence, then you can take the much larger interval. If you want more precision, then you have to sacrifice some level of confidence. Here is a small bit of R code that helps demonstrate this: set.seed(1234) rep.int(NA,100) -> lens.S -> lens.L for(ijk in 1:100) { n
