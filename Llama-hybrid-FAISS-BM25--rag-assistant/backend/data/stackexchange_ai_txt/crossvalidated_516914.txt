[site]: crossvalidated
[post_id]: 516914
[parent_id]: 
[tags]: 
Better understanding what is the model in Bayesian approach

I was trying to understand model likelihood and now with great confidence I can also call it: integrative likelihood marginal likelihood predictive likelihood the evidence since I read the paper : The following formula: $$ \begin{align} p(X\mid M)=\int_\theta p(X \mid \theta, M) \,p(\theta \mid M)\, d\theta \tag{1} \end{align} $$ is marginalized out parameters formula . This formula say when $\theta$ vary, we will get model likelihood given $X$ , since $X$ is fixed (all data we have). But, the shorter formula also called evidence would be: $$ \begin{align} p(X)=\int_\theta p(X \mid \theta) \,p(\theta)\, d\theta \tag{2} \end{align} $$ This leads me to a conclusion that if not explicitly set we always assume $p(X)$ is $p(X \mid M)$ because we need to have the model always . Question: We never write $p(M \mid \theta)$ so we never write the model depends on parameters because the model is not a random variable I guess. It is just a parameter from the finite/infinite set of models. Or maybe the model is a random variable after all. Can you elaborate and help me sort how parameters are connected with the model. As a preparation to this question I have several similar treads in here and I am actively reading 2 books on Bayesian subject but this is still unknown to me.
