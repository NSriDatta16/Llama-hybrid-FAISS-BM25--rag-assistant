[site]: crossvalidated
[post_id]: 226251
[parent_id]: 225899
[tags]: 
Interesting question. In a practical sense, Bayesian and Likelihood-based approaches only differ in that the former requires the specification of a prior. If you can come up with a Likelihood-based model: $$L(\theta;Data) = f(Data\vert\theta),$$ then you "just" need to specify a prior for $\theta$, $\pi(\theta)$. The way you typically go through the process of checking the residuals against a set of assumptions, adjusting/removing variables, rerunning the model, checking the assumptions again, and again is probably based on the fitted model using maximum likelihood estimation. In a Bayesian framework, however, people take advantage of having access to the full posterior distribution. Then, you can either obtain a point estimator from this distribution (e.g. posterior mean, posterior median, ...) and conduct the same analysis using this estimate. Alternatively, and more formally, you can integrate the parameters out with respect to the posterior distribution and use the predictive posterior distribution $$f(y\vert Data) = \int f(y\vert\theta)\pi(\theta\vert Data)d\theta.$$ For instance, in this paper the authors check the goodness of fit of a Bayesian linear regression model based on the predictive residuals, something like a Bayesian QQ plot. Of course, if your model is complicated, sampling from the posterior distribution may be a difficult task, but conceptually, it is possible to conduct the same type of analysis, with some care.
