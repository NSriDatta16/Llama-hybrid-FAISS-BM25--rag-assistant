[site]: datascience
[post_id]: 120297
[parent_id]: 120289
[tags]: 
presents are the model hidden states. This return value is used during inference, to avoid recomputing the previous steps' hidden states at every inference step (see sample.py ). Autoregressive language models do not generate sentence representations, just predict the next token. Typically, it's been masked language models (like BERT) the ones that generated a sentence-level representation, usually training it over some sentence-level task (e.g. next sentence prediction in BERT). Therefore, GPT-2 is simply not meant for what you want. set_embedding_weights is a method of the GPT2LMHead to set the fully connected matrix to be equal to the embedding matrix. This way, you reuse the same parameters for the embedding matrix and for the logit projection matrix. This is a frequent approach to regularize the model (i.e. avoid overfitting reducing the total number of parameters). I believe this is the piece of research that introduced such an approach. As I mentioned in point (2), GPT-2 is not meant for this. One option would be to average the output representations for all tokens. Another option would be to use a model that is specifically designed to obtain sentence-level representations, like BERT.
