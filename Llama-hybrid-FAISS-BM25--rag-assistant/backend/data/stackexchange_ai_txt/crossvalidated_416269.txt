[site]: crossvalidated
[post_id]: 416269
[parent_id]: 
[tags]: 
Why can't a 3D point cloud be fed into a CNN?

I've been reading this blog post about deep learning on point clouds and the author gives an example that is supposed to illustrate the two main problems with applying convolution to point clouds. According to him, the problems are variance to ordering (point clouds have no inherent natural ordering of their points) and desertion of shape (although there's no connectivity between the points, there is a topology defined on the whole set of points). Now, with his picture of the example I'll try to describe the issue I have with his example. He gives three point clouds $A$ , $B$ and $C$ , each has four points, that are ordered in a certain manner ( $A$ and $B$ have the same ordering, but different shapes; $B$ and $C$ have the same shape, but different orderings) and each point within a point cloud exhibits some feature (visualized by a different color). Then he convolves each point cloud with a 2x2-kernel s.t. $f(A) = f(B)$ and $f(B) \neq f(C)$ . At this point, he states that convolution doesn't respect that a point cloud is unordererd (convolutions of $B$ and $C$ are not equal although only their ordering is different) and ignores the topological structures ( $A$ and $B$ are clearly different w.r.t. their point positions but result in the same convolution). But, the issue I see with this example is that he totally ignores the positions of the points in 3D space during convolution. If taken into account, convolution would yield different results, right? However, I can't think of a complete example that points out the same issues with convolution on point clouds. If anyone can, please share your insight, this will be very much appreciated.
