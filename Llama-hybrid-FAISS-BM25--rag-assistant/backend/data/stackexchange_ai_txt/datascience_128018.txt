[site]: datascience
[post_id]: 128018
[parent_id]: 126476
[tags]: 
Correct, In Batch Normalization, the mean and standard deviation are calculated feature-wise, and the normalization step is done instance-wise. For each feature, the mean and standard deviation are calculated across the instances in the batch, and then each instance is normalized using these feature-wise statistics. This is in contrast to Layer Normalization, where the mean and standard deviation are calculated instance-wise, and the normalization step is done feature-wise. This means that for each instance, the mean and standard deviation are calculated across the features, and then each feature is normalized using these instance-wise statistics In Group Normalization, the mean and standard deviation, as well as the normalization operation, are done by calculating the mean and standard deviation of a subgroup in the input tensor and then applying a scale and offset factor to normalize the activations of a single sample. This makes Group Normalization suitable for recurrent neural networks as well, as it normalizes the activations of a single sample rather than working on batches
