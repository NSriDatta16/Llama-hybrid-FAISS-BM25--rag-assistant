[site]: crossvalidated
[post_id]: 612646
[parent_id]: 
[tags]: 
Classical linear regression with no independent sample

I like to organize my studies always with the weakest hypotheses possible. In this case, I want to understand well what assumptions I should add to be able to study linear regression models in time series. I want to analyze how far I can go in the OLS context assuming that my sample is not independent. At the end of the day, in the world of time series this is one of the first assumptions that fails. So let's start with the classical assumptions of linear regression. First, suppose the true model given by \begin{equation}\label{I}\tag{I} y= \beta_0 + x_1 \beta_1 + ...+x_K \beta_K + u= x'\beta + u, \quad E(u|x)=0 \end{equation} Now consider a sample $(y_i,x_i)_{i=1}^n$ (identically distributed but not independent) such that: $(y_i,x_i)_{i=1}^n$ satisfies (\ref{I}): $$y_i= \beta_0 + x_{i1} \beta_1 + ...+x_{iK} \beta_{K} + u_i= x_i'\beta + u_i,\quad i=1,...,n$$ in matrix notation, we have $$y= X\beta + U$$ Suppose that $X'X$ is non singular (almos sure); With this two assumptions I can show the existence of $\hat \beta= (X'X)^{-1}X' y$ Now, suppose that $$E(U|X)=E\left( \begin{bmatrix} u_{1} \\ \vdots \\ u_{n} \\ \end{bmatrix}\Bigg| \, \begin{bmatrix} x_{11} & \cdots & x_{1K} \\ x_{21} & \cdots & x_{2K} \\ x_{n1} & \cdots & x_{nK} \end{bmatrix} \right)=0$$ with this aditional assumption, the traditional books show that $E[\hat \beta]=\beta$ . And I think that an $AR(1)$ process does not satisfy this assumption. The fourth assumption in the classical linear regression model is the normal distribution and the no correlation of the errors: $$U \sim N(0. \sigma^2 I), \quad I \,\, \hbox{identity matrix }$$ In this case, we have $\hat \beta \sim N(0, \sigma^2 (X'X)^{-1})$ . But I think (I'm not sure) that this hypothesis fails not assuming independence in the sample, because: $$u_i = y_i - x_i' \beta$$ Thus, it is likely that the correlation matrix of $U$ is not $\sigma^2 I$ . But this is not a problem, we can assume that $$U \sim N(0. \sigma^2 \Omega)$$ In this case, we have $\hat \beta \sim N(0, \sigma^2(X'X)^{-1} X' \Omega X (X'X)^{-1})$ , assuming that $\Omega$ is known. I would still need to mention a hypothesis of efficiency, but in order not to go on too long, I ended up here. As far as I've read, many of the classic books don't make the case for the sample not being independent. They start from the hypothesis of an iid sample. I think this is really bad, as it doesn't allow you to make a natural transition into the world of time series. So my concrete question is: are all the conclusions I mentioned true assuming the hypotheses of linear regression, but in case the sample is not independent? Is there something I'm getting wrong?
