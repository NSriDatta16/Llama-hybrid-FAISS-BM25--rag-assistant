[site]: crossvalidated
[post_id]: 436929
[parent_id]: 436905
[tags]: 
A general rule of thumb is that if the complexity of your data is low, then you probably don't need a complex model for those data. Meaning, if your time series is a short series or not very noisy, it may be better to choose a simpler model than neural networks (very simple models often do pretty well in a lot of time series cases). Naive methods, moving averages, and linear models are far more common over neural networks in practice for simple forecasting tasks. As for choosing the best neural network, what you have been doing is often called hyperparameter tuning or model tuning: finding the combination of parameters that give you the best model (like number of neurons and number of layers). The most straightforward method is to do what you're doing. Train a bunch of models and compare the metric. This is typically done by creating a test set or holdout set from your and testing the models on that. If you have 24 months of data, train your networks on the first 18 months and then test them to see how they perform on the last 6 months of unseen data. For most deep learning tasks in practice, no more than 2 or 3 layers are really needed (most data in practice, especially time series, will need more complexity than that). But again, it always depends on the data and situation. See what works best.
