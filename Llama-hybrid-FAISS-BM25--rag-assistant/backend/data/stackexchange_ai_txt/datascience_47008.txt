[site]: datascience
[post_id]: 47008
[parent_id]: 
[tags]: 
Is normalizing the validation set of time series a kind of look ahead bias?

Here's the data normalization process of a time series in a paper about stock prediction using LSTM: Split train and test set based on time (e.g. training set: 2001-2010, test set:2011-2012). This looks fine to me. Normalize the training set by subtracting the mean and dividing them by the standard deviation of the training set. Train the data, using 20% of samples as the validation set. Keras's model, by default, uses the last 20% for validation. So, in the training phase, the model knows a little about the validation set through the mean and standard deviation in step 2. On one hand, the model is training using some of the future information. On the other hand, no information of the test set is leaked to the model. Is this a kind of look ahead bias? And what is the best practices in this case?
