[site]: crossvalidated
[post_id]: 517512
[parent_id]: 517488
[tags]: 
Comment: This the germ of an idea, not yet a solution. [I am assuming the mean $\mu$ is also unknown.] Suppose $n = 100$ observations from a standard normal distribution. You might find the distance $D$ between order statistics 97 and 100. Then $1/D$ should estimate $1.$ Several runs give various answers. In R, set.seed(123) diff(sort(rnorm(100))[c(97,100)]) [1] 0.4004199 diff(sort(rnorm(100))[c(97,100)]) [1] 1.243827 diff(sort(rnorm(100))[c(97,100)]) [1] 0.337785 diff(sort(rnorm(100))[c(97,100)]) [1] 0.8870224 Find the average of a million runs: set.seed(2021) d.4 = replicate(10^6, diff( sort(rnorm(100))[c(97,100)] ) ) mean(d.4) [1] 0.7060959 sg.est = d.4/.706 mean(sg.est); sd(sg.est) [1] 1.000136 [1] 0.5530556 So, on average the distance between the 97th and 100th order statistics divided by 0.706 estimates $\sigma$ with standard error about 0.553. Here is a histogram of such estimates of $\sigma$ along with quantiles 0.025 and 0.975 of the estimates. hist(d.4/.706, prob=T, col="skyblue2") abline(v=quantile(sg.est, c(.975,.025)), col="red") Maybe we can find normal quantiles that give approximately 0.706, without simulation. One possibility: diff(qnorm(c(.97,.995))) [1] 0.6950357 Something like this might generalize for other values of $n$ between 50 and 200.
