[site]: crossvalidated
[post_id]: 504063
[parent_id]: 185400
[tags]: 
Late answer, but some key points can be added. I personnally think of model stacking as "the natural sequel" of model averaging. And there is a reason why model averages are often better than single models. Model averaging Usually, two (different) models with similar performance often perform better than the best model when the average of the prediction is used. This is particularly true when the penalty is a convex function (MSE, RMSE...) and is a consequence of Jensen's inequality Model (weighted) averaging Model averaging can be seen as a particular case of model stacking. If you use a linear model on top on the "first stage" models, you are simply optimizing the weights given to each model (while model averaging was just giving the same weight to each model). Model stacking Then, when you drop the linear assumption on the model you train on the "stage 1 features" you simply increase the size of the space in which you "search" your model. The larger this space is, the more likely you are to find better performance. I presented this in more details in model stacking: a tutorial You may also be interested in this article: Stacked generalization by David H. Wolpert, which is one of the first (as far as I know) scholar publication about this method. Edit I could not find many other references online so I detailed the arguments above on my blog: why does model staging works ?
