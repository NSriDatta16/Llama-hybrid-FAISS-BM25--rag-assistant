[site]: crossvalidated
[post_id]: 614601
[parent_id]: 239166
[tags]: 
This is only a guess, but I suspect the regularization is interacting with the logistic regression optimizer. In principle, if you can find optimal loss-minimizing parameters, regularization won't increase performance, and instead is likely to lower it (on the training set). However, for large data sets, there are typically stochastic or iterative solvers used to learn the regression parameters, and these will not generally find an optimal solution. For example, the sklearn default in python is LBFGS, a low-memory variant of a quasi-Newtonian iterative solver. Intuitively, when you add regression, you may be restricting the optimization path to a smaller, "better behaved" region of the parameter space, making the optimizer work better in practice.
