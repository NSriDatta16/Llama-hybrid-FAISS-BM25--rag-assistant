[site]: stackoverflow
[post_id]: 3791680
[parent_id]: 3767381
[tags]: 
See Brian's answer. Run lots of copies of it. Use a shared storage system for keeping intermediate and final data. It might be helpful to take more memory-intensive parts of the crawler (HTML parsing etc) and put those in a separate set of processes. So have a pool of fetchers which read from the queue of pages to read, and put them into the shared storage area, and a pool of parser processes which read pages and write the results into the results database and queue new pages into the queue to read. Or something. It really depends on the purpose of your crawler. Ultimately if you're trying to crawl a lot of pages you'll probably need a lot of hardware and a very fat pipe (to your datacentre/ colo). So you'll need an architecture which allows the parts of the crawler to be split across many machines to scale properly.
