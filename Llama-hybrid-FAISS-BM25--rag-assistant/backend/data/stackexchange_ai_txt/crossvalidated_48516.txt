[site]: crossvalidated
[post_id]: 48516
[parent_id]: 48441
[tags]: 
If I understand correctly, this is just standard sequential binary prediction. You have an alphabet $\Sigma = \{-1,1\}$, and samples drawn from $\Sigma^*$ (the set of all finite length strings over your alphabet) Given a string $s_{1:t-1} \in \Sigma^*$ (the observations from time $1$ to $t-1$) you want to predict $s_{t}$. There are many techniques you could apply. What's going to be best is largely dependent on the nature of the data, especially things like how long each sequence is and how complex the dependency structure is. For example, it may be the case that $s_{t+1} = \text{sign}\left(\sum_{i=1}^{t} s_i\right) =$ the most frequent symbol is a good predictor. On the other hand if your data features dependencies separated by long time periods, something like $s_i = -1 \implies P(s_{i+100} = 1) >> P(s_{i+100} = -1)$, you're going to have a bad time as these types of relationship are notoriously hard to learn. Without knowing more it is difficult to suggest any particular approach but here are some potential ideas to get your started/give you things to google. $n^{th}$ order markov models $k$-nearest neighbors with some appropriate distance function (Levenshtein distance for example) hidden markov models recurrent neural networks context tree weighting
