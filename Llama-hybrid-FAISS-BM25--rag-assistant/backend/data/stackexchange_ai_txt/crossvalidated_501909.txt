[site]: crossvalidated
[post_id]: 501909
[parent_id]: 
[tags]: 
Probability of points less than a fixed distance apart in a vector space

I have a distribution $D$ of points in a normed vector space (it's $\mathbb{R}^n$ using the $L_\infty$ norm, but I don't think that matters). In this particular space, points that are less than a certain distance apart are considered "indistinguishable" (basically they're measurements that are within error margins of each other given the test that we're using). I want to try and figure out how many points I need to sample from $D$ to get, on average, less than a chosen proportion of points that are "isolated" (only indistinguishable from themselves). At the moment I'm doing it by simulation (sampling from a big dataset, drawing a closed ball around each point, and seeing how many points are isolated). However, it's taking a long time and I'm hoping that there's some theory to help me simplify the problem in the long run. Someone must have done this before - I'm feeling like there's something in packing theory, some distribution that I can use to model it, stuff like that - but I don't know what papers or books to read. Can anyone point me in the right direction?
