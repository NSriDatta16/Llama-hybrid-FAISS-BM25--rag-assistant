[site]: crossvalidated
[post_id]: 161329
[parent_id]: 161328
[tags]: 
I guess that you ask about parametrization of Normal distribution, as some statistical software such as BUGS , or JAGS use precision rather than variance. First, notice that it is not true for every statistical package, e.g. R or Stan use "traditional" standard deviation (square root of variance), so this is not always true. Why precision is used? It is mostly for some historical, obsolete, reasons. The general rationale, as I heard about it, was that we are rather interested in how "precise", i.e. focused around the mean, is our variable, rather then how "unprecise" or diffuse it is, since we often think of it in terms of mean. There is also another reason given by Kruschke ( Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan , p. 454): Because of its role in conjugate priors for normal likelihood function, the gamma distribution is routinely used as a prior for precision. But there is no logical necessity to do so, and modern MCMC methods permit more flexible specification of priors. Indeed, because precision is less intuitive than standard deviation, it can be more useful to give standard deviation a uniform prior that spans a wide range. The problem with precision is also that in most cases people are interested in knowing the variance, so you end up with inverting precision.
