[site]: crossvalidated
[post_id]: 621831
[parent_id]: 621825
[tags]: 
The textbook answer you'll see to this is something along the lines of "if there is a large gap between your training and test loss, you have overfit" , where of course how large "large" has to be is a little unclear. Personally I think the truth is much more subtle than this. I think probably the scenario in which you can see overfitting happening in the most transparent way is if you take your data and do a train/test split, and then train decision trees of ever-decreasing min_samples_leaf (or for something more discrete, ever-increasing max_depth). You'll see your training loss getting better and better until eventually it's perfect as the tree bottoms out into pure nodes, but your test loss will be optimal at some point in the middle. At min_samples_leaf sizes lower than the one which is optimal for your test set or equivalently max_depths higher than the optimal one, you have overfit. This is probably the only scenario I can think of in which there's a binary yes/no answer to whether your model has overfit, because you know the best you can possibly do with this data+model combination and you've introduced a higher level of complexity than optimal. You will in this case probably find that at this optimal point, the delta between train and test loss is very low and as you surpass this optimal point, this delta starts to grow very quickly as the train loss continues to improve and the test loss gets worse. However, a textbook example of where this isn't true is random forests. Imagine doing the same experiment as above but with a random forest. You will find for the same dataset, the optimal point in min_samples_leaf or max_depth will occur at a point of higher model complexity than it does for the decision tree (and the optimal test performance will be better). Often you'll find, quite astonishingly, that the best test-performance (if you have enough trees) is occurring at min_samples_leaf=1 or 2. Your train loss will be almost perfect and your test loss will certainly not be perfect, but this might be the best you can get your test loss. Have you overfit? Well you've intentionally overfit the individual trees, but has the overall random forest model overfit? That's a philosophical question imo. If you want to get more philosophical, I would say that as soon as you start to fit a model, you are starting to overfit. It's just that to begin with, you're mainly picking up signal + a bit of noise on that signal and as you continue, the rate at which you pick up signal starts to decrease and at some point in order to pick up any more signal, you pick up so much noise as to make the net effect a negative one. So all models are a somewhat overfit, the point of cross-validation is to pick the model which trades off overfitted- and underfittedness optimally on unseen examples. So to attempt to answer your three questions based on what I have just said: Cross validation doesn't detect overfitting, it helps you find the model which best trades off over- vs underfitting and thus helps you find the model which is likely to perform best on unseen data I think your second question isn't really related to the rest of what you asked/what I said, but it's a common enough question. Best practice is to use CV to find the best model parameters/architecture and then use that setup and retrain on all data you have available There is no such thing. A good score generally depends on the problem, but I think maybe you meant "what is a good gap between train and test loss", to which there also isn't really an answer. Use CV to find the best possible performance on a test set which is an estimator for how well you'll do on unseen data in production, and then decide whether that score is good enough for what you're trying to do (usually by evaluating the costs of False Positives and Negatives Vs the benefit of True Positives and Negatives)
