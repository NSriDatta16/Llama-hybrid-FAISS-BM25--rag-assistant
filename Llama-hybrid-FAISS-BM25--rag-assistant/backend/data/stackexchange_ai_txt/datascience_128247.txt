[site]: datascience
[post_id]: 128247
[parent_id]: 126915
[tags]: 
transformer tech is evolved version of LSTM/rnn based tech by improving on speed and quality. particularly the tokenization, pre-training method, multi-head attention, longer input size are improved or newer techniques from lstm based models. Tokenization addressed issue of scarceness. multiple attention heads to capture different types of inter-relationships between tokens such as morphological, syntactic, and even deeper semantic. This makes the embedding better in terms of how effectively it represents the semantics of the tokens in the given unique context. training on longer sequences captures very refined differences in meaning than older models.
