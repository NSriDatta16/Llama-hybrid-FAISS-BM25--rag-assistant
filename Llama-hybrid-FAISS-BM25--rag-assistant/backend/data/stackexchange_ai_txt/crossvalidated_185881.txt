[site]: crossvalidated
[post_id]: 185881
[parent_id]: 185858
[tags]: 
The Kullback-Leibler divergence is widely used in variational inference, where an optimization problem is constructed that aims at minimizing the KL-divergence between the intractable target distribution P and a sought element Q from a class of tractable distributions. The "direction" of the KL divergence then must be chosen such that the expectation is taken with respect to Q to make the task feasible. Many approximating algorithms (which can also be used to fit probabilistic models to data) can be interpreted in this way. Among those are Mean Field, (Loopy) Belief Propagation (generalizing forward-backward and Viterbi for HMMs), Expectation Propagation, Junction graph/tree, tree-reweighted Belief Propagation and many more. References Wainwright, M. J. and Jordan, M. I. Graphical models, exponential families, and variational inference , Foundations and Trendstextregistered in Machine Learning, Now Publishers Inc., 2008 , Vol. 1(1-2), pp. 1-305 Yedidia, J. S.; Freeman, W. T. & Weiss, Y. Constructing Free-Energy Approximations and Generalized Belief Propagation Algorithms , Information Theory, IEEE Transactions on, IEEE, 2005 , 51, 2282-2312
