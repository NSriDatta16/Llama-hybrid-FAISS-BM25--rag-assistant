[site]: crossvalidated
[post_id]: 197592
[parent_id]: 197590
[tags]: 
The answer is "not necessarily" â€” how correlated the variables are dictates how "noisy" the scatter plot is, but not how steep. In fact, the correlation and regression slope are telling you two quite different things. The slope estimates how many units $y$ increases, on average, per one unit increase in $x$ . The correlation measures the strength and direction of that linear relationship (how closely the points fit to a line, and whether the trend is increasing or decreasing) on a scale between $-1$ and $1$ , but which does not depend on the units of measurement of the $x$ and $y$ axes. Clearly you can have a steeper line which has a worse linear fit, for example in the following diagram. Note we can scale our $x$ and $y$ axes up or down by some scale factor (which is equivalent to a change in the units of measurement: for instance, switching from metres to centimetres makes the numerical values recorded 100 times larger) and this doesn't change the correlation. It will, however, change the slope. For example, consider the data set $\{(0,0), (1,2), (2,4), (3,6)\}$ . Clearly the line of best fit will be $y=2x$ and the correlation will be $r=+1$ (there is a perfect linear fit, and the line is sloping upwards). Now consider the data set $\{(0,0), (1,200), (2,400), (3,600)\}$ , where we have scaled up the $y$ values by a factor of $100$ ; this is equivalent to switching $y$ from being measured in metres to being measured in centimetres. The correlation is still $+1$ but the equation of the line of best fit is now $y=200x$ , with a much steeper slope of $200$ . Evidently we can have the same correlation but a different slope. Moreover, if we now introduce some "noise" (random error) to the second data set, we might end up with $\{(0,0.21), (1,199.57), (2,400.28), (3,600.09)\}$ . This slightly changes the fitted regression line, but not much: the new equation is $y = 200.035x - 0.015$ , with a slope of $200.035$ . This is still a much steeper line than the original $y=2x$ . And yet this time, the regression line is not a perfect fit, so the correlation is below one ( $r=0.9999992$ ). So we have found a line with higher slope but lower correlation, which shows that slope and correlation do not always go hand in hand. Considering the effect of adding some random noise, and of rescaling the axes, is quite instructive in general when trying to understand regression. Moreover, suppose you did have two data sets, where the first has higher (let's keep it positive) correlation and slope than the second; $r_1 > r_2 > 0$ and $b_1 > b_2 > 0$ . It will always be possible to find some scale factor $k$ that we can multiply the $y$ values of the second data set by, such that after rescaling the new data set has a steeper slope than the first data set. We just pick any $k > b_1/b_2$ . The new data set has slope $b_3 = k b_2 > b_1$ but its correlation is unchanged from before the scaling, $r_3 = r_2 . So from any pair of data sets that does obey "greater correlation with greater slope", we can transform the data sets into a pair where the set with greater correlation has a lower slope. Okay, so what do the correlation and slope have to do with each other? The ( least-squares regression ) slope and the ( Pearson product-moment ) correlation always have the same sign . The parenthetical remarks are to point out that regression slopes calculated some other way (e.g. least absolute deviations regression ), and other types of correlation coefficient (for example Spearman's rank ), need not obey the same relationship. The correlation and slope will be identical if the $x$ and $y$ variables have been standardised (rescaled to have unit standard deviation). (NB this doesn't apply to standardised slopes in multiple regression, when we introduce several predictor variables.) If we held the standard deviations of $x$ and $y$ constant , then it is true that "higher correlation means higher slope". Now, all three of these facts can be deduced (check you can prove each of them!) from the following formula: $$\text{slope} = \frac{\text{SD of y}}{\text{SD of x}} \cdot \text{correlation}$$ This might help you understand why the rescaling approach I used before changes the slope even though the correlation remains the same: rescaling $y$ by some factor will also scale its standard deviation. So for instance, doubling $y$ will double the slope. Now, what do you think will happen if we rescale $x$ instead? R code for plot set.seed(1234567) x1
