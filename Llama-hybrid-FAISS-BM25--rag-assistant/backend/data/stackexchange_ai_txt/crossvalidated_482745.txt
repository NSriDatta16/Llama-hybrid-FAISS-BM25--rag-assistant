[site]: crossvalidated
[post_id]: 482745
[parent_id]: 
[tags]: 
What are the state of the art optimization methods for neural networks?

Neural networks are usually trained with first order gradient methods and it's variations such as: batch gradient descent, stochastic gradient descent, momentum based gradient and so on.. However those are at least superfitially very simple first order aproximations and I don't know how well they perform in very deep ANN architectures or more complex models. In those cases are the first order gradient method still used or are there more effitient optimization methods for neural nets?
