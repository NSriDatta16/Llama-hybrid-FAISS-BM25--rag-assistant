[site]: crossvalidated
[post_id]: 455364
[parent_id]: 282987
[tags]: 
I found this question, because I was wondering about their similarities and differences too. I think it's very important to state that Hidden Markov Models (HMMs) do not have inputs and outputs in the strictest sense. HMMs are so-called generative models, if you have an HMM, you can generate some observations from it as-is. This is fundamentally different from RNNs, as even if you have a trained RNN, you need input to it. A practical example where this is important is speech synthesis. The underlying Hidden Markov states are phones and the emitted probability events are the acoustics. If you have a word model trained, you can generate as many different realisations of it as you want. But with RNNs, you need to provide at least some input seed to get out your output. You could argue that in HMMs you also need to provide an initial distribution, so it's similar. But if we stick with the speech synthesis example,, it is not because the initial distribution will be fixed (starting always from first phones of the word). With RNNs you get a deterministic output sequence for a trained model, if you are using the same input seed all the time. With HMM, you don't because the transitions and the emissions are always sampled from a probability distribution.
