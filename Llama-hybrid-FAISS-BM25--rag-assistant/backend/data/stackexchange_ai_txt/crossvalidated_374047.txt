[site]: crossvalidated
[post_id]: 374047
[parent_id]: 371911
[tags]: 
Your described methodology appears as an implementation of bagging ; it is usually seen as a variance reduction technique. Bagging commonly utilises regression or classification trees as base-learners but using a cubic spline is also perfectly fine; the Wikipedia link shows a use-case where the base learner in LOESS smoother. In short, when implementing bagging we combine $B$ approximating functions (learners) $f_b(X)$ , $b = 1, \dots, B$ which are train on $B$ bootstrap samples of the training data set. Our final bagging predictor is then the average $f(X)_{bagging} = \frac{1}{B} \sum_1^B f_b(X)$ . Two standard paper references are: 1. Breiman (1996) Bagging predictors and 2. BÃ¼hlmann & Yu (2002) Analyzing bagging . If you prefer books, Berk's book Statistical Learning from a Regression Perspective has a short but very clear and insightful chapter (Chapt. 4) on bagging too.
