[site]: crossvalidated
[post_id]: 553286
[parent_id]: 
[tags]: 
Apart from the Bias-Variance "Decomposition" - is there a Bias-Variance "Proof"?

I am sure at some point, many of us have come across the "Bias-Variance Tradeoff" : The "error" of any "estimator" (e.g an estimator can be considered as a linear regression model) can be broken down into a "bias" component , a "variance" component and a "irreducible" component. There are many references which demonstrate why this is true: In the real world, the Bias-Variance Tradeoff is used to express the following "phenomena" when creating statistical models by relating this tradeoff to "complexity": Simple Models (e.g. models with fewer parameters) tend to have low variance but high bias : this means that in general, they will not be able to capture complex patterns within the data, but their performance will tend to be similar on unseen data compared to seen data. Complex Models (e.g. models with more parameters) tend to have high variance but low bias : this means that in general, they will be able to better capture complex patterns within the data, but their performance will not tend to be similar (worse) on unseen data compared to seen data. My Question: In the above statements, "model complexity" is added to the Bias-Variance Tradeoff equation. Although the above statements are "logical" in nature (e.g. it is not unreasonable to believe that a model with many moving parts might generalize poorly) - but are there any proofs which ties in "complexity" to this tradeoff? For example: Is there an equation which further decomposes a statistical model's error into "bias", "variance", "irreducible error" and "complexity"? Is there a "proof" that explains why "simpler models" maintain similar performances on unseen data whereas "complex models" have a more volatile performance on unseen data? Are there any references for the above points? Thanks! References: https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff Notes I do not know if this is an oversimplification, but I have often come across "model complexity" as a function of the "number of parameters" (e.g. number of regression beta coefficients or weights in a neural network) - however, I am not sure how this idea of "complexity" would transfer over to "non-parametric" models (e.g. decision trees, random forest). Regarding the importance of model complexity, I have heard that many years ago, researchers devised some simple experiments to demonstrate ideas such as "non-linearly separable data can be better classified by non-linear models compared to linear models": this was called "X-OR" (e.g. In the picture below, a straight line can not separate the red points from the green points, but a diagonal line can. A straight line can be expressed using a single parameter (e.g. straight_line_model = a ) whereas a diagonal line can not be expressed using a single parameter (e.g. diagonal_line_model = a + b_x) - a diagonal line can be thought of as having more parameters and being inherently more "complex" than a straight line) :
