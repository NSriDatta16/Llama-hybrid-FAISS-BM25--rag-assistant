[site]: crossvalidated
[post_id]: 602561
[parent_id]: 602552
[tags]: 
Random forests use bagging (bagging is just a contraction of "bootstrap aggregation," see Elements of Statistical Learning Section 8.7). Bagging draws a bootstrap sample of the data (randomly select a new sample with replacement from the existing data), and the results of these random samples are aggregated (because the trees' predictions are averaged). But bagging, and column subsampling can be applied more broadly than just random forest. (There's also a discussing in ESL of how random forest is well-positioned to benefit from bagging, while other learning methods are not.) The boosting implementations that I'm familiar (e.g. xgboost) will also support random subsampling of columns. But your guess is correct, in the sense that column subsampling is really incidental to the boosting procedure itself. Boosting is about estimator $T+1$ "correcting" the errors of the previous $T$ estimators. Likewise, bagging is also incidental to the boosting procedure, but bootstrap resampling of the data could be implemented as well.
