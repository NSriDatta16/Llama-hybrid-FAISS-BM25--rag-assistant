[site]: crossvalidated
[post_id]: 189147
[parent_id]: 189135
[tags]: 
Random forest are ensembles of decision trees . By leaving out parts pf the variables and data they tend to abstract better, and overfit less. But if you don't have labels, overfitting is not much of a concern. However, you cannot even build a single decision tree without labels. The reason is probably unfixable: to decide upon a split (i.e. to creat a node in the tree) you need labels to evaluate the quality of your options... So no, I don't you can use it on unlabeled data. You could use a clustering algorithm to creat labels for your data. But that will likely be a self-fulfilling prophecy. Clustering algorithms are sensitive to scale, so of course the features with the largest scale/variance have the largest importance.
