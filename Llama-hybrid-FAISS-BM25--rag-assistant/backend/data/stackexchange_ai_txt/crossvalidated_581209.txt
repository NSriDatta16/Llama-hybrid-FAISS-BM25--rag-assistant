[site]: crossvalidated
[post_id]: 581209
[parent_id]: 
[tags]: 
How is ReLU used in neural networks if it doesn't squish the weighted sum into the interval of (0, 1)?

So as far as I understand from watching 3Blue1Brown's video on neural networks, all neurons operate on numbers ranging from 0 to 1. Since a weighted sum goes larger than that, a sigmoid function is used to squish it back into that interval. How does a ReLU work then if it doesn't force numbers into that interval? Or is it used in networks operating in numbers that can be greater than 1?
