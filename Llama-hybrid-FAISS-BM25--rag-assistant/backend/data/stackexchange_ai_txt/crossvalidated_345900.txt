[site]: crossvalidated
[post_id]: 345900
[parent_id]: 
[tags]: 
logistic regression derivative

Our equation for negative log likelihood loss function for logistic regression with regularized maximum likelihood is: $L(\beta) = -\sum^{n}_{i=1} log P(y_i|x_i) + \lambda \vert \vert \beta \vert \vert^2$ And it's derivative is: $\Delta L(\beta) = \frac{\partial L(\beta)^T}{\partial \beta} = \sum^{n}_{i=1}(p_i - y_i)\phi(x_i)+2\lambda\textbf{I}\beta = X^T(p-y)+2\lambda\textbf{I}\beta$ I am trying to understand what does the identity matrix do there. So far I thought, if you multiply the identity matrix with a scalar that would give you the scalar "embedded" diagonally in the matrix. But isn't elementwise multiplication with a scalar doing the same thing as multiplication with such a matrix? What is the reason for including it in this equation then?
