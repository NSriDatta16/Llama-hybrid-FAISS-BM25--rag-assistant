[site]: crossvalidated
[post_id]: 172828
[parent_id]: 172782
[tags]: 
Question 1: Regarding the difference in p-values (or F-values ) for Model 2 in the first ANOVA compared to the second, there is no explanation apparent to me. They should indeed be the same, since we are comparing at that point Model 1 to Model 2 in both instances. It would be very useful to get further input about the reason behind, but in the absence of an explanation, it is probably advisable to only include two models in each ANOVA call. The values calculated "manually" (I presume the correct values) are consistent with the first ANOVA call in your post with an F = 8.2852 . Question 2: You are comparing 51.692 and 0.126 BUT this bulkier difference in the second case is a good thing: What you should be paying attention to is the difference in the second ANOVA between 246.68 for Model 2 and 194.99 for Model 3 , which is much more of a drop in RSS than what you observe in in the first ANOVA, a change from 246.68 to 246.56 , tantamount to nothing. So by including the third regressor in the second ANOVA, you shave off a ton of residual (misfitted) information that is now captured, whereas you hardly made a dent in the first set of models. Question 3: The question is how to use ANOVA to find the best model, but I think the answer is clear by now - low p-value makes it a good bet that you should include the regressor under consideration. Yet, what is not clear at all is the process by which you move along a series of ANOVA tests. In other words, I know you know because I've taken the Coursera course, that at first you are acquainted with $R^2$, which you can calculate manually, and understand the idea here (among many other places); after the idea of overfitting is introduced, Adjusted-$R^2$ values are explained, shortly followed by, yes, ANOVA (F-test). All these and others, such as the AIC criterion are tests or criteria to select models or include / exclude regressors. For instance, AIC and BIC used for non-nested models, and ANOVA and Likelihood Ratio Tests for nested models (I'm quoting a comment by @f coppens in this post . The challenge, though, is to come up with models that make sense to test ( check this post ). Say you have 5 independent variables, and you are doing linear regression, or OLS, you can include or exclude each variable in the any conceivable model for a total of $2^{10}=1,024$ models before considering interactions (remember that mtcars has 10 variables besides mpg ). This has its own problems with collinearity for instance. You can resort to your knowledge of cars, or just run with raw computer power through all combinations either with forward or backward stepwise selection as very nicely explained in this document , using R code along the lines of: fit.full to select the model with the highest adjusted $R^2$ (or you can also run ANOVA ) with the idea of later looking into possible interactions, and running residuals diagnostics.
