[site]: crossvalidated
[post_id]: 201363
[parent_id]: 
[tags]: 
Kernel PCA increases dimensionality compared with PCA?

I was trying to use sklearn to perform kernel PCA with 28*28 = 784 dims data. At first I used PCA to reduce dimensionality and I chose to reduce to k dimensions where k could explain 95% of the variance. PCA gave me k = 174. Later on I tried kernel PCA with polynomial kernel of degree 3 and similarly, using the explained-variance approach I got k = 1993. Since 1993 > 784, kernel PCA actually increased the dimensionality, which was against my intention. Also, I used 5000 data for training and Kernel PCA gives me 5000 eigenvectors, from which I selected k = 1993. Why did kernel PCA give me 5000 eigenvectors? Why Kernel PCA increased dimensionality compared to PCA?
