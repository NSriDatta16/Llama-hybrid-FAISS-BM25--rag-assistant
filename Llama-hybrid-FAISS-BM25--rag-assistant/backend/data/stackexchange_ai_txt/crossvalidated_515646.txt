[site]: crossvalidated
[post_id]: 515646
[parent_id]: 
[tags]: 
Dirichlet Process vs Gaussian Process?

I'm studying the Dirichlet Process (DP) and looking to the Gaussian Process (GP), which I've had more experience with, to help make connections. The GP can receive $X_{train}$ , $X_{test}$ , and $Y_{train}$ as inputs then estimate $Y_{test}$ by applying gaussian conditioning rules to the former: $P(Y_{test} | X_{train}, X_{test}, Y_{train})$ . Perhaps the most pivotal element is $X_{train}$ as it has two key relationships (in graphical model formation.): X_train / \ X_test Y_train The kernel function articulates the similarity (perhaps proximity is a better word) between $X_{train}$ and $X_{test}$ . And using this information, the high dimensional gaussian is able to infer what values $Y_{test}$ might take on, by leveraging the relationships described by the above graph. So, the closer any $X_{train_i}$ and $X_{test_j}$ , the more "pull" the specific $Y_{train_i}$ would have on the inferred $Y_{test_j}$ . The key idea is that the GP is non-parametric (or perhaps infinitely parametric) because the relationships cannot be compressed into a fixed number of descriptive parameters. Rather, every training observation is a parameter, defining the mean and covariance functions which $Y_{test}$ is sampled from. Now, in the context of the DP, my understanding is that the GP and DP are very similar. The difference would be the conditioning technique. Rather than gaussian conditioning rules to infer the relationships described above, Dirichlet conditioning rules (which I'm not familiar with/aware of) would be used to infer $Y_{test}$ . Am I on the right track or is the DP completely different from the GP? Edit1: I've used "vanilla" Dirichlet distribution in modeling before. I generally conceive of it as a high dimensional beta distribution, but instead of returning a scalar between [0,1] , returns a vector of such values, which might be interpreted as affiliation with discrete classes and/or dimensions. So I'm presently considering the Dirichlet Process as the stochastic process equivalent of multi-class logistic regression, where instead of linear decision boundaries being inferred, the model is much more flexible due to conditional inference very similar to the GP, as described above. Edit2: After reviewing this video, https://www.youtube.com/watch?v=UTW530-QVxo , it seems that the DP is way more focused on clustering than what I had understood. I'm still curious about the conditioning mechanism, however, it seems that a non-parametric alternative to multi-class logistic regression is the wrong way to interpret the DP.
