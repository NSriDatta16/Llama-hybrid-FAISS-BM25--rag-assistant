[site]: crossvalidated
[post_id]: 126538
[parent_id]: 126484
[tags]: 
I will stick to the case of a simple linear regression. Generalisation to multiple regression is straightforward in the principles albeit ugly in the algebra. Imagine we have some values of a predictor or explanatory variable, $x_i$, and we observe the values of the response variable at those points, $y_i$. If the true relationship is linear, and my model is correctly specified (for instance no omitted-variable bias from other predictors I have forgotten to include), then those $y_i$ were generated from: $$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$ Now $\epsilon_i$ is random error or disturbance term, which has, let's say, the $\mathcal{N}(0,\sigma^2)$ distribution. That assumption of normality, with the same variance ( homoscedasticity ) for each $\epsilon_i$, is important for all those lovely confidence intervals and significance tests to work. For the same reason I shall assume that $\epsilon_i$ and $\epsilon_j$ are not correlated so long as $i \neq j$ (we must permit, of course, the inevitable and harmless fact that $\epsilon_i$ is perfectly correlated with itself) - this is the assumption that disturbances are not autocorrelated . Note that all we get to observe are the $x_i$ and $y_i$, but that we can't directly see the $\epsilon_i$ and their $\sigma^2$ or (more interesting to us) the $\beta_0$ and $\beta_1$. We obtain ( OLS or "least squares") estimates of those regression parameters, $\hat{\beta_0}$ and $\hat{\beta_1}$, but we wouldn't expect them to match $\beta_0$ and $\beta_1$ exactly. Moreover, if I were to go away and repeat my sampling process, then even if I use the same $x_i$'s as the first sample, I won't obtain the same $y_i$'s - and therefore my estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ will be different to before. This is because in each new realisation, I get different values of the error $\epsilon_i$ contributing towards my $y_i$ values. The fact that my regression estimators come out differently each time I resample, tells me that they follow a sampling distribution . If you know a little statistical theory, then that may not come as a surprise to you - even outside the context of regression, estimators have probability distributions because they are random variables, which is in turn because they are functions of sample data that is itself random. With the assumptions listed above, it turns out that: $$\hat{\beta_0} \sim \mathcal{N}\left(\beta_0,\, \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum(X_i - \bar{X})^2} \right) \right) $$ $$\hat{\beta_1} \sim \mathcal{N}\left(\beta_1, \, \frac{\sigma^2}{\sum(X_i - \bar{X})^2} \right) $$ It's nice to know that $\mathbb{E}(\hat{\beta_i}) = \beta_i$, so that "on average" my estimates will match the true regression coefficients (actually this fact doesn't need all the assumptions I made before - for instance it doesn't matter if the error term is not normally distributed, or if they're heteroscedastic, but correct model specification with no autocorrelation of errors is important). If I were to take many samples, the average of the estimates I obtain would converge towards the true parameters. You may find this less reassuring once you remember that we only get to see one sample! But the unbiasedness of our estimators is a good thing. Also interesting is the variance. In essence this is a measure of how badly wrong our estimators are likely to be. For example, it'd be very helpful if we could construct a $z$ interval that lets us say that the estimate for the slope parameter, $\hat{\beta_1}$, we would obtain from a sample is 95% likely to lie within approximately $\pm 1.96 \sqrt{\frac{\sigma^2}{\sum(X_i - \bar{X})^2}}$ of the true (but unknown) value of the slope, $\beta_1$. Sadly this is not as useful as we would like because, crucially, we do not know $\sigma^2$. It's a parameter for the variance of the whole population of random errors, and we only observed a finite sample. If instead of $\sigma$ we use the estimate $s$ we calculated from our sample (confusingly, this is often known as the "standard error of the regression" or "residual standard error") we can find the standard error for our estimates of the regression coefficients. For $\hat{\beta_1}$ this would be $\sqrt{\frac{s^2}{\sum(X_i - \bar{X})^2}}$. Now, because we have had to estimate the variance of a normally distributed variable, we will have to use Student's $t$ rather than $z$ to form confidence intervals - we use the residual degrees of freedom from the regression, which in simple linear regression is $n-2$ and for multiple regression we subtract one more degree of freedom for each additional slope estimated. But for reasonably large $n$, and hence larger degrees of freedom, there isn't much difference between $t$ and $z$. Rules of thumb like "there's a 95% chance that the observed value will lie within two standard errors of the correct value" or "an observed slope estimate that is four standard errors away from zero will clearly be highly statistically significant" will work just fine. I find a good way of understanding error is to think about the circumstances in which I'd expect my regression estimates to be more (good!) or less (bad!) likely to lie close to the true values. Suppose that my data were "noisier", which happens if the variance of the error terms, $\sigma^2$, were high. (I can't see that directly, but in my regression output I'd likely notice that the standard error of the regression was high.) Then most of the variation I can in $y$ see will be due to the random error. This will mask the " signal " of the relationship between $y$ and $x$, which will now explain a relatively small fraction of variation, and makes the shape of that relationship harder to ascertain. Note that this does not mean I will underestimate the slope - as I said before, the slope estimator will be unbiased, and since it is normally distributed, I'm just as likely to underestimate as I am to overestimate. But since it is harder to pick the relationship out from the background noise, I am more likely than before to make big underestimates or big overestimates. My standard error has increased, and my estimated regression coefficients are less reliable. Intuition matches algebra - note how $s^2$ appears in the numerator of my standard error for $\hat{\beta_1}$, so if it's higher, the distribution of $\hat{\beta_1}$ is more spread out. This means more probability in the tails (just where I don't want it - this corresponds to estimates far from the true value) and less probability around the peak (so less chance of the slope estimate being near the true slope). Here is are the probability density curves of $\hat{\beta_1}$ with high and low standard error: It's instructive to rewrite the standard error of $\hat{\beta_1}$ using the mean square deviation, $$\text{MSD}(x) = \frac{1}{n} \sum(x_i - \bar{x})^2$$ This is a measure of how spread out the range of observed $x$ values was. With this in mind, the standard error of $\hat{\beta_1}$ becomes: $$\text{se}(\hat{\beta_1}) = \sqrt{\frac{s^2}{n \text{MSD}(x)}}$$ The fact that $n$ and $\text{MSD}(x)$ are in the denominator reaffirms two other intuitive facts about our uncertainty. We can reduce uncertainty by increasing sample size, while keeping constant the range of $x$ values we sample over. As ever, this comes at a cost - that square root means that to halve our uncertainty, we would have to quadruple our sample size (a situation familiar from many applications outside regression, such as picking a sample size for political polls ). But it's also easier to pick out the trend of $y$ against $x$, if we spread our observations out across a wider range of $x$ values and hence increase the MSD. Again, by quadrupling the spread of $x$ values, we can halve our uncertainty in the slope parameters. When you chose your sample size, took steps to reduce random error (e.g. from measurement error) and perhaps decided on the range of predictor values you would sample across, you were hoping to reduce the uncertainty in your regression estimates. In that respect, the standard errors tell you just how successful you have been. I append code for the plot: x
