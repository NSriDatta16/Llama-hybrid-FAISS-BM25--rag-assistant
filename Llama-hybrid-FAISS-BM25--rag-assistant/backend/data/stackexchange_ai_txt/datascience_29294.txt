[site]: datascience
[post_id]: 29294
[parent_id]: 29293
[tags]: 
RMSE does not work that way. A RMSE of 13 might actually be great, it completely depends on how your target variable is scaled. For example, if your target variable was in the range [0,1e9] , then a RMSE of 13 is spectacular. On the other hand, if your target is in the range [0,1] , a RMSE of 0.5 is terrible. If you would like to try a metric that can be more readily interpretable as having a "good" or "bad" score, try the Mean Average Percent Error (MAPE). As far as why you get a lower MSE when you cross validate: you do not show us how you constructed your training and test sets, but my guess is that you basically just got unlucky and ended up with a training/test split that performed poorly on your holdout set. Your CV-MSE is clearly better than your single holdout MSE, but you should also check the spread of CV scores as well. In any event, for a dataset as small as yours I would recommend using bootstrap cross validation instead of k-fold.
