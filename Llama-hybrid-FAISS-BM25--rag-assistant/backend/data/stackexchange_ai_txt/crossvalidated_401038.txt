[site]: crossvalidated
[post_id]: 401038
[parent_id]: 400986
[tags]: 
For an undergraduate thesis, my opinion is that your methodology is sufficient as is, although your table is confusing; more discussion later. Without meaning offense, if you want to use a statistical concept, you should be sure you can explain it in plain English, and that you can interpret any associated coefficients. Linear vs count models When you fit a linear regression model (aka ordinary least squares, which is the one you almost certainly learned in intro statistics), you're fitting a model that explains the mean of $y$ as a function of covariates. For example, say $y$ is time spent on legislative activity. $y = \beta_0 + \beta_1x + \epsilon$ $x$ could be a binary dummy variable for being in a safe seat, or it could be the raw margin of victory in percent. Or we can make dummies for being in a safe seat and in a marginal seat (i.e. the average seat is the implied base category). This sort of model is acceptable for most continuous outcomes. Some types of outcomes are better modeled with other models. You don't need for $y$ to be normally distributed. You do want $\epsilon$ , the residual error, to be normally distributed if possible. Some types of outcomes are inherently better modeled using a different type of regression. Counts of things, e.g. number of fish caught or number of days in a hospital, are typically modeled using Poisson or negative binomial models (although a linear model may still not be too far wrong!). Now, in a Poisson model, you're in a very similar set up to OLS: $E(y|x) = exp(\beta_0 + \beta_1x)$ Counts are typically discrete items, e.g. 0, 1, 2, 3, ... As far as I recall, your $y$ doesn't absolutely have to be discrete, but you'd ideally want some explanation about why you chose a Poisson model if they aren't (e.g. perhaps you can show that the mean squared error is lower with a Poisson model, or perhaps because that model is widely used in similar political science applications). The Poisson model assumes that the variance of $y$ is equal to the mean. (Strictly speaking, I believe generalized Poisson models assumes variance is proportional to the mean.) Negative binomial models specifically model the mean, so if your variance isn't proportional to the mean, you'd be justified in using one. Generally, you can show the negative binomial model's dispersion parameter and test if it's equal to zero. If none of the above is intelligible, I'd discourage you from fitting a count model. Hierarchical models If you had repeated observations of the same MPs over two or more time periods, this is one situation when you'd use a hierarchical model. You're essentially imagining that each person has their own mean amount of hours (controlling for whatever else is in your model), i.e. $y_it = \beta_0 + \beta_1x_it + \u_i + \epsilon_it$ Above, $i$ indexes people, and $t$ indexes time (or it could be a different sort of clustering, e.g. maybe a political subdivision, on the assumption that MPs from the same subdivision might spend similar amounts of time legislating). If you think you are in this scenario, you should do some background reading on hierarchical or random effects models. You should learn to recognize when you're in this sort of situation, and you should learn what the variance of the random intercept ( $\u_i$ ) and the residual variance ( $\epsilon_it$ ) mean. I didn't see the context of the comment asking you about random effects. Zero inflated and hurdle models Maybe you have a lot of people with zero hours spent legislating. Maybe there are two separate processes at play: one causes MPs to decide if they should propose legislation or not, and a separate process determines the number of hours they spend legislating. That's the set up for the hurdle model you alluded to, as well as zero-inflated models. At this point, I confess I'm less familiar with hurdle models, but I believe they have a probability model for if you clear the hurdle or not (e.g. zero hours in this example), and a separate model for the mean of the outcome conditional on clearing the hurdle. Even if you don't use Stata, their examples are very clearly written, so you might benefit from reading their description of hurdle models. In contrast, a zero-inflated model assumes that one class (modeled by a probability model) always has $y = 0$ , and that another class has $E(y)$ distributed Poisson or negative binomial (NB: that class can have a zero outcome as well; the distinction is between a structural zero class and a 'regular' class). If you want to use one of these models, you'd want to do background reading to figure out scenarios where they conceptually work, you'd want to be able to explain the (now two sets of) coefficients in plain language, and you'd want to know the standard test (it's a likelihood ratio test) to compare a zero-inflated count model to a regular count model (and I'd assume there's a similar test for hurdle models). Again, if you couldn't explain the above in plain language if asked, I'd probably recommend not using this class of model. Bayesian vs Frequentist Most of us are trained in frequentist statistics. Bayesian statistics does offer some conceptual advantages, but you will need to do some work to comprehend the basics. Google this one on your own, but for undergraduates, I'd generally suggest doing frequentist statistics. With respect, your table is confusing For each cell, you present the mean and SD of (estimated) hours spent on legislative activity. That's fine. However, what is your model? Did you fit one Poisson model to each cell? If so, why not fit a Poisson model with a dummy variable for type of seat to the whole data? What is mean difference? Is the t-statistic for each cell relative to a count of zero? What is "significance" - if it is 1 - p-value, why didn't you just present the p-value? With respect, you mentioned some relatively advanced statistical models that I would expect someone with an MS in statistics or a PhD in an applied stats field or economics to be able to understand. At the same time, it's not clear that you really have the basics down. I would recommend you get the basics down before proceeding to something more advanced. As mentioned in comments, I think you'd be entirely justified fitting an OLS model, and discussing some of the models you had referenced in your post (provided you can at least provide a general sense of why they're improvements on OLS).
