[site]: crossvalidated
[post_id]: 604457
[parent_id]: 604351
[tags]: 
Simulating from the same model that you use to analyze the data is useful for understanding the model and its limitations. To some extent it's a sanity check that the model does not have some serious issues, because a model that does not work very well when its assumptions are met is problematic. You may say:"But how could this ever be the case?" And the answer includes, but is not limited to: You have messed up and something you assumed/derived/implemented is just not right. If you simulate a huge amount of data, you should hope to get something close to the parameter values you simulated with back. If you don't there's a hint the first bullet might apply. Parameter identifiability: I.e. the data you plan to collect/the experiment you do does not inform you at all - or at least not very well - about the parameter you are interested in. Example: randomized experiment with a placebo group and a single dose of a drug (let's say 50 mg), the outcome is wonderfully normally distributed and you fit a non-linear sigmoid Emax model assuming that $Y_i \sim \beta_0 + \beta_1 \text{dose}^h / (\text{dose}^h + \text{ED50}^h) + \epsilon_i$ with i.i.d. $\epsilon_i \sim N(0, \sigma^2)$ . All assumption for your model are met, it is even the true data generating model, but with just one active dose the parameters are not identified. This may sound like a very specific situation (and it is), it's just pointing out that you may occasionally be in that kind of situation (and not realize it, e.g. you specify multiple levels of variability that cannot be distinguished). Poor small sample performance (where "small sample" could occasionally be pretty large) e.g. due to using asymptotics. Example: Logistic regression fit using maximum likelihood with asymptotic standard errors (vs. exact logistic regression or similar) in case of 0 (or 1) events out of 100 (group 1) vs. 5 events out of 100 (group 2). You might not realize this is an issue, until you evaluate systematically what your model/its implementation does in these situations. Some particular issues keep occurring that cause issues in defining what the maximum likelihood estimate would be (like complete separation in logistic regression). Numeric optimization, convergence or sampling problems (that might be particularly common in some data situations). Other numerical problems (e.g. covariates that are on very different scales cause issues, finite difference approximation makes a MCMC sampler not work when proper auto-differentiation would have worked etc.)
