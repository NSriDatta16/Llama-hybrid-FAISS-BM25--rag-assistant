[site]: crossvalidated
[post_id]: 325406
[parent_id]: 325399
[tags]: 
I've started down my own path towards understanding the bayesian way of thinking and I'll share my perspective. I started off reading classic papers on the various samplers and going through derivations for the conjugate cases, and I don't think that got me very far. True, the elites are going to write their own samplers and exploit every conjugate opportunity possible. But if you want to get a good feel for the approach and potentially gain a few useful methods, there are more direct ways. My recommendation is to find a good bayesian modeling tool that takes care of the sampling and lets you focus on specifying the likelihoods and priors . For me, this has been Stan . It's based on a particular sampler that doesn't require much tinkering. The User's Guide and Reference Manual (available on the Documentation page) reads like a textbook, and you can learn a lot by going through the examples. When you have an idea for a new model, you can try it out and usually get something working without too much time. You can see some of my own experimentation here . We are in an age where the focus is on managing computations on enormous data sets, and software like Stan is going to encourage you to perform intense computation on even small data sets (depending on the model). But I think it's worth the time to study and understand. There are still plenty of "small data" problems out there, and it's nice to be able to frame ideas in machine learning (e.g., L2 regularization) in the bayesian context (where there is actually theory!).
