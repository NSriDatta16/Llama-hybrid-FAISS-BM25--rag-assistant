[site]: crossvalidated
[post_id]: 33505
[parent_id]: 
[tags]: 
Distribution of a centered standardized sample

Foreword : this is not homework, but a real problem : in a Bayesian model comparison context, I am trying to work out the correct prior density of mixed-model parameters which are, for computational reasons, computed as the centered and standardized means of other "raw" parameters, which are themselves given prior distributions. See Gelman A, Hill J. Data Analysis Using Regression and Multilevel/Hierarchical Models . 1st ed. Cambridge University Press; 2006. This textbook uses this trick (or variants thereof) quite often, and various simulations have convinced me of its efficacy, at least in selected cases. I currently do not have on hand any reference material (textbook, etc...) for this question, whose answer is quite probably a standard result. Could someone lend me a hand (if only by pointing me to a Web-accessible reference, which my Google attempts have been unsuccessful at referring me to) ? Let $X=X_1,\ldots X_N$ be a sample (i. e. vector of $N$ independent random variables sampled from the same) normal distribution ${\textrm N}(\mu_x, \sigma^2_x)$. Let $m={{\sum_{i=1}^n X_i} \over N}$ and $s^2={{\sum (X_i)^2} \over {N-1}}$ the unbiased estimators of the mean and variance of this distribution. What is the (joint) distribution of the components $Y_i$ of $Y={{X-m} \over s}$ ? I am sorely tempted to answer that they are $N$ independent normal standard random variables (i. e. distributed as ${\textrm N}(0, 1)$), and my intuition is seconded by various simulation results, but I have trouble proving it. What I know (or prove to my satisfaction) can be summarized as follows : $m$ has a ${\textrm N}(\mu_x, {{\sigma^2_x} \over N})$ distribution ; therefore, the common mean of the $Y_i$ is 0 (trivial from the definition of the mean). The common variance of the $Y_i$ is 1 (a trifle less trivial but can be worked out from the definition). ${s^2} \over {\sigma^2_x}$ has a $\chi^2_{N-1}$ distribution (classical result that I remember having worked out). Any textbook will state that $m \over s$ has a $t_{N-1}(0, 1)$ distribution ( Not trivial, but I remember having worked it out, the crucial point being the independence of $m$ and $s$. I think that this historically is the origin of the distinction of the $t$ distribution, BTW...). Obviously, $Y_i={{X_i} \over s}-{m \over s}$. However, neither $m$ nor $s$ are independent of the $X_i$. Therefore the distribution of ${X_i} \over s$ is unknown (to me), and the distribution of the sum is not the convolution of the distribution of its terms. Where am I goofing ?
