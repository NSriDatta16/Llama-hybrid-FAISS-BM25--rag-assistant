[site]: crossvalidated
[post_id]: 181104
[parent_id]: 181076
[tags]: 
It depends. You can always speed up cross validation using a coarser grid or reducing the number of fields. But they will harm performance. What are you cross-validating ? Sometimes, dropping generality of your code can bring you huge speed ups. Call $n$ the number of folds you are using and $l$ the size of the parameter grid. If you are working with KNN you can store the list of 100 closest neighbours and cross validate $k$ (for $k\in[1,100]$ ), the number of neighbours to select from this first list. Now you just have to train $n$ different "pre-KNN" and all the other models are evaluated instantly. The naive method would have used $nl$ trainings. An impressive work has been done regarding the selection of $C$ with SVMs : SVM path , but I don't see many people using (or implementing) it. Likewise, LASSO, ridge and elastic net regressions have algorithms calibrating the entire path without changing the complexity. If you are working with multiple trees based methods, like random forests or gradient boosting machines and are selecting the number of trees, you can save the trees and add a bunch of them to each model at every step (sklearn enables to extract the trees from a random forest as a list you can then slice) Edit After reading various questions about speeding up cross validation I ended up writing this blog article detailing existing (specialized or general) methods to achieve speed ups.
