[site]: crossvalidated
[post_id]: 85756
[parent_id]: 82954
[tags]: 
My first questions/thoughts are: I use a different training method. I like the Eric Wan diagrammatic method[1]. It allows me to compute a "gradient" for my network using an adjoint network. I can detect errors in one using the other and basic test cases. Coupled debugging. Have you compared your results to the built-ins? Did you make a technical error in the implementation of the code that you can detect there? I don't have resources right now to test this. I'm sure someone has made a NN that tests the Fischer Iris data using a NN. Have you looked in peer-reviewed published literature? The reason I ask is I was once tasked with something that sounded clever in Q-learning, but when I reviewed the literature, it was shown to be impossible. That went over very well with my professor. Never be afraid to stand on the shoulders of giants. Make a linear (planar) fit first, and then train the NN on the variation from the plane. This keeps your values closer to the origin, and makes your learning rates higher. It also makes the Neural Network do the heavy lifting of fitting the nonlinear part instead of wasting the training/learning on the planar part. A pseudo-inverse is computationally cheap, comparatively speaking. I didn't see any scaling or centering on the inputs. If you have input values in the millions and initial weights in the ones, then you are going to spend all your training iterations increasing the scale for the inputs. Subtracting the mean, and then dividing by something like the standard variation can help. [1] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.5262
