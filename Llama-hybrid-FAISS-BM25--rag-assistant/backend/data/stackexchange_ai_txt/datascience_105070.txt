[site]: datascience
[post_id]: 105070
[parent_id]: 
[tags]: 
Modeling events with an intermediate stage

For a lot of prediction problems, there's an intermediate stage which must occur for the target event to occur. For example, to graduate from college, one must first be accepted. For an internet ad to sell a product, someone must first click the ad. For a juvenile to reproduce it must mature to adulthood. Etc. More generally, we want to predict an event at t2 from data at t0, where the intermediate event happens at t1. Whether the interim event occurs is not known at t0, but our data contains records of the t1 events. Given that we have data on t1 and t2, it may be helpful to model the occurrence of the intermediate stage (t1) and incorporate it into the model for the target event (t2). One way to think about problems like these is p(t2 = true) = p(t1=true) * p(t2=true | t1=true) thought that's not the only way a model might handle such events. Anyway, I want to build a neural network to handle a problem that has such a structure from data that contains the t1 information. I curious to know whether there is research on NN models that uses an implicit or explicit model of "intermediate" events in their architecture. Alternatively, it might be helpful for me to know of some data sets that have such a structure so I can look for attempts to handle them. One approach that occurs to me is that you could try to train a model on two target variables (the t1 event and t2 event) backproping both losses (suitably weighted) and then just toss out the t1 even business at inference time. But I don't know what to look for regarding such models. (Note that for my case, the NN is pretty much a deployment requirement, so well developed research on non-NN stuff might be helpful but it should have some fairly clear mapping into a NN model.)
