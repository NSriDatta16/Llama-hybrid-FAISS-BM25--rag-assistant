[site]: crossvalidated
[post_id]: 475655
[parent_id]: 
[tags]: 
In training a triplet network, I first have a solid drop in loss, but eventually the loss slowly but consistently increases. What could cause this?

I haven't even finished 1 epoch, so I don't think it could any sort of overfitting. I am training on a very large amount of data (27 gb of text) so it'll still be a while before I even reach one epoch. The loss now has been increasing for twice as long as the loss had been decreasing, although the loss is still overall smaller since the increase is at a smaller rate. If it helps, my architecture is Bert, with 2 extra layers fully connected layers after Bert. I am using triplet loss via softmax/cross entropy.
