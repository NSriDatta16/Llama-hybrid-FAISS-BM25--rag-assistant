[site]: datascience
[post_id]: 106617
[parent_id]: 106613
[tags]: 
The page linked already gives a really good explanation: To see why this is an issue, consider the simplest method of over-sampling (namely, copying the data point). Let's say every data point from the minority class is copied 6 times before making the splits. If we did a 3-fold validation, each fold has (on average) 2 copies of each point! If our classifier overfits by memorizing its training set, it should be able to get a perfect score on the validation set! Our cross-validation will choose the model that overfits the most. We see that CV chose the deepest trees it could! The main point is that you need to make sure that the data is first split into train/test by using the cross-validation before applying any upsampling methods. This can be achieved by using a pipeline since it makes sure that the upsampling and model fitting are combined and are applied only after the data is split for cross-validation. It would still be possible to do this without a pipeline by splitting the two steps and applying them manually but you'd have to do the cross-validation splitting yourself by using a using indices from sklearn.model_selection.KFold (see the example provided) since cross_val_score and GridSearchCV have no option for this and only allow a single estimator (which itself can also be a pipeline of multiple steps).
