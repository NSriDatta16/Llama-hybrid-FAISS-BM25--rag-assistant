[site]: datascience
[post_id]: 126918
[parent_id]: 
[tags]: 
reverse summary matching

How can we develop a model to assess the relationship between a summary and a text, enabling it to determine their coherence but in reverse? Also, how to make an inference We have list of texts (2000) and their matching summaries (7000) one to many, can be couple of summaries that can describe part of the text (( Unrelated; our hidden goal [except from learning] it to check this approach for Java to Smali matching )) current_model = 'google/flan-t5-large' max_length = 512 # maximum sequence length num_epochs = 3 learning_rate = 0.01 batch_size = 4 class CustomDataset(Dataset): def __init__(self, df, tokenizer, max_length): self.df = df self.tokenizer = tokenizer self.max_length = max_length def __len__(self): return len(self.df) def __getitem__(self, idx): row = self.df.iloc[idx] text = row['text'] summary = row['summary'] # Select a random text that does not match the summary non_matching_row = self.df[self.df['text'] != text].sample(n=1) non_matching_dialogue = non_matching_row['text'].iloc[0] inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length) matching_labels = self.tokenizer(summary, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length) # Tokenize non-matching text non_matching_inputs = self.tokenizer(non_matching_dialogue, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length) return inputs, matching_labels, non_matching_inputs # Load pre-trained tokenizer tokenizer = T5Tokenizer.from_pretrained(current_model) # Create training dataset and dataloader train_dataset = CustomDataset(train_df, tokenizer, max_length) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Load pre-trained model model = T5ForConditionalGeneration.from_pretrained(current_model) # Define optimizer optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Training loop for epoch in range(num_epochs): for batch_idx, batch in enumerate(train_loader): inputs = batch[0] matching_labels = batch[1] non_matching_inputs = batch[2] optimizer.zero_grad() # Train on matching text matching_outputs = model(input_ids=inputs['input_ids'][0], attention_mask=inputs['attention_mask'][0], labels=matching_labels['input_ids'][0]) matching_loss = matching_outputs.loss # Train on non-matching text non_matching_outputs = model(input_ids=non_matching_inputs['input_ids'][0], attention_mask=non_matching_inputs['attention_mask'][0], labels=non_matching_inputs['input_ids'][0]) non_matching_loss = non_matching_outputs.loss if non_matching_outputs is not None else torch.tensor(0.0) total_loss = matching_loss + non_matching_loss total_loss.backward() optimizer.step() if batch_idx % 100 == 0: print(f'Epoch {epoch}, Batch {batch_idx}, Matching Loss: {matching_loss.item()}, Non-matching Loss: {non_matching_loss.item()}') # Save the trained model model.save_pretrained('flan-t5-large-trained') # Evaluation on the testing set test_dataset = CustomDataset(test_df, tokenizer, max_length) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) correct = 0 total = 0 # Iterate over the testing set for inputs, matching_labels, non_matching_inputs in test_loader: matching_outputs = model.generate(input_ids=inputs['input_ids'][0], attention_mask=inputs['attention_mask'][0], max_length=512, num_beams=1, early_stopping=True) matching_generated_summary = tokenizer.decode(matching_outputs[0], skip_special_tokens=True) # Check if the generated summary matches the original summary is_matching = matching_generated_summary == tokenizer.decode(matching_labels['input_ids'][0], skip_special_tokens=True) # Update the counts total += 1 correct += is_matching # Calculate accuracy accuracy = correct / total * 100 print(f'Testing Accuracy: {accuracy}%') ```
