[site]: crossvalidated
[post_id]: 332182
[parent_id]: 
[tags]: 
is a good idea to sample a big dataset and run pca on it?

Suppose that we have a dataset with many rows and that we can't afford to run PCA many times due to constraints. In this regard, I have two questions: Is it a good idea to take a sample from the data, run PCA on it to find a good number of components then use this number on the whole dataset? Is it ok to take the subspace built using this sample dataset and transform the original data using it?
