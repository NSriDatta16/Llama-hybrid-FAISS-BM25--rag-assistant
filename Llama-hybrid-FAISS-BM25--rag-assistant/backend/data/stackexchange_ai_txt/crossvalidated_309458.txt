[site]: crossvalidated
[post_id]: 309458
[parent_id]: 116304
[tags]: 
The main issue here is that in real problems you don't have f(x) or y_test because it is what you try to predict. The best way to deal with Bias-Variance trade-off is to analyse the model complexity. For example, in Random Forest you can increase the n_estimators or max_depth increasing the variance and reducing the bias. As you can see in the image, after max_depth=15 you don't have an improvement in the score. Give a higher value to max_depth will increase the complexity and the variance without a better score, hence 15 could be a good "Trade-Off". In the other hand, you can split your data set in Train and Test datasets, in this case, you still don't have the f(x) but at least you will have y_test (real values)
