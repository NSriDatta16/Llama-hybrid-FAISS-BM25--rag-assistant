[site]: crossvalidated
[post_id]: 577126
[parent_id]: 
[tags]: 
Classical VAE not learning 2D gaussian mixture distribution using MSE loss

I've been exploring VAE for non-image data. I consider small to medium-sized continuous vector spaces and I want to learn the distribution of a dataset in that space. As a warm up exercise, I tried the dummy case of learning a 2d Gaussian Mixture distribution, na√Øvely thinking this would be a piece of cake for the VAE. Summary : I need to use a tfp.layers.MixtureNormal as decoder output because the classical VAE only generates centroids (and edges linking them). Is it expected? Or is it due to a bad network architecture and/or layer activation? (i've been using Dense layers, mixing tanh and linear, and MSE reconstruction loss) Details: I posting because I am puzzled by the results (obviously). First, I try a relatively shallow vanilla VAE with MSE reconstruction error, I see three issues the distribution latent space is very much structured like the input distribution, if shomehow crammed into the target N(0,1). Adding depth or width doesn't seem to help. The generation process is focussed on the Gaussians centroids, and their widths is essentially not represented. It seems it's not even trying to get the covariance of the normal. Adding depth or width doesn't seem to help, it just reduces the accumulations in virtual modes (not in input) The "gaps" in the latent space cause significant generated data to essentially create edges between the mode centroids. Some figures below might make my points clearer. I tried to use a multi-modal, probabilistic decoder output ( tfp.layers.MixtureNormal ) and get much better results, but not consistently. Overall, it helps if I pack enough components that at least one ends up on each mode. But I can easily end up with some of my input component being covered by a single ouput component (which is not good). On the plus side, the latent space is very smooth. This is very good for generation, but I fear that it means my reconstruction is a bit too random between modes. There are many configurations in which things get a bit hairy, but it seems the probabilistic decoder is helping a ton ( at the cost of needing more parameters), while the classical decoder just cannot get the job done. Is it expected? Am I missing something obvious? I know VAE are supposed to be blurrier but here it seems it's more like it collapses. Any comment? I add the code below note (edit): I now tried many other things but very few can fit the distribution at all, and even fewer without encoder collapse (regularization must be tuned down). Code : versions : tensorflow==2.9.1 tensorflow-probability==0.16.0 imports : import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import torch import tensorflow as tf tfk = tf.keras tfkl = tfk.layers import tensorflow_probability as tfp tfpl = tfp.layers tfd = tfp.distributions Model : class EmbeddingGenerator(): def __init__(self, dimension, probabilistic_multi_modal_output, num_components): self.probabilistic_multi_modal_output = probabilistic_multi_modal_output # layers size self.input_dim = dimension self.hidden_dim = dimension*num_components self.latent_dim = dimension self.output_dim = dimension # number of components to fit self.num_components = num_components # build encoder self.prior = tfd.Independent(tfd.Normal(loc=tf.zeros(self.latent_dim), scale=1), reinterpreted_batch_ndims=1) self.encoder = tfk.Sequential([ tfkl.InputLayer(input_shape=self.input_dim), tfkl.Dense(self.hidden_dim, activation='linear'), tfkl.Dense(self.hidden_dim, activation='tanh'), tfkl.Dense(tfpl.IndependentNormal.params_size((self.latent_dim,)), activation='linear', use_bias=True), tfpl.IndependentNormal(self.latent_dim, activity_regularizer=tfpl.KLDivergenceRegularizer(self.prior)) ]) # build decoder self.decoder = tfk.Sequential([ tfkl.InputLayer(input_shape=[self.latent_dim]), tfkl.Dense(self.hidden_dim, activation='linear'), tfkl.Dense(self.hidden_dim, activation='tanh'), ]) # build last layer and loss depending on whether decoder has a deterministic # or probabilistic output if probabilistic_multi_modal_output: output_layer = tfp.layers.MixtureNormal(self.num_components, event_shape=(self.output_dim,)) param_size = tfpl.MixtureNormal.params_size(self.num_components, (self.output_dim,)) self.decoder.add(tfkl.Dense(param_size, activation='linear')) self.decoder.add(output_layer) self.loss = lambda x, x_hat : -x_hat.log_prob(x) else: self.decoder.add(tfkl.Dense(self.output_dim, activation='linear')) self.loss = tf.keras.losses.MeanSquaredError() # Build full network self.net = tfk.Model( inputs=self.encoder.inputs, outputs=self.decoder(self.encoder.outputs[0]) ) self.optimizer = tf.optimizers.Adam(learning_rate=1e-3) self.net.compile(optimizer=self.optimizer, loss=self.loss) self.encoder.summary() self.decoder.summary() def sample(self, n): z = self.prior.sample(n) if self.probabilistic_multi_modal_output: out=self.decoder(z).sample() else: out=self.decoder(z) return out Data generation : n = 100000 ncom_real = 4 dimension = 2 train_dist = tfd.MixtureSameFamily( mixture_distribution=tfd.Categorical(probs=[1]*(ncom_real)), #np.random.rand(ncom_real) components_distribution=tfd.MultivariateNormalDiag( loc=[[1,3],[5,-7],[-3,1],[-7,-5]], # [tf.zeros(dimension)-np.random.randint(low=-10, high=10, size=dimension) for _ in range(ncom_real)] scale_identity_multiplier=[0.5]*(ncom_real))) train_tensor = train_dist.sample(n) Training : model_deter = EmbeddingGenerator(dimension=dimension, probabilistic_multi_modal_output=False, num_components=4*ncom_real) model_deter.net.fit(train_tensor, train_tensor, epochs=10) model_proba = EmbeddingGenerator(dimension=dimension, probabilistic_multi_modal_output=True, num_components=4*ncom_real) model_proba.net.fit(train_tensor, train_tensor, epochs=10) Sampling : train_tensor = train_tensor proba_synth_tensor = model_proba.sample(n) deter_synth_tensor = model_deter.sample(n) proba_encoded_tensor = model_proba.encoder(train_tensor).sample() deter_encoded_tensor = model_deter.encoder(train_tensor).sample() Viz : df = pd.concat([ pd.DataFrame(proba_encoded_tensor.numpy()).assign(role='proba'), pd.DataFrame(deter_encoded_tensor.numpy()).assign(role='deter') ]).reset_index(drop=True) sns.displot(data=df, x=0, y=1, hue='role', kind='hist', rug=True, height=10, bins=1000) plt.show() df = pd.concat([ pd.DataFrame(train_tensor.numpy()).assign(role='train'), pd.DataFrame(proba_synth_tensor.numpy()).assign(role='synthetic_proba'), pd.DataFrame(deter_synth_tensor.numpy()).assign(role='synthetic_deterministic') ]).reset_index(drop=True) sns.displot(data=df, x=0, y=1, hue='role', kind='hist', rug=True, height=10, bins=1000) plt.show() Result with 1 linear + 1 tanh hidden layers Result with 6 hidden layers (lin-tanh-tanh-lin-tanh-tanh), hidden width = dimension*4*n_components Corresponding embedding space :
