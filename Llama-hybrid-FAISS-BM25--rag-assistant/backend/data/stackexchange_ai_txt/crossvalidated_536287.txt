[site]: crossvalidated
[post_id]: 536287
[parent_id]: 
[tags]: 
Disagreement between ICC and paired t-test

I am trying to determine whether the values obtained by an algorithm calculation are systematically different from the the criterion values. The criterion values are obtained via manual observation with 3 different judges. The values obtained are as follows: judge1_count = [1456 1430 1471 3024 3802 4334 3812 4140 1089 1860 2201 1107 1134]; judge2_count = [1458 1427 1473 3023 3835 4350 3791 4129 1090 1867 2212 1115 1147]; judge3_count = [1452 1441 1473 3030 3845 4360 3820 4160 1074 1863 2216 1119 1139]; Each column represents a different independent sample from which the 3 judges give a count I first test for inter-rater reliability by calculating ICC(2,1) (two-way random effect, single model), giving me an ICC value of about 0.99, which makes sense just by observing how close the counts are for each sample. Next I introduce the algorithm counts: algo_count = [1434 1422 1453 2973 3688 4061 3703 4072 1018 1837 2180 1090 1116] And once again apply ICC(2,1), giving me an ICC of again around 0.99. However, when I apply a paired t-test of the algorithm count and the criterion count (where the criterion count is averaged across the 3 judges per sample), the null hypothesis is rejected (p = 0.0078), and by observation it can be seen that the algorithm counts are systematically smaller than the criterion counts, so this does make some sense. My question then is that are these results logical, or have I missed something in regard to how I apply the ICCs? This is my first time using them so I'm hesitant about whether I've used this function properly. For reference I repeated the task in both MATLAB and R and got out the same ICC values.
