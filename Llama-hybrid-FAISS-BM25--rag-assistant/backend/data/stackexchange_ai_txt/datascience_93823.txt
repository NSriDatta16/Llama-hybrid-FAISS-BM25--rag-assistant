[site]: datascience
[post_id]: 93823
[parent_id]: 
[tags]: 
What's the difference between multiclass categorical crossentropy, mlogloss and multi:softprob?

As far as I understand, an objective is something I'm trying to optimize and an evaluation statistic is something I use to look for overfitting. I stumbled upon 4 losses that seem to be the same, but I'm not quite sure. In XGBoost, one of the objectives for multi class classification is multi:softprob ( https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters ). I couldn't find any formulas in the documentation, but apparently this objective returns "predicted probability of each data point belonging to each class", so an " ndata * nclass matrix". One of XGBoost's eval_metric s is mlogloss . The documentation (same link as above) links to sklearn.metrics.log_loss , which is "log loss, aka logistic loss or cross-entropy loss". sklearn 's User Guide about log loss provides this formula: $$ L(Y, P) = -\frac1N \sum_i^N \sum_k^K y_{i,k} \log p_{i,k} $$ So apparently, mlogloss and (multiclass categorical) cross-entropy loss are the same. The Otto Group Product Classification Challenge talks about "the multi-class logarithmic loss" and gives the same formula as above, so looks like mlogloss , cross-entropy loss and multi-class logarithmic loss are all the same. Now, I'm not particularly sure what multi:softprob is. multi:softmax is softmax - the activation function used in neural networks, for example. However, it's not a measure of error, like cross-entropy loss, right? Why is it one of the objectives in XGBoost, then? Unfortunately, there doesn't seem to be any useful information about multi:softprob , except that it's not the same as softmax because softprob outputs a vector of probabilities and softmax - "a class output" (so the ID of a class, I presume?). Am I correct that mlogloss , cross-entropy loss and multi-class logarithmic loss are the same thing? How are they different from multi:softprob ? What is multi:softprob anyway?
