[site]: crossvalidated
[post_id]: 531638
[parent_id]: 531633
[tags]: 
Your approach is called importance sampling (specifically, I think, self-normalised importance sampling). It can be used for Bayesian inference The difficulty with this approach is that the number of points you need can be very large when the space is high-dimensional. In particular, if $f$ is a posterior distribution (as is most often the case in MCMC), it is likely to be concentrated on a small volume of the parameter space. If you sample $X$ from some other unrelated distribution, you will need a very large sample space to get any points where the posterior density is high. Also, if you have an unbounded space you can't even sample uniformly from it. You need to rescale your $p(x)$ by the density you're actually sampling the $X$ s from, and it's important that this density goes to zero no faster than $f$ does in the tails, otherwise you get very influential points in the tails. The point of MH and other MCMC methods is that they can sample preferentially from areas where $f$ is large, and so you end up needing fewer points than if you just sampled and reweighted. The improved targeting can compensate for any extra computation per point.
