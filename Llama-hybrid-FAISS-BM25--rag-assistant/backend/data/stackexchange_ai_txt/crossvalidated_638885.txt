[site]: crossvalidated
[post_id]: 638885
[parent_id]: 
[tags]: 
Metrics for comparring multiple models

I am building logistic regression models of some data to test a hypothesis and using cross-validation for each one. The cross-valdiation takes place within different subjects (mice in this case) of an experiment so the CV being within different individuals lets me get that individuals behaviour. But to summarize what the best fit is I'm going to be computing some averages of model performance and comparing to how they do vs null models created by resampling with replacement again within individuals. The method is based on this paper here: https://www.nature.com/articles/nature25510#Sec19 The Model Comparison is described as follows... "All models were fit separately for each individual rat (n = 25), using 200 runs of fivefold cross-validation. For each run we calculated the log-likelihood of the test dataset given the best-fit parameters on the training set (logl). We also calculated the log-likelihood of the test dataset for the mean value of %Left (the experimentally measured fraction of trials in which the rat went left). This gives us a null log-likelihood reference value (logl0). In order to quantify the efficiency of each model we defined the cross-validated bit/trial (CV-bit/trial) as the trial-averaged excess likelihood of the model compared to the null model: $\frac{\left(\log l-\log l_0\right) / n_{\text {trials }}}{\log (2)}$ For each model, we first chose the optimal regularization value (Î») that would maximize the CV-bit/trial. To compare different models, we calculated the median value of CV-bit/trial across 10,000 fits for each subject. Because in this method we measure the log-likelihood using the cross-validated data, it automatically addresses the overfitting problem, such that if additional parameters of one model result in overfitting in the training set, it would penalize it in the cross-validated test set." I dislike their computation of a null by just taking the percent of outcomes which were positive (a left turn). I think it would be more thorough to compute a null for each model and subtract the mean value of a performance metric across all my folds (across all subjects) vs the mean of the null models. What criticisms of this strategy come to mind? Also should I use loglikelihood or Pseudo-Rsquared? Are there simpler approaches I could take?
