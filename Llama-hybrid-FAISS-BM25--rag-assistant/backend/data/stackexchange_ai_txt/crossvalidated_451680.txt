[site]: crossvalidated
[post_id]: 451680
[parent_id]: 
[tags]: 
Bayesian Neural Network in Keras: transforming simple ANN into BNN

I am starting to learn about Bayesian Neural Networks. As such, apologies if my question may be too simple. As a first step in my learning curve, I would like to transform a traditional ANN to a BNN. The ANN is very simple: 3 hidden layers, fully connected, with activation function given by tanh except for the output layer which has linear activation function. This network should represent a mapping between my observables (in input) and the (6) parameters of my model, in output. The input observables are one-dimensional functions, which can be regarded as time series of 2001 time components, if one considers the independent variable as time. The ANN architecture is shown below. As I said, I would like to transform this network into a Bayesian one. Following the excellent tutorial here I would think that the main changes to implement to obtain a BNN are: the 'neg_log_lieklihood' and the 'DenseVariational' layers. Would these be the only transformations needed? #!/usr/bin/env python from __future__ import division import numpy as np import matplotlib.pyplot as plt from keras.layers import Input, Dense, Lambda, Reshape, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout from keras.models import Model from keras import backend as K from keras import losses from keras import optimizers from keras import regularizers # np.loadtxt training and testing data X_train = #np.loadtxt ... X_test = # np.loadtxt ... y_train = # np.loadtxt ... y_test = # np.loadtxt ... # dimension of input (and label) n_x = X_train.shape[1] # 2001 n_y = y_train.shape[1] # 6 Dense_1 = 1500 Dense_2 = 700 Dense_3 = 200 x = Input(shape=(n_x, )) hidden = Dense(Dense_1, activation='tanh')(x) hidden = Dense(Dense_2, activation='tanh')(hidden) hidden = Dense(Dense_3, activation='tanh')(hidden) output = Dense(n_y)(hidden) net = Model(x, output) epochs = 400 def recon_loss(y_true, y_pred): return K.mean(K.square(y_true - y_pred)) optim = optimizers.Adam(lr=0.001) net.compile(optimizer=optim, loss=recon_loss, metrics=[recon_loss]) net_hist = net.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data = (X_test, y_test))
