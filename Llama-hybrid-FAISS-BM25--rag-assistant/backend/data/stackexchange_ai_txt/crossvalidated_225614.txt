[site]: crossvalidated
[post_id]: 225614
[parent_id]: 
[tags]: 
Adversarial learning gradient derivation

I'm working through Convolutional Neural Network paper here on adversarial learning and I'm having trouble with the derivative proof of adversarial logistic regression. The correct answer presented (on page 4) is: $$E_{x,y~p_{data}}\zeta(y(\epsilon||w||_1-w^Tx-b))$$ However, my derivation, based on the loss function from page three gives the sign of the gradient as $\newcommand{\sign}{{\rm sign}} -\sign(y(W^T))$ rather than $-\sign(W)$. I've been going through my math for the past two hours and if anyone could shed any insight, that would be really helpful. I've written out my logic below. \begin{align} E_{x,y~p_{data}}\zeta(-y(w^Tx-b)) \\ \zeta(z) &= \log(1 + \exp(z)) \\[5pt] \frac{\partial{E_{xy}}}{\partial{x}} &= \partial x(\frac{1}{1+e^{-y(w^Tx+b)}}) \\[5pt] &=\frac{e^{-y(w^Tx+b)}}{1+e^{-y(w^Tx+b)}}\partial x (-y(w^Tx+b)) \\[5pt] &=\frac{e^{-y(w^Tx+b)}}{1+e^{-y(w^Tx+b)}}(-yw^T) \\[5pt] \sign\bigg(\frac{\partial{E_{xy}}}{\partial{x}}\bigg)&=-\sign(yw^T) \end{align}
