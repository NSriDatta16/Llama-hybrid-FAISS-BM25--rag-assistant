[site]: crossvalidated
[post_id]: 503801
[parent_id]: 503684
[tags]: 
I think you're close. The neural net is not the solution to the differential equation $\dot{x} = F(x(t))$ , but rather the function governing the dynamics. That is to say, the neural net is $F$ , not $G$ . I think this much is clear in the first section of the paper. The approach is motivated by recurrent and residual neural networks, which look like $$ \mathbf{h}_{t+1} = \mathbf{h}_{t} + f(\mathbf{h}_{t}, \theta_t) \qquad (1) $$ Here, I am under the assumption $f$ is a neural net (or maybe a single layer of a neural net). As more layers are added and the step size decreases (though it isn't clear to me what the step size would be for RNNs) the authors make comparisons between (1) and the Euler discretization of an ODE $y(t+h) \approx y(t) + hy'(t, y(t))$ As for what data is needed to train the Neural ODE, I remember one if the authors (I think it was Duvenaud) gave a talk in which he lamented that he had continuous time data that could not be modelled by time series methods to satisfaction. I take it from that talk + the examples in the paper that you need a) continuous time data, b) outcomes at the measured times, and optionally c) additional information about the system under study at those times (e.g. if you were modelling stocks, the additional information could be other economic indicators).
