[site]: crossvalidated
[post_id]: 213048
[parent_id]: 213039
[tags]: 
This is a perfectly valid method. The method that give the best prediction score will be considered the one to use. However you might want to add more detail to your ensemble model. For instance RF and NN would outputs probabilities. So you could combine the probabilities to compute the majority vote instead of 0-1. SVM do not output probabilities so you could leave it as 0-1 or there is actually a way to compute probabilities on top SVM see https://www.researchgate.net/post/Can_we_assign_probability_to_SVM_results_instead_of_a_binary_output2 Another common thing to do, is to feed the output of your 3 classifiers to a logistic regression, so they all have their own weights. In a sense, it is a majority vote with "optimized weights". If you want more details and look into various techniques I would recommend reading this article: http://mlwave.com/kaggle-ensembling-guide/
