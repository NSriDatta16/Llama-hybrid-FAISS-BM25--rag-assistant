[site]: datascience
[post_id]: 37352
[parent_id]: 37340
[tags]: 
I found in the explanations that it "considers each class as important as the other" but still don't understand how. Does it duplicate rows of the minor categories? No, it adds a weight to each example, depending on its class. The majority class will be assigned a small weight while the minority ones will be assigned larger weights. This weight are considered during the training phase of your model, so that each example of a minority class impacts the parameter updates more than one from the majority class. For example, suppose class 1 has a weight of $0.5$, class 2 has a weight of $1.2$ and class 3 has a weight of $1.5$. Each example from class 3 will impact the parameter updates 3 times more than an example from class 1 , etc. You can see what weights scikit-learn has selected through this function. You can also select your own weights (if you feel that some class should be more important than what scikit-learn has selected) by modifying the dictionary. When I used that option, it was obvious that the classifier were predicting other Mark classes more frequently, but there are more number of right predictions than the wrong ones. I'm not sure what the question is, but if you are asking " why do I have more misses now than before? ". Because you used that option, your model has learned to classify examples while treating the classes of equal importance . But the classes don't have the same frequencies . For example if you miss-classify 1 out of 4 examples, you would miss more predictions from the majority class than from the minority ones. Is this good or bad? Depending on your goal. If you want to simply maximize your algorithm's correct predictions, then you might get a better result without balancing your classes. In this case you should use a metric like accuracy . If your goal something else, for instance to have the same accuracy in each class, you should select a more appropriate metric (e.g. a macro-averaged one). Is there an alternative to class_weights ? Yes, in most cases it is preferable to balance the number of samples in your classes. This is done by either over-sampling the minority classes, under-sampling the majority ones, or a combination of both. If you want to try out this approach, I'd suggest using imbalanced-learn .
