[site]: crossvalidated
[post_id]: 212840
[parent_id]: 
[tags]: 
Understanding recurrent SVM in volatility estimation of GARCH model

I read Chen et al. "Forecasting volatility with support vector machine-based GARCH model" (2010) where they implented a recurrent SVM procedure to estimate volatility by a GARCH based model. The model is of the form $y_t = f(y_{t-1}) + u_t \qquad \qquad \ \ \ (1)$ $u^2_t = g(u^2_{t-1}, w_{t-1}) + w_t \qquad (2)$ At first they got estimates for $u_t$ by estimating $(1)$ by a SVM. Then, the following recurrent SVM algorithm was proposed to estimate $(2)$. Recurrent SVM Algorithm: Step 1: Set $i = 1$ and start with all residuals at zero: $w_t^{(1)} = 0 $. Step 2: Run an SVM procedure to get the decision function $f^{(i)}$ to the points $\{x_t, y_t\} = \{u_{t - 1}^2, u_t^2 \}$ with all inputs $x_t = \{u_{t - 1}^2, w_{t-1} \}$ Step 3: Compute the new residuals $w_t^{i+1} = u_t^2 - f^{(i)}$. Step 4: Terminate the computaion process if the stopping criterion is satisfied; otherwise, set $i = i + 1$ and go back to Step 2. The proposed stopping critrerion is based a Ljung-Box-Test for the residuals $w_t$. Only if the $p$-values of the test in five consecutive periods are higher than 0.1 the process is stopped. As real world example the log-returns of the New York Stock Exchange (NYSE) composite stock index for the period from January 8, 2004 to December 31, 2007 was used. The last 60 observations where used as test sample. Hence, the estimation was done with the first 940 observations. In their study, the process converged after 121 interations. (Question:) However, my implementation in R does not converge. I think I have a misunderstanding of the concept. Because I think I implemented it exactly as stated. My R code is the following rm(list = ls()) library(quantmod) library(e1071) #Get NYSE data and convert to log returns id 0.1, p_count + 1, 0) #Extract residuals for next estimation step w
