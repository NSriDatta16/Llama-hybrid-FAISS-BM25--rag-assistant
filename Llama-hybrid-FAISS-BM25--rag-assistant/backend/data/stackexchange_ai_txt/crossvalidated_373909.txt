[site]: crossvalidated
[post_id]: 373909
[parent_id]: 373890
[tags]: 
I will write an answer assuming some form of regression (-like) model. You say neural network, much the same will apply, nevertheless it will be helpful to understand the issues in a simpler setting. And you should probably try some simpler model before throwing the data at a neural network ... So let $Y_i$ be the response, $x_i$ a covariate vector (all variables except brand indicator, possibly including interactions, not including brand), and $I_i$ (values 0 or 1) the brand indicator. We can write a model with linear predictor as (error terms not included) $$ Y_i = \beta_0 + \beta^T x_i +\alpha I_i. $$ This will simply give two parallel lines, the models for the two brands will only have different intercepts, otherwise equal. Then we can include all the interactions between brand and other variables $$ Y_i = \beta_0 + \beta^T x_i + \alpha I_i + \gamma^T \cdot \begin{cases} x_i \\ 0 \end{cases} $$ since the product between $x_i$ and the brand indicator will be either $x_i$ or the zero vector. This now splits nicely in two equations, one for each brand, as $$ Y_i=\begin{cases} \beta_0 + \beta^T x_i & \text{for $I_i=0$} \\ \beta_0+\alpha+(\beta+\gamma)^T x_i &\text{for $I_i=1$} \end{cases} $$ so the model with all interactions is algebraically the same as fitting two separate models . (In practice it will not be exactly the same, because fitting separately will give two different variance estimates. If there are many groups, this can lead to a serious loss of degrees of freedom for variance estimation). So this should give some hinting about the difference. If you want an in-between model, with some but not all interactions, you should fit one model. EDIT To make it more clear, a simple simulated example in R: set.seed(7*11*13) # My public seed n Then the model summaries: summary(modsep0) Call: lm(formula = Y ~ x1 + x2 + x3, data = mydata, subset = Sex == 0) Residuals: Min 1Q Median 3Q Max -7.0752 -1.0416 -0.0627 1.0081 6.0612 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.2936 2.6214 2.401 0.0202 * x1 0.9968 0.1104 9.029 5.34e-12 *** x2 -0.2664 0.2369 -1.125 0.2662 x3 0.1193 0.1975 0.604 0.5487 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 2.962 on 49 degrees of freedom Multiple R-squared: 0.64, Adjusted R-squared: 0.618 F-statistic: 29.04 on 3 and 49 DF, p-value: 6.166e-11 summary(modsep1) Call: lm(formula = Y ~ x1 + x2 + x3, data = mydata, subset = Sex == 1) Residuals: Min 1Q Median 3Q Max -5.7700 -1.9096 0.3434 2.1454 5.8608 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 4.4998 2.5982 1.732 0.0905 . x1 0.9738 0.1431 6.805 2.47e-08 *** x2 0.5550 0.2700 2.055 0.0459 * x3 1.6171 0.2246 7.199 6.64e-09 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 2.888 on 43 degrees of freedom Multiple R-squared: 0.9075, Adjusted R-squared: 0.9011 F-statistic: 140.7 on 3 and 43 DF, p-value: |t|) (Intercept) 6.29355 2.59092 2.429 0.0171 * Sex1 -1.79380 3.69469 -0.486 0.6285 x1 0.99681 0.10912 9.135 1.51e-14 *** x2 -0.26644 0.23415 -1.138 0.2581 x3 0.11926 0.19517 0.611 0.5427 Sex1:x1 -0.02304 0.18152 -0.127 0.8993 Sex1:x2 0.82139 0.36019 2.280 0.0249 * Sex1:x3 1.49783 0.29991 4.994 2.79e-06 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 2.927 on 92 degrees of freedom Multiple R-squared: 0.9824, Adjusted R-squared: 0.981 F-statistic: 732.4 on 7 and 92 DF, p-value: Then start comparing coefficients, start with the intercepts (remember that the Sex variable could not be included separately in the separate models, so is there part of the intercept) so start with 6.29355-1.79380 [1] 4.49975 and continue from there ...
