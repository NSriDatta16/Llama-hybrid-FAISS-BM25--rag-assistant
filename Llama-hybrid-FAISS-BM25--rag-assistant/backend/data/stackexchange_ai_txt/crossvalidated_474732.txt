[site]: crossvalidated
[post_id]: 474732
[parent_id]: 474535
[tags]: 
Since you would end up with one embedding per word and you need to somehow transform them into a single vector that will be the input to the classifier. The simplest and surprisingly good is just doing an average of the embeddings. You just look up the word embeddings in a look-up table and compute the average. Usually, removing stopwords helps a lot, however, in your particular task, function words are a strong indicator of a sentence being a question. Deep learning knows better ways of combining embeddings into a single vector. The most straightforward way would be using either an RNN and a 1-D CNN with max-pooling. Note that both embedding averaging and one-hot bag-of-words features that you are using do not consider word order which plays an important role here. Introducing simple categorical features like: "Does the sentence start with Wh ?" or "Does it end with question mark?" would help a lot, probably more than word embeddings.
