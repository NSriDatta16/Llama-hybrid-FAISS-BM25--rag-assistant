[site]: crossvalidated
[post_id]: 255186
[parent_id]: 56084
[tags]: 
A bit late but here would be my suggestion. What you want is to combine those two scores in a classifier. i.e you want to use both measures of similarity, rather than "enhancing" tfidf with the character based similarity. Then you can just use both of those scores as features to some ML classifier. The things to note here are that you need to make sure both scores are proper distances, so that you can fit a classifier over all examples. Tf-idf with cosine is always bounded by 0-1 as are the character based distances you listed, so you should be fine here. Just use tfidf-cosine as your x1 feature, and levenshtein/gap/jaro as x2. One other thought is that I would expect a non-linear classifier to outperform a linear one. This is because the character distance doesn't really matter except in relation the the token based distance. If you think about it, if tfidf-cosine similarity is 1 (perfect bag of words overlap), it doesn't matter at all what the string similarity is, as that could be really low if the words are out of order, which you don't care about. Conversely, if the tfidf-cosine is low (low BOW overlap), the string similarity becomes important, as if this is really high it gives good evidence of a similar document that has misspellings. A typical example of this is in comparing e-commerce product titles, often you have a title that is the same as the other but has a different order, or has the same order but uses various abbreviations of words and thus has low tfidf. This is very similar to a XOR problem, which linear classifiers can't solve. In this case I would opt for a tree based method like Random Forest, or you could try svm/logistic regression but include the interaction terms explicitly. Lucas
