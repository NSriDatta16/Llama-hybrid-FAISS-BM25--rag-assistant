[site]: crossvalidated
[post_id]: 138760
[parent_id]: 138748
[tags]: 
There are three general strategies I can think of for NNs with varying input sizes: Preprocess the inputs to save the same size. For example, people often resize images (ignoring aspect ratio) to a standard square resolution for NNs. In the language case, you might convert all words to a symbolic representation (e.g. "john"=1, "james"=2, "maurice"=3, "kelly"=4, "doe"=5) if that makes sense in your application. Use a sliding window. The network gets to see a fixed-size portion of the input, and then you slide the window by some fixed stride and run it again (from scratch), repeat until you hit the end, and then combine all the outputs in some way. Same as #2, but using a recurrent neural network so that the network has some internal state that carries over between each stride. This is how NNs process speech audio, for example. Obviously this is a more dramatic change to the architecture than the other options, but for many language tasks this might be necessary (if you have long inputs and need to combine information across the string in a complicated way).
