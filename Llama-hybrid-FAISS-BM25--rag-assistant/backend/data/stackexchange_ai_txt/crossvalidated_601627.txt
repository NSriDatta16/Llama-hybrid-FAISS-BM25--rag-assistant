[site]: crossvalidated
[post_id]: 601627
[parent_id]: 601464
[tags]: 
The standard way is to include the BOS token in the vocabulary, which will get an embedding like any other token in the vocabulary. During tokenization, you need to make sure that you add the token. Most of the standard tokenizers do so (e.g., Huggingface tokenizer have an add_special_tokens attribute for that). Freezing the embedding to all zeros should also work. In such a case, the remaining embeddings will be forced to rescale, such that the BOS token has a zero embedding.
