[site]: crossvalidated
[post_id]: 188835
[parent_id]: 
[tags]: 
Can I replace traditional DSP with statistical methods

I work a lot with noisy time traces with usually 2.5e6 data points and sampling frequency ~ MHz. I usually apply some "traditional" digital signal processing utilities like low/high-pass filters to extract interesting frequency components. Or create spectrograms/scalograms to analyze the evolution of different frequency bands. Usually they are not stationary signals, coming from discharge pulses. But often they are at least locally stationary. But more and more I realize that in the end I need to perform some averaging and the end goal is usually time evolution of some statistical property, like coherence (something like frequency dependent correlation coefficient) or covariance of some properties. I'm wondering whether I should therefore ditch my usual DSP style (mostly in Python) and go for the statistical style from the beginning in R. I'm quite lured by the huge number of packages in CRAN and the possibility of not writing so much boiler-plate code. However, all the different complicated names of various models and methods make it seem quite impenetrable to me as an outsider so possibly I'm expecting miracles because I don't fully understand all the limitations of the models and such. So I seek guidance here.
