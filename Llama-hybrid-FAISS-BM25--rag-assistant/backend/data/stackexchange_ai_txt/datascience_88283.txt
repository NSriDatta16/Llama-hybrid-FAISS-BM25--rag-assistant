[site]: datascience
[post_id]: 88283
[parent_id]: 88268
[tags]: 
If I understand correctly you are essentially looking for a way to use the CPU's RAM as a swap for the GPU's RAM. Unfortunately this isn't as easy to accomplish and might require some low level work. So if you're looking for a simple argument to add to your keras model, as far as I know there is none. Some options: Some older tensorflow APIs supported this functionality (e.g. dynamic_rnn - see swap_memory parameter). However these are now deprecated in favor of the equivalent keras layer, that to my knowledge does not support this functionality. Theoretically you could use compatibility mode to utilize this. You can specify what device each tensor will be loaded on beforehand . This isn't equivalent to swapping, but you could offload some computation to be regularly performed on the CPU. You can do this with tf.device . The same idea as above is often applied to loading and preprocessing the data, which is done on the CPU, while everything concerning the training ops is handled by the GPU. This is convenient because you can have a batch being loaded by the CPU at the same time that the model is being trained by the GPU. I know this is far off from what you're asking, because it requires your model to fit in the GPU entirely, however it fits in the general theme of offloading work to the CPU. This is fully supported by tf.data.Dataset . Use some trickery like gradient checkpoints to reduce the memory footprint of your models (with the expense of computational speed). This does not have anything to do with offloading work to the CPU, but it could help you fit your model into memory.
