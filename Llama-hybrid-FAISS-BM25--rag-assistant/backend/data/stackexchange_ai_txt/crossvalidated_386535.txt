[site]: crossvalidated
[post_id]: 386535
[parent_id]: 
[tags]: 
Why can't we use back propagation in "Hard attention" but we can use it in "RELU" function and max-pooling?

RELU, argmax function(in hard attention) and max-pooling are non-differentiable functions but We use back-propagation with RELU and max-pooling without any problems. What does make "Hard attention" different than them?
