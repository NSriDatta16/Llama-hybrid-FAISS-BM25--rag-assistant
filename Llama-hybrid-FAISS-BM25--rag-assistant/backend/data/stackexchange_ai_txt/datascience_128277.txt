[site]: datascience
[post_id]: 128277
[parent_id]: 
[tags]: 
Dimenson reduction from a cosine similarity matrix

I have a silly little question: I have 200 press articles (string), I vectorize these articles with an embedding model (sentence embedding), so I have 1024 values per article. I then have a 200 x 1024 matrix (vec_posts), and I calculate the cosine similarity between all these vectors (with scikit learn's cosine_similarity) from sklearn.metrics.pairwise import cosine_similarity cos_emb = cosine_similarity(vec_posts, vec_posts) I get a cosine similarity matrix of 200 x 200 (cos_emb) Finally, I reduce the dimensions in this way: from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.manifold import TSNE scaler = StandardScaler() cos_emb_scaled = scaler.fit_transform(cos_emb) pca = PCA(n_components=.99, random_state=42) cos_emb_pca = pca.fit_transform(cos_emb_scaled) tsne = TSNE(n_components=2, random_state=42) cos_emb_tsne = tsne.fit_transform(cos_emb_pca) And I use a scatter plot to visualize. I wonder if what I'm doing is mathematically correct? In a way, each item is characterized by the similarity between all the others. I'd like to hear your opinion on this.
