[site]: crossvalidated
[post_id]: 140377
[parent_id]: 
[tags]: 
Question about Continuous Bag of Words

I'm having trouble understanding this sentence: The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). What is the projection layer vs the projection matrix? What does it mean to say that all words get projected into the same position? And why does it mean that their vectors are averaged? The sentence is the first of section 3.1 of Efficient estimation of word representations in vector space (Mikolov et al. 2013) .
