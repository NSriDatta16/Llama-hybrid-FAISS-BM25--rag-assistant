[site]: crossvalidated
[post_id]: 140192
[parent_id]: 140163
[tags]: 
You should use the forecast package , which supports all of these models (and more) and makes fitting them a snap: library(forecast) x I would advise against smoothing the data prior to fitting your model. Your model is inherently going to try to smooth the data, so pre-smoothing just complicates things. Edit based on new data: It actually looks like arima is one of the worst models you could chose for this training and test set. I saved your data to a file call coil.csv , loaded it into R, and split it into a training and test set: library(forecast) dat Next I fit a bunch of time series models: arima, exponential smoothing, neural network, tbats, bats, seasonal decomposition, and structural time series: models Then I made some forecasts and compared to the test set. I included a naive forecast that always predicts a flat, horizontal line: forecasts As you can see, the arima model gets the trend wrong, but I kind of like the look of the "Basic Structural Model" Finally, I measured each model's accuracy on the test set: acc The metrics used are described in Hyndman, R.J. and Athanasopoulos, G. (2014) "Forecasting: principles and practice" , who also happen to be the authors of the forecast package. I highly recommend you read their text: it's available for free online. The structural time series is the best model by several metrics, including MASE, which is the metric I tend to prefer for model selection. One final question is: did the structural model get lucky on this test set? One way to assess this is looking at training set errors. Training set errors are less reliable than test set errors (because they can be over-fit), but in this case the structural model still comes out on top: acc (Note that the neural network overfit, performing excellent on the training set and poorly on the test set) Finally, it would be a good idea to cross-validate all of these models, perhaps by training on 2008-2009/testing on 2010, training on 2008-2010/testing on 2011, training on 2008-2011/testing on 2012, training on 2008-2012/testing on 2013, and averaging errors across all of these time periods. If you wish to go down that route, I have a partially complete package for cross-validating time series models on github that I'd love you to try out and give me feedback/pull requests on: devtools::install_github('zachmayer/cv.ts') library(cv.ts) Edit 2: Lets see if I remember how to use my own package! First of all, install and load the package from github (see above). Then cross-validate some models (using the full dataset): library(cv.ts) x (Note that I reduced the flexibility of the neural network model, to try to help prevent it from overfitting) Once we've fit the models, we can compare them by MAPE (cv.ts doesn't yet support MASE): res_overall Ouch. It would appear that our structural forecast got lucky. Over the long term, the naive forecast makes the best forecasts, averaged across a 12-month horizon (the arima model is still one of the worst models). Let's compare the models at each of the 12 forecast horizons, and see if any of them ever beat the naive model: library(reshape2) library(ggplot2) res Tellingly, the exponential smoothing model is always picking the naive model (the orange line and blue line overlap 100%). In other words, the naive forecast of "next month's coil prices will be the same as this month's coil prices" is more accurate (at almost every forecast horizon) than 7 extremely sophisticated time series models. Unless you have some secret information the coil market doesn't already know, beating the naive coil price forecast is going to be extremely difficult . It's never the answer anyone wants to hear, but if forecast accuracy is your goal, you should use the most accurate model. Use the naive model.
