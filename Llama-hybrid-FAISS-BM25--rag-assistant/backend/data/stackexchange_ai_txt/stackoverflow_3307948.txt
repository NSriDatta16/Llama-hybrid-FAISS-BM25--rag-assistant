[site]: stackoverflow
[post_id]: 3307948
[parent_id]: 
[tags]: 
"embarrassingly parallel" programming using python and PBS on a cluster

I have a function (neural network model) which produces figures. I wish to test several parameters, methods and different inputs (meaning hundreds of runs of the function) from python using PBS on a standard cluster with Torque. Note: I tried parallelpython, ipython and such and was never completely satisfied, since I want something simpler. The cluster is in a given configuration that I cannot change and such a solution integrating python + qsub will certainly benefit to the community. To simplify things, I have a simple function such as: import myModule def model(input, a= 1., N=100): do_lots_number_crunching(input, a,N) pylab.savefig('figure_' + input.name + '_' + str(a) + '_' + str(N) + '.png') where input is an object representing the input, input.name is a string, and do_lots_number_crunching may last hours. My question is: is there a correct way to transform something like a scan of parameters such as for a in pylab.linspace(0., 1., 100): model(input, a) into "something" that would launch a PBS script for every call to the model function? #PBS -l ncpus=1 #PBS -l mem=i1000mb #PBS -l cput=24:00:00 #PBS -V cd /data/work/ python experiment_model.py I was thinking of a function that would include the PBS template and call it from the python script, but could not yet figure it out (decorator?).
