[site]: crossvalidated
[post_id]: 285536
[parent_id]: 285271
[tags]: 
On the difference between Naive Bayes and Recurrent Neural Networks First of all let's start off by saying they're both classifiers, meant to solve a problem called statistical classification . This means that you have lots of data (in your case articles) split into two or more categories (in your case positive/negative sentiment). The classifier's goal is to learn how the articles are split into those two categories and then be able to classify new articles on it's own. Two models that can solve this task are the Naive Bayes classifier and Recurrent Neural Networks. Naive Bayes In order to use this classifier for text analysis, you usually pre-process the text ( bag of words + tf-tdf ) so that you can transform it into vectors containing numerical values. These vectors serve as an input to the NB model. This classifier assumes that your features (the attributes of the vectors we produced) are independent of one another. When this assumption holds, it is a very strong classifier that requires very little data to work. Recurrent Neural Networks These are networks that read your data sequentially, while keeping a "memory" of what they have read previously. These are really useful when dealing with text because of the correlation words have between them. The two models (NB and RNN) differ greatly in the way they attempt to perform this classification: NB belongs to a category of models called generative . This means that during training (the procedure where the algorithm learns to classify), NB tries to find out how the data was generated in the first place. It essentially tries to figure out the underlying distribution that produced the examples you input to the model. On the other hand RNN is a discriminative model. It tries to figure out what the differences are between your positive and negative examples, in order to perform the classification. I suggest querying "discriminative vs generative algorithms" if you want to learn mire While NB has been popular for decades RNNs are starting to find applications over the past decade because of their need for high computational resources. RNNs most of the time are trained on dedicated GPUs (which compute a lot faster than CPUs). tl;dr: they are two very different ways of solving the same task Libraries Because the two algorithms are very popular they have implementations in many libraries. I'll name a few python libraries since you mentioned it: For NB: scikit-learn : is a very easy to use python library containing implementations of several machine learning algorithms, including Naive Bayes. NaiveBayes : haven't used it but I guess it's relevant judging by the name. Because RNNs are considered a deep learning algorithm, they have implementations in all major deep learning libraries: TensorFlow : Most popular DL library at the moment. Published and maintained by google. theano : Similar library to tf, older, published by the University of Montreal. keras : Wrapper for tf and theano. Much easier. What I suggest you use if you ever want to implement RNNs. caffe : DL library published by UC Berkeley. Has python API. All the above offer GPU support if you have a CUDA enabled NVIDIA GPU. Python's NLTK is a library mainly for Natural Language Processing (stemming, tokenizing, part-of-speach tagging). While it has a sentiment package, it's not the focus point. I'm pretty sure NLTK uses NB for sentiment analysis.
