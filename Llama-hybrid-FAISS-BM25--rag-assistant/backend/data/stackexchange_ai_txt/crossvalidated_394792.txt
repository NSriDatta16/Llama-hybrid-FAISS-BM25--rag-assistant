[site]: crossvalidated
[post_id]: 394792
[parent_id]: 
[tags]: 
Symmetry group in posterior distribution/inference

Here's a scenario: Suppose I collect a dataset $\{x_i\}_{i=1}^k\subseteq\mathbb R$ of data points $x_i$ , and I wish to explain it using a mixture of two Gaussians; assume the unknown parameters are the means $\mu_1$ and $\mu_2$ of the mixture model. Following Bayesian methodology, I formulate some posterior distribution $P((\mu_1,\mu_2)|\{x_i\}_i)$ , from which I can use Monte Carlo methods to sample lots of likely $(\mu_1^j,\mu_2^j)$ pairs. The problem is, swapping $(\mu_1,\mu_2)$ to $(\mu_2,\mu_1)$ describes the same mixture of Gaussians; a symmetry group---in this case $\mathbb{Z}/2$ ---acts on the random samples I'm generating without changing their interpretation. This prevents me, for example, from directly averaging the pairs that come out of my sampler. That is, the expectation $\mathbb E_{(\mu_1,\mu_2)\sim P(\cdots)}[(\mu_1,\mu_2)]$ really doesn't make sense since the ordering of the two elements is arbitrary in each sample. Is there a general mathematical/statistical framework for coping with group-invariant probability measures? What inference algorithms do people use in this scenario? Do notions like mean/covariance extend to quotient spaces? Any pointers are much appreciated. This seems like a fundamental/classical problem, and likely I'm just missing Google search terms! My apologies if I have abused "Bayesian language" here---this is not my typical research field.
