[site]: crossvalidated
[post_id]: 241094
[parent_id]: 241062
[tags]: 
In addition to @mariodeng's answer which explains why the random forest trained with default parameters is worse here, here's an explanation why it may not be better than single trees in your experiment anyways: Aggregated/ensemble models are not universally better than their "single" counterparts, they are better if and only if the single models suffer of instability. With 1000 training rows and only 3 columns, you are in a comfortable training sample size situation in which even a decision tree may get reasonably stable. (For 3d data you can easily check the variation you have in the assignment of input space to the classes when rerunning the experiment.) If the predictions of the trees are stable, all submodels in the ensemble return the same prediction and then the prediction of the random forest is just the same as the prediction of each single tree. So then not only will the overall performance be the same, it will be the same cases that are predicted correctly and wrongly, respectively. This is the case in your example: table (predict(dtFit, test) [, 2], predict (rfFit, test)) # 0 1 # 0 46 0 # 1 0 54 why not 100% accurate? You train on data that is not representative for the test cases: the test cases cover regions of the input space that never appear in the training data. There is no way for a model to know which class (if any - or maybe a 3rd? ...) cases far outside training space should belong to. Particularly for highly nonlinear partitioning models (such as the decision trees), leaving training space will typically rather sooner than later lead to disaster. If you plan to train on one class only, you need to look into so-called one-class classifiers which try to establish independent boundaries for each class. One-class classification of your toy data should give you the result that the out-of-training-space cases do not belong to any of the known classes. Decision trees are a partitioning method, they cannot do one-class classification.
