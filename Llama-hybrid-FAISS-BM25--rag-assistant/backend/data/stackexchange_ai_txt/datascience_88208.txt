[site]: datascience
[post_id]: 88208
[parent_id]: 88162
[tags]: 
Preliminary note as far as I know, people may use the term feature extraction in slightly different ways: referring to automatic methods used for dimensionality reduction which involve transforming the features (and not only selecting a subset of features ). referring to the general process of designing and engineering the features before training/testing a model on these features. Please be aware that I'm not 100% sure that the terminology I use myself is standard. (but I don't think it matters for the question you're asking). Answer Used in the context you mention, the term certainly refers to the general process of feature engineering (2nd point above). In traditional ML models the model directly uses the features exactly as they are provided to the training algorithm. For example, a decision tree classification model tries to find the best conditions in the features to discriminate the data. This means that the algorithm selects a single feature and directly uses its range of values to build a condition, something like this: if color == 'green' then go to subtree 1 else go to subtree 2 Thus the type of information that can be used by a model is strictly limited by the algorithm and the features which are provided to it: there's no way to consider any information which is not directly available as a feature. As a consequence the stage of feature engineering is crucial. For example in text classification there are a lot of choices to make: provide the words frequency or TFIDF as features? Provide all the word or remove stop words? Lemmatize or not? Add POS tags? And these are just some very standard questions, specific tasks may require a very detailed analysis. Importantly, it's not possible to just provide all the features one can think of because this would almost certainly cause the model to overfit due to the high number of features. Generally in this stage the goal is to give the model the best and most precise clues that will help it make correct predictions, and also avoid giving it too many irrelevant features to prevent overfitting. That's the stage that DL doesn't need: thanks to a much more complex architecture and algorithmic process, a DL model can itself select and combine the features it needs to perform well. I thought discover it is part of the task of the machine learning algorithm. Sure, but in traditional ML in order for the model to discover the most important patterns for a specific task, the features have to be prepared in a way which maximizes the chances of the model to find these. Simile: if a teacher tells their students to revise page 63 of the textbook for the test, it's likely that the students will perform better than if they have no clue about the topic and need to revise the whole textbook.
