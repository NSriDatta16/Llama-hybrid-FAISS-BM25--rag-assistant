[site]: crossvalidated
[post_id]: 576782
[parent_id]: 576776
[tags]: 
That 25% of the data is unused by that learner (i.e. that iteration) of the XGBoost model (assuming subsample=0.75 ). This is normal, what they do via the subsample argument is to implement bagging by subsampling once in every boosting iteration. This means that, as you described, a portion of the data is not used by that specific base-learner during the $i$ -th iteration. In the $i+1$ -th iteration, sub-sampling of the whole dataset is performed once again. Through bagging (or more formally bootstrap aggregation) we practically bootstrap our estimator and allow ourselves to have a more robust overall result - think of it as estimating the sample mean via bootstrapping; just the "mean" here is the "expected prediction for each item" in our sample instead of a single "expected prediction for the sample's central tendency". And note that irrespective of the subsampling proportion we might use (e.g. 10%), we can always provide estimates for all (i.e. 100%) of our sample items. Some will be out-of-sample of course and some in-sample. In that way, we can also estimate error gradients for all our sample items if needed.
