[site]: crossvalidated
[post_id]: 457895
[parent_id]: 391834
[tags]: 
A few possibilities come to mind. If you have multiple datasets, you could use a Diebold-Mariano test. The tag wiki contains information, as well as pointers to the original and a follow-up publication. The DM test is very commonly applied in the time series forecasting community, but there is nothing specifically "time-seriesey" about it. If you have only a single dataset, then you could in principle bootstrap your model fits for both models and assess whether one improves on the other in (say) 95% of cases. If you do this on the same test set in each replicate (only bootstrapping the training data), then all you can conclude is whether one model is better on this particular test set , so the bare minimum I would do would be to wrap the exercise in a cross-validation step. And even so, applying this machinery on a single dataset would be very prone to just making you very confident that an overfitting model is better than it actually is.
