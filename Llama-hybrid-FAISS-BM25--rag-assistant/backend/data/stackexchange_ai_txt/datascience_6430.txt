[site]: datascience
[post_id]: 6430
[parent_id]: 6380
[tags]: 
Relative to other models, Random Forests are less likely to overfit but it is still something that you want to make an explicit effort to avoid. Tuning model parameters is definitely one element of avoiding overfitting but it isn't the only one. In fact I would say that your training features are more likely to lead to overfitting than model parameters, especially with a Random Forests. So I think the key is really having a reliable method to evaluate your model to check for overfitting more than anything else, which brings us to your second question. As alluded to above, running cross validation will allow to you avoid overfitting. Choosing your best model based on CV results will lead to a model that hasn't overfit, which isn't necessarily the case for something like out of the bag error. The easiest way to run CV in R is with the caret package. A simple example is below: > library(caret) > > data(iris) > > tr > train(Species ~ .,data=iris,method="rf",trControl= tr) Random Forest 150 samples 4 predictor 3 classes: 'setosa', 'versicolor', 'virginica' No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 120, 120, 120, 120, 120 Resampling results across tuning parameters: mtry Accuracy Kappa Accuracy SD Kappa SD 2 0.96 0.94 0.04346135 0.06519202 3 0.96 0.94 0.04346135 0.06519202 4 0.96 0.94 0.04346135 0.06519202 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 2.
