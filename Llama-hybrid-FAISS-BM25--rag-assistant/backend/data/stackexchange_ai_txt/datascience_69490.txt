[site]: datascience
[post_id]: 69490
[parent_id]: 69482
[tags]: 
The less-common words don't carry much information, because they don't appear frequently around other words; they don't appear frequently at all. Also sometimes low-count words are typos that are going to be noise anyway. Trying to learn about them isn't helping much, and may be prone to overfitting their embeddings because they appear rarely, which hurts generalization. Treating them as all one big 'unknown' word doesn't lose much, and allows for a simpler network to learn the embedding, which is faster. If the window is too wide, it's learning positive associations between words further away in the text, but if they're too far away that's probably not a good assumption, and hurts the quality of the model.
