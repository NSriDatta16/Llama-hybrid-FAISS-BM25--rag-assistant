[site]: datascience
[post_id]: 44493
[parent_id]: 44307
[tags]: 
As far as I understand the topic embeddings are not portable between graphs since every graph has their own characteristics which you need to learn. At the field of word embeddings, it is different since domain stays the same (words). In the field of graphs, you usually train the model on the same graph that will be embedded later. It is also important to train on the same graph because the labels of nodes are usually nodes IDs which are graph specific. In the field of words embeddings, the method recognizes words by their name (sequence of letters) while in graphs nodes are recognized by their IDs. Hope that it explains a bit. If you want to understand graph embeddings, in general, I suggest reading this story .
