[site]: crossvalidated
[post_id]: 69255
[parent_id]: 69208
[tags]: 
I don't believe such a cutoff exists, although the variable importance plots can be informative. Carry out two experiments. Rerun the random forest and see how the list changes. Delete an observation and also observe. In my experience, the answer relates to what is the goal of feature selection. For example, why not use every variable to make predictions. The random forest can easily do that. We usually use feature selection for a reason, for example, seeking a rule using just a small number of features that can easily be measured in the future. In my case, the number was set by the technology I plan to use the diagnostic on. More importantly, if you are using feature selection, this has to be repeated at each iteration of cross-validation. Searching this site for "cross-validation", "feature selection", and "stepwise regression" will give you a start.
