[site]: datascience
[post_id]: 67083
[parent_id]: 
[tags]: 
Understanding computations of Perceptron and Multi-Layer Perceptrons on Geometric level

I am currently watching amazing Deep Learning lecture series from Carnegie Melllon University, but I am having little bit of trouble understanding how Perceptrons and MLP are making their decisions on a geometrical level. I would really like to understand how to interpret Neural Networks on geometric level, but sadly I am not able to understand how computations of a single Perceptron relate to simple Boolean functions such as OR, AND, or NOT, which are all shown on picture below (e.g. what would be the required value of weights and input in order to model specific decision boundary). Hopefully, if I were to understand how these computations relate to geometric view shown on picture above, I would be able to understand how MLPs model more complicated decision boundaries, such as the one shown on picture below. Any help would be appreciated (concrete answer, reading resources, anything at all!). Thanks in advance!
