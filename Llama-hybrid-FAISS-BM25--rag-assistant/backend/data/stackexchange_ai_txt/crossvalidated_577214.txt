[site]: crossvalidated
[post_id]: 577214
[parent_id]: 576925
[tags]: 
Yes, I think the explanation you give is quite correct. You always need the decoder which is used to parametrize $p_{\theta}(x|z)$ , because what you are really doing is maximizing a lower bound on the likelihood based on the generative model decomposed as $p_{\theta}(x,z)=p_{\theta}(z)p_{\theta}(x|z)$ . More precisely, when training a VAE you need to solve two problems simultaneously. The first is about learning the parameters $\theta$ of the true distribution $p_{\theta}(x,z)=p_{\theta}(z)p_{\theta}(x|z)$ , and this part is about maximizing the likelihood, which you would normally do with the EM algorithm (maximizing ${\rm E}_{z\sim p_{\theta}(z|x)}(p_{\theta}(z,x))$ ). However, you do not know how to compute the true posterior $p_{\theta}(z|x)$ . So there is a second problem of learning the parameters $\phi$ of the variational approximation $q_{\phi}(z|x)$ of the true posterior which is parametrized by the encoder. If you see training a VAE as minimizing ${\rm KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$ with respect to $\phi$ and $\theta$ , you also end up using the expression with the ELBO and the loglikelihood (and thus $p_{\theta}(x,z)=p_{\theta}(z)p_{\theta}(x|z)$ and thus making use of the decoder) since you don't know the true posterior. Note that in other contexts you could just want to fit a variational posterior to a true and known posterior $p(z|x)$ by minimizing ${\rm KL}(q_{\phi}(z|x)||p(z|x))$ , but this is not what is done in VAE training.
