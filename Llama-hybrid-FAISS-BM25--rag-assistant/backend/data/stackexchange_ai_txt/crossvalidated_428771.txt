[site]: crossvalidated
[post_id]: 428771
[parent_id]: 428734
[tags]: 
It is useful to point out that Andrew Stuart has a webpage, it includes a link titled: " Lecture Notes: ICM2014 " (.PDF) which downloads his lecture: "Uncertainty Quantification in Bayesian Inversion". His webpage also lists a link to one of his papers: " The Bayesian Approach To Inverse Problems " ( July 2 2015 ), by Masoumeh Dashti and Andrew M. Stuart. What other reasons are there for someone to use a Bayesian Perspective in the field of uncertainty quantification? Testing the input spaces to determine optimum combinations for a preferred output. " Quantified Uncertainty in Thermodynamic Modeling for Materials Design " (Jan 29 2019), by Noah H Paulson, Brandon J Bocklund, Richard A Otis, Zi-Kui Liu, and Marius Stan "Phase fractions, compositions and energies of the stable phases as a function of macroscopic composition, temperature, and pressure (X-T-P) are the principle correlations needed for the design of new materials and improvement of existing materials. They are the outcomes of thermodynamic modeling based on the CALculation of PHAse Diagrams (CALPHAD) approach. The accuracy of CALPHAD predictions vary widely in X-T-P space due to experimental error, model inadequacy and unequal data coverage. In response, researchers have developed frameworks to quantify the uncertainty of thermodynamic property model parameters and propagate it to phase diagram predictions. ... Starting parameter values for MCMC posterior sampling are obtained by performing an initial (and deterministic) linear fitting of the Gibbs energy with thermochemical data, employing generic polynomials for temperature dependence and Redlich-Kister polynomials for compositional dependence. ESPEI provides a choice among uninformative (improper), uniform, Gaussian and triangular prior distributions. In this work, we observe that MCMC chains are likely to wander extensively in parameter space unless parameter values are reasonably bounded by the priors. This is expected since CALPHAD models are typically underdetermined when compared strictly with the data, and require expert input to maintain “physical reasonableness.” Consequently, we choose triangular prior distributions with minimum and maximum parameter values and the center of mass corresponding to ± 0.5 and , respectively, where is the starting parameter value. Within ESPEI, the Affine Invariant Ensemble Sampler [19] algorithm (via the emcee Python package) is employed for MCMC Bayesian inference. In this algorithm, 150 MCMC walkers explore parameter space and optimally guide future walker movements. The walkers are initialized from a random sampling of univariate Gaussian distributions centered at $\rho$ with a 0.1 $\rho$ standard deviation. We run ESPEI until the walkers percolate a region in parameter space whose bounds do not appreciably change with increasing iterations for each parameter.". Choosing sets of data that are easiest to use together, rather than fighting to fit correlated data with different coloring and noise profiles. " Bayesian Strategies to Assess Uncertainty in Velocity Models ", Bayesian Anal., Volume 7, Number 1 (2012), 211-234. By Camila C. S. Caiado, Michael Goldstein, and Richard W. Hobbs "Quantifying uncertainty in models derived from observed seismic data is a major issue. In this research we examine the geological structure of the sub-surface using controlled source seismology which gives the data in time and the distance between the acoustic source and the receiver. Inversion tools exist to map these data into a depth model, but a full exploration of the uncertainty of the model is rarely done because robust strategies do not exist for large non-linear complex systems. There are two principal sources of uncertainty: the first comes from the input data which is noisy and band-limited; the second is from the model parameterisation and forward algorithm which approximate the physics to make the problem tractable. To address these issues we propose a Bayesian approach using the Metropolis-Hastings algorithm .". Input trimming using Bayes linear statistics and Bayesian inference where the input data is trimmed in order to identify the subset of the input space that could give rise to acceptable matches between model output and measured data. " Galaxy Formation: a Bayesian Uncertainty Analysis ", Bayesian Anal., Volume 5, Number 4 (2010), 619-669. By Ian Vernon, Michael Goldstein and Richard G. Bower "Over the last 13 years, they have developed a detailed computer model, known as Galform, which simulates the creation and evolution of approximately one million galaxies from the beginning of the Universe until the present day. The simulation produces various physical features of each of the galaxies which can be compared to observed galaxy survey data. The Galform model requires many input parameters to be specified in order to run the simulation. It is therefore necessary to explore the input parameter space and find the set of all input configurations that give rise to acceptable matches between model output and observed data. As the model run time is significant, this is a challenging task. Further, even to assess what constitutes an acceptable match, we must consider all of the uncertainties that are involved in the comparison between model and reality, including input parameter uncertainty, function uncertainty, observational error, forcing function uncertainty and structural uncertainty. Such a detailed level of uncertainty quantification has never been attempted for such a cosmological model. ... ... we must also recognise that, for a complex model such as Galform, uncertainty modelling is a process which is similar in many ways to the physical modelling process on which we are building. Quantifications of uncertainty depend on complex scientific judgements over which different experts may have different views. Further, expert knowledge is held collectively over a wide community of experimenters, observationalists, theoreticians and modellers. Therefore, it is as misleading to talk of a definitive assessment of the uncertainty associated with Galform as it would be to talk of a definitive form for the Galform model itself as experts would not currently agree on the precise form the Galform code should take. Assessment of uncertainty is an ongoing process for models which are, themselves undergoing continuous development. Our account documents one iteration in this ongoing process, albeit one for which the uncertainty analysis is carried out to a much greater level of detail than is usual in this field (or indeed in most analyses of complex physical models in any area of application of which we are aware).".
