[site]: crossvalidated
[post_id]: 209810
[parent_id]: 
[tags]: 
Computation of the marginal likelihood from MCMC samples

This is a recurring question (see this post , this post and this post ), but I have a different spin. Suppose I have a bunch of samples from a generic MCMC sampler. For each sample $\theta$, I know the value of the log likelihood $\log f(\textbf{x} | \theta)$ and of the log prior $\log f(\theta)$. If it helps, I also know the value of the log likelihood per data point, $\log f(x_i | \theta)$ (this information helps with certain methods, such as WAIC and PSIS-LOO). I want to obtain a (crude) estimate of the marginal likelihood, just with the samples that I have, and possibly a few other function evaluations (but without rerunning an ad hoc MCMC). First of all, let's clear the table. We all know that the harmonic estimator is the worst estimator ever . Let's move on. If you are doing Gibbs sampling with priors and posteriors in closed form, you can use Chib's method ; but I am not sure how to generalize outside of those cases. There are also methods that require you to modify the sampling procedure (such as via tempered posteriors ), but I am not interested in that here. The approach I am thinking of consists of approximating the underlying distribution with a parametric (or nonparametric) shape $g(\theta)$, and then figuring out the normalization constant $Z$ as a 1-D optimization problem (i.e., the $Z$ that minimizes some error between $Z g(\theta)$ and $f(\textbf{x}|\theta) f(\theta)$, evaluated on the samples). In the simplest case, suppose that the posterior is roughly multivariate normal, I can fit $g(\theta)$ as a multivariate normal and get something similar to a Laplace approximation (I might want to use a few additional function evaluations to refine the position of the mode). However, I could use as $g(\theta)$ a more flexible family such as a variational mixture of multivariate $t$ distributions. I appreciate that this method works only if $Z g(\theta)$ is a reasonable approximation to $f(\textbf{x}|\theta) f(\theta)$, but any reason or cautionary tale of why it would be very unwise to do it? Any reading that you would recommend? The fully nonparametric approach uses some nonparametric family, such as a Gaussian process (GP), to approximate $\log f(\textbf{x}|\theta) + \log f(\theta)$ (or some other nonlinear transformation thereof, such as the square root), and Bayesian quadrature to implicitly integrate over the underlying target (see here and here ). This seems to be an interesting alternative approach, but analogous in spirit (also, note that GPs would be unwieldy in my case).
