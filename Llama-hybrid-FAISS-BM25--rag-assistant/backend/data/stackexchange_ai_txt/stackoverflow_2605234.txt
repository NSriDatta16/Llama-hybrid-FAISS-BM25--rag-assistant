[site]: stackoverflow
[post_id]: 2605234
[parent_id]: 1545606
[tags]: 
For continuous data, k-means is very easy. You need a list of your means, and for each data point, find the mean its closest to and average the new data point to it. your means will represent the recent salient clusters of points in the input data. I do the averaging continuously, so there is no need to have the old data to obtain the new average. Given the old average k ,the next data point x , and a constant n which is the number of past data points to keep the average of, the new average is k*(1-(1/n)) + n*(1/n) Here is the full code in Python from __future__ import division from random import random # init means and data to random values # use real data in your code means = [random() for i in range(10)] data = [random() for i in range(1000)] param = 0.01 # bigger numbers make the means change faster # must be between 0 and 1 for x in data: closest_k = 0; smallest_error = 9999; # this should really be positive infinity for k in enumerate(means): error = abs(x-k[1]) if error you could just print the means when all the data has passed through, but its much more fun to watch it change in real time. I used this on frequency envelopes of 20ms bits of sound and after talking to it for a minute or two, it had consistent categories for the short 'a' vowel, the long 'o' vowel, and the 's' consonant. wierd!
