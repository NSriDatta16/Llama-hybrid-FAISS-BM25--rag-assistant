[site]: crossvalidated
[post_id]: 638942
[parent_id]: 622791
[tags]: 
Finally after bunch of readings on the topic I finally came out with a solution for my problem. I don't know if someone else would be interested in this topic, I will provide an answer to my own question though. From my understanding some of Bayesian penalized models (LASSO and Horseshoe for example) suppose a non normal prior on the beta coefficients, making the posterior calculation quiet tricky. However, and as originally proposed by Gelman in a Bayesian context and other authors after him, this kind of distributions can be rewrite as a mixture of gaussian. This hierarchical rewriting makes the prior a conjugate, hence facilitating MCMC runs or variational Bayes derivations. Here a great paper on this topic: https://www.researchgate.net/publication/340006729_A_practical_tutorial_on_Variational_Bayes Regarding my question about covariance parameters my initial understanding was that the prior has to be from exponential family in order to derive analytical solutions for the optimal variational distribution. In fact, the prior has to be conjugate. So, neither the original Laplace prior nor the Gaussian mixture are conjugate priors, making variational bayes closed-form solutions for the covariance parameters impossible. I finally decided to use a Wishart distribution for the covariance matrix using ideas from this paper to shrink on-diagonal and off-diagonal elements: DOI:10.1214/13-BA815. I do not know if it is the best way to approach sparse covariance matrix but I will not need any sort of approximations, used in G-Wishart or any kind of non-conjugate priors. Any ideas are welcome to discuss my choices.
