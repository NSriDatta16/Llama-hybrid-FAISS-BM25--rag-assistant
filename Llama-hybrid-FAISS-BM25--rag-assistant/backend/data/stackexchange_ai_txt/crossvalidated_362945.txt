[site]: crossvalidated
[post_id]: 362945
[parent_id]: 
[tags]: 
Can neural networks learn $g(x)$ from $\mathbb{E}[g(X_t)] = \int_{-\infty}^{\infty} g(x)p_t(x)dx$

Let $\mathbb{E}_x[g(X_t)]$ be the expected value of a random variable $X_t$ with known probability density $f_t(x)$ then for the continuous case $$\mathbb{E}[g(X_t)] = \int_{-\infty}^{\infty} g(x)f_t(x)dx$$ where $g(x)$ or the distribution of $g(x)$ is not known. Hence, one possible pair of the training set in a supervised setting would be $(f_t(x), \mathbb{E}[g(X_t)])$ evaluated at a fixed time $t$. Note that for most distribution the useful bounds are finite. Can neural networks learn the function $g(x)$ or its probability distribution? Reference: Law of the unconscious statistician I have added a time dependency but the problem is fundamentally the same. Please explain in as much detail as possible. This seems extremely useful if possible. *images removed
