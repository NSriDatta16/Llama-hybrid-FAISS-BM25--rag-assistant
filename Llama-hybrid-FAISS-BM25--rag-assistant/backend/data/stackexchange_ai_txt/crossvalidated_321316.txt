[site]: crossvalidated
[post_id]: 321316
[parent_id]: 321147
[tags]: 
Essentially, each mini-batch contains the average of the gradients of the individual errors. Therefore if you had two mini-batches, you could take the average of the gradient updates of both mini-batches to tweak the weights to reduce the error for those samples. Note however that each back-propagation step tweaks the weights in the right direction to diminish the error rather than computing the absolute best weights/biases setting for this particular minibatch. Of course you could repeat the process to optimize the weights for that minibatch but that is not what you want as you would obtain a very biased result for the samples in the minibatch.
