[site]: crossvalidated
[post_id]: 545040
[parent_id]: 545033
[tags]: 
I see you have any questions. Q&A sites like this are usually not best suited for such cases, I'd encourage you to ask one question at a thread next time. Also given the number of questions, it'd be probably a good idea to start with one of many great handbooks on machine learning. Nonetheless, I feel like the questions are fairly easy to answer, so let me try. How many trees should I run based on my sample size? Sample size and the number of trees are unrelated. With random forest, the general rule is that you use as many trees as you can . With a large sample size, the number would be likely limited by the memory available and the training time that is acceptable for you. Shall I set a limit on the depth of my tree? It is a hyperparameter, changing it would affect the results. Usual advice would be: do it and check what happens. Additionally, with shallower trees, computation time and memory use would be smaller. How could I tell if a feature is important or not? [...] What for? With only seven features and so much data, you don't really need to do the feature selection. I saw people visualize trees as part of their analysis output. However, if I end up training many trees (e.g., 1000 trees), shall I just randomly visualize 1 tree? [...] Visualizing a single tree is pointless. The point of random forest is that one would expect each of the trees to be (randomly) different. Visualizing a single tree tells you nothing about the overall model. There is no single, simple way to visualize random forest. I also wonder if XGBoost will be a better option than random forest in my case. [...] It depends. Again, the usual answer would be to try it and which model works better.
