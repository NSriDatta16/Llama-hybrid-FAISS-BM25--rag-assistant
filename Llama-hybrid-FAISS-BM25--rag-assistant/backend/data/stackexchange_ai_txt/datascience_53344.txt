[site]: datascience
[post_id]: 53344
[parent_id]: 53342
[tags]: 
If the algorithm inherently supports multi-label classification, then it's usually an implicit feature of the algorithm rather than an implementation detail. For example, MLPs inherently support multi-label classification because the output layer has a perceptron for each class, and each of these perceptrons output a probability for that class. The vector of outputs will predict an example's membership among all labels. Similarly, the the leaves of a Random Forest (or any other tree-based algorithm) can contain arbitrary-length vectors that describe the probability of the example belonging to each label. The one-vs-rest strategy is used generalize binary classifiers (e.g. logistic regression) to multinomial problems. Multinomial problems are distinct from multi-label problems. Edit: Clarified my answer w.r.t. the differences between multinomial and multi-label problems.
