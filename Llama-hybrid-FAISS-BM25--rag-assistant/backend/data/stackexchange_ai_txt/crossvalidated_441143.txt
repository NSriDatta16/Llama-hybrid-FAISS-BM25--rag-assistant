[site]: crossvalidated
[post_id]: 441143
[parent_id]: 
[tags]: 
When using L2 regularization outside of linear regression, do the same MAP estimation assumptions hold?

Some context is shared below, and my question is bolded at the end. MLE from observation noise In the linear regression setting, we learn model weights $\mathbf{w}$ to make scalar predictions $\hat{y}$ from samples $\mathbf{x}$ as $$ \hat{y} = \mathbf{w}^T\mathbf{x} $$ When one assumes the true underlying distribution is a linear combination and a Gaussian noise term, $$ y|\mathbf{x} = \mathbf{w}^T \mathbf{x} + \mathcal{N}(0, \sigma^2) $$ then maximum likelihood estimation (MLE) induces a mean squared error loss $$ \mathcal{L}_{MLE}(\mathbf{w}) = \sum_{i=1}^n (\mathbf{w}^T\mathbf{x}_i - y)^2 $$ such that minimizing $\mathcal{L}_{MLE}$ produces the MLE estimate of weights. MAP from weight distribution Further, if one assumes a Gaussian prior distribution on the model weights $\mathbf{w}$ with each weight $w_i$ having identical variance $\nu^2$ $$ w_i \sim \mathcal{N}(0, \nu^2) $$ then the analogous maximum a posteriori (MAP) estimation induces the L2 regularizer with regularization weight $\lambda = \frac{\sigma^2}{\nu^2}$ $$ \mathcal{L}_{MAP}(\mathbf{w}) = \sum_{i=1}^n (\mathbf{w}^T\mathbf{x}_i - y)^2 + \lambda||\mathbf{w}||^2_2 $$ such that minimizing $\mathcal{L}_{MAP}$ produces the MAP estimate of weights. I really appreciate that these common practices can be derived from first principles and simple distributional assumptions. But L2 regularization is used all over the place - practitioners will add an L2 loss on the weights for models of all sorts, from logistic regression to enormous neural networks. I appreciate that it works well, but it seems a bit mysterious. When used outside of the linear regression setting, does L2 regularization still express elegant distributional assumptions and first-principles?
