[site]: datascience
[post_id]: 69294
[parent_id]: 69290
[tags]: 
The cross entropy is equivalent (up to a constant) to the Kullback Leibler Divergence which has an interpretation based on information theory: (very crudely) it is the amount of information "lost" by representing the true labels via the distribution of their predicted values (measured in "nats" (e) or "bits" (2) depending on the base of the logarithm). I don't find it very useful as a "colloquial" interpretation or to think in units of bits. Typically, you would calculate more interpretable metrics like accuracy and inspect them along with the loss to get a picture of the performance. However, you can use the loss and compare it to a simple baseline (e.g. the model only predicting the average of your labels or only the majority class) and thus get a relative performance estimate.
