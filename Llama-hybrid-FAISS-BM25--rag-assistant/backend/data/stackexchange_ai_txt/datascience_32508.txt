[site]: datascience
[post_id]: 32508
[parent_id]: 
[tags]: 
xgboost cannot identify perfectly fitting regression line

For a dataset I want to use xgboost for the optimal ensembling of $n$ forecasts instead of just using their arithmetic mean for combination. I found that xgboost generates forecasts that are worse than many of the $n$ individual forecasts the moedl could choose for combination. I do not know why this can be the case. For the illustration of my observation I created the toy dataset below. The artificial target variable is generated by $$y = \frac{x_1+x_2}{2} \, \,\mbox{with } x_1, x_2 \sim N(0,1) $$ Given the deterministic relationship between $y$ and the two explanatory variables $x_1$ and $x_2$, xgboost could make perfect forecasts, but it does not. The linear model easily does. Since this is the most simple multivariate linear regression model I can think of and xgboost fails I wondering about the implications. Why is this the case? What are the limitations of tree models for regression? Why is then xgboost used for stacking and ensembling of forecasts if it cannot reproduce the MSE minimizing arithmetic mean as optimal combination mechanism? Note that the parameters of xgboost do not affect that. I tried many parameter settings and the results are never perfect. Data Generation library(tidyverse) library(xgboost) n xgboost xgtrain Linear regression model
