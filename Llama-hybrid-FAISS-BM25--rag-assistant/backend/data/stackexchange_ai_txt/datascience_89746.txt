[site]: datascience
[post_id]: 89746
[parent_id]: 89744
[tags]: 
MLP typically refers to a type of neural network called a 'Multi-Layer Perceptron'. As you can read on the Wikipedia page , these neural networks consist of neurons organized in layers. Each neuron has a (typically fixed) function that transforms its weighted input to produce the output for that particular neuron known as its activation function. A 'network architecture' refers to the way the neurons are laid out in layers in the network, sometimes including the activation function. In the given example, there are 120 neurons in the first input layer. These are connected to 7 neurons in the next layer, which in turn are connected to 24 neurons in the last output layer. The 120 input neurons in the linked article appear to include both history, i.e. a sliding_window as well as some known future information (temperature, from weather forecasts it seems).
