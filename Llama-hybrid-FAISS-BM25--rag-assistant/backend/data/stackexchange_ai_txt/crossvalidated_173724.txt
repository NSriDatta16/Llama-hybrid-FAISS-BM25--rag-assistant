[site]: crossvalidated
[post_id]: 173724
[parent_id]: 
[tags]: 
Why do I end up with a highly correlated matrix when I multiply two strictly positive random matrices?

I've been working with some large positive matrices for a machine learning problem I'm trying to solve. The problem involves multiplying a matrix $A$ that is a set of users and items, with a matrix $B$ that is a set of items and attributes. When we look at the columns of $B$, we see minimal correlations. However, when we create a new matrix, $C=A \bullet B$, we see that the columns of $C$ are highly correlated. This is clearly a problem for machine learning. One thing that we've tried is normalization. It seems like centering and scaling the the features of $B$ to a standard normal distribution fixes the problem of high correlations. Also, in order to maintain the positive constraints, if we $L1$ norm the rows of $A$ and the columns of $B$, then the correlations do not appear in the final product. The weirdness comes when we try to use $L2$ normalizations, which to me shouldn't be that different than $L1$. However, if we $L2$ norm the rows of $A$ and the columns of $B$, then the correlations still appear (although they're slightly weaker) ... This is intuitively very strange to me and I can't seem to figure out why this is. Does anyone have any insight into what's going on here? I've included R code below that recreates the issue with random matrices. library(corrplot) # randomly generate strictly positive matrices item_attributes
