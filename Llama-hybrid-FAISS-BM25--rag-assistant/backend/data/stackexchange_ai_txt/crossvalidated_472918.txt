[site]: crossvalidated
[post_id]: 472918
[parent_id]: 
[tags]: 
k-fold cross-validation vs. model building

I’m fairly new to the topic of cross-validation and have some questions; wonder if any of you might share your expert opinion on this. I’m trying to build a multinomial logistic regression model for prediction purposes and asked to do cross-validation. A set of candidate independent variables (IVs) are given to me. I use all the IVs and do multinomial logistic regression using k-fold cross validation; I see that the model performs poorly: Pseudo R2 are very low and only a few IVs are significant. So I need to find a better model with reduced number of predictors. My question is: Each time I try a new model (with a different set of IVs), do I do k-fold cross-validation? Then identify the best model and fit it on the entire data set? Or do I 1st do these procedures (i.e. trying different models with different set of predictors) on a particular sub-set of the entire sample, identify the most relevant/useful model, and then use k-fold cross-validation on the remaining sample only for this model? Or do I do those steps of finding the final model on the entire sample and then do k-fold cross-validation only for the final model? The concerns I have regarding the options #1 and #3 above are that, data that are used to develop the model should not be used for testing/validating the model as it will give too “optimistic” results. Options #1 and #3 above will include a portion of the dataset, that might overlap on both the model ‘development’ and ‘validation’ samples… Would appreciate your opinion. Thanks!
