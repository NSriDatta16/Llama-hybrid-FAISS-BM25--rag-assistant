[site]: crossvalidated
[post_id]: 203200
[parent_id]: 
[tags]: 
How to compare a logistic regression ad-click predictor against a random predictor?

Set up: assume we have a set of $n$ diverse ads and a single website, which serves as our advertising space. Whenever a user visits this website, he is shown a single ad from our set, and if he clicks on it we count that as a success. Problem: how to choose the best ad to show? By "the best" I mean the one with the highest probability of a click. I should also note that regardless of whether the user might be interested in any of our ads or not, we MUST display one of the ads. First attempt: I tried out a logistic regression model: $$ \hat{p}_{t}(\mathbf{w}_t, \mathbf{x}) = \frac{1}{1+\exp[-\mathbf{w}_t \cdot \mathbf{x}]},$$ with online gradient descent, i.e. at each step/observation $y_t$ suffering the log-loss associated with displaying ad $j$ with the highest probability $\hat{p}_{t}(\mathbf{w}_t, \mathbf{x}^j)$: $$ \textrm{Log-loss}_t(\mathbf{w}_t) = -y_t \log \hat{p}_{t}(\mathbf{w}_t, \mathbf{x}^j) - (1-y_t)\log (1-\hat{p}_t(\mathbf{w}_t, \mathbf{x}^j)),$$ where $y_t$ is either 1 (user decided to read more ), or 0 (no action). The feature vector $\mathbf{x}^j$ contains features describing the user, and features referring to ad $j$. To optimize the parameters $\mathbf{w}_t$, I modify them after each observation (after displaying the $j^\text{th}$ ad) using a gradient of the $\textrm{Log-loss}_t$ function. Thus, I arrive at a model $\hat{p}$ predicting the probability of a click. After tweaking various parameters of my model (features, learning rate of the online gradient descent, and other), I chose the one with the lowest $\textrm{Log-loss}$, as calculated on a validation set. Its test $\textrm{Log-loss}$ was around $0.022$ (to be fair, all models had very similar, very low values of the error metric). Alas: when testing my model "live", on the actual website, it performed slightly better than a random-choice-model (each ad chosen with a probability $1/n$). I verified the code, and it's safe to assume that this surprisingly poor performance is not due to a coding error. The $y_t = 1$ class is very rare (say, one in $10^5$ users decides to click an ad). As a consequence, my model estimates very low probabilities ($~10^{-5}$), which yields a low $\textrm{Log-loss}$. THE ACTUAL QUESTION: I would like to compare my models against the random-choice-model. If I had plugged the $1/n$ probability into the $\textrm{Log-loss}$ I wouldn't get a reliable assessment of the random-choice-model. So, my first thought was to figure out a re-scaling of the random-choice probability. Or, to find a skew-insensitive error metric (perhaps this would be the best solution, and I'm open to suggestions regarding this scenario). But then, I had a very different idea: to train my model by minimizing a modified $\textrm{Log-loss}$ function, e.g.: $$\textrm{Log-loss2}_t(\mathbf{w}_t) := \textrm{Log-loss}_t(\mathbf{w}_t) + 1-(\sum_{k=1}^n \hat{p}_{t}(\mathbf{w}_t, \mathbf{x}^k))^2,$$ where I use a regularization term, keeping the probabilities normalized. This way, I hope to: compare my models with the random-choice-model; incorporate probability estimates for all ads into the error function. The second points seems appealing, since whenever $y_t$ is $0$, the parameters $\mathbf{w}_t$ would change so that the probability of the ad displayed would decrease, but at the same time -- increase the probabilities of the remaining ads. Follow-up questions: am I trying to re-invent the wheel? Are there any obvious counter-arguments to constructing a custom log-loss error measure?
