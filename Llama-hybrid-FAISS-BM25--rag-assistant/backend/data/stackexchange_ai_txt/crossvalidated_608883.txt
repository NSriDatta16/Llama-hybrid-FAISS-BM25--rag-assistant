[site]: crossvalidated
[post_id]: 608883
[parent_id]: 508086
[tags]: 
Neural Networks as Gaussian Processes Consider a neural network with only one layer (i.e. no hidden layers, i.e. logistic regression): $$\operatorname{reg}: \mathbb{R}^N \to \mathbb{R}^M : \boldsymbol{x} \mapsto \boldsymbol{s} = \boldsymbol{W} \boldsymbol{x}.$$ If we replace the entries in $\boldsymbol{W} \in \mathbb{R}^{M \times N}$ by random values, such that $w_{ij} \sim \mathcal{N}(0, \sigma_w^2)$ , the resulting function will be a random/stochastic process. Now, let $\boldsymbol{w}_i$ be a row of $\boldsymbol{W}$ , such that $$s_i = \boldsymbol{w}_i \boldsymbol{x} = \sum_{j=1}^N w_{ij} x_j,$$ we can use the central limit theorem to conclude that $s_i$ follows a Gaussian distribution if $N \to \infty$ . Therefore, a large number of inputs ( $N$ ) turns the random process into a Gaussian process (because the outputs are now Gaussian). This is exactly the idea presented in your last piece of literature (Lee, 2018). Although Lee et al. write about infinite width in every layer, I would argue that you only really need it in the penultimate layer (i.e. the inputs to the final layer). Having infinite width everywhere just makes the computation of the mean and covariance functions tractable (at least for ReLU networks). The Effect of Loss Functions A loss function by itself will never be a Gaussian process because there is typically no randomness in a loss function. This being said, the combination of neural network and loss function can give rise to a random process. In order to assess whether this random process will still be Gaussian depends on the loss function itself. I believe that there are no practical loss functions that would preserve Gaussianity. E.g. when using the mean squared error, $(\operatorname{reg}(\boldsymbol{x} \mathbin{;} \boldsymbol{w}) - y)^2,$ it should be clear that the loss values will not be Gaussian. After skimming the papers that are referenced in your question, I am not entirely sure whether they really talk about loss functions as Gaussian processes: Pascanu et al. (2014) mention that they use random loss functions, sampled from a Gaussian process. This would be using GPs exactly as how you described them: a distribution of functions. Choromanska et al. (2015) seem to try to prove that a ReLU network with some loss function that uses randomness is related to a Gaussian process. At least that would be my interpretation since I do not know much about spin-glass models.
