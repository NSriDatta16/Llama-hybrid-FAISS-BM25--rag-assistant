[site]: crossvalidated
[post_id]: 587284
[parent_id]: 587283
[tags]: 
If I understand correctly your question, I would say there is no reason to have that. For instance, say that we observe a random variable $Y_n$ , that for a large number of samples is distributed as $\mathcal{N} (f(n), \sigma^2(n))$ , for some functions $f(n)$ and $\sigma(n)$ of the number of samples. Then we cannot conclude that $Y_n = \sum_i X_i$ for some random i.i.d. variables $X_i$ . Indeed, it may happen (under certain regularity conditions) that this phenomena that we observe is the convergence of the distribution of $\sqrt{n} (\hat{\theta}_n - \theta_0)$ to such normal, where $\theta_0$ is the true parameter. In this case, there is no reason to have that $\hat{\theta}_n$ is the weighted average of some random variables. Indeed if: $$ \Delta_{n, \theta_{0}}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} I_{\theta_{0}}^{-1} \dot{\ell}_{\theta_{0}}\left(X_{i}\right), $$ where $\dot{\ell}_{\theta}$ is the score function of the model. The Bernstein-von Mises theorem implies that: $$\sqrt{n}\left(\hat{\theta}_{n}-\theta_{0}\right) \stackrel{d}{\rightarrow} \mathcal{N} \left(\Delta_{n, \theta_{0}}, I_{\theta_{0}}^{-1}\right)$$ where $I_{\theta_{0}}^{-1}$ is the inverse of the Fisher information matrix. See, in this case $\hat{\theta}_n$ is the posterior estimated parameter, which often does not coincide with a sample average. However we observed the convergence to some normal $\mathcal{N} (f(n), \sigma^2(n))$ .
