[site]: crossvalidated
[post_id]: 193352
[parent_id]: 
[tags]: 
In RNN Back Propagation through time, why is the D(h_t)/D(h_(t-1)) diagonal?

I was going through backpropagation in time for RNN, in the deep learning book of Joshua Bengio et.al. (deep learning book , section 10.2.1 ). Given a network as: the book tells that the backpropagation of the error gradient respect with the state h(t) is given by: So, why is the matrix of the D(h_(t+1))/D(h_t) diagonal? How is its solution obtained?
