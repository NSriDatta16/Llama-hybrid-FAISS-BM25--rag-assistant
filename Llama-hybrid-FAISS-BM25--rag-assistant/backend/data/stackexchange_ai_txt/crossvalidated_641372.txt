[site]: crossvalidated
[post_id]: 641372
[parent_id]: 639961
[tags]: 
Equation (1) is just the expected loss function with randomly perturbed weights. The whole idea is to show that $J$ with a regularization is equivalent to $\tilde J$ under certain conditions. (You have typo there, the last expression should have the square of the first term). There is some notational complexity in the book. The neural network is also a function of $W$ as well as $\mathbf x$ . If we consider $x$ and $y$ as constants for a moment just for brevity, we can write the following using Taylor series : $$\hat y_\epsilon(W)=\hat y(W+\epsilon)\approx \hat y(W)+\nabla\hat y(W)^T\epsilon$$ This is a first-order approximation and it's a good one when $\epsilon$ is small. If we just substitute this into (1), we get the following: $$\tilde J=\mathbb E\left[(\hat y - y)^2 - 2 (\hat y - y) (\nabla\hat y)^T \epsilon + \|\nabla \hat y^T\ \epsilon\|^2\right]=\underbrace{\mathbb E[(y-\hat y)^2]}_J+\mathbb E\left[\|\nabla\hat y^T\epsilon\|^2\right]$$ Also note that $\mathbb E[\epsilon]=0$ and independent of other variables, thus the middle term cancels. The second term is basically, $\mathbb E[\|\nabla \hat y\|^2]\operatorname{Var}(\epsilon)=\eta E[\|\nabla \hat y\|^2]$ due to the complete independence of each $\epsilon_i$ . Notes The Taylor expansion assumes denominator layout in the vector differentiation without loss of generality. The expectation notation does not strictly need the probability distributions, therefore the simplified notation. Notation of $\hat y$ , $\epsilon$ and partial derivative wrt $W$ is also simplified for brevity.
