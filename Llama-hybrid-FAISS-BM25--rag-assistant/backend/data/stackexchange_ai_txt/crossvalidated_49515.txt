[site]: crossvalidated
[post_id]: 49515
[parent_id]: 49162
[tags]: 
You've actually hit on something of an open question in the literature. As you say, there are a variety of kernels (e.g., linear, radial basis function, sigmoid, polynomial), and will perform your classification task in a space defined by their respective equations. To my knowledge, no one has definitively shown that one kernel always performs best on one type of text classification task versus another. One thing to consider is that each kernel function has 1 or more parameters which will need to be optimized for your data set, which means, if you're doing it properly, you should have a second hold-out training collection on which you can investigate the best values for these parameters. (I say a second hold-out collection, because you should already have one which you are using to figure out the best input features for your classifier.) I did an experiment awhile back in which I did a large-scale optimization of each of these parameters for a simple textual classification task and found that each kernel appeared to perform reasonably well, but did so at different configurations. If I remember my results correctly, sigmoid performed the best, but did so at very specific parameter tunings--ones which took me over a month for my machine to find. My advice is that, if you have sufficient time and data to do some parameter optimization experiments, it could be interesting to compare the performance of each kernel in your particular classification task, but, if you don't, linear SVM performs reasonably well in text classification, has only the c-parameter to optimize (although many people just leaves this at default settings), and will allow you to focus on the aspects of your classification system that will have a greater contribution to final performance--the types of input features you use, and how you model them.
