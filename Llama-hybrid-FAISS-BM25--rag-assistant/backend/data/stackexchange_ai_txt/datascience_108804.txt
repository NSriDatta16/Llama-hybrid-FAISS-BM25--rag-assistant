[site]: datascience
[post_id]: 108804
[parent_id]: 52000
[tags]: 
Disclaimer: Only performs text classification (for now) For general purpose tasks, I recommend RBERT I've developed a package on CRAN called transforEmotion There is a vignette to get set up with Python (includes example of how to use package in R at the end) After, the rest is taken care of in the transformer_scores() function. The default model on CRAN is Facebook's BART Large I've done much more work on GitHub The default model on GitHub is Cross-Encoder's DistilRoBERTa (much faster than Facebook's BART Large with minimal trade-off for accuracy) The transformer_scores() function allows you to implement any huggingface text classification pipeline so long as there is a pipeline for it: https://huggingface.co/models?pipeline_tag=zero-shot-classification
