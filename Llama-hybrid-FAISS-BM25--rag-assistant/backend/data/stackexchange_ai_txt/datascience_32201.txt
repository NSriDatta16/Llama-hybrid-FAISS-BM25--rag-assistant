[site]: datascience
[post_id]: 32201
[parent_id]: 
[tags]: 
Really bad value of Val loss

I am using GTZAN dataset to make a CNN and classify by musical genres. I'm getting very good results except Val. loss (See Image) I am processing the audio files using Librosa, obtaining the spectogram and then using the power_to_db function. This is my CNN Model: class CNNModel(object): def __init__(self, config, X): self.filters = 32 # number of convolutional filters to use self.pool_size = (2, 2) # size of pooling area for max pooling self.kernel_size = (3, 3) # convolution kernel size self.nb_layers = 4 self.input_shape = (128, 625, 1) # cambiar por x.shape def build_model(self, nb_classes): model = Sequential() model.add( Conv2D( self.filters, self.kernel_size, padding ='same', input_shape = self.input_shape)) model.add(BatchNormalization(axis=1)) model.add(Activation('relu')) model.add( Conv2D( self.filters, self.kernel_size)) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size = self.pool_size)) model.add(Dropout(0.25)) model.add( Conv2D( self.filters + 32, self.kernel_size, padding ='same')) model.add(Activation('relu')) model.add( Conv2D( self.filters + 32, self.kernel_size, padding ='same')) model.add(MaxPooling2D(pool_size = self.pool_size)) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(nb_classes)) model.add(Activation("softmax")) #mirar return model I leave the link of my github in case you want to see the whole code. Every song is (128, 625) Shape, I used MinMaxScale to Scale the data. This is my loss function and my optimizer loss = losses.categorical_crossentropy, optimizer = optimizers.SGD(lr=0.001, momentum=0, decay=1e-5, nesterov=True) I have read about overfitting and that seems to be the cause but I do not know how to solve it at the code level. Update 1: With Dropout(0.9) I get this results: Thank you
