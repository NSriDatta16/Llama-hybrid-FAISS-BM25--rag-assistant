[site]: crossvalidated
[post_id]: 450575
[parent_id]: 450003
[tags]: 
In addition to @DemetriPananos' very good advise to look into ESL: I find fig. 1 in Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7), 1895â€“1923. extremely helpful in clarifying what one is talking about (including $\mathrm Err_\tau$ vs. $\mathrm Err$ ). If you are concerned with the performance of your modeling approach for new (not yet existing) data sets of the same type as the one you have ("analyse algorithms" branch in Dietterich) rather than with producing a good model from the data at hand (that's usually my task), you'll also want to have a look at Bengio, Y. and Grandvalet, Y.: No Unbiased Estimator of the Variance of K-Fold Cross-Validation Journal of Machine Learning Research, 2004, 5, 1089-1105 as that paper explains that you won't be able to measure all the relevant variance with the cross validation procedure. The situation is different (better) for the "analyze classifiers" branch in Dietterich's figure (because the variance uncertainty of what happens if a new dataset of size $n$ is drawn from the population doesn't matter for the classifier that can be obtained from the data at hand.). But you still have at least two sources of variance: uncertainty due to the limited number of tested cases, and uncertainty due to model instability. In addition, there may be further confounders (that e.g. cause clustering in your data and thus correlation between cases). These can be assessed with bootstrapping (and also iterated/repeated CV/jackknifing) One important point that can be extremely helpful in this context is that you may be able to say why the so far apparently better model isn't significantly better: because you've had too few test case One advantage of bootstrapping or iterated/repeated CV here is that you can set up the resampling procedure to take such confounders into account (e.g. if the measurements come from fewer subjects, you'd resample subjects). You can then construct a pairwise test by running the same splits for all formulae.
