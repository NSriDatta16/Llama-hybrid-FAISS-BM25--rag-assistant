[site]: crossvalidated
[post_id]: 363503
[parent_id]: 
[tags]: 
Evidence in favor of the Null / Zero for Regression in R

I have two studies, let's call them A and B . Both involve fitting a linear mixed model on what is essentially the same type of data; the two studies differ in only small ways. In A I found a significant (p no effect of condition in B (or almost no effect), so my hope is that I can demonstrate that the data from study B is more consistent with a slope of zero than with the data from study A . (I'd be comfortable running this as just a linear regression if necessary, as the random effects in the model do very little.) My understanding is that a Bayesian approach is the best way to provide 'evidence for the null', so I have been trying to find a way to do the analysis described above. My understanding is also that a Bayes Factor will be the measure I want, since it essentially describes the relative strength of the evidence in the data for two priors (?). But finding a way to actually conduct this analysis has been challenging. I've investigated several R packages that seem like they could do this job, including blme, BayesRS, and BayesFactor. This doesn't seem to be the normal use case for anything I've investigated, and I'm still not even sure how to define a prior based on my data from A. Most of what I've seen has just demonstrated how to compare Bayesian regression models to one another. Can anyone point me in the direction of the next steps for this? At this point I'm not sure what I'm missing or where else to search.
