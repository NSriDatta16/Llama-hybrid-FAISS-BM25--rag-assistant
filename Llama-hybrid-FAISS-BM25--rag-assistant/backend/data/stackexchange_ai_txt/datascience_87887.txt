[site]: datascience
[post_id]: 87887
[parent_id]: 6200
[tags]: 
General options: Group your features into groups by generating new labels that represent multiple features Use dimensionality reduction, e.g. PCA, Autoencoder, etc . A lot of these are implemented in Sklearn and the downside is that your features become confusing to analyze once it converts them to pure mathematically representations that can have a relation meaning that the algorithm 'learned' Algorithm dependent options: Normalization, which in Neural Networks defines the contributions of each feature in a specific layer to the final classification, regression, etc Dropout, this is similar to my comment , tells your model to randomly ignore a certain percentage of inputs between your Neural Network layers Observation: I described Neural Networks for the last two options, but you can also apply them in a few different algorithms, to give you an example I studied Trees a lot and I saw that you can apply it in Decision Tree, Random Forest, and in Extreme Gradient Boosting versions of trees like Ada Boost, Cat Boost, etc Generally I would see the data information, if you're using pandas info , describe , plot (works for each feature of your dataset), isnull().values.any() , etc; and mainly the visual plot to see its balance. In a few problems, I didn't know much about these and it played a huge role on the later decisions!
