[site]: crossvalidated
[post_id]: 114204
[parent_id]: 
[tags]: 
Linear SVM prediction time is scaling in an unexpected manner based on training data

I'm using LIBSVM to do some training as it was recommended by Andrew Ng and is used under the hood in SciKit Learn. LIBSVM is doing something different than what I expect though: My beliefs are as follows: LIBSVM when set to use a linear kernel is a reasonable implementation of a linear SVM A linear SVM model should just be a hyper plane and a margin. A n-1 dimensional hyper-plane can be represented by a single n dimensional vector and constant. A prediction performed against a single hyper-plane should be constant with respect to the number of training examples used to train the model. Linear kernel SVMs are roughly equivalent to logistic regression. In practice, LIBSVM models trained with a linear kernel show different prediction times depending on how many data points the model was trained with. When I look in the model file, there are many vectors in the file instead of a single one. Can anyone illuminate what I am missing?
