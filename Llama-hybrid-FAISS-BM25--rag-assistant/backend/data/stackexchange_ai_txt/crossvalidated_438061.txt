[site]: crossvalidated
[post_id]: 438061
[parent_id]: 438009
[tags]: 
100 layers is usually better than 10 for common tasks (image classification, object detection, segmentation, etc). We use more layers when we have lots of data and compute and we don't want to limit our system with low capacity model. 2x2 max pooling is common for reducing feature map size, and global average pooling at the end of a network is common. I have some engineering level of tunning a network, but they are all empirical. Yes, this is more or less the same strategy for everyone: throw hundreds of GPUs at the problem to try a bunch of configurations until you end up with something acceptable. Paper or books involving this matter would be good^ The best way to "get a feel" for what is a reasonable design choice and what isn't, is to skim recent papers and see what architecture/decisions they use/made.
