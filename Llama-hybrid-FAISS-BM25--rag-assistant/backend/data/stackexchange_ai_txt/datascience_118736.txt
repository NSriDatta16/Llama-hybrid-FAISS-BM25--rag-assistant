[site]: datascience
[post_id]: 118736
[parent_id]: 
[tags]: 
Which algorithms are suitable for time series classification for this form of data?

Newbie here, I have a large list of csv files that contain a series of probability distributions (for 5 classes). I'm trying to train a binary classifier that classifies each file into either a positive or negative class (think medical data). A file would look something like this: 5.716146182185134483e-01,1.570351925479201299e-02,7.879747111130347426e-03,2.257236373138224450e-03,4.025448790424259182e-01 9.282036396626092145e-01,1.350252924774404326e-02,1.202944234922503821e-03,1.077829667771727410e-04,5.698310388794716047e-02 7.827619863407961898e-01,1.238710984142047805e-01,4.863074012493366800e-03,1.025238277112187050e-04,8.840131740479445499e-02 9.164564167759865487e-01,5.029982363205267454e-02,1.889329141871147781e-03,3.470568579893339639e-05,3.131972476429063790e-02 8.438599468843770435e-01,1.249894809120763589e-01,4.892015882596660245e-03,6.318069857265228148e-05,2.619537562237720871e-02 7.927862899191280288e-01,1.836963229227321637e-01,8.236097888524881658e-03,7.672725903457503882e-05,1.520456201058032440e-02 6.938920913668409352e-01,1.154946186794907348e-01,4.775610289402269087e-03,6.998382596026862888e-05,1.857676958383056576e-01 5.260109666876285894e-01,5.373236552754552531e-02,2.351633924538396904e-03,5.219686669895120705e-05,4.178528369935886611e-01 3.048328823074770155e-01,6.639936048941325053e-02,2.493391932535035486e-03,3.446025521901748158e-05,6.262399050153556468e-01 4.090438902597091086e-01,8.938231982641510476e-02,3.129738246458822065e-03,4.481533923136918172e-05,4.983992363281855020e-01 3.390709739657606914e-01,7.605934289910418200e-02,2.752420617374790393e-03,4.097357351008719444e-05,5.820762889442502308e-01 6.149776057156663978e-01,7.070150093100460720e-02,2.396856449071802474e-03,3.329015183386138958e-05,3.118907467524231758e-01 5.370645930556066094e-01,1.300161777734864521e-01,3.121570148930582871e-03,2.977214131925493871e-05,3.297678868806570573e-01 4.034325013246508052e-01,1.843589776745651332e-01,4.607250776903500968e-03,3.698432440481392170e-05,4.075642858994759088e-01 4.864387774152492128e-01,2.113910367664585677e-01,4.652455876671098348e-03,2.834732864475179149e-05,2.974893826129763608e-01 2.994464950642671264e-01,3.608846139880280690e-01,7.909903942206513577e-03,2.764288414682062749e-05,3.317313441213514680e-01 3.384097307003371413e-01,3.577836317319948445e-01,9.840109811059946296e-03,3.679938546258189509e-05,2.939297283711455044e-01 . . . . While I understand I can take the argmax for each row and get a much simpler time series, in my particular use case (medical diagnosis) including the probabilities of different classes might be beneficial to the model. My initial plan was to either use transformer encoder -> MLP -> softmax ( from this article ) or a simple LSTM, but I don't know how I can fit this form of data to those algorithms. I'd appreciate any help.
