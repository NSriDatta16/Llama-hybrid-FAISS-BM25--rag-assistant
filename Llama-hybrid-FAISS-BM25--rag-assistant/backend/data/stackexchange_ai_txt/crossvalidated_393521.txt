[site]: crossvalidated
[post_id]: 393521
[parent_id]: 
[tags]: 
Feature selection in xgboost vs GBM in H2O

I am working on a big data set( more than 100 variables) and 30 million observations. I tried to build 100 models with a grid search using both XGBoost and GBM in H2O (Sparkling Water). I realized that the variable selection is completely different in each model when I use GBM, however in XGBoost my top 10 variables are always the same! (And those are variables that makes the most sense for my specific case) I was wondering why is this happening? Can I conclude that my xgboost models are more robust?
