[site]: crossvalidated
[post_id]: 376678
[parent_id]: 376674
[tags]: 
When a gradient boosting machine fits a tree $f(x)$ to the target variable $y$ , it calculates the error (used in the next iteration to fit the next tree) as: $$e = y - \epsilon f(x)$$ where in this case $\epsilon = 0.1$ , the learning rate. Now imagine that there is a fairly simple tree $g(x)$ that actually fits the data reasonably well (for a simple tree, that is.) At our first iteration, our model finds $g(x)$ and calculates the errors $e$ for the next step. At that step, $e$ will likely be fit quite well by the same tree, because we didn't subtract all of $g(x)$ from $y$ , we only subtracted $10\%$ of it. Writing rather loosely, $90\%$ of $g(x)$ is still in the error term $e$ . Because $e \neq y$ , the leaf values will be a little different, but the tree structure will be the same, so at the next iteration we'll fit $g(x)$ again. We may have to do this quite a few times before we have done enough fitting of $g(x)$ to $y$ for other tree structures to become obvious enough for the GBM to find in preference to more fitting of $g$ . The fundamental cause of this is that we are taking a step towards $g(x)$ of fixed length - $\epsilon$ - not performing a line search to find the optimum step length towards $g(x)$ at each iteration then taking one step of that length. This is a feature! It helps prevent overfitting. If you fit "optimum" trees at every step of the GBM, you very rapidly overfit, even with small trees. (Note that they aren't really optimum trees, as they are found through a greedy search heuristic.) Random Forests do fit "optimum" trees at every step, and deep ones too, but deal with the overfitting issue by averaging across trees. GBM takes a different approach, one which may appear to waste iterations in cases such as yours, but in general works quite well on a broad range of problems.
