[site]: crossvalidated
[post_id]: 544199
[parent_id]: 543417
[tags]: 
You have to feed in the RNN's own predictions into itself. Why? From what I know, you can only answer this question going back to the probabilistic interpretation. You basically use an LSTM to model a conditional distribution, although packages like Keras are don't bother telling the user about this. In general, I advice people to familiarize themselves with this. What follows is a brief exposition. You are using the RNN to model the distribution $$ p(x_t | x_{1:t-1}), $$ and you want to use that to get $$ p(x_{t+k} | x_{1:t-1}), $$ where $k$ is the prediction horizon. We need some probability theory to get an insight what the latter actually is. Using the sum-rule we get $$ \begin{align} p(x_{t+k}~|~x_{1:t-1}) =&~ \int p(x_{t:t+k}~|~x_{1:t-1})~dx_{t:t+k-1} \\ =&~ \int p(x_{t:t+k}~|~x_{1:t-1})~dx_{t:t+k-1} \\ =&~ \mathbb{E}_{x_{t+1:t+k-1}} \left [ p(x_{t+k}~|~x_{1:t+k-1}) \right ]. \end{align} $$ We an now apply this recursively and will finally arrive at $k=0$ where we can put in our LSTM model. Depending on your probabilistic assumptions–e.g. if you are using a squared loss you are implicitly using a Gaussian distribution–you need to sample from your LSTM's output in different ways. You can also try feeding in the point predictions (i.e. not sampling), but that answer will only be approximate and underestimate uncertainty.
