[site]: datascience
[post_id]: 121013
[parent_id]: 
[tags]: 
How K and V are extracted from encoder output in transformer?

I was trying to understand transformer architecture from "Attention is all you need" paper. The paper shows following transformer architecture: How $K$ and $V$ is extracted from $512$ dimensional encoder output (which is then fed to second multi head attention in decoder)?
