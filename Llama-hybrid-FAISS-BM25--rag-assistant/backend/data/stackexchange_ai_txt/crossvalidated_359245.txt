[site]: crossvalidated
[post_id]: 359245
[parent_id]: 
[tags]: 
Has Arcsinh ever been considered as a neural network activation function?

The function $y = arcsinh(x)=ln(x+\sqrt{x^2+1})$ has some nice features that I could imagine being useful as an activation function in a neural network. It has sigmoid behaviour around zero, but far from the centre exhibits logarithmic growth which means the slope of the function doesn't diminish nearly as quickly as the sigmoid function, which means the problem of one neuron becoming 'stuck' in a region where the slope is vanishingly small (and therefore can't move from its large value) is avoided. I suspect there's a problem with computation - I don't know how arcsinh is normally calculated, but it's probably too computationally intensive to be efficient compared to other activation functions. I've not seen any evidence of Archsinh used or even considered anywhere. Are my guesses about its features and drawbacks accurate?
