[site]: crossvalidated
[post_id]: 395173
[parent_id]: 364786
[tags]: 
This is a general fact from information theory. In the continuous case, the conditional (differential) entropy can be rewritten via: \begin{align} \mathbb{H}[Z|X] &= \mathbb{E}_x\left[ \mathbb{H}[Z|X=x] \right] \\ &= \int \mathbb{H}[Z|X=x] \,dx \\ &= \int\left[-\int p(z|x) \log p(z|x) \,dz\right] p(x) \,dx \\ &= -\iint p(x,z) \log p(z|x)\, dz\,dx \\ &= -\iint p(x,z)\log\frac{p(x,z)}{p(x)}\,dz\,dx\\ &= \underbrace{-\iint p(x,z)\log p(x,z)\,dx\,dz}_{\mathbb{H}[X,Z]} + \iint p(x,z) \log p(x) \,dx\,dz \\ &= \mathbb{H}[X,Z] + \int \log p(x) \underbrace{\left[\int p(x,z) \,dz\right]}_{p(x)} dx \\ &= \mathbb{H}[X,Z] - \underbrace{\left( -\int p(x) \log p(x) \, dx \right)}_{\mathbb{H}[X]}\\ &= \mathbb{H}[X,Z] - \mathbb{H}[X] \end{align} The discrete analogue is discussed here . I suppose it is somewhat debatable what makes the most sense. One thing to note is that we are minimizing $D_{\text{KL}}[q(x,z)\mid\mid p(x,z)]$ (or the JS rather), which means that maximizing $\mathcal{I}_q[x,z]$ should also be maximizing $\mathcal{I}_p[x,z]$ . So in that sense, perhaps it does not matter (since ideally $p(x,z)$ and $q(x,z)$ should be matched). As they note in 5.2, the adversarial losses enforce matching of the joint distributions (term 1 in eq 18; and as term 1 shrinks, term 3 converges to a constant), so by adding the $\beta \mathcal{I}_q[x,z]$ term, we ensure that we are minimizing an upper bound on the reconstruction error. Also it's worth recalling from the paper that they say (section 5.1) We propose to palliate mode collapse by maximizing the mutual information between the samples and the code. which was written with regards to regularizing the GAN generator. This $\beta \mathcal{I}_q[x,z]$ can be viewed as such as well.
