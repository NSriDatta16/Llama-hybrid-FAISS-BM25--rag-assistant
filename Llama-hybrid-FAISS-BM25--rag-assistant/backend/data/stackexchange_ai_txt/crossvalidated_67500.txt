[site]: crossvalidated
[post_id]: 67500
[parent_id]: 67375
[tags]: 
Based on the edit it sounds like what is desired is to generate, independently and uniformly at random, a partition of the $N=100$-vector $(1/100, 1/100, \ldots, 1/100)$ into $k=10$ pieces and to output the sums of elements in each piece. More generally we might ask how to do this for an arbitrary vector $x$. For example, the solution below can be used in the form set.seed(3) z to generate the $9$-vector $(100000000, 10000000, 1000000, 100000, 10000, 1000, 100, 10, 1)$ and partition it into six (possibly empty) parts. Its output--which although random is reproducible by setting the seed to $3$--is [1] "000000000" "100000000" "011100000" "000011100" "000000000" "000000011" [1] 0 1 4 7 7 This partition was generated by cutting the vector at positions $0, 1, 4, 7,$ and $7$. Visually this can be represented by placing vertical bars at both ends of the vector and the (random) bars inside to designate the cuts, $$(\vert\ \vert x_1 \vert x_2, x_3, x_4 \vert x_5, x_6, x_7 \vert \vert x_8, x_9\ \vert).$$ The partial sums are $0$ (for the initial empty cut between the first two bars), $100000000$ for the unary sum $x_1$, $011100000$ for the sum $x_2+x_3+x_4$, $000011100$ for the sum $x_5+x_6+x_7$, $0$ for the empty sum between two successive bars following position $7$, and $000000011$ for $x_8+x_9$. The method to generate these is a standard mechanism for generating random permutations of length $k-1$ (the for loop below): to permute an $n$-vector $a$, $a_i$ is swapped with $a_j$ where $j$ is uniformly selected in the range $[i, n]$ inclusive. This is done for $i=1, 2, \ldots, k-1$. The prefix $(a_1, a_2, \ldots, a_{k-1})$ contains the desired random permutation. The rest is just reprocessing the data into the desired form of output. To accommodate the possibility of adjacent cuts (leading to some partial sums of zero), we use a well-known method of considering the $k-1$ internal vertical bars as if they, too, were elements of $x$. This extends $x$ from an $N$-vector to an $n=N+k-1$-vector. The random permutation selects the $k-1$ positions among $1, \ldots, n$ at which the internal vertical bars will be placed. The post-processing subtracts $1$ from the first index, $2$ from the second, ..., and $k-1$ from the last so that they point to the positions within $x$ itself that occur just before each cut. To represent the two outside vertical bars it adjoins $0$ and $N$ to these indexes (but does not explicitly output them). A difference of cumulative sums (via cumsum and diff ) is an R -centric way to obtain the partial sums within each partition. As a check, let's generate (say) $10,000$ such partitions and look at histograms of (a) the $10,000 \times 10=100,000$ partial sums in the original problem and (b) the cut indices. We expect the former to average exactly $10 \times 1/100 = 1/10$ and the latter to be uniformly distributed among all possible values $0, 1, \ldots, 100$. First the code to do it: x Now the results: The figures look appropriate. The left hand histogram in particular provides a reference for any other attempted solution: if it does not produce the same distribution of partial sums (up to sampling error), then that solution is not generating uniformly distributed partitions. By the way, although this code is not well-suited for R implementation, it has a reasonable speed, taking about $N\times k / 50000$ seconds on this machine (two seconds for the example). That would be three to four minutes to solve the problem posed in the question with $N=10^6$ iterations and $k=10$. Because these partitions can be generated one at a time, there is no RAM limitation. R code # # Generate a random partition of vector `x` into `k`>0 (possibly empty) pieces # and return the vector of k partial sums of `x` within those pieces as well as # the k-1 cutpoints between them. (Each partial sum starts *after* the # preceding cut and ends at the current cut.) # # Output is a list with the sums and the corresponding cutpoints. # rpart
