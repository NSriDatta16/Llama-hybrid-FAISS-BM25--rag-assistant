[site]: crossvalidated
[post_id]: 369104
[parent_id]: 
[tags]: 
What is a sensible order for parameter tuning in neural networks?

There are so many aspects one could possibly change in a deep neural network that it is generally not feasible to do a grid search over all of them (e.g. activation function, layer type, number of neurons, number of layers, optimizer type, optimizer hyperparameters, etc.). It may not even be desirable if it were possible, as it amounts to comparing a huge number of models, which makes me think the 'best' the out-of-sample performance estimate could just be incidental. From what I understand it is therefore common to adjust the network in a greedy fashion, updating one or several out of many components at a time. Hence, when tuning the various components of a neural network by hand, what is considered to be a sensible order? (For example, changing the activation function at a late stage is maybe not a good idea .) This question also appears to be related, but the accepted answer is not really about the order of optimizing hyperparameters, but rather how to train a predefined set of models with different architectures.
