[site]: datascience
[post_id]: 77691
[parent_id]: 
[tags]: 
How to measure deviance resulting from different random seeds in machine learning?

I'm running an xgboost model to predict probabilities to a binary classification problem. Then I aggregate the results based on the Age variable (what is the aggregated risk of getting the sickness for Age x). I made a mistake and did not set the seed number to be constant so when I rerun the model I get slightly different aggregated results. Could you give me a reference why it is not substantial? So I can avoid running the model on several seed settings to get a confidence interval? I would not want to do it because the learning process takes a few hours. Thank you in advance!
