[site]: datascience
[post_id]: 35500
[parent_id]: 
[tags]: 
How is Importance-Sampling Used in Off-Policy Monte Carlo Prediction?

In the section, "Off-Policy Prediction via Importance Sampling", found in the chapter on monte carlo methods of the second edition of Sutton and Barto's, "Reinforcement Learning: An Introduction", the importance-sampling ratio $\rho_{t:T-1}$ of a sequence of action-state transitions $S_t, A_t, S_{t+1}, A_{t+1}, ..., A_{T-1}, S_T$ (where $T$ is the final time step, that is, $S_T$ is the terminal state of the episodic task) is defined as the ratio of the probability of that particular sequence of state-action transitions occurring under the target policy $\pi$ to the probability of that sequence occurring under the behavior policy $b$: where $p$ is the state-transition probability. The author introduces the importance-sampling ratio as a means of obtaining the state-value function of the target policy $v_{\pi}(s)$ from an episode generated according to $b$, as explained in the following excerpt from the text. My question is this: how exactly does multiplying the return under the behavior policy by the importance-sampling ratio in equation 5.4 produce the correct expectation for the state value function of the target policy? It is unclear to me how the ratio "transforms the returns to have the right expected value".
