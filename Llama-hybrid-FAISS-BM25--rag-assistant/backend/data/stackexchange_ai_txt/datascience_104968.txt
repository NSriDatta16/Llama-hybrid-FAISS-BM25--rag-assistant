[site]: datascience
[post_id]: 104968
[parent_id]: 84338
[tags]: 
I contend that this is a feature, not a bug. Going into the classification, not knowing the values of $x_1$ or $x_2$ , it is much more likely that your point belongs to $+1$ than $-1$ . Consequently, you shouldn’t just need decent evidence that a point is $-1$ . You should need overwhelming evidence. The red $+1$ group, loosely speaking, exists in the square $[-10,10]\times[-10,10]$ . The closest blue $-1$ point is at about $(12,15)$ , which is not all that far from the $+1$ zone. The decision boundary is telling you that $(12,15)$ is not sufficiently far from the $+1$ zone to overcome the high “prior” probability of being $+1$ . To get sufficiently far from the $+1$ zone not to be classified as $+1$ , you need to be above about $(15,17)$ . If you simulate $100$ and then $200$ and then $500$ and then $1000$ blue $-1$ points to go along with that same $1000$ red $+1$ points, you will see the decision boundary drift towards where you would expect it to be in between the two groups. You can do more with this idea of “prior” (and “posterior”) probability if you use a logistic regression to predict class membership probabilities. While this might warrant a new question, it might be more in line with the “mathematical” explanation that you want.
