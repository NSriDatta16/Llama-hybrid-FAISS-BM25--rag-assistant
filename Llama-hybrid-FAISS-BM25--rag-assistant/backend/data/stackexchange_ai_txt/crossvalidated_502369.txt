[site]: crossvalidated
[post_id]: 502369
[parent_id]: 
[tags]: 
Building the decoder of an autoencoder with same dimensions or parameters as the encoder?

I build an variational autoencoder from the example given on Keras and changed the layers. The resulting decoder has much more parameters than the encoder. For the decoder I mirrored the conv-Layers with conv_transpose-Layers that have the same parameters as the encoder conv-Layers. So my question is if I want to build an decoder do I mirror the dimensions of the encoder or do I mirror the encoder conv-Layers with conv_transpose-Layers in the decoder that have the same parameters as the conv-Layers? The model summary: Encoder: Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 64, 64, 3)] 0 __________________________________________________________________________________________________ conv2d (Conv2D) (None, 32, 32, 32) 1568 input_1[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 32, 32, 32) 0 conv2d[0][0] __________________________________________________________________________________________________ batch_normalization (BatchNorma (None, 32, 32, 32) 128 activation[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 16, 16, 32) 16416 batch_normalization[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 16, 16, 32) 0 conv2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 16, 16, 32) 128 activation_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 32) 9248 batch_normalization_1[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 16, 16, 32) 0 conv2d_2[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 16, 16, 32) 128 activation_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 8, 8, 64) 32832 batch_normalization_2[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 8, 8, 64) 0 conv2d_3[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 8, 8, 64) 256 activation_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 8, 8, 64) 36928 batch_normalization_3[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 8, 8, 64) 0 conv2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 8, 8, 64) 256 activation_4[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 8, 64) 36928 batch_normalization_4[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 8, 8, 64) 0 conv2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 8, 8, 64) 256 activation_5[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 32) 18464 batch_normalization_5[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 8, 8, 32) 0 conv2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 8, 8, 32) 128 activation_6[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 1, 1, 512) 1049088 batch_normalization_6[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 1, 1, 512) 0 conv2d_7[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 1, 1, 512) 2048 activation_7[0][0] __________________________________________________________________________________________________ flatten (Flatten) (None, 512) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ z_mean (Dense) (None, 512) 262656 flatten[0][0] __________________________________________________________________________________________________ z_log_var (Dense) (None, 512) 262656 flatten[0][0] __________________________________________________________________________________________________ sampling (Sampling) (None, 512) 0 z_mean[0][0] z_log_var[0][0] ================================================================================================== Total params: 1,730,112 Trainable params: 1,728,448 Non-trainable params: 1,664 __________________________________________________________________________________________________ Decoder: __________________________________________________________________________________________________ Model: "decoder" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 512)] 0 _________________________________________________________________ reshape (Reshape) (None, 1, 1, 512) 0 _________________________________________________________________ conv2d_transpose (Conv2DTran (None, 8, 8, 512) 16777728 _________________________________________________________________ batch_normalization_8 (Batch (None, 8, 8, 512) 2048 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 8, 8, 32) 147488 _________________________________________________________________ batch_normalization_9 (Batch (None, 8, 8, 32) 128 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 8, 8, 64) 18496 _________________________________________________________________ batch_normalization_10 (Batc (None, 8, 8, 64) 256 _________________________________________________________________ conv2d_transpose_3 (Conv2DTr (None, 8, 8, 64) 36928 _________________________________________________________________ batch_normalization_11 (Batc (None, 8, 8, 64) 256 _________________________________________________________________ conv2d_transpose_4 (Conv2DTr (None, 16, 16, 64) 65600 _________________________________________________________________ batch_normalization_12 (Batc (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_transpose_5 (Conv2DTr (None, 16, 16, 32) 18464 _________________________________________________________________ batch_normalization_13 (Batc (None, 16, 16, 32) 128 _________________________________________________________________ conv2d_transpose_6 (Conv2DTr (None, 32, 32, 32) 16416 _________________________________________________________________ batch_normalization_14 (Batc (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_transpose_7 (Conv2DTr (None, 64, 64, 32) 16416 _________________________________________________________________ batch_normalization_15 (Batc (None, 64, 64, 32) 128 _________________________________________________________________ conv2d_transpose_8 (Conv2DTr (None, 64, 64, 3) 867 ================================================================= Total params: 17,101,731 Trainable params: 17,100,067 Non-trainable params: 1,664 _________________________________________________________________ ````
