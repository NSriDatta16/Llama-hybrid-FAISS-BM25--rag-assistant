[site]: crossvalidated
[post_id]: 381306
[parent_id]: 373858
[tags]: 
I think the KL divergence term keeps the problem well-defined. Intuitively, you can think of it as a "coding cost", where specifying a very narrow posterior distribution is expensive. Consider the case where you have a single datapoint $x = 0$ , and your latent space is 1D with the standard VAE prior $\mathcal{N}(0,1)$ . One possible variational posterior would then have $\mu(x) = 0$ and $\sigma(x) = \sigma^*$ for some unknown optimal value of $\sigma^*$ . Then the decoder would simply be the identity function. The loss would be $$ \begin{align*} E_{z \sim \mathcal{N}(0, \sigma^*)} [\log P(0\mid z)] - \mathcal{D}_{KL}(Q(z\mid X)\parallel P(z)) &= \lambda (\sigma^*)^2 - \left( \log \frac{1}{\sigma^*} + \frac{(\sigma^*)^2}{2} - \frac{1}{2} \right) +c\\ &= \lambda'(\sigma^*)^2 - \log \sigma^* + c' \\ 2\lambda' \sigma^*-\frac{1}{\sigma^*} &= 0 \\ \sigma^* &= (2\lambda')^{-\frac{1}{2}} \end{align*} $$ where $\lambda$ is proportional to the precision of $P(X|z)$ and $\lambda' = \lambda - \frac{1}{2}$ . As long as $\lambda > \frac{1}{2}$ , then the KL term prevents the posterior from collapsing.
