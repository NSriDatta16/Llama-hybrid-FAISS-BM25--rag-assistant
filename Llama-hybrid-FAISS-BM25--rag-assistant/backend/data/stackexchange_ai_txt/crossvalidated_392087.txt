[site]: crossvalidated
[post_id]: 392087
[parent_id]: 
[tags]: 
What are conditions to apply the "transpose trick" in PCA?

The "transpose trick" in principal component analysis (PCA) involves replacing the computation of $\mathrm{eig}(A^TA)$ by $\mathrm{eig}(AA^T)$ , as it is known that if $v$ is an eigenvector of $AA^T$ , then $A^Tv$ is an eigenvector of $A^TA$ - compare https://stats.stackexchange.com/a/7144/77888 . However, I fail to see under which conditions that trick actually applies to PCA, as PCA "usually" involves "a normalization step of the initial data" that "consists of mean centering" ( Wikipedia ). The Stats link above simply says that for simplicity, suppose that the columns of A have already been normalized to have zero mean, while Fundamentals of Statistics says nothing about that whatsoever - despite explicitly stating that the PCA is based on the solution of the eigenvalue problem for the covariance matrix and stating that in the covariance matrix, the mean of each variable is subtracted before multiplication. So, what conditions need to be fulfilled for the transpose trick to work be directly applicable in PCA of a generally non-normalized matrix? That is, when is $\mathrm{eig}(\mathrm{cov}(A))$ related to $\mathrm{eig}(\mathrm{cov}(A^T))$ ? My motivation for this question stems from the fact that I have seen someone use [v,~]=eig(cov(data')); v(:,end) (instead of [v,~]=eig(cov(data)); data * v(:, end) ) in MATLAB to project the data onto the principal component associated with the largest eigenvalue, and I have the strong feeling that this substitution is valid (meaning, I get the same result up to some factor) only if the data has zero-mean rows and columns [because both data and data' are used in cov ].
