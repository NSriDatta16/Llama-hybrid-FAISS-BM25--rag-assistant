[site]: crossvalidated
[post_id]: 468811
[parent_id]: 468807
[tags]: 
You don't exactly know $\omega$ but you have some idea, a distribution based on the previous data you've seen, which is described by $p(\omega|X,Y)$ . If you had a constant $\omega_0$ , the posterior predictive distribution would be $p(y^*|x^*,\omega_0)$ , but the integral is basically an expected value (i.e. a weighted average) over all possible $\omega$ . By the way, the integral is at the same time comes from total probability law: $$p(y^*|x^*,X,Y)=\int \underbrace{p(y^*|x^*,\omega,X,Y)p(\omega|X,Y)}_{p(y,\omega|x^*,X,Y)}d\omega=\int p(y^*|x^*,\omega)p(\omega|X,Y)d\omega$$ The first term inside the integral is simplified as $p(y^*|x^*,\omega,X,Y)=p(y^*|x^*,\omega)$ , because when you actually know the model parameters, you don't need the training data to learn them. So, given the input $x^*$ , the output $y^*$ is assumed to be dependent on only the model parameters, $\omega$ .
