[site]: crossvalidated
[post_id]: 181705
[parent_id]: 
[tags]: 
"representative sampling" from a distribution

I'm drawing samples from a distribution to train a machine learning classifier (training it via mini-batches of 32 samples at a time). It's just a toy dataset, so I know that the samples are coming from a multivariate Gaussian with an identity co-variance matrix. Since the quantile function (inverse CDF) is tractable for such a distribution, I could evenly space the samples across the domain (0,1) of this function (e.g. points of the unit square in the 2D case) and presumably get a mini-batch of sample the encompass the entire distribution. Obviously I'd have to add a random offset to this grid of samples such that every mini-batch isn't identical, but otherwise it seems like this should speed up learning alot. My question is: is this approach reasonable or is there something I'm missing? And if it is, then is there name for it or literature around it? Thanks!
