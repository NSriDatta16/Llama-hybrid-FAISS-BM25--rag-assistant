[site]: stackoverflow
[post_id]: 1890798
[parent_id]: 1880109
[tags]: 
I run a BizTalk system that has similar problems and can empathize with what you are seeing. I don't know if it's the same, but I thought I'd share my experience in case. In the same manner restarting the send/receive seems to fix the problem. In my case I found a direct correlation to memory usage by the host processes. I used performance counters to see when a given host was throttled for memory. By creating extra hosts, and moving orchestrations and ports between them I was able to narrow down which business sets were causing the problem. Basically in my case restarting the hosts was the equivalent to the ultimate "garbage collection" to free up memory. This was of course until enough instances came through to gobble it up again. I'm afraid I have not solved the issue yet, but a few things I found to alleviate the issue: Raise the memory to a given process so that throttling does not occur or occurs later Each host instance, while informative, does have an overhead that is added. Try combining hosts that are not your problem children together to reduce the memory foot print. Throw hardware at the problem, ram is cheap I measure the following every few minutes in perfmon so I can diagnose where the problem is: BizTalk:MessageAgent(*)\Process memory usage (MB) BizTalk:MessageAgent(*)\Process memory usage threshold Memory\Available MBytes A few other things to take a look at. Make sure any custom pipelines use good BizTalk memory practices (i.e. no XML DOM manipulation hiding somewhere, etc). Also theoretically reducing the number of threads for a given host should lower the amount of memory it can seize at one time. I did not seem to have much luck with this one. Maybe the BizTalk throttling overrode it as others have mentioned, I don't know. Also, on a final note, if you dump the perfmon results to a csv, with Excel you can make some pretty memory usage graphs. These might be useful for talking to management about buying more hardware. That's assuming your issue fits this scenario as well.
