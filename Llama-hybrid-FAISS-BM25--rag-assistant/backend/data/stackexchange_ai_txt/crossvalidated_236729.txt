[site]: crossvalidated
[post_id]: 236729
[parent_id]: 
[tags]: 
Learning an efficient $f$(text) â†’ Beta$(\alpha, \beta)$

Say we want to learn an efficient encoding of a function $f(\text{text})\rightarrow \text{Beta}(\alpha, \beta)$, where text is e.g. an URL. We want to do this for millions of URLs. That is, given a dataset made of millions of $\text{URLs}$ and $f(\text{URL})$'s, we we want to learn an efficient encoding $\hat{f}$ of $f$ that minimizes the error between $f(\text{URL})$ and $\hat{f}(\text{URL})$ (e.g. the KL divergence between the Betas). I believe this is an example of learning embeddings (sparse representation learning). I am guessing variational autoencoders or DNNs could help here, but I am seeking advice in general. What is the literature and/or state of the art in this topic?
