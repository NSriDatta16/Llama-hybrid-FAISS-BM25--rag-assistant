[site]: crossvalidated
[post_id]: 69878
[parent_id]: 
[tags]: 
Identifying potential for missingness in a seasonal time series to bias period-averages

I have high-frequency time series (observations every few minutes) for which I wish to compute daily averages. The data exhibit a strong diel cycle. Sometimes observations are missing in the time series. For example, I could have 50% of the data missing on a given day. The effect of this missingness on the daily average isn't immediately obvious given the proportion of missing data. Evenly spaced missingness has little affect (say that 5-minute and 10-minute data give ~same daily average), whereas if all of the missing data are concentrated around the daily minimum/maximum then the daily mean would be biased. However, the effect of missingness on the daily average not only dependent on the percent of data missing, but also on the evenness of that missingness. Does anyone know of a good approach for characterizing the potential for missingness to bias the daily average? I'm envisioning some sort of index that incorporates the extent and temporal distribution of missing values, and other factors I may not be considering. Thanks for any guidance. The figure and R code below use a simple sine wave to illustrate my thinking: # =================================== # = Series w/ complete observations = # =================================== X
