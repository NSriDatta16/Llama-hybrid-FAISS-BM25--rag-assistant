[site]: datascience
[post_id]: 123862
[parent_id]: 123859
[tags]: 
In addition to the @brewmaster321's answer, in my opinion the hyper-parameters you need to care about are: Learning rate: especially if you use SGD as optimizer, which does not adapt the LR unlike Adam. Usually you want to grid search over a log-space, i.e. [ $10^{-5}$ , $3\cdot 10^{-4}$ , $10^{-3}$ , ...]. Activation function. The most common are ReLU and leaky-relu. There are also some activations that are suggested for a given application and even neural net layer (e.g. tanh for recurrent layers.) Weight decay : used to regularize your NN. As for the LR, you want to search in log-space. #layers and #units/filters . These mainly impact the complexity of your NN. Basically, more layers and units (also called neurons) increase the number of learnable parameters (or weights) making your model bigger, larger (i.e. requires more storage space and memory), and slower (higher latency in inference, and slower training.) Indeed, these could increase the capability of the model to fit more complex data so reaching better performance. But notice that more parameters increase the chance of overfitting, and so you need to regularize more. Also inspecting the learning curves (apart from metrics) may provide you hints about which hyperparameter you need to tune.
