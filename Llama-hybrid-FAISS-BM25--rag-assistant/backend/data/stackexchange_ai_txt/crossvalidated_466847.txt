[site]: crossvalidated
[post_id]: 466847
[parent_id]: 464962
[tags]: 
From Goodfellow, Bengio and Courville's book, Deep Learning: A key insight (Hinton et al., 2012c) involved in dropout is that we can approximate $p_\mathrm{ensemble}$ by evaluating $p(y|\mathbf{x})$ in one model: the model with all units, but with the weights going out of unit $i$ multiplied by the probability of including unit $i$ . The motivation for this modification is to capture the right expected value of the output from that unit. We call this approach the weight scaling inference rule. There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks, but empirically it performs very well. (Emphasis mine)
