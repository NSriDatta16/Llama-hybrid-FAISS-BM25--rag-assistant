[site]: stackoverflow
[post_id]: 1004110
[parent_id]: 
[tags]: 
Pros and Cons of massive table that controls all data flow with stored procs

DBA (with only 2 years of google for training) has created a massive data management table (108 columns and growing) containing all neccessary attribute for any data flow in the system. Well call this table BFT for short. Of these columns: 10 are for meta-data references. 15 are for data source and temporal tracking 1 instance of new/curr columns for textual data 10 instances of new/current/delta/ratio/range columns for multi-value numeric updates :totaling 50 columns. Multi valued numeric updates usually only need 2-5 of the update groups. Batches of 15K-1500K records are loaded into the BFT and processed by stored procs with logic to validate those records shuffle them off to permanent storage in about 30 other tables. In most of the record loads, 50-70 of the columns are empty through out the entire process. I am no database expert, but this model and process seems to smell a little, but I don't know enough to say why, and don't want to complain without being able to offer an alternative. Given this very small insight to the data processing model, does anyone have thoughts or suggestions? Can the database (SQL Server) be trusted to handle records with mostly empty columns efficiently, or does processing in this manner wasted lots of cycles/memory,etc.
