[site]: crossvalidated
[post_id]: 226923
[parent_id]: 
[tags]: 
Why do we use ReLU in neural networks and how do we use it?

Why do we use rectified linear units (ReLU) with neural networks? How does that improve neural network? Why do we say that ReLU is an activation function? Isn't softmax activation function for neural networks? I am guessing that we use both, ReLU and softmax, like this: neuron 1 with softmax output ----> ReLU on the output of neuron 1, which is input of neuron 2 ---> neuron 2 with softmax output --> ... so that the input of neuron 2 is basically ReLU(softmax(x1)). Is this correct?
