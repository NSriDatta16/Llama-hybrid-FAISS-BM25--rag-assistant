[site]: crossvalidated
[post_id]: 545540
[parent_id]: 545536
[tags]: 
For an explanation of why this is happening see GLM for count data with all zeroes in one category . I know I could do a bayesian approach but before I go into that, I would like to know if there's a work-around that allows me to do this analysis using a frequentist I think you could just add 1 to all observations and then get sensible estimates: model summary(model) Call: glm.nb(formula = counts + 1 ~ management, init.theta = 11.87310622, link = log) Deviance Residuals: Min 1Q Median 3Q Max -2.4588 -0.3605 0.0000 0.4648 1.7339 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 5.0752 0.1228 41.330 The rationale would be that zeros are not really a possible outcome and they are just a consequence of the sampling. By adding 1 you reset to a more realistic lower bound. Of course, this is acceptable if adding 1 doesn't skew the dataset too much, which doesn't seem to be the case here. I think the logic is not too dissimilar from a Bayesian approach. I add simulation results to illustrate the effect of adding 1 to the raw counts. Incidentally, I think this also illustrates the tradeoff between bias and variance . I keep the counts for groups A, B, and C constant (I don't think this matters here). For group D I simulate counts from a negative binomial distribution with varying mean and size. Then I fit the glm.nb model with and without adding 1. For simplicity, I made group D to be the intercept. Here's a summary plot of the estimates for group D, note that I reset the estimates below -10 to -10 for ease of visualization: Horizontal lines are the true means. Each box is 500 simulations. When the true mean is very low (about Conversely, with low means, the estimates from raw counts are very unstable between replicates (high variance). After repeating the same experiment you could get considerably different results. When the true mean is above about 5, the effect of adding 1 starts vanishing. I think it's up to the analyst to decide how to proceed but by gut feeling I would say that adding 1 "make sense" here. I think the issue is not so much the numerical stability of the uncorrected estimates but rather whether they are meaningful. Code: library(data.table) library(ggplot2) counts EDIT: I see quite a bit of skepticism about this solution and I'm slightly surprised about it since adding 1 to count data is not unheard of. For example, gene expression data from sequencing technology come as count data. A very respected method of differential expression analysis is limma-voom (Gordon Smyth is a coauthor, my apologies if I'm misquoting). From the paper : The counts are offset away from zero by 0.5 to avoid taking the log of zero, and to reduce the variability of log-cpm for low expression genes. This is exactly what I'm proposing here. Gene expression is never really zero and the difference between a count of 0 and a count of 1 is biologically irrelevant anyway, typically. Is it the same for the OP's case? I don't know but I suspect it is. Does it really matter whether group D has exactly 0 zebras or just 1 or 2 that went undetected in just 6 observations? If the answer is no, then adding 1 is more sensible than producing -Inf coefficients.
