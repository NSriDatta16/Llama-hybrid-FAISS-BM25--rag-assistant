[site]: crossvalidated
[post_id]: 555011
[parent_id]: 
[tags]: 
DL back-propagation: Does the loss necessarily need to be calculated from the direct output from a network?

In deep learning, we typically pass the output of a network we are training along with a training sample (batched or not) into a loss function. We then take the gradient of this loss function with respect to the weights of the network and use the gradient to update the weights and minimize the loss. I don't have the best grasp of the calculations involved in back-propagating this loss, so I am curious as to whether or not we can use values other than the direct output from the network to calculate the loss. Practically, this could look like passing the output of the model and the training sample through another fully trained neural network before calculating the loss. We could take the respective outputs of this network (for the training sample and the output of the first network), calculate the MSE loss between the two outputs (which should be differentiable) and back-propagate the gradient of this loss back to the original network. This intermediate neural network should be differentiable, so we should be able to calculate the gradient with respect to the original neural network's parameters, right? Edit Here's another way to format the question: In a neural network training loop, the model we are training produces a value $x_{predicted}$ that we compare to a label $x_{true}$ . We calculate the gradient and update the parameters by calculating $MSE(x_{predicted}, x_{true})$ since this is differentiable wrt. the parameters of the model. Suppose we have another neural work that takes in input of the same shape as $x_{predicted}$ and $x_{true}$ , called $F()$ . Is $MSE(F(x_{predicted}), F(x_{true}))$ still differentiable wrt. the parameters of our original model?
