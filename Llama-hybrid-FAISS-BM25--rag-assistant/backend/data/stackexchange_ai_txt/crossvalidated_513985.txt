[site]: crossvalidated
[post_id]: 513985
[parent_id]: 
[tags]: 
reducing overfitting does not improve performance

I'm training a multi-class classifier on text documents using a very classic (and somewhat old-fashioned) method on a data set consisting in relatively long text documents (average of 3000 tokens). The 16 classes are somewhat unbalanced. I do: stratified split into train/val/test sets: train and test set have respectively 120'000 and 30'000 examples. usual text preprocessing and cleaning; feature engineering: TF-IDF vectorizer on the 150'000 most frequent 1-2-3grams train a Linear SVM classifier evaluate on test set I observe a significant overfitting: train accuracy = 93%, test accuracy = 73% . In order to reduce the overfitting I add a feature selection step between steps 3 and 4 by selecting the 50'000 best features out of the 150'000 n-grams features using univariate feature selection (see https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection ). The performance is train accuracy = 80%, test accuracy = 73% , on the same train/test split than above. We observe much less overfitting but the accuracy is the same although we should expect an increase of performance. What does this mean? How do you interpret this behavior?
