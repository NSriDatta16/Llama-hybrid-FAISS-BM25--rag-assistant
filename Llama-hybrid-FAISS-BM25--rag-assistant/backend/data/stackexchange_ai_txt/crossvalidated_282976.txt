[site]: crossvalidated
[post_id]: 282976
[parent_id]: 210130
[tags]: 
In addition to your source you could have a look at van Buuren (2012, p. 128) . Here, the author of mice explains some reasons to select a subset of variables in more detail. I don't know a built in option in mice, which selects variables, but you might want to have a look on automatic variable selection methods, e.g. elastic nets and lasso via the glmnet package or random forests via the randomForest package. You would have to do the selection variable by variable and you would have to specify a predictor matrix in mice manually. But with these methods you would not have to select the predictors upon your "subjective belief". Is over-fitting a valid concern? Like you said, most literature suggests to include as many variables as possible. However, I also haven't found any extended discussions about the topic of over-fitting yet (if somebody else knows some good literature, please share).
