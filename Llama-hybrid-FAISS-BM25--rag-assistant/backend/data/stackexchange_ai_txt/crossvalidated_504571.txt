[site]: crossvalidated
[post_id]: 504571
[parent_id]: 
[tags]: 
Are VAE useful for Maximum-Likelihood estimation?

In the "Auto-encoding Variation Bayes" Paper they state under "2.1 Problem Scenarios" that the VAE is a solution to: "1. Efficient approximate ML or MAP estimation for the parameters $\theta$ ". I am wondering if that makes sense. If one would just want to do an ML-Estimate for $\theta$ one would not need the recognition model/decoder $q_\phi$ , right? One could just use the reparametrization trick on the model/encoder network $g_\theta$ , which adds Gaussian noise on its last layer: $$p_\theta(x) = E_{\epsilon}\left[~ \mathcal{N}\left(x ~|~ \mu(g^1_\theta(\epsilon)), \sigma(g^2_\theta(\epsilon)\right) ~\right]$$ where $\mu$ and $\sigma$ are outputs of a neural network. This would basically reduce to doing least squares on a generator network to fit the samples. The VAE now basically also will do this but on top train the approximate posterior over $z$ . Am I right in understanding, that this approximate posterior is the actual subject of interest, since otherwise just doing the above least squauares approach would be much simpler?
