[site]: crossvalidated
[post_id]: 314463
[parent_id]: 
[tags]: 
How to estimate normalization constant during optimization of complex parameterized PDF using MLE?

If I have some data in $\mathbb{R}^N$ space, with $N$ beiling large and I want to estimate the density function of this data, $\mathbf{P}_{data}(x)$, using the fantastic property of neural networks that allows approximation of very complex functions...I will then start with some parameterized formulation $\mathbf{P}_{model}(x|{\theta})$, with ${\theta}$ being the weights in the neural network. It seems of course I could use the traditional MLE formulation to estimate model parameters, ${\theta}^* = \mathrm{argmax}_{\theta}\prod_{i=1}^m \left[\mathbf{P}_{model}(x|{\theta})\right]$. The problem I see though is that MLE traditionally relies on a known PDF being used....satisfying the property that $\int_x\mathbf{P}_{model}=1$. Thus I'm not sure how to do an optimization where the normalizing constant changes throughout the optimization, and/or the integral of $\int_x\mathbf{P}_{model}$ is very hard. I also can't quite see how the normalizing constant could be pulled in to the backprop equation for weight updates when it is seems like a separate constraint (unless this can be formulated as a constrained optimization problem)? Doing some reading, I've seen some hints that importance sampling (or some kind of MCMC method) could be used here, but I've not been able to figure out how to implement it, or if its even tractable on a typical deep learning machine (>= 8 cores, 1 high end GPU, >100 GB RAM, etc). How can I solve this problem...is it even possible?
