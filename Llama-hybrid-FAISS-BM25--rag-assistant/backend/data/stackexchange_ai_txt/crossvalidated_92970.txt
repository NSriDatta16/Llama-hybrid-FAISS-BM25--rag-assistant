[site]: crossvalidated
[post_id]: 92970
[parent_id]: 
[tags]: 
How does the Bayes' theorem equation generalize all sorts of regression/classification models?

I have been reading “ Pattern Recognition & Machine Learning ” written by Christopher M. Bishop for some time, but I am still a beginner. I wish to get a bigger view that summarizes regression and classification models. The book begins with Bayes’ theorem stating that: $$ p(\textbf{w} \mid D) = \frac{p(D \mid \textbf{w})p(\textbf{w})}{p(D)} $$ Where $D$ is the observed data, $w$ is model parameters. From this equation, how does it generalize all sorts of regression or classification models? Specifically, I wish to know how it relates to following model types: Clustering, e.g. k-means clustering Density estimation, e.g. k-nearest neighbours Classification, e.g. Gaussian classifier Regression, e.g. Bayesian linear regression I know each of the technique, but how does the likelihood and prior probability functions in the Bayesian relate to them?
