[site]: crossvalidated
[post_id]: 428227
[parent_id]: 
[tags]: 
Sum R-Squared over uncorrelated features

I am currently developing an automatic approach to eliminate noisy samples from a data set. I clustered all the samples, and then I am iterating over all the clusters, eliminate the samples of the current cluster and try to measure how much the predictive power of the features changes in regard to the target value. After I iterated over all the clusters I pick the cluster that yielded the best performance increase, when eliminated and remove it permanently. Then the cycle starts again... One important detail: Many samples form one 'block', which shall be used to predict the target value. So samples can be removed from each block and the target can still be predicted. Now usually, I would train a model with each eliminated cluster to evaluate the performance, but in my case that would be way too computationally expensive. So what I am doing instead: I perform a PCA over all the remaining samples and then I calculate the R-Squared between each feature and the target value. Finally I sum up all the R-Squared values over all the features and take that as the score. Now I assume I can sum the R-Squared values, because after the PCA there should not be any correlation between the features - at least in theory. My question is: Is there any point I am missing here, or is summing the R-Squared value over uncorrelated features a valid way to estimate predictive power? Is this a fair method to estimate the predictive power of a model, once trained on the data? If not, why? Many thanks in advance!
