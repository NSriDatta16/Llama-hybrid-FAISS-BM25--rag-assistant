[site]: crossvalidated
[post_id]: 343306
[parent_id]: 
[tags]: 
Notation to represent the batch-normalised value of x

What notation is used to represent the normalised or z-score value of a vector $x$? Or, given: $? = \dfrac {x - \mu } {\sqrt{\sigma^2}}$ Where $\mu$ and $\sigma$ are calculated across the whole population of $x$. Is there a canonical, agreed or widely used symbol to replace "$?$"? My specific context is batch normalisation. If the vector $z = Wa + b$ is the biased, weighted sum of a layer in a neural network, what notation would I use to represent that vector after it has been normalised? It may be a little confusing since $z$ is a commonly used variable for this pre-activation weighted sum, and I want the notation to represent the z-score version of $z$.) Is the answer the same in the context of scalars, vectors and matrices? A similar question exists for physics , but I would like to know the answer from the machine learning perspective.
