[site]: crossvalidated
[post_id]: 580579
[parent_id]: 580562
[tags]: 
As you point out, the ELBO is approximated by $\log p(x|z) + \log p(z) - \log q(z|x)$ with reparameterized samples from $q(z|x)$ . The first term, $\log p(x|z)$ , is, in expectation over $q(z|x)$ , often called the likelihood loss ( reconstruction loss is a synonym in the context of VAEs). Such a loss, but without the expectation over $q(z|x)$ , appears also in neural networks without latent variables $z$ , in the form of $\log p(y|x)$ , e.g. in classification/regression. We have freedom over the choice of the modeled distribution $p(x|z)$ , and depending on that distribution get a different loss function. The likelihood loss for a Bernoulli output distribution (per pixel) is sigmoid_cross_entropy_with_logits and for a Gaussian output distribution (per pixel) is the MSE loss. The latter is because (up to an additive and a multiplicative constant) the quadratic error equals the log of the Gaussian likelihood/density. Similarly for other loss functions, also for the Huber loss, you can find a distribution $p(x|z)$ that corresponds to it. It is therefore completely reasonable to use any such loss functions. However, the loss function should fit the output domain. If it's discrete, you shouldn't use a continuous loss function and vice versa. But with the Huber loss for continuous values, you are on the right track. It's also not very elegant to use an infinite range output distribution / corresponding loss if the output is on a finite range like pixel values, but since all models are wrong to some extend anyway, no need to bother I guess, if your library doesn't offer something for finite ranges. Note that if your values are between 0 and 255, it's always possible to normalize them if that is convenient.
