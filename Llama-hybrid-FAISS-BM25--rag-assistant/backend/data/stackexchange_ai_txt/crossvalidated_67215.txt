[site]: crossvalidated
[post_id]: 67215
[parent_id]: 67204
[tags]: 
Suppose you and I are coaching track teams. Our athletes come from the same school, are similar ages, and the same gender (i.e., they're drawn from the same population), but I claim to have discovered a Revolutionary New Training System that will make my team members run much faster than yours. How can I convince you that it really does work? We have a race. Afterward, I sit down and compute the average time for the members of my team and the average time for the members of yours. I'll claim victory if the mean time for my athletes is not only faster than the mean for yours, but the difference is also large compared to the "scatter", or standard deviation, of our results. This is essentially a [$t$-test][1]. We're assuming that the data arises from distributions with specific parameters, in this case a mean and standard deviation. The test estimates those parameters and compares one of them (the mean). It is, consequently, called a parametric test, since we are comparing these parameters. "But Matt", you complain, "this isn't quite fair. Our teams are pretty similar, but you--due to pure chance--ended up with the fastest runner in the district. He's not in the same league as everyone else; he's practically a freak of Nature. He finished 3 minutes before the next-fastest finisher, which reduces your average time a lot, but the rest of the competitors are pretty evenly mixed. Let's look at the finish order instead. If your method really works, the earlier finishers should mostly be from your team, but if it doesn't the finish order should be pretty random. This doesn't give undue weight to your super-star!" This method is essentially the [Mann-Whitney U Test][2] (also called the Wilcoxon Rank Sum Test, Manning-Whitney-Wilcoxon Test, and several other permutations besides!). Note that unlike the $t$-test, we're not assuming that the data comes from specific distributions, nor are we computing any parameters for them. Instead, we're comparing the relative ranks of the data points directly. That's the major distinction--parametric tests model things with distributions and compare the parameters of these distributions; non-parametric tests....don't and operate more directly on the data. As with parametric tests, non-parametric test statistics are also constructed so that the $p$ values are uniformly distributed on [0,1] under the null hypothesis and clustered towards 0 in the presence of an effect. You would report and interpret them just like the results of a parametric test. I'm not sure about the relative popularity of parametric and non-parametric methods. Some non-parametric methods (e.g., histograms!) are in nearly universal use; others might be over or under-used. I suspect that the Mann-Whitney U Test ought to be used more, rather than less frequently. It's about as efficient as a $t$-test on normally distributed data and actually does better than the $t$-test on sufficiently non-normal data. It's also fairly robust to outliers. Plus, you can use it on ordinal data too (e.g., finish order rather than just finish time), which makes it more broadly applicable than a $t$-test.
