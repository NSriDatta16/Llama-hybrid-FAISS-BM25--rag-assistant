[site]: datascience
[post_id]: 113947
[parent_id]: 66703
[tags]: 
A 1-hidden-layer neural network is a universal approximator, but adding more layers can give a more efficient representation. A simple example: suppose you want to match the pattern ‘((A or B or C) and (D or E or F)) or G’ using simple on-off neurons. Using 2 hidden layers: neuron 1 in the first hidden layer (h1_1) recognizes (A or B or C), neuron 2 in the first hidden layer (h1_2) recognizes (D or E or F); then in the second layer h2_1 recognizes the simultaneous activation of h1_1 and h1_2; then the output is active when either h2_1 or G is active. This requires 3 hidden neurons. Using one hidden layer you have to distribute all the possible pattern matches to separate neurons in the first hidden layer. So h1_1 would recognize ‘ADG’, h1_2 would recognize ‘BDG’, etc. and the output neuron is active when any of the hidden neurons is active. This requires 9 hidden neurons. Multilayer networks tend to be more efficient any time you have that sort of nested and-or logic. Image processing is just one of those cases.
