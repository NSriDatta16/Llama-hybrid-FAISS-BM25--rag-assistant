[site]: crossvalidated
[post_id]: 79234
[parent_id]: 
[tags]: 
Training a convolutional neural network

Based on my research on convolution neural networks, every other layer in such a network has a subsampling operation, in which the resolution of the image is reduced so as to improve generalization of the network. So, a CNN could consist of an alternation of convolution and subsampling layers. However, when using backpropagation to train a convolutional neural network, I don't quite understand how one would train a convolutional layer. When you are training a convolution layer, don't you need the weights of the next layer to calculate delta? Based on my understanding of backprop, the equation to find delta of a hidden layer is as follows: Neuron.Delta = Neuron.Output * (1 - Neuron.Output) * ErrorFactor But, to find ErrorFactor, you need the weights of the connections between the current layer and the next one. And if the next layer is a subsampling layer, then there will not be any weights to use to calculate delta. My current solution to this problem is to simply look at the next layer's weights to calculate the delta of the current layer. So, if layer 1 is a convolution layer, layer 2 is a subsampling layer, and layer 3 is a convolution layer, I would look at the weights connecting layers 2 and 3 to calculate the delta at layer 1. Is this a correct understanding of how to train a convolution neural network?
