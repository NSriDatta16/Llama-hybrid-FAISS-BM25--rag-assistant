[site]: crossvalidated
[post_id]: 396083
[parent_id]: 
[tags]: 
How does cross-validation work exactly?

I'm having a hard time figuring out how exactly cross validation works in practice: To do K-fold cross validation on a data set, you divide your data into K sets. Then for each fold $i$ , $1 \leq i \leq K$ , you fit a model $M_i$ , for which you get an out of sample error $MSE_i$ . Now you have a cross validation error: $CV = \frac{1}{K}\sum{MSE_i} $ You repeat this process for different sets of hyper-parameters, chosen using grid search, or Bayesian optimization, or some other suitable method, and go with the set of hyper-parameters that give you the lowest $CV$ . So far it is clear. My understanding is: the models $M_i$ will have the same hyper-parameters, but they won't have the same fitted parameters (coefficients in a linear model, weights in a neural network, etc...). So which model from the $M_i$ models do you actually go with? Or is it the case that once you have chosen you hyper-parameters using CV, you then refit a new model using the entire test data set? If this is the case, isn't there a chance that the new model performs worse than all of the $M_i$ .
