[site]: crossvalidated
[post_id]: 474012
[parent_id]: 351741
[tags]: 
Your NN is not necessarily overfitting. Usually, when it overfits, validation loss goes up as the NN memorizes the train set, your graph is definitely not doing that. The mere difference between train and validation loss could just mean that the validation set is harder or has a different distribution (unseen data). Also, I don't know what the error means, but maybe 0.15 is not a big difference, and it is just a matter of scaling. As a suggestion, you could try a few things that worked for me: Add a small dropout to your NN (start with 0.1, for example); You can add dropout to your RNN, but it is trickier, you have to use the same mask for every step, instead of a random mask for each step; You could experiment with NN size, maybe the answer is not making it smaller, but actually bigger, so your NN can learn more complex functions. To know if it is underfitting or overfitting, try to plot predict vs real; You could do feature selection/engineering -- try to add more features or remove the ones that you might think that are just adding noise; If your NN is simply input -> rnn layers -> output, try adding a few fully connected layers before/after the rNN, and use MISH as an activation function, instead of ReLU; For the optimizer, instead of Adam, try using Ranger. The problem could be the loss function. Maybe your labels are very sparse (a lot of zeros), and the model learns to predict all zeros (sudden drop in the beginning) and cant progress further after that. To solve situations like that you can try different metric, like pos_weight on BCE, dice loss, focal loss, etc. Good luck!
