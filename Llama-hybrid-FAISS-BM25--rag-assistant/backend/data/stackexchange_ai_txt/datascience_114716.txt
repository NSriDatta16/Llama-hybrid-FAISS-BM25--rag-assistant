[site]: datascience
[post_id]: 114716
[parent_id]: 114509
[tags]: 
You have to store training samples for prediction because you need to calculate the similarities between the point for which you make a prediction and the landmarks, $f_i = \mathrm{sim}(x, l^{(i)})$ . This is similar to the $k$ -nearest neighbors algorithm where you predict the value of a test point based on the values of a few points around it. However, in the case of SVM classification, you don't have to store all training samples. You need to store only a small number of those samples that are used to create the boundaries between classes. These samples are called support vectors . The training samples far from the class boundaries are not useful for prediction, and therefore don't need to be stored. More details here: link . I am not sure though, if a similar reduction of stored training data can be made for SVM regresion. In any case, such algorithms like SVM, and kNN are usually not applied to raw samples (such as e.g. raw images, videos, sound waveforms) as in the case of deep learning. They are applied to features which are aggregated functions of raw samples, and take much less storage space than raw samples themselves.
