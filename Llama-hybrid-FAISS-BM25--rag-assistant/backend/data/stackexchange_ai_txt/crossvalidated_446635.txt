[site]: crossvalidated
[post_id]: 446635
[parent_id]: 446412
[tags]: 
isn't it better [...] to simply regenerate new data-sets? Real data from the actual data-generating process of your task is always best. Unfortunately, it is typically also expensive Think of process control in an industrial production: faulty batches will (hopefully) be rare, and the factory will not produce a couple of faulty batches with this and that fault profile for you to get better data. In my field, the pysical samples themselves are often not that expensive (and the particular measurig methods I mostly work with can often be employed in a non-destructive manner) but getting reference data/labels is. We therefore sometimes prepare so-called lab samples that are of a similar but known composition to the real-life samples. You may see this as a physical simulation. It is well known that a method trained (calibrated) with such lab samples needs to be validated on real samples. and maybe even otherwise scarce . Think modeling a rare disease: rare disease implies that there are not many patients that have it. In â‰ˆ15 years of professional experience in chemometrics, I've so far met once someone who told me that for the task they are currently working on they are not limited by too few labeled samples. In my field, being able to simply obtain good independent real data is like a big lottery win. Both resampling and synthesis of data are ways to deal with specific aspects of having too small real data sets. (I'm a bit puzzled by your use of "generating" data sets - generate sounds to me very much like simulated/synthetic data. I measure real data - even if there are experimental designs behind that measurement, so the data is obtained in a rather "deliberate" fashion.) Synthetic data can be useful for testing and/or training. You need to be aware of the meaning of the synthetic data and the limitations of your synthetic data generation process, though. E.g. I do machine learning on chemical data (spectroscopy). I may synthetically introduce noise that simulates variation that may happen in real data to particular "failure/error modes" of the instrumentation, e.g. the excitation laser wavelength of a Raman spectrometer may be unstable, and I can simulate what that would do to the spectrum. Limitation: Nevertheless, the simulated data may have slight but systematic differences to real data subject to such excitation wavelength variation because my simulation necessarily has an interpolation step that doesn't occur in real life. You also need to be clear whether you need that synthetic data for testing only or also for training. Cross validation with synthetic data The most crucial aspect for the correctness of cross validation is that the splits are independent. Synthetic data is not independent of the real sample it was derived from. To avoid data leaking between training and test splits: You can split the data so that all synthetic data derived from a particular real case is in the same split (treating the synthetic data like repeated measurements) If more than one real case is involved with the synthesis this quickly gets messy, and if all cases are involved together in the data synthesis this approach becomes impossible. Also, with this approach synthetic data will be used both for training and testing. You can split the original real data and then derive synthetic data for each training split (if you want to use synthetic data during training) and each testing split (if you want to use synthetic data for testing) you generate the synthetic data.
