[site]: crossvalidated
[post_id]: 265284
[parent_id]: 263284
[tags]: 
There are two matrices, $V$ and $V'$. Both have their first dimension equal to the size of the vocabulary. The vector $v_w$ is the row from $V$ that corresponds to the word from the input. The vector $v'_w$ is a row from $V'$ that corresponds to the output word. When the word "representations" is used in this context it refers to a low-dimensional vector learned as part of the neural network. These learned representations tend to be really useful for all sorts of stuff as opposed to a one-hot encoding representation that is not useful for anything.
