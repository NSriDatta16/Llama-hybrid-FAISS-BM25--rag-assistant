[site]: stackoverflow
[post_id]: 3705320
[parent_id]: 
[tags]: 
fast threshold and bit packing algorithm ( possible improvements ? )

I am working on an algorithm that performs a global thresholding of an 8-bit grayscale image into a 1-bit ( bit packed, such that 1 byte contains 8 pixels ) monochrome image. Each pixel in the Grayscale image can have a luminance value of 0 - 255. My environment is Win32 in Microsoft Visual Studio C++. I am interested in optimizing the algorithm as much as possible out curiosity, The 1-bit image will be turned into a TIFF. Currently I am setting the FillOrder to be MSB2LSB (Most Significant Bit to Least Significant Bit ) just because the TIFF spec suggests this ( it doesn't necessarily need to be MSB2LSB ) Just some background for those who don't know: MSB2LSB orders pixels from left to right in a byte just as pixels are oriented in an image as the X coordinate increases. If you are traversing the Grayscale image from left to right on the X axis this obviously requires you to think "backward" as you are packing the bits in your current byte. With that said, let me show you what I currently have (This is in C, I have not attempted ASM or Compiler Intrinsics yet just because I have very little experience with it, but that would be a possibility ). Because the Monochrome image will have 8 pixels per byte, the Width of the monochrome image will be (grayscaleWidth+7)/8; FYI, I assume my largest image to be 6000 pixels wide: First thing I do (before any image is processed) is 1) calculate a look up table of amounts I need to shift into a specific byte given an X coordinate from my grayscale image: int _shift_lut[6000]; for( int x = 0 ; x With this lookup table I can pack a monochrome bit value into the current byte I am working on with something like: monochrome_pixel |= 1 which ends up being a huge speed increase over doing monochrome_pixel |= 1 The second lookup table I calculate is a Lookup Table that tells me the X index into my monochrome pixel given an X pixel on the Grayscale pixel. This very simple LUT is calculated like such: int xOffsetLut[6000]; int element_size=8; //8 bits for( int x = 0; x This LUT allows me to do things like monochrome_image[ xOffsetLut[ GrayX ] ] = packed_byte; //packed byte contains 8 pixels My Grayscale image is a simple unsigned char* and so is my monochrome image; This is how I initialize the monochrome image: int bitPackedScanlineStride = (grayscaleWidth+7)/8; int bitpackedLength=bitPackedScanlineStride * grayscaleHeight; unsigned char * bitpack_image = new unsigned char[bitpackedLength]; memset(bitpack_image,0,bitpackedLength); Then I call my binarize function like such: binarize( gray_image.DataPtr(), bitpack_image, globalFormThreshold, grayscaleWidth, grayscaleHeight, bitPackedScanlineStride, bitpackedLength, _shift_lut, xOffsetLut); And here is my Binarize function ( as you can see I did some loop unrolling, which may or may not help ). void binarize( unsigned char grayImage[], unsigned char bitPackImage[], int threshold, int grayscaleWidth, int grayscaleHeight, int bitPackedScanlineStride, int bitpackedLength, int shiftLUT[], int xOffsetLUT[] ) { int yoff; int byoff; unsigned char bitpackPel=0; unsigned char pel1=0; unsigned char pel2=0; unsigned char pel3=0; unsigned char pel4=0; unsigned char pel5=0; unsigned char pel6=0; unsigned char pel7=0; unsigned char pel8=0; int checkX=grayscaleWidth; int checkY=grayscaleHeight; for ( int by = 0 ; by I know this algorithm will potentially miss some trailing pixels in each row, but don't worry about that. As you can see for every monochrome byte, I process 8 grayscale pixels. Where you see pel8 For every increment of X I pack a bit into a higher order bit than the previous X so for the first set of 8 pixels in the grayscale image 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 This is what the bits in the byte look like (obviously each numbered bit is just the threshold result of having processed the corresponding numbered pixel but you get the idea) 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 PHEW that should be it. Feel free to have some fun with some nifty bit twiddling tricks that would squeeze more juice out of this algorithm. With compiler optimizations turned on this function takes on average 16 milliseconds on a roughly 5000 x 2200 pixel image on a core 2 duo machine. EDIT: R..'s suggestion was to remove the shift LUT and just use constants which is actually perfectly logical...I have modified the OR'ing of each pixel to be as such: void binarize( unsigned char grayImage[], unsigned char bitPackImage[], int threshold, int grayscaleWidth, int grayscaleHeight, int bitPackedScanlineStride, int bitpackedLength, int shiftLUT[], int xOffsetLUT[] ) { int yoff; int byoff; unsigned char bitpackPel=0; unsigned char pel1=0; unsigned char pel2=0; unsigned char pel3=0; unsigned char pel4=0; unsigned char pel5=0; unsigned char pel6=0; unsigned char pel7=0; unsigned char pel8=0; int checkX=grayscaleWidth-32; int checkY=grayscaleHeight; for ( int by = 0 ; by I am now testing on an Intel Xeon 5670, using (GCC) 4.1.2. Under these specs the hardcoded bitshift are 4 ms slower than using my original LUT algorithm. In the Xeon and GCC, the LUT algorithm takes on average 8.61 ms and the hard coded bitshift takes on average 12.285 ms.
