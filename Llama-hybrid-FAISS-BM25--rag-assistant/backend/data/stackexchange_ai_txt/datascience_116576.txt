[site]: datascience
[post_id]: 116576
[parent_id]: 115940
[tags]: 
Implementations of RL policies (for example, in Stable Baselines) will often have a boolean argument deterministic . If we set this flag to true, then the policy will return the argmax over the distribution, and we would get the optimal actions for each state, even if the policy's distribution over actions doesn't put 100% of probability mass on the optimal choice. Two important reasons that we don't take the argmax action during training, and instead sample a random action from the categorical distribution induced by the softmax: Only the softmax distribution is differentiable. You can't differentiate through an argmax. Without some nondeterminism in the policy, we would fail to explore the MDP.
