[site]: crossvalidated
[post_id]: 103710
[parent_id]: 103625
[tags]: 
I'm stealing this wholesale from the Stan users group. Michael Betancourt provided this really good discussion of identifiability in Bayesian inference, which I believe bears on your request for a contrast of the two statistical schools. The first difference with a Bayesian analysis will be the presence of priors which, even when weak, will constrain the posterior mass for those 4 parameters into a finite neighborhood (otherwise you wouldn't have had a valid prior in the first place). Despite this, you can still have non-identifiability in the sense that the posterior will not converge to a point mass in the limit of infinite data. In a very real sense, however, that doesn't matter because (a) the infinite data limit isn't real anyways and (b) Bayesian inference doesn't report point estimates but rather distributions. In practice such non-identifiability will result in large correlations between the parameters (perhaps even non-convexity) but a proper Bayesian analysis will identify those correlations. Even if you report single parameter marginals you'll get distributions that span the marginal variance rather than the conditional variance at any point (which is what a standard frequentist result would quote, and why identifiability is really important there), and it's really the marginal variance that best encodes the uncertainty regarding a parameter. Simple example: consider a model with parameters $\mu_1$ and $\mu_2$ with likelihood $\mathcal{N}(x | \mu_1 + \mu_2, \sigma)$. No matter how much data you collect, the likelihood will not converge to a point but rather a line $\mu_1 + \mu_2 = 0$. The conditional variance of $\mu_1$ and $\mu_2$ at any point on that line will be really small, despite the fact that the parameters can't really be identified. Bayesian priors constrain the posterior distribution from that line to a long, cigar shaped distribution. Not easily to sample from but at least compact. A good Bayesian analysis will explore the entirety of that cigar, either identifying the correlation between $\mu_1$ and $\mu_2$ or returning the marginal variances that correspond to the projection of the long cigar onto the $\mu_1$ or $\mu_2$ axes, which give a much more faithful summary of the uncertainty in the parameters than the conditional variances.
