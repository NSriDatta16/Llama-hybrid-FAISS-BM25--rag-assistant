[site]: crossvalidated
[post_id]: 307043
[parent_id]: 305148
[tags]: 
@Aksakal gave the answer in comments, entropy is unitless. Still, there are some details to elaborate. First, sometimes units are given as "nats" or "bits", for the cases of use of natural logs/binary logs, respectively. But these are not really measurements units (like meter, kg, ...) in the physical sense, it corresponds more to writing the measurement in decimal or binary number systems. A measurement of length in meters can be written in decimal or binary, that does not really change the unit of measurement of length used. The unit is meter in both cases. Some details: We treat Shannon (discrete) and differential (continuous) entropy separately. $$ \DeclareMathOperator{\E}{\mathbb{E}} H(X) = -\sum_x p(x) \log p(x) = -\E_X \log p(X) $$ where $p$ is the probability mass function of a discrete random variable. Then $$ H_d(X) = -\int f(x) \log f(x) \; dx = -\E_X \log f(X) $$ where $f$ is the probability density function of a continuous random variable. Now, from general principles the unit of measurement of the expectation (mean, average) of a variable (random or not) is the same as the unit of measurement of the variable itself. This leaves us with the unit of measurement of $\log p(x), \log f(x)$ respectively. Again, from general principles (see lognormal distribution, standard-deviation and (physical) units for discussion and references) the arguments of transcendental functions like $\log$ must be unitless. That rises a problem, while $p(x)$ certainly is unitless, since probability is an absolute number, the density $f(x)$ measures probability pr unit of $x$ , so if unit of $x$ is $\text{u}$ , then unit of $f(x)$ is $\text{u}^{-1}$ . So, for the equation defining differential entropy $H_d$ to be dimensionally correct, we must assume the argument to log contains a "hidden" constant with numerical value 1 and unit $\text{u}$ . But the conclusion follows, that both Shannon and differential entropy is unitless. Still, one must remember that differential entropy scales with the unit of measurement of $X$ , as discussed in https://en.wikipedia.org/wiki/Differential_entropy
