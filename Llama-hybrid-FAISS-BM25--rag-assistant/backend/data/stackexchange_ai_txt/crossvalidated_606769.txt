[site]: crossvalidated
[post_id]: 606769
[parent_id]: 
[tags]: 
Regarding the objective of PPO training in instructGPT

The following objective is taken from the paper 'Training language models to follow instructions with human feedback' : which is used to fine-tune the pre-trained language model using Proximal Policy Optimization (PPO). In the original paper , the objective of PPO is as follows: comparing the two objectives we can see the term with beta in equation 2 must be the KL term in equation 5. My question is, why is the KL term in equation 2 computed with respect to (x,y)? Shouldn't it be with respect to $\phi$ which parameterizes the policy $\pi$ ?
