[site]: crossvalidated
[post_id]: 253405
[parent_id]: 
[tags]: 
What are some methods for learning from data in probabilistic graphical models especially Bayesian Networks?

While most books and papers on Probability Graphical Models (PGMs) describe a nice representational method for Bayesian Networks BNs (and/or Dynamic Bayesian Networks DBNs), where a Joint Probability Distribution/Table (JPD)/(JPT) computation is factored into individual Conditional Probability Distributions/Tables (CPDs)/(CPTs), they almost take it for granted to elaborate on the methods for learning these CPDs/CPTs to effectively implement these techniques. Example: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1963499/ specifies "Classification/regression models can be used to learn the parameters for each node in the network." The way I understand so far CPDs/CPTs can either come from domain knowledge of the system or from the data about the system. What I do wonder is how can CPDs/CPTs be learned from data? Such as: What if the data does not have all states equally represented or have missing values for certain states? What if the data is extremely sparse when there are more states possible which have been expressed very rarely or not at all? Example, let's say we want to find P(Vote|State) and have a dataset with multiple features. In this case would counts of known Votes in known States be sufficient for specifying a CPT or are there specific methods to calculate this with greater accuracy? Elaborate literature references are welcome.
