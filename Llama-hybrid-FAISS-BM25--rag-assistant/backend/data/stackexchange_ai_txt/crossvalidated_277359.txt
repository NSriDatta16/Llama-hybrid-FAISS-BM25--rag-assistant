[site]: crossvalidated
[post_id]: 277359
[parent_id]: 277309
[tags]: 
It's a good question. There are similar works in the field of clustering (aka constrained clustering , where you provide "must link" and "cannot link" connections as input to the algorithm), but I'm not aware of anything along these lines for classification using neural networks. As a first attempt, I would probably do something along these lines: First, convert your labels to the format $([true\_labels], [false\_labels])$. For example, if I do animal classification: Image1 , which I know is of a cat, would be labeled $([cat], [])$ Image2 , which I'm sure is not a whale would be $([], [whale])$ I would then adapt the loss function appropriately. For example, standard 0-1 loss could be extended to this case by paying a penalty of 1 for either of the two following scenarios: Either classifying an animal differently than its' true label (classifying Image1 as whale) or classifying an animal to be a something which I know is not the true label (classifying Image2 to be a whale). For example, in the simple case in which we allow a single label for each case (true label and false label), then the regular 0-1 loss, $\sum _{x,y} {1[h(x)\neq y ]} $, will become: $\sum _{x,y=(y_{true},y_{false})} {1[h(x)\neq y_{true} \lor h(x)=y_{false}]} $
