[site]: crossvalidated
[post_id]: 189652
[parent_id]: 
[tags]: 
Is it a good practice to always scale/normalize data for machine learning?

My understanding is that when some features have different ranges in their values (for example, imagine one feature being the age of a person and another one being their salary in USD) will affect negatively algorithms because the feature with bigger values will take more influence, is it a good practice to simply ALWAYS scale/normalize the data? It looks to me that if the values are already similar among then, then normalizing them will have little effect, but if the values are very different normalization will help, however it feels too simple to be true :) Am I missing something? Are there situations/algorithms were actually it is desirable to let some features to deliberately outweigh others?
