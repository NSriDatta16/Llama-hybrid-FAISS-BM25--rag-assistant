[site]: crossvalidated
[post_id]: 438037
[parent_id]: 438026
[tags]: 
Performance on unseen data is more important. However, a wide gap between in-sample and out-of-sample performance would be indicative of overfitting, while poor performance in both would indicate that you have not yet captured the trend. For instance, say that you want accuracy above 90% but get 30% (both out of sample). How do you proceed? If your in-sample performance is 100%, you would focus on variance reduction methods, such as neuron dropout in a neural net, using fewer parameters in a logistic regression, or elastic net regularization. If your in-sample performance is only 31%, then you know that you haven't modeled much of anything yet, and you have to focus on bias reduction. You can always have a high-bias/low-variance model by randomly guessing based on the ratio of categories. (Example: If you have 10,000 photos of dogs and 10,000 photos of cats, randomly guess "dog" and "cat" 50% of the time each. You will have 50% accuracy in-sample and 50% accuracy out-of-sample.) The trick to machine learning is chipping away at the bias without sending the variance sky-high.
