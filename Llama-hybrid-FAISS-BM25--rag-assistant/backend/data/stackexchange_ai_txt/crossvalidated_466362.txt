[site]: crossvalidated
[post_id]: 466362
[parent_id]: 
[tags]: 
When performing validation, should you also tune the number of hidden units and hidden layers?

I understand tuning the learning rate, momentum, batch size, etc. and finding the best set of parameters using the validation set. However, I don't understand when people say that you should also tune the unit of hidden units as well as the number of hidden layers for a neural network. https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/ hyperparameter tuning in neural networks Wouldn't tuning the number of hidden units and layers fundamentally alter the structure of your model/architecture? It is no longer that you are comparing the same model with different parameters for training it. You are comparing model A, model B, etc... Another problem is that you are essentially enlarging your problem. The original problem was to change some parameters of your model and see which performs the best outside of your training data. The new problem becomes, find a neural network model out of the space of all neural networks (as you can just always stack more units and more layers), and pick the best performing one. In my opinion, for a valid comparison, you need to fix your architecture, then tune some parameters associated with how this architecture was trained, instead of coming up with several (usually small amount, out of infinitely many) architectures and then see which one performs the best. Can someone chime in?
