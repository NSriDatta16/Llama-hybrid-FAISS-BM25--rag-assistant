[site]: crossvalidated
[post_id]: 31117
[parent_id]: 30997
[tags]: 
In a great simplification, the point of SVM is to make function of predictors $f(X_i)$ so that $f(X_i) 0$ for B; the core of SVM is that this $f$ is done so that the distance between maximum value of $f$ for A and minimal for B is as large as possible, not counting outliers. So the linear SVM (for which $f=A_iX_i+B$) is a kind of weird linear regression, thus in fact can be just described in means of inter-class correlation minimization (plus minus usual attribute normalisation step) and claimed nothing interesting. However, this is not really the point -- you can swap linear kernel with other, arbitrarily sophisticated method of comparing observations; this way your $f$ can become arbitrary interesting while the margin should protect you from overfitting.
