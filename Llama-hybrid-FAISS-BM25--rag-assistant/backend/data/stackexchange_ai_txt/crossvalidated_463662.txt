[site]: crossvalidated
[post_id]: 463662
[parent_id]: 
[tags]: 
BackPropagation and Flatten layer in CNN

everybody. I'm trying to create CNN(Convolutional Neural Network) without frameworks(such as PyTorch,TensorFlow,Keras and so on) on Python. Who don't know or forgot what is exactly CNN is: To reference : https://en.wikipedia.org/wiki/Convolutional_neural_network In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series. CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "fully-connectedness" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage. And you know there are layers as Conv2D,Max/AveragePooling2D, Flatten and Dense(FC) layers. And I have 2 questions: Should we compute gradients (such as $\frac{\partial J}{\partial A_i}$ , $\frac{\partial J}{\partial Z_i}$ , $\frac{\partial J}{\partial A_{i-1}}$ and so on) in Flatten Layer or not? And if no, then how should I compute $\frac{\partial J}{\partial A_i}$ and $\frac{\partial J}{\partial Z_i}$ of first layer of Conv2D? With $\frac{\partial J}{[\frac{\partial g(A_i)}{\partial x}]}$ or with $\frac{\partial J}{\partial dA_{i+2}}$ (P.S. as you know iteration of BackPropagation is reverse, so I used i+n for denote the previous layer)?
