[site]: crossvalidated
[post_id]: 15248
[parent_id]: 11689
[tags]: 
I will denote by $K$ the number of classes and by $p$ the dimension of the feature space. I assume you use a procedure that can be rephrased as (it is the case of LDA, see conclusion): 1- For all $k=1,\dots,K$ , estimate $\hat{P}_k:\mathbb{R}^p\rightarrow \mathbb{R}$ from a learning set. 2- For all $k=1,\dots,K$ set $$V_k=\{x\in \mathbb{R}^p:\; \forall j=1,\dots,K\;\; \widehat{\mathcal{L}}_{kj}(x)=\log(\hat{P}_k(x)/\hat{P}_{j}(x))\geq 0 \}$$ The almost non trivial point here is that $(V_k)_{k=1,\dots,K}$ forms a partition of $\mathbb{R}^p$ , up to a set null measure (under suitable regularity condition satisfyed in the case of LDA). (i.e. $\bigcup_{k=1,\dots,K} V_k=\mathbb{R}^p$ and $mes (V_i\cap V_j) =0$ for any $i\neq j$ ). In other words: it is not possible (expept in pathologic cases such as $\hat{P}_i=\hat{P}_j$ ) that for two different classes $i,j$ , $x\in V_i$ , and $x\in V_j$ , i.e. $x$ is classified in class $i$ and in class $j$ . Proof: The result is true because you can rewrite: $$V_k=\{x\in \mathbb{R}^p:\; \forall j=1,\dots,K\;\; \log(\hat{P}_k(x))\geq \log (\hat{P}_{j}(x)) \}$$ $$V_k=\{x\in \mathbb{R}^p:\; k=arg\max_{j=1,\dots,K}\;\; \log(\hat{P}_j(x))\}$$ Hence $V_i\cap V_i \subset \{x:\;\hat{P}_j(x)=\hat{P}_i(x)\}$ which should be of null measure The intuition behind this result is that you certainly have two much separating functions ( $K(K-1)/2$ as you noticed in your question) but these are build from $K$ densities only... Conclusion Usual "LDA" belongs to that type of algorithm with $\forall k=1,\dots,K$ , $\hat{P}_k$ gaussian with mean $\hat{\mu}_k$ and covariance $\hat{C}$ estimated in step 1-. As a conclusion, the answer is no (there can't be problem such as the one you state) whatever the number of classes $k$ and the feature space dimension $p$ . A problem that can occur with dimensionality reduction: If you are in a high dimensional space it might be a good idea to reduce the dimension of your problem. If you choose "one subspace" to work with then there is no problem. However, if you choose to reduce the dimension per pairs of class (which is often natural, there is one good space to work with per pairs) this will result in a "separation function" $\widehat{\mathcal{L}}_{kj}(x)$ that cannot be decomposed into $f_k-f_j$ ...
