[site]: crossvalidated
[post_id]: 48572
[parent_id]: 48520
[tags]: 
K-means is not a distance based clustering algorithm . K-means searches for the minimum sum of squares assignment , i.e. it minimizes unnormalized variance (= total_SS ) by assigning points to cluster centers. In order for k-means to converge, you need two conditions: reassigning points reduces the sum of squares recomputing the mean reduces the sum of squares As there is only finite number of combinations, you cannot infinitely reduce this value and the algorithm must converge at some point to a local optimum. Whenever you intend to change the assignment functions, you have the risk of making the algorithm not terminate anymore, like a dog chasing its own tail. Essentially both steps have to agree on the objective function. We do know that the arithmetic mean is the optimum choice with respect to sum of squares . And for the first step, we can just compute $\sum_i (x_i-\mu_{ji})^2$ for each mean $j$ and choose whichever is minimal. Technically, there is no distance computation here . Mathematically, assigning by least sum of squares is equal to assigning by closes squared Euclidean distance, which (if you waste the CPU cycles for computing sqrt ) equals minimal Euclidean distance assignment. So the intuition of assigning each point to the closest mean is correct, but not what the optimization problem does. between_SS probably is the weighted sum of squares between two means, to measure how well cluster centers are separated (note: cluster centers, it does not compare the actual clusters - technically, the cluster Voronoi cell touches the neighbor clusters Voronoi cell). Note that with k-means you can improve the naive clustering quality by increasing k. The quality measured here is a mathematical value, which may not match the users requirements. Iris is actually a quite good example, where k-means often converges to less than satisfactory results, even given the external information that there should be exactly 3 clusters. If you want a distance-based variation of k-means , look at k-medoids . Here convergence is ensured by replacing the mean with the medoid: Each object is assigned to the nearest cluster (by an arbitrary distance measure) The cluster center is updated to the most central object of the cluster, i.e. with the smallest average distance to all others. In each step, the sum of distances reduces; there is a finite number of combinations, therefore the algorithm must terminate at some local minimum.
