[site]: crossvalidated
[post_id]: 643467
[parent_id]: 643459
[tags]: 
Let's consider an example in R : the true time per token is $\mu=1{,}000$ and its standard deviation is $\sigma=150$ (in otherwise unspecified units). I'll generate three million in total because simulations are cheap and precision will be higher: n [1] 60100.43 56909.72 59584.78 59774.39 60773.56 ... We now have a bunch of times it takes to generate 60 tokens. Can we recover the true mean and standard deviation? Your first suggestion is indeed correct: mean(sample)/n #> 999.98 ## Pretty much equal to 1,000 Now, for the standard deviation (or variance, $\sigma^2$ ) you have to be careful: rather than just the sum of all observations this uses the difference of each observation versus a mean. That means that the more observations you pool, the smaller the average difference is going to be, because you're already "pre-averaging" & increasing precision when pooling if you consider this a single observation. We can see this by applying the above calculation naÃ¯vely to the standard deviation: sd(sample)/n #> 19.38 ## Not near 150 at all?? In fact, what we have just calculated is a standard error of the mean : it is the standard deviation of the sampling distribution of $\mu$ (based on the pool size), and isn't solely related to how variable the original measurements were ( $\sigma$ ). Rather, it tells you how precisely you have estimated a mean. As you can see from the formula $\sigma/\sqrt{n}$ it also depends on how many measurements you have (pooled): the more, the closer your estimate of the mean will be to the true mean. We want to know the standard deviation of the original measurements however, not their mean estimate, so we can simply reverse the division of the standard error by the square root of the pool size: sqrt(n)*sd(sample)/n #> 150.09 ## Close enough to 150 This should also help you figure out why you would divide the standard deviation by the square root of the number of observations, and why both are useful in their own way: rather than getting the variance of your original measures (a population characteristic), you then get the variance of a sample statistic of it (how well you've estimated a parameter). In fact, we can calculate the theoretical value of the standard error of the mean (for pools of 60 samples) and compare that to the value we estimated above: sigma/sqrt(n) #> 19.36492 ## Compare to observed value of 19.37738 Also note that a standard deviation won't change as you increase sample size (beyond reduced sampling variability & converging to the true value), because as a population characteristic it is constant. With sufficient samples you can make a standard error become essentially zero, including many of its derived statistics such as confidence intervals and P -values. Finally, all of the above assumes that means and standard deviations are sufficiently appropriate summary statistics of your outcome, i.e. that the distribution isn't too skewed or heavy/light-tailed.
