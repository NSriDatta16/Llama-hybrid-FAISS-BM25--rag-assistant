[site]: datascience
[post_id]: 64519
[parent_id]: 64119
[tags]: 
You basically have two problems on your hand: learning the Q-function learning the policy The Q-functions tells you how rewarding an action $a$ would be in state $s$ . Once you take the action $a$ and transition from state $s$ to $s'$ , you are rewarded $r$ right away. How rewarding it was to take the action $a$ in $s$ is the sum of the received reward $r$ and the Q-value of the state that you end up in, hence you want to minimize $$r + Q(s', a', w) - Q(s, a, w)$$ where $w$ are the parameters of the function approximator and $a'$ the action you would take in the following state $s'$ . This is the TD Error and mimimizing it will learn the Q-function. So, the TD Error is used to learn the Q-function, basically the same way as it is done in plain-vanilla (Deep) Q-Learning. But while in plain-vanilla (Deep) Q-learning you would base the actor's policy on the Q-function entirely (plus some randomness to not get stuck), in the actor-critic context the Q-function is just a means to the end of training the policy itself. For this reason, the learned Q-function then serves as an optimization objective for the policy by learning the policy parameters $u$ such that it outputs the action $a$ which maximizes $Q(s,a,w)$ (see your other question ). The chain rule pops up in that context, because you are not trying to do regression by minimizing some error, but rather you are trying to maximize $Q$ with respect to one of its arguments.
