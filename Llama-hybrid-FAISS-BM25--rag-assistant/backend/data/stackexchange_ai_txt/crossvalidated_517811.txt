[site]: crossvalidated
[post_id]: 517811
[parent_id]: 517799
[tags]: 
Starting at the end. AIC and BIC are heuristics that are applied to prevent overfitting. These heuristics are often used if testing for the effect of adding a feature to a model is not easy. In the case of linear regression one can test the effect of adding a feature using an F-tests. In case of forward regression an F-test is applied. The progressive models are called hierarchical because the larger models contain the smaller parent model, and an extra predictor. KFold and Leave-One-Out are the machine learning equivalents of these tests in a way. Forward selection uses a gready strategy, as in just add the next best predictor. Best subset does an exhaustive search over all combinations. These methods can render different results. If the predictors all have predictive power and are not correlated, then one would expect a similar result for both methods. Where it gets tricky is if the predictors are correlated; i.e. there is multi-collinearity. One of the effects of correlated predictors is an inflation of the confidence interval of the predictors. Adding all predictors might inflate the confidence intervals so much that none of the predictors seem significant. This mechanism could explain your observations. Make a correlation matrix to corroborate this hypothesis. You could test the individual predictors as a post-hoc analysis in a small regression omdel, showing that each predictor has some power. But that the configuration of the best subset has the best predictive power. Note that if one performs lots of tests, then some Bonferroni correction is usually warranted. In case of post-hoc analysis this is less relevant, the post-hoc analysis is more of a side argument. Note that if any predictor is added by either forward or best subset, that means that there is and predictive effect that can not be attributed to coincidence at the alpha level. Also note that there is a difference between a significant effect and a relevant effect. This issue is more relevant for larger datasets; small effects can result in statistically significant results, without actually predicting very much. So, are the predictions good enough? Or do you just want to prove some predictive power?
