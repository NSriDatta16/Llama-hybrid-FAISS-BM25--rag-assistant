[site]: crossvalidated
[post_id]: 571648
[parent_id]: 
[tags]: 
Do value and key of additive attention need to have the same dimension?

For the documented tensorflow-keras implementation of additive attention, it is stated that the input tensors are: query: Query Tensor of shape [batch_size, Tq, dim] . value: Value Tensor of shape [batch_size, Tv, dim] . key: Optional key Tensor of shape [batch_size, Tv, dim] . Now, it is clear that query and value need to have the same dim because they are going to be summed component-wise scores = tf.reduce_sum(tf.tanh(query + key), axis=-1) but I can not see why the key needs to have the same dim that either value or query. Is is just some implementation trick for this concrete library, or has it got a deeper justification?
