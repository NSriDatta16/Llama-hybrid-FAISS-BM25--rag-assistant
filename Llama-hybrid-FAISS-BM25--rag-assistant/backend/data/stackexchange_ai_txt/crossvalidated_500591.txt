[site]: crossvalidated
[post_id]: 500591
[parent_id]: 500489
[tags]: 
Yes, this is possible. Assuming your data is not degenerate and you have enough input variables (or a suitable kernel), your data will be separable and the SVM will be a hard-margin solution. In this case the separating hyperplane lies exactly between pairs of observations (in kernel space). In fact, each of your observations will necessarily be a support vector, and since you know what the support vectors are, you can find the weights simply by solving a linear system. This follows directly from the KKT conditions of the primal problem. Whether this yields something better than random depends on the amount of noise in your data. If labels are 95% random, a single observation does not add a lot of information to your model, and most such "single shot" SVMs will split out wrong labels. However, in ML it is common to assume that the labels are near to noise-free, and then you are likely (but not guaranteed) to perform better than random.
