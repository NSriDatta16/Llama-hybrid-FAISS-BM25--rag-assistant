[site]: crossvalidated
[post_id]: 523715
[parent_id]: 523695
[tags]: 
In this blog post you can find a review of those metrics, it also mentions the weighted metrics that you use. If you look closely, same as in your case, accuracy and weighted recall are equal in their example. They would always be equal by definition as you will see below. Let me use the data example from the blog post linked above. import numpy as np from sklearn import metrics # Constants C="Cat" F="Fish" H="Hen" # True values y_true = [C,C,C,C,C,C, F,F,F,F,F,F,F,F,F,F, H,H,H,H,H,H,H,H,H] # Predicted values y_pred = [C,C,C,C,H,F, C,C,C,C,C,C,H,H,F,F, C,C,C,H,H,H,H,H,H] C = metrics.confusion_matrix(y_true, y_pred) print(C) print(metrics.classification_report(y_true, y_pred, digits=3))) prints the following [[4 1 1] [6 2 2] [3 0 6]] precision recall f1-score support Cat 0.308 0.667 0.421 6 Fish 0.667 0.200 0.308 10 Hen 0.667 0.667 0.667 9 accuracy 0.480 25 macro avg 0.547 0.511 0.465 25 weighted avg 0.581 0.480 0.464 25 Now, let's calculate the quantities by hand. First, notice that in the confusion matrix C the true labels are in rows and predicted ones in columns. Accuracy is simple, we have the true positive counts in diagonal, so we divide them by the total number of samples: np.sum(np.diag(C)) / np.sum(C) # 0.48 Recall that recall is defined as the ratio between true positives and the true labels (size of the class), i.e. np.diag(C) / np.sum(C, axis=1) # array([0.66666667, 0.2 , 0.66666667]) If you look at the scikit-learn's documentation , weighted recall uses a weighted average weighting the per class recall scores by the size of the class i.e. you calculate something like this np.sum(np.diag(C) / np.sum(C, axis=1) * np.sum(C, axis=1)) / np.sum(C, axis=1).sum() # 0.48 Did you notice something fancy about the calculation? There's this part / np.sum(C, axis=1) * np.sum(C, axis=1) that cancels out. We divide by the class size to calculate recall per class, then we multiply by the class size to weigh the results. Also np.sum(C, axis=1).sum() can be reduced to np.sum(C) , so we can simplify and rewrite the whole thing to np.sum(np.diag(C)) / np.sum(C) # 0.48 Does it look familiar? This is accuracy. TL;DR as mentioned in the linked blog post , using the micro-F1, micro-precision, and micro-recall does not make much sense, since they are equal to accuracy. The same applies to weighting recall by the class size, as it is just an unnecessary complicated way of calculating the accuracy.
