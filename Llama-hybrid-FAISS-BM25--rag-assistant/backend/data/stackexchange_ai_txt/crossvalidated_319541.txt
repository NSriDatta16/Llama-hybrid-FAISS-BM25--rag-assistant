[site]: crossvalidated
[post_id]: 319541
[parent_id]: 318724
[tags]: 
The general solution to your problem is Maximum Likelihood Estimation (MLE) of your parameters $\theta$. Once they are obtained as $\hat{\theta}$, you substitute them into your pdf for the unknown parameters, i.e. you estimate the pdf of your random variable as $\hat{f}(x_i) = f(x_i|\hat{\theta})$. This allows you to construct the the predictive distribution of your Cauchy Random Variable. For the univariate case , this paper is an excellent resource . For the univariate Cauchy with center $\mu$ and scale $\sigma$, one has a closed form if you have $3-4$ observations. If you have $n>4$ observations, the MLE exists$^{\ast}$. If you have $n$ observations, you will have to solve two equations that are easily derived by setting the first derivative of the log-likelihood to zero, see here for their exact form. (In their notation, $x_0 = \mu$ and $\sigma = \gamma$.) Solving this problem numerically has an implementation in the R language, see here . For the multivariate case , all you need to note is that the multivariate Cauchy distribution is simply a multivariate $t$-distribution where the degree of freedom parameter is set to $1$, as was already pointed out in the comments. For the multivarate-$t$, you can do MLE inference as explained excellently in this answer , which is based on the paper that eric_kernfeld has pointed out. I did not find ready-to-roll implementation for this algorithm, but as you will see when you take a look at the supplied answer in the post, it should really easy to implement it yourself. Difference to Bayesian prediction : In the Bayesian setting, you would put a prior on the parameters $\mu$ and $\sigma$, modelling your uncertainty about them as a random variable. Thus, you will get posterior distributions for both parameters, which indicate the relative certainty you have about them given your data. If you have the posterior $q(\mu, \sigma|x_1,\dots,x_n)$, you then obtain your predictive distribution as $\int f(x|\mu, \sigma)q(\mu, \sigma|x_1,\dots,x_n)d\mu d\sigma$, integrating out your uncertainty. In contrast, the MLE-setting will give you point estimates of $\mu$ and $\sigma$ that you plug into your pdf's functional form. Equivalently, you could say that MLE leads to a posterior with point mass $1$ at the tuple $(\hat{\mu}, \hat{\sigma})$ and $0$ probability at any other value. Thus, you ignore all parameter uncertainty in this case, and you rely on the fact that $\hat{\theta}$ is asymptotically equivalent to $\theta$, meaning that $\hat{f}(x) \to f(x)$ (uniformly over $x$). $^\ast$Well, that is unless for the exotic case where $n$ is even and $n/2$ of your observations take value $x_1$ while the other half takes value $x_2$, which happens with probability zero because the Cauchy distribution is continuous.
