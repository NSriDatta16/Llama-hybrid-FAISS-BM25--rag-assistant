[site]: crossvalidated
[post_id]: 457645
[parent_id]: 
[tags]: 
why not use mixed activation functions in a single layer of a neural network?

I'm playing about with writing a pseudo programming language for neural networks - looking for the minimal number of layers to achieve a given task (working up from simple logic and arithmetic) Something I've quickly come across is that functions like swish or ReLU are great for concise arithmetic, whilst sigmoids are great for logic. It's much more efficient in many cases to be doing these both at once - i.e. to have multiple different activation functions in a single layer. Is this done? And if not why not? My experience with ML is low (I try to do most things with pencil and paper) so I may just be unaware, but asking around some friends who work with ML (mostly in physics/astro) they haven't seen it either.
