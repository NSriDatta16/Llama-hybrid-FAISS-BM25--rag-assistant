[site]: datascience
[post_id]: 64608
[parent_id]: 
[tags]: 
k-fold cross validation with RNNs

is it a good idea to use k-fold cross-validation in the recurrent neural network (RNN) to alleviate overfitting? A potential solution could be L2 / Dropout Regularization but it might kill RNN performance as discussed here . This solution can affect the ability of RNNs to learn and retain information for longer time. My dataset is strictly based on time series i.e auto-correlated with time and depends on the order of events . With standard k-fold cross-validation, it leaves out some part of the data, trains the model on the rest while deteriorating the time-series order. What can be an alternate solution?
