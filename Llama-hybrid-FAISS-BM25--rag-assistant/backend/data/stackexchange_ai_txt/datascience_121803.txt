[site]: datascience
[post_id]: 121803
[parent_id]: 
[tags]: 
How to properly visualize high-dimensional embeddings along with the decision boundary in 2-D?

I have a number of embeddings (300-dimensional FastText vectors for each instance of each class) that I apply a classifier to (Logistic Regression for now). I want to visualize the embeddings as well as the decision boundary as part of model debugging so I can see which classes are not linearly separable, which instances are misclassified etc. I'm not sure if using PCA or K-PCA is a good idea here. I'm looking for a procedure that will maintain the same structure (if two instances are close in the higher dimension they should still be so in the 2-D one) while making sure that the decision boundary is still correct. How should I go about this? Thanks.
