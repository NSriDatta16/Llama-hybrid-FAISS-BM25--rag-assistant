[site]: datascience
[post_id]: 83987
[parent_id]: 83984
[tags]: 
There is no "Right" answer to this question , but you should take in mind the following guidelines: Embedding layer is a compression of the input, when the layer is smaller , you compress more and lose more data. When the layer is bigger you compress less and potentially overfit your input dataset to this layer making it useless. The larger vocabulary you have you want better representation of it - make the layer larger. If you have very sparse documents relatively to the vocabulary, then you want to "get rid" of unnecessary and noisy words - you should compress more - make the embedding smaller.
