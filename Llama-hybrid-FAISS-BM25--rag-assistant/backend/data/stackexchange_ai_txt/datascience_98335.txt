[site]: datascience
[post_id]: 98335
[parent_id]: 
[tags]: 
Deep Learning book - trying to understand Bernoulli formulas

In the section 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions of The Deep Learning Book there is a section: (z is defined as $z=w^Th+b$ and $\hat{y}=\sigma(z)$ ) Why did they start with yz and how exactly did they jump from (6.22) to (6.23)? Also in the next paragraph they return back to "The loss function for maximum learning of a Bernoulli parametrized by a sigmoid is": In the first part we dropped dependence on x. Now x is back, but P(y | x) formula is the same as P(y) . How is this possible?
