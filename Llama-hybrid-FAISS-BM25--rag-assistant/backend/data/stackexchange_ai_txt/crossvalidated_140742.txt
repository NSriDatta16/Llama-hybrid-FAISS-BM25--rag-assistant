[site]: crossvalidated
[post_id]: 140742
[parent_id]: 139528
[tags]: 
Please don't hesitate to point it out if I am wrong. First, I have so say, in the second fit, you call glm in a wrong way! To fit a logistic regression by glm , the response should be (binary) categorical variable, but you use p , a numeric variable! I have to say warning is just too gentle to let users know their mistakes... And, as you might expect, you get similar estimates of coefficients by the two fits just by COINCIDENCE. If you replace logit.p with logit.p , ie, changing the variance of the error term from 0.2 to 0.7 , then the results of the two fits will be greatly different, although the second fit ( glm ) is meaningless at all... Logistic regression is used for (binary) classification, so you should have categorical response, as is stated above. For example, the observations of the response should be a series of "success" or "failure", rather than a series of "probability (frequency)" as in your data. For a given categorical data set, you can calculate only one overall frequency for "response=success" or "response=failure", rather than a series. In the data you generate, there is no categorical variable at all, so it is impossible to apply logistic regression. Now you can see, although they have similar appearance, logit-linear regression (as you call it) is just an ordinary linear REGRESSION problem (ie, response is a numeric variable) using transformed response (just like sqr or sqrt transformation), and logistic regression is a CLASSIFICATION problem (ie, response is a categorical variable; don't get confused by the word "regression" in "logistic regression"). Typically, linear regression is fitted through Ordinary Least Squares (OLS), which minimizes the square loss for regression problem; logistic regression is fitted through Maximum Likelihood Estimate (MLE), which minimizes log-loss for classification problem. Here is a reference on loss functions Loss Function, Deva Ramanan. In the first example, you regard p as the response, and fit a ordinary linear regression model through OLS; in the second example, you tell R that you are fitting a logistic regression model by family=binomial , so R fit the model by MLE. As you can see, in the first model, you get t-test and F-test, which are classical outputs of OLS fit for linear regression. In the second model, the significance test of coefficient is based on z instead of t , which is the classical output of MLE fit of logistic regression.
