[site]: crossvalidated
[post_id]: 616795
[parent_id]: 616762
[tags]: 
I did not know how bayesian neural networks perform this operation prior to this answer, however I think the equation you mention is not specific to Bayesian neural networks but is indeed a generic Bayesian model averaging operation. You can refer to Fragoso and Neto 2018 for an introduction and review of bayesian model averaging techniques. The equation you state corresponds to equation (6) from this review (albeit with a, more generic, continuous formulation instead of a discrete one), and computes the marginal posterior distribution of a model output across all models. In your case the chosen output is the model's final prediction $y^*$ , but you could perform this operation on different outputs (e.g intermediate embedding layers of a neural network) It can indeed be termed inference (in the machine learning sense) since you are making a prediction on unseen data based on a set of previously trained models. Regarding the Bayesian neural network, after a quick glance over the Jospin et al 2022 paper you mentioned I think the two following quotes answers your question: A BNN is defined slightly differently across the literature, but a commonly agreed definition is that a BNN is a stochastic artificial neural network trained using Bayesian inference. Stochastic neural networks are a type of ANN built by introducing stochastic components into the network. This is performed by giving the network either a stochastic activation (Figure 3b) or stochastic weights (Figure 3c) to simulate multiple possible models θ with their associated probability distribution p(θ). Thus, BNNs can be considered a special case of ensemble learning. Thus the bayesian averaging performed for bayesian neural networks corresponds to averaging (e.g through MCMC) over the stochastic parameters of your network (the $w$ from your equation corresponds to activation functions or weights values). During training, instead of fixing precise values for your weights, you learn the parameters of the chosen distributions for the activation functions and weights (e.g expectation and variance), which results in an ensemble of possible neural networks. At inference time you sample this ensemble and perform bayesian averaging.
