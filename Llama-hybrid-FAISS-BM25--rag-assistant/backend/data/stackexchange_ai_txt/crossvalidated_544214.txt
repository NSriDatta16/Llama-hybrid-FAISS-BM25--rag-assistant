[site]: crossvalidated
[post_id]: 544214
[parent_id]: 543417
[tags]: 
Your case, in which you explicitly feed the network with the previous value, is a little bit different than the usual case, i'll explain about the general case later. First to your case: Let's ignore for a moment the batch size used for training. After training a model, you can predict with batch size of 1 i.e. the next value. If you want to predict the value of $y_t$ you need to feed your LSTM with the features $x_t$ , in your case $x_t = y_{t-1}$ . So you can imagine having a loop, each time you save the previous prediction and then feed it to the LSTM (along with other features if you have them). If you want prediction for 14 days, you will have to run it day over day. Now regarding batch size. First you need to distinguish between 2 parameters, batch_size and num_steps which is how many time steps you train together in order to predict the next value. What you need, in your case, is batch_size = 1 & num_steps = 1. In the general case, LSTM will feed the last value $y_{t-1}$ for you automatically. This is how it "remembers" the past (or more accurately it will feed the last hidden state, $H_{t-1}$ , which represents the previous values $y_{t-1}$ (and recursively also older $t$ 's). For example: Time series in which the features are the daily revenue of a company $C$ i.e. $x_t$ = the revenue of company $C$ at day $t$ and the label is the stock price. i.e. $y_t$ = the stock price of company $C$ at day $t$ . In this case you can feed the LSTM with batch of $x_t$ and get the corresponding $y_t$ predictions.
