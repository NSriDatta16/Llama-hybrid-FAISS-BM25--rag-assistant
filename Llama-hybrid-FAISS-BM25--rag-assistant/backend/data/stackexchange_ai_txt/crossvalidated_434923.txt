[site]: crossvalidated
[post_id]: 434923
[parent_id]: 434922
[tags]: 
The vanishing and exploding gradients in any neural network, not just RNN. I think they are caused by the multiplication of the gradients across layers (chain rule). If you have a neural network that represents as (not exact): $$f(x) = A(B(C(x)))$$ The derivative or gradient calculation would be: $$ \frac{df}{dx}=\frac{dA}{dB}\frac{dB}{dC}\frac{dC}{dx} $$ If the network has many layers, the multiplications add up, which causes either gradients vanishing or exploding, especially for RNN. See here: https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb
