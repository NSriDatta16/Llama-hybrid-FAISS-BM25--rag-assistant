[site]: crossvalidated
[post_id]: 427211
[parent_id]: 427118
[tags]: 
I think your question is interesting. Actually I don't think @Sympa has answered why the intercept is different between the models with and without the interaction term. A spoiler: I do not have a clear answer to that question but I'll try to do my best to help out. Updated spoiler: at the end it seems a real answer showed up. The function lm() uses, by-default, the dummy coding (have a look at the contrasts argument). You can find a good explanation of the different codings for categorical predictors in R here . For the simplest case, when you have a single categorical predictor X your intercept definitively represents the average value of the response variable for X=0, that is for X equal to the reference (baseline) level. R, by default, orders the level of your categorical variables alphabetically (e.g. if sex = male and female, female is the reference level). You can check the levels of your categorical variables using the function levels() and change their order using the function relevel() . A reproducible example is always welcome and I have just created one below that mimics your data structure. set.seed(1234567895) lang_family |t|) (Intercept) 16.05157 0.08225 195.149 |t|) (Intercept) 15.98321 0.10802 147.959 You can see that, as in your case, the intercept is the value of the response variable when both the predictors are at their reference level (as I would have expected) only for the model containing the interaction but not for the model without the interaction. I would really appreciate if someone chime in with a simple and understandable explanation of that. I have found a somehow similar question here but the answers do not dispel any doubts. After reading that post and googling here and there, I am pretty sure the answer has to do with the model being saturated (containing all possible main effects and all possible interaction effects) or not. The interecept is what one would expect for the saturated model but not for models that are not saturated. A simple, conceptual, explanation of why that would be great... I have edited this answer after the @Repmat comment (thanks for that!), sometimes what is obvious to one is not to others...by the way it was not obvious to me. So, I have learnt a new and very useful thing today that is how the estimate of the intercept is calculated in a model (with dummy coding). This explains why in a non-saturated model the intercept is not what I thought to be and that one must be very careful with its interpretation, actually it should not be interpreted ! Obviously I am not a mathematician and I like to see with my eyes how things look like. Therefore, applying the equation provided in the @Repmat comment to the fake dataset of my previous answer: # This is the mean of the response variable: mean.WER
