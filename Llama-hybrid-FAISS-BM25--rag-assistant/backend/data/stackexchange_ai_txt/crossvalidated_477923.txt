[site]: crossvalidated
[post_id]: 477923
[parent_id]: 477848
[tags]: 
There are two papers by Arora, Risteski, and Zhang (2018) and Arora, and Zhang (2017) showing that despite great hopes for achieving that, GANs seem to be pretty susceptible to mode collapse. Quoting abstract of the second paper: [...] A recent theoretical analysis in Arora et al (to appear at ICML 2017) [...] showed that the training objective can approach its optimum value even if the generated distribution has very low support ---in other words, the training objective is unable to prevent mode collapse. The current note reports experiments suggesting that such problems are not merely theoretical. It presents empirical evidence that well-known GANs approaches do learn distributions of fairly low support, and thus presumably are not learning the target distribution. [...] As about your second question, the answer is also "no". Nothing in GANs forces generator and discriminator to learn the same thing, they only need to do their task correctly (generate or discriminate). Of course, as a side effect they both learn something about target distribution, but this doesn't have to be same thing.
