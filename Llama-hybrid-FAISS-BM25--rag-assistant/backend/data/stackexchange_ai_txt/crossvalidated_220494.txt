[site]: crossvalidated
[post_id]: 220494
[parent_id]: 
[tags]: 
How does the Adam method of stochastic gradient descent work?

I'm familiar with basic gradient descent algorithms for training neural networks. I've read the paper proposing Adam: ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION . While I've definitely got some insights (at least), the paper seems to be too high level for me overall. For example, a cost function $J(\theta)$ is often a sum of many different functions, therefore a vast amount of calculations have to be made to optimize its value; stochastic gradient descents - as far as I'm understanding the topic - calculate the optimization only for a subset of the these functions. To me it is unclear, how Adam does this and why this results in a decreased training error for the whole of $J(\theta)$. I think Adam updates its gradient by taking into account the previous gradient. They call it something like utilizing the momentum? What exactly is this momentum? According to the algorithm on page two in the paper, it is some kind of moving average, like some estimates of the first and second moments of the "regular" gradient? Practically, I would suspect that Adam enables one to use larger effective step sizes for decreasing the gradient and therefore the training error in combination with the stochastic approximation. Thus, the resulting update vector should "jump" around more in spatial dimensions, rather describing some curve like normal gradient descent algorithms would do. Can someone de-mystify how Adam works? Especially how it converges, specifically why Adam's method work and what exactly the benefit is?
