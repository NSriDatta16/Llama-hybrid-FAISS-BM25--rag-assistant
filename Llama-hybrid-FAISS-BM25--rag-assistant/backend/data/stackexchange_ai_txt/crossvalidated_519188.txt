[site]: crossvalidated
[post_id]: 519188
[parent_id]: 505583
[tags]: 
To see the forest for the trees, i.e., step back and look at the big picture, there are several types of feature selection methods that have been developed specifically for regression - all of which are performed during the regression procedure. Sometimes these approaches are called "fishing expeditions," since you are looking for good features that help model performance. In regression, examples are forward selection, reverse elimination, etc. I usually use reverse to "preserve subset correlation." That is, if you select the best fish from the top of the bucket, you may break up correlation they have with other fish (they are holding hands, but you rip them apart). Using the reverse elimination method will preserve sets of variables which are both correlated and help explain the model variance. In machine learning, there are numerous other filtering and wrapping approaches for feature selection, like best ranked N, recursive feature elimination, greedy plus-take away, sequential forward-reverse floating methods. Your question deals with the potential for selection bias, in which the features selected can bias the results because only features which help the model are used. Wrapping methods select features during the discrimination or training (learning) procedure, and may induce more bias than filtering methods, which select features independently or before modeling. For classification analysis, filtering methods can be as simple as running hypothesis tests involving t-tests, ANOVA, Mann-Whitney, Kruskal-Wallis to identify features that are predictive of (best discriminate) class. Whereas for regression, filtering can involve correlation analysis before regression to identify features which correlate best with outcome. Correlative features can then be employed during regression analysis (assuming some type of normality obviously or "central tendency"). Overall, I commonly use filtering and avoid wrapping methods, but do on occasion use reverse elimination selection during regression (linear, logistic). For classification, I usually use greedy plus-take away methods for filtering in order to accomplish feature selection, and then train/test classifiers which never "touched" the feature selection methods.
