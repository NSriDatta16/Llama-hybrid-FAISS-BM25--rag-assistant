[site]: crossvalidated
[post_id]: 243458
[parent_id]: 243446
[tags]: 
I think that everything that hxd1011 says is correct, however if one is interested in prediction rather than description, CoD can rear it's ugly head. For example if one is using Akaike I]information Criteria to decide on model accuracy, then the value is proportional to the number ,p, of variables. Since a lower AIC is interpreted as meaning higher model quality, the number of variables used effects model quality. The same things occurs with the Bayesian information criteria, but there the BIC value depends on log(n)*p, so the effect is even more pronounced. If these examples aren't 'exponentialish' enough, then consider a best subsets regression. Again, for prediction, it may well be that the best model doesn't contain all the variables. Best subsets looks at all the distinct models one gets by considering all the different subsets of the p variables. It then uses some criteria (frequently AIC or BIC !) to choose the 'best' model. If there are p variables there are $\binom {p}{k}$ such models using exactly k of the variables and summing over all k we get that one has to compare (via some computation) $\sum_{k=0}^{k=p} \binom {p}{k} = 2^p $ different models. There is an exponential ! One reason for the use of various regularized regression methods is that the number of models one needs to check with best subsets is exponential in p ! Originally, this was a comment, but it is too long and I don't see how editing it will be possible, so I've posted this comment as an answer.
