[site]: crossvalidated
[post_id]: 290565
[parent_id]: 290553
[tags]: 
A neural network would be overkill for this task, since the linear transform you have described is essentially a neural network with no hidden layers and no activation function. NN's should be used when the transformation is highly nonlinear. Instead, you should perform linear regression, which will directly output the matrix F. The solution to your problem is: $$\hat F = (X^TX)^{-1}X^T X_t$$ where $X$ is the matrix consisting of each of your datapoints $x$ as a row, and X_t is defined in the same way. If you're really bent on using neural networks to solve this, you should construct a neural network with no hidden layers, no activation, and no bias. The weights of the network will then converge to $F$. In fact, if you have multiple layers in your network, then under certain conditions, their product will converge to F. So as for how to do it: Make your favorite neural network, but don't use a bias and don't use an activation function. If $F$ is full rank, you should also make sure each layer of the network has at least 1024 units in it. Then, train the network and take the product of all the weights. This will give you $F$.
