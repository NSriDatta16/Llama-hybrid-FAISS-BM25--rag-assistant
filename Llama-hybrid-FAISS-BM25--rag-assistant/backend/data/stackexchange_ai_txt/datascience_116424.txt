[site]: datascience
[post_id]: 116424
[parent_id]: 116421
[tags]: 
"The current structure is just an embedding layer followed by a dense layer with no nonlinearity." - According to this, your model is currently linear. If it falls short in its performance, you should start by adding another dense layer. Your overall structure will become: embedding, dense layer (followed by a ReLU activation) A second dense layer. Make sure the hidden layer is wide enough (i.e. has enough neurons) to pick up different patterns. The problem you're describing with your current architecture is, most probably, that with ReLU as the final layer, you cannot output negative numbers (ReLU clips negative values to 0). For this reason, ReLUs are only very rarely used after the last layer of a deep learning model.
