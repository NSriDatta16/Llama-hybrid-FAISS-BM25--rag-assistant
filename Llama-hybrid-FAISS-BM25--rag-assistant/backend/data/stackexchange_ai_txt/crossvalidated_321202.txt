[site]: crossvalidated
[post_id]: 321202
[parent_id]: 321180
[tags]: 
I was a little confused by the answer above, hence I'll give it another shot. I think the question is not actually about 'classical' linear regression but about the style of that particular source. On the classical regression part: However, the linearity assumption by itself does not put any structure on our model That is absolutely correct. As you have stated, $\epsilon$ might as well kill the linear relation and add up something completely independent from $X$ so that we cannot compute any model at all. Is Greene being sloppy? Should he actually have written: $E(y|X)=XÎ²$ I do not want to answer the first question but let me sum up the assumptions you need for usual linear regression: Let us assume that you observe (you are given) data points $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$ for $i=1,...,n$. You need to assume that the data $(x_i, y_i)$ you have observed comes from independently, identically distributed random variables $(X_i, Y_i)$ such that ... There exists a fixed (independent of $i$) $\beta \in \mathbb{R}^d$ such that $Y_i = \beta X_i + \epsilon_i$ for all $i$ and the random variables $\epsilon_i$ are such that The $\epsilon_i$ are iid as well and $\epsilon_i$ is distributed as $\mathcal{N}(0, \sigma)$ ($\sigma$ must be independent of $i$ as well) For $X = (X_1, ..., X_n)$ and $Y = (Y_1, ..., Y_n)$ the variables $X, Y$ have a common density, i.e. the single random variable $(X, Y)$ has a density $f_{X,Y}$ Now you can run down the usual path and compute $$f_{Y|X}(y|x) = f_{Y,X}(y,x)/f_X(x) = \left(\frac{1}{\sqrt{2\pi d}}\right)^n \exp{\left( \frac{-\sum_{i=1}^n (y_i - \beta x_i)^2}{2\sigma}\right)} $$ so that by the usual 'duality' between machine learning (minimalization of error functions) and probability theory (maximization of likelihoods) you maximize $-\log f_{Y|X}(y|x)$ in $\beta$ which in fact, gives you the usual "RMSE" stuff. Now as stated: If the author of the book you are quoting wants to make this point (which you have to do if you ever want to be able to compute the 'best possible' regression line in the basic setup) then yes, he must make this assumption on the normalicity of the $\epsilon$ somewhere in the book. There are different possibilities now: He does not write this assumption down in the book. Then it is an error in the book. He does write it down in form of a 'global' remark like 'whenever I write $+ \epsilon$ then the $\epsilon$ are iid normally distributed with mean zero unless stated otherwise'. Then IMHO it is bad style because it causes exactly the confusion that you feel right now. That is why I tend to write the assumptions in some shortened form in every Theorem. Only then every building block can be viewed cleanly in its own right. He does write it down closely to the part you are quoting and you/we just did not notice it (also a possibility :-)) However, also in a strict mathematical sense, the normal error is something canonical (the distribution with the highest entropy [once the variance is fixed], hence, producing the strongest models) so that some authors tend to skip this assumption but use in nontheless. Formally, you are absolutely right: They are using mathematics in the "wrong way". Whenever they want to come up with the equation for the density $f_{Y|X}$ as stated above then they need to know $\epsilon$ pretty well, otherwise you just have properties of it flying around in every senseful equation that you try to write down.
