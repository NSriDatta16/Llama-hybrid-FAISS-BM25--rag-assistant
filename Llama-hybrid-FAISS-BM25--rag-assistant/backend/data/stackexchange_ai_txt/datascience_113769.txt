[site]: datascience
[post_id]: 113769
[parent_id]: 73093
[tags]: 
When specifying from_logits=True basically means that when calculating error on dataset using the cost function J , where it depends how you define your J where J also depends on some function of X ( dataset ), then instead of calculating f(x) and then putting it in J, we can directly put f(x) in J. Now this might seem a bit confusing. Lets see in detail. So basically Logit means a simple Logistic regression unit used in Neural Networks as a Neuron. Now we calculated error associated with each Neuron of a layer and so on for all layers. For a simple neuron/Logit, the cost function J is defined as J = 1/m Σ-ylog(f(x))-(1-y)log(1-f(x)) , where m is size or number of training examples, y is given predictions and f(x) is lets say sigmoid function . Now sigmoid function is defined as f(x) = 1/(1+e^(-x*w+b) where x is the input for that layer, w are the weights associated with that neuron and b is the bias associated with that neuron. Normally when from_logits=False , then first f(x) is calculated and then put in the formula for J but when from_logits = True , then f(x) is directly put into the formula J . Now it might seem that both are the same thing but this is actually not the case. The result could be same if you are lucky but due to precision errors in decimal points, the value of J could come out to be different. This can effect the accuracy of your model. Thus when you directly place formula for f(x) into J, then tensorflow actually rearranges the terms to perform better calculations. This also kills any error that might arise from precision errors. If to say in very simple words : when from_logits = False , then J = 1/m Σ-ylog(f(x))-(1-y)log(1-f(x)) when from_logits = True , then J = 1/m Σ-ylog(1/(1+e^(-x*w+b))-(1-y)log(1-1/(1+e^(-x*w+b))
