[site]: crossvalidated
[post_id]: 539622
[parent_id]: 539548
[tags]: 
There are many excellent thoughts here. There is a short answer to the question. If you want to gauge relative evidence you can sometimes get away without using any outside information. Relative evidence can be summarized by a likelihood ratio in the likelihoodist school of statistics. For example one may use study data to compute the likelihood ratio assuming that true mean blood pressure is 120mmHg vs. the mean being 140mmHg. Or you can quantify evidence more indirectly using p-values (evidence against something, only). If you want to quantify absolute evidence there is no mathematical way to compute "absolute" probabilities without having a prior distribution. So if you wanted to compute the probability that the true mean blood pressure is between 135mmHg and 145mmHg you would need a prior. Likewise if you wanted to compute the probability that a medical treatment lowers mortality instead of just using a frequentist hypothesis test to compute the probability of getting data stranger than ours if the treatment does nothing about disease risk you would need a prior. The most compelling stories I've seen about the Bayesian approach are Nate Silver's The Signal and the Noise and Bernoulli's Fallacy by Aubry Clayton. More thoughts are here and here . An analogy in medical diagnostic testing is often useful. Sensitivity and specificity are sometimes used as test characteristics. These condition on the actual disease status so only provide relative information. To turn them into absolute information (probability of disease) one must use Bayes' rule to factor in disease prevalence (the prior). So the prior distribution is the mathematical cost of being able to make direct statements that are not just about relative evidence.
