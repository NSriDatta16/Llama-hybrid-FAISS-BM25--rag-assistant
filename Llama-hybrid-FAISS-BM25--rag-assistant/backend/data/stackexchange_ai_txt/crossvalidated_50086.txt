[site]: crossvalidated
[post_id]: 50086
[parent_id]: 
[tags]: 
Bayesian approach and least-squares approach to multivariate regression with structural design

Assume for example a trivariate Gaussian model: $$ {\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*) $$ with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. The Bayesian conjugate theory of this model is well known. This model is the most simple case of a multivariate linear regression model. And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is the extension to the case when the multivariate mean ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance Rossi & al's book accompanied by the crantastic bayesm package for R . We know in addition that the Jeffreys prior is a limit form of the conjugate prior distributions. Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. Example Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. Assume that the samples are independent and that the series of the three measurements $(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$. This example falls into the context of Multivariate linear regression with a within-design structure . See for instance O'Brien & Kaiser 1985 and Fox & Weisberg's appendix So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows: $$ (**) \left\{\begin{matrix} {\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ {\boldsymbol \mu} = X {\boldsymbol \beta} \end{matrix}\right. $$ with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 & t_1 \\ 1 & t_2 \\ 1 & t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the within design , or the repeated measures design , or the structural design (I would appreciate if a specialist had some comments about this vocabulary). I think such a model can be fitted as a generalized least-squares model, as follows in R : gls(response ~ "between covariates" , data=dat, correlation=corSymm(form= ~ "within covariates" | individual )) (after stacking the data in long format). My first question is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ? My second question is not Bayesian: I have recently discovered some possibilities of John Fox's great car package to analyse such models with ordinary least squares theory (the Anova() function with the idesign argument --- see Fox & Weisberg's appendix ). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with R using ordinary least squares ?
