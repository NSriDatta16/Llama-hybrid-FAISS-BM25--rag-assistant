[site]: datascience
[post_id]: 114146
[parent_id]: 114124
[tags]: 
From sklearns clustering documentation Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations. Although PCA is advised, I would look into truncated SVD as it is supposed to work better on sparse data. Use the explained_variance_ratio output to work out what cummulative explained variance is suitable for you (ie 95%, 99%). Hopefully you can reduce the features to ~50 although there is no hard rule about this. Lastly I guess we should ask do you know this data has cluster ground truth?
