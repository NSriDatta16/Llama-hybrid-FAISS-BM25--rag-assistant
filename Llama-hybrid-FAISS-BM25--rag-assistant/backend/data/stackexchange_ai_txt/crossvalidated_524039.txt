[site]: crossvalidated
[post_id]: 524039
[parent_id]: 
[tags]: 
Why is Bahdanau's attention sometimes called concat attention?

I am learning the intuition behind the attention mechanism from https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html and there is something that I don't quite get. Both entries make reference to some concatenation happening in the decoding stage. From reading Bahdanau's paper, nowhere states that the alignment score is based on the concatenation of the decoder state ( $s_i$ ) and the hidden state ( $h_t$ ). In Luong's paper , this is referred to as the concat attention (the word score is used, though) $$ \text{score}(h_t; \bar{h}_{s}) = v_a^T \tanh (W_a [h_t; \bar{h}_{s}] ) $$ or in Bahdanau's notation: $$ a(s_{i−1}, h_j) = v_a^T \tanh (W_a [s_{i−1}; h_{j}] ) $$ In Bahdanau's paper , the alignment score is defined as $$ a(s_{i−1}, h_j) = v_a^T \tanh (W_a s_{i−1} + U_ah_{j} ) $$ And the only concatenation happening is that of the forward and backward hidden states in the bidirectional encoder. It seems like Vaswani's definition of additive attention makes more sense. Where is this idea of concatenation coming from? Possibly related Is it true that Bahdanau's attention mechanism is not Global like Luong's?
