[site]: crossvalidated
[post_id]: 242302
[parent_id]: 214879
[tags]: 
It may just be that your model doesn't explain a lot of the variance in the data, but here are some points to consider. I couldn't access you data btw, but here goes. Having an AIC of ~400 doesn't mean that your model fits badly. "AIC as a value by itself is meaningless. It derives meaning from comparison with the AIC values of other models" (Symonds & Moussalli, 2011). What's important is the difference between the AIC of your models. There is some debate over whether a minimum difference in AIC between models is useful. I recommend the Symonds & Moussalli paper as an introduction to understanding AIC, particularly the part of AIC weights. Unless you have some other variables you haven't included I would report the models as they are. Either choosing the model that you think best applies to you data, i.e. zero-inflation and a chosen distribution, OR as you have relatively few models do an all subset AIC approach. Fit a model with and without treatment for all three distributions. Then calculate the Akaike weights for them to indicate the probability of each model being the best model. If treatment is very important then some combination of the models with it in should have much larger weights than the other models. Looking at your residuals plots, the first one shows some points with high fitted values. First I would colour the points by treatment to see if the high ones are all from the same treatment. Also I would see if predict(glmmNB, type="response") is clearer. I'm not 100% on this but I think fitted defaults to using the random effect whereas predict() uses the population level. The second plot looks alright although possibly low variance in the third group. It's important to check if unequal variance is affecting your results: either transform the data or drop the treatment just to see what changes. The third plot is a qq plot for investigating normality which is not necessary for a poisson/negative binomial model. I would also point out that just because your data is overdispersed doesn't mean it is necessarily zero-inflated. You could check it by comparing models where zeroInflated = T and F. Assuming that your data are zero-inflated you may want to consider a hurdle model instead (turn off zeroInflation first), which can be fitted in glmmamdb with family="truncpoiss" , "truncbinom" or "truncbinom1" . In essence use zero-inflated models if you believe some of your zeros are "false" i.e. due to measurement error (Chapter 11 of Zuur et al 2009). Zuur, A.F., Ieno, E.N., Walker, N.J., Saveliev, A.A. and Smith, G.M., 2009. Mixed effects models and extensions in ecology with R. New York: Springer. 574 p. Symonds, M.R. and Moussalli, A., 2011. A brief guide to model selection, multimodel inference and model averaging in behavioural ecology using Akaikeâ€™s information criterion. Behavioral Ecology and Sociobiology, 65(1), pp.13-21.
