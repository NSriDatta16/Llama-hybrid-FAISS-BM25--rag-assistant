[site]: crossvalidated
[post_id]: 297029
[parent_id]: 
[tags]: 
Written my first LSTM - looking for feedback as well as ways I can improve it's learning abilities

I have written an LSTM using Tensorflow that reads in sequences of 25 chars from a large text file of Shakespeare's plays. All charatcers are encoded to and from one-hot vectors. From what I can tell, it is working as it should be. I trained it for quite some hours, and it reached about 80% accuracy - and was still increasing when the training finished. However, when validating the model's performance on an unseen 20% of the text file, accuracy was only about 50%. Hence, I am now looking for any critiques on my code, and more so ways in which I can improve and optimise my model. Refelcting this loss in performance when using unseen data, it also produced quite a poor text of its own. Firstly, my model would begin generating text, but quickly become stuck in a loop of repeating the same sequence of characters over and over again, never breaking the loop. To combat this problem, I added a DropoutWrapper with an output_keep_prob of 80%. This did indeed stop the model from repeatedly spitting out the same sequence of characters. However, it is still quite poor at producing any meaningful text at all - even any words at all. Some of the syntax and punctuation is decent, but nothing more. My main questions are with regards to all the variables that can be manually tuned before training my NN. Things such as: the number of hidden cells the number of hidden layers (I haven't tried adding more yet) the learning rate the forget bias (what range of values can this take, and what does a value such as 2.0 actually mean?) the difference between output/input/state keep probabilities, and when to use which How is one meant to successfully decide on the values for all of these variables, when it takes hours to test each combination? In a nutshell, any advice on how I can increase the learning ability and overall accuracy of my model's ability to predict the next characters, and possibly make it more efficient in doing so would be greatly appreciated. It appears to be overfitting the training data, but I am lost with how to combat this. As a side note, despite my model clearly not working well on the validation set, I am also unsure if my generateText() method is the best way to go about creating its own text. Here is my code, as well as a snippet of text produced after ~90,000 training iterations over ~6-7 hours: CODE: from __future__ import print_function import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import random text = open("/home/kev/Documents/NeuralNetworks/CharacterPredicter/shakespeare.txt", 'r').read() #text = "abcdefghij" vocab = list(set(text)) textSize = len(text) vocabSize = len(vocab) testingStartChar = int(textSize * 0.8) charToInt = {char : i for i, char in enumerate(vocab)} intToChar = {i : char for i, char in enumerate(vocab)} sequenceLength = 25 # Must be strictly [[[0, 1, ..., 0], [1, 0, ..., 0], [0, 0, ..., 1]] for _ in range(batchSize): # [[1, 0, ..., 0], [0, 0, ..., 1], [0, 1, ..., 0]]] inputSequence = [] labelSequence = [] #if isTraining: # charPointer = random.randint(0, testingStartChar - seqLength) # Uncomment for random sequences if isTraining and charPointer + sequenceLength + 1 >= testingStartChar: charPointer = 0 if not isTraining and charPointer + sequenceLength + 1 >= textSize - 1: charPointer = testingStartChar for _ in range(seqLength): oneHotInput = charToOneHot(text[charPointer]) oneHotLabel = charToOneHot(text[charPointer + 1]) inputSequence.append(oneHotInput) labelSequence.append(oneHotLabel) charPointer += 1 inputs.append(inputSequence) labels.append(labelSequence) return inputs, labels, charPointer # Converts a single character into a # one-hot vector of length vocabSize def charToOneHot(char): oneHot = [0] * vocabSize oneHot[charToInt[char]] = 1 return oneHot # Converts a list of sequenceLength characters # into a list of corresponding one-hot vectors def charsToOneHots(chars): oneHots = [] for char in chars: oneHots.append(charToOneHot(char)) return oneHots # Converts a single one-hot vector # into a single char def oneHotToChar(oneHot): charIndex = np.argmax(oneHot) return intToChar[charIndex] # Converts a list of sequenceLength one-hot vectors # into a list of corresponding characters def oneHotsToChars(oneHots): chars = [] for batchIndex in range(len(oneHots)): batchChars = [] for charIndex in range(len(oneHots[batchIndex])): batchChars.append(oneHotToChar(oneHots[batchIndex][charIndex])) chars.append(batchChars) return chars # Converts a 3D array of predictions of shape [batchSize, sequenceLength, vocabSize] # into a list of one-hot vectors where the 1 corresponds to the prediction with the # highest score. These are then converted into batchSize lists of sequenceLength # predicted characters def predictionsToChars(predictions): chars = [] for batchIndex in range(len(predictions)): batchChars = [] for charIndex in range(len(predictions[batchIndex])): predictionOneHot = np.zeros_like(predictions[batchIndex][charIndex]) maxChar = np.argmax(predictions[batchIndex][charIndex]) predictionOneHot[maxChar] = 1 batchChars.append(oneHotToChar(predictionOneHot)) chars.append(batchChars) return chars x = tf.placeholder(tf.float32, [None, sequenceLength, vocabSize]) y = tf.placeholder(tf.float32, [None, sequenceLength, vocabSize]) xTensors = tf.unstack(x, axis=1) # sequenceLength tensors of shape [batchSize, vocabSize] W = tf.Variable(tf.random_normal([hiddenDimension, vocabSize])) b = tf.Variable(tf.random_normal([vocabSize])) cell = tf.contrib.rnn.BasicLSTMCell(hiddenDimension, forget_bias=forgetBias) cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=outputKeepProb) outputs, states = tf.nn.static_rnn(cell, xTensors, dtype=tf.float32) # sequenceLength list of tensors of shape [batchSize, hiddenDimension] predictions = [tf.add(tf.matmul(output, W), b) for output in outputs] # sequenceLength list of tensors of shape [batchSize, vocabSize] predictions = tf.transpose(predictions, [1, 0, 2]) # tensor of shape [batchSize, sequenceLength, vocabSize] correctPredictions = tf.equal(tf.argmax(predictions, axis=2), tf.argmax(y, axis=2)) # tensor of shape [batchSize, sequenceLength], true if one-hot vector of predicted char = label accuracy = tf.reduce_mean(tf.cast(correctPredictions, tf.float32)) # real number in [0, 1] giving the total accuracy of predictions for this batch loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, labels=y)) # real number giving the total error in predictions for this batch optimiser = tf.train.AdamOptimizer(learningRate).minimize(loss) # adjusts the paramaters based on the loss ############################################################################ # PLOTTING ############################################################################ def plotAccuracy(x, y, iteration): # Plots the average accuracy per 1000 iterations plt.axis([0, iteration, 0, 100]) plt.xlabel("Iteration") plt.ylabel("Accuracy (%)") plt.grid(True) plt.plot(x, y, c='b', lw=1.0) plt.savefig("/home/kev/Documents/NeuralNetworks/CharacterPredicter/AccuracyGraph.png") plt.clf() ############################################################################ # GENERATING ############################################################################ def generateText(): randIndex = random.randint(0, textSize - sequenceLength) seedSequence = text[randIndex : randIndex + sequenceLength] inputOneHots = charsToOneHots(seedSequence) with open("/home/kev/Documents/NeuralNetworks/CharacterPredicter/output.txt", "w+") as output: generatedText = "" lineLength = 0 for _ in range(10000): dict = {x: [inputOneHots]} pred = session.run(predictions, dict) predChars = predictionsToChars(pred)[0] # [0] because in the shape of [['a', 'b', 'c', ... , 'z']] inputOneHots = charsToOneHots(predChars) if predChars[-1] == "\n": lineLength = 0 elif predChars[-2] == " " and lineLength >= 70: # Keeps line length under ~70 chars output.write("\n") lineLength = 0 lineLength += 1 output.write(predChars[-1]) # Write most recently predicted char with tf.Session() as session: session.run(tf.global_variables_initializer()) ############################################################################ # TRAINING ############################################################################ xPlots, yPlots = [], [] # Store points to graph average accuracy pre 1000 iteraions currentYPlots = [] # Stores the accuracies for the current 1000 iterations pointer = 0 # Stores index of current char being read from text for iteration in range(trainingIterations): batchX, batchY, pointer = generateData(batchSize, sequenceLength, isTraining=True, charPointer=pointer) dict = {x: batchX, y: batchY} session.run(optimiser, dict) iteractionAccuracy = session.run(accuracy, dict) currentYPlots.append(iteractionAccuracy * 100) if (iteration + 1) % printStep == 0 or iteration in (0, trainingIterations - 1): #inputOneHots = session.run(x, dict) #labelOneHots = session.run(y, dict) #preds = session.run(predictions, dict) #correct = session.run(correctPredictions, dict) iterationLoss = session.run(loss, dict) print("Iteration:\t" + str(iteration + 1) + " / " + str(trainingIterations)) print("Accuracy:\t" + str("%.2f" % (iteractionAccuracy * 100) + "%")) print("Loss:\t\t" + str(iterationLoss) + "\n") #print("Inputs:\n" + str(oneHotsToChars(inputOneHots))) #print("Labels:\n" + str(oneHotsToChars(labelOneHots))) #print("Prediction:\n" + str(predictionsToChars(preds))) #print("Correct:\n" + str(correct) + "\n") if ((iteration + 1) % 1000 == 0 and iteration > 0) or (iteration == trainingIterations - 1): yPlots.append(np.mean(currentYPlots)) xPlots.append(iteration + 1 - 500) plotAccuracy(xPlots, yPlots, iteration) currentYPlots = [] #generateText() ############################################################################ # VALIDATING ############################################################################ testX, testY, _ = generateData(numValidationSequences, sequenceLength, isTraining=False, charPointer=testingStartChar) testAccuracy = session.run(accuracy, {x: testX, y: testY}) print("Testing Accuracy: " + str("%.2f" % (testAccuracy * 100) + "%")) generateText() SAMPLE GENERATED TEXT: A: Haug ae w. T: a offus thinoly igresure t ad o hind somanick, ile, s. TIfus, th, atougenigenend O: Meth d om wamissts he t, y, Sinneatined woney, Thotom banored ashinerenoury, wor shevis s thesusen, onisicedet oulinnane a bedsth ffusse. TI hofs aistest TI g I Tono tis, toning thy ars ouco ifop AS th anorot, idle. DUSenedaleth thabor oumbedr sHe t alagr Angoo mpinofine, tince nsths Thath, W berut feronenses oregonshameroete there wis thordssances th, alyon tlstinourel yol, aire, ago ap tr, lessesunes s at, to fofa, y'skshelfonongies tovethe, anen anedaswes, St alarverilinssinsy As, Pinone! An, win Wat, ate an t, At t h ofune te, ones tursed inesind tend yopste ons. men a oystrieats: Tchalyodonas, ts, outhedethene tex Nevondeds tethededeshus anmanean, anedoneloranoush, m thn t, finedits s: o t y anenino b.
