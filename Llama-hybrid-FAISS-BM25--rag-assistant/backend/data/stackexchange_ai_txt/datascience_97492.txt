[site]: datascience
[post_id]: 97492
[parent_id]: 
[tags]: 
LSTM Many to one with multiple time steps for time series (multi class classification)

I want to do a time series multi-class classification for fault detection and diagnosis with time-series sensor data set which contains a sequence of 50 records of normal data and sequences of another 50 records for each fault type. All together there are 3 types of data including normal condition. so the output will be a one hot encoded vector. And since there are three different data series collected for each condition, I have decided to go with 3 time steps which LSTM will take each condition as a one series that will be trained. Data set summery is 4 input features,3 class labels, and 150 samples. This model currently gives me the output vector of possible class labels predicted for each samples i have provided in the predict function like below. model.predict([[[6.5, 2.8, 4.6, 1.5],[6.3, 2.76, 4.4, 1.3],[5.5, 3.8, 1.6, 0.5]]]).argmax(-1) >>>array([[1, 1, 0]]) But here, considering all the four samples i have provided in predict function, is there any posibility for me to predict a one class label? This is possible when i reshape my train data to have only one time step. But i believe that using 3 time steps is ideal here since i have 3 types of sequences( 3 conditions). Please guide me on how to achieve this with LSTM. Following is the current implementation i have done with multi time steps. #load dataset dataframe = pandas.read_csv("chiller_plant.csv",header=None) dataset = dataframe.values X=dataset[:,0:4].astype(float) Y=dataset[:,4] # Encode the output variables encoder = LabelEncoder() encoder.fit(Y) # convert output variables into the numbers encoded_Y = encoder.transform(Y) # Convert integers to dummy variables (one-hot encoded) dummy_Y = np_utils.to_categorical(encoded_Y) from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,dummy_Y,test_size=0.2) #20% is allocated for the testing X_train = X_train.reshape(40, 3, 4) y_train = y_train.reshape(40, 3, 3) y_train.shape,X_train.shape ((40, 3, 3), (40, 3, 4)) # Create the Neural Network Model def create_nn_model(): #create sequential model model = Sequential() model.add(LSTM(100,dropout=0.2, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True)) model.add(Dense(100, activation='relu')) model.add(Dense(3,activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) return model model = create_nn_model() model.summary() > Model: "sequential_6" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm_6 (LSTM) (None, 3, 100) 42000 _________________________________________________________________ dense_12 (Dense) (None, 3, 100) 10100 _________________________________________________________________ dense_13 (Dense) (None, 3, 3) 303 ================================================================= Total params: 52,403 Trainable params: 52,403 Non-trainable params: 0 model.fit(X_train,y_train,epochs=200,batch_size=5)
