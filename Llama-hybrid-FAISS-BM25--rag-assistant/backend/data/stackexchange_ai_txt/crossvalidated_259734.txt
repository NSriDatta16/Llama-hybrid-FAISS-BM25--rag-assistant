[site]: crossvalidated
[post_id]: 259734
[parent_id]: 259704
[tags]: 
As has been stated, while we often consider the case of iid errors in linear regression, this does not have a direct equivalent in most generalized linear models (including logistic regression). In logistic regression, we typically employ the assumption of independence of outcomes that all have a very strict relation (i.e. linear effects on the log probabilities). But these result in random variables that are not identical, nor are they decomposable into a constant term plus an iid error as is the case with linear regression. If you really want to show that the responses have some sort of iid relation, then follow me for the next paragraph. Just know that this idea is a little off the beaten path; you may not get full credit for this response on a final if your professor is lacking in patience. You maybe familiar with the inverse-cdf method for generating random variables. If not, here's a refresher: if $X$ has cumulative distribution function $F_X$, then I can produce random draws from $X$ by first taking random draws $q \sim \text{uniform(0,1)}$ then calculating $X = F_X^{-1}(q)$. How does this relate to logistic regression? Well, we could think that the generating process for our responses has two parts; a fixed part relating the covariates to the probabilities of success, and a random part that determines the value of the random variable conditional on the fixed part. The fixed part is defined by the link function of logistic regression, i.e. $p = \text{expit}(\beta_o + \beta_1 x)$. For the random part, let's define $F_Y( y | p)$ to be the cdf for a Bernoulli distribution with probability $p$. Then we can think of the response variable $Y_i$ being generated by the following three steps: 1.) $p_i = \text{expit}(\beta_o + \beta_1 x_i)$ 2.) $q_i \sim\text{uniform(0,1)}$ 3.) $Y_i = F^{-1}(q_i | p_i)$ Then the standard assumption in logistic regression is that $q_i$ is iid.
