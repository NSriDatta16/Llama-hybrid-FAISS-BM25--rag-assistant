[site]: crossvalidated
[post_id]: 428818
[parent_id]: 
[tags]: 
How can I aggregate the estimates for the rate of decay from different experimental runs?

I have data for a device that dispenses material and I want to use an exponential decay model in python to relate the flow rate to the mass left inside the device, in particular $flow = \beta_0 + \beta_1e^{\beta_2.mass}$ where $\beta_0$ , $\beta_1$ and $\beta_2$ are the parameters of the model. I use the curve_fit function from scipy.optimize with the above dependency and obtain estimates of the parameters. The data contains measurements from several full runs of the device and I produced parameter estimates for each of them. My goal is to report a single set of parameters to be used for future predictions. I was wondering if there is any justified alternative to just taking the average of the estimates for each parameter between the different runs. Loosely speaking, if one run was more noisy than another it seems reasonable to attribute less importance to it due to it being less reliable.
