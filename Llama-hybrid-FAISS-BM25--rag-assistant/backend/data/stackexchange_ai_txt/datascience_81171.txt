[site]: datascience
[post_id]: 81171
[parent_id]: 
[tags]: 
Batch Normalization For Federated Learning

As you know, in a federated learning setting, clients train local versions of the joint global model with their Non i.i.d data and each submit an update to the global model which would be aggregated into the next joint global model. The normalization which happens by Batch Normalization layers during the training phase is based on the local batch statistics. My question is, How should one aggregate these local statistics (batch normalization parameters) for the global model so they represent the global statistics of all the data? I am talking about beta, alpha, moving mean and variance for each batch normalization layer. Should we treat them like weight and biases of fully connected (or Conv) layers and simply average them?
