[site]: crossvalidated
[post_id]: 395126
[parent_id]: 
[tags]: 
Attention for short sequence length. Is it reasonable?

Will the attention mechanism be useful for the short sequence length? Let's say your training corpus has each query of MAX length 10. and most queries are of word length 3-4 words. How reasonable is to use Attention mechanism in such a scenario? Because on a high level the paper "Attention is all you need" paper inclines to capture global dependencies in the corpus.
