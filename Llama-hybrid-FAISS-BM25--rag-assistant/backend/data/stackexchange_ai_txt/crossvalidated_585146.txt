[site]: crossvalidated
[post_id]: 585146
[parent_id]: 
[tags]: 
Can I use batchnorm in CNN + RNN, and where to place it exactly?

I have designed a following neural network that combines CNN, RNN and Dense layers. It aims to predict a positive or negative outcome for the time step t+1 , given a fixed sequence of steps t-7 ... t . Here is a rough sketch of layers, I'm using Mxnet but the question applies to any framework: Conv2D() Activation() Conv2D() Activation() Conv2D() Activation() GRUCell() Dense() Activation() Dense() Activation() Dense() Initially, I've placed a BatchNorm() layer before every Activation() layer in my model. However, after doing some research, I've read that batch norm may not work well for recurrent networks, such as LSTM or GRU. So, the question is, would batch norm make sense after every convolutional and dense layer, before the activation? Or would that affect the performance of my recurrent cell, in this case GRU? Or perhaps I could place it only after convolutional or only after dense layers? Or finally modify my GRU cell as according to this paper: https://jaywhang.com/assets/batchnorm_rnn.pdf to also make it batch normalized, along with every other layer in my model? What is the best practice here?
