[site]: crossvalidated
[post_id]: 124928
[parent_id]: 
[tags]: 
Coarse-resolution subsampling of time-series data

Suppose I have time series data with a very fine resolution, e.g. 100 datapoints per second. I want to report this data to some service that can only take data at 1 point per second. I need to do something with those 100 datapoints and only report 1. I could pick one at random, pick the first, pick the last, report the mean, alternate between reporting the 80th percentile and 20th percentile, etc. What sort of statistics associated with a time series data tend to be important to preserve when resampling/subsampling? What techniques for subsampling do a good job of preserving said statistics? Sorry for the open-ended and somewhat subjective nature of this question, any guidance, industry standards, or reference papers would be appreciated. EDIT: For the context I'm interested, the raw metrics from the source are coming from a server in a cluster. Metrics will include things like current heap memory usage or disk usage in MB, number of subprocesses currently running, and time taken to perform a certain type of task we want to monitor.
