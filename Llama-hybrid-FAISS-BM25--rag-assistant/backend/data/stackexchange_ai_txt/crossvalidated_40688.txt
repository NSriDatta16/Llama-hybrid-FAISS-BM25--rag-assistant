[site]: crossvalidated
[post_id]: 40688
[parent_id]: 40638
[tags]: 
Say you have a single time series you want to learn on. Then you can use the first half for the development of your model and the second half for testing. You now cut your two halves into windows individually and can shuffle those of the training set. From what I know, the shuffling is actually not done because of generalization, but because of optimization. It is sometimes more efficient to optimize a sum of functions (in this case, one loss function per time window) if you do not look at their sum, but estimate the gradient by looking at the gradients of a subset of the sum. Look at recent publications by Nicolas Le Roux and Marc Schmidt for this topic.
