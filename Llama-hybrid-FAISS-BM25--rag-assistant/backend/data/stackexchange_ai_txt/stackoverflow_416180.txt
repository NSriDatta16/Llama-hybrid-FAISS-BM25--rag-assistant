[site]: stackoverflow
[post_id]: 416180
[parent_id]: 412019
[tags]: 
F# Has Better Performance than C# in .NET math algorithms. So rewriting neural network in F# might improve the overall performance. If we re-implement LUT benchmarking snippet (I've been using slightly tweaked version) in F#, then the resulting code: executes sigmoid1 benchmark in 588.8ms instead of 3899,2ms executes sigmoid2 (LUT) benchmark in 156.6ms instead of 411.4 ms More details could be found in the blog post . Here's the F# snippet JIC: #light let Scale = 320.0f; let Resolution = 2047; let Min = -single(Resolution)/Scale; let Max = single(Resolution)/Scale; let range step a b = let count = int((b-a)/step); seq { for i in 0 .. count -> single(i)*step + a }; let lut = [| for x in 0 .. Resolution -> single(1.0/(1.0 + exp(-double(x)/double(Scale)))) |] let sigmoid1 value = 1.0f/(1.0f + exp(-value)); let sigmoid2 v = if (v = Max) then 1.0f; else let f = v * Scale; if (v>0.0f) then lut.[int (f + 0.5f)] else 1.0f - lut.[int(0.5f - f)]; let getError f = let test = range 0.00001f -10.0f 10.0f; let errors = seq { for v in test -> abs(sigmoid1(single(v)) - f(single(v))) } Seq.max errors; open System.Diagnostics; let test f = let sw = Stopwatch.StartNew(); let mutable m = 0.0f; let result = for t in 1 .. 10 do for x in 1 .. 1000000 do m And the output (Release compilation against F# 1.9.6.2 CTP with no debugger): Max deviation is 0.001664 10^7 iterations using sigmoid1: 588.843700 ms 10^7 iterations using sigmoid2: 156.626700 ms UPDATE: updated benchmarking to use 10^7 iterations to make results comparable with C UPDATE2: here are the performance results of the C implementation from the same machine to compare with: Max deviation is 0.001664 10^7 iterations using sigmoid1: 628 ms 10^7 iterations using sigmoid2: 157 ms
