[site]: crossvalidated
[post_id]: 558727
[parent_id]: 556048
[tags]: 
Consider what $R^2$ intends to do. $$ R^2 = 1 - \dfrac{\overset{N}{\underset{i=1}{\sum}}\big(y_i -\hat y_i\big)^2}{\overset{N}{\underset{i=1}{\sum}}\big(y_i -\bar y\big)^2} $$ Focus on the fraction: $\dfrac{\overset{N}{\underset{i=1}{\sum}}\big(y_i -\hat y_i\big)^2}{\overset{N}{\underset{i=1}{\sum}}\big(y_i -\bar y\big)^2} $ . That is the ratio of residual sum of square (RSS) values for two models. In the numerator, we have the RSS for the model we are considering. In the denominator, we have the RSS for a naïve model that always predicts the mean of $y$ ; that is, $\hat y_i = \bar y$ for all $i$ . When you go to an out-of-sample test, to fit in the spirit of comparing to a naïve model that always guesses the mean of $y$ , compare the out-of-sample $y$ -values to the mean of the in-sample $y$ . After all, it wouldn't really be a model trained on in-sample data if it predicted the out-of-sample mean. When we get to $Q^2$ , we apply a similar idea. PRESS is equal to the sum of the squared residuals for the holdout samples of leave-one-out cross validation (LOOCV). To fit with the spirit of $R^2$ of comparing to a naïve model that always guesses the in-sample mean, we calculate the squared differences between out-of-sample points and their predictions from naïve models that always guess the in-sample mean. Applied to LOOCV, this means that we find the squared residual coming from comparing $y_1$ to the mean of all other $y$ values, which I will denote by $\bar y_{-1}$ . Then we do it again for $y_2$ and $\bar y_{-2}$ . Then again for $y_3$ and $\bar y_{-3}$ ... This is not equal to the $TSS$ value from the usual $R^2$ . y Consequently, for $Q^2$ to preserve the idea of comparing our model to the naïve model that always predicts the in-sample mean, the denominator in $Q^2$ should not be the same as the $TSS=\overset{N}{\underset{i=1}{\sum}}\big(y_i -\bar y\big)^2$ denominator in $R^2$ . From the question you linked, it looks like they are going ahead and using the usual $TSS$ denominator anyway. The only value that I see to this is that using it for model selection is, in some sense, equivalent to using the full PRESS statistic, much like how using MSE is equivalent to using $R^2$ (lowest MSE for a model on a data set corresponds to highest $R^2$ ), except that PRESS penalizes excessive model complexity. Then we divide through by some kind of normalizing constant. I do not consider that to be in the spirit of an $R^2$ that is tweaked for the LOOCV setting. However, if the goal is to find a model that minimizes out-of-sample square loss and the approach to choosing such a model involves LOOCV, the $Q^2$ calculation that uses the usual TSS is fine and equivalent to using the LOOCV sum of squared residuals. (I haven't done the calculation or tried to come up with a counterexample in R , but it might turn out that model selection based on $1-\dfrac{PRESS}{TSS}$ and $1-\dfrac{PRESS}{\overset{N}{\underset{i=1}{\sum}}\big(y_i - \bar y_{-i}\big)^2}$ are equivalent. I would be quite delighted to see this be the case, and it would not surprise me, either.) EDIT Define $DSS:=\overset{N}{\underset{i=1}{\sum}}\big(y_i - \bar y_{-i}\big)^2$ ("Dave's Sum of Squares"). Yes, the two turn out to be equivalent in the sense that a lower PRESS value will always result in a a higher $1-\frac{PRESS}{TSS}$ and also a higher $1-\frac{PRESS}{DSS}$ . Consequently, any ranking of model quality based on $1-\frac{PRESS}{TSS}$ will be the same as the ranking of model quality based on $1-\frac{PRESS}{DSS}$ . If $P_1$ and $P_2$ are the $PRESS$ values from competing models of the same data, then: $$ P_1 1-\dfrac{P_2}{TSS} \iff 1-\dfrac{P_1}{DSS} > 1-\dfrac{P_2}{DSS}$$ $$ \text{AND}$$ $$ P_1 > P_2 \iff 1-\dfrac{P_1}{TSS} In fact, establishing this involves no statistics or machine learning, just algebra. We would agree that showing the following is fair for middle school or high school algebra, right? (Assume that $D,T,X,Y>0$ , since they will be in our machine learning problems.) $$ X 1-\dfrac{Y}{T} \iff 1-\dfrac{X}{D} > 1-\dfrac{Y}{D}$$ $$ \text{AND}$$ $$ X > Y \iff 1-\dfrac{X}{T} $1-\dfrac{k}{T}$ and $1-\dfrac{k}{D}$ are just monotonic transformations of $k$ , so the order is preserved (yes, reversed, but the goal is to have low PRESS and high $Q^2$ , so we want the order reversed).
