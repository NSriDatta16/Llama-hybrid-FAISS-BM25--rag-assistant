[site]: crossvalidated
[post_id]: 283244
[parent_id]: 
[tags]: 
Batch Norm & Input Norm Comparisons

Consider the following simple neural network which is trying to figure out a function with 1 input feature which directly maps to a y = 1 when X >= 12500 and a y = 0 when X import numpy as np from keras.layers import Input, Dense, Dropout from keras.layers.normalization import BatchNormalization from keras.models import Model X = np.arange(25000) # X = (X - np.mean(X)) / np.std(X) y = np.append(np.zeros(12500,), np.ones(12500,)) input_1 = Input(shape=(1,)) dense_1 = Dense(20, activation='tanh', )(input_1) # dense_1 = BatchNormalization()(dense_1) output_1 = Dense(1, activation='sigmoid')(dense_1) model = Model(inputs=[input_1], outputs=[output_1]) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit([X], [y], epochs=10, batch_size=20) For the most part the architecture of the NN is unimportant to me. Notice the 2 commented out lines. I am going to permute uncommenting these 2 lines and report on the results and ask some follow up questions. I will call the first commented line input normalization and the second batch normalization. You can make the assumption that I have a general understanding of the mechanics of how backprop, gradient descent and batch normalization work. ----- No Normalization ----- OUTCOME: Essentially 0 learning ----- Epoch 1/10 25000/25000 [==============================] - 2s - loss: 0.7042 - acc: 0.4983 Epoch 2/10 25000/25000 [==============================] - 2s - loss: 0.6979 - acc: 0.4925 Epoch 3/10 25000/25000 [==============================] - 2s - loss: 0.7022 - acc: 0.4982 Epoch 4/10 25000/25000 [==============================] - 2s - loss: 0.6942 - acc: 0.5004 Epoch 5/10 25000/25000 [==============================] - 2s - loss: 0.6951 - acc: 0.4965 Epoch 6/10 25000/25000 [==============================] - 2s - loss: 0.6940 - acc: 0.5048 Epoch 7/10 25000/25000 [==============================] - 2s - loss: 0.6957 - acc: 0.5056 Epoch 8/10 25000/25000 [==============================] - 2s - loss: 0.6970 - acc: 0.4962 Epoch 9/10 25000/25000 [==============================] - 2s - loss: 0.6938 - acc: 0.5040 Epoch 10/10 25000/25000 [==============================] - 2s - loss: 0.6957 - acc: 0.5065 ----- Batch Normalization only ----- OUTCOME: Essentially 0 learning ----- Epoch 1/10 25000/25000 [==============================] - 4s - loss: 0.6932 - acc: 0.4981 Epoch 2/10 25000/25000 [==============================] - 3s - loss: 0.6968 - acc: 0.5069 Epoch 3/10 25000/25000 [==============================] - 3s - loss: 0.6957 - acc: 0.4983 Epoch 4/10 25000/25000 [==============================] - 3s - loss: 0.6931 - acc: 0.5018 Epoch 5/10 25000/25000 [==============================] - 3s - loss: 0.6925 - acc: 0.5046 Epoch 6/10 25000/25000 [==============================] - 3s - loss: 0.6979 - acc: 0.4964 Epoch 7/10 25000/25000 [==============================] - 3s - loss: 0.6938 - acc: 0.4994 Epoch 8/10 25000/25000 [==============================] - 3s - loss: 0.6941 - acc: 0.5004 Epoch 9/10 25000/25000 [==============================] - 3s - loss: 0.6929 - acc: 0.4996 Epoch 10/10 25000/25000 [==============================] - 3s - loss: 0.6922 - acc: 0.4987 ----- Input normalization only ----- OUTCOME: Extremely accurate @ epoch 1 ----- Epoch 1/10 25000/25000 [==============================] - 2s - loss: 0.1518 - acc: 0.9965 Epoch 2/10 25000/25000 [==============================] - 2s - loss: 0.0384 - acc: 0.9984 Epoch 3/10 25000/25000 [==============================] - 2s - loss: 0.0225 - acc: 0.9990 Epoch 4/10 25000/25000 [==============================] - 2s - loss: 0.0150 - acc: 0.9989 Epoch 5/10 25000/25000 [==============================] - 2s - loss: 0.0106 - acc: 0.9992 Epoch 6/10 25000/25000 [==============================] - 2s - loss: 0.0078 - acc: 0.9995 Epoch 7/10 25000/25000 [==============================] - 2s - loss: 0.0059 - acc: 0.9994 Epoch 8/10 25000/25000 [==============================] - 2s - loss: 0.0044 - acc: 0.9995 Epoch 9/10 25000/25000 [==============================] - 2s - loss: 0.0035 - acc: 0.9995 Epoch 10/10 25000/25000 [==============================] - 2s - loss: 0.0028 - acc: 0.9997 ----- Input and batch normalization ----- OUTCOME: Slowly becomes very accurate ----- Epoch 1/10 25000/25000 [==============================] - 3s - loss: 0.2139 - acc: 0.9115 Epoch 2/10 25000/25000 [==============================] - 3s - loss: 0.0820 - acc: 0.9662 Epoch 3/10 25000/25000 [==============================] - 3s - loss: 0.0535 - acc: 0.9786 Epoch 4/10 25000/25000 [==============================] - 3s - loss: 0.0332 - acc: 0.9863 Epoch 5/10 25000/25000 [==============================] - 3s - loss: 0.0271 - acc: 0.9897 Epoch 6/10 25000/25000 [==============================] - 3s - loss: 0.0230 - acc: 0.9912 Epoch 7/10 25000/25000 [==============================] - 3s - loss: 0.0227 - acc: 0.9916 Epoch 8/10 25000/25000 [==============================] - 3s - loss: 0.0175 - acc: 0.9931 Epoch 9/10 25000/25000 [==============================] - 3s - loss: 0.0150 - acc: 0.9946 Epoch 10/10 25000/25000 [==============================] - 3s - loss: 0.0149 - acc: 0.9940 Here are my questions... I think I know the answer to this one but to completeness... Why does the model without normalization not learn anything? This one is less clear to me... Why does batch normalization not correct this? Between the 2 models with input normalization why does the one with batch norm learn more slowly? This seems to be the opposite of what batch norm is supposed to do.
