[site]: crossvalidated
[post_id]: 371369
[parent_id]: 
[tags]: 
Metric to evaluate accuracy of time series decomposition?

I am trying to de-seasonalize a large number of business sales data. They all come from the same industry and follow idiosyncratic seasonality patterns. The number of businesses is quite large, and I am looking for a metric that can tell me if I've effectively deseasonalized an individual restaurant. Decomposition methods such as STL assume: $y_t=S_t + T_t + R_t$ where $y$ is the data, $S$ is the seasonal component, $T$ is the trend component and $R$ is the remainder. My instinct is a simple metric $\frac{Var(R_t)}{Var(y_t)}$ to see how much of the variance of the original data was explained. A large value (closer to 1) suggests that the model has not successfully decomposed it. Is there an academically or industry accepted practice in measuring effective seasonality aside from the "eyeball test"? I have not found any in Hyndman's Forecasting textbook. If not, does my metric make sense intuitively?
