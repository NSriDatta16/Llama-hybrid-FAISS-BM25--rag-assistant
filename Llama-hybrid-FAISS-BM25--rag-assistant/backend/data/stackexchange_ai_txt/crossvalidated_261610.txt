[site]: crossvalidated
[post_id]: 261610
[parent_id]: 
[tags]: 
Training error in Deep Neural Network decreases after being stable for several epochs

I designed a neural network for a regression task with the following parameters: 42 inputs (I tried to reduce them with PCA but results are similar) 300 hidden neurons with ReLu activation 15 output neurons with tanh activation batch size 20 RMSProp optimizer mean square error for the cost function 0.01 learning rate 0.5 dropout in the training phase 1000 epochs At each epoch, the training set is fed to the model with small batches and the training error is computed. When the training is over, the training error's trend looks like this: After the first decreasing phase, the error reaches a plateau. At this point, the training set has been processed several times. However, after a lot of epochs the error decreases again. How can I find the reason for this behaviour?
