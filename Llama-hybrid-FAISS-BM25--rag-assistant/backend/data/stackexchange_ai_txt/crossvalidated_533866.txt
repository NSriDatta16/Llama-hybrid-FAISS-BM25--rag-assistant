[site]: crossvalidated
[post_id]: 533866
[parent_id]: 
[tags]: 
Why do we prefer unbiased estimators instead of minimizing MSE?

I was thinking about why, usually , $\hat{\sigma}^2=\hat{p}(1-\hat{p})$ is used to estimate the variance in a Bernoulli population instead of $s^2=\hat{p}(1-\hat{p})\frac{n}{n-1}$ . $s^2$ is unbiased, but in some cases would lead to strange situations. Suppose you find $\hat{p}=0.5$ . $s^2$ would be grater than $0.25$ , which is obviously wrong (since $0 ). In this situation, it would be fair to cap $s^2$ at $0.25$ to avoid results that don't make sense at all. This would also reduce the $MSE$ but would make the estimator biased again. Instead, it makes more sense to just use $\hat{\sigma}^2$ . This made me think about why unbiased estimators are usually preferred, even if they have a higher MSE. I have 3 questions: Is this the reason why biased $\hat{\sigma}^2$ is usually used instead of udjusted $s^2$ for Bernoulli populations? Are there any other reasons? Why would you want to choose an esimator which is further away from the truth on average, just to make it unbiased? Why is udjusted $s^2$ preferred over $\hat{\sigma}^2$ for other distributions, even if its $MSE$ is higher? (The estimator that minimizes MSE divides the sum of the squared residuals by $n+1$ , but I've never seen it used, why?) Moreover, $s^2$ is unbiased, but $\sqrt{s^2}$ is biased for estimating the standard deviation, so it isn't that obvious why we should be choosing it (why isn't a different adjusted estimator used just for the standard deviation?). I'm editing the question based on the comments I've received: It seems like the second question is too general because in the majority of cases an unbiased estimator doesn't exists, also there isn't usually a single estimator that minimizes MSE. So, please, consider the second point I've asked only for situations where there is an unbiased estimator which isn't the best one in terms of MSE.
