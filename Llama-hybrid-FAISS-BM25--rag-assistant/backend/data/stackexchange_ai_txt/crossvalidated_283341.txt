[site]: crossvalidated
[post_id]: 283341
[parent_id]: 283234
[tags]: 
I think I have found the solution to my own question. The jackknife estimator is The last expression is the same as Big Agnes has written, where theta_hat(.) stands for the average of the estimators with the i-th observation deleted. If one looks at the middle expression however: it is obvious that if the original estimator is unbiased, the only way for the jackknife estimator to be unbiased is when theta_hat(.) equals theta_hat, in case that the original estimate is unbiased - the jackknife estimator is biased. I have found however, that if one "pumps" theta_hat(.) by multiplying it by n/n-1 then jackknife estimator works just fine, and gives the same results as the ordinary estimator. I don't have an algebraic proof. Intuitively, it makes sense that if one removes one observation, and sums all the rest, then in order for the estimate to be in the same "order of magnitude" as the previous one, one should multiply it by n/n-1... Therefore, IMHO, when one estimates a total using the jackknife, one should use Edit : An example : Suppose we have four observations : 1,2,3,4 We take $\hat\theta$ to be their sum and assume it is unbiased. The four estimates of $\hat\theta_{-i}$ are 9,8,7,6 $\hat\theta_{(\cdot)}$ is 7.5 and $ \hat\theta_J = n\hat\theta - (n-1)\hat\theta_{(\cdot)} = 17.5 $ Which is biased, since we assumed that the ordinary sum estimate is unbiased. However, if we "pump" $\hat\theta_{(\cdot)}$ by n/n-1, the formula becomes $ \hat\theta_J = n\hat\theta - n\hat\theta_{(\cdot)} = 10 $ Update : In class, the professor has solved the exercise, and actually he did what I have suggested. However, I did not not obtain a more rigorous explanation for the correction from him. You have said that If you are assuming independent observations from a common population, there is no parameter which we will estimate with the sum. I will try to think of a way to make my assumptions more explicit, hoping that it will help. This is all done in the context of sampling from a finite universe. The units in the populations are not random. However, the specific sample that we take is random. Suppose the universe consists of {1,2,3} and you can only take a sample of the size of one OR a sample which is identical to the whole population. $\pi_i$ is defined for each unit of the population, and its definition is "the probability that the unit i will be included in the sample that we take". We can take 4 samples all in all, and I want to assume we choose among them with equal probability. So $\pi_i$ , the probability that each unit will be selected is 2/4 (There are four possible samples. Look at unit 1 for example. It will be selected if (1) is chosen, or if (1,2,3) is chosen ). So, $\pi_i $ = 1/2 for each of the three members of the population. The horvitz-thompson (HT) estimator is $ \sum\frac{X_i}{\pi_i} $ As far as I understand, the way to show it is unbiased for this specific case is the following : We need to show that if we take the result the estimator gives for each of the possible samples , multiply it by the probability that this specific sample will occur, we will get the desired result. HT(1) = 2 HT(2) = 4 HT(3) = 6 HT(1,2,3) = (1*2+2*2+3*2) = 12 When we multiply each of the results above by the probability that the specific sample will be chosen (1/4), and add them, we get exactly what we wanted : the total of the population. This is just a demonstration that in the context of sampling from finite populations, you can have an unbiased estimate of the total. I hope I got it right, this is the first course I am taking on sampling theory.
