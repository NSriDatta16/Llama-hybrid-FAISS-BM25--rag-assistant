[site]: datascience
[post_id]: 80019
[parent_id]: 
[tags]: 
Sharing parameters of an activation across layers of a neural network

Keras now provides advanced parametric activation layers like Leaky-ReLU PReLU. Each time I add this layer to a sequential model, an additional trainable parameter is added to graph. How can I make sure the trainable parameter of activation is shared across all layers. Thanks
