[site]: crossvalidated
[post_id]: 475121
[parent_id]: 
[tags]: 
Time delay detection in time series

I'm trying to align series of weather data. I have two temperature and pressure sensor which in some time periods has been shifted by some time. Example data with offset equal to -600min I need to detect periods when the delay occurs and shift it so the time series will be better aligned to each other. I tried windowed cross correlation on the raw data between sensors but it shows a lot of noise and the change point of the delay is strongly dependent on the window size. Time delay (offset) in minutes with 24h window I was thinking of detection of the change point of the cross correlation and then doing the cross correlation on the bigger chunks of the data so the value will be more stable in this periods. Do you have any idea how to find the delays or at least make cross-correlation less varying through the time without loosing the exact change point? EDIT: I tried calculating cross correlation in 3 different window sizes - 24h, 1W, 2W and joining it with median. The result for full data set is below Delay result in minutes for full data set. From beginning to 10.2013 the value should be equal -600. From 10.2013 to 03.2016 it should be equal -300. From 03.2016 it should be -600. Do you have some idea how to make data not varying so much?
