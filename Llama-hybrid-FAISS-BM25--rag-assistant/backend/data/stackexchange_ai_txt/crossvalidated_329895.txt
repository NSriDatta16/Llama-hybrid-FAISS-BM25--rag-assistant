[site]: crossvalidated
[post_id]: 329895
[parent_id]: 329204
[tags]: 
I have been thinking about the same question a lot lately, and I’d guess many others in psychology are as well. First off, each of your questions relates to whether a choice is made objectively versus subjectively, but (as others here have noted) you haven’t fully explained what would constitute (in your view) an objective versus subjective choice. You may be interested in the Gelman & Hennig 2015 paper that unpacks a variety of values wrapped up in common usage of the “objective” and “subjective” labels in science. In their formulation, “objective” relates to values of transparency, consensus, impartiality, and correspondence to observable reality, whereas “subjective” relates to values of multiple perspectives and context-dependence. Related to your Question 3, in the Bayesian view, probability is defined as quantifying uncertainty about the world. From what I understand, there is a tension apparent across “subjectivist Bayesian” (probabilities reflect individual states of belief) and “objectivist Bayesian” schools of thought (probabilities reflect consensus plausibility). Within the objectivist school, there is a stronger emphasis on the justification of the prior distribution (and the model more generally) in a transparent way that comports with consensus and that can be checked, but the choice of model is certainly context-dependent (i.e., depends on the state of consensus knowledge for a particular problem). In the frequentist conception, probabilities reflect the number of times an event will occur given infinite independent replications. Within the Neyman-Pearson framework, one stipulates a precise alternative hypothesis and a precise alpha, accepts the precise null or the precise alternative (that the population effect is exactly equal to the one stipulated) on the basis of the data, and then reports the long-run frequency of doing so in error. Within this framework, we rarely have a precise point estimate of the population effect size but rather a range of plausible values. Therefore, conditional on a given alpha, we don’t have a precise estimate of the Type 2 error rate, but rather a range of plausible Type 2 error rates. Similarly, I’d agree with your general point that we typically do not have a precise sense of what the costs and benefits of either a Type 1 error or a Type 2 error will actually be. Meaning we are often faced with a situation where we have very incomplete information about what our hypothesis should be in the first place, and even less information about what would be the relative costs and benefits of accepting vs rejecting this hypothesis. to your questions: Can false-positive/false-negative rates and their cost ratios ever be objectively justified in most social science contexts? I think so, in that a justification can be transparent, can comport with consensus, can be impartial, and can correspond to reality (to the extent that we are using the best available information we have about costs and benefits). However, I think that such justifications are also subjective, in that there can be multiple valid perspectives regarding how to set alpha for a given problem, and in that what constitutes an appropriate alpha can be meaningfully context-dependent. For example, in recent years, it has become clear that many effects in the literature reflect Type M or Type S errors. They may also reflect Type 1 errors, to the extent that a replication study is able to provide evidence for the null of exactly zero effect. Related to this observation, there is an emerging consensus that the p-value threshold for a claim with certainty should be kept the same or made more stringent (i.e., no one is arguing for a blanket increase of alpha to .10 or .20). Similarly, there is an emerging consensus that p values should not be used as a criterion for publication (e.g., the Registered Report format). To me, this reflects a kind of “objective” source of information — i.e., to my reading there is a growing consensus that false claims are costly to the field (even if we can’t put a dollar amount on these costs). To my reading, there is no clear consensus that failing to meet a p-value threshold is a dramatic cost to the field. If there are costs, they may be mitigated if failing to meet a p-value threshold doesn’t impact whether the estimate makes it into a published paper. If so, what are generalizable principles one could follow to justify these analytic choices (and maybe an example or two of them in action) I am not sure, but I would lean toward some kind of principle that the decisions should be made on the basis of transparent (local or global) consensus judgements about the costs and benefits of different kinds of analytic choices in a particular context, even in the face of woefully incomplete information about what these costs and benefits might be. If not, is my analogy of the potential subjectivity in choosing cost ratios--as being akin to Bayesian prior selection--a reasonable one? Yes, across frequentist and Bayesian traditions, there is room for subjectivity (i.e., multiple perspectives and context-dependence) as well as objectivity (i.e., transparency, consensus, impartiality, and correspondence to observable reality) in many different aspects of a statistical model and how that model is used (the chosen prior, the chosen likelihood, the chosen decision threshold, etc.).
