[site]: crossvalidated
[post_id]: 311384
[parent_id]: 
[tags]: 
Is there a way to calculate R-squared in OLS without computing the coefficients?

The background of my question is that for e.g. the White heteroskedasticity test or the Breusch-Godfrey (LM) autocorrelation test, we are generally only interested in the R-squared of the "auxiliary" regression. However, the only way of computing said R-squared that I am aware of involves deriving the coefficients etc. This can potentially consume a lot of time due to a large number of regressors and thus a large dimension of the matrix that needs to be inverted (in the case of the White test, regressing the squared residuals on the independent variables, their squares and cross-products - the number of regressors is thus a quadratic function of the number of independent variables). Is there an "alternative way" to calculate (or perhaps approximate) R-squared? (I know that the problem could be avoided by using different tests - e.g. Breusch-Pagan instead of White for heteroskedasticity, Durbin-Watson instead of Breusch-Godfrey. However, I am interested in this question both for the fun of it and because the mentioned alternative tests can be inferior to the ones mentioned at the beginning.)
