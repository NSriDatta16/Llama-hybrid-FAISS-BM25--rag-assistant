[site]: datascience
[post_id]: 49057
[parent_id]: 49033
[tags]: 
One-hot vector is called "localist" because it contains information only about a single data point, and does not give clues about other points, in contrast to a distributed representation (e.g. result of an embedding algorithm) that contains information about other data points too. For example, one-hot encoding of two animals and two flowers is as follows dog = [1, 0, 0, 0] cat = [0, 0, 1, 0] rose = [0, 1, 0, 0] tulip = [0, 0, 0, 1] Distance between any pair is the same , and effectively by knowing the vector of "rose" we gain no information about the related "tulip" or unrelated (or less related) "dog" and "cat", hence the name "localist". But if we happen to find a distributed representation like dog = [1.0, 0.1] cat = [1.1, 0.2] rose = [0.0, 1.2] tulip = [0.1, 1.3] The vector of "rose" alone gives us clues about other flowers like "tulip", which are closer , and also about other entities like animals, which are farther , hence a name like "globalist". Two notes: Being "localist" is not a zero / one thing. The more clues a representation gives about other data points, the less it is localist. The notion of "relatedness" is task dependent. For example, in the task of "grouping things with similar colors", a red car is more related to a red rose than a blue car. Here is a random image of those entities for fun! If all pairs look equally different, your brain works with "localist" vectors, thus has no clue ;)
