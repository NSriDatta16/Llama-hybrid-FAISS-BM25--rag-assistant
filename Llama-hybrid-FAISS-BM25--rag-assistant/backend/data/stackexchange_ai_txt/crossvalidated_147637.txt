[site]: crossvalidated
[post_id]: 147637
[parent_id]: 
[tags]: 
Will normalizing training and testing data separately cause under/overfitting?

Suppose I have training and testing data and I want to train a classifier (e.g. SVM). Typically, features are normalized before classification to ensure some features aren't weighted more heavily than others. Is there any risk that I will get a bad estimate of performance if I use different scaling parameters for my training and testing data? (This assumes I'm normalizing to the same range, like [0,1], for example.)
