[site]: datascience
[post_id]: 67858
[parent_id]: 67857
[tags]: 
In a Kaggle-like competition, the data your submission is evaluated on is not part of what is given to you. It may always produce a different result than what you estimate in your training process. But, if it's very different, then you have likely over-fit and your model is high variance, so, your test error is probably higher than the training error here. In particular, you don't seem to be using early stopping. Your submission error is also then going to be higher than training error. You mean your test error is lower than submission error, right? they may be quite different than your test error due to the high model variance. The submission error on average might be higher than your test error, even when done properly, because one tends to 'overfit' the hyperparameter tuning in any validation process. (But here I don't see hyperparam tuning even?) A more correct process would be, at least, to try to find the best tuning params with some cross-validation process, then refit the model on all the data you have using those params.
