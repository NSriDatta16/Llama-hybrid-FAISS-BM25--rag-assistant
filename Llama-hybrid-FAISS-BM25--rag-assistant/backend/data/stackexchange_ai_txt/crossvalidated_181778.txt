[site]: crossvalidated
[post_id]: 181778
[parent_id]: 
[tags]: 
k-nn classifier, suspect results, maybe underfitting?

I am using the k-nn classifier in Weka to experiment on the "audiology" dataset . I am using 10-fold validation, and recording average error rates for 1 I am trying three different version of k-nn. The one labelled "1" using simple voting, that is, each neighbour's vote is a given a weight of 1 indepently of how close said neighbour is. The one labelled 2 gives each vote a weight of 1/distance, so neighbours which are are further will have elss of a same in how the sample is classified The one labelled 4 gives a weight of 1-distance. Note that these are as implemented in weka, in the method make Distribution, class IBk: switch(this.m_DistanceWeighting) { case 2: weight = 1.0D / (distances[i] + 0.001D); break; case 4: weight = 1.0D - distances[i]; break; default: weight = 1.0D; } (I understand weka automatically normalizes feature vectors so all distances are in range 0 to 1?) This is the result: I am struggling to interpret this. By using k = 1 (or anyway a very small k) I would expect the classifier to underfit the data, but it seems to be then it obtains the best results? Any thoughts on this?
