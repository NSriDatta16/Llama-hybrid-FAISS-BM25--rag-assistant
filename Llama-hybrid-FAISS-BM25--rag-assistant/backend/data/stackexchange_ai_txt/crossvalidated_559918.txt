[site]: crossvalidated
[post_id]: 559918
[parent_id]: 559808
[tags]: 
Let me give a quick chime in. I'm a data analyst for DNA methylation data. I have an awesome dataset that is about 1,000 people x methylation measure in over 3 million locations each x 3 points in time. That's nearly 10 billion data points. If I want to analyze this data set... well, let's say some processes can take several days to weeks to run. Alternatively, I can do a data reduction with PCA/UMAP and work with a much much smaller dataset that will give me fairly accurate and generalizable results within minutes. So even if my explained variation is lower, it can really make sense. Something else that is worth considering is what are your results being used for? If my output is being used to make an important life-or-death decision then I really want to minimize any and all errors. If my output is going to inform further future research then I have a larger margin of error to work with. Just a quick example, let's say my analysis yields the top 20 drugs that could be effective to treat a certain cancer. If this is going to a patient, I want drugs 1-3 to represent the best, 2nd best, and 3rd best treatment options respectively. Here the result has to be super precise, with very little error. If, however, the goal is to try these out in cell cultures, then I don't really care if the top result happens to be the 3rd best or the 1st one. As long as my top 25% contains drugs that are more likely to work I don't really care about the order. (A lot of assumptions in this statement but just to exemplify). To summarize, dimensionality reduction can also help with computation time for big data analysis. This is highly dependant on what the output will be used for.
