[site]: crossvalidated
[post_id]: 587775
[parent_id]: 
[tags]: 
Feature engineering to prepare an XGBoost classifier

Is there something in particular one should take into consideration when preparing data to a XGBoost Classifier? As it is tree based, it doesn't make any assumptions about data distribution like linear models does. However, does that mean we can't improve result by transform the distribution of the features? If no, is it strictly an empirical question - try and evaluate - or is there some general guideline one could lean on?
