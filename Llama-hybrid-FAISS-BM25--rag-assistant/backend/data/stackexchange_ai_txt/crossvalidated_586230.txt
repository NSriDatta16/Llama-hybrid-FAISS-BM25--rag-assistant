[site]: crossvalidated
[post_id]: 586230
[parent_id]: 
[tags]: 
How do I whether sample populations match calculated distributions?

I'm building a toy program where the user can do algebra on uniform distributions on $1, \ldots, n$ and lists of integers. Example expressions are $\texttt{high 3 of 4d6}$ and $\texttt{first 5 of shuffle 1..52}$ . My program has two major functionalities: calculate the distribution of the input expression, and sample it many times. I would like to test that those two functionalities are in agreement across a broad range of expressions (probably themselves picked by a random process). I'm no stats person; what math should I do to get a useful answer? Note that the expressions may contain unbounded recursion. I'm happy to ignore the complexities of that for the purpose of this set of tests. If I do then all distributions have finite support. Given that, if I sample a value which has zero probability in the calculated distribution then clearly something is wrong. Otherwise, my idea is to compute some quantification of the distance between the computed and observed sample, e.g. Euclidean distance between probability vectors in $[0, 1]^n$ where the (calculated) support has $n$ elements. I think this distance should go down as the number of samples goes up, in the sense of converging to zero but not sample-by-sample monotonicity (because randomness). The law of large numbers says that this convergence holds for the expected value, unless I'm badly mistaken. But how do I test for convergence with only finitely many samples? And is there a better error/distance quantification? In the past, on a different toy, I've had some success computing a correlation coefficient between (1) how many times I ran a machine learning algorithm; and (2) how far away the learned answer was from a calculated normative answer. A negative correlation coefficient strongly suggested my system was learning something wrong, which I later confirmed by fixing a small typo bug. I guess the idea I can transfer is that for my current toy the number of samples should correlate negatively with the distance to the computed distribution, across some set of sample counts picked arbitrarily (e.g. kept moderate to limit the amount of time spent per expression). Is this a sensible test? Is it inefficient? Is there a better test? Ideally I'd love a method that (a) lets me quantify how often I should expect spurious failures; and (b) lets me tune a parameter which reduces spurious failures by spending more effort. But randomness being randomness I guess they can't be avoided. And since I plan on randomly sampling my test expression, this is probably not something that can be done in general.
