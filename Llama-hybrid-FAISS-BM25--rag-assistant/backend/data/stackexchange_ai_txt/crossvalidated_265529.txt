[site]: crossvalidated
[post_id]: 265529
[parent_id]: 263284
[tags]: 
You can look at Figure 1 in this paper: http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf The first matrix $W$ maps one-hot vectors to word embeddings, i.e. its rows are the "input" representations, aka the word embeddings the model produces. The second matrix $W'$ is just a matrix of model's weights, it maps the hidden layer to the output vector of vocabulary size, to which you will apply the softmax. In the paper they call the columns of this matrix $W'$ as "output" representations.
