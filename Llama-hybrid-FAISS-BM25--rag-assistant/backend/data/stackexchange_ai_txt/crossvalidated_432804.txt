[site]: crossvalidated
[post_id]: 432804
[parent_id]: 432776
[tags]: 
The graph is just a probability mass function (pmf), so the length of each bar indicates the prior probability that people ascribe to each of the "concepts" listed along the y axis. This particular example is a little confusing out of context. That context is Josh Tannenbaum's PhD Thesis " A Bayesian framework for concept learning ", and is related[*] to an experiment he calls "the numbers game". Participants play with a computer,which chooses a rule or concept that generates numbers between 1-100 (e.g., "even numbers" generates 2,4,6,...,100). It doesn't tell the participants the rule itself, but instead shows them one of the generated examples, like {16}. Subjects are then asked to guess another number that the computer could also generate. Someone thinking that the rule is "powers of two" might say {32}; someone else expecting numbers beginning with 1 might respond with {100} or {11} instead. The computer then shows each participant more numbers, like {4,8, 32}, and they are asked to refine their guesses. In the numbers game, people favor concepts like "the set of all odd (or even) numbers" over others, which influences their guesses. Having seen just {16}, people tend to guess any numbers between 2-32, but after seeing {16,2,8,64}, guesses on even numbers are vastly more common. Tannenbuam argues that this is driven by a Bayesian-like updating process, which starts from a set of "intuitive" priors and refines them based on the evidence and internal 'simulations' of which could happen. A lot of his work is interesting and well-written, if this sort of thing strikes your fancy. [*] To be clear, this particular plot isn't real data; it's a hypothetical version he uses as an example, but it does match what he later finds experimentally.
