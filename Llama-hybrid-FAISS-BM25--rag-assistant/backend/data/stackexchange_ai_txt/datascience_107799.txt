[site]: datascience
[post_id]: 107799
[parent_id]: 88634
[tags]: 
So this may not be as direct of an answer as you were hoping for, but the truth is that it really depends. IF the images are properly embedded for your problem, i.e. the geometric relationship between points is truly reflective of the differences in features that you care about, then this should be simple and Nuclear Hoagie's clustering idea could work (as long as you don't have so many clusters that a number of dissimilar ones get combined when restricting the number of clusters to 100), as would a random sampling or a method that tries to find the set of 100 points that span as much of the search space as possible while remaining roughly equidistant from each other (possibly better in cases where it's very hard to define clusters due to the distribution of points). In other words, it sounds like the issues you're having are largely due to the embeddings and the data itself (i.e. blurry images should be thrown out or heavily preprocessed and sharpened so the blurry feature isn't emphasized so heavily in the embedding). Also the number of images your working with sounds very low if you're training anything from scratch as opposed to an off the shelf embedding model or something well-trained using transfer learning). How are you generating your embeddings and what kind of image preprocessing are you doing?
