[site]: crossvalidated
[post_id]: 288861
[parent_id]: 
[tags]: 
how to perform marginalization of posterior to find the predicative distribution for bayesian logistic regression

I want to know how to show that $$p(a) = \int \delta(a - w^T x) q(w) dw$$ is gaussian, where $q$ is gaussian and $x$ is fixed, and $\delta$ is the dirac function. Everything below is just some motivating background and thoughts on the problem. In bayesian logistic regression, we end up with a posterior distribution on the weights $w$ of the model. Using the laplacian approximation, we approximate $p(w|D)$ as a normal distribution (where $D$ is our dataset). Now we want to find the predicative distribution, which is $$p(y|D) = \int p(y|w,x) p(w|D) dw $$ where $y$ is the label of the new datapoint $x$ which we want to classify. $$p(y|w,x) = \sigma(w^Tx)$$ Also, using the notation from Bishop's PRML, we write $q(w)$ in place of $p(w|D)$ So now we want to solve $$\int \sigma(w^Tx)q(w) $$ Bishop evaluates this integral in the following way: $$\int \sigma(w^Tx)q(w) dw = \int \int \sigma(a) \delta(a - w^T x)\ da\ q(w) \ dw$$ (where $\delta(z)$ integrates to 1 and is 0 wherever $z \neq 0$). Reordering the integrals, we have $$\int \sigma(a) p(a) dw$$ where $p(a)$ is the distribution $$p(a) = \int \delta(a - w^T x) q(w) dw$$ Here is where my question is. According to Bishop, $p(a)$ can be seen as the marginal distribution of a joint gaussian distribution. We know this is gaussian too, so we can just solve for the mean and variance. However, I don't understand how this can be shown. I can show that integrating out one dimension of a multivariate distribution results in a gaussian marginal on the remaining dimensions. However, that doesn't seem to transfer over here, where we are integrating over all the directions orthogonal to $x$.
