[site]: datascience
[post_id]: 44331
[parent_id]: 44324
[tags]: 
Stohastic gradient descent loss landscape vs. gradient descent loss landscape I don't know how using the training data in batches rather than all at once allows it to steer around local minimum in the example, which is clearly steeper than the path to the global minimum behind it. So, stochastic gradient descent is more able to avoid local minimum because the landscape of batch loss function is different than the loss function of whole dataset (the case when you calculate the losses on all data and then update parameters). That means the gradient on the whole dataset could be 0 at some point, but at that same point, the gradient of the batch could be different (so we hope to go in other direction than the local minimum). Neural network architecture and loss landscape In order to escape the local minimum, your neural architecture can also help. For example, see this work: Visualizing the Loss Landscape of Neural Nets . It shows that skip connections can smoothen your loss landscape and, hence, help the optimizers to find the global minimum more easily. Local minimums vs global optimum Finally, there are some works suggesting that the local minimums have almost the same function value as the global optimum. See this question and answer.
