[site]: datascience
[post_id]: 76914
[parent_id]: 
[tags]: 
What does all zero mean decrease in accuracy (MDA/permutation importance) signify?

I have a model I've trained on ~3400 features with ~500 samples (40:20:20 train:test:val) that I've calculated MDA on using the eli5 package. However, all the features are zero when I calculate this with the train or test dataset. My dataset is a fairly sparse dataset with mostly zeros and I'm trying to classify disease. Here is a sample: The model is a random forest that is predicting disease class (0 or 1). the dataset is imbalanced with appx (90% 0 and 10% 1) My model accuracies are as follows: train: 0.9923469387755102 test: 1.0 val: 0.9764705882352941 confusion matrix for test [[76 0] [ 0 8]] these are my gini importances: MDAs are all zero for testing and training. Why is that and what does it mean for the generalizability of the model?
