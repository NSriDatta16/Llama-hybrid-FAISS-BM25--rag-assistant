[site]: crossvalidated
[post_id]: 501675
[parent_id]: 
[tags]: 
huge neural networks for small datasets

In this period my colleague is working on a computer vision task involving a dataset very small (it's a classification task with a number of examples for class ranging from 20 to few hundreds). She used transfer learning, trying Pytorch default networks, including ResNet 18 and ResNet 52, if I remember well. After only few epoch the nets start overfitting, but if you stop then, the performance is good, and it's quite better for ResNet 52, which, you know, is a HUGE net, counting millions of parameters. It is also quite slow of course. I think I understand why ResNet 52 can be better than its little brother ResNet 18, my explication is that the deeper architecture reflects better the hidden features that make most difference between images of different classes. Anyway, my question is not about why transfer learning works so well in this cases, my point is different and it is that there has to be a way to train a similar model, but smaller and faster. It's obvious to me that an overwhelming part of the net is not really helping, of course ResNet was actually invented with overparametrization in mind, but this dataset is so small that redundancy here is sky high. I'm thinking about optimization here: we could make the algorithm much faster. Do you know any interesting strategy? Anything we may have missed?
