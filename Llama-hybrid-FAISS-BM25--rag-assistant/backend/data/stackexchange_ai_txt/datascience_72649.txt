[site]: datascience
[post_id]: 72649
[parent_id]: 
[tags]: 
Algorithm to calculate nerual network training time?

Before starting a new machine learning side project, it would be very useful to estimate how long it will take to run 1, 10, 100, 1k epochs. A crude estimate is more than sufficient (i.e. 1 epoch would take 1 second, 10 seconds, 1 minute, 1 hour, etc..). Given the variables below, can you recommend any heuristics that could provide an estimate? Problem type (e.g. Image Segmentation) Model type (e.g. PyTorch Unet) Dataset (e.g. 10k images, 512x512) Compute (e.g AWS p2.xlarge) Library (e.g. PyTorch) Is an empirical method (e.g train on smaller subsets of the data and scale accordingly) a better approach to solving this problem?
