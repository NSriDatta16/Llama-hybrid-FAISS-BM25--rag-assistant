[site]: crossvalidated
[post_id]: 621369
[parent_id]: 621169
[tags]: 
The word2vec architecture is nicely visualized here: With regards to your question, we have the "left" embeddings are the weights, $W_l$ , of the embedding layer itself (one-hot index to vector hidden layer), and the "right" embeddings, $W_r$ , which transform these embeddings to predict for a certain class (depending on the pretraining objective). If $x$ is the input token and $\sigma$ a softmax activation, then our prediction will look like: $$\max \mathrm{\sigma} (W_r (W_lx) + b)$$ When you mention power, I assume it's predictive ability. The thought process you're following seems like If the accuracy of the prediction is coming from $W_r$ , doesn't that mean $W_l$ will be useless? But notice that in our formula above for the prediction, the inputs to $W_r$ are the weights in $W_l$ ! If $W_r$ successively predicts, then $W_l$ must have structure (in the embedding weights) that is predictable, i.e. one can train predictions (like downstream tasks in NLP) on these embeddings.
