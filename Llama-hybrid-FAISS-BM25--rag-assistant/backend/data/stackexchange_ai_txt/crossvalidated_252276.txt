[site]: crossvalidated
[post_id]: 252276
[parent_id]: 252261
[tags]: 
For clarity, I think you should replace $max_a(Q', a)$ with $max_a(Q(S', a))$ as there is only one action-value function, we are just evaluating Q on actions in the next state. This notation also hints at where the $p(s'|s, a)$ lies. Intuitively, $p(s'|s, a)$ is a property of the environment. We do not control how it works but simply sample from it. Before we call this update we first have to take an action A while in state S. The process of doing this gives us a reward and sends us to the next state. That next state that you land in is drawn from $p(s'|s, a)$ by it's definition. So in the Q-learning update we essentially assume $p(s'|s, a)$ is 1 because that is where we ended up. This is ok because it's an iterative method where we are estimating the optimal action-value function without knowing the full dynamics of the environment and more specifically the value of $p(s|s', a)$. If you happen to have a model of the environment that gives you this information you can change the update to include it by simply changing the return to $\gamma p(S'|S, A)max_a(Q(S', a))$.
