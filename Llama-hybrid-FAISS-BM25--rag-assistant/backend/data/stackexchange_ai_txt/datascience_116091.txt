[site]: datascience
[post_id]: 116091
[parent_id]: 
[tags]: 
comparison between gaussian Naive bayes and logistic regression

I am following the lectures CORNELL CS4780: Machine learning for Intelligent Systems. Link:- https://www.youtube.com/watch?v=GnkDzIOxfzI&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS&index=11&ab_channel=KilianWeinberger In the above video, professor Kilian told that instead NAIVE BAYES We want to predict P(Y=1 | X), where Y $\in$ {0,1} is the class label and X is data having several features. Rather than directly finding P(Y=1 | X), we find P(X | Y=1) and P(Y=1). (Bayes rule). Then we estimate the parameters of the distributions P(X | Y=1) and P(Y=1) and plug this estimate to get P(X | Y=1)*P(Y=1) which is $\propto$ P(Y=1 | X). Then we compare P(Y=1 | X) and P(Y=0 | X). Whichever is larger, we predict that label for X. Finding P(X | Y=1) basically means modelling observed data X using some known distribution. It is kindof like training our model. We can say that we are "approximating" observed data X using some known distribution. For gaussian naive bayes, he showed that we have a closed form for P(Y=1 | X). $$P(Y=1 | X)= \frac{1}{1+exp(-w^Tx)} $$ Now he says(at tme 28:00) that "Instead of using naive bayes, we will directly use the closed form $P(Y=1 | X)= \frac{1}{1+exp(-w^Tx)} $ i.e. instead of estimating the parameters for P(X | Y=1) and P(Y=1), we will estimate the parameter w of $\frac{1}{1+exp(-w^Tx)}$ ." And then he shows a demo(at time 44:55) of Naive Bayes classifier vs Logistic regression classifier. On the dataset that he used, the logistic regression classifier performed better than naive bayes classifier. My question is that, since we are basically making the same assumptions in both naive bayes and logistic regression, why are we getting different results ? I mean, using gaussian naive bayes, we derived some formula then we are estimating the parameters of this formula and getting better(or different) result than estimating the parameters of gaussian naive bayes. I think that parameters of naive bayes and the parameters of the formula derived by naive bayes are similar things. So, why is this happening? Kindly clarify this to me. Thanks!
