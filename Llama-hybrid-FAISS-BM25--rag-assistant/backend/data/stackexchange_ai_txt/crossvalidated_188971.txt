[site]: crossvalidated
[post_id]: 188971
[parent_id]: 
[tags]: 
Determining the range of a random distribution from a finite set of numbers generated from that distribution?

My problem is as follows: I am provided a set of 5 integers that have been randomly generated from the range of 1 to n. Using these 5 integers, I have to determine the original n. Obviously, this method won't be perfect every time, but the goal is to determine a method that will work most effectively for any 5 random numbers generated for any value of n. My friends and I came up with an extremely simple method that works either extremely well or extremely poorly. Our method is to take the maximum number from the set of randomly generated numbers, and simply multiply this number by 1.2. Our reasoning (possibly severely flawed) was that 1 and the 5 integers in the set would serve as 6 points out of 7, the seventh point being n. There are 6 segments between 7 co-linear points and the average of the known distances is the largest number divided by 5, and so our method boils down to simply multiplying the largest integer by 6/5. Since we are just beginners in the worlds of statistics, I was wondering if there is a proven solution to this kind of problem that we would not have learned yet. Can some kind of advanced statistics be used to determine a single method that will work the best overall for every value of n greater than 1?
