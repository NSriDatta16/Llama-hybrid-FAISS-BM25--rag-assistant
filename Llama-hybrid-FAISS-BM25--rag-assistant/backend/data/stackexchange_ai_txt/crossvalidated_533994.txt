[site]: crossvalidated
[post_id]: 533994
[parent_id]: 533942
[tags]: 
To add to gunes 's answer, the score should improve as you get more and more samples if you have a proper model/estimate, one which increases its complexity as $n$ , the sample size, grows. That is the whole point of statistics, to have consistent estimators as $n\to \infty$ . In other words, the typical behavior should be that things improve when increasing $n$ , if you have a good estimator. For classification error, this roughly translates to this: For good estimators, your performance eventually approaches that of the optimal Bayes classifier (the best possible and the inherent information limit of the problem). In other words, the excess risk (your estimator's risk minus the Bayes risk) should go to zero in classification as $n \to \infty$ for reasonable estimators. That the performance flattens out (say in the case of the random forest) is more a sign of underfitting . Your model's complexity reaches a limit and cannot incorporate more samples to improve, assuming that there is still room to improve (i.e., you are below that Bayes error rate). You can think about how in random forest one can increase the complexity (Hint: try keeping the number of samples you have at the leaves of the trees constant or not growing significantly with $n$ and see how it goes.)
