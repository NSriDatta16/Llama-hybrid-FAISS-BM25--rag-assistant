[site]: crossvalidated
[post_id]: 626376
[parent_id]: 
[tags]: 
Relative variable importance/explained variation from a single model fit

I am seeking a measure of relative variable importance or relative explained variation that will apply to all types of linear and nonlinear regression models and that requires only fitting one model. As an example suppose we have the model y ~ x1 + x2 + x1:x2 with the last term representing an interaction term. The model may be of any type including logistic, ordinal, and Cox proportional hazards regression models. Various importance measures have been developed based on Wald $\chi^2$ statistics, and one can compute a measure of importance of x1 by comparing the 2 degree of freedom Wald statistic for x1 and x1:x2 with the 3 d.f. Wald $\chi^2$ for the complete model. This is all done using the full model fit. This will not transport to Bayesian models though. One simple thought is to compute the linear correlation coefficient between lp1 and lp where lp1 is the portion of the linear predictor that involves x1 (which will have two terms combined into one sum, using two estimated regression coefficients) and lp is the overall model fit's linear predictor $X\hat{\beta}$ . I know that traditional $R^2$ -based measures will use two $R^2$ : from the large and reduced models and this will handle collinearities appropriately, but I'm seeking measures that work reasonable well from a single model fit. Does anyone have an idea of how well the corr(lp1, lp) approach will work or have a better idea for estimating relative variable importance from a single model fit? Measures that operate on the estimated full-model regression coefficients will work in both frequentist and Bayesian settings. Update : Another thing to try. For a relative explained variation in the response that is due to x1, let $\hat{y}$ be the full model linear predictor $X\hat{\beta}$ . Fit a linear model to the linear predictor: $\hat{y} \sim x1 + x2 + x1:x2$ . The $R^2$ from this fit will be 1.0. Quantify the relative importance of x1 by the partial $R^2$ that is due to the 2 degrees of freedom involving x1, i.e., omitting the x2 main effect from the contrast. In a linear model one can compute any needed partial $R^2$ from contrasts without refitting anything. Are there downsides or strange collinearity issues? Update 2 : This second method yields an exactly correct result when the initial model is a linear model, as the relative $R^2$ values from predicting the linear predictor, when multiplied by the $R^2$ in the original fit, give exactly the partial $R^2$ in the original fit. So I'm pretty happy with method 2. For nonlinear models it is using the linear model as a bridge to get relative explained variation. Update 3 : @Michael M discussed global surrogate models below. This would extend the update in Update 2 along the following lines: Fit a predictive model using any desired method, including random forests (if the sample size is incredibly large) Predict the predictions from that fit using a fairly saturated linear model, i.e., one that uses regression splines for all continuous predictors and includes all two-way interactions (tensor splines). If this fit has $R^{2} > 0.9$ we would call it an adequate approximation to the random forest. Use that fitted linear model to decompose explained outcome variation (relative $R^2$ ) in the random forest in an interpretable way. In the R rms package this decomposition combines any effect with higher-order effects involving that variable, so the partial $R^2$ for a predictor would count its nonlinear and interactive effects. Update 4 : I've implemented the new general purpose relative explained variation method. Explanation and examples are here . It would be nice to compare its performance with random permutation-based methods.
