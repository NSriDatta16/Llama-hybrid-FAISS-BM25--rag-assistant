[site]: crossvalidated
[post_id]: 489606
[parent_id]: 
[tags]: 
Efficient way to compare algorithms performance with missing samples

I'm currently doing some experiments to compare more than 70 different configurations (different parameters of an evolutionary algorithms) that have 30 runs each (repeated due to stochastic results). Since each configuration does not follow a normal law (which I verified with K-S test for each config), I've been advised to use a non-parametric test such as Friedman, in order to ensure that they are statistically different, and find which ones are better than others. However, I faced some issues with this test. If I understand well, the test only tell me: that all configurations follow the same distribution, or one of them has a different distribution. How do I know which one is different ? Is it the configuration 1, 2, ..., 70 ? From what I understood, I should run a post-hoc analysis of Friedman. Is that correct ? or it should be a different way ? Here I hypothesis I should go on post-hoc analysis of Friedman. I've found scikit-posthocs where I can run conover_friedman, nemeny_friedman, siegel_friedman... which one should I use ? I did not find a clear tutorial, that easily give me pros/cons for each. From some examples, it looks to return a matrix of p_values between two configurations. So for a p_values between two configuration, if it's below alpha (0.05 for example), I can reject h0, is that right ? Is it possible to rank the configurations thanks to the non-parametric tests ? based on their p-values ? maybe what i'm asking has non-sense, I don't know... If it is possible what are the existing methods ? What i'm trying to do is to say: configuration 7 > configuration 10 > configuration 3, ... or saying that configuration 7 is the best, configuration 10 is the second, and so on.. I could rank them based on the average performance of the 30 runs of each, but since they do not follow a normal law, I'm looking for another way based on these non-parametric tests. It could happen that for a configuration, some runs are missing (issues related with Out Of Memory, or Time out of the optimization which is very long). In this case, which test handle missing runs/samples ? For now I'm talking about 70 configurations to compare, but it could easily be 1000 to compare, should I go to other tests in this case ? (still with 30 runs per configuration, and sometime a little bit less, see question 4.) EDIT 1: Each run has an associated seed (for the stochastic aspect), however two different configurations with the same seed, are not necessarily considered as "paired" samples. Some of them are, some other are not. Laurent
