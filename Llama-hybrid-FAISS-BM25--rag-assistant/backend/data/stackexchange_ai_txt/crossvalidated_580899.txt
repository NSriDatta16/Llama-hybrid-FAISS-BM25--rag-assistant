[site]: crossvalidated
[post_id]: 580899
[parent_id]: 
[tags]: 
Is backprop computed for every element in the minibatch and then averaged for every weight?

I'm trying to fully understand backpropagation by computing it by hand. Often is cited that is just an alternation of the derivative of the preactivation and the derivative of the activation, however, I don't fully understand how it's actually implemented. In particular, when I calculate the derivative with respect to the non linear activation function, there is a dependence on the input (preactivation, which is obviously different for every input element), which bring me to think that the backpropagation is calculated for every element of the minibach, and then for every weight in the network, we take the mean of the gradients with respect to that However, even though this kinda works in my brain, I don't get how this is efficient, even though i think is pretty simple to parallelize, so maybe it's done as explained (for every element in the minibatch, we calculate the gradient and we take the mean of them, for every weight) and it's easy to parallelize, or I'm missing something
