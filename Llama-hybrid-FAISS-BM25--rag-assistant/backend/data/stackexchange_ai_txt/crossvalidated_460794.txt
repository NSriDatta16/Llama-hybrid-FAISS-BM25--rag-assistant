[site]: crossvalidated
[post_id]: 460794
[parent_id]: 
[tags]: 
IID Assumption in Sequential Supervised Learning

I have seen two posts about this before ( Are RNNs inherently flawed? Supervised Learning assumes IID data but sequential data is not IID , Realistically, does the i.i.d. assumption hold for the vast majority of supervised learning tasks? ), however, the answers did not really satisfy me - thus I would like to open this topic again. I am wondering how the iid assumption about our data distribution works for recurrent problems / models. Say we apply an LSTM to a sequential problem. We usually put up an iid assumption, because we use MLE to find best parameters, i.e. maximize P(X|θ), with X being the data and θ the parameters, and for calculating this break this down to P(x_1|θ) * ... * P(x_n|θ) (more specifically, assume we are doine classification, thus "data" and the x_i's are actually our predictions). Now assume our first task is classifying full sequences, i.e. having multiple points per sequence and only outputting one prediction per sequence. Then, I believe the iid assumption still correctly holds. However, when we now predict one label in each step, these depend on previous predictions, and our above made assumption and reformulation of the likelihood should be wrong - is that correct?
