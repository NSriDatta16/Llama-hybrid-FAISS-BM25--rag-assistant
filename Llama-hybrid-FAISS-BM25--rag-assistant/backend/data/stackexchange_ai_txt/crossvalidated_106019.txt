[site]: crossvalidated
[post_id]: 106019
[parent_id]: 106016
[tags]: 
0. The correlation (0.0775) is small but (statistically) significantly different from 0. That is, it looks like there really is correlation, it's just very small/weak (equivalently, there's a lot of noise around the relationship). 1. What averaging within bins does is reduce the variation in the data (the $\sigma/\sqrt{n}$ effect for standard error of a mean), which means that you artificially inflate the weak correlation. Also see this (somewhat) related issue . 2. Sure, fewer bins means more data gets averaged, reducing noise, but the wider they are, the "fuzzier" the average becomes in each bin because the mean isn't quite constant - there's a trade-off. While one might derive a formula to optimize the correlation under an assumption of linearity and the distribution of the $x$'s, it wouldn't take full account of the somewhat exploitable effect of noise in the data. The easy way is to just try a whole variety of different bin boundaries until you get what you like. Don't forget to try varying the bin-widths and bin-origins. That strategy can occasionally prove surprisingly useful with densities , and that kind of occasional advantage can be carried over to functional relationships - perhaps enabling you to get exactly the result you hoped for . 3. Yes. Possibly start with this search , then perhaps try synonyms. 4. This is a good place to start; it's a very popular book aimed at non-statisticians. 5. (more seriously:) I'd suggest smoothing (such as via local polynomial regression/kernel smoothing, say) as one way to investigate relationships. It depends on what you want, exactly, but this can be a valid approach when you don't know the form of a relationship, as long as you avoid the data-dredging issue. There's a popular quote, whose originator appears to be Ronald Coase : "If you torture the data enough, nature will always confess."
