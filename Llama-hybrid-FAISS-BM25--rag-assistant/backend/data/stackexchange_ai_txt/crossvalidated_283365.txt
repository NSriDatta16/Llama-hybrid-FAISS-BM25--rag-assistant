[site]: crossvalidated
[post_id]: 283365
[parent_id]: 200070
[tags]: 
That depends on the analysis steps following the normalization If nothing else is said, then it commonly refers to normalizing the features under consideration across all samples (e.g. to afterwards classify samples or to predict their value w.r.t to some quantitative attributes, or to conduct dimensionality reduction techniques under the requirement of avoiding some bias introduced by the hetereogeneous range of attributes) In specific fields however, in particular in analysis of microarray data, normalization along the samples is a widely used preprocessing step to remove unwanted variation during quality control (hopefully mostly technical noise, but it also affects real biological differences of course). You may e.g. want to have a look at https://en.wikipedia.org/wiki/Quantile_normalization . This normalization technique affects even both directions at the same time (samples and features): Look for the feature with the smallest value within each sample (may be a different attribute for each of the samples) Collect all these smallest values and calculate the average of them Assign this new value to the original places you took it from, so that all samples now have the same value at the attribute that originally showed the smallest value within the respective sample Do the same with the 2nd smallest value, 3rd,... until all data are processed this way Finally the range of all data is the same for any of the samples. This data set is then the basis for further processing.
