[site]: crossvalidated
[post_id]: 613239
[parent_id]: 612829
[tags]: 
To me, this seems to come from an over-reliance on interpreting the output of regression models, in contrast to estimating and interpreting specific contrasts and relationships from a model. By this I mean that a researcher should specify the contrasts they want to make (which could be all pairwise comparisons between race categories) and then compute those from the model in a model-agnostic way. Identifying estimands (i.e., quantities to estimate) that are functions of the predicted values of the model means that the model parameters themselves do not need to be interpreted. Some examples of model-agnostic estimands include the following: $E[Y|\text{race} = B, X = x]$ (the predicted value of the outcome for those with $\text{race} = B$ and covariates $X$ set to a specific profile $x$ ) $E[Y|\text{race} = B, X = x] - E[Y|\text{race} = W, X = x]$ (the contrast between the predicted value of the outcome for those with $\text{race} = B$ and $\text{race} = W$ with $X$ set to a specific profile $x$ ) $E\left[E[Y|\text{race} = B, X]\right]$ (the average predicted outcome for those with $\text{race} = B$ ) $E\left[E[Y|\text{race} = B, X]\right] - E\left[E[Y|\text{race} = W, X]\right]$ (the contrast in average predicted outcomes between those with $\text{race} = B$ and $\text{race} = W$ ) None of these quantities reference the type of model being fit or the parameterization of the model. In that sense, they are model-agnostic. Depending on the parameterization of the model, they may be equal to familiar model outputs, as I demonstrate below. We'll use the lalonde dataset from MatchIt , which contains observations from a study of the effect of a job training program ( treat ) on 1978 earnings ( re78 ) and includes several covariates, including race (with three categories, "black" , "hispan" , and "white" ). We'll consider the relationships between the racial categories and the outcome in the control group. library("marginaleffects") data("lalonde", package = "MatchIt") lalonde_c First, let's fit a model for the outcome given race and the covariates in the control group. We'll set the reference category of race to "white" to match your example. lalonde_c #> Call: #> lm(formula = re78 ~ race + age + educ + married + I(re74/1000), #> data = lalonde_c) #> #> Residuals: #> Min 1Q Median 3Q Max #> -15538 -4674 -1305 4277 18486 #> #> Coefficients: #> Estimate Std. Error t value Pr(>|t|) #> (Intercept) 2795.48 1705.87 1.639 0.1020 #> raceblack -1054.18 832.24 -1.267 0.2060 #> racehispan 685.42 949.68 0.722 0.4709 #> age -35.51 33.59 -1.057 0.2911 #> educ 248.43 118.86 2.090 0.0372 * #> married 353.07 744.98 0.474 0.6358 #> I(re74/1000) 458.51 55.38 8.279 1.65e-15 *** #> --- #> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> #> Residual standard error: 6524 on 422 degrees of freedom #> Multiple R-squared: 0.2113, Adjusted R-squared: 0.2001 #> F-statistic: 18.84 on 6 and 422 DF, p-value: We get coefficients comparing the expected values of the outcome between black and white and between hispan and white . One might ask whether the expected values of the outcome differ between hispan and black ; the model doesn't answer this question. Failing to report his contrast is precisely the bias the reviewer is identifying. Instead, we can compute the expected value of the outcome for an average covariate profile (i.e., for units that are average on the other variables) and contrast them. predictions(fit, newdata = datagrid(race = levels)) |> summary(by = "race") #> #> race Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> white 7043 400 17.62 black 5989 718 8.34 hispan 7728 848 9.12 #> Columns: race, estimate, std.error, statistic, p.value, conf.low, conf.high comparisons(fit, variables = list(race = "pairwise"), newdata = "mean") #> #> Term Contrast Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> race black - white -1054 832 -1.267 0.205 -2685 577 #> race hispan - black 1740 1107 1.572 0.116 -430 3909 #> race hispan - white 685 950 0.722 0.470 -1176 2547 #> #> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, re78, race, age, educ, married, re74 Note that no matter which contrast coding scheme you use or which category you choose to be the reference category, you will get exactly the same predicted values and contrasts between categories. For example, below we set the reference category to "black" and fit the same model and compute the same quantities. fit summary(by = "race") #> #> race Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> black 5989 718 8.34 white 7043 400 17.62 hispan 7728 848 9.12 #> Columns: race, estimate, std.error, statistic, p.value, conf.low, conf.high comparisons(fit, variables = list(race = "pairwise"), newdata = "mean") #> #> Term Contrast Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> race hispan - black 1740 1107 1.572 0.116 -430 3909 #> race hispan - white 685 950 0.722 0.470 -1176 2547 #> race white - black 1054 832 1.267 0.205 -577 2685 #> #> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, re78, race, age, educ, married, re74 You might notice that the contrasts between black and white and between hispan and white estimated from comparisons() are the same as those computed from the model coefficients. So this method isn't necessarily doing anything new when the model is simple like this one. But another benefit of this approach is that you can compute quantities that have the same interpretations as the ones above but come from models that are completely uninterpretable. For example, let's say we fit the model below, which contains interactions and polynomial terms: fit The coefficients are uninterpretable, so would you present them in a regression table? Would you give the reader a deluge of meaningless coefficient values with meaningless tests associated with them with the hopes they would find a way to interpret them correctly? I wouldn't. And yet this model is more likely to capture the true relationship between the predictors and the outcome because it is more flexible and makes fewer restrictions (i.e., it doesn't assume the relationships are purely linear and additive). Still, though, we can compute quantities with exactly the same interpretations as those above: predictions(fit, newdata = datagrid(race = levels)) |> summary(by = "race") #> #> race Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> white 7176 665 10.79 black 5313 1237 4.29 hispan 5960 1261 4.73 #> Columns: race, estimate, std.error, statistic, p.value, conf.low, conf.high comparisons(fit, variables = list(race = "pairwise"), newdata = "mean") #> #> Term Contrast Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 % #> race black - white -1862 1404 -1.326 0.185 -4615 890 #> race hispan - black 647 1766 0.366 0.714 -2815 4108 #> race hispan - white -1216 1425 -0.853 0.394 -4009 1578 #> #> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, re78, race, age, educ, married, re74 Note that this strategy also obviates the need to decide on centered vs. uncentered, standardized vs. unstandardized, raw polynomial vs. orthogonal polynomial, and any other regression adjustments that are made to enhance the interpretability of the model but don't change its fit. The model simply isn't meant to be interpreted; rather, it should be probed for interpretable quantities that could be computed no matter how the model is parameterized. So, to summarize, you (and your colleague) should do the following: Fit a model that is likely to capture the true outcome-generating process, regardless of whether it is interpretable or not. In fact, it would be better to choose an uninterpretable model (e.g., one with many interactions and polynomials) to fit the data better. Do not report the model coefficients and tests, except possibly as a table deep in the appendix. Nothing in this table should be interpreted, but it might be useful for those seeking to replicate your results to see what values your estimated coefficients take. Note that it doesn't matter how nominal variables are coded; all codings yield the same model fit and predicted values. Fitting an uninterpretable model can help sell the decision not to rely on model coefficients. Report model-agnostic quantities that answer the specific substantive questions you want to answer, e.g., whether there are disparities between race groups. Because these quantities are model-agnostic, it doesn't matter which category is the baseline category in the model. You would report all comparisons that make sense to make, whether between the majority category and minority categories or between minority categories. Only if you selectively report comparisons will bias show through; the choice of how the model is parameterized reveals no bias because it has no effect on how the reported quantities are estimated. This last point is the key point. The reviewer is commenting that how the model is parameterized reveals bias by the analyst when prioritizing a majority category as "baseline" or "normal". But this only occurs when the analyst only reports and interprets the comparisons that involve the majority category because those happen to correspond to coefficients in the model when parameterized in a specific way. Severing the relationship between the model parameters and the interpretation of the model results by choosing model-agnostic estimands eliminates this bias. Using simple models because they are more interpretable is no excuse for bad statistics practices, so don't prioritize fitting interpretable models. Fit good models, and interpret quantities that are model-agnostic in a just way.
