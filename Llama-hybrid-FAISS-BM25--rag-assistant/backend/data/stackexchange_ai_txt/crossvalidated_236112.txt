[site]: crossvalidated
[post_id]: 236112
[parent_id]: 235862
[tags]: 
Well, the original neural networks, before the backpropagation revolution in the 70s, were "trained" by hand. :) That being said: There is a "school" of machine learning called extreme learning machine that does not use backpropagation. What they do do is to create a neural network with many, many, many nodes --with random weights-- and then train the last layer using minimum squares (like a linear regression). They then either prune the neural network afterwards or they apply regularization in the last step (like lasso) to avoid overfitting. I have seen this applied to neural networks with a single hidden layer only. There is no training, so it's super fast. I did some tests and surprisingly, these neural networks "trained" this way are quite accurate. Most people, at least the ones I work with, treat this machine learning "school" with derision and they are an outcast group with their own conferences and so on, but I actually think it's kind of ingenuous. One other point: within backpropagation, there are alternatives that are seldom mentioned like resilient backproagation , which are implemented in R in the neuralnet package, which only use the magnitude of the derivative. The algorithm is made of if-else conditions instead of linear algebra. They have some advantages over traditional backpropagation, namely you do not need to normalize your data because they do not suffer from the vanishing gradient problem .
