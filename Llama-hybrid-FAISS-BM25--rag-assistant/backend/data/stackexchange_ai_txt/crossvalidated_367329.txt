[site]: crossvalidated
[post_id]: 367329
[parent_id]: 312766
[tags]: 
It's probably because learning rate scheduling is used to automatically reduce the learning rate when the optimizations reaches a plateau. Learning rate scheduling is a very common strategy for training neural networks. But I can't exclude that some other effect could be at work. Sadly, complete descriptions of the exact procedures used to train and tune networks are not always reported in peer-reviewed studies, making it very challenging to understand what, precisely, accounts for different properties of the resulting model. If the paper does not describe using a learning rate schedule, you would have to e-mail the authors to know definitively what accounts for the steep drops in the learning rate.
