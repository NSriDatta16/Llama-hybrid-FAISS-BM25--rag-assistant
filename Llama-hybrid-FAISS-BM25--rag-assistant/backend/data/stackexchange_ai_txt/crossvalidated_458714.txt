[site]: crossvalidated
[post_id]: 458714
[parent_id]: 
[tags]: 
Correlation between two variables as time series with multiple samples

Main objective I have two variables ( $A$ and $B$ ) as two time series for $n$ samples, and I would like to test if $A$ and $B$ are correlated. Let's first assume that these $n$ samples are independent to each other. Description and questions Essentially, for sample $i$ I have two paired time series: $\mathbf{a}_i = [a_{i,1}, a_{i,2}, \dots, a_{i,T_i}]$ and $\mathbf{b}_i = [b_{i,1}, b_{i,2}, \dots, b_{i,T_i}]$ . Each sample always has its said two series of the same length $T_i$ ; but across samples the number of observations varies (i.e., $T_i$ varies for different $i$ ). Q1 : I could compute Pearson's $r$ for each sample, and end up with $n$ $r$ 's (correlation coefficients). For the next step, is it reasonable to perform a test with the null hypothesis $H_0: r=0$ ? If so, what would be the appropriate test statistics? Could I use one sample t-test (one-sided, $H_1: r > 0$ ) here? Q2 : Since these are time series, should I be worrying and removing the trend before my analysis? Could I differentiate the time series and repeat the analysis mentioned in Q1? (i.e., looking at distribution of pairwise Pearson's $r$ across the samples.) Q3 : I also have similar datasets of other conditions as controls, where either the signal of $A$ or of $B$ is considered as background . (See more on detail below...) Could I instead having the null hypothesis $H_0: r_{\rm{exp-condition}} = r_{\rm{control-condition}}$ for each control condition that I have? (But I don't have same amount of samples for each condition; although they won't be paired anyway in the first place since each sample can only take one condition.) Should I perform a two-sample t-test with unequal sample size and unequal variance (Welch t-test)? More detail... This is a microscopy dataset (multi-channel confocal fluorescence images) which looks at live biological samples. The real question to ask here is: if the amount of an underlying object (molecule) $\alpha$ correlates the amount of another underlying molecule $\beta$ . This would partially help us investigating our working model where molecule $\alpha$ helps the generation of molecule $\beta$ , but only when $\beta$ already exists (and being above some concentration). The value of variable $A$ reflects the quantity of said underlying molecule $\alpha$ at the region where the data was taken; same thing goes for $B$ and $\beta$ . And for each type of molecule, there is a threshold value below which we can't tell the difference between some $\alpha$ exist vs. $\alpha$ doesn't exist -- this is what I meant by background in Q3, and we usually consider the number of $\alpha$ is negligible when the value of $A$ is below that threshold. In our experiment, if there is $\alpha$ , we could perturb the sample and make it more $\alpha$ within the region observed. So there should be a trend of increasing number of $\alpha$ (thus increasing value of $A$ ). However, I guess there could be other time-dependent factors that contribute to the overall trend, so that's why I was trying to look at the differentiated form before computing pairwise correlation coefficient (Q2). We make measurements (e.g., pixel-intensity from channel A and channel B, averaged over a region of sample -- can just be thought of as some way to measure how many $\alpha$ and $\beta$ molecules are there) for a sample over a period of time, over which two time series ( $\mathbf{a}_i$ and $\mathbf{b}_i$ ) for sample $i$ are recorded. The said procedure is repeated several times to generate the full dataset. Q4 : And here is one last point to make the analysis a bit more complex -- instead of measuring a single sample at a time ( yes , I lied in the previous paragraph...), several samples could be measured simultaneously. It's not too strong of an assumption to pretend that the $n$ samples are independent, but I would like to know if and how to take care of this aspect for the analysis, if some of the samples are not that independent.
