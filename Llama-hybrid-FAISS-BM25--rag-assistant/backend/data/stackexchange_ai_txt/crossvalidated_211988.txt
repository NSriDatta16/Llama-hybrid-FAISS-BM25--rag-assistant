[site]: crossvalidated
[post_id]: 211988
[parent_id]: 211310
[tags]: 
The two formulas give the same result. SVM assumes: $wx+b=0$ (the decision boundary $wx_{sp}+b=1$ ($x_{sp}$ is a support vector with $y=1$ (eq.1 $wx_{sn}+b=-1$ ($x_{sn}$ is a support vector with $y=-1$ (eq.2 By eq.1, $b=1-wx_{sp}$, which is MIT's notation. By eq.2, $b=-1-wx_{sn}$, adding up both sides yields $2b=-wx_{sp}-wx_{sn}$, then $b=-\frac{wx_{sp}+wx_{sn}}{2}$, which is Andrew Ng's notation. Andrew Ng's notation is more numerically stable because its is the average of two points (though theoretically there should be no difference), an even more stable way is to take the average of all the support vectors as you mentioned. In the soft margin case, not all the support vectors lie on the margin (i.e. not all of them satisfy eq.1 or eq.2). We need to take out the vectors that lie on the margin (i.e. support vectors that satisfy $0
