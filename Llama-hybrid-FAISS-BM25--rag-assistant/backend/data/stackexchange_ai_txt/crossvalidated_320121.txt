[site]: crossvalidated
[post_id]: 320121
[parent_id]: 
[tags]: 
When is a likelihood a likelihood?

The probability density function is well characterized: it has to be measurable, and must integrate up to 1, and it must be non-negative over its support. In this case, the parameter vector is seen as fixed, and housed within a family of parametric distributions, like Exp($\theta$) or Weibull or... For estimation and statistics, the concept of a likelihood is introduced. Roughly it is a function of the parameters rather than the data, since when we maximize the likelihood, we find the parameters that give the most "likely" set of data according to that family of parametric distributions rather than the other way around. However, as I understand it: the likelihood has no functional requirements as a function of $\theta$. Sure, it will be non-zero as it is the product of densities. But it may not add up to 1 if we integrate over $\theta$'s support (we would have to multiply by a prior to get that kind of Bayesian result). We then get into these various forms of likelihood: a quasi likelihood, a pseudo likelihood, conditional likelihood, a partial likelihood, and so on and so forth. These respectively represent situations when we say, "Well... it's not a proper likelihood, but I'm going to maximize it and see what happens anyway." The delegation of such titles suggests that only when we know for certain that the probability model is appropriate for the data, the use of the term "likelihood" is warranted. But would that ever be the case in most practical scenarios? Surely that involves assumptions which are neither verifiable or interesting. Take as an example the paired t-test. This is the maximum conditional likelihood. I don't care about the between pair mean differences, so by subtracting the paired observations, I can directly model the univariate within-pair mean differences. Why is it not possible to say I am using a normal probability model for the within pair differences? Can I not call such a thing a likelihood? Is there a more practical or robust way of understanding maximum likelihood estimation? Should we not be calling everything a pseudo - or quasi - likelihood in all estimation and inference?
