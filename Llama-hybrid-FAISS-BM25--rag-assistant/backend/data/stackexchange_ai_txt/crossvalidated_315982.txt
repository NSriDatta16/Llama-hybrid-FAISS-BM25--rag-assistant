[site]: crossvalidated
[post_id]: 315982
[parent_id]: 314623
[tags]: 
You correctly noticed that what both models output is the conditional probabilities for the target variable given the explanatory variables $\theta = p(y \mid x_1,x_2,\dots,x_m)$ . If the target variable $Y$ is binary, then it follows a Bernoulli distribution , and both models use the same distribution in the likelihood function. Even more, you would see the same distribution used in likelihood function by other binary probabilistic classifiers. Similarly, you could have different ways to minimize the same loss function, e.g. squared loss can be minimized using ordinary least squares, or complicated deep neural network with linear output layer using some optimizer. There is not much to derive in here, simply Bernoulli distribution is the distribution for binary data, yet you can find more details in the paper The Naive Bayes Model, Maximum-Likelihood Estimation, and the EM Algorithm by Michael Collins. The difference between the models is that naive Bayes algorithm is a generative model, while logistic regression is a discriminative model (see this paper by Jurafsky and Martin, or Ng and Jordan, 2002 ), they also make other assumptions about the data (naive Bayes assumes conditional independence of variables, logistic regression assumes specific linear functional form) and few other differences as described in this Quora thread . So both algorithms approach the same problem in a different way, using a different form of the model, different parameters, and different ways of estimating them. In naive Bayes, you estimate the conditional probabilities indirectly from the data and apply the Bayes theorem , while in logistic regression you use linear estimator, logistic link function, and Bernoulli likelihood function that is maximized to directly estimate the probabilities . When calculating BIC for naive Bayes the number of parameters $k$ in the formula is the number of probabilities to be estimated, so if the model is $$ p(y, x_1, x_2, \dots, x_m) = p(y) \prod_{j=1}^m p(x_j \mid y) $$ each $p(y)$ and $p(x_j \mid y)$ are distinct parameters to be estimated from the data, which is achieved by calculating the empirical probabilities. When using exactly the same variables, both logistic regression and naive Bayes classifier would have the same $k$ since naive Bayes has prior $p(y)$ where logistic regression has intercept $\beta_0$ plus the parameters per variable. Jurafsky D. and Martin, J.H. (August 7, 2017) [Logistic Regression.][3] [In:] *Speech and Language Processing*. Online draft. Ng, A. Y. and Jordan, M. I. (2002). [On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes.][9] [In:] *Advances in neural information processing systems*, pp. 841-848.
