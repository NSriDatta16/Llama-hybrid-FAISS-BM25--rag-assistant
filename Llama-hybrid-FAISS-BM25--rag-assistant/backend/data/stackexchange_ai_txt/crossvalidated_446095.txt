[site]: crossvalidated
[post_id]: 446095
[parent_id]: 446085
[tags]: 
A key idea to remember about a test or validation set in machine learning is that you use it to mimic the real application of using your model to make predictions on data where you truly don’t know the answer. Do your split; then ignore the holdout data for the remainder of the training. In your case, this is exactly what Cagdas Ozgenc described: diagonalize the covariance matrix of just the training data, and then use those eigenvectors on the holdout data to create the out-of-sample principal components. Once you’re happy with how you’ve tuned your model, then you would combine all data and train the model on everything, but at that point, you’ve decided that you think the model is sufficiently general and will not overfit so badly. This would be like Apple training Siri on 800 in-sample data points and then validating on 200 out-of-sample data points. Then they figure out what model they want. Then they train the model parameters on all 1000 data points, and that model is what makes it into iPhones that people buy.
