[site]: crossvalidated
[post_id]: 260490
[parent_id]: 260172
[tags]: 
It's worth noting two general facts of MDPs and IRL: The reward function is a function mapping $S \times A \times S$ to the real line. (Many examples use only $s'$ to simplify exposition.) This does not differ from reinforcement learning to inverse reinforcement learning: The goal of IRL is to produce a function that explains observed, optimal behavior. Together, these two facts demonstrate that the form of the function output by IRL depends entirely on the state and action space. How should we visualize this function? That can get hairy pretty quickly. Let's take a simple, deterministic grid world wherein reward depends only on the new state $s'$. You can think of the reward function for this problem as a set of rewards corresponding to state; visualize it as a heat map of the grid, with darker squares corresponding to higher rewards. Now suppose that reward depends on the direction taken to arrive in the state. You'll now need four maps, each with a blank row or column. (One can't move rightward into the left-most square, e.g.) Suppose finally that transitions are not deterministic, and that rewards depend on the full tuple $s,a,s'$. I'll leave this one to the interested, but suffice to say it depends on the transition function. (You'll need more maps.)
