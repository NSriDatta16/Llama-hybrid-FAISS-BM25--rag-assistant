[site]: crossvalidated
[post_id]: 357458
[parent_id]: 348621
[tags]: 
Interesting question. As it happens, in our 1990 paper with Diebolt , we do something similar by modifying the likelihood from a regular mixture likelihood to a likelihood that is the marginal of the completed likelihood (meaning considering allocations as well as observations) such that no component can be empty or correspond to a single observation. The reason for that restriction is to allow for improper priors on all parameters of the mixture. The resulting Gibbs sampler then rejects allocations such that any component is empty or corresponds to a single observation, which means keeping the previous allocation vector $(z_1,\ldots,z_n)$ and re-updating the parameter vector, almost the dual of what you are proposing. This scheme is valid against the new likelihood. Later, in 2000, Larry Wasserman established that this model produces convergent inference about the parameters. To answer more directly the question, I do not think that un-changing a single component parameter if no allocation occurs for that component is a valid MCMC move. All component parameters have to remain the same.
