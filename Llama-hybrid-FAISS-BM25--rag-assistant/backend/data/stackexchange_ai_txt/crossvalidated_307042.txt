[site]: crossvalidated
[post_id]: 307042
[parent_id]: 307036
[tags]: 
Many deep learning models learn their own features from the raw input data during training (e.g., 2D Convolutional Neural Networks for images). So in many cases, you don't even have to worry about passing variables explicitly to your model. In some other cases, you still need features, but only core features (e.g., words in NLP). These features are represented as vectors in an embedding space that captures similarity (e.g., that 'president' is close to 'Obama'). The embedding space either comes from unsupervised pre-training (word2vec, glove) or is initialized randomly, and the vectors are tuned during training via backpropagation. The architecture of the network is responsible for learning feature combinations, like the difference between 'not bad, quite good' and 'not good, quite bad'. The 'Feature combinations' paragraph of Section 3 of Goldberg, Y. (2015). A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57, 345-420. very well explains this (I really recommend reading the whole Section 3, it is excellent): The combination features are crucial in linear models because they introduce more dimensions to the input, transforming it into a space where the data-points are closer to being linearly separable. On the other hand, the space of possible combinations is very large, and the feature designer has to spend a lot of time coming up with an effective set of feature combinations. One of the promises of the non-linear neural network models is that one needs to define only the core features. The non-linearity of the classifier, as defined by the network structure, is expected to take care of finding the indicative feature combinations, alleviating the need for feature combination engineering.
