[site]: datascience
[post_id]: 124210
[parent_id]: 
[tags]: 
What does "explicit latent variable optimization" mean?

The paper MotionLM: Multi-Agent Motion Forecasting as Language Modeling states: Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective However, I am not sure what "explicit latent variable optimization" means exactly. I understand how models like image-generating GANs sample from the latent space (ex. Gaussian noise) to generate real images (image space). I looked at one of the referenced papers PRECOG and saw that they mention latent variables: but I can't come up with a concise idea of what explicit latent variable optimization means. PRECOG samples from Gaussian noise (I believe this is the latent space) and then uses the samples in their loss calculation but only the model mapping from Gaussian noise to to multi-agent trajectories $\mathbb{S}$ is learned - the latent space remains Gaussian noise sampled from a $\mathbb{Z}\sim\mathcal{N}(\mathbb{O},\mathbb{I})$ distribution. Optimizing over the latent space seems to me like it would require changing the parameters of the normal distribution.
