[site]: datascience
[post_id]: 55353
[parent_id]: 45475
[tags]: 
Your understanding is not correct: in the encoder-decoder attention, the Keys and Values come from the encoder (i.e. source sequence length) while the Query comes from the decoder itself (i.e. target sequence length). The Query is what determines the output sequence length, therefore we obtain a sequence of the correct length (i.e. target sequence length). In order to understand how the attention block works maybe this analogy helps: think of the attention block as a Python dictionary, e.g. keys = ['a', 'b', 'c'] values = [2, 7, 1] attention = {keys[0]: values[0], keys[1]: values[1], keys[2]: values[2]} queries = ['c', 'a'] result = [attention[queries[0]], attention[queries[1]]] In the code above, result should have value [1, 2] . The attention from the transformer works in a similar way, but instead of having hard matches , it has soft maches : it gives you a combination of the values weighting them according to how similar their associated key is to the query. While the number of values and keys has to match, the number of queries is independent.
