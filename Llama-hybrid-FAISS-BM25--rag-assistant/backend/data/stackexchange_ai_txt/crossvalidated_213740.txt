[site]: crossvalidated
[post_id]: 213740
[parent_id]: 213569
[tags]: 
Hidden Markov Model assumes a relationship between $y_n$ and $y_{n+1}$. For example say we are doing natural language processing, and $y_n$ denotes the $n$-th world in a sentence. If we know $y_n$ is "stack" then the probability of $y_{n+1}$ being "overflow" might be higher than knowing $y_n$ being something else say "cat". While Naive Bayes does not make that assumption, instead it assumes that the observation sequence is i.i.d . Its more like $y$ is a random word from a random sentence, then knowing $y_n$ does not affect $y_{n+1}$. Moreover, "plugging in the previous state(Y-1) as a feature(Xn) on the Naive Bayes" would make it a "reversed" Markov chain, as the arrow is now from $y_n$ to $y_{n-1}$. It assumes the same relationship if, in that natural language processing case, you read from right to left.
