[site]: crossvalidated
[post_id]: 376014
[parent_id]: 375996
[tags]: 
Linear regression is defined in terms of a linear function: $$ \hat y = w_1 x_1 + w_2 x_2 + \dots + w_k x_k + b $$ where $\hat y$ is the prediction for the target variable $y$ , $x_1,x_2,\dots,x_k$ are the features, $w_1,w_2,\dots,w_k$ are the weights and $b$ is a bias term. In terms of neural networks, you have multiple inputs $x_1,x_2,\dots,x_k$ and a single output $\hat y$ . So there is a single densely-connected layer, a single output layer with one unit and no hidden layer. There is also no activation (or link function ), as with the link function we would rather be talking about generalized linear models (e.g. logistic regression ). But there is more in the definition of GLMs than just the link function. Linear regression means also using squared loss and no regularization. Using linear regression for predicting binary outputs is a suboptimal choice , same for counts , and there are specialized GLMs for many different problems . So basically yes, we define and use linear regression for continuous outputs. All this said, I don't really think that calling linear regression a neural network makes much sense. Sure, there are people who will call it a neural network or even artificial intelligence , but unless you want to monetize your startup and need to make much marketing fuss around the big data blockchain deep learning artificial intelligence that you do, it would sound rather ridiculous.
