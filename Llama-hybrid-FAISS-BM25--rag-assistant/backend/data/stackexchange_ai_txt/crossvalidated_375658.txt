[site]: crossvalidated
[post_id]: 375658
[parent_id]: 375655
[tags]: 
This is an example test case, so its worth building up slowly perhaps. for your particular test case there seems a natural neural network implementation with relus (remember that in 1 D relus implement a piecewise linear function). namely : 1 layer of relus take difference in coordinates and approximates each of the squared functions. so consider each of your differences, and plot the full range of differences vs the square of difference). How many 'knot points' would you need to approximate the quadratic function by piecewise linear steps - thats the number of relus you need (for each coordinate difference). I am guessing you will need 1000s of relus? Now you need a layer to implement the square root. again plot the sum of squared differences vs its square root. How many relus do you need (for your range of values)...? finally you just need the output node that sums them all up. TLDR - I suspect you need a much larger number of relus. [ so a simple 'closed form' solution to this problem (with regular relus) is just place knot points (ie bias value) equally spaced over your input range (for each function), and weight value to second derivative at the knot point of the function you are trying to approximate (squared /squareroot).] [![#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Created on Tue Nov 6 22:22:01 2018 """ import matplotlib.pyplot as plt import numpy as np start = 1e6 stop = 1e7 knot_spacing = 1e5 data_spacing = 1e4 knots = np.arange(start, stop, knot_spacing) data = np.arange(start, stop, data_spacing) def relu(x): return np.where(x>0, x, 0) grad_xsq = (knots\[1:\]**2 - knots\[:-1\]**2)/knot_spacing # 1st grad is actual delta, then following are change in delta's grads = np.insert((grad_xsq\[1:\] - grad_xsq\[:-1\]), 0, grad_xsq\[0\]) relu_approx = (knots\[0\] ** 2 + relu(grads\[np.newaxis, :\] * (data\[:,np.newaxis\] - knots\[np.newaxis, :-1\])).sum(axis=1)) mat = np.stack((data, data **2, relu_approx), axis=1) ax=plt.gca() ax.plot(mat\[:,0\], mat\[:,1:\]) ax.legend(\['quadratic', 'relu'\]) ax.legend() rmse = np.sqrt(((mat\[:,1\] - mat\[:,2\])**2).mean()) print('rmse is {:.2E} with {} knots'.format(rmse, len(knots)))][1]][1]
