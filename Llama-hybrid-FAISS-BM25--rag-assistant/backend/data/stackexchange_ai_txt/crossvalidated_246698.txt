[site]: crossvalidated
[post_id]: 246698
[parent_id]: 246695
[tags]: 
I would begin by characterizing the observed data with a time series model (arima + identified determistic dummy indicators ) that reasonably separate signal from noise. With that model ( and estimated parameters ) I would then inject random errors to get a realization/synthetic data set. I would do this N times to get a family of possible realizations. One of the issues that should be addressed as you construct your useful model is to incorporate identifiable deterministic structure such as pulses/level shifts/time trends/seasonal pulses . Clearly pulses can arise at any time so your procedure/script should encode such possibilities while level shifts/time/trends/seasonal pulses would be invariant/fixed in your synthetic data. I am currently delivering/researching simulated forecasting providing a family of forecasts for each period in the future based upon a model and a set of parameters see AR(1) forecasting . Your very interesting question may motivate me to routinely deliver the N traces that you are after as they become actionable input to a decision-theoretic post processor. The "fact" that you can characterize the observed set as normal is insufficient as the observations are auto-correlated thus simulation needs to incorporate this characteristic.
