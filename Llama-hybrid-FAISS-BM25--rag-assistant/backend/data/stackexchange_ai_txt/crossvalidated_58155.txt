[site]: crossvalidated
[post_id]: 58155
[parent_id]: 
[tags]: 
Combined individual error

This is from my college project management course. Reading through an example question here it says: When estimating in parts, the total error will be less than the sum of the part errors. This makes sense to me. For a 1000-hour job with estimating accuracy of Â± 50%, the estimate range is from 500 to 1500 hours. This also makes sense If the estimate is independently made in 25 parts, each with 50% error, the total would be 1000 hours, as before and the estimate range would be from 900 to 1100 hours Okay, How did we get that? To combine independently-made estimates Add the estimated values. Combine the variances (squares) of the errors. Cool, following you... With 25 estimates for a 1000-hour job Each estimate averages 40 hours The standard deviation is 50%, or 20 hours Here's where it stops making sense. It goes on to take the square root of sum of the variances which makes sense, but where did he get the standard deviation being 50%? I could be missing something here?
