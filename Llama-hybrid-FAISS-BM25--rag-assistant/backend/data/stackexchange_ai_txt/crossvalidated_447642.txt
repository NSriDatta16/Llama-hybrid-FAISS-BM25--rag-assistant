[site]: crossvalidated
[post_id]: 447642
[parent_id]: 447640
[tags]: 
A proper way to update your prior would be to apply Bayes: $$p(\theta|E) =\frac{p(E|\theta)}{p(E)}p(\theta) = \frac{p(E|\theta)p(\theta)}{\int p(E|\theta')p(\theta')d\theta'}$$ Sometimes, it's something you just have to ram into an MCMC or something, but in this case we don't need to - we can use a conjugate prior . What a conjugate prior buys us, is that a particular combination of distribution's integral has a closed-form solution . For Bernoulli, the conjugate prior is given by the Beta distribution . It makes sense, behaves nicely, and its domain contains the values we want, let's go with it. I'm interpreting 'assume ignorance' as using the most diffuse prior that gives you the probability you want, i.e. $0.5$ . So, let's start with a $Beta(1,1)$ prior, which has a mean of 0.5 and is uniform over $(0, 1)$ . The posterior predictive distribution in this case is simply given by: $$ p(Wake) = \frac {1+successes} {(1+successes) + (1+failures)} = \frac {1+successes} {2+observations} $$ The two 1s in the denominator comes from the fact that we used the $Beta(1, 1)$ prior; they essentially act as pseudo-observations , and if we used, say, $Beta(2, 3.5)$ instead - we'd be using 2 & 3.5 accordingly. One way to think about it is that it doesn't make sense to admit not waking up on time as an option if you had never observed anyone oversleeping, so you observed it at least once before. Bear in mind this is only an illustrative analogy; Beta can take a lot of weird parameters like, say, 0, or $e$ , or whatever. Anyway, in the particular case you mentioned with just one observation, your new estimate would be: $$p(Wake) = \frac {1+1}{2+1} = \frac {2}{3} = 66.6(6)\%$$
