[site]: crossvalidated
[post_id]: 351685
[parent_id]: 
[tags]: 
Conditional probability of posterior distribution for bayesian linear regression

In Deep Learning Chapter 5.6, Bayesian Linear Regression is introduced. I'm confused by the following formula: $$ p(w | X, Y) \propto P(Y | X, w) P(w)$$ $X$ is a sample vector input data. $Y$ is the corresponding vector of output data. w is the scalar we want to estimate the linear function. How is this derived? My attempts to derive this result in the following: $$ \begin{align} P(w | X, Y) & = \frac{P(w, X, Y)}{P(X, Y)} \\ & = \frac{P(X)P(w|X)P(Y|w, X)}{P(X)P(Y|X)}\\ & \propto P(w|X)P(Y | X, w) \end{align}$$ Apply the definition of conditional probability. Use the chain rule to expand numerator and denominator. Use proportionality to throw away all parts only involving $X$ and $Y$. My derivation results in $P(w|X)$, where the book reads $P(w)$. Note: This example features univariate regression, whereas the book actually introduces multivariate regression. The derivation, however, should remain the same. Just change $w$ to a vector and $X$ to a matrix.
