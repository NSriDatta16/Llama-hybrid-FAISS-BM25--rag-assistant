[site]: crossvalidated
[post_id]: 113490
[parent_id]: 
[tags]: 
Transforming time series to compensate for change in variance

I have a time series (shown below) that comes from a sensor whose calibration was changed in the middle of last year. As part of this change, the sensor's reading of the variance (or volatility) of the observations increased, although actually, this volatility did occur before the change in calibration, but it was not being fully detected. As the data the sensor detects is highly seasonal (at both the weekly and yearly level), I want to create an accurate forecast of what the future readings will be, using the new level of calibration. I don't want to throw out the data from before the calibration, as we have only collected a little more than a year's worth of data. I've tried traditional transformations (like Box-Cox) and a seasonal decomposition using stl() in R's forecast package, but the transformation does not solve this variance issue, as there's different variances at the same level of the data (say at 0.0, for example). Are there any other techniques are there (that are available in the R programming language) that allow me to transform the data from before mid-2013 to match the variance of the data that comes later?
