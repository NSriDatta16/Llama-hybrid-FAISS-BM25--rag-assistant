[site]: crossvalidated
[post_id]: 156240
[parent_id]: 
[tags]: 
Expected Euclidean distance between normal prior and posterior as sample size changes

Suppose the prior for some random variable $x$ is normal with mean $\mu$ and variance $v$. Denote the prior density by $f(x|\mu,v)$. Given normal likelihood with known variance $v$, the posterior for $x$, after obtaining $n$ iid samples with average $m$, is also normal with mean $\mu'=\frac{\mu+nm}{n+1}$ and variance $v'=\frac{v}{n+1}$. Denote by $f(x|\mu',v')$ the posterior density. Normal prior and normal likelihood also imply that ex ante distribution for sample average $m$ is also normal, with mean $\mu_m=\mu'$ and variance $v_m=\frac{n+1}{n}v$. The expected Euclidean distance between the prior and posterior is $$ \int\left[\int[f(x|\mu,v)-f(x|\mu',v')]^2\mathrm dx\right]f(m|\mu_m,v_m)\mathrm dm \tag{1} $$ I want to see how additional samples may change the distance. And if we treat $n$ as a continuous quantity, then we could differentiate $(1)$ with respect to $n$. Is there a known closed form for such a derivative? If not, could anyone give me some hint on how to find this derivative? In particular, since $m=\frac1n\sum_{i=1}^nx_i$, differentiating $n$ would imply differentiating with respect to a summation limit. Is there an analog of the Leibniz rule for summation that I can use here?
