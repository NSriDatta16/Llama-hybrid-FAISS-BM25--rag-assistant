[site]: crossvalidated
[post_id]: 252636
[parent_id]: 250269
[tags]: 
Thanks everyone for further interesting discussion. Rather than making my comments, point by point, I’ll offer some general reflections. Bayes. I have nothing at all against Bayesian approaches. From the beginning I’ve expected that a Bayesian analysis, assuming a flat or diffuse prior, would give the same or very similar prediction intervals. There is a para on p. 291 in the 2008 article about that, partly prompted by one of the reviewers. So I’m pleased to see, above, a working through of that approach. That’s great, but it’s a very different approach from the one I took. As an aside, I have chosen to work on advocacy of confidence intervals (the new statistics: effect sizes, CIs, meta-analysis) rather than Bayesian approaches to estimation (based on credible intervals) because I don’t know how to explain the Bayesian approaches to beginners sufficiently well. I haven’t seen any truly introductory Bayesian textbook that I feel I could use with beginners, or that is likely to be found accessible and convincing by large numbers of researchers. Therefore, we need to look elsewhere if we want to have a decent chance of improving the way researchers do their statistical inference. Yes, we need to move beyond p values, and shift from dichotomous decision making to estimation, and Bayesians can do that. But much more likely to achieve practical change, imho, is a conventional CI approach. That’s why our intro statistics textbook, recently released, takes the new statistics approach. See www.thenewstatistics.com Back to reflections. Central to my analysis is what I mean by knowing only the p value from the first study. The assumptions I make are stated (normal population, random sampling, known population SD so we can use z rather than t calculations as we conduct inference about the population mean, exact replication). But that’s all I’m assuming. My question is ‘given only p from the initial experiment, how far can we go?’ My conclusion is that we can find the distribution of p expected from a replication experiment. From that distribution we can derive p intervals, or any probability of interest, such as the probability that the replication will give p The core of the argument, and perhaps the step worth most reflection, is illustrated in Figure A2 in the article. The lower half is probably unproblematic. If we know mu (usually achieved by assuming it equals the mean from the initial study) then the estimation errors, represented by the thick line segments, have a known distribution (normal, mean mu, SD as explained in the caption). Then the big step: Consider the upper half of Figure 2A. We have NO information about mu. No information—not any hidden assumption about a prior. Yet we can state the distribution of those thick line segments: normal, mean zero, SD = SQRT(2) times the SD in the lower half. That gives us what we need to find the distribution of replication p . The resulting p intervals are astonishingly long—at least I feel astonishment when I compare with the way p values are virtually universally used by researchers. Researchers typically obsess about the second or third decimal place of a p value, without appreciating that the value they are seeing could very easily have been very different indeed. Hence my comments on pp 293-4 about reporting p intervals to acknowledge the vagueness of p . Long, yes, but that doesn’t mean that p from the initial experiment means nothing. After a very low initial p , replications will tend, on average, to have smallish p values. Higher initial p and replications will tend to have somewhat larger p values. See Table 1 on p. 292 and compare, for example, the p intervals in the right column for initial p = .001 and .1—two results conventionally considered to be miles apart. The two p intervals are definitely different, but there is enormous overlap of the two. Replication of the .001 experiment could fairly easily give p larger than a replication of the .1 experiment. Although, most likely, it wouldn’t. As part of his PhD research, Jerry Lai, reported ( Lai, et al., 2011 ) several nice studies that found that published researchers from a number of disciplines have subjective p intervals that are far too short. In other words, researchers tend to under-estimate drastically how different the p value of a replication is likely to be. My conclusion is that we should simply not use p values at all. Report and discuss the 95% CI, which conveys all the information in the data that tells us about the population mean we are investigating. Given the CI, the p value adds nothing, and is likely to suggest, wrongly, some degree of certainty (Significant! Not significant! The effect exists! It doesn’t!). Sure, CIs and p values are based on the same theory, and we can convert from one to the other (there’s lots on that in Chapter 6 of our intro textbook). But the CI gives way more information than p . Most importantly, it makes salient the extent of uncertainty. Given our human tendency to grasp for certainty, the extent of the CI is vital to consider. I’ve also attempted to highlight the variability of p values in the ‘dance of the p values’ videos. Google ‘dance of the p values’. There are at least a couple of versions. May all your confidence intervals be short! Geoff
