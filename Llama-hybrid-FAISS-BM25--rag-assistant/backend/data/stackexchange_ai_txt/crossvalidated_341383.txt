[site]: crossvalidated
[post_id]: 341383
[parent_id]: 339192
[tags]: 
One thing you can do is to compare the ability of several models (probably linear with lognormal link to outcome) to predict the actual outcome. A model containing only the (logarithm of) expert assesment as predictor A model containing only some objective values (e.g. budget, genre, ...) as predictor A model containing all of the predictors The difference in residual error is a measure of the amount of information the predictors add. Also in case of model 1) the intercept is a measure of bias. Since your sample size is low, it might make sense to take a full Bayesian treatment which will work correctly even with small sample size (e.g. you will get a posterior distribution of the variables of interest and if your data do not contain a lot of information this will be reflected as a wide posterior distribution). Further you may want to use a hierarchical model to pool some of the coefficients together and avoid overfitting. I made an example for a slightly similar case in another answer . If you are using R, rstanarm might be a good choice covering all of the above. Also, as pointed out in the comments, having some kind of deviation of your prediction might be useful and it is also IMHO at least a good excercise for experts to try to guess their uncertainty (most likely they will underestimate it). EDIT: To answer the question fully: the only thing you can learn about a single prediction is its actual error - and you already know it, there is no statistics for single instances. Fitting the models I described will tell you something about the behavior portfolio-wide. If you had a lot of data, you might be able to investigate interactions, e.g. "are experts better at judging high budget films?", most modelling packages let you predict with interactions easily, but you need a lot of data to be able to estimate them reliably. I could go on for a while, so please ask for details/clarifications in the comments if this line of reasoning is compelling to you.
