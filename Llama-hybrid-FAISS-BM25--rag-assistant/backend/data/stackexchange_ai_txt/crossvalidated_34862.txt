[site]: crossvalidated
[post_id]: 34862
[parent_id]: 33867
[tags]: 
The right way to monitor convergence is actually to measure the very objective you are optimizing. In your case, this is the average of the error over the whole training set. So the correct way would be to do a pass over the training set evere $k$ iterations only for the purpose of calculating your loss. This is of course terribly inefficient for big data sets, but everything else is basically a heuristic. Here are some that I use: Calculcate the loss on a fixed batch every $k$ iterations (use as many samples as you can afford computation time wise). This removes the stochasticity. Ideally, you would do this on a held out validation set that you do not use for estimation of the gradient and which also tells you when to stop optimizing. (That is, when that error rises). Take a moving average (similar to you second plot). Fit an exponential to your error curve and see whether the gradient of that fit is low. Check the absolute values of your gradients and stop if they fall below some threshold.
