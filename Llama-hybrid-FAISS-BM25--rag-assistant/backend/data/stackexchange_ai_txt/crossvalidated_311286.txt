[site]: crossvalidated
[post_id]: 311286
[parent_id]: 310904
[tags]: 
In multinomial logistic regression, $$ p(k) = \frac{e^{x\beta_k}}{\sum_i e^{x\beta_i}} $$ where $i, k$ are possible class labels, $x$ - input data, $\beta_i$ - coefficient vector for the class $i$. Given class $k$ and base class $j$, log-odds are calculated as $$ \log\frac{p(k)}{p(j)}=\log\frac{e^{x\beta_k}}{e^{x\beta_j}} = x(\beta_k-\beta_j) $$ The object is classified as $k$ instead of $j$ if $p(k)>p(j)$, that is, if $x(\beta_k-\beta_j)>0$. The last inequality is linear in $x$. That's why decision boundary of logistic regression is always linear. Orientation of the decision boundary between $k$ and $j$ is determined by $\beta_k$ and $\beta_j$. If elements of $\beta_k$ are much larger than of $\beta_j$, this orientation is determined mostly by $\beta_k$. Thus, if there is a class $k$ with especially large $\beta$, all the decision boundaries would be dominated by it, and would be nearly parallel.
