[site]: crossvalidated
[post_id]: 624851
[parent_id]: 360227
[tags]: 
The word embeddings are inputs to both the forward LM and the backward LM. This is stated explicitly in the paper, but I like the visualization of it in Figure 3 of Devlin et al. (2019). For the sake of averaging, they nevertheless treat the forward and backward representations as distinct, so word_emb shows up twice. The lstm_outputs_1 and lstm_outputs_2 values are concatenations of the forward and backward hidden states, for each layer (1 and 2). Each LM has a 512-dimensional hidden state. When the authors perform averaging, they essentially concatenate the 512-dimensional word embeddings word_emb to themselves to make a 1024-dimensional vector per word. (Think of it however you want: either (a) this is the base layer, so they have three 1024-dimensional representations that they average, or (b) they have a forward model whose 3 representations they average, then the same for the backward model, and they concatenate the averaged forward and averaged backward representations.)
