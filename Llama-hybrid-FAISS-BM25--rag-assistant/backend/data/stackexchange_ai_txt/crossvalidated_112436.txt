[site]: crossvalidated
[post_id]: 112436
[parent_id]: 112300
[tags]: 
That's an interesting problem. I think it nicely highlights the power of Bayesian statistics as you can build these custom models based on how you think your data was generated and then invert them. To answer your question: I would suggest to instead do the mixture inside of the likelihood where you can choose the specific likelihood function for each mixture component. I rewrote your model a bit (and had to change some parameters and priors to make it work). You might need to adapt it to fit your bill but it should demonstrate the trick with the custom likelihood function I alluded to. import numpy as np from matplotlib import pyplot as plt import pymc as pm import scipy.stats as stats # DGP parameters N = 300 p0 = 0.5 alpha0 = 0.3 beta0 = 0.2 sigma0 = 0.01 sigma_noise = 1.0 # DGP x = np.random.normal(0.5, 0.2, N) y_line = np.random.normal(alpha0+beta0*x,sigma0) line_bool = np.array(stats.bernoulli.rvs(p0, size=N)) y_rand = np.random.normal(0.5,0.2, N) y_all = y_line*line_bool + y_rand*(1-line_bool) # plot fig = plt.figure(figsize=(8,8)) ax1 = fig.add_subplot(111,aspect='equal') ax1.scatter(x[line_bool==0], y_all[line_bool==0],c='b') ax1.scatter(x[line_bool==1], y_all[line_bool==1],c='r') # prior for the assignment probability is a beta distribution that puts a lot of probability mass near 0.5 p = pm.Beta("p", 1, 10, value=0.5) assignment = pm.Bernoulli("assignment", p, size=N) # priors for the line parameters intercept_prior = pm.Normal('intercept', 0, 10**-2) slope_prior = pm.Normal('slope', 0, 10**-2) eps = pm.HalfNormal("eps", .1) # Define a custom log-likelihood @pm.stochastic(observed=True) def y_obs(value=y_all, assignment=assignment, intercept=intercept_prior, slope=slope_prior, eps=eps): # Define regression logp = 0 if np.any(assignment): if np.allclose(eps, 0): eps = 0.001 center_line = intercept + slope * x[assignment] logp += pm.normal_like(value[assignment], center_line, eps**-2) if np.any(~assignment): # For the other points, chose whatever likelihood you want. logp += np.log(1) * np.sum(~assignment) #logp += pm.uniform_like(value[~assignment], -1, 1) return logp # define the model model = pm.Model([p, assignment, intercept_prior, slope_prior, eps]) # sample from the posterior mcmc = pm.MCMC(model) map_ = pm.MAP( model ) map_.fit() mcmc.sample(50000, 20000, 3) # look at the posterior from pymc.Matplot import plot as mcplot mcplot(mcmc.trace("p", 2), common_scale=False) mcplot(mcmc.trace("intercept", 2), common_scale=False) mcplot(mcmc.trace("slope", 2), common_scale=False) mcplot(mcmc.trace("eps", 2), common_scale=False) fig = plt.figure(figsize=(8,8)) ax1 = fig.add_subplot(111, aspect='equal') ax1.scatter(x[line_bool==0], y_all[line_bool==0],c='b') ax1.scatter(x[line_bool==1], y_all[line_bool==1],c='r') for a, b in zip(mcmc.trace('intercept')[::50], mcmc.trace('slope')[::50]): ax1.plot(x, a + x*b, alpha=.2, color='.5') As you can see, for the noise I used a constant density that has mass everywhere (ill defined) but you can replace it with whatever you like. Here is a posterior predictive plot that shows it's recovering the linear regression:
