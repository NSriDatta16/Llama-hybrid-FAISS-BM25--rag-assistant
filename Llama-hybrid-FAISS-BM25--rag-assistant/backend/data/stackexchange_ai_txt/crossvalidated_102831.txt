[site]: crossvalidated
[post_id]: 102831
[parent_id]: 102828
[tags]: 
Shouldn't bigram tokenization reduce the amount of shared tokens across the classes and increase the predictive power of the non-shared tokens as a result (at least marginally)? By using bigrams instead of single words, you are also reducing your amount of data, in that $n_{bigrams} I am not an expert in NLP, but I think the same logic can be applied to stop words. I would test how these are affected by increasing your data set size if labelling is not very time consuming.
