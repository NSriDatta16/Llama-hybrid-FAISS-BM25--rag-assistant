[site]: crossvalidated
[post_id]: 547380
[parent_id]: 
[tags]: 
Removing trend and seasonality does not seem to result in a stationary time series?

I have some sales data, that I want to do time series analysis on. On the plot there are clear trend and seasonality visible. To test whether a series is stationary I have created a function that performs Augmented Dickey-Fuller test as well as plots the means, standard deviations and autocorrelations of 3 partitions of a series. Here are the results for the original data. I have then tried removing trend and stationarity in two ways. Taking lag 1 difference for the trend and than lag 52 (the cycle is a year and the data points are in weeks) for seasonality. Modeling the trend with linear regression and subtracting values of the regression line, than modeling the seasonality with a polynomial and subtracting its values as well. I am a little confused by the results and not sure how to interpret them. Removing trend and stationarity, regardless of the method, resulted in greater differences between means, standard deviations and autocorrelations in partitions. Only the standard deviations were similar, when using the second method. Here are the results for the first method and the second one. Should I favour the first method since the p-value is smaller? Should I disregard the means, standard deviations and autocorrelations, that test for weak stationarity and say the series is stationary based just on the plot and Dickey-Fuller test? Both of the resulting series look kinda similar and stationary, but I am not sure how to proceed. Edit: Those are the plots for the adjusted time series. Method 1: Method 2:
