[site]: crossvalidated
[post_id]: 186447
[parent_id]: 186434
[tags]: 
There are priors that encode variability. However, these can't be priors over the parameters of a distribution of an individual example. These priors have to involve multiple examples. For one example of encouraging variability in latent features, see Wang, Ye, and David B. Dunson. "Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process." Advances in Neural Information Processing Systems. 2015. Here's a simple exampe: If the $x_i$ are your data points, then a likelihood term $$\sigma\left(\frac{(x_i-x_j)^2}{\theta}\right) \qquad \forall i\ne j$$ where $\sigma$ is the logistic sigmoid, encourages variability. A large $\theta$ pushes the $x_i$s apart. You can still encode your uncertainty by having wide priors on the $x_i$. Note how this constraint is over pairs of examples â€” a constraint over an individual example cannot enforce variability.
