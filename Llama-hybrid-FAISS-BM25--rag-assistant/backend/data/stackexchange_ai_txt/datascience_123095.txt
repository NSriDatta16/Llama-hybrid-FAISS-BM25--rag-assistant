[site]: datascience
[post_id]: 123095
[parent_id]: 
[tags]: 
Different size of deep learning models but similar inference-time

I have three different semantic segmentation models with large differences in size. The first one includes 30,000,000 trainable parameters, the second one about 20,000,000 and the third one about 200,000 with model file size about 280, 150 and 3.5 Mb respectively. However, when I run the Keras predict() function with each model, the inference-time is about 0.06 sec per image in all the models. The code is like the following: start = time.time() pred_mask = .predict(np.expand_dims(img, axis=0)) end = time.time() print(end-start) How is it possible ? Even if the machine is fast with GPU-enabled I think that should observe a difference in inference-time since the third model is about 150 times smaller than the first one.
