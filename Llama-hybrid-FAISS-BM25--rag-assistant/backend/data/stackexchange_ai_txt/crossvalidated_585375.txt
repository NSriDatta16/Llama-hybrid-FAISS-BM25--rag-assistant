[site]: crossvalidated
[post_id]: 585375
[parent_id]: 
[tags]: 
why not using sample variance (instead of MSE) to estimate the error variance in linear regression?

Assuming the true equation for Y is linear as below: $$Y_i =\beta_1X_i +\beta_0 + \epsilon_i$$ Assuming X is fixed, then the variance of each Y is: $$var(Y_i )=var(\epsilon_i)=\sigma^2$$ In order to estimate $\sigma ^2$ , we usually use the mse: $$\hat{\sigma}^2=\frac{1}{n-p-1}\sum_i(Y_i -\hat{Y_i})^2 $$ My question is, why can't we estimate $\sigma^2$ using the sample variance below? $$\hat{\sigma}^2=\frac{1}{n-1}\sum_i (Y_i-\bar{Y})^2$$ The sample variance is also an unbiased estimator, so instead of predicted value we just use the sample average of Y?
