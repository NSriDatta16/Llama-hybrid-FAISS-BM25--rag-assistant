[site]: crossvalidated
[post_id]: 169261
[parent_id]: 168629
[tags]: 
In one of your comments you state that you not only have values of $f(x)$ and $c(x)$ at various $x$, but can also evaluate $f(x)$ and $c(x)$ at new points. This means you are essentially doing ordinary numerical optimization. The lack of an analytical form for $f(x)$ and $c(x)$ is not an issue for most optimization algorithms, because they wouldn't use it even if they had it. They generally construct an easily optimized surrogate based on values and derivatives from the last few points, optimize the surrogate to get a new candidate optimum, and iterate. The only requirement for such a procedure is the ability to evaluate $f$ and $c$ and perhaps some derivatives. All that is needed for convergence to a local optimum is some mild smoothness/regularity conditions on $f$ and $c$. Global convergence is hard to prove unless you're doing (quasi-)convex optimization.*** If you are able to compute gradients or Hessians of $f$ and $c$, there is an extensive classical literature on optimization algorithms that provably converge to local optima. Numerical Optimization by Nocedal and Wright is a good introduction. This literature is the foundation of all mathematical optimization theory, even if gradients and Hessians are not available in your problem. If you cannot compute gradients or Hessians, you are doing derivative-free optimization (DFO). There are convergence results for unconstrained DFO, and for DFO with linear and bound constraints. For DFO with general nonlinear constraints I do not know of any proof results. There is the COBYLA algorithm by Michael Powell, one of the most productive researchers in DFO, but no proofs to my knowledge. This paper and this book are written by top DFO researchers and should help you get acquainted with the literature on what has been proven and how. ***It's always worth thinking for a moment about whether your problem has a quasiconvex flavor, because the benefits are potentially enormous. If you have two points $x_1,x_2$ and their costs $f(x_1), f(x_2)$, and you make a weighted average of the points $x = \alpha x_1 + (1-\alpha) x_2$ for $\alpha \in [0,1]$. Is the cost $f(x) \leq \max(f(x_1),f(x_2))$? Put another way, is a weighted average at least as good as the worst component that went into it?
