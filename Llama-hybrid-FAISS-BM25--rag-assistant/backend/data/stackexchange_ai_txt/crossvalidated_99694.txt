[site]: crossvalidated
[post_id]: 99694
[parent_id]: 
[tags]: 
What does it imply if accuracy and recall are the same?

I did a number of machine learning experiments to predict a binary classification. I measured precision, recall and accuracy. I noticed that my precision is generally quite high, and recall and accuracy are always the same numbers. I used the following definitions: $\text{Precision} = \frac{TP}{(TP + FP)}$ $\text{Recall} = \frac{TP}{(TP + FN)}$ $\text{Accuracy} = \frac{(TP + TN)}{(P + N)}$ I have some difficulties to interpret accuracy and recall. What does it mean if these two number are always the same in my case?
