[site]: crossvalidated
[post_id]: 227039
[parent_id]: 225595
[tags]: 
first of all you are mixing two different things, linear regression and non linear Hebbs learning (''neural networks''). Even tought both approaches aim to solve the same problem, they way they do it differs. Main difference is that in nonLinear Hebb learning you are training a perceptron which has typically a non linear activation function, as sigmoid or soft sign (or many others). Because of this, it's formulation changes a bit: \begin{equation} \vec y = f(W\vec x + \vec b) \end{equation} where $f$ is the activation function, $W$ are the weights and $\vec b$ the bias (please note this a CS notation, in neuroscience notation differs. For instance the bias is called threshold instead). Here you can find a book which provides a very detailed introduction to Neural Networks, I think you could go trough it, especially the first chapters and see how NNs are working (also suggest reading Back-Propagation chapter as if the categories are not linear separable best to use Back-Prop learning, which is really just a generalization of Hebbian learning for non-linear units and multi-layer networks). For what it concerns the weight update: \begin{equation} \Delta W = \gamma y(\vec x)\vec x \end{equation} where $y(\vec x)$ is the output of the netowrk when presented the input $\vec x$. For a more detailed explanation, with a nice (and more formal than mine) math background, I suggest you see this , and also for a more detailed description of non linear Hebb learning see this . EDIT: answering OP comment here. There is something confusing in your problem statement. I try to rephrase it as I understood it, but feel free to correct me if I got something wrong: You have an initial dataset $X \in R^d$ with $|X| = n$, which then get transformed into $Y \in R^m$ trough a non-linear $g: R^d \rightarrow R^m$. I guess this function $g$ is unknown to you and you want to estimate it with a Neural Network, as linear Hebb's learning would bring you nowhere close to your goal (can't estimate a non linear function with linear Hebb). In this case what you have is: \begin{equation} Y' = f(WX + \vec b) \end{equation} and your error function would be (the error function is what you try to minimize when learning, in this case is the reconstruction error): \begin{equation} E = ||Y-Y'||_2 \end{equation} therefore, what you want to learn is the weight matrix $W$ of size $d \times m$ and the bias vector $\vec b \in R^m$. The function $f$ IS NOT $g$ or trying in any way to be close to it. This is typically a sigmod function or a soft sign and it does not change over time. What changes is the matrix $W$ that is initially set to random and then trained trough your dataset $X$ and expected result (supervised) $Y$. So to put it in your terms: you want to estimate $W$ such that the recostruction error $E$ is small. I really suggest you for this problem to take a look at normal Neural Networks and forget for a second about Hebbs.
