[site]: crossvalidated
[post_id]: 512262
[parent_id]: 512250
[tags]: 
Yes, predictions from most other prediction methods (like random forests or CARTs) will also only give "averages". More specifically, all these tools output single numbers, or point predictions . They typically aim at giving you the conditional expectation of the outcome, given covariate values. So what you are looking for is predictive densities . In the time series forecasting community, these are also called density forecasts . Unfortunately, there are much less theory, work and tools for density forecasting than for (expectation) point forecasting. On the one hand, that is because point forecasting is easier to do. On the other hand, it's because point forecasting is easier to explain. There are a few ways you can go about this. However, no approach will be completely free of assumptions. Standard Ordinary Least Squares assumes homoskedastic normally distributed errors, so all it needs to estimate is the variance of the error term. (The mean of the error is zero, because we are predicting the expected value.) You have already determined that this does not work for you. An alternative, e.g., for time series forecasting, is autoregressive conditional heteroskedasticity, or ARCH (or GARCH), where the distribution of the errors changes over time. Another approach would be something like a Poisson or Negative Binomial regression, where the output would be a predicted Poisson or NegBin distribution, and the full distribution will depend on your covariates. Yet another way is using Neural Networks for density forecasting. There is some work on that, mainly by binning data and then predicting probabilities that an outcome will fall into a given bin. Finally, you could also fit your model and collect in-sample residuals, then resample from those residuals and add the resamples to the predicted expected values. This assumes that the distribution does not depend on the covariates, but at least it makes no assumptions on the distributional family. You can weaken the homoskedasticity assumption by stratifying the resampling, but then you are of course still making assumptions on the strata. Which approach makes most sense will depend on what you are actually doing. For instance, retail sales forecasting can be done quite meaningfully using NegBin regression, because observations are overdispersed counts, so this makes intuitive sense. If you have continuous data, then some GLM might be useful, e.g., using a gamma family. You may want to consider asking a more specific question.
