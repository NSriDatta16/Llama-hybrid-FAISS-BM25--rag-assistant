[site]: crossvalidated
[post_id]: 624564
[parent_id]: 
[tags]: 
Quantify Independence/Jointness of Tasks in Multitask Model

Suppose I have two identical tasks that are over two related but not identical domains- for example, next word prediction in Hamlet in both English and in Spanish. I want to train a model on a mix of both tasks at the same time using an encoder-decoder model, in order to project both tasks into the same latent space- but aside from just looking at how different sentences cluster in that embedding space, I don't really know how I could determine whether the model is learning each task separately or leveraging some meaningful correspondence between the two (I.e., learning to translate between English and Spanish vs. solving each task separately). Is there a way to numerically measure the "jointness vs independence of tasks" using some calculable metric like accuracy/precision (maybe something like KL Divergence in embedding space? Not sure)?
