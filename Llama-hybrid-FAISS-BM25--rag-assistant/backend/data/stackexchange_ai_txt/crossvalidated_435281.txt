[site]: crossvalidated
[post_id]: 435281
[parent_id]: 217703
[tags]: 
I'm unable to comment due to being a newly active user on StackExchange. But I think this is an important question because it's so friggin simple to understand yet difficult to explain. With respect, I don't think the accepted answer is sufficient. If you think about the core operations of a standard feed-forward NN, with activations of the form s(W*x+b) for some nonlinear activation function s , it's actually not obvious how to "get" multiplication from this even in a composed (multi-layered) network. Scaling (the first bullet in accepted answer) does not seem to address the question at all ... scale by what? The inputs x and y are presumably different for every sample. And taking the log is fine as long as you know that's what you need to do, and take care of the sign issue in preprocessing (since obviously log isn't defined for negative inputs). But this fundamentally doesn't jive with the notion that neural networks can just "learn" (it feels like a cheat as the OP said). I don't think the question should be considered answered until it really is, by someone smarter than me!
