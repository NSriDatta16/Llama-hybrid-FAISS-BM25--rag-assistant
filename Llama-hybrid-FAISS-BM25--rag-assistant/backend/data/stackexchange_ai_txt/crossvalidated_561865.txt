[site]: crossvalidated
[post_id]: 561865
[parent_id]: 561164
[tags]: 
As soon as one moves from discrete to continuous search spaces, it becomes necessary to specify a distribution on the parameter space in order to perform the random search. Then it is evident that thie performance of the random search will very strongly depend on the features of this distribution. In fact, one of the key developments in the history of training neural network was the development of various random weight initialization procedures (Glorot, He, etc.). So in a sense the random search is already being used as a (very important) first step for training the networks. In fact, there is recent work showing that pure random-search like approaches can be used to train neural networks to high accuracy . This is related to the Lottery Ticket hypothesis, which has already been mentioned by msuzen. But what is even more dramatic, is that it turns out that large randomly-initialized neural networks contain subnetworks that can nearly match the performance of the trained model with no training ( Zhou et. al. Ramanujan et. al. ). You may note, though, that I have done a bit of a sleight of hand. In the linked papers, they look for the subnetworks by basically searching over the space of all subnetworks of the original network. It is not as if they are only sampling 60 subnetworks at a time. But this underscores a crucial observation which makes the random search approach somewhat feasible or neural networks: Sampling a single, large network is equivalent to sampling a massive number of small networks . This is because a large network has a very large number of subnetworks. The catch is that they search space is much more than 60: in fact, the combinatorics make an exhaustive enumeration out of the question. So in the linked papers, they have to use specialized search algorithms to identify the best (or near best) subnetwork. I am not claiming that this is the best way to train a neural network, but in principle random search is a feasible training procedure. You ask "Why do we use gradient descent instead of random search?". So really this is not just a question about random search, but also about gradient descent. It has been hypothesized that the stochastic gradient descent algorithm itself is actually key to the remarkable generalization abilities of neural networks. (Here are a few examples of papers that take this approach) This is sometimes called "algorithmic regularization" or "implicit regularization". A simple example: suppose you fit an underdetermined linear regression using gradient descent. There are multiple global minima, but it turns out that the the GD will always converge to the minimum that has smallest norm. So the point is that gradient descent has a bias towards certain kinds of solutions, which can be important when the models are over parametrized, with many gloabl minima. You can easily find a ton of literature on this by googling these key words. But here is the upshot: suppose that we could actually train neural networks using 60 iterations of random search. Then stochastic gradient descent would still probalby be the preferred way to train them, because the solutions found by random search have no useful regularization .
