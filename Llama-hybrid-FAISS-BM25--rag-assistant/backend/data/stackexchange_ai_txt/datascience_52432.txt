[site]: datascience
[post_id]: 52432
[parent_id]: 
[tags]: 
Why is my test data accuracy higher than my training data?

I'm using four years of data, training on the first 3 and testing on the fourth. Using LSTM w/ Keras. My test data set (which has no overlap at all with the training) is consistently performing better than my training data. How should I interpret this? It seems very unusual. Here's the trail end of the model output. You can see my training accuracy for a given epoch hovers around 80%, but test output jumps to about 86%: Epoch 8/10 9092/9092 [==============================] - 9s 964us/step - loss: 0.9870 - acc: 0.8185 Epoch 9/10 9092/9092 [==============================] - 9s 1ms/step - loss: 0.9670 - acc: 0.7996 Epoch 10/10 9092/9092 [==============================] - 9s 937us/step - loss: 0.9799 - acc: 0.7895 Test Set Accuracy: 85.96% predicted 0 1 actual 0 2639 238 1 211 111 Edit: Here's my code to create & compile the model: embedding_vector_length = 32 days = 30 model = Sequential() model.add(Embedding(2080, embedding_vector_length, input_length=days) model.add(LSTM(100)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print(model.summary()) model.fit(train_x, train_y, epochs=3, batch_size=64,class_weight={0:1.,1:1}) scores = model.evaluate(test_x, test_y, verbose=0) print("Test Set Accuracy: %.2f%%" % (scores[1]*100))
