[site]: crossvalidated
[post_id]: 452720
[parent_id]: 452715
[tags]: 
In Bayesian inference, $p(s|\theta)$ is called the likelihood function, which is a distribution but seen as a function of the sample (in this case the signal $s$ ). $p(s)$ is indeed a scalar (as a function of $\theta$ ), called the normalizing constant which makes $p(\theta|s)$ a proper density. You can see this easily by writing $$p(s) = \int p(s|\theta)p(\theta).$$ Now, to make inferences about $\theta$ , we are are not usually interested in the distribution $p(\theta|s)$ , but in a quantity related to this distribution (e.g. the mean). In a textbook example you would find a distribution proportional to $p(\theta|s)$ by multiplying the likelihood and the prior. This posterior depends on the observed signal $s$ . Ideally, you would then identify a parametric family for the posterior from which you can easily evaluate the quantity of interest. If the posterior is not easily obtainable, methods like MCMC are used to generate samples from this distribution and estimate the quantity of interest. You can read more about this in Probabilistic Programming and Bayesian Methods for Hackers . This book is not so theoretical and I think it's a good reference to understand the basics of Bayesian inference.
