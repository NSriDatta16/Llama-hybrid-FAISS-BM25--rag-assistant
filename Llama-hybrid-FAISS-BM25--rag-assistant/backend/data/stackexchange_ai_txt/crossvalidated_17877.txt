[site]: crossvalidated
[post_id]: 17877
[parent_id]: 
[tags]: 
Probability of accidentally producing significant results

I have a problem, which I think should be very common, but I mostly have been taught stochastic at university and most of my statistics knowledge is self-taught. Hence I have no clue on how to look for this on google or other search engines. The basic question is, if I am testing a hypothesis against the null-Hypothesis on a significance level of p* (lets say p*=0.05 for examples), what are the chances that I might reject H0 by accident. As far as my understanding goes, the usual tests estimate the Probability of seeing the given data even if H0 holds. So this would be P(Data|H0) . I am then rejecting the Hypothesis if P(Data|H0) . Is this understanding correct so far? One way I could model this is by estimating P(P(Data|H0) , however this leaves me with a probability of a probability, which looks weird to me, and I have no clue how to calculate this. Or is my thinking to complex in this case and the probability I am looking for is just p* ? However the formula P(Data|H0) is calculating a dependent probability and I am looking for an independent probability. This suggest I would need something akin to bayesian statistics to solve this, but I have only just heard of this term and would need lot's of reading to be able to use this by myself. The reason I am asking this is, that in my case I am testing a great number of models each against the same p* . Now I want to know how many of the models that turn out to get pass this test are actually just false positives caused by the high number of hypotheses I tested. Maybe there is a better way to estimate this directly and my thinking is much too complicated here.
