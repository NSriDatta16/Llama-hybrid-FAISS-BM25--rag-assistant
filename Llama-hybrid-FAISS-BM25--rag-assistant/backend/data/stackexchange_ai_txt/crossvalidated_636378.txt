[site]: crossvalidated
[post_id]: 636378
[parent_id]: 636374
[tags]: 
Yours is a common fear among those who were trained as frequentists, but are now entering the Bayesian world. First off: you can have your model (likelihood) "dictate" the shape of your priors if you insist. Examples of such formal rules are the Jeffreys prior or, more generally, reference priors . With these types of priors you aim to minimize the influence of the prior on the posterior, pretending to have no prior knowledge whatsoever. With noninformative priors, however, you deprive yourself of the main advantages of Bayesian modelling. For instance, carefully chosen informative priors expand the space of models you can estimate as compared to frequentist (or non-informative Bayesian) approaches, by providing identification for an otherwise unidentified likelihood. You probably encountered this fact already, albeit under different names, for example "regularization" . And last but not least, it stands to reason that you barely ever not know anything about the parameters of your model. Remember that a prior does not fix the parameter at specific values; it just declares certain ranges more plausible than others. And often you do know that some parameter is more likely to be positive than negative, or more likely to be between 0 and 1 than between 100 and 1000, for instance.
