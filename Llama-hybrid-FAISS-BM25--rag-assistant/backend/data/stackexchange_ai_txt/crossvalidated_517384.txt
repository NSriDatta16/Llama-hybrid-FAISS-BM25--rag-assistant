[site]: crossvalidated
[post_id]: 517384
[parent_id]: 517373
[tags]: 
The purpose of VAE is generating images and not reconstructing the same image. i.e. at the end of the process of VAE you should be able to generate new images out of random sampled data. In your task, if i understand correctly the validation part, you are interested in reconstructing the same image, using both the encoder and the decoder, and not generating a new one (i.e. decoder only). So, it makes sense to me that you will overfit, as the loss of the VAE is not 100% suitable for this task. One thing you can try is to give more focus to the reconstruction loss. The VAE loss have two parts: Reconstruction loss KL divergence between the prior and the encoder's output - This is the regularization term that will make it possible to generate new images out of any sample from the random latent space. The final loss is the sum of these two losses. In your case if you want a good performance on the validation set i.e. generate the same image, you should put lower weight on the last term of the loss. i.e. instead of $$loss = reconstructionLoss + KL(p(z), q_x(z))$$ use: $$loss = reconstructionLoss + \beta * KL(p(z), q_x(z))$$ and try different $\beta$ values that are less than 1.0 - i'd start with 0.1 and see if it improves.
