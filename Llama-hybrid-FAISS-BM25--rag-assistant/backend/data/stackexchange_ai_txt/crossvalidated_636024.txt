[site]: crossvalidated
[post_id]: 636024
[parent_id]: 
[tags]: 
Why is there a residual connection around each attention layer followed by a layer normalization step in the in the decoder network?

I got the following question in a quiz, and it seems that my answer is not correct. I don't understand why: Why is there a residual connection around each attention layer followed by a layer normalization step in the in the decoder network? The answers: To help with the interpretability. To speed up the training, and significantly reduce the overall processing time. To break the symmetry in the back-prop. To help with the parallel computing component during the training. I answered "To break the symmetry in the back-prop.", but it seems that they answer is wrong! I don't understand why my answer is wrong. Also, I don't get how any of the other answers can be correct!
