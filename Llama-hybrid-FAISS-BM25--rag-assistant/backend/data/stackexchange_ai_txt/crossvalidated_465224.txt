[site]: crossvalidated
[post_id]: 465224
[parent_id]: 
[tags]: 
Overfitting worse with more data

I am training a neural network for a binary text classification task. The network has some custom layers but also includes a full BERT base stack. However, the BERT parameters are fixed, leaving about 6000 additional parameters which are trainable. I was training the network on a very small dataset of 20 data points, and, with a batch size of 6 and an Adam optimizer with a learning rate of 0.01 and epsilon 1e-07, found overfitting to occur after about 10 epochs (training log loss 0.7 and continues to increase with more epochs). I then increased the size of the training dataset to approximately 25,000 samples and kept the remaining hyperparameters the same. While training, I found overfitting to occur within the first 7-8 batches of the first epoch. Again, I am not fine-tuning BERT in either the small or large data regime. I expected overfitting to be less dramatic on a larger dataset. Why is this not the case?
