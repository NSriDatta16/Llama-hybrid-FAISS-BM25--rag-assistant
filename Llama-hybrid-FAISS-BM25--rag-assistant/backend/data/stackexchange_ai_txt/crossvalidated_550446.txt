[site]: crossvalidated
[post_id]: 550446
[parent_id]: 496270
[tags]: 
Center loss is a strategy for constructing widely-separated classes. A common problem with ordinary supervised learning is that the latent features for the classes can end up being tightly grouped. This can be undesirable, because a small change in the input can cause an example to shift from one side of the class boundary to the other. This plot from the paper shows how the activations of 2 neurons display the (a) training data and (b) testing data. Especially near the coordinate (0,0), there is an undesirable mixing of the 10 classes, which contributes to a larger error on the test set. By contrast, the center loss is a regularization strategy that encourages the model to learn widely-separated class representations. The center loss augments the standard supervised loss by adding a penalty term proportional to the distance of a class's examples from its center. The center loss includes a hyperparameter $\lambda$ which controls the strength of the regularization. Increasing $\lambda$ increases the separation of the class centers. These images were taken from Yandong Wen et al., " A Discriminative Feature Learning Approach for Deep Face Recognition " (2016).
