[site]: crossvalidated
[post_id]: 489755
[parent_id]: 
[tags]: 
vanishing gradient and gradient zero

There is a well known problem vanishing gradient in BackPropagation training of Feedforward Neural Network (FNN) (here we don't consider the vanishing gradient of Recurrent Neural Network). I don't understand why vanishing gradient does not mean the zero gradient namely the optimal solution we want? I saw some answer said vanishing gradient is not exactly the zero gradient, just means the update of parameter is very slow. However, in the gradient decent, we don't want to achieve the exact zero gradient and we will stop when the parameter unchanges within $\epsilon,$ which is the same case of vanishing gradient. So can anyone give me a clear answer?
