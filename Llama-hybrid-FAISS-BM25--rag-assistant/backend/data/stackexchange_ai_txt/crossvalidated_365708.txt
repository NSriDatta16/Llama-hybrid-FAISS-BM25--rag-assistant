[site]: crossvalidated
[post_id]: 365708
[parent_id]: 30162
[tags]: 
Updated answer, May 2020: There is actually a field of research that deals particularly with this question and has developed practically feasible solutions. It is called Covariate Shift Adaptation and has been popularized by a series of highly cited papers by Sugiyama et al., starting around 2007 (I believe). There is also a whole book devoted to this subject by Sugiyama / Kawanabe from 2012, called "Machine Learning in Non-Stationary Environments". I will try to give a very brief summary of the main idea. Suppose your training data are drawn from a distribution $p_{\text{train}}(x)$ , but you would like the model to perform well on data drawn from another distribution $p_{\text{target}}(x)$ . This is what's called "covariate shift". Then, instead of minimizing the expected loss over the training distribution $$ \theta^* = \arg \min_\theta E[\ell(x, \theta)]_{p_{\text{train}}} = \arg \min_\theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \theta)$$ as one would usually do, one minimizes the expected loss over the target distribution: $$ \theta^* = \arg \min_\theta E[\ell(x, \theta)]_{p_{\text{target}}} \\ = \arg \min_\theta E\left[\frac{p_{\text{target}}(x)}{p_{\text{train}}(x)}\ell(x, \theta)\right]_{p_{\text{train}}} \\ = \arg \min_\theta \frac{1}{N}\sum_{i=1}^N \underbrace{\frac{p_{\text{target}}(x_i)}{p_{\text{train}}(x_i)}}_{=w_i} \ell(x_i, \theta)$$ In practice, this amounts to simply weighting individual samples by their importance $w_i$ . The key to practically implementing this is an efficient method for estimating the importance, which is generally nontrivial. This is one of the main topic of papers on this subject, and many methods can be found in the literature (keyword "Direct importance estimation"). So, finally getting back to the original question, this method consists not of resampling or creating artificial samples, but of simply weighting the existing samples in an appropriate manner. Interestingly, none of the papers I cited below cites this branch of research. Possibly, because the authors are/were unaware of it? Old answer This is not an attempt at providing a practical solution to your problem, but I just did a bit of research on dealing with imbalanced datasets in regression problems and wanted to share my results: Essentially, this seems to be a more or less open problem, with very few solution attempts published (see Krawczyk 2016, "Learning from imbalanced data: open challenges and future directions" ). Sampling strategies seem to be the most popular (only?) pursued solution approach, that is, oversampling of the under-represented class or undersampling of the over-represented class. See e.g. "SMOTE for Regression" by Torgo, Ribeiro et al., 2013 . All of the described methods appear to work by performing a classification of the (continuously distributed) data into discrete classes by some method, and using a standard class balancing method.
