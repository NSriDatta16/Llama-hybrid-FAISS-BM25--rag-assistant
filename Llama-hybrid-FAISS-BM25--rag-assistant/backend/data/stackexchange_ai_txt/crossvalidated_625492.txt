[site]: crossvalidated
[post_id]: 625492
[parent_id]: 625473
[tags]: 
If you plan on restricting your space of candidate models to something like ARIMA, then I can see this exercise of trying to pick an appropriate subset of the data might make sense. Looking at the example plots, I would not model either of these with ARIMA. However, I prefer a different approach which I'll suggest for your consideration: model the way the time series changes. The example plots from the OP show two different cases of non-stationarity. One approach is to simply throw various machine learning models on various extracted features from the time series. This can be made black-box and automatic via some hyperparameter tuning. This approach can be successful in tackling the way that the stochastic process is non-stationary, but let's focus on white-box modelling. The first time series appears to be a sequence of pulses. If any of these peaks can be reasonably assumed to be one-off events, I would just model them as explicit functions of time added to the apparent baseline. If these are themselves events that you expect to re-occur according to some pattern, then I would model that. Taking a probabilistic approach you would sample the arrival times and 'sizes' (e.g. height and width) of the pulses first, then add them into the stochastic process as you calculate it forward in time. I'd have to explore the timing and sizes of the peaks in the data to better understand how to proceed. The second time series appears to be a step change in the signal which can readily be modelled with indicator variables. This is quite a simple regression model (for which you could add distributional assumptions onto): $$Y_t := \beta_1 \mathbb{I}(t \geq \tau) + \beta_0$$ where $\tau$ is a parameter which you could either assign once, or tune, or assign a prior distribution over. The main limitations with this white-box approach (in my experience) has been that it is more time-consuming.
