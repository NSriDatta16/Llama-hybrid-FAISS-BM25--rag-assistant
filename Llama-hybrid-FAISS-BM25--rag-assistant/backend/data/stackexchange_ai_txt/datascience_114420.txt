[site]: datascience
[post_id]: 114420
[parent_id]: 114395
[tags]: 
I have seen something similar to this actually, but I hadn't fully resolved it. I still haven't exactly, but I'll provide my current thoughts. The problem is the logit link's nonlinearity. A logistic regression will have well-calibrated probabilities because it minimizes log-loss; hence you see matching 0.15. But that does not imply that the log-odds are similarly calibrated: the logit function cannot be interchanged with the expectation $$\mathbb{E}(\sigma(\text{model_logodds})) \neq \sigma(\mathbb{E}(\text{model_logodds})).$$ If the model logodds are all negative or all positive , then the logit is convex/concave and Jensen's inequality would apply to give you a direction for that inequality. Since your model is on an imbalanced set, the predicted logodds are probably mostly negative, and so I think the larger average probability is not unexpected. (If the shift were to probabilities rather than logodds (either additive or multiplicative), then you'd get $\mathbb{E}(\text{model_probs} \dotplus \text{shift}) \approx 0.15 \dotplus \text{shift}$ and so you could define the probability shift to ensure calibration in the large. That won't be calibrated "in the small" though, and worse you may well get shifted-predicted-probabilities that aren't in $[0,1]$ .) All this leaves me with an apparent contradiction though: if you build a logistic regression directly on the balanced dataset, it should be well-calibrated its coefficients aside from the intercept should be unbiased estimators for the true ones its intercept should be shifted by the quantity in question (2 and 3 from e.g. https://stats.stackexchange.com/q/67903/232706 , 1 from https://stats.stackexchange.com/q/208867/232706 ) but 2 and 3 together imply it's your shifted model, which contradicts your demonstration and 1. I'm going to leave this as an answer for now, but will try to find out more and either revise here or ask as a new question (or bounty your question).
