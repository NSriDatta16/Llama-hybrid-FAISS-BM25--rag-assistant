[site]: datascience
[post_id]: 63828
[parent_id]: 
[tags]: 
model with features of different sizes

I want to train a model (either classification or regression, doesn't matter) with features/inputs of different sizes, but I am not sure how to do it. For example, for each data-point, feature 1 and 2 are just real numbers and feature 3 is a vector of length 100, so if I have $n$ data-points, then I have a $n \times 1$ vector for feature 1, another $n \times 1$ vector for feature 2, and a $n \times 100$ matrix for feature 3. The first question is what kind of model I should use or try? Gradient boosting, random forest, or neural network? The second question is how to feed the features of different sizes to the model? Just concatenate them to form a $102 \times 1$ vector for each data-point? Thanks.
