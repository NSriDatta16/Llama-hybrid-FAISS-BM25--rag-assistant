[site]: crossvalidated
[post_id]: 274635
[parent_id]: 
[tags]: 
Calculating error of mean of time series

This is probably a stupid question. I'm mathematically literate, but my knowledge of statistical methods is embarrassingly hopeless. I have multiple time series representing various quantities for different systems, obtained through computational (molecular dynamics) simulation. I would like to state the mean and standard deviation of each of these quantities for each system, along with an estimate of the error of the mean of the time series. I've been averaging the values over all time steps to get the mean, and calculating standard deviation using the basic sample formula. I've then used this standard deviation to obtain the standard error of the mean as $$\mathrm{SE}_{\bar{x}} = \frac{\sigma_x}{\sqrt{n}}$$ (where $\sigma$ = standard deviation and $n$ = number of time steps, which is very large). But it occurs to me that the data in a time series is likely to be correlated, so this isn't a valid way to obtain the standard error of the mean. Is this correct? If so, that's where I get stuck. What statistical tests can I perform to obtain an estimate of the error of the mean?
