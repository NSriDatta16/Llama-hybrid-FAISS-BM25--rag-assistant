[site]: crossvalidated
[post_id]: 639672
[parent_id]: 
[tags]: 
Determine if the best output of a large number of model fits is due to overtraining

I have a machine learning model (e.g. Gradient Boost Regressor). I use hyperparameter tunning (e.g. random search) in a Cross Validation loop (e.g. 5 folds) in order to recover the optimal parameters of the model. When I run the hyperparameter tuning for a very large number of iterations (e.g. 10^6), I get 10^6 different models. By chance, one of these models may provide a very good regression score that happens to overfit in all the CV-folds. Is there a good statistical test you would recommend to detect if this result is due to overfitting or random chance? I was thinking something along the following: normalise the regression scores from the models and fit a standard normal distribution check if the best score is above the 95, 99 percentile if it is then it is probably due to overfitting Is there something better you would recommend?
