[site]: crossvalidated
[post_id]: 499271
[parent_id]: 
[tags]: 
Is it possible to learn a neural networks for a constant target?

I saw a paper ( https://arxiv.org/abs/1910.10147 ) where they learn L based on the following cost function: $$(D_1L(q(k-1), q(k)) + D_2L(q(k),q(k+1)))² = 0$$ Here $D$ is the derivative with respect to ith argument of L. This can be obtained via automatic differentiation and it is fully compatible with backpropagation. So the target is always zero independently of the input of L but how does the optimization method knows the real values of L for a given input? I mean as long as the cost function is satisfied with a zero value the model is learning but it can be learning wrong values for L. For instance, if the real value of $D_1L(q(k-1), q(k))$ is 10 then the real value of $D_2L(q(k),q(k+1))$ has to be -10. However the neural network may be learning wrong values for L as long as they are symmetric, for example -3 and 3. This is an ill posed problem due to the infinite possible solutions. Am I missing something or the method in that paper simply doesn't make any sense? Is it possible to learn a neural network in regression task with constant target? Edit: The goal is not to predict always that value. The goal is to learn L by predicting always zero (see the cost function). The question here is in order to learn L in such a way that it respects the cost function, there are infinite possible solutions, for instance if $D1L(q(k−1),q(k))$ = x then D2L(q(k),q(k+1))=-x and its sum gives 0 respecting the cost function. But as you can see x can assume any value as long as the cost function is respected, when in reality the goal is to learn a specific L. But the optimization doesn't know a priori value for L so I guess it should just assume a random value for x. In the paper they don't mention the ill posedness of the problem, either they solved it or not. I would like to know if what I'm thinking makes sense or if I'm just missing something.
