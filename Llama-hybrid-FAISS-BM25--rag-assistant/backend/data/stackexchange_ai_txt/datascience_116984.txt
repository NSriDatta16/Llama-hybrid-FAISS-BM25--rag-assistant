[site]: datascience
[post_id]: 116984
[parent_id]: 116975
[tags]: 
Before compressing 3D volumes, I recommend compress 2D surfaces as it is simpler for identifying divergence causes and it would be also easy to scale up to 3D. Then, you can try SGD or RMSProp instead of AdamW. Even if Adam is a great algorithm, it is less robust than SGD: Adam uses a different update rule that incorporates moving averages of the model's gradients and second moments. This can cause Adam to oscillate more, especially when the data is noisy or the optimization landscape is heavily non-convex. In contrast, SGD only uses the gradient of the loss function, which can make it more stable and easier to tune. Finally, you can use Velo, which is a learned optimizer. It should be able to solve complex convergence issues as you have. https://github.com/google/learned_optimization/tree/main/learned_optimization/research/general_lopt
