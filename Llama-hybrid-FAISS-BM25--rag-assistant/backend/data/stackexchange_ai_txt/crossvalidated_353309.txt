[site]: crossvalidated
[post_id]: 353309
[parent_id]: 149202
[tags]: 
In a Binomial experiment, we are interested in the number of successes: not a single sequence. When calculating the Likelihood function of a Binomial experiment, you can begin from 1) Bernoulli distribution (i.e. single trial) or 2) just use Binomial distribution (number of successes) 1) Likelihood derived from Bernoulli trial The probability of success of a single trial is \begin{align} P(y \mid p) = p^y(1-p)^{1-y} \end{align} and for a sequence of trials \begin{align} P(y_1,...,y_N \mid p) &= \prod_{i=1}^Np^{y_i}(1-p)^{1-y_i} \\ &=p^{\sum_{i=1}^Ny_i}(1-p)^{N-\sum_{i=1}^Ny_i}. \end{align} For clarity, let's define the number of successes as \begin{align} k = \sum_{i=1}^Ny_i \end{align} Giving us: \begin{align} P(y_1,...,y_N \mid p) &= p^{k}(1-p)^{N-k}. \end{align} However, we are not interested in this single sequence, but all the sequences that produce similar number of successes. This is similar to the relationship between the Bernoulli trial and a Binomial distribution: The probability of sequences that produce $k$ successes is given by multiplying the probability of a single sequence above with the binomial coefficient $\binom{N}{k}$. Thus the likelihood (probability of our data given parameter value): \begin{align} L(p) = P(Y \mid p) &= \binom{N}{k}p^{k}(1-p)^{N-k}. \end{align} 2) Likelihood derived from Binomial distribution The Binomial probability \begin{align} P(Y \mid p) &= \binom{N}{k}p^{k}(1-p)^{N-k}. \end{align} already is the probability of $k$ successes over $N$ trials, not a single observation or a single sequence of observations. Thus the Likelihood is not a product of these -- this would be the likelihood of several (independent) binomial experiments repeated, which is what you were getting at in your question! Where the confusion comes from? A lot of sources simply drop the binomial coefficient of the Likelihood function \begin{align} L(p) \propto p^{k}(1-p)^{N-k}, \end{align} without actually stating that this being done, or are simply not rigorous enough in their derivation: Using the likelihood of a single sequence instead. Given fixed observations, $\binom{N}{k}$ is a constant and thus doesn't affect calculating MLE estimate or MCMC sampling from the posterior, and this is why they can get away with the mistake.
