[site]: crossvalidated
[post_id]: 51630
[parent_id]: 
[tags]: 
Moments are summaries of random variables' characteristics. Specifically, the $j$th moment of a random variable $X$ is defined as $$ \mu_j^{'} = {\rm E} (X^j), \quad j = 1, 2, \ldots $$ and the $j$th central moment of $X$ is $$ \mu_j = {\rm E} [(X -\mu_1^{'})^j], \quad j = 1, 2, \ldots . $$ The first moment $\mu_1^{'}$ is the expectation of $X$, often denoted $\mu = {\rm E} (X)$, and the second central moment is the variance, often denoted $\sigma^2 = {\rm var}(X) = {\rm E} [(X - \mu_1^{'})^2]$. Analogous definitions hold for batches of data where the expectation is taken with respect to the empirical distribution function. Equivalently, "$E$" is replaced by averaging over the data. When the batch is a sample (of a population or process) these are known as "sample moments." A notable use of moments is the method of moments , a procedure for statistical inference, which estimates the population distribution by matching its moments to (specified) empirical moments.
