[site]: datascience
[post_id]: 54242
[parent_id]: 54194
[tags]: 
It is not unusual that some method (given some data) predict only one class (meaning probabilities are all larger or smaller than 0.5). I had this case recently with a logistic regression application. The reason usually is that you donâ€˜t have good features (x) to predict y. So first thing: have a look at correlation of X,y. What you can do to improve your fit is basically to use boosting. Boosting is also tree based and similar to RF it is also an ensemble technique. But boosting gives higher weight to observations which are hard to predict. LightGBM or Catboost are good routines for boosting. In my case, Logit predicted only one class with an AUC of about 0.3. LightGBM was much better and much more balanced in terms of prediction with an AUC of about 0.7. You could also try Logit with L1 regulation (Lasso). Maybe some of your features are not very helpful in making predictions. Lasso shrinks these features, which helps to make okay predictions.
