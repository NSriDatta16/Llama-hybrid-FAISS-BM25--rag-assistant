[site]: crossvalidated
[post_id]: 405141
[parent_id]: 360485
[tags]: 
It's true that a vector-matrix derivative isn't really defined. However in backpropagation, you never have to compute the derivative (jacobian) of any node in the computation graph (despite the many tutorials insisting you do). All that is necessary is the ability to compute the "vector jacobian product", or VJP. The VJP is defined to be the product of the gradient (of the function output) with the jacobian of the function, although it doesn't require you to explicitly write out the jacobian. It just so happens that the VJP of a function like $f(W,a) = Wa$ with respect to the weights $W$ is $ga^T$ , where $g$ is the backpropagated gradient. I've also explained this in a bit more detail in this other answer here, in case it helps: Higher Order of Vectorization in Backpropagation in Neural Network And yes, I think you are right about the missing transpose
