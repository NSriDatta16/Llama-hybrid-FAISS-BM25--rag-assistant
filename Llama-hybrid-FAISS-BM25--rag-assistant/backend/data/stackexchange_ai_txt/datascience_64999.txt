[site]: datascience
[post_id]: 64999
[parent_id]: 
[tags]: 
Issue with implementation of Sequence to Sequence Autoencoder

Hi, I want to implement Sequence to Sequence Autoencoder for text message classification purpose whether it is spam or ham. The complete code is: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from keras.models import Model from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, RepeatVector, Flatten from keras.optimizers import RMSprop from keras.preprocessing.text import Tokenizer from keras.preprocessing import sequence from keras.utils import to_categorical #Load the data df = pd.read_csv('/spam.csv',delimiter=',',encoding='latin-1') # Drop the columns that are not required for the neural network. df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True) df.info() # Create input and output vectors. and Process the labels X = df.v2 Y = df.v1 le = LabelEncoder() Y = le.fit_transform(Y) Y = Y.reshape(-1,1) # Tokenize the data and convert the text to sequences. # Add padding to ensure that all the sequences have the same shape. # There are many ways of taking the max_len and here an arbitrary length of 150 is chosen. max_words = 1000 max_len = 150 tok = Tokenizer(num_words=max_words) tok.fit_on_texts(X) sequences = tok.texts_to_sequences(X) sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len) # Split into training and test data. X_train,X_test,Y_train,Y_test = train_test_split(sequences_matrix,Y,test_size=0.2) X_train = np.reshape(X_train, (X_train.shape[0], timesteps, X_train.shape[1])) # Model Definition latent_dim = 128 timesteps = 1 input_dim = 150 #no of features inputs = Input(shape=(timesteps, input_dim)) encoded = LSTM(128,return_sequences=True)(inputs) encoded = LSTM(64)(encoded) decoded = RepeatVector(timesteps)(encoded) decoded = LSTM(128,return_sequences=True)(decoded) decoded = LSTM(input_dim, return_sequences=True)(decoded) sequence_autoencoder = Model(inputs, decoded) sequence_autoencoder.compile(optimizer='RMSProp', loss='mse') sequence_autoencoder.fit(X_train, X_train, epochs=100, batch_size=5, shuffle=True) When, I'm executing this, getting loss something 7000+. Also, the loss is almost remaining constant even after 100 iterations. Not understanding where I'm going wrong. Is there an issue with setting timesteps = 1? If I set timesteps = max_len then getting an error: ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (4457, 150) . I tried to solve this by changing X_train=np.expand_dims(X_train, -1) so that shape of X_train becomes (4457, 150, 1). Then I'm getting error " ValueError: Error when checking input: expected input_1 to have shape (150, 150) but got array with shape (150, 1) " Please help me to come out of this issue. Thanks in advance! After successful training of sequence_autoencoder model, I'll take only encoder part from this and attach a classifier layer for classification. Note: Even LSTM model is sufficient for this task, however, I want to reduce the dimensionality of features extracted from the input and then apply classifier for classification. Another aim is also to understand how to implement LSTM autoencoders. I have taken help from https://www.kaggle.com/kredy10/simple-lstm-for-text-classification/ for text preprocessing implementation and referred https://blog.keras.io/building-autoencoders-in-keras.html for model building.
