[site]: crossvalidated
[post_id]: 643813
[parent_id]: 
[tags]: 
Large Scale Missing Data & Imputation of Time Series Data in Neural Networks

I know there has already been a lot of discussion about this topic, but I have reasons to believe it still remains unanswered and lacks several justifications. Suppose we have an time series feature named $X_{i}$ . Our goal is to include $X_{i}$ into our dataset, in order to train a Neural Network. We know that $X_{i}$ is an important feature, however, around 30% of its values are missing, which is kinda big. It is also important that an imptutation method is carefully selected, because training multiple times a neural network, especially in time series, can be quite costly and time-consuming. In the literature (and internet), I have come across 3 popular imputation methods: Fill missing with 0 . I am think that filling with 0 could work as a dropout method for that specific feature with a rate of 30%, forcing the model to look into other features to calculate its output. Although this could work, we have a time series dataset, so it might confuse the model when value suddenly drops from positive to zero or rise from negative to zero. Fill missing with unique values . If we know for example that the value is always positive, replacing $nan$ with -1 could possible allow the model to separate cases where this feature is unknown. For example, when a layer outputs $y=WX+w_{0}$ , $y$ will be smaller in cases $X_{i}$ is missing, which could eventually affect the decision of the model. Fill missing using mean . Although this method is very popular, I am not quite sure why this would work, especially in case of time series. By imputing 30% of the data with the same value (mean), we eventually provide no useful (or might even wrong) information to our model. Especially in time series, I am not quite convinced it should work. It doesn't matter . Given that there are many other features in the dataset and that the time series model has many parameters, as well as regularization methods are integrated into the architecture, the imputation method might not have huge impact. This does make sence. There are of course many other methods, such as GANs, or imputing using nearest-neighbors, etc. However, all these methods require first an imputation model to be deployed and then training & testing the neural network, which is quite time consuming and costly, so it's hard to be implemented. Which method you think (based on your experience) is the most robust, meaning that it might not yield the best results, but it is at least decent and provides a good solution to this problem.
