[site]: crossvalidated
[post_id]: 611351
[parent_id]: 611328
[tags]: 
It's hard to generalize for all the optimization procedures, but if we're talking about neural networks and gradient descent, it's usually a good idea to normalize for the gradient updates to behave well. In general, it's hard to come up with a case where after normalizing, you're worse off. The following post might be useful on different opinions, I tend to agree with the answers given at the end: Is it necessary to scale the target value in addition to scaling features for regression analysis?
