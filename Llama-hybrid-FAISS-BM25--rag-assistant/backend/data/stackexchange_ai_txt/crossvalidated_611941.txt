[site]: crossvalidated
[post_id]: 611941
[parent_id]: 471013
[tags]: 
There are some issues you need to take into consideration before you perform a downscaling with Random Forest (or any other regression method). The first thing is the scale factor (the observed vs the target spatial resolution). Although there is no universal agreement about that, a rule of thumb is if your scale factor is greater than 5, then the downscaling becomes challenging due to uncertainties introduced by the big difference between the spatial scales. What you could do, if you want to go from 800m to 50m pixel size, is to conduct sequential downscaling steps (e.g., from 800m to 500m from 500m to 100m, from 100m to 50m). In many cases, that's not feasible because there aren't any covariates at the intermediate spatial scales. Before you select the algorithm for downscaling, you should conduct a explanatory spatial data analysis. Perhaps, you won't be needing a complex model (such as RF) and multiple linear regression would be sufficient for your task (simpler models are always preferred over complex ones). A high R-squared value (>0.8) for multiple linear regression means that you could use such model. Apart from the regression model, you will need another model to downscale the residuals of the regression (I would recommend area-to-point Kriging for various reasons, but that's up to you to decide). The reason you need to downscale the residuals is the the regression model alone, almost never can have an R-squared of 1. I would recommend the article of Wang et al. (2016) for a more detailed explanation about some of the issues I mentioned.
