[site]: stackoverflow
[post_id]: 1757524
[parent_id]: 1757363
[tags]: 
You could try two things: Make your hashCode method return something simpler and more effective such as a consecutive int Initialize your map as: Map map = new HashMap( 30000000, .95f ); Those two actions will reduce tremendously the amount of rehashing the structure is doing, and are pretty easy to test I think. If that doesn't work, consider using a different storage such a RDBMS. EDIT Is strange that setting the initial capacity reduce the performance in your case. See from the javadocs : If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur. I made a microbeachmark ( which is not by anymeans definitive but at least proves this point ) $cat Huge*java import java.util.*; public class Huge { public static void main( String [] args ) { Map map = new HashMap( 30000000 , 0.95f ); for( int i = 0 ; i So, using the initial capacity drops from 21s to 16s because of the rehasing. That leave us with your hashCode method as an "area of opportunity" ;) EDIT Is not the HashMap As per your last edition. I think you should really profile your application and see where it the memory/cpu is being consumed. I have created a class implementing your same hashCode That hash code give millions of collisions, then the entries in the HashMap is reduced dramatically. I pass from 21s, 16s in my previous test to 10s and 8s. The reason is because the hashCode provokes a high number of collisions and you are not storing the 26M objects you think but a much significant lower number ( about 20k I would say ) So: The problems IS NOT THE HASHMAP is somewhere else in your code. It is about time to get a profiler and find out where. I would think it is on the creation of the item or probably you're writing to disk or receiving data from the network. Here's my implementation of your class. note I didn't use a 0-51 range as you did but -126 to 127 for my values and admits repeated, that's because I did this test before you updated your question The only difference is that your class will have more collisions thus less items stored in the map. import java.util.*; public class Item { private static byte w = Byte.MIN_VALUE; private static byte x = Byte.MIN_VALUE; private static byte y = Byte.MIN_VALUE; private static byte z = Byte.MIN_VALUE; // Just to avoid typing :) private static final byte M = Byte.MAX_VALUE; private static final byte m = Byte.MIN_VALUE; private byte [] a = new byte[2]; private byte [] b = new byte[3]; public Item () { // make a different value for the bytes increment(); a[0] = z; a[1] = y; b[0] = x; b[1] = w; b[2] = z; } private static void increment() { z++; if( z == M ) { z = m; y++; } if( y == M ) { y = m; x++; } if( x == M ) { x = m; w++; } } public String toString() { return "" + this.hashCode(); } public int hashCode() { int hash = 503; hash = hash * 5381 + (a[0] + a[1]); hash = hash * 5381 + (b[0] + b[1] + b[2]); return hash; } // I don't realy care about this right now. public boolean equals( Object other ) { return this.hashCode() == other.hashCode(); } // print how many collisions do we have in 26M items. public static void main( String [] args ) { Set set = new HashSet(); int collisions = 0; for ( int i = 0 ; i Using this class has Key for the previous program map.put( new Item() , i ); gives me: real 0m11.188s user 0m10.784s sys 0m0.261s real 0m9.348s user 0m9.071s sys 0m0.161s
