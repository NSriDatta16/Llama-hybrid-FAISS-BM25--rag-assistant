[site]: crossvalidated
[post_id]: 308056
[parent_id]: 
[tags]: 
Bayesian smoothing using Dirichlet prior : why not MAP?

I am reading about smoothing methods for language model ( I am working on unigram model). If you are not familiar with unigram model, it is closely related to multinomial distribution (with the multinomial coefficient ignored in most cases ). We want to model unigram with a parameter vector $\theta$. It is well-known that unigram model suffers from the zero probability phenomenon where a lot of MLE estimated $\theta _i$ equal to zero (due to the sparsity of the data ), which resorts to smoothing methods so as to assign non-zero probabilities to $\theta_ i$s. Bayesian smoothing with Dirichlet prior is a preferred smoothing technique in the field of information retrieval. It "encodes" pseudo-counts through the prior in the form of a Dirichlet distribution ( $\theta \sim Dir(\alpha)$ - the parameter $\alpha$ is useful because it can favors probability masses for different $\theta$s). The $\theta$ is then determined by: $$p(\theta|D)\propto p(D|\theta )\times p(\theta |\alpha )$$ ($D=(d_ 1,d_2,...,d_n)$ comprises the counts of different words in the data document D- and is also a value vector of the multinomial distribution ). Leveraging the fact the Dirichlet distribution is a conjugate prior of the multinomial distribution, one can determine the posterior distribution by the form of another Dirichlet distribution: $$p(\theta |D)\sim Dir(D+\alpha)$$ The estimated $\theta$ is eventually chosen to be the mean of the $Dir(D+\alpha)$ ( the mean of a Dirichlet distribution has a closed-form formula ). My question is : Why do people choose the mean instead of the mode of the posterior (why don't they perform MAP while the mode of the posterior Dirichlet distribution also has a closed-form ?) Thank you for your attention to my question.
