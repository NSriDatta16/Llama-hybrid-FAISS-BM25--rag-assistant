[site]: crossvalidated
[post_id]: 214006
[parent_id]: 213905
[tags]: 
My question is - faced with the decision after seeing the first 10 flips, as in the simulation, why would I ever use the Bayesian estimate? It is biased, performs worse empirically, and even has lower standard error. I think the problem here is that the simulation isn't properly capturing the situation in which you have to make a decision after seeing ten flips. The question that both estimators are trying to answer is: Given that I flipped a coin ten times and got a certain number of heads, what is my best guess for $p$? What you did is: simulate this process (of flipping a coin ten times and getting an estimate for $p$) one hundred times, and then averaging the estimates for $p$ over these hundred repeats. But the average of these estimates isn't in general an estimate for $p$, unless you are treating the whole process as flipping a single coin $1000$ times, in which case your Bayesian estimate would be ((number of heads) +1)/1002. Since the frequentist estimate of $p$ is unbiased, and you used the same value of $p$ for every trial, it's absolutely to be expected that the average of $100$ of these estimates will be close to the true $p$, because this is the definition of unbiasedness. Since the Bayesian estimate is biased, in general averaging $100$ of them is not going to get you closer to the true $p$. What you should do to compare the two estimators of $p$ is: repeatedly simulate ten coin flips, compute the two estimates of $p$ and see which one is closer to the true $p$. In other words, you should look at something like $$\text{average over many trials of } | \text{true } p - \text{estimated } p |$$ instead of what you did, which was $$| \text{true } p - \text{average over many trials of estimated } p |.$$ You will find that the Bayesian estimate is better, unless you pick the $p$'s to be close to $0$ or $1$ most of the time (for example, if you take $p=0.9$ for every trial, then the frequentist estimate will be better.)
