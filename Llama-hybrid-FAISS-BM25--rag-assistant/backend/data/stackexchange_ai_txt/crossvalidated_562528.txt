[site]: crossvalidated
[post_id]: 562528
[parent_id]: 562506
[tags]: 
Least squares estimates, when the outcome variable Y=0, 1, will result in some predicted probabilities that are negative or greater than 1.0. As these are illegal values, this is not good performance to say the least. Maximum likelihood estimates (of which least squares is an example only for normally distributed continuous Y) are guaranteed to be efficient as $N \rightarrow \infty$ . Not only will MLEs be efficient but they respect the structure of the problem by restricting estimated probabilities to be in $[0,1]$ . So think of it this way: Always use MLEs (or penalized MLEs or Bayesian estimates which are also based on the likelihood function). In the case of conditionally normally distributed Y (normal residuals) the MLEs happen to be the least squares estimates. The equivalence in the special Gaussian case stems from the fact that the log likelihood function is proportional to the sum of squared errors plus a constant.
