[site]: crossvalidated
[post_id]: 81596
[parent_id]: 81576
[tags]: 
In short: by validating your model. The main reason of validation is to assert no overfit occurs and to estimate generalized model performance. Overfit First let us look at what overfitting actually is. Models are normally trained to fit a dataset by minimizing some loss function on a training set. There is however a limit where minimizing this training error will no longer benefit the models true performance, but only minimize the error on the specific set of data. This essentially means that the model has been too tightly fitted to the specific data points in the training set, trying to model patterns in the data originating from noise. This concept is called overfit . An example of overfit is displayed below where you see the training set in black and a larger set from the actual population in the background. In this figure you can see that the blue model is too tightly fitted to the training set, modeling the underlying noise. In order to judge if a model is overfitted or not, we need to estimate the generalized error (or performance) that the model will have on future data and compare it to our performance on the training set. Estimating this error can be done in several different ways. Dataset split The most straightforward approach to estimating the generalized performance is to partition the dataset into three parts, a training set, a validation set and a test set. The training set is used for training the model to fit the data, the validation set is used to measure differences in performance between models in order to select the best one and the test set to assert that the model selection process does not overfit to the first two sets. To estimate the amount of overfit simply evaluate your metrics of interest on the test set as a last step and compare it to your performance on the training set. You mention ROC but in my opinion you should also look at other metrics such as for example brier score or a calibration plot to ensure model performance. This is of course depending on your problem. There are a lot of metrics but this is besides the point here. This method is very common and respected but it puts a big demand on availability of data. If your dataset is too small you will most probably lose a lot of performance and your results will be biased on the split. Cross-validation One way to get around wasting a large part of the data to validation and test is to use cross-validation (CV) which estimates the generalized performance using the same data as is used to train the model. The idea behind cross-validation is to split the dataset up into a certain number of subsets, and then use each of these subsets as held out test sets in turn while using the rest of the data to train the model. Averaging the metric over all the folds will give you an estimate of the model performance. The final model is then generally trained using all data. However, the CV estimate is not unbiased. But the more folds you use the smaller the bias but then you get larger variance instead. As in the dataset split we get an estimate of the model performance and to estimate the overfit you simply compare the metrics from your CV with the ones acquired from evaluating the metrics on your training set. Bootstrap The idea behind bootstrap is similar to CV but instead of splitting the dataset into parts we introduce randomness in the training by drawing training sets from the whole dataset repeatedly with replacement and performing the full training phase on each of these bootstrap samples. The simplest form of bootstrap validation simply evaluates the metrics on the samples not found in the training set (i.e. the ones left out) and average over all repeats. This method will give you an estimate of model performance which in most cases are less biased than CV. Again, comparing it with your training set performance and you get the overfit. There are ways to improve the bootstrap validation. The .632+ method is known to give better, more robust estimates of the generalized model performance, taking overfit into account. (If you're interested the original article is a good read: Improvements on Cross-Validation: The 632+ Bootstrap Method ) I hope this answers your question. If you are interested in model validation I recommend reading the part on validation in the book The elements of statistical learning: data mining, inference and prediction which is freely available online.
