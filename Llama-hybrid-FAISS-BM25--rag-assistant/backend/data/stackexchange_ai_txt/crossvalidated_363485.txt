[site]: crossvalidated
[post_id]: 363485
[parent_id]: 363460
[tags]: 
The reason t-distribution is used in inference instead of normal is due to the fact that the theoretical distribution of some estimators is normal (Gaussian) only when the standard deviation is known, and when it is unknown the theoretical distribution is Student t. We rarely know the standard deviation. Usually, we estimate from the sample, so for many estimators it is theoretically more solid to use Student t distribution and not normal. Some estimators are consistent, i.e. in layman terms, they get better when the sample size increases. Student t becomes normal when sample size is large. Example: sample mean Consider a mean $\mu$ of the sample $x_1,x_2,\dots,x_n$. We can estimate it using a usual average estimator: $\bar x=\frac 1 n\sum_{i=1}^nx_i$, which you may call a sample mean. If we want to make inference statements about the mean, such as whether a true mean $\mu The problem's that we rarely know $\sigma$, but we can estimate its value from the sample $\hat\sigma$ using one of the estimators. In this case the distribution of the sample mean is no longer Gaussian , but closer to Student t distribution.
