[site]: datascience
[post_id]: 114997
[parent_id]: 114995
[tags]: 
At the minimum (as someone else mentioned), you should not ever have more variables than samples. However after that, there isn't an easy or straightforward answer to your question, unfortunately. You can have hundreds or even thousands of variables (dummy or not) and it could be fine. It just depends on how much data you have (the number of samples) and the algorithm you are using. I think the more precise question(s) that you are asking is: How do you know if you are overfitting? How do you prevent overfitting? You can use as many variables as you like and check to see if you are overfitting. This is the purpose of training data and test data. If your performance is much worse on the test data than training it means you have too many variables. Your goal is that the model run on training and test data should have similar performance. A huge part of data science and machine learning is dealing with overfitting and learning how to prevent it. There are general methods that can help that are independent of whatever algorithm you choose such as feature selection (i.e. forward selection), dimensionality reduction (i.e. PCA) & cross-validation. You should pretty much always use cross-validation when building a model. There are algorithm specific hyperparameters to prevent overfitting as well. This is what is most commonly used. It is actually a useful exercise to go look at most commonly used algorithms and try to understand what all the hyperparameters do. Most of the hyperparameters are for preventing overfitting. In conclusion: If you're looking for a straightforward answer, there isn't going to be one. If someone tells you some rule of the thumb for this, then they aren't really helping you.
