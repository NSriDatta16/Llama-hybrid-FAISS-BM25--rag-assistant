[site]: crossvalidated
[post_id]: 180192
[parent_id]: 180177
[tags]: 
Interesting question. I, too, am interested in song classification but based on the Million Song Dataset which contains dozens of technical features about American songs recorded and digitized since the 60s. Specifically wrt your question, a paper by Xing and Xie titled Robust Cauchy Principal Components Analysis -- http://www.cs.cmu.edu/~pengtaox/papers/cpca.pdf -- has an excellent overview of the challenges Gaussian and linear PCA face when the data is sparse, dense, nonlinear and/or extreme valued. They propose Cauchy PCA for deriving probabilistic and robust principal components when normal assumptions aren't met. The authors have code for this routine in Matlab, but have not developed, e.g., an R module. So, this may not be a solution for you if you don't have access to Matlab and/or are not comfortable writing your own SVD algorithm for this routine. Another resource is Rick Wicklin's robust approach to PCA that is not probabilistic and is most similar to the "Laplace" PCA described in Xing and Xie's paper available from this link at the SAS Institute ... https://support.sas.com/resources/papers/proceedings10/329-2010.pdf His approach is written in the SAS/IML language for matrix algebra. There are other, less complete workarounds. These include transforming or rescaling your extreme-valued features to something approximating a normal PDF. I would consider exploring any of the following options in hopes of finding a set of PCs based on more traditional methods that make sense: 1) ipsative scaling -- this involves finding the maximum value for each feature and then normalizing (dividing) the entire vector for that feature by that value 2) range (or interquartile range) normalizing -- this is an approach recommended by Glenn Milligan in his papers on cluster analysis and involves finding the range (IQ range) for a feature and normalizing the vector by that value 3) MAD standardization -- you know what mean standardization is, i.e., standardizing a feature to a mean of 0 and a std dev of 1. MAD standardization involves subtracting the median value for a feature and dividing by the MAD (mean absolute deviation) One thing to note is that traditional PCA is designed for continuously distributed features and does not work well with mixtures of scale types, e.g., continuous and dummy variables. One thing I've never explored is how robust OLS PCA is to mixtures of transformations for continuously distributed variables since OLS is not scale invariant. In other words, what if you find that a mixture of mean standardizing, natural log transformations, ipsative scaling and MAD standardizing produces a "meaningful" set of PCs? How might this mixture impact the resulting PCs? One solution to that conundrum might be to use maximum likelihood as your estimating algorithm since ML is scale invariant. One additional comment is that while MANOVAs and/or a DFA would help validate your groupings, you should probably introduce an exploratory and unsupervised cluster solution step into that protocol. Having arrived at a reasonable set of PCs and a clustering of the bird songs, the remainder of your research protocol simply falls into place.
