[site]: crossvalidated
[post_id]: 368165
[parent_id]: 368148
[tags]: 
The algorithm would typically just be considered another hyperparameter. E.g. in the widely used scenario of grid search, you train a variety of models and select an optimal one. So in addition to training a variety of SVM with, say, varying cost parameter, you also train random forests and then select from that larger variety of models. This selection or optimization can be done with the help of an (inner) cross validation. The obtained model then needs to undergo verification (aka validation), which can be done with another independent "outer" cross validation: that's the nested cross validation. You could also use a 3-level cross validation setup, where the outermost again performs verification of the final obtained model. The 2nd and 3rd level cross validation procedures would then be part of the training process, e.g. level two comparing the best random forest vs. the best SVM (and selecting accordingly), the innermost (3rd level) being the algorithm-specific hyperparameter tuning. However, compared with the approach above with just 2 levels, here your innermost optimization of algorithm-specific hyperparameters has fewer test cases available and is thus subject to more uncertainty. Unless you have huge numbers of cases available, this can cause your whole optimization to fail (see figure 8 in the Cawley paper @FelixvanDoorn linked).
