[site]: crossvalidated
[post_id]: 466232
[parent_id]: 
[tags]: 
Relationship between variational inference and sampling in a Boltmzann-machine-like network

In this paper concerning a Boltzmann-machine-like network and its variational mean field approximation, the authors write In the stochastic system as well as the deterministic system, units evolve to minimize the free energy , $F= \langle E\rangle - HT$ ... where the expectation is taken with respect to the fully-factoring joint distribution on the units and $H$ is that distribution's entropy. By the "deterministic system" they mean the mean field approximation of their Boltzmann machine. I assume by the ``stochastic system" they mean the version of their network in which configurations of units are determined by random sampling, in the manner that configurations of a traditional Boltzmann machine are acquired by Gibbs sampling. I understand variational approximation as a way of avoiding a costly sampling approach, but I don't understand why the authors claim that the sampling approach itself ``minimizes" variational free energy. A Gibbs sampling chain should explore configurations of the model with both low energy and large volume, since, intuitively, those are regions with high probability. Otherwise, we'd expect sampling to always give us the mode of the distribution, which is decreasingly likely in high dimensions. Is there another interpretation to the author's statement? And is there, in general, anything we can say about `` energy" of the states visited by the states of a Markov chain in the stochastic system? Any literature on this would also be appreciated.
