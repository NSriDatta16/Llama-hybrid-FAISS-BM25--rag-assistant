[site]: crossvalidated
[post_id]: 443543
[parent_id]: 333953
[tags]: 
Since nobody has attempted a response to this, I thought I'd give it my best shot. I feel like this is a partial answer at best, but here goes! Neuroscience people like sparsity because it seems to be how our brain works. Other people like sparsity in neural networks for the same reason why SVM people use kernels. The "kernel trick" in SVMs is where you implicitly embed your points in a higher-dimensional space so that the classes which were previously not separable by a linear subspace become linearly separable. There's a higher chance that the function you're trying to learn is a "simple" one when thought of as a function of more features. Of course, I have no proof of this and I'd love to see a better answer.
