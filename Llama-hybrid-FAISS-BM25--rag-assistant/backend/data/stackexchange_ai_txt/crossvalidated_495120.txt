[site]: crossvalidated
[post_id]: 495120
[parent_id]: 494900
[tags]: 
Just to add to @cbeleites answer (which I tend to agree with), there is nothing inherently different about nested cross validation that it will stop the issue in the OP. Nested cross validation is simply the cross validated analog to a train/test split with cross validation performed on the training set. All this serves to do is reduce variance in your estimate of the generalization error by averaging splits. That said, obviously reducing variance in your estimate is a good thing, and nested CV should be done over a single train/test split if time allows. For the OP as I see it there are two solutions (I will describe it under a single train/test split instead of nested CV but it could obviously be applied to nested CV as well). The first solution would be to perform a train/test split and then split the training set into train/test again. You now have a training set and two sets. For each model family perform cross validation on the training set to determine hyper-parameters. For each model-family select the best performing hyper-parameters and obtain an estimate of generalization error from test set 1. Then compare the error rates of each model family to select the best and obtain its generalization error on test set 2. This would eliminate your issue of optimistic bias due to selecting the model using data that was used for training however would add more pessimistic bias as you have to remove data from training for test set 2. The other solution as cbeleites described, is to simply treat model selection as hyper-paramters. When you are determining the best hyper-parameters, include model-family in this selection. That is, you aren't just comparing a random forest with mtry = 1 to a random forest with mtry = 2... you are comparing random forest with mtry = 1 to mtry = 2 and to SVM with cost = 1 etc. Finally I suppose the other option is to live with the optimistic bias of the method in the OP. From what I understand one of the main reasons leading to the requirement of a test set is that as the hyper-parameter search space grows so to does the likelihood of selecting an over-fit model. If model selection is done using the test set but only between 3 or 4 model families I wonder how much optimistic bias this actually causes. In fact, I would not be surprised if this was the largely predominate method used in practice, particularly for those who use pre-built functionality a la sci-kit learn or caret. After all these packages allow a grid search of a single model-family, not multiple at the same time.
