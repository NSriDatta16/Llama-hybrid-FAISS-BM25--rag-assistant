[site]: crossvalidated
[post_id]: 506304
[parent_id]: 
[tags]: 
Why is least squared loss sometimes written multiplied by $\frac{1}{2}$?

The base least squares formula that makes sense to me is: $$\sum(y^{(i)}-h_{\theta}(x^{(i)}))^2$$ Where $h_{\theta}$ is the hypothesis function, $y^{(i)}\in R, x^{(i)} \in R$ . Then sometimes I see the function multiplied by $\frac{1}{m}$ where $m$ is the number of data points, which makes sense that's taking an average, but other times I see it just multipled by $\frac{1}{2}$ like in these Stanford ML notes: http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/ . But I don't really know the purpose of this, besides it cancels out the 2 when you take the derivative of the loss, but that doesn't really seem to provide much utility and I can't really think of any other benefit it gives.
