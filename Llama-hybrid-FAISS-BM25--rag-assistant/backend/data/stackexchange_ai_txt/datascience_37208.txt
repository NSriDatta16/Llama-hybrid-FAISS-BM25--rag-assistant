[site]: datascience
[post_id]: 37208
[parent_id]: 
[tags]: 
Simplifying gradients of weights (RNN)

I understand that these are the gradients of the weights/biases in an RNN (correct me if I am wrong): This is a lot to compute and I’m aware that these equations can be simplified for ease of use. My question is what are these simpler versions, and how are they derived? I am using 6 timesteps in my network, hence the numbers used above. Here are the formulas for forward prop, just for context: One other thing. I just want to make sure I’m computing the loss correctly. I’m computing y-yhat at each time step, and appending it to an array, which at the end will have the same dimensions as x ([6, 4] - a very tiny dataset, I know.)
