[site]: datascience
[post_id]: 102049
[parent_id]: 101880
[tags]: 
BERT models work with sentences, not words: the self-attention in the transformer architecture is considering each token in respect to all other tokens in the sentence. "I'm going to implement Transformers in Python" vs. "I'm going to watch Transformers on TV later tonight" - there should be enough surrounding context to distinguish them. I.e. the John Firth quote: "You shall know a word by the company it keeps." Note that the output of the the embedding layer (which is usually fed in to the first transformer layer) is the same for identical tokens (*): at this point the model has no way to tell the difference. It is only the interaction with the other tokens, as it passes through each layer, that enable a distinction to emerge. Being able to distinguish between them also relies on the model having seen enough training sentences where each sense of a word was used. *: pedantically, position codes are added in, before feeding into layer 1.
