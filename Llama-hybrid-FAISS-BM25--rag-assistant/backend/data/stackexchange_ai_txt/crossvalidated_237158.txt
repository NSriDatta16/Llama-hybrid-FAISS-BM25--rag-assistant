[site]: crossvalidated
[post_id]: 237158
[parent_id]: 
[tags]: 
Back propagation in seq2seq models

I've implemented a seq2seq model for character-based text mirroring as a part of Udacity's Deep Learning class ( here's the code ). My model is very basic because it's a single LSTM as both encoder and decoder. You can see that it performs rather poorly (it only managed to learn how to mirror the first few characters of text). But I can't extend it to two LSTM's because it's confusing to me how backprop should work with such architecure. We don't have labels to get the errors for encoder states. Is it necessary to somehow compute gradients manually at the encoder's output and back-propagate it further, or Tensorflow's graph will handle it automatically?
