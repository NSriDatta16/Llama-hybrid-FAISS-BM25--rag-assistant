[site]: datascience
[post_id]: 34172
[parent_id]: 34171
[tags]: 
But when we feed the NN with more than one training example then which formula do we use ? Do we calculate the average of all dw (derivative of the loss function wrt the weights) or do we sum all of'em then multiply by the learning rate and substract them from the initial weights or what ? This is batch gradient optimisation, so you average the gradients over all training examples in your batch . Side note : In principle, you could use several training epochs (= several iterations over your entire dataset) and update the weights once per epoch. However, for large datasets (like ImageNet), you divide your dataset into batches and update the weights once per batch. You could also further subdivide each batch into minibatches and update once per minibatch. The subdivision is entirely up to you and is kind of ad hoc, but a recent paper suggests that consistently higher performance can be obtained with small minibatches of size between 2 and 32 . Also, your learning rate should be scaled appropriately depending on the batch size.
