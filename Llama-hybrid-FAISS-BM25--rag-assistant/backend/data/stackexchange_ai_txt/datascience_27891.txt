[site]: datascience
[post_id]: 27891
[parent_id]: 27888
[tags]: 
Nice question! Some Remarks For imbalanced data you have different approaches. Most well-established one is resampling (Oversampling small classes /underssampling large classes). The other one is to make your classification hierarchical i.e. classify large classes against all others and then classify small classes in second step (classifiers are not supposed to be the same. Try model selection strategies to find the best). Practical Answer I have got acceptable results without resampling the data! So try it but later improve it using resampling methods (statistically they are kind of A MUST). TFIDF is good for such a problem. Classifiers should be selected through model selection but my experience shows that Logistic Regression and Random Forest work well on this specific problem (however it's just a practical experience). You may follow the code bellow as it worked simply well then you may try modifying it to improve your results: train = pd.read_csv(...) test = pd.read_csv(...) # TFIDF Bag Of Words Model For Text Curpos. Up to 4-grams and 50k Features vec = TfidfVectorizer(ngram_range=(1,4), max_features=50000) TrainX = vec.fit_transform(train) TestX = vec.transform(test) # Initializing Base Estimators clf1 = LogisticRegression() clf2 = RandomForestClassifier(n_estimators=100, max_depth=20, max_features=5000,n_jobs=-1) # Soft Voting Classifier For Each Column clf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft', n_jobs=-1) clf = clf.fit(TrainX, TrainY) preds = clf.predict_proba(TestX)[:,1] Please note that the code is abstract so TianX, TrainY,TestX,etc should be properly defined by you. Hints Be careful about what is StopWord. Practically many people (including myself!) made this mistake to remove stop words according to pre-defined lists. That is not right! Stop words are corpus-sensitive so You need to remove stopwords according to information theoretic concepts (to keep it simple you need to know TFIDF kind of ignores your corpus-specific stopwords. If you need more explanation please let me know to update my answer). VotingClassifier is a meta-learning strategy in the family of Ensemble Methods . They take benefit from different classifiers. Try them as they work pretty well in practice. Voting schema simply takes the results of different classifiers and return the output of the one which has the highest probability to be right. So kind of democratic approach against dictatorship ;) Hope it helps!
