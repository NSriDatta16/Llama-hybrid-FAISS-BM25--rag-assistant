[site]: crossvalidated
[post_id]: 350959
[parent_id]: 
[tags]: 
Single loss value for gradient descent in neural network optimization

Suppose I train 2 neural networks in stock trading. First network produces enter-trade signals (very sparse). The second network produces exit-trade signals and starts to give signals right after the first gives positive signal. Question is whether it is mathematically sane to get gradient using just one loss value, which is a function of a squared difference of cumulative return (after 10K trades, for example) from theoretical cumulative return. Doing this trick for both networks, like sharing gradient. E.g., L Clarification example of what I meant: Suppose neural networks have been randomly initialized. sum_ret_theoretical Can I train NN based on extremely sparse loss values (1 per epoch/episode) that is a function of accumulated NN performance. An example of that maybe a NN that is designed to approximate the sum (mean) of the sample of values from variable X, without looking at each x to get loss.
