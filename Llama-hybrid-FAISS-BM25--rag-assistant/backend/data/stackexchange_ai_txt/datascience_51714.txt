[site]: datascience
[post_id]: 51714
[parent_id]: 
[tags]: 
Neural networks, optimization math intuition

When I look into the following partial derivative, I see it as being the key element of any optimization algorithm out there. Correct me if I'm wrong, but this gets us the slope of the loss function, so we can go opposite to that slope, therefore minimizing the loss. $$\frac{\partial \theta}{\partial \mathcal{L}}$$ where: $\theta$ is the weights, and the $\mathcal{L}$ is the loss; Does that make sense? Is there any other calculation step that is arguably more fundamental to the optimization of neural networks other than this derivative? This topic is specially important for me right now, because I was thinking of tattoing this derivative, as a cool A.I. tattoo, and I want it to be fundamental and simple.
