[site]: crossvalidated
[post_id]: 354785
[parent_id]: 
[tags]: 
Choosing an appropriate statistical distance which punishes entropy

Problem Description: I have with me experimental statistics of a system and I wish to fit a theoretical model so that the computed statistics on the model fit the experimental ones. I am using an RBM neural network for this problem though that is not very important to my question. The problem is that the experimental results suffer from noise and the noise is such that it overemphasizes the smaller probabilites and suppresses the larger ones. For example, in the image shown, the red bars represent the "true" behaviour of the system while the green ones represent the results of experimental measurements. I would like this to be taken into account. How should I do that? Is there some form of statistical distance which is appropriate for such situations? Comparison with KL Divergence: I had been using KL-Divergence as the statistical distance earlier and found that it is especially bad. Even worse that euclidean distance. I did some theoretical calculations and understood that this is so because KL-Divergence promotes probability distributions that loosely speaking have higher entropy - so that the modelled distribution will be even more biased than the experimental one along the same trend. Therefore, I want a distance that behaves in some way opposite to KL-Divergence. (Reverse KL does not help either, I tried that too).
