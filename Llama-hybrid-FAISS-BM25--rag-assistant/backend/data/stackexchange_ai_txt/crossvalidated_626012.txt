[site]: crossvalidated
[post_id]: 626012
[parent_id]: 
[tags]: 
Are Bagged Ensembles of Neural Networks Actually Helpful?

I've been looking into ways to estimate uncertainty for regression tasks on neural networks. One of the obvious options is ensemble modeling. Consider an ensemble of neural networks that all have identical architecture. I would assume that the variation that comes from initialization and batch shuffling would be sufficient to estimate uncertainty without needing variation in the training set. I've seen some sources that suggest adding variation in the training set through bootstrap aggregation (bagging). My intuition tells me that each ensemble member from the bagged ensemble will perform poorer with the added variation in the training set but my uncertainty estimate will probably be more conservative. I'd also expect point estimates from the bagged ensemble to have more error than point estimates from the ensemble without resampling, though I suppose that might be domain and data dependent. Does anyone here have experience with this and could you tell me if my intuition is right or wrong? I'm specifically interested in ensemble neural networks with and without the variation that comes with resampling of the training set for each network. Other techniques to estimate uncertainty such as using BNNs are a separate topic. EDIT : I had another thought after writing this. Assume that prior to training we've with held a test set. Then we split the training data into a training set and a validation set where the validation set is used to determine the best model (best model is the model that had the smallest validation loss). Instead of resampling via bootstrap, why not resample a different random split for training/validation? This helps to make sure the validation data is not completely wasted since it is likely some of the validation data for one model will be used as training data for another model. Typically that data is discarded when training a single neural network this way. I'd assume this ensemble would generalize better than a bagged ensemble since not only will each model have more unique samples, but the ensemble overall will also be trained using more data. Or am I missing something? My intuition is still telling me bagging for neural networks is not a good thing to do. The attached paper I found seems to agree with my intuition. https://www.gatsby.ucl.ac.uk/~balaji/why_arent_bootstrapped_neural_networks_better.pdf
