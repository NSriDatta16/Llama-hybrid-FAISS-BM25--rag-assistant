[site]: crossvalidated
[post_id]: 326745
[parent_id]: 326484
[tags]: 
question 1 I think the answer is probably no. My reason is we don't really have a definition for "uninformative" except for somehow measuring how far the final answer is from some arbitrarily informative model/likelihood. Many uninformative priors are validated against "intuitive" examples where we already have "the model/likelihood" and "the answer" in mind. We then ask the uninformative prior to give us the answer we want. My problem with this is I struggle with believing that someone can have a really good, well informed model or model structure for their population, and simultaneously have "no information" about likely and unlikely parameter values for that model. For example using logistic regression, see "A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION. FOR LOGISTIC AND OTHER REGRESSION MODELS" I think the discrete uniform prior is the only one we could reasonably say is the "first-first" prior. But you run into problems of using it, thinking you have "no information", but then suddenly having reactions to "unintuitive" answers (hint: if you don't like a bayesian answer - you might have left information out of the prior or likelihood!). Another problem you run into is getting the discretisation right for your problem. And even thinking of this, you need to know the number of discrete values to apply the discrete uniform prior. Another property to consider for your prior is the "tail behaviour" relative to the likelihood you are using. on to question 2 Conceptually, I don't see anything wrong with specifying a distribution without the use of a prior or likelihood. You can start a problem by saying "my pdf is ... and I want to calculate ... wrt this pdf". Then you are creating a constraint for the prior, prior predictive, and likelihood. The bayesian method is for when you have a prior and a likelihood, and you want to combine them into a posterior distribution. It's probably a matter of being clear on what your probabilities are. Then the argument shifts to "does this pdf/pmf represent what I say it represents?" - which is the space you want to be in I think. From your example, you are saying the single distribution reflects all the available information - there is no "prior" because it's already contained (implicitly) in the distribution you are using. You can also apply bayes in reverse - what "prior", "likelihood" and "data" gives me the actual prior I am considering? This is one way you can see that a $U (0,1) $ prior for a $Bin(n,p) $ likelihood "looks" like it corresponds to a "posterior" for a $Beta (0,0) $ "prior" with $2$ observations - $1$ from each category. on the so called blatantly wrong comment To be honest, I would be very interested to see how any numbet of observation could be used to predict a "statistically independent" observation. As an example, if I tell you I'll generate 100 standard normal variables. I give you 99, and get you to give me your best prediction for 100th. I say you cannot make a better prediction for the 100th than 0. But this is the same you would predict for the 100th if I gave you no data. Hence you learn nothing from the 99 data points. However, if I tell you that it was "some normal distribution", you can use the 99 data points to estimate the parameters. Then the data are now no longer "statistically independent", because we learn more about the common structure as we observe more data. Your best prediction now uses all 99 data points
