[site]: crossvalidated
[post_id]: 304786
[parent_id]: 304785
[tags]: 
I had a chance to examine the faithful data set in R, to score some plausible metrics: a two-sample Kolmogorov-Smirnov test (distance and p-value), Kullbackâ€“Leibler divergence, and Mutual Information for a range of different numbers of bins. An overview of the data: The duration between Old Faithful geyser eruptions can be plotted as a histogram, but what is the trade off of #bins vs. fidelity to the data set? A 4 bin histogram shows too much averaging A 14 bin histograms shows the two main modes A 34 bin histogram show more minor modes And a 54 bin histogram shows all of the data at the measurement accuracy: Results of different metrics: Using the metrics of information loss we can plot how they change with an increasing number of bins: Here, the KS metrics remain mostly unchanged above 27 bins, KL Divergence (in my implementation) is a useless metric, but mutual information best captures the increasing accuracy with increasing bin count. Thoughts? Teh codez: library(entropy) library(ggplot2) library(dplyr) library(reshape2) data("faithful") duration % group_by(nbins) %>% do(metrics(.$nbins)) %>% ungroup() ggplot(melt(df.sweep,id.vars='nbins'),aes(x=nbins,y=value))+geom_point()+facet_wrap(~variable,scales='free_y')
