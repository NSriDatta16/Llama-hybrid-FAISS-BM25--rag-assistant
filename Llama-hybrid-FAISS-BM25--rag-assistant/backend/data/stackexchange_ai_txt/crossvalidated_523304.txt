[site]: crossvalidated
[post_id]: 523304
[parent_id]: 523207
[tags]: 
This is a less statsy answer, and more of a practical/human-oriented answer. While all the other answers are correct in their own manner. I think it is more to do with your audience. Let me explain: If you're learning about overfitting then you're going to be pretty new to ML/Stats/Data Science. Therefore, not everyone taking said class, reading said blog post or watching said video is going to understand more complex models like GBMs, SVMs, GMMs and so on... However, if you are learning ML you should sure as hell will understand the concept of linear regression (or line of best fit) and one would hope you'd understand polynomials (otherwise maybe you should be taking high school maths classes not trying to learn ML just yet). Finally, you can create and recover the generative model precisely. This, in a learning phase, helps people fully understand what is going on and allow them to easy replicate results and play with the models themselves. NB : Maybe the real question is "Why aren't more ML concepts introduced as polynomial regression examples, since they are so intuitive, expressive and accessible by everyone with a high school mathematics education?" . The honest answer to this is probably: "Because GBMs, SVMs, GMMs etc are freaking cool!".
