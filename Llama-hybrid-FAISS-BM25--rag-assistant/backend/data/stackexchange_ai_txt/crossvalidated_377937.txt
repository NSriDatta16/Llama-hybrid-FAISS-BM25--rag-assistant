[site]: crossvalidated
[post_id]: 377937
[parent_id]: 377901
[tags]: 
To understand why the likelihood is not a pdf, we first have to understand what a function is. Most importantly, a function has a parameter and maps inputs from this parameter to some output. Most importantly, a pdf takes some continuous variable as input (parameter) and maps this to a probability density. Therefore, a pdf must integrate to 1 if integrated over this parameter. E.g. for $p(X|\theta)$ the variable $X$ is the parameter, and therefore we must have $\int_\Omega p(X|\theta) dX = 1$ , where $\Omega$ is the space from which $X$ can be chosen. The most important point here is, that $\theta$ is not a parameter. This is a bit confusing because in other branches of mathematics anything in the "(" and ")" is the parameter. Better to think of this as a special way of writing $p_\theta(X)$ . For the likelihood, we have $L(\theta|X)$ and now $\theta$ is a parameter, and $X$ is not. Again, think of this as a special way of writing $L_X(\theta)$ . So for the likelihood to be a pdf, it would have to integrate to 1, when integrated over $\theta$ . However, usually, we have $\int_\Theta L(\theta|X)d\theta\neq1$ , where $\Theta$ is the complete set of possible model parameters. So why can the likelihood be used in the Bayesian theorem? In essence, the likelihoods for different data (different $X$ ) but the same parameters (same $\theta$ ) are a conditional pdf, namely $p(X|\theta)$ . This means, if only need the value $p(X|\theta)$ (as in the Bayesian theorem), it does not matter, where you get it from. Thus, you can use the likelihood to calculate that value. This is the difference, if the likelihood is seen as a function (over $\theta$ ) or if a single value is used.
