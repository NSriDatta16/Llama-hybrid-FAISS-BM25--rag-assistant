[site]: datascience
[post_id]: 6581
[parent_id]: 253
[tags]: 
I do think Leaning Hadoop framework (hard way) is not a requirement of being a Data Scientist. General knowledge on all big data platforms is essential. I will suggest to know concept on it and only part need from Hadoop is the MapReduce http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html A Data Scientist does not build cluster, administer ... is just make "magic" with data and does not care where is coming from. The term "Hadoop" has come to refer not just to the base modules above, but also to the "ecosystem", or collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Spark, and others. Most important is the Programing language, math and statistics for working with data (you'll need to find a way to connect with data and move forward). I wish I had somebody to point me to the concept and do not spend weeks on learning framework and build from scratch nodes and clusters, because that part is Administrator role and not Data Engineer or Data Scientist. Also one thing: all are changing and evolving but math, programing, statistics are still the requirements. accessing data from hdfs is essential, for example PROC Hadoop, Hive, SparkContext or any other driver or pipe (treat hadoop as a point of accesing data or storage :) already are in place tools or frameworks what take care of resource allocation and management, performance.
