[site]: crossvalidated
[post_id]: 529891
[parent_id]: 529890
[tags]: 
While the estimate of the variance is unbiased, the estimate of the standard deviation is not. An unbiased estimate $V_{est}$ is one such that the expected value of the estimate is the true value $V$ being estimated: $E[V_{est}] = V$ . But applying a nonlinear operator like the square or square root destroys this property. In general, $E[\sqrt{V_{est}}] \neq \sqrt{V}$ so $E[\sigma_{est}] \neq \sigma$ . That is, the estimated std deviation $\sigma_{est}$ will be biased even though $V_{est}$ is not. The distribution of V estimates is not symmetric -- since V must be positive, it will have a longer tail above the true variance and a shorter, fatter tail below the true variance . When we take the square root of V, the new distribution's mode can be found by taking the square root of the location of the mode, but we can't say the same thing about the mean. Other answers on earlier questions take this discussion further. For example, this answer points out that Jensen's inequality can prove in which direction the estimated standard deviation will be biased and this answer shows a plot of the bias for different sample sizes. But my personal favorite is this answer which proves using simple expectation algebra that the estimated standard deviation will (in general) be less than the true standard deviation of the data. This answer relies on the identity $\mathrm{Var}[S_n] = \mathrm{E}[S_n^2] - \mathrm{E}^2[S_n]$ which would have to also be explained to the uninitiated, but there is an elegance to it that ties it into basic statistical theory nicely.
