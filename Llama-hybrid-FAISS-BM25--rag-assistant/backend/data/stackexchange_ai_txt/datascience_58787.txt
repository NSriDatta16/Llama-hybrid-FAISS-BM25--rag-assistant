[site]: datascience
[post_id]: 58787
[parent_id]: 58764
[tags]: 
Have you considered using some regularization / optimization techniques? You could have a look to the Adam Optimizer for gradient descent: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ And to some regularization techniques like Dropout: https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/ Of particular interest for you, a commonly used technique is to train the neural network with batches of examples, instead of one-by-one. This is called mini-batch gradient descent, and helps avoiding local minima : https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/ and Batch Normalization: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c I hope these information can help you. If you are working in python with a framework like PyTorch or TensorFlow, you will just have to declare a couple of things to use these techniques. Victor Huerlimann
