[site]: crossvalidated
[post_id]: 407033
[parent_id]: 
[tags]: 
Can I take a random sample of my very large data set to overcome non-independence?

I am trying to run a regression model on a very large time series data set (comparing flow noise to vehicle speed, pitch and dive state). Because my samples are taken about every minute (with some gaps of a few hours, so not perfectly spaced), the independence assumption is violated. I have selected a GLS model because I am also dealing with non-constant variance. I built in a variance structure to deal with that. I tried to build in a covariance structure, but the run times are not practical. I have 15,000+ data points, so my thought was I could take a random sample of my 15,000 points and now the independence assumption is satisfied, and I can run the GLS just with the variance structure. I could run 1000+ iterations with new random samples each time and compare. My question is - is this valid? Am I missing anything here? And as a follow up - my data also has outliers. I am removing them using Cook's distance. Should that be done before or after taking the random sample?
