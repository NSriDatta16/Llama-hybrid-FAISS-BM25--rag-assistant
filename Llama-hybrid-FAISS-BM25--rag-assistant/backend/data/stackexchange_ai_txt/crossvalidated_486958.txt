[site]: crossvalidated
[post_id]: 486958
[parent_id]: 271701
[tags]: 
Why isn't step function used? What is bad about using a step function in an activation function for neural networks? I assume you mean the Heaviside step function $$ H(x)= \begin{cases} 1 & x \ge 0 \\ 0 & x The key feature of $H$ is not that the gradients are sometimes zero, it's that the gradients are almost always zero. $H$ has gradient 0 everywhere except at $x=0$ . This means that optimizing a model (e.g. neural network) using gradient-based methods because the gradient is almost always zero. (Indeed, its derivative is the Dirac delta function.) This means that the weights will almost always never move because the gradient step has zero length. Contrasting $H$ to other functions such as ReLU should make it clear why $H$ is unsuitable but other functions with 0 gradient portions can succeed nonetheless. Models using the ReLU have shown marked success, even though the gradient is zero whenever $x . This is because usually not all inputs attain 0 gradient, so weights and biases for $x > 0$ will still update as usual. (However, having 0 gradient "on the left" does give rise to a problem similar to the problems with the Heaviside step function: some weight configurations will always be zero, so these weights are "stuck" and never updated. This is called the dying ReLU phenomenon .) A function having a negligible set where the gradient is not defined is not fatal. The ReLU derivative is not defined at $x=0$ (though the ReLU function is subdifferentiable ), but this is inconsequential, both because (1) it rarely happens that floating point arithmetic gives $x=0$ exactly and (2) we can just fudge it by using some number in $[0,1]$ as the gradient for that solitary point -- this arbitrary choice does not make an enormous difference to the final model. (Of course, using a smoother function instead of ReLU will avoid this entirely.) What are the effects of using step function? There are steep shifts from 0 to 1, which may not fit the data well. The network is not differentiable, so gradient-based training is impossible. In what ways are sigmoid/tanh superior over step? The sigmoid layer which combines the affine transformation and the nonlinear activation can be written as $$ \sigma(x) = \frac{1}{1 + \exp(-ax-b)}. $$ For certain $a$ , we can view $\sigma$ as a smooth approximation to the step function. A special case of $\sigma$ which has $a$ very large will behave very similarly to $H$ , in the sense that there is a rapid increase from 0 to 1, just as we have with $H$ . Likewise, if we need a decreasing function or a constant function, these are also special cases of $\sigma$ for different values of $a$ . Successful model training will find $a,b$ that achieve a low loss, i.e. choose the parameters which are shallow or steep as required to fit the data well. But when using $\sigma$ , we still get differentiability, which is the key to training the network.
