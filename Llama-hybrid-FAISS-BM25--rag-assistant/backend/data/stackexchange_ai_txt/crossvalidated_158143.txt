[site]: crossvalidated
[post_id]: 158143
[parent_id]: 
[tags]: 
kMeans unsupervised feature learning on multiple layers

I'm trying to develop an unsupervised feature learning pipeline. I have a train set with 512x512 images. I've extracted 16x16 patches, performed preprocessing steps (normalization and whitening). Clustering is performed with kMeans algorithm. Parameter k is set to sqrt(n/2), where n is number of patches. After that, patches are extracted with stride=4 from each image from training set and mapped to feature space using learned centroids. Pooling is performed and I'm left with feature representation of each image in 9748-dimensional space. These features are fed to linear SVM and classification is performed. The method described above is a single layer feature representation of each image. I would like to try learning features on multiple layers of abstraction with this method. I'm not sure how this is done. Based on this paper , I understand that identical pipeline should be applied to learned features. Does this mean that I should take a vector with shape (9748, ) for each image derive some x-dimensional patches from here, learn a clustering algorithm for these x-dimensional patches, employ feature mapping on features with learned centroids, get a second level feature representation and concatenate first layer and second layer features? I'm not sure how to derive patches from 9748-dimensional vector because there's no similar logic as deriving patches from images. Also, I'm not sure if I correctly understood this second layer feature learning. Complete method is described in this paper . Citations: [1] COATES, A., NG, A. Y., AND LEE, H. An analysis of single-layer networks in unsupervised feature learning. In AISTATS (2011), pp. 215â€“ 223. [2] COATES, A., NG, A. Y. Learning Feature Representations with K-means. Originally published in: Neural Networks: Tricks of the Trade, 2nd edition, Springer LNCS 7700, 2012
