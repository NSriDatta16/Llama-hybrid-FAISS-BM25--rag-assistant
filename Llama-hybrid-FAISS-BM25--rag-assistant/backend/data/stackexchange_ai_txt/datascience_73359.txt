[site]: datascience
[post_id]: 73359
[parent_id]: 
[tags]: 
Are weights of a neural network reset between epochs?

If an epoch is defined as the neural network training process after seeing the whole training data once. How is it that when starting the next epoch, the loss is almost always smaller than the first one? Does this mean that after an epoch the weights of the neural network are not reset? and each epoch is not a standalone training process?
