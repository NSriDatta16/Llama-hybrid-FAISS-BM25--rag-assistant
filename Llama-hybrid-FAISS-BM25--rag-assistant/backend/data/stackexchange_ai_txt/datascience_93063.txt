[site]: datascience
[post_id]: 93063
[parent_id]: 93034
[tags]: 
BERT does not give word representations, but subword representations (see this ). Nevertheless, it is common to average the representations of the subwords in a word to obtain a "word-level" representation. You may try to handle this as a normal tagging problem, where the tag of each word is the class associated with the word, much like part-of-speech (POS) (e.g. this ) tagging or named entity recognition (NER) (e.g. this ). Normally, you associate the tag to either the first or the last subword token in the word. If you prepare a dataset that way, you could fine-tune BERT to perform word tagging with the classes you need. If you only have the words, you could find some text corpus (ideally of the intended domains) and apply the described data preparation process.
