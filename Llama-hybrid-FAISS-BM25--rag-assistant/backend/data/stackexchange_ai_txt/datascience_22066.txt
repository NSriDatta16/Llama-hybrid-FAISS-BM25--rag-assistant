[site]: datascience
[post_id]: 22066
[parent_id]: 18750
[tags]: 
Although, sampling techniques like up sampling, down sampling exist to solve this imbalance class issue. Smote sampling is also available in several packages. Another useful technique is to assign weights to the different observations (according to the frequencies in data) so that minor classes are not modelled as noise(usually minor classes are given more weights to instruct the model the the minor classes are valuable and not outliers or noise). Although the overall accuracy would be great for a classifier but the misclassification error is very high for the minor class. CARET package in R helps you prepare a basic framework to handle this class-imbalance problem. with CreateDataPartition function you can generate stratified datasets. Kindly explore this package in R documentation. I would recommend to divide the four classes into chunks of equal sizes and make one dataset that would have equal representation of classes in one dataset. then the different datasets would have re-representation of minor classes but the dataset would have stratified sampling. These dataset can then be divided into training set and test set to do cross-validation. we can get the generalisation error by averaging out from the (say 10 fold). cross-validation. For second step for Hyperparameter tunning, internal cross fold validation can be performed to improve the prediction performance of the model.
