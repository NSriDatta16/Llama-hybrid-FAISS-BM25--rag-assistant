[site]: crossvalidated
[post_id]: 80213
[parent_id]: 80181
[tags]: 
This sort of technique us commonly called coordinate descent in machine learning, and can be used to fit a variety of models, IIRC it is guaranteed to converge to the optimal solution provided the cost function is convex. It has been used with SVMs, linear regression, logistic regression, with L2 or L1 penalty functions etc. A related problem is optimising the hyper-parameters of a model (kernel parameters or regularisation parameters etc.) where the process can be viewed as alternate optimisation of model and hyper-parameters, see Gradient-Based Optimization of Hyperparameters by Yoshua Bengio
