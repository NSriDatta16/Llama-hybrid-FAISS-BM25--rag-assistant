[site]: datascience
[post_id]: 121534
[parent_id]: 
[tags]: 
Unable to reproduce result for CIFAR10 with ResNet50 backbone and SIMCLR self-supervised algorithm

I am recently working on self-supervised learning, particularly simclr paradigm. I found it hard to reproduce results on cifar10 with linear evaluation. I now get round 60%, while the paper reports 80%+ with batch size 512 and 100 epoch. There is a huge gap. My current pipeline is as follows: Select augmentation method Contrasting embeddings of two views in a batch to pretrain resnet50 with first 7x7 conv replaced with 3x3 conv and first max pool removed. We add a MLP to project the 2048-d output of resnet50 down to 128-d to do contrasting, Linear(2048, 2048) -> relu() -> Linear(2048, 128) Remove projection head. Attach linear evaluation head Linear(2048, 10) . Freeze resnet backbone and train linear head. Optimizer I use is LARS with initial learning rate 1 cosine annealed to 1e-3. I wonder is there any intellectual misunderstanding of this algorithm or some engineering bug results in such discrepancy in performance.
