[site]: crossvalidated
[post_id]: 540514
[parent_id]: 
[tags]: 
comparing odds of two gaussians

I am trying to compare the odds of two events happening (what are the odds of one happening first). I know that the first one occurs in an average of 10 months with a sigma of 3 months. The second occurs in an average of 84 months with a sigma of 30 months. First, is there an analytical way of solving this? Second, I attempted to solve this by simulating it (Monte-Carlo), but it is not giving me the expected results. In particular, if I ask it to just simulate just one of the events, the reported average is way before the expected average. I tried simulating both with a gaussian and a cumulative gaussian. What am I missing? import math import random from scipy.stats import norm otherStart = 2*365 #estimate for earliest donation otherEnd = 8*365 #estimate for latest donation otherMu = (otherStart + otherEnd)/2 #average otherSig = (otherEnd - otherStart)/5 #get some semblance of a sigma meStart = 7*31 meEnd = 13*31 meMu = (meStart + meEnd)/2 meSig = (meEnd - meStart)/5 def cdfDist(x,mu,sig): #cdf function y = norm.cdf(x,mu,sig) return y #def gaussDist(x,mu,sig): # y = (1/(sig*math.sqrt(2*math.pi)))*math.exp(-0.5*((x-mu)/sig)*((x-mu)/sig)) # return y def rollDiceUntilWin(): day = 0 #start at day 0 while True: chancesOther = random.uniform(0, 1) #roll a dice otherOdds = cdfDist(day+2*365,otherMu,otherSig) #find odds for that day (it has been 2 years since that counter started) # otherOdds = gaussDist(day+2*365,otherMu,otherSig) if (chancesOther 10000): # day = 0 if __name__ == "__main__": numRolls = 100 #number of rolls meGive = 0 #counter of wins otherGive = 0 for i in range(numRolls): outCome = rollDiceUntilWin() if outCome == 1: otherGive = otherGive + 1 #if win, add 1 if outCome == 2: meGive = meGive + 1 chances = meGive / (meGive + otherGive) #get chances print(chances)
