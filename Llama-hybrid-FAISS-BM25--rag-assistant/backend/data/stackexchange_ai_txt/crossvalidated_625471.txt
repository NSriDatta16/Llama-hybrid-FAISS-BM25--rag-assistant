[site]: crossvalidated
[post_id]: 625471
[parent_id]: 625458
[tags]: 
The solution to OLS is not unique unless the design matrix is of full rank. If the matrix is rank deficient, there are actually several possible "solutions" using the pseudoinverse of $(X^{T}X)$ . The pseudo inverse satisfies $(X^{T}X) = (X^{T}X)(X^{T}X)^{-1}(X^{T}X)$ and exists but is not unique for rank deficient matrices. It gives a solution to the OLS likelihood equation. In general an activation function is some smooth monotonic function that relates two nodes in a neural network. Under this condition, your assumption is true. However, a link function in a GLM may or may not satisfy this condition. Your intuition helps here: realize you are trading an infinite space of functions for a handful of parameters in the real space - this is more efficient. For probability models in the exponential family of distributions, when the activation function happens to be from the natural parameterization, that is so that the inverse derivative of the condition mean is equal to the variance, then the solution to Newton Raphson or Fisher Scoring will converge to the MLE across the space of all initial conditions. The assumption is that the parameter is not on the boundary of the parameter space.
