[site]: crossvalidated
[post_id]: 10200
[parent_id]: 
[tags]: 
Is there a name for the high sensitivity of frequency of extreme data points to the mean of a normal distribution?

I remember hearing an argument that if a certain population of people has a mean IQ of 110 rather than the typical 100, it will have far more people of IQ 150 than a similarly-sized group from the general population. For example, if both groups' IQs are normally distributed and have the same standard deviation of 15, we expect about 0.2% of people from the high-IQ group to be over 150, and 0.02% from the general population. There are about ten times as many "genius-level" people from the high-IQ group. This is despite there being only three times as many people of IQ 120 or more in the high-IQ group, and only twice as many people of IQ 110 or more. Therefore, when we meet someone with IQ over 150, we can strongly suspect they're from the high-IQ group, even though on average that group does not have a huge advantage. Is there a special name for this effect? Similarly, if two populations have the same mean, but population A has a higher variance than population B, there will be more data points above a certain high threshold from population A. (I have heard this argument given to explain the high proportion of men as opposed to women in the highest tiers of mathematical achievement. It was claimed that men and women had the same mean ability, but men had higher variance.) Does this effect also have a name? I apologize for the somewhat controversial nature of the examples. I'm only interested in the names for the effects, not in IQ and mathematical ability in different sorts of groups. I simply cited these examples because that's the context under which I heard these phenomena described.
