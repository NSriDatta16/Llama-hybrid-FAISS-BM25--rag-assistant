[site]: crossvalidated
[post_id]: 114377
[parent_id]: 113056
[tags]: 
You could use a randomization test, which is kind of like a statistical test except you don really have to check the parameters of the distributions. Here's how it could work to compare two two classifiers A and B: get the accuracies (or whatever metric) for maybe 10 or 20 runs for each classifier, so you have a set of scores $a_i$ and a set of scores $b_i$. compare average of $a_i$ and average of $b_i$, say $avg(a_i)>avg(b_i)$, and $avg(a_i) - avg(b_i) = d$ null hypothesis: A and B are actually the same, we just got better scores $a_i$ by chance. The question is: how likely is it that we observe a difference of $d$ between means? This is where you can do randomization or else a statistical test: put all your $a_i$ and $b_i$ scores in a set, and repeatedly divide them randomly in two. How often do you observe a difference of $d$ or more? That's an estimate of the probability that A and B are indeed equivalent. I say 10 or 20 runs for each classifier, if time permits because with more data you will have more reliable probability estimates... but if you have many candidate feature sets that's a lot of comparisons to be made. Note that if you have to choose a feature /parameter set that will maximize your expected accuracy (or whatever metric), then eventually you should probably choose the one with the highest average accuracy. The only thing you get here is a measure of confidence for the optimality of your choice. Finally I would add a word of warning about this kind of optimization: by optimizing (i.e. picking the best feature set) on the test data you are actually making that test data become training data (for the optimization). That's ok if you're just trying to get the best classifier possible given the available data, but you have to be aware that its accuracy (...) was tested on training data, and on new data it probably won't perform as well.
