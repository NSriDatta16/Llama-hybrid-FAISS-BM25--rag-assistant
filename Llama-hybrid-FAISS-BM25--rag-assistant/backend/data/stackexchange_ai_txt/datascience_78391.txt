[site]: datascience
[post_id]: 78391
[parent_id]: 
[tags]: 
How to normalise(?) an [x,y] time series data set

Problem: I'm currently parsing a time series dataset, of [x,y] coordinates. The data isn't complete - it contains gaps and jitter, and I would like to fill these gaps / normalise the jitter using statistical analysis. Background: I'm currently reading up on non-linear regression (specifically polynomial regression -> PR) - which seems to be the best fit (pun intended) for my problem. I realise that PR deals with arcs "turning in one direction" so, I'm going to try to refactor my code to work with smaller sample sizes - and work my way along the time series. Questions: Am I on the right track? Is there a name for what I'm trying to do? i.e. using polynomial regression across an ongoing graph (curve fitting? trendline? continuous regression?) Is there another technique I can/should use, that provides a better "fit" for my data?
