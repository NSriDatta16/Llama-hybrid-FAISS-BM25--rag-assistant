[site]: datascience
[post_id]: 118781
[parent_id]: 118779
[tags]: 
No, there is no threshold for the amount of training data that can ensure ~100% accuracy for arbitrary models and data. Not all models have the same capacity (e.g. linear regression can represent linear functions on the inputs) and not all deterministic process have the same complexity. Also, not every sample of a process input and output values reflect all the possible cases that can be. This entirely depends on the model and the data. No, a deterministic model does not necessarily imply that models achieve high accuracy. Like the first point, whether models are able to achieve better accuracy or not depends entirely on the model we choose and the data. Yes, overfitting is a problem in your case. Achieving high accuracy on the training dataset does not equate to achieving high accuracy on the whole input data domain. If at inference time the model sees data that it has not seen during training, overfitting is always a risk. Also, take into account that there are multiple sources of stochasticity. A deterministic process lacks aleatoric uncertainty but may have epistemic uncertainty, e.g. stochastic measurement errors, lack of data in some areas of the process' input/output domains. All these factors don't necessarily mean that using a machine learning model to represent a deterministic process is a bad idea. Just that machine learning is not a silver bullet for representing any process, either deterministic or not. In this situation, the model's performance should be evaluated with the same rigour as with any other stochastic process. As a side note, if you know a closed-form expression for your deterministic process, maybe you can go for a mathematical approximation like the Taylor expansion , the Laurent series , the Chebyshev polynomials , or the Pad√© approximant .
