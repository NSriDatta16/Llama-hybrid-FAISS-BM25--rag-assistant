[site]: datascience
[post_id]: 30706
[parent_id]: 26881
[tags]: 
We first center our data by subtracting the mean of the batch. We also divide by the standard deviation, so our formula becomes: $ z = \frac{x - \mu}{\sigma} $ where: $ x $ is the pixel value $ \mu $ is the arithmetic mean of the channel distribution $ \sigma $ is the standard deviation of the channel, calculated as such: $ \sigma = \sqrt{\frac{\sum^N_{i=1} |x_i-\mu|^2}{N}} $ By standardizing the data in this fashion, we obtain a mean ($\mu$) = 0 and a standard deviation ($\sigma$) = 1. This allows SGD to converge faster. It is easier to see how feature scaling can benefit simpler models such as SVMs, but CNNs benefit similarly. (Mini-)Batch normalization makes every pixel have a similar distribution, which helps SGD converge faster. This is because networks generally have shared hyperparameters. For instance, the learning rate is a multiplier for all weights in a network, rather than having a learning rate for every weight; therefore, we want a single learning rate to have proportional influence in all regions. Otherwise, the learning rate could make some weights over-correct and other weights under-correct. This article does a great job of explaining CNN preprocessing. I'll try to highlight the key points (the following images are pulled from it) Let's consider a face recognition challenge: Taking the mean (left) and standard deviation (right) of the batch, we get the following: Our centered data resembles a face, but more importantly our standard deviation image shows great variance along the borders and everywhere except the face. From our z-score formula, we can predict that the standardized values near the border and irrelevant areas will be relatively squashed more than values around the face. The resultant normalization appears as such: Normalizing the input images is widely used and often considered an essential preprocessing step. However, making an absolute statement such as, "normalization is always beneficial" or "normalization is never beneficial" would be misguided. In the scenario you presented, normalizing the data would not benefit the CNN, but the network would still be able to learn the data well. However, this is a very primitive situation, and most real-world problems have significantly more noise and variance for which to account. In general, normalizing the input image will improve SGD convergence time.
