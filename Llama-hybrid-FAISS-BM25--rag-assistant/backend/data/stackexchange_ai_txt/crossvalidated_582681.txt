[site]: crossvalidated
[post_id]: 582681
[parent_id]: 582637
[tags]: 
These simplified formulae from Stanley Сhan's Introduction to Probability for Data Science provide some good intuition on the train/test error: MSE train = σ 2 (1 - d/N) MSE test = σ 2 (1 + d/N) where σ 2 is noise variance (data measurement errors and such), N is the sample size, d is a measure of model complexity (d Overfitting means that a model fits too closely to the training samples so that it fails to generalize (d is comparable to N): train error is low, test error is high. When the model is too simple and uderfits data (d When your test error is suspiciously low, that likely means you've got some sort a of data leakage from a test set (e.g. when you preprocess a whole dataframe before splitting). As of validation vs test error, a separate validation split normally serves as an intermediate test for e.g. best model/hyperparameter selection. Since you purposely select what performs best on this set, it can be treated as a sort of leakage as well. Thus the final test data should remain unseen until the very end of the model development process.
