[site]: datascience
[post_id]: 55460
[parent_id]: 55095
[tags]: 
DCGAN is more about network architecture alterations, while WGAN is an change to the loss function . There's nothing stopping you from using the DCGAN architecture with the WGAN objective function: all this means is minimizing an approximate Wasserstein loss, rather than a Jensen-Shannon divergence, using a particular network architecture. The WGAN (or its followups, e.g. WGAN-GP) objective is agnostic to the architecture. The only thing (that I can think of) that you need to watch for is the use of batch norm: DCGAN recommends putting it everywhere, but (at least for WGAN-GP) it messes with statistics of the critic regularization.
