[site]: crossvalidated
[post_id]: 236987
[parent_id]: 
[tags]: 
The simplest seq2seq model for word mirroring

I'm trying to put together a very basic seq2seq model for character-level word mirroring (it's the last assignment on Udacity's Deep Learning class). Here's the code , pretty boilerplate:( So my seq2seq is actually a single LSTM which works with fixed-length text sequences (input and output are of the same fixed length). Firstly, it encodes the input string character-by-character storing its state, and then it starts generating the output using the state and the last character generated (it actually was generating characters from the very beginning, but we didn't store anything but the last). So, no GO and EOS characters - just a fixed number of encoding steps + the same with decoding. Loss function is simply cross entropy between generated and gold sequences. The problem is, this model only learns to output the last letter of the input, over and over again. What is it: is something fundamentally wrong with my model, or is it a bug in the code? For one, are there any obvious Tensorflow-specific bugs? (I'm new to Tensorflow, and it's not very clear to me how gradient descent works in this setup.)
