[site]: crossvalidated
[post_id]: 330304
[parent_id]: 330303
[tags]: 
I suspect that during training, dropout increases the variance of the activations/feature maps, which batch normalization compensates for. At test or validation time, dropout is "disabled", which suddenly lowers the variance, and screws up the batch normalization (which uses the variance as estimated by a weighted moving average during training). In practice, dropout is not used very often anymore, and batch norm + weight decay + data augmentation serves as sufficient regularization in most cases. edit: A very relevant paper which answers pretty much the same question: https://arxiv.org/abs/1801.05134
