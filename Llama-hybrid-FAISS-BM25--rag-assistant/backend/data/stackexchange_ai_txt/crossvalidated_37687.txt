[site]: crossvalidated
[post_id]: 37687
[parent_id]: 36171
[tags]: 
To answer my own question (I realize this material is probably actually stated elsewhere on this site): I have found that when using complex classifier models (such as an SVM with an RBF kernel or an regularized discriminant classifier) there can be extremely large variance between folds in cross validation, if a large proportion of the data is withheld for testing within each fold. I have found a practical strategy for nested cross-validation is to use leave one out cross validation for the outer loop and k-fold cross validation (in my case k=10) for the inner loop. While this approach does mean that the final performance estimate has a larger variance, I have found it results in a 'better' model; as a larger proportion of data are used to find the optimal model in the inner CV loop.
