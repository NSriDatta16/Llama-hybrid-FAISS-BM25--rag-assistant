[site]: crossvalidated
[post_id]: 309883
[parent_id]: 309861
[tags]: 
The statement as reported is wrong: A most standard example is provided by the James-Stein estimator : given $X\sim\mathcal{N}_p(\theta,I_p)$ $(p>2)$, assuming $\theta$ is estimated under the square error loss,$$L(\theta,\delta)=||\theta-\delta||^2$$the estimators$$\delta_0(x)=x\qquad\text{and}\qquad \delta_{2(p-2)}(x)=\left(1-\frac{2(p-2)}{||x||^2}\right) x$$have exactly the same risk: $$\mathbb{E}_\theta[||\theta-\delta_0(X)||^2]=\mathbb{E}_\theta[||\theta-\delta_{2(p-2)}(X)||^2]=p$$(The proof goes by Stein's technique of the unbiased estimator of the risk and can be found [e.g.] in my Bayesian Choice book .) Actually, when two estimators share the same risk functions, they are inadmissible under strictly convex losses in that the average of these two estimators is improving the risk: $$R(\theta,\{\delta_1+\delta_2\}/2) I presume the confusion stemmed from the concept of completeness , where having a function with constant expectation under all values of the parameter implies that this function is constant. But, for loss functions, the concept does not apply since the loss depends both on the observation (that is complete in a Normal model) and on the parameter.
