[site]: crossvalidated
[post_id]: 612473
[parent_id]: 
[tags]: 
Statistical test to determine if Active Learning has provided a significant improvement?

I am conducting various active learning experiments on two Biomedical Relation Extraction corpora: 2018 n2c2 challenge: 41000 test samples DDI Extraction corpus: 5700 test samples and using four different machine learning methods: Random Forest, BiLSTM-based model, Clinical BERT, and Clinical BERT with an extended input. Initially, I evaluated the performance of all 4 methods on both corpora using all the available data (passive learning setting). Then, I conducted additional experiments using 3 different active learning query strategies (random sampling, least confidence, and BatchBALD) on both corpora, using up to 50% of the data. All experiments were repeated 5 times with different random seeds. The specific active learning process followed in the experiments is as follows: Randomly select 2.5% of the total dataset to create the labeled dataset, while the remaining data forms the unlabeled pool. In the active learning step, query 2.5% of the total data, retrain the model from scratch, and test the newly trained model on the test set, measuring precision, recall, and F1-score. Stop the process when 50% of the entire dataset has been annotated, i.e. 19 iterations have been done. Is there a statistical test suitable for this experimental (C=2 corpora, M=4 methods, S=2 query strategies + random baseline, 5 repetitions of each experiment) setup that allows me to determine if one of the query strategies has performed signifantly better?
