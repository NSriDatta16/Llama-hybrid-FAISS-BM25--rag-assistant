[site]: datascience
[post_id]: 36872
[parent_id]: 
[tags]: 
Hindsight Experience Replay: what the reward w.r.t. to sample goal means

Referring to the paper on Hindsight Experience Replay Is it right that sampled goals which are visited states should be followed by a positive (or non-negative) rewards in order to allow an agent learn? On page 5 of the paper, a "Algorithm 1 Hindsight Experience Replay (HER)" scheme reads in particular: for t = 0, T-1 do r_t := r(s_t, a_t, g) Store the transition (s_t || g, a_t, r_t, s_(t+1) || g) in R Sample a set of additional goals for replay G := S(current episode) for g' âˆˆ G do r' := r(s_t, a_t, g') Store the transition (St||g', a_t, r', s_(t+1)||g') in R end for end for where: g : current goal R : replay buffer All other symbols with a dash indicate that they were sampled in addition to the actual current goal within the current episode. It means (as long as I understand) that for the sampled goals (g') the reward is now a function of action taken in state given the sampled goal. It is not very clear whether the agent will learn the task in case the reward is still the old function (which is non-positive for all states that are different from the final goal). As an example, in a grid-world an agent gets -1 reward while not in the final cell of its destination, but with the new goals introduced, the agent's reward with respect to its current state is not r, but r' (reward after reaching goal). Illustrating example (grid-world):
