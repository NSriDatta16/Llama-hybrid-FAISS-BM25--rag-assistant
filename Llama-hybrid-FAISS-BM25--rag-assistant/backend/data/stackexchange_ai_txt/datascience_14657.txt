[site]: datascience
[post_id]: 14657
[parent_id]: 14655
[tags]: 
You mention two decision trees. Traversing a decision tree is very cheap, so running a feature instance through multiple trees is very fast, you could just take all the decision trees from the data centers and average the outcome, maybe weigh it by the (crossvalidated) strength of the models. Random Forests are powerful models that also combine decision trees in this way, except that they are done on random subsets of the features (and in some cases also random subsets of the data).
