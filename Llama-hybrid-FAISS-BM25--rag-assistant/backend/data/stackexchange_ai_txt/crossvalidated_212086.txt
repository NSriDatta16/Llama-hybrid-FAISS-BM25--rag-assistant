[site]: crossvalidated
[post_id]: 212086
[parent_id]: 212075
[tags]: 
If you bin the questions by frequency, you can get something like the datapoints you're looking for. To use your example, for questions with frequency 1, Alice gets a 0.5, for questions with frequency 2, Bob gets a 1, for questions with frequency 133, Bob gets a 0 and Alice gets a 1. You may want to first apply some sort of shrinkage--knowing that Alice was helpful for 5 out of 5 questions in a class is more useful than knowing that she was helpful for 1 out of 1 questions in a class. If you think helpfulness is something like a Bernoulli trial, then the Beta distrbution makes sense (which is simple--instead of $\frac{right}{total}$ you use $\frac{right+1}{total+2}$). You may also want to do scale-sensitive binning--it might make sense to consider all frequencies above 100 as one group, for example, so that you can get a reasonable rate (it's likely that multiple rare questions will have the same frequency, but unlikely that multiple common questions will have the same frequency). You can now do a number of regression or classification techniques--a logistic regression, like suggested by mdewey, could be appropriate.
