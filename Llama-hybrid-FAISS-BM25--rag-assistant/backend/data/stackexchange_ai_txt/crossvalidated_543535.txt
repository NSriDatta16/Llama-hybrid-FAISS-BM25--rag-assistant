[site]: crossvalidated
[post_id]: 543535
[parent_id]: 
[tags]: 
Loss function in Supervised Learning vs Statistical Decision Theory

I am confused by the different definitions of Loss Function in statistical decision theory vs machine learning. In statistical decision theory, a loss function is typically defined as $L(\theta, \delta(X))$ , where $\theta$ is the true, unknown parameter, $\delta(.)$ is the decision rule, and $X$ is data (generated from $\theta$ ?). See for example lectures of the Theory of Statistics class. In machine learning, it seems the loss function is defined as $L(y, f(X))$ , where $y$ is the true label and $f(x)$ is some model. See, for example, Elements of Statistical Learning chapter 2.4. My question is if they are talking about the same thing. It seems different. For example, if I am going to predict the next coin toss of an unknown coin, I can then model the coin toss as following a Bernoulli distribution with an unknown parameter $\theta$ . Let $X$ be some historical data. Then it appears that the loss function from statistical decision theory is computing my prediction $\delta(X)$ against the unknown parameter $\theta$ whereas in ML, it is computing the same prediction $\delta(X)$ (or $f(X)$ ) against the true label? I am having trouble reconciling the two concepts.
