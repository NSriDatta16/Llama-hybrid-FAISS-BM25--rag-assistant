[site]: crossvalidated
[post_id]: 320819
[parent_id]: 319722
[tags]: 
The network in the referenced DQN paper estimates state-action values for a single state and all possible actions at once. It returns a vector $[\hat{q}(S,a_1), \hat{q}(S,a_2), \hat{q}(S,a_3)...]$ given an input state $S$ - the action space is the same for all states, and is just associated with each possible controller input (plus I assume a "no input" choice). The action choice is then made using $\text{argmax}_a$ over this vector. What exactly happens in each fully connected layer? I understand what the convolutional layers do. How do the fully connected layers decide on which action to take? They don't "decide" what action to take, but calculate the action value for each $a$ - the fully connected layers are just a normal multi-layer neural network used for regression, taking the feature maps extracted from the pixels as input. The actual decision is left to the agent, but the predictions of the network guide the agent to the best estimate of the optimal action given any input state. An alternative, and equally valid, approach is to have the network predict value given an (S, A) pair. Then in order to select an action, the network needs to predict a minibatch of the state plus all possible actions in order to construct the same vector, and select the maximum. Depending on the nature of the environment and possible actions, you may choose either architecture (predict values of all actions at once from state representation or predict one action at at time from state & action representation), they are roughly equivalent theoretically.
