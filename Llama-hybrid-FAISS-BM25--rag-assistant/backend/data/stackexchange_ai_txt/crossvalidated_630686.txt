[site]: crossvalidated
[post_id]: 630686
[parent_id]: 592368
[tags]: 
What you're describing is essentially approached as incremental learning or online learning, where a model is updated as new data comes available without starting from scratch. In your particular case, you've already trained a model on dataset D1 and have a new batch of data, D2 , and you want to update your model to perform as if it had been trained on the combined dataset D1 U D2 , only using D2 for further training. There are a few approaches you could consider: Fine-tuning : Continue the training process with the additional data. This would involve using the parameters learned from D1 as the starting point and training the network further on D2 . You would typically want to use a smaller learning rate to make sure that the model does not deviate too much from what was learned initially. Additionally, it's sometimes beneficial to freeze certain layers to preserve learned features and only train the latter layers on the new data. Elastic Weight Consolidation (EWC): This is a technique to mitigate catastrophic forgetting which can happen when a neural network forgets the knowledge about previous data when it is trained on new data. Whether any of these methods would give you a result equivalent to training on D1 U D2 "from scratch" is not guaranteed and would be problem-dependent. It would also be affected by the size and distribution of D1 and D2 . Theoretical Justification : In terms of a theoretical proof, this is challenging because equivalence would depend on the optimization path taken by the neural network which is influenced by many factors including loss landscape, learning rates, batch sizes, and so forth. However, the theoretical justification for these methods often comes from Bayesian learning principles and the theory of transfer learning . The idea is that the parameters learned from D1 form a prior distribution for the parameters that you want to learn for D1 U D2 . When you start training on D2 , you're updating this prior with new evidence. This is conceptually similar to Bayesian updating of belief distributions. Further notes : Consider the size of D2 relative to D1 . If D2 is much smaller, its impact on the model might be limited unless you apply techniques like oversampling or data augmentation. Monitor overfitting. Since the model is already trained on D1 , it might overfit D2 more quickly.
