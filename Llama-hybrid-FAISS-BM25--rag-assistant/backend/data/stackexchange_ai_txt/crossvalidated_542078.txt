[site]: crossvalidated
[post_id]: 542078
[parent_id]: 542074
[tags]: 
I think you’re misunderstanding the concept of the true distribution of the data. When we’re taking about the (true) distribution of the data or about learning it, we have in mind a mathematical model where the data we observed is a realization of some underlying distribution (or data generating process). Data comes from a distribution, there’s no “distributions”. Going back to your example, if there were two distinctions “semantic” and “syntactic”, this would mean that you are assuming that some of the sentences are generated only based on semantic rules, other only based on syntactic rules. Unlikely your data consists of meaningless, nonsense sentences, that are valid according to syntactic rules. The distribution of the data are all the process that lead to generating your data. You can have a model that is designed to learn something like syntactic rules based on the data, but you would need to specifically guide the model to do so. Within GAN, or any other neural network, you can have components, or subnetworks, learning different tasks. For this you would usually need data labeled according to all the tasks and different loss functions.
