[site]: crossvalidated
[post_id]: 366153
[parent_id]: 
[tags]: 
Random Forest - Is it a good approach to bin categories to reduce the size of the model?

I have a dataset with several categorical columns which I was planning to flatten into binary categories. Let's say I have three features in my dataset. Feature1: has got 300 different values (numeric values) Feature2: has got 450 different values (string values) Feature3: has got 900 different values (string values) Now instead of having a three with three nodes, one per feature, I decided to flatten this three features into binary features. So at the end I will have 300+450+900 = 1650 binary features where only 3 will have a value 1, and the other 1647 will have a zero. I am not sure if it is the best approach, so I was considering to bin the different options into groups of 10 options of the same feature into the same bin. So at the end I will have just 165 binary features which will represent the original 1650. We the binning I will be loosing information, but I the model size will be smaller. I don't have much experience with Random Forest and I am not sure how many features it can handle without problem, and from what number I should start considering the binning approach. I also would like to mention that I am planning to use RandomForest with the Spark-scala API. (Just in case it may be relevant for the question)
