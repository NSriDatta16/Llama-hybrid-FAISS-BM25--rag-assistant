[site]: crossvalidated
[post_id]: 275835
[parent_id]: 275677
[tags]: 
In his famous paper Why Most Published Research Findings Are False , Ioannidis used Bayesian reasoning and the base rate-fallacy to argue that most findings are false-positives. Shortly, the post-study probability that a particular research hypothesis is true depends - among other things - on the pre-study probability of said hypothesis (i.e. the base rate). As a response, Moonesinghe et al. (2007) used the same framework to show that replication greatly increases the post-study probability of a hypothesis being true. This makes sense: If multiple studies can replicate a certain finding, we are more sure that the conjectured hypothesis is true. I used the formulas in Moonesinghe et al. (2007) to create a graph that shows the post-study probability in the case of a failure to replicate a finding. Assume that a certain research hypothesis has a pre-study probability of being true of 50%. Further, I'm assuming that all studies have no bias (unrealistic!) have a power of 80% and use an $\alpha$ of 0.05. The graph shows that if at least 5 out of 10 studies fail to reach significance, our post-study probability that the hypothesis is true is almost 0. The same relationships exist for more studies. This finding also makes intuitive sense: A repeated failure to find an effect strengthens our belief that the effect is most likely false. This reasoning is in line with the accepted answer by @RPL. As a second scenario, let's assume that the studies have only a power of 50% (all else equal). Now our post-study probability decreases more slowly, because every study had only low power to find the effect, if it really existed.
