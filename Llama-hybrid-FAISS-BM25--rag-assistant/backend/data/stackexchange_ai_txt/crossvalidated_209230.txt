[site]: crossvalidated
[post_id]: 209230
[parent_id]: 209215
[tags]: 
Strictly speaking, the classic Heckman two-step estimator does not require that any variables be excluded from the second stage. However, the results are considered unreliable without some excluded variables. Without an exclusion, the model is only identified due to the normality assumption. With an exclusion, assuming normality is not essential. The classic selection model is: $$ y = x\beta + \epsilon $$ $$ d = z\gamma + u $$ where $\epsilon, u \sim N(0,\Sigma)$ and $y$ is observed only if $d>0$. $d$ is unobserved, but the event $d>0$ is observed. In this setting, $$ E[y|x,d=1,z] = x\beta + \rho \sigma_u \lambda(z\gamma), $$ where $\lambda()$ is the inverse mills ratio. This result relies on $\epsilon, u \sim N(0,\Sigma)$. If they are not normally distributed, you can still show that $$ E[y|x,d=1,z] = x\beta + f(P(d=1|z\gamma)) $$ where $f$ is some function that depends on the joint distribution of $\epsilon$ and $u$. Consider the case when $x=z$ and for simplicity imagine they're scalars. Suppose there is some true $\beta_0$ and $f_0$. Then for any value of $\beta$, you could set $f(P(d=1|x\gamma)) = x(\beta_0 - \beta) + f_0(P(d=1|x\gamma))$ and obtain the same $E[y|x,d=1,z]$. In other words, for any value of $\beta$, there is some distribution of $\epsilon$ and $u$ that makes that value of $\beta$ fit the observed data. Having variation in $P(d=1|z\gamma)$ that is independent of $x$ prevents these sort of shenanigans. With enough variation in $P(d=1|z\gamma)$ there is a unique $\beta$ and $f()$ that match the observed $E[y|x,d=1,z]$. Having variables included in $z$ and excluded from $x$ gives you the necessary variation. Assuming normality and having excluded variables generally works okay even without normality because $\rho \sigma \lambda(z\gamma)$ serves as an approximation to $f(P(d=1|z\gamma))$.
