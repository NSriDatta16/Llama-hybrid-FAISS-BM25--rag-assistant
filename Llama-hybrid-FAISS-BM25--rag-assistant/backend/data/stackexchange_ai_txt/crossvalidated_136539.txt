[site]: crossvalidated
[post_id]: 136539
[parent_id]: 
[tags]: 
Jitter/Variation in normal distribution

I am studying frame delay variation (FDV) in packet networks. I will explain what that is (or what I interpret it to be) in more detail in a second. I'm assuming the time difference between two packets follows a normal distribution for various reasons. My empirical tests are not matching what my intuition was expecting. Allow me to explain further. I am taking 10,000 samples from a Gaussian distribution with mean 6 and standard deviation 1. This is my latency. So for example: 5.5, 6.5, 7.0, 5.0 For each sample I use the absolute value of the difference from the previous sample to get my FDV. So for the above it would be: 1.0, 0.5, 2.0 I then average the result. For this simple case I get: 3.5/3 ~ 1.17 When I run this against 10,000 samples I was expecting the FDV to be equal to the standard deviation or the variance (both of which are 1) but instead it appears I am getting 1.25. Does this make sense to anyone? Is there some way to calculate whatever this is for a given normal distribution?
