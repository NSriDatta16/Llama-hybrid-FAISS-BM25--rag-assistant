[site]: datascience
[post_id]: 55823
[parent_id]: 
[tags]: 
How to speed up deep learning research?

I am working on multimedia enhancement tasks using deep generative neural networks. I only have access to one 11 GB GPU, and I really want to speed up my research. I am currently using a deep residual GAN, all GPU memory is occupied with a batch size of 8. I want to experiment with using different architectures but every single trial takes at least 3 days. I have an idea about comparing multiple models in parallel, by reducing the model sizes, batch size or the input size. Once I find the best performing model, I can scale it up. Is this a bad idea? What are some other methods?
