[site]: crossvalidated
[post_id]: 226247
[parent_id]: 226230
[tags]: 
To do gradient descent, you need continuous parameters, and the loss function has to be differentiable with respect to them. Random forests have discrete hyperparameters (e.g. tree depth, number of trees, number of features, etc.). So, unfortunately, gradient descent won't work in this context. Another point is that the loss function (presumably test set error) may have multiple local minima, and any local search procedure can get trapped. Grid search and random search can be used to explore a broad range of hyperparameter space, and you can hone in on good regions after your initial search. Grid search is popular but, as Simone mentioned, random search can be faster in some cases. This is because some hyperparameters may not strongly affect performance relative to the others. Grid search will waste time exploring different values of the unimportant hyperparameters while holding the more important hyperparameters fixed. Random search updates all hyperparameters on each step, so it has a better chance of hitting the important ones. Bergstra and Bengio (2012) . Random Search for Hyper-Parameter Optimization.
