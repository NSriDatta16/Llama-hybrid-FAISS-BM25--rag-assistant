[site]: crossvalidated
[post_id]: 497535
[parent_id]: 497507
[tags]: 
Naive Bayes is not the best example in here, hence your confusion. Naive Bayes is the whole classification algorithm, that tells us how to make classifications given the data, by calculating the conditional probabilities and combining them, by making the naive assumption of independence. I said that this is not the best example, because the algorithm uses Bayes theorem to combine the probabilities, so people sometimes confuse this with thinking that the algorithm has something in common with Bayesian statistics, it doesn't. You are correct, in naive Bayes the probabilities are parameters, so $P(Y=y_k)$ is a parameter, same as all the $P(X_i|Y=y_k)$ probabilities. The standard, maximum likelihood, approach is to calculate the probabilities using MLE estimators, e.g. for $P(Y=y_k)$ you would count the cases where $Y=y_k$ and divide by the sample size, same for conditional probabilities, but there you would count within subsets. Bayesian, maximum a posteriori, estimate for such probabilities would need from you to assume prior distributions for the parameters. The popular model for binary data would be beta-binomial model , or Dirichlet-categorical for categorical variables, where we have closed-form solutions for the posterior distributions of the parameters.
