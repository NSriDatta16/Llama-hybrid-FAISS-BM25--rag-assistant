[site]: crossvalidated
[post_id]: 529890
[parent_id]: 
[tags]: 
What does the numpy std documentation mean when it says it is always biased?

The documentation for the numpy np.std() function states: The average squared deviation is typically calculated as x.sum() / N , where N = len(x) . If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of the infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ddof=1 , it will not be an unbiased estimate of the standard deviation per se . (emphasis added) What does the final statement in bold mean? Where does the estimate break down and become biased?
