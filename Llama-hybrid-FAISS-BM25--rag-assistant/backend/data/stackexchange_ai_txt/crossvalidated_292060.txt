[site]: crossvalidated
[post_id]: 292060
[parent_id]: 291611
[tags]: 
Let's start with: So, I think I can say that if I calculate a predicted value, x, there is a 68% chance for the "true value" to be within 1 $SE_x$ of x. Is this correct? No, that is not correct unless you were using Bayesian analysis. There is either a zero percent or a one hundred percent chance the true value is in the region. There is no way to tell. Confidence intervals are not measures of the precision of your work. They do not measure uncertainty regarding the parameter. It is tied to the role chance plays in the sampling distribution. What they do tell you is that if you were to perform the experiment an infinite number of times, then at least 68% of those experiments would contain the true value of the parameter within $\pm{1}SE.$ They provide no probability that the data is inside an interval. There are an infinite number of intervals for any given $\alpha$ level of confidence. There isn't just the one you gave. There are other ways that are also standard to construct error bars, such as nonparametric ones. Your colleague is correct if the conditions of linearity, independent errors, normal errors, and homoscedasticity are met along with the requirement that the prediction is inside the valid set allowed in the scientific model. If not, then it is the wrong form for the predictive interval. Obviously, the predictive interval is far wider than the confidence interval. It contains both the impact of chance on your parameter estimate plus the effect of chance in nature itself. Nature doesn't know you made a prediction. There is a chapter in Decision Theory: Principles and Approaches by Giovanni Parmigiani on scoring rules for predictions. It is generic rather than applied though. If you were looking for "give me the formula to plug in," then this is the wrong book. The lecture you attached provides a discussion of predictions. As to your question, "I want to know how to calculate the uncertainty in my prediction," then you need to use a Bayesian method. Bayesian methods are built around uncertainty, Frequentist methods are built around chance. They are subtly different things. In Pearson-Neyman methods, the null hypothesis and the sample space drive the math. In Bayesian methods, information and a model of nature drive the math. Rather than just switch to Bayesian analysis, however, I recommend you learn a bit about both and ask yourself what you really want to know. They provide different things using the same data. They are not two different ways to solve a problem, they solve two different problems. There is a good discussion, here on Stack Exchange, on the difference between Bayesian and Frequentist intervals. It is at What's the difference between a confidence interval and a credible interval? It may provide you with some clarity with what you really want to ask and know, and the limits of what either method can provide.
