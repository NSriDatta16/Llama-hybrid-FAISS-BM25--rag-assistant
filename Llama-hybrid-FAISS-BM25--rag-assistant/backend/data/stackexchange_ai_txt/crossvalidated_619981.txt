[site]: crossvalidated
[post_id]: 619981
[parent_id]: 333697
[tags]: 
XGBoost is well calibrated providing you optimise for log_loss (as objective and in hyperparameter search). ML models tend to "default" to overfitting (as opposed to eg logistic regression, where you default to using just the linear terms - not all possible interactions and power terms etc) In the below I took an example from the betacal package (as suggested by @usÎµr11852 ) and used xgboost with default parameters or searching for the best hyperparameters (using cal dataset for calibration and hyperparamater search and test for out of sample). I then compare log loss and calibration curves of default vs log loss optimised hyperparameters. Whilst calibration helps the default model, it has minimal or even negative effect on logloss for the optimised model. (I treat log_loss also as metric for calibration). I should note that I'm slightly dubious about the dataset. whilst test logloss ~.15 it sometimes can be as low as .08 (just based on data split) #%% import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from xgboost import XGBClassifier from sklearn.calibration import calibration_curve from sklearn.metrics import log_loss import scipy.stats as stats from betacal import BetaCalibration import plotnine as p9 def log_loss_s(target,pred): ll=-np.where (target==1,np.log(pred),np.log(1-pred)) ll_m = ll.mean() ll_s = ll.std() ll_sem= ll_s / len(ll) return {"mean": ll_m, "std": ll_s, "sem": ll_sem} def random_search(results, params_base,params_dist,iter, best_model=None): """ append to results in place""" if results: results_s = list(sorted(results,key = lambda x: x["best_score"])) best_score =results_s[0]["best_score"] assert best_score == best_model.best_score else: best_score = None for i in range(iter): params = params_base.copy() for key,dist in params_dist.items(): params[key]=dist.rvs() xgb = XGBClassifier(**params) xgb = xgb_opt.fit(x_train, y_train,eval_set=eval_set,verbose=0) if best_score is None or xgb_opt.best_score
