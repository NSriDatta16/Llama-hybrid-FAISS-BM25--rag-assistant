[site]: datascience
[post_id]: 115407
[parent_id]: 
[tags]: 
Standardization after log transformation

I have a few question about log transformation and standardization. First: Should I standardize my features after doing log transformation? Second: I still do not understand, because when doing log transformation it change the shape distribution to normal distribution. But when do standarization (standardscaler) it will not change the distribution shape. So which one should I use if I have skewed features? Third: I have seen in many Kaggle competitions that people tend to use standardscaler over log transformation event though the features are skewed. What is the reason? If it is okay to use standard scaler, does that mean that machine learning models can also work well even having skewed features?
