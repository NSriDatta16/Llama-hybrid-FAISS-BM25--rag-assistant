[site]: stackoverflow
[post_id]: 2502133
[parent_id]: 2002835
[tags]: 
i don't think there's a quick answer to this. if i was you, i would buy/borrow a copy of Advanced Programming in the Unix Environment (APUE) by Stevens and Rago and read Chapter 15 and 16 on IPC. It's a brilliant book if you really want to understand how *nix (a lot of it applies to any POSIX system) works down to the kernel level. If you must have a quick answer, i would say the following (without putting a huge amount of thought into it), in descending order of efficiency: Local Machine IPC Shared Memory/Memory Mapped files Named Pipe/FIFO (only between related processed - i.e. fork) Unix Domain Socket Network IPC/Internet Sockets Datagram Sockets Stream Sockets Raw Sockets At both levels, you are going to have to think about how the data you transfer is encoded/decoded and trade off between memory usage and CPU utilization. At the Network level, you will have to consider what layers of protcols you are going to run on top of. Most commonly, at the bottom of the application layer you will be choosing between TCP/IP or UDP. TCP has a lot more overhead as it is does error correction, checksumming and lots of other stuff. if you need in order delivery of messages you need to use TCP as opposed to UDP. On top of these are other protocols like HTTP, SOAP (on top of HTTP or another protocol like FTP/SMTP etc.). A binary protocol is going to be more efficient as long as you are network bound rather than CPU bound. If using SOAP on the MS.Net platform, then binary encoding of the messages is going to be quicker across the network but may be more CPU intensive. I could go on. it's not a simple question. Learning where the latencies are and how buffering is handled are key to being able to make decisions on the trade offs you are always forced to with IPC. I'd recommend the APUE book above if you really want to know what is going on under the hood...
