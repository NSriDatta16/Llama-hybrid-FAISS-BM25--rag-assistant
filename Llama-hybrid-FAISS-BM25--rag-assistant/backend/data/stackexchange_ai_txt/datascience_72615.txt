[site]: datascience
[post_id]: 72615
[parent_id]: 
[tags]: 
Negative impact of "important" features on model performance

I have a random forest regressor with a set of base features, fit & optimised with sklearn random search algorithm. When I add a set of additional features and retrain (again with random search optimisation) the model performance becomes worse. Afterwards, I use multiple algorithms to determine the feature importance of all features - recursive feature elimination (RFE), built-in RF feature importance, permutation importance. Some of the newly added features actually show a high importance (with all algorithms). So how can it be that features, which show high importance actually reduce the model performance (significantly)? I could not yet find any satisfying answer to this problem (or misunderstanding of mine).
