[site]: datascience
[post_id]: 29895
[parent_id]: 
[tags]: 
Cost/loss functions for multi-tasking regression neural networks

The mean square loss function is the standard for regression neural networks. However, if I have a neural network learning two tasks (two outputs) at once, is it more advisable to train on the sum of the relative errors for the different outputs or the sum of the mean square errors of both tasks? Intuitively, the mean square loss function favours the task with larger expected values so it's "unfair" to the other task. I'm leaning towards using relative error but are there any caveats to using it as the loss function? Update: Problem with using relative error is that it's a percentage and therefore the gradient updates will be very small. I think it may be more practical to use an error function proportional to the mean relative errors but scaled so that they are large enough.
