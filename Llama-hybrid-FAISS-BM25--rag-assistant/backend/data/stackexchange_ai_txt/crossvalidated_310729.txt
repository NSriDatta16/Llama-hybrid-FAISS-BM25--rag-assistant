[site]: crossvalidated
[post_id]: 310729
[parent_id]: 310615
[tags]: 
Pragmatically, I would train first without dropout and check validation loss to see if there is overfitting. If there isn't, then there is no problem. If there is, then go back and add dropout. I don't think you can determine if there will be overfitting before hand. It's important to keep in mind that you can overfit even if the number of parameters is much less than the size of the data. This is probably the case in most image-segmentation tasks, where there are on the order of 1 million labels per image and only several million parameters in the network, yet it's still possible to overfit here.
