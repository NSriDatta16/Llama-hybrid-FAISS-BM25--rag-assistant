[site]: datascience
[post_id]: 90075
[parent_id]: 
[tags]: 
why would you mask out padded activations from the training loss?

I've followed taming-lstm for training a LSTM model on a NLP task in batches with various sentence lengths. One of his main points is: Trick 3: Mask out network outputs we donâ€™t want to consider in our loss function and at least for my task, that was simply not true. I've tried different batch sizes, with heterogeneous sentence lengths in each batch, and reached the same model quality when I trained with the complex loss the author is suggesting and a simple cross-entropy loss. Is it just for my specific task and in other cases there's a sound theoretical/empirical evidence that this trick helps? It's certainly complicating the code...
