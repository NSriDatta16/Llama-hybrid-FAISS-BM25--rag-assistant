[site]: crossvalidated
[post_id]: 494931
[parent_id]: 
[tags]: 
Equivalent definitions of Markov Decision Process

I'm currently reading through Sutton's Reinforcement Learning where in Chapter 3 the notion of MDP is defined. What it seems to me the author is saying is that an MDP is completely defined by means of the probability $p(s_{t+1},r_t | s_t, a_t)$ . However according to wikipedia , and many other references I've found an MDP is a quadruple of set of states, set of actions, transition probability $P_{s_{t+1,s_t}}^{a_t} = p(s_{t+1} | s_t, a_t)$ and an expected reward $R_{s_{t+1},s_t}^{a_t}$ . Now I'm assuming one definition implies the other, unless I'm wrong so please tell me... The thing is Sutton manages to derives from it's definitions the wikipedia one. However as I can't find the other way around I was trying to show the converse. The only hunch I had was observing that $$ R_{s_{t+1},s_t}^{a_t} = \sum_{r_{t+1},s_{t+1}} r_{t+1} p(r_{t+1}, s_{t+1} | s_t, a_t) $$ and maybe there're conditions that actually would allow me to retrieve $p(r_{t+1},s_{t+1} | s_t, a_t)$ but I cannot honestly see it. Can you tell me then if the two are equivalent and why?
