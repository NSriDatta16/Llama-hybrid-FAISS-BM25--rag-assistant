[site]: crossvalidated
[post_id]: 541049
[parent_id]: 
[tags]: 
Why the asymmetric design between (Q, K) and V in tranformer's attention?

In the Attention is all you need paper, the self-attention layer is defined as $\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V$ . I would like to know why a more symmetric design with regards to those 3 matrices isn't favored. For example, the design could have been made more symmetric with a 4th matrix: $\text{AttentionAlt}(Q, K, V, W) = \frac{1}{\sqrt{d_k}} \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)VW^T$ This aspect strikes me because about the only structural element (putting the masks aside) in the whole transformer architecture which isn't symmetric with regard to encoder and decoder blocks. Note: The matrices are explained with an Information Retrieval (IR) analogy: $Q$ is the Query, $K$ is the Key and $V$ is the value. However, this analogy is relatively weak: $Q$ and $K$ are completely symmetrical with one another which is not the case with IR. In the encoder and decoder block, the multi-head attention can be seen as a way to loosely introduce symmetries for those three matrices. However, in the encoder-decoder block, $Q$ and $K$ are connected to the encoder block, while $V$ is connected to the decoder block.
