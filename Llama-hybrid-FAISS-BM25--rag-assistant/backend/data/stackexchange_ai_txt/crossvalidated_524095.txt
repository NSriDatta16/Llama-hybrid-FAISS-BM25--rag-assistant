[site]: crossvalidated
[post_id]: 524095
[parent_id]: 
[tags]: 
how to interpret Principal component Analysis result for feature selection

I am implementing PCA on unsupervised data. I chose 10 components and the data features are almost 400. Generated two tables one is telling variance of every principal component. The other is telling the correlation between every component and feature. How to interpret these values and select features. I have to look at variance or look at the correlation table. In table 2 Principal component 1 has a high correlation with F1, F2, F4, F5, F7, F8 by looking at this which one to drop and pick. Can anyone help me in interpreting this? and help me in choosing the best feature by looking at the PCA result. And the last thing will be is it goood to choose 10 PC for 400 features data The black figure generated through this code pca_df = pd.concat(pca_list, axis=1).T.set_index('n') print(pca_df) the second table shown in figure used this code features_df = (pd.concat(feature_weight_list) .pivot(index='n', columns='features', values='values')) print(features_df) # sns.set_palette(palette) import seaborn as sns import matplotlib.pyplot as plt import os import warnings warnings.simplefilter(action='ignore', category=FutureWarning) import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) #os.chdir('data') # from colorsetup import colors, palette # sns.set_palette(palette) #%matplotlib inline data = pd.read_csv('C:/Users/user122/Downloads//mydata.csv') print(data.shape) ##DROP ALL CPUMNS WITH NaN data= df.dropna(axis=1, how="any") columns_names=data.columns.tolist() print("Columns names:") print(columns_names) print(data.shape) data.head() # fill all null wioth zero data.fillna(0) #data=data.drop(\['Name', 'QQ', 'Times'\], axis=1) print(data.dtypes) #convert to floats for col in data.columns: data\[col\]= data\[col\].astype(np.float) #preserve orignal data data_orig = data.copy() #o we're going to examine the correlation between each one of our different features. # And recall this will be important as when we are doing PCA. What we will be looking # for is if two features are very highly correlated, they're not adding any extra information # and we want to remove or reduce those or combine a few to end up with less features overall. So if they're highly correlated, we can probably remove some without losing much variance from the overall data set. We're then going to perform any transformations and scale our data using whatever scaling method you prefer. corr_mat = data.corr() for x in range(corr_mat.shape\[0\]): corr_mat.iloc\[x,x\] = 0.0 print(corr_mat) corr_mat.abs().idxmax() #examine the skew values and log transform log_columns = data.skew().sort_values(ascending= False) log_columns = log_columns.loc\[log_columns > 0.75\] log_columns #log transformation for col in log_columns.index: data\[col\]= np.log1p(data\[col\]) #scale data again by using min max scalar just to mix things u #from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler() for col in data.columns: data\[col\]= mms.fit_transform(data\[\[col\]\]) #isualize relation between 2 var sns.set_context('notebook') sns.set_style('white') sns.pairplot(data); from sklearn.preprocessing import FunctionTransformer from sklearn.pipeline import Pipeline log_transformer =FunctionTransformer(np.log1p) estimators = \[('log1p', log_transformer), ('minmaxscale', MinMaxScaler())\] pipeline = Pipeline(estimators) #convert the orignal data data_pipe = pipeline.fit_transform(data_orig) np.allclose(data_pipe,data) from sklearn.decomposition import PCA pca_list = list() feature_weight_list = list() #fit a range of PCA models for n in range(1,6): #create fit the model PCAmod = PCA(n_components=n) PCAmod.fit(data) #X_test.fillna(X_test.mean()) #store the model and variance pca_list.append(pd.Series({ 'n': n, 'model': PCAmod, 'var': PCAmod.explained_variance_ratio_.sum()})) #claculate and store feature importance weights = PCAmod.explained_variance_ratio_.reshape(-1,1)/PCAmod.explained_variance_ratio_.sum() overall_contribution = np.abs(PCAmod.components_)*weights abs_feature_values = overall_contribution.sum(axis=0) feature_weight_list.append(pd.DataFrame({ 'n': n, 'features': data.columns, 'values': abs_feature_values/abs_feature_values.sum()})) pca_df = pd.concat(pca_list, axis=1).T.set_index('n') print(pca_df) #create a table of feature importance for each data column feature_weight_list\[1\] features_df = (pd.concat(feature_weight_list) .pivot(index='n', columns='features', values='values')) print(features_df) sns.set_context('talk') ax = pca_df\['var'\].plot(kind='bar') ax.set(xlabel='no of dimesions', ylabel='percent explained variance', title='explained variance vs dimesions'); #plot for feature importance ax=features_df.plot(kind='bar') ax.legend(loc='upper right') ax.set(xlabel='no of dimesnions', ylabel= 'relative importance', title='feature importance vs dimesions');
