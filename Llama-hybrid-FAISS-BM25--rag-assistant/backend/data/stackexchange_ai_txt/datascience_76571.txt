[site]: datascience
[post_id]: 76571
[parent_id]: 
[tags]: 
Understanding output probabilites of xgboost in multiclass problems

I would like to understand the output probabilities of a xgboost classifier (or any other decision tree ensemble based classifier) in the case of a multiclass problem. For example: We have 5 different classes and a trained model on some data belonging to those classes. I would expect, given some test data, the model would give output probabilities of up to 100% when it is really sure the data belongs to some class. Obviously the probabilites still have to add up to 1. On the contrary I was told that in the case of 5 different classes, the model would be already sure when outputting 20% (100/5) for the data. I also see this in a problem I am facing, the probabilities are never higher than 30%. I would like to understand this apparent fact, ideally pointing me to a source where this is explained.
