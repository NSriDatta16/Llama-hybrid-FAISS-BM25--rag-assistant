[site]: crossvalidated
[post_id]: 642407
[parent_id]: 
[tags]: 
Features available during training but not at prediction

Broadly, my motivation is to understand if/how features available during training but not at prediction can be used to improve the prediction accuracy of a machine learning model. This question is about implementing a specific idea I wanted to test. SHAP values are a measure of the contribution of each feature to the overall prediction of a machine learning model. The definition of SHAP values involves model predictions for every subset of features. For predictions from a tree-based model using a feature subset, my understanding of how this works is: At each node, if the feature at the node is in the subset, we proceed as usual. If the feature at the node is not in the subset, we assign a weight to the left and right child of that node, based on how the training data at that node divided into left and right. Rather than winding up with a single leaf node for each tree, we end up with a probability distribution on leaf nodes. The prediction is then the weighted mean of the values at those leaf nodes. XGBoost has built-in functionality to compute SHAP values (the predict method of a booster object is given the parameter pred_contribs=True ). I am wondering: is there a way to get predictions for subsets of features from XGBoost , or from other tree-based models, apart from writing the whole calculation from scratch? I'm not positive whether XGBoost is actually computing these subset predictions as part of SHAP, or whether it is using some different heuristic.
