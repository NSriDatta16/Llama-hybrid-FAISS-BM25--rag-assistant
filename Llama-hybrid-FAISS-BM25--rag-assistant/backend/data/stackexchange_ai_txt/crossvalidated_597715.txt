[site]: crossvalidated
[post_id]: 597715
[parent_id]: 569801
[tags]: 
I'm a bit confused by the previous replies, I might of course be missing something. Using neural networks leads indeed to an identifiability problem, i.e., multiple parametrizations lead to the same model, and therefore, to the same loss. That being said, a $L2$ regularization changes the loss of these identical models to favorize those with lower $L2$ parameter norm. If $H$ is the hessian of your empirical risk, then the hessian of your $L2$ penalized empirical risk is $H'=H+\lambda I$ , where $I$ is a diagonal matrix and $\lambda$ is your penalization coeficient. From there you can compute $\lambda'$ , the value such that $H'$ is positive definite for all parameters, which is the absolute of the smallest eigenvalue of $H$ for the parameters minimizing that eigenvalue. Now we have $H'=H+\lambda'I > 0$ , from what I understand this is a necessary and sufficient condition for contrained risk to be convex. Of course a notable problem is the fact that neural networks have models with identical $L2$ norm and identical empirical risk at the same time, and this for any given set of parameters, this is due to the neurons in a same layer being interchangeable. This means that your convex empirical risk will necessarily have identical parameters at all interchangeable positions, has this is the only way a convex function can tolerate interchangeability, which makes the hidden layers all have effective width $1$ . I am not $100\%$ sure, but I'd wager using a $L2$ penalty with monotone parameterwise weight could do the trick in maintaining the width, as it would impose an order. For example, for the p-vector of parameters $\theta$ , instead of using $\lambda||\theta||^2_2$ , you could use $\theta^Tdiag(\lambda, \lambda^2, ..., \lambda^p)\theta$ .
