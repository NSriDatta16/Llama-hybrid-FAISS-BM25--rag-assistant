[site]: datascience
[post_id]: 103623
[parent_id]: 103603
[tags]: 
The first challenge you will face is transforming the textual labels of symptoms into numeric features that can be understood by a model. The simplest techniques are ordinal encoding, where each word is assigned to a number between 1 and N and one-hot encoding (OHE), where each word becomes an unique N-length vector, with all elements == 0 except for one. A good starting point for your project would be to one-hot encode each symptom, and for each entry in your dataset, sum the one-hot encoded symptoms to obtain a vector that describes all symptoms. This vector would be the input to your model. For example, if you had the following one-hot encodings: "vomiting" -> [1, 0, 0] "nausea" -> [0, 1, 0] "irritability" -> [0, 0, 1] Then an instance where there is both vomiting and nausea would be encoded as [1, 1, 0]. The drawback of the simple techniques is that the word encodings will not be very semantically meaningful. Notice that the distance between "vomiting" and "nausea" is the same as the distance between "vomiting" and "irritibility", even though vomiting and nausea are probably semantically closer in this context. So if the simple techniques don't work well enough for you, then you can try word embeddings. Models such as Word2Vec and GloVe transform each word into a vector where distance in the vector space is meaningful. So "nausea" and "vomiting" will be close to each other, but "vomiting" and "irritibility" will be farther away. Nowadays there are even more powerful embeddings trained on huge corpura of text. Examples include BERT and GPT-2 . But I think this would be major overkill for your use case. The next challenge is selecting a model that can map the encoded words to the disease. This is a multi-class classification problem , so that rules out binary classifiers. If there are a lot of different symptoms then your input vector is going to be high-dimensional, so it would be nice to have a model with built in feature selection or dimensionality reduction. Starting out, I would recommend a ensembled decision tree model like Random Forest or XGBoost . These models are easy to train and generally perform well on this type of problem. Also, if you're using scikit-learn, you can easily get probabilities for each class with the predict_proba() method. If the tree-based models don't work well, you could try other simple approaches like kNN or Naive Bayes If you need more complexity, neural networks and deep learning are always on the table. But if your dataset has only 1450 examples, that's probably not enough to train most DNNs. So to summarize, you need to encode the symptoms numerically and a good first try would be one-hot-encoding. You also need to pick a classifier that works well for this type of problem and can output probabilities rather than a single prediction. I think a good start would be the tree ensembles from scikit-learn. If the simple options don't work well enough, you can step up complexity on either the encoding side or the modeling side.
