[site]: crossvalidated
[post_id]: 294756
[parent_id]: 
[tags]: 
Why nowadays ML algorithm rarely use optimizing functions based on newton method?

I am currently working on coursera Andrew Ng's machine learning course, and wonder why nowadays deep learning algorithms do not use fminunc (assuming a Matlab/Octave environment) for optimizing weights. In my knowledge, fminunc does not have to set learning rate so I think it would be convenient to optimize weights compared to gradient-descent models. Is there any disadvantages when i use fminunc instead of gradient descent optimizing functions?
