[site]: crossvalidated
[post_id]: 475181
[parent_id]: 
[tags]: 
multi layer perceptron tested in raw data vs edited data

I have a dataset of 113x113 gray images divided into 3 classes where I have applied different multi layer neural network classification algorithms taking as features the total of pixels 113x113=12769 features. Although I haven't achieved more than 65% os success when testing it on the testing data. However, when I test a new neural network but taking now into account just only 9 new features that are calculated from the raw data such as the mean of the pixels, the standard deviation,... so in this case I get to achieve 74%. How can I get better results? Does this make sense? Are not the neural networks supposed to be tested only in raw data from images so as to find out patterns? model = Sequential() model.add(Dense(512, input_shape=(12769,),activation='relu')) model.add(Dense(78,activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) vs model = Sequential() model.add(Dense(64, input_shape=(9,),activation='relu')) model.add(Dense(12,activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
