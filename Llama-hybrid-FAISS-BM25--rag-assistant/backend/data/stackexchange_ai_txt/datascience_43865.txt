[site]: datascience
[post_id]: 43865
[parent_id]: 
[tags]: 
What can I infer from similar accuracy using two different DNN architectures?

I am working on a 'two-class classification of multivariate time-series' problem. I used two different approaches: 1) Manual time-series feature engineering (such as slope, intercept, variance, etc. using tsfresh python library) followed by a multi-layer perceptron. 2) Conv1D followed by LSTM followed by Dense layers (keras/Python) without any feature engineering. They both yield very similar results (~40% precision, ~95% recall). Can I infer anything from the similarity of results from two different architectures?
