[site]: crossvalidated
[post_id]: 185744
[parent_id]: 
[tags]: 
How to interpret lags in cointegration test for constructing the mean reverting series

For bivariate time series cases, the Engle-Granger two step cointegration test is essentially testing the linear combination for a unit root. The format of the error term is thus: $$ y_t - \gamma x_t = \epsilon_t $$ When a lag is used in the ADF test, say of length $n$, is the mean reverting series: $$ y_t - \gamma x_{t-n} = \epsilon_t $$ Or is it something else? I am trying to understand how the lag changes the mean reverting series. Also, does this generalize to other cointegration tests and models? Edit Example: Let $y$ = housing prices in San Francisco and $x$ = housing prices in New York City. The approximation process is given by the linear combination $y_t-\gamma x_t = \epsilon_t$. Testing $\epsilon_t$ for a unit root via ADF results in a high probability of cointegration with a lag of $n$. Does this mean the mean reversion process is $y_t - \gamma x_{t-n} = \epsilon_t$, meaning there is a lag in the relationship such that the housing prices in San Francisco are cointegrated with the housing prices in New York $n$ time steps ago? If not, how does the lag express itself in the relationship of the housing prices?
