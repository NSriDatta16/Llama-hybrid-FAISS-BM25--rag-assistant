[site]: crossvalidated
[post_id]: 428831
[parent_id]: 
[tags]: 
Conceptual problem with logistic regression using LASSO regularization and cross-validation

When performing k-fold cross-validation (CV) then k training sets are used to generate k models. This is not a problem to understand when each model contains the same predictors. The conceptual difficulty I have is that with L1 regularization, which can shrink predictors to zero, it's possible that different predictors are reduced to zero in different iterations through the CV. For example if there are $3$ predictors $x_1, x_2, x_3$ then the first cycle through the CV loop may generate a model using predictors $x_1, x_2$ but the 2nd cycle may generate a model using predictors $x_1, x_3$ and so on. Is it OK/justified to then average the estimates of the test error from each fold, when the models may be using entirely different predictors?
