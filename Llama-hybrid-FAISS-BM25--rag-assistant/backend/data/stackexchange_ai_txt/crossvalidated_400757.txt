[site]: crossvalidated
[post_id]: 400757
[parent_id]: 400571
[tags]: 
Differential privacy is an example of a privacy enhancing technology that acts on data during a computation (such as during training on a machine learning model, or via a query mechanism over an aggregated set of data). In this "global" privacy setting, the raw data is stored somewhere but derived data (e.g. the machine learning model or the output statistic of a query mechanism) is considered differentially private if it does not reveal the presence or absense any any single user's contribuiton to the original dataset. The paper that you link to uses publicly available Facebook data (demographic data and connections between users). In order for differential privacy to be "applied" to this scenario, Facebook would have to build and maintain a query service and restrict access to raw data such that only noisy aggregate statistics are returned. For example, you might query how many people with certain attributes identify as LGBTQ+ and you would get a single number with some added random noise to protect the privacy of the individual users. Alternatively, Facebook could reveal individual user statistics using a "local" differential privacy setting. In this case you would visit a single user's profile and the information returned would be very noisy. For example a 30 year old person's profile might show his age as 35 or 27 with noise added. Obviously, in reality, Facebook would never employ either of these two applications of differential privacy. It is not in Facebook's interest to hide this information from users and so anyone who associates themselves with Facebook risks breaches of privacy such as those discussed in the paper.
