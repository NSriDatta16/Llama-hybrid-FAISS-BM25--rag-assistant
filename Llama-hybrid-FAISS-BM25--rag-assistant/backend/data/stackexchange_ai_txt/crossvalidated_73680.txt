[site]: crossvalidated
[post_id]: 73680
[parent_id]: 
[tags]: 
Combining posterior probabilities from multiple classifiers

I am new to machine learning and can't get my head around this problem. I have two patient datasets, the first ($D_1$) contains $Y,Z,X$ that convey blood-sample information and the second ($D_2$) contains $W,T,X$ that convey x-ray information. In both datasets, $X$ is the common diagnostic output. Since there are two distinct datasets, I modelled a solution with two Naive Bayes models as below, where $X$ is the common dependent variable. ( I am aware I can use other techniques than Naive Bayes but that is not the aim of my question.) $P(X|Y,Z,D_1) \propto P(Y|X,D_1) \cdot P(Z|X,D_1) \cdot P(X,D_1)$ $P(X|W,T,D_2) \propto P(W|X,D_2) \cdot P(T|X,D_2) \cdot P(X,D_2)$ I want to combine the posterior probabilities of X in these models: $P(X|Y,Z)$ and $P(X|W,T)$ to determine an overall outcome probability $P(X|Y,Z,W,T)$ (diagnostic outcome) of a new patient. How can I combine these probabilities? Can it be done as below? $P(X|Y,Z,W,T) \propto \frac{P(X|Y,Z) * P(X|W,T)}{P(X,D_{combined})}$ If so, what is the relationship between the priors $P(X,D_1)$, $P(X,D_2)$ and $P(X,D_{combined})$?
