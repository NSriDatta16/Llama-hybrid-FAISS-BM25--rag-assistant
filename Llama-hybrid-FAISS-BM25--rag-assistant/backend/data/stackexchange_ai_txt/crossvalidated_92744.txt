[site]: crossvalidated
[post_id]: 92744
[parent_id]: 92741
[tags]: 
You seem to misunderstand support vectors. Support vectors are not at distance exactly 1 to the separating hyperplane. Support vectors arise as a result from the constraints in the optimization problem. The constraints basically require that every data instance should be out of the classification margin and on the right side of the separating hyperplane defined by $\mathbf{w}$ and $b$. The following Wikipedia image illustrates this idea nicely: For positive instances $\mathbf{x}_p$, we require that $\mathbf{w}^T\mathbf{x}_p - b \geq 1$ and for negative instances $\mathbf{x}_n$ this means $\mathbf{w}^T\mathbf{x}_n - b \leq -1$. Through clever use of the binary class labels (e.g. $y_i \in \{-1, +1\}$) we can write this in a single form. For notational simplicity, these constraints can be written as follows for linear SVM: $$y_i\big(\mathbf{w}^T\mathbf{x}_i - b\big) \geq 1 - \xi_i, \quad \forall i$$ where $\mathbf{y}$ is the vector of training labels, $\mathbf{w}$ is the separating hyperplane, $\phi(\cdot)$ is the embedding function and $b$ is the bias. $\xi$ is a vector of slack variables which allow soft margin classification. These slack variables are positive when the constraint is violated and zero otherwise. Such a constraint exists for every training instance. Support vectors are those instances for which $\xi_i$ is necessarily nonzero, e.g. $\mathcal{S} = \{i : y_i\big(\mathbf{w}^T\mathbf{x}_i - b\big)
