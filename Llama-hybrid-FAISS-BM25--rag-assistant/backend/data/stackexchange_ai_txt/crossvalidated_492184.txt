[site]: crossvalidated
[post_id]: 492184
[parent_id]: 
[tags]: 
ML data leaking subtlety

On a 2 class classification problem (think spam detection, with some spam (positive), but a lot more non-spam (negative)), is it OK to share some negative samples between the validation and the training data set, but separate the positive samples, on the assumption that the negative population is on average the same between training and testing? The negative population essentially provides a background against which to detect the positives, and so it doesn't matter if negative samples are leaked. Is that a good or a bad idea? What this is hinting at is that, in some cases, it could be acceptable to leak differentially, depending on the class.
