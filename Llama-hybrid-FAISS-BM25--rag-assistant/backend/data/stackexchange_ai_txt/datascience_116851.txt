[site]: datascience
[post_id]: 116851
[parent_id]: 116845
[tags]: 
If you have a highly imbalanced dataset, a different model architecture or loss function may be better suited to the task. Although, if you wanna go with xgboost I think you can make your model more "liberal" with its classifications by adjusting the decision threshold for classification. By default, xgboost models use a threshold of 0.5 for binary classification, but you can adjust this threshold to be more or less conservative with your predictions. Why don't you try using a threshold of 0.4 or 0.3 , this might make the model more liberal with its classifications. You can also try oversampling the minority class in your training data to give the model more examples to learn from, in this way the model can learn to better distinguish between the minority and majority classes, and may make the model more confident in its predictions of the minority class. Use SMOTE, this might help you.
