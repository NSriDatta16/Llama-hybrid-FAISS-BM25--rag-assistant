[site]: datascience
[post_id]: 123693
[parent_id]: 123689
[tags]: 
While your professor maybe trying to drive home a different point, I think the answer is in Naive assumption in Bayes Classifier You've probably heard about Bayes Theorem in conditional probability: $$ P(Y|X) = \frac {P(X|Y) P(Y)} {P(X)} $$ Which is just: Prob of Y given X = Prob. of X given Y Multiplied by Prob. of Y whole divided by Prob. of X If you look at it from a Data Science perspective, Y is the dependent feature, you can't get Y from X directly (like future house prices given certain conditions) certainly not from your test data, but you can get X from Y using your training data (already listed House Prices with their conditions) and make your way to the target from there. The above example showed the Bayesian equation for just one independent feature, but can you imagine the equation if there are 25 such features and all of them are related?! (house quality, location, no. of rooms, living and lot measure, etc.) This would be very difficult to calculate and cumbersome as well. The permutations and combinations alone are mind boggling. Hence, we make the Naive assumption to save us from all this trouble, that the features are independent of each other. This would help us simplify our problem from an exponential to a linear one: $$ P(X_1, X_2, X_3 ... X_n | Y) = P(X_1|Y) P(X_2|Y) P(X_3|Y)... P(X_n |Y)$$ If you look closely, these are the advantages of using it: It handles high dimensionality of data very well. Increasing no. of features does not hinder our analysis. For a fairly small amount of data it performs better than more complex algorithms. Bottom line: It's not ideal for regression problems or to calculate absolute probabilities, what it does best is perform great for a small amount of data with surprisingly good results. Hence, in no way is the statement correct that: "there's no reason to ever use Naive Bayes".
