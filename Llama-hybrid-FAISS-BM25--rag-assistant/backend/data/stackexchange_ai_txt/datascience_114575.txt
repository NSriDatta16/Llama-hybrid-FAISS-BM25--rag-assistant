[site]: datascience
[post_id]: 114575
[parent_id]: 114560
[tags]: 
If you mean preprocessing methods involving BoxCox/YJ transforms, they do reduce the outlier effects and thus can (potentially) reduce bias. They do much more than that however (like distorting linear relationships) so whether it's actually helpful depends on many factors. In practice, if you use gridsearch-like routines for cross-validation of your models, you can search for the optimal preprocessing function in a pipeline as well. Sklearn PowerTransformer() and QuantileTransfomer() (including the uniform transformation for the latter) are able to yield better overall metrics compared to StandardScaler() / MinMaxScaler() / RobustScaler() in many cases, however there doesn't seem to be any rule of thumb here, except maybe for neural networks which tend to prefer quantile transformations.
