[site]: crossvalidated
[post_id]: 220223
[parent_id]: 175132
[tags]: 
Although asked quite a while ago, I bumped into this question and saw it had no answers. Then found this from data science stack exchange: https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers Quoting user104493: "There is no gradient with respect to non maximum values, since changing them slightly does not affect the output. Further the max is locally linear with slope 1, with respect to the input that actually achieves the max. Thus, the gradient from the next layer is passed back to only that neuron which achieved the max. All other neurons get zero gradient."
