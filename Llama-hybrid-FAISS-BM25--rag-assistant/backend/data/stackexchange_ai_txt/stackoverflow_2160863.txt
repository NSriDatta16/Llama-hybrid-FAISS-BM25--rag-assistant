[site]: stackoverflow
[post_id]: 2160863
[parent_id]: 
[tags]: 
New to Hadoop and dumbo, how to correctly sequence these operations?

Consider the following log file format: id v1 v2 v3 1 15 30 25 2 10 10 20 3 50 30 30 We are to calculate the average value frequency (AVF) for each data row on a Hadoop cluster using dumbo. AVF for a data point with m attributes is defined as: avf = (1/m)* sum (frequencies of attributes 1..m) so for the first row, avf = (1/3)*(1+2+1) ~= 1.33. An outlier is identified by a low AVF. Programming problem We have the following pseudo/python code: H = {} # stores attribute frequencies map1(_, datapoint): # for attr in datapoint.attrs: yield (attr, 1) reduce1(attr, values): H[attr] = sum(values) map2(_, datapoint): sum = 0 m = len(datapoint.attrs) for attr in datapoint.attrs: sum += H[attr] yield (1/m)*sum, datapoint reduce2(avf, datapoints): # identity reducer, only sorts datapoints on avf yield avf, datapoints Problem is, how do we plug our set of data points into both map1 and map2 , as well as use the intermediary hash H in map2. Having H globally defined as above seems like going against the MapReduce concept.
