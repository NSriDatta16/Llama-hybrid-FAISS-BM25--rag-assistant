[site]: stackoverflow
[post_id]: 3946592
[parent_id]: 3944452
[tags]: 
You are describing a regression problem, this is a classic machine learning problem. There are thousands of scientific papers and entire textbooks written about only this topic. I would suggest taking a look at some machine learning classes online or check out a standard machine learning text . The general approach is similar to what you have mentioned, solving for linear coefficients on the variables to minimize some loss usually sum of squared errors (L2 Loss). This is desirable because it is a convex function and therefore contains a single minimum and the weights can be solved for in polynomial time. However, like you mentioned the true function may not lie in this function class and you will have a poor estimate. The approach in this case is not to try some sort of non-convex optimization with some obscure particle swarm methods or genetic algorithms or whatever other global optimization technique. Your statement "... may be a "local optima", and having some kind of evolutionary algorithm would yield me a global one." is a naive one. Global optimization is NP-Hard, these techniques are only approximations and carry absolutely no guarantees regarding run time or optimality, and they almost never work. A much more accepted approach is to use "feature expansion" which takes your variables X, Y, ..., Z and applies a nonlinear transformation to some new set phi(X), phi(Y), ..., phi(Z) . At this point you can find an optimal linear weighting for each feature with least squares (if you use L2) or whatever. How to find good features is an open problem in machine learning but there are boatloads of ideas and freely available algorithms to do this.
