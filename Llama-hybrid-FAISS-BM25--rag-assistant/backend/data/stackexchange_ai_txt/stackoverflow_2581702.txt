[site]: stackoverflow
[post_id]: 2581702
[parent_id]: 2561999
[tags]: 
The primary weakness with the system as you described it is that you are "given" the page content, why not go and get the page content for yourself? A Web publisher creates a new page on their site that includes a script from your server. When a visitor reaches that new page, that script sends a get request to your server. Your server goes and gets the content of the page (possibly by using the referrer header to determine the source of the request). Your server processes the text content and returns a response (via JSONP) that includes an HTML fragment listing links to related content around the Web. This response is cached and served to subsequent visitors from a server side cache / proxy When the TTL for the cached version expires, the proxy will forward the request on to your app and the whole cycle starts again from step 3. This stops malicious content from being "fed" to your server and allows you to provide some form of API key that ties requests and domains or pages together ( i.e. api key 123 only works for referrers on mydomain.com - anything else is obviously spoofed ). Due to the caching / proxy your app is protected to some degree from any form of DOS type attack as well because the page content is only processed once every time the cache TTL expires ( and now you can handle increasing loads by extending the TTL until you can bring additional processing capability on). Now your client side script is insanely small and simple - no more scraping content and posting it - just send an ajax request and maybe populate a couple of parameters ( api key / page ).
