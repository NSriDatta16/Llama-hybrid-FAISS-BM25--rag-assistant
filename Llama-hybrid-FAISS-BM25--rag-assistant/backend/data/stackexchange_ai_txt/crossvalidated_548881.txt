[site]: crossvalidated
[post_id]: 548881
[parent_id]: 548739
[tags]: 
The main problem is that you don't have enough cases to accomplish what you seek. If the outcome variable is continuous so that you might have used ordinary least squares in the $n>p$ situation, you generally need about 15 cases per predictor that you are evaluating to get reliable results that might apply to another data sample from the same population. If you already have two predictors serving as "control variables" you would already be pushing the limit even if you had $n>p$ with $n=16$ . Yes, you can in principle use the penalization provided by either ridge or LASSO to deal with the $p>n$ problem. But you should be very careful in trying to evaluate "statistical significance" of any coefficient estimates you get. Chapter 6 of Statistical Learning with Sparsity goes into detail about the issues with LASSO. You can get significance estimates for a fixed choice of penalty parameter , but it's not clear to me how you take into account use of the data to choose that penalty. My answer here , which you cite as supporting significance testing for ridge and LASSO, did suggest a particular form of bootstrapping for ridge (not LASSO) as a possibility, but mostly argued against trying to evaluate anything other than predictive ability. With so few cases your best bet would be ridge, which will include all the predictors and thus avoid the variable-selection problem with LASSO, which is likely to be unstable among data samples in terms of the particular predictors returned. You should repeat the modeling with multiple bootstrap samples to show yourself how variable the results can be, depending on the data sample. But with so few cases you shouldn't put much faith in the results of any "significance" test, and instead use these data as a pilot study to inform further work. For further guidance, read Frank Harrell's course notes , with particular attention to Chapter 4 on Multivariable Modeling Strategies.
