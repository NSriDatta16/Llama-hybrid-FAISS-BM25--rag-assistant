[site]: crossvalidated
[post_id]: 126848
[parent_id]: 110687
[tags]: 
Nonparametric maximum likelihood estimates exist only if you impose special constraints on the class of allowed densities. Suppose that you have a random sample $x_1,\dots,x_n$ from some density $f$ with respect to Lebesgue measure. In the nonparametric setting, the likelihood is a functional which for each density $f$ outputs a real number $$ L_x[f] = \prod_{i=1}^n f(x_i) \, . $$ If you are allowed to choose any density $f$ , then for $\epsilon>0$ you can pick $$ f_\epsilon(t) = \frac{1}{n}\sum_{i=1}^n \frac{e^{-(t-x_i)^2/2\epsilon^2}}{\sqrt{2\pi}\epsilon} \,. $$ But then, because $$ L_x[f_\epsilon] \geq \frac{1}{\left(n\sqrt{2\pi}\epsilon\right)^n} \, , $$ making $\epsilon$ small you can make $L_x[f_\epsilon]$ grow unboundedly. Hence, there is no density $f$ which is the maximum likelihood estimate. Grenander proposed the method of sieves, in which we make the class of allowed densities grow with the sample size, as a remedy to this aspect of nonparametric maximum likelihood. Exagerating a little bit, we may say that this property of nonparametric maximum likelihood is "the mother of all overfitting" in Machine Learning, but I digress.
