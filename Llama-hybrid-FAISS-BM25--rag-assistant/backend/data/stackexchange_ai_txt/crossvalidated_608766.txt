[site]: crossvalidated
[post_id]: 608766
[parent_id]: 
[tags]: 
In getting to the standard deviation, why do we square d.f.?

this question is similar to another on this platform, but that one adresses the general question of rooting the variance, while this question is about rooting d.f. - furthermore, that question has an elaborate answer which is too complex for me to interpret.. So, generally to get the std.dev., the square root of the variance is calculated. In doing so, this includes taking the root of the degrees of freedom (n-1). This seems very strange to me. Maybe I'm just not thinking clearly about this. So, std.def = √(variation/d.f.) I'd think this ought to be: Std.dev = √(variation) / d.f. Why is this not the case? Why do we root the degrees of freedom to get to the std.dev (which, afaik, should reflect the average deviation from the mean). Edit with a bit more info: When calculating any average, we take the sum total of all values, and divide by the amount of values. Now, for std.dev. commonly d.f. is taken instead of n, which is also useful for my usecase. So, when calculating the average distance from the mean, I'd expect to first get the total distance from the mean, and then divide that by d.f. to get the average. As a workaround for the problem with negative/positive values in calculating a total distance to the mean, the total distance would be calculated as the variation (I.e. Sum of Squares). But then, to go back to the same unit of measurement that we started with the root of R^2 should be taken. Lastly, we divide this total distance from the mean by d.f. If we follow the existing formula for std.dev. then we use std.dev=√(variation)/√(d.f.). I don't get why we also take the root of d.f. here. Compared to a basic calculation of a mean, I'd compare the existing std.dev. formula to: √(Sum of values^2)/√(number of values), which seems very strange to me, and I can't really wrap my head around why this is done.
