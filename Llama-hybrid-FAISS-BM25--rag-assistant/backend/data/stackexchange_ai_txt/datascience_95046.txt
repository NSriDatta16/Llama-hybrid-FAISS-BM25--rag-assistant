[site]: datascience
[post_id]: 95046
[parent_id]: 37596
[tags]: 
Using pretrained model as a starting point would give better results even classes you want to classify don't present in the original dataset. Because first layers of the CNN models just learn primitive features like lines, circles and they are relevant for other image classification tasks and classes that not present in the imagenet dataset too, so I don't think answers that stated if the class is not present then there is no transfer learning is true. Low level representations are just same for almost every object. It is the reason why transfer learning in image related tasks are successful. But of course you should adapt pretrained network to your task by replacing original classification layer with a fully connected layer to classify your classes, a binary classifier in that case, and you should train layers close to end to extract high level representations from your dataset. As I said before low level representations already learned by first layers and they are okay, so make weights of last a few layers and fully connected layer trainable and just freeze anything else. It is called fine-tuning. Model you already have also might have been trained in this way, so you should compare results of both to have an idea which one is better.
