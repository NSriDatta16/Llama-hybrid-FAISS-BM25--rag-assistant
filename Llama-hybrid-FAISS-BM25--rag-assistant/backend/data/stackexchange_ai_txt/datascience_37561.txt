[site]: datascience
[post_id]: 37561
[parent_id]: 
[tags]: 
Efficient dimensionality reduction for large dataset

I have a dataset with ~1M rows and ~500K sparse features. I want to reduce the dimensionality to somewhere in the order of 1K-5K dense features. sklearn.decomposition.PCA doesn't work on sparse data, and I've tried using sklearn.decomposition.TruncatedSVD but get a memory error pretty quickly. What are my options for efficient dimensionality reduction on this scale?
