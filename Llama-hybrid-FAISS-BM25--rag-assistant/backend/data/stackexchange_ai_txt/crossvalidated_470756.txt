[site]: crossvalidated
[post_id]: 470756
[parent_id]: 
[tags]: 
Approximate ratio of no. features to no. samples to force a feature selection

I know that feature selection is adopted by many machine learning scientists with the hope of reducing model complexity, avoiding curse of dimensionality, avoiding overfitting lowering training times etc. However, we need to find an exact answer to the question, is feature selection absolutely necessary? If yes, when? , just like the ones asked here and here . It was suggested that feature selection may be considered as a stage to avoid because it eliminates data and the problem is NP-complete. Anyhow, feature selection is indeed useful if we have thousands of features and a few tens to hundreds of data samples (data points). I specifically like to know what would the approximate ratio of no. of features and no. of samples be, for a binary classification, that forces a feature selection process. In my specific binary classification problem, I have 80 features and 500 data samples. Without any feature selection, I get 94% 10-fold CV accuracy with 2% std. I have run CV for many iterations with different random data selections as well and the accuracy still lies around 94% . In this case do I need to perform feature selection? I would greatly appreciate if somebody could provide a relevant publication too, as this is for an academic study.
