[site]: crossvalidated
[post_id]: 284966
[parent_id]: 
[tags]: 
Visualizing model trajectories for Neural Networks using function approximator

Erhan et al. in their 2010 paper discusses how pre-training improves deep networks: http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf#page=15 In there, they compare different neural network models by visualizing the function representation for each network. For a given model, Function representation is defined as a vector of outputs for a given set of inputs (the link above points directly to the description of function representation). Inline below is a figure comparing model trajectories: Question: Since, function representation is defined as a vector of output values for a given set of inputs, I assume there exists a vector $V_{c}$ with all the correct outputs. Now ideally, both the model trajectories (with pre-training and without pre-training) should try to converge towards this correct vector $V_{c}$ with progression in training iterations. However, if we look at the figure above, the model trajectories seem to diverge, instead of converging towards a correct vector $V_{c}$. Why are the trajectories diverging instead of converging? PS: Here is a link to a video that describes this figure: https://youtu.be/MJs9JHr8C-s?t=177
