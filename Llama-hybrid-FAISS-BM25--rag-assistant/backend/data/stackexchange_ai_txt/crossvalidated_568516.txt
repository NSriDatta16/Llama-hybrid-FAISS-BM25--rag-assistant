[site]: crossvalidated
[post_id]: 568516
[parent_id]: 
[tags]: 
What is the best way to organize the classes if we're training multiple classification models?

Let's say we have 6 classes to predict. And we want to train 2 Multinomial Logistic Regression models (one model for 3 classes and another for the other 3) instead of training one global Multinomial Logistic Regression model to predict all 6 at once. My two questions are: Is it actually beneficial to train multiple models instead of just one global one? (knowing that in my actual case I have hundreds, sometimes thousands, of classes which is why intuitively it seems to me that training one global LR model would give poor results) . If so, what is the best way to organize the classes: put the most similar classes in each of the 2 groups (the hypothesis being that having a "specialized" model for the most similar and thus hard to distinguish inputs would help the model better tease out the differences) , or put the most dissimilar classes in each of the groups (the alternative hypothesis being that it would make it easier for each of the "specialized" models to distinguish between their respective classes). More context: To give more details about my actual situation: it's an intent classification task based on text data. I have a chatbot that can have up to 1000 or more intents to classify. Instead of having one big bot that holds all the data (which is utterances data that is vectorized with TF-IDF) , I have multiple specialized bots (one for small-talk, one for greetings, etc.) in order to improve comprehension. Which is why I'm wondering if this approach is the good one in the first place (or if there is better) , and if so then would that stratification approach described in bulletpoint 2 yield any improvement. Thanks in advance.
