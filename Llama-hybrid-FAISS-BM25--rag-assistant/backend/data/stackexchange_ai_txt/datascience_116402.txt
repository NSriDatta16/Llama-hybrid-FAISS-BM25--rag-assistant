[site]: datascience
[post_id]: 116402
[parent_id]: 
[tags]: 
Embedding performing unexpectedly worse than multi-hot encoding

I'm pretty new to ML and have set up a Keras model with a number of categorical features of users & items to predict positive reactions. Each feature is a list of IDs from a database - i.e. [1, 5, 18] - related to either a user or item. Initially I built a model that turns these lists into multi-hot encodings (using the IntegerLookup layer) before concatenating them all. This seems to work fairly well. I've since learned about embedding and have converted the model to use a Keras Embedding layer. My hope would be that it should improve performance as it'll learn which options are similar to each other, but I find the following: If I use GlobalMaxPooling1D to flatten the embeddings I end up with a huge reduction in parameter numbers (from 1.4 million to 80k) but performance of the model is significantly worse. I tried using GlobalAveragePooling1D but this doesn't work. (I get a loss of NaN, I think because some of the features in my validation set are empty lists?) If I use a Flatten layer then the model performs slightly better with embeddings but the number of parameters is very large - 4 million - and I have to make sure all the feature lists are the same length. Is this to be expected, or am I not using embeddings correctly? I'd hoped to get better predictions with fewer parameters but it is seemingly not possible. The topography of my network is as follows feature 1 (shape (128,)) feature 2 (shape (128,)) | | embedding 1 (mask_zeros=True) embedding 2 (mask_zeros=True) \ / \______________concatenate_____________/ | GlobalMaxPooling1D or Flatten | Dense(256, activation=relu) | Dense(1, activation=sigmoid)
