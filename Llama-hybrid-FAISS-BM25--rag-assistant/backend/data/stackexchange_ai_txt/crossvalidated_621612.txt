[site]: crossvalidated
[post_id]: 621612
[parent_id]: 621597
[tags]: 
As you mention, you can always round the output of your floating point PCA/SVD to 0/1, and that may work well enough for your purpose. But, there are other options which are tuned to the binary case and may be able to do more with fewer components (if your data has rank higher than the rank you truncate the principal subspace to). First, since this is stats overflow, there are probabilistic options. Standard principal components analysis is derived under the assumption that the data has a multivariate normal distribution. In other words, the answer to "which low-dimensional subspace contains the most variance?" has a nice answer in the Gaussian case. Still, for non-Gaussian data (like binary data, typically modeled with the Bernoulli distribution), similar questions can be asked and answered, and these may be more appropriate for your context. For instance, you could consider looking into logistic PCA -- its relation to PCA is something like the relationship between logistic regression and the usual (Gaussian) linear regression. A. Landgraf's R package of this name may be of interest, and that website points to some nice related literature. As in logistic regression, there are lots of related models which could be of interest. For instance, although Poisson NMF models nonnegative integer data, it is also sometimes used on binary count data (although its reconstructions may not be binary). And second, there is lots of literature about low-rank Boolean matrix factorizations -- this webpage has many references and a tutorial. Googling this term and your programming language of choice should turn up useful implementations of standard algorithms. For instance, the Nimfa package in Python includes some.
