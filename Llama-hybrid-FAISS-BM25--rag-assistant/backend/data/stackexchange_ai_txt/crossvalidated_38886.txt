[site]: crossvalidated
[post_id]: 38886
[parent_id]: 
[tags]: 
Properties of conditional probability distributions

This is a problem from a machine learning pset that I'm self-learning from http://www.seas.harvard.edu/courses/cs281/assignment-1.pdf . Suppose we are provided with a hierarchy of three distributions: $p(\alpha)$, $p(\theta | \alpha)$ and $p(y | \theta, \alpha)$. Write expressions to compute the following related distributions: $p(y|\theta)$, $p(y|\alpha)$, $p(y)$, $p(\theta|y,\alpha)$, $p(\alpha|y,\theta)$, $p(\theta)$, $p(\theta|y)$, $p(\alpha|y)$ I see that I integrate over the joint distribution to get the marginal probability $$p(y|\theta) = \int p(y|\theta,\alpha)p(\alpha)d\alpha$$ This gives me the first expression in terms of the givens. However, I don't know how to get the others. I know I can set up various equations using Bayes' Theorem $$p(\theta|\alpha) = p(\alpha|\theta) * p(\theta)/p(\alpha)$$ But the above equation has two unknowns ($p(\theta)$ and $p(\alpha|\theta)$). Is there another property that I can exploit besides marginalizing the joint distribution and utilizing Bayes' Theorem?
