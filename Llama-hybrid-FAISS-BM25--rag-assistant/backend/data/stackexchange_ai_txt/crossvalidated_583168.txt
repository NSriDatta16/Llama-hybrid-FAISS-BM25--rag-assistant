[site]: crossvalidated
[post_id]: 583168
[parent_id]: 
[tags]: 
Is it possible to express a decision tree as a kernel machine?

This paper argues that models trained with gradient descent like neural networks can be expressed as kernel machines with an interesting kernel function. The kernel is $$ K(x, x') = \int_{c(t)} \nabla_w y(x) \cdot \nabla_w y(x') \, dt$$ where $c \colon [0, 1) \rightarrow \mathbb{R}^n$ is the path taken by the weight vector $w \in \mathbb{R}^n$ during gradient descent, and $y \colon \mathbb{R}^n \rightarrow \mathbb{R}$ denotes the network output. Is there something similar for regression (decision) trees or gradient-boosted ensembles of regression trees? Can we write a decision tree $f$ as being a kernel regression like $$f(x) = \sum_{i} a_i K(x, x_i) + b$$ for training points $\{x_i\}$ , some kernel function $K$ and learned weights $\{a_i\}, b$ ?.
