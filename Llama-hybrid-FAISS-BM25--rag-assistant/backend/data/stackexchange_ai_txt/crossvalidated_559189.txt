[site]: crossvalidated
[post_id]: 559189
[parent_id]: 
[tags]: 
Positive and negative signal paths in the deep learning NN

I'm new to the deep learning field and I have a question regarding DLNN. It appears that the general approach is to only use positive values and a single (positive) threshold value for the activation function. Why? Why do neurons only increase influence and never decrease it? For the sake of symmetry shouldn't they be able to do it as well? I could imagine a "negative" network matching the normal one exactly in terms of neurons (kind of the second page) with every "negative" neuron influencing the same path as the "positive" one does, but in a negative way, reducing the resulting value.
