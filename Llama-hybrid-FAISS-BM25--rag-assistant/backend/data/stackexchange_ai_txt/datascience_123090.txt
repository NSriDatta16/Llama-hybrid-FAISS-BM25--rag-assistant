[site]: datascience
[post_id]: 123090
[parent_id]: 123068
[tags]: 
In a typical RL setting, which is model-free RL, you won't have step 1 and 2. Step 1 would be the definition of your environment (e.g. by means of gym ) and reward function , which together define the task you want to solve. In RL you usually train and test (and even validate) on the same environment, although you want to change the random seed to experience different episodes. So both step 2 and 5 are a bit ill defined. What you can do during training is to periodically evaluate on your test env, and so determine the best model. Then, after the training is complete, you evaluate again on the same test env to get the final performance. Also it is a good practice to perform $N$ evaluation episodes (not just 1), and then average the metrics. For doing so there are off-the-shelf RL libraries, like sb3 , tensorforce , keras-rl , torchRL , tonic , clearnRL , and so on (there are a lot.) Moreover, if you want to do model-based or Offline/batch RL you would also work on some dataset (either previously collected, or refreshed from time to time.) If so, within the RL loop you'll have a supervised phase which looks like the pipeline you have just described, but overall is still different.
