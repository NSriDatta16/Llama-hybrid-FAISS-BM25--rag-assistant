[site]: crossvalidated
[post_id]: 208267
[parent_id]: 202544
[tags]: 
Option 1 (adding an unknown word token) is how most people solve this problem. Option 2 (deleting the unknown words) is a bad idea because it transforms the sentence in a way that is not consistent with how the LSTM was trained. Another option that has recently been developed is to create a word embedding on-the-fly for each word using a convolutional neural network or a separate LSTM that processes the characters of each word one at a time. Using this technique your model will never encounter a word that it can't create an embedding for.
