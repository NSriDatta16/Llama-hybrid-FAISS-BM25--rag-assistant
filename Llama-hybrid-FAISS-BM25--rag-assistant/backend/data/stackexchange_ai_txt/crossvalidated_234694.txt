[site]: crossvalidated
[post_id]: 234694
[parent_id]: 
[tags]: 
Can all neural network with DAG topology be trained by Back-prop?

Can all neural network having directed acyclic graph (DAG) topology be trained by back propagation methods? You can assume that the activation functions of all neurons are differentiable. I mean by the gradient based methods like Stochastic gradient decent, AdaGrad,Adam, etc. If it's true, is there a reference (academic paper is the best) providing the proof of it?
