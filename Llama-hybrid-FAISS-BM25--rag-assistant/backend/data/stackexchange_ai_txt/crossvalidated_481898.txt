[site]: crossvalidated
[post_id]: 481898
[parent_id]: 481891
[tags]: 
Doing it right the first time is best. First, in practice, this should be an unlikely situation. Maybe you have re-engineered a pharmaceutical process hoping that the new process has a higher yield than the current one with $\mu_0 = 100,$ so you'd take data from runs of the new process, average them and test $H_0: \mu= 100$ vs. $H_a: \mu > 100.$ Maybe your town has changed the widths of lanes and the sequencing of traffic lights on its main road hoping that the average late afternoon travel average travel time on the main stretch is reduced from the former $\mu_0 = 20$ min. Then you'd get travel times under the new configuration to test $H_0: \mu = 20$ vs $H_a: \mu Maybe your old supplier whose product had 200mg of active ingredient per bottle has gone out of business and you are checking to see the amount of active ingredient in a former competitive supplier is the same as for the old supplier. Then you'd test $H_0: \mu = 200$ vs $H_a: \mu \ne 200,$ based on the average of $n$ randomly selected bottles from the prospective new supplier. So ordinarily you would test one of the three kinds of tests and act according to the results of the test. One hopes you would have done a 'power and sample size' computation beforehand so you'd take a large enough sample $n$ in order to have a good chance (say 90%) of rejecting if there is a meaningful difference from $\mu_0.$ Then you would likely take the result of the one test as sufficiently good evidence to act upon. But best-laid plans don't always work out. However, as a direct answer to your question, let's suppose you have taken data to test $H_0: \mu = 100$ vs. $H_1: \mu at the 5% level, and cannot reject. Here are data simulated in R that would give such a result. set.seed(806) x = rnorm(10, 98, 15) t.test(x, mu=100, alt="less") One Sample t-test data: x t = -0.69053, df = 9, p-value = 0.2536 alternative hypothesis: true mean is less than 100 95 percent confidence interval: -Inf 104.6308 sample estimates: mean of x 97.20135 The mean of my $n=10$ observations is $\bar X = 97.2,$ which is below the hypothetical mean $\mu = 100,$ but not enough smaller to be considered statistically significant. Maybe we put the wrong assumptions into our power computation so we didn't use a large enough $n.$ In this case, there's no use testing $H_0: \mu = 100$ vs. $H_1: \mu > 100$ because $\bar X could never lead to rejection. But what do we do if we guessed completely wrong and got data such as those in the simulation below? set.seed(806) x = rnorm(10, 110, 15) t.test(x, mu=100, alt="less")\ One Sample t-test data: x t = 2.2703, df = 9, p-value = 0.9753 alternative hypothesis: true mean is less than 100 95 percent confidence interval: -Inf 116.6308 sample estimates: mean of x 109.2014 Of course, we can't reject in favor of $H_a: \mu based on a sample mean $\bar X = 109.2.$ Then we might be tempted to try testing $H_0: \mu = 100$ vs. $H_1: \mu > 100.$ [In R, the notation p.val gives just the P-value of the test, not the full printout.] t.test(x, mu=100, alt="gr")$p.val [1] 0.02466914 So we could have rejected a test of $H_0: \mu = 100$ vs. $H_1: \mu > 100$ at the 5% level because the P-value $0.025 Doing multiple tests on the same data is always dangerous. If we try enough different things, we might accidentally get a rejection on one of our tries--just by chance. (The result would be a 'false discovery'.) Rejecting at the 2% level isn't a really strong result, but if is really important to resolve the true value of $\mu,$ then we might consider getting fresh data and doing the right test the second time. Or maybe making a two-sided 95% confidence interval to get a good guess at the actual value of $\mu$ in order to plan our course of action.
