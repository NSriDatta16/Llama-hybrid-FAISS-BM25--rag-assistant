[site]: crossvalidated
[post_id]: 220190
[parent_id]: 197348
[tags]: 
I think the approach you've outlined sounds reasonable, but I suspect it's probably not what they're doing, given that they're using gradient boosting. They're probably extracting normalized feature importances from the gradient boosted trees, like this in this random forest example from scikit-learn ( http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html ). Trees are commonly used to identify the most useful/important features. Feature importance is calculated by looking at how much error each variable reduces when it is used in a tree split. For each variable, these reductions are summed over all splits where the variable is used, and then the summed error reductions are normalized by dividing by the sum of the variable with the largest total error reduction. Trees' variable importances don't indicate direction (good or bad), just relative importance, but that's likely sufficient for Netflix's purposes since it's unlikely that any touch point would have a negative effect.
