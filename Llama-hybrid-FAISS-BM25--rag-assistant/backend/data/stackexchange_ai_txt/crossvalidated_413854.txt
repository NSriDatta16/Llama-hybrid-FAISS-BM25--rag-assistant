[site]: crossvalidated
[post_id]: 413854
[parent_id]: 413853
[tags]: 
Without seeing your code is hard to answer. That said, base-learners in a random forest (i.e. the trees) usually are not confined to a small tree depth. The most common setting is to confine the tree depth by the minimum number of samples per tree or just prune the tree based on impurity decreases (e.g. like the RPART doc suggests). On the contrary, gradient boosting machine methodologies usually , actively discourage very deep trees to avoid over-fitting (e.g. Catboost doc suggest that the optimal depth range are from 4 to 10). As such, we might have the same number of trees/base-learner in both methods but the random forest will probably have "chunkier" base-learners leading to an overall larger model size.
