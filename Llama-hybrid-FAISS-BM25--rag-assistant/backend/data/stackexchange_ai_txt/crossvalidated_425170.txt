[site]: crossvalidated
[post_id]: 425170
[parent_id]: 
[tags]: 
Maximum likelihood: Bernoulli

I would really appreciate if anyone you can explain how it went from step 1 to the answer provided below. This is from the book Doing Data Science by Cathy O'Neil and Rachel Schutt pages 101 to 102. I have provided my answers with the steps I took to get there. $p(x|c) = \Pi_j\theta_{jc}^{x_j}(1-\theta_{jc})^{(1-x_j)}$ Take ln both sides: $ln(p(x|c)) = ln(\Pi_j\theta_{jc}^{x_j}(1-\theta_{jc})^{(1-x_j)})$ Known: $\Pi_j \theta_{jc}^{x_j} = \theta_{jc}^{(\sum_{j} x)}$ $\Pi_j(1-\theta_{jc})^{(1-x_j)} = (1-\theta_{jc})^{n - \sum_j x}$ Substitute the known into the equations: $ln(p(x|c)) = ln(\theta_{jc}^{(\sum_{j} x)} * (1-\theta_{jc})^{n - \sum_j x})$ Expand the muliplication: $ ln(p(x|c)) = ln(\theta_{jc}^{(\sum_{j} x)}) + ln((1-\theta_{jc})^{n - \sum_j x})$ Simply: $ln(p(x|c)) = (\sum_{j} x)ln(\theta_{jc}) + ({n - \sum_j x})ln(1-\theta_{jc})$ $ln(p(x|c)) = (\sum_{j} x)ln(\theta_{jc}) - (\sum_j x)ln(1-\theta_{jc}) + (n)ln(1-\theta_{jc})$ My answer: $ln(p(x|c)) = (\sum_{j} x)ln(\frac{\theta_{jc}}{1-\theta_{jc}}) + (n)ln(1-\theta_{jc})$ Text book answer: $ln(p(x|c)) = (\sum_j x_j) ln(\frac{\theta_j}{1-\theta_j}) + \sum_j log(1-\theta_j)$ Thanks in advance!
