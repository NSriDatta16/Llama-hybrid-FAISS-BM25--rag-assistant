[site]: crossvalidated
[post_id]: 87479
[parent_id]: 
[tags]: 
What are "coefficients of linear discriminants" in LDA?

In R , I use lda function from library MASS to do classification. As I understand LDA, input $x$ will be assigned label $y$ , which maximize $p(y|x)$ , right? But when I fit the model, in which $$x=(Lag1,Lag2)$$ $$y=Direction,$$ I don't quite understand the output from lda , Edit: to reproduce the output below, first run: library(MASS) library(ISLR) train = subset(Smarket, Year > lda.fit Call: lda(Direction ~ Lag1 + Lag2, data = train) Prior probabilities of groups: Down Up 0.491984 0.508016 Group means: Lag1 Lag2 Down 0.04279022 0.03389409 Up -0.03954635 -0.03132544 Coefficients of linear discriminants: LD1 Lag1 -0.6420190 Lag2 -0.5135293 I understand all the info in the above output but one thing, what is LD1 ? I search the web for it, is it linear discriminant score ? What is that and why do I need it? UPDATE I read several posts (such as this and this one ) and also search the web for DA, and now here is what I think about DA or LDA. It can be used to do classification, and when this is the purpose, I can use the Bayes approach, that is, compute the posterior $p(y|x)$ for each class $y_i$ , and then classify $x$ to the class with the highest posterior. By this approach, I don't need to find out the discriminants at all, right? As I read in the posts, DA or at least LDA is primarily aimed at dimensionality reduction , for $K$ classes and $D$ -dim predictor space, I can project the $D$ -dim $x$ into a new $(K-1)$ -dim feature space $z$ , that is, \begin{align*}x&=(x_1,...,x_D)\\z&=(z_1,...,z_{K-1})\\z_i&=w_i^Tx\end{align*} , $z$ can be seen as the transformed feature vector from the original $x$ , and each $w_i$ is the vector on which $x$ is projected. Am I right about the above statements? If yes, I have following questions: What is a discriminant ? Is each entry $z_i$ in vector $z$ is a discriminant? Or $w_i$ ? How to do classification using discriminants?
