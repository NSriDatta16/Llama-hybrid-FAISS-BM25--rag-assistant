[site]: crossvalidated
[post_id]: 520739
[parent_id]: 123820
[tags]: 
An estimator $\hat{\theta}$ of $\theta$ may (correctly) depend on features of the data that provide information on $\theta$ (i.e. depend on the sufficient statistic $T(X)$ ), but it may also, in general, depend on features that do not provide information on $\theta$ . The latter dependence is a source of variance that we'd like to eliminate -- there is no reason why two samples, with the same value of the sufficient statistic, should provide different estimates for $\theta$ . So a simple way to eliminate this variance is to just average the estimator over all samples with the same value of $T(X)$ . This gives rise to a new estimator $E(\hat{\theta}|T(X))$ , known as the Rao-Blackwellization. The proof of the theorem is very analogous to the intuition, in that we simply do an ANOVA of the variance in $\hat{\theta}$ : $$\mathrm{Var}(\hat{\theta})=\mathrm{E}(\mathrm{var}(\hat{\theta}|T))+\mathrm{var}(\mathrm{E}(\hat{\theta}|T))$$ The latter is the variance in the Rao-Blackwellized estimator while the former is the variance we want to eliminate. See Sufficient statistics and the Rao-Blackwell theorem for some tangential Bayesian explanation of sufficient statistics, if it is not clear why we want to eliminate this source of variance.
