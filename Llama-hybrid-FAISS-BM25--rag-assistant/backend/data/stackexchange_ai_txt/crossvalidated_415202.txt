[site]: crossvalidated
[post_id]: 415202
[parent_id]: 415199
[tags]: 
It sounds like you are using a neural network. As an aside, apart from over-sampling, you could use a SMOTE technique, and generate synthetic data. You results are curious, given that you have already balanced your data. I don't have a direct answer. First thing I would check is that you really are balancing them correctly. I would also review whether you are using the most appropriate loss function. Try alternating between different loss functions to see whether this issue manifests equally between them. However, accuracy probably isn't the best performance metric to use for imbalanced data like this. Instead I would look into using both accuracy and recall, and you'll probably end up on f1 score as a result. Also, consider AUC too (I just noticed that you are already looking at this). I often find gradient boosted trees do better on heavily imbalanced problems where I don't want to perform any major data sampling. In part, this is because you have a direct parameter which informs the model a priori how the classes are imbalanced. For example, XGB uses parameter Scale_pos_weight. This is the ratio of number of negative class to the positive class.
