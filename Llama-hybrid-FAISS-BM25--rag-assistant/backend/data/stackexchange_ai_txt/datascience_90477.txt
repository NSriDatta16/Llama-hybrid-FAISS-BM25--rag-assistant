[site]: datascience
[post_id]: 90477
[parent_id]: 
[tags]: 
Reinforcement Learning reward is not converging to zero

I am new to reinforcement learning. I am trying to build an RL algorithm which will predict cloud hardware capacity required for an org in terms of compute, storage, memory. The algorithm which i have developed uses boltzmann equation and a DQN. My reward is a simple calculation based on how much the prediction is off from utilisation threshold provided by user (For example if the user provides that he wants the compute utilisation to be 80%, my reward will be the difference between optimum value and predicted value). During training, the RL uses a random logic to predict next set of values. The challenge i am facing is that the reward is never converging to zero is certain episodes even when the score reaches to 20000. Is it normal the reward might not converge to zero in certain episodes? Also, do i need to reward mechanism during test? I think the reward mechanism is not needed for test as i would be saving my trained model and using the weights file during test.
