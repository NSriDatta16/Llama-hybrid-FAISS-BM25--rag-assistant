[site]: crossvalidated
[post_id]: 285606
[parent_id]: 285386
[tags]: 
Let's begin by simplifying the situation. By centering and scaling all data variables appropriately we may create new ones $X,Y,Z$ whose averages are zero and whose lengths, thought of as Euclidean vectors, are unity. When this is done, as is well-known, the regression models are equivalent to $$Z = \alpha Y + \varepsilon_{ZY}$$ and $$Y = \gamma X + \varepsilon_{YX}$$ and the errors $\varepsilon_{ZY}$ and $\varepsilon_{YX}$ still have zero means and are independent (within each model). The least-squares solutions are the cosines of the angles among these (unit vectors), $\hat\alpha=\cos(ZY)$ and $\hat\gamma = \cos(YX)$. These cosines are usually called the correlation coefficients. Their squares are the $R^2$ statistics for the regressions (which haven't changed as a result of the centering and scaling). Using $x$ as a proxy for $y$ is equivalent to using $X$ as a proxy for $Y$. When that occurs, the least squares estimate in the model $$Z = \beta X + \varepsilon_{ZX}$$ is $\hat\beta = \cos(ZX)$. This allows us to recast the question as one of (three-dimensional) Euclidean geometry: How well can the (cosine of the) angle $XY$ be estimated based on estimates of the cosines of the other two angles, $ZY$ and $YX$? The answer is fairly obvious from pictures: the angle $XY$ cannot be any greater than the sum of $ZY$ and $YX$ and it cannot be any less than the size of the difference of $ZY$ and $YX$. (This statement needs appropriate modification when the sum exceeds $90$ degrees: I'll leave that for the interested reader to work out.) As an extreme example, suppose both angles are $45$ degrees: thus, their cosines are each $\sqrt{1/2} \approx 0.71$ (which is often considered a largish correlation) and both $R^2$ statistics are $1/2$. The angle $XZ$ may range from $45+45=90$ degrees down to $|45-45|=0$ degrees. In the former case, $Z$ and $X$ are orthgonal and $\hat\beta=0$: there is no "linear relation" between them, so $X$ is the worst possible proxy for $Y$. In the latter case $X$ and $Z$ are perfectly correlated and $X$ is the best possible proxy for $Y$. Contemplation of a few such examples leads one to conclude that unless both $R^2$ statistics (for $Z$ against $Y$ and $Y$ against $X$) are sufficiently close to $1$, it is possible $X$ would be a poor proxy for $Y$. As an approximation, if by "good proxy" you mean the $R^2$ for $Z$ vs $X$ will be at least some (moderately large) threshold $C$, then you will want $R^2_{ZY} + R^2_{ZX} - 1$ to exceed $C$ to assure this. This figure shows $X$ situated relative to $Y$. Given the depicted angle between $Z$ and $Y$, the possible locations of $Z$ lie on the red spherical circle shown. Parts of this circle (near the right) are relatively close to $X$, but other parts (near the back left) are nearly at right angles to $X$. Depending on where $Z$ actually is on that circle, $X$ might or might not be close to ("a good proxy") for $Y$. An R implementation to simulate a dataset of any size and any valid set of correlation coefficients (as specified by angles or directly by modifying rho ) follows. It will enable you to experiment with the possibilities. library(MASS) # exports `mvrnorm` # # Specify dataset properties. # The angle ZX must lie between the extremes of XY+YX and |XY-YX| in order # for the specified dataset to be possible. # n
