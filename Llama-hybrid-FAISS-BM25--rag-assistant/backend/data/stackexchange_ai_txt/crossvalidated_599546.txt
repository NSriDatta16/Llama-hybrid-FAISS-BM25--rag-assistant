[site]: crossvalidated
[post_id]: 599546
[parent_id]: 599513
[tags]: 
A clue as to what's happening is the presence of log()s around the names of the index function coefficients for the logged variables but their absence in the AMEs table below. In a probit model, you have something like $$\mathbf{Pr}[y = 1 \vert \ln(x),z ] = \Phi(\alpha + \beta \cdot \ln(x) +\gamma \cdot z).$$ The marginal effect of $\ln x$ is $$ME_1 = \frac{\partial \mathbf{Pr}[y = 1 \vert \ln(x),z ]}{\partial \ln x}=\varphi(\alpha + \beta \cdot \ln(x) +\gamma \cdot z) \cdot \beta$$ This is constrained to be in [0,1]. The marginal effect of $x$ when you log it is $$ME_2 = \frac{\partial \mathbf{Pr}[y = 1 \vert \ln(x),z ]}{\partial x}=\varphi(\alpha + \beta \cdot \ln(x) +\gamma \cdot z) \cdot \beta \cdot \frac{1}{x}$$ since $\frac{d \ln(x)}{dx} = \frac{1}{x}$ Given the way you specified the index function, R is helpfully calculating the average of the second quantity for you (though the $ME_1$ is reasonable too, more below). It's the multiplication by $\frac{1}{x}$ that is causing the out-of-bounds issue. I would wager that $\ln(avg_libdem)$ tends to be less than one. You can interpret $AME_1$ as an semi-elasticity since $$\frac{\partial \mathbf{Pr}[y = 1 \vert \ln(x),z ]}{\partial x}=\varphi(\alpha + \beta \cdot \ln(x) +\gamma \cdot z) \cdot \beta \cdot \frac{1}{x}$$ can be rearranged as $$\frac{\partial \mathbf{Pr}[y = 1 \vert \ln(x),z ]}{100 \cdot\frac{\partial x}{x}}=\frac{1}{100} \cdot \varphi(\alpha + \beta \cdot \ln(x) +\gamma \cdot z) \cdot \beta$$ The numerator is a change in probability. The denominator is a 1% change in x. Note that without scaling by $\frac{1}{100}$ , you would have a $100\%$ change in the denominator, and since derivatives are approximations of a small change, you can get very large effects when you consider a doubling. So 1.2713 can be interpreted as the model saying that a 1% change in avg_libdem is associated with a $\frac{1.2713}{100}=.012713$ percentage point increase in the probability of escape (or 1.27 percentage points on a [0,100] scale). I like dividing since the other MEs are on the [0,1] scale to keep it apples-to-apples. Personally, I also find the $ME_1$ more intuitive, so I would trick R by logging myself or I would just not log RHS variables (unless there was some good reason to do so, like being consistent with previous literature). Here's some R code showing this calculation on another dataset: > library(foreign) > library(margins) > auto auto $log.price price) > m1 (margins(m1)) Average marginal effects glm(formula = foreign ~ log.price + mpg, family = binomial(link = "probit"), data = auto) log.price mpg 0.4009 0.04127 > (AME_price_m1 $coefficients['log.price']*1/auto$ price)) [1] 7.310459e-05 > m2 (margins(m2)) Average marginal effects glm(formula = foreign ~ log(price) + mpg, family = binomial(link = "probit"), data = auto) price mpg 7.31e-05 0.04127 Here 0.0000731 means that a $1 increase in price is associated with an infinitesimal change in the probability of foreign manufacture, on a [0,1] scale. And 0.4009 means that $1\%$ increase in price is associated with an $\frac{0.4009}{100}=0.004$ increase in the probability of being foreign on a [0,1] scale. Or a doubling of price is associated with 40 percentage point increase on a [0,100] scale.
