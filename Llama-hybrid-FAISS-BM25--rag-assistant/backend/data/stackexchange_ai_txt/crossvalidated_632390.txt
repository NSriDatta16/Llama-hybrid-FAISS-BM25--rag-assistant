[site]: crossvalidated
[post_id]: 632390
[parent_id]: 630918
[tags]: 
I've had yet further thoughts about this which again don't really fit into any of the previous answers I have posted, so here goes... I began thinking about this problem at a much more basic level. To make any further conclusions about the LL values and distributions observed, we should first be able to make some conclusions from an ideal situation That would be: A set of data drawn from a known distribution with no outliers If we had such an idealized situation perhaps we could make some conclusions by comparing a LL distribution in the ideal world compared to a distribution we actually observed in some real world experiment To make it concrete, let's do some numerical experiment with Gaussians. We want to know something like if we draw a certain number of samples from a Gaussian and fit a Gaussian model to it using the method of Maximum Likelihood, then what is the distribution of log-likelihood values obtained? Then further - can we say anything about these values from theory? Let's begin with two figures. The underlying model is a Gaussian with $\mu=0, \sigma=1$ We draw $n$ samples from this distribution Each histogram color is for a different value of $n$ The value of $n$ used are 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 for the first figure, and 2, 3, 4, 5, 6, 7, 8, 9, 10 for the second figure. The value 1 is special, we shall say more about it later For each experimental run, we have a chosen value of $n$ . We draw $n$ samples and then obtain a measurement of the log-likelihood value via the method of Maximum Likelihood This process is repeated $10000$ times. That is to say, each histogram contains $10000$ entries A few conclusions: From the first figure, it appears that the expected LL value is approximatly a linear function of the number of samples drawn ( $n$ ) From the second figure we note it is possible to obtain LL values greater than zero, but that these are highly improbable even for relatively small values of $n$ Let's perform a linear fit between $n$ and the LL value. The residuals for that look like this: We can see it is not quite linear The errorbars drawn are obtained from the standard deviation of each LL distribution Despite the large error, the variance between each of the points is low and we can clearly conclude a non-linear model is required to obtain a good fit to the data I can't say anything further about that at this point. Theoretical Considerations The special value $n=1$ might guide us towards something. We note that for $n=1$ we do not have any sensible value for the Gaussian parameter $\sigma$ . However, we know that the likelihood is maximized by choosing an arbitrary value for $\sigma$ and setting $\mu$ equal to the single value drawn from the distribution. Given that our model has the form of a Gaussian $$p(x_i;\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2\right)$$ we can say that the Maximum Likelihood is obtained by setting $x_1=\mu$ , and therefore the likelihood value obtained is $$\frac{1}{\sqrt{2\pi\sigma^2}}.$$ Note that this is not the log-likelihood, but likelihood, which is being considered here. We can convert between them. From this we might conclude that on average we will obtain values with some variance and that the size of this variance might tell us something about how spread out the datapoints are and therefore what the expected value of our likelihood function might be for larger values of $n$ . I attempted to calculate it for $n=2$ , but I need more time to check my results on this. I think the calculation is fairly simple to do.
