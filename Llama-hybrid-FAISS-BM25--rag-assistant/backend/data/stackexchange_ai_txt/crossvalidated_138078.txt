[site]: crossvalidated
[post_id]: 138078
[parent_id]: 
[tags]: 
implementating the bayesian linear prediction with NIG prior

In Bayesian linear regression when the covariance of weights is unknown; one can set Normal-Inverse-Gamma prior. Based on "Machine Learning: a Probabilistic Perspective", Page 235, \begin{equation} \begin{aligned} p(\mathbf{w},\sigma^2|D) &= \mathrm{NIG}(\mathbf{w}, \sigma^2 | \mathbf{w}_N, \mathbf{V}_N, a_N, b_N) \\ \mathbf{w}_N &= \mathbf{V}_N( \mathbf{V}_0^{-1}\mathbf{w}_0 + \mathbf{X}^{\mathrm{T}}\mathbf{y}) \\ \mathbf{V}_N &= (\mathbf{V}_0^{-1} + \mathbf{X}^{\mathrm{T}}\mathbf{X} )^{-1}\\ a_N &= a_0 + n/2 \\ b_N &= b_0 + \frac{1}{2}(\mathbf{w}^{\mathrm{T}}_0\mathbf{V}^{-1}_0\mathbf{w}_0 + \mathbf{y}^{\mathrm{T}}\mathbf{y} - \mathbf{w}^{\mathrm{T}}_N \mathbf{V}_N^{-1}\mathbf{w}_{N} ) \\ \end{aligned} \end{equation} And drives the posterior with this prior and likelihood, we should be able to derive the following posterior as following, \begin{equation} \begin{aligned} p(\mathbf{w},\sigma^2|D) &= \mathrm{NIG}(\mathbf{w}, \sigma^2 | \mathbf{w}_N, \mathbf{V}_N, a_N, b_N) \\ \mathbf{w}_N &= \mathbf{V}_N( \mathbf{V}_0^{-1}\mathbf{w}_0 + \mathbf{X}^{\mathrm{T}}\mathbf{y}) \\ \mathbf{V}_N &= (\mathbf{V}_0^{-1} + \mathbf{X}^{\mathrm{T}}\mathbf{X} )^{-1}\\ a_N &= a_0 + n/2 \\ b_N &= b_0 + \frac{1}{2}(\mathbf{w}^{\mathrm{T}}_0\mathbf{V}^{-1}_0\mathbf{w}_0 + \mathbf{y}^{\mathrm{T}}\mathbf{y} - \mathbf{w}^{\mathrm{T}}_N \mathbf{V}_N^{-1}\mathbf{w}_{N} ) \\ &=\cdots \\ p(\sigma^2|D) &= \mathrm{IG}(a_N, b_N) \\ p(\mathbf{w}|D) &= \tau(\mathbf{w}_N, \frac{b_N}{a_N}\mathbf{V}_N, 2a_N) \end{aligned} \end{equation} When I look at the implementation here ; I get really confused ! It is not a 1-1 match with the book. I tried to implement the formula in matlab by myself which you can find it here. clear, clc x = randn(20, 3); w_true = [2,5,1]'; y = x * w_true + rand(20, 1); V0 = zeros(3); w0 = [0,0,0]'; a0 = 0.001; b0 = 4; for cnt = 1:100 vN = inv(inv(V0) + x'*x ) ; wN = vN * (inv(V0)*w0 + x'*y); aN = a0 + cnt/2; bN = b0 + 0.5 * (w0'*inv(V0)*w0 + y'*y - wN'*inv(vN)*wN); V0 = vN; w0 = wN; a0 = aN; b0 = bN; end sig=1./gamrnd(a0,1./b0) %inverse gamma However, $b_n$ tends to get very large and basically, the code does not meet the formula. Also I am not sure about the t-student distribution in Matlab. I appreciate if you make some comments on the implementation of this model, either by helping me to improve my code, or explaining on the author's code
