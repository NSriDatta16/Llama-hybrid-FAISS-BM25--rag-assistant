[site]: crossvalidated
[post_id]: 82502
[parent_id]: 71727
[tags]: 
To answer the above two questions: 1) In layman terms, Kullback-Leibler divergence, as displayed in R when using the FSelector package, is the relative amount of information that can be gained by using a given potential predictor variable. 2) It is NOT a valid approach to select a desired number of variables for a binary logistic regression model (e.g., selecting the 8 highest "ranked" variables for use in a parsimonious model) as no such approach exists. Most importantly, it does not take into account collinearity among predictor variables, as it simply rank orders variables by information gain. Information gain is very similar to the c -statistic or information value in terms of being a univariate measurement for classification strength for a given predictor variable. As such, it is useful to rank variables, but other methods must also be applied to further build parsimonious models (eg. PCA and Lasso or Ridge Regression). I hope this helps someone!
