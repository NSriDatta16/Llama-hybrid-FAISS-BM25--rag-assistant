[site]: crossvalidated
[post_id]: 638721
[parent_id]: 637986
[tags]: 
Would love to also understand how closed-form solutions for the variances of these distributions can also be calculated (i.e. non simulations) if possible. Below is an example of Markov chain with 5 states plus a 6th absorbing state. The transition matrix for such terminating Markov chain can be described as consisting of several parts $$T = \begin{bmatrix} {\color{gray} S} & {\color{red} {s}} \\ 0 & {\color{blue} 1} \end{bmatrix}$$ The transitions between the different non-terminating states. (In gray) The transitions towards the absorbing/terminating state. (In red) The transitions from the absorbing state to itselve. (In blue) Let's consider the state-vector $X(k)$ at step $k$ , which describes the fractions of the different states that are being occupied. Note that we have $X(k) = X(k-1) \cdot T$ , ie. you can compute the $k$ -th state vector from the $k-1$ -th state vector by multiplying with the transition matrix. By induction we can express it in terms of the begin state $\pi = X(0)$ multiplied by a power of the transition matrix (repeated multiplications) $$X(k) = \pi T^k$$ The upper left block of $T^k$ will be of the form $S^k$ and is the part of the transitions between the non-terminating states. If we consider a reduced states-vector with only the non-terminating states, $X'(k)$ , which is the states-vector $X(k)$ without the terminating state, then these are $$X'(k) = \pi' {\color{gray}S}^k$$ And from these states $X'(k)$ a fraction ${\color{red}s}$ will terminate so the probability to end in the $k$ -th step is $$P(K_{terminate} = k) = X'(k-1) \cdot {\color{red}s} = \pi' {\color{gray}S}^{k-1} {\color{red}s}$$ To compute the raw moments you can use $$E[K^n] = \sum_{k=1}^\infty k^n P(K = k)$$ E.g. $$E[K] = \sum_{k=1}^\infty k P(K = k) = \pi' \left[\sum_{k=1}^\infty k S^{k-1}\right] s$$ $$E[K^2] = \sum_{k=1}^\infty k^2 P(K = k) = \pi' \left[\sum_{k=1}^\infty k^2 S^{k-1}\right] s$$ and substitute $$\sum_{k=1}^\infty k S^{k-1} = (S-I)^{-1}(S-I)^{-1}$$ $$\sum_{k=1}^\infty k^2 S^{k-1} = -(S+I)(S-I)^{-1}(S-I)^{-1}(S-I)^{-1}$$ Alternatively, the expressions are easier when we use $$\begin{array}{rcl} E[K] &=& \sum_{k=0}^\infty P(K > k)\\ E[K^2] &=& \sum_{k=0}^\infty 2k P(K > k)\\ \end{array}$$ and $P(K > k) = \pi' S^{k} \cdot \mathbf{1}$ where $\mathbf{1}$ is a column vector with ones, which effectively sums up the fractions in all the non-terminateds states and that is equal to the probability of not being yet in the terminating state. Here is an R computer code that demonstrates that these computations give the same result as a simulation (with some small difference because the simulations are only approximate) ### just some funny matrix for transitions S = matrix( c(0.1,0.3,0.2,0.1,0.2, 0.3,0.2,0.1,0.1,0.1, 0.1,0.1,0.2,0.2,0.1, 0.2,0.1,0.2,0.1,0.3, 0.1,0.3,0.1,0.1,0.2), 5, byrow = TRUE) T = cbind(S,1-rowSums(S)) T = rbind(T,c(0,0,0,0,0,1)) T start = c(0.2,0.2,0.2,0.2,0.2,0) simulate = function() { t = 0 state = sample(1:6,1, p = start) ### simulate transitions untill termination while (state
