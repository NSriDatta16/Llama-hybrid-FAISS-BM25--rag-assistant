[site]: crossvalidated
[post_id]: 157798
[parent_id]: 157796
[tags]: 
1) It seemed that any arbitrary data sets can be classified into groups, as long as we apply k-mean clustering. Is that possible some data sets naturally don't have any structure and cannot be divided into groups (e.g., uniform distributed in all feature space and don't have any pattern)? I am concerned about whether my data set really has any structure or not, since the cumulative proportion of components is so low. Borrowing an analogy from regression modeling, you may estimate non-zero correlations, but you need to use inference to determine if these differences are significant. Or, if you were interested in prediction, you might use split-sample validation to confirm that the regression model indeed has predictive value. Clustering is exactly like that. Estimating the number of clusters as well as predicting to which clusters individuals belong is a bit difficult to calibrate. Bayesian models make it a bit easier. If you're not comfortable with that, take it as a prediction validation problem. You should consider split sample, leave-one-out, or k-fold and compare their merits and flaws. 2) Is the 3 clusters recommendation given by NbClust reasonable? How can I validate this? It depends entirely upon your desired application. The most practical way to verify classification from PCA is to look at distributions and at least "verify" that they make sense. For instance, I recall the Netflix prize statistical modeler first applied PCA and found 2 principal components in a scree plot accounted for a substantial amount of variation. When he applied rotation and plotted the movies, he found the axes distinctly represented fantasy vs. reality and action vs. intimacy.
