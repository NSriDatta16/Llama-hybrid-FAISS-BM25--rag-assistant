[site]: crossvalidated
[post_id]: 183885
[parent_id]: 2641
[tags]: 
$P(x|\theta)$ can be seen from two points of view: As a function of $x$, treating $\theta$ as known/observed. If $\theta$ is not a random variable, then $P(x|\theta)$ is called the ( parameterized ) probability of $x$ given the model parameters $\theta$, which is sometimes also written as $P(x;\theta)$ or $P_{\theta}(x)$. If $\theta$ is a random variable, as in Bayesian statistics, then $P(x|\theta)$ is a conditional probability, defined as ${P(x\cap\theta)}/{P(\theta)}$. As a function of $\theta$, treating $x$ as observed. For example, when you try to find a certain assignment $\hat\theta$ for $\theta$ that maximizes $P(x|\theta)$, then $P(x|\hat\theta)$ is called the maximum likelihood of $\theta$ given the data $x$, sometimes written as $\mathcal L(\hat\theta|x)$. So, the term likelihood is just shorthand to refer to the probability $P(x|\theta)$ for some data $x$ that results from assigning different values to $\theta$ (e.g. as one traverses the search space of $\theta$ for a good solution). So, it is often used as an objective function, but also as a performance measure to compare two models as in Bayesian model comparison . Often, this expression is still a function of both its arguments, so it is rather a matter of emphasis.
