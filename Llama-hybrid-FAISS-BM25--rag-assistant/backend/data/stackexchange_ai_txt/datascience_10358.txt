[site]: datascience
[post_id]: 10358
[parent_id]: 9832
[tags]: 
Q-values are a great way to the make actions explicit so you can deal with problems where the transition function is not available (model-free). However, when your action-space is large, things are not so nice and Q-values are not so convenient. Think of a huge number of actions or even continuous action-spaces. From a sampling perspective, the dimensionality of $Q(s, a)$ is higher than $V(s)$ so it might get harder to get enough $(s, a)$ samples in comparison with $(s)$ . If you have access to the transition function sometimes $V$ is good. There are also other uses where both are combined. For instance, the advantage function where $A(s, a) = Q(s, a) - V(s)$ . If you are interested, you can find a recent example using advantage functions here: Dueling Network Architectures for Deep Reinforcement Learning by Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot and Nando de Freitas.
