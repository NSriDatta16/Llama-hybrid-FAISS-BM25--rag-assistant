[site]: crossvalidated
[post_id]: 187216
[parent_id]: 187190
[tags]: 
DCT stands for Discrete Cosine Transform . Actually there are different types of DCT, yet they share the same general properties. Yours appears to be a type-II DCT, which was recently discussed in Does DCT Type-2 lossless or lossy? . It is an orthogonal transform, with numerous signal/image processing applications, for instance MP3 and JPEG audio and image commpression. It is used prefered over discrete Fourier transforms for different reasons (optimality under correlation modeling, speed, realness), and share the idea of representing data $v$ as a sum of sines/cosines with different frequencies. The vector $u$ contains the multiplicative coefficients for each of these frequency indices, and as the same dimension as $v$ (step 2 in your figure). The concept used in compression is to reduce the precision on coefficients in $u$, and even discard some of them (step 3 in your figure). The latter amounts to cancelling higher frequencies in your data. Four effects are expected: recovering data with cancelled coefficients will yield barely perceptible differences, it can even reduce the noise level in your data (like a low-pass filtering), the energy and information in your data will be better concentrated, last and foremost, your transformed vectors have fewer samples (you will perform the PCA on the truncated $u$, and add the orthogonal DCT to the projection). It is important to remember that DCT was designed (Discrete Cosine Transform, Ahmed, Natarajan & Rao, 1974) and compared with the Karhunen-Lo√®ve transform (KLT). And the KLT bears similarities with PCA. Loosely speaking, the DCT is a decorrelating transform, a form of non-adaptive KLT, working for data having a covariance matrix concentrated along the diagonal. So every vector $v$ will have its $u$. The difficult exercice is to choose a correct $c\le k$ for all of them, which depends a lot on your data and computation needs. Upon inspection, you can decide that $c=k/2$ or $c=k/4$ suffices. You are lucky enough to have 60 dimensions so you can try a lot of fractions $1/2, 1/3,1/4, 1/5, 1/6$ and check whether you do not get loss in performance with respect to the full data. A more automated option consists in finding $c$ by computing how much coefficients you need to keep $90\%$ of the data energy, for instance. As it is orthogonal, close in mind to tthe KLT, you can use the method given in Compute the cumulative energy content for each eigenvector : For example, you may want to choose $L$ so that the cumulative energy $g$ is above a certain threshold, like 90 percent The difference would be that you do not sort by decreasing eigenvalue (modulus), but by frequency index.
