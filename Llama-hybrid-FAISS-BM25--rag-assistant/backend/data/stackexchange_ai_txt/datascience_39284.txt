[site]: datascience
[post_id]: 39284
[parent_id]: 
[tags]: 
We are using PostgreSQL to store big data and are concerned it may crash the on-board Neural Net. Any thoughts?

We have figured out how to write Big Data to our PostgreSQL database. We would like to run this (normalized) data into the onboard Neural Network in Orange. Our lead programmer said he thought Orange may very well choke on this large amount of data if we feed it directly into the Orange Neural Net from Big PostgreSQL Data. Does anyone here have any experience with this problem? Should we maybe write a widget that caches the data and feeds it into the neural network in discrete chunks? Has anyone here attempted to feed gigabytes of data into the Orange Neural Net? Is TensorFlow a better solution?
