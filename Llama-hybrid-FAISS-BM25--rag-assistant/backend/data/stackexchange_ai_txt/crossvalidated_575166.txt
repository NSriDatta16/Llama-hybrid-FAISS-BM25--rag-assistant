[site]: crossvalidated
[post_id]: 575166
[parent_id]: 
[tags]: 
In a tranformer, the same word can have different attention weights in different sentences?

I'm trying to understand the transformer architecture for NLP. The main issue is regarding the attention weights. The same word can have different attention weights in different sentences, right?
