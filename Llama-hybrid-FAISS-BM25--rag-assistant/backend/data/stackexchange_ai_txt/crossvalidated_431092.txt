[site]: crossvalidated
[post_id]: 431092
[parent_id]: 226923
[tags]: 
ReLU is a literal switch. With an electrical switch 1 volt in gives 1 volt out, n volts in gives n volts out when on. On/Off when you decide to switch at zero gives exactly the same graph as ReLU. The weighted sum (dot product) of a number of weighted sums is still a linear system. For a particular input the ReLU switches are individually on or off. That results in a particular linear projection from the input to the output, as various weighted sums of weighted sum of ... are connected together by the switches. For a particular input and a particular output neuron there is a compound system of weighted sums that actually can be summarized to a single effective weighted sum. Since ReLU switches state at zero there are no sudden discontinuities in the output for gradual changes in the input. There are other numerically efficient weighted sum (dot product) algorithms around like the FFT and Walsh Hadamard transform. There is no reason you can't incorporate those into an ReLU based neural network and benefit from the computational gains. (eg. Fixed filter bank neural networks.)
