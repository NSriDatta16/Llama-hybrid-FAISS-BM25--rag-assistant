[site]: crossvalidated
[post_id]: 416558
[parent_id]: 416553
[tags]: 
On the contrary, cross-validation is a good way to combat overfitting! Why $k$ -fold CV? Suppose you have a model and you want an estimate of its out-of-sample performance... You could assess the prediction error on the same data used to fit the model (i.e. the training error), but this is obviously not a good indicator of out -of-sample performance. If the model is indeed overfitting, it will perform poorly on new observations, but you will still observe a low training error. Alternatively, you could split your data into two part (train/test) and only use the train set to fit the model. The rest of the data, never seen by the model in any way, is then used to get an estimate of the out-of-sample performance. Great! But what if we had used a different split? As it turns out the variance between results obtained from different splits can be quite large... so large in fact, that data splitting is only reliable for really large $n$ . This is what $k$ -fold CV attempts to tackle, by doing the following repeatedly: Fit your model with $n - \frac{n}{k}$ observations; Observe its performance on the remaining $\frac{n}{k}$ observations, which were not used to fit your model . You repeat this process $k$ times, each time leaving out the next $\frac{n}{k}$ observations for testing, until all observations have been used once as a test set. You then sum the errors on the test set of each fold (or compute a weighted average), and you have an estimate of out-of-sample performance that is less sensitive to the particular splits used, because there are now $k$ of them. $^\dagger$ Can this cause overfitting? Now to answer your question: Since each fold will be used to train the model (in $k$ iterations), won't that cause overfitting? Each fold is indeed used to train the same model... from scratch . So while there is indeed overlap between training sets, and thus you are indeed fitting model s on (partially) the same data multiple times, you are not reusing the data to update your estimates! If your model would overfit in a particular fold, then the training error of that fold would be lower than the testing error of that fold. Hence, when summing/averaging the errors of all folds, a model that overfits would have low cross-validated performance. $\dagger$ : Even better, if you can afford it computationally, is to repeat $k$ -fold CV multiple times.
