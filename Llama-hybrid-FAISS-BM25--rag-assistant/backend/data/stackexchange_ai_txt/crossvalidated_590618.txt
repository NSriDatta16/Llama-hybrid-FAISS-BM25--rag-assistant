[site]: crossvalidated
[post_id]: 590618
[parent_id]: 
[tags]: 
Derivation of (5.76) in "Pattern Recognition and Machine Learning"

The book "Pattern Recognition and Machine Learning" by Christopher M. Bishop says in page 248 ... for softmax outputs we have: $$\frac{\partial y_k}{\partial a_l}=\delta_{kl}y_k-y_ky_l.\tag{5.76}$$ Note that in the original book, the index $l$ is printed as $j$ . This typo was pointed out in the errata and fixed above. Backpropagation is an easy concept because it is just the chain rule of derivatives, but the book is so poorly written that I don't understand how this derivative is obtained. First I don't know for sure what notation $\delta_{kl}$ means. From equation (5.75) $\frac{\partial y_k}{\partial a_l}=\delta_{kl}\sigma'(a_l)$ , I can infer that $\delta_{kl}=\frac{\partial y_k}{\partial z_l}$ because this is the only way for me to explain this equation: $\frac{\partial y_k}{\partial a_l}=\frac{\partial y_k}{\partial z_l}\frac{\partial z_l}{\partial a_l}$ and $\frac{\partial z_l}{\partial a_l}=\sigma'(a_l)$ , assuming the hidden units use sigmoidal activation functions as with output units. But if it is the case, from equation (4.106) and (5.49) of the same book, we have, for softmax activation, $$\frac{\partial z_l}{\partial a_l}=h'(a_l)=z_l(1-z_l),$$ and in turn, following the same assumption that each hidden unit uses softmax activation, $$\frac{\partial y_k}{\partial a_l}=\frac{\partial y_k}{\partial z_l}\frac{\partial z_l}{\partial a_l}=\delta_{kl}z_l(1-z_l).$$ But I cannot equate the righthand expression with that of (5.76). So, can you please help me with the derivation of (5.76)? Thanks a lot.
