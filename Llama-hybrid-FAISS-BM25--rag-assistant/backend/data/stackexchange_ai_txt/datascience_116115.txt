[site]: datascience
[post_id]: 116115
[parent_id]: 
[tags]: 
Question about input value in Gradient descent

I am currently going through Udacity´s online course "Intro to Deep Learning with PyTorch". In one of the videos covering the Gradient descent algorithm they show the formula for how the weigths and biases will be updated using the Gradient descent algorithm. For the weigths, they write the following formula for updating. w i ` i - α(ŷ-y)*x i . Where α(ŷ-y)*x i = dL / dw i However, what I have a hard time to understand is the effect of the input value (x i ) on the gradient. A higher input value will lead to a larger change in the weigth and vice versa, but a larger input value doesn´t mean necessarily that the loss will be higher, so why should it affect the gradient value? Can anyone help me understand what I am missing?
