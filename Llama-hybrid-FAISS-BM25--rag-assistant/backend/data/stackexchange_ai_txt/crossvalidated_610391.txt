[site]: crossvalidated
[post_id]: 610391
[parent_id]: 
[tags]: 
High loss while training word embedding

I'm trying to train (Skip-Gram) word embeddings on a dataset derived from a small piece of data (500 lines ~2500 words ~840 unique words). I'm think I'm aware that this is too small of a dataset to get any useful results. However, when I train the model, I would've expected the loss to go to ~0, because of the small size of the dataset (just overfitting to the data), instead the cross-entropy-loss only decreases to around 4. I've tried different optimizers (SGD, Adam) and different sizes of the hidden layer (i.e. the size of the embedded vectors), but no luck in decreasing the loss further. Is my intuition wrong here - is it expected that the loss remains high?
