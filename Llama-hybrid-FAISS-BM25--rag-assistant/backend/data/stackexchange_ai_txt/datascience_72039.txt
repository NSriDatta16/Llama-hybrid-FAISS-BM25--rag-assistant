[site]: datascience
[post_id]: 72039
[parent_id]: 55991
[tags]: 
As the other answers already state: Warmup steps are just a few updates with low learning rate before / at the beginning of training. After this warmup , you use the regular learning rate (schedule) to train your model to convergence. The idea that this helps your network to slowly adapt to the data intuitively makes sense. However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...) to compute correct statistics of the gradients. Therefore, a warmup period makes little sense when training with plain SGD. E.g. RMSProp computes a moving average of the squared gradients to get an estimate of the variance in the gradients for each parameter. For the first update, the estimated variance is just the square root of the sum of the squared gradients for the first batch. Since, in general, this will not be a good estimate, your first update could push your network in a wrong direction. To avoid this problem, you give the optimiser a few steps to estimate the variance while making as little changes as possible (low learning rate) and only when the estimate is reasonable, you use the actual (high) learning rate. Add-on As Michael pointed out, my answer might suggest that running one epoch with zero learning rate should do the job. However, there are a few reasons why this might not work as well: One epoch with lr=0 is just a lost epoch in the end. If you're iterating through the data you can as well have a very small learning rate to do at least something. After this first "useless" epoch, you would have accurate statistics. However, if you directly jump in with a high learning rate, your network (and therefore also the gradients) might change a lot. Therefore, you want to slowly increase the learning rate so that the statistics get a chance to move along with the updates. Disclaimer: these arguments are intuitions/speculations rather than plain facts. Feel free to let me know if this does work in practice!
