[site]: datascience
[post_id]: 16121
[parent_id]: 14964
[tags]: 
In the basics a good visual split is a good starting point. And yes, it is smart to keep in mind how the algorithms divide the space. A good strategy, I personally like to apply is to start of with simple learners to learn how your data is structered. Hoe well does NN work, are there hints of local behavior? How well does Naive Bayes work? Is the concept complex or do individual features hold information? Etc. As for selecting the features: You could try ranking your features on methods that compare their use (such as information gain), or simply write a scheme that tries all combinations of two on your two methods (it's only 9 * 8 runs). If the space was a little bigger I would suggest a combination of the two. You might also want to try combining features (fi: PCA).
