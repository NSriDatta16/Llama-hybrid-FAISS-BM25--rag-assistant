[site]: crossvalidated
[post_id]: 436193
[parent_id]: 
[tags]: 
How to understand the difference of EM algorithm between PRML and 'Machine learning - a probabilistic perspective'

The M step of EM algorithm in chapter 11.4.7 Machine learning - a probabilistic perspective is denoted: $$ \theta^{t+1} =\underset{\theta}{argmax}Q(\theta, \theta^{t})=\underset{\theta}{argmax}\sum_{i}E_{q_{i}^{t}}[logp(x_i, z_i|\theta)]$$ $\theta$ is the parameter, $t$ is the step, $Q$ is the expectation, $q_{z_i}$ is a arbitrary distribution over the hidden varaiable $z_i$ , $p(x_i,z_i)$ is the distribution of the complete data. The expectation can be written as: $$ Q(\theta, \theta^t)=\sum_{i}E_{q_{i}^{t}}[logp(x_i, z_i|\theta)] =\sum_{i}\sum_{z_i}q(z_i^t)logp(x_i, z_i| \theta) $$ In the E step: $$q(z_i^t)=p(z_i^t|x_i, \theta^{t})$$ then: $$Q(\theta, \theta^t)= \sum_{i}\sum_{z_i}p(z_i^t|x_i,\theta^t)logp(x_i, z_i| \theta) \quad \quad(1)$$ The M step in the chapter 9.4 PRML is denoted: $$ Q(\theta, \theta^{old})=\sum_{Z}q(Z)ln(p(X,Z|\theta))=\sum_{Z}P(Z|X|\theta^{old})lnp(X,Z|\theta) $$ $X$ is all of the observed variables, $Z$ is all of the hidden variables. In the E step, $$ q(Z)=p(Z|X, \theta^{old})=\frac{p(X,Z|\theta^{old})}{\sum_{Z} p(X,Z|\theta^{old})}=\frac{\prod_{n=1}^{N}p(x_n, z_n|\theta^{old})}{\sum_Z\prod_{n=1}^{N}p(x_n, z_n|\theta^{old})}=\prod_{n=1}^{N}p(z_n|x_n, \theta^{old})$$ then: $$Q(\theta,\theta^{old})=\sum_{Z}\prod_{n=1}^{N}p(z_n|x_n, \theta^{old})\sum_{n=1}^{N}lnp(x_n, z_n|\theta) \quad \quad(2)$$ I don't understand why the equation (1) and equation(2) is different?
