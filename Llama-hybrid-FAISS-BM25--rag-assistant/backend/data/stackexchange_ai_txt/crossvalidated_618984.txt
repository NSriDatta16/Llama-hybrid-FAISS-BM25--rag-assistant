[site]: crossvalidated
[post_id]: 618984
[parent_id]: 
[tags]: 
Why the posterior predictive distribution is called the 'gold standard' in the Dropout paper?

The dropout paper states that: With unlimited computation, the best way to regularize a fixed-sized model is to average the predictions of all possible settings of the parameters, weighting each setting by its posterior probability given the training data. This can sometimes be approximated quite well for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we would like to approach the performance of the Bayesian gold standard using considerably less computation. My understanding is that it is talking about the posterior predictive distribution: $p(y|X, D) = \int p(y|X, w) p(w|D)$ , where $D$ is the training set. What I don't understand is why it is calling it the 'gold standard' and the 'best way to regularize a fixed-sized model.' I have not seen any optimaLity conditions referring to this. Can someone give me some hint about why they say this is the gold standard?
