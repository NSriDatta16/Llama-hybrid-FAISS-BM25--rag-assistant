[site]: crossvalidated
[post_id]: 306862
[parent_id]: 
[tags]: 
Cross entropy versus Mean of Cross Entropy

In many neural network applications, people are prone to define loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels,logits) [tensorflow functions] as a loss function. Why add tf.reduce_mean (compute the expected value)?
