[site]: crossvalidated
[post_id]: 485684
[parent_id]: 80253
[tags]: 
Statistician Francis Anscombe demonstrates both the importance of graphing data before analyzing it, and checking the effect of outliers and other influential observations on statistical properties by showing that evaluating data relationships based solely on statistical measures is inadequate compared to preliminary visual inspection. He does this with four different datasets, each consisting of two variables $x$ and $y$ , different for each: Each of the $x$ vectors in the four graphs have the same mean and variance, same goes for the $y$ vectors. Pairwise, $(x,y)$ have the same correlation and regression line in all four graphs. The first scatter plot (top left) appears to be a simple linear relationship, corresponding to two variables correlated where $y$ could be modelled as gaussian with mean linearly dependent on $x$ . The second graph (top right) is not distributed normally; while a relationship between the two variables is obvious, it is not linear, and the Pearson correlation coefficient is not relevant. A more general regression and the corresponding coefficient of determination would be more appropriate. In the third graph (bottom left), the distribution is linear, but should have a different regression line (a robust regression would have been called for). The calculated regression is offset by the one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.816. Finally, the fourth graph (bottom right) shows an example when one high-leverage point is enough to produce a high correlation coefficient, even though the other data points do not indicate any relationship between the variables. If there is non-linearity between data series, (Pearson's) correlation is inadequate and other dependence measures such as Spearman's rank correlation and mutual information should be used.
