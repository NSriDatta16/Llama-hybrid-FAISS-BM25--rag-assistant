[site]: crossvalidated
[post_id]: 460161
[parent_id]: 
[tags]: 
Why BERT use learned positional embedding?

Compared with sinusoidal positional encoding used in Transformer, BERT's learned-lookup-table solution has 2 drawbacks in my mind: Fixed length Cannot reflect relative distance Could anyone please tell me the considerations behind such design?
