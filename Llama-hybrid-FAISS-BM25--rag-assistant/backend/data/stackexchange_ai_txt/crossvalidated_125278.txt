[site]: crossvalidated
[post_id]: 125278
[parent_id]: 124681
[tags]: 
A good approach to this kind of problem can be found in section 4 of the paper The Bayesian Image Retrieval System, PicHunter by Cox et al (2000). The data is a set of integer outcomes $A_1, ..., A_N$ where $N$ is the number of trials. In your case, there are 3 possible outcomes per trial. I will let $A_i$ be the index of the face that was left out. The idea is to postulate a generative model for the outcome given some model parameters, and then estimate the parameters by maximum-likelihood. If we show faces $(X_1,X_2,X_3)$ and the participant says that $(X_2,X_3)$ are the most similar, then the outcome is $A=1$, with probability $$ p(A = 1 ~|~ X_1, X_2, X_3) \propto \exp(-d(X_2,X_3)/\sigma) $$ where $d(X_2,X_3)$ is the distance between faces 2 and 3, and $\sigma$ is a parameter for the amount of "noise" (i.e. how consistent the participants are). Since you want an embedding in Euclidean space, your distance measure would be: $$ d(x,y) = \sqrt{\sum_k (\theta_{xk} - \theta_{yk})^2} $$ where $\theta_x$ is the (unknown) embedding of face $x$. The parameters of this model are $\theta$ and $\sigma$, which you can estimate from data via maximum-likelihood. The paper used gradient ascent to find the maximum. The model in the paper was slightly different since the paper used known attributes of the images to compute distance, rather than an unknown embedding. To learn an embedding you would need a much larger dataset, in which each face was shown multiple times. This basic model assumes that all trials are independent and all participants are the same. A nice benefit of this approach is that you can easily embellish the model to include non-independence, participant effects, or other covariates.
