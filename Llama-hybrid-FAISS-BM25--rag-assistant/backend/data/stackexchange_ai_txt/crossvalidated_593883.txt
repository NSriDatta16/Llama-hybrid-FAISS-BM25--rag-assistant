[site]: crossvalidated
[post_id]: 593883
[parent_id]: 593814
[tags]: 
Problems with more features than samples are going to be a problem for any machine learning method or statistical classifier. This is because the number of parameters will generally increase with the number of features, and the more parameters you have, the more data you will need to estimate their values reliably. There is no critical point where this goes from being fine to being a problem - it depends on the nature of the data generating process (i.e. the problem) and potentially also on the classifier. Now the linear SVM will have less of a problem with this sort of dataset than most. Assuming the data are in general position (e.g. no co-linear points), then a linear classifier is guaranteed to be able to split the training sample without error if there are more features than samples. However, the SVM (with C equal to infinity) will give you the maximal margin classifier, which separates the data in such a way that the deviation from the training data required to produce an error is as large as it could possibly be. Thus it is likely to generalise better than some randomly chosen decision boundary that has zero error on the training set. However you may still get better performance by tuning the regularisation parameter, C. This is because the structural risk minimisation principle aims to select a model from a hypothesis class that is as small as possible. Increasing C increases the size of the hypothesis class (the number of linear models that can be represented), so we can chose a less complex classifier by reducing C. We start with a very powerful classifier, e.g. a linear model in a high-dimensional space, and reduce it's power by putting a constraint (C) on the norm of the weight vector. So we match the complexity of the model to match the complexity required by the learning task. Tuning the C parameter is central to the correct application of Support Vector Machines, so if you are not tuning it carefully (e.g. using a default setting ) you are using it incorrectly and it may not work very well.
