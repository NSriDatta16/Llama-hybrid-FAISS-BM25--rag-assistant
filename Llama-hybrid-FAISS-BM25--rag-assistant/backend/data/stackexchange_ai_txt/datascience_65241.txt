[site]: datascience
[post_id]: 65241
[parent_id]: 
[tags]: 
Why is the decoder not a part of BERT architecture?

I can't see how BERT makes predictions without using a decoder unit, which was a part of all models before it including transformers and standard RNNs. How are output predictions made in the BERT architecture without using a decoder? How does it do away with decoders completely? To put the question another way: what decoder can I use, along with BERT, to generate output text? If BERT only encodes, what library/tool can I use to decode from the embeddings?
