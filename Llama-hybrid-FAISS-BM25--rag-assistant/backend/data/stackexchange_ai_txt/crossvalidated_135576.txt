[site]: crossvalidated
[post_id]: 135576
[parent_id]: 135545
[tags]: 
build one GLM model using the whole training set consisting of 30 obs., or to make cross validation - building multiple models and then average the coefficients? Building multiple (linear) models and average their coefficients is not what is usually called cross validation. It is a particular way of aggregating models. Aggregated (aka ensemble) models perform better than the individual models of the ensemble iff the individual models suffer from variance, i.e. are unstable. Otherwise, aggregation does not help. will you recommend one-leave-out cross validation as the sample size of a training set is relatively small? No*, particularly with small sample sizes I'd recommend to use either out-of-bootstrap or iterated/repeated $k$-fold (or equivalent, leave-$n$-out) cross validation. The many iterations will allow you to judge stability, which yields a basis for the decision for or against model aggregation. * That is, unless the sample is so small that there isn't any chance to leave out more than 1 case. Roughly speaking, I'd recommend LOO for $n =$ 3 or 4. For $n \le$ 15, I'd evaluate all combinations of leaving out 2 cases, after that I'd go for iterated $k$-fold or out of bootstrap with a total number of surrogate models in the order of magnitude of 100 - 1000.
