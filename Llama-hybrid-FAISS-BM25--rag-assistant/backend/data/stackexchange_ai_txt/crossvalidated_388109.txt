[site]: crossvalidated
[post_id]: 388109
[parent_id]: 388095
[tags]: 
You train each tree, $T$ , with a subset of the training set (a bootstrap sample), and some of the samples are left out. So, for each tree in the forest, there are out of the bag samples . If I didn't misunderstand you, you test these tree-specific OOB samples on the tree, $T$ , and calculate the error, do this for every tree, and average the errors , which is not the way you do normally, and in the explanation in your image. Here, you test your samples on trees , not the forest . Just like you have a subset of samples for each tree, conversely, for each sample, $x_i$ , you have a set of trees that your sample is out of bag , e.g. $T_i=\{T_{i1},T_{i2},...,T_{im}\}$ set of trees that don't contain $x_i$ in their sample subset. You calculate predictions of $x_i$ on this smaller forest, average them, which creates $\hat{y}^{oob}$ and then calculate your error. You do this for each sample in your dataset, and obtain the OOB error.
