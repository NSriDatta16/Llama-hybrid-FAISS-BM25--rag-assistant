[site]: crossvalidated
[post_id]: 385405
[parent_id]: 385402
[tags]: 
The likelihood is a so-called conditional density. It is a probability density function on the data space (for $D$ ) given any parameter $\theta$ that we pass over to the function. When integrating over it with respect to $D$ we obtain the conditional distribution of the data given a certain parameter. This conditional distribution is a Markov Kernel. Maybe this helps: The fundamental Problem that is to be solved in any kind of parametric statistics is: There is some underlying parameter $\theta^*$ . We do not know this parameter. We obtain a sample $D \sim p(\cdot|\theta^*)$ . Given this sample $D$ , we now want to identify $\theta^*$ . Frequentist statistics uses estimators, and hypothesis tests. Bayesian statistics uses posterior measures. Hence the likelihood is the density from which the data is sampled. One more confusion to mention: Note that a density function in a particular point does in general not refer to a „probability“ - a probability being a value between 0 and 1. The probability is what we obtain, when integrating over a density function with respect to the correct measure.
