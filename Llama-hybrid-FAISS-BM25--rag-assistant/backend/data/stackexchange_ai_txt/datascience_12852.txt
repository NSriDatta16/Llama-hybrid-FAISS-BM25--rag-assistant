[site]: datascience
[post_id]: 12852
[parent_id]: 11452
[tags]: 
I know that neither xgboost, nor sklearn offer what you want. I have checked R, and didn't find it either. But random forests are easy models to implement, so you can just produce one by yourself: Here, in python using sklearn: import numpy as np from sklearn.tree import DecisionTreeClassifier from scipy.stats import mode class MyRandomForest: def __init__(self, Pcol, Pobs, n_estimators=10): self.n_estimators = n_estimators self.Pcol = Pcol # vector self.Pobs = Pobs # scalar def fit(self, X, y): self.classes_ = np.unique(y) assert len(self.Pcol) == X.shape[1] self.cols = [] self.ms = [] while True: j = np.random.rand(X.shape[1]) If you are using Linux, you can use multiprocessing to easily make it run in parallel. What I did was just: train a sequence of individual trees, using your probabilities save both the models and whichever columns were sampled for training because we want to use only those for evaluation (or we will get an error) using mode to get the most voted prediction Note: Pobs is the fraction of observations to use. You may also want to change np.random.choice(..., ..., False) to np.random.choice(..., ..., True) , or make it configurable, to allow for bootstrap with resampling. Usually, random forests are trained using Pobs=1 and resample=True .
