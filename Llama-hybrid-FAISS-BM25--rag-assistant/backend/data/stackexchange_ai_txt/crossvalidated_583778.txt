[site]: crossvalidated
[post_id]: 583778
[parent_id]: 
[tags]: 
How to handle highly correlated observations (rows)

What is the best practice to handle highly similar/ autocorrelated observations (rows) in a data set. These highly similar rows could come from recording (some of the) observations at too close timestamps. This is for example a problem also found in pseudoreplication . For highly correlated features (columns), one typically applied preprocessing step is dropping some of the columns which show high correlation or which have a high variance inflating factor (VIF). How would one deal with very similar rows (which possibly have their origin in correlated observations) in the case of a classification task? In the extreme case of duplicated rows some people drop the duplicated row. One strategies for removing highly correlated observations could be to compute a n x n distance matrix between all n observations or do a clustering and drop observations which are too close The question I have are: a) is there literature related to this question? b) would removing/subsampling rows with too high correlation improve generalization of the trained classifier? I would be happy if you could present an example or a counter example (or maybe both) in two dimensions for a binary classification. c) how would an efficient implementation of dropping very similar rows look like in pandas? PS: I found a related question here Subsampling to account for spatial autocorrelation of observations however it has no answers. PPS: not randomly moving very similar samples from the training set into the test set is also important for a reliable validation (see pseudoreplication). As far as I know this is typically solved by some group base shuffling which requires however to define a feature based on which this grouping can happen. Clarification about the type of date I am looking at: Classification with repeated measurements $x_i$ at different times $i$ from one subject (e.g. measurements 1,2,3,4 come from subject 1, measurements 5,6,7,8,9 come from subject 2,...). Clearly, we have to avoid leakage and make a group split such that measurements from one subject only appear exclusively in the test or train split. However, having 4 strongly correlated measurements from subject 1 might not provide useful, one could aggregate some statistics about each subject, especially if the amount of measurements per subject is random. In this case, gathering the statistics would give equal weight to each subject. The situation becomes, however, more involved if, e.g., subject 1 and subject 2 themselves come from one region in sample space and they would have a small distance between them. Then they are correlated and gathering statistics over both could be beneficial. Is there any best practice which can be applied here? Any method, you would recommend? Regression with autocorrelated time series. How would you go about it if the samples show high auto-correlation. Subsampling could be done, but is there more? For both cases, could you please provide a verbose description of methods. PPPS: this paper https://arxiv.org/abs/2206.14486v1 suggests pruning observations depending on clustering for improved neural net training
