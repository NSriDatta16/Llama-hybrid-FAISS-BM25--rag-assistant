[site]: crossvalidated
[post_id]: 562284
[parent_id]: 
[tags]: 
proof out of bag evaluation

In Geron's book "Hands-on Machine Learning with Scikit-Learn and Tensorflow" there is this sentence on page 187 "By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor." And in the footnote he mentions this ratio approaches $1-\exp(-1)$ as $m$ grows (the author means that approaches $\infty$ , I think). How should I prove this? I have no idea apart from: there are $m^m$ possible training sets.
