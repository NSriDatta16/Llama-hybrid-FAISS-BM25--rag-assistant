[site]: crossvalidated
[post_id]: 263523
[parent_id]: 228670
[tags]: 
The truncated normal distribution is better for the parameters to be close to 0, and it's better to keep the parameters close to 0. See this question: https://stackoverflow.com/q/34569903/3552975 Three reasons to keep the parameters small(Source: Probabilistic Deep Learning: with Python, Keras and Tensorflow Probability ): Experience shows that trained NNs often have small weights. Smaller weights lead to less extreme outputs (in classification, less extreme probabilities), which is desirable for an untrained model. Itâ€™s a known property of prediction models that adding a component to the loss function, which prefers small weights, often helps to get a higher prediction performance. This approach is also known as regularization or weight decay in non-Bayesian NNs. And in this blog: A Gentle Introduction to Weight Constraints in Deep Learning , Dr. Jason Brownlee states that: Smaller weights in a neural network can result in a model that is more stable and less likely to overfit the training dataset , in turn having better performance when making a prediction on new data. If you employ ReLU you'd better make it with a slightly positive initial bias : One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid "dead neurons".
