[site]: crossvalidated
[post_id]: 336983
[parent_id]: 
[tags]: 
Law of Large Numbers Convergence for Different Means

Considering the LLN stating that: If we have $\{X_i\}$ $iid$ then as $n \rightarrow \infty$ it follows that $\bar{X} \rightarrow \mu = E[X_i]$, almost surely In my case, we have a logistic model that based on a set of characteristics for each $i$, $\{K_i\}$ produces an estimate for the individual probability as a conditional expectation in the form: $E[X_i |K_i]$. However, there aren't enough observations with identical characteristics $\{K_i\}$ which would lead to identical estimates, so that the realized values should converge to the estimated ones according to LLN: $\bar{X_i} \rightarrow E[X_i|K_j]$ for each sufficiently large group of observations $i\in j$ with identical sets of characteristics $\{K_j\}$ In that case, under what assumption can we say something about the convergence below: $\bar{X_i} \rightarrow \overline{E[X_i|K_i]}$ , where $K_i$ are different i.e. when having $n$ different conditional expectations and $n$ different realizations, under what conditions can we expect the average of the realized variables to be equal to the average of the expectations? Can we assume the $\{X_i\}$ to be $iid$ given that, we have already differentiated them based on the characteristics $\{K_i\}$? Or more generally, how can we test the average predictive power of the model if we cannot compare the average realized and predicted values?
