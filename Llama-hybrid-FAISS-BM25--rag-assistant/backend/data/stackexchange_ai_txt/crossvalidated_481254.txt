[site]: crossvalidated
[post_id]: 481254
[parent_id]: 481156
[tags]: 
The softmax function is a function . For a fixed input, you get a fixed output, just as for any other function. Someone might have invented a softmax distribution in some context, but that's not the meaning of "softmax function," as described in your post, or commonly used in neural networks. The Dirichlet distribution is a distribution. Drawing from a Dirichlet distribution with some parameters will almost surely give a different result each time. The Dirichlet distribution is not a distribution over categories , it's a distribution over probability vectors. The commonality that you've found between the softmax function and the Dirichlet distribution is that the results (function outputs, random draws) are always probability vecotrs : the vectors are non-negative, and sum to 1. The normal distribution is a probability distribution which assigns positive probability to any real number. A linear function yields real number for any input. But you wouldn't conflate the two, because they're clearly different things in important ways. Likewise, the softmax function and Dirichlet distribution are distinct.
