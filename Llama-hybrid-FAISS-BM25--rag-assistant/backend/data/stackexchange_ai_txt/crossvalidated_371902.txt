[site]: crossvalidated
[post_id]: 371902
[parent_id]: 135958
[tags]: 
You don't necessarily pick any of the k 'models'. You evaluate one model on k folds to get its average performance across (train, test) splits. So, in this classification case, if your evaluation metric is let's say auc, then you get the average auc across k folds. You can compare this with another k-fold run with some other hyper-parameter's configuration on the same model / different model. From here, you can choose the best hyper parameters / model. So the basic need of k-fold CV is to evaluate your model's performance across train test sets to get the most confidence, otherwise, if you only evaluate it across one set, then you cannot say with much confidence how your model will perform across a different set of train test.
