[site]: datascience
[post_id]: 97892
[parent_id]: 
[tags]: 
Poor predictions

I have some input data (11 real values) and I need to do regression of 3 output values. I have a big training set with data. When I train the model, the RMSE is roughly at 45, which I consider to be very good. I have some problems with my model when it comes to do predictions with new data. It seems that the model is not performing well when exposed to new data. Here is the code: normalizer = preprocessing.Normalization() normalizer.adapt(x_train) model = tf.keras.models.Sequential([ normalizer, tf.keras.layers.Dense(50, activation=tf.nn.relu), tf.keras.layers.Dense(50, activation=tf.nn.relu), tf.keras.layers.Dense(50, activation=tf.nn.relu), tf.keras.layers.Dense(50, activation=tf.nn.relu), tf.keras.layers.Dense(3) ]) model.compile(optimizer='Adam', loss=self._root_mean_squared_error) es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3) history = model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=50, shuffle=True, callbacks=[es_callback]) I have some questions: Do you see something wrong with how I create the model? I assume that the normalizer layer acts as a normalizer for input queries, is that right? Meaning, I do not normalize the input data when I query the neural network.
