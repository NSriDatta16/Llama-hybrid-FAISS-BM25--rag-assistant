[site]: crossvalidated
[post_id]: 93845
[parent_id]: 
[tags]: 
How to perform cross-validation for PCA to determine the number of principal components?

I'm trying to write my own function for principal component analysis, PCA (of course there's a lot already written but I'm just interested in implementing stuff by myself). The main problem I encountered is the cross-validation step and calculating predicted sum of squares (PRESS). It doesn't matter which cross-validation I use, it's a question mainly about the theory behind, but consider leave-one-out cross-validation (LOOCV). From the theory I found out that in order to perform LOOCV you need to: delete an object scale the rest perform PCA with some number of components scale the deleted object according to parameters obtained in (2) predict the object according to the PCA model calculate PRESS for this object re-perform the same algorithm to other objects sum up all the PRESS values profit Because I'm very new in the field, in order to be sure that I'm right, I compare the results with the output from some software I have (also in order to write some code I follow the instructions in the software). I get completely the same results calculating the residual sum of squares and $R^2$, but calculating PRESS is a problem. Could you please tell to me if what I implement in the cross-validation step is right or not: case 'loocv' % # n - number of objects % # p - number of variables % # vComponents - the number of components used in CV dataSets = divideData(n,n); % # it is just a variable responsible for creating datasets for CV % # (for LOOCV datasets will be equal to [1, 2, 3, ... , n]);' tempPRESS = zeros(n,vComponents); for j = 1:n Xmodel1 = X; % # X - n x p original matrix Xmodel1(dataSets{j},:) = []; % # delete the object to be predicted [Xmodel1,Xmodel1shift,Xmodel1div] = skScale(Xmodel1, 'Center', vCenter, 'Scaling', vScaling); % # scale the data and extract the shift and scaling factor Xmodel2 = X(dataSets{j},:); % # the object to be predicted Xmodel2 = bsxfun(@minus,Xmodel2,Xmodel1shift); % # shift and scale the object Xmodel2 = bsxfun(@rdivide,Xmodel2,Xmodel1div); [Xscores2,Xloadings2] = myNipals(Xmodel1,0.00000001,vComponents); % # the way to calculate the scores and loadings % # Xscores2 - n x vComponents matrix % # Xloadings2 - vComponents x p matrix for i = 1:vComponents tempPRESS(j,i) = sum(sum((Xmodel2* ... (eye(p) - transpose(Xloadings2(1:i,:))*Xloadings2(1:i,:))).^2)); end end PRESS = sum(tempPRESS,1); In the software ( PLS_Toolbox ) it works like this: for i = 1:vComponents tempPCA = eye(p) - transpose(Xloadings2(1:i,:))*Xloadings2(1:i,:); for kk = 1:p tempRepmat(:,kk) = -(1/tempPCA(kk,kk))*tempPCA(:,kk); % # this I do not understand tempRepmat(kk,kk) = -1; % # here is some normalization that I do not get end tempPRESS(j,i) = sum(sum((Xmodel2*tempRepmat).^2)); end So, they do some additional normalization using this tempRepmat variable: the only reason I found was that they apply LOOCV for robust PCA. Unfortunately, support team did not want to answer my question since I have only demo version of their software.
