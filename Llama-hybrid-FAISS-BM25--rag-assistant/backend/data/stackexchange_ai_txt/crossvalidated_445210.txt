[site]: crossvalidated
[post_id]: 445210
[parent_id]: 445205
[tags]: 
Logistic regression with a logit link function tries to find the expected (average) odds of Y given X. But since, we are assuming the dataset to be gaussian in the first place, we know that the mean and std_dev that gives maximum likelihood is nothing but the mean and std_dev of given dataset. Yes, the sample mean would give the Maximum Likelihood estimate of a Gaussian mean, but that is of X. You want to know Y given X. That's why, in regression, there are beta coefficients/parameters which you are aiming to find (as opposed to maximizing for mu). Otherwise you'd just use the sample mean and variance as you say. Then, why not just take the mean of X, and std_dev of X and just calculate y. (If I've understood you correctly) Then imagine you don't have a particular value of X at the sample mean for X to look up what Y is, or if you have multiple different values of Y at the sample mean of X. You'd be stuck. On the other hand, just consider a regression equation $\hat{Y}=\beta_0+\beta_1x$ If you want to know the expected value of Y given x, you take some value of $x$ and multiply it with the $\beta_1$ (and add the intercept). The beta lets you know what y will be given or for any particular value of x. This is taking into account all of the X's, instead of looking up what value of Y is at the mean of X. With regards to maximizing likelihoods, in regression with Gaussian distributions it's simple; you whack the $\beta$ 's on to X and maximize for $\beta$ . But in logistic regression you need a link function which turns the output into a linear function between X and Y. The log odds gives nice interpretations so canonically people tend to use $logit(p)=ln\frac{p}{1-p}$ which means $ln\frac{p}{1-p}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+...$ or in matrix form $ln\frac{p}{1-p}=X\beta$ So $p=\frac{e^{X \beta}}{1+e^{X \beta}}$ X can be almost any distribution, the important thing is the distribution of Y. If it's a binary variable, then a bernoulli distribution should be maximized. $L(Î²|y)=\prod_{i=1}^n(p^{y_i}_i (1-p_i )^{1-y_i} $ recalling that $p=\frac{e^{X \beta}}{1+e^{X \beta}}$ Log and then differentiate that likelihood with respect to $\beta$ . Normally you would then set that derivative to equal to 0 and solve for $\beta$ but there is no closed form solution for that equation, so you'd have to use some algorithm to approximate $\beta$ .
