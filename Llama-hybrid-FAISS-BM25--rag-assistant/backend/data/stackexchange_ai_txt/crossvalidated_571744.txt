[site]: crossvalidated
[post_id]: 571744
[parent_id]: 
[tags]: 
Train a Final Machine Learning Model with Tensorflow

Based on a previous question and on this article , it is suggested that you split the data between train and test (or train/validate/test). But once you have control of your model, you should retrain or train again using the entire dataset to get more points and obtain a (theoretically) better performance. This approach is easily applied wit Scikit-learn, but I am struggling to apply it with the TensorFlow/Keras package. Actually, it seems that Tensorflow does not conceive this method, as you end up always dedicating part of the data for the validation set, in order to control for early stops and the best model setup (ModelCheckpoint, or anything considering the "val_loss"). On the other hand, if you retrain a model from scratch with the entire dataset and without any validation control scheme, you might easily end up overfitting the model. Therefore, I would like to ask if there is any procedure in Tensorflow/Keras that would allow to update or extend the model from the training phase to the full final phase, where you incorporate the rest of the data used to validate, without running in overfitting or suboptimization issues.
