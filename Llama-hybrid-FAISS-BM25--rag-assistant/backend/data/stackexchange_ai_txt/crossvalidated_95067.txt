[site]: crossvalidated
[post_id]: 95067
[parent_id]: 95055
[tags]: 
One general strategy for any classifier is to use some form of cross-validation to map the prediction output score to a probability, using the following scheme: Leave out part of the dataset. Train classifier. Make prediction on left-out data. For each prediction score, calculate what is the probability of making an error given that score or a better score. This can also be done by fitting a function that maps prediction score to probabilities. A natural choice would be mapping to a logistic function or some other form of sigmoidal function that can compress any range of scores to [0,1] as required from a probability. For SVMs, this technique is called Platt scaling or Platt probabilities and is even included in some SVM implementations.
