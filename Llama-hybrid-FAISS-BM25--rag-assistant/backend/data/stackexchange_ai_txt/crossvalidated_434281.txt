[site]: crossvalidated
[post_id]: 434281
[parent_id]: 434218
[tags]: 
The optimal $b$ and $w$ are usually chosen to be minimizers (if they exist) of some objective function. in the case of logistic regression (which is your neuron model plus a designated objective function), the objective function to be minimized is the cross-entropy empirical risk function: $$ \label{1} \tag{1} R(w, b \mid \mathcal{D}) = \frac{1}{|\mathcal{D}|} \sum_{(x, y) \in \mathcal{D}} \left(-y \log\left(\frac{1}{1 + e^{-(wx + b)}}\right) - (1 - y) \log\left(\frac{e^{-(wx + b)}}{1 + e^{-(wx + b)}}\right)\right), $$ where $\mathcal{D}$ is your training dataset. This formula is derived from the the Bernoulli log-likelihood function if we make the assumption that the examples in your dataset are i.i.d., and the conditional distribution of your target $y$ given the feature $x$ is $$ y \mid x \sim \operatorname{Bernoulli}\left(\frac{1}{1 + e^{-(wx + b)}}\right). $$ With your posited training set $\mathcal{D} = \{(-1, 0), (1, 1)\}$ , the empirical risk objective function becomes $$ \begin{aligned} R(w, b \mid \mathcal{D}) &= -\frac{1}{2} \log\left(\frac{1}{1 + e^{-w-b}}\right) - \frac{1}{2}\log\left(\frac{e^{w - b}}{1 + e^{w - b}}\right) \\ &= \frac{1}{2} \left(\log\left(1 + e^{-w-b}\right) + \log\left(1 + e^{w - b}\right) - w + b\right), \end{aligned} $$ so to find the "optimal" $w$ and $b$ you simply have to find the minimizers of this function. Unfortunately, it can be shown that this particular empirical risk function does not have a global minimum (or a local one, since \eqref{1} is a convex function), so you are right to say that in this setup there are no optimal $w$ and $b$ . You can make the empirical risk $R(w, b \mid \mathcal{D})$ arbitrarily close to $0$ (e.g., by fixing $b$ and making $w$ arbitrarily large). In practice it is common to augment the empirical risk function by a regularization term . This is a data-independent function $\Omega$ (i.e., it depends only on $w$ and $b$ , not on $\mathcal{D}$ ). Instead of minimizing the empirical risk $R(w, b \mid \mathcal{D})$ directly, one minimizes the sum $$ R(w, b \mid \mathcal{D}) + \Omega(w, b). $$ The role of $\Omega$ is to "discourage model complexity" by penalizing $w$ and $b$ if their absolute values get too large. Common forms for $\Omega$ are the $L^2$ and $L^1$ regularization functions: $$ \begin{aligned} \Omega(w, b) &= \lambda (w^2 + b^2), & \Omega(w, b) &= \lambda (|w| + |b|), \end{aligned} $$ respectively, where $\lambda > 0$ is a hyperparameter that must be chosen prior to estimating $w$ and $b$ to determine how much to penalize large values of $w$ and $b$ . The following pictures demonstrate what happens to the objective function surface when going from no regularization to $L^2$ regularization with increasing values of $\lambda$ . In the pictures, blacker regions indicate where the objective function is the smallest (i.e., where near-optimal $w$ and $b$ live).
