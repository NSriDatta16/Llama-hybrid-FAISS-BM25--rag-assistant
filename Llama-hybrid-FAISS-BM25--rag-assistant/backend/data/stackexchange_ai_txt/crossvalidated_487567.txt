[site]: crossvalidated
[post_id]: 487567
[parent_id]: 394345
[tags]: 
A slightly more formal answer: Let $X, y$ be your training data. Let $k_a$ , $k_b$ be two kernels you want to compare with parameter vectors $\theta_{k_a}$ and $\theta_{k_b}$ . Now consider the sklearn python function (simplified version of the second print statement in the question) # X, y is the training data X, y = make_regression() def f(k): return GaussianProcessRegressor(kernel=k).fit(X, y).log_marginal_likelihood() where k is a Kernel instance (e.g. k=RBF() ). Mathematically this is (see Rasmussen eq 5.8 for the full form of the likelihood) $$f(k) = \max_{\theta_k'} log P(Y|X, \theta_k', k)$$ That is the maximum marginal likelihood you can achieve on your training data when committed to using kernel k . Note that this might be positive since $y$ is continuous and hence point probabilities above 1 are possible. This is not equal to (C.20) in the paper you cited . With this (C.20 in the paper), you compare the class of kernel $k_a$ with the class of kernel $k_b$ , without going down to the level of a concrete parametrisation. This is probably the best way to decide which kernel class is better. But, marginalising over parameters $\theta$ is computationally difficult and also requires a prior over $\theta$ , i.e. p(\theta_k | k). Both is not implemented in sklearn to my knowledge. Coming back to what sklearn can do (i.e. $f(k)$ as defined in this answer). I still think it is sensible to choose your kernel according to $\max \{ f(k_a), f(k_b) \}$ . It means that you compare 'the best' parametrisation of kernel A with 'the best' parameterisation of kernel B. I.e. you pick the best of these two concrete, parametrised kernels. To convince you that this is not an outrageous thing to do: Consider the composite kernel $k_c$ with parameters $\theta_{k_a}, \theta_{k_b}$ and $\gamma \in \{0, 1\}$ . $$ k_c(\cdot, \cdot) = \gamma k_a(\cdot, \cdot) + (1-\gamma) k_b(\cdot, \cdot) $$ Now picking between the two kernels (in the simple way) looks just like the normal max marginal likelihood approach of finding hyper parameters.
