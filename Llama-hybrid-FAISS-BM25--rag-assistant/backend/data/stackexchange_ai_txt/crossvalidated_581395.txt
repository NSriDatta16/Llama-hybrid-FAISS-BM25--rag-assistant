[site]: crossvalidated
[post_id]: 581395
[parent_id]: 
[tags]: 
Fitting the mean and variance of 1D Gaussian using the Metropolis algorithm

This probably some beginners mistake, but I have been stuck on this for a few hours now and cant spot the mistake. The scenario: From a predefined 'true' distribution with a defined mean $\mu_{true}$ and variance $\sigma_{true}$ I draw 100 observations. Based on these observations I want to find / estimate the mean $\mu_{mcmc}$ and $\sigma_{mcmc}$ I have implemented the Metropolis algorithm to generate the distribution and estimate the samples. My understanding of it is as follows: Given a prior that has a wide support (in which the true values fall), a correct (log-)likelihood function, a (symmetric) proposal distribution, and observations $x$ from the true distribution we can do the following: a. Start with an initial sample ( $\mu_0$ , $\sigma_0$ ) drawn from the prior b. Repeat the following $n$ times: Draw a new sample ( $\mu_i$ , $\sigma_i$ ) using the proposal distribution Calculate likelihood ratios between the current sample and the new sample using the provided likelihood function. The acceptance criterion = min(1, $\frac{\text{p(new sample)}}{\text{p(current sample)}}$ ) Draw a value from uniform distribution between 0 and 1 If the value is larger than the acceptance criterion we reject the new sample, and add the current sample (again) to the Monte Carlo chain. If the value is smaller or equal to the acceptance criterion we accept the new sample, and the new sample to the Monte Carlo chain. c. When finished 'burn' the first 1000 or so samples since they are representative of the actual distribution. The specifics: I use a uniform prior: [-5, 5] for $\mu$ and [0, 2] for $\sigma$ The proposal distribution is a fixed Gaussian with width 0.01 or uniformly sampling from the prior. As I understand these are symmetric and do not violate the Metropolis Algorithm. Below I have put the loglikelihood function, which is the mean of the likelihood for a Gaussian distribution. As Xi'an has mentioned in the comments, it should be the sum When estimating only the mean (assuming the variance is known) I get the distribution back perfectly. However, when I also try to estimate the variance I do not get the same distribution back it is far wider than than expected. Am I using a wrong likelihood function or something? def log_prob(observations, mu, sigma): return torch.mean(-0.5 * ((mu - observations) / sigma)) ** 2 - torch.log(torch.sqrt(2 * torch.pi * sigma ** 2))) Any help is appreciated! $\mu$ distribution (of the last 50000 samples for 100000), the red line is the true marginal over $\mu$ ." />
