[site]: crossvalidated
[post_id]: 531718
[parent_id]: 531706
[tags]: 
Yes, using some form of autoencoder training/pre-training to create good features has been a successful approach in many areas. E.g. for tabular data, using a denoising autoencoder was the winning approach in a recent Kaggle competition. Autoencoder pre-training (recovering masked features) has been used in the TabNet paper that gained a decent amount of attention. These first two examples were for tabular data, but similar things have long been done (and are still popular) for vision applications as discussed here . However, as you may notice it looks like a lot of the most successful version of autoencoders for pretraining representations have been ones that were trained to re-construct corrupted inputs into the uncorrupted inputs. Other ideas for what you can use autoencoders for would be file compression (e.g. for video calls, what's minimal information that still produces a video that looks decent to humans, but does not take a lot of bandwidth to transmit?).
