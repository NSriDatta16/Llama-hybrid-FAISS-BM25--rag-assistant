[site]: crossvalidated
[post_id]: 641853
[parent_id]: 
[tags]: 
Scaling laws for neural network memorization

I would like to ask a generalization of this question: How to perfectly overfit neural network to memorize input? Are there any scaling laws for neural network memorization? In other words, if I have M input nodes and N possible output nodes, what is the minimum number of fully connected neurons which can map any M to any N? To keep the problem simple, let's make the following assumptions: The network can be connected in any way. I don't care about the topology. I don't care about the algorithm used to set the weights of the neurons in the network. The inputs are binary and represent 2^M - 1 possible input values. The outputs are binary and represent 2^N - 1 possible output values. If there are subsets of the above (especially the topology) which are especially optimal for gradient descent or conversely, if there are subsets which gradient descent will have trouble optimizing, that's also really interesting.
