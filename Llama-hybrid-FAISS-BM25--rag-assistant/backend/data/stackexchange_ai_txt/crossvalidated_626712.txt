[site]: crossvalidated
[post_id]: 626712
[parent_id]: 184657
[tags]: 
An off-policy learner learns the value of the optimal policy independently of the agent's actions should be clarified as the data from which off-policy learner learns is independent of the agent's current policy (the policy in the moment the learner update its Q table(or Q network)) I prefer to understand why Q learnig is off-policy alongside DQN. In DQN, we can use memory buffer, where we definitely update the policy using data generated from old policy, which meets the definition above. As for q learning, what is confusing is that actually we don't use memory buffer(at least in sutton's book), the learning data is actually generated, or more precisely, influenced by our current policy! That is more like on-policy! But wait, that doesn't mean we can't use it in completely off-policy manner. Like Dqn, memory buffer is also applicable to q learning. Back to Q-learning vs SARSA, it is not hard to see that SARSA can't fit memory buffer, we can't use old-policy-generating data to update our q table. Becanse we use the current policy to estimate the next state's value V(s'). So Neil's answer did tell the crucial point behind all I say. But you can go further to fit the definition better.
