[site]: crossvalidated
[post_id]: 408497
[parent_id]: 408482
[tags]: 
What you face is a missing data problem, which you have tried to overcome with a particular calibration method. This is data imputation , on which there are over 400 questions on this site . Data imputation is often a good choice and can improve the power of a study provided that the reasons for missingness are completely at random or depend completely on data that are at hand. There are, however, some better ways to handle the imputation than what you have tried so far. First, your calibration method is leading to predicted ratios that exceed a theoretical limit of 1. That suggests that you might need to try a different calibration method. Although I'm still a bit sketchy on just what you mean by "a proportion between events A and B," I'll take that to mean that over some observation interval you have events of both type A and type B and that the number of type B events can never exceed that of the type A events. For example, perhaps all events are type A but only a portion of those have additional characteristics that make them type B. In that case your calibration/imputation method itself should enforce that requirement. You can do that with a logistic regression calibration (thus introducing your logit transformation at an earlier stage of your analysis). Although logistic regression is often used for analysis of binary outcomes, it also can accommodate counts of different types that can be thought of as generalizations of successes and failures. For example, in the scenario where all events are A but only a fraction are B, you could count B events as successes and non-B events as failures and directly use the glm() function in R to model the proportions in cases where you have data, and then use predictions from the logistic regression to impute the missing B fractions from the A observations in other cases. Then you would never have a predicted B/A ratio that exceeds 1. The second issue is that imputation necessarily introduces uncertainties into the analysis: the imputed data aren't as reliable as the actual observations. The standard way to deal with this is with multiple imputation . You make several separate imputations of the missing data with a probabilistic approach, analyze each of the imputed data sets separately, and then combine the information from all the analyses to take into account the variability introduced by the imputation. That might sound complicated but there is a widely used R package called mice that can do all this for you while using extremely flexible ways to do the imputations that might well perform better than the simple logistic regression I suggested above.
