[site]: datascience
[post_id]: 103764
[parent_id]: 47969
[tags]: 
Generally, you can indeed consider adding more layers and batchnorm/dropout to a neural network a means for controlling bias and variance of your model, respectively. However, increasing variance by stacking more layers doesn't always at all mean that you overfit your model. To diagnose that you are actually overfitting you should see that your training loss is much lower than your validation loss (image below). But as a general rule, you should aim for minimising that "gap" between your training and validation loss curves. This gap, aka generalisation gap appears to be getting minimised in your case with adding more layers (see ideal below). And that is absolutely fair.
