[site]: crossvalidated
[post_id]: 89766
[parent_id]: 89638
[tags]: 
This question can be resolved with a single picture, shown below, which immediately shows the answer is in the negative: you cannot conclude anything about the strength of the correlation between two predictors by comparing the coefficients in two models. The text of this answer explains the picture and the reasoning behind it. Background Many questions about relationships among variables in multiple regression (using ordinary or generalized least squares) end up being matters of plane Euclidean geometry. The reasons for this are The variables (independent and dependent) in multiple regression, when written as columns in a table of $n$ data rows, can be viewed as vectors in $n$ dimensional Euclidean space. The least squares fit is the orthogonal projection of the dependent variable on the subspace spanned by the independent variables. Multiple regression can be conducted as a sequence of "linear matches" . At each step there are vectors $x_1, \ldots, x_k$ and $y$ representing $k+1$ independent variables and a vector $z$ representing the dependent variable. Relative to $y$ , we may decompose any vector $w$ uniquely into the sum $$w = w_{\cdot y} + (w - w_{\cdot y})$$ where $w_{\cdot y}$ is parallel to $y$ and the residual $w - w_{\cdot y}$ is orthogonal to $y$ . For the next step, all the other vectors $\{x_i\}$ and $z$ are replaced by their residuals with respect to $y$ and one of the new $x_i$ is chosen to be the next $y$ . This process is illustrated below. Notice that when a "constant" is included in the regression, step (3) amounts to recentering all the variables: the constant $y$ is represented as the $n$ -vector $(1,1,\ldots,1)$ , whence $w_{\cdot y} = (\bar{w}, \bar{w}, \ldots, \bar{w})$ , and $(w - w_{\cdot y})$ subtracts the mean $\bar{w}$ from each component of $w$ . Thus "matching" is a natural generalization of recentering. Due to the recursion afforded by (3), almost anything we would like to learn about multiple regression can be discovered by considering just three vectors $x, y,$ and $z$ . Moreover, when all is done, $z$ will have been projected into the plane generated by $\{x,y\}$ , so when we are concerned only about the coefficients (and not the residuals), we can ignore the third dimension (determined by the direction of $z$ orthogonal to the $\{x,y\}$ plane). (One can go further and standardize the vectors $x$ and $y$ . This means they can be assumed to have unit length; standardization merely amounts to choosing suitable units of measurement for the underlying coordinates. However, re-standardizing the vectors after each matching step can change the apparent correlations among them, so I forgo that simplification here.) Consequently, we can analyze and intuitively understand this question using simple planar figures. Analysis In the illustration, the betas are least squares coefficients: $\beta_x$ is the coefficient of $x$ when matching $x$ alone to $z$ ; in other words, $\beta_x x$ is the projection of $z$ onto the line determined by $x$ . $\beta_{(x,y)}$ is the coefficient of $x$ in the regression of $z$ against $x$ and $y$ . Equivalently, it is the coefficient of $x_{\cdot y}$ when projecting $z_{\dot y}$ in the second step of the matching process: $y$ has been taken out of $z$ and $x$ . Moreover, the correlation coefficient $\rho$ between the predictors $x$ and $y$ is the cosine of the angle $\theta$ between them. Solution The question, translated into geometry, asks What can be said about $\rho = \cos(\theta)$ (whether it is near $\pm 1$ , near $0$ or exactly $0$ ) if we are only given $\beta_x$ and $\beta_{(x,y)}$ ? In the illustration $\beta_x$ is near $2/3$ and $\beta_{(x,y)}$ is about $-3/2$ , while $\theta$ is fairly small, corresponding to small positive correlation. This already shows that large changes in the coefficients of $x$ can occur even when $x$ and $y$ are not strongly correlated. When $\rho\ne 0$ , the vectors $x$ and $x_{\cdot y}$ are not parallel, whence they generate the entire $\{x,y\}$ plane. Accordingly, arbitrary values of $\beta_x$ and $\beta_{(x,y)}$ determine unique values of $z$ for which the coefficients of $x$ in the two models are $\beta_x$ and $\beta_{(x,y)}$ . $z$ is constructed by drawing the line perpendicular to $x$ at the point $\beta_x x$ and the line perpendicular to $x_{\cdot y}$ at the point $\beta_{(x,y)}x_{\cdot y}$ ; their intersection is $z$ . This proves that little can be concluded about $\theta$ (or, equivalently, $\rho$ ) regardless of what the two coefficients happen to be. (As we ought to expect, those coefficients say much more about the dependent variable $z$ than they do about the independent variables.) When $\rho=0$ , $\beta_x = \beta_{(x,y)}$ . However, as the preceding paragraph has shown, the converse--equality of the two betas--does not imply $\rho=0$ . Therefore, the only conclusion that can be supported is that if the coefficient of $x$ changes in the two regressions, then $x$ and $y$ must be correlated. (The textbooks usually state the contrpositive: when $x$ and $y$ are not correlated, adding $y$ to the model will not change the coefficient of $x$ .) However, the degree of correlation can be any nonzero value between $-1$ and $1$ inclusive, so we cannot draw any conclusion about the strength of the correlation.
