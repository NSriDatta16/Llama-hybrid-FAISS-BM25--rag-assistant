[site]: crossvalidated
[post_id]: 362420
[parent_id]: 118450
[tags]: 
In the case that you are interested in (1) a perceptron classifier and (2) the data are not linearly separable, you have specified the motivation of the so-called "soft-margin" SVM. Briefly, we seek a solution $$ \min_{w,b}~\lambda||w||^2_2 + \frac{1}{n}\sum_{i=1}^n \max \left\{ 0, 1 - y_i\left(w^\top x_i -b\right)\right\} $$ for $y_i \in\{-1,1\}$ for parameters $(w,b)\in \mathbb{R}^{p+1}$ where you have $p$ features. Configurations $(w,b)$ which put a sample on the "wrong side" of the hyperplane are penalized proportional to the distance from the hyper plane. Notably, if $\lambda$ is set small enough, the solution is very similar to that of the hard-margin SVM, otherwise known as a perceptron, because $\lambda ||w||_2^2$ becomes negligible, although the learning procedure becomes convex , whether or not linear separation is possible. More information can be found in Elements of Statistical Learning , chapter 12.
