[site]: crossvalidated
[post_id]: 609980
[parent_id]: 
[tags]: 
Calculating KL divergence with entropy and cross entropy for VAEs

When looking at implementations of VAE's online, specifically the KL divergence loss, the formula used is: $$ KL\hspace{1mm} Loss = -\frac{1}{2}(1+\log{\sigma^2}-\mu^2-\sigma^2) $$ or some variation of it. In the code accompanying the paper I am currently reading, the KL loss is calculated using entropy and cross entropy under the equality: $$ KL\hspace{1mm}Loss = Cross\hspace{1mm}Entropy - Entropy $$ with: $$ Cross\hspace{1mm}Entropy = \frac{1}{2}(\mu^2 + \sigma^2) + \log{\sqrt{2\pi}} $$ and $$ Entropy = \frac{1}{2}(\log{\sigma^2}+\log{2\pi e}) $$ This confuses me greatly, as subtracting the entropy from the cross entropy does not yield the conventional formula for the KL Loss mentioned above. Where do these entropy and cross entropy formulas come from and why do they not satisfy the KL divergence equality? Are there assumptions being made here that I am unaware of?
