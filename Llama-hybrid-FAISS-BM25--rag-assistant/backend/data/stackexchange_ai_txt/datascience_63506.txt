[site]: datascience
[post_id]: 63506
[parent_id]: 
[tags]: 
Neural Network from scratch: cost increasing over epochs

I'm trying to design a neural network from scratch. After training my neural network, I make a plot of the cost vs epochs, which I would expect to decrease throughout the runtime of the NN training, and eventually approach a horizontal asymptote (if everything is done correctly). However, when I plot the cost, it decreases down to a point, and then increases endlessly. I must be doing something wrong, perhaps in the back propagation step? I'm having trouble spotting my error. Here is a section of my neural_net class which contains all the functions that I call in the train function. class neural_net: def __init__(self, train_data, valid_data, test_data, train_labels, valid_labels, test_labels, input_dim, output_dim, neuron_nums, seed, output_function, learning_rate, epochs, et, ect): self.train_data = train_data self.valid_data = valid_data self.test_data = test_data self.train_labels = train_labels self.valid_labels = valid_labels self.test_labels = test_labels self.input_dim = input_dim self.output_dim = output_dim self.neuron_nums = neuron_nums self.seed = seed self.output_function = output_function self.learning_rate = learning_rate self.epochs = epochs self.error_threshold = et self.error_ch_thres = ect def seed_model(self): w1, w_out, b1, b_out = np.zeros(4) np.random.seed(self.seed) # now initialize the weight matrices to have random numbers that are small # and initialize the bias vectors to have zeros w1 = np.random.randn(self.neuron_nums, input_dim)*0.01 b1 = np.zeros(shape = (self.neuron_nums, 1)) w_out = np.random.randn(output_dim, self.neuron_nums)*0.01 b_out = np.zeros(shape = (output_dim, 1)) return w1, w_out, b1, b_out def forward_propagate(self, w1, w_out, b1, b_out, data_type): if data_type == "train": X = self.train_data labels = self.train_labels if data_type == "validate": X = self.valid_data labels = self.valid_labels if data_type == "test": X = self.test_data labels = self.test_labels X = np.transpose(X) z1, a1, z_out, output_prediction, cost = np.zeros(5) # use sigmoid for all hidden layers and then let user decide what output function is z1 = np.dot(w1,X) + b1 a1 = sigmoid(z1) z_out = np.dot(w_out,a1) + b_out if self.output_function == 'sigmoid': output_prediction = sigmoid(z_out) if self.output_function == 'linear': output_prediction = linear(z_out) if self.output_function == 'softmax': output_prediction = softmax(z_out) return z1, a1, z_out, output_prediction def compute_cost(self, output_prediction): labels = self.train_labels sq_errs = np.zeros(len(labels)) for i in range(len(labels)): sq_errs[i] = (output_prediction.T[i] - labels[i])**2 cost = np.sum(sq_errs) / len(labels) return cost def back_propagate(self, a1, outout_prediction, w1, w_out): X = self.train_data labels = self.train_labels l = len(X) X = np.transpose(X) dz_out = np.zeros_like(output_prediction.T) for i in range(len(output_prediction.T)): dz_out[i] = (output_prediction.T[i] - train_labels[i]) dz_out = np.transpose(dz_out) dw_out = (1/l) * np.dot(dz_out, a1.T) db_out = (1/l) * np.sum(dz_out, axis = 1, keepdims = True) dz1 = np.multiply(np.dot(w_out.T, dz_out), 1-np.power(a1,2)) dw1 = (1/l) * np.dot(dz1, X.T) db1 = (1/l) * np.sum(dz1, axis = 1, keepdims = True) return dw_out, db_out, dw1, db1 def update_parameters(self, w1, w_out, b1, b_out, dw1, dw_out, db1, db_out): lr = self.learning_rate w1 = w1 - lr*dw1 b1 = b1 - lr*db1 w_out = w_out - lr*dw_out b_out = b_out - lr*db_out return w1, w_out, b1, b_out def train(self): ep = 0 w1, w_out, b1, b_out = neural_net.seed_model(self) cost = 10000000 costs = [0] cost_change = 1000000 # create initial w's and b's with seed_model(), then loop the following until any of the stop condition are met: # forward_propagate to predict the output with the current model, evaluate the cost and cost change, # then back_propagate to get dw's and db's, and finally update the w's and b's with update_parameters while cost > self.error_threshold and ep self.error_ch_thres: z1, a1, z_out, output_prediction= neural_net.forward_propagate(self,w1, w_out, b1, b_out, data_type = "train") cost = NN.compute_cost(output_prediction) costs.append(cost) cost_change = np.absolute(costs[-1] - costs[-2]) dw_out, db_out, dw1, db1 = neural_net.back_propagate(self,a1, output_prediction, w1, w_out) w1, w_out, b1, b_out = neural_net.update_parameters(self,w1, w_out, b1, b_out, dw1, dw_out, db1, db_out) ep += 1 # create plot of cost vs epochs so we can visualize when the network should stop training plt.plot(costs[1:]) plt.title("cost vs epochs") return w1, w_out, b1, b_out For more clarity, my training dataset has 57 features and 2300 examples. Any help would be greatly appreciated. I hope the code is not too junky, I'm not a master of python.
