[site]: datascience
[post_id]: 72658
[parent_id]: 72153
[tags]: 
Two basic mistakes: No intercept Sorted data without shuffling In addition: rather small dataset. The intercept gives significant boost in the expressive capacity of a logistic regression, especially in problems with only one feature, like here. There is a reason why its default setting is True - it is one of these defaults with which you'd better not mix, unless you know exactly what you are doing. Omitting the intercept in the simple univariate case like here is easy to picture: it forces the regression line to pass through the origin (0, 0) - a huge constraint. Shuffling is especially important in such cases of artificial datasets, where at some point the values are sorted (as you do here). The reason is, while ML models can be very good at interpolating, they are extremely bad at extrapolating (predicting values outside their training range); and with sorted data, each of your validation CV folds tries to predict with data that are outside the respective training fold (and, unsurprisingly, does not do a good job). So, just shuffling the data, and fitting with these data and fit_intercept = True , we get: from sklearn.utils import shuffle x_s, y_s = shuffle(x_data, y_data, random_state=0) regularized_logistic_regression_model = LogisticRegressionCV( Cs = np.array([10**-8, 10**-4, 1, 10**4, 10**8]), fit_intercept = True, cv = 3) regularized_logistic_regression_model.fit(x_s.reshape(-1, 1), y_s) print(regularized_logistic_regression_model.C_) print(regularized_logistic_regression_model.coef_) print(regularized_logistic_regression_model.scores_) Results: [10000.] [[4.57770177]] {1: array([[0.61764706, 0.61764706, 0.70588235, 0.67647059, 0.67647059], [0.60606061, 0.60606061, 0.78787879, 0.84848485, 0.84848485], [0.60606061, 0.60606061, 0.57575758, 0.72727273, 0.72727273]])} Already much more sensible than the ones you report. Adding a little more data (300 samples instead of just 100), gives [1.] [[3.57243675]] {1: array([[0.52, 0.52, 0.72, 0.72, 0.72], [0.52, 0.52, 0.68, 0.67, 0.67], [0.51, 0.51, 0.67, 0.66, 0.66]])} Final note: although shuffling is a highly recommended practice in general, here (artificial random data, by definition) you could avoid the need for it if you leave the initial data as-is (i.e. without sorting them): x_data = np.random.normal(0, 0.3, 100) # no sorting I'm leaving the verification of this as an exercise.
