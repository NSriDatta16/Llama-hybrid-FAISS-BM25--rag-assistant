[site]: crossvalidated
[post_id]: 182554
[parent_id]: 182432
[tags]: 
When doing a regression with multiple regressors, you consider that each of your independent variable has a direct impact on the dependend variable, controlling for other variables . When assessing the significance of a model, you have two choices: Univariate significance test Multivariate significance test Here, you use the univariate test and conclude that one is not a good predictor (t-test). Also, since you only have two variables, your multivariate test amounts to testing "Are all my coefficients zero ?" through a F-test (I suppose, since it's a linear model) and the answer of your test is "No, there is at least one coefficient that is not zero." I generally advise to keep variables in the model even when they are not significant (well, if the p-value is 0.99, I safely remove it) and when you have enough degrees of freedom because they can reduce estimation bias in estimating the coefficient. Does your coefficient in front of temperature vary greatly when controlling for precipitations ? Are the coefficients statistically different from each other with/without precipitation ? The point underlying the previous paragraph is this: what is better ? Having a biased estimator that is precisely estimated, or an unbiased estimator that is less precisely estimated ? I favor the second. To sum up: if adding new regressors solves a bias problem, keep them even though they are not significant. If adding a new regressor does not add anything new (Log-likelihood ratio test), then discard it. My gut tells me that it solves a bias problem, given that the correlation between those two variables is sizeable.
