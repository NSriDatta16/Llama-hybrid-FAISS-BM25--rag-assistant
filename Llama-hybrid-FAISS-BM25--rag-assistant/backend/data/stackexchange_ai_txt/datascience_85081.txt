[site]: datascience
[post_id]: 85081
[parent_id]: 85079
[tags]: 
The idea of mean squared error is find the mean value of the squared errors. Therefore, you divide by the number of squared errors you add up, which is the number of samples. In more inference-oriented applications (e.g. linear regression and ordinary least squares), you may see the denominator given as $n-k$ or $n-p$ , where $k$ and $p$ and the number of parameters in the regression. This has to do with how MSE is an unbiased estimator of conditional variance, an issue unlikely to interest you in neural networks that do pure predictive modeling, but I do not want you to get confused about whatâ€™s going on when you see that.
