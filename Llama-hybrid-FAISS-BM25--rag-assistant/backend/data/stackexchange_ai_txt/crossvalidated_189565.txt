[site]: crossvalidated
[post_id]: 189565
[parent_id]: 189541
[tags]: 
The universal approximation theorem of neural networks states that a neural network with a bounded, continuous activation functions and given enough hidden units in only one hidden layer can approximate any function. So, in theory you should be able to get the correct output, but you do not know in advance what should be the right number of hidden units for your problem. So, you may not necessarily need to choose a number of hidden units less than the number of your input units (however that's usually the case in practice) Also, what I'm seeing is your network is a Perceptron network. If that's the case, then your network is only able to learn linearly-separable data points and you cannot learn complex functions. Sources Approximation Capabilities of Multilayer Feedforward Networks Approximation With Artificial Neural Networks
