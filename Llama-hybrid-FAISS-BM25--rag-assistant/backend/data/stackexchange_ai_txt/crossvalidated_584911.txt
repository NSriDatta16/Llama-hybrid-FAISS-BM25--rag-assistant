[site]: crossvalidated
[post_id]: 584911
[parent_id]: 584685
[tags]: 
Turning my comment into an answer. This practice is called parameter tying. They’ve set $U$ and $V$ to be equal to each other as a constraint, which regularizes the model. The information about token relatedness is assumed to carry over, at least to some degree, between the source and target—especially for related languages. You ask about whether this dates back to "the first RNN-LMs". Not quite. A paper that describes the technique in the context of neural sequence models is Press and Wolf (2017) —though it's older than that; machine translation folks had been using it in their toolkits since well before Press and Wolf's paper. You also ask about the transpose—that's so that the matrices are conformant. The original matrix $U$ maps from vocabulary to a latent space; the $V := U^{\intercal}$ matrix maps back from a latent space to the vocabulary. In terms of implementation and how widespread it is, it’s offered as a flag in the popular seq2seq libraries.
