[site]: crossvalidated
[post_id]: 633577
[parent_id]: 633546
[tags]: 
The problem isn't learnable because of how the data are generated. Without loss of generality, we can consider vectors with 2 elements. Then the values of each vector are given by a single scalar, an angle $\theta$ and the vectors are always constrained to have unit length (which converts cosine similarity to a dot-product). These simplifications make it easier to examine what the model is doing. The model parameters are given by the predicate vector is $p = [\cos \theta_p, \sin \theta_p]$ , and likewise the vector true $t$ and false $f$ . A single input to the model is $x = [\cos \theta_x, \sin \theta_x]$ , and the target is $y = [\cos \theta_y, \sin \theta_y]$ . First, we compute the cosine similarity of $p$ and $x$ : $$\begin{align} p^T x &= \cos \theta_p \cos \theta_x + \sin \theta_p \sin \theta_x \\ &= \cos(\theta_p - \theta_x) \\ &= \cos \Delta \\ \end{align}$$ So $p$ and $x$ are most similar when the angle between them is 0. Denote $m$ the result of the sums of the two Einstein summation notation expressions used in the code. We know $p^T x$ is a scalar, so we're just summing scalar multiples of the two vectors. $$\begin{align} m &= \begin{bmatrix} \cos \theta_t \cos \Delta + \cos \theta_f (1 - \cos \Delta) \\ \sin \theta_t \cos \Delta + \sin \theta_f (1 - \cos \Delta) \end{bmatrix} \\ \end{align}$$ The general expression for $m$ is handy to have. We'll consider the two special cases where $\cos \Delta = 0$ and $\cos \Delta = 1$ because they simplify things. These are the two cases that OP has expressed as the ultimate objective: map the input to $t$ when the input has a small angle to $p$ ; otherwise, map it to $f$ . $$\begin{align} m_{\cos \Delta = 1} &= \begin{bmatrix} \cos \theta_t\\ \sin \theta_t \end{bmatrix} \\ m_{\cos \Delta = 0} &= \begin{bmatrix} \cos \theta_f \\ \sin \theta_f \end{bmatrix} \end{align}$$ The loss is computed as $1 - m^T y$ where $y$ is the target. This loss is maximized at 2 and minimized at 0. In your example, all of the true cases have a single input vector and a single output vector. However, all of the inputs for the false cases are random, and all of the targets have the same vector. This is where the problem arises. When the input is a false case, then the desired result is that $p^T x = \cos \Delta = 0$ . This only arises when $p$ and $x$ are orthogonal. In this case, there are only 2 such vectors. But the randomized way that the inputs to the false case are generated means that these inputs are unlikely to correspond to vectors orthogonal to $p$ . So we have an impossible circumstance: we can't fix $p$ at a value that will minimize the loss on any 2 distinct false examples $x$ â€” unless those vectors are orthogonal to $p$ . In the code, the false examples are generated completely at random, so the normalized vectors are scattered all over the surface of the unit sphere. In a large enough sample, some of these samples will even be close to the $x$ for the true examples. Moreover, if we choose a $p$ that minimizes the loss on the true examples, then that will only minimize the loss on the false examples for the cases where the inputs to the false examples are orthogonal to it. This is unlikely to occur for any of your false examples, and even if one false example has this occur, it will be "averaged away" by all of the false examples for which it does not occur. You've stated in comments that if we rescale the vectors, then there are infinitely many orthogonal vectors. This is of no consequence for the learnability of the problem. Your network and loss function are entirely framed in terms of cosine similarity, so all that matters is the angle between the vectors. However, there are lots of kinds of neural networks designed to map inputs that belong to the same class to the same location in some latent space; one example is triplet-loss . They still require the data to have some learnable signal, though.
