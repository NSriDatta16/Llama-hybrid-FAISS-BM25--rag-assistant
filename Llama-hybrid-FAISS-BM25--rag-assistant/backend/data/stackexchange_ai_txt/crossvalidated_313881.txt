[site]: crossvalidated
[post_id]: 313881
[parent_id]: 313876
[tags]: 
This is not unusual for reinforcement learning and does not indicate anything is wrong. As the agent gets better at playing, estimating the reward does get more difficult (because it's no longer always 0). In addition, as the reward gets higher, and the average episode length gets longer, the amount of variance in the reward can also get larger, so it's challenging even to prevent the loss from increasing. A third factor, which you mentioned, is that the constantly changing poses a "moving-target" problem for the Q-network. My guess for why this doesn't happen in Pong is that Pong is a much simpler game and easier to predict than Space Invaders.
