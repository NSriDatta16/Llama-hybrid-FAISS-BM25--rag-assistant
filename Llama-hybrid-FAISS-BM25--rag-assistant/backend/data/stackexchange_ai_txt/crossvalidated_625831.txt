[site]: crossvalidated
[post_id]: 625831
[parent_id]: 
[tags]: 
Why do SAC and TD3 use multiple critic networks as opposed to single network with multiple outputs?

Q-function approximators based on neural networks tend to overestimate the Q-function. Accordingly, reinforcement learning algorithms such as Soft Actor-Critic (SAC) and Twin Delayed DDPG (TD3) use two separate critic networks which have single scalar output per sample and take minimum of their predictions to compensate for the inherent overestimation. Why do they use separate networks with scalar output as opposed to a single network with vector output per sample, where, as a last step, minimum is taken across this vector? Isn't this equivalent, but more efficient?
