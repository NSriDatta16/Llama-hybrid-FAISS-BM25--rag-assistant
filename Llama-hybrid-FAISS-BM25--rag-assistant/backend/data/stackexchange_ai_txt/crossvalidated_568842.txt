[site]: crossvalidated
[post_id]: 568842
[parent_id]: 411167
[tags]: 
A lot of the theoretical underpinnings of the SVM are based on the linear maximum margin classifier. However, the advantage of a kernel is that the SVM can be viewed as a linear maximum margin classifier that is constructed in a feature space that is implicitly defined by the kernel. As imposing a fixed kernel is equivalent to a fixed transformation of the data, which is equivalent to just solving some other fixed classification problem. This means that any theory that holds for a linear classifier also holds for a fixed kernel. However, in most practical cases, we don't use a fixed kernel. In general, the kernel has some hyper-parameters that we also learn from the data, e.g. via cross-validation. As soon as we tune the kernel parameters, we have immediately invalidated a lot of the theory on which it is based. Theoretical results become much harder to obtain as soon as you include learning the kernel. Having said which (IMHO) the main reason that the SVM works so well is that it really encourages us to think about regularisation and doing something (SRM) about over-fitting. That doesn't really rely too heavily on the theoretical justification.
