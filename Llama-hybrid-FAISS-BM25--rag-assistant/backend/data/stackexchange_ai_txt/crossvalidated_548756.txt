[site]: crossvalidated
[post_id]: 548756
[parent_id]: 
[tags]: 
How is adding noise to training data equivalent to regularization?

I've noticed that some people argue that adding noise to training data equivalent to regularizing our predictor parameters. How is this the case? Some of the examples listed on SE discussing this topic focus more on e.g. LSTMs and SVMs, but can we do this for simpler models like a multiple linear regression? How will it affect our parameters' confidence intervals? Will there be any differences in effects choosing between the various types of white noise, e.g. Gaussian vs uniform white noise?
