[site]: crossvalidated
[post_id]: 78278
[parent_id]: 76866
[tags]: 
There is no "k-means algorithm". There is MacQueens algorithm for k-means, the Lloyd/Forgy algorithm for k-means, the Hartigan-Wong method, ... There also isn't "the" EM-algorithm. It is a general scheme of repeatedly expecting the likelihoods and then maximizing the model. The most popular variant of EM is also known as "Gaussian Mixture Modeling" (GMM), where the model are multivariate Gaussian distributions. One can consider Lloyds algorithm to consist of two steps: the E-step, where each object is assigned to the centroid such that it is assigned to the most likely cluster. the M-step, where the model (=centroids) are recomputed (= least squares optimization). ... iterating these two steps, as done by Lloyd, makes this effectively an instance of the general EM scheme. It differs from GMM that: it uses hard partitioning, i.e. each object is assigned to exactly one cluster the model are centroids only, no covariances or variances are taken into account
