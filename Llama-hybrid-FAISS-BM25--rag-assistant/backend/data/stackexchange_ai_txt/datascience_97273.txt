[site]: datascience
[post_id]: 97273
[parent_id]: 97256
[tags]: 
The term bias is, to my knowledge, not related to ethics in the context of ML. Instead, it usually refers either to the bias–variance tradeoff or to a learnable parameter of a model, e.g. bias in a neural network . (Note that in statistics the term is commonly used to refer to biased estimators which is related to but more general than its use with regards to the bias-variance-tradeoff.) In contrast, when making a connection to ethics (aka fairness) you most likely use the term in a more general way or how it commonly used in science . (But it is important to note that this is not what bias refers to in ML.) However, even when applying the general scientific definition of bias its relation to ethics/fairness in ML is limited: Let's assume you apply a model to classify images of chairs and tables. If your dataset contains 99.999% chairs the naïve classifier which always predicts chairs would perform very well in terms of accuracy. (side note: let's ignore the fact that "very good" is task-specific and accuracy might not be the best metric here) This model would be very biased towards chairs in the general meaning of the term. However, we would not consider this an issue of ethics or fairness (unless you're a big fan of tables). Now let's assume that you have a model applied in a self-driving car task. One could think of a situation where the model needs to decide to either run over a group of two people or a group of three people. The decision the model has to be made is an ethical ML problem. (probably it's actually an AI issue, i.e. involves other sub-fields of AI too since self-driving cars usually apply techniques from multiple AI-fields and not just ML) Clearly, this is an ethical problem independent of any bias, i.e. you can have ethical considerations in ML without necessarily involving bias. As the above two examples show you can have ethical issues in ML without bias (in the general sense) and vice versa.
