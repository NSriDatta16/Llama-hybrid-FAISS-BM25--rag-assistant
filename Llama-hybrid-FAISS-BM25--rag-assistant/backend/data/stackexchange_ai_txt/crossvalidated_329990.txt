[site]: crossvalidated
[post_id]: 329990
[parent_id]: 
[tags]: 
Goodness of Fit Measure: Weighted Relative Least Squared

I'm looking for an appropriate goodness of Fit measure to fit the movement of my model to some empirical time series. (not sure if this is the right community or Data science would be better for this). Let's say I have three empirical time series $i$ each with ten values for ten different periods $t$. The value of a time series at a given period is $y_{i,t}$. My model can generate three time series called $x$ with values $x_{t,i}$. Now I want to reduce the difference between my model and the time series by minimizing a function called $\omega$. Unfortunately the variance and the magnitude of the time series is not the same, i. e. one series moves shows little variance between the values 1 and -1 and another time series has a high variance between 20 and -20. Based on the different of the time series I think something that reduces the relative difference between the empirical and the estimated time series would be appropriate. Namely relative least square which can be computed for one time series at a time $t$ with $((y_t -x_t)/y_t)^2$ . But the other problem is the variance. I need a measurement that considers that the time series with the higher variance should get a smaller weight in the minimizing process. One way to incorporate this would be to consider the standard deviation of the time series. But somehow this does not work out. Thank you. p.s. sometimes when I try to fit the model to the data all the curves look good while one curve is totally flat without any movement. So something is not right with the weighting I use.
