[site]: crossvalidated
[post_id]: 199225
[parent_id]: 78321
[tags]: 
I am late to this party, but I was playing with the concepts of tc-idf (I want to emphasize the word 'concept' because I didn't follow any books for the actual calculations; so they may be somewhat off, and definitely more easily carried out with packages such as {tm: Text Mining Package} , as mentioned), and I think what I got may be related to this question, or, in any event, this may be a good place to post it. SET-UP: I have a corpus of 5 long paragraphs taken from printed media, text 1 through 5 such as The New York Times . Allegedly, it is a very small "body", a tiny library, so to speak, but the entries in this "digital" library are not random: The first and fifth entries deal with football (or 'soccer' for 'social club' (?) around here), and more specifically about the greatest team today. So, for instance, text 1 begins as... "Over the past nine years, Messi has led F.C. Barcelona to national and international titles while breaking individual records in ways that seem otherworldly..." Very nice! On the other hand you would definitely want to skip the contents in the three entries in between. Here's an example ( text 2 ): "In the span of a few hours across Texas, Mr. Rubio suggested that Mr. Trump had urinated in his trousers and used illegal immigrants to tap out his unceasing Twitter messages..." So what to do to avoid at all cost "surfing" from the text 1 to text 2 , while continuing to rejoice in the literature about almighty Barcelona F.C. in text 5 ? TC-IDF: I isolated the words in every text into long vectors. Then counted the frequency of each word, creating five vectors (one for each text ) in which only the words encountered in the corresponding text were counted - all the other words, belonging to other text s, were valued at zero. In the first snippet of text 1 , for instance, its vector would have a count of 1 for the word "Messi", while "Trump" would have 0. This was the tc part. The idf part was also calculated separately for each text , and resulted in 5 "vectors" (I think I treated them as data frames), containing the logarithmic transformations of the counts of documents (sadly, just from zero to five, given our small library) containing a given word as in: $\log\left(\frac{\text{No. documents}}{1\, +\, \text{No. docs containing a word}}\right)$. The number of documents is 5. Here comes the part that may answer the OP: for each idf calculation, the text under consideration was excluded from the tally . But if a word appeared in all documents, its idf was still $0$ thanks to the $1$ in the denominator - e.g. the word "the" had importance 0, because it was present in all text s. The entry-wise multiplication of $\text{tc} \times \text{idf}$ for every text was the importance of every word for each one of the library items - locally prevalent, globally rare words . COMPARISONS: Now it was just a matter of performing dot products among these "vectors of word importance". Predictably, the dot product of text 1 with text 5 was 13.42645 , while text 1 v. text2 was only 2.511799 . The clunky R code (nothing to imitate) is here . Again, this is a very rudimentary simulation, but I think it is very graphic.
