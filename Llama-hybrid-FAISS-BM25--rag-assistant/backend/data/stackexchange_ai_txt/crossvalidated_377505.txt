[site]: crossvalidated
[post_id]: 377505
[parent_id]: 
[tags]: 
Likelihood of one datapoint given $k$ models

Introduction I'm currently facing a problem where I'm constraining a set of (physical) parameters $\theta_k$ with $k\in [1,2,...,K]$ via several independent datasets. One of those datasets, however, has only one datapoint , $Y$ , which is a measurement of a physical variable. Given a realization of this datapoint, $y$ , this puts a constraint on the parameters $\theta_k$ via a (known) function $f(\theta_k)$ . The big question here is the distribution of $Y | \theta_1, \theta_2, ....,\theta_K$ , i.e., the implied likelihood. If I only had one parameter, $\theta_k$ , then the probabilsitic model for $Y$ would be easy to derive in my case; it would be of the form $Y \sim f(\theta_k) + \epsilon$ , with $\epsilon \sim N(0,\sigma^2)$ with known $\sigma^2$ --- the error on the measurement of the realization $y$ of the variable $Y$ , which I'm assuming is gaussian (which is a very good assumption in my case, by the way). As we all know, the likelihood of this model would be easily written as $\displaystyle p(Y|\theta_k) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(\frac{-(y-f(\theta_k))^2}{2\sigma^2}\right).\ \ \ \ \ \ \ \textrm{(1)}$ However, in my scenario, this datapoint has to simultaneously satisfy $k$ probabilistic models for the $\theta_k$ parameters simultaneously, as explained in the first paragraph: it simultaneously constrains the $\theta_k$ parameters. My approach I've comed up with many approaches to the problem, but the one that seems to be the most sound is the following. Our aim is to obtain $p(Y|\theta_1,\theta_2,...) = \frac{p(\theta_1,\theta_2,...|Y) p(Y)}{p(\theta_1,\theta_2,...)},\ \ \ \ \ (2)$ where the right-hand side follows by Bayes Theorem. It turns out that $\theta_1, \theta_2,...$ are independent. This implies that $p(\theta_1,\theta_2,...)=\Pi_{k=1}^K p(\theta_k)$ and that $p(\theta_1,\theta_2,...|Y) = \Pi_{k=1}^K p(\theta_k|Y)$ . Replacing this in (2) gives: $p(Y|\theta_1,\theta_2,...) = p(Y)\frac{\Pi_{k=1}^K p(\theta_k|Y)}{\Pi_{k=1}^K p(\theta_k)}.\ \ \ \ \ (3)$ Now, from Bayes theorem, it follows that $p(\theta_k|Y) = p(Y|\theta_k)p(\theta_k)/p(Y)$ . Replacing this in (3) we get: $p(Y|\theta_1,\theta_2,...) = p(Y)\frac{\Pi_{k=1}^K p(Y|\theta_k)p(\theta_k)/p(Y)}{\Pi_{k=1}^K p(\theta_k)} = \left[p(Y)\right]^{1-K} \Pi_{k=1}^K p(Y|\theta_k),\ \ \ \ (5)$ where $p(Y|\theta_k)$ is given by equation (1) above. The question Does this sound as good as it does to me? What I see is that this is equivalent to a multiplication of the likelihoods of the "individual" likelihoods (the ones in equation (1)), but which are "penalized" by the $\left[p(Y)\right]^{1-K}$ term, which I suppose enters because we are not using $K$ datapoints (which would be what it would be implied if this term didn't exist) but, rather, our $k$ functions are being constrained by this one datapoint. However, I have never seen a likelihood like the one implied in equation (5); however, I wouldn't be surprised it exists somewhere in the literature. Pointers to similar cases would be well received as well!
