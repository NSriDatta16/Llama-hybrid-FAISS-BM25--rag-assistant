[site]: crossvalidated
[post_id]: 550921
[parent_id]: 153962
[tags]: 
This is adding prior knowledge to your model, in a somewhat Bayesian sense. Per the comments, not everyone would call this "regularization". I would, because it constrains the parameters to not vary wildly all over the place, just in a different way than (say) L1 or L2 regularization. When does this make sense? Anytime we are sure that a regression coefficient should be positive, but because of the uncertainties involved, an unconstrained model might fit a negative one. For example: As seanv507 writes , if we want to estimate item prices from total costs and item amounts, it makes sense to impose positive prices. I do retail sales forecasting. We typically constrain promotion effects to be positive, and price effects to be negative. Similarly, you may have cannibalization effects, which are typically very noisy and therefore prime candidates for any kind of regularization. If there is a promotion on Coca-Cola, we would expect Pepsi sales to go down, so in a model for Pepsi sales that includes promotions on Coca-Cola, we would constrain these coefficients to be negative (or at least non-positive). Yes, often you can make a case that the opposite sign of the coefficient could be possible . However, usually this involves rather tortuous logic, and you often find that constraining parameters makes for better models and predictions in the vast majority of cases, even if you will always find isolated cases that would have profited from unconstrained parameters.
