[site]: datascience
[post_id]: 122279
[parent_id]: 122274
[tags]: 
With this kind of low dimensional embeddings, you confirm visually some kind of relationship between the data points that you suspected by intuition. For instance, the following graph taken from the article From bilingual to multilingual neural-based machine translation by incremental training shows the UMAP 2D plot of the representations computed by a neural network for sentences in different languages. The plot shows that the different languages form clusters that are distant from each other, which makes us think that the neural network is not able to properly learn the same representation space for all language. Another typical example is with MNIST, the handwritten digit dataset. Here you can see plots of the bottleneck representation of an autoencoder and from PCA from this blog post where each point is the 2D representation of a digit image from the dataset and its color is its label (i.e. which number it is): In the figure, we can observe that, while PCA hardly separates the different numbers, the autoencoder indeed separates them.
