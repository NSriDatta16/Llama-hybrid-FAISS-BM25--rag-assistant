[site]: datascience
[post_id]: 13152
[parent_id]: 13057
[tags]: 
Daniel, What you did goes under the name of "oversampling." There is a sample of some "real" population, and you replace it with a sample from a "manufactured" population. The problem that makes sense in application is the estimation of $$P_r(Y=1|X) = \text{probability of response=1 in the $\mathbf{real}$ population given the predictor $X$} $$ but by using an oversample you are estimating $$P_m(Y=1|X) = \text{probability of response=1 in the $\mathbf{manufactured}$ population given the predictor $X$}$$ The two probabilities are related. I'll worked the details. I'll pretend the predicor $X$ is discrete. If $X$ takes numerical values one has to replace some probabilities by probability densities. $$\dots\dots\dots$$ To simplify the notation, let $\pi_1 = P_r(Y=1)$ and $\mu_1 = P_m(Y=1)$ be the probabilities of response in the real and manufactured populations, let $$ L_r = \frac{P_r(X=x|Y=1)}{P_r(X=x|Y=0)} = \frac{\frac{P_r(Y=1|X=x)}{P_r(Y=0|X=x)}}{\frac{\pi_1}{1-\pi_1} }$$ be the odds ratio of $Y=1$, i.e.: the ratio of the odds among cases with $X=x$ and the odds in the general $\mathbf{real}$ population. Finally, let $L_m$ be the corresponding ratio in the $\mathbf{manufactured}$ population. By Bayes' Theorem: $$ P_r(Y=1|X=x) = \frac{P_r(Y=1,X=x)}{P_r(X=x)} = \\ =\frac{P_r(X=x|Y=1)\space \pi_1}{P_r(X=x|Y=1)\space\pi_1 + P_r(X=x|Y=0)\space (1 - \pi_1)} = \\ =\frac{L_r\space \pi_1}{L_r\space\pi_1 + \space (1 - \pi_1)} \tag{1} $$ In a similar way, we get an analogous result for the manufactured population: $$ P_m(Y=1|X=x) = \frac{L_m\space \mu_1}{L_m\space\mu_1 + \space (1 - \mu_1)} \tag{2} $$ Since the manufactured sample is a random sample, stratified by $Y$, the conditional distribution of X within responders is the same as in the real population. Same as for non responders, i.e.: $$ P_r(X=x|Y=j) = P_r(X=x|Y=j) $$ for $j=0,1.$ If the sample stratified by values of Y was anything other than random sample these would not be true. It follows that $\boxed{ L_r = L_m }$. Next we solve for $L_m$ in terms of $P_m(Y=1|X)$ from (2) and replace in (1). $$\dots\dots\dots$$ $\mathbf{Digression}$: Here is my easy way to carry the steps, without mess. Two non-zero vectors $\mathbf{v_1}$, $\mathbf{v_2}$ are parallel iff there is $\lambda \ne 0$ such that $\mathbf{v_1}=\lambda \mathbf{v_2}.$ Below I will use this idea, and I will not care about the exact value of $\lambda$, so I will be using "$\lambda$" as a short-hand for "$\mathbf{\text{some non-zero mumber}}$." $\mathbf{\text{End Digression}}$: $$\dots\dots\dots$$ The easy way to solve is to observed that for non-zero messy values of $\lambda$ (not the same in each occurrence!) one has: $$ \begin{bmatrix} P_r(Y=1|X) \\ 1 \\ \end{bmatrix} = \lambda \begin{bmatrix} \pi_1 &0\\ \pi_1 &1-\pi_1 \end{bmatrix} \begin{bmatrix} L_r\\ 1 \end{bmatrix} , $$ and $$ \begin{bmatrix} P_m(Y=1|X) \\ 1 \\ \end{bmatrix} = \lambda \begin{bmatrix} \mu_1 &0 \\ \mu_1 &1-\mu_1 \end{bmatrix} \begin{bmatrix} L_m\\ 1 \end{bmatrix} . $$ Therefore, $$ \begin{bmatrix} L_m\\ 1 \end{bmatrix} = \lambda \begin{bmatrix} \mu_1 &0 \\ \mu_1 &1-\mu_1 \end{bmatrix}^{-1} \begin{bmatrix} P_m(Y=1|X) \\ 1 \\ \end{bmatrix} , $$ and so (remember that here $\lambda$ stands for "some non-zero number") $ \space \begin{bmatrix} P_r \\ 1 \end{bmatrix} = \lambda \begin{bmatrix} \pi_1 &0 \\ \pi_1 &1-\pi_1 \end{bmatrix} \begin{bmatrix} L_r \\ 1 \end{bmatrix} = \\ \text{ }=\lambda \begin{bmatrix} \pi_1 &0 \\ \pi_1 &1-\pi_1 \end{bmatrix} \begin{bmatrix} \mu_1 &0 \\ \mu_1 &1-\mu_1 \end{bmatrix}^{-1} \begin{bmatrix} P_m \\ 1 \\ \end{bmatrix} = \\ \text{ }=\lambda \begin{bmatrix} \pi_1 (1- \mu_1) &0 \\ \pi_1 - \mu_1 & \mu_1 (1- \pi_1) \end{bmatrix} \begin{bmatrix} P_m \\ 1 \\ \end{bmatrix} = \lambda \begin{bmatrix} \pi_1 (1-\mu_1) P_m \\ (\pi_1 - \mu_1) \; P_m + \mu_1 (1- \pi_1) \end{bmatrix}. $ Thus, $$ P_r = \frac{\pi_1 (1- \mu_1) P_m}{(\pi_1 - \mu_1) \; P_m + \mu_1 (1- \pi_1) } $$ $$\dots\dots\dots$$ Example: Let's work the details of a Binomial model, $$P_m(Y=1|X) = \frac{e^{\beta_0 + \beta X}}{1+e^{\beta_0 + \beta X}} $$ or in the "where $\lambda$ is some non-zero scalar" notation (I would not had digressed before if I did not had ulterior motive.. :) ): $$ \begin{bmatrix} P_m \\ 1 \\ \end{bmatrix} = \lambda \begin{bmatrix} e^{\beta_0 + \beta X} \\ 1 + e^{\beta_0 + \beta X} \end{bmatrix} $$ What is the implied model in the real population? $ \space \begin{bmatrix} P_r \\ 1 \end{bmatrix} = \lambda \begin{bmatrix} \pi_1 (1- \mu_1) &0 \\ \pi_1 - \mu_1 &\mu_1 (1- \pi_1) \end{bmatrix} \begin{bmatrix} P_m \\ 1 \end{bmatrix} = $ $\;$ $ = \lambda \begin{bmatrix} \pi_1 (1- \mu_1) &0 \\ \pi_1 - \mu_1 &\mu_1 (1- \pi_1) \end{bmatrix} \begin{bmatrix} e^{\beta_0 + \beta X} \\ 1 + e^{\beta_0 + \beta X} \end{bmatrix}= $ $\;$ $ = \lambda \begin{bmatrix} \pi_1 (1- \mu_1) e^{\beta_0 + \beta X} \\ \pi_1 (1- \mu_1) e^{\beta_0 + \beta X} + \mu_1 (1- \pi_1) \end{bmatrix} = \lambda \begin{bmatrix} \frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)} e^{\beta_0 + \beta X} \\ 1 + \frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)} e^{\beta_0 + \beta X} \end{bmatrix} .$ If we let $\tau = \ln(\frac{\pi_1 (1- \mu_1)}{\mu_1 (1- \pi_1)})$, we can absorve this constant in the exponents to get: $$ \begin{bmatrix} P_r \\ 1 \end{bmatrix} = \lambda \begin{bmatrix} e^{\tau + \beta_0 + \beta X} \\ 1 + e^{\tau + \beta_0 + \beta X} \end{bmatrix} .$$ Taking the ratio and simplifying the non-zero constant in numerator and denominator we get that fitting a logistic model to the manufactured population results in an implied logistic model for the real population, $\mathbf{\text{with the same coefficients for X}}$ and with a difference in the constant (in the logistic model) given by: $$ \beta_{real} = \tau + \beta_0 $$ $$\dots$$ Note that, according to your reference, the ratio of $\gamma_1 = Pr(Z=1|Y=1)$ and $\gamma_0 = Pr(Z=1|Y=0)$ should come up. Indeed: $$ \gamma_1 = Pr(Z=1|Y=1) = \frac{P(Z=1,Y=1)}{P(Y=1)} = \frac{P_r(Y=1|Z=1)P_r(Z=1)}{P_r(Y=1)} = \frac{P_m(Y=1)}{P_r(Y=1)} P_r(Z=1)= \frac{\mu_1}{\pi_1}P_r(Z=1) $$ likewise (i.e. change Y to 1-Y), $$ \gamma_0 = \frac{1-\mu_1}{1-\pi_1}P_r(Z=1) $$ so $$ \ln(\frac{\gamma_1}{\gamma_0}) = - \ln(\frac{\pi_1 (1-\mu_1)}{\mu_1 (1-\pi_1}) = \tau $$ $$\dots\dots\dots$$ Notes for full disclosure: I worked with the probability model. When one works with finite samples the example above suggests two ways of estimating the coefficients: * estimate coefficients using the sample from the real population * estimate coefficients using the manufactored populations It terns out that this two estimators are not the same (it is obvious if one consideres one estimator is based on more cases than the other). Both estimators are asymtopically consistent, but it can be shown the one based on the manufactored population is more biased (forgot the reference :( ). In the data science space we are more concern with the quality of the predictions than the parameters of the parameters used to make those predictions, so as long as you check results properly (e.g.: using a testing set to build models and another to validate them), the bias in the parameters should not deter us from using oversampling. $$\dots\dots\dots$$
