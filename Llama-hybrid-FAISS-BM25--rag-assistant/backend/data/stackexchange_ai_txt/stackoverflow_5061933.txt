[site]: stackoverflow
[post_id]: 5061933
[parent_id]: 
[tags]: 
Hashtables over large natural language word sets

I'm writing a program in python to do a unigram (and eventually bigram etc) analysis of movie reviews. The goal is to create feature vectors to feed into libsvm. I have 50,000 odd unique words in my feature vector (which seems rather large to me, but I ham relatively sure I'm right about that). I'm using the python dictionary implementation as a hashtable to keep track of new words as I meet them, but I'm noticing an enormous slowdown after the first 1000 odd documents are processed. Would I have better efficiency (given the distribution of natural language) if I used several smaller hashtable/dictionaries or would it be the same/worse? More info: The data is split into 1500 or so documents, 500-ish words each. There are between 100 and 300 unique words (with respect to all previous documents) in each document. My current code: #processes each individual file, tok == filename, v == predefined class def processtok(tok, v): #n is the number of unique words so far, #reference is the mapping reference in case I want to add new data later #hash is the hashtable #statlist is the massive feature vector I'm trying to build global n global reference global hash global statlist cin=open(tok, 'r') statlist=[0]*43990 statlist[0] = v lines = cin.readlines() for l in lines: line = l.split(" ") for word in line: if word in hash.keys(): if statlist[hash[word]] == 0: statlist[hash[word]] = 1 else: hash[word]=n n+=1 ref.write('['+str(word)+','+str(n)+']'+'\n') statlist[hash[word]] = 1 cin.close() return statlist Also keep in mind that my input data is about 6mb and my output data is about 300mb. I'm simply startled at how long this takes, and I feel that it shouldn't be slowing down so dramatically as it's running. Slowing down: the first 50 documents take about 5 seconds, the last 50 take about 5 minutes.
