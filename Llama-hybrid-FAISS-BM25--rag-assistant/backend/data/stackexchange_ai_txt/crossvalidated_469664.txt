[site]: crossvalidated
[post_id]: 469664
[parent_id]: 
[tags]: 
Does the entropy decreases with new observations when doing bayesian probability estimation?

I premise I don't know much information theory, except for some definition. I have this doubt, I'm not sure that makes sense. Suppose I'm estimating a discrete distribution, in a Bayesian setting, so I have a likelihood, a prior over the parameters, and I update these parameters for every new observation. Is there some relation between the entropy of the posterior and the number of observations? My intuition tells me that there must be some relation if we specify some conditions on the Bayesian model we adopt, and on the true distribution. For instance suppose that I set a prior such that the initial distribution is uniform over the possible outcomes and that the true distribution has not too high entropy, then more outcomes I observe the more the entropy of the posterior approximates the entropy of the true distribution, so it decreases. Or suppose I'm estimating the distribution of a random variable which is unimodal with not too high variance, and I start from a flat prior. Then the posterior should converge from something which is somehow uniform, to a bell shaped distribution with lower entropy. To your knowledge are there some results about this, even restricted to specific classes of Bayesian models? I hope this is not too vague.
