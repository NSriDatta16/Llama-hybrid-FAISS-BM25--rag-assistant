[site]: datascience
[post_id]: 47894
[parent_id]: 47893
[tags]: 
The final feature vector would be a concatenation like (for multi-class prediction): Question google count | option A google count | option B google count | option C google count | option C no. words | option A no. words | other features | label (1, 2, 3) There is no need to put features related to option A close to each other (or in any particular order), they just need to be on the same column for all rows regardless of the label. XGBoost parameters for multi-class classification are: 'objective': 'multi:softprob', 'num_class': 3
