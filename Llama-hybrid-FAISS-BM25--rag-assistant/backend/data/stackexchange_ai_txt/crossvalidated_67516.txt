[site]: crossvalidated
[post_id]: 67516
[parent_id]: 56253
[tags]: 
I am not sure of the format of your data, but here is one interesting approach. If your data are highly non-linear, then it's likely that the meaningful structure of the data reside on a lower-dimensional manifold and do not require the full dimensionality of the input space to be well-represented (otherwise, how could you know they were not linear?) If that assumption holds, then you could try two ways of projecting the data to a smaller space. One way would be based on an optimal embedding that arises from the natural structure of the data -- e.g. it "learns" the non-linear manifold structure of the data set. The other could be a dimensionality reduction algorithm that assumes the data is well represented by linear structure. I suggest using the ISOMAP algorithm for the first and traditional PCA (or randomized PCA if you have a lot of data) for the second. ISOMAP tries to use the graphical structure of the observed data to provide an optimal projection into a lower dimensional space such that the distance along the data's natural manifold is preserved. PCA instead says the data is roughly linear and all we need to do is re-orient ourselves to the directions of most importance, then truncate dimensions based on how much of the total data variability we want to keep track of. The interesting part is: assuming you've transformed your data by both of these methods, how to arrive at a summary statistic that captures the "amount" of non-linearity. A simple statistic (if your data is represented as column vectors in a feature matrix, such as the transpose of the RHS matrix from linear regression notation) is to just compute the Frobenius norm of the difference between the two embeddings: $$NLSTAT(X) = ||X_{ISOMAP} - X_{PCA}||$$ Note that you should rotate these two projections to have coinciding basis, so you don't get spurious differences from misaligned coordinates. You may also want to account for scale effects. Other norms could work too, and you could mix in some dependence on the selection of the number of dimensions $k$ to reduce to. The idea here is that if ISOMAP picks a drastically different set of lower-dimensional representations of the data points than what PCA picks, this would be a clue that the data set has a lot of exploitable non-linear structure.
