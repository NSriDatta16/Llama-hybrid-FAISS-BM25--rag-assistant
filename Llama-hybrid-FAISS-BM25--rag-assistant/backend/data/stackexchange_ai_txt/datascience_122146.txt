[site]: datascience
[post_id]: 122146
[parent_id]: 
[tags]: 
I have a strange loss behavior in Tensorflow/Keras. It goes smooth towards better values, and then suddenly it increases rapidly before falling again

I have a model that trains on a dataset of 100000 data points. The model is a single dense layer with a single neuron. During training, I use a batch size of 1000 and train for 250 epochs. After training I plot the loss. While everything in the end works out fine, I see some strange behaviour that I would like to understand. Zooming in on one of these regions where the loss increases gives the following picture. The individual orange points are the epoch loss, while the grey dots (almost not visible because they are so close) are the batch losses (average I presume). Can someone explain this behaviour? DATA INFORMATION: It might be important to know that the input data are vectors of 700 features. At the moment, these features are all scaled versions (different scaling for each feature), or the y-value. So the system is a quite easy one to solve. What I am trying to understand is the process that results in this behaviour. I have tried to investigate the loss by looking at the non-average batch-loss also, but I really cannot understand why the loss suddenly explodes like this.
