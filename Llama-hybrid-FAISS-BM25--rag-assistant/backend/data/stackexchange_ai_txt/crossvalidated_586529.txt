[site]: crossvalidated
[post_id]: 586529
[parent_id]: 586514
[tags]: 
Disclaimer: I assume your question refers to multi-layer perceptrons, used as classifiers. But, if you want to engage in hair splitting, there are other neural network architectures and applications which do not create a separating hyperplane. In a multi-layer perceptron, each neuron first computes a linear function on its inputs and then passes it through an activation function , which is almost always non-linear. All layers except the last are used to perform some kind of non-linear data transformation, with the eventual purpose of making the data linearly separable, or close to linearly separable. The neurons in the final, output layer also compute a linear function of their inputs. Typically, they would also pass it through a non-linear function, like a sigmoid, but this is not necessary for understanding the concept. It suffices to set a threshold on the linear output of the output neuron, and you get a separating hyperplane. Mathematically: Let us denote by $x_i$ the output of the $i$ -th neuron in the last hidden layer, i.e. the last layer before the output layer. Let there be just one output neuron in the output layer and let $w_i$ be the weight connecting the $i$ -th neuron to the output neuron. Also, let $w_0$ be the bias of the output neuron. Then, the output neuron computes the linear function: $$ y = w_0 + \sum_i w_i x_i $$ Let us say, we classify all observations for which $y as "class A" and the others as "class B". In other words, the equation $$ w_0 + \sum_i w_i x_i = 0 $$ describes the class boundary. This is again a linear function; an equation describing a hyperplane in the implicit form. Example: Let's say you have two neurons in the last hidden layer, with the outputs $x_1$ and $x_2$ . Then, the equation $$ w_0 + w_1 x_1 + w_2 x_2 = 0 $$ describes a one-dimensional class boundary (a straight line) in the two-dimensional space, spanned by $x_1$ and $x_2$ . If you want, you can reformat the equation to get a perhaps more common explicit form: $$ x_2 = -\frac{1}{w_2}(w_0 + w_1 x_1) $$ The same works for any number of dimensions. Update in response to comments: The above described network, with only one neuron in the output layer, is suitable only for classifying the data into two classes. For more classes, you'll need more output neurons. In practice, people usually use as many output neurons as they have classes. Each output neuron is "responsible" for recognising one class. It should output a high value if it "thinks" that the present observation belongs to "its" class, and a low value otherwise. Again, each output neuron determines a separating hyperplane, with points belonging to "its" class ideally on one, positive, high-valued side of the hyperplane and all other points on the other. Of course, in such case it is possible for the network to produce ambiguous results, e.g. two or more neurons claiming that a point belongs to their class. Resolving these ambiguities is beyond the scope of this question, but the basic idea is to trust more the neuron with the highest value. People commonly use some kind of normalisation, like softmax, to convert the network output to "class probabilities". As noted above, it is also common for the output neurons to apply a non-linear transformation to the previously computed linear function. Usual choices are $\tanh$ and logistic function, which are both sigmoid, meaning that they approach horizontal asymptotes as their arguments approach positive or negative infinity. With a suitable encoding of class labels as the values of these asymptotes, the training of the neural network consists of adjusting the weights so that its output, according to some error metric, approaches the true class labels.
