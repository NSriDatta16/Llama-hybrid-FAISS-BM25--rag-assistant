[site]: crossvalidated
[post_id]: 330491
[parent_id]: 
[tags]: 
Is planning in Dyna-Q a form of experience replay?

In Richard Sutton's book on RL (2nd edition) , he presents the Dyna-Q algorithm, which combines planning and learning. In the planning part of the algorithm, the Dyna-agent randomly samples n state-action pairs $(s, a)$ previously seen by the agent, feeds this pair into its model of the environment and gets a sampled next state $s'$ and reward $r$. It then uses this set $(s,a,r,s')$ to perform its usual Q-learning update. In a deterministic environment, the reward and next state are always the same for a given state-action pair $(s_t,a_t)\to(r_{t+1},s_{t+1}')$. In his chapter on Dyna-Q, Sutton does not refer to this process as being a form of experience replay , and only introduces the latter concept much later in the book. However I can't really see the distinction (if there is one) between those two processes. Is it correct to say that in a deterministic environment, planning in Tabular Dyna-Q is a form of experience replay ?
