[site]: crossvalidated
[post_id]: 415055
[parent_id]: 413712
[tags]: 
Yes, it is true that the functional dependence of nodes in a layer on the nodes in a previous layer is the same for all hidden layers (well you can use different activation functions, but let's assume you are using the same one). However, the nodes in different hidden layers are different functions of the input values. The general formula for the node values in layer $l$ given the node valuers in layer $l-1$ is \begin{equation} a^{l}_j = \sigma (z^l_j ) = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right) \end{equation} where $w^{l}_{jk}$ are the weights and $b^l_j$ are the biases. Consequently, given a set of input values $x_i$ the node values in the first hidden layer are \begin{equation} a^{1}_j = \sigma\left( \sum_k w^{1}_{jk} x_k + b^1_j \right) \end{equation} and in the second hidden layer the node values are \begin{equation} a^{2}_j = \sigma\left( \sum_k w^{2}_{jk} \sigma\left( \sum_k w^{1}_{jk} x_k + b^1_j \right) + b^2_j \right) \end{equation} Clearly, the node values are different functions of the input. Furthermore, when weights are updated different updates are used for different layers. The general formula for the weight change in a layer $l$ is calculated using the chain rule (assuming the learning rate $\eta=1$ for simplicity): \begin{eqnarray} \Delta w^{l}_{jk} = -\delta^l_j a^{l-1}_k\\ \delta^l_j = \delta^{l+1}_j \sigma'(z^l_j) \end{eqnarray} Starting with the last output layer $L$ we get: \begin{equation} \Delta w^L_{jk} = - \frac{\partial Loss}{\partial a^{L}_j } \sigma'(z^L_j) a^{L-1}_k \end{equation} and for the layer $L-1$ we get: \begin{equation} \Delta w^{L-1}_{jk} = - \frac{\partial Loss}{\partial a^{L}_j } \sigma'(z^L_j) \sigma'(z^{L-1}_j)a^{L-2}_k \end{equation} So the weights in different layers are updated differently. By the way, that's why a multi-layer neural network is qualitatively different, and better, than a one layer neural network with the same number of nodes.
