[site]: crossvalidated
[post_id]: 219691
[parent_id]: 219619
[tags]: 
I think this is a very good question. I always want to observe the "U" shape curve in cross validation experiments with real data. However, my experience with real world data (~ 5 years in credit card transactions and education data) does not tell me over fitting can easily happen in huge amount (billion rows) real world data . I often observe that you can try you best over fit the training set, but you cannot do too much (e.g., reduce the loss to 0), because the training set is really large and contains a lot of information and noise. At the same time, you can try the most complicated model (without any regularization) on testing data, and it seems fine and even better than some with regularization. Finally, I think my statements might be true only under the condition of you have billions data points in training. Intuitively, the data is much complex than you model so you will not over fit. For billion rows of data, even you are using a model with thousands of parameters, it is fine. At the same time you cannot afford the computation for building a model with million free parameters. In my opinion this is also why neural network and deep learning got popular these days. Comparing to billions of images in Internet, any model you can afford training is not enough to over fit.
