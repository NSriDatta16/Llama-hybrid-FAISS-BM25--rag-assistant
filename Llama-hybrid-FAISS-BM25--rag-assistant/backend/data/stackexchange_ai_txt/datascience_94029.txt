[site]: datascience
[post_id]: 94029
[parent_id]: 
[tags]: 
Exploration in Q learning: Epsilon greedy vs Exploration function

I am trying to understand how to make sure that our agent explores the state space enough before exploiting what it knows. I am aware that we use epsilon-greedy approach with a decaying epsilon to achieve this. However I came across another concept, that of using exploration functions to make sure that our agent explores the state space. Q Learning with Epsilon Greedy $sample = R(s,a,s') + \gamma \max_{a'}Q(s',a')$ $Q(s,a) = (1 - \alpha)*Q(s,a) + \alpha*sample$ Exploration function $ f(u,n) = u + k/n $ $Q(s,a) = R(s,a,s') + \gamma*max_{a'}f(Q(s',a'), N(s',a'))$ Where N(s',a') counts the number of times you have seen this (s',a') pair before. Now my question is, are these 2 above strategies just 2 different ways of Q learning? Because I can't seem to find much details on exploration function but can find plenty on epsilon greedy Q learning. Or can the exploration function be used along with the epsilon greedy Q learning algorithm as a form of some optimization? I am confused as to where exactly would we make use of this exploration function Q learning strategy. Any help/suggestions are much appreciated!
