[site]: crossvalidated
[post_id]: 51579
[parent_id]: 51557
[tags]: 
Let's say each user u is represented by a (sparse) vector of features f[u] ; each campaign c is also represented by a vector features in the same space f[c] . You could simply find a nearest neighbour (or several of those) by using kNN e.g. with Manhattan distance, or some other measure that suits your features better. Of course, this will not work nicely for a user for which you do not have data at all. Another approach is to use collaborative filtering i.e. showing a user stuff that similar users liked/viewed/clicked. In your settings you might prefer this one if you have at least some data about a user after registration. Both these approaches have a scalable implementation e.g. in Apache Mahout. I think the problem that you are referring to ( a brand new user ) is a so-called cold start problem , and I do not know whether it has an ultimate solution. Though there are methods that can tackle it, take a look at the survey (Methods and metrics for cold-start recommendations, Schein et al, 2002). P.S. As for Infer.Net, it is a very cool toolbox which possibly can also solve your problem - authors created an opponent recommender system for XBox with it, called TrueSkill. But it actually would require some knowledge in Bayesian learning. update If the number of campaigns is not too big, you could use naive bayes classification. Each campaign is a separate class c_i . The training phase boils down to computing likelihoods for separate features (e.g. p(country=usa|campaign=c_i) - probability that the country is usa given that his campaign is c_i ), each can be a frequency of a certain value of a feature. One should also compute priors p(campaign=c_i) - which shows how frequent campaign c_i is. Then in production for each c_i you compute posterior probability p(campaign=c|country=usa,viewed_item1=True,) , which is simply a product of prior and likelihoods. The campaign with the highest probability is assigned to the user. I would assume that one could use sparsity and do computations efficiently. Note that this approach assumes that all the features are independent.
