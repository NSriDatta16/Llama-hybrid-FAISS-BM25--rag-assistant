[site]: crossvalidated
[post_id]: 558081
[parent_id]: 558060
[tags]: 
you are right that the random forest or other tree ensemble methods make it hard for overfitting. Essentially, you can set the number of trees to be very large, it is uncommon to have 5000 trees. The more trees you have, the less variance there will be in your result. So this parameter is something you can just be less worried about, go for an arbitrarily large enough number and normally you will get good result. However, you should not grow the tree too deep. Otherwise, you end up having just very few examples in each node, this will likely make your training error extremely low and your test error a bit higher than you would except. Depends on the library you are using, normally there is a default number of minimum end nodes in the leaf. You can increase this number and see how it affects the result on the validation set. For example, in R, the default number is 5. Then you can increase this number until you observe that the validation error starts to go down. Maybe this happens already when you have minimum end nodes equals 10 or so (depending on the data you have).
