[site]: crossvalidated
[post_id]: 136463
[parent_id]: 136450
[tags]: 
The right order of things is unfortunately neither of the two. If you want to do feature selection , you ideally tune hyperparameters for each candidate set of features. Doing feature selection without optimizing hyperparameters for each candidate set of features is a bad idea, because the hyperparameters can heavily impact model performance. Conversely, doing hyperparameter tuning in advance can lead to biased parameters based on some features, which will then inevitably result in those features being selected even though they may not be the best. In practice, however, SVM is quite robust against uninformative features, so it may not even be necessary to do feature selection at all. Secondly, grid search is a fairly poor search strategy for hyperparameter tuning (even though it is the standard approach). I recommend to use specialized tuning libraries like Optunity or Hyperopt , which provide disciplined search methods that will find much better sets of hyperparameters using far fewer candidate sets (= time).
