[site]: crossvalidated
[post_id]: 595866
[parent_id]: 363190
[tags]: 
One very simple reason for the very start is that e.g. ReLU sets all inputs below zero to exactly zero. Because image inputs are usually standardized so that below average intensity of some color will have negative values, you'd more or less treat all below average values as the same (=0), if you applied ReLU on the input image. By first applying the convolutional layer, you give it the "chance" to create features based on important local combinations of pixels that have values >0 and don't get "squashed" by the activation function. Thereafter, architectures like VGG-16 or similar just keep alternating convolutional layers and ReLU activations with the occasional pooling layer thrown in (until you get to some final pooling + collapsing into dense layers). Another reason is that (like with many things in deep learning), people experimented with many different ways of doing things and some particular approaches happened to work well (i.e. compared with what else was tried, of course some things may have been overlooked, which is where it helps that this is a pretty active field with many people trying many different ideas).
