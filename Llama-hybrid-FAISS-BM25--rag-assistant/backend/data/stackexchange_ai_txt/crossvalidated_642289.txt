[site]: crossvalidated
[post_id]: 642289
[parent_id]: 
[tags]: 
Varying length of data for domain adaptation purposes

My goal is to use a pretrained transformer that gets data of fixed size (let's say, an array of 5X150), and encodes it into an embedding. I want to use such embedding for a regression task, by let's say adding a fully connected layer to the encoder, and train only that layer. The problem is that my input data is of varying length. I could on paper split and pad it to fit the encoder, but conceptually, it would not make sense to pair the different chunks of the split data to a single target score, as the score is given to the data as a whole (for example, a long text that is given some score for its quality; the score is given to the text as a whole, not only for the first paragraph). I thought of maybe averaging the embeddings of the different chunks (or using other pooling technique) and then use the averaged embedding as an input for the regression layer, but I'm not sure at which point to do that in the training pipeline (in the dataset module? in the main function?). If someone could suggest me a layout or a pipeline of how to handle this case, it would be much appreciated.
