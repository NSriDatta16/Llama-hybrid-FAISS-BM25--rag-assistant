[site]: crossvalidated
[post_id]: 247765
[parent_id]: 140367
[tags]: 
Background: The editorial in question is this one from Basic and Applied Social Psychology , a journal with a 2015 impact factor of 1.168, i.e., not highly quotable. Re: OP question , i.e., Is an NHSTP something different from a "test of hypothesis" or a "significance test"? The applicable editorial statements are 1) "...the null hypothesis significance testing procedure (NHSTP) is invalid..." [Sic, with alpha = 0.05] 2) "...authors will have to remove all vestiges of the NHSTP (p-values, t-values, F-values, statements about ‘‘significant’’ differences or lack thereof, and so on)." 3) "...confidence intervals [Sic, 95%] also are banned from BASP." 4) "...Bayesian procedures are neither required nor banned from BASP." [Sic, depends on which ones, they are either banned or not.] 5) "Are any inferential statistical procedures required?...No..." The motivation offered for this is in part "...the $p Answer to OP: These editors would probably claim a test of significance is often an improper test of hypothesis. For example, they state that "...Bayesian proposals that at least somewhat circumvent the Laplacian assumption [Sic, I know nothing a priori ]...[such that] there might even be cases where there are strong grounds for assuming that the numbers really are there..." This relates in part to the Fisher versus Neyman and Pearson argument as pointed out above by @Livid and for which the editorial would side with Fisher. Discussion: I am a firm believer in intellectual humility as a fundamental, and indispensable tenet of scientific method. If I, as a researcher, am not allowed to proceed from an assumption-less initial premise in which all prior theory is disbelieved, then I will lose all of my ability to examine data with a creative and open mind. The premise that all numerical processing must be absolute truth is an exposition of cupidity that is sublime. The only truth is data, and I would humbly paraphrase Box by stating that all models are false, especially and most certainly those that presume that any truth arises from anything that is not identically the data itself. That does not mean that I have to choose between Fisher and Neyman/Pearson, rather that I firmly believe neither premise taken alone, but rather examine things exhaustively until my hypotheses are supported and/or rejected to self-consistency of the ensemble. Only self-consistency can be used as a criterion, as no analysis can reveal an absolute truth. My way of doing things is not for everyone. Many prefer to plan testing in a rigid controlled experiment design that I would call 'top down'. However, controlled experiments are inefficient for data mining, pattern recognition, and generating hypotheses. They are useful for testing narrow questions, and that is when the controversy about NHSTP can arise. Without supporting evidence, e.g., an entire structure of self-consistency to rely on, any one test is open to criticism. This might be considered as Bonferroni in reverse; if multiple tests lead to an inescapably self-consistent ensemble, the chance of the ensemble occurring by chance alone is diminished. In planning experiments for psychology, the nonsense about not using $p because it is circumstantial will not improve a journal's content.
