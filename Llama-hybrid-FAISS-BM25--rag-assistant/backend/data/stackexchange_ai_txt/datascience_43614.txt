[site]: datascience
[post_id]: 43614
[parent_id]: 43592
[tags]: 
Yes, it is possible. In my case, with a specific reward function, my agent (snake) preferred to suicide instead of trying to reach the target, because of the "live penalty" (the agent receives a penalty each step, to speed up the exploitation phase over the exploration). Check this blog posts, they go through all of this in detail and they're so far the best ones I have found: https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html Also, since the author has a background in Cognitive Neuroscience, each algorithm is explained from a neuroscience point of view.
