[site]: crossvalidated
[post_id]: 327100
[parent_id]: 327090
[tags]: 
This can happen with ReLU activation functions, when a conv filter before the activation gets to a state when it always outputs a negative number and the following ReLU truncates the output, thus preventing any gradient propagation. Generally, CNNs are less prone to this problem (as the weights are shared between multiple locations), but it can still happen. A large network can usually deal with the fact that some of the units are "dead", however, you might try some other activation function such as Leaky ReLU or PReLU to avoid this problem.
