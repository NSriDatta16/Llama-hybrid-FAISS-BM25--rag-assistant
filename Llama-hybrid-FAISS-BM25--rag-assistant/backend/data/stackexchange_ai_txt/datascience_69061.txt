[site]: datascience
[post_id]: 69061
[parent_id]: 
[tags]: 
(Basic) statistics

I got a misunderstanding regarding basic statistics (I think) but I can't get my head around that: I did an online survey regarding the usage of some application. The user could answer 1 (I don't know that application), 2 (I know that) or 3 (I use that). Now I want to know, how many applications the average user uses: df['iuse'].mean() where df['iuse'] was calculated as the count of answer 3 in one returned answer. The result is something like 2.2. Now, I want to know, how many applications a user of application X uses: f = df['q1_1'] == 3 # For application 1, filter all answers where the user uses that df.loc[f,'iuse'].mean() That returns a number above 2.2 - for every single app: In short (and ugly): [(4.235294117647059, 85), (4.966666666666667, 60), (2.7495274102079397, 1058), (4.609195402298851, 87), (6.391304347826087, 23), (4.122950819672131, 122), (4.850746268656716, 67), (3.1860068259385668, 586), (2.72192513368984, 1122), (3.520231213872832, 346), (4.276595744680851, 94)] (left is the mean, and right is the count of answers for that application) Now: Why is the usage overall less than the usage when seen from a specific application? I would expect to have at least some numbers below the total mean, but they are all above that. I'm confused :( Thanks for any pointers and help Klaus
