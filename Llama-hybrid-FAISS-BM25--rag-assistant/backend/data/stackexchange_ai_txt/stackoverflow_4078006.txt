[site]: stackoverflow
[post_id]: 4078006
[parent_id]: 670734
[tags]: 
Here's a real-world example I am working on right now, from signal processing / control systems: Suppose you have some structure that represents the data you are collecting: struct Sample { time_t time; double value1; double value2; double value3; }; Now suppose that you stuff them into a vector: std::vector samples; ... fill the vector ... Now suppose that you want to calculate some function (say the mean) of one of the variables over a range of samples, and you want to factor this mean calculation into a function. The pointer-to-member makes it easy: double Mean(std::vector ::const_iterator begin, std::vector ::const_iterator end, double Sample::* var) { float mean = 0; int samples = 0; for(; begin != end; begin++) { const Sample& s = *begin; mean += s.*var; samples++; } mean /= samples; return mean; } ... double mean = Mean(samples.begin(), samples.end(), &Sample::value2); Note Edited 2016/08/05 for a more concise template-function approach And, of course, you can template it to compute a mean for any forward-iterator and any value type that supports addition with itself and division by size_t: template S mean(Titer begin, const Titer& end, S std::iterator_traits ::value_type::* var) { using T = typename std::iterator_traits ::value_type; S sum = 0; size_t samples = 0; for( ; begin != end ; ++begin ) { const T& s = *begin; sum += s.*var; samples++; } return sum / samples; } struct Sample { double x; } std::vector samples { {1.0}, {2.0}, {3.0} }; double m = mean(samples.begin(), samples.end(), &Sample::x); EDIT - The above code has performance implications You should note, as I soon discovered, that the code above has some serious performance implications. The summary is that if you're calculating a summary statistic on a time series, or calculating an FFT etc, then you should store the values for each variable contiguously in memory. Otherwise, iterating over the series will cause a cache miss for every value retrieved. Consider the performance of this code: struct Sample { float w, x, y, z; }; std::vector series = ...; float sum = 0; int samples = 0; for(auto it = series.begin(); it != series.end(); it++) { sum += *it.x; samples++; } float mean = sum / samples; On many architectures, one instance of Sample will fill a cache line. So on each iteration of the loop, one sample will be pulled from memory into the cache. 4 bytes from the cache line will be used and the rest thrown away, and the next iteration will result in another cache miss, memory access and so on. Much better to do this: struct Samples { std::vector w, x, y, z; }; Samples series = ...; float sum = 0; float samples = 0; for(auto it = series.x.begin(); it != series.x.end(); it++) { sum += *it; samples++; } float mean = sum / samples; Now when the first x value is loaded from memory, the next three will also be loaded into the cache (supposing suitable alignment), meaning you don't need any values loaded for the next three iterations. The above algorithm can be improved somewhat further through the use of SIMD instructions on eg SSE2 architectures. However, these work much better if the values are all contiguous in memory and you can use a single instruction to load four samples together (more in later SSE versions). YMMV - design your data structures to suit your algorithm.
