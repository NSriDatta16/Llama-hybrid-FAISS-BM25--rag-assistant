[site]: crossvalidated
[post_id]: 510401
[parent_id]: 370428
[tags]: 
More intuitively with logical deduction: i) P(x) is the "real scenario" , how things really happens, and Q(x) is the estimation ii) P(x) (-Log(Q(x)) * is the "loss / punishment" function, since if indeed when, as example, probability P(x) to occur to so high and Q(x) is very low, then as P and Q are always between 0 and 1, that loss / punishment function will be VERY HIGH (due to log of a very small number) So a) By definition: H(P) = Sum_over_x (P(x)*Log(P(x)) is the Entropy, and P is the "Ground True" (or known to have minimal errors if the ground true cannot be properly measured) b) then it follows, for ALL other estimated distribution/probabilities Qi, H(P,Qi) : Sum_over_x (P(x)*Log(Qi(x)) is always on average less accurate, and hence greater than H(P) Hence H(P,Q) >= H(P) is really by definition of the H(P). Note: Compare error A: P(x1) is 0.1, and Q(x1) is 0.5, log(0.5) = -0.69314718056, P(x1)* -Log (Q(x1)) = 0.0693..... error B: P(x2) is 0.5 vs Q(x2) is 0.1, log(0.1) = -2.302585093 P(x2)* -Log (Q(x2)) = 1.151..... In machine learning use case, P(xi) * -Log (Q(xi)) will penalize more those predict a lot lower probability when indeed it is a high probability case, whereas for a lower probability P(xi) such as 0.1 in error A above, You can look at the following for more information: https://machinelearningmastery.com/cross-entropy-for-machine-learning/#:~:text=Cross%2Dentropy%20is%20commonly%20used,difference%20between%20two%20probability%20distributions .
