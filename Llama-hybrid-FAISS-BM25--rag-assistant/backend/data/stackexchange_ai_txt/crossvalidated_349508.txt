[site]: crossvalidated
[post_id]: 349508
[parent_id]: 349496
[tags]: 
Let me take the liberty to rephrase the question as "What are the arguments that Andrew Gelman puts forward against hypothesis testing?" In the paper that is linked in the post, the authors take issue with using a mechanical procedure for model selection , or, as they phrase it: [Raftery] promises the impossible: The selection of a model that is adequate for specific purposes without consideration of those purposes. Frequentist or Bayesian hypothesis testing are two examples of such mechanical procedures. The specific method that they criticize is model selection by BIC, which is related to Bayesian hypothesis testing. They list two main cases when such procedures can fail badly: "Too many data": Say you have a regression model $y_i = \beta'x_i + \epsilon_i$ with, say, 100 standard normally distributed regressors. Say that the first entry of $\beta$ is $1$ and all other entries are equal to $10^{-10}$. Given enough data, a hypothesis test would yield that all estimates of $\beta$ are "significant". Does this mean that we should include $x_2,x_3,\ldots x_{100}$ in the model? If we were interested in discovering some relationships between feature and outcome, would we not be better off considering a model with only $x_1$? "Not enough data": On the other extreme, if sample sizes are very small, we will be unlikely to find any "significant" relationships. Does this mean that the best model to use is the one that includes no regressors? There are no general answers to these questions as they depend on the modeler's objective in a given situation. Often, we can try to select models based on criteria that are more closely related to our objective function, e.g. cross validation sample when our objective is prediction. In many situations, however, data-based procedures need to be complemented by expert judgment (or by using the Bayesian approach with carefully chosen priors that Gelman seems to prefer).
