[site]: crossvalidated
[post_id]: 234314
[parent_id]: 
[tags]: 
How does a U-Net group pixel classifications into a single spatial region?

The neural network known as a " U-Net " ( Ronneberger, Fischer, and Brox 2015) was a prominent technique in Kaggle's recent Ultrasound Nerve Segmentation contest, where high scores were awarded to algorithms that created pixel masks with a high degree of overlap with the hand drawn regions. (Photo from Christopher Hefele ) If one proceeds to classify every pixel (perhaps from a down-sampled image), there must be many ways to incorporate the prior knowledge that neighboring pixels will tend to have the same class, and furthermore that all the positive classifications must reside in a single spatial region. However, I can't figure out how these U-Nets are doing it. They do classify every pixel, albeit by way of a maze of convolutional and pooling operators: There are separation borders involved, but the paper notes that they are "computed using morphological operations" which I take to mean entirely separate from the U-Net itself. Those borders are only used to modify the weights so that more emphasis is placed on the pixels at the border. They do not appear to fundamentally change the classification task. In classifying every pixel, how does this deep convolutional neural network, called a "U-Net," incorporate the prior knowledge that the predicted region will be a single spatial region?
