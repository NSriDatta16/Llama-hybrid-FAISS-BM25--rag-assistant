[site]: crossvalidated
[post_id]: 114366
[parent_id]: 
[tags]: 
Is rstan or my grid approximation incorrect: deciding between conflicting quantile estimates in Bayesian inference

I have a model to achieve Bayesian estimates the population size $N$ and probability of detection $\theta$ in a binomial distribution solely based on the observed number of observed objects $y$: $$ p(N,\theta|y)\propto \frac{ \text{Bin}(y|N,\theta)}{N} $$ for $ \left\{N|N\in\mathbb{Z}\land N\ge \max(y)\right\}\times(0,1) $. For simplicity, we assume that $N$ is fixed at the same, unknown value for each $y_i$. In this example, $y=53,57,66,67,73$. This model, when estimated in rstan , diverges from the results obtained from a grid approximation of the posterior. I'm trying to pin down why. (Interested readers might find that this question is a follow-on to my answer here .) rstan Approximation For reference, this is the rstan code. raftery.model N; simplex[2] theta; } transformed parameters{ } model{ vector[I] Pr_y; for(i in 1:I){ Pr_y[i] Note that I cast theta as a 2-simplex. This is just for simplicity. The quantity of interest is theta[1] ; obviously theta[2] is superfluous information. Additionally, $N$ is a real value ( rstan only accepts real-valued parameters because it is a gradient method), so I wrote a real-valued binomial distribution. Rstan results mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat N 1078.75 256.72 15159.79 94.44 148.28 230.61 461.63 4575.49 3487 1 theta[1] 0.29 0.00 0.19 0.01 0.14 0.27 0.42 0.67 2519 1 theta[2] 0.71 0.00 0.19 0.33 0.58 0.73 0.86 0.99 2519 1 lp__ -19.88 0.02 1.11 -22.89 -20.31 -19.54 -19.09 -18.82 3339 1 Grid Approximation The grid approximation was produced as below. Memory constraints prevent me making a finer grid on my laptop. theta I used the grid approximation to produce this display of the posterior density. We can see that the posterior is banana-shaped; this kind of posterior can be problematic for euclidian metric HMC. (The severity of the banana shape is actually suppressed here since $N$ is on the log scale.) If you think about the banana shape for a minute, you'll realize that it must lie on the line $\bar{y}=\theta N$. (Additionally, the grid approximation displayed in this graph is not normalized for reasons of clarity - else the banana is a little too narrow to clearly make out.) Grid approximation results do.call(cbind, lapply(c(0.025, .25, .5, .75, .975), function(quantile){ approx(y=N, x=cumsum(rowSums(post.norm))/sum(post.norm), xout=quantile) })) [,1] [,2] [,3] [,4] [,5] x 0.025 0.25 0.5 0.75 0.975 y 92.55068 144.7091 226.7845 443.6359 2475.398 Discussion The 97.5% quantile for $N$ is much larger in my rstan model than it is for the grid approximation, but its quantiles are similar to the grid approximation otherwise. I interpret this as indicating that the two methods are generally in agreement. I do not know how to interpret the discrepancy in the 97.5% quantile, though. I've developed several possible explanations for what might be accounting for the divergence between the grid approximation and the results from rstan HMC-NUTS sampling, but I'm uncertain how to understand if one, both or neither explanation is correct. Rstan is wrong and the grid is correct. The banana-shaped density is problematic for rstan , especially as $N$ drifts off towards $+\infty$, so these tail quantities are not trustworthy. We can see from the plot of the posterior over the grid that the tail is very sharp at larger values $N$. Rstan is correct and the grid is wrong. The grid makes two approximations which may undermine the results. First, the grid is only a finite set of points over a subspace the posterior, so it is a rough approximation. Second, because it's a finite subspace, we're falsely declaring there to be 0 posterior probability over values $N$ larger than our largest grid value for $N$. Likewise, rstan is better at getting into the tails of the grid, so its tail quanitles are correct. I needed more space to clarify a point from Juho's answer. If I understand correctly, we can integrate $\theta$ out of the posterior to obtain the beta-binomial distribution: $$ p(y|N,\alpha,\beta)={N\choose y} \frac{\text{Beta}(y+\alpha, N-y+\beta)}{\text{Beta}(\alpha,\beta)} $$ In our case, $\alpha=1$ and $\beta=1$ because we have a uniform prior on $\theta$. I believe that the posterior should then be $p(N|y)\propto N^{-1}\prod_{i=1}^K p(y_i|N,\alpha=1,\beta=1)$ where $K=\#(y)$ because $p(N)=N^{-1}$. But this appears to wildly diverge from Juho's answer. Where have I gone wrong?
