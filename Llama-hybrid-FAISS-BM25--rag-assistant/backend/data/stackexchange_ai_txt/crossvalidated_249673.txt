[site]: crossvalidated
[post_id]: 249673
[parent_id]: 
[tags]: 
How many decimal places should be used for which purposes?

Let's say that $x$ is some statistic that represents your measurement of something in nature, such as (say) the sample average of human height in a country. Suppose that your measurement of $100$ randomly chosen objects (e.g. people) you find out that $x=179.0234234524312123545990088$. Obviously many of these decimal places are probably highly likely to be due to noise, and will probably change drastically if I re-sample again. One may use eyeballing, and follow his feelings to choose (say) only 4 decimal places, thus writing down $179.0234$. My question is: Is there any principled approach to decide how many decimal places should be chosen in order write down the number $179.0234234524312123545990088$ for various uses? For example, should we write $x$ as $179.02342$? $179.0234$? $179.023$? $179.02$? $179.0$? Or maybe just $179$ if the statistic is too unstable to render its fractions meaningful? So far I have been using my guts/feelings/eyeballing to decide what to display for a final answer. Usually I just choose $4$ decimal places. So usually I would write down $179.0234$. But the problem is that my approach so far is not principled, and is rather based on guesses/feelings/eyeballing. I wonder if there is any principled approach that can settle this debate down once and for all? For example, how many significant places should be used for computation and how many for display? There have been prior answers for display of results, for example, Number of significant figures to put in a table? and Why don't we use significant digits? but I do not feel that these addresse the issue of propagation of error during calculations.
