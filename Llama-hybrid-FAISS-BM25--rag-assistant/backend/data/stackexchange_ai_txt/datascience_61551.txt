[site]: datascience
[post_id]: 61551
[parent_id]: 
[tags]: 
Prevent overfitting when decreasing model complexity is not possible

I'm fairly new to machine learning and as an exercise for a more complicated task, I'm trying to do the following what I thought was a trivial task. Suppose as an input I have population density maps. These are 2D images with one layer, in which each pixel is the count of persons living in that area. From that data, I'd like my model to "estimate" (in fact it would be possible to calculate the exact solution) the total number of persons living on that density map. Essentially, the task consists of just taking the sum of the 2D input. I have tried many architectures and I found that the simpler the better. In fact a model containing no hidden layers performed best: from keras.layers.core import Dense from keras.layers import Flatten, Input inputs = Input(shape=(225, 350, 1)) x = Flatten()(x) x = Dense(1)(x) While this performs very well on the training data, it fares very poorly on the validation data. I know that this is a sign of overfitting, but how can I prevent overfitting given that is not possible to further decrease complexity of the model? Or would another approach / architecture be better altogether? Note that I have performed the usual data pre-processing (normalising inputs and outputs). Thanks in advance for any hints.
