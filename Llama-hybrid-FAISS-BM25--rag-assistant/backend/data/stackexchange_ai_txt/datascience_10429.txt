[site]: datascience
[post_id]: 10429
[parent_id]: 
[tags]: 
Can xgboost (or any other algorithm) give bad results with some bad features?

till now I was under the impression that machine learning algorithms (gbm, random forest, xgboost etc) can handle bad features (variable) present in the data. In one of my problems, there are around 150 features and with xgboost I am getting a logloss of around 1 if I use all features. But if I remove around 10 bad features (found using some technique) I am observing a logloss of .45. That is huge improvement. My question is, can bad features really make such big differences?
