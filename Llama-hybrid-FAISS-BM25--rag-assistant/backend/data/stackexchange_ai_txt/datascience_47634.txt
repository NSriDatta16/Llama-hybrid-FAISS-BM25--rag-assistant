[site]: datascience
[post_id]: 47634
[parent_id]: 47616
[tags]: 
This question has been asked so many times, yet I believe no widely accepted answer exists, especially in the case of black box models such as neural networks. A way to go may be sensitivity analysis, i.e. evaluate the change in the output of the model for small changes in the individual inputs. The higher the change in the output, the more important the feature.
