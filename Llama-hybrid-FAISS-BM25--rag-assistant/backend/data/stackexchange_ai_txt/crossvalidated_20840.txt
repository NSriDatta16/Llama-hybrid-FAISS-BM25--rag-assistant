[site]: crossvalidated
[post_id]: 20840
[parent_id]: 
[tags]: 
Convergence results for block-gibbs sampling?

Suppose you have some complex model you want to sample from by Markov chain Monte Carlo. There are many types of situations where you can divide your variables into, say, two groups, and efficiently and exactly sample the variables in one group conditional on the others. That is, we can sample from $ p(x | y)$ and $ p(y | x)$ and so we would run our Gibbs sampler by repeatedly drawing $x$ from $p(x|y)$ and $y$ from $p(y | x)$. Examples include the restricted Boltzmann machine, scale mixtures of Gaussians in certain image models, and some Bayesian models (one block for the state, and one for parameters). Anyway, in practice , being able to implement block Gibbs sampling seems to lead to much faster mixing of the Markov chain. But, are there any theoretical results on this? Presumably some conditions will need to be satisfied by the two distributions, which will lead to a bound on conductance , or a coupling of some sort. Thanks in advance!
