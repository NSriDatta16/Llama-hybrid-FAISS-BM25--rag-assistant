[site]: crossvalidated
[post_id]: 284395
[parent_id]: 284363
[tags]: 
"I believe it's impossible in a decision tree". Unfortunately, your belief here is incorrect. It is possible for a decision tree to improve by removing attributes, even when you are testing on the training data. Let me start with an example, and then talk a little about why this can happen. Example: Glass Data This data set is available in R in the mlbench package or it can be downloaded from the UCI Machine Learning Repository The data consists of measurements on 214 samples of glass. Each sample is described by 9 numeric attributes and one class variable (the type of glass). I will use R and the rpart package to build decision trees. With other algorithms, you might get different results. library(mlbench) ## For Glass data library(rpart) ## Decision trees library(rpart.plot) ## Nice plots data(Glass) ## First build and test a tree using all attributes GlassTree_All = rpart(Type ~ . , data=Glass) Pred_All = predict(GlassTree_All, newdata=Glass, type="class") sum(diag(table(Pred_All, Glass$Type))) [1] 168 ## Now leave out attribute 9, Fe (iron) GlassTree_Fe = rpart(Type ~ . , data=Glass[,-9]) Pred_Fe = predict(GlassTree_Fe, newdata=Glass[,-9], type="class") sum(diag(table(Pred_Fe, Glass$Type))) [1] 170 What this shows is that testing the trees on the training data, using all attributes 168 instances are correctly classified, while using the tree built without the Fe attribute, 170 instances are correctly classified. Looking at the two trees you can see that they are mostly the same, but towards the bottom of the All_Attributes tree, Fe is used. Of course, the tree without Fe is different from there down. One thinks that at each branch, the algorithm is picking the "best" attribute to split on and so making any changes would produce a poorer performing tree. But if that were really true, all decision tree algorithms would produce the same trees - and they don't. What really happens is that the algorithms use a test statistic (Information Gain, Gini) to try to pick a good attribute to split on, but there is no guarantee that it really is the best choice. The choice of the attribute determines how the remaining instances are partitioned and affects everything in the tree below the node. It is quite possible for the heuristic criteria used to choose the splitting attribute could make a bad choice that make lower parts of the tree difficult to separate into distinct classes. That happened here. Using Fe to split left no really good way to divide the remaining nodes. By removing Fe from the list of attributes, the algorithm made a different choice and the rest of the tree was better divided. This is especially likely near the bottom of the tree where there are fewer instances being distinguished at each node. Of course, if this phenomenon is possible for a single decision tree, you have to expect that a similar situation could happen with other methods that depend on decision trees, like random forests.
