[site]: crossvalidated
[post_id]: 460565
[parent_id]: 459432
[tags]: 
As you correctly recognise, during the first step 1 we cannot assign $f_{mâˆ’1}(x_i)$ to anything as we have yet to estimate $f$ . We usually set it as the mean of the $y_i$ across all the samples or some "version of the central tendency". Indeed for binary classification we use log-odds; effectively np.log(proba_positive_class / (1 - proba_positive_class)) . When we work with multi-class classification (assuming $M$ separate classes, $M$ >2) our raw predictions are of dimensions $N \times M, $ N being the number of samples. In that sense, we can calculate the log-odds for each single class label in a one-vs-all manner quite naturally using the relative frequencies of each class in our response vector. Notice that in reality given we do not assume some outlandish baseline, after the first few dozen iterations the difference will be nominal. For example, XGBoost sets its "initial guess" of the log-odds to be 0.50 and ignores the relative label frequencies. In a somewhat more educated vein, sklearn's gradient booster will set the "initial guess" of the log-odds as np.log(proba_kth_class) so not exactly the log-odds either; LightGBM follows with that logic too (i.e. boosts from average). Finally, yes, whatever the raw estimate is then we apply the softmax on it. Just be aware that for the mutli-class case we use exp(raw_preds - log(sum(exp(raw_preds)))) based on LogSumExp ; this is effectively the same as: $\frac{e^{z_i}}{ \sum_{i=1}^M e^{z_i}}$ , assuming that $z_i$ is our raw scores. Ah, and a quick example of how the softmax works: library(xgboost) data(iris) lb
