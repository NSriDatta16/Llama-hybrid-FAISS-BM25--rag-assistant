[site]: crossvalidated
[post_id]: 560242
[parent_id]: 
[tags]: 
Do "conditional" averaged coefficients *ever* make sense?

This question is related to one on Stack Overflow where the user wanted to obtain predictions and their standard errors using the "conditional" averaged coefficients from the R package MuMIn . I answered by demonstrating that if one uses vcov(model, full = FALSE) for an averaging object, the result may not even be positive definite. I further commented briefly that I don't think the conditional averages even make sense. That's really a topic outside of the SO guidelines, so I thought I'd pursue that more on CV. Briefly, in model averaging, the "full" averages are obtained by assigning values of zero as the regression coefficients for variables that are not included in a given model, and the "conditional" averages are averages only over the models where each variable is included. My thinking is that there is no conceptual difference between (1) excluding a variable from a model and (2) constraining its regression coefficient to be zero. Thus, the "full" average is exactly the average of the regression coefficients of all the models considered. Accordingly, the "conditional" averages are a distortion. Suppose, for example, we have a dozen are so models to be averaged, and only one of them, model #5, includes the predictor $x_{17}$ ; suppose that the coefficient of $x_{17}$ in model #5 is 2.50, and that model #5 has weight 0.01 in the model averaging. Then the conditional average for the coefficient of $x_{17}$ is 2.50, and the full average is 0.0250. Would one find it reasonable to make a prediction from the averaged model where $x_{17}$ 's coefficient is 2.50? I don't think that makes sense, especially considering that $x_{17}$ doesn't even appear in 99% of the models. In general, following the logic of this example, the variables that appear the least are the ones that get the most distorted influence with the conditionally averaged coefficients. So, my question is this: are the conditional averages of coefficients ever useful? And if so, why and in what context? Note: A closely related question is this , which unfortunately has received no answers. Addendum The user who posted the related SO question requests that I add this question: What happens if you have two models with similar weights (thus about 0.5 each), and one of the variables that appears in one of them has a coefficient of 90? Using the full average and by that reducing its coefficient to 45 might as problematic as using the conditional average in your example. My comment would be to consider an analogy: if I have a certain amount of money to invest, and I am told that I'll earn \$90 if I invest it in a certain fund, I should expect to earn only \$45 if I invest only half of my money in that fund. Presumably, I'd earn some other amount by investing the other half in some other fund, so that my total earnings will probably be more than \$45.
