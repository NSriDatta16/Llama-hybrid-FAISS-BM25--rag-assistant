[site]: crossvalidated
[post_id]: 551153
[parent_id]: 544048
[tags]: 
TLDR: Use mdl.train() since that uses batch statistics from the current task (but inference will not be deterministic anymore). You probably won't want to use mdl.eval() in meta-learning since that uses stats collected from (meta) training from different tasks . I believe the following is correct: BN intended behaviour: During inference (eval/testing) BN uses the running_mean, running_std collected during training (in meta-learning during meta-learning). Therefore, the (meta) eval performance might be worse if you use mdl.eval() since that would use batch statistics from different tasks (since during meta-learning the tasks at train, val and tests are different e.g. in Mini-Imagenet by having different classes available) During training the batch statistics is used but a population statistic is (attempted to be) estimated with running averages. However, the tasks are different for each split during meta-learning so you aren't using the "population statistics" you usually want. As a side note, I assume the reason batch_stats is used during training is to introduce noise that regularizes training (training under noise forces the model to be robust, since it has to perform the task even with the noise) in meta-learning I recommend using batch statistics all the time. Therefore always using mdl.train() -- even during testing/evaluation. This avoids using the running means from different task (e.g. different distributions) -- we are supposed to be seeing new tasks/distribution anyway. The price for increased accuracy/performance is loss of determinism -- but precision can be increased with a bigger (meta) batch (i.e. more tasks). This error (confidence interval) shrinks pretty quickly -- even if the sqrt(N) makes it seem it would not. Note the CI formula is z(significance_level) * std / sqrt(N) where N is the meta batch size. For accuracy std is maximally 1.0 and N can be increased to 500, which takes longer to run but the estimates become more precise. z(alpha=0.95) ~ 1.96 or z(alpha=0.99) ~ 2.58 which are fine with a bigger meta-batch. This is likely why I don't see divergence in my testing with the mdl.train() . So just make sure you use mdl.train() , since that uses batch statistics, reference: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d ) but that either the new running stats that cheat aren't saved or used later. This likely collects "cheating" statistics but it won't matter for us because we never run inference with .eval() in meta-learning. For more details see comments on question: https://stackoverflow.com/questions/69845469/when-should-one-call-eval-and-train-when-doing-maml-with-the-pytorch-highe/69858252#69858252
