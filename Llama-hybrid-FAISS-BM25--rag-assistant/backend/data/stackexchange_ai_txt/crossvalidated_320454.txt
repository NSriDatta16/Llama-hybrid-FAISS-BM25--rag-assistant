[site]: crossvalidated
[post_id]: 320454
[parent_id]: 320415
[tags]: 
Because there are important differences between polynomials and time series, let's look at this abstractly. Concepts and Notation Consider a set $S$ , a vector space $V$ , and the collection of all functions $V^S = \{a:S\to V\}$ . The two examples to keep in mind are both where $S=\mathbb Z$ is the set of integers and $V$ is either (a) $\mathbb R$ or (b) the (real-valued) random variables defined on a given probability space $(\Omega, \mathfrak F, \mathbb P)$ (which I will simply write as " $\Omega$ " below). In this situation, elements $a$ of $V^S$ are often called sequences , their values at integers $i$ are written $a_i$ instead of $a(i)$ , and are expressed in the form $$a = (a_i)_{i\in\mathbb Z} = (\ldots,\ a_{-2},\ a_{-1},\ a_0,\ a_1,\ a_2,\ \ldots).$$ The "obvious" rules for scalar multiplication and vector addition, namely $$(\lambda a)_i = \lambda (a_i)$$ and $$(a+b)_i = a_i + b_i$$ (for any $\lambda\in\mathbb R$ and $a,b,\in V^S$ ) make $V^S$ into a vector space. Further suppose there is an injective map $\sigma:S\to S$ , a so-called "shift." The one to have in mind is $\sigma:\mathbb Z\to\mathbb Z$ given by $\sigma(i)=i+1$ . This induces a map on from $V^{\sigma(S)}$ to $V^S$ , which (for clarity) I will write $[\sigma]$ , given by $$[\sigma](a)_i = a_{\sigma^{-1}(i)}.$$ Explicitly, for our example with $S=\mathbb Z$ , because $\sigma^{-1}(i)=i-1$ , the sequence $$-2\to a_{-2}, -1\to a_{-1}, 0\to a_0, 1\to a_1, 2\to a_2,\ldots$$ is mapped to the sequence $$-2\to a_{-3}, -1\to a_{-2}, 0\to a_{-1}, 1\to a_0, 2\to a_1, \ldots.$$ If you were to tabulate your sequences, like thus, $$a: \left(\array{\ldots & -2 & -1 & 0 & 1 & 2 & \ldots \\ \ldots & a_{-2} & a_{-1} & a_0 & a_1 & a_2 & \ldots }\right),$$ then applying this "backshift operator" $[\sigma]$ appears to reach back one step and pull the sequence one step to the right, assigning the immediately preceding value to the current index: $$[\sigma](a): \left(\array{\ldots & -2 & -1 & 0 & 1 & 2 & \ldots \\ \ldots & a_{-3} & a_{-2} & a_{-1} & a_0 & a_1 & \ldots }\right).\tag{1}$$ When $i$ is a time index, this procedure "looks back" one step in time. A key point is that $[\sigma]$ is a linear vector space map $[\sigma]:V^{\sigma(S)}\to V^S$ . This gives us a large set of tools from linear algebra and functional analysis for studying $[\sigma]$ . Applications We may understand a polynomial $$p = p_0 + p_1T + p_2 T^2 + \cdots + p_d T^d$$ (where " $T$ " is merely an abstract symbol) as determining a function, also written $p$ , from $\mathbb Z\to \mathbb R$ via $$p_t = p_0 + p_1 t + p_2 t^2 + \cdots + p_d t^d$$ for all $t\in \mathbb{Z}$ . (It is conventional to use " $t$ " as an index in this context to remind us of the application where $t$ indexes regularly spaced times .) The "backshift operator" $B$ refers to the induced action of $\sigma$ on $V^S = \mathbb{R}^\mathbb{Z}$ . As such, definition $(1)$ yields $$(B(p))_t = ([\sigma](p)_t) = p_{\sigma^{-1}(t)} = p_{t-1} = p(t-1) = p_0 + p_1(t-1) + \cdots + p_d(t-1)^d.$$ This answers your "more specific" question (and the rest is just a matter of algebra, working in the ring of linear operators). But let's go just a little further, because now the abstractions begin to pay off: we can immediately see the connection with time series. As another application, let $X=(X_t)$ be a time series: it's a sequence of random variables indexed by $\mathbb Z$ . Now $$(B(X))_t = X_{t-1}$$ is seen as the usual backshift operator for time series. . Uniting these applications is the idea that by taking the expectations of the $X_t$ for each $t$ separately, we map the space of time series $\Omega^\mathbb Z$ to the space of real-valued sequences $\mathbb R^\mathbb Z$ via $$E[(X)]_t = (E[X])_t.$$ Obviously this (linear) map commutes with the backshift operator: $$E \circ B = B \circ E.$$ There's nothing profound about this; I have simply stated that it doesn't matter whether you shift the sequences before or after taking expectations--that's the result I hope is obvious. Note, though, the abuse of notation: the two instances of " $B$ " in the preceding statement are linear operators on different spaces. If there's any chance of confusion--or when you're working through these ideas for the first time--it helps to remind yourself of these distinctions.
