[site]: crossvalidated
[post_id]: 109693
[parent_id]: 
[tags]: 
How to understand kernel functions and how to choose a suitable kernel?

I am trying to describe my understand of kernels in the Support Vector Machine(SVM) and why some of them are more popular, but I am not sure if I misunderstand these concepts: 1) There are a large number of kernel functions in SVM, such as RBF, polynomial, linear,etc. Generally, they can be separated into two groups: linear and non-linear. For linear kernels, they can separate data linearly in original space. Non-linear kernel functions separate data using non-linear boundaries in the original space (but after mapping, fitting the Mercer's theorem, the data are still separated linearly in the higher dimensional feature space). This makes non-linear kernel more flexible if data cannot be separated linearly in original space. 2) Compared to other non-linear kernels, why the RBF kernel is more popular in SVM? Can I assume the reason is that this kernel is infinitely differentiable (Gaussian), and this means that the data can be mapped to an infinite dimensional feature space. Other non-linear kernels can also do this, for instance, by adding parameters, a polynomial kernel can also map data to a very high dimensional feature space, but this is not convenient. If my understand is correct? 3) In Gaussian Processes (GP), we use kernel functions to measure the similarity between different data points. What characteristics of the squared exponential (SE) kernel make this kernel popular? (I know it is Gaussian and infinitely differentiable, any other reasons?) By the way, if I want to make dynamic data analysis (the data are not periodic), is this kernel still suitable and how to select a suitable kernel function? Thank you very much for your help and patience in advance.
