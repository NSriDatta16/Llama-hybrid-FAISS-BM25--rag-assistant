[site]: crossvalidated
[post_id]: 160337
[parent_id]: 160294
[tags]: 
Even if AIC or AIC(c) can be used for selecting among non-nested models, using them in the type of model building that you propose is not the best way to accomplish your goals reliably. With the multicollinearity you have among your predictors, simple attempts to select particular predictors are highly unlikely to generalize well to the underlying population, regardless of how well they fit your particular sample. The particular predictors that work with your sample might not work so well on a subsequent sample. The ability to generalize can be tested, for example, by cross-validation or bootstrapping techniques. See An introduction to Statistical Learning for ways to proceed. My guess is that the approaches in your question, except for the original PCA, would not generalize well. PCA can be a very good way to deal with multicollinearity, as correlated variables will tend to be in the same components. Regularization methods can also be used to help make models more generalizable. For example, the ridge regression method is very similar to PCA, where you essentially place a set of different weights on the principal components, instead of the all-or-none selection of components in standard PCA. If you need to select a subset of predictors, the LASSO has a better chance of doing so reliably. If your ultimate goal is prediction, note that you are probably better off erring on the side of keeping too many rather than too few predictors. Should you use such reliable model building methods and want to compare different types of models then, perhaps AIC or AIC(c) would be useful. But don't use the model-building approach suggested in your original question.
