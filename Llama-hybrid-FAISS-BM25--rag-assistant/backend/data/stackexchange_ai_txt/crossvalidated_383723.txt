[site]: crossvalidated
[post_id]: 383723
[parent_id]: 
[tags]: 
Screening data prior to PCA v. PLS

I have a very large time series matrix $X$ , where the number of observations (rows) $n$ is much smaller than the number of input variables (columns) $p$ . My aim is to use the information in $X$ to forecast future values of some target variable $Y_{t+h}$ (where $h$ may equal 0 in the nowcast case). The columns of $X$ are serially correlated and I've read in Bovin and Ng 2006 & Caggiano et al 2009 that factors drawn from a subset of $X$ perform better in real-world forecasts than factors generated using all of the columns of $X$ . This seems to be a practical consequence of the data we see in the real world -- though it creates some tension with the asymptotic theory (at least as I understand it). The methods proposed in these papers to subset $X$ seem sensible enough, but I wonder why subset $X$ and do PCA and then regression if your objective is to forecast $Y_{t+h}$ ? Why not employ PLS and use $Y_{t+h}$ to guide the extraction of factors from $X$ ? Along the way, views on the best ways to pre-screen $X$ before PCA would also be helpful.
