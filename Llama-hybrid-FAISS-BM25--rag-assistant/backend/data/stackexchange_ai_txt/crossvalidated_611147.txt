[site]: crossvalidated
[post_id]: 611147
[parent_id]: 530925
[tags]: 
The earliest reference I can find is a 1997 paper as shown - PubMed is a powerful tool to analyze trends in academic litearture. I obviously can't search "early stopping" apropos of nothing because it's apparently quite intuitive to terminate any laborious procedure early when desired precision is achieved, whether in ecological sampling, chemical and biochemical processes, medical imaging, and so on. But which terms to combine to address the specific question becomes vague, and a matter of capturing exogenous trends. Note: Cross-validation is not an algorithm which guarantees any form of "convergence". "Early stopping" according to these searches is not well defined. You may, for instance, put a "cap" on the number of iterations allowed, or allow a more generous "convergence" criterion. Regularization and cross-validation are different concepts, too. This question is tagged "machine learning" which, even itself, is a late entry to the field of methods. Prior to ML, there are, for instance, "one-step" estimators which are estimators such as GLS, GLMs, NLMMs, non-linear least squares, etc. which are estimated from iterative processes such as EM, NR, BGFS and whose theoretical results are known. A one-step estimator merely performs one iteration of the algorithm and produces an interesting and well behaved estimator in some cases whose variance is easy to express, and provides consistent tests of hypotheses. Theoretical work on one-step estimators has been explored since the 1980s.
