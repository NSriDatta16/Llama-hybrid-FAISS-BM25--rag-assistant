[site]: stackoverflow
[post_id]: 3416333
[parent_id]: 3415161
[tags]: 
Attention. Reality check ahead: Reading integers from a large text file is an IO bound operation unless you're doing something completely wrong (like using C++ streams for this). Loading 15M integers from a text file takes less than 2 seconds on an AMD64@3GHZ when the file is already buffered (and only a bit long if had to be fetched from a sufficiently fast disk). Here's a quick & dirty routine to prove my point (that's why I do not check for all possible errors in the format of the integers, nor close my files at the end, because I exit() anyway). $ wc nums.txt 15000000 15000000 156979060 nums.txt $ head -n 5 nums.txt 730547560 -226810937 607950954 640895092 884005970 $ g++ -O2 read.cc $ time ./a.out 1752547657 real 0m1.781s user 0m1.651s sys 0m0.114s $ cat read.cc #include #include #include #include int main() { char c; int num=0; int pos=1; int line=1; std::vector res; while(c=getchar(),c!=EOF) { if (c>='0' && c %d\n",sum); } UPDATE: and here's my result when read the text file (not binary) using mmap: $ g++ -O2 mread.cc $ time ./a.out nums.txt =>1752547657 real 0m0.559s user 0m0.478s sys 0m0.081s code's on pastebin: http://pastebin.com/NgqFa11k What do I suggest 1-2 seconds is a realistic lower bound for a typical desktop machine for load this data. 2 minutes sounds more like a 60 Mhz micro controller reading from a cheap SD card. So either you have an undetected/unmentioned hardware condition or your implementation of C++ stream is somehow broken or unusable. I suggest to establish a lower bound for this task on your your machine by running my sample code.
