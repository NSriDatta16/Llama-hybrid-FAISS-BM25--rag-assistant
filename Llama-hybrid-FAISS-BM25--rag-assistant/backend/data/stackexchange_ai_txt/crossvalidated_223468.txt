[site]: crossvalidated
[post_id]: 223468
[parent_id]: 
[tags]: 
Is it a good/bad idea to use test features to strengthen learning parameters?

I am fairly new to machine learning and have started implementing a few classification algorithms. From whatever I have read so far, I understand that data is absolutely crucial for training the classifier model - more the data, better it is at generalizing. Now a classifier can predict correctly or wrongly. So what I wish to know is this: Is it a good idea to use a new test feature set to modify the training parameters every time the test feature set gets correctly classified (with the intention being to strengthen the model)? What could be the problems if I tried this approach? Would there be the presence of some sort of bias or preference in the classification? Also, am I treading upon the domain of reinforcement learning here? Any thoughts upon this matter would be most appreciated...
