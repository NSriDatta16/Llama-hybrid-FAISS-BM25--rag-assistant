[site]: crossvalidated
[post_id]: 143285
[parent_id]: 
[tags]: 
How to increase the performance of random forest classifier?

I have a text classification task. These are the metrics for different languages at present: class1: 0.6823 class2: 0.7450 class3: 0.66 class4: 0.6719 How can I increase the performance of my random forest classifier in order to reach 90% accuracy? I already tried increasing the number of estimators and playing with the hyper-parameters that scikit provides, but I cannot significantly increase its performance. What hyper-parameter do I configure in order to increase its performance? This is my current setup: # For tfidf: tfidf_vect = TfidfVectorizer(norm=u'l1', use_idf=True, smooth_idf=True, sublinear_tf=False, min_df=2, stop_words=set(my_stop_words)) # For RF: rbf = RandomForestClassifier(n_estimators=10000, criterion='entropy', max_depth=10000, max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, min_density=None, compute_importances=None) What about using adaboost + random forest classifier in order to increase the performance? Is that possible?
