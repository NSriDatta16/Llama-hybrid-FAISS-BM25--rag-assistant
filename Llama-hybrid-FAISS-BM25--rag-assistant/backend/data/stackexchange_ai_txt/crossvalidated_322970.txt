[site]: crossvalidated
[post_id]: 322970
[parent_id]: 
[tags]: 
In Deep Learning, how much does randomness help generalization?

Let's assume at the beginning the NN is initialized with random weights, then the Backpropagation "shapes" the weights but the signal is strongest close to the final layer (where it gets computed) and the more the depth the more the signal gets attenuated (Vanishing Gradient) or distorted (Exploding Gradient), so the the Initial Layers should be fitted less optimally than the Final ones. This should make the initial layers be more affected by Random Initial Conditions than the Final ones Does it mean this random initialization actually helps the CNN to generalize or on the contrary does it mean the Initial Layers are trained sub-optimally hence there is space for improvement for example with different signal propagation mechanisms ?
