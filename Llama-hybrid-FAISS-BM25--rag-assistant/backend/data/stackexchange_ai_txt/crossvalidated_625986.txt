[site]: crossvalidated
[post_id]: 625986
[parent_id]: 625557
[tags]: 
To answer both your question and the comment: I am not interested in knowing why we don't want to overfit, or why we don't want to learn noise, but rather, why do the authors say that we want to learn a noisy target (deterministic + non-deterministic noise), but then says that the overfitting problem occurs because we are fitting onto the noise? Are we learning the noise or not? I'm afraid no-one, except maybe the authors, can give an answer to " why do the authors say ". But, regarding " Are we learning the noise or not ", I can offer my interpretation. From the part of the video you linked , I recon he wants to say that we want to learn the probability distribution $P(y | {\bf x})$ . Now, the probability distribution can be described by its CDF (or, alternatively, PDF) and the parameters. For example, the well-known one-dimensional normal distribution is given by its PDF: $$ f(y) = \frac{1}{\sigma \sqrt{2\pi}}\exp\left({-\frac{(y-\mu)^2}{2\sigma^2}}\right) $$ Here, $\mu$ and $\sigma$ are the parameters of the distribution. If they are constant, the whole distribution is constant, too, in the sense that it doesn't depend on ${\bf x}$ . But, we can make them functions of $\bf x$ , for example $\mu({\bf x}) = {\bf \beta x} + \beta_0$ . This is what we do in linear regression. So $P(y | {\bf x})$ describes how $y$ -values are distributed depending on ${\bf x}$ ---in case of linear regression they follow a normal distribution centred around $\mu({\bf x})$ . This is the noisy target function we want to learn. It consists of a deterministic part, $\mu({\bf x})$ , and the noise, which follows the normal distribution around zero. Overfitting happens when you mistake (part of) the noise for the deterministic part. This is usually the case when you assume the form of the deterministic part to be more complex than it really is. Returning to the linear regression example, you could assume the parameter $\mu$ to be some high-degree polynomial of $\bf x$ , while, in reality, it is (or can be better approximated by!) a simple linear function. The polynomial can fit random oscillations (" noise ") in the observed data better than a simple linear function because it has more degrees of freedom. But, as an approximation of the parameters of the true underlying probability distribution it is actually worse. So, to return to your question: Are we learning the noise or not? I believe the author wanted to say that we want to learn the distribution function behind the noise and to avoid treating the noise in the observations as the result of the deterministic part of the target function. A little side note by me: Taking the author's words literally, he seems to imply that in machine learning we try to learn the both the probability function and its parameters. In my experience, this never happens. The probability function is normally fixed in advance, based on our domain knowledge, and the only thing we try to learn are its parameters. But I doubt the author really wanted to say that. Expressing oneself precisely and clearly is high art. I apologise in advance if I wasn't precise or clear enough.
