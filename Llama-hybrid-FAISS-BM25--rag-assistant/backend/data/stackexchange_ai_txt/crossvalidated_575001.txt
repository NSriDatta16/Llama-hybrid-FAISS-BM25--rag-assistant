[site]: crossvalidated
[post_id]: 575001
[parent_id]: 
[tags]: 
How disentaglement in latent space can produce poor variety of instances in VAE..?

I'm reading about $\beta$ -VAE which essentially proposes a way to disentangle representations in the latent space. We can subjectively (I guess) identify axes carrying specific sources of variations of (for example) an image and modulate them to produce variety on generated instances. Shortly speaking what we do is modifying the weight of KL divergence in the loss function, which will be $$\mathcal{L}(\phi,\theta) = -\mathbb{E}_{\textbf{z}\sim q_\phi(\textbf{z}|\textbf{x})}[\log(p_\theta(\textbf{x}|\textbf{z})] + \beta D_{KL}(q_\phi(\textbf{z}|\textbf{x}) || p(\textbf{z}))$$ I'm trying to understand more what is happening here, if $\beta >1$ then we are posing more attention on minimizing the KL divergence, therefore we are enforcing our posterior to look closer to a normal distribution with zero mean and unit variance... how is this connected to disentaglement? Also, it has recently happened to me some sort of inverse situation (that I've noticed is pretty common): on trained (vanilla) VAEs for some of my projects, I observed that generated samples looked quite all the same, and I have partially resolved the issue by imposing a very small value of $\beta$ such as $0.0001$ . In this alternative case, how the disentaglement is related to the poor variability in generation?
