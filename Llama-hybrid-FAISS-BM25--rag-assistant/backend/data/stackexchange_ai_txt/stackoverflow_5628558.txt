[site]: stackoverflow
[post_id]: 5628558
[parent_id]: 
[tags]: 
C# predictive coding for image compression

I've been playing with Huffman Compression on images to reduce size while maintaining a lossless image, but I've also read that you can use predictive coding to further compress image data by reducing entropy. From what I understand, in the lossless JPEG standard, each pixel is predicted as the weighted average of the adjacent 4 pixels already encountered in raster order (three above and one to the left). e.g., trying to predict the value of a pixel a based on preceding pixels, x, to the left as well as above a : x x x x a Then calculate and encode the residual (difference between predicted and actual value). But what I don't get is if the average 4 neighbor pixels aren't a multiple of 4, you'd get a fraction right? Should that fraction be ignored? If so, would the proper encoding of an 8 bit image (saved in a byte[] ) be something like: public static void Encode(byte[] buffer, int width, int height) { var tempBuff = new byte[buffer.Length]; for (int i = 0; i I don't see how this really will reduce entropy? How will this help compress my images further while still being lossless? Thanks for any enlightenment EDIT: So after playing with the predictive coding images, I noticed that the histogram data shows a lot of +-1's of the varous pixels. This reduces entropy quite a bit in some cases. Here is a screenshot:
