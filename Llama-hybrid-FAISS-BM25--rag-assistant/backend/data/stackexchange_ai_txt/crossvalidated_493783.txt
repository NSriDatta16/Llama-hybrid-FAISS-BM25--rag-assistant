[site]: crossvalidated
[post_id]: 493783
[parent_id]: 493716
[tags]: 
First things first, "simulating a large number of data points with the samples from the posterior and taking the 5th and 95th quantiles of that simulated dataset" is a totally sensible approach, and the it's known as sampling from the predictive posterior distribution . Now, the Poisson distribution is not the best choice for total_qty , since total_qty is a continuous random variable, not a discrete one. I think we can improve this model as follows. Let's start with these priors: lambda_orders ~ dexp(1) # prior for avg num orders lambda_qty ~ dnorm(30, 10) # prior for avg qty ordered sigma_qty ~ dexp(1) # prior for st. dev. qty ordered [I increased the prior variance of lambda_qty , because it is the prior distribution , not the predictive distribution , so it's usually better to keep wider distributions. But if you have prior knowledge that lambda_qty is between 26 and 34 with high probability, you should keep that prior just the way you wrote it.] Then, we can keep a Poisson distribution for orders , since it represents a count. [If there's overdispersion one would prefer a negative binomial, but let's ignore it for now.] orders ~ dpois(lambda_orders) Finally, we can assume the value for each order follows a normal distribution, with mean lambda_qty and standard deviation sigma_qty . Adding a number orders of normal RVs with that distribution gives us a new variable with mean orders*lambda_qty and standard deviation orders*sigma_qty . Thus, we write: total_qty ~ dnorm(orders*lambda_qty, orders*sigma_qty) That concludes our Bayesian model! Hope it was helpful
