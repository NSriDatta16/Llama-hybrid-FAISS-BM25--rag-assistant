[site]: crossvalidated
[post_id]: 337170
[parent_id]: 313790
[tags]: 
Yes, $\ell_1$ regularization (as used by lasso) can be applied to other models too. As always, whether this will work well depends on the problem. In the context of linear regression, the lasso estimate of the coefficients is equivalent to the maximumum a posteriori (MAP) estimate assuming a Gaussian likelihood function and a Laplacian prior on the coefficients. This follows from the fact that the squared error term in the lasso cost function is proportional to the negative log of a Gaussian likelihood function. And, the lasso penalty term is proportional to the negative log of a Laplacian prior. So, minimizing the sum of the error and penalty terms is equivalent to maximizing the product of the likelihood and prior. A similar argument can be made for $\ell_1$ penalized logistic regression--this also corresponds to MAP estimation with a Laplacian prior on the coefficients. This concept can be generalized to other models: simply perform MAP estimation with a Laplacian prior on the parameters. As in the case of least squares and logistic regression, the Laplacian prior encourages parameters to be sparse as a consequence of the sharp peak at zero and heavy tails. Suppose we have data $D$, parameter vector $\theta \in \mathbb{R}^m$, likelihood function $p(D \mid \theta)$, and Laplacian prior $p(\theta \mid \lambda)$ (whose width is governed by $\lambda$). Then the MAP estimate is: $$\underset{\theta}{\text{argmax}} \enspace p(D \mid \theta) \ p(\theta \mid \lambda)$$ This is equivalent to minimizing the negative log likelihood with an $\ell_1$ penalty: $$\underset{\theta}{\text{argmin}} \ -\log p(D \mid \theta) + \lambda \|\theta\|_1$$
