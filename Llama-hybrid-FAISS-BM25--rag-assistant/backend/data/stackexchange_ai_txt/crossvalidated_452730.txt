[site]: crossvalidated
[post_id]: 452730
[parent_id]: 
[tags]: 
What is the intuition behind what Neural Networks do to data that is 1 dimensional?

Consider a data points $x \in R^1$ . I am trying to understand what it means to share parameters across a batch of data that is 1D. So in such a case we have: $$ h = \sigma( w [x^{(1)}, ..., x^{(i)}, ..., x^{(B)}] + b )$$ where $w \in R^{D_{out} \times 1}$ and $b \in R^{D_{out} \times 1}$ . If we expand it we get: $$ h = \sigma( [w x^{(1)} + b, ..., w x^{(i)} +b , ..., w x^{(B)} +b])$$ where each coordinate out the output is of the form: $$ h_{j,i} = \sigma( w_j x^{(i)} + b_j)$$ where its the $j$ th output (row) for example $i$ th (column). What is making hard for me to understand is that: The operation seems to be expanding every single number $x^{(i)} \in R^{1 \times 1}$ to a larger one $h^{(i)} \in R^{D_{out} \times 1}$ . But this expansion is weird because in the linear case this wouldn't do anything interesting because it would span the same space...so I am having an issue interpreting what might be going on it is true that $w$ and $b$ are being shared across data examples (or coordinates in the data vector) but I having difficulties understanding what that might mean. Does anyone have a good interpretation of what might such a model be doing to such data? I think I understand what convolution would do but the fact that the data is 1D and expanding it is odd...
