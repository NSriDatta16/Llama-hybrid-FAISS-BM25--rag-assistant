[site]: crossvalidated
[post_id]: 125304
[parent_id]: 124908
[tags]: 
My suggestions to your questions: 1) If you have class labels, you can just filter out those features (words) that score low on Information Theory statistics ( Information Gain, Chi Squared ). Some papers demonstrate that you just keep top 1-10% Information Gain top scoring terms and even increase classification accuracy. If you do not have class labels, that is, in unsupervised classification tasks like text retrieval or clustering, the experience in Information Retrieval (Salton's Vector Space Model) is that those terms ocurring between 1% and 10% of the documents are the most discriminative ones, that is, the ones that help best to separate the space of documents. 2) The similarity between two documents is most often computed using the cosine similarity , which is the scalar product of both vectors over the product of their norms. This meassure scales to documents of any length (that is, it allows to compare reports and tweets) and it is in the [0,1] range - allowing to say e.g. these documents are 75% similar. 3) The experience in text classification is that over-linear kernels do not pay . Classes in text classification problems are most often nearly linearly separable. For instance, Mitchell talks about the surprising effectivenes of Naive Bayes for text classification problems, being this method linear (as Naive, independence assumed). My recommendation is to train a linear model if possible, and to try random forests and deep learning as well, as they are currently showing excellent results in NLP and in Machine Learning in general.
