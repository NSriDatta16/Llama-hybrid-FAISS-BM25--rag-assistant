[site]: datascience
[post_id]: 86086
[parent_id]: 49573
[tags]: 
I have generally found that to be quite easy to do in Python without having to solve the boundary equation. (Code mostly adapted from that question in SO ) Import packages : import numpy as np from matplotlib import pyplot as plt from sklearn.linear_model import LogisticRegression Generate data : mu_vec1 = np.array([0,10]) cov_mat1 = np.array([[10,8],[8,10]]) x1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100) mu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector mu_vec2 = np.array([10,70]) cov_mat2 = np.array([[10,8],[8,10]]) x2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100) mu_vec2 = mu_vec2.reshape(1,2).T Plot generated data and save the associated figure : fig = plt.figure() plt.scatter(x1_samples[:,0],x1_samples[:,1], c= 'blue', marker='o') plt.scatter(x2_samples[:,0],x2_samples[:,1], c= 'orange', marker='o') plt.savefig('data_sample.png') Fit the logistic regression : X = np.concatenate((x1_samples,x2_samples), axis = 0) y = np.array([0]*100 + [1]*100) model_logistic = LogisticRegression() model_logistic.fit(X, y) Create a mesh, predict the regression on that mesh, plot the associated contour along simulated data and save the plot : h = .02 # step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model_logistic.predict(np.c_[xx.ravel(), yy.ravel()]) fig = plt.figure() plt.scatter(x1_samples[:,0],x1_samples[:,1], c= 'blue', marker='o') plt.scatter(x2_samples[:,0],x2_samples[:,1], c= 'orange', marker='o') Z = Z.reshape(xx.shape) plt.contour(xx, yy, Z, 1, colors='black') plt.savefig('data_contour.png') This generally works for more complex dependencies (non linearities in variables) or more complex models (svm as per linked SO answer).
