[site]: crossvalidated
[post_id]: 272269
[parent_id]: 
[tags]: 
Multilevel modelling of effects for positive values: Which distributions to use

I am currently trying to figure out what would be the best way to model a bayesian hierarchical regression for data, where the criterion value can only be positive and I am assume that the effects are not dependent on the base value for group level. I found some ideas on how to model this kind of data, however, they do not really seem to model this directly. We have measured reaction times. These must necessarily be positive. Let us also assume they are distributed according to a log-normal distribution. Furthermore, we also have one effect, which is assumed not to vary between participants (however, the overall "baseline" may be different for each participant). Now if we model the random effects as normally distributed, we would get a model similar to the one described here (Model2 without item effects): # Variables: # mu[n]: Estimated rt for trial n # rt[n]: Actual rt in trial n # sigma_e: Estimated SD of the errors # subj[n]: Subject in trial n # beta0,1: Estimated beta0 and beta1 for the population # beta0.subj[m]: Estimated difference to beta0 for subj[m] # sd.beta0,1: Estimated between subject variance for betas # x[n]: Predictor in trial n # N: Number of trials # M: Number of subjects # Priors for beta0,1; sd.beta0; sigma_e; left out for(m in 1:M) { beta0.subj ~ dnorm(0,sd.beta0) } for(n in 1:N) { mu[n] In this case, all betas and subject differences to the overall betas can take any value from the real numbers. However, the interpretation of the betas (and mu[n]) would be as a multiplicative factor. I.e. a beta1=1 would indicate that participants take exp(1) times as much time if x=1 . So a participant who has a beta0.subj=0 would be modelled to have another rt-difference between measurements where x=0 and x=1 compared to participant who has a beta0.subj=1 . Another possibility I could come up with would be to model the errors instead of the value distribution. But in that case, I cannot really find a way to model the constraint that all values must be positive: for(m in 1:M) { beta0.subj ~ dnorm(0,sd.beta0) # Probably not the correct distribution } for(n in 1:N) { log.rt[n] Now the beta1 really reflects the same amount of difference, independently of the difference to the overall mean. However, there is nothing in the model, which keeps mu[n] from becoming negative. At least for Stan, this would mean the sample is rejected in that case (not sure if JAGS handles it the same way), but I think it would be much more elegant to constrain the values during sampling, so that only estimates which are possible are taken. Is there a better way to handle this problem? Update: Clarification : As pointed out in the comments, there are some good choices for the variance parameters put forth by Gelman et al. (various papers and manuals). However, I am not so much interested in the variance parameters, but rather in confining domain of the posteriors to a meaningful range. As a similar example let us actually take the 8-school example: The dataset is as follows (taken from here ): school estimate sd A 28 15 B 8 10 C -3 16 D 7 11 E -1 9 F 1 11 G 18 10 H 12 18 The model Gelman et al. propose for this is as follows (Stan manual 2.14 p.201): parameters { real theta[J]; // per-trial treatment effect real mu; // mean treatment effect real tau; // deviation of treatment effects } model { y ~ normal(theta, sigma); theta ~ normal(mu, tau); mu ~ normal(0, 10); tau ~ cauchy(0, 5); } Which has the half-Cauchy on tau mentioned in the comments. Now my problem with that model is, that there is nothing in the model, which confines the estimates to values which are consistent. In this case the data is drawn from testing scores in the range between 200 and 800. However, in general this model would also produce a posterior value for e.g. p(theta[1] = 700|y,sigma) , which would be meaningless (even if the school had the lowest score before, after the training it would have an impossible high score). Of course the data is a limiting factor here, driving the value for the such high effects very close to zero. In the general case I cannot rely on this though. Furthermore, the limits may of course be specific to the measurement on a by case basis. Taking again the 8-school example, one could of course put a constraint on the thetas. However, that only gives a very rough confinement for invalid parameters. In my case I am both trying to estimate the baseline RT (without the manipulation) and the effect (currently only fixed, but that will also be a future addition). Now the estimate for the baseline RT and the effect must be consistent with each other. If I have a participant with a baseline RT of 100ms and a speedup of 105ms due to the manipulation (others may have higher baseline RTs making 105ms a good estimate of the effect for those participants), that would mean the mean RT for that participant after manipulation would be predicted as -5ms, which is of course not possible. However, I cannot reflect that constraint in the model and keep if from sampling in such inconsistent areas of the parameters space. The lognormal model above on the other hand nicely reflect these constraints. A relative improvement can never lead to negative RTs, independently of the baseline RT of each participant. But then the meaning of the effect is completely different (please don't challenge the assumptions of the effects being additive instead of relative. This is the way they are currently described, and this actually an assumption I am trying to challenge).
