[site]: datascience
[post_id]: 110850
[parent_id]: 110802
[tags]: 
I am not sure PCA is quite what you are after. I think it may help to visualize what you are after. I think the image is as follows for 5 features with 2 records (i.e. 2 rows 5 columns): Here, because there are only two records, features are 2D vectors. Would you be after capturing A, B, C in one cluster and D, E in another? If so, I think you should simply do eigenvalue decomposition of the covariance matrix. That will give you the eigenvectors along which you have greatest variance. For the example above, I would have 5x5 covariance matrix, with two eigenvectors that have large eigenvalues, and 3 more that have very small eigenvalues. The eigenvectors with large eigenvalues are then your clustering target, project your feature vectors along these eigenvectors. e.g. if $V_1$ and $V_2$ are your two eigenvectors, then compute magnitudes of dot-products $\left|A.V_1\right|$ and $\left|A.V_2\right|$ . Then assign feature $A$ to cluster 1 if $\left|A.V_1\right|>\left|A.V_2\right|$ and vice versa. Depending on your data it may be a good idea to group features that are aligned with eigenvectors that have very similar eigenvalues (as a form of regularization). PS: PCA does similar things but is geared toward a different purpose (i.e. some of the information provided by SVD of a design matrix, is similar to what you get from diagonalization of the covariance matrix)
