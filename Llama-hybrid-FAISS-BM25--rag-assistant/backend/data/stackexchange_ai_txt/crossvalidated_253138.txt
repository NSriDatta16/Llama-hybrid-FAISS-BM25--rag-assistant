[site]: crossvalidated
[post_id]: 253138
[parent_id]: 253137
[tags]: 
Firstly, k-means cluster analysis is mostly for continuously-scaled features (i.e. "means" or averages), since categorical data don't have averages. In addition, regarding probability distributions, the normal and standard normal distributions have 2 parameters: mean and variance -- hence, you commonly find averages (means) associated with analyses involving continuous data only. Categorical variables are straightforwardly nominally-scaled, for which there is no mathematical difference between groups (chevy, dodge, ford trucks; or red, green, blue without reference to 0-255 RGB bytes) -- and therefore averages are not determined. Ordinal is another type of categorical variable. The fastest way to understand K-means is to envision a centroid (set of averages) for each cluster. If you have 10 continuous variables, and $K=5$, then there are 5 centroids, each of which include a set of 5 averages, and for each cluster, the averages of the 5 features is based on the objects whose feature values for those 5 features are closest to a given centroid. Here's how K-means works. Assume a classroom with 25 students. At first, randomly assign the students to stand in each of the four corners of the room. Next, ask students to look around the room and migrate to a corner for which hair color, eye color, belt color, and pants/dress color is similar. Then, after this first step, repeat migration to a corner for which students match better and there is more purity among hair color, eye color, belt color, and pants/dress color within the corner. Keep doing this until no student needs to migrate to another corner for a better match. When done, there will be $K=4$ clusters, and the cluster a student is in is the "assigned" cluster, for which there is greater similarity among feature values. Keep in mind for the above example, if there are only two extremes for hair color, eye color, belt color, and pants/dress color, then only two corners would be needed, i.e. $K=2$. A major issue for KM cluster analysis is that the number of clusters (centroids) $K$ has to be set, so the clusters that are discernable in the data (based on centroids, where each $k$th cluster has its own set of means for the $p$-features involved) depends on $K$. Since you're using MATLAB, look into their "cluster validity" methods, for which the optimal number (i.e. $K$) can be identified. The silhouette index is one of the better metrics, and for a single dataset, there will be a silhouette index value for each value of $K$ (number of clusters) which is preset by the user prior to the analysis. The way to see the results when done is to plot the object-to-centroid distances (Euclidean) for the cluster to which each object is assigned, once the optimal value of $K$ has been found via cluster validity.
