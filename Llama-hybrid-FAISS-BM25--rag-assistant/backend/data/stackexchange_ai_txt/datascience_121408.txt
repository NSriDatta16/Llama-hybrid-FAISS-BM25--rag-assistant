[site]: datascience
[post_id]: 121408
[parent_id]: 
[tags]: 
Were any LLMs trained on Google books?

An important limiting factor on the performance of large language models, is the amount of training text available. Of course, using e.g. the Gutenberg archive of public domain books is an obvious thing to do. But the majority of extant books are in copyright. Over the last couple of decades, Google has scanned something like forty million books. Most of these are in copyright, so there are legal obstacles to making them available for download. As far as I know, this is the largest collection of high-quality text in existence. They provide online access to it on a very limited basis. I haven't heard of them ever giving or selling a complete copy to anyone; not sure they would have legal permission to do so. Has any LLM been trained, either by Google or anyone else, on this corpus?
