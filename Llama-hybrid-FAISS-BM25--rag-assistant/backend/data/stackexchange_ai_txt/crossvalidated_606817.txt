[site]: crossvalidated
[post_id]: 606817
[parent_id]: 
[tags]: 
Accelerate the fitting of an ECM-GARCH model by computing MLE gradient numerically?

I'm trying to fit an ECM model with variance following a GARCH-DCC model (GARCH with dynamic cross correlation). It has 16 parameters for 2 assets (ECM : 4 gammas, 2 lambda, GARCH: 2 alphas, 2 beta, 2 omega, DCC: alpha and beta). Since I'm pretty new in econometrics, I don't know really much what amount of time this should take. At the moment, my log-likelihood function takes 800ms to compute the log-likelihood of 2 time series of 10k points each with given ECM/GARCH-DCC parameters. Despite I find that at first sight, it's a lot, honestly I really don't know if it's the time scale one should expect for such an operation.. In order for np.minimize function to converge to a local minimum, it takes 40 minutes, with this output : Warning: Desired error not necessarily achieved due to precision loss. Current function value: 58015.695124 Iterations: 112 Function evaluations: 2475 Gradient evaluations: 165 As we see it needed to evaluate 2475 the function to have only 112 algo iterations. I don't know much about BFGS, but I guess it needs somehow to approximate the gradient/hessian by calling the loss function 16 times per iteration and get the 16 partial derivatives. Then my question is : how to accelerate this process ? Maybe I should use Tensorflow instead of numpy, that would take advantage of the GPU ? Profiling my loss function shows that np.dot and np.inv takes a lot of time. How drastically this could accelerate the process ? Is it worth taking the time to convert all my code to TF or not really ? main question : Is it possible to have a numerical expression for the gradient of an ECM with variance ~ GARCH-DCC ? This would then require the optimizer one .getGradient call instead of 16 loss calls. I can use an implementation of other optimizers in Python than np.minimize, that would allow me to give the gradient function along with the loss function. For example, this implementation of Adam algo: https://machinelearningmastery.com/adam-optimization-from-scratch/ Or maybe it's just me and 40 minutes is a normal time scale one could expect to find a local minimum to a ECM / GARCH-DCC model with 2 TS of 10k points.
