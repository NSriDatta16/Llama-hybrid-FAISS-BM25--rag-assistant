[site]: crossvalidated
[post_id]: 266274
[parent_id]: 266249
[tags]: 
Boosting is different from bagging (voting). I do not see a way to interpret boosting as "voting" (see my edit for additional details). Voting (especially majority vote) usually means combined decision from "separate / less correlated" week classifiers. In boosting, we are building one classifier upon another. So, they are not "separate peers" but one is "less weaker than another". My answers here gives boosting break down by iterations. How does linear base leaner works in boosting? And how it works in xgboost library? The example is trying to approximate a quadratic function by by boosting on decision stump. First two plots are ground truth and boosting model after many iterations. They are contour plots. X and Y axis are two features and function value is represented by color. Then I am showing first 4 iterations. You can see we are not averaging/voting 4 models, but enhance the model over each iterations. After seeing another answer, I feel the answer to this question depends on how we define "voting". Do we consider weighted sum as voting ? If yes, then I think we still can say boosting can be generalized with voting.
