[site]: crossvalidated
[post_id]: 137081
[parent_id]: 112451
[tags]: 
Say you have some data $X$ that comes from Normal distribution with unknown mean $\mu$ . You want to find what is the value of $\mu$ , however you have no idea how to achieve it. One thing you could do is to try several values of $\mu$ and check which of them is the best one. To do this you need however some method for checking which of the values is "better" then others. The likelihood function, $L$ , lets you to check which values of $\mu$ are most likely given the data you have. For this purpose it uses probabilities of your data-points estimated under a probability function $f$ with a given value of $\mu$ : $$ L(\mu|X) = \prod^N_{i=1} f(x_i, \mu) $$ or log-likelihood: $$ \ln L(\mu|X) = \sum^N_{i=1} \ln f(x_i, \mu) $$ You use this function to check which value of $\mu$ maximizes the likelihood, i.e. which is the most likely given the data you have. As you can see, this can be achieved with product of probabilities or with sum of log-probabilities (log-likelihood). In our example $f$ would be probability density function for normal distribution, but the approach can be extended into much more complicated problems. In practice you do not plug-in some guessed values of $\mu$ into the likelihood function but rather use different statistical approaches that are known to provide maximum likelihood estimates of the parameters of interest. There are lots of such approaches that are problem-specific - some are simple, some complicated (check Wikipedia for more information). Below I provide a simple example of how ML works in practice. Example First lets generate some fake data: set.seed(123) x and define a likelihood function that we want to maximize (the likelihood of Normal distribution with different values of $\mu$ given the data $X$ ): llik next, what we do is we check different values of $\mu$ using our function: ll The same could be achieved faster with an optimization algorithm that looks for the maximum value of a function in a more clever way that going brute force . There are multiple such examples, e.g. one of the most basic in R is optimize : optimize(llik, interval=c(-6, 6), maximum=TRUE)$maximum The black line shows estimates of log-likelihood function under different values of $\mu$ . The red line on the plot marks the $1.78$ value that is exactly the same as the arithmetic average (that actually is maximum likelihood estimator of $\mu$ ), the highest point of log-likelihood function estimated with brute force search and with optimize algorithm. This example shows how you can use multiple approaches to find the value that maximizes the likelihood function to find the "best" value of your parameter.
