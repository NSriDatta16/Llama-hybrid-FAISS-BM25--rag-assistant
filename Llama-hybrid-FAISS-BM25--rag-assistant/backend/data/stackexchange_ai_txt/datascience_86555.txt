[site]: datascience
[post_id]: 86555
[parent_id]: 
[tags]: 
Machine Learning - Precision and Recall - differences in interpretation and preferring one over other

I have summarising this from lot of blogs about Precision and Recall. Precision is: Proportion of actual positives that classifier has predicted as positive. meaning out of the sample identified as positives by classifier as positive, how many are actually positive? and Recall is: Proportion of actual positives were predicted as positive correctly. meaning out of the ground truth positives, how many were identified correctly by the classifier as positive? That sounded very confusing to me. I couldn't interpret difference between both of them and relate each to real examples. some very small questions about interpretation I have are: if avoiding false-positives matter the most to me, i should be measuring precision; And if avoiding false-negatives matters the most to me, i should be measuring recall. Is my understanding correct? Suppose, I am predicting if a patient should be given a vaccine, that when given to healthy person is catastrophic and hence should only be given to an affected person; and I can't afford giving vaccine to healthy people. assuming positive stands for should-give-vaccine and negative is should-not-give-vaccine, should I be measuring Precision? or Recall of my classifier? Suppose, I am predicting if an email is spam(+ve) or non-spam(-ve). and I can't afford a spam email being classified as non-spam, meaning can't afford false-negatives, should I be measuring Precision? or Recall of my classifier? What does it mean to have high precision(> 0.95) and low recall( 0.95) and high recall( Put simply, in what kind of cases is to preferable or good choice to use Precision over Recall as metric and vice versa. I get the definition and I can't relate it to real examples to answer when one is preferable over other, so I would really like some clarification.
