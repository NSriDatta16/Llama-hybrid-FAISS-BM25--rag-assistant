[site]: crossvalidated
[post_id]: 369949
[parent_id]: 
[tags]: 
Sampling Bayes factors under the null hypothesis to estimate a threshold of "significance" for hypothesis testing

Context: I have a psychology experiment with a 2 x 2 design (with Condition (label, no label) and ContrastType (head, tail) as my two factors) where I want to estimate whether the mean of each subgroup is different from zero, using Bayes factors. In the sample theory based framework I would do something like this in R: library(lmer); library(emmeans) model Now, if I want to have Bayes factors instead of $p$ -values, I would use brms::hypothesis like that: library(brms) p 0", "Intercept + ContrastTypeTail > 0", "Intercept + ConditionNo Label > 0", paste("Intercept +", "ConditionNo Label +", "ContrastTypeTail +", "ContrastTypeTail:ConditionNo Label", "> 0"))) The problem here is that, with my data, I have Highest Posterior Density Intervals not far from zero and non-significant results from lmer + emmeans , but fairly high Bayes factors for a psychology experiment (between 10 and 46). I was a bit surprised and I ran some simulations on my design to get a better idea of the likelihood of observing such $b$ -values under the null hypothesis (full code at the bottom). What I found was that the 95th percentile for Bayes factors was 14.5, and 25.5% of the Bayes factors from my simulations where above 3 (the somewhat common threshold value for results worth mentioning). Questions: Is it okay/a good idea to use the 95th percentile (or whatever other value) as a threshold to interpret my null hypothesis test Bayes factors as being worth mentioning or not? If I do so, is it better to run simulations with a larger sample size on my design, or use the actual sample size I have (that is, 69 observations, unequally divided between the different cells)? I also saw this question but the link to the course is broken and I'm not very familiar with the idea of minimal Bayes factor. The reason why I want to report Bayes factors is that Bayesian analysis is not yet too widespread in psychology, and I'm not sure how publishable would be an article without any kind of explicit hypothesis testing, sadly... Simulation code (a bit long): # LIBRARY IMPORTS ======================================================================= library(lme4) library(emmeans) library(brms) library(tidyverse) # BAYESIAN POINT NULL HYPOTHESIS TESTING ================================================ H_naught.test % as_tibble() return(t) }) return(bind_rows(e)) } # Define Bayesian analysis function, returning hyp. test bf bayesian.analysis 0", "Intercept + ContrastTypeTail > 0", "Intercept + ConditionNo Label > 0", paste("Intercept +", "ConditionNo Label +", "ContrastTypeTail +", "ContrastTypeTail:ConditionNo Label", "> 0"))) return(as_tibble(bf$hypothesis)) }) bf % mutate(Condition = factor(ifelse(grepl("NoLabel", Hypothesis), "No Label", "Label")), ContrastType = factor(ifelse(grepl("Tail", Hypothesis), "Tail", "Head"))) bayesian.time $BayesianEvidence$ Evid.Ratio, .95) stb.95 $SampleTheoryEvidence$ p.value, .05) bayesian.above3 $BayesianEvidence$ Evid.Ratio > 3) / 400 stb.bellow_dot05 $SampleTheoryEvidence$ p.value
