[site]: crossvalidated
[post_id]: 497247
[parent_id]: 
[tags]: 
LSTM: Best way to handle categories, in practice

There is an issue emerging in the practical use of Long-Short-Term-Memory (LSTM) Deep Neural Nets (DNN) with my use case. In typical machine learning scenarios one encounters in benchmark datasets, or in image analysis, there is full availability of categorical instances during training, and no previously unseen categories at test time or run time. That is to say, specifically, if there is a categorical column known as Fruits containing { 'apples', 'oranges'} in the currently available training set, then it is perfectly ordinary, during live operation, to encounter a novel type of Fruit such as Durian for which no training data existed. training_fruits = set(training_data['Fruits']) print(training_fruits) {'apples', 'oranges'} # Some time passes ... runtime_fruits = set(runtime_data['Fruits']) print(runtime_fruits) {'pears', 'oranges', 'durians'} A simple example of this use case might be a category of good where new products are released in the market every few months. In normal "machine learning", I can handle this in several ways. But the first step in any of these ways is to use a hashing algorithm to hash categorical values into some int32. This means that novel categorical values are encodable in a regularized fashion even when they have never been seen by the model or production infrastructure before. And, in my particular domain, results are perfectly satisfactory if a novel category is encountered...and substantially worse in general if categories are eliminated entirely. In short, a problem similar to the background class problem emerges: the features are insufficiently complex to describe the variety of models needed. There is a one-to-many relationship between input parameters and predictions, so the model chooses one particular function to model, and stays there. However, it seems like this "hashing trick" will not work at all for LSTM DNN. Further, in my particular use case, and the example above, there does not seem to be enough "support" in the categorical strings themselves to compute an embedding. Short of dropping the categorical information, or ruling Deep Learning out entirely, what is the best practice for handling this scenario, and encoding the categories for use by the LSTM?
