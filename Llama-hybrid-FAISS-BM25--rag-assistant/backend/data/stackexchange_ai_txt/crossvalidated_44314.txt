[site]: crossvalidated
[post_id]: 44314
[parent_id]: 44313
[tags]: 
Let's say you have principal components $v_1$ through $v_n$ . Any vector in $n$ -space can be translated into the basis of those principal vectors: $x = x_1v_1 + \dots + x_nv_n $ When you reduce a vector in $n$ -space to $d$ -space, you are projecting onto the first $d$ principal components and zero-ing out all the rest: $\hat x = x_1v_1 + \dots + x_dv_d + 0v_{d+1} + \dots + 0v_n$ So if you want to know the error, that would be all the zeroed out parts. I suspect the easiest way to do this is: given $x$ and precomputed principal components $v_1 \dots v_n$ . $x_1 \dots x_d := $ projection of $x$ onto first $d$ principal components $\hat x := x_1v_1 + \dots + x_dv_d $ $\|x-\hat x\|^2$ is the squared error of dimensionality reduction
