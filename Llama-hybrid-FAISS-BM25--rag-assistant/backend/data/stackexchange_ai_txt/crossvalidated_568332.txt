[site]: crossvalidated
[post_id]: 568332
[parent_id]: 
[tags]: 
Sequential Bayesian updating with binary data (in a case where a beta-bernoulli setup seems inappropriate)

An unknown parameter $\theta$ is randomly drawn at time $t=0$ according to prior p.d.f. $\mu_0(\cdot)$ that has support $[L,R]\subseteq\mathbb{R}$ . At each time $t\in\{1,2,...\}$ an agent makes an estimate $a_t\in\mathbb{R}$ of $\theta$ and observes an outcome $y_t\,|\,(a_t,\theta) \sim\text{Bernoulli}(\sigma(a_t,\theta))$ , where $\sigma:\mathbb{R}^2\to[0,1]$ yields the probability of success given $(a_t,\theta)$ . (Assume $y_t$ is independent of all draws $\{y_\tau\}_{\tau\neq t}$ and estimates $\{a_\tau\}_{t\neq t}$ from other periods.) Let $\mu_t(\cdot|a_1,y_1,...a_t,y_t)$ denote the posterior belief (formed using Bayes' rule) after observing the sequence $\{(\color{red}{a_\tau},\color{blue}{y_\tau})\}_{\tau=1}^t$ of $\color{red}{\text{estimates}}$ and $\color{blue}{\text{outcomes}}$ . My (Soft) Question: which $\mu_0$ and $\sigma$ would make analytically calculating posterior belief $\mu_t(\cdot|\cdot)$ tractable? [ Edit: An example and a simpler question is found below. Many thanks to user @Tim for the great suggestion!] Small Request: $\sigma$ could indeed be "anything," but ideally I would like something that is monotonically decreasing in how "far'' $a_t$ and $\omega$ are. (E.g. $\sigma(a_t,\theta)=\frac{1}{1+(a_t-\theta)^2}).$ To set ideas, I will now describe a class of functions that seems "reasonable" to me. Let $\sigma(a_t,\theta)=h(\theta-a_t)$ where $h:R\to[0,1]$ satisfies $h(0)=1>\max_{x\in\mathbb{R}\setminus\{0\}}$ and $h(x)=h(-x) \text{ and } h'(x)= -h'(-x) \ \forall x\in\mathbb{R}$ . (i.e. unimodal and symmetric about $x=0$ , where it achieves a maximum value of $1$ ). $\textbf{Example:}$ Suppose a $\text{Uniform}([0,1])$ ( $\equiv \text{Beta}(1,1)$ ) prior $\mu_0(\hat{\theta}):= \mathbb{1}_{[0,1]}(\hat{\theta})$ and $\sigma(a_t,\theta):=\frac{1}{1+(a_t-\theta)^2 k}$ , where $k>0$ is a known constant. Then, given $\theta$ and the first estimate $a_1$ , the first outcome $y_1$ has a $\text{Bernoulli}\left([{1+(a_t-\theta)^2 k}]^{-1}\right)$ distribution. After making estimate $a_1\in\mathbb{R}$ and observing outcome $y_1\in\{0,1\}$ , the agent forms posterior belief \begin{equation} \mu_1(\theta|a_1,y_1) = \begin{cases} \frac{1-[1+(a_1-\theta)^2 k]^{-1}}{1-\int_{0}^{1} [1+(a_1-\hat{\theta})^2 k]^{-1} d\hat{\theta}}\mathbb{1}_{[0,1]}(\theta) &, \text{if } y_1 = 0\\ \frac{[1+(a_1-\theta)^2 k]^{-1}}{\int_{0}^{1} [1+(a_1-\hat{\theta})^2 k]^{-1} d\hat{\theta}}\mathbb{1}_{[0,1]}(\theta) &, \text{if } y_1 = 1 \end{cases} \end{equation} where the above follows from Bayes' rule (after some cancellations and simplifications). Focusing on the $y=1$ case is sufficient for demonstrating that the resultant posterior belief is not a $\text{Beta}(\cdot,\cdot)$ distribution. Simplifying yields \begin{equation} \mu_1(\theta|a_1,y_1) = \frac{\sqrt{k}}{1+(\theta-x)^2} \frac{\mathbb{1}_{[0,1]}(\theta)}{\arctan\left(\frac{\sqrt{k}}{1-(k)(1-a_1)(a_1)}\right)}. \end{equation} I don't think I made an error in my derivation. The above is quite complicated, and I am afraid will get more complicated in $t>1$ . At the root of these complications is $\sigma(\cdot,\cdot)$ , so I suppose my question should focus on that. Simplified Question: assuming a $\text{Beta}(\alpha,\beta)$ , prior which choice of $\sigma(\cdot)$ guarantees that the posterior is also $\text{Beta}(\cdot,\cdot)$ ? (Feel free to assume $\alpha=1=\beta$ , i.e. $\text{Uniform}([0,1])$ ).
