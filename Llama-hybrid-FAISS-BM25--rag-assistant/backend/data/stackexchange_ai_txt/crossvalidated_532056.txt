[site]: crossvalidated
[post_id]: 532056
[parent_id]: 424663
[tags]: 
The standard version such as used in AlphaGo and AlphaZero do propose moves for themselves and the opponent the same way (i.e. based on the policy predictions of the neural network, and then updated based on trying moves per the policy and seeing how they turn out). EDIT: I just realized you did not specify that we're talking about a setting with a neural network providing policy predictions, so maybe not entirely standard in a more generic "let's just play out lots of options without any skew towards better/more plausible moves". This does not mean one could not try to do something clever to e.g. deal with situations where there are many equivalent options (e.g. in chess in a drawn endgame where a lot of options keep the draw, or to find new chess opening ideas in equal positions with many options where maybe some options are more likely to lead to human errors). You could imagine doing that using a neural network trained to imitate the moves of humans and to - to some extent - evaluate the options predicted by such a network in addition to those by stronger network trained using RL.
