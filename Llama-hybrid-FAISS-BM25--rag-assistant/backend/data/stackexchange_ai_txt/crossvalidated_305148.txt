[site]: crossvalidated
[post_id]: 305148
[parent_id]: 
[tags]: 
What are the units of entropy of a normal distribution?

I have a random process that follows a normal distribution. It's parameters are mean = 35 units, std.dev. = 8 units. I've seen from the wiki entry for the normal distribution that there is a formula to calculate the entropy. So plugging in the figures as:- $$ .5\log\left(2\pi e^1 8\cdot 8\right) $$ I get a value of 1.52, which I take to be per sample. My question is what are these units? What thing do I have 1.52 of? Information entropy is (typically) measured in units of bits, after Claude Shannon's definition. So can I take it that each sample generates 1.52 bits of entropy? Clearly recording those samples generates information and therefore occupies a real and discrete amount of storage space. Ergo entropy cannot be unit less.
