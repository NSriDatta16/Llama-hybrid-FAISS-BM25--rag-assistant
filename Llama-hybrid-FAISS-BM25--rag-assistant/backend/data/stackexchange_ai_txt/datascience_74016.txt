[site]: datascience
[post_id]: 74016
[parent_id]: 
[tags]: 
Is this an implementation of Reinforcement learning?

I've written code in Python to replicate the results of a brief and simple paper about reinforcement learning . A brief description of the problem: we have a generated time series of returns (that becomes the succession of states), a set of discrete actions (1,0,-1), and a linear approximation of the action-value function. So there is an offline learning algorithm that computes the action maximizing the action-value function approximator then computes the reward and receive the new state. Then it updates the parameters based on TD(0). My question is here: is this enough to classify this as RL? import numpy as np import matplotlib.pyplot as plt import pandas as pd import random import math from math import atan from numpy import array import statistics #preliminary functions: price to returns and an approximation of the gradient def PtoR(x) : x_1=x[1:] x_0=x[:(len(x)-1)] xx=(x_1-x_0)/x_0 return(xx) def der(f, x, der_index=[]): # der_index: variable w.r.t. get gradient epsilon = 2.34E-11 grads = [] for idx in der_index: x_ = x.copy() x_[idx]+=epsilon grads.append((f(x_) - f(x))/epsilon) return grads #generation of datas (time series) beta,z=np.random.random(2).reshape(-1,1) for i in range(1, 5001): a, b=np.random.randn(2) beta=np.append(beta,0.9*(beta[i-1])+b) z=np.append(z, z[i-1]+beta[i-1]+3*a) data=np.exp(z/(max(z)-min(z))) plt.plot(data) ret=PtoR(data) #possible actions a=(1,0,-1) #Action-value function approximator def qfun(par): return(par[0]+atan(par[7])*par[1]+atan(par[8])*par[2]+atan(par[9])*par[3]+atan(par[10])*par[4]+atan(par[11])*par[5]+atan(par[12])*par[6]) #Inizialization of parameters pars=np.random.random(7) gamma=0.7 alpha=0.08 azione=(ret[0:5]>0)*1-1*(ret[0:5] qfun(np.append(pars,np.append(-1,st)))): if(qfun(np.append(pars,np.append(1,st)))>qfun(np.append(pars,np.append(0,st)))): az=array(1) else: az=array(0) elif(qfun(np.append(pars,np.append(-1,st)))>qfun(np.append(pars,np.append(0,st)))): az=array(-1) else: az=array(0) #epsilon-greedy exploration/exploitation azione=np.append(azione,np.random.choice(np.append(az,a),1,p=(0.94,0.02,0.02,0.02))) #receive state and reward at t st1=np.array((ret[i-4],ret[i-3],ret[i-2],ret[i-1], ret[i])) reward=np.mean(azione[(i-4):(i+1)]*st1)/statistics.stdev(azione[(i-4):(i+1)]*st1) #verify what would have been the best action if(qfun(np.append(pars,np.append(1,st1)))>qfun(np.append(pars,np.append(-1,st1)))): if(qfun(np.append(pars,np.append(1,st1)))>qfun(np.append(pars,np.append(0,st1)))): azione_g=array(1) else: azione_g=array(0) elif(qfun(np.append(pars,np.append(-1,st1)))>qfun(np.append(pars,np.append(0,st1)))): azione_g=array(-1) else: azione_g=array(0) #update the parameters (not sure if the cicle here is useful or correct) for u in range(0,10): delta=reward+gamma*qfun(np.append(pars,np.append(azione_g,st1)))-qfun(np.append(pars,np.append(azione[i],st1))) gradiente=der(qfun, np.append(pars,np.append(azione[i],st)), der_index=range(0,7)) for t in range(0,7): pars[t]=pars[t]+alpha*delta*gradiente[t] returns=np.append(returns,returns[i-1]+azione[i]*ret[i])
