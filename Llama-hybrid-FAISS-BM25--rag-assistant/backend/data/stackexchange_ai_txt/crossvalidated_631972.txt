[site]: crossvalidated
[post_id]: 631972
[parent_id]: 
[tags]: 
Searching for a proper way to reduce the dimensionality of activations from a CNN

I am conducting an analysis to compare the similarities between different images across early and late layers in a CNN. The model I am working with is the pretrained DenseNet121 that comes with pytorch. My procedures are like this: Register forward hooks on features maps transition1.norm , transition2.norm and transition3.norm ; Use the registered model to identity a 400Ã—400 image; Take the activations from the registers and compute cosine similarities between images. However, the activations are large in sizes after Tensor.flatten() . In my case, the size of transition1.norm is 2,560,000, the size of transition2.norm is 1,280,000, and the size of transition3.norm is 640,000. These data devoured my disk space and slowed down my computation. I found actually a large part of the data are composed by zeros. As shown below: tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [3.3772e-02, 3.3772e-02, 3.1638e-02, ..., 3.1638e-02, 3.1638e-02, 3.2013e-02], [3.3772e-02, 3.3772e-02, 3.1638e-02, ..., 3.1638e-02, 3.1638e-02, 3.2013e-02], ..., [3.3772e-02, 3.3772e-02, 3.1638e-02, ..., 3.1638e-02, 3.1638e-02, 3.2013e-02], [3.3772e-02, 3.3772e-02, 3.1638e-02, ..., 3.1638e-02, 3.1638e-02, 3.2013e-02], [6.6977e-01, 7.4104e-01, 7.4104e-01, ..., 7.4104e-01, 7.4104e-01, 7.4104e-01]], ... TRUNCATED ... [[4.4522e-01, 2.8640e-01, 2.7194e-01, ..., 3.1157e-01, 1.7369e-01, 6.2406e-02], [1.2590e-01, 2.1647e-02, 9.2064e-02, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.8316e-02, 6.2189e-02, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], ..., [0.0000e+00, 0.0000e+00, 3.5624e-01, ..., 9.1325e-02, 6.8744e-02, 0.0000e+00], [0.0000e+00, 0.0000e+00, 3.3750e-01, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [8.0560e-02, 1.7131e-01, 4.5834e-01, ..., 3.0711e-02, 5.3941e-02, 1.1774e-01]]]], device='cuda:0') My question is: except for simply removing the dimensions where 0 appears for every stimuli, what are the most efficient and reliable ways to reduce the dimensionality of the activations without losing or distorting information from the original activations? Edit: To be clearer, I am not only asking about the methods to reduce dimensionality. The question is about whether there are standard procedures in deep learning to address liking questions, given that I only have experiences in statistics but not in deep learning and CNN etc.
