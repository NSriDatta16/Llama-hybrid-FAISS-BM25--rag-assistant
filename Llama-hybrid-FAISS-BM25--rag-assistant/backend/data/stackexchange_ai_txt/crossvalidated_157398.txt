[site]: crossvalidated
[post_id]: 157398
[parent_id]: 
[tags]: 
What is the purpose of the 'convenience' 1/2 fraction on the sum of squared errors?

I was studying the first chapter of Pattern Recognition and Machine Learning, by Christopher Bishop, and in the presentation of the sum of squared errors function $$ E(w) = \frac{1}{2}\ \sum\limits_{n=1}^N \{y(x_n,w) - t_n\}^2 $$ where $y$ is the polinomial function being modelled, $x$ its variable, $w$ the polinomial coefficients to be discovered and $N$ the size of the training set. In this book, and in other situations such as the Andrew Ng's video lectures, this $ \frac{1}{2} $ is "included for later convenience". Which convenience is that? I'm struggling to ignore this but, I simply can't avoid the fact that I don't know where this fraction is comming from and what is its impact on this calculation. What this one half means in the context of the sum of squared errors?
