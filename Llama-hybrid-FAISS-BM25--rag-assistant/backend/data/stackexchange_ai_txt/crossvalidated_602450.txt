[site]: crossvalidated
[post_id]: 602450
[parent_id]: 
[tags]: 
For human annotation projects, what are some commonly used metrics to assess grader reliability?

Lots of machine learning datasets are now created by having human raters annotate and provide labels to questions. Usually, a gold set is the most robust way of seeing if the raters are doing a good job. I am wondering what are some metrics commonly used for understanding how well a rater is doing?
