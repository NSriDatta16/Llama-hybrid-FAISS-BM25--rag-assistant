[site]: crossvalidated
[post_id]: 610643
[parent_id]: 610184
[tags]: 
Interesting question. From what I understand, you wish to combine the features you extract from the image, which have different shapes, 1042xN along with clinical information which is "constant" over the image, that includes, let us assume, some tabular data $X_i$ (patient $i$ 's age, marital status, for example). If I am correct and that is the case, then I my suggestion is actually to try to convert your signals from being 1042xN into a scalar and use it in regression at the second stage. In other words, my suggestion is to use a stacking ensemble. It also connected usεr11852 's suggestion but allows for more agile solutions. In general, this approach involves multiple steps: Convert your image from 1042xN to Cx1 , where C can be either an embedding dimension , or a score for the image (see elaboration on that bellow). Train a model using $[X_i,C_i]$ to predict $y_i$ , which, if I got it correctly, gets the value of 1 if patient $i$ has at least $K$ of their cells being malignant (frankly, this is a generic framework, you can define your $y_i$ as you go, but there are some caveats, see below). 1st stage At this stage, we look at the different tiles as a series of images . We aim to transform this series of images into one score in $[0,1]$ . You can choose between multiple approaches here. One, for example, could be, an average of the classifications of the different tiles. In your description, you said that this data is often used to build a per-tile score. So you can do the same, and calculate the aggregated score to be $g(T)=\dfrac{1}{n}\sum_{j=1}^N f(t)$ , where $f(t)$ is the per tile score given by model $f$ , and $j$ is the tile index running up to $N$ . Using the average is just intuitive but you can define any function you'd like, depending on your domain knowledge. Another approach to dealing with such data, is to treat the per-tile images as a time series. Under this approach you'd have to change the way you look at y. You can define it as the proportion of malignant cells per person, or a binary variable that gets the value of 1 if patient $i$ has more malignant cells than a certain threshold. This new $y_i$ is constant across tiles, and changes from only on the subject's level, in contrast with $y_{ij}$ which was the target for each tile. This is actually the way vision transformers analyze visual data today, and it has proven to be beneficial in terms of predictive capabilities. If you chose to use any of the above methods, now you have one number for each image, and this number reflects your belief regarding person i 's risk, that rises from their image. We can continue to the second stage. 2nd stage At this second stage, we are combining the diagnosis from the image, with the patient's metadata, just as in real life. How we would do that? Using your favorite model. A note about usεr11852 's suggestion If you want to increase the weight of the diagnosis of the image mechanically, instead of returning a scalar value as suggested, you can try to return a vector. This requires a bit different approach, because under this approach we won't use the predicted value of the sequence of the tiles, but rather their representation on some latent space instead. But since you want to change both of the axis (you don't want to add $1042$ scalars nor you don't want to add $N$ scalar instead, so the question that is rising is which dimension we want to reduce, and if both of them, then how?). Sorry, I don't have a quick answer for that. Caviates If you use stacking ensemble (that is the process that was described below, where you train a model on part of the data, and another model that one of its inputs is your first model's output), then you MUST NOT USE the same observations for that. That is, if you choose to train a tile-level classifier or a time-series model on those tiles, that will return a single score for an image, then you should exclude those patients for the second stage. Otherwise, the score's importance will be strongly upward biased, as the score is expected to be correlated with the dependent variable just from construction (it is because models ALWAYS have some degree of overfitting, but that is a discussion for another time). Good luck!
