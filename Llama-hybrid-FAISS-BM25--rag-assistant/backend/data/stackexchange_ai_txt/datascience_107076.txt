[site]: datascience
[post_id]: 107076
[parent_id]: 
[tags]: 
How can i get the vector of word using BERT?

I need to get word-vectors using BERT and got this function that i think it should be the one i need def get_bert_embed_matrix(sentences): device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") model_config = transformers.AutoConfig.from_pretrained('bert-base-uncased', output_hidden_states=True) model = transformers.AutoModel.from_pretrained('bert-base-uncased', config=model_config) tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased') for i in sentences: tokenized_text = tokenizer.tokenize(i) indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) tokens_tensor = torch.tensor([indexed_tokens]) model.eval() outputs = model(tokens_tensor) hidden_states = outputs[2] word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1) return word_embed_6 Does the method return vectors for sub-word or word ?
