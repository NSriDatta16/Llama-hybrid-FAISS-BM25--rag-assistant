[site]: crossvalidated
[post_id]: 379227
[parent_id]: 
[tags]: 
How to apply dropout in LSTMs?

Dropout in fully connected neural networks is simpl to visualize, by just 'dropping' connections between units with some probability set by hyperparamter p. However, how dropout works in recurrent neural networks (RNNs) with or without LSTM units, is not so clear to me. There are a lot of connection types that could be affected, but most importantly, there is no concept of redundancy in the connections as it is with fully forward networks. If I 'drop' a recursive connections in a RNN the whole network pathway will be broken. I assume dropout in RNNs must be applied to connections that have some concept of redundancy. Which connections exactly are affected by dropout in a RNN? I have an example of an implementation of a bidirectional RNN in tensorflow with dropout: def LSTM_NET(x, weights, biases): # Forward direction cell lstm_fw_cell = rnn.BasicLSTMCell(hidden_units, forget_bias=1.0) lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=(1-DropOut)) # Backward direction cell lstm_bw_cell = rnn.BasicLSTMCell(hidden_units, forget_bias=1.0) lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=(1-DropOut)) (output_fw,output_bw),state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32,sequence_length=length(x)) output = tf.concat([output_fw, output_bw], axis=2) # Linear activation, using rnn inner loop last output last = last_relevant(output,length(x)) return tf.add(tf.matmul(last, weights['LSTM_out']), biases['LSTM_out'])
