[site]: crossvalidated
[post_id]: 623538
[parent_id]: 
[tags]: 
Timeseries Anomaly Detection using Rolling Kurtosis?

I'm working on anomaly detection for multiple streaming time series datasets. Due to the vast number of datasets, I'm seeking a scalable, generalized method without resorting to adaptive thresholds tailored to each individual dataset. My current approach is as follows: For each dataset, I compute a rolling window of data points. Within each rolling window, I perform robust scaling using the median and MAD to make the data less sensitive to local outliers. I then compute the kurtosis of the robustly scaled data within the rolling window. A significant spike in kurtosis is considered indicative of an anomaly. One of the motivations behind this method is its potential scalability to multivariate datasets. For multivariate data, the kurtosis can be computed using $$ \text{kurtosis} = \frac{1}{m} \sum m_{ii}^2 $$ where $m_{ii}$ is the Mahalanobis norm of each data point with respect to the inerse of the covariance matrix of the rolling dataset. My questions are: Is using the kurtosis of robustly scaled data within a rolling window a valid and effective method for time series anomaly detection? How well does this method scale to multivariate datasets, especially given the proposed method for computing kurtosis? Are there potential pitfalls or limitations to this approach that I should be aware of? Are there alternative methods that offer similar scalability and generalization across multiple datasets without requiring dataset-specific thresholds?
