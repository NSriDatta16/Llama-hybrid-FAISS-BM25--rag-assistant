[site]: crossvalidated
[post_id]: 369226
[parent_id]: 
[tags]: 
How can we program Reinforcement learning without transition probability and rewards?

I would like to design the optimal task distribution system using Reinforcement learning. The best advantage of Reinforcement learning compared to traditional Dynamic programming is that it is not needed to know the information of dynamics such as transition probability. From what I studied for the last days, Reinforcement learning learned that information from experience. But what does it exactly mean learning from experience? For example, using what I want to design, We don't have exact data sets of task demand from service centers, but we know the distributions of task demand for them. In this case, every iteration, the next states, and the rewards can be random variables that we don't know exactly what could happen. But, because we know the distribution of task demand, we may be able to design following that distribution function in order to decide the next states and the rewards. Is it an available framework? Is it okay that the next states and the rewards (s',r) are different depend on iteration even though the same state and action (s,a)? In other words, one case can be (s1,a1) -> (s3,r1) but another case can be (s1,a1) -> (s5,r2).
