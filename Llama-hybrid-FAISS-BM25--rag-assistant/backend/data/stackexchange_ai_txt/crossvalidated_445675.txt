[site]: crossvalidated
[post_id]: 445675
[parent_id]: 
[tags]: 
Bishop: Understanding the prior and posterior for a curve fitting example (1.2)

In Bishop's Pattern Recognition and Machine Learning Book, he uses an example of fitting a polynomial to data collected from a sinusoidal curve with Gaussian noise. The goal is to find the most probable value for $\textbf{w}$ , the vector representing the coefficients of the polynomial, given $\textbf{x}$ and $\textbf{t}$ , the input values and corresponding target values of the data collected. $\beta$ represents the precision (inverse of the variance) of the values in $\textbf{t}$ . A prior distribution is given for the coefficients $\textbf{w}$ as $$p(\textbf{w}|\alpha) = \mathcal{N}(\textbf{w}|\textbf{0}, \alpha^{-1}\textbf{I}) = (\frac{\alpha}{2\pi})^{(M+1)/2}exp\{-\frac{\alpha}{2}\textbf{w}^T\textbf{w}\}$$ where $\alpha$ is the precision on the distribution for $\textbf{w}$ and M is order of the polynomial. He very soon says: $$p(\textbf{w}|\textbf{x},\textbf{t}, \alpha, \beta) \propto p(\textbf{t}|\textbf{x}, \textbf{w}, \beta)p(\textbf{w}|\alpha)$$ I'm confused as to how the terms on the right are gotten, using Bayes Theorem. What I know of Bayes theorem is $$p(\textbf{w}|\textbf{x},\textbf{t}, \alpha, \beta) = \frac{p(\textbf{t}, \textbf{x}, \textbf{w}, \beta, \alpha)}{p(\textbf{x},\textbf{t}, \alpha, \beta)}.$$ By the product rule, this is equal to $$\frac{p(\textbf{t} | \textbf{x}, \textbf{w}, \beta, \alpha)p(\textbf{w}, \textbf{x},\beta,\alpha)}{p(\textbf{x},\textbf{t},\alpha,\beta)} = \frac{p(\textbf{t} | \textbf{x}, \textbf{w}, \beta, \alpha)p(\textbf{w}|\textbf{x},\beta,\alpha)p(\textbf{x},\beta,\alpha)}{p(\textbf{x},\textbf{t},\alpha,\beta)}$$ I'm not sure how he gets those terms on the right as proportional. It could be that some of the variables are independent of other variables. I'm not sure. If this is the case, could someone explain? If it is for a different reason, please explain. Thank you.
