[site]: datascience
[post_id]: 78113
[parent_id]: 78109
[tags]: 
Setting a seed or fixing a random state controls randomness. When you want to do "controlled experiments", you need to control randomness to some extent to achieve reproduceable (and by that also comparable) results. You should have a good idea where it is necessary to control randomness: E.g. when you use linear regression or logistic regression, the results will always be the same (provided you use the same data and model specification). However, when you randomly split a data set for test and training, randomness will affect your test/train split. Now say you want to compare different model specifications of a linear regression to see what the best model is and you use a test/train set. In order to compare different linear model specifications, you should use the same data for training/testing. So in this case, you would need to set a seed in the test/train split. Otherwise - if you don't set a seed - changes in the model can originate from two sources. A) the changed model specification and B) the changed test/train split. There are also a number of models which are affected by randomness in the process of learning. Neural nets or boosted model - for instance - will produce somewhat different results after each model run if you don't set a seed. Also in this case, e.g. when you do hyperparameter tuning to find best models, controlling randomness is useful in order to ensure that results from different model runs are comparable.
