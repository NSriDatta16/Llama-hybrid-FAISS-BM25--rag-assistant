[site]: crossvalidated
[post_id]: 379135
[parent_id]: 379119
[tags]: 
There are a number of model-agnostic ways of interpreting complex machine learning models. Model-agnostic means that it can be applied to all sorts of models, including the random forest. Christoph Molnar put his entire book Interpretable Machine Learning: A Guide for Making Black Box Models Explainable on the internet, and he devotes an entire chapter to model-agnostic methods . Two popular methods are LIME (which stands for local interpretable model-agnostic explanations) and Shapley values. Both of these are described in the Molnar book listed above. Crudely put, LIME looks at a specific prediction, makes slight variations on your data to see predictions around that space and to learn about how the model makes predictions in that local space, and then trains an interpretable model (such as linear regression) based on this information. I'm not recalling precisely how Shapley values workâ€”other than remembering it is a clever extrapolation of a metric from game theory: Features are treated as cooperating in making predictions, and features receive a higher Shapley value for contributing to the prediction more. I would suggest reading the chapters in Molnar's book above. I would also suggest, for a more conceptual overview: A talk of Molnar's on these methods, found at YouTube . The Data Skeptic podcast episode on LIME . Two episodes of the Linear Digressions podcast on Shapley values. The two webpages ( here and here ) have lots of other links to the original papers, use-case write-ups, and other episodes of theirs on LIME and model interpretation. Apologies for not going too in depth on these methods, but these resources should point you in the right direction.
