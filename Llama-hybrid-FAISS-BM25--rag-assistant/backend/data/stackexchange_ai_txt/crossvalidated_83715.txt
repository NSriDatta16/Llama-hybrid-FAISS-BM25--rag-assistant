[site]: crossvalidated
[post_id]: 83715
[parent_id]: 83119
[tags]: 
Let me change the notation a bit and write the hat matrix as $$H = V^{\frac{1}{2}}X(X'VX)^{-1}X'V^{\frac{1}{2}}$$ where $V$ is a diagonal symmetric matrix with general elements $v_j = m_j \pi (x_j) \left[1 - \pi (x_j) \right]$. Denote $m_j$ as the groups of individuals with the same covariate value $x = x_j$. You can obtain the $j^{th}$ diagonal element ($h_j$) of the hat matrix as $$h_j = m_j \pi (x_j) \left[1 - \pi (x_j) \right] x'_j (X'VX)^{-1}x'_j$$ Then the sum of $h_j$ gives the number of parameters as in linear regression. Now to your question: The interpretation of the leverage values in the hat matrix depends on the estimated probability $\pi$. If $0.1 In this case the most extreme values in the covariate space can give you the smallest leverage, which is contrary to the linear regression case. The reason is that leverage in linear regression is a monotonic function, which is not true for the non-linear logistic regression. There is a monotonically increasing part in the above formulation of the diagonal elements of the hat matrix which represents distance from the mean. That is the $x'_j (X'VX)^{-1}x'_j$ part, which you might look at if you are only interested in distance per se. The majority of diagnostic statistics for logistic regressions utilize the full leverage $h_j$, so this separate monotonic part is rarely considered alone. If you want to read deeper into this topic, have a look at the paper by Pregibon (1981), who derived the logistic hat matrix, and the book by Hosmer and Lemeshow (2000). Pregibon, D. (1981) "Logistic regression diagnostics", Annals of Statistics, Vol. 9(4), pp. 705-724 Hosmer, D.W. and Lemeshow, S. (2000) "Applied Logistic Regression", 2nd Edition, John Wiley and Sons, Inc.
