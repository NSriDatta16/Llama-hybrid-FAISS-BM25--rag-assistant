[site]: crossvalidated
[post_id]: 588935
[parent_id]: 588928
[tags]: 
The Kolmogorov-Smirnoff statistic has some problems as a machine learning performance metric that are worth knowing. First, KS only assesses the maximum vertical distance between CDFs. Consequently, it will not be sensitive to differences elsewhere that a metric like Brier score or log loss will detect. Whenever either of those metrics encounter a prediction that differs from the observed value, there is a penalty. Second, as you have noted, the KS test misses reverse calibration. If my predictions are consistently the opposite of what they should be, there are ways to handle that, but I need to know, not be deceived into thinking my predictions are good. A combination of a poor score on a metric that catches reverse calibration and a high score on KS (so a low p-value) could be a signal that there is reverse calibration. I suspect, however, there to be considerably better alternatives.
