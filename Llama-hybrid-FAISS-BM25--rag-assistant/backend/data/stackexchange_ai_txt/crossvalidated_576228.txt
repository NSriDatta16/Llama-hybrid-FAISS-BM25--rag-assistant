[site]: crossvalidated
[post_id]: 576228
[parent_id]: 61783
[tags]: 
I see in most machine learning courses, that a model is validated on smaller training sets and prediction scores are evaluated to give a measure of prediction power, these models are then discarded and the model fit to the whole dataset for the final model. I have a couple of bug bears with terminology often used when talking about validation. For a start, each model fit on each fold of data (using k folds) or each loocv is a different model. No two will be the same (different coefficient values), therefore I think the term 'model validation ' is misleading, I prefer to say method validation in this case. Secondly, and more directly related to this topic, if the end product is to fit to the whole dataset for the final model, the loocv will always give a more better idea of how the final model will predict, as each will likely be very similar to the final model. Personally, I see little point in validating a model using models that are quite different from the model being validated. The more you stray from the model being validated, the less valid and relevant the validation actually is. Unless I am missing something, which I may well be, computational expense should be the only thing that would persuade one to move toward lower k folds
