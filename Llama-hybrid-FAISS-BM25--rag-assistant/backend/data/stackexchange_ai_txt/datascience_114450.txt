[site]: datascience
[post_id]: 114450
[parent_id]: 107212
[tags]: 
Usually, padding is excluded from mean pooling. One approach to derive sentence embeddings by mean pooling excluding padding tokens can be taken from Sentence Transformers . In their pooling source code you can see that they use the attention mask to exclude padding tokens. Here is a simplified implementation of what they do using Huggingface transformers (taken from here ): from transformers import AutoTokenizer, AutoModelForMaskedLM def mean_pooling(model_output, attention_mask): token_embeddings = model_output[0] #First element of model_output contains all token embeddings input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) return sum_embeddings / sum_mask tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base") model = AutoModelForMaskedLM.from_pretrained("xlm-roberta-base") encoded_input = tokenizer("hello", return_tensors='pt') model_output = model(**encoded_input) mean_pooling(model_output, encoded_input['attention_mask']) Since the input_mask_expanded is 0 for PAD tokens the resulting mean excludes these tokens when calculating sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) . A concrete example is discussed here on the SBERT Github repository.
