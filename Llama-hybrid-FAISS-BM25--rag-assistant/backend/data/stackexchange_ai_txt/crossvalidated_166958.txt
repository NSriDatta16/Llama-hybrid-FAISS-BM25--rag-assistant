[site]: crossvalidated
[post_id]: 166958
[parent_id]: 
[tags]: 
Multinomial Logistic Loss vs (Cross Entropy vs Square Error)

I observed that Caffe (a deep learning framework) used the Softmax Loss Layer SoftmaxWithLoss as output layer for most of the model samples . As far as I know, Softmax Loss layer is the combination of Multinomial Logistic Loss Layer and Softmax Layer . From Caffe, they said that Softmax Loss Layer gradient computation is more numerically stable However, this explanation is not the answer that I want, the explanation is just compare the combination of Multinomial Logistic Loss Layer and Softmax Loss layer instead of layer by layer. But not compare with other type of loss function. However, I would like to know more what is the differences/advantages/disadvantages of these 3 error function which is Multinomial Logistic Loss , Cross Entropy (CE) and Square Error (SE) in supervised learning perspective? Any supportive articles?
