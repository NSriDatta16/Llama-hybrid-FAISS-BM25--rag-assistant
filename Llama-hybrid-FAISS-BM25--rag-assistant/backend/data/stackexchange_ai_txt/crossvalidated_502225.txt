[site]: crossvalidated
[post_id]: 502225
[parent_id]: 502205
[tags]: 
Default logistic regression minimizes below function: $L(f(X, \beta), Y) = \frac{1}{N} \Sigma_i^N \left[-y_i log(f(x_i, \beta)) + (1-y_i)log(1-f(x_i, \beta))\right]$ Logistic regression with L1 penalty minimizes below function: $L(f(X, \beta), Y) = \frac{1}{N} \Sigma_i^N \left[-y_i log(f(x_i, \beta)) + (1-y_i)log(1-f(x_i, \beta))\right] + \lambda \Sigma_i^K |\beta_i|$ Where: $f(x_i, \beta) = \frac{1}{1+e^{-\beta^T x_i}}$ and $K$ refers to the dimension of $x_i$ Difference is second bit, that is: $\lambda \Sigma_i^K |\beta_i|$ If you are using Python, it is already implemented in sklearn. For any other language which L1 Logistic regression is not implemented, you can code up the above function and use any iterative optimization algo to obtain $\beta$ , such as gradient descent.
