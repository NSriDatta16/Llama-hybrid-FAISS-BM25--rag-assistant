[site]: crossvalidated
[post_id]: 97077
[parent_id]: 
[tags]: 
LIBSVM overfitting

I trained two svms (LIBSVM) with 15451 samples after I did a 10-fold cross-validation and found the best parameter values for gamma and C (RBF kernel). In one svm I used just 1 feature and in the second an additional one (to see whether this additional is improving prediction). After CV I have am accuracy of 75 % (SVM with one feature) and 77 % (SVM with that additional one). After testing on another 15451 instances I have an accuracy of 70 % and 72 % respectively. I know that this is called overfitting but is it significant here, since it is only a difference of 5 %. What could I do to avoid overfitting? Is it even good to use just one or two features and a relatively big training set?
