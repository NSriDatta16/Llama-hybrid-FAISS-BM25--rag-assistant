[site]: crossvalidated
[post_id]: 539583
[parent_id]: 539548
[tags]: 
A prior is useful for incorporating historical information, analogous to a fixed effect meta-analysis. Because Bayesians define parameters as random variables and probability as the belief of the experimenter they may feel much freer to incorporate information from other studies or their personal beliefs. In contrast, because frequentists are concerned with the performance of their testing procedure they may scrutinize the compatibility of studies. The frequentist needs to feel comfortable assuming the unknown fixed true quantity being investigated in each study is identical (even if the subject-level observations are not necessarily exchangeable). This may be a big assumption to make, analgous to assuming an informative Bayesian prior. Using a purely subjective Bayesian prior is analogous to performing a meta-analysis with hypothetical experimental data. I will argue against using a Bayesian prior and in favor of a frequentist meta-analysis, beginning with an example taken from Decision Making in Drug Development via Confidence Distributions (Johnson 2021) . The primary reason not to use a Bayesian prior is that a subjective interpretation of probability as a measure of belief is unfalsifiable (1) (2) . A confidence distribution $H(\theta,\boldsymbol{x})$ as a function of the hypothesis and observed data has the appearance of a cdf on the parameter space and depicts p-values and confidence intervals of all levels based on a particular testing procedure. This same information can be displayed as a confidence density and a confidence curve. The figure below depicts a meta-analysis using confidence distributions for a binomial proportion $\theta$ . Density (a) represents an informative prior distribution based on historical data and a vague conjugate prior with an estimate of 0.90 and a sample size of n = 50. This same information is depicted in (b) as a confidence density resulting from a likelihood ratio test. A similar confidence density can be produced by inverting a Wald test with a logit link. The posterior based on the current data binomial likelihood and a vague conjugate prior is shown in (c) with an estimate of 0.87 resulting from n = 30. This same information can be represented as a likelihood ratio confidence density, (d). Using Bayes theorem, (a) and (c) combine to form (e). Multipliying the historical and current likelihoods and inverting a likelihood ratio test forms (f). This multiplication of independent likelihoods is precisely what Bayes theorem accomplishes (plus normalization), without the inversion of a hypothesis test. The confidence densities above can be interpreted under a Neyman-Pearson framework using a pre-specified null hypothesis and type I error rate. they can also be interpreted under a Fisherian framework of evidential p-values to compare the plausibility of multiple hypotheses, not necessarily pre-specified. Many Bayesians will use a Dutch book argument in support of posterior probability. Here is an example where the long-run characteristics of a likelihood ratio test are used to form a Dutch book against two different Bayesian posteriors. (3)
