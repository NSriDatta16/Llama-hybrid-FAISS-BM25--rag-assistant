[site]: crossvalidated
[post_id]: 230025
[parent_id]: 
[tags]: 
How to create best board evaluation function from training positions?

I'm brand new to machine learning - let me know if I should correct terminology or extract anything out into separate questions. I'm attempting to create a board evaluation function for Othello. Each input position consists of 64 squares which can be one of {black, white, empty}. The output is the predicted score of the game which ranges from [-64, 64]. 3^64 is too many positions to calculate, so I first need to select the best features. My understanding is that other top Othello engines use linear regression with ~25 different board feature patterns as an input. The most important 8-square feature patterns are the 4 board edges which each have 3^8=6561 possible configurations. 10-square feature patterns have 3^10=59049 positions and 12-square feature patterns have 531441 positions. I'd expect the larger feature patterns to be more accurate, but also be more expensive to store in memory and train. I can store an Othello position in 16 bytes + 1 byte for the calculated score. So the scale I'm looking at is roughly ~100 million training positions at a time and I'd like to perform the analysis locally on a single machine. When a board is mostly full, I can easily generate millions of solved training positions, but it gets exponentially more expensive when there are more turns remaining in the game. Note that an Othello position has 8 symmetries (4 rotations + 4 mirrored rotations). Also many feature-patterns will be correlated if they share common squares (e.g. each corner square is in two different edge patterns and a diagonal pattern). Should I try to exploit this or exercise any particular cautions? After starting with a model based on the edge feature patterns, how should I try incorporating the next most important feature patterns? Most other programs I've looked at seem to only use a linear model for combining feature pattern terms. However, it seems like it'd be much more accurate to go one step further and combine related feature patterns. For example, two strong edge configurations that share a common corner should be worth more than simply adding their individual contributions. My gut tells me that the final evaluation function should: Calculate the output value for each of the input feature patterns Combine related feature pattern outputs to predict a final score for the entire board What methods should I look at for combining feature pattern terms? How should I initialize each of the feature pattern configurations from my initial set of training positions. Just average the scores? Is there a general rule-of-thumb for how many training position samples I should have for each possible feature pattern? See Too many models to choose from for a related unanswered question.
