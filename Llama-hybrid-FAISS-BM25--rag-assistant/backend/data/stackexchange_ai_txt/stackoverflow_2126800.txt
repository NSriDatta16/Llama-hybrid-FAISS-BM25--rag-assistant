[site]: stackoverflow
[post_id]: 2126800
[parent_id]: 2052853
[tags]: 
+1 for the SQL/Db solutions, keeps things simple --will allow you to focus on the real task at hand. But just for academic purposes, I will like to add my 2 cents. -1 for hashtables. (I cannot vote down yet). Because they are implemented using buckets, the storage cost can be huge in many practical implementation. Plus I agree with Eric J, the chances of collisions will undermine the time efficiency advantages. Lee, the construction of a trie or DAWG will take up space as well as some extra time (initialization latency). If that is not an issue (that will be the case when you may need to perform search like operations on the set of strings in the future as well and you have ample memory available), tries can be a good choice. Space will be the problem with Radix sort or similar implementations (as mentioned by KirarinSnow) because the dataset is huge. The below is my solution for a one time duplicate counting with limits on how much space can be used. If we have the storage available for holding 1 billion elements in my memory, we can go for sorting them in place by heap-sort in Î˜(n log n) time and then by simply traversing the collection once in O(n) time and doing this: if (a[i] == a[i+1]) dupCount++; If we do not have that much memory available, we can divide the input file on disk into smaller files (till the size becomes small enough to hold the collection in memory); then sort each such small file by using the above technique; then merge them together. This requires many passes on the main input file. I will like to keep away from quick-sort because the dataset is huge. If I could squeeze in some memory for the second case, I would better use it to reduce the number of passes rather than waste it in merge-sort/quick-sort (actually, it depends heavily on the type of input we have at hand). Edit: SQl/DB solutions are good only when you need to store this data for a long duration.
