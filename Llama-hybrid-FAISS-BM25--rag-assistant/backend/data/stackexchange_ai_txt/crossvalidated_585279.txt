[site]: crossvalidated
[post_id]: 585279
[parent_id]: 
[tags]: 
Not improvement in representation of encoder in AE

I am trying to train an autoencoder on tabular data containing categorical data. After training AE, I use the encoder for classification. I normalize numerical data and use one-hot encoding for categorical data. There are some works that improve the representation in encoder using some kind of augmentations like adding gaussian noise to values, swap noise (swapping the values between samples in the same column), and removing some values from input. THe input is augmented, and the AE reconstructs the original input without any change (denoising autoencoder, mask autoencoder is two well-known methods). The total input reconstruction by the decoder improves the representation. This is what is done in the previous the papers or projects. I compare two methods: (a) training classifier on the original data (b) training AE by augmentation and without augmentation; then use the encoder to train the classifier (output of the encoder is the input of the classifier). But when I implement these methods (a and b) by augmentation, I just see a drop or no change in the accuracy of the classifier. It shows that the encoder does not learn a better representation. I am not sure why it is wrong in my implementation. Does hyper-parameter tuning is very important to see the effect of augmentations? I hope you can help me with this problem to figure out the issue.
