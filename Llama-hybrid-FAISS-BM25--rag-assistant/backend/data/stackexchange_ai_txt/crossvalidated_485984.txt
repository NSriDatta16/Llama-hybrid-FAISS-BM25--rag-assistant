[site]: crossvalidated
[post_id]: 485984
[parent_id]: 485964
[tags]: 
Actually this is not really a big difference between Statistics and Machine Learning. Machine Learning theory is concerned with how well predictions work outside the training sample in terms of the loss function as well. I think this is usually referred to as generalization risk or generalization error there, see for example Bousquet & Elisseef: Stability and Generalization . Obviously if you only have the training sample, you can only evaluate the loss function on the training data. But many methods are based on some kind of training loss minimization, which implies that the training error (because it is optimized on the training data) will not generalize well and loss on new observations can be expected to be higher. This depends on the specific method and situation, but considering at least theoretically (or on separate test data) applying the loss function to new to be predicted data is a key tool for investigating this, and both Statistics and Machine Learning are concerned with this. (And you can sometimes choose methods or parameters based on expected generalization loss rather than plain training loss, at least where theory exists.)
