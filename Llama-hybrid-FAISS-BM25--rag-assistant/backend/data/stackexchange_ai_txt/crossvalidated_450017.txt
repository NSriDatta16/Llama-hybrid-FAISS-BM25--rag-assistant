[site]: crossvalidated
[post_id]: 450017
[parent_id]: 285909
[tags]: 
Statsstudent makes a great point when they say "I would use a logistic regression". The result of a chi-square test tells you that a difference exists, but you have no way of determining which the difference is without resorting to post hoc tests which could possibly deflate power. However, with a logistic regression, the comparison becomes richer. Here is the analysis in R. library(tidyverse) ad = tribble( ~'Ad', ~'Impressions', ~'Clicks', '1', 4269, 26, '2', 3155, 12, '3', 2510, 9 ) model = glm(cbind(Clicks, Impressions-Clicks) ~ Ad, data = ad, family = binomial()) 100*predict(model, type='response') The benefit of using a logistic regression is that we not only do we get conversion probabilities (0.61%, 0.38%, 0.36% for the ads respectively), we also get uncertainty in that estimate via confidence intervals. For this experiment, here are 95% confidence intervals for the conversion p low high 1 0.61 0.41 0.90 2 0.38 0.21 0.68 3 0.36 0.18 0.70 So, the first ad's results are consistent with conversions as low as .41% and as high as .9%. There seems to be some considerable overlap between the confidence intervals. Let's look at our model coefficients to determine if ads 2 or 3 yielded a statistically significant difference as compared to ad 1. term p.value 1 (Intercept) 6.68e-148 2 Ad2 1.76e- 1 3 Ad3 1.70e- 1 Ad 1 is the reference ad, so the tests for this model compare ad 2 vs ad 1 and ad 3 vs ad 1. In both cases, we fail to reject the null (p>0.05) and so we can't conclude that ad 2 or 3 results in smaller conversion than ad 1. In this particular experiment, ad 1 yielded superior conversion, but in a different sample that might not be the case. I'm being a bit fast and loose with my methods here, but this is the gist. A little more effort yields a richer set of inferences.
