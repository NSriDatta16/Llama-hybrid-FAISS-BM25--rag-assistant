[site]: datascience
[post_id]: 37906
[parent_id]: 
[tags]: 
Data augmentation / feature extraction on pre-trained convnets

I'm reading 'Deep Learning with Python' by Fran√ßois Chollet, which is an excellent book. He talks about using pre-trained convnets (in his example, VGG16) and then running smaller datasets to tweak them (eg the dogs vs cats kaggle dataset). My understanding is that there are two options: 1) Lop off the dense layers from (eg) VGG16. Run the small dataset (just cats and dogs) through the remaining convolutional layers. Take the output and then train a second, dense, convnet, as a separate, second step, on that. 2) Lop off the dense layers from VGG16. Replace them with new, randomly initalised, dense layers that take the input from the original convnet. Freeze the pre-existing convolutional part of the network, and then train the whole convnet with the smaller dataset. According to the book, approach (2) allows you to use data augmentation but approach (1) doesn't. I don't understand why that is the case, or indeed why the two approaches are different, since both are in effect taking the output from fixed, convolutional layers and then training a smaller, dense network on it. What have I misunderstood?
