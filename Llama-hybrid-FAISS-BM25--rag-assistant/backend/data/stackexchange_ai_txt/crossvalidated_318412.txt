[site]: crossvalidated
[post_id]: 318412
[parent_id]: 204589
[tags]: 
The following tactics may be useful: Different Algorithms: Decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. You can try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest. Generating Synthetic Samples: A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class. You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved. You can try the SMOTE (Synthetic Minority Over-sampling Technique) algorithm to increase the amount of the samples in the minority class with an oversampling method. Different Perspective: Actually, there are fields of study dedicated to imbalanced datasets. For example, you might like to consider are anomaly detection and change detection instead of classification. Performance Metrics: Accuracy is not the true metric to use when working with an imbalanced dataset. Following performance measures that can give more insight into the accuracy of the model than traditional metrics: confusion matrix , precision , recall , f-score , Kappa (or Cohen's Kappa ), and ROC curves . You can find more at 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset
