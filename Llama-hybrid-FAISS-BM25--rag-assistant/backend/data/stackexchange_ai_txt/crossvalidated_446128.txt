[site]: crossvalidated
[post_id]: 446128
[parent_id]: 446102
[tags]: 
I understand that this output tells me that A and D are important variables because they have high MeanDecreaseAccuracy values. However, D is the inverse of A (they are perfectly correlated) so why does D have a higher MeanDecreaseAccuracy value? In general, it is not a good idea to have two perfectly correlated variables in your model. In Random Forest this simply means that you might be including the same information multiple times - in this particular case it does not cause any problem because the variable you include is the only one that brings some information, but it might be detrimental to the algorithm performance in many other cases. More importantly, having correlated variables tends to mess up your Improtances. Why does D have higher Mean Decrease Accuracy if they are the same variable in the eyes of the model? I'd say chance. Mean Decrease Accuracy is computed making the difference of the Out Of Bag score of your model and the Out Of Bag score of the predictions of your model with a given variable permutated. Now, RF has two sources of randomness: the bootstrap, and the undersampling of the features (in your case with 4 variables and default mtry , each split is made with 2 variables), so the results could differ based on which data is bootstrapped and which features are selected in the trees. This might have particular effect if you don't have too many trees. When increasing the number of trees, the importance of the two features should converge. A similar reasoning applies to the second question. In particular I am not sure if the variable importances are computed averaging over multiple permutations, which would be the best way to go. Indeed, since your model has 71% OOB accuracy, any accuracy drop over 21% is rather meaningless, as it means that your model goes below 50% accuracy which is random guessing. At this link you can find some useful information on RF importances and some code for some proper Permutation Importances. how should I approach the selection of my variables? In this particular case you have very few variables, and it also looks like two of them are not meaningful. This means that all information is coming from just 1 variable, that you duplicated, so there is very little feature selection to be done. In general, try to avoid highly rank correlated variables (try Spearman or Kendall correlations). Unimportant variables usually do not affect performance too much, unless there is too many of them. As a general practice though, remove useless ones from your models.
