[site]: crossvalidated
[post_id]: 575432
[parent_id]: 
[tags]: 
Why variable representation plays a role in prediction?

I am working on binary classification using a random forest, where the data have 977 records and 6 columns. The class ratio is 77:23. I have two derived input variables. One variable is called no_of_non_responses , and the other is called as percentage_of_non_responses . Both variables are computed from the same original columns ( no_of_times_called and no_of_times_responded ). For example, if I call a customer 10 times and he responds only 4 times, we can know that his value for no_of_non_responses is 6 and percentage_of_non_responses is 60. But in my ML model, I see that only no_of_non_responses seems to improve predictive power by 2 points (78 to 80), whereas percentage_of_non_responses doesn't do that. But usually, percentages give a better idea of the variable (in this case non response %) when compared to count. Can I know why it is that, despite representing the same info in two different forms, one improves the predictive power whereas the other doesn't?
