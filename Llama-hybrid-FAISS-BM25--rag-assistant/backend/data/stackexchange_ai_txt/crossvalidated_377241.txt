[site]: crossvalidated
[post_id]: 377241
[parent_id]: 377236
[tags]: 
Suppose you have split your data into train-validation-test sets. You do not usually split it into train-test unless your models have no hyperparameters. Validation set is always used to tune hyperparameters of your models. Test set is used to assess the final performance of your model, and compare different classes of models (e.g. random forest vs neural network vs svm) by their performance. Cross-validation is tightly connected to the validation set and hyperparameter selection. In general, cross-validation splits your large mass of (train + validation) into training and validation sets repeatedly. This should provide you with an out-of-sample performance approximation, and based on it you choose your hyperparameters (model). However, you can treat your model class (random forest, svm, neural network) as hyperparameter. In this way, you can choose your final model class using cross-validation. Test set is used purely for reporting (how well your chosen model performs). According to some authors, you cannot use test set for ANY model selection, even if it is model class selection (svm vs random forest vs etc.). However, I would not compare broad model classes using cross-validation. It is best to decide which model to use based on your test set. By using your test set purely for reporting as statisticians suggest, you are losing a lot of data. However, using it to choose a model class as the last step does not influence the out-of-sample overfitting much, and this is what all scientific papers do when they claim that their state-of-the-art method outperformed some other method on some large dataset. As for #1 ) You are making a sympathetic error by confusing cross-validation division procedure and train-validation-test sets. Train-validation-test is a data set division, while cross-validation is a specific way HOW you can divide your mass of data into training and validation sets (while test set has nothing to do with cross-validation, and allocated separately). As for #2 ) Yes, you can do Lasso and Gradient Boosted Regression Tree comparison using validation set (and cross-validation split method), but it would be better to compare them on the test set, while cross-validation (validation set) is used to find hyperparameters of your GRT and Lasso regression separately.
