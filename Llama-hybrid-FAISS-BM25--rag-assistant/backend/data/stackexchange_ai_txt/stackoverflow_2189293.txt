[site]: stackoverflow
[post_id]: 2189293
[parent_id]: 2160863
[tags]: 
If I understand, the first step is to calculate a histogram: [attr, value] => frequency where frequency is the number of times that value ocurred in the attr column. The next step is to take the histogram table and the original data, for each line calculate the AVF, and sort them. I'd do it in two passes: one map-reduce pass to calculate the histogram, a second m-r pass to find the AVF using the histogram. I'd also use a single constant hash guilt-free, as getting the histogram values and the cell values to the same locality will be a messy beast. (For example, have map1 emit [attr val id] with [attr val] as key; and have reduce1 accumulate all records for each key, count them, and emit [id attr val count] . The second pass uses id as key to reassemble and then average each row). To calculate a histogram, it helps to think of the middle step as 'group' rather than 'sort'. Here's how: since the reduce input is sorted by key, have it accumulate all records for the given key, and as soon as it sees a different key, emit the count. Wukong, the ruby equivalent of dumbo, has an Accumulator , and I assume dumbo does too. (See below for working code). This leaves you with attr1 val1a frequency attr1 val1b frequency attr2 val2a frequency ... attrN attrNz frequency For the next pass, I'd load that data into a hash table -- a simple Hash ( dictionary ) if it fits in memory, a fast key-value store if not -- and calculate each record's AVF just as you had it. Here is working ruby code to calculate the avf; see http://github.com/mrflip/wukong/blob/master/examples/stats/avg_value_frequency.rb First Pass module AverageValueFrequency # Names for each column's attribute, in order ATTR_NAMES = %w[length width height] class HistogramMapper Second pass module AverageValueFrequency class AvfRecordMapper
