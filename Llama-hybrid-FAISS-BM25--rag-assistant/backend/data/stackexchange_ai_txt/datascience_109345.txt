[site]: datascience
[post_id]: 109345
[parent_id]: 109344
[tags]: 
There is a limiting factor here, which is the positional embeddings. In BERT, positional embeddings are trainable (not sinusoidal) and support a maximum of 512 positions. To exceed such a sequence length, you would need to extend the positional embedding table and have the extra entries be trained during the fine-tuning. This, however, would probably lead to performance degradation. So, technically possible but probably not Ok. One option would be to keep only the first (or the last) 512 tokens of the sequences as input to BERT and see if the resulting performance is fine for your purposes. As an alternative, you may use pre-trained long-context transformers like the LongFormer .
