[site]: crossvalidated
[post_id]: 620625
[parent_id]: 620547
[tags]: 
1 and 2 look good! For 3, you should choose the model based off of validation set results. Since youâ€™ve done multiple folds, you can take the hyper parameters that returned the highest average validation accuracy as best. The test set should be a complete hold out as an estimate to see how it will do when deployed. Another common practice is once you have run cross-validation, you would retrain the neural network (with the best hyperparameters) on the training+validation data, since the more data fed into these models, the better. For 4, You would do the same process as above, and only use the test set as a gauge, rather than using it to choose a better model.
