[site]: crossvalidated
[post_id]: 472945
[parent_id]: 472921
[tags]: 
You have many options and it depends on many things (dataset size, compute you have access to, etc.). First, you can choose an architecture that takes as inputs a sequence of words and that outputs a representation of a sequence. For example, a bidirectional-LSTM followed by max-pooling would do, or a CNN followed by max-pooling. (Not both sequentially as you wrote, though. I think you're a bit confused by what the CNN would output.) I would try with a pre-trained model in order to leverage huge corpora without supervision and without having to train too much myself. For example, BERT uses a special [CLS] token that is supposed to be a global sentence/document vector (trained to predict the next sentence), but there are other, more modern variants. Then, you can encode both the document and the comment separately, concatenate the representation and train a classifier (logistic regression or MLP) on top. Another option is to concatenate the two documents and use the [CLS] vector (or the equivalent in whatever Transformer-based model you use) to do your binary classification. In both case, an orthogonal choice is whether to fine-tune your Transformer/CNN/BiLSTM model or not.
