[site]: crossvalidated
[post_id]: 583656
[parent_id]: 
[tags]: 
Calculating "accuracy", "recall" etc. without classification

I have a set of models, that I'm comparing to each other with respect to prediction of a binary event. I'm using a few proper scores (Brier, log), but I also need accuracy, recall, sensitivity etc., since the people I work with are more likely to understand them. However, I'd like to do this without applying an arbitrary classification threshold, so I'm currently I'm calculating the following, with forecast probability $f_k$ and event indicator $e_k$ for observation $k$ : True positives $TP = \sum_k f_k[e_k = 1]$ ; True negatives $TN = \sum_k (1-f_k)[e_k = 0]$ ; False positives $FP = \sum_k f_k[e_k = 0]$ ; False negatives $FN = \sum_k (1-f_k)[e_k = 1]$ . This gives me an "average confusion" matrix, with real "counts" rather than integer ones, which I use to calculate the metrics as usual. Are there any issues / biases I'm introducing by doing this, in addition to the ones inherent to using improper scoring rules?
