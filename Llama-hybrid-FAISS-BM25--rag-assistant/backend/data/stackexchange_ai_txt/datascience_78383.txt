[site]: datascience
[post_id]: 78383
[parent_id]: 
[tags]: 
Which other algorithms fit residuals like XGBoost?

XGBoost and standard gradient boosting train learners to fit the residuals rather than the observations themselves. I understand that this aspect of the algorithm matches the boosting mechanism which allows it to iteratively fit errors made by previous learners. Which other algorithms or also train single or multiple learners to fit residuals? Does this method only make sense for learners built in a sequence? Or also for any ensemble methods? Is there a deep significance to fitting residuals or is this a mathematical convenience that allows for iterative improvement from learners built in succession?
