[site]: crossvalidated
[post_id]: 333176
[parent_id]: 332179
[tags]: 
For anyone stumbling on this post also looking for an answer, this twitter thread has added a lot of very useful insight. Namely: beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework discusses my exact question with a few experiments. Interestingly, it seems their $\beta_{norm}$ (which is similar to my normalised KLD weight) is also centred around 0.1, with higher values giving more structured latent space at the cost of poorer reconstruction, and lower values giving better reconstruction with less structured latent space (though their focus is specifically on learning disentangled representations). and related reading (where similar issues are discussed) Semi-Supervised Learning with Deep Generative Models https://github.com/dpkingma/nips14-ssl InfoVAE: Information Maximizing Variational Autoencoders Density estimation using Real NVP Neural Discrete Representation Learning
