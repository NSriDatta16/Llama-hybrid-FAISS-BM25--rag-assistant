[site]: crossvalidated
[post_id]: 505144
[parent_id]: 
[tags]: 
What assumptions about probability do different models make?

If we throw a dice n times and get an empirical, discrete probability distribution, one could say that it approximates the real underlying distribution. When we build a machine learning model like neural networks or logistic regression, libraries often offer predicted probabilities. Empirically, we can evaluate them with, e.g., RMSE or other metrics compared to known cases. However, what are the model assumptions regarding probability? For example, has the logistic function any specific property that makes it more suitable to predict probabilities than another function? Have neural networks any specific link to probability? To sum up: In which case can I use predicted probabilities from ML models for probability-related computations (e.g. expected value)?
