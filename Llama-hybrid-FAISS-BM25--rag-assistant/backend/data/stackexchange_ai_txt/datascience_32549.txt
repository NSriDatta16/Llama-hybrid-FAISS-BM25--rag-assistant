[site]: datascience
[post_id]: 32549
[parent_id]: 32544
[tags]: 
There are two basic approaches to summarization: abstractive and extractive . Extractive summarization is simpler and consists of selecting informative pieces/sentences from text. It is commonly performed using algorithms like TextRank (a variation of PageRank). An example implementation can be found in gensim . There are other approaches, for example check out sumy library. Abstractive summarization on the other hand consists of summarizing documents with shorter sentences which do not have to come from the input text (an abstract may paraphrase some fragments of text). It is commonly performed using Deep Learning Seq2Seq methods. Which approach is better for start? Extractive wins by far. First of all, you don't have to train any model, as these methods rely on information found solely in summarized text. Second, deep learning models, even if they exist and can be easily found *, are trained on specific texts, so they might not generalize well across application domains. But How should be the approach , when the dataset also has documents in french, chinese, hindi etc... If you have good tokenizers and stemmers for these languages you should be fine with TextRank, as it only relies on word counts (I'm not sure about Chinese though). BTW language detection is by far easier than summarization - just count number of stopwords of different languages that document contains. *They're not, for example Google published a paper sometime ago on the topic and didn't even disclose model, since the training dataset was proprietary. I was trying to find something for abstractive summarization sometime ago but I couldn't find anything that had good pretraining and worked out of the box.
