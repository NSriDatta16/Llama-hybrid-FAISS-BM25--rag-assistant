[site]: crossvalidated
[post_id]: 600660
[parent_id]: 600653
[tags]: 
I have found that non-negative-matrix-factorization is very useful for large numbers of binary features. In my experience, it tends to create a small number of dense feature vectors that are interpretable as plausible combinations of one-hot features that tend to go together. But this assumes that your hundreds of binary columns are the result of using one-hot or dummy encoding for several categorical variables. Entity embeddings could also be useful, if you (1) want to use a neural network and (2) have several high-cardinality categorical features to encode. If all of your columns encode a single categorical variable, then there's really not much to do: most models will just estimate a constant for each level of the category. (A corner case here is a model that uses shallow decision trees. Shallow trees won't be able to split the categories to purity, so some categories will be grouped together.)
