[site]: datascience
[post_id]: 76747
[parent_id]: 
[tags]: 
LSTM Neural Network gets stuck in a specific state when trying to predict new states over many time periods

I have built an LSTM neural network for category, or latent state, prediction. The data is more or less of the form: x1 = continuos number from current record x2 = continuous number from current record x3 = continuous number from current record x4 = state value for the current record Only one one-dimensional variable to be predicted. y1 = state value for the next record Fitted with an LSTM model of the form below: model_lstm = tf.keras.Sequential() model_lstm.add(tf.keras.layers.LSTM(10, return_sequences = True, input_shape = (xtrain.shape[1],xtrain.shape[2]))) model_lstm.add(tf.keras.layers.Dense(units=5, activation = "softmax", input_shape = (xtrain.shape[1],xtrain.shape[2]))) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy() opt = tf.keras.optimizers.Adam(learning_rate=.001) model_lstm.compile(loss=loss_fn, optimizer=opt, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]) Fitting an LSTM model of say this form makes it fit almost perfectly on training and validation data when presented with an individual sequence: [x1_t1, x2_t1, x3_t1, x4_t1] = [y1_t1] [x1_t2, x2_t2, x3_t2, x4_t2] = [y1_t2] . . . [x1_tn, x2_tn, x3_tn, x4_tn] = [y1_tn] however, it fails when simulating, a sort of walk where the predictions are used directly into the next inputs... [x1_t1, x2_t1, x3_t1, x4_t1] = [y1_t1] [x1_t2, x2_t2, x3_t2, y1_t1 ] = [y1_t2] . . . [x1_tn, x2_tn, x3_tn, y1_t(n-1) ] = [y1_tn] In fact it simply gets stuck in a state and doesn't move. Is there any solution to this? I am impressed with the accuracy but obviously it's not very useful unless a person only wanted one prediction at a time that he/she can immediately correct per time step output.
