[site]: crossvalidated
[post_id]: 484676
[parent_id]: 
[tags]: 
Why can't same-size layers of a neural network be combined or compressed into a single layer?

Probably a dumb question, obviously not seen in practice, but after reviewing linear algebra, I can't pinpoint the misunderstanding in this logic: We can represent input data to the network as a vector The vector is passed through weights in layers, which can be seen as transforms to the vector, which is passed on to the next transform In linear algebra, we can compose multiple transforms like a shear and rotation into an equivalent single transform So in a simple case where subsequent layers are of the same size or other ideal conditions, we should be able to compose multiple layers into a single layer, since really they're just transforms My only guess is that it's because we have to use nonlinear activation functions to model nonlinear distributions, and this composition equivalence doesn't apply to nonlinear transforms?
