[site]: crossvalidated
[post_id]: 90646
[parent_id]: 
[tags]: 
SVM basic theory?

I have some questions about SVM: In SVM there is a nonlinear and linear SVM. What is the difference between them? To do classification in SVM, we will find the linearly separable boundary (hyperplane). And if we cannot find it, we must project our input space into a features space. What does that feature space mean? I read on the internet that some explain that the features space is a scalar, and we get that scalar by inner product. For example, I have an input space matrix $A=[\{(1,2); 0\},\{(0,6);1\}]$; where 1, 2, 0, 6 are the features and 0, 1 are the labels. How can I project my input space A into feature space? Why do most people say that with kernel tricks, we actually don't project our data into a higher space? But we project it into a low space (because of that kernel tricks = scalar)? There are many kernel types: linear, polynomial, RBF and sigmoid. How can we determine which kernel type we have to use for our data? On this webpage , we can see that with polynomial and RBF kernel, we don't find the linearly separable boundary / hyperplane, but there is a curve, a group (for RBF). But as mentioned before, SVM finds the linear hyperplane? Why are they different? I appreciate all answers.
