[site]: crossvalidated
[post_id]: 512815
[parent_id]: 493162
[tags]: 
Another possible (non-symmetric) way of measuring the "distance" between any 2 distributions is the KL-Divergence . In this context, this MathOverflow post discusses specifically how to generalize the KL for more than 2 distributions. User Carlo Beenakker provides some references worth checking! For example (all credit goes to him): Information radius Average divergence Dissimilarity Also intuitively, I'd say that building up a sort of "Confusion matrix" $M$ with the KL divergences where $M_{a, b} = D_{KL}(X_a \| X_b )$ could be helpful (if you don't need to reduce the analysis to a single scalar). Another related metric worth checking is the Jensen-Shannon divergence . Hope this helped a bit!
