[site]: crossvalidated
[post_id]: 318384
[parent_id]: 
[tags]: 
Conditional Independence and Marginalization

I'm currently reading Bishops: Pattern Matching and Machine Learning and ran into an equation I didn't grasp. Page 373, Graphical Models, section 8.2.1. He says: ... Given the joint distribution: \begin{equation} p(a,b,c) = p(a|c)p(b|c)p(c) \end{equation} We can investigate whether a and b are independent by marginalizing both sides with respect to c , \begin{equation} p(a,b) = \sum_c p(a|c)p(b|c)p(c) \end{equation} and in general this does NOT factorize to the product: \begin{equation} p(a)p(b) \end{equation} ... My question is why it would not factor into the product above? Seems reasonable that once c has been marginalized out that's exactly the product you would get? Summing over all values of c should result in: \begin{equation} \sum_c p(c) = 1 \end{equation} and thus we should arrive at the product, \begin{equation} p(a,b) = \sum_c p(a|c)p(b|c)=p(a)p(b) \end{equation} What am I missing?
