[site]: crossvalidated
[post_id]: 605842
[parent_id]: 605837
[tags]: 
The answer to your question very much depends on the resource that you are using (i.e. there is no real right or wrong). When using the notation $\boldsymbol{z}^{(l)} = \boldsymbol{W}^{(l)} \boldsymbol{a}^{(l-1)} + \boldsymbol{b}^{(l)}$ and $\boldsymbol{a}^{(l)} = \phi\bigl(\boldsymbol{z}^{(l)}\bigr)$ , the error given by $$\boldsymbol{\delta}^{(l)} = {\boldsymbol{W}^{(l+1)}}^\mathsf{T} \boldsymbol{\delta}^{(l+1)} \odot \phi'\bigl(\boldsymbol{z}^{(l)}\bigr)$$ is the derivative w.r.t. the pre-activations, i.e. $\frac{\partial E}{\partial \boldsymbol{z}^{(l)}}$ . (Note: I took the liberty to use $\phi$ for the activation function and $\odot$ to denote the Hadamard (i.e. element-wise) product.) When considering the gradient w.r.t. the activations, i.e. $\frac{\partial E}{\partial \boldsymbol{a}^{(l)}}$ , the error would be $$\boldsymbol{d}^{(l)} = {\boldsymbol{W}^{(l)}}^\mathsf{T} \boldsymbol{d}^{(l+1)} \odot \phi'\bigl(\boldsymbol{z}^{(l)}\bigr).$$ The difference is subtle, but the recursion aligns the weights differently. However, you will rarely find the latter expression, because (almost) everyone uses the gradients w.r.t. the pre-activations. For some intuition as to why that is, I refer to this answer I gave to another question.
