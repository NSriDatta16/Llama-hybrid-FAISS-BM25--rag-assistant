[site]: stackoverflow
[post_id]: 965576
[parent_id]: 965336
[tags]: 
YMMV greatly with definitions here. IMHO the term "Unit Testing" has suffered from linguistic drift . (See my blog post on this for more information). It sounds like you have a good understanding of what a unit test is. In this case it may be easier to define what something is by explaining what it isn't. An integration test is effectively an automated test that isn't a unit test (provided your definition). I believe an integration test is a mutually exclusive category that encompasses all other techniques we use to automate tests where the components in a system are actually communicating with each other. This means, as you say, any external dependencies exist in your test's context. Others may or may not agree, but the important point to take away is that the maintainability of a given test increases as the size of your test context increases . The larger the context, the slower and less maintainable the test will be. Because of this, you really want to consider what you're going to get out of a test at this level. You'll really need continuous integration to maintain integration tests, and will probably need to schedule them to run on an interval if they take a long time to run. Often they'll be harder to diagnose failures for when they break (because they're more complex) and you'll so you want to be sure that the test provides clear business value if it is to be run continuously in your test suite. Another way to say this is, it's worse to have bad tests than no tests. This is why unit tests are really of the utmost importance to you as a developer. Testing at levels higher than an isolated unit/component provide less bang-for-the-buck. Naming and documenting can help a lot here, but just be careful. Write integration tests that are directly aimed at requirements/features or regressions/bugs. If it's a "smoke test" test things you care about the most, or things that break the most. You need to be pragmatic. Hope this helps.
