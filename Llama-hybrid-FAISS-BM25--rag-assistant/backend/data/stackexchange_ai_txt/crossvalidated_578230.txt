[site]: crossvalidated
[post_id]: 578230
[parent_id]: 
[tags]: 
Why not use exact probability in 0.632 or 0.632+ method with small sample size?

The .632 estimator (and extensions like .632+) developed by Bradley Efron are founded on the following premise. Suppose we have a data set with $n$ observations, and we draw $B$ nonparametric bootstrap samples (that is, each bootstrap sample consists of $n$ observations sampled with replacement with equal probabilities from the original sample). The probability that the $i$ th observation appears at least once in the $b$ th bootstrap sample, for any $i\in\left\{1,2,\ldots,n\right\}$ and $b\in\left\{1,2,\ldots,B\right\}$ , is $1-\left(1-1/n\right)^n$ , as is easily shown from the binomial distribution with $p=1/n$ . And $\displaystyle\lim_{n\to\infty}1-\left(1-1/n\right)^n=1-e^{-1}\approx 0.632$ . This leads to an argument for using weights of $1-0.632$ and $0.632$ in a weighted average of the training error and the out-of-bootstrap error respectively to obtain an estimate of in-sample prediction error. This is fine and good for large $n$ . However, for small $n$ , the probability can be quite far from $0.632$ ; for instance, for $n=10$ , $1-\left(1-1/n\right)^n\approx 0.651$ . Yet checking the source code for R functions implementing .632 or .632+ estimators (e.g., here ), they just use the weight $0.632$ in the estimator regardless of sample size. Would this not be a non-trivial source of bias for small sample sizes, and would it not therefore be better to always use a weight of $1-\left(1-1/n\right)^n$ rather than $0.632$ ?
