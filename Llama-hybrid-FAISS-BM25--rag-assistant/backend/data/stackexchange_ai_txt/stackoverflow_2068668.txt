[site]: stackoverflow
[post_id]: 2068668
[parent_id]: 
[tags]: 
Why is my return value wrong?

I have a vector class in C# (a fragment below). My issue is that when I call GetMagnitude(), it always returns 0.0f - even with the debugger running and I check that Sq has a valid value, as soon as it gets passed back into other function (eg: Normalize() ), it has return 0.0f. Can someone explain this and help me fix it? My guess is that it has something to do with double->float conversion but I just can't figure it. public class float3 { public float x; public float y; public float z; public float GetMagnitude() { float SumSquares = (float)( Math.Pow(x, 2) + Math.Pow(y, 2) + Math.Pow(z, 2)); float Sq = (float)Math.Sqrt(SumSquares); return Sq; } public void Normalize() { float inverse = 1.0f / GetMagnitude(); x *= inverse; y *= inverse; z *= inverse; } }
