[site]: crossvalidated
[post_id]: 363952
[parent_id]: 
[tags]: 
Why do SVMs using SMO algorithm work only when the initial values of the multipliers (the alphas) are zero?

I built an SVM in Python that uses the SMO algorithm described in the paper written by John Platt. If the alphas (Lagrange multipliers) are initialized to zero at the beginning everything works as expected. However, if they are initialized randomly using some probability distribution yielding values between 0 and C (or literally any other distribution), it doesn't work as it should. In fact the actual weight vector at the end of the training phase has huge values which implies that the margin is very small, and on top of that, it can't separate a linearly separable dataset. It looks completely broken. My question is: Why does it work only when the alphas have initial values of zero? Is there something wrong with my implementation or is it just a consequence of the way the SMO algorithm works (the paper from John Platt doesn't mention anything about the initial values of the alphas)?
