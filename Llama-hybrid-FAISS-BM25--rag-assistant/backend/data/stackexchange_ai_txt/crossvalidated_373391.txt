[site]: crossvalidated
[post_id]: 373391
[parent_id]: 372605
[tags]: 
Explained Variance is a natural fit for both k-means and PCA, because both methods perform a least squares optimization. It's easiest to see with k-means, which in every step reduces within-cluster variance - until convergence. PAM doesn't. Not unless you'd use PAM with squared Euclidean, at which point it will still only find a worse solution than k-means, and slower. So while it's straightforward for R^d data how to compute "variance explained" with PAM (total variance of all data - sum of total variances of each cluster) this metric is not an appropriate choice for PAM. You can, however, interpret variance explained as the quantity $$\frac{\sum_x\sum_y d(r(x),r(y))^2}{\sum_x\sum_y d(x,y)^2}$$ With or without the square, where r(x) the "reconstructed position" or "replacement" of x. Then this measures - to some extend, assuming that you are required to reduce distances overall, as it would be trivial to get arbitrarily large values - how well you approximate the data when replacing each point x with r(x). With PAM, the non-squared version seems most appropriate. It would be more meaningful to rather use some form of a squared normalized loss in reconstructing the pairwise distances, e.g., $$\frac{1}{N^2}\sum_i\sum_j \left(\frac{d(x,y)-d(r(x),r(y))}{d(x,y)}\right)^2$$
