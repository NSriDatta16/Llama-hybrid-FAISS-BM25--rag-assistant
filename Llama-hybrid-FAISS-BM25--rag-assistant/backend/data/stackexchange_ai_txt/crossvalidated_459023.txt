[site]: crossvalidated
[post_id]: 459023
[parent_id]: 
[tags]: 
Biased estimator obtained by optimal experiment design

I am using a model-based approach to infer the parameters of a given system. Namely, I represent my system by a model $\mathcal{M}$ with parameters $\theta$ . To estimate the true value of $\theta$ , I record the output $\mathcal{D}$ of my system to a given input, and use the likelihood of the data $p(\mathcal{D}|\mathcal{M},\theta)$ to compute the posterior distribution of my parameters $p(\theta|\mathcal{D},\mathcal{M})$ . My goal is to maximize the information I can get about $\theta$ , and hence to obtain a posterior distribution $p(\theta|\mathcal{D},\mathcal{M})$ as peaky as possible. I use Bayesian optimal experiment design to find the experimental protocol (i.e. the input to my system) which will maximize the information about $\theta$ . As explained in this article , the utility of a given experiment design can be defined Either as the gain in Shannon information about $\theta$ , that is to say the difference between the entropy of my posterior distribution and the entropy of my prior distribution; Or as the Kullback-Leibler divergence between the prior and the posterior. In each case, the optimal experiment design is the one that maximized the sharpness of my posterior $p(\theta|\mathcal{D},\mathcal{M})$ . But, by focusing only on minimizing the variance of my estimator, do I risk to maximize its bias ? With optimal experiment design techniques, I obtain a sharp and very informative posterior, but I have no guarantee that it will be close to the true value of $\theta$ . Is it possible to have degenerate cases in which estimators $\hat{\theta}$ obtained by optimal design have a low variance but a very high bias ? The literature seems to be only focused on minimizing its variance; any reference would be much welcome.
