[site]: crossvalidated
[post_id]: 550295
[parent_id]: 550290
[tags]: 
The current trend in machine learning research is to train huge models. Let me quote one article At OpenAI, an important machine-learning think tank, researchers recently designed and trained a much-lauded deep-learning language system called GPT-3 at the cost of more than $4 million. Even though they made a mistake when they implemented the system, they didn't fix it, explaining simply in a supplement to their scholarly publication that "due to the cost of training, it wasn't feasible to retrain the model." If a model is to expensive to train to fix a bug, certainly nobody is going to bother with $k$ -fold cross-validation. Moreover, if you have a huge dataset and subsample it, the subsample would be still huge enough to drive the variance low. Of course assuming that re-training would be deterministic, what is not true as there are known examples of non-reproducible results in machine learning, or getting different results with different random seeds.
