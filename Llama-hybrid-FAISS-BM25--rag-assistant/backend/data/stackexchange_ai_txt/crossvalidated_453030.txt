[site]: crossvalidated
[post_id]: 453030
[parent_id]: 221715
[tags]: 
I'm impressed no one mentioned it, but other best practices are to pad the sentences into a fixed size, initialize an embedding layer with the weights of Word2Vec and feed it into an LSTM. So it is basically what OP mentioned here, but including padding for handling the different lengths: Concatenating the vectors for all the words doesn't work, because it doesn't lead to a fixed-size feature vector. Example Consider the following sentence (taken from the Toxic Comment Classification Challenge ): "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27" First, we clean such sentence: "explanation why the edits made under my username hardcore metallica fan were reverted ? they weren ' t vandalisms , just closure on some gas after i voted at new york dolls fac . and please don ' t remove the template from the talk page since i ' m retired now . ipaddress" Next, we encode their words into integers: 776 92 2 161 153 212 44 754 4597 9964 1290 104 399 34 57 2292 10 29 14515 3 66 6964 22 75 2730 173 5 2952 47 136 1298 16686 2615 1 8 67 73 10 29 290 2 398 45 2 60 43 164 5 10 81 4030 107 1 216 And finally, if we perform padding with a length of 200, it would look like this: array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 776, 92, 2, 161, 153, 212, 44, 754, 4597, 9964, 1290, 104, 399, 34, 57, 2292, 10, 29, 14515, 3, 66, 6964, 22, 75, 2730, 173, 5, 2952, 47, 136, 1298, 16686, 2615, 1, 8, 67, 73, 10, 29, 290, 2, 398, 45, 2, 60, 43, 164, 5, 10, 81, 4030, 107, 1, 216], dtype=int32) We can force all sentences to have a maximum of 200 words, fill with zeros if they have less, or cut words that come later if they have more. Next, we initialize an embedding model with the weights of word2vec, here's an example using Keras: model.add(Embedding(nb_words, WV_DIM, weights=[wv_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)) wv_matrix contains a matrix with shape $‚Ñù^{nd}$ (number of unique words versus embedding dimension). And finally we add an LSTM layer after that, for example: embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences) x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences) References The full implementation of the above example if implemented in this Kaggle's Kernel . The code comes from this post .
