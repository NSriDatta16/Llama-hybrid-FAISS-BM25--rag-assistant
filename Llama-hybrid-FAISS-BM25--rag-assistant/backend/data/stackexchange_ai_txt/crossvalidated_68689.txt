[site]: crossvalidated
[post_id]: 68689
[parent_id]: 13237
[tags]: 
For nominal variables, the typical encoding in a neural network or electrical engineering context is called "one-hot" -- a vector of all 0s, with one 1 in the appropriate position for the value for the variable. For the days of the week, for example, there are seven days, so your one-hot vectors would be of length seven. Then Monday would be represented as [1 0 0 0 0 0 0], Tuesday as [0 1 0 0 0 0 0], etc. As Tim hinted, this approach can be generalized easily to encompass arbitrary boolean feature vectors, where each position in the vector corresponds to a feature of interest in your data, and the position is set to 1 or 0 to indicate the presence or absence of that feature. Once you have binary vectors, the Hamming distance becomes a natural metric, though Euclidean distance is used as well. For one-hot binary vectors, the SOM (or other function approximator) will naturally interpolate between 0 and 1 for each vector position. In this case, these vectors are often treated as the parameters of a Boltzmann or softmax distribution over the space of the nominal variable ; this treatment gives a way to use the vectors in some sort of KL divergence scenario as well. Cyclic variables are much trickier. As Arthur said in the comments, you'd need to define a distance metric yourself that incorporates the cyclic nature of the variable.
