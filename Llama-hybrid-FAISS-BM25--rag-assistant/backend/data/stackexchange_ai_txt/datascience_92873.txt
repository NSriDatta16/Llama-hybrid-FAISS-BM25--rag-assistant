[site]: datascience
[post_id]: 92873
[parent_id]: 92856
[tags]: 
I don't understand how a pre-trained model can adapt to my given corpus You are correct in thinking this way. It is not a magic wand. It learns the embedding values based on the underlying context of the corpus(e.g. news) which may work in the broad sense but not in a specific case. Two cities may get the embeddings based on their geographical location but that might not be the embedding we may like if we are comparing cities on crime rate/GDP etc. One of the best posts on Word Embedding [ Blog post by Sebastian Ruder ] has mentioned all the limitations in detail. An excerpt from the post, " ...One of the major downsides of using pre-trained embeddings is that the news data used for training them is often very different from the data on which we would like to use them. In most cases, however, we do not have access to millions of unlabelled documents in our target domain that would allow for pre-training good embeddings from scratch.... " If I have new words not present in the pre-trained model will I be able to use this pre-trained model to learn the embeddings for the new words? It will get a value from a bucket for all the out-of-vocabulary(OOV) list. Sometimes it can be the same value for all the new words. But there is some known strategy to deal with the scenario. Check the post for relevant papers. From the same posts, " ....One of the main problems of using pre-trained word embeddings is that they are unable to deal with out-of-vocabulary (OOV) words, i.e. words that have not been seen during training. Typically, such words are set to the UNK token and are assigned the same vector, which is an ineffective choice if the number of OOV words is large.... " But this issue will prevail even if you learn embeddings with your own corpus. So I believe it is not just limited to the pre-trained embeddings.
