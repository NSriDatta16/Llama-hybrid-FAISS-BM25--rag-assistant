[site]: crossvalidated
[post_id]: 371107
[parent_id]: 260838
[tags]: 
As the other answer notes, a common non-parametric Bayesian alternative to neural networks is the Gaussian Process . (See also here ). However, the connection runs much deeper than that. Consider the class of models known as Bayesian Neural Networks (BNN). Such models are like regular deep neural networks except that each weight/parameter in the network has a probability distribution describing its value . A normal neural network is then somewhat like a special case of a BNN, except that the probability distribution on each weight is a Dirac Delta. An interesting fact is that infinitely wide Bayesian neural networks become Gaussian Processes under some reasonable conditions. Neal's thesis, Bayesian Learning for Neural Networks (1995) shows this in the case of a single-layer network with an IID prior. More recent work (see Lee et al, Deep Neural Networks as Gaussian Processes , 2018 ) extends this to deeper networks. So perhaps you can consider large BNNs as approximations of a non-parametric Gaussian process model. As for your question more generally, people often just need mappings in supervised learning, which it seems Bayesian non-parametrics are not as common for (at least for now), mostly for computational reasons (the same applies to BNNs, even with recent advances in variational inference). However, in unsupervised learning, they show up more often. For instance: Goyal et al, Nonparametric Variational Auto-encoders for Hierarchical Representation Learning , 2017 Abbasnejad and Dick, Infinite Variational Autoencoder for Semi-Supervised Learning , 2017 Chen, Deep Learning with Nonparametric Clustering , 2015
