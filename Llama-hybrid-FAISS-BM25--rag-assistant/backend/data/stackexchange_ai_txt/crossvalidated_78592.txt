[site]: crossvalidated
[post_id]: 78592
[parent_id]: 78590
[tags]: 
I don't know if you split your dataset randomly (each sample receives a random subset of observations) or not. I assume that your split is random. why does the training error start so high, then suddenly drop, then start to rise again as training set size increases? This is merely a noise caused by small size of your training and test sets, as well as the random nature of the random forest model. As for the gap between the training and test set, such a gap is common for a model as complex as random forest, and the data of only 800 training examples (how many explanatory variables you have, by the way?)
