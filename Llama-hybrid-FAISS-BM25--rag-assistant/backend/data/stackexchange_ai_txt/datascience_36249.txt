[site]: datascience
[post_id]: 36249
[parent_id]: 36247
[tags]: 
When represented at a minute level, the pattern is going to recur after almost 1440 minutes. So your LSTM needs to learn a really long term dependency here. And LSTMs are not great at such long term dependencies. I'll have a contrary opinion here. Drop neural networks, fit a good linear model to get a benchmark. If the linear model (example outlined below) beats your LSTM error rate currently, invest in building a better (generalised) linear regression model instead. Also, start with a more coarse definition of output (predicting total for next 15 minutes / 1 hour). Create hand-rolled features for your data. Start with traffic at the same hour of the day yesterday, traffic in same hour of day for past 7 days, traffic on the same hour of the day on same weekday last week. Add more advanced features by defining better aggregation window than hour. Something like 6 - 9 PM, 12 midnight - 6 am. Add features related to recent data (ratio of cumulative traffic today compared to cumulative traffic till same time yesterday). Build a suitable linear model on this data, benchmark the error rate. If the traffic is being measured in number of hits, use poisson regression instead of simple linear regression. Complex neural net models like LSTM are best used when hand-rolled features are extremely difficult to think and implement. For many time series applications, generating features yourself is intuitive, simple to achieve (most libraries like Python pandas have great date-time functionality) and good first step. It is much better than teaching LSTM to differentiate between morning and afternoon. Fran√ßois Chollet's book on deep learning has a chapter on similar application. He tackles the problem of predicting weather in particular time window based on past data. He shows that many complex neural net architectures find it hard to beat simple baseline like averaging from same time period in past.
