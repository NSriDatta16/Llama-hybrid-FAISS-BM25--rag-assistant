[site]: crossvalidated
[post_id]: 20531
[parent_id]: 20525
[tags]: 
No, this isn't necessarily a problem, especially if the sample size is small. It could easily be that purely by chance more of the "easy" patterns are in the validation set and more if the "difficult" ones are in the training set. If you were to repeatedly re-sample the data to form randomly partitioned training and validation sets, you would expect the average error on the training set to be lower than on the validation set, but that does not mean that it will be lower on every run of the experiment. If your sample size is small, it suggests that this variability means that the validation set performance estimate has a high variability and isn't a reliable indicator of performance, so you should probably use some sort of (repeated) cross-validation or perhaps bootstrapping instead. I have seen this sort of thing before as I have been working on the problems caused in model selection caused by the variance of the model selection criterion. It doesn't necessarily indicate a problem with the model, but it does suggest that the sample of data is too small. If the relative class frequencies are very disparate, then it may be that the validation set happens to have fewer minority class examples than the training set, which might also affect the performance estimate, use stratified bootstrap or cross-validation, which maintains the same proportion of positive and negative patterns in the training set and validation set.
