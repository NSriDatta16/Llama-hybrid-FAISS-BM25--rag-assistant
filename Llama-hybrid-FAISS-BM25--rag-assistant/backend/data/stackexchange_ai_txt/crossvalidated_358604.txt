[site]: crossvalidated
[post_id]: 358604
[parent_id]: 
[tags]: 
How does weight sharing stabilize the learning in RNNs and CNNs?

I have read this multiple times, for example the SNN paper , that weight sharing tends to stabilize the learning in CNNs and RNNs. But i have never come across a reason for it. My question is why? Should not the weight sharing lead to more gradient explosions? In CNNs maybe the addition of all the gradients from feature map to a filter leads to some averaging (i am not sure about this as well), but RNNs? last i checked their weight sharing was in sequence and it leads to products of gradients which leads to gradient explosion and gradient vanishing. Please at least direct me in the right direction if i am missing something quite obvious
