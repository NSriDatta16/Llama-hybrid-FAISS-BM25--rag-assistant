[site]: crossvalidated
[post_id]: 611045
[parent_id]: 611043
[tags]: 
Go through the definition of conditional probability. $$ P\left(Y = y\vert \hat Y=y\right) $$ $$= \dfrac{ P\left(\left(\hat Y = y\right)\bigcap \left(Y = y\right)\right) }{ P\left(\hat Y = y\right) } $$ $$=\dfrac{ \dfrac{ \text{Count of How Many Points are Classified } y\text{ and really are } y }{ \text{Total Number of Classification Attempts} } }{ \dfrac{ \text{Count of How Many Points are Classified } y }{ \text{Total Number of Classification Attempts} } }$$ $$=\dfrac{ \text{Count of How Many Point are Classified } y\text{ and really are } y }{ \text{Count of How Many Points are Classified } y } $$ Note, however, that multinomial logistic regression does not make hard classifications. Instead, such a model returns the probabilities of class membership. You can map those probabilities to categories, and many people do just assign the label with the highest probability, but those probabilities can be useful. After all, wouldn't you be more comfortable making a decision with probabilities like $(0.95, 0.02, 0.03)$ than $(0.34, 0.33, 0.33)$ , despite the fact that both would be mapped to the same category according to a rule that assigns the category with the highest probability?
