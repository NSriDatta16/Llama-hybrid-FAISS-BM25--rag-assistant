[site]: crossvalidated
[post_id]: 312499
[parent_id]: 
[tags]: 
How to condition on the result of a test?

Let $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ i.i.d. and write $X = (X_1, \dots, X_n)$. Define the point hypotheses $H_0 : \mu = 0$ and $H_1 : \mu = \mu_1$ for some fixed $\mu_1 > 0$. Define the test statistic $T(x) := \bar{x} \sqrt{n} / s(x)$. Then $T(X) \sim_{H_0} t(n-1)$ and $T(X) \sim_{H_1} t(n-1, \mu_1 \sqrt{n}/\sigma)$, where the latter is the $t$ distribution with the appropriate non-centrality parameter. Let $\alpha \in (0,1)$ and $t_{1-\alpha}$ the $1-\alpha$ quantile of $t(n-1)$. Then $\mathrm{Pr}(t_{1-\alpha} \leq T(X) \,\vert\, H_0) = \alpha$. Denote $\beta := \mathrm{Pr}(T(X) Assume that we observe $x^* \in \mathbb{R}^n$, which yields $t^* := T(x^*)$ and assume moreover that $t_{1-\alpha} \leq t^*$. Let us treat $H_0$ and $H_1$ as events. We would like to know the odds of $H_1$ over $H_0$ conditioned on the observation of $x^*$, given some fixed estimate of the unconditioned odds. Alternative 1 (A.1): we simply say that the test is positive (at the $\alpha$ significance level with power $1-\beta$) since the test statistic went past the critical value. We hence condition on a positive test using Bayes' rule: $$\frac{\mathrm{Pr}(H_1 \,\vert\, t_{1-\alpha} \leq T(X))}{\mathrm{Pr}(H_0 \,\vert\, t_{1-\alpha} \leq T(X))} = \frac{\mathrm{Pr}(t_{1-\alpha} \leq T(X) \,\vert\, H_1)}{\mathrm{Pr}(t_{1-\alpha} \leq T(X) \,\vert\, H_0)} \cdot \frac{\mathrm{Pr}(H_1)}{\mathrm{Pr}(H_0)} = \frac{1-\beta}{\alpha} \cdot \frac{\mathrm{Pr}(H_1)}{\mathrm{Pr}(H_0)}$$ This seems appropriate when we treat the test as a black box, which only gives the "positive" and "negative" answers, and $\alpha$ and $\beta$ were obtained from simulations. This is very similar to what happens with medical tests. Alternative 2 (A.2): we condition on the fact that we obtained a test statistic of $t^*$ or larger: $$\frac{\mathrm{Pr}(H_1 \,\vert\, t^* \leq T(X))}{\mathrm{Pr}(H_0 \,\vert\, t^* \leq T(X))} = \frac{\mathrm{Pr}(t^* \leq T(X) \,\vert\, H_1)}{\mathrm{Pr}(t^* \leq T(X) \,\vert\, H_0)} \cdot \frac{\mathrm{Pr}(H_1)}{\mathrm{Pr}(H_0)}$$ This can also be understood as conditioning on the $p$ value of the observation. Alternative 3 (A.3): we condition on the fact that we obtained a test statistic of exactly $t^*$. Considering an appropriate limit, this can be written as: $$\frac{\mathrm{Pr}(H_1 \,\vert\, T(X) = t^*)}{\mathrm{Pr}(H_0 \,\vert\, T(X) = t^*)} = \frac{f_1(t^*)}{f_0(t^*)} \cdot \frac{\mathrm{Pr}(H_1)}{\mathrm{Pr}(H_0)}$$ Here, $f_i$ is the probability density function of the distribution of $T(X)$ under $H_i$. A.1 follows the principle of setting a significance level in advance and not changing it later. A.3 follows the principle of conditioning on all the available evidence (although one may argue that $x^*$ and not $t^*$ is the available evidence, but I don't know how to incorporate every aspect of it). A.2 is something in between. The graphs shown below give the factors $\frac{1-\beta}{\alpha}$, $\frac{\mathrm{Pr}(t^* \leq T(X) \,\vert\, H_1)}{\mathrm{Pr}(t^* \leq T(X) \,\vert\, H_0)}$, and $\frac{f_1(t^*)}{f_0(t^*)}$ for $\alpha = 0.05$ and $t^*$ ranging from $t_{1-\alpha}$ to $t_{1-\alpha}+1$, with $n=10$ and for $\mu_1,\sigma \in \{0.5, 1, 2\}$. The code is available (Dropbox link). A.1 (red) is always more pessimistic about $H_1$ than A.2 (green), so we can use the factor from A.1 for a lower bound on the factor in A.2. A.3 (blue) is substantially smaller than A.1 or A.2 for test statistics close to the critical value and when $\sigma$ is small compared to $\mu_1$. Particularly alarming are the cases $(\mu_1,\sigma)=(1,0.5)$ and $(\mu_1,\sigma)=(2,0.5)$ and $(\mu_1,\sigma)=(2,1)$. Here, conditioning as per A.3 is (strongly) in favor of $H_0$, even though the test is positive with respect to the significance level $\alpha$. What arguments would you give in favor of each of the alternatives? What I have stumbled upon here is perhaps nothing more than a version of Lindley's Paradox: the test is positive (that is, in favor of $H_1$) in NHST terms, but the likelihood ratio tells a different story. However, this does not answer the question why A.1 is popular in current literature , for example: Ioannidis, "Why Most Published Research Findings Are False", 2005, doi:10.1371/journal.pmed.0020124 Oâ€™Brien and Castelloe, "Sample-Size Analysis for Traditional Hypothesis Testing: Concepts and Issues", Chapter 10 in "Pharmaceutical Statistics Using SAS", 2007 On the other hand, the following are clearly in favor of A.3: Hooper, "The Bayesian interpretation of a P-value depends only weakly on statistical power in realistic situations", 2009, doi:10.1016/j.jclinepi.2009.02.004 Colquhoun, "The reproducibility of research and the misinterpretation of p-values", 2017, doi:0.1098/rsos.171085 The first, in a section called "Problems with the diagnostic testing argument", argues against A.1 by saying that "if we only record that $P
