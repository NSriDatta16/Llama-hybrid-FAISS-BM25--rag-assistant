[site]: crossvalidated
[post_id]: 391769
[parent_id]: 
[tags]: 
Why are all predictions made by XGBoost distinct?

If I understood correctly the XGBoost is a framework that operates on gradient tree boosting. It means that behind the scenes, it uses a decision tree to make a prediction. So, from what I read in the Introduction to Statistical Learning by G.James et al., I should obtain a finite number of predicted values. This deduction comes from a particular fragment (chapter 8, p. 306), namely: We divide the predictor space—that is, the set of possible values for X1,...,Xp into J distinct and non-overlapping regions, R1,...,RJ. For every observation that falls into the region Rj, we make the same prediction , which is simply the mean of the response values for the training observations in Rj. Suppose that in Step 1, we obtain two regions and that the response mean of the training observations in the first region is 10, while the response mean in the second region is 20. Then for a given observation X = x, if x ∈ R1, we will predict a value of 10, and if x ∈ R2, we will predict a value of 20. After having trained the model with maximum depth of 10 (that would give 1024 distinct nodes), I made predictions on a test set and I found out that I have 524000 distinct values which means that I have a unique value for each row in the dataset. In the light of the quoted fragment, my question is - why is it so? Why are all my predicted values distinct? Here's my python code, if it's needed: XGB_PARAMETERS = { 'eta': 0.2, 'nthread': 4, 'min_child_weight': 30, 'max_depth': 10, 'colsample_bytree': 0.7, 'subsample': 0.8, 'eval_metric': 'rmse', 'silent':1 } model = xgb.train( XGB_PARAMETERS, d_test, 150, watchlist, early_stopping_rounds=10, maximize=False, verbose_eval=10) d_final_test = xgb.DMatrix(test[f2].values) ypred = model.predict(d_test) y_test = model.predict(d_final_test) test['duration'] = np.exp(y_test) - 1
