[site]: crossvalidated
[post_id]: 63179
[parent_id]: 63152
[tags]: 
I'll try to add to the intuitive operational description... A good intuitive way to think about a neural network is to think about what a linear regression model attempts to do. A linear regression will take some inputs and come up with a linear model which takes each input value times some model optimal weighting coefficients and tries to map the sum of those results to an output response that closely matches the true output. The coefficients are determined by finding the values which will minimize some error metric between the desired output value and the value that is learned by the model. Another way to say it is that the linear model will try to create coefficient multipliers for each input and sum all of them to try to determine the relationship between the (multiple) input and (typically single) output values. That same model can almost be thought of as the basic building block of a neural network; a single unit perceptron. But the single unit perceptron has one more piece that will process the sum of the weighted data in a non-linear manner. It typically uses a squashing function (sigmoid, or tanh) to accomplish this. So you have the basic unit of the hidden layer, which is a block that will sum a set of weighted inputs-- it then passes the summed response to a non-linear function to create an (hidden layer) output node response. The bias unit is just as in linear regression, a constant offset which is added to each node to be processed. Because of the non-linear processing block, you are no longer limited to linear only responses (as in the linear regression model). Ok, but when you have many of the single perceptron units working together, each can have different input weight multipliers and different responses (even though ALL process the same set of inputs with the same non-linear block previously described). What makes the responses different is that each has different coefficient weights that are learned by the neural network via training (some forms include gradient descent). The result of all of the perceptrons are then processed again and passed to an output layer, just as the individual blocks were processed. The question then is how are the correct weights determined for all of the blocks? A common way to learn the correct weights is by starting with random weights and measuring the error response between the true actual output and the learned model output. The error will typically get passed backwards through the network and the feedback algorithm will individually increase or decrease those weights by some proportion to the error. The network will repeatedly iterate by passing forward, measuring the output response, then updating (passing backwards weight adjustments) and correcting the weights until some satisfactory error level is reached. At that point you have a regression model that can be more flexible than a linear regression model, it is what is commonly called a universal function approximator. One of the ways that really helped me to learn how a neural network truly operates is to study the code of a neural network implementation and build it. One of the best basic code explanations can be found in the neural network chapter of (the freely available) 'The Scientist and Engineer's guide to DSP' Ch. 26. It is mostly written in very basic language (I think it was fortran) that really helps you to see what is going on.
