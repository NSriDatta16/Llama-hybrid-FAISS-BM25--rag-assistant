[site]: crossvalidated
[post_id]: 463798
[parent_id]: 
[tags]: 
Question about Decision boundary in Logistical Regression

I am a Machine Learning newbie and studying Logistical Regression. For the data shown above, a straight line cannot separate all the positive and negative decisions. One thing I could understand is that I would need a higher order polynomial such as θ 0 + θ 1 x 1 + θ 2 x 2 + θ 3 x 2 1 + θ 4 x 2 2 But consider that, prediction is h θ (x) = 1/(1 + e - z ) where z = θ 0 + θ 1 x 1 + θ 2 x 2 (a straight line) and the cost equation is -y.log(h θ (x)) + (1 -y).log(1 - h θ (x)). For this cost equation, whenever h θ (x) tends to 1 and y is 0 OR whenever h θ (x) tends to 0 and y is 1, the cost tends to infinity. So my questions are: Will the above mentioned cost function be a convex function? If we use gradient descent, will it converge to global minimum? I think that gradient descent will not converge as the cost tends to infinity whenever prediction and actual do not match. Whether I am correct or wrong, please help with an explanation.
