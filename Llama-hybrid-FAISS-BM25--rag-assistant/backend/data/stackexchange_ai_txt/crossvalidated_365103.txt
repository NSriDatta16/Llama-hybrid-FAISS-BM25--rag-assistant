[site]: crossvalidated
[post_id]: 365103
[parent_id]: 
[tags]: 
Long repetitive output after changing vocabulary in seq2seq model

I trained a neural question generation model , which produces sensible questions for the vocabulary that they distributed with the paper. I wanted to run the model on a different set of word embeddings (i.e: glove.840B.300d). I only added PAD, SOS and EOS tokens, and picked the k most frequent words. Since the two sets of word embeddings come from the same distribution (Common Crawl), I expected this to work reasonably well. However, the resulting questions are now complete non-sense, excessively long sentences with a lot of repetitions. It's almost like the hypotheses "get stuck" with certain words. For example: For the answer the first degrees from the college were awarded in 1849 . The decoder produces the following output for the original embeddings in what year were the first degrees from the college awarded ? And the following with the GLoVe vectors he . college college in . in . in degrees in degrees in college 1849 in in in 1849 from in in in 1849 from in from in from in from in from in from in from in from in in in 1849 in the in the the the the What could I be doing wrong? I didn't change the architecture at all. Should I? Is there anything that I should do besides what I have already done?
