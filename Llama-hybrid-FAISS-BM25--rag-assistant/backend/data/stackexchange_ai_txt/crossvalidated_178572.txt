[site]: crossvalidated
[post_id]: 178572
[parent_id]: 178533
[tags]: 
[Extract from our book Bayesian Essentials] A notion central to Bayesian model choice is the Bayes factor $$ B^\pi_{21}(x) = \dfrac{ \mathbb{P}^\pi(\mathfrak{M}=2|x) / \mathbb{P}^\pi(\mathfrak{M}=1|x)} { \mathbb{P}^\pi(\mathfrak{M}=2)/ \mathbb{P}^\pi(\mathfrak{M}=1)}\,, $$ which corresponds to the classical odds ratio, the difference being that the parameters are integrated rather than maximized under each model. This quantity is the ratio of the posterior probabilities under equal prior weights on $\mathfrak{M}=1,2$. Obviously, the Bayes factor depends on prior information through the choice of the model priors $\pi_1$ and $\pi_2$, $$ B^\pi_{21}(x)=\frac{\int_{\Theta_2}\ell_2(\theta_2|x)\pi_2(\theta_2)\,\text{d}\theta_2} {\int_{\Theta_1}\ell_1(\theta_1|x)\pi_1(\theta_1)\,\text{d}\theta_1}=\frac{m_2(x)}{m_1(x)}\,, $$ and thus it can clearly be perceived as a Bayesian likelihood ratio which replaces the likelihoods with the marginals under both models. Consider now the special case when we want to assess whether or not a specific value of one of the parameters is appropriate, for instance $\mu=\mu_0$. While the classical literature presents this problem as point null, we simply interpret it as the comparison of two models, $\mathscr{N}(\mu_0,\sigma^2)$ and $\mathscr{N}(\mu,\sigma^2)$. We define $\pi_1(\omega)$ as the prior under the restricted model (labelled $M_1$) and $\pi_2(\theta)$ as the prior under the unrestricted model (labelled $M_2$). The corresponding Bayes factor is then $$ B^\pi_{21}(x)=\dfrac{\overbrace{\int_{\Theta} f(x|\theta)\pi_2(\theta)\,\text{d}\theta}^\text{unrestricted marginal $m_2(x)$}} {\underbrace{\int_{\Omega} f(x|(\delta_0,\omega))\pi_1(\omega)\,\text{d}\omega}_\text{restricted marginal $m_1(x)$}} $$ In the very special case when the whole parameter is constrained to a fixed value, $\theta=\theta_0$, the marginal likelihood under model $M_1$ coincides with the likelihood $\ell(\theta_0|x)=f(x|\theta_0)$ and the Bayes factor simplifies in $$ B^\pi_{21}(x) = \dfrac{\overbrace{\int_{\Theta} f(x|\theta)\pi_2(\theta)\,\text{d}\theta}^\text{marginal of $x$, $m(x)$}}{f(x|\theta_0)}\,. $$ This applies directly to question (b): when testing between $$\bar{x}_n\sim\mathcal{N}(\mu_0,\sigma^2/n)\ \text{ and }\ \bar{x}_n\sim\mathcal{N}(\mu_0,\sigma^2/n)$$ for $\bar{x}_n=\mu_0+1.96\sigma/\sqrt{n}$, we have \begin{align*} f(x|\theta_0) &= \sqrt{n}\varphi(\sqrt{n}\{\bar{x}_n-\mu_0\}/\sigma)/\sigma\\ &= \sqrt{n}\varphi(1.96)/\sigma \end{align*} and \begin{align*} \int_{\Theta}f(x|\theta)\pi_2(\theta)\,\text{d}\theta &= \int_{\Theta} \sqrt{n}\varphi(\sqrt{n}\{\bar{x}_n-\mu\}/\sigma)/\sigma\times \varphi(\{\mu-\mu_0\}/\sigma)/\sigma \,\text{d}\mu\\ &= \sqrt{n}\varphi(1.96/\sqrt{n+1})/\sqrt{n+1}\sigma \end{align*} Therefore \begin{align*} B^\pi_{21}(x) &= \dfrac{\int_{\Theta} f(x|\theta)\pi_2(\theta)\,\text{d}\theta}{f(x|\theta_0)}\\ &= \dfrac{\sqrt{n}\varphi(1.96/\sqrt{n+1})/\sqrt{n+1}\sigma} {\sqrt{n}\varphi(1.96)/\sigma}\\ &= \dfrac{\varphi(1.96/\sqrt{n+1})}{\sqrt{n+1}\varphi(1.96)} \end{align*} which takes the value 2.02 for n=5 and 0.92 for n=50 (and 0.30 for n=500). The Bayes factor goes to zero with $n$, while the corresponding $p$-value remains constant at $\mathbb{P}(|X|>1.96)=0.05$. This is one version of the Lindley-Jeffreys paradox . The second version of this Lindley-Jeffreys paradox is to let the prior variance go to infinity, as suggested in part (c), with exactly the same outcome: the null hypothesis always ends up being preferred, no matter what the value of the observation is. To answer specifically question (b) of the question now that the deadline for the homework is over, the posterior probability can be derived from the Bayes factor as $$\mathbb{P}(\mu=\mu_0|x)=\dfrac{B^\pi_{12}(x)}{1+B^\pi_{12}(x)}$$
