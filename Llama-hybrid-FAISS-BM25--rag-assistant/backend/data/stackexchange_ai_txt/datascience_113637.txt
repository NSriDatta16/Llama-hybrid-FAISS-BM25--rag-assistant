[site]: datascience
[post_id]: 113637
[parent_id]: 113628
[tags]: 
Encoders "encode" your high dimensionality input into a lower dimensional space. One way to compare different encoders is to use such representation for building the same model (say a NN with a fixed amount of layers) and to see which one is performing best or if the performance loss for the lower dimensionality is acceptable. A downside of that stand on the training procedure that advantage one representation instead of another. Think for example as having very few data points and to compare a big encoder with a small one: probably the small one will be better because it won't overfit. You can also take this concept to the extreme and use those representations as inputs to a linear/logistic regression: in this way the chances of overfitting are very low and the better representation for the linear scenario will result in higher performances.
