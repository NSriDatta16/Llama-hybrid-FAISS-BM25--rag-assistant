[site]: crossvalidated
[post_id]: 287136
[parent_id]: 287117
[tags]: 
First of all, how can it be the case that you do not have the Confusion Matrix but statistics derived from it? If you run a binary classification model you can just compare the predicted labels to the labels in the test set in order to get the TP, FP, TN, FN. In general, the f1-score is the weighted average between Precision $\frac{TP}{TP+FP}$ (Number of true positives / number of predicted positives) and Recall $\frac{TP}{TP+FN}$, $$f1 = 2\times \frac{Precision \times Recall}{Precision+Recall}$$. Note that, Recall is equivalent to the True Positive Rate (TPR), also know as sensitivity . Furthermore, the False Negative Rate is related to the True Positive Rate in the following way: $FNR=1-TPR$. The True Negative Rate is also known as Specificity: $TNR=1-FPR$, As can be observed in your question. In order to calculate the f1-score we need to find the precision which is missing from your given values. Precision, can also be found by the following relation: $$Precision = \frac{Recall \times Prevalence}{Recall \times Prevalence + (1-TNR) \times (1-Prevalence)} $$ Where Prevalence is the ratio of positive conditions (number of positives in your test set) to the total population. You should have the number of positive conditions in your test data, so that you may get the f1-score. Note that the f1-score does not take the True Negatives into account. You may find further information here: Confusion Matrix Wikipedia Article . I recommend that you read it. Hope this helps.
