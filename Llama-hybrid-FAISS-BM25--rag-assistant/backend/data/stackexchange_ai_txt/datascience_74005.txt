[site]: datascience
[post_id]: 74005
[parent_id]: 
[tags]: 
In backpropagation, scale is also important?

I think backpropagation is needed to find the direction of gradient decent method. I also wonder, the scale is also important? I heard some issue of vanishing(or exploding) gradient problem. If the direction of the backpropagation is remained, we still could apply gradient decent method(cause we still know the direction to update) and could finally get optimal answer. If I'm correct, there is no real limit in deep learning? I mean, although the speed may be slow, we can always finish the train work of neural network?
