[site]: crossvalidated
[post_id]: 471563
[parent_id]: 471552
[tags]: 
Yes, it all depends on the algorithm and the cost function. For example, logistic regression's output is between 0-1 and you can't calculate cross entropy loss with labels -1 and 1. Similar things happen in neural nets, you'd want to use tanh in the last layer (because it's output is between -1 and 1). But, you won't be able to use the cross entropy loss function again. In SVM, usage of -1,1 is more common due to the problem formulation. In contrast to the comment under your question, hinge loss get along well with signed labels.
