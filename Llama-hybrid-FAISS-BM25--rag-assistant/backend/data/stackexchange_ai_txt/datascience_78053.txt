[site]: datascience
[post_id]: 78053
[parent_id]: 
[tags]: 
Transfer Learning for CNNs and Batch Norm Layers

In some transfer learning models, we set the training argument to False to maintain the pre-trained values of Batch Normalization for example , but the trainable attribute to False to freeze the weights. Then the new "top-layer" is added and we re-train the model. Afterwards for fine-tuning, we can re-train the weights and set the trainable attribute to True. However, what does the argument training=True do for a layer? The stackoveflow answer here does not make sense to me. When the argument training is True, that, to me, implies we are doing some type of learning: i.e. the BN mean and variance are being updated, Dropout is being applied, and the weights are from the backwards pass. What is the difference between training=True and training=False ? The Keras FAQ states it just means inference is being performed, but what was being trained when training=True ? Lastly, this is being nit-picky, but in this notebook Google does transfer learning with the MobileNet V2 model. In same from above they use Xception model. Both models have BN, but in the second tutorial, they pass the training=False argument in the base_model implying do not update BN. Whereas in the first they make no mention of training=False . Why might that be? I see the first one is copyrighted in 2019 and the second one in 2020, which might imply the discrepancy.
