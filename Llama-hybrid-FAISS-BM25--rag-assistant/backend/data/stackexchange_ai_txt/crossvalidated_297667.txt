[site]: crossvalidated
[post_id]: 297667
[parent_id]: 238637
[tags]: 
Great approach for designing a DNN! I would make the architecture deeper (e.g. add a number of layers and a bypass parameter for each layer). Also, it is recommended to use ReLU activation with He normal weight initialization and a keep probability of 0.5 for dropout during training. It's also common to use Batch Normalization right before the activation layer. In order to speed up parameter search, I would experiment with random search or bayesian optimization that uses Gaussian Processes (GPs) to explore parameter and trade-off between exploration and exploitation.
