[site]: crossvalidated
[post_id]: 402756
[parent_id]: 402668
[tags]: 
You said that your understanding of t-SNE is based on https://www.youtube.com/watch?v=NEaUSP4YerM and you are looking for an explanation of UMAP on a similar level. I watched this video and it is pretty accurate in what it says (I have some minor nitpicks, but overall it is fine). Funny enough, it almost applies to UMAP just as it is. Here are things that do not apply: Similarities are computed from distances using a different kernel; it is not Gaussian, but it also decays exponentially and it also has adaptive width, as in t-SNE. Similarities are not normalized to sum to 1, but still end up being normalized to sum a constant value. Similarities are symmetrized, but not just by averaging. The similarity kernel in the embedding space is not exactly t-distribution kernel, but a very very similar kernel. I think all of these differences are not very important and not very consequential. The actually important part is the part where in the video the narrator says (10m40s): We want to make this row look like this row [...] The video does not explain how t-SNE quantifies whether they are similar or not and how it goes on achieving that they look similar. Both parts are different in UMAP. But the quoted statement can apply to UMAP too. The way the UMAP paper is written, the computational similarities to t-SNE are not very apparent. Scroll down to Appendix C in https://arxiv.org/pdf/1802.03426.pdf and/or look here https://jlmelville.github.io/uwot/umap-for-tsne.html , if you want to see a side-by-side comparison of the computations that I list above and the loss functions of t-SNE and UMAP.
