[site]: crossvalidated
[post_id]: 252215
[parent_id]: 
[tags]: 
KL divergence between discrete data and model (choosing hyperprior over Dirichlet concentration parameter $\alpha$)

I have some categorical data that follow an unknown true multinomial distribution $p$ and a model with known multinomial distribution $q$. I want to estimate the KL divergence between $p$ and $q$ from the data: $$ \text{KL}(p||q) = - H(p) + H(p,q) $$ where the first term is the entropy of the data and the second term the cross-entropy (see Wikipedia ). $-H(p) = \mathbb{E}_p[\log p]$ is nonlinear in $p$, which causes trouble for naive estimators such as the plug-in estimator which can be massively biased. A good estimator for the entropy of the data is represented by the Nemenman-Shafee-Bialek (NSB) estimator. See here for the original paper, and this post from Sebastian Nowozin's blog for a gentler introduction. In short, a Bayesian estimator with a Dirichlet prior with fixed $\alpha$ (including $\alpha = 0$) is a bad idea for estimating the entropy, because the ``seemingly innocent'' prior completely dominates the posterior over the entropy. Instead, NSB uses a mixture-of-Dirichlet prior with a specific hyperprior $p_\text{NSB}(\alpha)$ on the concentration parameter that induces a relatively flat prior on the entropy. On the contrary, $H(p,q) = -\mathbb{E}_p[\log q]$ is linear in $p$ so we could use the plug-in or ML estimator for $\hat{p}$ (i.e., $p_\text{ML}(\alpha) \equiv \delta(\alpha)$), which is unbiased. Incidentally, this choice simplifies the computations a lot since it turns out that $H(p,q)$ is simply the (minus) log likelihood of the model (plus stuff). Now, if I were to separately estimate of the two terms, I would use two distinct priors for $p$, $p_\text{NSB}(\alpha)$ for the entropy and $p_\text{ML}(\alpha)$ for the cross-entropy. My problem is, how to deal with it when they are part of the same expression? Is it okay to be inconsistent in the choice of prior? Which prior should I use otherwise between the two? (I would lean towards NSB) Or, is there a third estimator with yet another prior over $\alpha$ built specifically for this case?
