[site]: crossvalidated
[post_id]: 254612
[parent_id]: 
[tags]: 
How to obtain optimal hyperparameters after nested cross validation?

In general, if we have a large dataset, we can split it into (1) training, (2) validation, and (3) test. We use validation to identify the best hyperparameters in cross validation (e.g., C in SVM) and then we train the model using the best hyperparameters with the training set and apply the trained model to the test to get the performance. If we have a small dataset, we cannot create training and test set (not enough samples). Therefore, we will do cross validation (k-fold, leave-one-out, etc) to evaluate the model performance. I have seen nested cross validation (whether repeated or stratified) has been used in the setting of small dataset, i.e., to generate generalized model performance while optimizing parameters selection. My question is, how can I obtain the best hyperparameters in nested cross validation (repeated/no repeated)? I am interested in doing this in scikit-learn, if possible. I am a bit confused about how to do it. I have read several resources but none gave me the definitive answer for this question: Nested cross validation for model selection Nested cross-validation and feature selection: when to perform the feature selection?
