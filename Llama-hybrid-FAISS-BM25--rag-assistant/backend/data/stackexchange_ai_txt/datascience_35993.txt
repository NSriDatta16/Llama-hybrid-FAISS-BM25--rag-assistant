[site]: datascience
[post_id]: 35993
[parent_id]: 
[tags]: 
Evaluation code understanding in wide and deep model of tensorflow

I'm doing some research on the wide and deep model developed by Google. And I got 2 questions on the training and evaluation code fragment. ( Complete code see here): train_hooks = hooks_helper.get_train_hooks( flags_obj.hooks, model_dir=flags_obj.model_dir, batch_size=flags_obj.batch_size, tensors_to_log=tensors_to_log) # Train and evaluate the model every `flags.epochs_between_evals` epochs. for n in range(flags_obj.train_epochs // flags_obj.epochs_between_evals): model.train(input_fn=train_input_fn, hooks=train_hooks) results = model.evaluate(input_fn=eval_input_fn) # Display evaluation metrics tf.logging.info('Results at epoch %d / %d', (n + 1) * flags_obj.epochs_between_evals, flags_obj.train_epochs) tf.logging.info('-' * 60) for key in sorted(results): tf.logging.info('%s: %s' % (key, results[key])) benchmark_logger.log_evaluation_result(results) if early_stop and model_helpers.past_stop_threshold( flags_obj.stop_threshold, results['accuracy']): break So my questions: default setting for epochs_between_evals is 2, but how is it possible for evaluating 1 epoch after training for every 2 epochs? Apparently there is one model.evaluate() after every model.train() , isn't it? Perhaps the train_hooks controls that in model.train() to break every 2 epochs, but the parameters of train_hooks don't include num_epochs or epochs_between_evals . I'm a bit confused. What does model.evaluate() actually do? As I check the source code, it seems it's just a forwarding pass to get evaluation outputs. Any other work done behind the scene?
