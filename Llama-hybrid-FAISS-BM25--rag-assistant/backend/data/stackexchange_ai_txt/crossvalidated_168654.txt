[site]: crossvalidated
[post_id]: 168654
[parent_id]: 168649
[tags]: 
I think you're combining two concepts. Whitening. Even though your variables are decorrelated, they may not be whitened -- where each variable must have a variance of 1. This is slightly important (as you noticed) for convergence and conditioning. But it's very important if you're regularizing your weights, e.g. with L1 regularization to encourage sparsity. For two dimensions with very different variance to have similar influence, their weights will also be very different -- but the larger will be more penalized (and biased) due to the regularization. It's also very useful for interpreting weights, as they are already variance-adjusted; larger weights indicate more informative input dimensions. De-meaning. This is necessary if you aren't optimizing each neuron's output threshold / input bias. Even if you are, demeaning starts the optimization with more neutral initial conditions. If you look at the sigmoid, an input with mean 100 and variance 1 would (virtually) never be able to produce an output below 0.5, no matter what the weight (assuming no bias or threshold).
