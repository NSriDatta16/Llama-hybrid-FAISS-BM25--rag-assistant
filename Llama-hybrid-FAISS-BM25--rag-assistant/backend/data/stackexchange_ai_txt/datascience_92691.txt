[site]: datascience
[post_id]: 92691
[parent_id]: 61533
[tags]: 
I will answer your questions one by one: By hidden layer we mean the layer that is inbetween the input and output. If number of layers = 1 with 10 hidden neurons (as shown in second figure) then is it essentially a neural network which is termed as an MLP. Is my understanding correct? The fundamental building block of a Neural Network is the perceptron . It's modeled on biological neurons: a unit that receives multiple inputs, weights them, and outputs a signal transformed. In its simplest form (i.e. a perceptron with sigmoid activation) it's practically identical to a logistic regression. When you compose many perceptrons on more than one layer you will form an MLP (Multi Layer Perceptron). The term "MLP" is used as a synonim of Neural Networks - in their basic feed forward architectures. Neural Networks with no hidden layer cannot even be considered as Neural Networks. The ones with just one hidden layer are called shallow Neural Networks, no one really uses them, since they are not as powerful as Deep Neural Networks. When you have more than one hidden layer, then you talk about Deep Neural Networks . They are the real big thing right now, the most powerful ML models available right now. Do we call the machine learning model as linear or nonlinear ? Or is this term associated to the mapping function ? ML models can be either linear or non-linear, the choice is yours. Different models will map input and output in different ways (linear or other non-linear ways). I think it's important to stress that Machine Learning is not a set of models, but an approach to data . You could use a linear regression (i.e. a linear model) with a ML approach, or an SVM or a Deep Neural Network (i.e. non-linear models) to solve the exact same problem. Which layer's mapping function determines this? If you refer to Deep Neural Networks, I believe all layers altogether are responsible of the non-linear mapping between inputs and outputs. Non-linearities can be learned only thanks ot a combination of depth and non-linear activation functions. The deeper a Network is, the more complex will be the non-linear patterns that it will able to learn. It's like if you need "all of them at once" to do that, in my opinion.
