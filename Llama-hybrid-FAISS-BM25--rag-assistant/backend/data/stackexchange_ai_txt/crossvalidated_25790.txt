[site]: crossvalidated
[post_id]: 25790
[parent_id]: 25786
[tags]: 
In general, feed-forward networks utilizing backpropagation are great for classification tasks in which you have a number of probabilistic cues which need to be integrated. They're obviously used for a great many other things, but this is an example of one driving many people to use them -- it's difficult to capture this sort of learning in a more Hebbian fashion. In any case where you have multiple probabilistic cues which may be of limited, but above-chance use in classification, they can be integrated in a cognitively plausible manner (as in natural language acquisition by children) -- the integration of multiple cues, any one of which is a weak cue for classification in and of itself, can result in very robust classification performance. For this sort of classification w/ multiple cues, AdaBoost would be an appropriate (and quite well-established) algorithm to use as a baseline. An off-the-shelf support vector machine might rival AdaBoost's performance. If you're looking for baseline classifiers that people are already familiar with, you should definitely use an SVM for at least one of them. There are also approaches that combine AdaBoost with SVMs.
