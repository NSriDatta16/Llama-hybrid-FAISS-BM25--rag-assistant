[site]: crossvalidated
[post_id]: 190026
[parent_id]: 
[tags]: 
Does Random Forest ever compare the splitting of one node to the slitting of a **different** node?

I thought I understood how a single decision tree is constructed as part of a Random Forest : The data is split recursively until some kind of stopping conditions are met. Each split is determined by computing by optimizing an objective function. The parameters to that objective function are the dimension (variable/feature) along which to split, and where along that dimension to split. My question : Are nodes ever compared to each other when determining where to make the next split? For example, let's say there are two nodes at the same level in the tree, should we find the best dimension and the best split location for each of those, compute the error metric of each of those, and then compare those errors to determine which node, dimension, and split location to use? The reason I ask is this article makes an argument for what it calls "confidence splitting" based on the premise that the error for a node with a few data points in its region, can be the same as the error for a node with many data points in its region. This is because gini-impurity and entoropy are scale-invariant (explained in the section "Traditional Node Splitting Criterions"). But, if the error of one node is never compared with the error of another node, it makes the articles point obsolete, correct?
