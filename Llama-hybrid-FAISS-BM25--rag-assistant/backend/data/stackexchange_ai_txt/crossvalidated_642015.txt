[site]: crossvalidated
[post_id]: 642015
[parent_id]: 641986
[tags]: 
The underlying argument for $\frac16$ is essentially based on symmetry of an ideal cube, from which you could argue that you can exploit the various symmetries of faces, edges and vertices to be able to interchange the face-labels without changing the relative outcome probabilities. (You would also need assumptions about uniformity in the physical composition of the die.) It's a logical argument that is *not actually true of any physical die, since physical dice are subject to imperfect manufacturing processes; they are not exactly cubic (though you can get away with less symmetry than that of a cube, they don't attain that less-demanding symmetry either). The long run frequency will typically show up as some level of unequal probability; the best you could perhaps hope for is you could not distinguish a die from a fair die within the lifetime of the die (many thousands of rolls, perhaps). Casino dice are manufactured very carefully and are typically so close to fair that you might not be able to readily distinguish their probabilities from uniform in even tens of thousands of rolls (by which time the dice would start to be altered by the repeated throwing), but dice used in some of my boardgames and tabletop roleplaying games are not. I have two wooden dice in my Settlers of Catan game that manage to roll an exceptionally high number of 7's (to the extent that it makes the game pretty much unplayable with those dice, since rolling a 7 has a very particular consequence). I also have a pink and black 20-sided die that turns right angles when you roll it; its bias toward 4 (among other outcomes) is strong enough that you can pick it up quickly, for all that the die is visually very close to symmetric. Presumably there's inconsistent density of material inside the die. More generally, any model for a process (Bernoulli, Poisson, etc, not just ones based on symmetry arguments) are just that $-$ models . That is, they're approximations of some real process, not exact descriptions. They abstract away details that you hope are not substantial in their impact, but in practice enough data will reveal imperfections. Take, for example, what is typically treated as one of the most ideal examples we know $-$ treating the number of clicks in a classical Geiger counter for detecting ionizing radiation when pointed at (say) a rock as a homogeneous Poisson process. It isn't quite an accurate model. For one thing, the absolute and relative amounts of different radioactive elements changes over time, and the counter itself doesn't produce two clicks for particles that are very close together in time. So we don't have an actual homogeneous Poisson process for the observed clicks, albeit its typically an excellent approximation. Since essentially none of these models are actually true , we cannot prove them in the sense of applying to the physical process we observe, even though they do follow from their axioms (/assumptions) and enough data will reveal the failure of assumptions (i.e. a long-enough but very much finite run, rather than the technical sense of " in the long run "). This is not a problem as long as the answers we obtain are close enough for our specific purposes. For a physical die, it won't actually have a probability to roll a $6$ of $\frac16$ ; but that's not especially important, as long as it's close enough that the model will yield answers that are close enough for our specific purposes. As George Box elegantly put it " Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful ".
