[site]: datascience
[post_id]: 47963
[parent_id]: 47961
[tags]: 
On time-series models All models that you have mentioned are correct and practical depending on the problem (the index $n$ is not required). The second one however produces redundant results which is a waste of computation. Even $$ (X_{t} ; X_{t+1} ; X_{t+2}) \rightarrow (Y_{t-1}) $$ is correct, if you are fitting on an archive and want to predict a year given the covariates from the next three years. But only the last model $$ (X_{t-2} ; X_{t-1} ; X_{t}) \rightarrow (Y_{t+1}) $$ is a forecasting model. So in general, if you want to interpolate into the next $k$ -th year from now $t$ , you should use: $$ (X_{t-2} ; X_{t-1} ; X_{t}) \rightarrow (Y_{t+k}) $$ or $$ (X_{t-2} ; X_{t-1} ; X_{t}) \rightarrow (Y_{t+1},...,Y_{t+k}) $$ Even a better model that takes advantage of known $Y$ 's in the past would be: $$ (X_{t-2}|Y_{t-2} ; X_{t-1}|Y_{t-1} ; X_{t}|Y_{t}) \rightarrow (Y_{t+k}) $$ where $|$ denotes vector concatenation to produce a 100 + 1 dimensional vector for each known year. As a personal opinion, for the time-series prediction task, 24 data points per year is very small compared to the dimension of $X$ , which is 100. 1200 samples for $X \rightarrow Y$ regression (ignoring the time) is more practical; if selecting 10 from 100 covariates is possible even better. Because of the small data set, I would suggest: $(Y_{t-m} ;...; Y_{t-1} ; Y_{t}) \rightarrow (Y_{t+k})$ for time series prediction, and $X \rightarrow Y$ regression for estimating the relation between X and Y. Relation to LSTM and RNN If we use LSTM/RNN to model time-series, they would be stateful. That is, when input $X_{t-2}$ is fed to an LSTM, it keeps an internal state (hidden state) to be combined with the next input $X_{t-1}$ and so on. Regarding the input/output dimension, here is an RNN animation from a post on medium by Raimi Karim that shows an arbitrary step among 3 steps of feeding $(X_{t-2} ; X_{t-1} ; X_{t})$ to the network: As you see, dimension and number of inputs are independent of output. We can feed 5 inputs $X_{t-4}$ to $X_{t}$ , each 100 dimension (100d) and receive a 1d output by setting the dimension of hidden states to 1d, or setting it to 10d and use an extra dense layer at the end to convert 10d to 1d, or receive a 50d output, or a 150d (three 50d) output, etc. Word "stateful" in Keras ( source ) LSTM and RNN are stateful by definition, this [badly named] variable in Keras means If stateful=True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Fabien Chollet For example, if each batch has 24 samples indexed from 0 to 23 (each sample could have the form $(X_{t-2}, X_{t-1}, X_{t}, Y_{t+1})$ ), then the last hidden state $h$ from 8th sample will be used as the initial hidden state for 8th sample in the next batch. Except for special cases that there is a temporal order between batches and their samples, this must be set to False.
