[site]: crossvalidated
[post_id]: 642734
[parent_id]: 
[tags]: 
Commodity Price Modeling with Time Series vs. Return

TLDR: Assuming that the current commodity prices are influenced by a variety of factors, including the prices from the previous period. Given this context, should I explore the relationship between current (as dependent variable) and lagged returns (as independent variable) to understand the impact of past price movements on current prices? Alternatively, would it be more appropriate to construct an ARIMA model to conduct the research? I am confused about the differences between these two methods, as both seem to explain the price movement. Background: I am trying to build a model explaining variables affecting commodity prices. Past literature has suggested that the previous price of such a commodity affects the current price. This leaves two choices: 1. Use prices directly and build an ARIMA model 2. Transform the price to lognormal form, take the first difference (i.e., build linear regression using return instead of prices), and include one lag period of return - this shows how the previous period's return affects the current period's return. Result from the first option: After identifying PACF & ACF graphs, ARIMA (1,1,1) is constructed, however, the AR(1) and MA(1) coefficients are not statistically significant. Result from the second option: The difference of lognormal form with lag 1, i.e., the previous period's return, is statistically significant. I'm not sure how to interpret these contrasting results and decide on the best method for my analysis. Any guidance or insights would be greatly appreciated.
