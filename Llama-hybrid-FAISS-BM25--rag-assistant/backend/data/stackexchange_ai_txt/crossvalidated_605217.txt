[site]: crossvalidated
[post_id]: 605217
[parent_id]: 605213
[tags]: 
The Gauss-Markov theorem concludes, under those assumptions, that the MLE for the regression coefficients are determined by least-squares. Therefore, in the model, (say $i\leq 1\leq n$ ), $$ y_i = \sum_{j=1}^m a_{i,j}x_j + b_i + \varepsilon_i $$ where $(\varepsilon_i)_{1\leq i\leq n}$ are IID normal, we can explicitly compute the MLE and find that the coefficients are determined by solving a least-squares linear system. Therefore, we can use the MLE to estimate the uncertainity (hence p-values) from the estimates of the coefficients. However, let us instead suppose that $(\varepsilon_i)_{1\leq i\leq n}$ are IID from double-sided $\text{Exp}(\lambda)$ distribution (so imagine the exponential distribution, but it goes also into the negative direction, there is probably a name to this distribution). Then we have that, $$ \left|y_i - \sum_{j=1}^m a_{i,j}x_j - b_i \right| \sim \text{Exp}(\lambda)$$ In this case the MLE will involve minimizing the sum of absolute-residuals instead of squared-residuals (because the PDF for the Normal is of the form $\exp(-x^2)$ (ignoring the constants), and the PDF for the Exponential is of the form $\exp(-x)$ (ignoring the constants)). Therefore, you have replaced your original problem, which was a least squared problem, into a linear programming problem which has no closed-form explicit formula to it. With least squares there is an explicit closed-form solution you can use to estimate parameters. There is an however algorithm for finding it but no explicit immediate formula. Therefore, in a problem like this, you would need to estimate the variation of the estimates using the simplex algorithm. (Alternatively, you can also run Bayesian regression for something like this.)
