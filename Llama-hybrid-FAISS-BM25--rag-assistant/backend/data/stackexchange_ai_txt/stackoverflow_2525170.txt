[site]: stackoverflow
[post_id]: 2525170
[parent_id]: 2524608
[tags]: 
Rather than keeping a full history, one can keep aggregated information about the past (along with a relatively short sliding history, to be used as input to the Predictor logic). A tentative implementation could go like this: In a nutshell: Managing a set of Markov chains of increasing order , and grading and averaging their predictions keep a table of individual event counts, the purpose is to calculate the probability of any of the 4 different events, without regards to any sequence. keep a table of bigram counts , i.e. a cumulative count the events observed [so far] Table starts empty, upon the second event observe, we can store the first bigram, with a count of 1. upond the third event, the bigram made of the 2nd and 3rd events is "added" to the table: either incrementing the count of an existing bigram or added with original count 1, as a new (never-seen-so-far) bigram. etc. In parallel, keep a total count of bigrams in the table. This table and the total tally allow calculating the probability of a given event, based on the one preceding event. In a similar fashion keep a table of trigram counts, and a running tally of total trigram seen (note that this would be equal to the number of bigrams, minus one, since the first trigram is added one event after the first bigram, and after that one of each is added with each new event). This trigram table allows calculating the probability of a given event based on the two preceding events. likewise, keep tables for N-Grams, up to, say, 10-grams (the algorithm will tell if we need to increase or decrease this). keep an sliding windows into the last 10 events. The above tables provide the basis for prediction; the general idea are to: use a formula which expresses the probabilities of the next event as a weighed average of the individual probabilities based on the different N-grams. reward the better individual N-gram length by increasing the corresponding weight in the formula; punish the worse lengths in the reverse fashion. (Beware the marginal probability of individual events needs to be taken into account lest we favor N-grams which happen to predict the most frequent events, regardless of the relative poor predicting value associated with them them) Once the system has "seen" enough events, see the current values for the weights associated with the long N-Grams, and if these are relatively high, consider adding tables to keep aggregate info about bigger N-Grams. (This unfortunately hurts the algorightm both in terms of space and time) There can be several variations on the general logic described above . In particular in the choice of the particular metric used to "grade" the quality of prediction of the individual N-Gram lengths. Other considerations should be put with regards to detecting and adapting to possible shifts in the events distribution (the above assumes a generally ergodic event source). One possible approach is to use two sets of tables (combining the probabilities accordingly), and periodically dropping the contents of all tables of one of the sets. Choosing the right period for these resets is a tricky business, essentially balancing the need for statistically significant volumes of history and the need for short enough period lest me miss on the shorter modulations...
