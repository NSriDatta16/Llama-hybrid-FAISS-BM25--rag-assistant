[site]: datascience
[post_id]: 77847
[parent_id]: 
[tags]: 
Do batch GD and stochastic GD give the same results?

If a neural network is trained on a dataset of M samples for N epochs, do batch GD and SGD give the same result? Is SGD is faster because utilize the hardware better? I am asking because I figured out that both (batch GD & SGD) give mathematically the same result, but I read SGD avoid local minima, how can this can be true if SGD & batch GD give the same result!?
