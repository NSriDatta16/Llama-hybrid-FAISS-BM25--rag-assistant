[site]: crossvalidated
[post_id]: 598575
[parent_id]: 598550
[tags]: 
This should be doable (in theory). The universal approximation theorems say that any decent function, such as a linear transformation, can be approximated as well as is desired, given a large enough neural network. If you give the linear regression features ( $X$ ) and outcomes ( $y$ ) as features to a neural network and the $\hat\beta$ as the outcome for the neural network, some network should be able to figure out that $\hat\beta=(X^TX)^{-1}X^Ty$ . (Itâ€™s also worth noting that a linear regression can be seen as a neural network with no hidden layer and the identity as an activation function.) Why anyone would want to do this is not clear to me, but math says that it can be done.
