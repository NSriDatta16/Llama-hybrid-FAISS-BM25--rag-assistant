[site]: crossvalidated
[post_id]: 282577
[parent_id]: 282567
[tags]: 
You have a count data problem, so I would start out with Poisson regression. That could change, as a result of preliminary analysis, with some other count-data model, like maybe negative binomial regression. The risk exposure of the sites will vary, first as a function of the size of site, measured by number of employees. So, number of employees could be used as an offset (with a log link function, use offset(log(NrEmployees)) in R). For more about use of offset, see Goodness of fit and which model to choose linear regression or Poisson . Then you use your other predictor variables in the linear predictor, categorical variables coded with dummys. For the continuous predictor variables, you could check for nonlinearities by including them with splines. Maybe interactions? You say there is high (0.7) correlation between number of injuries and number of injuries previous year. I would not include this in a first model, maybe much of this marginal correlation is taken care of by covariables. But after the first model is fitted, you could look if there are autocorrelations in the residuals, by computing from the residuals the average autocorrelation function. If there is autocorrelation in the residuals, maybe include number of injuries previous year as a lag variable. Then you have to look at the level of resudual variability, maybe there is overdispersion? Maybe, if that is the case, a negative binomial regression could be better. And, look at the plot of residuals for individual sites. The mean of residuals for individual sites should be approximately zero, if not, there is some indication of a site specific effect, which could be modelled via a random effects model. As a first check, I would look at the histogram of mean of residuals for each site. You say you have 100000 sites, so maybe before starting exploring the data you should set aside an evaluation sample, which you do not use for model fitting?
