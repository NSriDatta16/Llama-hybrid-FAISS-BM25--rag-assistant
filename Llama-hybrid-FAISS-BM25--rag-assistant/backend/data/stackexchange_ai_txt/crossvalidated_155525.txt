[site]: crossvalidated
[post_id]: 155525
[parent_id]: 
[tags]: 
Volatility clustering test in Stata, time frame issues

I tested AAPL for ARCH effects using Stata with two different datasets (one was a subset of the other) and I obtained significant arch effects in one subset but not in the whole dataset. I would have expected that if one of the series exhibited volatility clustering so would the other, they are both return time series of the same underlying after all. This made me question the procedure I was using to determine arch effects. I will now write down the procedure and some parts where I have doubts. The whole dataset is AAPL returns from 1999 up to 2015: First I try to determine an ARMA model that fits the data. In this case an ARIMA(1,0,1) (I am already working with returns) seems to have all lags with significant coefficients. Then I predict the residuals of the ARMA model. Regress them against themselves (fit a constant to them). Use Stata's test estat archlm, lags(1/15) and observe the p-value to determine if ARCH effects are present. Doing those 4 steps I accept the null hypothesis of no ARCH effects at a 5% significant level, in the whole data set. But now if I only take data from 2006 to 2015: In the first step I obtain no significant coefficients in an ARMA model: So I simply fit an ARIMA(0,0,0) (regress around a constant). And following the other three steps I do not accept the Null hypothesis of No ARCH effects even at a 0.1% significant level. This seems quite odd to me, I think the mistake must be in the fitting of the ARMA model. Could I know your algorithms for checking if there are ARCH effects in the data? Is there something wrong in the one I am using? EDIT: This is the complete order of commands I am currently doing, the variable A are the returns of whole dataset (the first picture). I still can't refuse the null hypothesis but just looking at the two datasets it seems to me they both have volatility clustering.
