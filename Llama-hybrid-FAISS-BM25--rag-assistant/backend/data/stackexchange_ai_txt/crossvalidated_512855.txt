[site]: crossvalidated
[post_id]: 512855
[parent_id]: 512016
[tags]: 
Although maximizing likelihood is equivalent to minimizing KL divergence from the data distribution to the model , this doesn't mean that every application of KL divergence is maximum likelihood, because often, the two things you're measuring divergence between are not data and model. In particular, VAEs are trained by maximizing something which is a lower bound on the likelihood, so in a sense they are really just trained by MLE. It happens that the lower bound has as one of its terms the KL divergence between the variational distribution $q(z|X)$ and the latent prior $p(z)$ . But since these aren't data and model, it doesn't make sense to think of the KL term as MLE.
