[site]: crossvalidated
[post_id]: 559828
[parent_id]: 
[tags]: 
Bayesian analysis example with convergence under Gibbs but not Metropolis-Hastings

Having a conceptual understanding of algorithms such as Metropolis-Hastings, Gibbs and Hamiltonian Monte Carlo can provide ideas of remediation to apply when models do not converge. This question focuses on Metropolis-Hastings and Gibbs, I had previously thought that sampler affected efficiency more than convergence. Gibbs does not seem to have superseded Metropolis-Hastings so they must each have relative benefits. https://www.stata.com/features/overview/gelman-rubin-convergence-diagnostic/ gives an example of a linear regression with non-convergence under Metropolis-Hastings but an excellent convergence under Gibbs sampling. “The default adaptive Metropolis–Hastings used by bayes: and bayesmh may suffer from low efficiency in the presence of many highly correlated model parameters [which is the case here]. In our example, the default priors used by bayes: regress—normal for coefficients and inverse gamma for the variance—allow us to use a more efficient Gibbs sampling.”. How do these default priors allow Gibbs? I understand the basic idea of Metropolis-Hastings and Gibbs, but currently not enough to understand how they could alter convergence of the example linear regression with highly correlated independent variables. What is the reason for this difference in convergence between the two sampling methods?
