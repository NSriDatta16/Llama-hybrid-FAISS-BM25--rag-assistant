[site]: crossvalidated
[post_id]: 86951
[parent_id]: 86931
[tags]: 
I'm pretty confident there isn't going to be one already in the literature, but if you seek a nonparametric test, it would have to be under the assumption of continuity of the underlying variable --- you could look at something like an ECDF-type statistic - say some equivalent to a Kolmogorov-Smirnov-type statistic or something akin to an Anderson-Darling statistic (though of course the distribution of the statistic will be very different in this case). The distribution for small samples will depend on the precise definitions of the quantiles used in the five number summary. Consider, for example, the default quartiles and extreme values in R (n=10): > summary(x)[-4] Min. 1st Qu. Median 3rd Qu. Max. -2.33500 -0.26450 0.07787 0.33740 0.94770 compared to those generated by its command for the five number summary: > fivenum(x) [1] -2.33458172 -0.34739104 0.07786866 0.38008143 0.94774213 Note that the upper and lower quartiles differ from the corresponding hinges in the fivenum command. By contrast, at n=9 the two results are identical (when they all occur at observations) (R comes with nine different definitions for quantiles.) The case for all three quartiles occurring at observations (when $n=4k+1$ , I believe, possibly under more cases under some definitions of them) might actually be doable algebraically and should be nonparametric, but the general case (across many definitions) may not be so doable, and may not be nonparametric (consider the case where you're averaging observations to produce quantiles in at least one of the samples ... in that case the probabilities of different arrangements of sample quantiles may no longer be unaffected by the distribution of the data). Once a fixed definition is chosen, simulation would seem to be the way to proceed. Because it will be nonparametric at a subset of possible values of $n$ , the fact that it's no longer distribution free for other values may not be such a big concern; one might say nearly distribution free at intermediate sample sizes, at least if $n$ 's are not too small. Let's look at some cases that should be distribution free, and consider some small sample sizes. Say a KS-type statistic applied directly to the five number summary itself, for sample sizes where the five number summary values will be individual order statistics. Note that this doesn't really 'emulate' the K-S test exactly, since the jumps in the tail are too large compared to the KS, for example. On the other hand, it's not easy to assert that the jumps at the summary values should be for all the values between them. Different sets of weights/jumps will have different type-I error characteristics and different power characteristics and I am not sure what is best to choose (choosing slightly different from equal values could help get a finer set of significance levels, though). My purpose, then is simply to show that the general approach may be feasible, not to recommend any specific procedure. An arbitrary set of weights to each value in the summary will still give a nonparametric test, as long as they're not taken with reference to the data. Anyway, here goes: Finding the null distribution/critical values via simulation At n=5 and 5 in the two samples, we needn't do anything special - that's a straight KS test. At n=9 and 9, we can do uniform simulation: ks9.9 # Here's the empirical cdf: cumsum(table(ks9.9)/10000) 0.2 0.4 0.6 0.8 0.3730 0.9092 0.9966 1.0000 so at $n_1 = n_2=9$ , you can get roughly $\alpha=0.1$ ( $D_{crit}=0.6$ ), and roughly $\alpha=0.005$ ( $D_{crit}=0.8$ ). (We shouldn't expect nice alpha steps. When the $n$ 's are moderately large we should expect not to have anything but very big or very tiny choices for $\alpha$ ). $n_1 = 9, n_2=13$ has a nice near-5% significance level ( $D=0.6$ ) $n_1 = n_2=13$ has a nice near-2.5% significance level ( $D=0.6$ ) At sample sizes near these, this approach should be feasible, but if both $n$ s are much above 21 ( $\alpha \approx 0.2$ and $\alpha\approx 0.001$ ), this won't work well at all. -- A very fast 'by inspection' test We see a rejection rule of $D\geq 0.6$ coming up often in the cases we looked at. What sample arrangements lead to that? I think the following two cases: (i) When the whole of one sample is on one side of the other group's median. (ii) When the boxes (the range covered by the quartiles) don't overlap. So there's a nice super-simple nonparametric rejection rule for you -- but it usually won't be at a 'nice' significance level unless the sample sizes aren't too far from 9-13. Getting a finer set of possible $\alpha$ levels Anyway, producing tables for similar cases should be relatively straightforward. At medium to large $n$ , this test will only have very small possible $\alpha$ levels (or very large) and won't be of practical use except for cases where the difference is obvious). Interestingly, one approach to increasing the achievable $\alpha$ levels would be to set the jumps in the 'fivenum' cdf according to a Golomb-ruler . If the cdf values were $0,\frac{1}{11},\frac{4}{11},\frac{9}{11}$ and $1$ , for example, then the difference between any pair of cdf-values would be different from any other pair. It might be worth seeing if that has much effect on power (my guess: probably not a lot). Compared to these K-S like tests, I'd expect something more like an Anderson-Darling to be more powerful, but the question is how to weight for this five-number summary case. I imagine that can be tackled, but I'm not sure the extent to which it's worth it. Power Let's see how it goes on picking up a difference at $n_1=9,n_2=13$ . This is a power curve for normal data, and the effect, del, is in number of standard deviations the second sample is shifted up: This seems like quite a plausible power curve. So it seems to work okay at least at these small sample sizes. What about robust, rather than nonparametric? If nonparametric tests aren't so crucial, but robust-tests are instead okay, we could instead look at some more direct comparison of the three quartile values in the summary, such as an interval for the median based off the IQR and the sample size (based off some nominal distribution around which robustness is desired, such as the normal -- this is the reasoning behind notched box plots, for example). This should tend to work much better at large sample sizes than the nonparametric test which will suffer from lack of appropriate significance levels.
