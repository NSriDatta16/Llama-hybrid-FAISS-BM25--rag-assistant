[site]: crossvalidated
[post_id]: 463650
[parent_id]: 463604
[tags]: 
Maybe another philosophical point to add is the following: In mathematics we have a very clear and precise understanding what a random variable is in these days. I always imagine it as follows: The basic set people often talk about is $\Omega$ . Every $\omega \in \Omega$ is one 'configuration' containing all possible information of a universe we might live in. A random variable $X$ is a map (with certain properties) from $\Omega$ to the space of possible outcomes... let's say we talk about a dice then $X : \Omega \to \{1,2,3,4,5,6\}$ . I always imagine that as an instance of the Laplace daemon: https://en.wikipedia.org/wiki/Laplace%27s_demon : Given all information about the current temperature in the room, the initial angle and position of the dice, the mood of the person that rolled the dice, ... (i.e. really ALL information available encoded in $\omega$ ) we can perfectly predict the landing position of the dice because it is merely physics and $X$ is that deterministic Laplace daemon. Of course, philosophically one could say: Is that really how the universe works? Doesn't quantum mechanics and the Planck universe tell us that we cannot deterministically understand the universe with all the mathematics and physics that we have nowadays (according to what I understood, the Planck universe tells us that there is a minimal unit of space and if we go below that size then essentially we cannot do physics anymore)? Yes, it does. Therefore the question whether or not that is a good definition of random variable is still 'open' I would say. If it is still unclear whether that is a good definition or not, why are we using it so often for the theory behind the models we produce? The only reason here is: It seems to work well in practice. It's like in physics: Shouldn't we throw away all physics that we know if we can see that we can never fully understand the universe with these methods? Well, we could certainly do so but then we would not have fancy features like GPS ... so we rather keep the imperfect version and make the best out of it. How is that all related to reality? Let say that we have collected some dataset $(x_i, y_i)_{i=1,...,n}$ . Why do we make assumptions that these come from random variables in the first place? Sometimes we want to prove that a certain model is best suited for this dataset and in order to do that we need to have some ground rules set. However, we can never really verify even whether the $x_i, y_i$ come from random variables $X_i, Y_i$ in the mathematical sense above, let alone whether they satisfy some (e.g. linear) relationship!!! So in reality, even if we have true randomness, we can never really be sure about it. Is that a shitty way of doing data science? Well yes and no: Again, it is a way that is not perfect but we use it nevertheless because we can earn a lot of money with it (and make things better for the world and come up with nice things like google internet search, etc).
