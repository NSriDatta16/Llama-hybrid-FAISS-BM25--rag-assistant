[site]: crossvalidated
[post_id]: 163964
[parent_id]: 163920
[tags]: 
So I grabbed my copy and checked what exactly he means. I think this paragraph is supposed to only refer to the specific example he is discussing in that chapter (which is, for reference, chapter 6; the example deals with determining the $n$ parameter in a binomial($\alpha$,n) distribution with known p), and I think you might be reading too much into that quote Let's take a slightly simpler example: the good old weighted coin. We are trying to determine the parameter $\alpha$ of binomial($\alpha$,n) given data in which $m$ observations were positive. The likelihood is then: $$ l_n(\alpha) \propto \alpha^m (1-\alpha)^{n-m} $$ In this example, we can see indeed that one additional observation (raising $n$ by 1) has a degree one contribution to the likelihood function, either multiplying it by $\alpha$ or by $(1-\alpha)$. This justifies that using a prior distribution of the form $$ p(\alpha) \propto \alpha^{m_p} (1-\alpha)^{n_p-m_p} $$ corresponds to assuming that you have already viewed $m_p$ successes in a $n_p$ length sequence before. However, I do not believe that this point is generally true: likelihoods very rarely contribute just a linear contribution. Here is a dumb example: imagine that you are doing the weighted coin experiment, but for some reason one observation actually corresponds to 10 new throws instead of only 1. You can still quantify the mean contribution of an observation in terms of a Gausian with certain parameters that depend on derivatives of the log-likelihood. Look at fisher information and bernstein-von Mises theorem if you want to learn more
