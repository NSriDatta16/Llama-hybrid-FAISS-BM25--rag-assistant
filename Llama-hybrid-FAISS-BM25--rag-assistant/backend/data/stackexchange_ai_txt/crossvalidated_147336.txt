[site]: crossvalidated
[post_id]: 147336
[parent_id]: 147218
[tags]: 
Convergence isn't guaranteed by this analysis when the objective isn't strongly convex. The required number of iterations grows as $\alpha$ gets smaller. You may not be able to wait around long enough for convergence if $\alpha$ is really small. The accumulation of round-off errors might also cause problems in practice. There are some results on the convergence of stochastic subgradient descent algorithms in Bertsekas's new book on Convex Optimizaton Algorithms. With these results you can get convergence with probability one to a solution with an objective function value within $\epsilon$ of the optimal value. However, you may have to use a tiny step size that makes this impractical.
