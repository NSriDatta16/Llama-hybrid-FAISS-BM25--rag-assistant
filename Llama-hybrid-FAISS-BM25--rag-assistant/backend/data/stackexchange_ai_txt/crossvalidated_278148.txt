[site]: crossvalidated
[post_id]: 278148
[parent_id]: 278147
[tags]: 
I am not familiar with this method, but based off what you describe, but my understanding is that this is a method to assess the quality of a binary model, i.e. a model that is used to predict a YES/NO or 0/1 type outcome. If you would simply toss a coin to predict a binary outcome with an underlying B(n, 0.5) distribution, you would expect to predict right 50% of the time. This means you would on average predict 50% of the Ones right and 50% of the Zeros right. If you were to then use the Kenndy method, you would get that the sum of the percentages of cases correctly predicted would add up to 1 - this is why he says that the statistic should exceed 1 - basically it should be better than a coin toss. I think you could implement it in R with the following user-defined function: kennedy2 This should work if your predicted values and the observed values have the same index. To see whether this is what you were looking for, consider the following example. The model in this example is a very simple Linear Probability Model, I am sure there is better code for this. The object is to test whether using the GPA and the GRE of a student, we can correctly predict his admission (YES/NO outcome): reslm .5){ pred[i] .4){ pred2[i] .3){ pred3[i] If you change the cutoff value for the prediction a bit (.5, .4, .3), you can see that the Kennedy statistic improves, i.e. the model is correct more frequently.
