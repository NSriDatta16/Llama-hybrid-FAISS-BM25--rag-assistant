[site]: crossvalidated
[post_id]: 525132
[parent_id]: 
[tags]: 
Why does my model perform best when I first use PCA with n_components = the number of columns?

I'm training on a dataset with lots of multi-collinearity and after one-hot encoding the categorical variables, there are 90 columns not including the target variable. It seems like this would benefit from dimensionality reduction so I've performed a grid search with 3, 5, 7, 9 and 90 principal components. To my surprise, 90 principal components results in the best performance by far. It even performs better than when I don't use PCA at all. Why is this? Is there actually any dimensionality reduction happening with 90 components? If not, then why does this give better results than when PCA is excluded entirely? I'm passing the output of PCA to a gradient boosted classifier. By "better performance" Im referring to a combination of accuracy, recall and AUC.
