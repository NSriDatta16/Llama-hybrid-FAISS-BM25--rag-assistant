[site]: crossvalidated
[post_id]: 498001
[parent_id]: 487593
[tags]: 
As others have pointed out, the reason $β_{λ=0}$ (OLS) appears to have lower MSE than $β_{λ>0}$ (ridge) in your example is that you computed both values of $β$ from a matrix of four (more generally, $N$ ) observations of two (more generally, $P$ ) predictors $X$ and corresponding four response values $Y$ and then computed the loss on these same four observations. Forgetting OLS versus ridge for a moment, let's compute $β$ manually; specifically, we seek $β$ such that it minimizes the MSE of the in-sample data (the four observations). Given that $\hat{Y}=Xβ$ , we need to express in-sample MSE in terms of $β$ . $MSE_{in-sample}=\frac{1}{N}\|Y-Xβ\|^2$ $MSE_{in-sample}=\frac{1}{N}[(Y-Xβ)^T(Y-Xβ)]$ $MSE_{in-sample}=\frac{1}{N}[Y^TY-2β^TX^TY+β^TX^TXβ]$ To find the value of $β$ minimizing this expression, we differentiate the expression with respect to $β$ , set it equal to zero, and solve for $β$ . I will omit the $\frac{1}{N}$ at this point since it's just a scalar and has no impact on the solution. $\frac{d}{dβ}[Y^TY-2β^TX^TY+β^TX^TXβ]=0$ $-2X^TY+2X^TXβ=0$ $X^TXβ=X^TY$ $β=(X^TX)^{-1}X^TY$ Which is a familiar result. By construction, this is the value of $β$ that results in the minimum in-sample MSE. Let's generalize this to include a ridge penalty $λ$ . $β=(X^TX+λI)^{-1}X^TY$ Given the foregoing, it's clear that for $λ>0$ , the in-sample MSE must be greater than that for $λ=0$ . Another way of looking at this is to consider the parameter space of $β$ explicitly. In your example there are two columns and hence three elements of $β$ (including the intercept): $ \begin{bmatrix} β_0 \\ β_1 \\ β_2 \\ \end{bmatrix} $ Now let us further consider a point of which I will offer no proof (but of which proof is readily available elsewhere): linear models' optimization surfaces are convex , which means that there is only one minimum (i.e., there are no local minima). Hence, if the fitted values of parameters $β_0$ , $β_1$ , and $β_2$ minimize in-sample MSE, there can be no other set of these parameters' values with in-sample MSE equal to, or less than, the in-sample MSE associated with these values. Therefore, $β$ obtained by any process not mathematically equivalent to the one I walked through above will result in greater in-sample MSE. Since we found that in-sample MSE is minimized when $λ=0$ , it is apparent that in-sample MSE must be greater than this minimum when $λ>0$ . $\Large{\text{A note on MSE estimators, in/out of sample, and populations:}}$ The usefulness of the ridge penalty emerges when predicting on out-of-sample data (values of the predictors $X$ on which the model was not trained, but for which the relationships identified in the in-sample data between the predictors and the response are expected to hold), where the expected MSE applies. There are numerous resources online that go into great detail on the relationship between $λ$ and the expected bias and variance, so in the interest of brevity (and my own laziness) I will not expand on that here. However, I will point out the following relationship: $\hat{MSE}=\hat{bias}^2+\hat{var}$ This is the decomposition of the MSE estimator into its constituent bias and variance components. Within the context of linear models permitting a ridge penalty ( $λ>=0$ ), it is generally the case that there is some nonzero value of $λ$ that results in its minimization. That is, the reduction (attributable to $λ$ ) in $\hat{var}$ eclipses the increase in $\hat{bias}^2$ . This has absolutely nothing to do with the training of the model (the foregoing mathematical derivation) but rather has to do with estimating its performance on out-of-sample data. The "population," as some choose to call it, is the same as the out-of-sample data I reference because even though the "population" implicitly includes the in-sample data, the concept of a "population" suggests that infinite samples may be drawn from the underlying process (quantified by a distribution) and hence the influence of the in-sample data's idiosyncracies on the population vanish to insignificance. Personally, after writing the foregoing paragraph, I'm even more sure that the discussion of "populations" adds needless complexity to this matter. Data were either used to train the model (in-sample) or they weren't (out-of-sample). If there's a scenario in which this distinction is impossible/impractical I've yet to see it.
