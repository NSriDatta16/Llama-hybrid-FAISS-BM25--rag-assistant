[site]: crossvalidated
[post_id]: 612683
[parent_id]: 
[tags]: 
How could XGBoost beat perfect logistic regression?

I try to compare the logistic regression with XBGoost on the simulated data. What I found is that XBGoost AUC is better than that of logistic regression, even when logistic regression predict perfect probability (the probability used to generate binary outcome) . Please see details below: Simulate X: generate 4 random variables (x1,x2,x3 and x4).See code section A. Similate Y: Let log_odds = x1+x2+x3+x4 (setting all coefficient to 1 and intercept to 0). Then convert log_odds to probability, and use probability to generate binary outcome. See code section B. Fit logistic regression. The estimated coefficients are very close to ones used for similation. The AUC is 0.834. coef: [[0.92180079 1.07390035 0.97258221 0.80164048]] Intercept [-0.00462648]. See code section C. Fit XGBoost. The AUC is 0.908 .See code section D. Simulate testing set with different random seed. Logistic regression AUC is 0.83 6, and XGBoost AUC is 0.907 . See code section E. As I understand, when I use simulated probability to generate binary outcomes, I was introducing randomness to the data, which could not be modelled/predicted. However, if the logistic regression already predict probabilities that are so close to the simulated ones, how could XGBoost generate better performance. Is this a problem of AUC, my test design, or my code? Thank you very much in advance! import random import numpy as np import pandas as pd import xgboost as xgb from xgboost import XGBClassifier import sklearn from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler from sklearn.metrics import classification_report from numpy.random import seed from numpy.random import rand # Section A: Simulate X random.seed(1) seed(1) n=10000 x1=np.array(rand(n))*4-2 x2=x1+np.array(rand(n)) x3=-np.array(rand(n))*1.9 x4=np.array(rand(n))*1 print(sum(x1
