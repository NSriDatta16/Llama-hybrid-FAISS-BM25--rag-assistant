[site]: crossvalidated
[post_id]: 349442
[parent_id]: 349440
[tags]: 
The way I usually do this is to stack the test folds (so you get one test prediction for the entire dataset) and ensemble the train folds. If your classifier generates probabilities, this is often done by averaging the probabilities and then thresholding to get the labels. Otherwise, with a binary classifier, it’s common to use a majority rule (and with 10-fold cross-validation, you’ll have 9 train predictions per dataset entry, so majority rule will give no ties). You compare models with their test set evaluation metric, and you use the train and test evaluation metrics to guard against overfitting. Confidence intervals for prediction are commonly obtained through bootstrapping, but you can also do it when cross-validating. See for example https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf
