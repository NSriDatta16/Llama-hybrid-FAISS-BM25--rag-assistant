[site]: crossvalidated
[post_id]: 436306
[parent_id]: 
[tags]: 
Can the GAN objective function be written as related to a log-likelihood of some "classical" statistical model?

Can the objective function that GAN (Generative Adversarial Network) models optimize be written as a lower bound of the log-likelihood of some "classical" statistical model? I am reading through the (as I understand) first description of a GAN , and a lot of the notation, vocabulary and writing/description conventions are unfamiliar to me. For posterity, the objective function is $$ \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z))]. $$ Notation: $x$ : observed data vector $z$ : standard normal variates $D$ : a "discriminator" that maps data to the probability of that data being "real" $G$ : the "generator" which maps standard normal variates into something that looks like data Actually, to be totally honest, I'm not positive that this is the objective because, in addition to what I mentioned earlier, they appeal to game-theoretic concepts (which I'm not familiar with). Equation (1) of that same paper writes that $$ \min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z))]. $$ Is that right, though? Or should they be dropping the $\min$ and the $\max$ on the left hand side?
