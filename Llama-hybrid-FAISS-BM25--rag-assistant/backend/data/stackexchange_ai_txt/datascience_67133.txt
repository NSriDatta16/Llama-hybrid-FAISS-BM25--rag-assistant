[site]: datascience
[post_id]: 67133
[parent_id]: 67129
[tags]: 
It is expected that accCV : the former is the accuracy on the test folds (averaged over all the splits), so represents models' scores on unseen-to-them data. Similarly, you would expect accTrain > accTest . There are two main reasons to evaluate a model, whether by k-fold cross-validation or simple train/test split: for hyperparameter optimization / model selection, or to estimate future performance. (N.B., k-fold should generally give better predictions than simple train/test split.) If you make any decisions based on the scores, then they no longer represent unbiased estimates of future performance. If, in your setup, you make no decisions based on step 2, then you should expect accCV ~ accTest , and there's no real reason to include that step. If you do make decisions based on step 2, you may expect accCV > accTest , though the gap is probably substantially smaller than the gap in accTrain > accTest . You may see discrepancies here, due to natural variation in the datasets, or perhaps data leakage.
