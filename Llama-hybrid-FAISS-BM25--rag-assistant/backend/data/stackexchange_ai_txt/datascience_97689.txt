[site]: datascience
[post_id]: 97689
[parent_id]: 
[tags]: 
Adding noise after LSTM layer

I am building a Natural Language Inference neural network model that learns to identify if one sentence (hypothesis) follows from another sentence (premise). So the input to my network is 2 sentences, a premise and a hypothesis. I have an LSTM-based network with an LSTM layer, a fully connected layer and a fully connected output layer that outputs the probability of a premise-hypothesis pair belonging to one of the two classes. In order to improve generalization ability of my network, I have added a Keras Gaussian noise layer after the LSTM layer that separately encodes the premise and the hypothesis, so I add noise to the LSTM encoding of the premise and the LSTM encoding of the hypothesis and the resulting two vectors then are passed to the fully connected layer. I tried several regularization methods, I tried using dropout in different ways, but this one seemed to perform the best. Does it make sense to add noise to LSTM representations of sentences? Or the more logical approach is to add noise to input of the network? Is this even a sensible regularization method? And what exactly does adding noise to LSTM encodings of sentences do?
