[site]: crossvalidated
[post_id]: 534528
[parent_id]: 55377
[tags]: 
$x_1,...,x_n\sim Pois(\lambda)$ (a) We want to estimate $\theta=e^{-\lambda}$ , which is exactly $P(x_1=0)$ . We take $T(x)=I\{x_1=0\}$ as an estimator. It might look dumb, but it is an unbiased estimator: $$E[T(x)]=1\cdot P(x_1=0) + 0\cdot P(x_1 \neq 0)=P(x_1=0)=\frac{e^{-\lambda}\cdot\lambda^0}{0!}=e^{-\lambda}=\theta$$ Now, let's loot at the MSE: $$E[(T(x)-\lambda)^2]=E[(T(x)-\theta+\theta-\lambda)^2]=E[(T(x)-\theta)^2]-2E[(T(x)-\theta)(\theta-\lambda)]+E[(\theta-\lambda)^2]$$ $T(x)$ is an unbiased estimator of $\theta$ and thus $2E[(T(x)-\theta)(\theta-\lambda)]$ falls. Using the MSE decomposition we get $b^2(T,\lambda)=(\theta-\lambda)^2$ , so the bias of $T$ w.r.t. $\lambda$ is simply $\theta-\lambda$ . Now, it is time for the mighty CRB: $$Var(T)=E[(T(x)-\theta)^2]-(\theta-\lambda)^2 \geq \frac{\left(1+\frac{d}{d\lambda}b(T,\lambda)\right)^2}{I(\lambda)}$$ a momentary pause: the Fisher information for the Poisson is $I(\lambda)=\frac{1}{\lambda}$ , so: $$Var(T)\geq\lambda \left(1+\frac{d}{d\lambda}(\theta-\lambda)\right)^2$$ another pause: we were hinted to differentiate w.r.t. $\theta$ , so we need do substitute $\lambda=-log(\theta)$ and substitute the derivation: $$\frac{db}{d\lambda}=\frac{db}{d\theta}\cdot\frac{d\theta}{d\lambda}=\frac{db}{d\theta}\cdot\frac{d}{d\lambda}(e^{-\lambda})=\frac{db}{d\theta}\cdot(-e^{-\lambda})=-\theta\cdot\frac{db}{d\theta}$$ So we substitute: $$Var(T)\geq -log(\theta) \left(1-\theta\frac{d}{d\theta}(\theta+log(\theta))\right)^2=-log(\theta) \left(1-\theta\left(1+\frac{1}{\theta}\right)\right)^2=-log(\theta) (1-\theta -1)^2$$ and finally get: $$Var(T)\geq-log(\theta)\theta^2$$ (b) $$L(x,\lambda)=\prod_{i=1}^{n}{\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}}=\frac{1}{\prod{x_i!}}e^{-n\lambda}\lambda^{\sum{x_i}}=\frac{1}{\prod{x_i!}}exp(-n\lambda)exp(log(\lambda)\sum{x_i})$$ So by denoting $\eta=log(\lambda), S(x)=\sum{x_i}, A(\eta)=n\lambda, h(x)=\frac{1}{\prod{x_i!}}$ we get a representation of the likelihood function as an exponential family. Reparametrizing using $\theta$ we get $\eta=log(-log(\theta)), A(\eta)=-n log(\theta), S(x)=\sum{x_i}, h(x)=\frac{1}{\prod{x_i!}}$ , again en exponential family. This is important as we get that $S(x)=\sum{x_i}$ is a sufficient statistic due to properties of exponential families (Fisher-Neyman factorization theorem). $S(x)$ is the sum of $x_1,...,x_n\sim Pois(-log(\theta))$ , so $S(x)$ itself is a Poisson RV with parameter $-nlog(\theta)$ . Now, let $g(S(x))$ be a function s.t $E[g(S)]=0$ : $$E[g(S)]=\theta^{-n}\sum_{s=0}^{\infty}{g(s)\cdot\frac{\left(-n log(\theta) \right)^s}{s!}}$$ The only term that can be zero is $g(s)$ , so if $E[g(S(x))]=0$ we conclude that $g(s)=0$ for all values of $s$ , hence $S(x)=\sum{x_i}$ is a complete statistic. QED! (c) This is almost cheating: The corollary from the Lehmann-Scheff√© theorem is that if S is a complete sufficient statistic, then applying the R-B procedure would yield the UMVUE. last momentary pause: we were told "combine your findings in Parts (a) and (b)", so let's do it In part (a) we got $Var(T)\geq-log(\theta)\theta^2$ , in part (b) we got that $S(x)=\sum{x_i}$ is complete sufficient statistic, R-B has yielded $T^*$ , so $Var(T^* )\geq-log(\theta)\theta^2$ . That's it. PS It would be proper to thank Prof. Pavel Chigansky of HUJI, who has taught us this exact problem in his statistical inference course.
