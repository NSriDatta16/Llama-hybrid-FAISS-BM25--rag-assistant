[site]: crossvalidated
[post_id]: 364041
[parent_id]: 362676
[tags]: 
Did you use sequentially ordered training, validation and test sets? The use of the terminology X_valid_resampled seems to suggest that at some point in the generation of your validation set, you reshuffled your dataset. Time series data are not i.i.d., they are serially correlated - if you don’t respect this property when performing reshuffling, you will end up with useless results. In other words, you’re only allowed to reshuffle the order in which you feed to your NN the various time series (individuals), but, in theory , inside each time series you shouldn't reshuffle the order of the time samples. The reason is that theoretically LSTMs should be able to learn long-term recurrencies, i.e., by storing the memory of the subsequences they've already seen in the hidden state, they should be able to learn and predict arbitrarily long patterns, longer than the length of a subsequence you feed them. In order for LSTMs to learn such patterns, of course during training the state of a LSTM must not be initialized to 0 between two batches: Keras calls these LSTMs stateful . In practice , multiple experiments have shown that LSTMs are often not able to learn long-term (theoretically even infinite) recurrences. For this reason, often the subsequences are reshuffled among different training batches, and the state of the LSTM is initialized to 0 after each batch ( stateless LSTMs). However, each subsequences is still made of successive time samples. Also, the subsequences used for the training set must precede in time those used for the validation set, and those used for the validation set must in turn precede in time those used for the test set, otherwise we would be leaking information from the training to the validation/test set. Other than this, it’s hard to answer your question without seeing exactly which preprocessing steps you followed. Have a look at this question How to apply Neural Network to time series forecasting? Even though it’s about time series regression , rather than classification, still it explains quite well how to preprocess your data set in order to successfully train your RNN. Also, since you mention a tutorial in your question, I think this tutorial more clearly describes the process of generating the subsequences needed for training, validation and testing. Finally, it may be that you already preprocess your data correctly. In this case, a possible explanation for your crappy test results may be overfitting. As a matter of fact, a clear issue with your architecture is that you haven’t used any regularization, except that implicitly provided by the stochastic optimizer. You need appropriate regularization methods for recurrent architectures: look into dropout and recurrent dropout.
