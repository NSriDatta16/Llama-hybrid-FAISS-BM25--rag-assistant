[site]: crossvalidated
[post_id]: 601853
[parent_id]: 
[tags]: 
Why are K and V the same in the second attention layer of a Transformer's decoder?

(For this example, let's say we're using a Transformer to translate from English to French.) For the decoder in a Transformer, the second attention layer takes K and V from the encoder and then takes Q from the decoder's previous attention layer. First of all, how do K and V get from the encoder to the decoder? Is the encoder's output copied twice and then directly fed into the decoder? Or is it transformed to K and V with their respective weight matrices? Second, why are K and V the same? I understand that Q is supposed to be the resulting French sentence and that K tells the model how to map the English sentence to French. But why is V the same as K? The only explanation I can think of is that V's dimensions match the product of Q & K. However, V has K's embeddings, and not Q's. I'm assuming Q and K & V have different embedding values because they are two different languages.
