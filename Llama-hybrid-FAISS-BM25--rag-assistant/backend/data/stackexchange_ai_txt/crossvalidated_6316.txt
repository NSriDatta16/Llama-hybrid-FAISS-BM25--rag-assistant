[site]: crossvalidated
[post_id]: 6316
[parent_id]: 256
[tags]: 
Boosting employs shrinkage through the learning rate parameter, which, coupled with k -fold cross validation, "out-of-bag" (OOB) predictions or independent test set, determine the number of trees one should keep in the ensemble. We want a model that learns slowly, hence there is a trade-off in terms of the complexity of each individual model and the number of models to include. The guidance I have seen suggests you should set the learning rate as low as is feasibly possible (given compute time and storage space requirements), whilst the complexity of each tree should be selected on basis of whether interactions are allowed, and to what degree, the more complex the tree, the more complex the interactions that can be represented. The learning rate is chosen in the range $[0,1]$. Smaller values ($ k -fold CV (or OOB predictions or independent test set) is used to decide when the boosted model has started to overfit. Essentially it is this that stops us boosting to the perfect model, but it is better to learn slowly so we have a large ensemble of models contributing to the fitted model.
