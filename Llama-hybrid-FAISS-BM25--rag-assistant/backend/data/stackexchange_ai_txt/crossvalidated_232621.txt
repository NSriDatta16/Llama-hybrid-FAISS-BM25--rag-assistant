[site]: crossvalidated
[post_id]: 232621
[parent_id]: 232604
[tags]: 
$P(x_i|\theta)$ is the probability that you observe $x_i$ for a given parameter value $\theta$. In a sample you observe several $x_i, i=1,2,\dots,n$ simultanuously and (assuming independence) the probability that you observe all the values in your sample simultanuously is $\prod_i P(x_i|\theta)$. So maximising this with respect to $\theta$ means that you look for the value of $\theta$ that makes your (full) observed sample most probable. $\sum_i P(x_i|\theta)$ is (assuming that that the observations do not intersect) would be the probability that you observe at least one of the $x_i$ . So if you maximise this then you look for the value of $\theta$ such that at least one of the $x_i$-values is observed. As you have observed all the $x_i$ values, it makes sense to compute the probability that you all observed these simultanuously I think. This is also what @Alex says in his comment on the ''joint probability'' . As @Tim argues (+1) there are reasons to log-transform it and as the 'log' is monotic you will find the same $\theta$. Due to your comment ''the Bayesian approach does not maximize the joint prob'' I make the following remark: if you observe one value $x_1$ then the Bayes approach would compute the posterior $P(\theta|x_1)\propto P(x_1|\theta) p(\theta) $. If I observe a second observation $x_2$ then I will update my estimate using the ''updated prior'', so you compute $P(\theta|x_2)\propto P(x_2|\theta) p^{upd}(\theta) $ and that prior would certainly be the one that already takes into account the other (independent) observations ($x_1$), so $p^{upd}(\theta)$ would be $P(\theta|x_1)$ and the new posterior becomes $P(\theta|x_2)\propto P(x_2|\theta) P(x_1|\theta) p(\theta) $. After observing the full sample you find the posterior $P(\theta|x)\propto \prod_i P(x_i|\theta) p(\theta) $, so what you say: ''The Bayesian apporach does not maximize the joint prob'' is not fully correct; the Bayesian approach is also using the joint probability of the full sample, i.e. $\prod_i P(x_i|\theta)$, in the likelihood function.
