[site]: datascience
[post_id]: 87663
[parent_id]: 87662
[tags]: 
A [CLS] token is added to the beginning of the sentence, and a [SEP] token is added to the end. These two special tokens have specific functions: [CLS] is needed because it was used in the training loss of BERT to keep the first output position for a different purpose than the rest: it was used for the "next sentence prediction" loss, which trained the model to tell if the 2 text segments passed as input were consecutive in the original text or not. When finetuning BERT, the output of this position is normally used for sentence classification tasks. When BERT is used for feature extraction, the output vector at that position is used as sentence embedding. [SEP] is used to mark the ending of the sentence.
