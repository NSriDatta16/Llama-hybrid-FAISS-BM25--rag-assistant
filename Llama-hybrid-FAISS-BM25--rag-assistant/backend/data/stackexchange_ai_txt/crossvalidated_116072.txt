[site]: crossvalidated
[post_id]: 116072
[parent_id]: 
[tags]: 
$\tanh$ activation function and sparsity constraint

According to LeCun's paper "effient backprop" [1] the $\tanh$ activation function should be preferred over the logistic activation function for the hidden units in neural networks. For the $\tanh$ units an output of $a_i = -1$ is considered as inactive. But is correct especially for the sparsity constraints (e.g. sparse autoencoders)? An inactive unit should act like not present. But an unit with $a_i = -1$ contributes to the $\text{logit}$ $\sum_i (w_ij * a_i)$ extremely. In contrast a unit with output $a_i = 0$ would not contribute to the sum. But for output $a_i = 0$ the $\tanh$ unit is absolutely not in the saturated regime. What output is considered as inactive if there is an sparsity constraint to force most of the units to be inactive? Is there a paper about the "sparsity of activation" for $\tanh$ units? [1] Y. LeCun, L. Bottou, G. Orr and K. Muller: Efficient BackProp, in Orr, G. and Muller K. (Eds), Neural Networks: Tricks of the trade, Springer, 1998
