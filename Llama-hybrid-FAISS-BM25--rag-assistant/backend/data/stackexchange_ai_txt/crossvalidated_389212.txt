[site]: crossvalidated
[post_id]: 389212
[parent_id]: 389200
[tags]: 
The purpose of a prior is not to reduce uncertainty per se. Rather it is to express what is already known -- and generally agreed upon -- about the parameter or parameters in the model likelihood. Priors can be informative or uniformative. If little is known about a parameter, then an uniformative prior (maximally entropic) is appropriate. But often, we know more than we think we do. For instance, we know variance cannot be negative, so a prior that sets the prior probability to zero for $\sigma^2 is sensible. Pleading ignorance gets old quickly, and informative priors are more fun. If you are doing a series of experiments over several days, your prior today may be your posterior from yesterday. You can daisy-chain several days of results together this way. For algebraic convenience , priors are usually chosen to be "conjugate" to the likelihood function, so the functional form of the posterior is the same as the prior. The best example of this is the beta function prior for the Bernoulli/binomial model likelihood. Worth noting: if two researchers disagree on a prior, then they will disagree on the interpretation of the results of the experiment. That is not wrong; it is simply unfortunate, and the resolution is for them to agree on the prior, ideally a priori (before the fact), but a posteriori (after the fact) works, too. As long as a prior does not insist that a particular value for a parameter has a probability identically equal to zero, then any prior will eventually shift toward the MLE, as enough data comes in. I think MLK Jr. was enunciating a basic Bayesian truth when he said "the arc of the moral universe is long, but it bends toward justice." Yes, priors/prejudices disappear when a) enough data is collected, and b) we keep our minds open and do not set any prior probabilities exactly to zero.
