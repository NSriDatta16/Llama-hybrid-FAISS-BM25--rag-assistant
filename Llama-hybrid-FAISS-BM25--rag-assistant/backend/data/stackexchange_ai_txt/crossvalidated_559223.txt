[site]: crossvalidated
[post_id]: 559223
[parent_id]: 559206
[tags]: 
Your observation is correct. GARCH is an autoregressive model and its $h$ -step-ahead predictions tend to lag $h$ steps behind, as is the case with most autoregressive models. We often model time series processes as being hit by a new zero-mean stochastic shock every period. A special case that illustrates the lagging predictions best is an AR(1) with a zero intercept and a unit slope (in other words, a random walk): $$ y_t=c+\varphi_1 y_{t-1}+\varepsilon_t $$ where $c=0$ and $\varphi_1=1$ . An optimal (under square loss) $h$ -step-ahead point forecast is $\hat y_{t+h|t}=y_t$ , i.e. the last observed value. Thus even if we were able to estimate $c$ and $\varphi_1$ with perfect precision, our optimal (!) forecast would seem to lag by $h$ steps. Similar logic applies in the more general case of $c\neq 0$ and $\varphi\neq 1$ , though the argument for the general case is more nuanced. GARCH being an autoregressive model suffers from the same problem. (The fact that GARCH is autoregressive in terms of conditional variance rather than conditional mean does not change the essence. See this answer for more detail.) But recall that that need not be a sign of forecast suboptimality, as even optimal forecasts may be characterized by it. This applies to GARCH to a large extent; in typical applications of GARCH models, conditional variance is often found to be quite close to a random walk.
