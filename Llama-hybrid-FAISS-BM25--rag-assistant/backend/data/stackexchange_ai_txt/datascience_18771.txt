[site]: datascience
[post_id]: 18771
[parent_id]: 18762
[tags]: 
There are no rules to infer neural network hyper-parameters from a problem description. With only number of features, number of examples and the fact you have a binary classification problem, it is far too little to even make an educated guess. First, are you sure you are ready to build a deep learning model for your data? Have you looked at the data, or scatter plotted it reduced to two dimensions using PCA or t-SNE, to get a feel for how easy your data is to separate (easy to separate raw data implies using simpler/shallower model)? You could try a basic model such as logistic regression or SVM in order to establish a benchmark so you can tell whether the deep model was doing anything useful. Assuming you are ready to go ahead, the time-honoured approach is to try out variations, and measure the results using cross-validation. You can do this methodically, by for instance starting with one hidden layer with 64 neurons, and either adding/removing layers or adding/removing neurons in each layer. Generally these searches don't cover all possible variations, but stop once you have tried those that seem interesting and have reached a reasonable result for your problem. Do note that other hyper-parameters will affect results and can interact with your choices for numbers of neurons and layers. You cannot isolate your choice of network depth from other choices, such as optimisation method, activation functions, regularisation. When starting to build a model, it is just as reasonable to spend a long time exploring these other factors as it is to look at network size/shape. It is very easy to get a deep NN to over-fit your data. Cross-validation is therefore a necessity as you explore hyper-parameters for your problem. One reasonable method of searching against number of neurons is to increase them until you start to notice over-fitting, then adjust regularisation to stop the over-fit. At that point, with all other factors remaining the same, there is probably no need to explore networks with many more neurons (although deeper/shallower networks may still be worth exploring, and if you do so, you will once again want to explore number of neurons per layer).
