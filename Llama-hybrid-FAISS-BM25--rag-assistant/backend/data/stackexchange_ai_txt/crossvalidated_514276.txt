[site]: crossvalidated
[post_id]: 514276
[parent_id]: 
[tags]: 
Is there a better neural network architecture for this problem?

I've got an interesting multiple-response task that I believe a neural network could be useful for solving in an elegant way. I've got 6 predictors and 3 responses. The 6 predictors can be broken up in to 3 different groups of $\alpha$ , $\beta$ and $\gamma$ with 2 columns each. The data is generated such that $$ f(x_{\alpha_1},x_{\alpha_2},x_{\beta_1},x_{\beta_2},x_{\gamma_1},x_{\gamma_2}) = (y_A, y_B, y_\Gamma) $$ but there is a structure to the grouping of the columns such that if I were to rearrange the columns (but preserve the inner grouping order), I would obtain $$ f(x_{\beta_1},x_{\beta_2},x_{\gamma_1},x_{\gamma_2},x_{\alpha_1},x_{\alpha_2}) = (y_B,y_\Gamma,y_A) $$ I note that the situation is not as simple as just creating a model for $f(x_{\alpha_1},x_{\alpha_2}) = y_A$ and applying it to the other sets of predictors because $y_A$ also depends on $x_{\beta_1},x_{\beta_2},x_{\gamma_1},x_{\gamma_2}$ . In my situation, the relationship between the predictors and the responses $f$ is nonlinear, but very low noise, so I could naively plug all the data in to a big fully connected neural network and with some tuning, get a very good fit. However, I imagine there is a very clever neural network architecture that could be used here that makes use of weight sharing or something similar because the order of the columns/inputs only matters to an extent. I imagine the benefits of an improved architecture here would allow it to learn from fewer samples and perhaps the constraint demonstrated in the equations above would make it more robust? Is there some fancy architecture or trick that could or should be employed here? EDIT: It also turns out that the sum of the responses is equal to 1, which might also be useful in designing a better architecture. $$\sum_{i \in \{A,B,\Gamma\}} y_i = 1$$
