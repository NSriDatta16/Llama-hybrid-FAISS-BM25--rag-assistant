[site]: crossvalidated
[post_id]: 426941
[parent_id]: 
[tags]: 
Is there a derivation for the Posterior Predictive Distribution?

I came across this term in the deep learning book: $p(x_{m+1}|x_1 ... x_m) = \int p(x_{m+1}|\theta)p(\theta|x_1 ... x_m)d\theta$ After some research I find that this term is the definition of the posterior predictive distribution. Is there a mathematical or intuitive proof of this? I'd prefer mathematical but anything helps right now. EDIT: By proof, I am asking how LHS is equal to the RHS. For example, does this also work? If so, how do I arrive at PPD from the Bayes theorem? $\frac{p(x_{m+1},x_1 ... x_m)}{p(x_1 ... x_m)} = \int p(x_{m+1}|\theta)p(\theta|x_1 ... x_m)d\theta$
