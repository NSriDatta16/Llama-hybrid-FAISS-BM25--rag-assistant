[site]: crossvalidated
[post_id]: 292008
[parent_id]: 
[tags]: 
Leave-one-subject-out CV for large dataset

I have a large dataset of biomedical data from $40$ subjects consisting of $4$ features and around $2500$ observations per subject. Observations are categorized into six classes, i.e. each observation can belong to one of six classes. I am using an SVM classifier with Gaussian kernel with a one-vs-all classification scheme. For leave-one-subject-out cross-validation, I have two methods in mind: Train the classifier on $39$ subjects and test on $1$ subject. This takes a long time, since it involves training the classifier on $39$ subjects in each fold. Remember, each fold has to do one-vs-all classification. Split the dataset into groups of $5$ subjects, and perform cross-validation within the group, i.e. train the classifier on $4$ subjects and test on the fifth. Both cross-validation schemes require training the classifier $40$ times (once for each subject), but in the second case training is faster since the classifier is training on $4$ subjects in each fold. Questions: Is the second cross-validation technique valid? What are some critiques about such cross-validation? Is there a better way to perform cross-validation on such a large dataset?
