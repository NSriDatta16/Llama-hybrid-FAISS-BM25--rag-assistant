[site]: crossvalidated
[post_id]: 201332
[parent_id]: 201171
[tags]: 
Correct me if I am wrong. Univariate analysis will make over-segmentation for sure. I see that mean of data points remain constant during the time. Unsupervised way: What if you model the distribution based on all data points of the time series, than choose a resolution of your algorithm (size of sliding window), then (based on your distribution) construct a statistic (similar to chi-squared statistic which can be constructed from squared normal distributions) and then detect windows that have a statistic value lower than 5% quantile of your distribution? I mean, left-sided test (that will mean that data points within sliding window fit your distribution extremely good, so they are close to the mean). You can do it online, but the accuracy for the first data points will be extremely low. Supervised way: Also you can label windows of fixed size for plateaus manually and create a distribution for them and consider all deviations from this multivariate distribution as "not a plateaus". Also you may construct 2 distributions (with manual labelling of regions) and consider likelihood ratio as a measure. These are standard approaches, may be not really sophisticated. For online recognition I would recommend to feed your classifier with the first $n$ data points to train it (you need to have at least some estimations) or use algorithm that can do backwards and re-analyse the first data points.
