[site]: datascience
[post_id]: 62612
[parent_id]: 
[tags]: 
Encoding logic rules in machine learning

For some problems you have certain rules that you know for a fact are true. A very simplistic example would be - "A grade below 40% is a failing grade" There is no ambiguity in the statement and for that system it will always be correct. Encoding such rules within any machine learning model should allow it to avoid simplistic mistakes and theoretically improve accuracy. I'm aware there are several niche libraries for this and an entire subfield as well. However how would I combine such logic rules within a typical model (CNN,LSTM ,etc) in a platform like TensorFlow, PyTorch ? Edit : What I'm looking forward is to add explicit label rules (similar to what Snorkel [ https://github.com/snorkel-team/snorkel ] uses to generate noisy labels) to help the model learn. These relations while present in the data might be hard for the model to learn or require large amounts of training data/time. In some scenarios a combination of deterministic rules and ML would be better than using either alone.
