[site]: datascience
[post_id]: 112625
[parent_id]: 112598
[tags]: 
The original code from the Bert model doesn't have any mention of special tokens [CLS], [SEP], or [EOS]. So it seems that the data in input is already organized to fit Bert's input format. There is a Bert convention about those special tokens: # The convention in BERT is: # (a) For sequence pairs: # tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP] # type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 # (b) For single sequences: # tokens: [CLS] the dog is hairy . [SEP] # type_ids: 0 0 0 0 0 0 0 # # Where "type_ids" are used to indicate whether this is the first # sequence or the second sequence. The embedding vectors for `type=0` and # `type=1` were learned during pre-training and are added to the wordpiece # embedding vector (and position vector). This is not *strictly* necessary # since the [SEP] token unambiguously separates the sequences, but it makes # it is easier for the model to learn the concept of sequences. # # For classification tasks, the first vector (corresponding to [CLS]) is # used as the "sentence vector". Note that this only makes sense because # the entire model is fine-tuned. This is from the feature extraction process . Consequently, [CLS] tokens are useful to get a row of position tokens, and [SEP] tokens are useful to differentiate the questions from answers through type_ids. The position and type tokens are converted to tensors. However, the [CLS] and [SEP] tokens are not converted to tensors because their function is just to delimit the input data. In the Bert model , the token_type_ids are used in the post-processor embedding, together with the position_ids to an output. if use_token_type: if token_type_ids is None: raise ValueError("`token_type_ids` must be specified if" "`use_token_type` is True.") token_type_table = tf.get_variable( name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range)) # This vocab will be small so we always do one-hot here, since it is always # faster for a small vocabulary. flat_token_type_ids = tf.reshape(token_type_ids, [-1]) one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size) token_type_embeddings = tf.matmul(one_hot_ids, token_type_table) token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width]) output += token_type_embeddings Then, this output is normalized and a dropout is applied. In conclusion, special tokens are defined by a convention, and the 2 main ones are [CLS] and [SEP] which delimit the 2 main types of vectors necessary for the Bert model for the question/answer process. Note: You can define [CLS] or [SEP] with other names in the Pretrained tokenizer from HuggingFace with the sep_token and the cls_token attributes.
