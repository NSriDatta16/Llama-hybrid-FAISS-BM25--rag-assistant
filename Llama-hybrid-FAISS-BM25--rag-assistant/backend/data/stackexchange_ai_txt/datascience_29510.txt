[site]: datascience
[post_id]: 29510
[parent_id]: 29507
[tags]: 
If you use max-pooling layers, they may be insensetive to small shifts but not that much. If you want your network to be able to be invariant to transformations, such as translations and shifts or other types of customary transformations, you have two solutions, at least as far as I know: Increasing the size of data-set Using spatial transformers Take a look at What is the state-of-the art ANN architecture for MNIST and Why do convolutional neural networks work . Thanks to one of our friends, another way is to use transfer learning after data-augmentation.
