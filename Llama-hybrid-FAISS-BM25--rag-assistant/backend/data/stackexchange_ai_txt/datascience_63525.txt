[site]: datascience
[post_id]: 63525
[parent_id]: 63503
[tags]: 
I dont think its necessary combining 4 models into one by averaging probabilities (how do you know that they have same weights? let the learner handle those weights) since you are using same features and learner which is an ensemble (it combines weak learner into one natively already). Therefore it's better to think about labeling and class balances of those 4 models. Is this overfitting the data? Is it even possible to overfit the data when using random forests? Is there something wrong with this intuition? In order to say that you are overfitting or not, train and test errors should be available . Simply training error is very low compare to test error it implies overfitting. Secondly, ensemble models are likely to overfit due to their greediness. And finally, you mentioned a bit class imbalance. So you should check metrics such as roc auc , recall and precision according to your case rather than accuracy. Therefore my suggestion is that: Apply hyper parameter tuning for all 4 models. Report mean training & test errors with relevant performance metric (for imbalanced consider to use roc auc). If there are huge differences between test and training errors, try to change your model/feature set etc since you are overfitting (eg: .99 for training, .70 for test). Otherwise pick the best estimator among 4 models and use this as final estimator. Final comment: As complexity increases, models are more likely to overfit . (logistic regression is less likely to overfit compare to ensemble trees such as rf, gbm, xgboost). Thus always tune your parameters when using ensembles. Hope it helps!
