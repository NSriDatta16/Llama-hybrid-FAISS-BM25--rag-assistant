[site]: crossvalidated
[post_id]: 365555
[parent_id]: 326110
[tags]: 
It won't be the same. Check this for how XGBoost handles weights: https://github.com/dmlc/xgboost/issues/144 Weighting means increasing the contribution of an example (or a class) to the loss function. That means the contribution of the gradient of that example will also be larger. That's why (as you will see in the discussion I linked above) xgboost multiplies the gradient and the hessian by the weights, not the target values.
