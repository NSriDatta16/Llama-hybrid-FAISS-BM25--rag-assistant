[site]: crossvalidated
[post_id]: 185039
[parent_id]: 
[tags]: 
Why is lasso in matlab much slower than glmnet in R (10 min versus ~1 s)?

I observed that the function lasso in MATLAB is relatively slow. I run many regression problems, with typically 1 to 100 predictors and 200 to 500 observations. In some cases, lasso turned out to be extremely slow (to solve a regression problems it took several minutes). I discovered that this was the case when the predictors were highly correlated (e.g., air temperature time series at neighboring grid points of an atmospheric model). I compared the performances of the below example in matlab and in R. y is the predictand vector with 163 elements (representing observations) and x is the predictor matrix with 100 rows and 163 observations corresponding to the observations in y. I applied the MATLAB function lasso as follows: [beta_L,stats]=lasso(x,y,'cv',4); The same in R, using glmnet: fit.lasso=cv.glmnet(predictor.ts,predictand.ts,nfolds=4) Both MATLAB and R are based a coordinate descent algorithm. The default value for the number of lambda values is 100 for both lasso and glmnet. The convergence threshold for the coordinate descent is per default 10^-4 in matlab, and even lower in R (10^-7). The R function takes one second on my computer. Matlab takes several minutes, with most of the computation time spend in the coordinate descent algorithm. When the predictors are less correlated (e.g., different variable types of a numerical atmospheric model) lasso in Matlab is not so slow, but still takes ~30 - compared to ~ 1 s in R). Is matlab lasso really much more inefficient than glmnet, or do I miss something?
