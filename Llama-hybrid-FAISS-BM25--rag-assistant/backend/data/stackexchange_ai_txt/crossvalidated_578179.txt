[site]: crossvalidated
[post_id]: 578179
[parent_id]: 
[tags]: 
Why is off-centered prior necessary for HMC sampler?

In this PyMC3 tutorial on Bayesian Mixed Effects Models , there is some Re-parameterization "to avoid chain divergences." with pm.Model(coords=coords) as hierarchical: # Hyperpriors intercept_mu = pm.Normal("intercept_mu", 0, sigma=1) intercept_sigma = pm.HalfCauchy("intercept_sigma", beta=2) slope_mu = pm.Normal("slope_mu", 0, sigma=1) slope_sigma = pm.HalfCauchy("slope_sigma", beta=2) # Define priors sigma = pm.HalfCauchy("sigma", beta=2, dims="group") # Reparameterise to avoid chain divergences # β0 = pm.Normal("β0", intercept_mu, sigma=intercept_sigma, dims="group") β0_offset = pm.Normal("β0_offset", 0, sigma=1, dims="group") β0 = pm.Deterministic("β0", intercept_mu + β0_offset * intercept_sigma, dims="group") # Reparameterise to avoid chain divergences # β1 = pm.Normal("β1", slope_mu, sigma=slope_sigma, dims="group") β1_offset = pm.Normal("β1_offset", 0, sigma=1, dims="group") β1 = pm.Deterministic("β1", slope_mu + β1_offset * slope_sigma, dims="group") # Data x = pm.Data("x", data.x, dims="observation") g = pm.Data("g", data.group_idx, dims="observation") # Linear model μ = pm.Deterministic("μ", β0[g] + β1[g] * x, dims="observation") # Define likelihood pm.Normal("y", mu=μ, sigma=sigma[g], observed=data.y, dims="observation") Consider that β1 = pm.Normal("β1", slope_mu, sigma=slope_sigma, dims="group") was swapped out for the below: β1_offset = pm.Normal("β1_offset", 0, sigma=1, dims="group") β1 = pm.Deterministic("β1", slope_mu + β1_offset * slope_sigma, dims="group") Why is this necessary and what does it accomplish? Edit: I see the comment link to Stan forum, is there a slightly more concise/higher level explanation available? I understand that HMC doesn't do well with high curvature areas; but why is multiplying the slope by an additional Normally distributed random variable helpful?
