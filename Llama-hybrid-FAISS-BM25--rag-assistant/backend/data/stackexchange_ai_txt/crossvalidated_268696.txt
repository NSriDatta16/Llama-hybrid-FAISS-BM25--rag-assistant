[site]: crossvalidated
[post_id]: 268696
[parent_id]: 
[tags]: 
Feature selection and subsetting when most features are missing many data points

I have a data set for which I would like to build a predictive model, and it contains quite a few features where most of the data is missing. The feautres are also highly correlated pair-wise and I would like to do some feature-selection scheme or apply PCA to reduce the dimensionality. The missingness looks like this (this is generated by the VIM R-package): Here, on the right hand plot, each row shows a combination of features and red means missing data. The vertical histogram on the far right shows how frequent the combination is. The left histogram shows the number of missing data points in each feature. I know that a typical method of dealing with the situation is to do data imputation and then, for example, apply PCA. But in this particular case, data imputation seems unreasonable due to the high percentage of missing data in each feature. My current approach is to simply remove the most sparse features (e.g. the ones having more than 50% missing), and then simply remove data points where at least one of the features has missing data. I know that this is assuming so-called "missing completely at random", but I think this is OK for this case. However, my worry is that this method potentially excludes useful combinations of features (what if, for example one of the features with only 40% present data has high predictive power?). Is my approach bad, and what other methods are there for dealing with this kind of situation? Are there methods, for example, that can do feature selection without imputing data first?
