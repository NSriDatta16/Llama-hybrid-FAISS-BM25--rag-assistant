[site]: crossvalidated
[post_id]: 633226
[parent_id]: 
[tags]: 
What embeddings are used in decoder-only models like GPT?

Decoder-only models do not use an encoder. Hence, they do not get the embedding from it. I went through this nice description of a decoder-only transformer -based model I do understand the training phase but need some clarification regarding one thing in particular. What is the word embedding that is being used? Does it start with a usual word embedding like Glove or word2vec and then update it through training? If so how is that done? During inference what is the word embedding that's used? It would be nice if you gave an example as well, like GPT.
