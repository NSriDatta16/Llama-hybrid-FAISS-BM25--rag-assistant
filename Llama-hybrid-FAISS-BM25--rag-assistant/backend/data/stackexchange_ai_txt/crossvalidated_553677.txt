[site]: crossvalidated
[post_id]: 553677
[parent_id]: 553260
[tags]: 
I intend to get one metric per test that I can plot for all the 100 models to show that the models from which I am fetching variable importance are good enough. Getting a single metric per test will require some thought and care, as illustrated in the following excerpt from your predicted survival curves, the 8th and 10th rows of your predicted survival probabilities plotted against the corresponding survival times ( unique.death.times ): The predicted survival curves cross. Case 8 has higher predicted survival probability than Case 10 until about Time = 19 , where the order of survival probability switches. At what time point, or over which range of time points, do you want to do your comparisons of "accuracy"? How will you take censoring into account? This is a bigger problem for random forest than for Cox models, in which predicted survival curves don't cross as a function of covariates and you can, for example, simply compare the rank-order of linear-predictor values to that of survival times for all comparable pairs of cases to get a time-independent C-index/AUC. The R pec package was designed to help with evaluating predictions from random survival forests, as explained here . It produces "prediction error curves" based on the integrated Brier score, the mean-square difference between observed survival (0/1) and predicted survival probabilities over time, while taking censoring into account. You will have to use your understanding of the subject matter to decide what time span to evaluate. The R Survival task view section on "Predictions and Prediction Performance" has links to other approaches to evaluating time-dependent survival predictions that you might find helpful. A further suggestion: with so few test cases, you won't have a lot of precision in your estimates and the estimates might depend highly on your test/train split. You might consider fitting the entire data set and evaluating out-of-bag performance instead.
