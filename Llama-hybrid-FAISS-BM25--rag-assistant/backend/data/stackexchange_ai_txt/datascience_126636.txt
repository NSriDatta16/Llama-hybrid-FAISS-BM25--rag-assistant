[site]: datascience
[post_id]: 126636
[parent_id]: 
[tags]: 
Clarifying the arguments of "Understanding the difficulty of training deep feedforward neural networks"

I made the decision to try to push through the paper "Understanding the difficulty of training deep feedforward neural networks" . (The paper is given as a reference in "Hands-On Machine Learning," in the chapter on training, and given its brevity and my math background I thought I'd try it on as an exercise.) I'm getting hung up at the following points: Chiefly, I can't understand the setup being introduced in section 4.2, the main part I was hoping to follow closely. The authors make mention of a ``linear regime" or linear activations, which confuses me given the focus on non-linear activation functions of the previous section; is their $f$ meant to be taken as linear? How, in particular, do equations $(4)$ and $(5)$ follow? (The paragraph preceding them as justification is very unclear to me.) Is the cost function fully generic, in this case? For completeness, what exactly is the variance of a gradient here? It doesn't appear we're using a covariance matrix (sample or otherwise) definition here, but rather elementwise coordinate variances, summed. Is this correct? If so, why is this done? I notice in Figure 2 of the paper that the apparent "variance" of the activation of the last layer seems to be quite low, heavily saturated around zero. I am trying to reconcile this with a comment from "HOML": In short, [the authors] showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. (I realize the statements aren't necessarily mutually exclusive, but how does "increasing variance" lead so suddenly to saturation at one end and narrow variance? Neither in "HOML" nor in the paper thus far has it become clear to me how the variance of the gradients (which again is not a fully clear notion to me, see (2)) bears directly on the saturation. This seems like it should be "obvious" but isn't clicking for me. What's the connection? (Thank you for any clarifications! :) )
