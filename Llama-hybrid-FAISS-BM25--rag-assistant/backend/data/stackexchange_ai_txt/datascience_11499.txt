[site]: datascience
[post_id]: 11499
[parent_id]: 11478
[tags]: 
You are predicting binary class: 0 - sale failed, 1 - sale succeed. In the decision tree, the information value is sorted. So for example in the first node, you have 35011 predictions of 0 and 1785 predictions of class 1. But then, in your code you have this: class_names=["Won","Lost"]) . So you are telling your decision tree that name of class 0 is "Won" and name of class 1 is "Lost". I assume it should be inverted. So in fact, the model always predict 0 - sale failed. Which seems reasonable as you said that out of 38000 samples, only 1700 are class 1. Further, your dataset is highly unbalanced. So when cross validation gives accuracy of 95%, it says nothing, because only if your model gives all prediction 0, you will have accuracy of...95%. For those cases, use models which can put weights to classes (SVM, Logistic regression or Random Forest classifier in scikit have this possibility). Think about undersample your dataset. Or add more of class 1 - dig in your database or do it artificially (SMOTE), Plot confusion matrix and give CV different score measure - precision or recall, depends on your situation. Last thing, don't expect that your decision tree with depth of 3 will perform well. It is nice to show to business, but it gives poor prediction.
