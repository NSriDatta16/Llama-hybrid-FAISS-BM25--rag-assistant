[site]: datascience
[post_id]: 65820
[parent_id]: 64080
[tags]: 
Here is a more complete article related to your question. The idea of pruning is simple and logical, however when looking at the bigger picture it is not as straight forward to implement. One of the main reason is that operations on the deeper layer depends on the previous layers and hence tampering with early layers might affect the next layers. Hence, if you were to prune one layer in the middle the following layers will have in some way pruned as well (I am not sure about the detail how to correctly implement this). As discussed above it is quite a pain to implement correctly ( and hence not a recommended practice_. A better alternative for this is knowledge distillation which is to make a smaller scale network but we train it such that it will mimic the output of a more complicated teacher model. There are tons of good example where this works e.g. Distillbert. Here is a nice link related to this which also contains tons of reference.
