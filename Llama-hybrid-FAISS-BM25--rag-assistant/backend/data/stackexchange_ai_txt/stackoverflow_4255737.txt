[site]: stackoverflow
[post_id]: 4255737
[parent_id]: 
[tags]: 
IDEAs: how to interactively render large image series using GPU-based direct volume rendering

I'm looking for idea's how to convert a 30+gb, 2000+ colored TIFF image series into a dataset able to be visualized in realtime (interactive frame rates) using GPU-based volume rendering (using OpenCL / OpenGL / GLSL). I want to use a direct volume visualization approach instead of surface fitting (i.e. raycasting instead of marching cubes). The problem is two-fold, first I need to convert my images into a 3D dataset. The first thing which came into my mind is to see all images as 2D textures and simply stack them to create a 3D texture. The second problem is the interactive frame rates. For this I will probably need some sort of downsampling in combination with "details-on-demand" loading the high-res dataset when zooming or something. A first point-wise approach i found is: polygonization of the complete volume data through layer-by-layer processing and generating corresponding image texture; carrying out all essential transformations through vertex processor operations; dividing polygonal slices into smaller fragments, where the corresponding depth and texture coordinates are recorded; in fragment processing, deploying the vertex shader programming technique to enhance the rendering of fragments. But I have no concrete ideas of how to start implementing this approach. I would love to see some fresh ideas or ideas on how to start implementing the approach shown above.
