[site]: crossvalidated
[post_id]: 513294
[parent_id]: 513283
[tags]: 
I'm assuming you've already explained that the goal of generative modelling is to estimate the probability distribution of the data $p(\mathbf{x})$ . This is generally done in three steps: Propose a probability distribution $q(\mathbf{x};\theta)$ that will be used to approximate the true distribution $p(\mathbf{x})$ by estimating the parameters $\theta$ given the observed data. Choose a method to estimate $\theta$ , such as maximum likelihood estimation. Estimate the parameters $\theta$ . The only difference between VAE's, GMM's, autoencoders, and really any other model occurs in step 1, since the distribution $q(\mathbf{x};\theta)$ is different in all of these. I personally think the step between GMM's and VAE's is too big when trying to implement them in practice, but theoretically, they just propose two different $q$ distributions, so this could be the connection between the two concepts. You can find a list of generative models here , but again the only difference between these are the choice of $q$ , and consequently the choice of conditional and unconditional independencies between random variables in the model.
