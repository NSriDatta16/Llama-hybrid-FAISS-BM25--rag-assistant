[site]: crossvalidated
[post_id]: 430869
[parent_id]: 388130
[tags]: 
I had the same question, and I think indeed is not explained very clearly in the paper, if you haven't seen features like that before. With global conditioning the idea is exactly as the paper mentions it to use a speaker embedding, which is yes, the one hot vector, that you have mentioned above. The main concept here is that is has to be something that is not a time series, or does not influence the input in a time-wise fashion. It could be gender of speaker, religion of speaker, mood of speaker, etc. The equation states that what you do then is have two inputs to your neural network, and the global one you just propagate through linear connections instead of convolutions. It is not an audio, not an image, so convolution does not really make sense , filtering one-hot vectors is not a good idea in general. With local conditioning , now you have a time series data. This could be also categorical, but then it's two dimensional, because it is a categorical time series across time. In TTS, this would be graphemes (written text) or phones (pronunciation of the written text), so that we learn a function which will be able to syntesise speech for. The problem is that the sampling rate of such signal is not necessarily the same. Their first solution was just to "repeat/broadcast values", but they report it have not worked well, so that is why you use a transposed convolution for upsampling and learning a good upsampled representation. For processing, a 1x1 convolution is used, which is I think for matching the dimensionality of the convolution filterbank.
