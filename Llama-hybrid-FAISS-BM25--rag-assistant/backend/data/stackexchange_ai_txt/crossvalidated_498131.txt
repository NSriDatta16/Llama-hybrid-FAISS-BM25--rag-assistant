[site]: crossvalidated
[post_id]: 498131
[parent_id]: 422211
[tags]: 
... and the data for all tasks are available. Continual learning (CL) is usually framed under the assumption that training data for previously seen tasks is not available for training on the current task. Under this assumption, "parallel multi-task training" (or joint-training as it is usually termed in CL literature) is presented as a sensible upper bound for performance of continual learning architectures: We included the following two baselines: None: The model was sequentially trained on all tasks in the standard way. This is also called fine-tuning , and can be seen as a lower bound. Offline: The model was always trained using the data of all tasks so far. This is also called joint training , and was included as it can be seen as an upper bound. Three scenarios for continual learning (2019) As shown in their results, the joint-training method does in this instance provide better performance to all models tested (table 4, table 5). Note: the assumption that this will provide an upper bound, while reasonable and empirically justified for current CL architectures, is not rigorously defended. It is imaginable that for sufficiently similar tasks, for a specific ordering of tasks (cf curriculum learning ), performance on previously seen tasks' test data may increase under a continual learning framework compared to parallel joint training: Joint Training is often used as an upper bound performance for CL. However, in the presence of a strong positive forward and backward transfer this may not be true. Continual Learning for RNNs: An Empirical Evaluation
