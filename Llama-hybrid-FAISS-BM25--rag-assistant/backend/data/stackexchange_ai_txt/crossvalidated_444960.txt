[site]: crossvalidated
[post_id]: 444960
[parent_id]: 
[tags]: 
GAN only generating a subset of image classes

Fairly new to training GANs, trying to train on MNIST using adversarially learned inference (based on this paper ), and am running into the following problem: the network seems to settle into a solution where it only generates images from a subset of the image classes (e.g. the generator will only produce 1's, 7's and 9's, no other digits). The digits that it does generate look realistic, but it would be ideal if it could cover the set of image classes more evenly. Still playing with hyperparameters to try and get better results, but I'm wondering, is this even something that one should generally expect from a GAN? Or is it necessary to use a conditional GAN to ensure that all classes are covered by the generator? What techniques are commonly used to try and ensure better coverage of the data distribution?
