[site]: crossvalidated
[post_id]: 15282
[parent_id]: 15281
[tags]: 
Ad 1. Your formula vaguely represents the variance of residuals, and related indicators (such as R²) are commonly used as quality criteria. R², which is commonly used to describe the goodness-of-fit of a model, compares the ratio of the variance of the score that is explained by the ELO-ranking to the variance that is not explained by it. The formula is: $R² =\frac{ESS}{TSS}$ while $ESS = \sum ($predicted score$ − $true score$)²$ $TSS = \sum ($true score$ − $average true score$)²$ This R² should be corrected to account for the number of parameters and observations: $\bar{R²} = 1-(1-R²)\frac{n-1}{n-p-1}$ where $n$ is the number of observations and $p$ is the number of parameters. One option is to use the model with the highest R², but others may know more about model selection. Ad 2. Why would you apply a visual method for parameter optimization? Why don’t you apply some sort of regression analysis to determine those parameter values that minimize the standard error of regression? It seems to me you already put quite a lot of effort in testing your rank (75 runs and a number of csv files and plots). If you specify how your new ranking scheme is calculated, and how the data you base it on is structured, I bet someone will walk you through the necessary steps to perform a statistical analysis. If you want to stick with visualisation, I would plot the values of your “residuals”, i.e. the predicted scores minus the actual ones, or distributions thereof. Finally, I wonder whether chess scores can be objectively assessed at all. If one player wins against another one, does this always mean he should have had better rating? I would be interested to know what your rating is based on.
