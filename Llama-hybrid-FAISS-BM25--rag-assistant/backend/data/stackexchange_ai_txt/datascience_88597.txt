[site]: datascience
[post_id]: 88597
[parent_id]: 58822
[tags]: 
Refer to this Github Gist . For convenience I copy-pasted the code from the link here (comments ommited): def get_f1_score(confusion_matrix, i): TP = 0 FP = 0 TN = 0 FN = 0 for j in range(len(confusion_matrix)): if (i == j): TP += confusion_matrix[i, j] tmp = np.delete(confusion_matrix, i, 0) tmp = np.delete(tmp, j, 1) TN += np.sum(tmp) else: if (confusion_matrix[i, j] != 0): FN += confusion_matrix[i, j] if (confusion_matrix[j, i] != 0): FP += confusion_matrix[j, i] recall = TP / (FN + TP) precision = TP / (TP + FP) f1_score = 2 * 1/(1/recall + 1/precision) return f1_score When you want to calculate F1 of the first class label, use it like: get_f1_score(confusion_matrix, 0) . You can then average F1 of all classes to obtain Macro-F1. By the way, this site calculates F1, Accuracy, and several measures from a 2X2 confusion matrix easy as pie.
