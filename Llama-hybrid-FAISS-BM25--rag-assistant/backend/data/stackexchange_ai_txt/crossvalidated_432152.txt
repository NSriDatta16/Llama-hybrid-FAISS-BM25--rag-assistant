[site]: crossvalidated
[post_id]: 432152
[parent_id]: 
[tags]: 
How to denote the number of parameters needed to define such a model where variables have different number of states with big O notation?

"Machine Learning: A Probabilistic Perspective by Kevin Patrick Murphy" in page 44 says A joint probability distribution has the form p(x1,...,xD) for a set of D > 1 variables, and models the (stochastic) relationships between the variables. If all the variables are discrete, we can represent the joint distribution as a big multi-dimensional array, with one variable per dimension. However, the number of parameters needed to define such a model is $O(K^D)$ , where K is the number of states for each variable. In this example Suppose that 3 balls are randomly selected from an urn containing 3 red, 4 white, and 5 blue balls. If we let X and Y denote, respectively, the number of red and white balls chosen, then the joint probability mass function of X and Y, p(i,j) = P{X=i,Y=j} p(0,0)=P(X=0,Y=0) means 0 red, 0 white and 3 blue balls. p(3,0)=P(X=3,Y=0) means 3 red, 0 white and 0 blue balls. All of variable ( $x_1$ , ..., $x_n$ ) have the same number of possible values (number of states), $K_1=...=K_D=4$ How to denote the number of parameters needed to define such a model where variables have different number of states? $K_1$ does not necessarily equal to $K_D$
