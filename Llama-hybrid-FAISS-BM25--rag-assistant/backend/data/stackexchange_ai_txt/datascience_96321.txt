[site]: datascience
[post_id]: 96321
[parent_id]: 
[tags]: 
Estimating the (normal?) distribution of the grayscale values of a region in camera footage

The problem: Given a small ( $25\times25$ ) region on a $1080$ p image extracted from camera footage of a railway system, I am trying to detect whether there is a train present or not. I proceed to take the grayscale average of this region across 2 minutes of footage (corresponding to about 1200 frames). This gives plots such as the following for one camera. Now if I can find the range of the part of the curve which corresponds to there being no train present, then I can infer that whenever the grayscale values do not lie in this range, there is a train present. What is a good way to find the range of this 'no-train' part of the curve? Currently, what I am doing is simply finding it by eye, but of course this isn't scalable. I was thinking to perhaps use the fact that the curve is normal? I am no expert in statistics, but when there is no train present, the only variation is due to measurement errors, so it isn't unreasonable to call it normal.
