[site]: crossvalidated
[post_id]: 318475
[parent_id]: 
[tags]: 
M-estimation for regression

An M-estimator $(\beta,\sigma)$ is defined as the parameters that minimize $$ \displaystyle \sum_{i=1}^n \rho\left(\dfrac{y_i -x_i^T \beta}{\sigma}\right)$$, for some robust function $\rho$. I understand that typically it's going to bound the influence of an outlier (observation with outlying $y$-part). Then they introduce some weight functions that give a low weight for an observation with an outlying $x$-part and implement it into the $\rho$ function (GM estimation). But why is that needed in fact? Good leverage points follow the pattern of the gross of the data, so they are in fact not bad for estimating the regression parameters? So how are they bad for estimation $\beta$?
