[site]: crossvalidated
[post_id]: 613041
[parent_id]: 365778
[tags]: 
Reduce the number of parameters in the model. The existing answers focus on different regularization strategies that can improve fit, given a model architecture that remains fixed (same configuration, number of layers, number of neurons in each layer). However, the simplest and easiest step to reducing overfitting in a neural network is to reduce the number of parameters in the model. This can mean some combination of fewer layers in the model; and fewer parameters in each layer. This can reduce overfitting because a model with a larger parameter count has a greater flexibility to fit the data. Intuitively, this is analogous to the simpler case of adding degrees of freedom to a linear model. A linear model with at least as many degrees of freedom as the number of observations can achieve a perfect fit to the training data, because it can interpolate between each training data point. However, this is unlikely to generalize well because, by perfectly interpolating the training data, the model has also fit to the noise in the target variable. From an overfitting perspective, the goal of adjusting the number of parameters in the model is to achieve the correct trade-off between achieving a good fit to the data and a fit that will generalize to new data.
