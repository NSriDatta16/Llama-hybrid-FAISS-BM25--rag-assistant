[site]: datascience
[post_id]: 25310
[parent_id]: 25309
[tags]: 
There is a bit too little information on the actual inputs, outputs and use-case to answer this question properly. However, I will try to give you some intuition for thoughts: What you are essentially doing is reducing the dimensions of the input. This is legit as you seem to be intending to do that: for each row you provide a vector of [1000x1] and want to get [400x1] that represents your original input the best way possible. Let's look at your architecture: In the hidden layer you are using a ReLu activation function, so your outputs from that layer will be sparse: zeroes and ones. Then you apply a linear layer, so your transformation on the output layer is of the format b*v+bias where b is the vector of the hidden layer and v the weight vector between hidden and output layer. This implies that v does not influence the zero elements of b, but solely the bias determines their values after the transformation. It's hard to tell whether that's good or bad. Anyway, you may want to think about trying out continuous functions at the hidden layer, such as a Sigmoid. Furthermore, using a uniform distribution for weight initialization of your network is not quite ideal in most cases. A truncated normal distribution is a common choice of initialization, however, again, it depends on your input and output data. Maybe you would like to provide more information on the data for us to help you better? If your goal is to get a more low-dimensional representation of your data and not care about the exact target values, you may want to look into AutoEncoders .
