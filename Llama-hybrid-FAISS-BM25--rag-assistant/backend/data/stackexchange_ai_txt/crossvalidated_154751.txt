[site]: crossvalidated
[post_id]: 154751
[parent_id]: 102852
[tags]: 
Robert's answer makes some important points, but here's another aspect: The statement that "feature scaling or weighting is important in surpervised learning" is not generally true. LDA estimates the within-class covariance and implicitly transforms data such that the covariance is $I$. Pre-scaling features will lead to accordingly scaled LDA weights, but the classification performance will be the same. LDA is invariant , not just under feature scaling, but under arbitrary invertible linear transforms. So in a way LDA does learn "optimal scales" (and "optimal coordinates") by itself. The same does not hold for SVM. The standard hard-margin SVM maximizes the width of the margin, and this width is measured as the Euclidean distance between the margin borders. The Euclidean distance is not invariant under non-orthogonal linear transforms including feature scaling, and therefore the maximal margin may be found to be reached for different classifier weights (which are not just scaled versions of SVM weights without pre-scaling). The SVM therfore does not learn "optimal scales" by itself. This means that the necessity to "avoid attributes in greater numeric ranges dominating those in smaller numeric ranges" therefore only holds for some supervised learning methods.
