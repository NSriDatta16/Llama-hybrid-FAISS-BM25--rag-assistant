[site]: crossvalidated
[post_id]: 477771
[parent_id]: 
[tags]: 
Multi-layer regression (with non-linearity in between): Iterated regression algorithm?

Suppose we observe $z_1,\dots,z_n\in\mathbb{R}$ and $x_1,\dots,x_n\in\mathbb{R}^b$ generated from the following model, for $i=1,\dots,n$ : $$y_i=x_i^\top \beta,$$ $$z_i={f(y_i)}^\top \gamma + \varepsilon_i,$$ $$\varepsilon_i\sim\mathrm{N}(0,\sigma^2),$$ where $f:\mathbb{R}\rightarrow\mathbb{R}^c$ is a known twice continuously differentiable function with a strictly positive derivative, $\beta\in\mathbb{R}^b$ and $\gamma\in\mathbb{R}^c$ , with $\beta>0$ and $\gamma>0$ (i.e. all elements are weakly positive, and at least one of each is strictly positive). (We do not observe $y_1,\dots,y_n$ .) For convenience, define $g_\gamma:\mathbb{R}\rightarrow\mathbb{R}$ , for $y\in\mathbb{R}$ , by: $$g_\gamma(y)={f(y)}^\top \gamma.$$ Since $f$ is continuously differentiable with a strictly positive derivative, and $\gamma>0$ , $g$ is also twice continuously differentiable with a strictly positive derivative, so has a continuously differentiable inverse by the inverse function theorem. Note that $z_i=g_\gamma(x_i^\top \beta) + \varepsilon_i$ , thus: $$g_\gamma^{-1}(z_i)=g_\gamma^{-1}\left({g_\gamma(x_i^\top \beta) + \varepsilon_i}\right) \approx x_i^\top \beta + (g_\gamma^{-1})'(g_\gamma(x_i^\top \beta))\, \varepsilon_i = x_i^\top \beta + [g_\gamma'(x_i^\top \beta)]^{-1}\, \varepsilon_i ,$$ where the approximation is valid for small $\varepsilon_i$ (the error is on the order of $\varepsilon_i^2$ ). Further note that: $$g_\gamma'(x_i^\top \beta)=g_\gamma'(g_\gamma^{-1}(z_i-\varepsilon_i))\approx g_\gamma'(g_\gamma^{-1}(z_i))- g_\gamma''(g_\gamma^{-1}(z_i)) [g_\gamma'(g_\gamma^{-1}(z_i))]^{-1}\, \varepsilon_i,$$ where the approximation is again valid for small $\varepsilon_i$ (the error is on the order of $\varepsilon_i^2$ ). Combining these two approximations gives: $$g_\gamma^{-1}(z_i)\approx x_i^\top \beta + [g_\gamma'(g_\gamma^{-1}(z_i))]^{-1}\, \varepsilon_i ,$$ where the error is again on the order of $\varepsilon_i^2$ as the $ g_\gamma''(g_\gamma^{-1}(z_i)) [g_\gamma'(g_\gamma^{-1}(z_i))]^{-1}\, \varepsilon_i$ term in the denominator only has a second order effect. This suggests the following iterative algorithm: Choose some initial $\gamma_0$ . Set $k:=1$ . Estimate $\beta_k$ via the (non-negative constrained, heteroskedastic) regression: $g_{\gamma_k}^{-1}(z_i) = x_i^\top \beta + [g_{\gamma_k}'(g_{\gamma_k}^{-1}(z_i))]^{-1}\, \varepsilon_i$ . (This is a simple quadratic programming problem.) Set $y_{i,k}:=x_i^\top \beta_k$ . Estimate $\gamma_k$ via the (non-negative constrained) regression: $z_i={f(y_{i,k})}^\top \gamma + \varepsilon_i,$ If $\beta_k$ and $\gamma_k$ have converged then stop, otherwise set $k:=k+1$ and return to step 2. Questions: Is there any reason to think this algorithm converges? Is it a known algorithm? Am I sensible to make the additional approximation described above of $g_{\gamma_k}'(x_i^\top \beta_k)$ by $g_{\gamma_k}'(g_{\gamma_k}^{-1}(z_i))$ , which ensures that the Jacobian term in the likelihood coming from the change of variables is not a function of $\beta_k$ ? Would it be better to use $g_\gamma'(x_i^\top \beta_{k-1})$ ? I am tagging this post with "expectation-maximization" as although it is not obvious to me that this is an EM algorithm, I expect that if it works it will probably turn out to be one. I am also tagging this post with "neural-networks" as the structure here is essentially identical to that of a neural network with a single non-linear layer.
