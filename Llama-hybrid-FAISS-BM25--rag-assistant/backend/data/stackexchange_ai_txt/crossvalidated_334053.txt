[site]: crossvalidated
[post_id]: 334053
[parent_id]: 334049
[tags]: 
If you work with continuous data, you might want your generator (or autoencoder, note your GAN paper mentions using encoder-decoder architecture) to be robust to small noise, since corrupted input is similar to original input (for example noisy images still look alike). I think this idea was first proposed for autoencoders in Extracting and Composing Robust Features with Denoising Autoencoders . The gist of it is that in order to regularize your generative model you learn to reconstruct inputs from corrupted versions. For a simpler version of this, recall that MSE linear regression can be interpreted as a Maximum Likelihood procedure for $\beta$ where $y = X\beta + \epsilon$, $\epsilon$ being normally distributed noise.
