[site]: crossvalidated
[post_id]: 423325
[parent_id]: 
[tags]: 
Analytical unbiased estimates for bootstraping

I've realized that calculating average of a central statistic over all possible bootstrap samples is equivivalent to using an unbiased version of it (sample central). Meaning that given a sample $\vec{X}=\{{X_1,\dots,X_n}\}$ with a function $f(\vec{X}):\mathbb{R}^n\rightarrow\mathbb{R}$ you get: $\mathbb{E}_{\vec{x}\in B(\vec{X})}[f(\vec{x}-\bar{x})]=\hat{f}(\vec{X} - \bar{X})$ Where $B(\vec{X})$ is a set of all possible $n^n$ bootstrap samples of $\vec{X}$ and $\hat{f}$ is an unbiased version of $f$ . In other words instead of calculating $f$ for each bootstrap sample or random subsample of the set (as it usually done, because $n^n$ gets huge very easily), one can derive $\hat{f}$ instead. Classic example is variance $f(\vec{x})=m_2=\frac{1}{n}\sum x_i^2$ with $\hat{f}=\frac{n-1}{n}f$ . And similiar expressions for other moments $m_k=\frac{1}{n}\sum x_i^k$ exist. Further this can be extended to Taylor expansion of any $f$ , for example it's easily could be applied to logistic regression and neural nets to derive robust unbiased $\Delta W$ to do gradient descent. The downside is though it requires calculcating multivariate central moments of your data and there are lot of them with dimensionality going up. But certainly for low dimensions and easy enough model it is more efficient and precise than bootstrapping, or is it? I just asking maybe for more information on this 'technique', is it practical or not.
