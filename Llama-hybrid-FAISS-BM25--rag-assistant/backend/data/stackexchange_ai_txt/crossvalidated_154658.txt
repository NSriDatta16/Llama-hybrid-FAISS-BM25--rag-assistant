[site]: crossvalidated
[post_id]: 154658
[parent_id]: 
[tags]: 
Probabilistic Derivation of Linear Regression Estimating Equations

So this has been something I've been studying for some time. The first time I studied it I glossed over the details and parts that I really didn't understand. Now I'd like to tackle these points. My sources of study are Andrew Ngs CSC229 lecture notes and Bishops Pattern Recognition and Machine Learning Textbook. So we have $y(i) = dot(θ, x_i) + ε_i$, where θ is the parameter vector, and x_i the ith feature vector. We assume ε_i be the noise of the model, and let it be gaussian i.e. $ε(i) ∼ N (0, σ^2)$. Now I'm confused how we can infer the following: $y_i | x_i; θ ∼ N (dot(θ, x_i), σ^2).$ Thats my first uncertainty. Second, the way the likelihood function is defined confuses me a bit. Its defined as $L(θ) = p(y|X; θ)$. Now, is the likelihood function always defined like this. Like from Bayes theorem, we have $p(w|D) ~ p(D|w)p(w)$ where D is the observed data $D = {t1, . . . , tN }$ and w is the parameters. The Bishop textbook defines p(D|w) as the likelihood function. So would D in our context be y and w be θ. I guess I'm just confused about all the variables etc.
