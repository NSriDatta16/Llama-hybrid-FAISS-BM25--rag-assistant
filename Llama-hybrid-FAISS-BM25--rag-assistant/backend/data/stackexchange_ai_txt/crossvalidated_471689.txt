[site]: crossvalidated
[post_id]: 471689
[parent_id]: 471679
[tags]: 
You need adequate dimension reduction to avoid overfitting . Overfitting comes from maintaining too many fully weighted features in a model, regardless of the particular modeling approach used. I'll focus here on regression techniques. LDA and SVM might or might not work better than regression in your case, but that's not because of differences with respect to overfitting. PCA sits well within the classic set of tools for dimension reduction in ordinary and logistic regression. To avoid overfitting with PCA-based regression techniques you just don't retain all the principal components. In your case you might retain on the order of 3 to 5 components.* It has the advantage that you don't throw away all of the information from any your original 58 features, you just re-weight them according to their contributions to the retained principal components. When predictors are highly correlated, as seems likely in your study, PCA-based approaches protect you from getting results that are highly dependent on the sample at hand. The correlated predictors tend to be represented in the same principal components. So if one of a pair of correlated predictors happens to be most important in your data sample, there's a good chance that the corresponding principal component will still do well in a sample where the other of the pair is dominant. Selection of a subset of the original features, as you propose, loses that advantage. Standard principal-components regression does make an all-or-none choice of retained components. The method of ridge regression can be thought of as retaining all the principal components but weighting them differentially. That relative weighting penalizes the magnitudes of the regression coefficients of the original features to avoid overfitting. For prediction in cases like yours, with a moderate ratio of cases to features, that can be a very useful choice, whether for logistic or ordinary regression. Cross-validation is typically used to choose the level of penalization in a way that minimized overfitting. So PCA is perfectly acceptable as a way to get the dimension reduction you need, however you happen to apply it. An Introduction to Statistical Learning is a reasonably accessible reference for further study on these and many other topics. *To avoid overfitting in typical biomedical studies you should retain about 1 fully weighted predictor per 15 cases of the minority class in logistic regression, and 1 per 15 total cases in ordinary regression.
