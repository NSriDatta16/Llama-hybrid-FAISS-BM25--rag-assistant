[site]: crossvalidated
[post_id]: 635607
[parent_id]: 60484
[tags]: 
To answer this question, let's imagine you are doing an experiment. Maybe to measure the weight of a ball. Let's assume we don't know the true/precise weight of the ball. Of course, your measurement device and conditions are not perfect and it has some errors due to various reasons. So, you performed 5 experiments and you got these values - 1 kg, 1.1 kg, 0.9 kg, 0.94 kg, and 1.01 kg. Now, what does standard deviation tell you? It tells you how scattered your measurements are. Assuming these experiments are normally distributed around true weight (can be 1 kg in our case), standard deviation tells us the expected error in your single measurement. Now suppose you want to reduce error in your measured weight. But, you can not change your measuring device or improve your measurement conditions. So, what's the solution? The solution is you can take the average of your individual experiments. Since, your measurement errors are normally distributed around the true weight, the positive and negative errors will cancel out if you take the average and the average of all 5 measurements will be more precise than any individual experiment. But the question is can you quantify the approximate difference between the average of your 5 values and the true weight? The answer is yes and this is what your standard error gives. So, what if you are not satisfied with the precision of measurement that you get by taking the average over 5 experiments and you want to further improve the precision. In that case, you can perform 5 more experiments. Now, if you take average over these 10 experiments, your average measured weight will be closer to the true weight than the average of 5 experiments. It's intuitive that if you continue increasing the number of individual experiments, your precision will continue to increase. But wait. What about your return on investment/efforts? Indeed conducting an experiment is not an easy task. It requires your manual efforts. If you move from 1 experiment to the average of 5 experiments, you are able to improve precision by say 10%. Then if you add 5 more experiments, your precision will not increase by 10% but somewhat less than that. and if you add 5 more in addition to the previous one, your increase in precision will be further lesser. and so on. Now the question is can you quantify the decrease in return on investment? The answer is yes. The precision is approximately proportional to the square root of the number of experiments. Let's see it graphically with a Python experiment. Let's sample 100000 random values between 0 to 1. Ideally, if these random values are distributed uniformly between 0 and 1, the expected value of the mean is 0.5. For finite sample sizes, there will be a finite difference (let's call it 'error') in the mean of the sample and the expected value of 0.5. This error should decrease as we increase the sample size. If we plot this error, against the sample size, it will look like the blue line in the following graph (Note: here this error is averaged over many experiments to reduce the fluctuations). Compare it with the square root of the number of experiments shown in orange color. We can see that both of them have similar shapes. In fact, if we plot the ratio of error and sqrt(n), it is a constant (shown with the green line). Woww!! That means the error in the measured mean indeed decreases as square root of the number of experiments. That's what the standard error formula told us. Interestingly, if you do some other experiment, the ratio might change, but the error will still be proportional to 1/sqrt(n). Hope, now you have an intuition of standard error and the sqrt(n) in it. The code to generate the above picture is this: import numpy as np from matplotlib import pyplot as plt def plot(err, b=0,c='max'): n = len(err) if(type(c)==str): if(c=='max'): c = n x = np.arange(1,n+1) sqrtn = 1/np.sqrt(np.arange(1,n+1)) plt.plot(x[b:c], err[b:c], label='error') plt.plot(x[b:c], sqrtn[b:c], label='sqrt(n)') ratio = err/sqrtn plt.plot(x[b:c], ratio[b:c], label='error/sqrt(n)') plt.xlabel('Number of random number samples') plt.legend(loc='best') plt.show() def calc_err(n, seed=777001): np.random.seed(seed=seed) cum_n = np.arange(1,n+1) x = np.random.rand(n) cumsum = np.cumsum(x) cummean = cumsum/cum_n mu = 0.5#x.mean() #print(mu) err = np.absolute(cummean-mu) return err n = 100000 seed = 77701 seed_count = 1000 err_sum = np.zeros(n) for seed in range(77700, 77700+seed_count): err = calc_err(n, seed) err_sum = err_sum+err err = err_sum/seed_count plot(err, b=1, c='max')
