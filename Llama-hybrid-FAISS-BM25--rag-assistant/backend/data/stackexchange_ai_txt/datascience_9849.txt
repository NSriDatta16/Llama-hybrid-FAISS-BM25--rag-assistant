[site]: datascience
[post_id]: 9849
[parent_id]: 
[tags]: 
How to implement a BiLSTM to classify speech data

I would like to know how should I provide the inputs to a BiLSTM if I am going to classify speech files (.wav) files. What is the proper way to label the data? Do I have to label each individual frame with the target label (which I think is meaningless because content of a single frame doesn't imply that the end label of the given file is what it is. Eg: First frame of utterance_0.wav doesn't directly say output label is 0) Therefore what would be the correct way of labelling these data? And how should I maintain the .mfcc values calculated per each .wav files. (I have around 4500 files and trying to generate coefficients when we try to do the training would add extra overhead) Please correct me if I am wrong. Since I am new to this ML approaches and training networks and RNN, I would like to have some guidance on above questions.
