[site]: crossvalidated
[post_id]: 23571
[parent_id]: 23569
[tags]: 
Rather than asking " when use" let's look at " why use" - I believe this nicely leads us to the "when" answer. My understanding is that dimensionality reduction is mainly done to speed up learning (many features lead to longer computations) and compress data (many features take a lot of disk/memory space). In this view, you should reduce dimensions only if running time or data size is "unacceptable", and you reduce the feature space until things become "acceptable". "Unacceptable" is, obviously, defined solely by the task at hand. Modern computers can handle a lot of computations and store a lot of data - which is why, I think, you was told that 500 features is not too much. There are few other reasons for dimensionality reduction I can think of: matrix inversion problems - an algorithm can build a matrix from sample set, and if some features are interdependent this makes the marix non-invertible. But in practice it's not a big deal and gets circumvented via Moore-Penrose pseudoinverse so, in my view, this one should not be the reason for dimensionality reduction. data visualization - the rule of thumb here is to extract features until you're left with a maximum of two, due to a deficiency in human cognition :)
