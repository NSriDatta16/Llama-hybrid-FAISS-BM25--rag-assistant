[site]: crossvalidated
[post_id]: 542122
[parent_id]: 
[tags]: 
Conditional variance of the absolute sum of zero-mean i.i.d. random variables

Why does the following conditional variance formula hold, and does it hold in general? $$\text{Var} \left( \left \vert \sum^n_{i=1} U_i \right \vert \, \middle | \,D_n \right) = n \text{Var}(U_i \vert D_n).$$ Here $U_i$ are zero-mean independent and identically distributed random variables, and nothing more about their distribution is stated. Further information on $U_i$ and $D_n$ is provided below. I am comfortable with the standard result that for i.i.d. random variables $X_i$ , $$\text{Var} \left( \sum^n_{i=1} X_i \right) = \sum^n_{i=1} \text{Var}(X_i) = n \text{Var}(X_i),$$ but the presence of the absolute values in the formula of interest has thrown me. Further information on $U_i$ and $D_n$ . In response to the helpful comments and answer by James Martin and Ben , it is clear that the equation does not hold in general, and that additional context on the $U_i$ and $D_n$ is required. Here is an extract of the proof where the equation is used: Where $D_n = {(X_1, Y_1,), \dots, (X_n, Y_n)}$ and $D_n' = (X_1', Y_1'), \dots (X_n', Y_n')$ are two sets of i.i.d. random variables sampled from unknown joint distribution $D_n, D_n' \overset{i.i.d}{\sim} P(X, Y)$ . The conditional expectation in $U_i$ is such that $\mathbb{E}_{D_n' | D_n}[\cdot \vert D_n]$ . $f: \mathbb{R}^d \rightarrow \{0, 1 \}$ is a classifier function , while $\tilde{f}$ is a random classifier function, dependent on $D_n$ . The argument comes from the proof of the Vapnik-Chervonenkis inequality in an expository note by Robert Nowak (2009) , following the strategy of Devroye, Gy√∂rfi and Lugosi (1996) in their proof of Glivenko-Cantelli theorem.
