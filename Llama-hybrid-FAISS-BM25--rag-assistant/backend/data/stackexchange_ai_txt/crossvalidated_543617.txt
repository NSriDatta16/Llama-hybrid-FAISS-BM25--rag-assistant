[site]: crossvalidated
[post_id]: 543617
[parent_id]: 
[tags]: 
Number of weights to be learnt in the encoder decoder attention in the transformer model

I have a doubt about the number of weights to be learned in the encoder-decoder attention layer in the transformer model (attention is all you need). Some blogs articles say the $K$ and $V$ (key and value) matrices come from the memory (encoder output). This confused me. But If I'm right there should be: 8 matrices $W_K$ 8 matrices $W_V$ 8 matrices $W_Q$ 1 matrix $W_O$ Right?
