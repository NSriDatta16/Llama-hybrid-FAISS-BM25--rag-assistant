[site]: crossvalidated
[post_id]: 590202
[parent_id]: 590188
[tags]: 
It is hard to give a qualified answer without the data, so I'll have to speculate: $100 \times 100$ is a $10,000$ -dimensional feature space (and that's if we're taking greyscale only). You have no more than $300 \cdot 18 = 5400$ vectors in that space, which is terribly few. Classification based on raw pixels values only is likely close to impossible. Again, it depends on your data: It might work if the images are highly standardised, like the handwritten letters in the MNIST dataset, but unlikely for natural photographs. So, the first thing I'd do would be preprocessing: Ensuring that the faces in the images occupy the same positions, are of the same size, in the same dynamic range etc. Next, I'd identify (actually, train a segmentation algorithm to identify) relevant parts of the image---eyes, mouth, nose, cheek...---to use as the features. I'd try to radically reduce the dimensionality of these features, e.g. represent the mouth as a curve or two, defined by its/their starting and ending points and the curvature. The final classifier (SVM) should work on these features, not on directly on the pixels.
