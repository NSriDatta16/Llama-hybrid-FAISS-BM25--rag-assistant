[site]: crossvalidated
[post_id]: 534356
[parent_id]: 
[tags]: 
Are Random Forests trained with the whole dataset?

I was reading "Hands On Machine Learning" by Aurelien Geron, and the following text appeared: As we have discussed, a Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. The max_samples argument defines how many elements of the data set go into each model trained, in this case, to the Decision Tree Classifiers. My question is, how does this make sense? Because in theory, if we passed the whole training set to each individual model, wouldn't all the models be exactly the same? Wouldn't it make more sense to train each model with a random part of the data set, in order to get completely different classifiers each time? So it can accurately do predictions. This is how you would create a Random Forest: from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier bagging_classifier = BaggingClassifier( base_estimator = DecisionTreeClassifier(), # Model to use n_estimators=500, # Number of models to train max_samples=100, # Amount of samples to train each model (Putting the length of the whole dataset is what he's proposing) )
