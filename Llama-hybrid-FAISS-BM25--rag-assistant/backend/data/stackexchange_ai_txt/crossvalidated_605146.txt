[site]: crossvalidated
[post_id]: 605146
[parent_id]: 
[tags]: 
Deterministic formula for average number of unique items picked

I am curious how to formulate a deterministic answer to a problem I have in mind. I have computed it stochastically, but am unsure of how to frame and compute the problem deterministically. My prob & stats classes are a few decades behind me... Let's say that we have 100 people, and each person is asked to choose a number between 1 and 100 (inclusive). Maybe everyone chooses the same number, but this is unlikely. It is also unlikely that everyone chooses a different unique number. How many of the numbers from 1 to 100 will get picked? Stochastically, it seems that somewhere between 59 and 67 of the numbers will be picked (10th percentile and 90th percentile respectively) with a median example resulting in 63 of the numbers being chosen. Is there a deterministic way calculate this? What concepts are relevant? How does the formula vary for a different number of people? And for a different number of numbers to choose? Update : I'll include the Python script I ran to clarify what I meant by "Stochastically, it seems...": import random import statistics trials = 10000 n = 100 m = 100 lens = [] for j in range(trials): k = [] for x in range(n): k.append(random.randint(1,m)) lens.append(len(list(set(k)))) print(statistics.median(lens), statistics.quantiles(lens, n=10))
