[site]: datascience
[post_id]: 121387
[parent_id]: 
[tags]: 
Help with trying to reimplement ADEC paper (clustering using AE and GAN). Encoder loss is not decreasing

I am trying to reimplement the ADEC paper ( https://arxiv.org/abs/1909.11832 ) which mixes an autoencoder with a GAN network, but I am facing the issue that the encoder loss does not decrease. I have successfully implemented the pretraining phase and am getting the results that I would expect, the finetuning phase however does not seem to work yet. The results before the finetuning phase are actually better than after, which can be observed when reducing the dimensionality with t-SNE and showing the embeddings in a plot. The used dataset is MNIST and the colors are the true labels. After pretraining: After finetuning: My current implementation for the finetuning phase is the following: class ADECClustering(tf.keras.Model): def __init__(self, encoder, decoder, discriminator, initial_centers, alpha=1, **kwargs): super().__init__(**kwargs) self.encoder = encoder self.decoder = decoder self.discriminator = discriminator self.update_qp = True self.train_decoder_only = True self.mse = tf.keras.losses.MeanSquaredError() self.centers = tf.Variable(initial_value=initial_centers, trainable=True) self.alpha = alpha self.KL = tf.keras.losses.KLDivergence() def calculate_Q(self, embeddings): # Copied from DEC implementation (https://github.com/fferroni/DEC-Keras/blob/master/keras_dec.py) q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(embeddings, 1) - self.centers), axis=2))**2 /self.alpha) q = q**((self.alpha+1.0)/2.0) q = K.transpose(K.transpose(q)/K.sum(q, axis=1)) return q def calculate_P(self, Q): # Copied and slightly adapted from DEC implementation (https://github.com/fferroni/DEC-Keras/blob/master/keras_dec.py) weight = Q**2 / K.sum(Q, axis=0) return K.transpose(K.transpose(weight) / K.sum(weight, axis=1)) def calculate_encoder_loss(self, images): embeddings = self.encoder(images) Q = self.calculate_Q(embeddings) P = self.calculate_P(Q) KL_term = self.KL(P, Q) x_rec = self.decoder(embeddings) d_out = self.discriminator(x_rec) expectation_term = tf.math.reduce_mean(tf.math.log(tf.clip_by_value(1-d_out,1e-10,1.0))) return KL_term + expectation_term, embeddings, KL_term def calculate_decoder_loss(self, images, embeddings): x_rec = self.decoder(embeddings) total_loss = tf.math.reduce_mean(self.mse(images, x_rec)) return total_loss, x_rec def calculate_discriminator_loss(self, images, x_rec): component_1 = tf.math.reduce_mean( tf.math.log(tf.clip_by_value(self.discriminator(images),1e-10,1.0)) ) component_2 = tf.math.reduce_mean( tf.math.log(tf.clip_by_value(1-self.discriminator(x_rec),1e-10,1.0)) ) total_loss = component_1 + component_2 # Negative loss, because the paper uses gradient ascent in the algorithm outline. return -total_loss def train_step(self, images): with tf.GradientTape() as encoder_tape: encoder_loss, embeddings, KL_term = self.calculate_encoder_loss(images) if not self.train_decoder_only: train_vars = [ self.encoder.trainable_variables, [self.centers] ] grads = encoder_tape.gradient(encoder_loss, train_vars) tv_list = [] for (grad, var) in zip(grads, train_vars): for g, v in zip(grad, var): tv_list.append((g, v)) self.optimizer.apply_gradients(tv_list) with tf.GradientTape() as tape: decoder_loss, x_rec = self.calculate_decoder_loss(images, embeddings) train_vars = [ self.decoder.trainable_variables ] grads = tape.gradient(decoder_loss, train_vars) tv_list = [] for (grad, var) in zip(grads, train_vars): for g, v in zip(grad, var): tv_list.append((g, v)) self.optimizer.apply_gradients(tv_list) if not self.train_decoder_only: with tf.GradientTape() as tape: discriminator_loss = self.calculate_discriminator_loss(images, x_rec) train_vars = [ self.discriminator.trainable_variables ] grads = tape.gradient(discriminator_loss, train_vars) tv_list = [] for (grad, var) in zip(grads, train_vars): for g, v in zip(grad, var): tv_list.append((g, v)) self.optimizer.apply_gradients(tv_list) if self.train_decoder_only: return {"decoder_loss": decoder_loss} else: return {"encoder_loss": encoder_loss, "KL_term": KL_term, "decoder_loss": decoder_loss, "discriminator_loss": discriminator_loss} After pretraining I am training the discriminator for a few epochs and then start the finetuning process using the model above. During finetuning, the encoder loss seems to increase instead of decreasing. What could be the reason for this? I have also tested a very weak discriminator. When doing so, the discriminator losses and encoder losses seem to fluctuate a lot, but the final result is still terrible. The optimizer used for finetuning is SGD with a learning-rate of 0.001 and momentum of 0.9 as specified in the paper. The encoder and decoder have a layout of n-2000-500-500-10 and 10-500-500-2000-n, respectively. I am also training the decoder on its own after each epoch, similarly to the paper. Even when only the KL-term is used for the encoder loss instead of taking the discriminator into account as well, the results seem to become worse, even though the KL-term is decreasing: After further investigation I have found out that all samples get assigned to the same cluster even after training for only one epoch. However, I still do not know why this is the case.
