[site]: stackoverflow
[post_id]: 4071890
[parent_id]: 4070650
[tags]: 
I don't understand one detail of this code: the queue is marked as task_done if there is any new link found in self.queue_links , but not as a matter of course in self.crawl . I'd have thought that this code would make more sense: def crawl(self, tgt): try: url = urlparse(tgt) self.crawled_links.append(tgt) print("\nCrawling {0}".format(tgt)) request = urllib2.Request(tgt) request.add_header("User-Agent", "Mozilla/5,0") opener = urllib2.build_opener() data = opener.open(request) doc = parse(data).getroot() for tag in doc.xpath("//a[@href]"): old = tag.get('href') fixed = urllib.unquote(old) self.queue_links(fixed, url) self.queue.task_done() except: # TODO: write explicit exceptions the URLError, ValueERROR ... pass def queue_links(self, link, url): if not link.startswith("#"): if link.startswith('/'): link = "http://" + url.netloc + link elif not link.startswith("http"): link = "http://" + url.netloc + "/" + link if link not in self.crawled_links: self.queue.put(link) I can't say, though, that I have a complete answer to your question. Later: the docs for Queue.task_done suggest that task_done should be 1:1 with Queue.get calls: Queue.task_done()Â¶ Indicate that a formerly enqueued task is complete. Used by queue consumer threads. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete. If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue). Raises a ValueError if called more times than there were items placed in the queue. Were you getting [uncaught] ValueError exceptions? It looks like this might be so.
