[site]: datascience
[post_id]: 33525
[parent_id]: 33524
[tags]: 
Not sure if I can get the direction of your "why", but here's a try: If you were to use some sort of interpolation, you would have an explicit model of this function in the mathematical sense. This would be a white box, as you actually have the formula/algorithm to do the prediction. If you use a conventional decision tree or linear regression you are still in the same white-box category in the sense that your trained model can be expressed by a manageable set of formulas or rules. However, in typical machine learning scenarios there are at least two factors that limit the transparency of this box: We care more of the result, and less of the explanation. Thus, we might use different approaches. As long as they fit in the same "interface" as you define it with your function, we don't really care which one is being used. This allows for A/B testing of different approaches and putting in place the one that currently yields best results. After some time, based on new training data or on new advances in machine learning, we can decide to use another algorithm, but the general setup remains the same. Contemporary algorithms are extremely complex, both because the dimensions of the input and output are much higher than our intuition can cope with, and (inherently) because they use many more iterations. Examples of these iterations could be the layers of deep networks, but also the range of algorithms used in ensemble methods like random forests.
