[site]: crossvalidated
[post_id]: 424537
[parent_id]: 
[tags]: 
derivative of cross entropy yields log-odds, does that make sense?

I am looking for a proof how to derive the logistic regression from cross-entropy loss, i.e. derive the form of a sigmoid from cross entropy. my thoughts are these: $\ell = y_i \ln{p_i} + (1-y_i)\ln{(1-p_i)}$ $\frac{\partial \ell}{\partial y_i} = \ln{p_i} - \ln{(1-p_i)} = \ln{\frac{p_i}{1-p_i}}$ and after setting this equal to linear predictor $x'\beta$ I can end up with the sigmoid function. However my question is, what does the derivation of likelihood w.r.t. $y_i$ mean and can I do these operations? In every literature there is only written that we take $\ln{\frac{p_i}{1-p_i}}$ as a link function because it transforms input from $(0,1)$ to real numbers which are desired, but I can not find a connection with cross entropy or Kullback-Leibler Divergence. Thanks.
