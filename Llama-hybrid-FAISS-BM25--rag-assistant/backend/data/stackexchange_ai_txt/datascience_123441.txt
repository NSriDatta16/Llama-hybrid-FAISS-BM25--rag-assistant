[site]: datascience
[post_id]: 123441
[parent_id]: 69440
[tags]: 
You created a bottleneck by reducing dimensions and than upscaling later. Bottlenecks are normal in neural nets, we used them in our CNN-s when working with images and audio. However, using them in linear classifiers is not really something I can say I saw before. Your data is noisy by nature, text is famously hard to work with (because of the way meaning in text is built over multiple sentences and phrases that can be changed by a single word three sentences later) so making a bottleneck is the last thing I would advise you to do... especially when using only one hidden layer - make the first hidden layer a lot wider than the first one (let's say, 4096 and bigger) and then start slowly cutting it in half until you get to your last layer. You are doing a classification, so the output layer should have a SoftMax activation function and not a linear one. The dropout could be a bit bigger, put it around 0.33 and up to 0.5 (your dropouts of 0.2 and 0.1 are for CNNs and RNNs). I hope your data is more than 500k objects in size, even that could happen to be not sufficient for the task at hand given you are working with text, represented as an 1200-elements array with 500 different classes.
