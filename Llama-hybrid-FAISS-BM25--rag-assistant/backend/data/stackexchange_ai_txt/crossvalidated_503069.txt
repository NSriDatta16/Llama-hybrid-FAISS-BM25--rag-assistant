[site]: crossvalidated
[post_id]: 503069
[parent_id]: 
[tags]: 
How are missing values exactly handled in C4.5 decision trees?

I quote Tom M. Mitchell's words on this topic: " A second, more complex procedure is to assign a probability to each of the possible values of A rather than simply assigning the most common value to A(x). These probabilities can be estimated again based on the observed frequencies of the various values for A among the examples at node n. For example, given a boolean attribute A, if node n contains six known examples with A = 1 and four with A = 0, then we would say the probability that A(x) = 1 is 0.6, and the probability that A(x) = 0 is 0.4. A fractional 0.6 of instance x is now distributed down the branch for A = 1, and a fractional 0.4 of x down the other tree branch. These fractional examples are used for the purpose of computing information gain and be further subdivided at subsequent branches of the tree if a second missing value must be tested. This same fractioning of examples can also be applied after learning, to classify new instances whose attribute values are unknown. In this case, the classification of the new instance is simply the most probable classification, computed by summing the weights of the instance fragments classified in different ways at the leaf nodes of the tree. This method for handling missing attribute values is used in C4.5 (Quinlan 1993). " Now, my questions are: How these fractional 0.6 and 0.4 used to calculate information gain at nodes further down the tree? How would the subdivision take place if a second missing value occurs? How this fractioning would be exactly used in classifying the new examples?
