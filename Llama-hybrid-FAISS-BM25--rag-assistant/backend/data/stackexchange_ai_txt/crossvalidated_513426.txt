[site]: crossvalidated
[post_id]: 513426
[parent_id]: 
[tags]: 
How to deal with a PDF that takes CDF data as input? (Metalog distributions)

I am trying to utilise the Metalog distribution in a Machine Learning project. For this project, I need to be able to obtain likelihoods using the PDF of the distribution. https://en.wikipedia.org/wiki/Metalog_distribution#Probability_density_function Here is the PDF function, taken from the Wikipedia page above: $ m_k(y)= \left\{ \begin{array}{ll} {y(1-y)\over{a_2}} & \mbox{for } k=2\\ \Bigl({a_2\over{y(1-y)}}+a_3\Bigl({1-y\over{y(1-y)}}+\ln{y\over{1-y}}\Bigr)\Bigr)^{-1} & \mbox{for } k=3\\ \Bigl({a_2\over{y(1-y)}}+a_3\Bigl({1-y\over{y(1-y)}}+\ln{y\over{1-y}}\Bigr)+a_4\Bigr)^{-1} & \mbox{for } k=4\\ \Bigl({1\over{m_{k-1}(y)}}+a_k{k-1\over2}(y-0.5)^{(k-3)/2}\Bigr)^{-1} & \mbox{for odd } k\geq5\\ \Bigl({1\over{m_{k-1}(y)}}+a_k\Bigl({(y-0.5)^{k/2-1}\over{y(1-y)}}+({k\over2}-1)(y-0.5)^{(k/2-2)}\ln{y\over{1-y}}\Bigr)\Bigr)^{-1} & \mbox{for even } k\geq6,\\ \end{array}\right. $ As we can see, the PDF is written in terms of cumulative probability the $y$ instead of the usual input variable $x$ . For fitting I believe I need to compute the Empirical CDF function, e.g. by sorting the data and assigning the $i^{th}$ datapoint the cumulative probability $y=\frac{i}{n}$ where $n$ is the number of datapoints (please correct me if I'm wrong). But what about new datapoints after the model has been fitted? What is the "proper" way of taking a new datapoint $x$ and computing the probability density $f(x)$ ? Should I keep the entire training set so I can calculate its new ECDF every time? Should I create a discrete lookup table for $y$ and assign my new value to the closest point? Or is the Metalog just a poor choice? If so, can anyone recommend a distribution with similar flexibility (the main requirement is to be more flexible than a Beta / Dirichlet distribution)?
