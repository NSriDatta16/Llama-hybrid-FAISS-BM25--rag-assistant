[site]: crossvalidated
[post_id]: 395225
[parent_id]: 
[tags]: 
Parameter Transformation Bayesian Learning

I read that given a parameter $\theta$ and a transformation $\phi = g(\theta)$ (where $\theta$ is the parameter of your prior distribution), the distribution of the transformed parameter would be: $p_{\phi}(\phi) = p_{\theta}(g^{-1}(\phi))|\frac{d\theta}{d\phi}| = p_{\theta}(\theta)|\frac{d\theta}{d\phi}|$ Can someone explain to me how the second part of the RHS equation occurs, ie $|\frac{d\theta}{d\phi}|$ ?
