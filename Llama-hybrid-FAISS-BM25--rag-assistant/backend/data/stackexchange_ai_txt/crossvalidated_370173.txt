[site]: crossvalidated
[post_id]: 370173
[parent_id]: 370165
[tags]: 
In general it seems like you're on the right track. Things you should clarify to help you proceed: It seems like you are extracting the first 6 dimensions of the ~300 dimensions of the word2vec embedding. This is not advisable. What is in numbers2.csv? Note that word2vec embeds words, not sentences. If you want a model to do this for you could look to infersent of universal sentence embeddings. How do you plan on handling multiple words? With text, the typical approach is to use recurrent models. https://pytorch.org/docs/stable/nn.html#recurrent-layers What are you trying to predict? This may change things. I think you should couple the two parts of the code, your embedding should be able to take into new raw text, tokenize it, and embed it correctly. You can load these embeddings from a pre-trained source (say with gensim as you are already using.)
