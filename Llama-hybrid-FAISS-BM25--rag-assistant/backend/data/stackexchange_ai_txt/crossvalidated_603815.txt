[site]: crossvalidated
[post_id]: 603815
[parent_id]: 603796
[tags]: 
Following my comments, I have made an example to show my suggested approach. First I simulate some data with a binary outcome, and two continues predictors and a binary variable for strategy: # Some R code to simulate data library(boot) # Package contains the logit transform T Then I fit a random forest model to the data: # -- fitting random forest library(randomForest) library(dplyr) rf.fit = randomForest(y ~ ., data = df[1:T-1,]) # I keep the last row out as future observation For brevity I skipped the cross-validation and hyperparameter tuning steps here. Now we use the fitted model to predict the probability of the outcome for both scenarios (using strategy 1 and 2): strategy_1 = df[T,] strategy_2 = strategy_1 %>% mutate(x3 = 1) # switching the strategy of the query from 0 to 1 prob_1 = predict(rf.fit, newdata = strategy_1 ,type="prob") prob_2 = predict(rf.fit, newdata = strategy_2 ,type="prob") # --- Printing probability of success for each strategy print(prob_1[2]) print(prob_2[2]) > print(prob_1[2]) [1] 0.192 > print(prob_2[2]) [1] 0.222 As we expected (according to the beta_3 coefficient), strategy 2 is more favourable showing higher probability of success.
