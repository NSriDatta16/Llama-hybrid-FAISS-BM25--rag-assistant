[site]: crossvalidated
[post_id]: 179203
[parent_id]: 
[tags]: 
Markov Chain Monte Carlo (MCMC) with transformed data

I want to obtain an estimate of a parameter $\Theta$ in a model for a random variable $X$ dependent on $\Theta$ with known but complicated likelihood $L(\Theta|X) = p(X|\Theta)$. $X$ is not directly observable, but I can observe transformation (also a r.v.) $Y$, which is a function $g$ of $X$ and $\Theta$: $Y = g(X; \Theta)$. The function $g$ is nonlinear but 1-to-1 in $X$, and nonlinear (possibly not 1-to-1) also in $\Theta$. I aim at estimating $\Theta$ with MCMC (Gibbs (/MH) sampler); therefore I introduce another latent variable $R$, for which the densities $p(R|\Theta, X)$ and $p(\Theta|R,X)$ have a simple form (not really Gaussian, but close), and so does the (complete) likelihood $L(\Theta | X, R) = p(X|\Theta, R)$. To focus, my question is: In the usual setup, is there a Jacobian determinant to be included in the conditional densities, and if so, how is it to be set up? And if not, what could be the issue with the below implementation? So far, I have worked without a Jacobian in light of $g$ being 1-to-1 in $X$. Here is the pseudo-code which was designed to produce a parameter estimate $\Theta^*$: choose initial $\Theta^{(0)}$; m=0 while m $\leq$ NMaxIterations do { (a) re-transform $X^{(m)} = g^{-1}(y; \Theta^{(m)})$ (b) sample $R^{(m+1)} \sim p(R | \Theta^{(m)}, X^{(m)})$ , and then $\Theta^{(m+1)} \sim p(\Theta | R^{(m+1)}, X^{(m)})$ (c) m=m+1 } $\Theta^* = mean((\Theta^{(N_B)}, ..., \Theta^{(NMaxIterations)}))$, where $N_B$ is some burn-in period. Simulation studies (I simulate a realization of $X$ and transform it to produce data $Y$ via $g$ before running the above algorithm) show that if $g$ is the identity, $\Theta^{(m)}$ converges to the true parameter pretty quickly, as hoped for. However, already for a function $g$ that depends only linearly on $\Theta$, such as $g(x; \Theta) = \Theta x$, the algorithm diverges pretty quickly (Inf or NaN in $\Theta^{(m)}$ for a small $m$). The argument for step (2a) was so far that, to write the problem in a classic MCMC setup, I suppose formally one has to include also sampling from the latent variable $X$, that is, erase step (2a) above and insert the following as first draw before the sampling of $R^{(m+1)}$ in step (2b): sample \begin{eqnarray} X^{(m)} & \sim & p(X | \Theta^{(m)}, y, R^{(m)}) = \delta(X-g^{-1}(y; \Theta^{(m)})) \end{eqnarray} (where I guess, the variable $R^{(m)}$ on which the density $p$ is conditioned upon can be omitted since $X$ does not depend on it), but then this corresponds to nothing else than re-transforming $y$ using the current $\Theta^{(m)}$ to $X^{(m)}$. I am grateful for any insights. Thank you very much!
