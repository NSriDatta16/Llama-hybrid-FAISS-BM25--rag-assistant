[site]: crossvalidated
[post_id]: 581128
[parent_id]: 257579
[tags]: 
There are some special matrix notations in machine learning community, which are implicit by experts but often confuse new practitioners. Some of such notations are described briefly in page 698 of "Pattern Recognition and Machine Learning" by Christopher M. Bishop. So, your life will be easier if you are familiar with that page. If we have a scalar function (e.g., trace) of a matrix, $f(\mathbf A)$ , where $\mathbf A=(A_{ij})$ , you can take derivative with regard to element $A_{ij}$ : $$\frac{\partial f(\mathbf A)}{\partial A_{ij}}.$$ This is well defined in any calculus text. But in machine learning community, people assembly these simple derivatives into a matrix, or "write this result more compactly" as said by Bishop's book: $$\frac{\partial f(\mathbf A)}{\partial\mathbf A}=\left(\frac{\partial f(\mathbf A)}{\partial A_{ij}}\right).$$ People who knows Matrix calculus would recall that this is a so-called denominator layout notation. But as that wikipage says, machine learning community does not follow a single layout for different derivative types. So, it is wise to figure out what layout the author is following before assuming something. At least Andrew Ng is following denominator layout notation for scalar-by-matrix derivatives. Let me start by citing simple product rule of derivative $$\bigl(f(x)g(x)\bigr)'=f'(x)\color{green}{g(x)}+\color{green}{f(x)}g'(x).$$ This rule means that by taking derivative, a single product becomes a sum of two products, with one factor being further derived but another factor being considered a constant which I marked in a relieving color of green. Now let's think of trace of the product of two matrices. Here the two matrices are general; their product does not have to be square. Yes, this is the way things are in machine learning community. Trace is the sum of leading diagonal, but unsquared matrices have no problem having a leading diagonal (see here ). Now let's think that the elements of the above two matrices are all functions of $A_{ij}$ , and next take derivative of the trace of the multiplication of two matrices w.r.t. $A_{ij}$ . Only one summand of the trace is involved, right? That single summand comes from one row of the left matrix and one column of the right matrix, again a sum of products, right? Each summand of such a sum has a form of $f(A_{ij})g(A_{ij})$ , so if we take the derivative, it splits into a sum of two products: $$f'(A_{ij})\color{green}{g(A_{ij})}+\color{green}{f(A_{ij})}g'(A_{ij}),$$ where the green part is considered a constant. Now let's expand the above to matrix form because the same thing happens for all $A_{ij}$ 's. We have $$\eqalign{ \frac{\partial\operatorname{Tr}\bigl(f(\mathbf A)g(\mathbf A)\bigr)}{\partial\mathbf A}&=\operatorname{Tr}\bigl(\frac{f(\mathbf A)}{\partial\mathbf A}\color{green}{g(\mathbf A)}+\color{green}{f(\mathbf A)}\frac{\partial g(\mathbf A)}{\partial\mathbf A}\bigr)\\ &=\frac{\partial\operatorname{Tr}\bigl(f(\mathbf A)\color{green}{g(\mathbf A)}\bigr)}{\partial\mathbf A}+\frac{\partial\operatorname{Tr}\bigl(\color{green}{f(\mathbf A)}g(\mathbf A)\bigr)}{\partial\mathbf A}, }$$ where the green parts at the right hand are regarded as constant matrix. Here I used the fact $\operatorname{Tr}(\mathbf A_1+\mathbf A_2)=\operatorname{Tr}\mathbf A_1+\operatorname{Tr}\mathbf A_2$ and linearity of derivative so $\partial\mathbf A$ can penetrate $\operatorname{Tr}$ . This step requires some math thinking capability. I hope you can get through it. Plugging $f(\mathbf A)=\mathbf A\mathbf B$ and $g(\mathbf A)=\mathbf A^T\mathbf C$ , we get the second equation. The circle $\circ$ represents matrix $\mathbf A$ in Andrew Ng's derivation. The following derivation is easier because of established identities in page 698 of Bishop's book. Now we have that the original derivative is equal to $$\frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\mathbf A\mathbf B\color{green}{\mathbf A^T\mathbf C})+\frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\mathbf C).$$ Since matrix $\mathbf B$ and $\mathbf C$ are constant too w.r.t. elements $A_{ij}$ and hence w.r.t. matrix $\mathbf A$ , we color them in green as well: $$\frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\mathbf A\color{green}{\mathbf B\mathbf A^T\mathbf C})+\frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C}).\tag{1}$$ Now we advance to the third equality of Andrew Ng's derivation of derivative. The first term of the above result (1) turns into $(\color{green}{\mathbf B\mathbf A^T\mathbf C})^T=\mathbf C^T\mathbf A\mathbf B^T$ , according to identity (C.24) in page 698 of Bishop's book: $$\frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\mathbf A\color{green}{\mathbf B})=\color{green}{\mathbf B}^T.\tag{C.24}$$ Now you should know why I tirelessly mark constant matrix as green. The first term in the third equation of Andrew Ng's derivation is confusing in that he missed the trace. I am really a bit tired at this point. Take a break of typing, but you keep on reading. The second term contains a transpose of $\mathbf A$ . We want to take the derivative w.r.t. this transpose $\mathbf A^T$ to take advantage of identity (C.24). To that end, remember we use denominator layout to arrange partial derivatives in the resulting derivative matrix. If the variable matrix is $\mathbf A^T$ , the denominator layout will result in nothing but a transpose of the matrix $\frac{\partial}{\partial\mathbf A}$ . So, we have $$\frac{\partial}{\partial\mathbf A^T}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C})=\left(\frac{\partial}{\partial\mathbf A}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C})\right)^T,$$ which leads to the second term of the third equality. In order to derive $\frac{\partial}{\partial\mathbf A^T}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C})$ based on identity (C.24) where the variable matrix is the left matrix of the multiplication, we apply the cyclic property of trace to deduce $\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C})=\operatorname{Tr}(\mathbf A^T\color{green}{\mathbf C\mathbf A\mathbf B}).$ In the second term of the fourth equality, Andrew Ng wrote $Cf(A)$ out of the $\operatorname{Tr}$ operator. I think that's a typo. Now it is easy to apply Bishop's identity (C.24) to obtain $$\eqalign{ \frac{\partial}{\partial\mathbf A}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C}) &=\left(\frac{\partial}{\partial\mathbf A^T}\operatorname{Tr}(\color{green}{\mathbf A\mathbf B}\mathbf A^T\color{green}{\mathbf C})\right)^T\\ &=\left(\frac{\partial}{\partial\mathbf A^T}\operatorname{Tr}(\mathbf A^T\color{green}{\mathbf C\mathbf A\mathbf B})\right)^T\\ &=\bigl((\color{green}{\mathbf C\mathbf A\mathbf B})^T\bigr)^T\\ &=\mathbf C\mathbf A\mathbf B.}$$ We have proved that the first term of (1) equals $\mathbf C^T\mathbf A\mathbf B^T$ . Above we get the second term $\mathbf C\mathbf A\mathbf B$ . Now we can conclude the whole proof by combining them as the final result of the derivative: $\mathbf C^T\mathbf A\mathbf B^T+\mathbf C\mathbf A\mathbf B$ .
