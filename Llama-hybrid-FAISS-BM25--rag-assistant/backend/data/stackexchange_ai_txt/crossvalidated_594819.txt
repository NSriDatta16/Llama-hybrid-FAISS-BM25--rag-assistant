[site]: crossvalidated
[post_id]: 594819
[parent_id]: 
[tags]: 
Uncertainty score from Monte Carlo dropout

When using a neural network for multi-class classification, there are situations where it is useful to estimate the uncertainty of the network's predicted class. One leading method for estimating uncertainty is Monte Carlo dropout, as introduced by Gal et al. . Let $f(y|x)$ denote the softmax output of a neural network on input $x$ , for class $y$ . Their approach is to train $f$ with dropout, keep dropout enabled at test time, and estimate $\mathbb{E}[f(y|x)]$ and $\textrm{Var}[f(y|x)]$ by drawing a few samples from $f(y|x)$ , i.e., by running the neural network a few times on input $x$ and computing the mean and standard deviation of the outputs. So far, that all makes sense, but then I'm not clear how to get an uncertainty score from that. If I want a single score that measures the uncertainty in the network's prediction of the class of $x$ , what is the standard way to obtain an uncertainty score from this information? The paper does doesn't appear to suggest any particular method. The paper describes how to estimate the posterior predictive distribution, but not how to obtain an uncertainty score from it. I can imagine multiple possibilities for how one could construct an uncertainty score from the posterior predictive distribution obtained via Monte Carlo dropout: We could use the probability of the predicted class, from the posterior predictive distribution, as our uncertainty score, i.e., $\max_y \mathbb{E}[f(y|x)]$ . We could use the entropy of the posterior predictive distribution as our uncertainty score, i.e., $$H(\mathbb{E}[f(\cdot|x)]) = - \sum_y \mathbb{E}[f(y|x)] \log \mathbb{E}[f(y|x)].$$ We could estimate the probability that a future evaluation of the neural network will output the same class as the predicted class, using a normal approximation. In particular, we could approximate $p(y|x)$ as a Gaussian $\mathcal{N}(\mu_y,\sigma_y^2)$ with mean $\mu_y = \mathbb{E}[f(y|x)]$ and variance $\sigma_y^2 = \textrm{Var}[f(y|x)]$ . For a two-class classifier, let $y$ be the class predicted at test time (i.e., $y=\arg\max_{y'} \mathbb{E}[f(y'|x)]$ ) and $\neg y$ the other class. Then the probability that a future evaluation of the classifier will also output $y$ is $u=\Pr[X\ge 0]$ where $X \sim \mathcal{N}(\mu_y-\mu_{\neg y},\sigma^2_y+\sigma^2_{\neg y})$ . We can estimate $u$ via the error function and then use $u$ as our uncertainty score. (For a multi-class classifier, we can reduce to a two-class classifier by summing all of the logits for classes other than $y$ and treating that as the logit for a single other class $\neg y$ , using two-class softmax, and estimating normal distributions for those softmax outputs.) All of these seem to me like they would be reasonable choices. What is the accepted practice? What is the standard way to use Monte Carlo dropout to get an uncertainty score? If there is no standard, which is most effective?
