[site]: crossvalidated
[post_id]: 630267
[parent_id]: 
[tags]: 
Evaluation metrics for chunking and synthesis steps for Q&A system

I am doing research and interested in the following question: What are the evaluation metrics for chunking, retrieval and synthesis steps for Q&A, when I do NLQ with LLMs? I am looking for scientific approach, not just human verification. I am familiar with this article for retrieval, but cannot find anything on on metrics for evaluation of chunking and synthesis. Basically, in a QnA system - when would you know you have achieved sufficient accuracy? How many questions would we need to see manually? And what brings this process to a stop?
