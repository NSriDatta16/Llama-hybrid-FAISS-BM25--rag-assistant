[site]: crossvalidated
[post_id]: 471126
[parent_id]: 471058
[tags]: 
I don't think I've heard of something like this being done before in the way you're describing. Can you do it? Yes. Is it a good idea? Well, I'm not so sure -- as I understand, you're essentially using your trained model to cherry-pick your data so that your dataset only has data points where your model achieves a certain confidence -- which means your model metrics are going to be biased. If you want to threshold, an alternative way to do this would be to not modify the dataset in this way, but use your threshold in this way instead: If top-1 probability > threshold, output top-1 class as the prediction. Else if top-1 probability To choose a threshold -- I'm unsure how the ROC curve method works, since you'll have to binarize your labels for that to even make sense. For the multi-class case, I suppose you could micro- or macro-average a bunch of one-versus-rest binary classifiers for each class on your dataset. As for per-class thresholds -- that might be overkill. I would try simply doing a universal threshold first, and if that yields undesirable results (you'll have to define that for your case), you could potentially try the same threshold-picking strategy, and adopt the same one-versus-all strategy to derive per-class ROC curves.
