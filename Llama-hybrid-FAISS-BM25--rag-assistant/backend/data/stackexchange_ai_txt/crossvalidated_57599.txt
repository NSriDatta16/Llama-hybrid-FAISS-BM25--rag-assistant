[site]: crossvalidated
[post_id]: 57599
[parent_id]: 57162
[tags]: 
The author is just abusing the notation a bit, but essentially he's applying the chain rule. $\mathbf{x} = (x_{1}, ..., x_{n})$ represents the input pattern to the network. Now, for each input pattern, new samples are generated by applying the transformation represented by $s(x_{n},\xi)$. Let us denote these new patterns by $\mathbf{t} = s(\mathbf{x},\xi)$ (for transformed). Now, $\mathbf{y} = \mathbf{y}(\mathbf{t};\mathbf{w})$ is the mapping realized by the neural network. To see how the response of the network is affected by changes in the transformation parameter you apply the chain rule (I follow your component-wise decomposition), $$\frac{\partial y_{k}}{\partial \xi} = \sum_{i=1}^{D} \frac{\partial y_{k}}{\partial t_{k}} \frac{\partial t_{k}}{\partial \xi} = \sum_{i=1}^{D} J_{ki} \tau_{i}$$ Here $\mathbf{x}$ is fixed, and you vary $\xi$. I guess he wanted to stress the fact that the components of the Jacobian are just the expressions obtained when talking about the backpropagation algorithm. It is just a concrete instance of the concept of directional derivative .
