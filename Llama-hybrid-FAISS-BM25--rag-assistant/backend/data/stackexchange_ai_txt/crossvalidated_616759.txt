[site]: crossvalidated
[post_id]: 616759
[parent_id]: 616746
[tags]: 
Theoretical Underpinnings and Reliability First, it seems you have a clear hypothesis on what you want to do, but you don't have as clear theoretical reasons for including some of your variables (I'm sure you do, but they are not listed here). In order to apply any form of inference to your statistics, you should have some basic understanding of what variables are important to include and which are not so important. Some of your variables clearly fit (network building for example), while others do not seem related to your research question (e.g. cost efficiency). I'm guessing your outcome variable, based on your question, is entrepreneurial opportunity, and given it is a 4-point Likert scale composite, it is not surprising that the data is not normally distributed. Is this the case when the composite is aggregated together (such as a sum)? What distribution does it belong to? With some more information, one can ascertain how this should be modeled. If you do end up aggregating your items together, you should at the very least check reliability of your items and see if they even belong together. Traditional ways are Cronbach's $\alpha$ , but if you prefer a factor analysis approach, McDonald's $\omega$ is often superior. In any case, you just want to make sure your items are actually related. Control Groups My biggest concern is that you do not have a control group to compare against, so a t-test isn't even on the table. I'm not sure how far you are into the data collection process, but I would consider measuring these things for a group that is "urban" so you do not make wild claims about rural women without having some baseline to compare against. While you can run the analysis as-is and compare to some past data, what conclusions you can draw from such an analysis may be very limited depending on a variety of influences. Regression and Example In any case, if you create some composites with each of your variables here, you could simply run a regression and attempt to either normalize your response variable or use an appropriate link function for your regression. Here I have simulated a regression in R with Poisson-distributed (aka right-skewed) data to show how it can be done. First, I simply create some data mimicking at least some of your variables. #### Simulate Data #### library(tidyverse) set.seed(123) group % as_tibble() hist(entrep) # response is right skewed Visualizing our data, we can see our variables are all right-skewed: #### Visualize Data #### tib %>% gather() %>% ggplot(aes(x=value))+ geom_histogram( binwidth = 2, color = "black", fill = "steelblue" )+ facet_wrap(~key, scales = "free") Then I fit a regression using a Poisson link function to match our skewed variable and suppress the intercept with -1 to make interpretation easier since this is technically a categorical regression (with group as our categorical factor): #### Run Regression #### fit The results are shown below: Call: glm(formula = entrep ~ network + cost + group - 1, family = poisson, data = tib) Coefficients: Estimate Std. Error z value Pr(>|z|) network 0.166067 0.008632 19.239 Because I have suppressed the intercept, each group is compared. We can see that rural group has a slightly higher average entrepreneurial opportunity score than the urban group. We can also determine that the other continuous variables are significant, though we should check their standardized beta coefficients to see the magnitude of the effects: #### Check Betas #### library(lm.beta) lm.beta(fit) We can see that cost has the most substantial impact, yielding a $.16$ standard deviation increase in the response with each standard deviation increase in cost: Call: glm(formula = entrep ~ network + cost + group - 1, family = poisson, data = tib) Standardized Coefficients:: network cost groupurban grouprural 0.15508255 0.16000287 0.02523122 0.02780836
