[site]: crossvalidated
[post_id]: 368104
[parent_id]: 
[tags]: 
Why, *intuitively*, in regular parametric problems, does uncertainty go down at a $\sqrt{ n }$ rate on the SE/posterior SD scale?

consider the simplest regular statistical inference problem: $( y_1, \dots, y_n | F ) \sim$ $\text{IID}$ from a cumulative distribution function $F$ on $\mathbb{ R }$ with mean $\mu$ and finite variance $\sigma^2$ (the finiteness of $\sigma$ ensures the existence and finiteness of $\mu$ ); let $\mathbf{ y } = ( y_1, \dots, y_n )$ the nonparametric maximum likelihood estimator for $\mu$ , which cannot be improved upon either from the frequentist or bayesian point of view without additional information/assumptions about $F$ , is of course $\bar{ y } = \frac{ 1 }{ n } \sum_{ i = 1 }^n y_i$ the repeated-sampling (RS, frequentist) variance of $\bar{ y }$ is of course $V_{ RS } ( \bar{ y } ) = \frac{ \sigma^2 }{ n }$ , leading to the familiar standard error formula $SE_{ RS } ( \bar{ y } ) = \sqrt{ V_{ RS } ( \bar{ y } ) } = \frac{ \sigma }{ \sqrt{ n } }$ (with this same information base, a bayesian would of course get the same answer for the standard deviation of her/his posterior distribution for $\mu$ given $\mathbf{ y }$ ) this formula, which we could call the square root law , depends vitally on the fact that the repeated-sampling variance of a sum of IID observables is the sum of their variances: the variance and sum operators commute under independence so the issue comes down to this: why, intuitively, do we live in a universe in which it is the variance scale on which uncertainty about the sum of independent observables is additive, and not some other scale? if i were trying to intuitively explain this fundamental fact to an intelligent person who has had little exposure to quantitative thinking, i do not regard it as satisfying (again intuitively) to offer the following argument: (1) define a kolmogorov-style probability triple $( \Omega, \mathcal{ F }, P )$ (2) define real-valued random variables $Y_i$ as functions from sets in $\mathcal{ F }$ to $\mathbb{ R }$ (3) define the concept of expectation $E ( Y )$ of a random variable $Y$ (4) define the concept of variance $V ( Y ) = E [ Y - E ( Y ) ]^2$ (5) define the concept of independence of a finite collection $\mathbf{ Y } = ( Y_1, \dots, Y_n )$ of random variables (6) prove that under independence, the variance of a sum of random variables equals the sum of their variances (7) deduce the square root law from (1-6) so, i repeat my question: why, intuitively, do we live in a universe in which it is the variance scale on which uncertainty about the sum of independent observables is additive, and not some other scale? cogent thoughts on this topic would interest me; thanks in advance for your interest, and best wishes
