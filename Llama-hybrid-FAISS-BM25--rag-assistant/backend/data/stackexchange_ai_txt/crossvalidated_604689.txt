[site]: crossvalidated
[post_id]: 604689
[parent_id]: 
[tags]: 
What does the number of support vectors tell us?

I've trained a SVM regression model, and noticed a large number of support vectors required. I have found the following discussions where it seems people are concluding that a larger number of support vectors means overfitting: Is a high number of support vectors a sign of overfitting? What is the relation between the number of Support Vectors and training data and classifiers performance? In my case, there are 688 observations in the training set, out of which 589 are support vectors. However, my performance metrics on the test set are great when compared to the training set, the $R^{2}$ for train/test is 0.914/0.883 and $MSE$ is 0.0021/0.0026. To choose the hyperparameters I did use cross validation, and the optimal parameters are $C=220$ , $\epsilon=0.01$ and $\gamma=3\times10^{-4}$ . If you are having doubts about the test set, I separated it immediately upon loading the data, and all the scaling is fitted on the training set. So everything indicates that the model makes good predictions, and that there is no overfitting. So what's the actual deal with the number of support vectors? Are they really a sign of overfitting, or just of a complex system? I'm wondering about these questions regardless of my specific case. EDIT: Sorry forgot to mention. I'm using RBF function, which as far as I understand uses the dual form, which needs to learn $N$ parameters, equal to the number of observations, instead of number of features. In my case I have 40 features.
