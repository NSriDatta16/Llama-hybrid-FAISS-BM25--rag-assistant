[site]: crossvalidated
[post_id]: 400568
[parent_id]: 313955
[tags]: 
You know that your Data may be distributed in an absolute different way than you latent space. You are sampling your latent space given/from your data. The Bernoulli Dstribution for your latent space can either only be obtained through backprop. or through a parameterization given your Latent distributionÂ´like a reverse generative model. Though you have: --data distro-- --Bernoulli distro-- X1 - ... - Y1 X2 - ... X3 - :W: - Y2 X4 - ... X5 - ... - Y3 A generative model would learn a Gassian/Beta/Gamma/Poisson data noise with Bernoulli prior, so for the encoding case, you need a Bernoulli data noise with any data prior. The parameters can be obtained with the KL-Divergence or EM... Be sure not to learn too much values. E.g. $diag( \sigma_.^2)$ and the bias $\mu_x$ for simple space invariance. But you could even use just a recurrent / convolutional layer for the spacial invariance. Note that usually your $f(x,W)$ shall already depict your mean value of Y! (And $f^{-1}(W^{-1},y)$ mathematically you data mean... if I didn't overcomplicate something.) Bayesian it would be: $P(X|Y,\Theta) = \frac{P(Y|X,\Theta)P(x)}{P(Y)}$ which you can approximate with $\frac{P(Y|X,\Theta)P(X)}{\sum_{x' \in ROI(X)}P(Y|x',\Theta)P(x')}$ where $ROI(X)$ would be the region of interest from X given the $M'$ highest marginal probabilities over your Bernoulli distribution. There are papers about binary to binary generative models which you should read considering the math you are going to implement!
