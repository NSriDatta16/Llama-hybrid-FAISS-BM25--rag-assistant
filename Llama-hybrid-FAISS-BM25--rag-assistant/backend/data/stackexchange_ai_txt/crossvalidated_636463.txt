[site]: crossvalidated
[post_id]: 636463
[parent_id]: 229415
[tags]: 
Consider a simple linear regression model for this explanation. The goal is to predict a target variable $y$ using a set of features $x$ and a set of weights $\textbf{w}$ . Without Regularization, the objective in standard linear regression is to minimize the Mean Squared Error (MSE): $$MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i − \textbf{w}^{T} \textbf{x}_{i})^{2}$$ Here, $y_i$ is the observed target value, $x_i$ is the feature vector for the $i$ -th sample, $\textbf{w}$ is the weight vector, and $N$ is the number of samples. When we add L2 regularization, we modify the objective to include a penalty on the size of the weights. The loss function becomes: $$ L = MSE + \lambda \sum_{j=1}^{M} w_j^2 $$ ​ where $\lambda$ is the regularization strength and $M$ is the number of weights. In a Bayesian setting, we start with a prior belief about the distribution of the parameters (weights) and update this belief based on the observed data. A Gaussian prior on the weights implies that we believe, before seeing any data, that the weights are likely to be small. Mathematically, this is expressed as: $$ \textbf{w} ∼ N(0,σ)$$ This means each weight $w_j$ is drawn from a Gaussian distribution with mean 0 and variance $\sigma^2$ . The logarithm of the Gaussian prior probability for the weights is: $$ logP(w)= −\frac{1}{2σ^2} \sum_{j=1}^M w_j^2 + constant $$ Notice that this is a sum of the squares of the weights, similar to the L2 penalty term. In Bayesian inference, MAP estimation involves finding the weights that maximize the posterior probability, given the observed data. The posterior is proportional to the likelihood times the prior: $$ P(w∣Data)∝P(Data∣w)⋅P(w) $$ Taking the logarithm (which preserves the location of the maximum): $$ logP(w∣Data)=logP(Data∣w)+logP(w) $$ For our linear regression model, maximizing $logP(w∣Data)$ involves minimizing the negative of this expression, which includes the MSE (as the negative log-likelihood of the data given the weights under Gaussian noise assumption) and the log of the Gaussian prior. Finally, the L2 regularization term $ λ \sum_{j=1}^{M} w_j^2 $ in the loss function is equivalent to the negative log of the Gaussian prior $− \frac{1}{2σ^2} \sum_{j=1}^{M} w_j^2$ in the MAP formulation, up to a constant factor. If we set the regularization parameter $λ$ equal to $\frac{1}{2σ^2}$ , then minimizing the L2 regularized loss function is the same as maximizing the posterior probability under a Gaussian prior.
