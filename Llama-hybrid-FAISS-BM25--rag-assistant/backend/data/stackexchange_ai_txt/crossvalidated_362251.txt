[site]: crossvalidated
[post_id]: 362251
[parent_id]: 357377
[tags]: 
My answer is stated in terms of undersampling (sample the common event to get balance) whereas the OP is asking about oversampling (sampling w/replacement or upweighting the rare events). Both approaches will get you to a balance of response types and the mathematical arguments are similar (why I'm keeping my post up rather than deleting) but the implementations differ (which thing you sample and how you sample-or upweight). Everything in my answer about downsampling ( mutatis mutandis ) should also be true for oversampling. It really depends on which model you're working with. For example, with logistic regression it is very common approach to sample from the common event to induce more balance. However, this approach is very sensitive to correct specification of model and the feature set. The reason this is done is clearly explained in an article by Gary King and one of his students (now a prof at UCSD). There is also a really nice article by Will Fithian and Trevor Hastie about iterating this approach. Frank Harrell has commented on another related post and his opinion is not to do the sampling. I disagree with his opinion. I've used the case-control approach in applied work several times to much success. I've also studied how and why it works or doesn't. If you have lots of data it really shouldn't matter but if you samples are expensive to collect and the sample size isn't big enough for traditional asymptotics to kick in, I'd use the down sampling approach. For Support Vector Machines (SVMs), this isn't necessary as far as I can tell, and I'm unaware if this is done for SVMs. I don't think it would hurt all that much though. It is unclear to me if this is an issue with Neural nets though. Maybe someone else will chime in on that aspect in a different comment. If not and I get some time to investigate I'll come back and update my answer, but I wouldn't wait around for me to do that, might be awhile.
