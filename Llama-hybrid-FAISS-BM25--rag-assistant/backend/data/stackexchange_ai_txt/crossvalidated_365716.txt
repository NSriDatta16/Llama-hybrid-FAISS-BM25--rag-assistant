[site]: crossvalidated
[post_id]: 365716
[parent_id]: 
[tags]: 
What does it mean when a low number of quadrature points gives a very different GLMM fit?

I am interested in a logistic regression model with 10 fixed-effects parameters and random intercepts, which I can fit using the lme4::glmer function in R. The value of the nAGQ parameter determines the number of quadrature points used in approximating the likelihood, with 1 corresponding to a Laplace approximation and 0 using "a faster but less exact form of parameter estimation for GLMMs by optimizing the random effects and the fixed-effects coefficients in the penalized iteratively reweighted least squares step" ( glmer documentation ). The fixed-effects parameter estimates appear to stabilise once nAGQ gets above 8-10: (The coloured horizontal lines show the estimates from the same model fitted using mgcv::gam (red), gamm4::gamm4 (blue) and SPSS's GENLINMIXED (green) command) I appreciate that as we increase the number of quadrature points, we get a closer approximation to the log-likelihood, but I came across this message posted by Jim Lindsey to the r-help list in 2001: When discussing GLMMs using Gauss-Hermite integration, the question of approximations is essentially a red herring. No matter the number of quadrature points, the likelihood is always exact (as exact as any likelihood can be on a digital computer). It is a finite mixture. The approximation question arises in the sense that this finite mixture is more or less close to a Gaussian mixing distribution, which is a completely artificial choice in the first place. It is quite possible for the model with very few quadrature points to fit better than one with sufficient for a very close approximation to the normal mixing distribution, indicating that normal mixing is not a good choice. It is false to say that the properties of this approximation, in the latter sense are unknown. GLMMs using Gauss-Hermite go back at least to an unpublished tech report by Don Pierce in the mid 70s and the published paper by John Hinde in 1982. Since then, there is a vast literature on the subject of the approximation in the second sense above, especially for the model in question, mixed logistic regression, including work by Alan Agresti, Murray Aitkin, David Brillinger, Bruce Lindsay, etc. (There is also another literature on approximations replacing Gauss-Hermite, such as Breslow and Clayton.) The most recent published reference that I am aware of is earlier this year, but I have refereed others that are not yet in print. 15 to 20 quadrature points gives an extremely close numerical approximation to the normal mixing distribution for most data sets, for what that is worth. My question has a few parts: (1) Does this mean that if the results from using a PIRLS fit or Laplace approximation are very different to using a higher number of quadrature points, it suggests that the Gaussian assumption for the random effects is not well-supported by the data? (2) If so, would it ever make sense to take the PIRLS / Laplace results as the fitted model? Or is a higher number of quadrature points always 'better' (convergence & computational time issues not withstanding)? (3) In my example, the log-likelihood of the fitted model takes its maximum at nAGQ = 4 (see below). Does this mean that this is the optimal fit (I appreciate that it is only a tiny difference), or are these fitted log-likelihoods not comparable because they are different approximations? (Background information: I am trying to help someone who is only comfortable using SPSS. I haven't been able to find any documentation of the GENLINMIXED fitting algorithm, but the parameter estimates that it gives are very close to those of glmer with nAGQ = 0 )
