[site]: crossvalidated
[post_id]: 284820
[parent_id]: 283748
[tags]: 
In a Frequentist approach, $\theta$ is a static value to estimate, so $P(X|\theta)=P_\theta(X)$ is an unconditional distribution of $X$ parameterized by $\theta$. If $\theta^*$ is the true parameter of the underlying Markov process, then $P(X|\theta^*)=P(X)$. Whether $P(X|\theta)$ is a conditional distribution depends on how you use it. As a function of the observed data $x$, $P(x|\theta)$ is a distribution. However, for maximum likelihood estimation we are interested in finding a static value for $\theta$ that maximizes $P(x|\theta)$ and thus treat $P(x|\theta)$ as a function of $\theta$ with fixed $x$. The result is not a distribution since integrating $P(x|\theta)$ over $\theta$ doesn't give 1. The notation $\mathcal{L}(\theta|x)$ is used to make this distinction clear. Note that here the $|$ means "given" in a non-probabilistic sense, and a less common but clearer notation would be $\mathcal{L}_x(\theta)$. In a Bayesian setting, we treat $\theta$ as a random variable and $P(\theta)$ is the prior belief over possible parameters $\theta$. In this case, it makes sense to write $P(x|\theta)=P(x,\theta)/P(\theta)$. For more explanation you can take a look at my recent question: Why do people use $\mathcal{L}(\theta|x)$ for likelihood instead of $P(x|\theta)$? . I noticed a small mistake in your question: The true parameter $\theta$ is not "the likelihood function for $\theta$". The likelihood function $\mathcal{L}(\theta|x)=P(x|\theta)$ is usually a function of observed data $x$. In contrast, the true parameter $\theta^*$ depends on the random variable $X$ that follows the underlying distribution, so the optimal parameter $\theta^*={argmax}_{\theta'}{P(X|\theta')}$ cannot be found.
