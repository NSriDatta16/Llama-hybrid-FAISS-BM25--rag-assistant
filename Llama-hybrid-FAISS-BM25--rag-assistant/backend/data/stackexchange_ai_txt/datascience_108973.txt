[site]: datascience
[post_id]: 108973
[parent_id]: 
[tags]: 
Is there any works in the direction of dimensionally reducing the size of DNNs?

I am talking about a scenario where you first train a "huge" Neural Network and then try to scale it down without sacrificing much of the accuracy. I am not talking about quantization of weights, biases, etc. My thought comes from the perspective of coarse-grained molecular dynamics in physics. Which is like smoothing out the energy landscape. Here in the neural network context too we have an "energy landscape"(loss landscape?). So is there any works I can look up which has done something similar?
