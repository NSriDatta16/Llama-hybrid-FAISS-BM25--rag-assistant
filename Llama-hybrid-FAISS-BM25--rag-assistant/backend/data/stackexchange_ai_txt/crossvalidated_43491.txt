[site]: crossvalidated
[post_id]: 43491
[parent_id]: 43421
[tags]: 
As the models have been chosen by optimising the cross-validation error, I suspect this is part of the reason for the cross-validation accuracy being a lot higher than the test error. Essentially the model has been tuned by optimising the cross-validation error, which means it then provides an optimistically biased estimate of performance. The bias introduced by this king of thing can be surprising large, and can easily result in drawing incorrect conclusions, see G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ) Consider a problem were the aim is to predict the outcome of 10 flips of an unbiased coin. The "model predictions" are formed by generating 10 flips of other unbiased coins (so that the skill of the predictions is exactly zero). However if you choose the best of these models on the 10 flips you observed, you are likely to get one that appears to have some skill and the more models you have to choose from, the greater the apparent skill. However, the best model will still have skill of zero if the coin is flipped another ten times. Likewise, if you choose your model by optimising the CV accuracy, the choice is partly due to genuine gains in generalisation, but also partly due to random chance (you have over-fitted the cross-validation estimate). As a result, the CV accuracy is higher than the true generalisation performance of the model (estimated by the test set). @cbeleites also makes an excellent point (+1) that if the test and training sets are small, then the performance estimates (both CV and test set) will have a high variance, so it wouldn't be too surprising for one to be higher or lower than the other.
