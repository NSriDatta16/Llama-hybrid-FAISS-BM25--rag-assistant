[site]: crossvalidated
[post_id]: 549331
[parent_id]: 
[tags]: 
Testing if single features have predictive power alone

I am working on a classification task where the target variable is binary. The feature space is huge (about 280 features) and I am almost certain that some of them are completely irrelevant for classification. Now, I am thinking about training one neural network for each feature. If the cross validation accuracy for a certain feature is above 50% I suggest the feature provides some important information. If it is at 50% or below it is basically not providing any important information as the result is not better than assigning classes randomly. However, it came to my mind that it might be possible that one feature does a bad job at classification on its own but is very important for the accuracy in combination with other features. Is this possible? So for instance: Feature 1 on its own provides 49.98% accuracy, Feature 2 on its own provides 64% accuracy but Feature 1 & Feature 2 together provide 74% accuracy. Is this possible?
