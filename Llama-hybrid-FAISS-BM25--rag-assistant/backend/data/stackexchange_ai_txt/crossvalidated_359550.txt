[site]: crossvalidated
[post_id]: 359550
[parent_id]: 359456
[tags]: 
Firstly, note that $Precision = \frac{TP}{TP+FP}$, whereas $Recall = \frac{TP}{TP+FN}$, where $TP$ is the number of true positives, $FP$ is the number of false positives, and $FN$ is the number of false positives. Since you have mentioned that your data is skewed towards the positive case, (depending on how you have trained your model and chosen your probability cutoff) your predictions will hence likely be biased towards the positive case also. This means your model will be more likely to predict an observation as being positive than negative, leading to more frequent false positives occurring. Since $Precision$ is inversely proportional to the number of false positives, $FP$, this increase in $FP$ leads to a larger decrease in $Precision$ relative to the decrease in $Recall$. This happens as the bias in the data towards the positive class means on average, the negative class is less likely to be predicted, and hence less 'negatives' are predicted compared to what we would expect with a more balanced dataset. Less negative predictions explain the relative decrease in less false negative predictions, and why $Recall$ is higher, by a similar logic to the above. Side note: For your graphs, are they zero-indexed (0-9) when they should be 1-10? Not a big issue but it may make it harder to read results off the graph if you constantly have to shift the x-index forward by one, and so forth.
