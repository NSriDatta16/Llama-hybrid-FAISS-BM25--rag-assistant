[site]: crossvalidated
[post_id]: 618897
[parent_id]: 573599
[tags]: 
I would have preferred to just comment shimao's answer but my reputation does not suffice. First, a small correction of the answer: Some GAN varieties like WGAN assume independence between samples in a batch, which is a good reason to avoid batch norm. As far as I know, this is not correct. The author's of WGAN actually encourage usage of batch norm [1,2]. However, the improved Wasserstein GAN (with Gradient Penalty) , requires to omit or replace Batch Norm. They recommend Layer Norm as a replacement: Our penalized training objective is no longer valid in this setting, since we penalize the norm of the critic’s gradient with respect to each input independently, and not the entire batch. To resolve this, we simply omit batch normalization in the critic in our models, finding that they perform well without it. Our method works with normalization schemes which don’t introduce correlations between examples. In particular, we recommend layer normalization [3] as a drop-in replacement for batch normalization. Second, I would like to add that the usage of differentially private training is another reason against the usage of Batch Norm, as described in Google's DP-fy ML paper : However, BatchNorm uses current batch’ mean and standard deviation information to rescale each instance in the batch during the forward pass. This creates dependency between instances from the batch and makes it hard to reason about per-example sensitivity for DPSGD. https://github.com/martinarjovsky/WassersteinGAN/issues/14#issuecomment-283092028 https://github.com/martinarjovsky/WassersteinGAN/pull/6#issuecomment-277738812
