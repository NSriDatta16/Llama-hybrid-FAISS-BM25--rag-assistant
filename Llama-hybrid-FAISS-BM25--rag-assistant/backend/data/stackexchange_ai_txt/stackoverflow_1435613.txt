[site]: stackoverflow
[post_id]: 1435613
[parent_id]: 1435583
[tags]: 
This seems like a really bad idea for several reasons, not the least of which is that Google will cache copies of your pages, so that even if I do not authenticate against your site, I will be able to see the content of web pages and other documents served from behind the protected portion of your web site. As far as detecting web crawlers goes, I wouldn't trust any User Agent. You could probably compile a list of IP addresses the crawlers originate from, but as soon as Google adds another IP address, you will be denying that crawler access. Doing a reverse DNS lookup on every request to ensure the domain of the visitor is googlebot.com as suggested at Verifying Googlebot could be a big performance hit if your site is busy.
