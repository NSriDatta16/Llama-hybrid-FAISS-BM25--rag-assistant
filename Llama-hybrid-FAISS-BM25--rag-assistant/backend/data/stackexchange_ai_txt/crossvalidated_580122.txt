[site]: crossvalidated
[post_id]: 580122
[parent_id]: 580086
[tags]: 
Observed populations are realisations of data-generating processes and we want to compare processes rather than historical fact In some cases we may have captive populations and really good data capture - for instance we may know the exact hospital stay duration of everyone in several university hospitals who has been admitted for a specific condition over 2021-2022. There may be sampling discussions around the consistency of the definitions (e.g. around condition, admitted etc. and whether there are different distributions over time (one hospital may do more emergency admissions at weekends, one might have had fewer admissions during a more strained COVID-related period)) but let's set those aside for now. We can say what the average stay duration for each hospital was exactly, but if we want to say "people in hospital A were hospitalised for longer and not just by random chance" we actually want to compare the data generating processes. We might start by modelling the process as 'People in hospital X get a random duration distributed N(mu_1, sigma) for hospital A and N(mu_2, sigma) for hospital B', then start adding more complexity to account for other effects such as the level of stress on the hospital, in-week periodicity, different levels of variation, etc. etc. If you're not interested in healthcare, let's say I rolled a die and got the following results: table(floor(runif(10000, 1, 7))) 1 2 3 4 5 6 1677 1675 1612 1641 1690 1705 Great, we have perfect observations that over 10,000 rolls we have an average of 3.5107. But that's history now and we can't do much about that. The question we might want to answer is 'is this die fair', and then we're back to comparing the process which generated the observations above with the process which gives us each number with a 1/6 chance.
