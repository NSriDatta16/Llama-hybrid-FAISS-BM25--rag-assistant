[site]: crossvalidated
[post_id]: 628098
[parent_id]: 620193
[tags]: 
I'm very new to the diffusion model community, so take my answer with a grain of salt. But let me attempt to answer your question in a more general sense. TL;DR : These models perform better. I'm sure they have tried other architectures, and you could use simpler architectures, but you probably wouldn't be able to get far with them or obtain clean results and state-of-the-art scores. Regarding transformer abilities , in several ML applications, you can find the use of attention-based architectures (from regression to computer vision and NLP). For example, recently, I have been working with one-dimensional regression functions and we have found an architecture that utilizes the attention heads: transformer abilities just improve its performance, so why not use it? Regarding residual networks , I think the answer here is a bit more clear. Although you don't have to use them, the residual methodology is used to perform the following: the (encoded) temporal information can easily be passed to the architecture for conditional diffusion models, the class information can also be easily passed Both are passed just by adding (encoded) information to decided layers in the architecture (I suggest watching the following great tutorial , particularly the part on classifier free guidance (min 17:00). Finally, to convince yourself, you could check the following coding tutorial, such as: Weights & Biases tutorial where you can see directly that both temporal and class information are added (kind of) as a form of residual connection So to conclude, although these models have the desired effect and ease implementations (e.g, incorporation of temporal information, adding class-specific information), you could test other architectures, but for complicated tasks, even on the mnist dataset, I doubt that you could get good performance.
