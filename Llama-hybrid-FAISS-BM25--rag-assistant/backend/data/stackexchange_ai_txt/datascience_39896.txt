[site]: datascience
[post_id]: 39896
[parent_id]: 39881
[tags]: 
As per your assumption, if you're scaling the features so that all of them are transformed between 0 and 1, the case where you're inputting 7 will result in a rescaled feature which will be greater than 1 (7/6 > 1). So, this tells us that your scaling technique of dividing inputs by a specific number isn't a really good scaling approach. For your specific problem, scaling won't really be necessary, as explained here . The important chunk says: If the input variables are combined linearly, as in an MLP, then it is rarely strictly necessary to standardize the inputs, at least in theory. The reason is that any rescaling of an input vector can be effectively undone by changing the corresponding weights and biases, leaving you with the exact same outputs as you had before. However, there are a variety of practical reasons why standardizing the inputs can make training faster and reduce the chances of getting stuck in local optima. Also, weight decay and Bayesian estimation can be done more conveniently with standardized inputs. But if you're still interested in scaling the data, here's a nice discussion which can be explored to get an idea about standardized scaling/normalization techniques.
