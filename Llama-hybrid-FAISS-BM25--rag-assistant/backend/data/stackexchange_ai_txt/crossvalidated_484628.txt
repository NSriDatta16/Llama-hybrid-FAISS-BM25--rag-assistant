[site]: crossvalidated
[post_id]: 484628
[parent_id]: 484616
[tags]: 
When you combine models it's called an Ensemble model. Ensembles use voting, weighting, and or averaging, and there are different algorithms. (Averaging) Most decision tree functions will give you the probability. You could just average the two. (Boosting) Lets say your logistic regression model performs better with the bottom 50% of values of a continuous independent variable, and your decision tree performs better with the top 50% of values of that continuous variable. Your ensemble could choose stronger model for that scenario. (Voting) As a different example, if you had three models, you could use simple voting, and not use an analysis of the training data (which was done above). In response to the comment, I've addressed the "how are models combined" with three examples, and provided the term "ensemble." The "should I" is a different question, which will depend.
