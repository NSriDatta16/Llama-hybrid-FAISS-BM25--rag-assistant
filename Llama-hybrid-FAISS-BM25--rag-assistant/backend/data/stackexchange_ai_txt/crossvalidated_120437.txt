[site]: crossvalidated
[post_id]: 120437
[parent_id]: 
[tags]: 
How to apply the output layer function in a neural network

I am implementing a Neural Network in a somewhat different fashion. I train my neural network locally using a small subset, and export the weights. My goal is to test the neural network in a distributed parallel programming solution using those exported weights. This means that I have to rebuild the NN topology. My activation function for the Hidden layer is the logistic function: $$\frac{1}{1+e^{\theta^TX}}$$ Where $\theta$ is the weight vector and $X$ is the input vector. Once I calculate that for the hidden layer, I don't know how to proceed to the output layer. What is the next calculation/step that I have to implement? P.S: my network dimensions are 255-75-1.
