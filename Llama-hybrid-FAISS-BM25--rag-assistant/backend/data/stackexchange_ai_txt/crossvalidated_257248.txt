[site]: crossvalidated
[post_id]: 257248
[parent_id]: 257127
[tags]: 
You are forgetting about the weights. A Neural Network consist of activations (greater or equal to zero with the ReLu activation function) and weights. The weights can be positive or negative. By weighting a positive ReLu output negatively a negative input for the next layer (e.g. a layer with a sigmoid activation function) emerges. The possible weighted activations of a ReLu layer are thus $[-\infty, \infty]$. The sigmoid layer squashes (sum of) inputs between $[0,1]$ and can be used for a probabilistic interpretation. If you have multiple outputs you can use a softmax layer to normalize all your outputs to sum to 1.
