[site]: crossvalidated
[post_id]: 248079
[parent_id]: 
[tags]: 
How to report average error rate/performance/error metrics from train function in R package caret regarding the hold-out samples from cross-validation

Through R and based on a microarray gene expression dataset (60 samples in total-30 cancer and 30 control samples) and R package caret, i have performed a feature selection regarding a binary categorical outcome (Disease status). My final selected subset, is comprised of both gene features as also clinical continuous variables. As i would like to inspect if these 5 selected clinical variables (among the rest subset of 36 genes), improve the classification procedure instead of using only the gene features, my main issue/concern is the following: 1) I should using my total dataset as a training set with two feature groups: one with only the genes from the subset, and then with all the subset(including the clinical features), to see if this gives better results? But if my notion is correct, can i somehow from the output of the train function from caret package extract any metrics like accuracy or average test error from the testing subsets using in the 10-fold cross-validation ? 2) Or it would be more accurate--despite the small sample size--, to even split my dataset in 80% training and 20% test, and use the small test subset to report metrics, such as AUC, etc ? I did not mention the possibility of using external datasets, because these clinical features have been only measured on the same patients that were also used to create the microarray gene expression data, thus i would like to initially investigate if they have some potential/promising results despite the small sample size, regarding any improvement of prediction of the Disease Status !!
