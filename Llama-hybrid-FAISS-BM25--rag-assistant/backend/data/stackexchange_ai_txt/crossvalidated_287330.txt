[site]: crossvalidated
[post_id]: 287330
[parent_id]: 133261
[tags]: 
You are absolutely right that deciding what terms in a model are relevant by looking at p-values (or AIC or BIC) - or even worse if this is done iteratively by adding and removing terms using e.g. stepwise regression - is not a really good approach for getting the best performing model by almost any standard (and certainly not for out of sample prediction). Perhaps with a single model fitted (from which non-significant terms are not removed), a huge sample size compared to the number of model terms, some sensible multiplicity adjustment and no collinearity, one might look at this as indicating for which terms the evidence is the clearest that they influence the outcome (without necessarily ruling out that other terms also play a role). That's about the most charitable interpretation and may make some sense, if you are not really that interested in prediction. These kind of approaches seem to come from a traditional obsession with p-values and are still somewhat prevalent in some fields. However, even in medical applications universities and companies that have good statistics departments/support typically no longer do that. As you suggest approaches like trying all models with some kind of cross-validation or bootstrapping to try to compensate for the potential overfitting is an improvement on stepwise regression. However, there are many more attractive approaches that are (rightly) gaining in popularity such as Bayesian model averaging, shrinkage priors (such as the horseshoe), LASSO (as an "older" idea), frequentist model averaging (e.g. using weights proportional to exp(-AIC/2)) and various other machine learning type of approaches (e.g. random forests). Many of these have in common that the goal is not to pick one single model, but rather to get good out-of-sample predictions taking into account the uncertainty around what the optimal model is. E.g. in model averaging approaches all models will usually contribute somewhat in real-life examples and situations where one single is clearly better than any other considered model are rare.
