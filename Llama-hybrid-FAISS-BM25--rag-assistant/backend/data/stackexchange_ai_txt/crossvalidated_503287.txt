[site]: crossvalidated
[post_id]: 503287
[parent_id]: 503101
[tags]: 
Overview of what the book is saying According to the book, there are two classes of interest. Let these classes be $\omega_1$ and $\omega_2$ . Also, given a feature vector $\mathbf{x}$ , let: $$ y = \begin{cases} +1 & \text{if} \quad \mathbf{x} \in \omega_1 \\ -1 & \text{if} \quad \mathbf{x} \in \omega_2 \end{cases} $$ According to the book: The basic idea of the algorithm is to assign a previously unseen pattern to the class with closer mean. Given the training dataset $\mathcal{D}$ that consists of feature vectors with their corresponding class labels: $$ \mathcal{D} = \{\mathbf{x}_1|\omega_1,\mathbf{x}_2|\omega_1,...,\mathbf{x}_N|\omega_1,\mathbf{x}_1|\omega_2,\mathbf{x}_2|\omega_2,...,\mathbf{x}_M|\omega_2\} $$ The book also defines the following vectors as the mean vectors for each class: $$ \mathbf{c}_+ = \frac{1}{m_+} \sum_{\{i \vert y_i = +1\}} \mathbf{x}_i \\ \mathbf{c}_- = \frac{1}{m_-} \sum_{\{i \vert y_i = -1\}} \mathbf{x}_i $$ Where $m_+$ is the number of feature vectors that belong to class $\omega_1$ (this is the same as $N$ in $\mathcal{D}$ ) and $m_-$ is the number of feature vectors that belong to class $\omega_2$ (this is the same as $M$ in $\mathcal{D}$ ). This means that, given a not-seen-before feature vector $\mathbf{x}$ , the label $y$ of this feature vector is defined as follows: $$ y = \begin{cases} +1 & \text{if} \quad \text{dist}(\mathbf{x},\mathbf{c}_+) \text{dist}(\mathbf{x},\mathbf{c}_-) \end{cases} \tag{1} \label{eq1} $$ Where $\text{dist}(\cdot)$ is an arbitrary distance function. Note that: $$ \mathbf{x} \in \omega_1 \iff \text{dist}(\mathbf{x},\mathbf{c}_+) \text{dist}(\mathbf{x},\mathbf{c}_-) $$ Relationship to the Bayes classifier The Bayes classifier assigns a feature vector $\mathbf{x}$ to class $\omega_1$ if: $$ p(\omega_1|\mathbf{x}) > p(\omega_2|\mathbf{x}) $$ And assigns a feature vector $\mathbf{x}$ to class $\omega_2$ if: $$ p(\omega_1|\mathbf{x}) In other words: $$ y = \begin{cases} +1 & \text{if} \quad p(\omega_1|\mathbf{x}) > p(\omega_2|\mathbf{x}) \\ -1 & \text{if} \quad p(\omega_1|\mathbf{x}) The goal then is to show that equation \ref{eq1} is the same as equation \ref{eq2}. Since: $$ p(\omega_1|\mathbf{x}) = \frac{p(\mathbf{x}|\omega_1)p(\omega_1)}{p(\mathbf{x})} $$ Using the law of total probability in the denominator: $$ p(\mathbf{x}) = p(\mathbf{x}|\omega_1)p(\omega_1) + p(\mathbf{x}|\omega_2)p(\omega_2) $$ So: $$ p(\omega_1|\mathbf{x}) = \frac{p(\mathbf{x}|\omega_1)p(\omega_1)}{p(\mathbf{x}|\omega_1)p(\omega_1) + p(\mathbf{x}|\omega_2)p(\omega_2)} $$ Dividing the numerator and the denominator by $p(\mathbf{x}|\omega_1)p(\omega_1)$ : $$ p(\omega_1|\mathbf{x}) = \frac{1}{1 + \frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}} $$ Since: $$ \frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)} = \exp\left(\text{ln}\left(\frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}\right)\right) $$ Then: $$ p(\omega_1|\mathbf{x}) = \frac{1}{1 + \exp\left(\text{ln}\left(\frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}\right)\right)} = \sigma\left(\text{ln}\left(\frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}\right)\right) \tag{3} \label{eq3} $$ Where $\sigma(\cdot)$ is the logistic function . Note that since the right hand side of equation \ref{eq3} is only a function of the feature vector $\mathbf{x}$ , then let: $$ \sigma\left(\text{ln}\left(\frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}\right)\right) = \sigma(f(\mathbf{x})) $$ Where: $$ f(\mathbf{x}) = \text{ln}\left(\frac{p(\mathbf{x}|\omega_2)p(\omega_2)}{p(\mathbf{x}|\omega_1)p(\omega_1)}\right) $$ Since: $$ \begin{align} p(\omega_1|\mathbf{x}) + p(\omega_2|\mathbf{x}) &= 1 \\ p(\omega_2|\mathbf{x}) &= 1 - p(\omega_1|\mathbf{x}) \end{align} $$ Then the decision rule for the Bayes classifier becomes: $$ y = \begin{cases} +1 & \text{if} \quad \sigma(f(\mathbf{x})) > 1 - \sigma(f(\mathbf{x})) \\ -1 & \text{if} \quad \sigma(f(\mathbf{x})) Or: $$ y = \begin{cases} +1 & \text{if} \quad \sigma(f(\mathbf{x})) > \frac{1}{2} \\ -1 & \text{if} \quad \sigma(f(\mathbf{x})) Notice that for the logistic function, given an arbitrary input $a \in \mathbb{R}$ , $\sigma(a) \geq 0.5$ for $a \geq 0$ and $\sigma(a) for $a . This means that the Bayes classifier decision rule can be further simplified to: $$ y = \begin{cases} +1 & \text{if} \quad f(\mathbf{x}) > 0 \\ -1 & \text{if} \quad f(\mathbf{x}) Equation \ref{eq1} can be re-written as: $$ y = \begin{cases} +1 & \text{if} \quad \text{dist}(\mathbf{x},\mathbf{c}_+) - \text{dist}(\mathbf{x},\mathbf{c}_-) 0 \end{cases} \tag{5} \label{eq5} $$ Notice any similarity between equation \ref{eq5} and equation \ref{eq4}? They are in fact almost the same, we just need to flip the sign of $y$ for one of them. However, what is more interesting is that: $$ f(\mathbf{x}) \equiv \text{dist}(\mathbf{x},\mathbf{c}_+) - \text{dist}(\mathbf{x},\mathbf{c}_-) $$ The relationship above is almost true. What is missing is $\mathbf{c}_+$ and $\mathbf{c}_-$ in $f(\mathbf{x})$ . This leads us to the central idea of almost all of supervised machine learning. According to the Bayes classifer decision rule, we need to accurately estimate both $p(\omega_1|\mathbf{x})$ and $p(\omega_2|\mathbf{x})$ . This in turn requires estimating $p(\mathbf{x}|\omega_1),p(\mathbf{x}|\omega_2),p(\omega_1),$ and $p(\omega_2)$ . However, why estimate $p(\mathbf{x}|\omega_1),p(\mathbf{x}|\omega_2),p(\omega_1),$ and $p(\omega_2)$ when we can estimate $p(\omega_1|\mathbf{x})$ and $p(\omega_2|\mathbf{x})$ directly? See this question for more details. Recall that: $$ p(\omega_1|\mathbf{x}) = \sigma(f(\mathbf{x})) $$ Therefore, to estimate $p(\omega_1|\mathbf{x})$ and $p(\omega_2|\mathbf{x})$ directly, we can choose some function $f$ and parameterize it by some parameters $\theta$ and estimate these parameters using the training dataset $\mathcal{D}$ : $$ p(\omega_1|\mathbf{x}) = \sigma(f(\mathbf{x};\theta(\mathcal{D}))) $$ In your case, this function is: $$ \text{dist}(\mathbf{x},\mathbf{c}_+) - \text{dist}(\mathbf{x},\mathbf{c}_-) $$ And the parameters are $\theta = (\mathbf{c}_+,\mathbf{c}_-)$ , which are estimated using $\mathcal{D}$ .
