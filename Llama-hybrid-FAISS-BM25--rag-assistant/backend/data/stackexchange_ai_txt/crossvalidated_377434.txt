[site]: crossvalidated
[post_id]: 377434
[parent_id]: 
[tags]: 
Machine Learning - Prediction Interval - Cheating?

I work at a company that is trying to use machine learning methods in particular gradient boosting and neural networks to make predictions on stock market data, so using historical data to predict what the price of a stock/asset will be $x$ time periods from the present. We are using these methods for regression as opposed to classification, and it is my habit being trained in experimental sciences to always give a regression prediction in terms of a $\pm$ , giving an interval of prediction, rather than just one number . My manager (who doesn't seem very technical) told me that this is unacceptable/cheating, as I'm using an interval to cover the fact that my algorithm is unable to produce a single correct number. I'm a bit confused about this attitude, as coming from laboratory sciences (chemistry) we always cite every result in terms of a $\pm$ . So, I was wondering what the stats experts on here thought? Just out of curiosity, I checked my Machine Learning textbook by Hastie, Witten, et. al., and they use the MSE on the test set to give a $\pm$ on the predictions from an example they use on gradient boosting, so it seems standard to do this... Thanks.
