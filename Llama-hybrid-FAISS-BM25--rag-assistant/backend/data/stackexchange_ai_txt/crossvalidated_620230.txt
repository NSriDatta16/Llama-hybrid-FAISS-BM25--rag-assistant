[site]: crossvalidated
[post_id]: 620230
[parent_id]: 620113
[tags]: 
I would say that the Bayesian approach forces the practitioner to take the uncertainty in the parameters into account. The posterior predictive distribution automatically embeds both observation noise and parameter uncertainty. For conjugate distributions the posterior predictive can be found in closed form. In the other cases it is well defined theoretically but hard to find/compute. A large number of approaches (MCMC, HMC, etc), tools and probabilistic programming languages (STAN, pymc, pyro, etc) have been developed specifically to address this sort of questions. In the frequentist approach the are ways to account for the uncertainty in the parameters, as in the linear regression example in the OP. However there are two issues. One is that they are only available in closed form for a limited number of cases (like linear regression) but not for most cases (even simple deviations from LR like with Huber loss or lasso regularisation). The second issue (probably related to the first one) is that in practice you'll find that in many cases (I would dare to say in most cases) practitioners actually make the mistake of fitting some parameters to some data (typically using maximum likelihood) and then use the fitted model (with point estimate of the parameters) to make predictions on new data. There are methods to make predictions with an associated interval (e.g. with "conformal predictions" techniques) but they are often add-ons, rather than part of the model like in the Bayesian approach. On the other hand, since they are less reliant on assumptions than Bayesian techniques, they may be more robust and accurate in practice. In other words my impression is that it is virtually impossible to "forget" to account for parameters uncertainty in Bayesian statistics while it's possible (and often done "in the field") in frequentist statistics.
