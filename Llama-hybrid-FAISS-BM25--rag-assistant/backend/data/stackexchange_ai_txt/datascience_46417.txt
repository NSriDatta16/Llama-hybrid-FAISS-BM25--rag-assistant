[site]: datascience
[post_id]: 46417
[parent_id]: 46274
[tags]: 
Your understanding is correct. Finding correlations between variables is simple but turning them into causal assertions needs an extra effort. Causal inference is used mostly to reach a "prescription" in the form of "do X so that Y happens". When not to use Causal inference: If it is possible to do experiments, causal inference can be avoided. For example, A\B tests let you study the effect of a change in two groups and reach a causal conclusion. For example, result of an A\B test would be "users in group A which see a button with color intensity 50 clicked 10% more than group B with color intensity 40", so do X='increase color intensity of the button' so that Y='more click' happens. With larger, more uniform groups, your assertion would be more reliable. Causal inference in Machine learning: In most machine learning projects these type of experiments are possible and mostly cheap, therefore why bother? Moreover, specially in predictive projects, value comes from correlated relations. Knowledge of causal relations, which are a subset of correlated relations, does not add value. Causal inference: When you work with historical data or you can only "observe" the data without affecting it, causal inference comes into play. Generally, causal inference is a controversial topic as it tries to extract causal relations from observational data (as opposed to experimental data in A\B tests). To best of my knowledge, the main contributor to causal inference is Prof. Judea Pearl. His underlying tools are probabilistic graphical models (PGMs) and do-calculus. These tools let us explicitly encode our assumptions about the mechanics of data generation, and reach causal conclusions. So when an assertion like "do X, so that Y happens" goes wrong, we can track the problem in our assumptions in a principled way. For example, we may have ignored an important hidden variable that by including it our conclusion would change. He fundamentally says, anyone who reaches a "prescriptive" conclusion is doing causal inference, so it is better to lay out your assumptions explicitly, to prevent correlation-causation problems going unnoticed. Some interesting resources: Simpson's paradox is an entry point to become interested in causal inference, This paper of Judea Pearl connects the paradox with causal inference.
