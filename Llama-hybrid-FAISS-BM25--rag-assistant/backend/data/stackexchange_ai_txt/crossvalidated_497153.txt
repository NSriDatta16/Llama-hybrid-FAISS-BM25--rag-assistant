[site]: crossvalidated
[post_id]: 497153
[parent_id]: 497070
[tags]: 
Your response variable is binary - fitting a linear regression model , lm(), to a binary variable is not the best thing you can do. As you can see from your own response, a linear regression model can produce "probabilities" whose values are non-sensical (e.g., > 1). By definition, probabilities can only take values between 0 and 1. Why not use a more appropriate model for your data, namely a binary logistic regression model ? In R, you would fit this model with: model 100) ~ cyl + wt + disp + am, data = mtcars, family = binomial(link="logit")) The R command summary(model) will summarize the glm() model fit, showing how the log odds that hp > 100 depend on the values of the predictor variables included in the model. The glm() model can be used as a basis for predicting the probability that hp > 100 as a function of the cyl, weight, disp and am values for the cars represented in your data: prob Now, let's say you want to focus only on those cars with am = 0 represented in your data; for these cars, you can estimate the "average" probability that they have hp > 100. This involves subsetting prob, as computed above, so that it includes only probabilities for which am = 0, and then computing the average of the subsetted probabilities (across the combinations of cyl, weight and displ values represented in that subset). You can get a 95% confidence interval for the "average" probability using bootstrapping. Essentially, repeat the subsetting process described above for each of B = 999 (say) bootstrap samples and save your "average" probability for each such sample. Then order the bootstrap "average" probabilities from smallest to largest and report the appropriate quantiles of their distribution as the endpoints of your confidence interval.
