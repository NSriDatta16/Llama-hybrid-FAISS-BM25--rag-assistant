[site]: datascience
[post_id]: 118715
[parent_id]: 118697
[tags]: 
In general, BERT is a much stronger model. Word embeddings only represent isolated words, whereas BERT considers the sentence context and how the words interact. With user-generated data, word embeddings might have plenty of OOV. In contrast, BERT uses subword tokenization that might partially compensate for that, although it is not ideal. It might be better to search for BERT-like models pre-trained specifically on social network data (e.g., TwHIN-BERT ). The only disadvantage is higher computational complexity than classical machine learning over word embeddings. As with any large model, it is prone to overfitting and catastrophic forgetting when not fine-tuned carefully. This might be the reason why you get slightly worse results with BERT. You can try smaller learning, training for fewer epochs, or freezing some layers.
