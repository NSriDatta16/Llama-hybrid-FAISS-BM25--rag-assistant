[site]: crossvalidated
[post_id]: 352040
[parent_id]: 352029
[tags]: 
One-hot encoding is the most widely used general purpose method for unordered categorical features. It's a non-lossy encoding of the feature set & allows models to handle interactions between categories; furthermore, certain model fitting procedures are amenable to sparse representations of the feature set that one-hot produces, allowing for efficient models using one-hot encoding. That said, there are alternative encodings that may be useful, particularly in scenarios where dimensionality & data size are an issue: Impact coding - map categorical features to a single continuous value representing the conditional probability of the response. This comes with several difficulties including high bias estimates in sparse subspaces of the feature set, as well as potential for data leakage (user must be disciplined in their approach to cross-validation) A somewhat related approach is to fit a model on a subset of the one-hot encoded data and extract a smaller set of features (perhaps including interactions between different sets of features). An example of this is RuleFit , which fits tree ensembles to both continuous & categorical features, extracting a concise set of binary features from the fitted trees representing subsets of features (both continuous & categorical) as well as interactions between features. For more structured or domain specific data, previously developed embeddings may exist (e.g. word2vec for text data) As a final note, ordered categorical data will offer a richer set of feature engineering techniques- be mindful of this property of the data.
