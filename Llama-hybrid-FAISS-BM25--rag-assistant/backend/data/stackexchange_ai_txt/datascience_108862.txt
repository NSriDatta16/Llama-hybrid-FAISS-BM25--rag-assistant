[site]: datascience
[post_id]: 108862
[parent_id]: 108829
[tags]: 
Tested bert-base-german-cased from huggingface today. The results are still different, but much more similar. Also, the tokenizer splits words in a desired way. This might already work for my use case, so marking the question as answered. token1 token2 cos-similarity [CLS] [CLS] 0.982 Haupt Haupt 0.933 - ##satz 0.824 und und 0.967 Neben Neben 0.951 ##satz ##satz 0.958 [SEP] [SEP] 0.977 Code to reproduce: from transformers import BertTokenizer, BertModel, AutoModel import numpy as np tokenizer = AutoTokenizer.from_pretrained("bert-base-german-cased") model = AutoModel.from_pretrained("bert-base-german-cased") def calc_cosine_similarity(a, b): return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) def calc_bert_embeddings(text): """Returns a (N,768) vector representing embeddings for the N tokens.""" tokens = tokenizer(text, return_tensors='pt') output = model(**tokens) return tokens.tokens(), output["last_hidden_state"][0,:,:].detach().numpy() tokens1, vec1 = calc_bert_embeddings("Haupt- und Nebensatz") tokens2, vec2 = calc_bert_embeddings("Hauptsatz und Nebensatz") for idx in range(len(tokens1)): similarity = calc_cosine_similarity(vec1[idx,:], vec2[idx,:]) print(f"{tokens1[idx]};{tokens2[idx]};{similarity:.3f}")
