[site]: datascience
[post_id]: 42657
[parent_id]: 
[tags]: 
What are the possible approaches to fixing Overfitting on a CNN?

Currently I am trying to make a cnn that would allow for age detection on facial images. My dataset has the following shape where the images are grayscale. (50000, 120, 120) - training (2983, 120, 120) - testing And my model currently looks like the following - I've been testing/trying different methods. model = Sequential() model.add(Conv2D(64, kernel_size=3, use_bias=False, input_shape=(size, size, 1))) model.add(BatchNormalization()) model.add(Activation("relu")) model.add(Conv2D(32, kernel_size=3, use_bias=False)) model.add(BatchNormalization()) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, use_bias=False)) model.add(BatchNormalization()) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) #TODO: Add in a lower learning rate - 0.001 adam = optimizers.adam(lr=0.01) model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy']) model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=number_of_epochs, verbose=1) After running my data on just 10 epochs I started to initially see decent values but at the end of the run my results were the following and it has me concerned that my model is definitely over fitting. How many epochs: 10 Train on 50000 samples, validate on 2939 samples Epoch 1/10 50000/50000 [==============================] - 144s 3ms/step - loss: 1.7640 - acc: 0.3625 - val_loss: 1.6128 - val_acc: 0.4100 Epoch 2/10 50000/50000 [==============================] - 141s 3ms/step - loss: 1.5815 - acc: 0.4059 - val_loss: 1.5682 - val_acc: 0.4059 Epoch 3/10 50000/50000 [==============================] - 141s 3ms/step - loss: 1.5026 - acc: 0.4264 - val_loss: 1.6673 - val_acc: 0.4158 Epoch 4/10 50000/50000 [==============================] - 141s 3ms/step - loss: 1.3996 - acc: 0.4641 - val_loss: 1.5618 - val_acc: 0.4209 Epoch 5/10 50000/50000 [==============================] - 141s 3ms/step - loss: 1.2478 - acc: 0.5226 - val_loss: 1.6530 - val_acc: 0.4066 Epoch 6/10 50000/50000 [==============================] - 141s 3ms/step - loss: 1.0619 - acc: 0.5954 - val_loss: 1.6661 - val_acc: 0.4086 Epoch 7/10 50000/50000 [==============================] - 141s 3ms/step - loss: 0.8695 - acc: 0.6750 - val_loss: 1.7392 - val_acc: 0.3770 Epoch 8/10 50000/50000 [==============================] - 141s 3ms/step - loss: 0.7054 - acc: 0.7368 - val_loss: 1.8634 - val_acc: 0.3743 Epoch 9/10 50000/50000 [==============================] - 141s 3ms/step - loss: 0.5876 - acc: 0.7848 - val_loss: 1.8785 - val_acc: 0.3767 Epoch 10/10 50000/50000 [==============================] - 141s 3ms/step - loss: 0.5012 - acc: 0.8194 - val_loss: 2.2673 - val_acc: 0.3981 Model Saved I assume the issue might be related to the number of images I have for each output class, but other then that I am a bit stuck in moving forward. Is there something wrong in my understanding/implementation? Any advice or critique would be well appreciated this is more of a learning project for me.
