[site]: crossvalidated
[post_id]: 370793
[parent_id]: 364584
[tags]: 
Here is my guess (I do not know much about this topic either, just wanted to add my two cents to this discussion). I think that you're right, there's no point in training a classical model and using its predictions as data, because as you say, there's no incentive to the optimiser to do any better. I would guess that randomised-starting algorithms are more likely to find the same optimum because they'd be "more sure" that the previously found optimum is correct, due to the larger data set, but this is irrelevant. That said, the first answer you received has a point - that example on Wikipedia talks about clustering, and I think that makes all the difference. When you've got unlabelled data, you essentially have a bunch of unlabelled points lying on some shared "latent feature space" as the other labelled ones. You can only really do better than a classification algorithm trained on the labelled data, if you can uncover the fact that the unlabelled points can be separated and then classified based on what class the labelled points belong to, on this latent feature space. What I mean is, you need to do this: $$labelled\;data \rightarrow clustering \rightarrow classification$$ ... and then repeat with unlabelled data. Here, the learned cluster boundaries will not be the same, because clustering doesn't care for class labels, all it accounts for is transforming the feature space. The clustering generates a latent feature space, on which the classification boundary is learned, and this depends only on labelled data. Algorithms that do not perform any sort of clustering, I believe, will not be able to change their optimum based on the unlabelled data set. By the way, the image that you linked does a fair job I think of explaining what's going on here; a decision boundary is learned based solely on the clustering algorithm. You have no idea what the correct classes are here - it may be the case that they're all random - we don't know. All we can now is that there seems to be some structure in the feature space, and there seems to be some mapping from the feature space to the class labels. Don't really have references but on this Reddit post , as I understand it, there's a discussion about a GAN performing semi-supervised learning. It is a hunch of mine that it implicitly performs a clustering, followed by classification.
