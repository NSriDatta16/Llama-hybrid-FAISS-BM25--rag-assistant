[site]: datascience
[post_id]: 118451
[parent_id]: 118375
[tags]: 
Softmax cannot give you zero probabilities, by definition. If this non-zero probability mass is a problem for your predictions, you may want to look into SparseMax, which is a sparse variation of softmax that can actually assign zero to part of the probability space. It was introduced in the article From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification (published at ICML'16). You can find implementations in both Pytorch and Tensorflow on github. It may be interesting to check out the recent research of the first author, Andr√© F.T. Martins (see his Google Scholar profile , who has been working on sparse neural networks.
