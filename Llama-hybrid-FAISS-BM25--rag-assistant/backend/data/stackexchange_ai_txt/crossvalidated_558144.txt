[site]: crossvalidated
[post_id]: 558144
[parent_id]: 558060
[tags]: 
The usual hyperparameters for a RF are number of trees number of attributes that are randomly selected for the split search when constructing each tree limits on the tree depth (for each tree) in sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html these are the n_estimators, max_features and max_depth in the R package randomForest https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest they are ntree, mtry, and there is no equivalent to the max tree depth. The number of trees is usually not important and could be fixed if the number of high. If you have no concerns regarding computation time, ser the number of trees to 1000 and I believe, there is no need to search for other values. If you do, the search should be geometric like 100, 200, 500, 1000, 2000, and so on. The number of features to select is by default the square root of the number of atributes (for classification) or the number of attributes for regression. In my experience, this hyperparameter is not that important and if you have limits on the time to do the hyperparameter search, you can accept the default. If you want to search, in your case test for 6 ,7 10, 12 and maybe 20 (for classification) The last hyperparameter (limits of the tree depth) is also not significant, in my experience. The randomForest package, controls the depth by the minimum number of cases to perform a split in the tree construction algorithm, and for classification they suggest 1, that is no constraints on the depth of the tree. Sklearn uses 2 as this min_samples_split. If you plan to search this hyperparameter, I think it is wiser to control the minimum number of samples to split the tree, and 1, 2 or 5 seems reasonable values. To summarize, in my experience the defaults for the RF hyperparameters are usually good enough (provided ntree is large - I think sklearn default of 100 trees is too low - it was even lower in previous versions of the package). This is the main advanatge of RF - usually you do not need to search for hyperparameters and it is trivially parallelizable if training time is a problem, and it is likely on of the three best algorithms for most classification problems (together with RBF SVM and gradient boosting).
