[site]: crossvalidated
[post_id]: 252577
[parent_id]: 
[tags]: 
Bayes regression: how is it done in comparison to standard regression?

I got some questions about the Bayesian regression: Given a standard regression as $y = \beta_0 + \beta_1 x + \varepsilon$. If I want to change this into a Bayesian regression, do I need prior distributions both for $\beta_0$ and $\beta_1$ (or doesn't it work this way)? In standard regression one would try to minimize the residuals to get single values for $\beta_0$ and $\beta_1$. How is this done in Bayes regression? I really struggle a lot here: $$ \text{posterior} = \text{prior} \times \text{likelihood} $$ Likelihood comes from the current dataset (so it's my regression parameter but not as a single value but as a likelihood distribution, right?). Prior comes from a previous research (let's say). So I got this equation: $$ y = \beta_1 x + \varepsilon $$ with $\beta_1$ being my likelihood or posterior (or is this just totally wrong)? I simply can't understand how the standard regression transforms into a Bayes one.
