[site]: crossvalidated
[post_id]: 300105
[parent_id]: 
[tags]: 
PCA, dimensionality, and k-means results: reaction to duplicating of variables

There are many excellent conversations on CV about the curse of dimensionality when applied to methods like k-means . The answer in the same post and other research (e.g., the paper titled "When Is ‘Nearest Neighbor’ Meaningful: A Converse Theorem and Implications" ) point that if the "intrinsic" domensionality of the data is lower then the curse does not apply. However, I am coming from the angle of practical application, which seems to be not clearly answered. Let's say we want to do k-means clustering in high dimensions. The data consists of $n$ rows and $d$ features. To make the conversation simpler, we can assume that the $d$ features are not independent and the intrinsic dimensionality of the data is actually $m$ such that $m All of the $m$ features are duplicated multiple times to create the data with $d$ features. For example, all the features are duplicated once and we get $d = 2m$. Some of the $m$ features are duplicated to make up the $d$ features. For example, only the first feature is duplicated. Now if PCA is run on this data then in both cases it will correctly say that the data is actually $m$ dimensional as 100% variance will be explained by the $m$ eigenvalues. Thus, one can conclude that the curse of dimensionality does not apply in either case and use all $d$ features to calculate distances between the points. However, the Euclidean distances in the two cases will be very different. Specifically, in the first case it will be the same as the $m$ dimensional data, while in the second case it will be biased. Question : As we do not know the correlation structure of the data, should all analysis (e.g. k-means) be run on the $m$ PCA scores instead of the $d$ dimensional data? EDIT : I am adding some R code and the resulting plot to show that the k-means results actually differ. The data x is available here . The data x1 simply duplicates the features of x and the data x2 duplicates only the first feature of x. PCA on both x1 and x2 show that first two components retain ~100% of cumulative variance. So I expect that all the plots on the right hand column to show similar clustering. set.seed(1) x
