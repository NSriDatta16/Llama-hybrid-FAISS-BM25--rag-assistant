[site]: stackoverflow
[post_id]: 4406767
[parent_id]: 4406574
[tags]: 
To be honest it looks like you're missing some major experience almost necessary to implement a system like to describe. Additionally "on a very small scale" definitely contradicts with a million files in less than a month. I'll try to give answers to your questions. Organizing files is much about giving them reasonable names. If you let the user choose the filename, watch out that you filter that correctly to block attacks based on filenames like "../../../etc/passwd" (You SHOULD understand that.). I recommend you using hashes as filenames. Additionally you can assign them public "filenames" (actually aliases via a database). After uploading calculate a hash of the file. If the number of files increases you can store them in directories named after the first 2 chars of the hash. This is what Git VCS does and i really like that. What exactly do you mean by that? If you plan to have 1 single upload server and mirror the uploaded files to other servers you can easily separate these processes. Create an easy upload page and write another mirror script which sends the files e.g. via FTP to the other servers. Otherwise if you are about to create something called a cluster (several web servers for the same purpose performing load balancing and providing high availability), then there's no short answer on how to do this. Many wise men earn much money because they have the necessary experience and skills to implement such systems. If you are keen enough to do this on your own, you should go read some books about that. I don't want to question your motivation but why do you want to prevent the usage of download managers? These are very helpful to resume aborted downloads and thus help to lower the traffic of your server. It saves you traffic, bandwith, energy costs and CPU time. Is that too bad for you? More technically you need to configure your HTTP server, like e.g. Apache, to disable resume. I have no clue what the appropriate option is but I reckon there is one. Alternatively you can provide the files via a PHP script instead of linking to the file directly. The script gets the ID of the file via URL parameter and sends the content of the file (which must not reside in WWW root in this case) back to the client. This way it is your own responsibility to implement resumes or not and thus you can "disable" it easily. If you're actually about to avoid multiple downloads I would instead recommend using complex IDs like hashes (nobody can guess the link to download the file) and implementing some script which deletes the file after complete download. As I said disabling download managers harms you and your users. I hope this is helpful to get a general image of the complexity of your idea.
