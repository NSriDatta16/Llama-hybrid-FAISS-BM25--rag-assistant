[site]: datascience
[post_id]: 47127
[parent_id]: 33442
[tags]: 
To know how RPN work for training, we can dive into the code wrote by Matterport, which is 10,000 stared and tf/keras implementation Mask R-CNN repo. You can check the build_rpn_targets function in mrcnn/model.py If we used the generated anchors (depends on your anchor scales, ratio, image size ...) to calculate the IOU of anchors and ground truth, # Compute overlaps [num_anchors, num_gt_boxes] overlaps = utils.compute_overlaps(anchors, gt_boxes) we can know how overlaps between anchors and ground truth. Then we choose positive anchors and negative anchors based on their IOU with ground truth. According to Mask R-CNN paper, IOU > 0.7 will be positive anchors and # 1. Set negative anchors first. They get overwritten below if a GT box is # matched to them. anchor_iou_argmax = np.argmax(overlaps, axis=1) anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax] rpn_match[anchor_iou_max = 0.7] = 1 To effectively train RPN, you need to set up the RPN_TRAIN_ANCHORS_PER_IMAGE carefully to balance training if there is few objects in one image. Please note that there can be multiple anchors match one ground truth since we can give the bbox off-set for each anchor to fit the ground truth. Hope the answer is clear for you!
