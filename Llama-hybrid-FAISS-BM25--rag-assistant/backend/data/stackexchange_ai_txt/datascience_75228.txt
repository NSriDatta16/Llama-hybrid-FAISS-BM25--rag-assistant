[site]: datascience
[post_id]: 75228
[parent_id]: 75003
[tags]: 
It is common in applied machine learning to have the model with the lowest generalization error, as measured by score on validation data, also have the biggest delta from the score on the training data. There is nothing inherently wrong with overfitting, it depends on the goal of the project. The typical goal of applied machine learning is high predictive ability on unseen data, aka low generalization error. It is okay if the model "memorizes" more of the training data if that helps the model improve generalization. Given the trend that as the number of layers increases generalization error decreases, the performance of the model might improve if the number of layers continues to increase.
