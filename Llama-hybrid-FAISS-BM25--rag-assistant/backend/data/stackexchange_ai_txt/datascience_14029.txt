[site]: datascience
[post_id]: 14029
[parent_id]: 14028
[tags]: 
I think the key here is that when in training, having same weights (and same neurons in the hidden layer) will not lead to optimal solution, it is the different weights that will lead to lower difference between actual and predicted values. I coded up this really dumb Neural Network (as a learning exercise). Maybe it could be of some help. import numpy as np class NeuralNetwork(object): def __init__(self, X, Y, hidden_layer_dim): self.X = X / np.max(X) self.Y = Y / np.max(Y) # Used for training self.hidden_layer_dim = hidden_layer_dim def initialize_weights(self): self.w1 = np.random.normal(0,1, (self.X.shape[1], self.hidden_layer_dim)) self.w2 = np.random.normal(0,1, self.hidden_layer_dim) def forward(self, xi): """ x1 is 2d array """ # This method is also used for training xi = xi / np.max(xi) z2 = np.dot(xi, self.w1) a2 = sigmoid(z2) z3 = np.dot(a2, self.w2) y_hat = sigmoid(z3) return y_hat def dump_train(self, n_iterations): min_mse = np.inf for i in range(n_iterations): w1 = np.random.normal(0,1, (self.X.shape[1], self.hidden_layer_dim)) w2 = np.random.normal(0,1, self.hidden_layer_dim) z2 = np.dot(self.X, w1) a2 = sigmoid(z2) z3 = np.dot(a2, w2) y_hat = sigmoid(z3) diff = self.Y - y_hat mse = np.dot(diff, diff) if mse Result: weight 1: [[-0.13787113 -1.30913914 0.64624687] [-1.76733779 0.77449265 1.61122177]] weight 2: [-1.42489674 -1.94360005 2.56365303] The weights are different.
