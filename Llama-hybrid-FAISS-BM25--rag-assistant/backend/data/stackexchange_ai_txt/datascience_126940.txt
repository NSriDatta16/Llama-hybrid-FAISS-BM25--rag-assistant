[site]: datascience
[post_id]: 126940
[parent_id]: 
[tags]: 
What are the differences between Embedding Layer and Roberta Embedding?

I'm reading an article about the Embedding Layer: The Embedding Layer learns word embeddings from raw text. It is initialized with small random numbers and can be learned simultaneously with a neural network in a supervised way using backpropagation for a specific task, such as text classification. So, the Embedding Layer learn word embeddings from scratch, correct? In comparison to Roberta's word embeddings, using a pretrained model from HuggingFace's Transformer, are there any differences between them ?
