[site]: crossvalidated
[post_id]: 60008
[parent_id]: 55068
[tags]: 
The paper suggested by learner above is a good suggestion of how this can be accomplished. I would also suggest using the leave-one-out cross-validation predictions to fit the sigmoid function, rather than the actual output on the training examples as extra insurance against over-fitting the sigmoid. However, if you are interested in more than binary classification, then you might be better off using kernel logistic regression instead of the SVM, where the output is an estimate of a-posteriori probability of class membership, and so will be in the range [0-1]. However the training criterion used for KLR takes the who range of probability into account rather than concentrating on the decision boundary. Prof. Vapnik quite rightly says that if you want to solve a problem, you ought to choose a method that solves it directly, rather than solving a more general problem and then simplifying the solution (as the former approach concentrates on what is actually important). I would argue that you should also avoid solving a more simple problem and then post-processing it to approximate the solution to a more general problem - which is essentially what we are doing when fitting a sigmoid to the output of an SVM. If we want probabilities, we should estimate them directly using KLR; if we want binary classification, we should use and SVM, rather than thresholding the probabilities provided by KLR.
