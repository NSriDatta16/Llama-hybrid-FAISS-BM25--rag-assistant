[site]: crossvalidated
[post_id]: 91976
[parent_id]: 91967
[tags]: 
Your second paragraph, in a sense, hints at a answer to the first. In time series processes, where you are at point $t$ is partly dependent on where you were just recently, at point $t-1$. Observations in a time series are not independent in most cases. Whether your behaviour is driven or chaotic, you cannot just easily escape the position you were at immediately preceding time $t-1$. If a second ago you were in your kitchen you can't find yourself next moment in any place of you home with equal probability: you are likely to be somewhere still around your kitchen. Time series has "sliding memory". No wonder that it almost always has considerable autocorrelation with itself by lag 1. Greater lag usually relaxes the autocorrelation since "memory" for the past vanishes. But if the behaviour is cyclic to some extent, with period $p$, you find yourself at $t$ close to where you were at $t-p$. Thus, the autocorrelation with lag $p$ will be strong enough, stronger than with lag $p-1$ or lag $p+1$. Autocorrelograms (ACF) and partial autocorrelograms (PACF) are the main tools to detect autocorrelations with various lags. Cross-correlations are similar to autocorrelations, only here a time series is correlated not with itself (with a lag) but with some parallel time process (with a lag, lag might be 0). Auto/cross correlations extend from time series to any series. A series of observations is their sequence. Are observations not random in that they are tied in a chain or in rings? Examining autocorrelations may discover it.
