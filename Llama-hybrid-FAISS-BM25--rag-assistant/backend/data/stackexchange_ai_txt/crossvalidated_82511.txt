[site]: crossvalidated
[post_id]: 82511
[parent_id]: 82503
[tags]: 
To put @ziggystar's response in terms of machine learning jargon: the idea behind bootstrap aggregation techniques (e.g. Random Forests) is to fit many low-bias, high-variance models to data with some element of "randomness" or "instability." In the case of random forests, instability is added through bootstrapping and by picking a random set of features to split each node of the tree. Averaging across these noisy, but low-bias, trees alleviates the high variance of any individual tree. While regression/classification trees are "low-bias, high-variance" models, linear regression models are typically the opposite - "high-bias, low-variance." Thus, the problem one often faces with linear models is reducing bias, not reducing variance. Bootstrap aggregation is simply not made to do this. An addition problem is that bootstrapping may not provide enough "randomness" or "instability" in a typical linear model. I would expect a regression tree to be more sensitive to the randomness of bootstrap samples, since each leaf typically only holds a handful of data points. Additionally, regression trees can be stochastically grown by splitting the tree on a random subset of variables at each node. See this previous question for why this is important: Why are Random Forests splitted based on m random features? All that being said, you can certainly use bootstrapping on linear models [LINK] , and this can be very helpful in certain contexts. However, the motivation is much different from bootstrap aggregation techniques.
