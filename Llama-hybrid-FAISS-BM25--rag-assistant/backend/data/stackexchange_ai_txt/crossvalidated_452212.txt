[site]: crossvalidated
[post_id]: 452212
[parent_id]: 452157
[tags]: 
I'm opening up the statement made in the comments. A neural network with linear activation and many layers is equivalent to a neural network with one layer. For example: $$W_2(W_1x+b_1)+b_2=Wx+b$$ So, in the end, your equation is $Wx+b=y$ , i.e. just a linear regression. The cost function is already convex, because it is norm-squared of an affine function. Also, more strongly, the problem is a convex problem .
