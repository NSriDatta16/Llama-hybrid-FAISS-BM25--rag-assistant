[site]: datascience
[post_id]: 113936
[parent_id]: 
[tags]: 
Weird consequence of not freezing layers in Neural Network

I was researching about "why are we freezing layers" and I came across the answer says "to not lose the information of pre-trained model" But; we are just freezing early layers (I know why). For example: our data is so similar to the data that the model trained on. Let's say we are not freezing any layer. The model will make very small mistakes and convergence will be less, we will not be destroying any information (even if, weights will change very little). Am I wrong? If I am not, then why are we freezing any layer?
