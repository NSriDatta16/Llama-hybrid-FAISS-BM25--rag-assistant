[site]: datascience
[post_id]: 94688
[parent_id]: 94686
[tags]: 
The input to an LSTM must be a batch of sequences of vectors of real numbers, i.e. 3D tensor). Textual inputs are discrete tokens, so they are a batch of sequences of integers (indexes to the vocabulary table), i.e. a 2D tensor. Before any reshaping, you must transform each integer value into a vector. For that, the usual approaches are one-hot encoding and embedding tables: With one-hot encoding, you encode each integer index as a vector with 359 elements, where all are 0s except the position at the integer index, which is a 1. With an embedding table , you keep a table with 359 vectors of an arbitrary dimensionality (the embedding size). In either case, you get the "extra dimension" that you were lacking with integer indices.
