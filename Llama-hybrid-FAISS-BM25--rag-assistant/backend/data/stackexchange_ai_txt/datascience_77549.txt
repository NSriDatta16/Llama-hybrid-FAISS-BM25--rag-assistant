[site]: datascience
[post_id]: 77549
[parent_id]: 77545
[tags]: 
Using all features would usually lead to over-fitting, that is your model would not generalise well to unseen data. To overcome this, while using as much information possible we resort to feature selection ( sometimes feature generation ), and dimensionality reduction. Dimensionality reduction techniques like Principal Component Analysis (PCA) tries to find new features that explain maximum variability in data, using the existing features. In this way, we ideally reduce the number of features in the model, while making sure most of the variability is explained. So as a result: This new model is computationally faster since we are no longer using all the features and burdening our model. Generalises well since we are not over-fitting leading to greater accuracy on unseen data.
