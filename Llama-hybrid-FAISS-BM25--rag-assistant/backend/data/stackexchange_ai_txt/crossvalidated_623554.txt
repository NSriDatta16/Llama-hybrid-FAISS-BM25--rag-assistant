[site]: crossvalidated
[post_id]: 623554
[parent_id]: 
[tags]: 
Trying to understand the theory behind my similar / better results than XGBoost using a calibrated linear model (GAM)

I just opened a discussion on reddit asking about why/how the calibrated linear models I've been training have been getting similar / better results than XGBoost in my experiments. I was told to cross post here with my question to field some answers from seasoned statisticians. Calibrated linear models have the following form: f(x) = b + a_1 * c_1(x_1) + ... + a_n * c_n(x_n) We have the linear combination (with optional bias) of the calibrated values where each calibrator is a piece-wise linear function (for numerical features) or a 1:1 mapping (for categorical features). The calibrators are learned along with the rest of the network via SGD. This paper section 7 talks about calibrators in more detail. My understanding is that these calibrated linear models are a form of GAM, but I'm hoping to better understand why I'm able to get these results even without more complex feature crossing. A bonus of these calibrated models is that you can constrain the linear coefficients as well as the shape of individual calibrators to easily guarantee certain shape constraints (e.g. monotonicity). However, I didn't use any constraints in my experiments.
