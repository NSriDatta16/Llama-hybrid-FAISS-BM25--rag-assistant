[site]: crossvalidated
[post_id]: 552554
[parent_id]: 552531
[tags]: 
Great questions. There are lots of ways to answer these. John Madden does an excellent job, but I'm going to crib a little bit from Ben's answer here regarding sufficient statistics. The loss function for a Gaussian linear model (as Ben notes) is $$ \ell_{\mathbf{y}, \mathbf{x}}(\boldsymbol{\beta}, \sigma)=-n \ln \sigma-\frac{1}{2 \sigma^{2}}\|\mathbf{y}-\mathbf{x} \boldsymbol{\beta}\|^{2}$$ In a lot of pedagogical material on fitting models, we would use the data to compute gradients of the loss and perform some sort of optimization routine. Some code to do this might look like def compute_loss(beta, X, y): number_rows = len(X) loss = 0 # Potentially expensive! for i in range(number_rows): loss += (y[i] - X[i]@beta)**2 loss/=number_rows return loss If we have lots of data, then this loop (and any additional iterations over the data to compute gradients, for example) might be expensive to compute. But, because we can write down sufficient statistics for the exponential family, we can improve our computation drastically. As Ben writes, the loss can be rewritten as $$ \begin{aligned} &=-n \ln \sigma-\frac{1}{2 \sigma^{2}} \mathbf{y}^{\mathrm{T}} \mathbf{y}-\frac{1}{2 \sigma^{2}}\left(2 \boldsymbol{\beta}^{\mathrm{T}} \mathbf{T}_{1}-\boldsymbol{\beta}^{\mathrm{T}} \mathbf{T}_{2} \boldsymbol{\beta}\right) \\ &=h(\mathbf{y}, \sigma)+g_{\beta}\left(\mathbf{T}_{1}, \mathbf{T}_{2}, \sigma\right) \end{aligned} $$ Where $\mathbf{T}_{1} \equiv \mathbf{T}_{1}(\mathbf{x}, \mathbf{y}) \equiv \mathbf{x}^{\mathrm{T}} \mathbf{y}$ and $\mathbf{T}_{2} \equiv \mathbf{T}_{2}(\mathbf{x}, \mathbf{y}) \equiv \mathbf{x}^{\mathrm{T}} \mathbf{x}$ . We can thus compute these quantities one time and fit our models, as opposed to recomputing them at each update step. Let's compare on a simulated problem. I will assume $\sigma=1$ and does not require estimation for simplicity. I'll vary the number of observations and the number of covariates in the model and compare the time to optimize the loss assuming when using sufficient stats and when using a naive approach. Shown below is a plot of the expected time to completion plus/minus a standard deviation. We can see that as the data become larger in size, using sufficient statistics is advantageous. Now, this whole answer is a bit of a straw man. I've not shown you the results when not using sufficient statistics and not using loops (so maybe leveraging some linear algebra). But the point about sufficient statistics remains. That we can represent all the information in a sample with a single number, computed once, is a very valuable property to have. Code to reproduce experiments: import numpy as np import pandas as pd from scipy.optimize import minimize from itertools import product import matplotlib.pyplot as plt import seaborn as sns sns.set_theme(style="darkgrid") # First, simulate big data! def make_data(N, p): X = np.random.normal(size = (N,p)) beta = np.random.normal(2, 2, size = p) y = X@beta + np.random.normal(size = N) return X,y # Next, set up a loss function to optimize using loops def solve_naive(N, p): X, y = make_data(N, p) def loss_and_grad(w): number_rows, number_columns = X.shape grads = np.zeros_like(w) loss=0 for i in range(number_rows): res= (y[i] - X[i]@w) loss+= res**2/number_rows grads+= -2*res*X[i]/number_rows return loss, grads r=minimize(loss_and_grad, x0=np.zeros(p), jac=True) return loss_and_grad # Next, set up a loss function to opimize which only uses sufficient statistics def solve_sufficient(N, p): X, y = make_data(N, p) T1 = X.T@y T2 = X.T@X const = y.T@y n = len(X) def loss_and_grad(w): loss = const - (2*w@T1 - w@T2@w) grads = -2*T1 + (T2@w + T2.T@w) return loss/n, grads/n r=minimize(loss_and_grad, x0=np.zeros(p), jac=True) return loss_and_grad # A helper function to time the optimization for various datasets def time_to_optimize(N, p): naive_optimization_times = %timeit -o -n 10 solve_naive(N, p) suff_optimization_times = %timeit -o -n 10 solve_sufficient(N, p) suff = pd.DataFrame({'times':suff_optimization_times.timings, 'type': 'Sufficient Statistics'}) naive = pd.DataFrame({'times':naive_optimization_times.timings, 'type': 'Naive'}) df = pd.concat((suff, naive)) df['N'] = N df['p'] = p return df if __name__ == '__main__': Ns = [1_000, 10_000, 100_000] ps = [10, 100, 250] prods = product(Ns, ps) frames = [] for N, p in prods: frames.append(time_to_optimize(N, p)) df = pd.concat(frames).reset_index(drop=True) fig, ax = plt.subplots(dpi = 240, figsize = (8, 5)) grid = sns.lineplot(data=df, x='N', y='times', hue='type', style='p') grid.set(xscale="log", yscale="log", xlabel = 'Number of Observations', ylabel='Execution Time (Seconds)') grid.legend(loc='best', prop = {'size':6})
