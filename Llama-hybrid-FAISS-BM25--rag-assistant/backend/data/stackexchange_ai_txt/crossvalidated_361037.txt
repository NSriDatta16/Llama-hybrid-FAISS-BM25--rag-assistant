[site]: crossvalidated
[post_id]: 361037
[parent_id]: 361020
[tags]: 
Multicollinearity simply imlies that one or more of the features in your dataset are useless to the model. Thus you get all the problems associated with more features (i.e. curse of dimensionality ), but none of the benefits (e.g. making the classes easier separable). Many ML algorithms are impervious to problems of this nature. Algorithms that internally perform any form of feature selection and are good with high dimensional data (e.g. tree-based algorithms, lasso) are robust against multicollinearity. $L_1$ regularization mainly helps models as it provides sparse solutions, robust against multicollinearity. $L_2$ doesn't help as much. Read this article if you are interested on the differences of the two. As a final note, multicollinearity isn't as big a problem in Machine Learning as you make it out to be. That being said, if such a problem is detected it is almost always beneficial to perform some sort of feature selection, or even PCA to help decorrelate the features.
