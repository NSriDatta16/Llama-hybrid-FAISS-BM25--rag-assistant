[site]: crossvalidated
[post_id]: 479253
[parent_id]: 478467
[tags]: 
UPDATE This journal article explains why linear interpolation is both "too optimist" and is also "incorrect," due to the non-linear properties of the Precision-Recall curve: https://www.biostat.wisc.edu/~page/rocpr.pdf Due to the non-linear nature of the precision-response curve, linear interpolation results in erroneous overestimations. With an averaging rule, missed changes in slope average out. With interpolation, they do not. Thus if points don't cover all the slope changes in the real curve, interpolation errors add up. This is why interpolation is "too optimistic," and why the midpoint rule generally has half the error that the trapezoidal rule has. https://math.libretexts.org/Courses/Mount_Royal_University/MATH_2200%3A_Calculus_for_Scientists_II/2%3A_Techniques_of_Integration/2.5%3A_Numerical_Integration_-_Midpoint%2C_Trapezoid%2C_Simpson%27s_rule http://math.cmu.edu/~mittal/Recitation_notes.pdf https://activecalculus.org/single/sec-5-6-num-int.html This article is referenced as "[Davis2006]" in the scikit-learn documentation as the explanation as to why linear interpolation is inappropriate and "too optimistic" here. See: https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics Also, The function sklearn.metrics.average_precision_score does not use the rectangle rule, or any Riemann sum, right or otherwise. It uses "average precision." The formulas are very different. Note that f(x) is very, very different than Pi. Due to the formulas for precision and recall, the Average Precision is actually computing an average, with discrete values between 0 and 1. Regarding Riemann, f(x) = y. This gives you the height to multiply the delta with. There is no averaging there. Average precision is most analogous to the midpoint rule, as they are both doing averages. Note that R uses the same formula for Average Precision: https://www.rdocumentation.org/packages/yardstick/versions/0.0.4/topics/average_precision
