[site]: crossvalidated
[post_id]: 317299
[parent_id]: 
[tags]: 
Probability of error for consistent classifier (Random Forests)

I'm going through the paper " Consistency of Random Forests and Other Averaging Classifiers " and I'm stuck on a seemingly simple part of the proof for Proposition 1. Here's the relevant part: What I'm not getting is the last sentence which claims that $$ P(g_n(X, Z) \neq Y | X=x) = (2\eta(x) - 1) P(g_n(x, Z) = 0 + 1 - \eta(x)) \tag{a}\label{a} $$ Further up the authors define $\eta(x) = P(Y = 1|X=x)$ My assumption is that $$ P(g_n(X, Z) \neq Y | X=x) = P(g_n(X, Z) = 1 |Y=0, X=x) + P(g_n(X, Z) = 0 | Y=1, X=x) $$ i.e. the probability of the classifier $g_n$ being wrong is the probability of predicting one when the true value is zero, plus the probability of predicting zero when the true value is one. We know that $\eta(x) = P(Y = 1|X=x)$ so $P(Y = 0|X=x) = 1 - \eta(x)$ I still don't see how one arrives at $\ref{a}$ though.
