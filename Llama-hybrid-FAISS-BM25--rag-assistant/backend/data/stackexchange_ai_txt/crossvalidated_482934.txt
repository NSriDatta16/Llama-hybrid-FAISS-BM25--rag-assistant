[site]: crossvalidated
[post_id]: 482934
[parent_id]: 
[tags]: 
Confusion between basis functions and SVM feature mapping

I'm new to Machine Learning. I have just read an article about basis functions. Apparently, the basis functions create a non-linear regression line to capture a variety of differently complicated models (quadratic, cubics, etc...). What I saw is that every transformation of new feature made is in the same dimension as in the original input space. For example, the picture below Here the basic function vector has 5 terms: $$ \pmb{\phi}(x) = [\phi_{0}(x), \phi_{1}(x), \phi_{2}(x), \phi_{3}(x), \phi_{4}(x)]^{T} = [1, x, x^{2}, x^{3}, x^{4}]^{T} $$ But in SVM feature mapping, the input will be mapped in a higher-dimensional feature space, like in the above case, the basic function vector contains 5 values so it could be seen as mapping from 1-D input space to 4-D feature space (not including the bias-term $\phi_{0}(x)$ ). $$\pmb{\Phi} : [x] \mapsto [x, x^{2}, x^{3}, x^{4}]^{T} $$ So I need someone to provide me with better explanation. From what I understand, basis function break the linearity by transforming linear function to non-linear one. Whereas, feature map finds a hyperplane in high-dimensional space that can separate the data well. However, both these concepts seem to revolve around vector of basis functions $\pmb{\phi}$ ...
