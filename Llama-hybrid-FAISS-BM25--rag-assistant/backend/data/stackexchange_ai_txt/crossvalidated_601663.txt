[site]: crossvalidated
[post_id]: 601663
[parent_id]: 
[tags]: 
Aleatoric and Epistemic Uncertainty in the Framework of Bayesian and Frequentist

Beginning with definitions of Aleatoric and Epistemic Uncertainty from this paper : Aleatoric: Aleatoric uncertainty refers to the intrinsic uncertainty of a particular system and the observed data. It arises due to the intrinsic and irreducible stochastic variability in the data-generating process. Aleatoric uncertainty — or data uncertainty — cannot be readily reduced as it is inherent to the measurement data. Epistemic: Epistemic uncertainty — or model uncertainty — arises from our ignorance about the underlying physical process itself reflecting our lack of knowledge about its structure or its parameters. In machine learning, epistemic uncertainty is associated with model structure. I am wondering about what (or if there is) the relationship between Epistemic and Aleatoric Uncertainty to Bayesian and the Frequentist paradigm is. From the Machine Learning book by Bishop section 1.2.3: "In both the Bayesian and frequentist paradigms, the likelihood function p(D|w) plays a central role. However, the manner in which it is used is fundamentally different in the two approaches. In a frequentist setting, w is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets D. By contrast, from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over w." So is epistemic uncertainty Bayesian and aleatoric uncertainty frequentist? Or another question how does a Bayesian represent aleatoric uncertainty? Is it all somehow captured in the posterior distribution of the weight parameters?
