[site]: crossvalidated
[post_id]: 500409
[parent_id]: 
[tags]: 
Independency vs ignorance - why is conditioning on a variable interpreted as ignoring the other one? (partial dependency plots)

Setting Let $f(X)=f(X_S, X_C)$ be an estimator of $y$ using random variables (or random vectors) $X_S$ and $X_C$ . The authors (see below) mention that: $$ \tilde{f}_S (X_S) = \mathrm{E}( f(X_S, X_C) | X_S) = \int_{-\infty}^{+\infty}f(X_S,X_C)p(X_C|X_S)dX_c $$ is the the effect of $X_S$ on $f(X)$ ignoring the effects of $X_C$ . $p(X_C|X_S)$ is the conditional PDF of $X_C$ and $p(X_C)$ the marginal one. The authors also mention that the partial dependency function: $$\bar{f}_S (X_S)= \mathrm{E}_{X_C} \, f(X_S, X_C) = \int_{-\infty}^{+\infty}f(X_S,X_C)p(X_C)dX_c$$ is not ignoring the effects of $X_C$ Question Why can conditioning be interpreted as ignorance, since $\tilde f$ still depends on $X_C$ through $p(X_C|X_S)$ which can be different for every value of $X_S$ ? What is the difference between the terms "ignorance" and "dependency"? If ignorance means simply integrating the variable it out, i.e. letting it only play a constant or multiplicative role for all values of the other variable, why is $\bar f$ also not considered as ignoring $X_C$ , since that's what it leads to? In other words, why is marginal expectancy not ignorance while conditional is? References: From Hastie, Tibshirani & Friedman (Elements of Statistical Learning, 2nd ed) and Friedman 1999 (Greedy Function Approximation: A gradient boosting machine) in the context of partial dependency plots, i.e. black-box model interpretation. PDP - Hastie, Tibshirani & Friedman (Elements of Statistical Learning, 2nd ed) Difference between averaging and ignoring the partial dependencies? (Question contains the term ignoring, but focuses on mathematical side; I am more interested in the intuition and terminology.)
