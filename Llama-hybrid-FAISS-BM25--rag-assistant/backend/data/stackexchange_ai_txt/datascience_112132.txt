[site]: datascience
[post_id]: 112132
[parent_id]: 112129
[tags]: 
To answer your question let's first go through how CNN works. When we give a CNN an input image, it sees an array of numbers that correspond to the pixel intensities of the input image. The intensity of a pixel can range from 0 (black) to 255. (white). CNN will produce numerical values that indicate the likelihood that an image belongs to a particular class. In order to classify images, CNN searches for basic features like edges and curves before progressing through a number of convolutional layers to more abstract ideas. To identify edges, it scans the image both horizontally and vertically using filters. Consider filters as a weight matrix that is multiplied by pixel intensities to create a new image that preserves the characteristics of the source image.The output value in the corresponding pixel position of the output image is then obtained by adding these products which is called feature map. The output of the previous cnn layer becomes the input of the subsequent cnn layer . In essence, each layer of the input is specifying the places in the original image where specific low-level features can be seen. Dimension reduction is done in Pooling Layer. Now when you apply a set of filters on top of that, the output will be activations that represent higher-level features. As you move through the network and through more convolution layers, you receive activation maps that represent more and more complicated features. So, CNN basically works how we humans look at images first by it's distinguishable features(low-level) and then minute details(high-level). Why high-level features are more meaningful than low-level features? Take a example of dog and cat they both have 4 legs(low-level features) but what will distinguish them is maybe their eyes, ears (high-level features) which will help them in classifying better. why high-level features are extracted in the last layers? Why not in the first layer? It's because for large image suppose (224,224) we extract high level then weights will be 224*224*3 = 150528 . This full connectivity is not required and can lead to overfitting. So, we connect neurons to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron or filter size. Rather than using Fully connected layer in first layer it's used in last layer to use high level features extracted from Convolution + Pooling Layer for classifying images as it reduces weights significantly ,so is much faster and effective. Refer: https://cs231n.github.io/convolutional-networks
