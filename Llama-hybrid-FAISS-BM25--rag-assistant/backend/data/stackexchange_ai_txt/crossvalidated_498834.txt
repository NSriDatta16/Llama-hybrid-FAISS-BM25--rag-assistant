[site]: crossvalidated
[post_id]: 498834
[parent_id]: 498830
[tags]: 
The embeddings are trained jointly with other parameters of the model and are randomly initialized randomly. It just back-propagates the error all the way down to the embedding layer. Using pre-trained embedding would not make sense from various reasons: Transformer uses subword inputs, but current methods for word embedding work well only for word-like tokens. Subword input allows the model to generalize for unseen morphological forms and learn how to transliterate proper names. The embeddings are shared between the encoder and the decoder and the output layer of the model and the model benefits a lot from the parameter tying. This would be hardly possible with pre-trained embedding. Even though bilingual word embeddings exist, they are trained to be language-neutral, which would lead to mixing the languages in the output layer.
