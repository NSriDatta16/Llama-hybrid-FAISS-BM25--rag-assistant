[site]: crossvalidated
[post_id]: 603619
[parent_id]: 463706
[tags]: 
Also to my understanding, in a neural network classifier with 1 hidden layer, you have a mixture of functions (sigmoids, relus, etc) that are aggregated ... So my question is: do neural networks fall in the general domain of mixture models? If so, why are they never referred to as such? You can consider a single layer models as a mixture model. And it is not true that they are never referred to as such. The link between neural networks and mixture models is not an unmentioned topic in the literature: Why is the posterior of a neural network gaussian process equal to the posterior of a neural network in the limit of infinite width layers? and for special cases, neural network Gaussian process , also deep networks are in the limit equivalent to a mixture model. But, neural networks can be more complex and compound several layers together. Also, interactions might be non-linear. So while there are probably historical reasons for the term neural network, neural networks are also a lot different from mixture models and in practice they are different techniques. Neural networks are more about creating networks with multiple complex links, where mixture models are more straightforward single layer models with predefined elements to be mixed. To take your example with the 5 layer model (which I believe can be expressed in 3 layers as well) the neural network is about blending the layers together. I have tried to re-express it in the image below (this is my own brain doing the links in the neural network): In the last layer you have two nodes E and F that give an output between 0 and 1 depending on the values of the nodes before that C and D, but effectively they are functions of the original input X and Y and in some sense you can see it as a model that adds up a mixture of functions with extremely many parameters. The power of network models is that With every layer you exponentially grow the number of elements in the mixture. You don't define the functions that are being mixed. Instead out you let the network define these functions out of a much larger infinite space of potential functions.
