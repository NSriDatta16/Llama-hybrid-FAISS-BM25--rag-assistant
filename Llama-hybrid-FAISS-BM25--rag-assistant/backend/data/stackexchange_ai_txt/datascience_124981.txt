[site]: datascience
[post_id]: 124981
[parent_id]: 124969
[tags]: 
Start with a batch of (audio, text) pairs denoted $( X_a, X_t )$ . The text data is passed through a text encoder $ f_t $ to produce a text embedding $ \hat{X}_t = f_t(X_t) $ . The audio data is passed through an audio encoder $ f_a $ to produce an audio embedding $ \hat{X}_a = f_a(X_a) $ . The embeddings $ (\hat{X}_t, \hat{X}_a) $ are different dimensions, so we pass each of them through a linear transformation to project them to the same dimensionality. This gives $ E_t = L_t(\hat{X}_t) $ and $ E_a = L_a(\hat{X}_a) $ . The paper forgets the hat notation in equation 2. Then we do a pair-wise comparison of all elements in $ E_a $ against all elements in $ E_t $ via $ C = \tau * (E_t â€¢ E_a^\top) $ $ C $ is an $ N \times N $ matrix ( $N$ being the batch size) where the rows represent text items and columns represent audio items. We then define our loss function $ \ell_k = \dfrac{1}{N} \sum_{i=0}^{N} log(diag(softmax(C))$ and compute our loss $\mathcal{L} = 0.5 * (\ell_{text}(C) + \ell_{audio}(C))$ Now do your question - how is $\ell_{text}(C)$ different from $\ell_{audio}(C)$ , and are we doing the same computation twice? The answer is no because we are computing the softmax along different dimensions. Note the description along text and audio axis respectively . When we compute $\ell_{text}(C)$ , we compute the softmax along the rows of $C$ , while $\ell_{audio}(C)$ computes the softmax along the columns of $C$ . Now to your next question about only considering the diagonal elements. All the off-diagonal elements contribute to the softmax calculation. The loss basically says "make the softmax term along the main diagonal really big". To do this, the model has to increase the similarity of items on the diagonal (bringing the same items closer together), but also decrease the similarity of items off the diagonal (moving different items farther apart). The softmax along a row/column of $C$ accomplishes the contrastive task of maximizing/minimizing similarity between paired/unpaired items.
