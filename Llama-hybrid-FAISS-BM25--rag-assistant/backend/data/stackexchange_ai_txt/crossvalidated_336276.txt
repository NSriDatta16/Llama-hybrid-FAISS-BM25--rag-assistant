[site]: crossvalidated
[post_id]: 336276
[parent_id]: 
[tags]: 
Is the sum of two decision trees equivalent to a single decision tree?

Suppose we have two regression trees (tree A and tree B) that map input $x \in \mathbb{R}^d$ to output $\hat{y} \in \mathbb{R}$. Let $\hat{y} = f_A(x)$ for tree A and $f_B(x)$ for tree B. Each tree uses binary splits, with hyperplanes as the separating functions. Now, suppose we take a weighted sum of the tree outputs: $$f_C(x) = w_A \ f_A(x) + w_B \ f_B(x)$$ Is the function $f_C$ equivalent to a single (deeper) regression tree? If the answer is "sometimes", then under what conditions? Ideally, I'd like to allow oblique hyperplanes (i.e. splits performed on linear combinations of features). But, assuming single-feature splits could be ok if that's the only answer available. Example Here are two regression trees defined on a 2d input space: The figure shows how each tree partitions input space, and the output for each region (coded in grayscale). Colored numbers indicate regions of input space: 3,4,5,6 correspond to leaf nodes. 1 is the union of 3 & 4, etc. Now suppose we average the output of trees A and B: The average output is plotted on the left, with the decision boundaries of trees A and B superimposed. In this case, it's possible to construct a single, deeper tree whose output is equivalent to the average (plotted on the right). Each node corresponds to a region of input space that can be constructed out of the regions defined by trees A and B (indicated by colored numbers on each node; multiple numbers indicate the intersection of two regions). Note that this tree is not unique--we could have started building from tree B instead of tree A. This example shows that there exist cases where the answer is "yes". I'd like to know whether this is always true.
