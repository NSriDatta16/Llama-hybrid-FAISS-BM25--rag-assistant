[site]: crossvalidated
[post_id]: 481990
[parent_id]: 
[tags]: 
BERT masked lenguaje model. How can calculate the embedding of the MASK token?

On the training step of the masked lenguaje model, we constuct the embedding of the "masked" token using the embeddings of the contextual words, right? Then with a softmax layer we predict the "masked" word". If we construct the "masked" embedding with the contextual tokens, we would need to calculate the dot product of the query of the "masked" embedding and the key of each contextual tokens. My question is....how can we calculate the query of the "masked" token if we dont know the input embedding of it (because we "masked" it intentionaly)?
