[site]: crossvalidated
[post_id]: 252319
[parent_id]: 
[tags]: 
Am I approaching low probability events in my neural network correctly?

I have a logistic neural network with binary output. Of the 2 outcomes it attempts to predict, the first is very likely (occurring 90% of the time) such that my neural network will only ever predict this first outcome, albeit with a different level of certainty (about 86%-94%). However, the second outcome, when it does occur, is devastating enough to substantially hurt the performance of the neural network. The fact that my neural network never predicts it has led me to seek another means of assessing its likelihood. I have settled upon dealing with its likelihood relative to the first outcome; if the second outcome is above 10% (first outcome is therefore less than 90% likely), I have coded it to predict the 2nd outcome, thus arousing the warranted caution of the user. Is my attempt to sensitize my model to this second outcome the right approach given my objective, or is there a more rigorous approach I should take?
