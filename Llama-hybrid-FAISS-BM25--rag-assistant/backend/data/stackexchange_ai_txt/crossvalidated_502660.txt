[site]: crossvalidated
[post_id]: 502660
[parent_id]: 502646
[tags]: 
There are some challenges when you have more variables than observations. Linear models (e.g. Linear regression and logistic regression) won't be able to be fit on the raw data, so you will either need to do variable selection or project the data onto a lower dimensional space a la PCA or similar methods. Additionally, you could use a penalized method like Ridge or Lasso to fit the model with all variables. I'd anticipate you'd need a large penalty, which may mean that most variables are 0 if not close to 0. Maybe that is a good thing for you, maybe not. The problem is compounded when you take a traditional train/test split with the data. If you held out even 30% of your data, that is 15 samples of your 50. That means you have very little data to train on and even less to evaluate the performance out of sample. In this scenario, I would recommend validation with the bootstrap and estimate optimism of the training error so that you can use all the data for training. Above all else, my recommendation would be to get more data. Fifty samples is not a lot, and if your intent is to determine which variables are more important over others, I think you're going to make errors just based on how many variables you have and how few observations you have.
