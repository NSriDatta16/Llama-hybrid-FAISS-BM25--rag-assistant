[site]: datascience
[post_id]: 37108
[parent_id]: 37105
[tags]: 
You could say every type of neural network gets 1d input data. It's just more convenient to think about 2d-CNNs taking 2d data because the convolution operation is best illustrated by moving squares across a grid, and similarly for max-pooling. But you could easily write out all the multiplications, additions, and max operations you performed in one line of algebra. And in this line you could easily flatten the 2d input indices into 1d indices. The information passed in and the calculations and output of the network would be exactly the same, but this representation would lose its real-world interpretation so there's no reason to think of it this way. The point is that there are different ways of representing exactly the same information, and we choose a 1d representation for FNN inputs and sometimes higher dimensions for CNN inputs because it corresponds to the physical structure of the real-world problem and it is easiest think about. This is why most libraries have you input the data in these shapes. But if you think about what's happening at the lowest level of computation on your computer you won't necessarily find the same structure.
