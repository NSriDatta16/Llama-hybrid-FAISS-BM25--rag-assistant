[site]: crossvalidated
[post_id]: 563193
[parent_id]: 563184
[tags]: 
The equation above is for binary classification. The purpose of linear SVM is to find a maximal margin hyperplane, where the hyperplane is defined by equation $w^T x_i - b = 0$ . If for any value $x_i$ which is not on the hyperplane we compute $w^Tx_i-b$ we would get a positive value if $x_i$ is on one half of the space and a negative one if the $x_i$ is on the other half. But we want more, we want to have a margin also. The points between the two margins will contain no training points. As such, all the points will have a distance from the hyperplane which is greater than a constant ( $1$ is chosen for convenience, any other constant will do the job, since it's actual absolute magnitude will be absorbed). Thus, we will have $w^T x_+ - b \ge 1$ for positive labeled data points and $w^Tx_- - b \leq -1$ for negative labeled ones. The reason why we chose the constant the threashold as $1$ and $-1$ is because we also encode the $y_i$ to have values $1$ for positive samples and $-1$ for negative ones. If you multiply the first inequality with $y_i$ which is $1$ for positive samples, we get $y_+(w^T x_+ -b) \ge 1$ . If we multiply the second equation with $y_i$ which is $-1$ for negative samples we get $y_-(w^T x_- - b) \ge 1$ . Notice that both equations are identical and we can write them in a single form as $y_i(w^T x_i -b) \ge 1$ . The reason why we go with this form is because is much simpler to work with a single equation rather than two. The second reason is that we connect also the output to the equations, since this is the other part that we have from training data (additional to $x_i$ instances) and we want to learn from the relation between those two parts.
