[site]: crossvalidated
[post_id]: 351042
[parent_id]: 351028
[tags]: 
As Jeremy mentioned in the comments, if your goal is prediction, then there is no sense in creating an interpretable factor. You can use flexible regression or machine learning methods directly on the items. Adding a factor structure only limits the relationship between the items and the outcome by imposing independence constraints, which seem unnecessary to suit your goals. The goal of SEM is to test a constrained covariance structure among the the observed variables and estimate the parameters that minimize the difference between the constrained and observed covariance structures. If your goal is optimal prediction, none of that is relevant, and SEM is not the tool you should use. Indeed, going down your road, you can construct a silly example that demonstrates the absurdity of using SEM to optimize prediction of an outcome. In your SEM, simply constrain the error variance of the outcome to be 0, the factor loadings to all be 0, and the mean and variance of the factor to be the mean and variance of your outcome. Your estimated coefficient will be 1, and the factor will have an R2 of 1.0, perfect prediction. Does this satisfy your desiderata? It's a factor whose relationship with the indicators is unimportant and it predicts the outcome extremely (i.e. perfectly) well. Clearly this is problematic. The point is that with SEM, you can devise any constraints to satisfy your desires, but the goal of SEM is not to maximize prediction; rather, its very purpose is to reproduce the population covariance matrix of the observed variables with model constraints. It simply is not the tool for your job.
