[site]: crossvalidated
[post_id]: 584078
[parent_id]: 584063
[tags]: 
As you say, the k-nearest neighbor algorithm has a number of hyperparameters such as the number of neighbors $K$ (but also e.g. distance function, how you summarize outcomes amongst neighbors etc.). If you want to predict with the model for new data, then you want to choose these hyperparameters based on something that mimics the task of interest (i.e. predicting for new unseen data). Assuming there's no temporal order, clusters or other issues in the data that need to be accounted for, randomly splitting into training data (where the model is fit) and validation data (where we see how it performs to inform our hyperparameter choice) makes sense. Of course, one split gives you a bit of a noisy performance assessment. Thus, randomly splitting the data into $k$ different folds and training $k$ different models (for each leaving out a different validation part) gives you a less noisy assessment after averaging the performance of the different splits. In practice, we tend to use a $k$ between 3 and 10, and if the signal is too noisy still could also repeat this a few times (e.g. 2 to 10). Alternatively, there's also bootstrapping approaches, but we'll ignore those. You have to fix your validation scheme (i.e. how your cross-validation is done), as you cannot assess/compare different ones. I.e. you cannot pick $k$ (or different random number seeds for splitting etc.) through cross-validation, because you'd compare performance with more or less, and different training data. Similarly, it makes no sense to only look at different validation folds for different hyperparameter values (what can make sense is method (3) below, but that seems very different from what you asked). If you can only afford (i.e. takes too long to train models) to try one training-validation split (or if the dataset is so huge that this would already give a reliable assessment), then you'd want to use the same split for all hyperparameter combinations to ensure comparability to inform your choice. In some special situation you could use some other source of information to pick these (e.g. in a Kaggle competition one could use the alignment between CV performance and public leaderboard score to assess the CV scheme), but in practice that's not usually an option. We may have a lot of hyperparameter values to consider and cannot possibly try all possible hyperparameter combinations (esp. if some are continuous). To deal with that a number of strategies have been proposed: Creating a grid of values (all combinations of all values of all hyperparameters we consider) and trying all the values on the grid (aka sklearn.model_selection.GridSearchCV to use the Python scikit-learn name for it that you used). Create a grid of values and randomly select some values on the grid to try (aka sklearn.model_selection.RandomizedSearchCV to use the Python scikit-learn name for it that you used). Racing methods (avoid training some models in (1) or (2) when some hyperparameters already do so badly on some splits that they can be clearly abandoned) More targeted search methods (such as Bayesian hyperparameter optimization) that attempt to try more informative or more promising hyperparameter values based on the CV results for those that were already tried. As you can see, the main difference between the two approaches is what hyperparameter values are tried, the cross-validation would be done the same in either case.
