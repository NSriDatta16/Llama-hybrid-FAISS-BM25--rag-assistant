[site]: crossvalidated
[post_id]: 51505
[parent_id]: 
[tags]: 
How to get SGD to reach global optimal point in logistic regression?

I am trying to write a tool which involves implementing logistic regression. With the batch gradient descent method, the convergence is guaranteed as it is a convex problem. However, I find that with the stochastic gradient decent method typically converges to some random points (i.e., not very close to the minimum point resulted from the batch method). I have tried different ways of decreasing the learning rate, and different starting points of weights. However, the performance (e.g., accuracy, precision/recall, ...) are comparable (to the batch method). I understand that this is possible, since SGD (stochastic gradient descent) uses an approximation to the real cost each step. I guess this matters since otherwise the interpretation of the weights would not make much sense even the accuracy is comparable. Does it matter that SGD uses an approximation the real cost at each step? If it matters, how can it be made to converge or get close to the global optimal point?
