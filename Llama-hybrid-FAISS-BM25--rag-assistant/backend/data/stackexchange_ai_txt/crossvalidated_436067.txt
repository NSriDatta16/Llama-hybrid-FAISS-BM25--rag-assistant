[site]: crossvalidated
[post_id]: 436067
[parent_id]: 
[tags]: 
How can this L(2,1) problem be reduced to the orthogonal procrustes problem?

NOTE: Don't take this too serious -- the question is actually due to my misreading $\|y_i - Wx_i\|^2$ as $\|y_i - Wx_i\|_2$ , see the answer. Smith et al. in Offline bilingual word vectors, orthogonal transformations and the inverted softmax desribe learning an "alignment transformation" between word embeddings by solving the problem $$ \begin{align} & \min_W \sum_{i=1}^N \|y_i - Wx_i\|_2, \text{ s.t. } W^{T}W = I. \end{align} $$ over embeddings $x_i$ , $y_i$ from aligned dictionaries $X_D$ and $Y_D$ . The constraint ensures orthogonality to make $W$ "self aligned". This problem can be quite easily written as $$ \max_W \sum_{i=1}^N y^T_i W x_i,\quad \text{ s.t. } W^{T}W = I. $$ Now, instead of using gradient descent, they claim that the optimal solution is given analytically through an SVD: $$ W^* = U V^T, \text{ where } U \Sigma V^T = Y^T_D X_D $$ But I cannot make sense of why this is valid. The SVD solution, as I have found out, is the solution of the similar "orthogonal procrustes problem" with the Frobenius norm, $$ \begin{align} & \min_W \|Y - W X\|_F, \quad \text{ s.t. } W^{T}W = I; \end{align} $$ but we here have a different entrywise norm : $$ \begin{align} & \min_W \|Y - W X\|_{2,1}, \quad \text{ s.t. } W^{T}W = I. \end{align} $$ Why does the same solution apply? Is it some norm inequality I'm missing? (I tried researching other sources for this approach, but all just either use GD or do not justify this solution any better. Cf. Xing et al. , 2015)
