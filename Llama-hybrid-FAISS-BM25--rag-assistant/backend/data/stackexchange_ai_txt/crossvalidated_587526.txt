[site]: crossvalidated
[post_id]: 587526
[parent_id]: 586672
[tags]: 
This answer is not a proof that those priors will render the problem identifiable but an intuitive explanation that it should work. As you have pointed out, the problem with ordinary factor analysis (FA) is that there are infinitely many different linear maps $A: z\to x$ which lead to the same likelihood of the data in this FA model. Thus, the idea is to weigh all those $A$ differently by giving them different prior probabilities. The original FA could be thought of as all $A$ having the same weight (an improper prior ), and by giving them different priors (weights) the above tie is broken and, hopefully, there is only one $A$ left that gives the largest likelihood to the data $\{x_t\}$ . I am not sure what you refer to by "it is just a prior which is not enforced in the posterior solution", but the prior does change the posterior. You have shown that, if $A$ is a solution to the original FA, i.e. an MLE solution (frequentist) or a MAP for a constant improper prior (Bayesian), then all $A^\prime = AQ$ , with $Q$ any orthonormal map, are also MLEs (MAPs). I.e., they all have the same a posteriori density. Thus, intuitively, the plan is to choose a prior that is not constant on this set of matrices $A^\prime$ , which also makes the posterior there nonconstant. And while we are at it, we should also design the prior such that certain nice features of $A$ , e.g. sparsity, are favored. In this case, it makes sense to use e.g. horseshoe priors. In other words, the solution with horseshoe priors will be the sparsest matrix that is also "near" the set $\{A^\prime=AQ| Q\;\mbox{orthonormal}\}$ . Finally, note that restricting to LT matrices can be interpreted as a similar approach, where priors are just very strongly penalizing all matrices with nonzero values in the upper triangle. Edit: Why did I write that "the solution with horseshoe priors will be the sparsest matrix that is also "near" the set $\{A^\prime=AQ| Q\;\mbox{orthonormal}\}$ .", why only "near"? Let's give this set a name, $S:=\{AQ|Q\in O(k)\}$ , where $O(k)$ are the orthonormal matrices. For the improper prior $p^i$ , the posterior $f^i$ has infinitely many MAPs, given by the set $S$ . Note, that this doesn't mean that all the other matrices $A$ are impossible. They are very well possible, they are just not as likely. In fact, when computing the Bayesian predictive distribution , all $A$ are integrated over. Next, we create a new posterior $f^n$ by multiplying the old posterior $f^i$ with a nonconstant prior $p^n$ (and normalizing), $f^n\sim f^i p^n$ . Now it is not clear that the maximum of $f^n$ should also be a maximum of $f^i$ , ie. be in $S$ . In general, $f^i$ is preferring matrices in $S$ , while $p^n$ is preferring sparse matrices, and thus the product $f^n$ will be a compromise which in general is not completely sparse (read: not a maximum of the horseshoe prior $p^n$ ) and not completely in $S$ .
