[site]: crossvalidated
[post_id]: 127025
[parent_id]: 105696
[tags]: 
Depending on the method you use, the variables you set and the whole environment in general you run a bigger or lower risk of overfitting. So actually, splitting your data into training and test set is a good practice to verify how 'general' is the estimate your model gives for new input, this is: how well does it understand the relation between features of your data and how far is it from being to biased to your initial observations (training set). So if you use a linear regressor with a 40-degree polynomial the obtained model would probably be more biased onto your training data that a 3-degree polynomial (this depends hugely on your dataset size and complexity). The same does for SVM: the kernel you use and in general the way you transform data will impact the results. It can be said that for most cases SVM's are a very robust method against overfitting, at the contrary of linear regressors. The result does not depend that greatly on how you split you dataset, but on how good is data, how well is represented for the algorithm you use and how good are your experiments to find near-optimal parameters.
