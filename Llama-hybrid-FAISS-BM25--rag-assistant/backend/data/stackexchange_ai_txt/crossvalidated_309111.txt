[site]: crossvalidated
[post_id]: 309111
[parent_id]: 308837
[tags]: 
At the core, this is a logistic regression model. First, there are many improvements in pymc3 3.2 that should make this model more efficient, like using the (default) NUTS sampler: import pymc3 as pm temperature = challenger_data[:, 0] D = challenger_data[:, 1] # defect or not? import theano.tensor as tt with pm.Model() as model: beta = pm.Normal("beta", mu=0, tau=0.001) alpha = pm.Normal("alpha", mu=0, tau=0.001) p = pm.Deterministic("p", tt.nnet.sigmoid(beta*temperature + alpha)) observed = pm.Bernoulli("bernoulli_obs", p, observed=D) trace = pm.sample() Essentially then you want to introduce more covariates. You do this just like in a linear regression, e.g. if you have orings: import pymc3 as pm temperature = challenger_data[:, 0] D = challenger_data[:, 1] # defect or not? import theano.tensor as tt with pm.Model() as model: beta_temp = pm.Normal("beta", mu=0, sd=1) beta_orings = pm.Normal("beta", mu=0, sd=1) alpha = pm.Normal("alpha", mu=0, sd=1) p = pm.Deterministic("p", tt.nnet.sigmoid(beta_temp*temperature + beta_orings*orings + alpha)) observed = pm.Bernoulli("bernoulli_obs", p, observed=D) trace = pm.sample() Note that I also changed the priors to be more informed, which is always a good idea (but it assumes your covariates are z-scored so might not be a good idea here). There is also more convenient syntax in the glm submodule, see here: http://docs.pymc.io/notebooks/GLM-logistic.html Finally, you get a better hit-rate for your questions on http://discourse.pymc.io/ .
