[site]: datascience
[post_id]: 76559
[parent_id]: 
[tags]: 
Is it appropriate to use random forest not for prediction but to only gain insights on variable importance?

For a binary classification problem, I have a small data set with 200 observations. There are around 20 potential variables but based on variance importance I think only 2 or 3 are important for classification. This data set is too small to train a random forest model for prediction purposes, but is it okay to run a random forest model and use the variable importance feature to understand which variables are important? If only a handful of features are by far the most important, then, I think despite the small data size, it's an appropriate approach in that it'll tell me what I want. The results can then be used to perhaps build a simple decision tree or as a preliminary analysis for a future larger test. Any flaws with this thinking?
