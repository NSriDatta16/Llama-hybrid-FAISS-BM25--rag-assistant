[site]: crossvalidated
[post_id]: 362578
[parent_id]: 362425
[tags]: 
If you want a basic definition of an ANN, you might say that it's a directed-graphical-model, where inputs and outputs are processed at each node via an activation function, and most of the time gradient descent is used to train it. So the question really becomes: what models out there can be expressed as graphical models? I'm not an expert but, I believe theoretically some ANNs can be shown to be Turing complete, which means that they should be able to do any possible set of calculations (with a possible infinite number of resources, mind you). I'm also going to interpret your question in the following way: For any given model, can I slap together an ANN model to emulate that model, as close as possible, and in a reasonable amount of time? A vanilla neural network can emulate a decision tree, by using heaviside step-activations. The problem is that such unit activations have zero gradient, so normal gradient descent won't work. You might say, "no problem, just use a modified form of gradient descent." However, that's still not enough. For a better example, take something like XGBOOST, which isn't just gradient-boosted forests. There's a whole lot of extra work that goes into choosing split points, pruning, optimizing for speed, etc. Maybe after enough modifications you can make a similar-looking ANN, but it's not at all clear that such an ANN would perform at least as well, nor if it's optimized to do the job. I think that's an important point, because while it might be theoretically satisfying to conclude that ANNs can do anything, practically this might be completely useless. For example, you could try making an ANN using ReLu activations to approximate $f(x)=e^{x}$, but that's just plain dumb, as considerably more efficient and accurate methods are at your disposal.
