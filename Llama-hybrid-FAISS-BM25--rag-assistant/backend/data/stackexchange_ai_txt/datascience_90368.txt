[site]: datascience
[post_id]: 90368
[parent_id]: 
[tags]: 
Trade-off between number of channels and size of convolutional filters

As far as I understand, the common practice in the modern CNN architectures is to use a smaller convolutional filters, but deeper networks with more channels. One of the reason behind this is that one can have less parameters with the same receptive field. For instance, one can have a $5 \times 5$ receptive field in the following cases: $5 \times 5$ convolutional filter : $25 + 1 = 26$ parameters two subsequent $3 \times 3$ convolutional layers : $2 (3 \times 3 + 1) = 20$ parameters two subsequent $5 \times 1$ and $1 \times 5$ convolutional layers $2 (5 \times 1 + 1) = 12$ parameters I the latter 2 cases, one can also add a non-linearity between the layers. However, I wonder, whether the deeper architectures would be always advantegeous - because the larger filter may extract more sophsitcated interaction between features on particular scale and is more expressive. Also , probably, one can achieve the same quality with larger filters with the use of smaller number of them. There is a recent paper - https://arxiv.org/pdf/2101.10143.pdf - where larger filter are preferred in terms of spectral characteristic of the data and reduction of some spectral leakages and artifacts. Is there theory, with a certain level of rigor, which strategy - larger filters/shallower networks - smaller filters/deeper networks is preferable for a particular kind of problem? I would strongly appreciate references to papers and research in this field!
