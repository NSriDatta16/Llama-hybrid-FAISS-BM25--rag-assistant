[site]: crossvalidated
[post_id]: 635996
[parent_id]: 509798
[tags]: 
There are two main Objects to apply the dropout: Neurons , is proposed in Srivastava et al. [2014] , called Dropout , and used in the fully-connected networks Weights Matrix , is proposed in Wan et al. [2013] , called DropConnect , and this is the method used in the Transformer And there is a paper which focus on the Self-Attention Layer dropout tricks: DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks
