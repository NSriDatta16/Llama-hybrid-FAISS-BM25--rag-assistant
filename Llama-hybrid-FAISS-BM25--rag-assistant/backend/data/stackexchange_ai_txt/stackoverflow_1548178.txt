[site]: stackoverflow
[post_id]: 1548178
[parent_id]: 1548026
[tags]: 
I must second jk's answer with all my heart. Knuth's quote is somewhat in-applicable in real life outside some small domain of problems which don't involve architecting . As an example, I recently had to spend 3-4 months re-architecting from scratch a fairly involved system because the original design assumed that 100% of data will be loaded in one chunk from database. Which was fine for 3 years until the data set grew to ~100x of what the original designer anticipated, and the system started to alternate between merely running out of memory at 2G usage or crashing horribly. Now, the solution was conceptually simple - allow retrieval of data from DB in batches. Doing so intelligently worked. But what could have been extra couple of weeks of work in the original design stage had they bothered to think about performance turned into 3 month ordeal because every little function, object and API as well as overall architecture were explicitly written to assume "100% of data is there". In general, any situation where the volume of data is a performance concern and chunking of data is the main feasible solution, that chunking MUST be designed for beforehead .
