[site]: crossvalidated
[post_id]: 187227
[parent_id]: 187076
[tags]: 
The most important part is how you "phrase" the classification problem, meaning how you represent the input and what you want to output. Seeing as you have so many different event types you need to learn an embedding of these. This can be done directly in e.g. Keras. You can see this example on how to learn an embedding directly from data. Another approach would be to learn an embedding beforehand using a unsupervised approach such as word2vec. However, this requires more work on your part as you need to come up with a relevant task and train that to generate the embedding. Given that you have enough data it is easier (although slightly less effective) to learn the embedding directly. For the output I would not predict all the different types of events, but only the special events and a "background class" to keep the problem feasible. If you really want to be able to predict every single class then you need to use some tricks (look into how word2vec does it). Regarding the time between events. You could simply add that to your LSTM as an additional dimension (see e.g. this for an example of how to do that in Keras). This would be easy to do and would allow the LSTM to take the temporal difference into account. I do not know of any way to visualize the motifs by "unrolling" the temporal nature of the network. You might be able to generate some motifs using a generative network, but it would likely be difficult to interpret. One way to explore motifs could be to simply find the top 100000 most common sequences of non-special events of e.g. length 20-100, input them into the trained model and extract the probability output from the final softmax layer. In this way you could find sequences that are connected to certain special events. However, it's difficult to say whether this motif approach is feasible/useful without looking at your data.
