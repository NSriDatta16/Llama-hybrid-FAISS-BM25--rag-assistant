[site]: crossvalidated
[post_id]: 147209
[parent_id]: 147021
[tags]: 
I personally think Random Forests are not impervious to overfitting. Overfitting is always a possibility, for any model. One possible way to counteract overfitting is by always using cross-validation. If you want to detect overfitting, you can plot learning curves. Here, you are going to train the model multiple times, each time with a larger training set. Afterwards, you calculate the score on both the training set and the test set and plot these scores. Or, as Scikit Learn puts it: "A learning curve shows the validation and training error of an estimator for varying numbers of training samples." If the training error is low and the validation error is much higher, your model is overfitting. To address your second question; when your data consists of 99% 1's and 1% 0's, this will certainly affect your final result!
