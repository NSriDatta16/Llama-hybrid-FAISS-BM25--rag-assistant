[site]: crossvalidated
[post_id]: 466855
[parent_id]: 466851
[tags]: 
The bootstrap is certainly one way of assessing internal validation of a model. Ewout W. Steyerberg in his book Clinical Prediction Models describes how the bootstrap can be used to estimate optimism corrected performance. The procedure is as follows: Construct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model. Draw a bootstrap sample (Sample*) with replacement from the original sample Construct a model (Model*) in Sample*, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample*; Apply Model* to the original sample without any modification to determine the test performance; Calculate the optimism (Bootstrap performance - test performance). Repeat steps 1-5 many times, at least 200, to obtain a stable mean estimate of the optimism. Subtract the mean optimism estimate from the apparent performance to obtain the optimism corrected performance. In this scheme, the apparent performance is determined on the sample where the model was derived from. In machine learning, this is often referred to as training error. If you're working with popular tools like caret or sklearn, Frank Harrell writes 10-fold repeated cross validation, repeated 100 times is an excellent competitor to this procedure As for an interval estimate of the prediction error, the result of the above procedure provides an approximate sampling distribution to the optimism, and so you should be able to just subtract the apparent performance from each of the optimism bootstrap results, then estimate the interval by taking appropriate quantiles or by using bias adjusted bootstrap confidence intervals. I would search for literature on this though, because although this sounds reasonable, I am not confident it is methodologically sound.
