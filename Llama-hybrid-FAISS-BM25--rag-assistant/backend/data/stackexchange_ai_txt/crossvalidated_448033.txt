[site]: crossvalidated
[post_id]: 448033
[parent_id]: 258166
[tags]: 
One important thing to note as well is that the cross entropy is not a bounded loss. Which means that a single very wrong prediction can potentially make your loss "blow up". In that sense it is possible that there are one or a few outliers that are classified extremely badly and that are making the loss explode, but at the same time your model is still learning on the rest of the dataset. In the following example I use a very simple dataset in which there is an outlier in the test data. There are 2 classes "zero" and "one". Here is how the dataset looks like: As you can see the 2 classes are extremely easy to separate:above 0.5 it is class "zero". There is also a single outlier of class "one" in the middle of class "zero" only in the test set. This outlier is important as it will mess with the loss function. I train a 1 hidden neural network on this dataset, you can see the results: The loss starts increasing, but the accuracy continue to increase nonetheless. Plotting a histogram of the loss function per samples shows clearly the issue: the loss is actually very low for most samples (the big bar at 0) and there is one outlier with a huge loss (small bar at 17). Since the total loss is the average you get a high loss on that set even though it is performing very well on all the points but one. Bonus: Code for the data and model import tensorflow.keras as keras import numpy as np np.random.seed(0) x_train_2 = np.hstack([1/2+1/2*np.random.uniform(size=10), 1/2-1.5*np.random.uniform(size=10)]) y_train_2 = np.array([0,0,0,0,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1]) x_test_2 = np.hstack([1/2+1/2*np.random.uniform(size=10), 1/2-1.5*np.random.uniform(size=10)]) y_test_2 = np.array([0,0,0,1,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,1,1]) keras.backend.clear_session() m = keras.models.Sequential([ keras.layers.Input((1,)), keras.layers.Dense(3, activation="relu"), keras.layers.Dense(1, activation="sigmoid") ]) m.compile( optimizer=keras.optimizers.Adam(lr=0.05), loss="binary_crossentropy", metrics=["accuracy"]) history = m.fit(x_train_2, y_train_2, validation_data=(x_test_2, y_test_2), batch_size=20, epochs=300, verbose=0) TL;DR Your loss might be hijacked by a few outliers, check the distribution of your loss function on individual samples of your validation set. If there are a cluster of values around the mean then you are overfitting. If there are just a few values very high above a low majority group then your loss is being affected by outliers :)
