[site]: datascience
[post_id]: 25189
[parent_id]: 25151
[tags]: 
Your problem is a classification problem with 3 classes. The appropriate function for it is softmax function, whereas you are using Relu. Another thing is that you need a cost function. The function that you optimize and the one you do calculate derivatives of with respect to parameters. The cost function and last layer activation function must match each other. So first you choose appropriate "last" activation funtion for your problem and then cost function. For iris you can use softmax function with cross entropy function. The derivatives of these you can easily find. :) Another thing are the dimensions. The result of softmax function is the vector of the size (number of classes, 1) for each observation whereas you have (1,3) dimensions. Regarding the backpropagation algorithm for the other layers it is looks ok, but the last layer equation is wrong and should be like the one below: where C is the cost function and we calculate derivative of C with respect to a (activation of last layer) and multiply element-wise by derivative of a (here it should be softmax function with respect to z). dw and db for last layers have correct equation. I hope this hints will help you come up with final neural network implementation for iris classification problem.
