[site]: stackoverflow
[post_id]: 4870138
[parent_id]: 
[tags]: 
Cowardly PHP script just quits when error is encountered

I have a cURL function that spiders all the webpages specified in an array. The array is called $to_be_spidered, I have the function being executed like so: $to_be_spidered = array('http://google.com', 'http://mysterysite.com', 'http://yahoo.com'); for ($i = 0; $i != count($to_be_spidered); $i++) { $ch = curl_init(); curl_setopt($ch, CURLOPT_USERAGENT, $userAgent); curl_setopt($ch, CURLOPT_URL,$target_url); curl_setopt($ch, CURLOPT_FAILONERROR, true); curl_setopt($ch, CURLOPT_FOLLOWLOCATION, true); curl_setopt($ch, CURLOPT_AUTOREFERER, true); curl_setopt($ch, CURLOPT_RETURNTRANSFER,true); curl_setopt($ch, CURLOPT_TIMEOUT, 0); // set cURL timeout $html= curl_exec($ch); // error handling if (!$html) { echo " cURL error number:" .curl_errno($ch); echo " cURL error:" . curl_error($ch); exit; } // etc. etc... } Now the problem is, if a webpage returns an error like a 404, the script is killed. For example if mysterysite.com is not found the script does not attempt to spider yahoo.com. It just quits that and all the links after. I would like it to quit attempting to spider the error link and move on to the next link in the queque. I tried changing "exit" to "continue" but no luck. It still stops. Am I doing something wrong or is this specific to using cURL?
