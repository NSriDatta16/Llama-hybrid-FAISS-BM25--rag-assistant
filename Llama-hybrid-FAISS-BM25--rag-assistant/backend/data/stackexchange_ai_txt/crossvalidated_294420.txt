[site]: crossvalidated
[post_id]: 294420
[parent_id]: 294412
[tags]: 
First, hypothesis testing isn't usually used in a machine learning context. The typical goal of machine learning is to make accurate predictions about future observations, whereas hypothesis testing is a method for drawing inferences about unobserved parameters. (See this answer .) Second, the p-value in a hypothesis test is the probability of observing data with the test statistic value $t = T$ given a hypothesis $H$: $p = Pr(T = t | H)$. When $H$ says that some parameter is equal to some particular value, this probability should be straightforward to calculate. For example, say $\theta$ describes the probability that a coin comes up heads, $T_n$ is the fraction of coin flips that come up heads in $n$ trials, and in one particular experiment you observe $T_n = .6$, or 60% of the flips come up heads. Then you can calculate $p = Pr(T_n = .6 | \theta = .5)$ to test the hypothesis that the coin is fair. So, to test the hypothesis that $\theta \neq .5$, you would have to calculate $Pr(T_n = .6 | \theta \neq .5)$. Which is generally going to be intractable, if not incoherent. Following on whuber's comment, I think what you want is to estimate the accuracy of your classifier. Read up on using cross-validation to do that.
