[site]: crossvalidated
[post_id]: 519795
[parent_id]: 
[tags]: 
Standarizing over variates on multivariate time series

I want to run a clustering algorithm (means with multidimensional euclidean distance) on multivariate time series. Each variate of the time series has different units so it needs to be normalized. I have seen that in most cases, time series are normalized by subtracting the mean and std of each time series. I may be missing something but it looks like this removes the effect of the actual values on the time series. For instance, if a time-serie with a given shape has a mean of 30 on its raw form and another time series has the same shape but a mean of 20, after removing the mean and the std of both, they will look exactly the same. For a dataset X with shape (n_samples, n_timestamps, n_dimensions), it would look like: >>> X = (X-np.mean(axis=1))/X.std(axis=1) In my dataset, the difference in values is important. I was thinking about normalizing only the variates of the time series. I would do that by calculating the mean of all the values of each dimension and susbtracting it from all the time-series. This transformation would be: >>> X = (X-np.mean(axis=(0,2)))/X.std(axis=(0,2)) Is there something wrong in doing it this way?
