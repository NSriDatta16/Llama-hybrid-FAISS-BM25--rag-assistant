[site]: crossvalidated
[post_id]: 414894
[parent_id]: 242907
[tags]: 
Unlike common classification problems where loss function needs to be minimized, GAN is a game between two players, namely the discriminator (D)and generator (G). Since it is 'just a game', both players should fight for the same ball! This is why the output of D is used to optimize both D and G. Instead of the term 'loss', I would rather use 'objective'. Please note that the common tensor flow implementation of GANs using Cross Entropy with Logits (CEWL) for both Discriminator and Generator is not fully in line with Ian's equations in https://arxiv.org/abs/1406.2661 . Yet, using CEWL seems to give better results than applying Ian's equations! Here are the underlying equations for CEWL implementation: $D_{o}=\frac{1}{2m} \sum_{i=1}^{m}\left[D_l\left(G\left(z^{(i)}\right)\right)+\log \left(1+e^{-D_l\left(G\left(z^{(i)}\right)\right)}\right)+\log \left(1+e^{-D_l\left(x^{(i)}\right)}\right)\right]$ $G_{o}=\frac{1}{m} \sum_{i=1}^{m}\left[\log \left(1+e^{-D_l\left(G\left(z^{(i)}\right)\right)}\right)\right]$ Both functions above are to be minimized. The second term in D's objective function has conflicting impact, yet it works!
