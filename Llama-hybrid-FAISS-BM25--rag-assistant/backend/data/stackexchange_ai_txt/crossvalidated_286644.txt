[site]: crossvalidated
[post_id]: 286644
[parent_id]: 286640
[tags]: 
A weighted average of any sequence $x_1, x_2, \ldots, x_n$ with respect to a parallel sequence of weights $w_1, w_2, \ldots, w_n$ is the linear combination $$(w_1 x_1 + w_2 x_2 + \cdots + w_n x_n)\ /\ (w_1 + w_2 + \cdots + w_n).\tag{1}$$ An exponentially weighted average (EWA), by definition, uses a geometric sequence of weights $$w_i = \rho^{n-i} w_0$$ for some number $\rho$ . Since the common factor of $w_0 \ne 0$ will cancel in computing the fraction $(1)$ , we may take $w_0=1$ if we wish. The EWA depends on the weights only through the number $\rho$ . Moreover, the denominator of $(1)$ simplifies to $1 + \rho+\rho^2 + \cdots + \rho^{n-1} = (1-\rho^n)/(1-\rho)$ , enabling us to write $$\operatorname{EWA}_\rho(x_1, \ldots, x_n) = \frac{1-\rho}{1-\rho^n}(\rho^{n-1}x_1 + \rho^{n-2} x_2 + \cdots + \rho\, x_{n-1} + x_n).$$ What makes these particularly nice is that as the sequence $(x_i)$ grows, its EWA is very simple to update, because $$\eqalign{ \operatorname{EWA}_\rho(x_1, \ldots, x_n, x_{n+1}) &= \frac{1-\rho}{1-\rho^{n+1}}(\rho^n x_1 + \rho^{n-1} x_2 + \cdots + \rho\, x_n + x_{n+1}) \\ &= \rho\frac{1-\rho^{n}}{1-\rho^{n+1}}\operatorname{EWA}_\rho(x_1, \ldots, x_n) + \frac{1-\rho}{1-\rho^{n+1}}x_{n+1}.\tag{2}}$$ Although that looks messy, it's really very simple: the updated EWA is a weighted average of the previous EWA and the new value $x_{n+1}$ . We don't need to hold on to all the $n$ preceding values: we only need the most recent EWA. Even better, usually $|\rho| \lt 1$ (to downweight the "older" values compared to the "newer" ones later in the sequence), which means once $n$ is sufficiently large, the values of $\rho^n$ and $\rho^{n+1}$ are negligible compared to $1$ , whence $$\frac{1-\rho^n}{1-\rho^{n+1}} \approx 1;\quad \frac{1-\rho}{1-\rho^{n+1}} \approx 1-\rho $$ to high accuracy. With this approximation in mind, the update $(2)$ becomes $$\operatorname{EWA}_\rho(x_1, \ldots, x_n, x_{n+1}) = \rho\operatorname{EWA}_\rho(x_1, \ldots, x_n) + (1-\rho)x_{n+1}.\tag{2a}$$ This rule $(2a)$ is sometimes used to define the EWA, recursively. Now, provided $\rho \gt 0$ (which is very nearly always the case), it's straightforward to show that the weighted average lies between the extremes of the data values, so in particular $$\max(x_1, \ldots, x_n) \ge \operatorname{EWA}_\rho(x_1, \ldots, x_n) \ge \min(x_1, \ldots, x_n).$$ When $x_1=x_2=\cdots=x_n=c,$ say, then obviously the EWA must be $c$ itself (which is both the max and min of the $x_i$ ). Here are some illustrations of how the EWA works. At left is the $\operatorname{EWA}_\rho$ of $(1,2,\ldots, 10)$ as $\rho$ ranges from $0$ to $1$ . As $\rho\to 1$ , the EWA approaches the arithmetic mean because all the weights $\rho^{n-i}$ approach equal values of $1$ . When $\rho \approx 0$ , all but the last value ( $x_{10}=10$ ) are heavily downweighted, producing an EWA close to the last value. At middle and right are sequences of dots showing $x_1, \ldots, x_{50}$ and two EWA smooths: one for a high value of $\rho$ , which downweights older values less, and one for a lower value of $\rho$ , which--by downweighting older values more--tends to be less smooth but also closer to the recent $x$ values. Here is an R implementation, via the function ewa , along with illustrations of its use to create the figures. ewa 1) w 300) + rnorm(length(i), sd=0.25) z
