[site]: crossvalidated
[post_id]: 464660
[parent_id]: 
[tags]: 
If L2 regularization parameter is high and learning rate low, can cost of cross entropy loss function increase?

I coded a neural network from scratch. When regularization parameter is too high and the learning rate too low, cost increases. I suspect that the added cost (associated with regularization) to loss function is responsible for this. When the regularization parameter is set to zero I always get a nice decrease in cost function. Can you please explain what is happening?
