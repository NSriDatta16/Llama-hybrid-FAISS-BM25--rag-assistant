[site]: crossvalidated
[post_id]: 597050
[parent_id]: 
[tags]: 
Is reaching perfect performance on the training set feasible for large neural networks in regression?

Suppose I have a large training sample, say a million observations, and want to perform regression with a large ReLU-activated neural network (both deep and wide, with many times more parameters than the sample size). In principle, it should be possible to interpolate the training data and achieve perfect performance (or at least perfect up to some very tiny "numerical error"). I'm wondering whether it's feasible in practice to find such an interpolating network by training the network. Is this something that is practically feasibly, say by performing SGD with square loss for a long time and with many random initializations? Or is this way too computationally expensive in large-scale cases like this, and we need to be satisfied with some non-zero training loss? What about the case where the neural network has less parameters than the sample size? Or the case where we have more parameters, but we restrict the number of non-zero parameters being less than the sample size (ie, sparsity)? I know about the potential overfitting issue, but I'm interested in this more out of theoretical curiosity.
