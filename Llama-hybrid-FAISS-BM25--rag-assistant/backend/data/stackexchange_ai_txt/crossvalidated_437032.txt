[site]: crossvalidated
[post_id]: 437032
[parent_id]: 270546
[tags]: 
If you're more interested in the "mechanics", the embedding layer is basically a matrix which can be considered a transformation from your discrete and sparse 1-hot-vector into a continuous and dense latent space. Only to save the computation, you don't actually do the matrix multiplication, as it is redundant in the case of 1-hot-vectors. So, say you have a vocabulary size of 5000, as your input dimension - and you want to find a 256 dimension output representation of it - you will have a (5000,256) shape matrix, which you "should" multiply your 1-hot vector representation to get the latent vector. Only in practice instead of multiplying you just take the index... Source: Andrew Ng (One way that helps me think of it in theory , is as a Dense layer only without bias or activation... ) The weights of this matrix are learned through training - you could train it as a Word2Vec, GloVe, etc. - or on the specific task that you are dealing with. Or you can load pre-trained weights (say GloVe) and continue training on your specific task.
