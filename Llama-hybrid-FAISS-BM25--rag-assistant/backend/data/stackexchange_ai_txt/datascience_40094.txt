[site]: datascience
[post_id]: 40094
[parent_id]: 40076
[tags]: 
First for RNN part, you should consider to read documentation about input_shape and batch_input_shape . Then your Activation layer seems to be useless because you already apply relu activation in RNN layer. Finally, your network returns output's shape of (20,) (RNN return (20,) and activation don't change shape) and you compare this with y_train with shape of (1,) . Shapes dismatch so it return this error EDIT : You can print model.summary() to monitor input/output 's shape
