[site]: crossvalidated
[post_id]: 230716
[parent_id]: 
[tags]: 
How do I determine when a distribution has reached a steady state?

I have a dataset of the properties of a population over time. The data come from a (sort-of) agent-based model, where agents reproduce (with evolution) and die. After some time, the population should reach a "steady state", where the agents have evolved to a survival strategy that is stable/self-sustaining over a long period of time. I would like to determine when the population has reached this point (has stopped changing or become stable over time). It's possible that there is more than one survival strategy, so there could be two different sub-populations with different values for the property, both of which are stable. Even after this point, the distribution will change slightly as agents die and reproduce, so there should be some tolerance for variation in whatever test I use. But agents that are not well-adapted should die more quickly than agents that are optimal, so the distribution should not change too much once the optimal strategy (or strategies) has been reached. I'm not interested in outliers especially - there will always be outliers with this sort of model, but they generally die very quickly. The mean and median are pretty meaningless (pun not intended) for this data, because there could be more than one strategy that is viable (i.e. two peaks in the density distribution). As noted above, the size of the population can change too, so the standard deviation is probably not a particularly useful metric either. Due to the sheer amount of data I have, I'm looking for a few numbers that I can put in a table rather than something like a QQ plot. What's the correct test to use here? I'm looking for something like an Augmented Dickey-Fuller test only on a distribution, rather than a single point. What I had come up with myself was to use a Kolmogorov-Smirnov statistic between the distribution at time 0 and the distribution at time $n$ (call this $KS_0$), and between the distribution at time $n$ and at time $n - 1$ (call this $KS_{n-1}$). When the distribution is in a steady state, $KS_0$ should be stationary around some constant (the distribution will have moved away from the distribution at $t=0$, but is otherwise not changing) and $KS_{n-1}$ should be approximately stationary around 0 (the distribution has stopped changing between time steps). I could then even use a ADF test on the $KS_0$ and $KS_{n-1}$ timeseries to test the stationarity of those. This is probably somewhat crude, but I don't need to be especially rigorous. I am a physicist, not a mathematician, so it's most likely a lack of terminology that has meant I am unable to find such a test by myself. Answers that I can implement in Python are preferred!
