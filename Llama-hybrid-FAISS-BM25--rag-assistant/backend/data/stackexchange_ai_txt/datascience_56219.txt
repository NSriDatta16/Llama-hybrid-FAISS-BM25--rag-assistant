[site]: datascience
[post_id]: 56219
[parent_id]: 56215
[tags]: 
If you have a very complex machine learning models (i.e. one with a lot of parameters ) and you try to train it on a fairly small dataset (i.e. few samples), then the model has the capability of memorizing those samples. This means that it will learn a set of weights where for every single one of the input samples, it will predict its label exactly! This is apparent beacause the model reaches a training loss of zero . This is referred to as overfitting and is a problem because, while the model is performing adequately on the training set, it can't generalize on unseen data. If you want to read on how to prevent this I suggest reading this post .
