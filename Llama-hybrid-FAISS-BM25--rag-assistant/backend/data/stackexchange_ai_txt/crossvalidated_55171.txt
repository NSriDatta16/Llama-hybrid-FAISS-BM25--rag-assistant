[site]: crossvalidated
[post_id]: 55171
[parent_id]: 
[tags]: 
Mixture model as a prior distribution

I've just started working with Bayesian models. My question is in the context of hierarchical Bayesian model. Suppose you have n models to train. However, some of these models are similar to each other in some sense and I could assume that they their parameters are drawn from the same prior distribution. Another subgroup of that models are also similar to each other and their parameters share the same prior distribution. And it repeats for all subgroups (unknown in advance). So, my question is: Is it possible to use a mixture of distributions as a prior distribution for all these n models? In such hierarchical model each component of the mixture model respond to a group of models. The underlying idea is to cluster these n models by employing a mixture model as a prior distribution. Does it make sense? Thanks!
