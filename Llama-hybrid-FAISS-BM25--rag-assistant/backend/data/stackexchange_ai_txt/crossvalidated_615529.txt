[site]: crossvalidated
[post_id]: 615529
[parent_id]: 
[tags]: 
Accumulate the gradient on all conditions before or after backpropagating

I have been working on coding a simple Neural Network, and I have come across a question that I would like to discuss with you. I am trying to approximate two functions with the Neural Network: $f_1(x) = x^2$ and $f_2(x) = (1-x)^2$ . To calculate the local losses, I am using the following equations: $L_1 = \|NN(x) - x^2\|$ and $L_2 = \|NN(x) - (1 - x^2)\|$ My question is regarding the optimization process. Should I perform the optimization for each condition separately and not cumulatively? Or should I accumulate the gradient on all conditions before backpropagating? I am not sure which approach would be more appropriate?
