[site]: crossvalidated
[post_id]: 371719
[parent_id]: 371712
[tags]: 
One example arises from rank deficiency. Suppose that you're conducting an OLS regression but your design matrix is not full rank. In this case, there are any number of solutions which obtain the maximum likelihood value. This problem isn't unique to OLS regression, but OLS regression is a simple enough example. Another case arises in the MLE for binary logistic regression. Suppose that the regression exhibits separation ; in this case, the likelihood does not have a well-defined maximum, in the sense that arbitrarily large coefficients monotonically increase the likelihood. In both cases, common regularization methods like ridge penalties can resolve the problem.
