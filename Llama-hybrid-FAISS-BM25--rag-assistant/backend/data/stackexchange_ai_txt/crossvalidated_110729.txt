[site]: crossvalidated
[post_id]: 110729
[parent_id]: 26792
[tags]: 
They are not that similar, but they are related though. The point is, that both kNN and RBF are non-parametric methods to estimate the density of probability of your data. To see this let us first consider the case of kernel methods. Say you consider a region of the feature space $R$. If you draw sample points from the actual probability distribution, p(x), independently, then the probability of drawing a sample from that region is, $$ P = \int_{R} p(x) dx $$ What if you have $N$ points? The probability that $K$ points of those $N$ points fall in the region $R$ follows the binomial distribution, $$ Prob(K) = {{N} \choose {K}}P^{K}(1-P)^{N-K} $$ As $N \to \infty$ this distribution is sharply peaked, so that the probability can be approximated by its mean value $\frac{K}{N}$. An additional approximation is that the probability distribution over $R$ remains approximately constant, so that one can approximate the integral by, $$ P = \int_{R} p(x) dx \approx p(x)V $$ where $V$ is the total volume of the region. Under this approximations $p(x) \approx \frac{K}{NV}$. The idea of kernel methods is to split the feature space in several regions, estimate the counts for each region, and use those point estimates to interpolate across the whole feature space. That may sound gibberish. First let us see how we can rewrite the estimate for the probability. Let $\{x_{i}\}_{1}^{N}$ be your data set. Consider a region $V = h^{d}$ which corresponds to a hypercube of side length $h$ in the $d$-dimensional feature space. The Heaviside function is defined by, $$ H(x) = \begin{cases} 1, \text{if } |x| Then we can write the total number of points that fall within the hypercube, $V$ as, $$ K = \sum_{n}H\left(\frac{x-x_{n}}{h}\right) $$ where $x$ is the center of the hypercube. Or else, V is the neighborhood centered in $x$ and this is the number of points close to $x$. If we substitute back, $$ p(x) \approx \sum_{n} \frac{1}{h^{d}}H\left(\frac{x-x_{n}}{h}\right) $$ The case of RBF is a smoothed version, where $H$ is taken to be a Gaussian. For the case of kNN, please refer to this other question . Notice that this two algorithm approach the same problem differently: kernel methods fix the size of the neighborhood ($h$) and then calculate $K$, whereas kNN fixes the number of points, $K$, and then determines the region in space which contain those points. So yes, they are related, but nor equivalent. P.S. SVM does not estimate the density of probability, but finds the separating hyperplane. Here I just compared kernel methods vs. kNN for density estimation. Even for novelty detection, does SVM not estimate the p.d.f. but its support (those points for which $p(x) \gt 0$, which is a different story.
