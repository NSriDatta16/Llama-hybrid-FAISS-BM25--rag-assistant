[site]: crossvalidated
[post_id]: 560060
[parent_id]: 560038
[tags]: 
You're right, rules of thumb don't always apply: Don't just ignore train results . It's a good idea to review test/train results to check for anomalies like a poor train and great test result. This is what you're doing by examining your loss plot, but there are more methods to further analyse this. A good test result after a bad train result would suggest that there's a discrepancy between train and test sets. When using simple k-folds CV, you can in a given fold have a case where the train set is very complex and the train set is very simple. An example could be imaginary cancer patient data (assuming that most data points are labeled 0 for non-cancer) where you train on labels like this 01100 and test on labels like this 00001. That could, depending on your model, result in a situation where your model predicts all cases as 0, giving bad train results, and very good test results. This is why averaging models can be useful. For a regression problem (meaning continuous data), it's not so easy to visualize the cause of the result. But in any case, you should use a scoring function that makes sense for your data . To measure your regression model performance, you should plot the model coefficients, β0 and β1 . This would give you an actual indicator of model performance. It does not make much sense to expect a model with a good test result and a bad train result to be better than a model with a good train and test result, because the good test result could be a chance result. To sum up, and answer your last question : No, the model from epoch ~200 should most likely not be passed over by the best test result from epoch ~80, because your plot doesn't adequately visualize the model performance. It's hard to give more precise advice, because you haven't said what type of data you are modeling or given any data examples, so I assumed you were modeling continous data. For binary data (which is more easily modeled and interpreted - so consider if it's possible to convert your data to binary): You can visually examine your binary model using methods like a confusion matrix to see what causes the result. You should use binary score metrics to examine the model performance - such as the F1 score - if it's suitable for your data . If you plot F1 score for each epoch, you'll hopefully see it increasing over time.
