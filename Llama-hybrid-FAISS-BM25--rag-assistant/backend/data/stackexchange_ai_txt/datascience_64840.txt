[site]: datascience
[post_id]: 64840
[parent_id]: 
[tags]: 
Incentivizing curiosity in a sparse reward environment

I'm quite new to reinforcement learning, but have been exploring different kinds of architectures (DQN, dueling DQN, actor critic, etc.) and evaluating their ability to solve certain problems. The problem I'm applying RL to has a continuous observation space (albeit very low dimensional), and a discrete (binary) action space where the agent can either "act" or "do nothing". Opportunity for reward is quite sparse, and comes along randomly approximately once every thousand time steps, and the spacing of the agent's actions around the opportunity has a profound impact on the probability of success. Every time the agent "acts", it uses part of a finite resource, which is replenished randomly according to a self exciting Hawkes process. Put more visually, this is what 70ish time steps of gameplay might look like. "." is do nothing, "x" is shoot, and "O" is when an opportunity comes along. ....x......x....x.x..x......O..x.....x...x..x.x...........x........x..x... To reiterate, the probability of "O" succeeding depends entirely on the spacing of the two "x" around it. I'm working on getting a dueling DQN structure to work right now, but the problem I'm having is that the agent seems to get stuck in a local minima, where he shoots at every time step, and only slows down when his resources start to run low. However, this is not the optimal play strategy, as if he were to shoot more widely spaced apart, the probability of success would actually increase (there is a sweet spot). But he seems to be content with the tiny probability of success given xOx even though x....O.....x would be far more likely to succeed. I have been using a steadily decaying epsilon to try to promote exploration, but the agent really never "explores" all the possible strategies (such as acting once every 5 time steps, once every 10 time steps, once every 100...). I suspect that one of the reasons for this is because even when epsilon is very high, and the agent takes a random action, on average, it will be taking an action every 2 time steps, so it has no incentive to see what the reward structure looks like if it shoots more infrequently. The reason I'm using RL to solve this problem at all is because I think the optimal solution is probably quite nuanced, where he shoots at the best rate for success when he is high on resources, then shoots more slowly when his resources start dwindling, until they get replenished, at which point he shoots at the optimal rate again. I recently ran across this paper on curiosity driven exploration, and it seems vaguely applicable to the problem I'm having. But before going down that complicated route, I wanted to post here to see if I'm missing anything obvious. Here is a list of things i've tried in order to promote wider exploration: Give a small penalty every time the agent shoots since it is using a resource Have an exponentially growing penalty as the resource runs out to try to get him to shoot slower. This works, but it doesn't incentivize shooting slower when he has plenty of resource Give a small reward proportional to the "goodness" of the spacing every time he acts to try to help him find the best spacing. This approach was a total fail because he ends up shooting every time step and just gathering all the tiny little rewards, rather than shoot once every n time steps and get big reward. I really don't know what else to do, and would greatly appreciate any advice.
