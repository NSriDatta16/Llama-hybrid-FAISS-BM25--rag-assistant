[site]: crossvalidated
[post_id]: 192188
[parent_id]: 191645
[tags]: 
Precision is what fraction actually has cancer out of the total number that you predict positive, precision = ( number of true positives ) / (number of positives predicted by your classifier) Recall (or true positive rate) is, what fraction of all predicted by your classifier were accurately identified . true positive rate = true positives / ( True positive + False negative) Coming to F-score, it is a measure of trade-off between precision and recall. Lets assume you set the thresh-hold for predicting a positive as very high. Say predicting positive if h(x) >= 0.8 , and negative if h(x) you have huge precision but low recall. You have a precision of (15)/(15+20) = 42.8% (15 is the number of true positives 20 total cancerous, subtracted 5 which are wrongly predicted) If you want to have a high recall [or true positive rate], it means you want to avoid missing positive cases, so you predict a positive more easily. Predict positive if h(x) >= 0.3 else predict negative. Basically having a high recall means you are avoiding a lot of false negatives . Here your true positive rate is ( 15 / (15+5) )= 75% Having a high recall for cancer classifiers can be a good thing, you totally need to avoid false negatives here. But of course this comes at the cost of precision. F-score measures this trade-off between precise prediction vs avoiding false negatives. Its definition can be arbitrary depending upon your classifier, lets assume it is defined as the average between precision and true positive rate. This is not a very good F-score measure because you can have huge recall value, and very low precision [eg predicting all cases positive] and you will still end up with an F-score which is same that when your precision and recall are well balanced. Define F score as : 2 * (Precision * Recall) / (Precision + Recall) Why? If you have very low precision or recall or both, your F-score falls; and you'll know that something is wrong. I would advise you to calculate F-score, precision and recall, for the case in which your classifier predicts all negatives, and then with the actual algorithm. If it is a skewed set you might want more training data. Also note that it is a good idea to measure F score on the cross-validation set. It is also known as F1-score. http://arxiv.org/ftp/arxiv/papers/1503/1503.06410.pdf https://www.google.co.in/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=a+probabilistic+theory+of+precision+recall+and+f+score
