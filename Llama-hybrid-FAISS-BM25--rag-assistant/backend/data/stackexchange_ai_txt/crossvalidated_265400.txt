[site]: crossvalidated
[post_id]: 265400
[parent_id]: 
[tags]: 
Deep Learning: How does beta_1 and beta_2 in the Adam Optimizer affect it's learning?

My impression is that beta1 and beta2 affects how much the adam optimizer 'remember' it's previous movements, and my guess is that if the training isn't performing well, these values should be decreased, am I right? However, what is the general intuition one should have when deciding whether to tweak these parameters or not, or by how much to change these values?
