[site]: datascience
[post_id]: 106328
[parent_id]: 106326
[tags]: 
One way to approach is to separate the steps. Learn an embedding space of either words and/or documents. Learning an embedding makes no assumptions about the distributional form of the data. The data (e.g., words or documents) could be uniform, normal, or another distribution. The result is an embedding space. Gaussian Mixture Model (GMM) clustering of entities in the embedding space. GMM assumes a multivariate normal distribution best fits the data since it only estimates the parameters related to a multivariate normal distribution. The more the underly features are not a multi-variate normal distribution, the worse a GMM model will fit. It is up to you as the modeler to decide if the features are normally distributed enough that a GMM is a useful model. If GMM is not a useful model, then choose a non-parametric clustering algorithm (e.g., kernel density estimation) that has fewer assumptions than a GMM.
