[site]: datascience
[post_id]: 76735
[parent_id]: 76677
[tags]: 
There are lots of things that might lead to this performance. With the very limited information provided I can only guess and suggest the reasons. I have personally used lots of Pacman variations (including this one) with RL models successfully. My first initial guess is that you are stopping exploration too early. Pacman environments are very hard to solve (up to the last level). You can get a reasonable performance though within a couple of days training (depending on your system). I highly doubt that the agent underfits/overfits as it will take time for the agent to reach a good score. Also underfitting/overfitting would be better observable when you test your agent (you do not update weights). I would let exploration to become minimum once the agent reaches the last episode. Then I would test the current state of the agent with very little or none amount of exploration (full exploitation).
