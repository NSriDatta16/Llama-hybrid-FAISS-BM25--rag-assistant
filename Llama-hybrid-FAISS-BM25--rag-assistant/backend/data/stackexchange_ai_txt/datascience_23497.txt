[site]: datascience
[post_id]: 23497
[parent_id]: 
[tags]: 
What limits the long-term dependencies learned by an LSTM?

LSTMs and GRUs are meant to learn longer-term dependencies than basic RNN architectures that do not have gated architectures. However, both LSTMs and GRUs are still typically trained with truncated backpropagation through time. Naively, it seems that the choice of truncation length will limit how many time-steps the network can "remember". Is this the case, or can an LSTM trained with BPTT truncated to n steps learn dependencies from more than n steps in the past?
