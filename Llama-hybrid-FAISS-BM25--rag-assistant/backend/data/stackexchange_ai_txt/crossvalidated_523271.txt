[site]: crossvalidated
[post_id]: 523271
[parent_id]: 
[tags]: 
How the Negative Sampling algorithm chooses the negative samples (k) in character-based embeddings for the Word2Vec model?

In the context of word-based embeddings, the Negative Sampling algorithm chooses negative samples (k) from the most frequent words in the corpora which usually present less meaningful information than rare words [1]. How the algorithm chooses the negative samples to train character-based embeddings since the corpora consist of characters only? What is the selection criterion of (k) when it comes to characters? [1] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems, pp. 3111–3119, 2013.
