[site]: datascience
[post_id]: 62989
[parent_id]: 
[tags]: 
Classification - Divide the interval (0 - 1] to lets say 100 classes and use each class to make a calculation

class-1 represents 0.01, class-i represents 0.01*i, class-100 represents 1.00. Thus, when the classifier predicts the class-y and it should have predicted class-(y+1) there is a small error so we can accept class-y. Is there a way to express this behaviour in a neural network? Maybe with a distribution or something? PS: Not interested in regression.
