[site]: crossvalidated
[post_id]: 21091
[parent_id]: 21067
[tags]: 
First I would ask what do you mean by robust logistic regression (it could mean a couple of different things ...). Nevertheless, assuming that you are using "robust" in the sense that you want to control for heteroscedasticity in binary outcome models what I know is the following: 1) You should read in detail the 15th chapter of the Wooldridge 2001 Econometrics of Cross Section and panel data book (or any other equivalent book that talks about binary outcome models in detail). 2) Heteroscedasticity in binary outcome models will affect both the "Betas" and their standard errors. 2a) BETAS: Heteroscedasticity in binary outcome models has functional form implications. Basically F(XB) != LOGISTIC( XB ) if you have it, then your estimation will be inconsistent regardless of what you do ... If you are absolutely sure about the type of heteroskedasticity you are having, this is, how your error changes as X changes, then you can correct your covariates accordingly to control for this. Since that is unlikely there is nothing you can do about it. Now the fact that the estimation of Betas is inconsistent might not be very relevant anyway since the partial effects may still be a good approximation of the real partial effects. The whole point here is that heteroscedasticity in binary outcome models implies functional form mispecification and should be treated accordingly. As an example think about probit vs logit. For your data, only one of these models can be the correct data generation process (if any). So when you estimate both of them you must know that at least one of the models will surely have inconsistent betas. But if go and look at their partial effects you won't see much of a difference ... Go and test for heteroscedasticity first to see if this can be an issue. There are several tests arround .... 2 b) Standard Errors: Under heteroscedasiticty your standard errors will also be miscalculated by the "normal" way of estimating these models. In R what i would suggest is that you use bootstrap since i am not sure if there are any packages available that correct for kinds of misspecification and/or that allow for intragroup correlation .... If this has nothing to do with what you asked and as Rolando2 pointed out in the comment you are trying to penalize outliers in the regression then you should know that your use of the lrm function is not correct : you are calling it with the default parameters in which case, quoting from the documentation : The default is penalty=0 implying that ordinary unpenalized maximum likelihood estimation is used Hope this helped somehow, Miguel
