[site]: crossvalidated
[post_id]: 56313
[parent_id]: 51569
[tags]: 
There are some substantial preprocessing steps that I am not seeing here. You can transform the information in ways that are radically different than taking a geographic average as you do when downsampling, and that throw away much less information. Options: Consider making an empirical CDF of the intensity values in the images and using them as inputs. If you have 8-bit pixels then this is only 255 inputs. Consider making an empirical CDF of the intensity values in sub-panes of the image (8x8 panes) and using them as inputs. Consider using GMM on your image to approximate some level of rescaled intensity values for your images, then reversing them such that when the image intensity values are input the probability of membership is output. Use the membership probability as input to your model. Perform a Gaussian smooth on numeric derivative over the image for "approximate edge" location (or use Sobel or whatever) and then perform the above transformations and use as inputs to your network. I think that idea 1 is your best bet for good classification on very sparse data. You are engaging the "curse of dimensionality" and if you don't have lots of compute resources and lots of training data then you can get results that are pretty meaningless.
