[site]: crossvalidated
[post_id]: 546968
[parent_id]: 
[tags]: 
Where is Boosting applied in Gradient boosting techniques?

In boosting, the primary idea is to re-adjust weights of training instances, so that subsequent models learn how to fit difficult-to-classify samples. From Wikipedia Boosting (Machine Learning) : While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy. After a weak learner is added, the data weights are readjusted, known as "re-weighting". Misclassified input data gain a higher weight and examples that are classified correctly lose weight.[note 1] Thus, future weak learners focus more on the examples that previous weak learners misclassified. However, in Gradient Boosting , I do not see any reference of training instance re-weighting being mentioned. Also, there is no such parameter in the implementation on Scikit-Learn . I do see a parameter called learning rate , that combines subsequent models, however, that seems to be fixed and not adjustable for each subsequent model.
