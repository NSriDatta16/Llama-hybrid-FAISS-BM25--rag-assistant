[site]: crossvalidated
[post_id]: 628812
[parent_id]: 
[tags]: 
How do i apply conformal prediction to achieve voxel-wise uncertainty quantification in a 3D binary segmentation problem?

Context: Typically in an image segmentation problem we go from the model's output logits to sigmoids to discrete labels (0 or 1 in this case). Akin to a binary classification problem, we can set up a calibration dataset to find the distribution of q and define alpha. Q1: I understand how conformal prediction can give me sets/intervals with guaranteed confidence. I don't understand how a prediction set can have any value in a binary classification/segmentation setting. Should I instead go for guaranteed intervals for the sigmoid outputs? Q2: How do I go from a guaranteed prediction set/interval to a measure of uncertainty? Specifically, I want an uncertainty map in the same dimension as my input images (128**3 in this case), i.e. voxel-wise UQ. I've not seen any study implementing CP for UQ except for this: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9103229 Most studies I've seen implement Bayesian methods for UQ (e.g. training 8 different models and computing the entropy of the predictions at each voxel as a measure of uncertainty) Q3: Is there such a thing as cross-calibration in the context of CP? Similar to going from a hold-out validation to cross-validation, my idea is to perform a 5-fold cross-calibration to get a better estimate of the distribution of q. Lastly, I'd appreciate any resources you could point me to.
