[site]: crossvalidated
[post_id]: 258361
[parent_id]: 258354
[tags]: 
You can see how this sort of thing can happen by looking at an extreme case. Sorry I can't show you SAS code, but here is a dummy example in R: library(randomForest) library(MASS) N using all the variables, looking at 1/3rd of them by default at each split: rf1 using only the good variable: rf2 using all the variables, but tying all the variables at each split rf3 rf1 Call: randomForest(x = X, y = y) Type of random forest: regression Number of trees: 500 No. of variables tried at each split: 3 Mean of squared residuals: 0.04463831 % Var explained: 95.32 1> rf2 Call: randomForest(x = X[, 1, drop = FALSE], y = y) Type of random forest: regression Number of trees: 500 No. of variables tried at each split: 1 Mean of squared residuals: 0.01377959 % Var explained: 98.56 1> rf3 Call: randomForest(x = X, y = y, mtry = 10) Type of random forest: regression Number of trees: 500 No. of variables tried at each split: 10 Mean of squared residuals: 0.01192359 % Var explained: 98.75 So you see that random forest is not completely invulnerable to noise variables. Why does this happen? Because in each split of each tree, the method of random subspaces will select from among the available variables and the useful ones may not be among that split! When we set the algo to look at each variable at each split, we get the same performance as when we only look at the good one.
