[site]: crossvalidated
[post_id]: 18648
[parent_id]: 18336
[tags]: 
I interpret your question ( and others may have their own interpretation ) as asking for different ways to detrend data. In answering your question I am clearing up some unfinished discussions regarding data recently analyzed (how does CohortB relate to CohortA ). There is free software available on the net to fit local splines to a time series thus enabling a powerful way of detrending http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=7305899 is one source for STL2. It is defined as "An algorithm is described which generates a piecewise linear approximation to tabulated data which is within specified tolerances of the data points" The only problem is that you have to pre-define how many individual break points i.e the number of individual trends you require and you have to manually cleanse your series of aberrant data points. Too many splines (local trends) and you connect two dots . Too few splines (local trends) and you connect none. An alternative approach is to use Intervention Detection schemes promoted by many statisticians including myself. You can review my comments and references for the statistical theory of Intervention Detection at Box-Jenkins model selection . This body of work forms valid statistical tests to incorporate a minimally sufficient number of breakpoints not simply the result of eye-balling. Now to give you an example of this approach to "detrending" which necessarily includes the isolation of one-time unusual values I will use the CohortB series from How to compare 2 non-stationary time series to determine a correlation? which by itself suggests as many as 5 local time trends. When one accounts for the influence/impact of the supporting series there are no additional trends required as the predictor variable has “explained the growth”. For example the original series clearly suggests the need for one or more local time trends and even the possibility that there is curvature or a quadratic component . The search for the number of local lines requires the simultaneous isolation of "unusual points" as we don't want to be incorrectly influenced by them. details the five local trends separated by 4 breakpoints (1,9,19,29) for the 38 data points. Graphically we have . Note that there were 6 influential one-time events that needed to be identified or purged from the original series in order to "see" the five local trends. The number of unusal points has nothing to do with the number of trends and in general is not statistically relatable. Proof that the model is reasonably adequate is found in the plot of the residuals and the ACF of the residuals . Shows the ACF of the residuals The forecast for CohortB in conjuction with the actual and fit is . In summary evaluating the the slopes of the 4 different breakpoints shows a continuing reduction in the expected value of CohortB (without the influence of CohortA). In summary CohortB is growing at a decreasing rate and the "reason" for the decreasing rate is due to CohortA ( see previous discussion ). [X1(T)][(+ .850)] :TIME TREND 1 +[X2(T)][(- .188)] :TIME TREND 9 +[X3(T)][(- .159)] :TIME TREND 19 +[X4(T)][(- .0996)] :TIME TREND 29 In summary if you have the number of local trends then you can use the equation to de-trend thus avoiding your unit root issue. I hope this helps you. This analysis fulfills the typical statistical formulary to suggest powerful new ways to detrend data by obtaining a useful description of the data ( five local time trends ) individual deviations are isolated from exceptional deviations the final model residuals suggest the degree of randomness It is based upon peer-reviewed methodology by many senior statisticians / time-series experts and can be easily programmed by the references I have given .
