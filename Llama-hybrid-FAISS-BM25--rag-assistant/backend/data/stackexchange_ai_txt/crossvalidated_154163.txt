[site]: crossvalidated
[post_id]: 154163
[parent_id]: 154148
[tags]: 
Actually, this can be done to both FFNN's and RNN's, as long as the correct output is given after your inference. For instance, imagine you have a robot that needs to learn the consequences of its actions on the environment. You could use a FFNN for this task and, after each performed action, the robot will receive the correct output by observing the environment and correct its weights (it is called inverse model learning and you can find a paper which applies RNN's to this kind of task here ). Applying this to RNN's on time-series prediction tasks may be more obvious: you predict the next value, then observe the actual value and make corrections. This may or may not be the best approach depending on your problem. For instance, you may have found a good model for your data and don't want to risk messing with it. It is common, for instance, in the videogames industry: if you happen to train (offline) some NN as the controller for an agent, you turn off learning during the game, so it doesn't make something unexpected which breaks the game after release. UPDATE: testuser suggested citing the term temporal difference learning . It is another great example where you can (in this case you need to ) keep learning while acting. It is similar to the robot verifying the consequences of its action, but here it corrects its predictions about the value/quality/reward of the next state.
