[site]: datascience
[post_id]: 23063
[parent_id]: 23050
[tags]: 
There are two important concepts that you need to understand. Scaling your features won't affect their "importance" in your neural network Intuitively, your neural network will itself learn which feature is important or not by learning weights. Scaling your features will speed up your convergence and will limit the risks of overshooting or being stuck in a local optimum. It also makes numerical sense. Intuitively, scaling features can make your training faster because your "path to convergence" will probably be shorter if your features are scaled (right picture). It also makes numerical sense to scale your features because if you have very large and very small values, some of your weights might drop very low and this could cause some numerical issues and hinder the performance of your model.
