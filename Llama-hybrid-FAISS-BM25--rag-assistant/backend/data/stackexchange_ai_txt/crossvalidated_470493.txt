[site]: crossvalidated
[post_id]: 470493
[parent_id]: 469776
[tags]: 
Machine translation cannot be easily evaluated by measuring classification accuracy. For every source sentence, there are very many valid translations, but your reference sentence is only one of the many possible correct ones. Evaluation of machine translation is still an open problem. There is even an annual competition where teams try to develop a metric that correlates best with human assessments. The current most popular metrics are BLEU score based on n-gram precision and METEOR score based on monolingual alignment and F-measure. Recently, also metrics based on BERT seem to perform very well. All these metrics operate on the output string and are independent of the translation model. With sequence-to-sequence models that condition each output symbol on the previous one (i.e., autoregressive models), it might make sense to measure the accuracy given the prefix from the reference sentence, i.e., you have a prefix My name is and you want to predict John . In such a setup, you still have a problem that the model might prefer a different wording and the accuracy would unfair to the model, but in most contexts, there is the only option that is obviously true, so you get a reasonable estimate of the accuracy. Now, the problem is that unlike the previously mentioned metrics, the accuracy depends on how you segment the data. You could for instance artificially boost the accuracy by using small subword units, without actually improving the translation quality. Also, there are experimental models that are no autoregressive and generate the output partially in parallel in random order. In that case, measuring the accuracy does not make really sense.
