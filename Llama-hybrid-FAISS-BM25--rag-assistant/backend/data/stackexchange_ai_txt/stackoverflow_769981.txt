[site]: stackoverflow
[post_id]: 769981
[parent_id]: 769963
[tags]: 
To understand why it is necessary to distinguish between int and long literals, consider: long l = -1 >>> 1; versus int a = -1; long l = a >>> 1; Now as you would rightly expect, both code fragments give the same value to variable l . Without being able to distinguish int and long literals, what is the interpretation of -1 >>> 1 ? -1L >>> 1 // ? or (int)-1 >>> 1 // ? So even if the number is in the common range, we need to specify type. If the default changed with magnitude of the literal, then there would be a weird change in the interpretations of expressions just from changing the digits. This does not occur for byte , short and char because they are always promoted before performing arithmetic and bitwise operations. Arguably their should be integer type suffixes for use in, say, array initialisation expressions, but there isn't. float uses suffix f and double d . Other literals have unambiguous types, with there being a special type for null .
