[site]: datascience
[post_id]: 86773
[parent_id]: 86718
[tags]: 
No I don't think this is good: this means that you replace both variables with only an estimated value of C2 based on C1 (or of C1 based on C2, it's not totally clear to me). Assuming you really want to avoid having two highly correlated features, you could instead: just arbitrarily pick one of them (which is similar to what you're doing but "cleaner") simply use the average of the two values (which might be similar to what you intended to do?) Over a whole set of features you could use more advanced methods such as feature extraction . But it's important to make sure you actually need to do this: correlated features are a problem for some ML methods, but not all of them. For example having correlated features with Decision Trees wouldn't cause any problem. In any case it's worth experimenting with/without the feature in order to observe what actually happens with your data.
