[site]: crossvalidated
[post_id]: 393728
[parent_id]: 393477
[tags]: 
There are three issues here, one technical and two statistical. The technical issue is that lmer seems to be giving incorrect values for the fixed effects. Based on your frequency tables, the log-odds in each combination of Ambiguity and Uncertainty is: Ambiguity No Yes Uncertainty No 1.75 1.00 Yes 1.65 1.26 Logistic regression models the log-odds, so the fixed-effects intercept (estimate for no Ambiguity, no Uncertainty) should be close to 1.75. Your mixed models are providing extremely and unrealistically high values of that intercept (10 or 8 on the log-odds scale, with corresponding odds in the thousands), as @GerardSanroma noted. At first I wondered whether your use of numeric ID codes for the individuals might be involved, but it seems that lmer() converts those to a factor . You might nevertheless want to double-check by explicitly converting the ID values to a factor. It's usually safest to specify yourself the numeric/factor/ordinal/logical classes of predictors, rather than count on software to make the conversion for you. To rule out hidden glitches in your data or code (I didn't find any), fit a standard fixed-effect glm() model to your data (ignoring the random effects for now). You should get a value about 1.7 for the intercept, a small coefficient for Uncertainty (about 0.1) and a larger coefficient for Ambiguity (about 0.6). I think, however, we have to look elsewhere for the technical problem with your mixed models. The default optimization in lmer() can find a local rather than a global optimum. This phenomenon is illustrated on this page and this page . As noted on those pages, if that's the case you can try specifying a different optimizer via a parameter setting within the call to lmer() : control=lmerControl(optimizer="nloptwrap") The first statistical issue is one that you didn't raise but that is seen in the table of log-odds above. Your model for the fixed effects included only direct effects for Ambiguity and Uncertainty. The implicit assumption is that the effect of Ambiguity is independent of the level of Uncertainty, and vice-versa. That doesn't seem to be the case in your results. On the log-odds scale (which the logistic model is using), Ambiguity has almost twice the magnitude of effect in Uncertainty=No as in Uncertainty=Yes . The direction of the effect of Uncertainty differs depending on Ambiguity. It seems important to allow for and to test an interaction term ( Ambiguity:Uncertainty ), even more important than to allow for random effects. Leaving out an important interaction term can easily lead to Simpson's paradox. The second statistical issue is what you can reasonably hope to control for with random effects. With an Ambiguity:Uncertainty fixed-effect interaction term that seems likely to be significant, it would be unwise to try to allow for random main slopes without also including corresponding random interactions. Yet you have at most 1 of each of the 4 Ambiguity-Uncertainty combinations tested per individual. You have fewer observations than the number of effects that you are trying to estimate: the fixed effects of Ambiguity, Uncertainty and their interaction, and corresponding random effects for each individual. Thus there is no hope of "accounting for the intra-individual sensitivity to these manipulations." With your first model (intercept only) you could allow for differences among individuals in their overall tendency to choose Punishment=yes , which might provide some advantage. It doesn't seem wise to go much beyond that in terms of random effects with your data.
