[site]: crossvalidated
[post_id]: 248697
[parent_id]: 248652
[tags]: 
"I've been over the code many times, and I'm almost certain there's nothing wrong with it." There is an issue with the code, actually, in that you fail to replicate the current value of the Markov chain x when rejecting the candidate value can , while you should: thus the conditional update if u should be corrected as done in the R code below: if (u Furthermore, there is a conceptual (and common) error in averaging the $f(x_i)$'s when the $x_i$'s are simulated from the posterior distribution, $g(x)\propto f(x)\mathbb{I}_{(0,\pi)}(x)$. Indeed, the expectation of a term in this average is$$\int_0^\pi f(x)g(x)\,\text{d}x=\int_0^\pi f(x)\times\frac{f(x)\mathbb{I}_{(0,\pi)}(x)}{\int _0^\pi f(x)\,\text{d}x}\,\text{d}x=\frac{\int_0^\pi f^2(x) \,\text{d}x}{\int _0^\pi f(x)\,\text{d}x}$$ Using the posterior sample as you do in the Monte Carlo approximation hence amounts to using the likelihood twice, i.e. integrating $f^2$ rather than $f$. The Harmonic Mean Estimator One immediate way to deduce an approximation of the integral$$\mathcal{I}=\dfrac{1}{\pi}\int_0^\pi f(x)\text{d}x=\dfrac{\cos(\pi)-\cos(0)}{\pi}=\frac{2}{\pi}$$ which is the marginal likelihood, is to use the sample $(x_i)$ from the posterior inside an harmonic mean estimate: $$\hat{\mathcal{I}}=1\big/\frac{1}{n}\sum_{i=1}^n \frac{1}{f(x_i)}$$This is a consequence of the identity$$\mathbb{E}[f(X)^{-1}]=\int_0^\pi f(x)^{-1}\frac{f(x)\mathbb{I}_{(0,\pi}(x)}{\mathcal{I}\pi}\text{d}x=\int_0^\pi\frac{\mathbb{I}_{(0,\pi}(x)}{\mathcal{I}\pi}\text{d}x=\frac{1}{\mathcal{I}}$$ The estimator returns an unbiased estimate of the inverse integral, but its major drawback is that it often has an infinite variance, hence is unreliable no matter what the number of simulations. Radford Neal has called this approach the worst Monte Carlo method ever and I concur. Only use it when you want to check that the value of the integral is what another approach tells you it should be. As in this exercise. f Here is the corresponding result for n=1e5: > I [1] 2.000591 which coincides with the true value. Hence the harmonic mean did not hit too small a value of $\sin(x)$ to induce large variations in the average. But, nonetheless, $1/\sin(x)$ has an infinite integral on $(0,\pi)$ and the resulting harmonic mean estimator has an infinite variance. An explanation for the surprisingly good performances of this "worst Monte Carlo method ever" is that sigma = 0.5 does not often return values of $\theta$ that are close to $0$ or to $\pi$. If instead one uses sigma = 0.01 in the above code, the result is quite different: > I [1] 3.039536 and highly variable > I [1] 0.5901059 Hence it cannot be trusted and should not be used.
