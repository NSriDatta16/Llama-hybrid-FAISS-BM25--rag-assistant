[site]: crossvalidated
[post_id]: 329325
[parent_id]: 
[tags]: 
Trading in precision for better recall in neural network

There's always a tradeoff between precision and recall. I'm dealing with a multi-class problem, where for some classes I have perfect precision but really low recall. Since for my problem false positives are less of an issue than missing true positives, I want reduce precision in favor of increasing recall for some specific classes, while keeping other things as stable as possible. What are some ways to trade in precision for better recall?
