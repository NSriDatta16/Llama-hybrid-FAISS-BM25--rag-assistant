[site]: datascience
[post_id]: 118227
[parent_id]: 118226
[tags]: 
My calculation was based on a wrong understanding of the self attention mechanism. In Attention is all you need the authors point out that they won't use the full $768 \times 768$ matrices when they make use of multi-head attention but rather use $768 / h$ as the internal dimension where $h$ is the number of heads.
