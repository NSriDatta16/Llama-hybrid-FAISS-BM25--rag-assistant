[site]: crossvalidated
[post_id]: 237975
[parent_id]: 
[tags]: 
Why do Deep Learning libraries force the cost function to output a scalar?

Let's say we have a neural net with: 5 input neurons some arbitrary amount of hidden layers 3 output neurons Let's say we're using minibatches of size 32. So, if we input a 5x32 matrix into the neural net, we will then get out a 3x32 matrix of output activations. Assume we are using a simple MSE loss function. We take the difference of our 3x32 target matrix and 3x32 output matrix, and do elementwise squaring of each entry in the resulting new 3x32 matrix, which we'll call M. What I'm confused about, is that I've seen lots of code in Theano and other deep learning libraries that takes the mean of this matrix for its cost function (i.e. outputs a scalar--not a vector). For example, T.mean(T.pow(T-Y, 2)) . Why is this? In my example, shouldn't Theano backprop M, not a scalar value? Even when I'm using minibatches, are these libraries just backpropping scalars?
