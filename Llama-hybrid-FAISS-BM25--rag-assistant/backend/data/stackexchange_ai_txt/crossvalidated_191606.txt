[site]: crossvalidated
[post_id]: 191606
[parent_id]: 191584
[tags]: 
This answer should be viewed as being a compilation of comments which resulted in the OP's confusion being resolved, and is not intended as being the most general or rigorous statement on the subject. Its goal is to aid interpretation. Jeremias K's answer shows the formula for the variance of a weighted sum of random variables. In particular, taking the special case of all weights being $1/n$ for a weighted sum of n independent random variables, the variance of the weighted sum equals $1/n^2$ times the sum of the variances of the individual random variables. If the random variables are also identically distributed, then the variance of the weighted sum equals $1/n$ times the variance of an individual random variable. As you take the average of more and more i.i.d. (Gaussian) random variables, the variance of that average will decrease, as seen from the preceding formulation, which is a result which should be quite familiar. Why does the variance of that average decrease vs. an individual random variable? Well, taking the case of 2 independent random variables, an extreme value in one variable tends not to be matched with a similar direction extreme value in the other variable, hence making the average less extreme than the individual random variables, hence the decreased variance. The magnitude of this effect increases as the number of random variables being averaged increases.
