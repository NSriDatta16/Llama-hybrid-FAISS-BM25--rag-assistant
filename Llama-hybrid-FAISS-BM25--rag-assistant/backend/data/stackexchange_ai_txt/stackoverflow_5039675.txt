[site]: stackoverflow
[post_id]: 5039675
[parent_id]: 5025754
[tags]: 
You may want to look at(which talks about optimizing the number of slots): http://wiki.apache.org/hadoop/LimitingTaskSlotUsage Here is my opinion on the same: 1) Hive would ideally try optimize the number of reducers based on the expected amount of data that gets generated after the map task. It would expect the underlying cluster to be configured to support the same. 2) Regarding whether it may not be a good idea to tweak this count or not: First lets try to analyze what could be the reason for the execution time to come down from 248 minutes to 155 minutes: Case1: Hive is using 400 reducers Problem: Only 70 reducers can run at a given point of time. Assuming no JVM reuse. Creation of the JVM's again and again would add a large overhead. Not sure on this: Expecting 400 reducers would cause a problem like fragmentation. As in, suppose I know that only 70 reducers can run then my intermediate file storing strategy would be depend on that. But, with 400 reducers the whole strategy goes for a toss. Case2: Hive is using 70 reducers - Both the problems get addressed by setting this number. I guess its better to set the number of maximum available reducers. But, I am no expert at this. Would let the experts comment on this.
