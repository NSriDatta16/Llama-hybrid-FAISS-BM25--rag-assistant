[site]: datascience
[post_id]: 114886
[parent_id]: 
[tags]: 
How to train deep learning model on high dimensional dataset with limited memory and disk

For large datasets in terms of rows, usually it is handled by splitting data into pieces and feeding them into the model one at a time using tf.datasets or custom generator. However what if number of columns increases to 1000 ~ 2000? Should I just cut row into smaller pieces or is there a more efficient way? [EDIT] assuming dimensionality reduction and other data preprocessing steps has already been done
