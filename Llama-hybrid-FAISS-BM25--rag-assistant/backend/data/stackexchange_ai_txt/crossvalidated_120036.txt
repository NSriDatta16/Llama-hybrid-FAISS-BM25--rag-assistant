[site]: crossvalidated
[post_id]: 120036
[parent_id]: 118704
[tags]: 
Given that you only have three human experts, I would not worry about the true score and instead I would evaluate your machine score three times: first against H1, second against H2, third against H3. To evaluate, I think the difference squared (i.e. MSE) is fine. So, average the MSE against each of H1, H2 and H3, to get an evaluation of the quality of your machine score. The lower the number the better. If you are not trying to compare/optimize multiple machine algorithms, and instead only have a single system to evaluate, and you simply want to get a feel for "is it good or not", then you could do the same evaluation for the humans, i.e. H1 against H2 and H3, H2 against H1 and H3, and H3 against H1 and H2. If your average MSE is higher than all of H1, H2 and H3, it suggests your algorithm is not so good. All this suffers a bit from only having three human experts, who (judging by the three rows you showed) do not agree among themselves very well. Every human expert you can add will increase the accuracy of your conclusions.
