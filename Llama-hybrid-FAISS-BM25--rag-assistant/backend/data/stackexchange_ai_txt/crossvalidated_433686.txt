[site]: crossvalidated
[post_id]: 433686
[parent_id]: 
[tags]: 
Hidden Markov Model Training

I am reading more about sequence prediction tasks NLP specifically and am trying to fully understand HMMs and Viterbi. It seems that the latent structure for HMMs is just two matrices one for state transitions and the other for emissions which are based solely on corpus counts. Is this how HMM is trained just counts?
