[site]: datascience
[post_id]: 93262
[parent_id]: 93256
[tags]: 
One approach is to do dimensionality sampling, that is drop some features and see the resulting dataset that arises. If there is some objective importance metric (eg Correlation, PCA) that quantifies the 2 features as more important, you can try that directly. Else one can try iteratively to drop features and test the resulting dataset. This approach does not mean that information is lost, it may even fit better if some features are simply noise. Another related approach is to merge features in a way that maintains some hierarchy of importance. For example create a new combined feature from 2 features $x_1$ , $x_2$ as $x_{12} = x_1^n + x_2$ , which maintains that feature $x_1$ is more important than $x_2$ in the combined feature
