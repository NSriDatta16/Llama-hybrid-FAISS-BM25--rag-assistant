[site]: crossvalidated
[post_id]: 381558
[parent_id]: 
[tags]: 
Ockham's Razor in Bayesian Modelling

This question might be a little philosophical / generate discussion. I hope, there may still be some useful answers. I am currently thinking about how Ockham's Razor relates to Bayesian statistical testing. I found, that often Ockham's razor is implemented by priors on models of different complexity ( this paper attributes the idea to Jeffreys). However, in Bayesian statistics a common method for model selection is to have a discrete random variable "switch" between each of the models, and then use the resulting posterior of that discrete variable to get the probabilities. This is equivalent to comparing the (often intractable) integrals over the parameter spaces of the models (weighted by the priors on the parameters). So when doing Bayesian model selection, models are automatically penalized for their complexity. This is simply the case, because for more complex models we are integrating over a much larger and more complex parameter space, and most elements of this parameter space don't fit the observations. This is the same Ideas, that can be found in the above paper . In this case, Ockham's Razor is NOT implemented by the priors, but rather results automatically and manifests itself in the posterior. But is this enough to account for different model complexity? For example, if I have two sets of Bernoulli trials with repeated samples, e.g. $Y_{1,i}=(1,1,0,1,1,...)$ and $Y_{2,i}=(0,0,1,0,0,...)$ it would be possible to test if $Y_1$ and $Y_2$ are generated by the same probability (H1) or by different probability (H2). Given enough samples from $Y_1$ and $Y_2$ it should be possible to construct a situation where $P(H1|D)=P(H2|D)$ (assuming equal prior probabilities of H1 and H2). However, given that H1 has only one parameter, whereas H2 has two, should this really be taken as evidence, that both are equally likely?
