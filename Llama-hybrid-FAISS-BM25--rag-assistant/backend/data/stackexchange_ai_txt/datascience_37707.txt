[site]: datascience
[post_id]: 37707
[parent_id]: 
[tags]: 
Mapping “event” series, with segments of variable length, to time series for loss calculation

I have a time series of data which I map to a time series of non-exclusive binary features (in my case, short-time audio spectrograms to features of the audio). I will refer to a collection of these classes that remains roughly constant over some time as “event”. For example, my audio stream might contain the word “leave”, so my time series of classes might contain l-sound 00000000000011111111100000000000000000000000000000000000000000000 vowel 00000000000000000000011111111111111111110000000000000000000000000 fricative 00000000000000000000000000000000000000011111111111000000000000000 high 00000000000000000001111111111111111111000000000000000000000000000 noise 00011111010000000000000000000000000000000000000111111100000000000 which correspond to – taking into account that transitions beween events may be noisy – the events sequence l-sound 00010000 vowel 00001000 fricative 00000100 high 00001000 noise 01000010 All of these features are binary and not mutually exclusive, and every (realized) combination of features corresponds to one label, so if # stood for exactly the feature set {noise} , i for {high, vowel} and ʎ for {l-sound, high} etc., the columns of this event sequence might be given as the string _#_liv#_ . I have vastly more target data in this compressed, event-centered string format, a lot more than I have time-series of binary feature vectors. Because the mapping from event sequence to time series is just a somewhat noisy stretching of the data, probably even with characteristic lengths for each event, my intuition suggests that there should be a better, more deterministic way to use this data than making a seq2seq learn its internal structure. I have tried to build a separate calculation graph that maps a set of lengths to differences of sigmoid functions, in order to obtain differentiable functions that approximate interval characteristic functions ( 0011110000 etc.), hoping that an optimizer might be able to shift the boundaries towards the actual ranges. It does not look promising. Given that my model currently produces time-series data, but I have a lot of event-sequence data, how can I best use my event-sequence data to train the model?
