[site]: crossvalidated
[post_id]: 323054
[parent_id]: 
[tags]: 
Accelerate the optimizing process of Deep Learning Model

Bayesian Optimization is currently the best optimization method for deep learning model. Although it already offer a good optimizing time but I want to accelerate by Instead of training every configurations for 100 epochs, I could use... say, 10 or 20 epochs. Instead of training on full training data, I could train using 30-40% of the training data. Assuming that a good model will converge pretty fast after the first few epochs or little data. It could give us a insight of how well a model performs. After the optimizing process , the best model will be picked and train again on full epochs or full training data to increase the performance or generalization. My question: will these hypothesis work?
