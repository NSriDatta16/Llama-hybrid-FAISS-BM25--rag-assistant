[site]: crossvalidated
[post_id]: 318960
[parent_id]: 318824
[tags]: 
After reading this thread referred to by @AlexR., I seem to figure it out myself. First of all, using PCA as a feature learning algorithm may not be very useful. It's systematic, and thus dull in some sense. In fact, whitening transformation doesn't have to be PCA at all (which is PCA whitening), and simply identifying a set of orthogonal coordinates isn't really feature learning. It's better to just treat it as a pre-processing algorithm here. This invalidates my first question. Now to answer the second question. For images, it's not difficult to realize that adjacent pixels are correlated. They tend to be similar. Besides, distant pixels tend to be different. I guess this is what the following has said in plain language (quoted from this thread). Natural images have a lot of variance/energy in low spatial frequency components and little variance/energy in high spatial frequency components. In terms of autoencoder, this means inputs that correspond to adjacent pixels are positively correlated. The effect is that autoencoder tends to learn the big picture while omit the details (see this ). An explanation from spatial frequency perspective is: When using squared Euclidean distance to evaluate the reconstruction of an autoencoder, this means that the network will focus on getting the low spatial frequencies right, since the error scales with the variance of the signal. Another intuitive explanation I came up with myself is like this: It's easier to attain a lower cost function value if you focus more on the big picture, and ignore the details. This is because adjacent pixels tend to be similar, you get one right, then you get all right mostly. That is, when the big picture error is low, the details error tends to be low as well. The converse doesn't hold. By applying the whitening transformation, this correlation and variance difference are eliminated, and the learning process pays equal attention to both the big picture and details. And according to this article, this usually results in a more effective training/optimization process, and a lower error in terms of both the big picture and details. To conclude Whitening transformation is not PCA. In this context, it's simply a pre-processing algorithm that regulates the data, and nothing more.
