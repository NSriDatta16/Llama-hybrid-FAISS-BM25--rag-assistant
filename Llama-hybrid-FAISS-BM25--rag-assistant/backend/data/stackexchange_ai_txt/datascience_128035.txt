[site]: datascience
[post_id]: 128035
[parent_id]: 
[tags]: 
Use of Gradient with respect to feature instead of model parameters

Generally, for any machine learning/deep learning system, we compute a loss, $L = l(x, \theta, y)$ which is a function of the input feature vector $x$ (after activation), model parameters $\theta$ (which actually is encoded in $x$ since $x$ is the final feature vector), and $y$ , the label. Now, to learn, we compute $\frac{\partial L}{\partial \theta}$ and update the parameters, $\theta$ using this. However, pytorch (and other libraries) also provide a way to hook the gradients of the feature vectors as well, ( torch.tensor.register_hook() ). Let us say I hook this to $x$ . What exactly is this gradient? I believe that this is $\frac{\partial L}{\partial x}$ . And if so, what is the use of this gradient? What information can this gradient give us? Have there been any works on exploring this gradient and its uses? Any references would be appreciated. Thanks!
