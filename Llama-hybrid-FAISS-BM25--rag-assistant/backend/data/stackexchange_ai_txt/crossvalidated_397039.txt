[site]: crossvalidated
[post_id]: 397039
[parent_id]: 
[tags]: 
How many data points to see if one model is significantly better than other models in a finite set?

I am working on a linear regression problem, let's say I'm looking for a model that predicts housing prices as a function of number of rooms and square feet. Assume for each attempt to price a house, we randomly pick a model from a fixed set of three models and use it to estimate the price. If it is within 10% of the actual price without going over, we count that as a success. The question is, how many successes do I need before I am statistically certain (or measurably confident) that a model is actually the best one, given that there's a 1/3 chance any given model will be selected for the job. For example we may guess on 300 houses (each model gets 100 tries on average), and each model has 5, 3, and 2 successes respectively. Is this enough to know that the one with 5 is best, and not just best because it happened to be selected on houses that worked with this particular model. For example, given three linear regression models, specified by: [{"square_feet":1.5,"bedrooms":100, {"square_feet":1.8,"bedrooms":110, {"square_feet":1.2,"bedrooms":120] if we tried to predict the price of a house with 1000 square feet and 3 bedrooms, and we randomly select the first model - it would predict (1.5*1000) + (3*100) = 1800.
