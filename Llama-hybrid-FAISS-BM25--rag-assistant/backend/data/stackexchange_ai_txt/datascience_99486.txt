[site]: datascience
[post_id]: 99486
[parent_id]: 
[tags]: 
What is few-shots extrapolation?

I'm reading the paper " Learning how to ask " by Qin & Eisner and in the abstract, they mention that using prompts, language models can perform tasks other than text generation. Examples include fill-in-the-blanks (BERT) and few-shots extrapolation (GPT-3). I am not sure I understood correctly what the authors mean by few-shots extrapolation. Do they mean extracting factual and other types of knowledge by using a few examples? Thanks.
