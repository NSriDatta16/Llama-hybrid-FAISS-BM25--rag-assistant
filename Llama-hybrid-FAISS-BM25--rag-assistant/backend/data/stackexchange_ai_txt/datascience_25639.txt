[site]: datascience
[post_id]: 25639
[parent_id]: 
[tags]: 
what does smooth/soft probablity mean?

I was recently reading the Knowledge Distillation paper, and encountered the term smooth probabilities . The term was used to denote when the logits were divided a temperature. Neural networks typically produce class probabilities by using a softmax output layer that converts the logit, zi , computed for each class into a probability, qi , by comparing zi with the other logits where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes. What does that mean intuitively?
