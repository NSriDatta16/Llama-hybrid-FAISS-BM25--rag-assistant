[site]: crossvalidated
[post_id]: 112339
[parent_id]: 
[tags]: 
Does feature standardization always make sense?

I wonder if feature scaling like this makes always sense for neural networks: Let $T$ be the training set and $x_i \in \mathbb{R}^n$ with $d_i \in T$ be the feature vector of $d_i$. Then add another preprocessing step so that $x_i' \gets \frac{x_i - \text{mean}(T)}{\max(T) - \min(T)}$ where $\max$ and $\min$ get applied seperately for each dimension. This preprocessing step guarantees that for each feature you will get a mean of $0$ and a range of 1. I've heard that this is desired for neural nets. Do you know any sources for that? (Or sources that claim that feature normalization is not always good?) Note : The range is1, not necessary the variance. The variance of a random variable $X$ is calculated like this: $$Var(X) = E(X^2) - (\underbrace{E(X)}_{=0})^2 = E(X^2)$$. If you have, for example, $X$ with $P(-0.5) = 0.5 = P(+0.5)$ you have a variance of $Var(X) = E(X^2) - E(X)^2 = (0.5 \cdot 0.25 + 0.5 \cdot 0.25) - 0 = 0.25$. As $\max(X) - \min(X) = 0.5 - (-0.5) = 1$ and $\text{mean}(X) = 0$, feature standardization will not change anything
