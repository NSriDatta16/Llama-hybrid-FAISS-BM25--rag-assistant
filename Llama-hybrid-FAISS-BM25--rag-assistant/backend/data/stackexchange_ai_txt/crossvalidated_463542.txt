[site]: crossvalidated
[post_id]: 463542
[parent_id]: 
[tags]: 
Sklearn PCA Calculation Seems to Use Truncated Division as Opposed to Floating Point Division

I am working with the following dataset: Housing dataset From this dataset, I am only interested in the following columns: GrLivArea (independent variable), and SalePrice (dependent variable). What I want to do is to essentially manually calculate the slope of the PCA line in two dimensions. I know from some reading around that I can use the following formula for the slope. I also know that I can use the PCA function from Sklearn to quickly get the slope as well. Here is my code: import numpy as np import pandas as pd # function from exercise solutions def orthogonal_regression (U,V): """ The input parameters are the two uncentered arrays U and V respectively containg the x and y coordinates of the data points Start by centering the arrays """ U = U-np.mean(U) V = V-np.mean(V) U2 = np.multiply(U,U) V2 = np.multiply(V,V) UV = np.multiply(U,V) U2sum = np.sum (U2) V2sum = np.sum (V2) UVsum = np.sum (UV) Term1 = V2sum-U2sum Term2 = Term1 * Term1 Term3 = 4. * UVsum * UVsum Slope = (Term1+np.sqrt(Term2+Term3))/(2.*UVsum) return Slope # load data houses = pd.read_csv(path + 'Houseprices.csv') # method 1 X = houses['GrLivArea'] y = houses['SalePrice'] # method 2 xy = houses[['GrLivArea', 'SalePrice']].values print('check the difference between vectors:', 'GrLivArea:', np.sum(xy[:, 0] - X), 'SalePrice:', np.sum(xy[:, 1] - y), '\n') print('compare first 3 values of GrLivArea before scaling:', xy[:, 0][:3], X.values[:3], '\n') print('compare the types before scaling:', xy[:, 0].dtype, X.dtype, xy[:, 1].dtype, y.dtype, '\n') xy[:, 0] = xy[:, 0] / 1000. X = X / 1000. xy[:, 1] = xy[:, 1]/100000. y = y / 100000. print('compare first 3 values of GrLivArea after scaling:', xy[:, 0][:3], X.values[:3], ' Something interesting happens when I run this code. Here is the ouput: Depending on how I set up my X and y (method 1 or method 2), I get a different result due to the scaling even though I scale them exactly the same way. At this point I would expect method 1 to be more accurate of the two and match that in Sklearn as it does not truncate. However, if I run the following code to generate the Sklearn output: from sklearn.decomposition import PCA pca = PCA (n_components=1) pca.fit (xy) slope_pca = pca.components_[0,1]/pca.components_[0,0] print (slope_pca) The output is: 1.7280... Now my question is, why is this the case? Why does Sklearn not match the non-truncated scaled numbers? It seems Sklearn is doing truncated division somewhere which should not be correct.
