[site]: crossvalidated
[post_id]: 641912
[parent_id]: 
[tags]: 
Support Vector Classifiers for Overlapping Classes

I am currently studying support vector classifiers (SVC), more specifically, the solution to the Lagrangian (Wolfe) dual function with the help of the book " The Elements of Statistical Learning " by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. It is said that from the Karush-Kuhn-Tucker (KKT) conditions, we can infer that for a support vector $x_i$ that lies on the edge of the margin, the corresponding Lagrange multiplier $\alpha_i$ lies in $0 with $C$ being the cost parameter in the Lagrange primal function. Given the KKT conditions how can we infer that $\alpha_i \neq C$ for a support vector $x_i$ on the margin edge? Thank you in advance.
