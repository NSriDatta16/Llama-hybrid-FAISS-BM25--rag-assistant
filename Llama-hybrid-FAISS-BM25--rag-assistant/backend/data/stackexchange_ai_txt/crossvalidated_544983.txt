[site]: crossvalidated
[post_id]: 544983
[parent_id]: 544980
[tags]: 
In any machine learning model, you're try to learn a set of rules which can be applied in the future to make predictions. Overfitting happens when your rules are too specific to the data which you trained on: When feature $X_1$ is equal to 100.456 then my target will be equal to 47.85. A better model will have more general rules which work out of sample: When feature $X_1$ is large, my target tends to be very large too. When feature $X_2$ is less than 50, my target tends to be between 10 and 100. Some regression algorithms are more prone to learning rules which are overfit to outliers. Ensemble algorithms are typically more robust in this respect. Having a simpler target (3 classes) might make it easier for your setup to learn more general rules but I don't think it's possible to make any exact statement about classification problems being less prone to overfitting. You could still learn a rule like: When feature $X_1$ is equal to 100.456 then my target will be class 'Good'. The rule learned here is perhaps more likely to hold true out of sample since the output space is coarser. But I would still call it 'overfit'.
