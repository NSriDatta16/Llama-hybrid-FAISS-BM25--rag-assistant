[site]: crossvalidated
[post_id]: 292568
[parent_id]: 241854
[tags]: 
It is possible to understand it from a Bayesian point of view. Ridge regularization for linear regression is a Bayesian method in disguise. See : https://en.wikipedia.org/wiki/Lasso_(statistics)#Bayesian_interpretation (it is easier to understand explained on the wikipedia"s Lasso page, but it's the same idea with Ridge). The convention I use for regularization is the following. Minimize: $\left(\displaystyle\sum_{i=1}^N(y_i-\beta x_i)^2\right)+\lambda\|\beta-\beta_0\|^2$. Assume that the noise has variance $\sigma^2=1$ for simplicity (otherwise replace $\lambda$ by $\lambda/\sigma^2$ everywhere). Regularization with coefficient $\lambda$ means assuming a normal prior $N(0;\frac{1}{\lambda}I)$: "I expect as a prior belief that the coefficients are small": The prior distribution is a normal distribution with mean $0$ and "radius" $\sqrt\frac{1}{\lambda}$. Regularizing towards $\beta_0$ means assuming a normal prior $N(\beta_0;\frac{1}{\lambda}I)$: "I expect as a prior belief that the coefficients are not far from $\beta_0$": the prior distribution is a normal distribution with mean $\beta_0$ and "radius" $\sqrt\frac{1}{\lambda}$. This prior often results from a previous training that gave $\beta_0$ as an estimate. The strength of your belief $\lambda$ is the statistical power of your first training set. A big lambda means that you had previously a lot of information, your belief is only slightly changed for each new sample: a small update by sample.
