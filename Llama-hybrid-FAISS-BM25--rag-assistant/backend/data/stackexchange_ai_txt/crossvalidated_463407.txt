[site]: crossvalidated
[post_id]: 463407
[parent_id]: 
[tags]: 
Why is homoscedasticity (homogeneity of variance) important in neural network layers?

I'm studying the famous Xavier initialization paper ( Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot and Bengio, 2010) ) and had a question. When they explain the forward pass of neural networks, they sample the weight initialization values from a Uniform distribution of equal variance. The intuition is apparently because homogeneity of variance allows training to flow smoother. However, I'm curious as to why this is the case? That is, why does HoV allow "smoother" training?
