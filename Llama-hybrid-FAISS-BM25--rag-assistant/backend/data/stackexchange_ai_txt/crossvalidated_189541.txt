[site]: crossvalidated
[post_id]: 189541
[parent_id]: 
[tags]: 
Size of hidden layer in neural networks for learning specific logical rules

According to this answer , a general rule of thumb is that your hidden layer size should be between your input and output sizes. In developing my JavaScript neural network, this has proven to be about right. However, for boolean logic it seems to be wrong. For example, an xor task seems to need more than 2 hidden neurons in order to complete (I have tested this on my own net and it seems about right, as well as this other JS net here needing 3 neurons). Additionally, a subtraction task seems to need a great many as well, at least on my network. Is there a separate rule of thumb for these types of logic tasks?
