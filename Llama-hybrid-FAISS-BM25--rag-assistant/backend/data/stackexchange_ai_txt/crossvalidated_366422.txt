[site]: crossvalidated
[post_id]: 366422
[parent_id]: 
[tags]: 
How to determine statistical significance of mean being larger than zero?

I have a set of numbers generated by an unknown to me random distribution. When I calculate the average of these numbers, I get a number that is larger than zero. I want to know how likely it is that the mean of the distribution is really larger than zero. To answer this question I have designed the following non-parametric test: I calculate the mean value of the numbers that I have. From each number that I have in my set I subtract the mean. In that way I get a new set of numbers (whose mean is equal to zero per definition). I randomly choose a number from the new set as many times as there number in the original set. Each choice is independent. It means that the same number can be chosen several times or not chosen at all. I calculate the average of the set of randomly generated numbers. I do it many times and check how often the average is as large as the real one. Here is an example, to make it more clear. Let us assume that my original set of numbers is [1.1, 3.7, 2.2, 7.8, 9.4] The mean of these numbers is 4.84 . I subtract this mean from all my numbers and as a result I get: [-3.74, -1.14, -2.64, 2.96, 4.56] I sample 5 numbers from this new set. As a result I could get: [4.56, 4.56, -2.64, -3.74, -1.14, 2.96] I calculate the mean of these numbers (it is 0.76 ). Then I sample from the same normalised set of numbers ( [-3.74, -1.14, -2.64, 2.96, 4.56] ) and calculate its mean. I do it many times and check how often these means are larger than the original mean. So, now the question is if this test is meaningful or it has some logical flows. Of course I my real case I have more values that 5 (around 5000). Also their average is not as larger as 4.84 (it is much closer to zero (around 0.004 )).
