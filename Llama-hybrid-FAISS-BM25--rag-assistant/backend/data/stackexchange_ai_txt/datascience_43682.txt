[site]: datascience
[post_id]: 43682
[parent_id]: 
[tags]: 
Why does Feature Importance change with each iteration of a Decision Tree Classifier?

After applying PCA to reduce the number of features, I am using a DecisionTreeClassifier for a ML problem Additionally I want to compute the feature_importances_. However, with each iteration of the DecisionTreeClassifier, the feature_importances_ change. Iteration #1 Iteration #2 Why would it change? I thought the initial split was made on a feature to "produce the purest subsets (weighted by their size)". Acting on the same training set, why would that change? Thanks in advance for any help.
