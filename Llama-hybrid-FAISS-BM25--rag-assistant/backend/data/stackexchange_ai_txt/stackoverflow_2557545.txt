[site]: stackoverflow
[post_id]: 2557545
[parent_id]: 2557289
[tags]: 
I'm pretty sure you mean cost function rather than error function. SOM does not require an error function (nor a cost function). At the top level of the Machine Learning taxonomy, SOM is an unsupervised learning technique --no target vector, and therefore no "target-vector" minus "value_at_the_current_iteration" to minimize. Another way to think of it: The role of a cost function is to minimize some cost; in ML, it's the delta between model calculation and supplied data. In SOM, no data is supplied to the algorithm for this purpose. (I realize that this is somewhat confusing because the input data from which the network is created is often referred to as "training data"--probably that's the role of input data in supervised ML techniques, which are far more common than unsupervised ones. It's probably also confusing because Teuvo Kohonen, the person credited with 'inventing' SOM, originally referred to them as a class of neural networks--and of course NN is a supervised technique and does rely on a cost function (often gradient descent.)) Finally, just to make sure, i checked my own SOM code as well as the code from the ML textbook by Marsland, "Machine Learning: An Algorithmic Perspective". In both my code and his, the only stopping criterion whatever value for "maximum iterations" the user passed in when he called the main function.
