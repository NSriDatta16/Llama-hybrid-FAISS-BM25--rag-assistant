[site]: crossvalidated
[post_id]: 192614
[parent_id]: 192572
[tags]: 
A nice example would be a betting scenario where a frequentist and Bayesian bet against each other about some future outcome and the frequentist has positive expected value. I will not give you this example because such an example would favor a Bayesian approach unless the Bayesian chooses a bad prior which is a cop-out example not really worth writing about. The frequentest approach is not designed to obtain the highest expected value in betting scenarios (luckily the world of statistics and probability is much more broad than just that). Rather, frequentist techniques are designed to guarantee certain desirable frequency properties, particularly that of coverage. These properties are important for parameter estimation and inference in the context of scientific research and inquiry. I encourage you to check out this link here to a blog post by Dr. Larry Wasserman. In it he talks about frequency guarantees in more depth (see the examples he gives). Suppose we had some data $Y$ and we think it is distributed according to some conditional distribution $Y \sim f(Y|\theta^*)$ (if you like you can imagine that $Y$ is normally distributed and $\theta^*$ is the mean and\or variance). We do not know the value of $\theta^*$ , so we have to estimate it. We can use either a frequentist or Bayesian approach to do so. In the frequentist approach we would obtain a point estimate $\hat \theta$ and a confidence interval for that estimate. Assuming $\theta^*$ exists and the model is valid and well behaved, the frequentist $(1-\alpha)$ confidence interval is guaranteed to contain $\theta^*$ $(1-\alpha)$% of the time regardless of what $\theta^*$ actually is . $\theta^*$ could be 0, it could be 1,000,000, it could be -53.2, it doesn't matter, the above statement holds true. However, the above does not hold true for Bayesian confidence intervals otherwise known as credible intervals. This is because,in a Bayesian setting, we have to specify a prior $\theta \sim \pi(\theta)$ and simulate from the posterior, $\pi(\theta|Y) \propto f(Y|\theta)\pi(\theta)$. We can form $(1-\alpha)$% credible intervals using the resulting sample, but the probability that these intervals will contain $\theta^*$ depends upon how probable $\theta^*$ is under our prior. In a betting scenario, we may believe that certain values are less likely to be $\theta^*$ then others, and we can assign a prior to reflect these beliefs. If our beliefs are accurate the probability of containing $\theta^*$ in the credible interval is higher. This is why smart people using Bayesian techniques in betting scenarios beat frequentist. But consider a different scenario, like a study where you are testing the effect of education on wages, call it $\beta$, in a regression model. A lot of researchers would prefer the confidence interval of $\beta$ to have the frequency property of coverage rather than reflect their own degrees of belief regarding the effect education on wages. From a pragmatic standpoint, it should also be noted that in my earlier example, as the sample size approaches infinity, both the frequentist $\hat \theta$ and Bayesian posterior $\pi(\theta|Y)$ converge onto $\theta^*$. So as you obtain more and more data, the difference between the Bayesian and frequentist approach becomes negligible. Since Bayesian estimation is often (not always) more computationally and mathematically rigorous than frequentist estimation, practitioners often opt for frequentist techniques when they have "large" data sets. This is true even when the primary goal is prediction as opposed to parameter estimation/inference.
