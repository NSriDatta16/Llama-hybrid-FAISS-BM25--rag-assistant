[site]: crossvalidated
[post_id]: 384008
[parent_id]: 383972
[tags]: 
Selecting a too small batch size is a problem for both LSTMs and any other neural network, not because you're "missing out on information" but because the high variance in the gradient might hamper convergence of the optimizer. You don't take advantage of the long term memory any more or less by changing batch size. As a rule of thumb, somewhere between 4 and 1024 is probably the optimal batch size, but you can't really tell without actually trying it out.
