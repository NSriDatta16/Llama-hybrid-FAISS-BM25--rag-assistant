[site]: crossvalidated
[post_id]: 503434
[parent_id]: 503319
[tags]: 
A comment turned into an answer: You seem be confusing two things: (1) The "logit" being nonlinear in $p$ (2) assuming that the logit of p is linear in the covariates. The first point has no bearing on the second point unless somehow you believe that the probabilities themselves should be linearly dependent on the covariates, which is perhaps even more absurd considering that p has to remain in [0,1]. The best way to see why logistic regression makes sense is to try to model the probability $p$ as a function of $x = (x_1\dots,x_{K})$ . You quickly realize that perhaps you need some sort of transformation that restricts the values to $[0,1]$ and some thought might lead to a model like $$ p = \phi(\beta^T x) $$ where $\phi(\cdot)$ is a function from $\mathbb R$ to $[0,1]$ . One example will be $\phi = \text{logit}^{-1}$ which leads to logistic regression. Another example is $\phi = $ CDF of the standard normal distribution which leads to Probit regression, and so on. You can always make the model more complex by say assuming $p = \phi( P_\beta(x))$ where $P_\beta(x)$ is a polynomial in $x$ of degree higher than 1. The logit case also has the following interpretation: Let the binary observation be $Y$ with density (i.e., PMF) $p(y) = p^{y} (1-p)^{1-y}$ for $y \in \{0,1\}$ . This is an exponential family $$ p(y) = \exp( y \theta - \log(1 +e^{\theta})) $$ with canonical/natural parameter $\theta = \log\frac{p}{1-p}$ . The logistic regression assumes this canonical parameter to be linear in the covariates. A similar consideration as point 1 above goes into modeling a parameter which takes values in $[0,\infty)$ such as a rate $\lambda$ . Then, again, a natural first model is $\lambda = \phi(\beta^T x)$ where $\phi(\cdot)$ maps $\mathbb R$ to $[0,\infty)$ and a natural choice for $\phi$ is $\phi(x) = e^x$ .
