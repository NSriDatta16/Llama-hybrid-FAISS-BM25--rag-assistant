[site]: crossvalidated
[post_id]: 483640
[parent_id]: 475696
[tags]: 
A few points to make You mentioned you're using WGAN, I strongly suggest using gradient penalty instead of clipping if you aren't already. The generator loss is not very meaningful in WGAN. Also in general, there is nothing wrong with negative numbers at all. Read the WGAN paper. The theory is dense, but there are important details there. For example, you should train your discriminator more than your generator. Recommended values are 5-10 discriminator iters per generator iter. The discriminator loss is (an approximation of) the negative Wasserstein distance between the generator distribution and the data distribution. So it's actually very interpretable and useful for diagnostics.
