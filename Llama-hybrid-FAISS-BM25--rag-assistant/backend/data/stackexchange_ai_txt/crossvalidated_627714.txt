[site]: crossvalidated
[post_id]: 627714
[parent_id]: 
[tags]: 
Embeddings/tokenizers for a transformer with binary-valued data

I'm trying to train an encoder-decoder transformer model for completion of binary-valued data. The each input is basically a length-n bitstring $x = (x_1, \dots, x_n) \in \{0,1\}^n$ , generated according to some probability distribution, which has been masked to hide a subset of bits during training. I am confused about what kind of embedding to use for this kind of input/output data. In the case of translation, one would tokenize the input text into $N$ tokens then pre-train an embedding from $\{1, \dots, N\} \rightarrow \mathbb{R}^d$ , which seems reasonable when $N \gg d$ . But for binary data using a single bit for a token means $N=2$ , and so its not clear what the advantage is of embedding such a small alphabet of tokens into some high dimensional space. What are some justifiable choices for embedding or tokenizing binary-valued data for use in a transformer?
