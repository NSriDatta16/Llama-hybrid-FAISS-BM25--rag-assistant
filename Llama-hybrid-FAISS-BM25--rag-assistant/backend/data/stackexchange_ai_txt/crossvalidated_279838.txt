[site]: crossvalidated
[post_id]: 279838
[parent_id]: 
[tags]: 
how can I integrate probabilities from two GMMs?

Without going into the details of why exactly I must do this, I have four GMMs (two sets for two classes), and I need to integrate their predictions. Two of them are trained on two classes from dataset A, and the other two on dataset B. To be clear, the datasets (A and B) are different, but the class labels are the same (let's say "cat" and "dog" images for simplicity). You could think of dataset A and B as just being two sets of dog and cat images, for which I learn GM densities. Using just the first two for example, the models (in scikit-learn) can return either probability or LL of a new test point. I can then compare these values for the two cat and dog densities and choose the larger of the two to classify. I can do this with the second set of GMMs too. What I want to do is sort of integrate the two sets of models. One way I assume this can be done is to simply average the ultimate categorical predictions they make, but I'd like to do this a bit less hacky. It's not clear to me that one of these is the regularizing "prior" in respect to the other, since both may be of comparable confidence. Does anyone know how I can think about this more formally? Should I multiply the two LLs for the single new test datapoint? Multiply the probabilities instead? It also seems like finding some sort of optimal weighting of the two would be in order.
