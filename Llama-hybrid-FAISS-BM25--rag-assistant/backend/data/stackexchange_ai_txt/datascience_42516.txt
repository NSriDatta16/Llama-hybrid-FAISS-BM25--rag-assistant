[site]: datascience
[post_id]: 42516
[parent_id]: 38191
[tags]: 
In most of the cases in the industry, we should go for the stable performance rather than the strong accuracy. But still, it depends. Here's an example from my current business. I currently deploy deep learning algorithms for algorithmic trading as my job. My algorithms do not predict the market, yet they do predict the trading strategy for every 15 minutes for FOREX EUR/USD, XAU/USD, etc. Not going deep with the specifics, let's say my algorithm predicts the true trading strategy with an average 65% (+-3%) stable accuracy. It can provide long-term stable profits for my portfolio. However, if I have average 70%(+-15%) accuracy, my algorithm could have 3-weeks of aggressive profits, whereas it could fail the same portfolio over one week. Actually, this described scenario is a real life example from what I have experienced from most of my models. However, if my task was to detect wanted criminal suspects from certain features in the public from CCTV cameras by computer vision, it could be acceptible that having an unstable performance. The reason why is that it's fine that the algorithm can sometimes made mistakes upon regular people when identifying them, yet it could find specific targets better due to its overall performance. By analogy, a strong performing unstable model has some kind of magic but the wand may not be working sometimes; whereas a stabilized model is a regular German machinery, working most of the times with no magical powers.
