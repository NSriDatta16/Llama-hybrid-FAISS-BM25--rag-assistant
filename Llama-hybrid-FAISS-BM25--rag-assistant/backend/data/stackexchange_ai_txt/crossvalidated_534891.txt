[site]: crossvalidated
[post_id]: 534891
[parent_id]: 534889
[tags]: 
When you train a neural network using stochastic gradient descent or a similar method, the training method involves taking small steps in the direction of a better fit. Each step is based on one minibatch of data, and an epoch means you have made one step based on every data point. But that's only one small step! Typically, you need to take many more steps than just one based on each data point in order to get a good fit. It's hard to predict in advance how many steps and how many epochs will be needed -- partly, it's hard to tell how close to the minimum you are, and partly, you may well not want to go all the way to the minimum because of overfitting. It is common to monitor the goodness of fit on test data as you train, so that you can stop when the test-data accuracy stops decreasing or starts increasing.
