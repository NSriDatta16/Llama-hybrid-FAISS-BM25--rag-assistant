[site]: crossvalidated
[post_id]: 349692
[parent_id]: 
[tags]: 
Comparison of classification algorithms: How can I interpret the results?

I am experimenting on a dataset of about 18,000 articles, 12000 tagged Fake and 6000 tagged Real. I'm building a fake news classifier and I'm comparing 4 classification algorithms: Multinomial Naive Bayes, Linear SVM, Logistic regression and Random Forest. I'm using two different approaches: 1) content-based: Bag of words with TFIDF 2) context-based: about 80 numerical features for each article (Sentiment analysis, part of speech, average length of sentences,% of stopwords, etc.) The results I obtained are the following: 1) In the first case MNB, SVM and LogReg have similar performances (accuracy 91-93%), while Random Forest obtains results lower of 5-6%. 2) In the second case, RF obtains the best results (accuracy: 87%), followed by LogReg (accuracy: 80%), SVM (accuracy: 74%) and MNB (accuracy: 60%). From a theoretical point of view, how can these results be explained?
