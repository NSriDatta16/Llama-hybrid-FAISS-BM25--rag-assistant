[site]: crossvalidated
[post_id]: 549510
[parent_id]: 549508
[tags]: 
Its just algebra. In the first edition of the book, equation 4.12 states that the posterior probability for each class can be written using Bayes rule as $$ p_k = \dfrac{\pi_k f(x, \mu_k)}{\sum_k \pi_k f(x, \mu_k)} $$ Here, $\pi_k$ is the prior probability of class $k$ and $f(x, \mu_k)$ is a gaussian density with mean $\mu_k$ . The density of $x$ for each class is assumed to have the same variance. Now comes the algebra. Note $$ \log \left( \dfrac{p_1}{p_2} \right) = \log\left( \dfrac{\pi_1 f(x, \mu_1)}{\pi_2 f(x, \mu_2)} \right) $$ Leveraging some properties of logs $$ = \underbrace{\log(\pi_1/\pi_2)}_{c_0} + \underbrace{\log(f(x, \mu_1)/f(x, \mu_2))}_{c_1(x)}$$ EDIT: Gordon Smyth correctly notes that a much stronger statement can be made about $c(x)$ , namely that it is linear in $x$ . This is also rather straight forward to show. Since LDA makes the assumption of homogeneity in the variance term between classes, $\log(f(x, \mu_1) / f(x, \mu_2))$ simplifies to $$ \log(f(x, \mu_1) / f(x, \mu_2)) = -\dfrac{1}{2\sigma^2} \Big( (x-\mu_1)^2 - (x- \mu_2)^2\Big)$$ More algebra shows the $x^2$ term cancels out, yielding $$ = - \dfrac{1}{2\sigma^2} ( -2 \mu_1 x + \mu_1^2 + 2\mu_2x - \mu_2^2) = \dfrac{1}{\sigma^2} \Big( (\mu_1 - \mu_2)\cdot x + 2(\mu_2^2 - \mu_1^2) \Big)$$ which is clearly a linear function in $x$ . You could do some re-arranging so that $c_0$ has all the constant terms and $c(x)$ has only terms involving $x$ , but it largely doesn't matter.
