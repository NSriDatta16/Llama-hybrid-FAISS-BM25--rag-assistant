[site]: crossvalidated
[post_id]: 8452
[parent_id]: 8435
[tags]: 
This depends on what you mean by "multivariate". Doing a quick google search, the term seems to refer to "more than 1 co variate". So the model equation is for "ordinary logistic regression" you have: $$log(\frac{\pi}{1-\pi})=\beta_0 + \beta_1 X_1$$ For "multivariate logistic regression" you have: $$logit(\frac{\pi}{1-\pi})=\beta_0 + \beta_1 X_1 + \beta_2 X_2 +\dots+ \beta_p X_p=X\beta$$ Where $X\beta$ is the "matrix version" - just for more compact notation. But as @onestop points out, the term "multivariate" could also refer to more than 1 probability. The model equations are then: $$logit(\frac{\pi_1}{\pi_m})=X^{(1)}\beta^{(1)}$$ $$logit(\frac{\pi_2}{\pi_m})=X^{(2)}\beta^{(2)}$$ $$\dots$$ $$logit(\frac{\pi_{m-1}}{\pi_m})=X^{(m-1)}\beta^{(m-1)}$$ Where the brackets $(1),(2),..,(m-1)$ just indicate that the co variates and betas could be different in each model. The "multinomial" simplifies back to the "multivariate" when $m=2$ (only 2 categories), and the "multivariate" simplifies back to the "ordinary logistic regression" when you have only 1 co variate. Now these are all "independence" based models, in that given the covariates and betas, any one individual's probability tells you nothing about any others. When you move from this framework, which is called "generalised linear modelling" or GLM in the literature, to the "extra M" of GLMM, the observations are now assumed to somehow depend on each other. The easiest way I can see a reason for this happening is that you know a particular attribute will increase or decrease the probability of a certain group of observations, but you have not measured that covariate in your data (e.g. you are analyzing the incidence of cancer among people, but you do not know which people in your data set are smokers. However have variables which could predict smoking status.). The usual GLM model goes: $$DATA=MODEL+NOISE^{(1)}$$ (NOTE: although the likelihood isn't linear for GLMs, the usual fisher-scoring IRLS or newton-rhapson algorithm means this is the effective model one is actually estimating - because it is based on a 1 term taylor series expansion "the delta method"). Whereas the GLMM goes: $$DATA=MODEL+MODEL(UNOBSERVED)+NOISE^{(2)}$$ The unobserved model is sometimes written as $Z\gamma$. So your model equation becomes: $$logit(\frac{\pi}{1-\pi})=X\beta+Z\gamma$$ The vector $\gamma$ is often called a "random effect" (and is usually assumed to have a normal distribution) and the vector $\beta$ is called a "fixed effect" (although I don't particularly like this terminology). And the idea is to "decompose" the noise into the "correlated part" and the "independent part". You fit the "fixed effect" part of the model - this is like the "global model". The GLMM basically allows you to recognise that there can be "local effects" in your data - systematic deviations from the "global model" for small groups, in a very similar way to ANOVA and ANCOVA. In fact, all of the GLMMs I have fit, my $Z$ matrix has been a classical design matrix for ANOVA. So basically what the GLMM does is make a compromise between the global model and the local model. The compromise depends on the estimated variance for the "random effect" and on the estimate variance of the noise.
