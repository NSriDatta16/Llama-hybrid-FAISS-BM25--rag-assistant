[site]: crossvalidated
[post_id]: 616152
[parent_id]: 
[tags]: 
Why are Bayesian mixed-effects models (e.g., brms) more able to estimate complex models than Frequentist mixed models (e.g., lme4)?

It is commonly suggested that if you are having trouble getting your lme4, Frequentist mixed-effects model to converge, you can either (a) simplify and drop random effects in the model, or (b) pivot to Bayesian mixed-effects models using brms ( https://m-clark.github.io/posts/2020-03-16-convergence/ ). I often accept this as true that Bayesian mixed-effects models can estimate complex models with maximal random effects (Barr et al., 2013) that Frequentist models cannot. However, I am unclear on the reasons for why this is, specifically why Bayesian models can estimate more complex models than Frequentist? Is it primarily due to the prior regularizing the random effects and "biasing" them away from the boundaries, so that you don't get weird aberrancies like correlations of 1/-1 like you sometimes see in Frequentist lme4?
