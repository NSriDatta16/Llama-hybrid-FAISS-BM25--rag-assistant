[site]: crossvalidated
[post_id]: 437164
[parent_id]: 299328
[tags]: 
The standard method of training a neural network is using some or another variation of a first-order optimization method. The simplest and most straightforward method of is gradient descent update the model parameters (weights, biases) makes linear steps in the direction of steepest descent, so the update rule has the form $$ x^{(t+1)} = x^{(t)} - \eta \nabla f\left(x^{(t)}\right) $$ where $\eta > 0$ is the learning rate and $f$ is your loss function and $x^{(\text{t})}$ is the value of the parameters at iteration $t$ . There are more ornate variations on this idea, such as momentum and Adam, but they all have some direct connection to this basic update equation. Picking a good learning rate is important because if you set it too large, your model will get worse and if you set it too small, progress will be painfully slow. So there is usually some experimentation inovled in picking a good learning rate. How can change in cost function be positive? What should I do when my neural network doesn't learn?
