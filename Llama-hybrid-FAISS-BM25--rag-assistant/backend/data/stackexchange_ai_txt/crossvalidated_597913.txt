[site]: crossvalidated
[post_id]: 597913
[parent_id]: 
[tags]: 
Do Neural Networks tend to have Zero Mean Errors in each Output?

My NN (a few linear layers with ReLUs + batch normalization, no activation in the last layer) learns to approximate vector-valued labels $y_z$ from data $z\sim\rho_z$ in a supervised way, i.e. net $(z)=y'_z\approx y_z$ . I was wondering if there exists any literature that investigates the distribution of the error in each component of the output, i.e. the distribution of $(y'_z)_i-(y_z)_i$ . Ideally, I want this to have zero mean, i.e. $$ \mathbb{E}_{z\sim\rho_z}[(y'_z)_i-(y_z)_i]=0. $$ Do any such results exist, or probably also results that suggest this is not typically the case?
