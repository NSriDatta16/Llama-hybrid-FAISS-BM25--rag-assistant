[site]: crossvalidated
[post_id]: 562642
[parent_id]: 
[tags]: 
Training Set: Why Loss Flatten but Accuracy continues to increase?

I took a the Coursera course: Convolutional Neural Networks in TensorFlow, and one of the quiz questions is When exploring the graphs, the loss levelled out at about .75 after 2 epochs, but the accuracy climbed close to 1.0 after 15 epochs. What's the significance of this? How is it possible that the loss can flatten while accuracy continue to increase? I understand that it is possible for the loss to fall but accuracy remains the same. This is possible when the predictions are probabilities, and the loss falls when the probabilities come closer to the threshold probability (usually 0.5). But the predicted values are still wrong hence accuracy doesn't up. So do I stop training after 2 epochs? Is there over-fitting after 2 epochs? Why is that so?
