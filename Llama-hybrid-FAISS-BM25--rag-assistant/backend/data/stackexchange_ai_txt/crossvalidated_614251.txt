[site]: crossvalidated
[post_id]: 614251
[parent_id]: 614244
[tags]: 
Yes, it makes sense to characterise GBMs as "universal function approximators" and in particular "greedy" ones, as put forward in Friedman's (2001) (uber-classic) Greedy function approximation: A gradient boosting machine . The greediness here stemming on how we gradually/stage-wise increase the ensemble's capacity by adding units from the same family of known universal approximators (here trees). For that matter, let's remember that Boolean functions (i.e. trees) can be represented as real polynomials; a succinct (and surprisingly readable) intro on that can be found in Nisan & Szegedy (1992) On the degree of boolean functions as real polynomials . I have found Section 12.5 Universal approximation from the online blog version of the book Machine Learning Refined by Watt et al. a nice overview of how universal approximations come into play in ML, that section includes a small sub-section on tree-based universal approximators in particular too.
