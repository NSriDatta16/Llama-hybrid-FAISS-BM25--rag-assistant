[site]: crossvalidated
[post_id]: 441726
[parent_id]: 441643
[tags]: 
The assumptions of OLS are vague, and they only ensure optimality within a very restrictive set of estimators, specifically those which are linear estimators of the mean of a distribution. What this basically boils down to is that the best way to construct a weighted average for estimating a mean is to choose weights that are inversely proportional to the variance. This is a very restrictive consideration that does not consider, for example, the possibility that the median may also be a valid measure of location, since it is a nonlinear estimator. The reason normality is often assumed is that if you are trying to estimate the mean of the distribution, then certain general inferential practices are justified. First, if the distribution is normal, estimation and inference based on F-tests and t-tests are valid for any sample size. Second, if the distribution is not normal, but it is symmetric, unimodal, and has finite constant variance with a reasonably large support, then for sufficiently large samples, inferences based on t-tests and F-tests will be approximately valid due to the Central Limit Theorem. If the mean of the distribution is not a reasonable measure of location, then OLS is not a reasonable method of estimation. For example, if the errors are either bimodal or skewed, one would not use the mean to summarize such a distribution. If the errors are such that the mean is reasonable, it may well be that another estimator that is nonlinear could be an improvement. For example, if the errors follow a Laplace Distribution, then the MLE (sample median) is an improvement over OLS. However, in practice, it can be difficult to distinguish errors that follow a Laplace Distribution from those which follow another distribution that has similar properties. Since, in these situations, there may not be a large price to incorrectly assuming normality if the sample size is sufficiently large, then the OLS estimation with normality assumptions will likely produce valid conclusions anyway. So, to summarize, the main reasoning for normality assumptions in OLS is more based on the idea that as long as it is not grossly violated, the Central Limit Theorem will save you in large samples. It provides exact inference, if normality is justified. If normality, specifically, is not valid but is not grossly invalid, other methods may be superior, but the cost of a wrong assumption is likely to be small, relative to the fact that it may be too difficult to verify that the errors follow a specific distribution.
