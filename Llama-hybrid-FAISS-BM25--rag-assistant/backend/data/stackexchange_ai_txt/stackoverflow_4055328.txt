[site]: stackoverflow
[post_id]: 4055328
[parent_id]: 4055316
[tags]: 
You need a simple robots.txt file. Basically, it's a text file that tells search engines not to index particular pages. You don't need to include it in the header of your page; as long as it's in the root directory of your website it will be picked up by crawlers. Create it in the root folder of your website and put the following text in: User-Agent: * Disallow: /imprint-page.htm Note that you'd replace imprint-page.html in the example with the actual name of the page (or the directory) that you wish to keep from being indexed. That's it! If you want to get more advanced, you can check out here , here , or here for a lot more info. Also, you can find free tools online that will generate a robots.txt file for you (for example, here ).
