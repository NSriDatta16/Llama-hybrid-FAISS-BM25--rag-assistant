[site]: datascience
[post_id]: 102944
[parent_id]: 101959
[tags]: 
Further research indicates the most relevant technique to address this problem is data valuation, also known as data shapley. Here are 2 relevant papers on the topic: Data Shapley: Equitable Valuation of Data for Machine Learning What is your data worth? Equitable Valuation of Data I spent a week implementing this technique on my project. It is highly effective at ranking training data from least useful to most useful in terms of performance on the test set. I then review the lowest value records for data quality problems, of which there are frequently obvious ones. If you have an acceptable test set to optimize against this is a great technique. The downsides are: You need a clean test set that is representative of the population. If you're using cross validation, you may be evaluating against bad cv data points. Most examples in the paper are classification, while my problem is regression. It is much easier to identify a mis-labeled image than a continuous target. Extremely expensive compute. You'll end up training anywhere from 3 N to 10 N models, where N is the number of samples in the training dataset. No library exists to do it. I ended up grabbing chunks of code from the paper's github , but you'll likely need to implement something yourself. Here is most striking result I got: Each line is a different model, at 0 all of the training data is included. 10 means we have removed 10% of the training data, starting with the lowest value records. As we remove more low value data, the test RMSE decreases. All RMSE's have been normalized so all models can be plotted together.
