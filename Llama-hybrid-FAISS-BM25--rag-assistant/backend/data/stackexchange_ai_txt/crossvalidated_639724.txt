[site]: crossvalidated
[post_id]: 639724
[parent_id]: 639685
[tags]: 
Building on Why information criterion (not adjusted $R^2$) are used to select appropriate lag order in time series model? , we may write information criteria with $K$ parameters as (for general regression models and not just AR(p)) $$ IC=\log(\widehat{\sigma}^2)+g(n)K, $$ where $K$ denotes the number of parameters and $\widehat{\sigma}^2$ the error variance estimate $SSR_K/T$ . E.g., the AIC has $g(n)=2/n$ and thus $ng(n)=2$ , so that the condition is not satisfied (and the link shows that we do not have consistency for AIC). In turn, the BIC, which is consistent, has $g(n)=\ln (n)/n$ , so that the condition is satisfied. We choose, as usual, $\mathcal{M}_1$ if $IC_1 , else select $\mathcal{M}_2$ . We will show that the probability that the IC for the true order (model $\mathcal{M}_1$ with $p=p_0$ in the AR(p) case) is less than the IC for any larger lag order (a model $\mathcal{M}_2$ with $p>p_0$ in the AR(p) case) with probability tending to one, so that the probability that an overly large order is chosen goes to zero. From the link, we choose the smaller model $\mathcal{M}_1$ with $K_1$ parameters over the larger with $K_1+K_2$ parameters with probability $$ P\bigl(IC_1 Continuing the linked proof, we may rewrite this, if the smaller model $\mathcal{M}_1$ is indeed the correct one as (in the notation, we "condition on" the smaller model) \begin{eqnarray*} P\bigl(IC_1 if $ng(n)\to\infty$ . Again, the second-to-last line follows because the statistic is the LR statistic in the linear regression case that follows an asymptotic $\chi^2_{K_2}$ null (which it is because we consider the case in which the smaller model is the correct one) distribution. The last line follows because the $\chi^2_{K_2}$ random variable is $O_p(1)$ , and hence smaller than the r.h.s w.p.a 1 if the latter diverges. Why do we not asymptotically pick the lag order too small? This case heuristically can be covered by noting that a relevant lag would then "go into" the error term, which will then have a larger variance $\hat\sigma^2$ and hence a large IC, so that the argmin will be at the true lag order. More specifically, we may then reverse the role of the correct and false model in the above derivations, with $\mathcal{M}_1$ the smaller, incorrect and $\mathcal{M}_2$ the larger correct model. We then have that, under the "alternative", the LR statistic $$n[\log(\widehat{\sigma}^2_1)-\log(\widehat{\sigma}^2_2)]=O_p(n),$$ so that the probability that the larger model will be chosen now is $$ \begin{eqnarray*} P\bigl(IC_1>IC_2|\mathcal{M}_2\bigr)&=&P(O_p(n)>ng(n)K_2|\mathcal{M}_2) \\ &\to&1, \end{eqnarray*} $$ as $g(n)\to0$ implies that $ng(n)=o(n)$ , i.e., the lhs in the inequality diverges faster that the rhs, implying consistency. A little Monte Carlo for $\bar p=3$ and $p_0=2$ illustrating that the selection $\hat p$ centers on the correct $p_0$ for larger $n$ : ARMA.BIC
