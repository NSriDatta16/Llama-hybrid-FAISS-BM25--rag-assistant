[site]: crossvalidated
[post_id]: 413878
[parent_id]: 413789
[tags]: 
I think this is perfectly normal because autoencoders are actually compressors. It's like you're zipping your files. If the information content inside your data is high, you can't compress it. There must be redundancies, which is typically caused by dependencies, and non-uniform data distribution. For example, if your image pixels are totally random, JPEG can't do much for you. Going back to original problem, you create a data with information content that cannot be efficiently compressed. The uniformity of data distribution makes it worse since every sample has the same likelihood. In the extreme case, think about a distribution where your samples are very much concentrated around zero. Then, the autoencoder can focus on reconstruction of those samples and catch redundancies better.
