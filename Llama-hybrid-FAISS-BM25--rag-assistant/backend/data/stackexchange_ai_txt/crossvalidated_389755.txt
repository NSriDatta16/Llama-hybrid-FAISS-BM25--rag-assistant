[site]: crossvalidated
[post_id]: 389755
[parent_id]: 389541
[tags]: 
TanH gives output from (-1; 1). ReLu is cut off negative values same as sigmoid, however weights could be negative too so you can expect training process will leads to get negative weights to handle this problem. Second thing is that an input and an output to neural network should be translated into domain [-1;1] or [0;1] so if you choose second option problem of negative values disappear. If you choose [-1;1] NN could easily handle this by biases. However if you think that using ReLu seems to be unnatural for this problem. You can also try using different activation function, i.e choose sigmoid on hidden layers and pre-train them separetly to speed-up handling with vanishing gradient.
