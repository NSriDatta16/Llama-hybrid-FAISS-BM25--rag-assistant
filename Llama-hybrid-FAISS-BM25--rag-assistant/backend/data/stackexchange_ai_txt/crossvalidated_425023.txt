[site]: crossvalidated
[post_id]: 425023
[parent_id]: 425020
[tags]: 
I'm a bit rusty on Markov chains, but suppose you know how to calculate $P_1(T_0 . Then Starting from state 0, you have two cases: The state remains 0 with probability $P(0, 0) = 0.5$ , after which it's certain that you reach state 0 before state 5, or The state changes to 1 with probability $P(0, 1) = 0.5$ , after which you have probability $P_1(T_0 of reaching state 0 before state 5. So $P_0(T_0 would just be $P(0, 0) \cdot 1 + P(0, 1) \cdot P_1(T_0 .
