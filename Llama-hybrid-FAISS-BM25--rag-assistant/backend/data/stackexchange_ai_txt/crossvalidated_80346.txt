[site]: crossvalidated
[post_id]: 80346
[parent_id]: 31788
[tags]: 
"Prediction" that uses the Hilbert space structure is, as Michael Chernick says above, orthogonal projection of $Y_{t+1}$ onto the subspace generated by the "predictors" $\{ Y_u, u \leq t\}$. This is linear regression in the population sense and not the same as orthogonal projection onto $L^2(\sigma(Y_t, Y_{t-1}, \cdots))$, i.e. conditional expectation with respect to $\sigma(Y_t, Y_{t-1}, \cdots)$. The latter subspace is in general much larger than the one generated by $\{ Y_u, u \leq t\}$. An explicit calculation is just linear algebra. Geometrically, a Hilbert space is no different from $\mathbb{R}^n$ with the Euclidean inner product (except that it's not locally compact but that's not relevant here). Denote the autocovariance function of ${Y_t}$ by $\gamma(h)$. One-step ahead prediction means finding $\phi_u$, $u = 1, 2, \cdots$ s.t. $$ \|Y_{t+1} - \sum_{u \geq 1} \phi_u Y_{t+1-u}\|^2 $$ is minimized. If $\langle Y_s, Y_s + h \rangle = \gamma(h) = 0$ for all $h > p$, then $$ \begin{bmatrix} \gamma(0) & \gamma(1) & \cdots & \gamma(p-1) \\ \gamma(1) & \gamma(0) & \vdots & \gamma(p-2) \\ \vdots & \vdots & \ddots & \vdots \\ \gamma(p-1) & \gamma(p-2) &\cdots &\gamma(0) \end{bmatrix} \begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \end{bmatrix} = \begin{bmatrix} \gamma(1) \\ \gamma(2) \\ \vdots \\ \gamma(p) \end{bmatrix}. $$ Assuming the matrix on the left hand side is positive definite, invert it and you're done. For $n$-step ahead prediction, shift the right hand side forward by $n$. In the general case where $\gamma$ does not have finite support but, say, is absolutely summable, taking matrices of increasing size gives an approximating sequence.
