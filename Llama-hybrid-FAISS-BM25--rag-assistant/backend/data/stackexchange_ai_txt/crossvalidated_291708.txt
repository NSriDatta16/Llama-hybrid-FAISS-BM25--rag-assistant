[site]: crossvalidated
[post_id]: 291708
[parent_id]: 
[tags]: 
How to update filter weights in CNN?

I am trying to build a Convolutional Neural Network (CNN) from scratch to learn its basics. However, I haven't found any information about how the weights from the kernels get updated in each iteration using backpropagation. The only explanation I found on the internet was this one but I'm not sure if that is right or if I didn't implement it correctly in MATLAB. I'm using "same" convolution and $3\times3\times1$ kernels. As far as I understand, all the positions of the kernels appear at all the locations of the images. So, according to the explanation above, I should sum all the contributions for each "appearance" and use that as the update of each value of the kernel. I tried to implement this but I eventually end up with all NaNs all over the network, so something is diverging. I believe the problem is in this line: kernel{1} = kernel{1} + sum(sum(0.01*eta * delta_conv{1} .* x_out_conv{1})); kernel{2} = kernel{2} + sum(sum(0.01*eta * delta_conv{2} .* x_out_conv{2})); where kernel is a cell-array containing the two kernels (one for each convolution layer), eta is the learning rate, delta_conv is the gradient coming from the output of that layer and x_out_conv is the current output for each neuron in the layer. That is constantly adding values, and it eventually goes out of control. Is that the way to update the weights of the kernels in a CNN? EDIT : To clarify, I'm using the convention from Hertz, Krogh & Palmer's Introduction to the Theory pf Neural Computation . When I talk about 'deltas', I'm referring to: where $w$ are the weights, $E$ is the error function, $\eta$ the learning rate, $\zeta$ the desired outputs, $O$ the actual output, $g()$ the activation function, $\xi$ the inputs of the network and $h$ the linear output such that $g(h) = V$, the output of each neuron. The rule to update the weights then becomes:
