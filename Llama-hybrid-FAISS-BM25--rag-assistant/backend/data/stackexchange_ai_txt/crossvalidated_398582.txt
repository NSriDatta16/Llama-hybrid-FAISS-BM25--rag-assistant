[site]: crossvalidated
[post_id]: 398582
[parent_id]: 
[tags]: 
training prior network on conditional variational autoencoder

The objective function of the conditional VAE is defined as: $L_{CVAE} = -KL(q(z|x,y)||p(z|x)) + \frac{1}{L}\sum_{l=1}^{L}\text{log } p(y|x,z^{(l)})$ Here x is input; y is output; z is latent variable. q(z|x,y) is recognition network. p(z|x) is prior network. p(y|x,z) is generation network. How is the prior network, p(z|x), learned? In a regular VAE, the prior network would just be p(z), which is fixed to be standard normal distribution. For reference, the objective function of the VAE is: $L_{VAE} = -KL(q(z|x)||p(z)) + \frac{1}{L}\sum_{l=1}^{L}\text{log } p(x|z^{(l)})$ In the conditional VAE, the recognition and generation network can be backpropagated and trained. I don't see how p(z|x) can be trained. Is it okay to just use standard normal distribution for p(z|x) as well? It seems that this is what http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf (page 67) is suggesting.
