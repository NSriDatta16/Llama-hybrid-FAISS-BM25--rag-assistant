[site]: crossvalidated
[post_id]: 183331
[parent_id]: 182940
[tags]: 
This is basically a question about $p$-values and maximum likelihood. Let me quote Cohen (1994) in here What we want to know is "Given this data what is the probability that $H_0$ is true?" But as most of us know, what it [$p$-value] tells us is "Given that $H_0$ is true, what is the probability of this (or more extreme) data?" These are not the same (...) So $p$-value tells us what is the $P(D|H_0)$, while we are interested in $P(H_0|D)$ (see also the discussion on Fisherian vs Neyman-Pearson framework). Let's forget for a moment about $p$-values. The probability of observing our data given some parameter $\theta$ is the likelihood function $$ L(\theta | D) = P(D|\theta) $$ that is one way of looking at statistical inference. Another way is Bayesian approach where we want to learn directly (rather than indirectly) about $P(\theta|D)$ by employing the Bayes theorem and using priors for $\theta$ $$ \underbrace{P(\theta|D)}_\text{posterior} \propto \underbrace{P(D|\theta)}_\text{likelihood} \times \underbrace{P(\theta)}_\text{prior} $$ Now, if you look at the overall picture, you'll see that $p$-values and likelihood answer a different questions than Bayesian estimation. So, while maximum likelihood estimates should be the same as MAP Bayesian estimates under uniform priors, you have to remember that they answer a different question. Cohen, J. (1994). The earth is round (p American Psychologist, 49, 997-1003.
