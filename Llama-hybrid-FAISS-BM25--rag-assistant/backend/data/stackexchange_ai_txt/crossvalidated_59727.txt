[site]: crossvalidated
[post_id]: 59727
[parent_id]: 59717
[tags]: 
Bayesian and Frequentist statistics rest upon a different understanding of what a probability distribution is. To a Bayesian, probability reflects degree of belief. The purpose of an experiment (or data collection) is to update one's beliefs about the parameter values of the system. And since our beliefs concern parameters (unknown) rather than data (which we see), a probability distribution is attached to the parameters. Confusingly, a pdf is also attached to the data. To a frequentist, a probability distribution reflects the frequency with which events occur. A theoretical distribution is (in theory), the (measure theoretic) limit of the observed frequency distributions. The observed frequency distribution must, in turn, reflect the sampling scheme used to collect the data, since what is at issue is the probability of given items showing up in the sample you select. But you know all this. I did my doctorate at CMU, when Morrie Degroot and Jay Kadane were there. It was a big Bayesian shop in those days, and perhaps still is. At no time did anyone talk about the cost-benefits of Bayesian vs frequentist analyses. It was a simple question of right vs wrong. No one at CMU would have thought of using a Bayesian method for time series (say), but not in everything else. This may account for the absence of papers on this subject. In general, Bayesian methods cost more than frequentist ones, since the calculations are more complex and laborious. That used to matter. Now it doesn't, which may explain the proliferation of Bayesian methods. There is also a mental cost involved with Bayesian methods, since you have to justify to yourself why, in an exploratory situation where you know nothing and you have an apparently symmetric and Normal-like sample, the sample average is not an acceptable estimate of the mean. And don't get me started on improper priors. Improper priors mean that you can only justify the sample mean by using mathematical techniques only a graduate student in analysis could understand. And yet ordinary people use sample averages all the time. You can justify OLS and the mean without reference to probability at all (see linear algebra and basic calc). Why not do so? Frequentists do not "rely on prior information". Frequentists, like everyone else, have assumptions and build these into their models. Frequentists do not apply probability distributions to those parameters of the system that are of scientific interest. I put it this way to get around latent variable models and hiercharchical or empirical Bayes models.
