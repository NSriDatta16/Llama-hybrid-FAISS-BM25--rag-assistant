[site]: datascience
[post_id]: 85143
[parent_id]: 85141
[tags]: 
I assume its crashing because not enough RAM. So I also assume your data is quite big. Your search grid is quite big. So this will definitely take some time. In order to speed up the training and overloading the RAM. You can fit the model in a subsample of data. Theoretically if your data is big enough and you sample it, when you use the whole model the optimal hyperparameters should be quite similar. Once you do, see what is the actual improvement of changing hyperparameters, hyperparameters in xgboost shouldn't be sooooo important. They will make a change but not a huge change. So my suggestion will be to sample your data by 1% or something similar to search for hyperparameters and then fitting the model in the whole training set. And not search in such a huge hyperparameter space.
