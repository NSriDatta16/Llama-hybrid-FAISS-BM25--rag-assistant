[site]: crossvalidated
[post_id]: 540700
[parent_id]: 
[tags]: 
Do the dual variables in SVMs depend on the kernel function?

Most texts I've seen hide a lot of what's under the hood in SVMs (support vector machines). The book I'm reading says SVMs can be solved using quadratic programming by using slack variables, giving solution: $$w^* = \sum _{i=1}^N \alpha _i \boldsymbol x_i$$ where $\boldsymbol x_i$ are the training samples and $\alpha _i$ are the dual variables. There's an analogy here to the kernel trick applied to ridge regression where in the ridge regression case , $$\boldsymbol \alpha = (K + \lambda I)^{-1} \boldsymbol y$$ In other words, $\alpha_i$ depends on the Gram Matrix $K$ which depends on the kernel function. Question 1: in the SVM case do the dual variables $\alpha_i$ also depend on the kernel function? If not, why? If so, how is this implemented/dealt with in finding the solutions? My concern here is the kernel trick for ridge regression only worked because we could write down $\boldsymbol \alpha$ in terms of $X X^T \rightarrow K$ via kernel trick. But if we're solving $\boldsymbol \alpha$ using quadratic programming, it's not clear how we can get the $X$ terms in the form $XX^T$ to apply the kernel trick (and hence avoid transforming the data features explicitly)? Question 2: do most SVM packages find the solution in the dual space ( $\alpha$ 's) rather than primal space ( $w$ 's)?
