[site]: datascience
[post_id]: 62533
[parent_id]: 62529
[tags]: 
Being PCA the same as SVD, I'll explain the approach with the last one. SVD decomposes a matrix $A$ in three others, so that $A = U * \Sigma * V^t$ , where $\Sigma$ is diagonal consisting of $A$ 's singular values in descending order. The sum of all the singular values of $A$ is known as energy of matrix $A$ . You want to retain most of the matrix's energy when reducing dimensions, so what you have to do is calculate how many top singular values you need to retain to preserve the most of the energy. There are two ways of achieving this: 'Elbow method': plot the singular values and see from which on the energy isn't adding much to the total energy anymore. calculate how many of the top singular values you need to preserve most of the energy. Usually you want to preserve $\simeq 90\%$ of the energy. So you have to find $m / \frac{\sum_{i=0}^{m} sv_i}{\sum_{i=0}^{n} sv_i} \simeq0.9$ . I recommend you expand my answer with the links below. I especially recommend the mentioned book. SVD on Wikipedia: https://en.wikipedia.org/wiki/Singular_value_decomposition Leskovec's book Mining of Massive Datasets, the chapter on Dimensionality reduction: http://infolab.stanford.edu/~ullman/mmds/ch11.pdf
