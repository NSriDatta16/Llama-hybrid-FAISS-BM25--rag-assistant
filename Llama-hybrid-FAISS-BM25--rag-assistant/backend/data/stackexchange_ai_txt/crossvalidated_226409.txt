[site]: crossvalidated
[post_id]: 226409
[parent_id]: 
[tags]: 
Weighted arithmetic mean weight choice in a simplified Bayes estimator

A Bayesian estimator as defined in the Wikipedia article Practical example of Bayes estimators balances the prior knowledge of the entire data set with the knowledge of the subset. This is usually used when we have a small sample from the subset. What is a good weight choice for the priori knowledge constant for a Bayesian Estimator? For example, let's say we have set of restaurants. Those restaurants can be liked or disliked. If we treat "likes" as 1 and "dislikes" as 0 (and clicking on like or dislike as a vote), then we can treat the likability of restaurants as a Bernoulli trial. For example, let's say for all of the restaurants in the country the average "likes"/votes is 0.7 or 70% . Now a new restaurant opens up. It is a burger joint and 1 person clicks "like". Should that restaurant get a rating of 100% and immediately jump to the top of the best foodies list? Definitely not. There is only 1 vote. A way to handle this is with a Weighted arithmetic mean: w = (m * national_average + restaurant_votes * restaurant_average) / ( m + restaurant_votes) Doing the math, we get: (4 * 0.7 + 1 * 1.0) / (4 + 1) = 0.76 So the new burger joint gets a rating of 76% likability. But what should the value of m be? Is 4 a good choice? Is the El Torito place really better than the Star of India?? If one treats each star rating as up to five likes, then the above applies. Looking at the Wikipedia article Practical example of Bayes estimators , it gives this example from IMDB and looking back in 2012, the constant of m was chosen to be 3000. Why 3000? Given the above formula what is a good weight value for m ? The Naive Bayes spam filtering: Dealing with rare words article suggests a value of 3 is a good value if it is a random variable with beta distribution . The Agresti-Coull Interval hints at a choice of the prior knowledge of z^2 for 3.8416 or essentially 4 given the rule of thumb "add 2 successes and 2 failures". Is this really a Bayesian estimators question? Looking at this Bayes' Estimators , the formulas look a lot more complex... Update: This paper adds insight to the choice of weight: TO THE BASICS: BAYESIAN INFERENCE ON A BINOMIAL PROPORTION It relates to a level of certainty . References: Agresti-Coull Interval Practical example of Bayes estimators Naive Bayes spam filtering: Dealing with rare words
