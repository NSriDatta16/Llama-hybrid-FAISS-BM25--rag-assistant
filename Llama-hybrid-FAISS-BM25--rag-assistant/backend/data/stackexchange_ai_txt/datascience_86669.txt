[site]: datascience
[post_id]: 86669
[parent_id]: 
[tags]: 
Context Based Embeddings vs character based embeddings vs word based embeddings

I am working on a problem that uses English alphabets in the text but the language is not English. Its a mixture of English and different language text. But all words are written using English alphabets. Now, word-based pre-trained embedding models will not work here as it gives a random embedding to out of vocabulary words. Now my question is that how the Context-based pre-trained embeddings deal with "out of vocabulary" words? Besides, what's the difference between context-based embeddings and character-based embeddings?
