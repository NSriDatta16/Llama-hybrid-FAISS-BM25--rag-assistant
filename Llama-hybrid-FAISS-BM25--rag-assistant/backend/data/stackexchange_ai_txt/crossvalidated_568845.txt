[site]: crossvalidated
[post_id]: 568845
[parent_id]: 412058
[tags]: 
The clever thing with the various Markov-Chain Monte-Carlo (MCMC) samplers (like JAGS, WinBUGS, Stan, pymc3 etc.) is that they do not need to calculate the posterior itself. They only need the (log density function of the) prior distribution and the (log-)likelihood. The product of these two (or the sum, when working on the log-scale) is only proportional to the posterior (we lack a normalizing constant). Being able to work with that may not sound like a huge bit of progress, but actually helps a lot, because we can usually not find the necessary normalizing constant analytically. In contrast, it is often pretty easy to write down the log-likelihood and the log density of the prior distribution. That (either conditional on some parameters for Gibbs sampling, or unconditionally for e.g. Metropolis-Hastings or Hamiltonian Monte Carlo) is enough to put MCMC samplers to work.
