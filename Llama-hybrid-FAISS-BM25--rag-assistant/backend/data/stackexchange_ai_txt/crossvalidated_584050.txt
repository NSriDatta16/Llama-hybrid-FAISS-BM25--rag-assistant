[site]: crossvalidated
[post_id]: 584050
[parent_id]: 583588
[tags]: 
How does the 70 % accuracy of your CV compare to the rF's oob estimate? The behaviour you observe is to be expected for random forests, see also my old answer: https://stats.stackexchange.com/a/208018/4598 The submodels of a random forest are unpruned decision trees. Training data for such an unpruned tree will basically be recognized as known and assigned to the correct class. Since each case in the training set for the random forest is training case for ca. 68 % of the trees, these correct predictions will also dominate majority vote. Training error is therefore usually not even reported for random forests -- and it cannot tell you anything about the generalization error of the forest, nor whether the forest is overfit. This is not a flaw in the algorithm, it's by design. Usually, when worrying about overfitting the idea is that the model may be too complex and thus unstable . Random forest by design uses overfit trees as submodels. The ensemble prediction then takes care of the overfitting. The out-of-bag estimate of generalization error that is usually computed alongside random Forest training, however, in the usual implementation depends crucially on independence between the data rows. If rows are dependent (clustered/hierarchical/nested data structure), the oob esimate will have an optimistic bias. In that case, a cross-validation whose splits achieve independence will help to get a more realistic estimate of the generalization performance. An off-the-shelf cross validation randomly assigning rows won't - it has the same problem the oob estimate has with such data. I am using a grid search to find the best hyperparameters When evaluating the grid search results, you do take into account that some of the rF hyperparameters behave differently from how most hyperparameters for other models behave, do you? In particular, don't expect a classical optimum for the number of trees.
