[site]: crossvalidated
[post_id]: 470355
[parent_id]: 470348
[tags]: 
Setting aside what Cross Validated tends to think of classifier metrics that depend on thresholds, $^{\dagger}$ I think I see your problem. We tends to care very little about machine learning performance on in-sample data. Adding more and more parameters allows us to play connect the dots (so to speak) and memorize the training data. What we care about is how the machine learning model performs on unseen data, since this mimics how real machine learning works (e.g. Apple or Amazon doing speech recognition on sentences that have yet to be spoken). Apply your models to data that you've held out from the training data, and see if you get the same issue of the simpler model having higher F1 score. $^{\dagger}$ See, for instance, my post from the past few weeks that has an excellent answer to an issue that a practicing data scientist may encounter. (TLDR: look at the predicted probabilities, not the classifications based on a particular threshold.) There are lots of other posts on CV about these "proper scoring rules", too.
