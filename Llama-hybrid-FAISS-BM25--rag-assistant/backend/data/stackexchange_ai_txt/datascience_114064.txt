[site]: datascience
[post_id]: 114064
[parent_id]: 
[tags]: 
Hyperparameter Tuning Guidelines Across Lots of Models

I am performing a set of experiments in which I need to tune a fairly small set of hyperparameters but over a very large space of models trained on relatively related datasets and I'm trying to essentially limit the amount of trials I need to run in order to find somewhat optimal hyperparameter settings for this space. To clarify, I am deconstructing a number of classification datasets which have a binary label space but also have a categorical variable, for example, sentiment classification of product reviews but these can then be further split by the type of product; so one model would be sentiment classification of Video Game reviews and the other would be sentiment classification of Movie reviews and these datasets would be split into seperate binary classification models where I try to predict the sentiment only on that subdomain. What I am asking is are there general best practices to avoid evaluating every combination of hyperparameters over every dataset. I know that there are options such as random over grid search or even something fancier like Bayesian optimisation, or even narrowing down the space by performing one technique and identifying groups of poor parameters etc. But even with a small pool of combinations that I'm willing to try, with each dataset and each task the number of trials explode. Does it make sense to evaluate every combination over every task or is it reasonable to pick a random subset of tasks, find the best parameter settings through some technique, and just use those parameters going forward or is this considered to be a bit of an optimistic assumption? My idea behind this is that the datasets are sufficiently related that it is reasonable to assume a set of hyperparameters that generalise well to say, five tasks, will likely generalise to others. I know this question may be largely down to preference but I'm essentially trying to find out best practices and take some shortcuts without taking shortcuts that are potentially detrimental to my research process.
