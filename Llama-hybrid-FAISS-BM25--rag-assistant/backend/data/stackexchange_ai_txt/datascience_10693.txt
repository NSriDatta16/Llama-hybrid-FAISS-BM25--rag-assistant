[site]: datascience
[post_id]: 10693
[parent_id]: 10692
[tags]: 
NNLM has following sets of parameters ). Using $V$ to denote the number of words in the vocabulary: The matrix which creates the embedding for each word in the context . This distributed embedding is in $\mathbb{R}^{m}$ space. This matrix is $C :{V \times m}$ The matrix which transforms the concatenated list of word embeddings (active in the current context of size $n-1$) to the hidden layer (of size $h$). This matrix is $H:{(n-1)m\times h}$ The matrix which maps the hidden layer to unmormalized probabilities for each word in the vocabulary. This matrix is $V :{V \times h}$ The matrix connecting the context word embeddings to the output layer. This connection is optional and has dimensions $W:{V \times (n-1)m}$ Note: while training using SGD, for a single example only $n-1$ words in the context are active out of words $V$ in the vocabulary. I have also omitted the parameter vectors for the bias terms $b:V \times 1$ when computing unnormalized outputs and when computing the hidden layer ($d: h \times 1$). Hence my current understanding is that number of parameters in NNLM is: $$dim(C)+ dim(H)+ dim(V) + dim(W)= (n-1)m \times V + h \times (n-1)m+ V \times h + V \times (n-1)m$$
