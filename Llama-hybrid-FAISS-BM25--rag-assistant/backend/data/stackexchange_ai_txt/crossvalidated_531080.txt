[site]: crossvalidated
[post_id]: 531080
[parent_id]: 530735
[tags]: 
I managed to prove the statement thanks to passerby51 for hinting the jointly Gaussian distribution. The proof goes as follows: Show averages in each contrast are independent Show contrasts are uncorrelated Use Theorem 1: nonsingular transformation of independent random variables $X_1,\dots,X_n$ , where $X_i \sim \mathcal{N}(0, \sigma_i^2)$ , is jointly Gaussian Use Theorem 2: jointly Gaussian random variables that are uncorrelated are independent I am assuming (a) a fixed-effect ANOVA model $Y_{\alpha l} = \mu_{\alpha} + \epsilon_{\alpha l}; \epsilon_{\alpha l} \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$ where $\alpha$ denotes factor combination and $l$ denotes repeat test, and (b) experiment is balanced ( $r$ repeat tests per combination). Accordingly, an average can be expressed as $\bar{X}_i \triangleq \frac{1}{r|I|}\sum_{\alpha \in I, l}Y_{f(i,\alpha) l}$ . For example, in a 3-factor experiment, an average would be $\bar{X}_j \triangleq \frac{1}{acr}\sum_{ikl} Y_{ijkl}$ where $\alpha=ik$ , $f(j,\alpha)=ijk$ , and $I=\left\{(i, k) \mid i \in [1, a], k \in [1, c] \right\}$ . For any two averages to be independent, I assume they do not share any $\epsilon_{\alpha l}$ , i.e. each average $i$ contains an exclusive set of factor-level combinations $f(i, I)$ . For example, in a 2-factor balanced experiment, for $\bar{X}_i \triangleq \frac{1}{br}\sum_{jl} Y_{ijl}$ (average response for factor-level $i$ ), any $\bar{X}_i$ and $\bar{X}_j$ do not share a $\epsilon_{\alpha l}$ with the same index, thus $\bar{X}_i \perp \bar{X}_j$ for $i \neq j$ . As passerby51 neatly showed, two contrasts are uncorrelated iff $\sum_i a_ib_i\text{Var}(\bar{X}_i) = 0$ . Now by using representation $\bar{X}_i = \frac{1}{r|I|}\sum_{\alpha \in I, l}Y_{f(i,\alpha) l}$ for averages, we have $$\text{Var}(\bar{X}_i) = \frac{1}{r^2|I|^2}\sum_{\alpha \in I,l}\sigma^2=\frac{\sigma^2}{r|I|} \Rightarrow \sum_i a_ib_i\text{Var}(\bar{X}_i) = \frac{\sigma^2}{r|I|}\sum_i a_ib_i \overset{\sum_{i=1}^n a_ib_i=0}{=} 0$$ We express contrasts $C_1 = \sum_{i=1}^{n} a_i \bar{X}_i$ and $C_2 = \sum_{j=1}^{n} b_j \bar{X}_j$ in a matrix form as follows $$\begin{align*} \begin{bmatrix} C_1\\ C_2\\ \dots \end{bmatrix} &= \begin{bmatrix} a_1& \dots & a_n\\ b_1& \dots & b_n\\ . & \dots & . \end{bmatrix} \left( \begin{bmatrix} \bar{X}_1 - \mathbb{E}[\bar{X}_1] \\ \bar{X}_2 - \mathbb{E}[\bar{X}_2]\\ \dots \end{bmatrix} + \begin{bmatrix} \mathbb{E}[\bar{X}_1] \\ \mathbb{E}[\bar{X}_2] \\ \dots \end{bmatrix} \right) \\ &\Rightarrow \mathbf{C} = A\mathbf{X} + A\mathbf{\mu} = A\mathbf{X} +\mathbf{\mu'} \end{align*}$$ where $\mathbb{E}[\bar{X}_i]=\frac{1}{|I|}\sum_{\alpha \in I}\mu_{f(i,\alpha)}$ , and $X_i \triangleq \bar{X}_i - \mathbb{E}[\bar{X}_i] = \frac{1}{r|I|}\sum_{\alpha \in I, l}\epsilon_{f(i,\alpha) l} \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2/r|I|)$ . The other rows of matrix $A$ (3rd row and below) are filled with dummy values just to have $n$ linearly independent rows, thus a nonsingular $A$ ; here orthogonality of contrasts came into play. Now using the fact that jointly Gaussian distribution is closed under marginalization of the rest of (dummy) contrasts, Theorem 1 implies that $C_1$ and $C_2$ are jointly Gaussian. Using Theorem 2, statistical independence of two orthogonal contrasts follows immediately. Theorems 1 and 2 can be found in this Lecture by Prof. Robert B. Ash
