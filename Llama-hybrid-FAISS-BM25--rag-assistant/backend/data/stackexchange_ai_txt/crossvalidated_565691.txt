[site]: crossvalidated
[post_id]: 565691
[parent_id]: 565690
[tags]: 
If you're willing to use a Bayesian approach instead, you could try brms . The model seems to converge then: Input library(dplyr) library(brms) mydata % mutate(age = c(scale(age))) m Output Family: hurdle_lognormal Links: mu = identity; sigma = identity; hu = logit Formula: outcome_log ~ age + sex * age + smoking * age + obesity * age + diab * age + hypt * age + hyperchol * age + ckd * age + (1 + age | pat_id) hu ~ sex + smoking + obesity + diab + hyperchol + ckd + (1 | pat_id) Data: mydata (Number of observations: 304) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Group-Level Effects: ~pat_id (Number of levels: 146) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 0.29 0.03 0.23 0.35 1.00 1156 1968 sd(age) 0.07 0.05 0.00 0.20 1.01 403 520 sd(hu_Intercept) 7.03 1.97 4.09 11.48 1.00 874 1446 cor(Intercept,age) -0.12 0.47 -0.90 0.86 1.00 1848 1946 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 1.47 0.12 1.24 1.70 1.00 1204 2014 hu_Intercept 4.68 2.54 0.56 10.67 1.00 985 1301 age 0.02 0.14 -0.25 0.29 1.00 1192 1922 sexMale 0.12 0.08 -0.03 0.28 1.00 984 1678 smokingNeversmoker -0.33 0.08 -0.49 -0.17 1.01 1130 1588 obesityYes -0.10 0.10 -0.30 0.09 1.00 1079 1792 diabYes 0.30 0.13 0.05 0.57 1.00 1161 1839 hyptYes 0.11 0.07 -0.04 0.25 1.00 1055 1835 hypercholYes 0.07 0.10 -0.14 0.27 1.00 1187 1871 ckdBelow90 -0.02 0.08 -0.17 0.14 1.01 864 1225 age:sexMale 0.04 0.08 -0.12 0.21 1.00 1406 2314 age:smokingNeversmoker 0.23 0.10 0.04 0.43 1.00 1386 2032 age:obesityYes -0.08 0.14 -0.35 0.19 1.00 1780 2447 age:diabYes 0.17 0.15 -0.13 0.47 1.00 1943 2514 age:hyptYes -0.16 0.09 -0.33 0.00 1.00 1554 2319 age:hypercholYes 0.16 0.13 -0.09 0.41 1.00 1372 2314 age:ckdBelow90 -0.03 0.09 -0.21 0.14 1.00 1390 1857 hu_sexMale -2.93 1.76 -6.87 0.07 1.00 1174 1367 hu_smokingNeversmoker 2.83 1.77 -0.24 6.99 1.00 1119 1477 hu_obesityYes -0.22 2.42 -5.20 4.35 1.00 1258 1722 hu_diabYes -26.76 17.68 -71.71 -5.47 1.00 1516 912 hu_hypercholYes -6.18 2.58 -12.12 -2.22 1.00 1075 1809 hu_ckdBelow90 -1.64 1.74 -5.20 1.47 1.00 1051 1638 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 0.15 0.01 0.13 0.17 1.00 1871 2724 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). This appears to identify some of the problem. In particular, it looks like there is a (quasi-) separation problem with diab in the hurdle equation. It's coefficient of -26 on the logic scale is really far away from zero. My guess is that this is one of the reasons that the frequentist GLMM is not converging with diab in the model. You can see the same, though to a lesser degree, with hyperchol . Edit - diagnosing the problem Essentially, the hurdle model is estimating a login on the 0 vs. not zero on the outcome. Then estimating a different model (in this case, a log-normal GLM) on the non-zero observations. One potential problem in logistic regression modes (or any model for binary dependent variables) is separation. The simplest example of this problem is when there is no variation in y for a particular category in a categorical x . Consider the cross-tabulation between zero/not-zero on the outcome and the diab variable. table( factor(I(mydata $outcome == 0), levels=c(FALSE, TRUE), labels=c("Not Zero", "Zero")), mydata$ diab) # No Yes # Not Zero 178 19 # Zero 107 0 If you think about what's happening here. The model is trying to recover the log of the odds ratio for Yes vs No observations: If we substitute the probabilities in from the cross-tab, we would get: The odds ratio would be (0/1)/(.375/.625) = 0. When we take the log of that value, it's negative infinity. So, the model is trying to send the coefficient out toward negative infinity. In Frequentist models, the telltale sign of this is coefficients very far away from zero (for the scale of the dependent variable) with very large standard errors. This is essentially what is happening here. In the Bayesian case, the prior may provide enough information for the model to converge. In the frequentist world, these problems are often solved with regularization (e.g., Firth login, a penalized-likelihood solution). In the Bayesian setting, these results can be approximated. Normal priors with smaller variances would do something like an L -2 penalty while using Laplace priors would generate something akin to the L -1 penalty.
