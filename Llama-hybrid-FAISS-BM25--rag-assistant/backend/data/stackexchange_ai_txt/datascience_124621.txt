[site]: datascience
[post_id]: 124621
[parent_id]: 124619
[tags]: 
In theory, if you were to pass the same data through a logistic regression or a SVM implemented in different packages, you should get the same results. And the same should apply when tuning a model in package A and re-creating the same one in package B with the optimal parameters found in package A. However, in practice, there will likely be some differences due to details that are implementation-specific (logistic regression or SVM might be implemented differently in Scikit-Learn and Linfa). Off the top of my head, the following might affect your experiment: Randomisation (can be controlled with random generator seeds). Default settings that are default in one library but not the other (can be controlled by explicitly specifying as many parameters as possible). Different parameters , as in some parameters might be named differently (e.g., learning rate is named learning_rate in Scikit-Learn but it's called lr in PyTorch) or might be non-existent depending on the implementation of the package in question. To make sure your experience is a success, you should check all parameters that are controllable between Scikit-Learn and Linfa and manually set as many as possible so you are sure you can replicate it in Linfa (while making sure the naming is the same, and if it's different map them correctly when you save them and load them in R). If you explicitly set all the parameters, use the same random seed, and have the same data with the same splits, then the results should be very close if not identical.
