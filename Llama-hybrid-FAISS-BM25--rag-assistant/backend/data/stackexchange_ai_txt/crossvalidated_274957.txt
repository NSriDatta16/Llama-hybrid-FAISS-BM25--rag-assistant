[site]: crossvalidated
[post_id]: 274957
[parent_id]: 274815
[tags]: 
A very interesting question...that may not have an answer (but that does not make it less interesting!) A few thoughts (and many links to my blog entries!) about that meme that all models are wrong : While the hypothetical model is indeed almost invariably and irremediably wrong , it still makes sense to act in an efficient or coherent manner with respect to this model if this is the best one can do. The resulting inference produces an evaluation of the formal model that is the "closest" to the actual data generating model (if any); There exist Bayesian approaches that can do without the model , a most recent example being the papers by Bissiri et al. (with my comments ) and by Watson and Holmes (which I discussed with Judith Rousseau ); In a connected way, there exists a whole branch of Bayesian statistics dealing with M-open inference ; And yet another direction I like a lot is the SafeBayes approach of Peter Gr√ºnwald , who takes into account model misspecification to replace the likelihood with a down-graded version expressed as a power of the original likelihood. The very recent Read Paper by Gelman and Hennig addresses this issue, albeit in a circumvoluted manner (and I added some comments on my blog ). I presume you could gather material for a discussion from the entries about your question. In a sense, Bayesians should be the least concerned among statisticians and modellers about this aspect since the sampling model is to be taken as one of several prior assumptions and the outcome is conditional or relative to all those prior assumptions.
