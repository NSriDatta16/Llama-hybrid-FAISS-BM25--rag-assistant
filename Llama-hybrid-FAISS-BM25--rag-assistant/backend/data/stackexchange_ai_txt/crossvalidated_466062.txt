[site]: crossvalidated
[post_id]: 466062
[parent_id]: 
[tags]: 
Why do AR(1) times series generated by two methods look similar but have different variance estimate in Python

I come across one question when I use two ways to generate AR(1) sequences. By definition, AR(1) sequence is $x_t = \phi_1 x_{t-1} + \varepsilon_t,\quad \varepsilon_t\sim N(0, \sigma^2)$ I found one function from statsmodels.tsa.arima_process.arma_generate_sample to generate the time series. In addition, it is possible to write the covariance matrix $\Sigma$ first and then generate the series $x = \Sigma e$ . To make the two results comparable, I modify the function from built-in function so that they accept the same random error $e$ . It also requires numpy, signal, stats modules to implement. I set the seed as np.random.seed(100) and the series length is 300 . It seems that the series by built-in function is usually sharper than my own code. For built-in function, I set ar=np.r_[1, -np.array([0.5])], ma = np.array([1]), scale=1 . $AR(1)$ time series"> And the variance of built-in and my own code is 1.38, 1.04 . I have two specific questions here. Why do two methods produce slightly different results under the same parameter and seed setup? I am guessing ma parameter from the built-in function because when I tune it from 1 to 0.9 , their difference at peak is reduced. But I am not sure how it is related to the actual change? Even though two time series look similar, why do their variance differ so much? (The true variance is 1 here.) Finally, I will attach the code I use to produce two plots. Modified built-in function def arma_generate_sample_fix_randn( ar, ma, n_sample, rand_num, scale=1, axis=0, burnin=0 ): if np.ndim(n_sample) == 0: n_sample = [n_sample] if burnin: # handle burin time for nd arrays, maybe there is a better trick in scipy.fft code newsize = list(n_sample) newsize[axis] += burnin newsize = tuple(newsize) fslice = [slice(None)] * len(newsize) fslice[axis] = slice(burnin, None, None) fslice = tuple(fslice) else: newsize = tuple(n_sample) fslice = tuple([slice(None)] * np.ndim(newsize)) eta = scale * rand_num return signal.lfilter(ma, ar, eta, axis=axis)[fslice] Covariance matrix by my own code def create_toeplitz_cov_mat(sigma_sq, first_column_except_1): r""" sigma_sq: scalar_like, first_column_except_1: array_like, 1d-array, except diagonal 1. return: 2d-array with dimension (len(first_column)+1, len(first_column)+1) """ first_column = np.append(1, first_column_except_1) cov_mat = sigma_sq * linalg.toeplitz(first_column) return cov_mat def create_ar1_cov_mat(sigma_sq, rho, n): r""" sigma_sq: scalar rho: scalar, should be within -1 and 1. n: integer return: 2d-matrix of (n,n) """ rho_tile = rho * np.ones([n - 1]) first_column_except_1 = np.cumprod(rho_tile) cov_mat = create_toeplitz_cov_mat(sigma_sq, first_column_except_1) return cov_mat def generate_ar1_noise_close_fake( sigma_sq, rho, seq_length, std_noise ): cov_mat = create_ar1_cov_mat(sigma_sq, rho, seq_length) cov_half_mat = np.linalg.cholesky(cov_mat) ar1_noise = cov_half_mat @ std_noise del std_noise return ar1_noise Main rho = np.array([0.5]) ma_1 = np.array([1]) n_length = 300 rand_n = np.random.normal(size=n_length) ar = np.r_[1, -rho] noise_arq_test_1 = BayesGenSeqObj.arma_generate_sample_fix_randn( ar, ma_1, n_length, rand_n, scale=1, axis=0, burnin=0 ) noise_arq_test_2 = BayesGenSeqObj.generate_ar1_noise_close_fake( 1, rho, n_length, rand_n ) print(np.var(noise_arq_test_1)) print(np.var(noise_arq_test_2)) noise_arq_test_diff = noise_arq_test_1 - noise_arq_test_2 plt.figure() # plt.plot(np.linspace(0, size_num, num=size_num), rand_n[0, :], label='init-randn') plt.plot(np.linspace(0, n_length, num=n_length), noise_arq_test_1, label="built-in") plt.plot(np.linspace(0, n_length, num=n_length), noise_arq_test_2, label="I write it") # plt.plot(np.linspace(0, size_num, num=size_num), noise_arq_test_diff[0, :], label='diff') plt.legend(loc="upper right") plt.show() Thank you for your help in advance.
