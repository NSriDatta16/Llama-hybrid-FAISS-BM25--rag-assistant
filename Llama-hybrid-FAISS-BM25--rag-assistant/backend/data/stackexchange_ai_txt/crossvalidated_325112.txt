[site]: crossvalidated
[post_id]: 325112
[parent_id]: 
[tags]: 
Why train value network on cumulative discounted return?

I recently read a lot of code regarding policy gradients, in particular, PPO and TRPO. To reduce variance the common approach is to train a network function as a baseline (value function, actor-critic style). Usually, this is done by training the value network on the discounted cumulative reward/return. Reference in tensorflow agents, for instance: https://github.com/tensorflow/agents/blob/master/agents/algorithms/ppo/ppo.py#L437-L446 The thing I find a little odd is that the value network usually does not know the current timestep. It is only given the current state as input. But using discounted cumulative return the value is much higher in an early timestep than in a later timestep. So why would you want to train the value network on the cumulative discounted return? Imagine having a state occurring at the start of an episode and close to the end, the cumulative discounted reward would be very different. Am I missing something here? Thanks, Magnus
