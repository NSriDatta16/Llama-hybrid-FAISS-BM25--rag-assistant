[site]: crossvalidated
[post_id]: 121367
[parent_id]: 121234
[tags]: 
The technique you seem be trying to use is what is sometimes called GSM (General Softmax) output layer, described by (Dieterrich & Bakiri, 1995) . In this technique you represent each class with a fixed binary code and train your network to output the right code for each training example. This has been reported to increase performance by reducing overfitting in cases in which there are a large number of classes and their distributions are far from uniform (e.g. Mohamed et Al., 2009 ). This approach isn't very popular though. It's more usual to just use a regular Softmax as the output layer, which is just the generalization of logistic regression for the multiclass setting. In most problems it's likely that the differences between approaches wouldn't be significant.
