[site]: crossvalidated
[post_id]: 277251
[parent_id]: 
[tags]: 
Picking priors in Bayesian MCMC linear regression

Let me preface this by saying that I am an applied statistician teaching myself Bayesian regression. If we can do this without Greek letters that would be great. Suppose I am modeling a continuous outcome on some predictor. We'll assume a jeffrey's prior for the variance. Now I need a prior for my predictor and the constant. Essentially what I am doing is setting up a distribution that the values for the parameters (the coefficients for the constant and predictor) will be drawn from, correct? Assuming I want informative priors, my immediate inclination is to go with a normal prior. I set the mean to my "best guess" of what the effect of the predictor ought to be and then set the variance. I like the normal distribution because it is essentially favoring values near my guess while discounting those far from it (i.e. the tails) and discounting them in a symmetrical way. I'm also specifically wondering if I can make use of the standard deviation to characterize my confidence in my guess. An example will illustrate. I get that larger variances mean less weight assigned to the prior belief. However, I am curious if I can interpret this in terms of standard deviations. For example, if I believe the effect will be 5, I set the mean at 5. Let's say I'm 95% confident that the effect will be between 15 and -5. This would call for a standard deviation of 5 so I'd set the variance at 25. Is this a proper way to describe my confidence in the prior? As another example, if I were absolutely positive that the effect was going to be somewhere between -10 and 20 I would use a uniform prior ranging from -10 - 20, correct? Am I also correct in assuming that all this is true regardless of the actual distribution of the independent variable (i.e. it could be continuous or dichotomous)?
