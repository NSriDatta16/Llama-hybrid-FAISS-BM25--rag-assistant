[site]: crossvalidated
[post_id]: 177298
[parent_id]: 177260
[tags]: 
The choice of the Berkeley Earth data set puts this into the realm of statistics, as their approach is data driven. The project was set up in 2010 to " re-analyze the Earth's surface temperature record " based on what the founders saw as "merit in some of the concerns of skeptics." The largest single private financial contribution in their first year came from the Charles G. Koch Charitable Foundation, not known as a great fan of the idea of anthropogenic climate change. They put a strong emphasis on data quality to start, and applied a set of well defined statistical methods to combine worldwide data over centuries into a spatio-temporal model of Earth temperatures. This re-analysis did not depend on any underlying theoretical dynamical models. In fact, one of their contributions is showing that complicated models are not required for modeling changes in global temperatures; atmospheric carbon dioxide and volcanic activity together essentially tell the whole story. They did not pick a particularly cold year to start, as you can see in Figures 1 and 2 of the paper linked in this paragraph. So the question basically becomes the following. Say you have statistical models for the changes of temperature at 3500 cities over time. Even if there were a large overall increase of temperature, wouldn't you expect at least one city to show no change or a decrease, just by the luck of the draw? How likely you are to see any negative values in this situation depends on both the average change and the error in estimating that change. You can get rough ideas of these from the list of 100 largest cities on the Berkeley Earth website . The list provides an estimate of warming since 1960 (in degrees C per century) for each city and an associated error term that I conservatively take to be the standard error of the estimate rather than 95% confidence intervals. Over those 100 cities, the mean change is 1.93 degrees C per century, with a root-mean-square standard error of 0.351. The ratio of the change to the standard error (z-score) is about 5.50. That is, any negative value is at least 5.5 standard deviations below the mean. If this z-score follows a normal distribution, as expected, the probability of a particular estimate having a value less than 0 is only about $2x10^{-8}$. The probability of all 3500 estimates being positive is: $$\left(1-2x10^{-8}\right)^{3500}=0.9999$$ I'd say that's a pretty good possibility. More sophisticated analyses may be possible. I haven't looked into the basis for their error estimates, and the global modeling presumably leads to correlations among city-based estimates that I didn't take into account. But to a first approximation, the errors in estimating changes are so small with respect to the magnitudes of the changes that it would be surprising to find any city with an estimated decrease in temperature.
