[site]: crossvalidated
[post_id]: 352503
[parent_id]: 350814
[tags]: 
Prof. Yaser Abu-Mostafa talks briefly about this in his Caltech course on machine learning during the first lecture. He identifies 3 essential points you have to consider before considering applying machine learning to your problem: 1st. a pattern exists In order to be able to use your features for predicting anything there has to be some relationship between those features and the thing you are predicting. An example of this might be trying to predict the persons height by using data about what he ate yesterday. There is probably no relation between these two so machine learning wouldn't apply. 2nd. the pattern cannot be written down mathematically If you can solve the relation between input variables and prediction using a mathematical formula then there is no need to apply machine learning. Example of this point might be using machine learning in trying to predict the odds in a game of roulette. You can do that by calculating all the probabilities using equations from probability theory. The calculated odds would be exact and machine learning would only produce less reliable solutions. 3rd. you have data Machine learning tries to estimate parameters based on examples. And without data you cannot start using machine learning. Example of this might be trying to predict who will win a war by using various data about political climate, technology each side has, the spending on military, etc. If you had data for a lot of wars you might be able to do this. But since wars are pretty rare and there is no way to produce more of it on demand - machine learning will not work. These are the main requirements - the essence of machine learning . Briefly the examples in the question: 1) We have fairly high-accuracy ground-truth labels in our dataset. This seems highly subjective and context dependant. Consider predicting the age of death for a person, when the data you have only has "a best guess" from their doctor. The data would be very noisy, but if we can reduce the unknown factor by 5% or so after applying machine learning - it might be worth while: the algorithm would be as good as the guess by professional. 2) The distribution from which the data is sampled stays relatively constant. This is not a hard requirement. There is a sub area of machine learning that tries to deal with problems like these, called Concept Drift 3) The output we are trying to learn is actually a function of the inputs we are given. This is same as the 1st one mentioned by prof. Abu-Mostafa. That the "Pattern Exists". 4) The effective number of independent samples in our dataset is high enough for the levels of noise in the dataset. This is very relevant but at the same time subjective, just like the 1st point mentioned in the question. For some problems improvement of a few percent might be considered good enough. 5) The metric we would like our model to optimize is quantifiable. Not sure if I understand this one. From the comments it seems it is talking about comparison of different solutions in order to select the better one. I cannot quickly think of a scenario where this would not be satisfied. Unless the practitioner doesn't really have a clear goal in mind.
