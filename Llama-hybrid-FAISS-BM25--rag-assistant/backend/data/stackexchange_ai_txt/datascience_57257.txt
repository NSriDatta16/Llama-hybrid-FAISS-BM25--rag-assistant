[site]: datascience
[post_id]: 57257
[parent_id]: 56761
[tags]: 
Since (word-based) one-hot encoding and real-valued vector representations are already mentioned in the question, I would only add the n-gram representation, especially the character-based n-gram representation. For word-based n-gram representations you consider not individual words, but their ordered combinations in the text and use the one-hot encoding for the combinations. E.g. for n=2 you might end up with the bigrams ["John likes", "likes to", "to watch", "watch movies"] and each of them would be assigned to some dimension using a static index. This also works with characters, so you can represent the word "encoding" e.g. with those 3-grams: ["enc", "nco", "cod", "odi", "din", "ing"]. The one-hot encodings of n-grams are typically added, so multiple occurances of the same n-gram are recognizable in the resulting Bag-of-n-grams representation. This kind of representation is especially useful for languages with rich morphology and/or compound words. In a one-hot representation each single word form would be encoded in its own dimension whereas a character n-gram approach helps preserve similarity between different forms. An example in the English language would be the similarity between "encode", "encoded" and "encoding" which would stay preserved this way. Similar techniques are also used by some word embedding algorithms which consider subword information like e.g. FastText . Also, although it's not directly an encoding, but depending on your use case and language it might be worth looking at different preprocessing options like lemmatization and stemming where you reduce different word forms to their base form. This would also affect the choice of representation, e.g. the word-based one-hot encoding might make more sense if you choose to use these preprocessing techniques.
