[site]: datascience
[post_id]: 21774
[parent_id]: 
[tags]: 
The k -nearest neighbors algorithm ( k -NN) is one of the simplest machine learning algorithms. It is non-parametric (makes no assumptions about the population distribution or sample size) and memory-based method that can be used for classification and regression. Let's say we want to train k -NN with a set of $N$ feature vectors $\boldsymbol{x}$ and class labels $y$: $\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\dots,(\boldsymbol{x}_N,y_N)\}$. In the training phase, no model is fitted but the feature vectors and class labels are stored in memory. In the classification/regression phase, a distance between a given query feature vector $\boldsymbol{x}_u$ and each training example is calculated, and k (user-defined constant) closest training examples in the feature space are selected. If it is a classification task, the predicted class label $\hat{y}_u$ is selected by a majority vote amongst $\boldsymbol{x}_u$'s k nearest neighbors. If it is a regression task, the predicted value $\hat{y}_u$ is the average of the values of $\boldsymbol{x}_u$'s k nearest neighbors. A downside of the majority voting is that if the class distribution is skewed, the frequent class examples can dominate the voting. A way to try to solve this issue is to weight points by the inverse of their distance, which leads to closer neighbors having greater influence than those further away. References Friedman, J., Hastie, T., & Tibshirani, R. (2009). The elements of statistical learning (2nd edition). New York: Springer Series in Statistics. Wikpedia: k-nearest neighbors algorithm
