[site]: crossvalidated
[post_id]: 526980
[parent_id]: 526873
[tags]: 
The typical way of training RNNs (and all NNs) is that you only run forward and backward pass on the training data batch only once, i.e., Do a forward pass and compute the loss; Do a backward pass and compute the loss derivate w.r.t. all trainable parameters. The deep learning frameworks can even accumulate the gradients over multiple batches (i.e., it remembers the gradients even if the input data is no longer loaded on the GPU). The same way the gradient from RNN time steps is accumulated. The optimizer step is done on the accumulated gradients.
