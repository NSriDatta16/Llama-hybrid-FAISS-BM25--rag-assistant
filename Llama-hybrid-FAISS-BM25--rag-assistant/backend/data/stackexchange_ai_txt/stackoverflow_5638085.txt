[site]: stackoverflow
[post_id]: 5638085
[parent_id]: 5637828
[tags]: 
Sometimes aggressive optimisation can break code just like you mentioned. If this is a project you are currently working on, then perhaps this is not a problem. However, if the code in question is legacy code that is fragile, poorly written, and not well-understood, then you want to take as few chances as possible. Also, not all optimisations are formally proven. That means that they may alter the behaviour of programs in undesirable ways. The best example I can think of is a Java one, but it should illustrate my point about optimisations in general. It is common to have code like this while( keepGoing ){ doStuff(); } Then value of keepGoing gets modified by another thread. Well one optimisation that the JVM will do, is see that keepGoing is not modified within the body of the loop, so it "elevates" it and checks before the loop, essentially transforming the code into: if( keepGoing ){ while( true ){ doStuff(); } } Which in a multi-threaded environment is not the same thing, but in a single-threaded it is. These are the kinds of things that can break with optimisations. This is a frequent source of " Heisenbugs ". PS- In Java the proper answer is the make keepGoing " volatile " so it cannot presume cached values and would do what you intend.
