[site]: crossvalidated
[post_id]: 217913
[parent_id]: 217906
[tags]: 
For the probabilities that "look the same", either there is not enough information in your data to better distinguish the two classes, and thus the prediction is mostly based on the number of instances rather than on the features informations, or you do not give enough power to XGBoost to fit the dataset. From your question, I think the latter is more likely. In order to pick more meaningful parameter values, you need to read a bit about what XGBoost is doing and what it means to tweak each parameter. From your code, and without more information, some of your parameter choices seem odd. I would advise you to stick to the defaults , analyse your results (bias/variance decomposition) and work from there. Some comments on your parameter choices Eval metric The evaluation metric you choose, merror , is the multiclass classification error rate. Since you have only two classes, it seems weird to use this one. Moreover, the error rate is not a stable error measure (it can jump from a very small change), so this may explain why your train-merror is all over the place. Pick the traditionnal logloss , unless you have good reasons to do otherwise. Tree Depth ( max_depth , subsample , colsample ) This is gradient boosting, not random forests. You should not use few, deep uncorrelated trees but lots of very small trees. On the max depth, the parameter doc recommends a depth of 6, contrasting with your 15. Note that the number of nodes is $2^{\text{depth}}$, so the difference in complexity is huge. I would go so far as starting with a depth of 1-3, and change it only if you have good reasons to do so. The same goes for subsampling - the goal of subsampling is to reduce variance of the model, leave it to the default (no subsampling) unless you have good reasons to do otherwise. Learning rate ( eta , nrouds ) The learning rate ( eta ) defines how "quickly" you learn. A higher learning rate mean learning faster, more rough distinction between your two classes whereas a smaller eta means learning more slowly and carefully. Your value of 0.05 is really small, and with only 25 trees, you are not able to learn something useful. For the number of rounds, you should aim for your XGBoost to stabilize at some point in the process (not using the number of rounds as a regularization parameter), so push it until the last 10% of rounds seems useless.
