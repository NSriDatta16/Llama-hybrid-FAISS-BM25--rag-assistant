[site]: crossvalidated
[post_id]: 255651
[parent_id]: 255633
[tags]: 
Text mining is a very wide field of research. Many approaches exist, using different kind of analysis. Then your question might be unclear since you don't give any detail. I will try to give you an overview. You're right when you say that we usually preprocess the texts at first. This is based on assumptions: is the punctuation meaningful ? you I split words connected by a dash sigh ? by an underscore ? Should I cut sentences ? Should I use n-grams ? These are your choices. You talk about term document matrix. We usually use TF-IDF to compute the words importance in the corpus of documents. After this step, you have a matrix where each row is a document, each column is a word. The elements of the matrix represent the importance of the corresponding words in the particular document. Then several methods exist to capture the proximity of two words. One is called Latent Semantic Analysis. The idea is to decompose the previous matrix with the singular value decomposition (SVD). You get a matrix of lower dimension in which we can consider similar vectors (using cosine similarity for example) representing similar documents. This similarity could be computed in the the original space (our first matrix). But the SVD compressed the information in a way that words description overlap on the reduced dimensions, such that similar words have similar vectors. Finally, when you compare two vectors in the TF-IDF space you get no similarity: v1 = [0,0,0,0.48,0,0,0,0,0,0,0,0,0,0,0] v2 = [0,0,0,0,0,0,0,0,0,0,0.34,0,0,0,0] cosine(v1,v2) = 0 Whereas if you look at their compressed version, you can get their similarity: v1 = [0.12,0.24,0.64] v2 = [0.10,0.30,0.60] cosine(v1,v2) = 0.468 To answer your question: the relative position of the two words doesn't affect word association. Actually, two words can be very close and never appear in the same document. But if their context (the others words in the documents) are similar, they will be close in the reduced space. Please read this article if you want to know more. At the end, if you want to capture sequences of words, you'd better look at recurrent or convolutional neural networks for text mining or Markov processes that attempt to learn the underlying generative process to build sentences. In both technique, I don't see why term document matrix would be useful since it doesn't capture sequences of words.
