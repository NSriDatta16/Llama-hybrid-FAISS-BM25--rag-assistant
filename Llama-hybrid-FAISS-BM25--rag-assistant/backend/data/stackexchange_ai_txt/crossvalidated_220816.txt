[site]: crossvalidated
[post_id]: 220816
[parent_id]: 
[tags]: 
Why training baseline of REINFORCE by MSE?

In following references [1] V. Mnih et al., "Recurrent Models of Visual Attention", NIPS, 2014 [2] http://torch.ch/blog/2015/09/21/rmva.html [3] M. Ranzato et al., "Sequence level training with Recurrent Neural Network", ICLR, 2016 they applied REINFORCE algorithm to train RNN. To reduce variance of the gradient, they subtract 'baseline' from sum of future rewards for all time steps. According to Appendix A-2 of [4]. W. Zaremba et al., "Reinforcement Learning Neural Turing Machines", arXiv, 2016 this baseline is chosen as expected future reward given previous states/actions. My question is training method to get 'baseline'. They train 'baseline' at each time by linear regression (i.e. objective = Mean Square Error) which takes hidden state of RNN as input. How come this training method make sense? Is there anyone to provide relevant reference? I really appreciate for your comments.
