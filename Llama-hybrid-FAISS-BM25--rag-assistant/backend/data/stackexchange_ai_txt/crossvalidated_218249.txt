[site]: crossvalidated
[post_id]: 218249
[parent_id]: 
[tags]: 
Fast RTRL(Real Time Recurrent Learning) for RNN

Assume generic RNN has update formula: $\mathbf{h}_{t+1} = f(\mathbf{x_t},\mathbf{h_t},\mathbf{\theta})$ [1] Where $\mathbf{x}$ is input vector, $\mathbf{h}$ is hidden state vector, and $\theta$ is network parameters. In RTRL,one need to maintain gradient (or jacobian matrix) of $\mathbf{h}$ w.r.t $\mathbf{\theta}$ at each step of iteration. $G_{t+1} = \frac{\partial f(x_t,h_t,\theta)}{\partial h}G_t+\frac{\partial f(x_t,h_t,\theta)}{\partial \theta}$ [2] Since $G$ often takes $O(n^3)$ memory ($n$ is number of hidden units), plus expensive GEMMs, RTRL in this form is rarely used in practical applications. Instead of using approximation methods like here , is it possible to design a $f$ such that equation [2] is fast to compute? Preferably no need to maintain $G$ at all. For example, if certain $f$ satisfies: $\frac{\partial f(x_t,h_t,\theta)}{\partial h}G_t=\lambda \frac{\partial f(x_t,h_t,\theta)}{\partial \theta}$ [3] Then there would be no need to maintain $G$ at all. Intuitively, the memory of such RNN should contain complete information to compute full gradient for all previous timesteps. This will allow RNN to be trained with one-step-backpropagation. UPDATE: For univariete case ($x$, $h$, $\theta$ are scalars), the following function satisfies [3]: $f(x,h,\theta) = c_1(x,\theta) h^{c_2(x,\theta)}$ where $c_1$ and $c_2$ are differentiable function w.r.t $x$ and $\theta$. This seems to suggest a "perfect memory" RNN should take power function form. I'm not a mathematician, I'm not sure what's the best way to extend this result to multivariete case while handling long term dependence well.
