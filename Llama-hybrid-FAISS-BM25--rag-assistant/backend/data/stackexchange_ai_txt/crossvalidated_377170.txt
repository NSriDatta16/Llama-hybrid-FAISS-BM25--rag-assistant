[site]: crossvalidated
[post_id]: 377170
[parent_id]: 377153
[tags]: 
The most natural approach to handle your situation would be a mixed effects or multilevel regression. You can also perform an inverse variance weighted regression. You have 100 benchmarks for both algorithms. Additionally, you plan to attempt each benchmark about 50 times to account for randomness in the benchmark results. Inverse variance weighting I'll begin with the inverse variance approach since it is easier. Assuming that the data for the 50 instances of a benchmark for an algorithm are reasonably normal, then the data can be summarized by its mean and variance. So for each benchmark and algorithm, calculate the mean and the variance of the 50 instances. And you will arrive at 100 average benchmarks for each algorithm with 100 variances. The averages become the outcome for a regression model where you correlate the scores by benchmark to account for the fact that each benchmark produced two averages. You also need to supply the variances as weights to the regression. Assume the benchmark averages are score , benchmark unique identifier is bench_id , algorithm indicator or dummy variable is algo , and benchmark variances are b_var , all contained in the dataset, dat . dat should be 200 rows long, then: library(lme4) res Should work. You'll get the difference between algorithms from its coefficients, and you'll get a bench random intercept standard deviation which I'll call $v$ , and a residual standard deviation which I'll call $s$ . If you perform: $$\frac{v^2}{v^2+s^2}$$ You'll get the average correlation between measurements from the same benchmark, this is known as the (residual) ICC. The syntax should be: as.numeric(VarCorr(res)) / (as.numeric(VarCorr(res)) + sigma(res) ^ 2) Complete multilevel setup This time, you have all the data in long form. Each row pertains to one of 50 measurements for an algorithm for one of 100 benchmarks. So your number of rows should be $10000\ (50\times 100\times 2)$ . The model you'll run still has a similar syntax to the 10000 long dataset: res1 I would prefer to run: res2 This would allow you to observe the correlation between the scores from both algorithms and test whether the algorithms vary differently. Of the three models, I prefer the first and the last. The first is a simple approach and if the normality assumption for the 50 instances per benchmark per algorithm is satisfied, then it is reasonable and defensible. Alternatively, the final model uses all the data in the most natural way. However, it assumes homogeneity of variance which the first approach does not. Final model assumes the variance of the 5000 measurements within each algorithm is the same across the 100 benchmarks. Hope this helps. It might be useful to pick up a basic guide to multilevel modeling using lme4 if you are not familiar with such models. For your problem, especially if you stick with the first approach, an introductory guide is sufficient. EDIT Assuming the dataset is called data.long , then: # to create dataset of mean scores dat $b_var score Then use dat for model 1 syntax above. This dataset will have only 14 rows since you currently have only 7 bench_ids. Try model 3 when you get more data with your combined.data .
