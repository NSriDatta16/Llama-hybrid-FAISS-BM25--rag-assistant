[site]: crossvalidated
[post_id]: 197826
[parent_id]: 
[tags]: 
Why isn't logistic regression learning the correct weights for a Y variable defined as logit of X and weights?

I am trying to see if the implementation of logistic regression in my is correct. In order to do that I first started to generate some random X data and specify some predefined weights. When I train the model using a standard implementation of Logistic regression the model doesn't learn the correct pre-specified weights. Am I doing something wrong in my implementation. Here what my code looks like: import numpy as np import statsmodels.api as sm X = sm.add_constant(np.random.randn(1000,3)*10) w = np.array([0.25, 0.1, 0.3, -1.5]) def logit(x): return 1. / (1. + np.exp(x)) y = logit(np.dot(X,w) + np.random.randn(X.shape[0])*0.0001) # y ~ logit(X.w + Random Noise) model = sm.Logit(y>0.5, X) # Label y as 1 if y > 0.5 else 0 res = model.fit() print res.summary2() Here is the output I get from my model: Warning: Maximum number of iterations has been exceeded. Current function value: 0.000000 Iterations: 35 Results: Logit ================================================================== Model: Logit Pseudo R-squared: 1.000 Dependent Variable: y AIC: 8.0000 Date: 2016-02-21 18:08 BIC: 27.6310 No. Observations: 1000 Log-Likelihood: -6.5515e-06 Df Model: 3 LL-Null: -692.70 Df Residuals: 996 LLR p-value: 4.3502e-300 Converged: 0.0000 Scale: 1.0000 No. Iterations: 35.0000 ------------------------------------------------------------------ Coef. Std.Err. z P>|z| [0.025 0.975] ------------------------------------------------------------------ const -28.4569 1024.9463 -0.0278 0.9779 -2037.3148 1980.4009 x1 -13.8000 411.2444 -0.0336 0.9732 -819.8241 792.2242 x2 -40.6588 1204.5892 -0.0338 0.9731 -2401.6104 2320.2927 x3 201.4672 5953.2122 0.0338 0.9730 -11466.6144 11869.5488 ================================================================== What am I doing wrong? Shouldn't the model learn the coefficients as the weights I specified above ?
