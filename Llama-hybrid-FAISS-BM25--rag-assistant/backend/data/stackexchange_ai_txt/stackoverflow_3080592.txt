[site]: stackoverflow
[post_id]: 3080592
[parent_id]: 
[tags]: 
Storage of parsed log data in hadoop and exporting it into relational DB

I have a requirement of parsing both Apache access logs and tomcat logs one after another using map reduce. Few fields are being extracted from tomcat log and rest from Apache log.I need to merge /map extracted fields based on the timestamp and export these mapped fields into a traditional relational db ( ex. MySQL ). I can parse and extract information using regular expression or pig. The challenge i am facing is on how to map extracted information from both logs into a single aggregate format or file and how to export this data to MYSQL. Few approaches I am thinking of 1) Write output of map reduce from both parsed Apache access logs and tomcat logs into separate files and merge those into a single file ( again based on timestamp ). Export this data to MySQL. 2) Use Hbase or Hive to store data in table format in hadoop and export that to MySQL 3) Directly write the output of map reduce to MySQL using JDBC. Which approach would be most viable and also please suggest any other alternative solutions you know.
