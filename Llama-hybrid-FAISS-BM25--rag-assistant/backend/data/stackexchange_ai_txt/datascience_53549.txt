[site]: datascience
[post_id]: 53549
[parent_id]: 
[tags]: 
Learning Embeddings for One Word

I have a non-conventional NLP task. I am looking to develop a sequence to a vector model. Instead of employing one-hot encoding for vectorization of the input, I am trying to see if it will be possible to learn to embed for the input text which is usually only one non-human word. For example: 1. GHJJRIDBDL7US = positive 2. LDNF3DM ULMFiT is the state of the art now. Any idea if and how it can be applied to this problem? Any idea on how to learn the embedding for such one-word data will be helpful.
