[site]: crossvalidated
[post_id]: 437231
[parent_id]: 
[tags]: 
Softmax function makes my machine to train much slower

I have two machines: CNN without softmax function and CNN with softmax function. But softmax function makes my machine to learn much slower and less accurate. Does anyone know why this happens? Here's my code: class CNN(nn.Module): def __init__(self): super().__init__() self.layer = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5), # [batch_size,1,28,28] -> [batch_size,16,24,24] nn.ReLU(), nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5), # [batch_size,16,24,24] -> [batch_size,32,20,20] nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,32,20,20] -> [batch_size,32,10,10] nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5), # [batch_size,32,10,10] -> [batch_size,64,6,6] nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2) # [batch_size,64,6,6] -> [batch_size,64,3,3] ) self.fc_layer = nn.Sequential( nn.Linear(64*3*3, 100), nn.ReLU(), nn.Linear(100, 10) ) def forward(self, x): out = self.layer(x) out = out.view(batch_size, -1) out = self.fc_layer(out) # out = nn.functional.softmax(out, dim=-1) # This is only different between CNN with softmax and the other one. return out I experimented the machines with the same data (MNIST) and accuracies of the models are 98% and 88%, respectively. Here are the losses: # CNN with softmax losses: cuda:0 tensor(2.3021, device='cuda:0', grad_fn= ) tensor(1.5593, device='cuda:0', grad_fn= ) tensor(1.5226, device='cuda:0', grad_fn= ) tensor(1.5126, device='cuda:0', grad_fn= ) tensor(1.4980, device='cuda:0', grad_fn= ) tensor(1.4964, device='cuda:0', grad_fn= ) tensor(1.4775, device='cuda:0', grad_fn= ) tensor(1.4848, device='cuda:0', grad_fn= ) tensor(1.4889, device='cuda:0', grad_fn= ) tensor(1.4862, device='cuda:0', grad_fn= ) # CNN without softmax: tensor(2.3156, device='cuda:0', grad_fn= ) tensor(0.2475, device='cuda:0', grad_fn= ) tensor(0.1152, device='cuda:0', grad_fn= ) tensor(0.1303, device='cuda:0', grad_fn= ) tensor(0.0699, device='cuda:0', grad_fn= ) tensor(0.0459, device='cuda:0', grad_fn= ) tensor(0.0548, device='cuda:0', grad_fn= ) tensor(0.0266, device='cuda:0', grad_fn= ) tensor(0.0422, device='cuda:0', grad_fn= ) tensor(0.0225, device='cuda:0', grad_fn= )
