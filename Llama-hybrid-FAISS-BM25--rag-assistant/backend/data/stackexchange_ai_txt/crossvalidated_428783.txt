[site]: crossvalidated
[post_id]: 428783
[parent_id]: 428778
[tags]: 
(Copying my answer from Stack Overflow) The 99 percentile of a union of two sets (even if they are of equal size) cannot be simply deduced from the 99 percentiles of both sets. For example: say that the first set has only 0's, and the second set has only 1's. Therefore, the 99 percentile of the first set is 0, and the 99 percentile of the second set is 1. But in that case, the 99 percentile of their union is 1 - not related to the average of 0 and 1. (Of course, in this example, it is their maximum - but it's not hard to construct a counterexample for that too...) There might be something to say if both sets come from a normal distribution, but if you're looking at the 99 percentiles of some real-world data, then the 99 percentile usually represents outliers that do not have a clean normal distribution.
