[site]: datascience
[post_id]: 81676
[parent_id]: 81654
[tags]: 
1 - The computation time of SGD is much lower than GD as you only use a subset of the whole data, that is why it is actually faster (time-wise) even though it seems you do more stuff. 2- With GD you compute your gradient on all the data you have, therefore the computed gradient gives you the best direction to minimize your function on the whole dataset. With SGD however, each gradient step only uses a subset of the data, the minimization direction is therefore best for this subset but it does not account for all your data. However as you randomly pick samples of your data, in average you will go in the right direction and the more samples you use, the more accurate (but expensive) your gradient is.
