[site]: crossvalidated
[post_id]: 521484
[parent_id]: 
[tags]: 
Handling a big number of Summary Statistics in ABC

I went through a big amount of literature in $ABC$ , in how it is possible to handle a large number of (many cases sufficient) summary statistics. Like a large number, I consider $K>>200$ , where $K$ denotes the number of sufficient summary statistics used in $ABC$ . A such a big number of statistics $K$ can result in a small acceptance rate and really slow/bad mixing of an $MCMC$ algorithm. A potential way that I read that can alleviate that problem is to $1)$ Use a model selection/reduction technique to choose a significantly smaller number of sufficient summary statistics from those $K$ , $2)$ Construct a number of $M$ summary statistics where $M and hopefully $M$ will have a decent amount of information for your data. In my case, using the first method $1)$ I still end up with a big number of sufficient summary statistics. For example, if I have $K=200$ after use of dimensionality reduction I end up with $170$ sufficient statistics where I consider it as a really big number of statistics. So, my questions can be summed up to the following, (I didn't include problem-specific details because I'm more interested if there exist the way of handling such a problems): $i)$ In cases where we have such a large number of (sufficient) summary statistics $K=200$ or $500$ or even larger, are we doomed to have bad $ABC-MCMC$ computations because it is quite unlikely to produce data that are closely related?? $ii)$ I'm might haven't searched well, but are there any research analysis/simulations that consider such a big amount of summary statistics??
