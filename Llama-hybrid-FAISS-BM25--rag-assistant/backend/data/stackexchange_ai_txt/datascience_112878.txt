[site]: datascience
[post_id]: 112878
[parent_id]: 
[tags]: 
How to evaluate Light Graph Convolutional Networks (LightGCN) correctly on sparse binary data?

I implemented the LightGCN at work to recommend k items to users according to the TensorFlow implementation of Microsoft: https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb . Now, I have some evaluation problems: After some epochs, all four validation metrics (recall, precision, mean average precision and ndcg) drop, but also the training loss. I understand that we optimize for recall and that Bayesian Personalised Ranking (BPR) loss seems to be the loss to go for. But still this trend is bothering me, does someone have a clue why this could happen? The prediction column gives me scores, which according to my understanding, represent the dot product of user and item embeddings. Now I would like this to map to either 1 (meaning the item should be recommended to the user) or 0 (item shall not be recommended to the user, such that a confusion matrix could be calculated. Even more, a probabilistic interpretation would be nice, which would allow define a recommendation threshold such that the recommender may also propose zero up to k items depending on the user (at the moment I only see the option of finding experimentally a reasonable quantile on the basis of the score values). Does anyone know by chance if this is possible and how to best achieve this?
