[site]: crossvalidated
[post_id]: 390429
[parent_id]: 390297
[tags]: 
As mentioned by @whuber in comments, these properties are heuristics and they talk about the rates of increase in bias and variance and not the bias and variance with respect to the model complexity. From Wikipedia: In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. And on bias-variance decomposition: The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. So, the property that has been referred to in the question --bias variance tradeoff -- is a property of a set of predictive models, in general. As noted by @whuber the image in the question is a way of demonstrating that property heuristically. And, the bias-variance decomposition is a way to analyze the generalization error of an algorithm for a particular problem.
