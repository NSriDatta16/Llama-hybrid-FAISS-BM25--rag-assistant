[site]: crossvalidated
[post_id]: 83768
[parent_id]: 63081
[tags]: 
From your quote " have a data set that has ~1100 features but a lot of these are redundant data / uncorrelated data " I would strongly recommend feature reduction. Jacob is right that it won't improve the accuracy (in terms of explained variance) in-sample (it will do the opposite) but it will improve reliability of the model out of sample. If you start fitting this type of data raw straight out of the box you are unlikely to come up with anything useful. The SVM itself is nothing more than an optimization on a linear boundary, what makes it powerful are transformations like kernel methods, PCA, etc. that produce data and models that make sense. Have good think about how you want to represent the structure in your data (perhaps first do some unsupervised learning on the raw data to discover structure). And no one can really tell you how to reduce the dimensionality (i.e. extract the salient features) of your problem without actually looking at the data. Once you have made a dent in the feature reduction make sure to do things like cross-validation and random sampling to avoid overfitting.
