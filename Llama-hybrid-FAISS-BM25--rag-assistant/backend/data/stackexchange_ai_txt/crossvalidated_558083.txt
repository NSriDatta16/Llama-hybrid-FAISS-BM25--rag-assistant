[site]: crossvalidated
[post_id]: 558083
[parent_id]: 558059
[tags]: 
Your data consist of nearly 10x more variables than observations, with 65 rows and nearly 732 columns. This is going to be tough, but we will persevere. Because the data have so many variables, you're going to need a method which can deal with a lot of variables and avoid overfitting. There are a few ways to do this, but first we need a baseline measure of performance. Sklearn's DummyRegressor is a good place to start. import numpy as np from sklearn.dummy import DummyRegressor from sklearn.metrics import mean_squared_error, make_scorer from sklearn.model_selection import GridSearchCV, cross_validate from sklearn.linear_model import Lasso from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline from sklearn.ensemble import RandomForestRegressor X = np.loadtxt('https://raw.githubusercontent.com/shoubhikraj/molecular-modelling/main/uv-vis-pred/X_train.txt') y = np.loadtxt('https://raw.githubusercontent.com/shoubhikraj/molecular-modelling/main/uv-vis-pred/y_train.txt') rmse = make_scorer(mean_squared_error, squared=False) results = cross_validate(DummyRegressor(), X, y, scoring = rmse) results['test_score'].mean() >>>46.09 Any model with an RMSE above 46 is not worth consideration, because it seems we can a achieve a lower RMSE just by guessing the sample mean. Let's move on to a linear model. Because we have so many variables, the linear model must either a) project the variables onto a lower dimensional space, and or b) use regularization. Let's use both in the following Lasso model model = Pipeline([ ('scale', StandardScaler()), ('pca',PCA()), ('lm', Lasso()) ]) param_grid = { 'pca__n_components': [3, 5, 7, 9, 11, 40], 'lm__alpha': np.logspace(-3, 3, 5) } gscv = GridSearchCV(model, param_grid=param_grid, cv=10, scoring = rmse) results = cross_validate(gscv, X, y, cv=10, scoring = rmse, verbose=4) results['test_score'].mean() >>>35.75 Awesome, through nested cross validation, the model I've used is estimated to have an out of sample RMSE of 35.75. Its worth considering another model, the random forest. This model is great when we have lots of variables because it randomly selects a subset to create splits. Here are the cross validation results. model = RandomForestRegressor(n_estimators=5000, max_features=1) results = cross_validate(model, X, y, cv=10, scoring = rmse) results['test_score'].mean() >>>37 Promising results. I bet with a grid search cross validation we could do better. Additionally, I would perhaps try boosted trees as they seem to do very well. Hope this helps.
