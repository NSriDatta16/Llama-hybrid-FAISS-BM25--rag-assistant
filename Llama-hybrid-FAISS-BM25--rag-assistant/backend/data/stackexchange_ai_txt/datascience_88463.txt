[site]: datascience
[post_id]: 88463
[parent_id]: 65502
[tags]: 
After much consideration, I reached out the following points: Visualization of the first prediction's explanation shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:]) according to this doc shows: 1. features pushing the prediction higher are shown in red (e.g. $\text{SHAP}_\text{day_2_balance} = 532$ ), those pushing the prediction lower are in blue (e.g. $\text{SHAP}_\text{PEEP_min} = 5$ , $\text{SHAP}_\text{Fi02_100_max} = 50$ , etc.) when $\text{Model}_\text{predicted output} = -2.92$ for your binary classification model. 2. Apart from @Sarah answer, the scale of SHAP values based on the discussion in this issue could transform via inverse_transform() as follows: x_scaler.inverse_transform(shap_values) 3. Based on Github the base value : The average model output over the training dataset has been passed $\text{Model}_\text{Base value} = 0.6427$ In other words, it is the mean prediction which can be computed by: Y_test.mean() 4. They haven't been shown due to their contributions to the quality rating are nearly zero. 5. Yes. This amount of contribution (magnitude) also has been highlighted under "Feature impact" in Figure 4 of the paper from Lundberg et al. [Nature BME] slightly better on the feature named "Total volume" with annotation as follows: in your case feature contributions to push the prediction to the left (lower end) for a certain observation: $\text{Feature impact}_\text{PEEP_min} > \text{Feature impact}_\text{Fi02_100_max} > \text{Feature impact}_\text{MV_dur_vae} > etc.$ 6. Features have a positive impact pushing the prediction higher ( are indicated in red), those have a negative impact pushing the prediction lower ( are shown in blue). Note: It is possible to compute the mean of variables using: X_train.mean() Although this issue is under discussion, from my personal perception it depends on their positive or negative impact local values. It would drive the prediction to the left or right for a certain observation. $\text{SHAP}_\text{predicted} > \text{variable}_\text{mean}$ or $\text{SHAP}_\text{predicted}
