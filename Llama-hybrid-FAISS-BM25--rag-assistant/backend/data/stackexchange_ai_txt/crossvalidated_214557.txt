[site]: crossvalidated
[post_id]: 214557
[parent_id]: 214535
[tags]: 
Often we don't know the population variance as such - but we have a very reliable estimate from a different sample. For instance, here is an example on assessing whether average weight of penguins has gone down, where we use the mean from a small-ish sample, but the variance from a larger independent sample. Of course, this presupposes that the variance is the same in both populations. A different example might be classical IQ scales. These are normalized to have a mean of 100 and a standard deviation of 15, using really large samples. We might then take a specific sample (say, 50 left-handed redheads) and ask whether their mean IQ is significantly larger than 100, using 15^2 as a "known" variance. Of course, once again, this begs the question whether the variance is really equal between the two samples - after all, we are already testing whether means are different, so why should variances be equal? Bottom line: your concerns are valid, and usually tests with known moments only serve didactic purposes. In statistics courses, they are usually immediately followed with tests using estimated moments.
