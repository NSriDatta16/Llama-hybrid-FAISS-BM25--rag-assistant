[site]: crossvalidated
[post_id]: 608648
[parent_id]: 436154
[tags]: 
Suppose that the training process is working & the loss is decreasing during an epoch. If this is the case, then we know that the average loss at the beginning of the epoch will be larger than the average loss at the end of the epoch. This means you have to make a choice: As a result, if you store all of the minibatch losses computed during the epoch, then compute the average, that average will be biased upwards, towards the loss value at the beginning of the epoch. The extent to which this matters depends on the magnitude of the difference between the loss at the beginning of the epoch and the loss at the end of the epoch. The closer together the beginning loss and end loss are, the smaller the bias will be. If you discard the minibatch losses computed during the epoch & recompute the loss for all samples at the end, then you're increasing the computational cost of each epoch because you're passing the training data to the model twice. You'll have to decide which one is the best fit for your needs. If you have a tight budget, (1) might make more sense. If you have a great need to precisely measure the training loss, then (2) might be better.
