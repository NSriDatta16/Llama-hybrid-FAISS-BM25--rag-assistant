[site]: crossvalidated
[post_id]: 422545
[parent_id]: 422502
[tags]: 
You are correct that $Z\mid \mu_x,\mu_y,\sigma^2_x,\sigma^2_y \sim \mathcal{N}(\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)$ . First note that this likelihood is not identifiable. This is because $\mu_x,\mu_y$ will evaluate to the same function as $\mu_x-1,\mu_y+1$ . Actually, you can add and subtract any number and get the same likelihood. This is why the likelihood has a “ridge.” However, the posterior is identifiable. This means that any kind of inference about a single parameter will definitely be very sensitive to the prior you choose. Assuming all of the variances are known, \begin{align*} p(z \mid \mu_x, \mu_y) &\propto p(z \mid \mu_x, \mu_y) p(\mu_x, \mu_y) \\ &\propto \exp\left[-\frac{(z- \mu_x - \mu_y)^2}{2\sigma^2_x + 2 \sigma^2_y} \right] \exp\left[-\frac{(\mu_x - m_0^x)^2}{2s_0^x} \right]\exp\left[-\frac{(\mu_y - m_0^y)^2}{2s_0^y} \right] \\ \end{align*} which is proportional to the exponential of $$ -\frac{1}{2(\sigma^2_x + \sigma^2_y)(s_0^x s_0^y)}\left\{ (z- \mu_x - \mu_y)^2s_0^xs_0^y + (\mu_x - m_0^x)^2s_0^y(\sigma^2_x + \sigma^2_y) + (\mu_y - m_0^y)^2s_0^x(\sigma^2_x + \sigma^2_y) \right\}. $$ This means that your posterior is bivariate normal , and after you collect the terms with $\mu_x\mu_y$ , you'll see you'll get a negative correlation. This makes intuitive sense, because if $\mu_x$ is really "big", then that will probably mean $\mu_y$ is really "small." Here's some R code that helps show this. It plots the log of the unnormalized posterior # library(devtools) # devtools::install_github("tbrown122387/mmcmc") library(mmcmc) logUnNormPosterior Here's the plot:
