[site]: crossvalidated
[post_id]: 200304
[parent_id]: 
[tags]: 
Inferring variable contribution to variance explained for random forests

I am trying to understand if there is a way to approximate what portion of the variance explained is being contributed by each independent variable in a random forest model. Just for illustration, I am borrowing the following model from the Stanford StatLearning class notes. This builds a random forest model for predicting median housing prices in Boston using the dataset provided with the MASS package. require(randomForest) require(MASS) set.seed(101) dim(Boston) train=sample(1:nrow(Boston),300) Fitting the model (just using a simple model here without any validation just for illustration) rf.boston=randomForest(medv~.,data=Boston,subset=train) rf.boston I get the following output Call: randomForest(formula = medv ~ ., data = Boston, subset = train) Type of random forest: regression Number of trees: 500 No. of variables tried at each split: 4 Mean of squared residuals: 12.34243 % Var explained: 85.09 Now R tells me that this model explains 85.09% variance in median housing prices. Additionally, I can run the importance command to figure out what variables turned out to be "significant" in my model. importance(rf.boston) IncNodePurity crim 1487.1777 zn 142.0280 indus 965.7756 chas 234.6918 nox 1741.9305 rm 7435.3378 age 655.6031 dis 1357.3411 rad 316.3278 tax 794.0953 ptratio 1858.7183 black 455.5382 lstat 6947.9121 Is there a way to use these two pieces of information (or using some other approach) to tell us what percentage of 85.09 was explained by crim , zn and so on. My goal here is to show this as a 100% stacked bar graph ordered by variable importance illustrating major drivers of the dependent variable (median housing prices in this example). Overall, I want to see if we can get outputs akin to shapely value regression as shown here (esp slide 21) using random forests.
