[site]: crossvalidated
[post_id]: 466458
[parent_id]: 
[tags]: 
What is the advantages of minimizing Wasserstein divergence rather than Pearson divergence in a GAN?

Generative Adversarial Networks (GANs) are generative models that jointly train two neural networks: a discriminator , that learns to say apart real data from generated data, and a generator , that learns to produce synthetic data that is realistic enough to fool the discriminator. Many types of GANs have been proposed, generally minimizing slightly different cost functions. For example, Least Square GANs minimize the Pearson divergence between real and fake data, while Wasserstein GANs minimize Wasserstein divergence. I wonder what are the advantages of using one divergence instead of the other. This also raises questions as: can we use gradient penalty with Least Square GANs?
