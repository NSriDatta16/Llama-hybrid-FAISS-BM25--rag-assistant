[site]: crossvalidated
[post_id]: 361152
[parent_id]: 
[tags]: 
XGBoost Poisson Objective Function When Data is Over-dispersed

I am modeling very over-dispersed count data with the goal of prediction. The data is not zero inflated (there are no zeros), but there are a lot of values of 1. > summary(sample$pass) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 2.00 8.00 75.85 24.00 36609.00 > var(sample$pass) [1] 387219.8 Log transforming the outcome still results in a very "count-ey" (low value heavy, and not bell-shaped or symmetric) distribution, although I should note that when I log transform the counts, the mean is pretty close to the variance and it is more classically poisson looking. In GLM-world, I know I can model this using a quasi-poisson or a negative binomial GLM to facilitate modeling the over-dispersion, but I am trying to implement gradient boosting using the xgboost package (in R) which gives options of using learning objective functions reg:linear , reg:logistic , reg:gamma , count:poisson (but not quasi-poisson or negbin) and uses appropriate corresponding loss functions. For the count:poisson objective (which is what I am currently using), it uses a negative log-likelihood evaluation metric (see documentation and this answer ). Is it appropriate to use this poisson:count objective function when implementing gradient boosting and the poisson negative log-likelihood evaluation metric considering my high over-dispersion? Perhaps it does not matter to the training of the model that the outcome is over-dispersed because the error eval metric (poisson negative log-likelihood) is only concerned with the mean, and does not use information on the variance to evaluate probability of model predictions (although not totally confident that this is true, so please correct me if I am wrong)? Also, how might the over-dispersion affect my prediction accuracy? I should note that it is important that my final predictions are only positive values. Thank you!
