[site]: crossvalidated
[post_id]: 304580
[parent_id]: 
[tags]: 
Taylor expansion in xgboost

I'm reading through the math of xgboost: https://xgboost.readthedocs.io/en/latest/model.html Under the ADDITIVE TRAINING section of the objective function, I saw that in the derivation of the objective at step $t$, a claim is made about how the GENERAL form of the objective function for any loss function (MSE, logistic loss, etc.) takes on the form of a 2nd order Taylor expansion. How do we know this is true in general? Is there a mathematical explanation of this that I'm just not seeing? I basically want to know how we are able to conclude that the following is the GENERAL form of the objective at step $t$: $$\text{obj}^{(t)} = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + constant$$ where the $g_i$ and $h_i$ are defined as: $$\begin{split}g_i &= \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})\\ h_i &= \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)}) \end{split}$$
