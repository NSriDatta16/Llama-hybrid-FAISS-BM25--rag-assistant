[site]: crossvalidated
[post_id]: 360818
[parent_id]: 
[tags]: 
Can I combine many gradient boosting trees using bagging technique

Based on Gradient Boosting Tree vs Random Forest . GBDT and RF using different strategy to tackle bias and variance. My question is that can I resample dataset (with replacement) to train multiple GBDT and combine their predictions as the final result? It is equivalent to build random forest using GBDT as the base learner The idea is that, GBDT can overfit dataset (similar to fully grow decision tree, low bias high variance). I hope that using bagging technique can also reduce this problem and wish to get better performance. Any suggestion?
