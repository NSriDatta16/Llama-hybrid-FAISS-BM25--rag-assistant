[site]: datascience
[post_id]: 96925
[parent_id]: 
[tags]: 
Which layer actually contains attention and how to map with input for classification scenario

I am writing a binary classification case and want to use Attention. The code is here - Code and model graph is shown below with attention area highlighted . I want to visualize which which word is being paid attention by which hidden layer value. Now my questions are: Which block in highlighted area should be used to match the attention with respect to input tokens? I have tried attention_weight as the size is same as of input layer. But the attention is values are same in each 200 values. I have tried attention_vector (Size 80) which contains different values for 80 cells. However, as this is not a Encoder-Decoder scenario, where multiple timestamps are representing different input tokens, my confusion is How to map hidden state value with tokens when both are present together. e.g. token=[1,4,2,10,3] attention_vector=[0.23, 0.11, 0.55] .. what kind of attention can be observed in these 2 vectors?
