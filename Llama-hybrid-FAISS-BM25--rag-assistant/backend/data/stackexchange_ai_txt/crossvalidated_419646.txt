[site]: crossvalidated
[post_id]: 419646
[parent_id]: 
[tags]: 
Bootstrapping for Control Variates

TLDR: I want to do Monte Carlo with control variates I work in the setting where you use Monte Carlo sampling to approximate the optimal coefficients for the control variates. I want to retain the unbiasedness of my estimator I don't want to waste my samples by forming an estimator where some of the samples are used to find the optimal coefficients, and the remaining samples are used to find the estimator itself. I want to know whether this exists already, and where in the literature I should look for further details. The more detailed / equation-heavy version: Suppose I'm interested in using Monte Carlo simulation to estimate \begin{align} \phi = \mathbf{E}_\mu [ f (X) ]. \end{align} Implicitly, I assume that drawing samples from $\mu$ is easy. Assume also that I'm only interested in deriving an unbiased estimator of $\phi$ . Suppose moreover that I know a function $c$ such that $\mathbf{E}_\mu [ c (X) ] = 0$ , and thus plan to use it as a control variate, by noting that \begin{align} \phi = \mathbf{E}_\mu [ f (X) ] = \mathbf{E}_\mu [ f (X) - \beta \cdot c (X) ] \quad \text{ for any } \beta \in \mathbf{R} \end{align} To obtain an estimator with minimal variance among such estimators, it can be shown that the optimal $\beta$ is given by \begin{align} \beta^* = \frac{ \textbf{Cov}_\mu ( f(X), c(X) ) }{ \textbf{Var}_\mu ( c(X))}. \end{align} However, it will usually be the case that $\beta^* $ is not known exactly, and so one might proceed as follows: Use $N_1$ samples from $\mu$ to form an approximation $\hat{\beta} (X_1, \cdots, X_{N_1} ) \approx \beta^* $ . (e.g. by taking a ratio of sample variance and sample covariance) Use another $N_2$ samples from $\mu$ to form the estimator \begin{align} \hat{\phi} = \frac{1}{N_2} \sum_{i=1}^{N_2} \left\{ f (X_i) - \hat{\beta} \cdot c ( X_i ) \right\}. \end{align} A benefit of using this two-stage approach is that $\hat{\phi}$ remains unbiased for $\phi$ . However, it seems fundamentally wasteful; one is not really using all of the information available. An alternative would be the following bootstrap-type procedure: Generate $(N_1 + N_2)$ samples from $\mu$ . For all possible partitions of $\{1, 2, \cdots, (N_1 + N_2) \}$ into a set $S_1$ of size $N_1$ and a set $S_2$ of size $N_2$ Use $\{ X_i \}_{i \in S_1}$ to form $\hat{\beta} ( S_1 ) \approx \beta^*$ Use $\{ X_i \}_{i \in S_2}$ to form \begin{align} \hat{\phi} (S_2) = \frac{1}{N_2} \sum_{i \in S_2} \left\{ f (X_i) - \hat{\beta} ( S_1 ) \cdot c ( X_i ) \right\}. \end{align} Form the final estimator $\hat{\phi}$ by averaging over all of the $\hat{\phi} (S_2)$ . The resulting $\hat{\phi}$ is then also unbiased for $\phi$ , but uses more of the information from the samples, and so should be expected to e.g. exhibit lower variance. The flip-side is that one now has to average over ${N_1 + N_2} \choose {N_1}$ estimators, which could become costly. It might be possible to alleviate this by only looking at a random subset of the possible partitions; in any case, this is not what I'm curious about right now. My questions is then: Does a procedure like this exist in the literature already? If so, I would appreciate being pointed to the relevant references, keywords, and so on. I'm particularly interested in knowing whether one can get a provable reduction in variance by using a procedure of this form.
