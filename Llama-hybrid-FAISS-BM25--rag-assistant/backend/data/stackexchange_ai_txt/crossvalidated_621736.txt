[site]: crossvalidated
[post_id]: 621736
[parent_id]: 621715
[tags]: 
If you want "insights" you should use a model that is interpretable by itself. When you fit XGBoost, it learns to approximate the distribution of the data. When you use SHAP to explain the results, it is a black box, it approximates the results obtained by XGBoost. So you end up with two layers of approximation. Also, SHAP provides only the local explanations of the individual predictions, by itself it won't pinpoint the global "physics of the system". So there comes a third layer of approximation, where you would need to interpret the results of SHAP and derive the global explanations from them. Moreover, the whole procedure does nothing to prevent overfitting, so the scenario where your model learns to mimic the training data but is not able at all to generalize outside it. Why not use something like good, old linear regression? It is directly interpretable, so it avoids most of the problems mentioned above.
