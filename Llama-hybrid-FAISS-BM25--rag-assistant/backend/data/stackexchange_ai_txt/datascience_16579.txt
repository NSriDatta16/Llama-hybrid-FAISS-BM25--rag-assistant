[site]: datascience
[post_id]: 16579
[parent_id]: 14264
[tags]: 
This is quite old, but I'll try to explain. You need labelled data if you wish to attempt supervised learning. Assemble a list row with key-value pairs that denote 'events' relevant to your problem: row { company: , company_attribute_A: , company_attribute_B: , company_attribute_C: , ..., tender_attribute_A: , tender_attribute_B: , tender_attribute_C: , ..., company_participation: } You wish to find out whether company X with attributes A,B,C will participate in a tender with its own attributes, so the training data is used to model the tender behaviour of a certain company by statistically analysing its history. Historical might only include 'positive' values: events that did happen. An idea might be to artificially add some events that do exist but in which the company did not participate, so as to include 'negative' values as well. For a company Y, you could do it like this: Take a historical event that DID happen for company X Check if company Y also participated in this event If so: ignore this case (it's also positive for company Y). If not: Add this same row to the list of data but add company Y's attributes to it and change the class value (the binary participation value) to 0. Try to mimic the actual distribution when adding negative data. Historically speaking, how likely are companies to participate? 50/50? 10/90? When you have this data set, you can divide it into training data (80%?), development data (10%?) and test data (10%?). Train your model on the training data. Move your test data somewhere where you can't touch it. Experiment and improve your model by testing it on the development data. Finally, when you're happy with the results or when you have to publish the findings of your model, test it on the withheld set of test data. Honestly, the selection of models and algorithms is secondary. The most important aspect of your problem is the set of attributes ( features ) you choose to collect. This is the most human part of data modelling / machine learning, because this is where you decide what you want to teach the machine. How you want to teach it (what algorithm or model you'll use) is also important, but not as important as the content. Try using simple models such as kNN, Bayesian flavours or Logistic Regression. This gives you a good overview of how well chosen your attributes are. In your case, I would investigate models that output probabilities. You mentioned SciKit. Check out this overview to see which classifiers support probability scores.
