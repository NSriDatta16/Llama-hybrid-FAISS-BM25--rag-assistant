[site]: crossvalidated
[post_id]: 177293
[parent_id]: 
[tags]: 
Does principal components analysis lose any information regarding the interdependence of the variables?

I have often heard that a copula describes in full the interdependence of a set a random variables. Lets say I want to generate a set of random variables that conform to an observed joint probability distribution. My first thought would be to estimate the correlation matrix, do a PCA to identify the most significant directions of randomness (eigen vectors) and then identify the marginal distribution of the transformed variables. These marginal distributions are, in general, not normal. I can then generate variables from these marginal distributions (which have zero correlation by construction) and then reverse the transformation to get variables with the desired interdependence. Leaving aside the issue of estimation risk (assume my correlation matrix is a good estimate of the actual correlation matrix), have I ignored any information regarding the interdependence of the random variables, that a copula would describe? If I knew the copula and all the marginal distributions (of the original variables), and I used this to generate new random variables, are there any conditions where I would get significantly different results from the PCA approach? If it helps to answer the question, I am interested in Monte Carlo simulation of financial markets.
