[site]: datascience
[post_id]: 65279
[parent_id]: 65278
[tags]: 
Generator cant learn if the discriminator error is too small. But generator should always be "ahead" of generator in order to learn. There is a paper explaining why the generator's gradient vanishes if the discriminator gets too strong. And how does this proportion look mathemactically optimal TL;DR Dont make it too strong, but make sure D is ahead to ensure optimal learning. GANS are notoriously unstable (expensive also)
