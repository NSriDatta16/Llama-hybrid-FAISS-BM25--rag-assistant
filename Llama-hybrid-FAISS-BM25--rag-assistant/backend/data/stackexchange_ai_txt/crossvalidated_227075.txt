[site]: crossvalidated
[post_id]: 227075
[parent_id]: 227034
[tags]: 
The "normal" distribution is defined to be that particular distribution. The question is why would we expect this particular distribution to be common in nature, and why is it so often used as an approximation even when the real data does not exactly follow that distribution? (Real data is often found to have a "fat tail", i.e. values far from the mean are much more common than the normal distribution would predict). To put it another way, what is special about the normal distribution? The normal has a lot of "nice" statistical properties, (see e.g. https://en.wikipedia.org/wiki/Central_limit_theorem ), but the most relevant IMO is the fact that is the "maximum entropy" function for any distribution with a given mean and variance. https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution To express this in ordinary language, if you are given only the mean (central point) and variance (width) of a distribution, and you assume nothing else whatsoever about it, you will be forced to draw a normal distribution. Anything else requires additional information (in the sense of Shannon information theory ), for example skewness, to determine it. The principle of maximum entropy was introduced by E.T. Jaynes as a way of determining reasonable priors in Bayesian inference, and I think he was the first to draw attention to this property. See this for further discussion: http://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/Gaussian-distribution.pdf
