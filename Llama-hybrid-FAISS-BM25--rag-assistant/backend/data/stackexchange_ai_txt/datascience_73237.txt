[site]: datascience
[post_id]: 73237
[parent_id]: 73197
[tags]: 
You have multiple options to "collapse" a variable-length input into a single value: Recurrent neural networks (RNN), either vanilla RNNs or more powerful variants like long-short term memories (LSTM) or gated recurrent units (GRU). With these, you "accumulate" the information of each time step in the input and at the end, you get your fixed-size output. Pooling, either average pooling or max pooling. You just compute the average/maximum of the representation across the time dimension. Padding. You just assume a maximum length for the data and create a neural network that receives a piece of data of such a length. For any input data that is shorter, you just add some "padding" elements at the end until they have the maximum size.
