[site]: crossvalidated
[post_id]: 63397
[parent_id]: 
[tags]: 
Time-delayed neural networks - Learning the "max" layer

In Unified Architecture for NLP paper time-delayed neural network proposed as a way to deal with variable length input. Input window slides over sequence and label each output with "time", then next layer takes max over time. But no explanation given how to back-propagate error through this max layer. We able to compute error only for specific "time", potentialy different for each max layer output node. Reading original paper for TDNN didn't clarify anything. For me it looks like completly different NN architecture without any mention of max layer. Can someone explain or point to explanation? My own googling skills failed.
