[site]: crossvalidated
[post_id]: 245648
[parent_id]: 245282
[tags]: 
It has become quite a common practice to use batch normalization as they are robust to bad initialization. You can read more about what problem batch normalization solves from this paper . You can also read more about the practical aspects of batch normalization from the Karpathy DL course which mentions- A recently developed technique by Ioffe and Szegedy called Batch Normalization alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. The core observation is that this is possible because normalization is a simple differentiable operation. In the implementation, applying this technique usually amounts to insert the BatchNorm layer immediately after fully connected layers (or convolutional layers, as weâ€™ll soon see), and before non-linearities.
