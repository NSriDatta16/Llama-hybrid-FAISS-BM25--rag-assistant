[site]: datascience
[post_id]: 118798
[parent_id]: 118797
[tags]: 
The training of a self-attention layer will result in the update of the $W$ matrices and the gradient being propagated back to the previous layer. At the end of the self-attention blocks, the back-propagated gradient will arrive to the embedded vectors, which will also be updated. As personal advice, I would suggest that you don't try to understand why self-attention works, just how it works. The analogies made in the linked post about the embeddings of "the", "cat" and "walk" are nonsense in my opinion. First, nowadays most neural text processing models work at the subword level, not at the word level. Also, self-attention layers are stacked, so the token identities are lost after the first layer (unless you are training something like a masked language model where you predict the very same input tokens at the same positions). *
