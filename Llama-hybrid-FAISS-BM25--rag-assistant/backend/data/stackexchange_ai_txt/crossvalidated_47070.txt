[site]: crossvalidated
[post_id]: 47070
[parent_id]: 46921
[tags]: 
(Commenting on above response and feedback) Thanks for reading the blog! The cross-entropy error function has a little cheat, truncating predicted values to [1e-10, 1-1e-10] as a cheap and easy way to prevent errors in the log functions. Otherwise, this is the standard formula. For the dataset, it is very possible to have datasets where a random forest is far superior to a log. reg. and the log. reg. adds nothing to the ensemble. Make sure, of course, that you are using hold-out data - a random forest will almost always have superior results on the training data due to having far more effective parameters.
