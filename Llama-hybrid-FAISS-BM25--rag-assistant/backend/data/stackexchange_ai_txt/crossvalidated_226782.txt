[site]: crossvalidated
[post_id]: 226782
[parent_id]: 226670
[tags]: 
One way to report the result would be to perform cross-validation , and report min, max, standard deviation and average. If you compare your results against some other method, you can use some significance test such as approximate randomization. The performance analysis in the paper could try to explain why the train/test impacts the results more than expected. As a side note: many papers unfortunately only report one number (e.g., "F1-score = 0.65"). in addition to the train/test split, different runs with different initialization might have quite some impact on the neural network's performance (e.g. see https://arxiv.org/abs/1603.03827 table 3)
