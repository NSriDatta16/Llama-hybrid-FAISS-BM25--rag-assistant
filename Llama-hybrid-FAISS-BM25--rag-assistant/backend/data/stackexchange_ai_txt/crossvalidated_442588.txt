[site]: crossvalidated
[post_id]: 442588
[parent_id]: 
[tags]: 
how to avoid overfittig with xgboost and how to increase accuracy

I am doing a binary classification problem, I got to train 85% accuracy, but test accuracy is 72%, I tried different parameters, Cross valid, But overfitting doesn't change, please help me on how to reduce overfitting. This is my parameters code: params={ "learning_rate" : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] , "max_depth" : [ 3, 4, 5, 6, 8, 10, 12, 15], "min_child_weight" : [ 1, 3, 5, 7 ], "gamma" : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ], "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ] } x_clf=XGBClassifier() random_search=RandomizedSearchCV(x_clf,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3) This is my cross-validation code: from sklearn.model_selection import StratifiedKFold errxgb=[] skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=None) # X is the feature set and y is the target for train_index, test_index in skf.split(X,y): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y[train_index], y[test_index] random_search.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=100,verbose=3) preds=random_search.predict_proba(X_test)[:,-1] print("err_xgb: ",roc_auc_score(y_test,preds)) errxgb.append(roc_auc_score(y_test,preds)) p= random_search.predict(test_df)
