[site]: crossvalidated
[post_id]: 484769
[parent_id]: 484755
[tags]: 
If we desire to model distribution of a binary (bernoulli-distributed) random variable, conditioned on a random vector ${\bf x}_n\in\mathbb{R}^M$ , we could assume that $$ t_n \vert {\bf x}_n \sim \text{Bern}(f({\bf x}_n)) $$ For some function $f:\mathbb{R}^M\to[0,1]$ . In a logistic regression, we choose $f({\bf x})=\sigma({\bf w}^T{\bf x}$ ), whilst for a feed-forward neural network (FFNN), we choose $f$ to be some complicated nonlinear function of the form $$ f({\bf x}) = \sigma\left({{\bf w}^{(L)}}^Th\left({{\bf w}^{(L-1)}}^Th(...)\right)\right) $$ Whereas the logistic regression leads to a simple iterative equation to find its minimum, which always leads to the same minimum for a fixed dataset, the FFNN is dependent on the number of layers, the choice of $h$ and the disired number of parameters. Hence, it can be much more complicated to train an FFNN.
