[site]: crossvalidated
[post_id]: 162603
[parent_id]: 
[tags]: 
Did I use gradient descent correctly?

In an attempt to learn Gradient Descent. I've created my own dataset which is a the total bill of a meal with tips. As I created the perfect data so I make every meal with 10% tip. I purposely create a very large dataset as I heard that Gradient Descent is faster than normal Linear Regression. So here's my code. x = np.random.randint(100, size=100000000) y = x * 0.10 x = x[:,None] now = time.time() clf = SGDRegressor() clf.fit(x, y) later = time.time() difference = int(later - now) print("Time spent for SGDRegressor is %d seconds" % difference) print("slope is %f and intercept is %s" % (clf.coef_, clf.intercept_[0])) However, the result is way off that my Linear Regression method. I'm not sure if I did something wrong. I noticed about feature selection in Machine Learning Coursera. I'm not sure if my example has problems with feature selection? Here's the code I use for normal equation now = time.time() slope, intercept, r_value, p_value, std_err = stats.linregress(x=large_total_bills, y=large_tips) predicted_tips = (slope * 700) + intercept later = time.time() difference = int(later - now) print('The customer will leave the tip of $%f' % predicted_tips) print('The time spent is %f seconds' % difference)
