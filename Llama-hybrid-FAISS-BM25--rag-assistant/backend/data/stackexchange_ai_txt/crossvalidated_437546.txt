[site]: crossvalidated
[post_id]: 437546
[parent_id]: 
[tags]: 
Exchangeability and Conditional independence in the context of bayesian prediction

I was following the Bayesian Biostatistics book by Emmanuel Lesaffre and Andrew B. Lawson, and I stuck at a part under exchangeability topic. I need help for interpretation of what the author said in here ; Exchangeability is central in prediction. In Section 3.4, we looked for the distribution of a future observation $\tilde{y}$ assumed to be similar to the past data {y1 , . . . , yn }. We assumed conditional independence of $\tilde{y}$ and {y1 , . . . , yn }, but since we did not know the true value of θ , the unconditional distribution of { $\tilde{y}$ , y1 , . . . , yn } is exchangeable but dependent. It is this kind of dependence that allowed us to learn from the past data. If we knew θ then past data would not help us in determining the distribution of $\tilde{y}$ because of the conditional independence. Even tough we dont know the true value of θ, in Bayesian context, we have a prior information of θ, even we have posterior probability of θ. So in this case, I couldn't be sure whether having a some information θ without knowing the true value of it, help previous values of y in determining the distrbution of $\tilde{y}$ or not. In other ways, is { $\tilde{y}$ , y1 , . . . , yn } conditionally independent or dependent ? they should be conditionally independent as the answer to the question above, otherwise we wouldnt have hierarchical independence as; $$ p(\tilde{y} | \theta, y)=p(\tilde{y} | \theta) $$
