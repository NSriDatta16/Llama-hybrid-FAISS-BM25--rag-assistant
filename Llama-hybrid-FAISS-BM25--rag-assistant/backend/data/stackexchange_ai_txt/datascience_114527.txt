[site]: datascience
[post_id]: 114527
[parent_id]: 108814
[tags]: 
One way to look at it is like multi-modality training, where all inputs are of the same modality - a practical example is to run a DNN on each video, and then concatenate the embeddings and run the rest of the network based on it. This is similar to the solution you describe but the training is unified and is done end to end. P.S. Another obvious solution is to concatenate the videos and feed them as a single input to a general video classifier.
