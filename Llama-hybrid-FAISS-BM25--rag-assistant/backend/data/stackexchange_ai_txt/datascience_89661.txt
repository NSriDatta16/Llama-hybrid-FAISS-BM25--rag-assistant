[site]: datascience
[post_id]: 89661
[parent_id]: 
[tags]: 
XGBoost with deep trees

I've been exploring the use of XGBoost in many different applications. Up to now, I always find the best results with shallow trees (from 1 to 3 levels), with the rest of the parameters very dependent on the problem. On my current assignment, I found that I get a much better performance if I use >300 trees, with a depth >20 !! I understand that this is saying a lot about the complexity of the issue, but I can stop wondering if there is some feature transformation I could do to change this. EX: by doing PCA and adding the resulting component to a dataset, one can sometimes not only replace several features by a smaller number but also capture higher level relashionships
