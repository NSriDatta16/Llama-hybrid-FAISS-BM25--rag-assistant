[site]: crossvalidated
[post_id]: 202547
[parent_id]: 
[tags]: 
Does a check failing to compare observed and predicted data qualify as a posterior predictive check?

I consider a Gaussian mixture distribution and I want to implement posterior predictive checks for choosing the model with the correct number of mixture components. I know the true number of components as I simulate the data myself. After running MCMC we obtain parameters for our mixture components and also the allocations for all of our observations. If my chain length from MCMC is 1000, for each of these 1000 sampled values I do the following: Calculate for every observation the Mahalanobis distance to every component other than the one to which it was allocated. So if observation 1 was categorized to be from component 2, then I take Mahalanobis distance of observation 1 to every component other than component 2. And then I take mean of all the Mahalanobis distances that I obtain for all the observations. Now plot these 1000 Mahalanobis distances. I found that when I had fitted more components than I needed then I had obtained a long tailed distribution. This didn't work out every time but sometimes it did. Since this method is not a posterior predictive check,what should I call it? Is it still some kind of posterior check? I also wanted to know what are the various well known posterior predictive checks to select the number of components in a mixture distribution (Although I must admit this is a separate question).
