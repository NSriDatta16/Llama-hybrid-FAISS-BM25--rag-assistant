[site]: crossvalidated
[post_id]: 66510
[parent_id]: 
[tags]: 
Why is r-squared so small when there does seem to be a dependency in my data?

What I need to do is to find a model that can predict what the observation should look like given a factor input. I am doing a simple linear fit in R (i.e., lm(observation~0+factor, data=d) ); the $R^2= 0.002$, which is really small. However, when I do a SELECT AVG observation by 0.001 BRACKET factor , the result is something like: factor | average observation ---------------------------------------- -0.003 -2 -0.002 -2 -0.001 -1 0.000 1 0.001 0 0.002 1 0.003 2 It definitely seems to me that there is a pattern here, but somehow this pattern is not captured by a linear model. Is my understanding correct?
