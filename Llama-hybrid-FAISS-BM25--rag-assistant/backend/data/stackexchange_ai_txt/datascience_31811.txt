[site]: datascience
[post_id]: 31811
[parent_id]: 
[tags]: 
Why don't convolutional computer vision networks use horizontally - symmetric filters?

If, for example, I have a neural network for classifying dog breeds, and I feed it an image of some dog, inherently it shouldn't matter whether I feed it the original image or the image, mirrored horizontally. I'd like to implement this symmetry in the network, and by my understanding of CNNs that means i'll generally need all the filters to be horizontally symmetric, and then the network would both be more robust and would take almost 40% less time to train assuming filter size = 5. But, by my research modern networks don't use filters like that. Well... Why? what disadvantages does this architecture have that I can't find a single mention of such idea?
