[site]: crossvalidated
[post_id]: 604544
[parent_id]: 604535
[tags]: 
Both Akaike information criterion and Bayesian information criterion balance the number of parameters and the magnitude of the likelihood: $$ AIC=2k-2\log(\hat{L}),\\ BIC=k\log (n)-2\log(\hat{L}) $$ Increasing the number of parameters increases the value of likelihood - posing the risk of overfitting and reducing the magnitude of the information criteria. In the same time the first term in these criteria grows with the number of parameters. The minimum is thus reached when the linear increase in the first term is balance by the decrease due to the growth of the likelihood. The number of data points is usually such that $2 (indeed, the simple forms of the criteria given above are not applicable for small samples ). In other words, the penalty for the number of parameters is higher for BIC than for AIC: $k\log(n)>2k$ , and BIC has the minimum at smaller number of parameters. Thus, smaller AIC means more risk of overfitting , whereas smaller BIC means more risk of underfitting . (The quote in the OP is somewhat ambiguous, since it can be interpreted as referring to the bigger/smaller penalty OR as to BIC smaller/bigger than AIC .)
