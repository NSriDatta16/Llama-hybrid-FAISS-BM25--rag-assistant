[site]: crossvalidated
[post_id]: 288022
[parent_id]: 120030
[tags]: 
Actually as you correctly pointed out, in the case of a single categorical variable (with potentially more than 2 levels), $\hat{\beta}_0$ is indeed the mean of the reference and the other $\hat\beta$ are the difference between the mean of that level of the category and the mean of the reference. If we extend a bit your example to include a third level to the race category (say Asian ) and chose White as the reference, then you would have: $\hat{\beta}_0 = \bar{x}_{White}$ $\hat{\beta}_{Black} = \bar{x}_{Black} - \bar{x}_{White}$ $\hat{\beta}_{Asian} = \bar{x}_{Asian} - \bar{x}_{White}$ In this case, the interpretation of all the $\hat{\beta}$ is easy and finding the mean of any level of the category is straightforward. For example: $\bar{x}_{Asian} = \hat{\beta}_{Asian} + \hat{\beta}_0$ Unfortunately in the case of multiple categorical variables, the correct interpretation for the intercept is no longer as clear (see note at the end). When there is n categories, each with multiple levels and one reference level (e.g. White and Male in you example), the general form for the intercept is: $$\hat{\beta}_0 =âˆ‘_{i=1}^{n}\bar{x}_{reference,i} -(n-1) \bar{x} ,$$ where $$\bar{x}_{reference,i}\small{\text{ is the mean of the reference level of the i-th categorical variable,}}$$ $$\bar{x}\small{\text{ is the mean of the whole data set}}$$ The other $\hat\beta$ are the same as with a single category: they are the difference between the mean of that level of the category and the mean of the reference level of the same category. If we go back to your example, we would get: $\hat{\beta}_0 = \bar{x}_{White} + \bar{x}_{Male} - \bar{x}$ $\hat{\beta}_{Black} = \bar{x}_{Black} - \bar{x}_{White}$ $\hat{\beta}_{Asian} = \bar{x}_{Asian} - \bar{x}_{White}$ $\hat{\beta}_{Female} = \bar{x}_{Female} - \bar{x}_{Male}$ You will notice that the mean of the cross categories (e.g. White males ) are not present in any of the $\hat\beta$. As a matter of fact, you cannot calculate these means precisely from the results of this type of regression . The reason for this is that, the number of predictor variables (i.e. the $\hat\beta$) is smaller then the number of cross categories (as long as you have more than 1 category) so a perfect fit is not always possible. If we go back to your example, the number of predictors is 4 (i.e. $\hat{\beta}_0, ~\hat{\beta}_{Black}, ~\hat{\beta}_{Asian}$ and $\hat{\beta}_{Female}$) while the number of cross categories is 6. Numerical Example Let me borrow from @Gung for a canned numerical example: d = data.frame(Sex=factor(rep(c("Male","Female"),times=3), levels=c("Male","Female")), Race =factor(rep(c("White","Black","Asian"),each=2),levels=c("White","Black","Asian")), y =c(0, 3, 7, 8, 9, 10)) d # Sex Race y # 1 Male White 0 # 2 Female White 3 # 3 Male Black 7 # 4 Female Black 8 # 5 Male Asian 9 # 6 Female Asian 10 In this case, the various averages that will go in the calculation of the $\hat\beta$ are: aggregate(y~1, d, mean) # y # 1 6.166667 aggregate(y~Sex, d, mean) # Sex y # 1 Male 5.333333 # 2 Female 7.000000 aggregate(y~Race, d, mean) # Race y # 1 White 1.5 # 2 Black 7.5 # 3 Asian 9.5 We can compare these numbers with the results of the regression: summary(lm(y~Sex+Race, d)) # Coefficients: # Estimate Std. Error t value Pr(>|t|) # (Intercept) 0.6667 0.6667 1.000 0.4226 # SexFemale 1.6667 0.6667 2.500 0.1296 # RaceBlack 6.0000 0.8165 7.348 0.0180 # RaceAsian 8.0000 0.8165 9.798 0.0103 As you can see, the various $\hat\beta$ estimated from the regression all line up with the formulas given above. For example, $\hat\beta_0$ is given by: $$\hat{\beta}_0 = \bar{x}_{White} + \bar{x}_{Male} - \bar{x}$$ Which gives: 1.5 + 5.333333 - 6.166667 # 0.66666 Note on the choice of contrast A final note on this topic, all the results discussed above relate to categorical regressions using contrast treatment (the default type of contrast in R). There are different types of contrast which could be used (notably Helmert and sum) and and it would change the interpretation of the various $\hat\beta$. However, It would not change the final predictions from the regressions (e.g. the prediction for White males is always the same no matter which type of contrast you use). My personal favourite is contrast sum as I feel that the interpretation of the $\hat\beta^{contr.sum}$ generalises better when there are multiple categories. For this type of contrast, there is no reference level, or rather the reference is the mean of the whole sample, and you have the following $\hat\beta^{contr.sum}$: $\hat\beta_0^{contr.sum}=\bar{x}$ $\hat\beta_i^{contr.sum}=\bar{x}_i-\bar{x}$ If we go back to the previous example, you would have: $\hat{\beta}_0^{contr.sum} = \bar{x}$ $\hat{\beta}_{White}^{contr.sum} = \bar{x}_{White} - \bar{x}$ $\hat{\beta}_{Black}^{contr.sum} = \bar{x}_{Black} - \bar{x}$ $\hat{\beta}_{Asian}^{contr.sum} = \bar{x}_{Asian} - \bar{x}$ $\hat{\beta}_{Male}^{contr.sum} = \bar{x}_{Male} - \bar{x}$ $\hat{\beta}_{Female}^{contr.sum} = \bar{x}_{Female} - \bar{x}$ You will notice that because White and Male are no longer reference levels, their $\hat\beta^{contr.sum}$ are no longer 0. The fact that these are 0 is specific to contrast treatment.
