[site]: crossvalidated
[post_id]: 636046
[parent_id]: 
[tags]: 
Simplified sample size calculation for binary classifier

I'm thinking about a problem from a recent project, where a neural network is used to detect some particular events- so basically it is a binary classifier (yes/no). We have a lot of data and want to somehow very roughly estimate a required sample size for further data collection. Usually I would consider learning curves and prefer a simulation based approach, as deep learning techniques are kind of a blackbox and not comparable to the typical sample size calculation for hypothesis tests. However, this is not possible as I don't have any access to the software, I just want to make a very rough guess based on the available data. So I have a large n, say 200000 and values for sensitivity and specificity, say 80% each. Further I have a prevalence, which is very low, say 0.1%. And now I was just thinking about an approach which is basically very old ( Buderer, 1996 ), where I can insert these things and get a sample size. Is this ok for a first idea? I have a slight understanding issue- it's not possible to increase my sensitivity by increasing n, is this correct? So it's not possible to get a formula for some target sensitivity, right?
