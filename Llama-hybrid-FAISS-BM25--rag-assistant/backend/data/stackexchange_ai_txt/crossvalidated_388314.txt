[site]: crossvalidated
[post_id]: 388314
[parent_id]: 365336
[tags]: 
Here's what I understood about VAEs: the posterior refers to p(z|x), which is approximated by a learnt q(z|x), where z is the latent variable and x is the input. the prior refers to p(z). Often, p(z) is approximated with a learnt q(z) or simply N(0, 1). The posterior explains how likely the latent variable is given the input, while the prior simply represents how the latent variables are distributed without any conditioning (in CVAEs conditions are added here as well). Hence, in training, we want to learn a good posterior approximation (Evidence) that explains the input, but in testing we want to generate random samples following the prior distribution (unless you want to condition them some how).
