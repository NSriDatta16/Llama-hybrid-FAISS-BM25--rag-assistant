[site]: crossvalidated
[post_id]: 220746
[parent_id]: 219885
[tags]: 
Important to note first: Bayesian inference does NOT automatically guard against overfitting. Adding additional variables will pretty much result in the same problems as in an non-Bayesian analysis. However, Bayesian model selection / model weights via marginal likelihood / Bayes factor CAN effectively include a penalty of model complexity, depending on how parameter priors are specified (generally, the wider the priors are, the more penalty of complexity - you can see this directly in the definition of the marginal likelihood). Moreover, it is possible to regularize regression problems, similar to lasso or ridge regression, in a hierarchical model (e.g. Park, T. & Casella, G. (2008) The Bayesian Lasso. J. Am. Stat. Assoc., 103, 681-686.). I would think that the quote refers to this, or simply to the idea of random effect structures, which impose a shrinkage to the random effect estimates.
