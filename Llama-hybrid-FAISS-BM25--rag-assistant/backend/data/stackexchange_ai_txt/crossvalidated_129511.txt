[site]: crossvalidated
[post_id]: 129511
[parent_id]: 20944
[tags]: 
You could first filter the stopwords and other meaningless frequent words, and then you could try some smaller amount and check how does it work. Generally, if you use big amount of words in your set, most of them will be pure noise and would not carry much information. Make few tries and check what rate is enough, but with predicting only two categories, I imagine that you could use much smaller amount of them. What to do with missing words? They do not occur so they have frequency of zero. On the other hand, Naive Bayes uses products heavily and if you multiply anything by zero you get zero. And in most (probably all) rows you will have some words that did not occurred, so your matrix will become a collections of zeros. Because of that it is better to choose some arbitrary small number and add it to all the values in your matrix so there is no zeros (and most ready made algorithms do this for you). Position of the words in the matrix does not matter. However, position of the words in text could matter, so you can include such a variable in your analysis (however that is beyond scope of simply using Naive Bayes algorithm). Final general remark: pay very much attention on cleaning and preprocessing the data since it is crucial in NLP, remember: garbage in, garbage out . Also deciding on which words to include in your training set is important step - taking "top $n$ words" could be not enough in many cases.
