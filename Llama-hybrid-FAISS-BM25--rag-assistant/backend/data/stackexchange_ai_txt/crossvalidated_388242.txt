[site]: crossvalidated
[post_id]: 388242
[parent_id]: 387952
[tags]: 
You ask an interesting question, but unfortunately you consider a case which is far removed from mainstream research: you consider vector regression, instead than multiclass classification (if you're actually interested in multiclass classification, then you wrote the wrong loss function) you don't use any regularization: no batch normalization, weight decay or SGD, and no learning rate decay. Actually, I'm surprised your neural network can learn anything at all! What are the targets? I cannot give a complete answer, but I can at least give you some indications. First of all, let's correct a couple errors: Not all the weights matrices are $W^{l} \in \mathbb{R}^{m \times m}$ . When $l \in \{1, L\}$ , you have resp. $W^1 \in \mathbb{R}^{m \times d}$ and $W^L \in \mathbb{R}^{k \times m}$ $\mathbb{E}||W||_F \neq 2\sqrt{m}*2/\sqrt{m} = 4$ It's the Euclidean operator norm (which is a proper induced norm) that satisfies this identity, not the Frobenius norm. See, e.g., https://arxiv.org/pdf/1608.06953.pdf . For the Frobenius norm and $l\not\in \{1,L\}$ I get: $$ \mathbb{E}||W^l||_F =\sqrt{\sum_i^m\sum^m_j\mathbb{E}[w_{ij}^2]}=\sqrt{m^2\frac{4}{m^2}}=2 $$ not sure why you're getting another result. Having said that, recent research suggests that part of the success in training modern neural networks is due to implicit control of the weight norm. If you're using batch norm and weight decay, then one can prove that weight decay, controlling the weight norm, prevents the effective step size to decrease, which would hinder optimization (see Hoffer et al., "Norm matters: efficient and accurate normalization schemes in deep networks" , 2018). In your case, since you're not using weight decay or batch norm, this isn't true. However, the change in weight norm does depend on the learning rate $\eta$ and on the initialization. As a matter of fact, $$W^l_{t+1}=W^l_t-\eta\frac{\partial\ell}{\partial W^l_t}(W^1_t,\dots,W^L_t)$$ Thus, it's quite obvious that, depending on $\eta$ , the change in $W^l_t$ and thus in its norm will be different. Without knowing anything about $\eta$ , I don't think much can be said, but you could have a look at the tools of nonlinear random matrix theory for deep learning: Jeffrey Pennington, Pratik Worah, Nonlinear random matrix theory for deep learning , 2017
