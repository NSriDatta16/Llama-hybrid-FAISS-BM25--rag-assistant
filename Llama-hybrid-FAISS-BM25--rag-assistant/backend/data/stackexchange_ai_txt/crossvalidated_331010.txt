[site]: crossvalidated
[post_id]: 331010
[parent_id]: 330940
[tags]: 
The dropout paper motivates this procedure as matching the actual output under test conditions with the expected output under training conditions: If a unit is retained with probability $p$ during training, the outgoing weights of that unit are multiplied by $p$ at test time as shown in Figure 2. This ensures that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time. The paper also explains how this is related to the conceptualization of Dropout as training exponentially many "thinned" networks that share weights. By virtue of scaling by $p$ at test time, the network functions as an ensemble model that approximately averages the outputs of the thinned networks. References: Srivastava et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting.
