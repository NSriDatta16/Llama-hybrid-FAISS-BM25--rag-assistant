[site]: datascience
[post_id]: 126520
[parent_id]: 100562
[tags]: 
During training, the k -NN algorithm simply stores $ m $ training data points with predefined classes. But the testing phase involves computing pairwise distances between each of $ n $ test data points and all training data, thus creating a distance matrix of $ m \times n $ size. Finally, the algorithm assigns each test point to the majority class of its $ k $ nearest neighbors in the training set. So you're right in the above thought process: for k -NN creating decision surfaces or hyperplanes is unimportant, because it operates on distances from training set to test set. However, there is another classifier named SVM, which aims at finding a hyperplane that separates two classes.
