[site]: datascience
[post_id]: 61855
[parent_id]: 61262
[tags]: 
This may seem obvious, but have you tried using a Boltzmann distribution for action selection instead of argmax? This is known to encourage exploration and can be done by setting the action policy to $$p(a|s) = \frac{\exp(\beta Q(a,s)}{\sum_{a'} \exp(\beta Q(a',s))},$$ where $\beta$ is the temperature parameter and governs the exploration-exploitation trade-off. This is also known as the softmax distribution. Put into code, this would be something like this: beta = 1.0 p_a_s = np.exp(beta * q_values)/np.sum(np.exp(beta * q_values)) action_key = np.random.choice(a=num_act, p=p_as) This can lead to numerical instabilities because of the exponential, but that can be handled e.g. by first subtracting the highest q value: q_values = q_values - np.max(q_vaues)
