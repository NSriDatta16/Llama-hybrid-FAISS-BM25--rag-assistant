[site]: crossvalidated
[post_id]: 71678
[parent_id]: 71670
[tags]: 
I think the question as in the headline is too broad to be answered in a useful way, the more so as it will probably depend on both the aggregating method and the statistic in question. This will even apply to the "mean": do you try to preserve signal shape and intensity (e.g. Savitzky-Golay filters), or do you try to preserve the area under the signal (e.g. loess)? Noise-related statistics are obviously affected: that is usually the purpose of the aggregation. I've seen at least one paper that then applies some statistics to the aggregated data [...] Is that valid? I would have thought that the averaging process would modify the result a fair bit, due to the reduced noise. This modification is most probably the purpose of the aggregating. In general, you are allowed to do a whole lot of things to your data, but you need to say what you are doing (and preferrably also why you do it) show the quality of the resulting model (test with independent data) What is a valid aggregation will also depend on your application. E.g.: I'm working with spectroscopic data. It is very common to aggregate single spectra into average spectra: the measurement process means certain limits to the quality of spectra I can obtain "in one shot". However, for many applications it is perfectly valid to specify an acquisition procedure that says that always $n$ repeated measurements should be taken and averaged. On the other hand, if the application is real-time/ online or inline analytics such as FIA (flow injection analysis) this implies restrictions on possible aggregation schemes.
