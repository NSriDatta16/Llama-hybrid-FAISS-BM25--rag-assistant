[site]: crossvalidated
[post_id]: 100985
[parent_id]: 100975
[tags]: 
Basically random forests is a bagged ensemble, with something additional which makes the averaged model more independent. If you remove the random part from random forests you will end up with bagging, or bootstrap aggregating. Bagging can be applied to any type of classifier or regressor. See Wikipedia page for more details on bootstrap aggregation. That is when you want to average the results. If you want only to estimate the error given by a single tree, (that tree is built each time, for each sample), than what you want looks like bootstrapping. See this Wikipedia bootstrapping page for starting points on these procedures. As far as I know there is plenty of knowledge on estimating with bootstrapping. Also, you can build an empirical distribution of misclassification error, no matter which model was used to predict. When you have stronger assumptions on the distribution of the model and the distribution of the error, of course, you might be in the position to infer also, which is the name of that distribution and it's parameters. However, to build an empirical distribution you need no assumption, I think and bootstrapping will provide good results.
