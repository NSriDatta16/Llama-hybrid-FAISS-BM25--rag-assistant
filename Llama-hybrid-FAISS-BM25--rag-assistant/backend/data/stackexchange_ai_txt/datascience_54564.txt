[site]: datascience
[post_id]: 54564
[parent_id]: 
[tags]: 
Do word embeddings help with out of vocab tokens?

I am performing sentiment analysis on a custom dataset of text with Keras but am a little confused about word embeddings. I have been able to train an "Embedding" layer and have also learned to load existing weights from Glove but am still facing a few problems. The main one being that there are certain "negative" words I know of but that do not appear in the vocab. Because of this, when i try examples with word that do NOT appear in the vocab (like the word "rubbish") the network does not know that this contains a negative sentiment. Is there a way to use Word2Vec / Glove / etc to pass in the word rubbish, find the similarity to the word garbage, and then pass that known word into the network instead? And if so is that handled by the "Embedding" layer or is it an additional step I need to perform during pre-processing? Additionally, how can I handle misspelled words? For instance, how can i associate "rubbbbbish" with "rubbish"? I am new to text classification and would really appreciate a bit of guidance!
