[site]: crossvalidated
[post_id]: 512519
[parent_id]: 
[tags]: 
Why are vanishing gradients an issue in a minimization problem?

From reading various articles and questions posted on this site, I understand how sometimes layers of a deep neural network may not learn at the same rates, and some (especially earlier) layers may end up having gradients close to zero which supposedly causes the network to learn at a much slower rate. I find this somewhat counterintuitive from a Mathematical perspective. My confusion is this: gradient descent is used to minimize an objective function and update the values of variables so that the gradient is as close to zero as possible. In this scenario, isn't it inevitable that some of the gradients will be 0? Why is this a problem mathematically? Why do layers with close-to-zero gradients still have much learning to do when they're already at an ideal value? Why can't other layers still be trained while some layers (say earlier layers) have gradients close to 0? Thanks!
