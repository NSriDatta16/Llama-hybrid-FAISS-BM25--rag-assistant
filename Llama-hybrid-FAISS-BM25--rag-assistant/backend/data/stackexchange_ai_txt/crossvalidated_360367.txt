[site]: crossvalidated
[post_id]: 360367
[parent_id]: 
[tags]: 
MLE when the likelihood function itself contains a random variable -- do I just integrate?

If I have a set of i.i.d. observations $\{x_1, x_2, \dots x_n \}$ drawn from a distribution $f(x ; \theta)$, I can form the MLE estimate $\hat{\theta}$ by finding the argmax of $\sum_i^n \ln\left[ L(\theta | x_i) \right]$. Suppose that the log-likelihood $l$ is given by $$ l(\theta; x_i) = \log\left[L(\theta ; x_i)\right] = g(\theta, x_i)c $$ Where $g()$ is known and $c$ is the same known deterministic constant for all $x_i$. Obviously, this does not present any trouble. However, suppose instead we have a random variable $C$, which may be realized differently for every datapoint $x_i$. This means the data are no longer identically distributed, though they remain independent. We are ignorant as to what each realization $c_i$ actually is, so all we can do is write $$ l(\theta ; x_i) = g(\theta, x_i)C $$ Now the likelihood for each observation is actually a random variable. Suppose $C$ has a known p.d.f. $h(c)$ with finite expectation and variance. Is it then correct to say that the MLE for $\theta$ can be found by integrating against this distribution and using the expectation of $C$? i.e. $$ \hat{\theta} = \mathrm{argmax}_\theta \sum_i^n g(\theta, x_i) \int c h(c) \, dc \\ \hat{\theta} = \mathrm{argmax}_\theta \sum_i^n g(\theta, x_i) \mathrm{E}(C) $$ ? EDIT This question was met with some confusion in the comments as to what exactly I'm trying to do so here's some background: A Reed-Frost model is a discrete stochastic process used to model the spread of infectious disease. We suppose that every individual ("patient") in a population can be in one of four states: Susceptible, Latent, Infectious, or Recovered. If we have $N$ patients total, then on each day $j$, $S_j + L_j + I_j + R_j = N$. The way the disease spreads is by contact -- for each susceptible the chance of getting the disease from a single infectious is given by the "contact probability" $p$. However since there are many infectious, it makes more sense to use the "avoidance probability" $q = 1-p$. Given that someone was susceptible on day $j-1$, the probability of them still being susceptible on day $j$ is $q^{I_{j-1}}$ (because they have to avoid infection from all of the contagious people, assuming all contacts are independent Bernoulli processes). If they are unlucky and get infected, they pass from the Susceptible state to the Latent state, in which they remain for the Incubation or Latency period $T_L$. Let's assume for the moment that $T_L$ is a known constant. After this duration has passed, the Latent patient becomes Infectious. They remain in the Infectious state for some time, after which they become Recovered (immune) and play no further role in the epidemic. It is rare to know with certainty when someone is infected, but if I "know" the incubation period is a constant, and I know when each patient becomes infectious (based on the symptoms they manifest), then just by counting backwards $T_L$ number of days from the moment of each onset of infectiousness I can determine when the infection occurred. My dataset also tells me when each patient recovered from infection, therefore, I can calculate all four of $S_j$, $L_j$, $I_j$ and $R_j$ for all times $j$. To estimate $q$, we need the likelihood. Given $q$, $I_{j-1}$ and $S_{j-1}$, the number of Susceptibles on day $j$ follows a Binomial: $$ S_{j} \sim \mathrm{Bin}(S_{j-1}, q^{I_{j-1}}) $$ The total likelihood is then the product of these Binomials over each day: $$ L(q;\vec{S}) = \prod_j {S_{j-1} \choose S_j} (q^{I_{j-1}})^{S_j} (1 - q^{I_{j-1}})^{S_{j-1} - S_j} $$ Taking the logarithm, dropping terms that don't depend on $q$, and rearranging, the log-likelihood is (up to a constant): $$ l(q) = \sum_j \left[ \ln(q)I_{j-1}S_j + (S_{j-1} - S_j)\ln(1 - q^{I_{j-1}}) \right] $$ To recap, we got this by assuming that the incubation period $T_L$ was a constant number of days for each patient. However, this is not a very realistic assumption, as in reality it can vary by quite a lot, and if I'm simulating this it will make a big difference to the possible outcomes. If instead we allow $T_L$ to be a random variable , so that each patient is Latent for a different length of time, this forces $L_j$ and $S_j$ to be random variables too. In particular, if I know the distribution of $T_L$, I can calculate $\mathrm{E}(S_j)$ for all $j$. As you can see, if you do that, the likelihood looks like what I posted before the edit -- $\mathrm{E}(S_j)$ only interacts multiplicatively with the $I_j$ and $q$ terms. Therefore, can I simply substitute $\mathrm{E}(S_j)$ into the above likelihood, and have it still give the right answer for $q$ ? Intuitively , it seems as though I can. I could estimate this via Monte Carlo by simulating many "phoney" epidemics consistent with my data: for each simulation $i$, give each patient $k$ their own latency period drawn from $T_L$: $T_L^{(i, k)}$. Count backwards by this number from the day they are infectious to find out when they got infected. Calculate $S_j^{(i)}$ for each day. Proceed with the above MLE estimation to find $\hat{q}_i$. Repeat this simulation many times with different draws from $T_L$, thus resulting in slightly different $\hat{q}_i$ estimates. Then at the end, take the average over all $\hat{q}_i$'s produced. Shouldn't this result in the same thing, on average, as what I described above by substituting in $\mathrm{E}(S_j)$?
