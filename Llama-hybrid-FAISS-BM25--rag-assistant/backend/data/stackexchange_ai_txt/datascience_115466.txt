[site]: datascience
[post_id]: 115466
[parent_id]: 115459
[tags]: 
There is no deterministic way to trade accuracy for generalizability. Integral part of machine learning is splitting your available data into a training set and test set (or even an intermediate validation set, depending on the amount of available data). With that said you train the model on the training set and then apply the model to the 'unknown' test set. In case it performs (much) worse on the test set your model likely overfits the training data, so you would generally try to achieve similar results on the test set as on the training set. By repeating this process of model building you achieve the 'best possible model' while guaranteeing generalizability on data that the model hasn't seen before. Of course there's more to it, but this would be the general approach to balance accuracy and generalization. As it seems to be a classification problem you are trying to solve, you can also optimize your model in regard to precision/recall or sensitivity and specificity, respectively (ROC curve etc. might help here). Quantifying those metrics would be part of model evaluation on the path to an optimal model outcome.
