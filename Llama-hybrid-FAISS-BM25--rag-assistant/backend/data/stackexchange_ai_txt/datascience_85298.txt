[site]: datascience
[post_id]: 85298
[parent_id]: 85176
[tags]: 
It is normal to get 100% accuracy in this case. I mean it is inevitable to get 100% accuracy. If you train your algorithm even with 10% of your data you will again get 100% accuracy. The abnormal thing in this code is using a vectorizer. Vectorizer is an algorithm that is used to extracting words from the documents as mentioned in the source : Tf-idf stands for term frequency-inverse document frequency and is often used in information retrieval and text mining. If you print what it extracts from your lista_asm variable, you will see almost every unique word in your feature's samples. print(tfidf_vectorizer.get_feature_names()) Output (just 6 of them fitted the image, in total 23125 of them available): As a result, it creates 23125 new features for you where all these features are unique words in your samples. When you model your data with these 'features', even a simple model can learn the task easily. As you have only 4 target values which are ['string' 'math' 'encryption' 'sort'] , SVM easily learns how to predict them your very spare feature matrix . In other words, if it sees a specific word or words it easily predicts your result. To more clarify it lets assume a simple training set as following: So, when it will see a string which includes apple orange it will easily label it as sort . Check this example if you want to understand how exactly Tf-idf Vectorizer works. Thus, getting that 100% accuracy is normal. Since I do not know what exactly those features stand for, I cannot comment on them, however, consider that you converted the samples of your lista_asm (which is a list of strings) to a single string and then extracted the keywords from it. If you would ask if it is a good solution or not. I think it is a weird solution. If your data a good representation of future data, then it will work correctly. I would say it is a good representation of your future data because when you even train your model with 10% percent of your data and test with 90%, it perfectly (with 100% accuracy) predicts the target. Additional note: Check the following result of the number of unique values for each category (only 1000 samples used). That is, the words that appear only in one specific category, not in others. and following code (It is a bit dirty and inefficient code since I wrote it quickly. Sorry.) import pandas as pd import re # Read the file into dataframe dataFrame = pd.read_json('dataset.json', lines = True) dataFrame = dataFrame[:1000] dataFrame['opcodes'] = dataFrame['lista_asm'].apply(lambda x: re.findall("'([^']*)'", x)) df = dataFrame[['opcodes', 'semantic']] print(df['semantic'].unique()) categories = df['semantic'].unique() for cat in categories: catList = [] refList = [] catDf = df[df.semantic == cat] restDf = df[df.semantic != cat] for index, row in catDf.iterrows(): catList.append(row['opcodes']) for index, row in restDf.iterrows(): refList.append(row['opcodes']) print(len([i for i in catList if i not in refList])) What the code does and the image above shows is to find words that are in one specific category and not in another. E.g. the words that appear in the sort category but not in others. string, math, encryption, sort have 222, 305, 197, and 276 (in only 1000 samples, as you increase the sample size these values blow up) unique words (that only belongs to its category) respectively. By using those unique words (not exactly the same but similar) your model differentiates the categories easily. Update 2: If you take a subset of these features (or words), your accuracy drops significantly, because your unique words are not there anymore. e.g. first 100 features: X_ros = X_ros[:,0:100] Paste this code before train_test_split . or first 1000 features X_ros = X_ros[:,0:1000] or let's use 20% of your features. X_ros = X_ros[:,0:4625] As you can see, 20 % of your features are not well-predictor of your 4 categories. But do not confuse it with max_features which If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. This chooses the top best features not a random subset of features. tfidf_vectorizer=TfidfVectorizer(max_features=10) Even 10 of those are able to identify the class as good as the first 20% of your features. Their combination is able to give you the correct class.
