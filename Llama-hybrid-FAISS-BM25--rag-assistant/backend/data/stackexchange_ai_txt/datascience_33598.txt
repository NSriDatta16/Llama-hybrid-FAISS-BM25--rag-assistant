[site]: datascience
[post_id]: 33598
[parent_id]: 33593
[tags]: 
Here's my two cents: Yes, if you want to perform enseble each model should be trained on the same set of data. The technique you described is stacking, because you "stack" each prediction and simply use the majority. Ensemble Learning, to me, si more "complex", infact you can combine the different predictions from k models via linear combination, where you estimate the coefficients of the combination itself. More precisely, you have k prediction for each of your test observations, from k models (ie: 3 or more), and you need to find the best combination (for each obs.) of these, to have the best "accuracy" (any kind of metric). Keep in mind that it's best to combine models that have low or even opposite correlation, so that you "diversify" your predictions, that would mean better results. Often peole train lots and lots of different model, than use a simple Neural Network using the predictions as inputs, and the true labels from the test set as the truth, just as a normal class model would do. See the papers in the References here as a starting point. Bagging is basically a special case of stacking via majority vote, see: Bagging. You propose to the same type of model (ie: decision trees) the train dataset multiple times, making new different extraction, with replacement. This is like submitting new entries, but basically you are using the same data all over. Then you average all the predictions again, this helps with overfitting but it's not always the case, because you are using the same model, and so each si highly correlated with the other. Yes, in boosting, just like bagging you use the same model (usually a weak learner). At each stage, the learner learns from the errors made at the previus stage. Here an excellent visual example of what's going on. To be clear, stacking and ensemble learning can be used with lots of different models on the same train set (it's even better if the model in the "ensemble" are quite different). You can combine each prediciton with different techniques, from simple average, to the more complex use of Neural Networks. Bagging and Boosting are based on the use of the same model, one using the same obs. multiple times, the latter uses the same weak learner on the same data, but each makes predictions using the errors made by the previous one, in a loop. Hope this helps you a little.
