[site]: crossvalidated
[post_id]: 402646
[parent_id]: 402625
[tags]: 
Let me define $n$ as the number of observations in your dataset, and $p$ as the number of covariates you are working with. This way, your matrix of covariates is $X\in\mathbb{R}^{n\times p}$ and your (binary) response variable is (with a little abuse of notation) $y\in\{0, 1\}^n$ . When you perform a principal component analysis you will get mainly two things as output. The matrix of principal components: This matrix, formed by the eigenvectors of $X$ , is obtained in a way such that the first principal component has the largest possible variance, and each succeeding component has the highest possible variance under the constraint that it is orthogonal to the preceding components. You say that you are keeping 5 principal components. This means that this matrix, let me call it $P$ will be a matrix of dimension $P\in\mathbb{R}^{p\times 5}$ The projection of your covariates matrix into the space generated by the principal components: This matrix, let me call it $T$ , will be a matrix of dimension $T\in\mathbb{R}^{n\times 5}$ . And you are using this $T$ matrix in your logistic regression model. Now, since the principal components are orthogonal, this means that there is no bit of information provided by the first PC that is also being provided by the second PC, or by any other PC, and in the same way, there is no bit of information provided by the second PC that is also being provided by other PC. In your words, all the components are mutually exclusive by definition The principal components are also defined in a way that they maximize the variance of the matrix $X$ . But this means that some PCs are not relevant for prediction, but are only relevant for describing the variance in $X$ . And this fact explains why, although in your case, the first $5$ principal components are considered enough for explaining the variance of $X$ , this does not mean that are the most relevant components for predicting the value of your response variable.
