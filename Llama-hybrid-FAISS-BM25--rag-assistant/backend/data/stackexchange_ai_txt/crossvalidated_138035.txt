[site]: crossvalidated
[post_id]: 138035
[parent_id]: 
[tags]: 
Variance calculation RELU function (deep learning)

weight initialization is important for modern deep learning. To understand [1,2], I would like to understand the following: $$ E[x^2] = 0.5 Var[y], $$ where $x= max(0,y)$, $E[.]$ is the expectation, $Var[.]$ the variance, $x,y$ are random variables. We assume $y$ to have zero mean and to be symmetrical around the mean. Thanx for an explanation/derivation K [1] http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf [2] http://arxiv.org/abs/1502.01852
