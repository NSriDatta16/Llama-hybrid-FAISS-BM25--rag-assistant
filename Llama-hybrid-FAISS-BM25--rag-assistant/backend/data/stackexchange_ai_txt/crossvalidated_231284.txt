[site]: crossvalidated
[post_id]: 231284
[parent_id]: 231024
[tags]: 
The model you are referring to, simple linear regression, a.k.a. "the line of best fit" (I am confusing model and estimation method here), is admittedly very simple (as the name says). Why studying it? I can see a lot of reasons. In the following I assume that the concept of random variable has been at least informally introduced, because you mentioned it in your question. pedagogical : of course, for you it's obvious that real-valued random variables with finite second order moments form an Hilbert space. Maybe it was already obvious when you first studied probability theory. But statistics is not only teached to math students: there is a wider public, from physics to economics, to computer science, to social science, etc. These students may encounter statistics early in their course of study. They may or may not have been expoused to linear algebra, and even in the first case, they may not have seen it from the more abstract point of view of a math course. For these students, the very concept of approximating a random variable by another random variable is not so immediate. Even the basic property of the simple linear model, i.e., the fact that the error and the predictor are orthogonal random variables, is sometimes surprising to them. The fact that you can define an "angle" between random variables ("nasty" objects! measurable functions from a probability space to a measurable space) may be obvious to you, but not necessarily to a freshman. Thus, if the study of vector spaces starts with the good ol' Euclidean plane, doesn't it make sense to start the study of statistical models with the simplest one? procedural : with simple linear regression you can introduce the concept of parameter estimation, and thus the method of least squares, standard errors, etc. in its simplest case. If you think this is trivial, keep in mind that a lot of professionals, who use statistics in their job/research but are not statisticians, are deeply confused about the frequentist confidence interval! Anyway, once the easiest case has been covered, you can go to multiple linear regression. Once this is mastered, then all the linear models are available for estimation. In other words, if I can fit the model $\xi = \beta_0+\sum_{i=1}^N \beta_i \eta_i +\epsilon$ (by OLS, or LARS in case regularization is needed, etc.), then I can fit all models of the kind $\xi = \sum_{i=0}^N \beta_i \phi(\eta_i) +\epsilon$. This is a really powerful class of models, which, as noted by @DaeyoungLim, can approximate all functions in the Hilbert space, if you have an infinite set of basis functions, and if they generate a vector subspace which is dense in the Hilbert space. practical : there are numerous successful applications of simple linear regression. Okun's law in economics, Hooke's law , Ohm's law and Charles's law in physics, the relationship between blood systolic pressure and age in medicine (I have no idea if it has a name!) are all examples of simple linear regression, with varying degrees of accuracy.
