[site]: datascience
[post_id]: 36628
[parent_id]: 
[tags]: 
Explanations about ADAM Optimizer algorithm

I'm a beginner in Machine learning and i'm searching for some optimizer for the gradient descent. I've searched many topics about that, and did a state of art of all these optimizers. I have just one problem, and i can't figure it out. Don't judge me please, but i would like to know ? Are we using ADAM optimizer alone or are we obliged to combine it with the SGD ? I didn't understand if it works alone or if it's here to optimize NOT the neural network but the SGD of the neural network? Thank you for your help!
