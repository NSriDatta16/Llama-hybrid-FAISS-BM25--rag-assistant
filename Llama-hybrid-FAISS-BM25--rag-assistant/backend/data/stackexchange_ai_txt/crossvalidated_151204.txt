[site]: crossvalidated
[post_id]: 151204
[parent_id]: 
[tags]: 
Neural Network training with Levenberg Marquardt without validation set

I wish to implement a matlab version of the well-known Levenberg-Marquardt algorithm to train a given set of data. The size of the available data is small - hence, making the use of cross validation to identify the model suboptimal. I intend to adapt the code using regularization in order to obtain the required model . The matlab code (without regularization) for training is given below: %%% An implementation of the Levenberg-Marquardt algorithm to train a neural network mu1 = 0.1 % update(damped) parameter p = rand(n); % p is the initial parameters, n is the size of parameters. t = modelFun(p); % initial evaluation of error function while mu1 Question: How can the maximum damped parameter and the stopping criteria be set? Is it sufficient to set the damped (update) parameter as the maximum number of iterations, or using an epsilon bound between the corresponding values of the function evaluations? I realized that there seem to be no convergence with respect to the function being trained in some cases. In such cases, the value of the update parameter (Levenberg Marquardt update parameters) continues to increase. Can anyone offer ideas on how to set the parameter, and also define a criterion by which the initial parameters can also be set?
