[site]: crossvalidated
[post_id]: 116481
[parent_id]: 115874
[tags]: 
There are several approaches to combine classifiers, and in your case, what you are doing is one of them. Also, it is reasonable to expect an improvement in the combined classifier compared to either individual classifiers if they are sufficiently different and complementary from each other. If you had 3 or more classifiers, you could have used techniques like majority-voting or Borda count. However, since you have only two classifiers, you need to go with a combination rule. The two simple heuristics, with corresponding assumptions on independence of the classifiers, are sum-rule and the product-rule. See this paper for an overview of when each method performs better. In a nutshell, when your underlying feature space is same/highly correlated, and your classifier errors are independent, an average/sum rule is more suited, and when you are training on two different sets of features, a product rule is preferred. However, given the simplicity, trying both and picking the most suitable one is not bad either. The other approach generalizes the sum-rule by estimating weights for the classifiers. Using the classifier outputs as features, a second-stage classifier is trained. This approach has been proposed under different names. For instance, stacked generalization is one such idea to train a second classifier using the outputs of the first classifier. Other similar approaches are mixture-of-experts, some ensemble learning methods, query-by-committee etc.
