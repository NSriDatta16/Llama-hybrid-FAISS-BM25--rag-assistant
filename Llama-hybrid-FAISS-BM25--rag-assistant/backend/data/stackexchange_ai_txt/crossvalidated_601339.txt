[site]: crossvalidated
[post_id]: 601339
[parent_id]: 
[tags]: 
K-fold Cross Validation with early stopping on a deep learning model

I know there are similar questions, but so far when the topic of early stopping comes up I can't find a decisive answer. I want to know if what I'm doing makes sense and if not why. When I say "test set" I'm talking about the hold out set that the model won't see at any point during training time. So my pipeline for k-fold validation is the following: I have my original dataset and I split it into 2 in a 80/20 proportion. These are my train and test sets. The test set won't be used until the end. I perform a 5 folds cross validation using my train split. Each split has a train subset and validation subset, again in a 80/20 proportion. For each split I train a model using the train subset, early stopping is performed using the validation subset. After training each split I'm left with 5 models. In broad terms these are my steps. Here's part 2 of my question: Now I want the estimate of that model's performance (precision, recall, f1, etc), should I evaluate all 5 model using my original test set (data it has not seen at any point) and take the mean of the results? For production could I take the fold with the smaller val loss? I know you are supposed to train over all the training data, but the problem is that I would need a validation set for early stopping.
