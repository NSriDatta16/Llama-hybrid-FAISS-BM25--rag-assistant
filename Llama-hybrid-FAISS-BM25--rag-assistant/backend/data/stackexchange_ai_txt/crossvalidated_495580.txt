[site]: crossvalidated
[post_id]: 495580
[parent_id]: 
[tags]: 
How does the Loss of a Neural Network effect training

Suppose I have a neural network, say an input layer taking in vectors $i \in \mathbb{R}^d$ , a hidden relu layer, and then a softmax output layer with a cross entropy loss (with no biases added for simplicity). My question, which starts out abstract but I hope to make more clear, is how does the loss effect training? Suppose also that we are training with Stochastic Gradient Descent, i.e. a weight update of the form $w_{i+1} = w_i - \nabla L (w_i)$ for our cross entropy loss $L$ . At each mini batch of $n$ labelled training examples, we compute $\nabla L (w_i) = \sum_{i=1}^n \nabla L_{w_i}(x_i)$ , and then decrement the weights by this value (assuming for simplicity no regularizations/weight decay etc.) What will be the difference in the update $-\nabla L(w_i)$ if the loss is very large, or very small? Further if at a given step the loss is $0$ , or very close to $0$ , then it seems the update value should be very small, but is this always the case? Any insights appreciated.
