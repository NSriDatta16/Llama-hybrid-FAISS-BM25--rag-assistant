[site]: stackoverflow
[post_id]: 3920449
[parent_id]: 3920432
[tags]: 
The problem is that once you have mojibake , there's no reliable way to convert it back to what it was supposed to mean. See this paragraph at Wikipedia for an explanation of the problem: Consider a text file containing the German word für in the ISO-8859-1 encoding. This file is now opened with a text editor that assumes the input is UTF-8. As the first byte ( 0x66 ) is within the range 0x00 – 0x7F , UTF-8 correctly interprets it as an f . The second byte ( 0xFC ) is not a legal value for the start of any UTF-8 encoded character. A text editor could therefore replace the byte with the replacement character symbol to warn the user that something went wrong. The last byte ( 0x72 ) is also within the code range 0x00 – 0x7F and can be decoded correctly. The whole string now displays like this: f�r . A poorly-implemented text editor might save the replacement in UTF-8 form; the text file data will then look like this: 0x66 0xEF 0xBF 0xBD 0x72 , which will be displayed in ISO-8859-1 again as fï¿½r . The replacement also destroys the original byte, making it impossible to recover what character was intended. You need to avoid incorrectly interpreting text using the wrong encoding from the beginning. Fixing it when it's broken is too late.
