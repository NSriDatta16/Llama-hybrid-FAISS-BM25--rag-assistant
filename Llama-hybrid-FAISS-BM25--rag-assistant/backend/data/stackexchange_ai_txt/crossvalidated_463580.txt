[site]: crossvalidated
[post_id]: 463580
[parent_id]: 463172
[tags]: 
A major problem is the small sample size reducing the degrees of freedom in model selection along with the model's required/sensitivity to normality of error assumption. Preserving degrees of freedom and being robust in methodology appears likely to be the best path. I would even advise generating random errors from possible parent distributions, and with knowledge of the actual parameter values, noting the variation in estimated parameter values and possible changes in test results. As such, a simple parsimonous model approach would be first to place the data in a regression format in accord with the following reduced model in the variable Methods: $$ Y_{i,j}-Ymedian = \beta *InsulinDummy_i + \gamma * MethodDummy_j + \varepsilon_{i,j} $$ where the dependent variable is the observed concentration of glucose centered around the population median, and the Insulin Dummy variable (also centered) is 1/2 if Insulin is present in test sample i, else -1/2. The Method Dummy variable is 2/3 for Method 1, else -1/3 for Methods 2&3 (repeat analysis, swapping out Method 1 for say Method 2, and repeat again swapping out Method 2 for Method 3). Note, the proposed model interpretation of the regression coefficients is that it may aid in accurately determining which side of the median an observation will fall. Given the small sample size, I suggest a probabilistic ( even Bayesian ) interpretation, whose accuracy can be assessed in simulated model testing. Next, the introduction of a robust regression analysis, where Least Absolute Deviations (LAD) is an option. Mathematically, LAD is linked to a Laplace distribution of error terms. One can compute coefficients employing iterative weighted Least-Squares, or, especially in the current context with 6 data points, employing the property that the model parameters determine a straight line that passes through two of the observed points in space. This implies examining permutations and testing total sum of absolute deviations. The selected points nearly always avoid outliers (unlike Least-Squares, where ANOVA also rests on a squared error criterion). To obtain confidence intervals on parameters, bootstrap re-sampling of error terms has been suggested ( see this) , which can also be assessed on accuracy in simulation runs. [EDIT] I thought my model is worthy of further exploration, so I built a worksheet based simulation model (convenient for the iterative LAD iteration, which involves examining point shifting, what points absolute errors are converging to zero (indicative of point pairs determining the LAD regression line). Here is a summary of a dozen simulation runs based on a uniform (-0.5 to +.5) error added to the model proposed above. Actual Underlying Simulated Parameter Values are: 1.250 and 0.100 Simulation Run Values: Average Observed Values 1.225 0.026 Observed Median 1.224 0.045 Run 1 1.001 0.324 Run 2 1.546 0.297 Run 3 1.350 -0.038 Run 4 1.283 -0.115 Run 5 1.593 -0.113 Run 6 1.498 -0.089 Run 7 0.863 0.151 Run 8 1.090 0.323 Run 9 1.102 -0.435 Run 10 1.166 -0.265 Run 11 1.451 0.128 Run 12 0.761 0.146 My take on the results are that the obtained summary statistics are amazing for my proposed parsimonous model based on 6 points with a uniform error distribution estimating 2 parameters on a data-centered model employing robust regression. Individual runs display, as expected, quite a range on the parameter values, but appear to more likely point to an effect greater than 1 for the first parameter (only 2 out of 12 are less than 1).
