[site]: crossvalidated
[post_id]: 151222
[parent_id]: 151216
[tags]: 
Well, yes, assumptions matter -- if they didn't matter at all, we wouldn't need to make them, would we? The question is how much they matter -- this varies across procedures and assumptions and what you want to claim about your results (and also how tolerant your audience is of approximation -- even inaccuracy -- in such claims). So for an example of a situation where an assumption is critical, consider the normality assumption in an F-test of variances; even fairly modest changes in distribution may have fairly dramatic effects on the properties (actual significance level and power) of the procedure. If you claim you're carrying out a test at the 5% level when it's really at the 28% level, you're in some sense doing the same kind of thing as lying about how you conducted your experiments. If you don't think such statistical issues are important, make arguments that don't rely on them. On the other hand, if you want to use the statistical information as support, you can't go about misrepresenting that support. In other cases, particular assumptions may be much less critical. If you're estimating the coefficient in a linear regression and you don't care if it's statistically significant and you don't care about efficiency, well, it doesn't necessarily matter if the homoskedasticity assumption holds. But if you want to say it's statistically significant, or show a confidence interval, yes, it certainly can matter. Much later edit: In many cases of publications I see that apply statistical methods, is not so much over-focus as there is a misplaced focus . That's a form of simultaneous over-focus and under-focus - focusing on some things when something else is the thing at issue or at least, is more important. Attention is paid to characteristics that might not matter at all for the properties of the inference, or may matter little while other things may matter much more. Here I don't just mean the specific assumptions (which is definitely a thing) but also the manner in which they're considered. It's important to understand the behaviour of the tools being used in population situations that may be somewhat like the circumstances you're in, as well as under counterfactuals you might be considering (such as under the null, perhaps, or under different effect sizes from the one that actually applies, such as one you used to compute a sample size). Tests of assumptions are commonly used but do not address these issues. As one example, if you're focused on assumptions made to guarantee the significance level is correct (or very nearly correct), using data that doesn't arise under H0 is not necessarily very telling. It may easily happen that two distributions would be identical if H0 were true (if the treatment had literally zero effect), but the spread or shape may gradually change as the effect increases, yielding noticeably different distributions at some plausible effect size. (e.g. consider increasing test scores due to some effective treatment squeezing up against the highest possible score -- which would change both spread and shape). This observation would have no impact on the behavior under H0 . There's many other situations where there's a focus on the data rather than the population properties of the procedure, which is the thing you're actually worried about. However, what concerns me even more is that I very regularly see people testing assumptions that the procedures they employ do not even make under either H0 (when computing significance level from null distributions given some assumptions) or at some place under H1 (when computing power, given some assumptions). This is astonishingly common. One very common example is checking variables (whether DVs or IVs) for marginal normality. So to an extent there may be some over-focus in some circumstances (which may largely be avoided if the "consider the impact on your inference of likely/potential violations" is the starting point) but the greater danger is simply focusing very strongly on things that weren't even assumptions.
