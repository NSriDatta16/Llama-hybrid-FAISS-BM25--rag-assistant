[site]: crossvalidated
[post_id]: 336825
[parent_id]: 46457
[tags]: 
Pardon my matrix calculus, been some time since I last used it. From my answer to Is Gradient Descent possible for kernelized SVMs (if so, why do people use Quadratic Programming)? , we can write the primal SVM (Hinge-loss with squared-$\ell_2$ regularization) objective as: $$J(\mathbf{u}, b) = C {\displaystyle \sum\limits_{i=1}^{m} max\left(0, 1 - y_i (\mathbf{u}^t \cdot \mathbf{K}_i + b)\right)} + \dfrac{1}{2} \mathbf{u}^t \cdot \mathbf{K} \cdot \mathbf{u}$$ We want to minimize this quantity. Deriving it with regards to $\mathbf{u}$ results in: $$\frac{\partial J(\mathbf{u}, b)}{\partial \mathbf u} = C {\displaystyle \sum\limits_{i=1}^{m} \left [y_i (\mathbf{u}^t \cdot \mathbf{K}_i + b) \lt 1 \right ] \cdot (- y_i \cdot\mathbf{K}_i}) + \mathbf{K}\cdot\mathbf{u}$$ The derivative regarding $b$ results in: $$\frac{\partial J(\mathbf{u}, b)}{\partial b} = C {\displaystyle \sum\limits_{i=1}^{m} \left [y_i (\mathbf{u}^t \cdot \mathbf{K}_i + b) \lt 1 \right ] \cdot\left(- y_i \cdot b\right)}$$ Where $\left[\cdot\right]$ is the Iverson bracket . Notice that the derivative is undefined when $y_i (\mathbf{u}^t \cdot \mathbf{K}_i + b) \equiv 1$, and in keeping with SGD tradition a reasonable value can be assigned to the step, or the sample can be skipped. From these equations, you can derive SGD gradients.
