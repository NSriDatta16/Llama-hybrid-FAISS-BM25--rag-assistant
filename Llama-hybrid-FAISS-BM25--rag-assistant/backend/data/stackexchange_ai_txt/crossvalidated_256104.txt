[site]: crossvalidated
[post_id]: 256104
[parent_id]: 255973
[tags]: 
In a nutshell, ANOVA is adding , squaring and averaging residuals . Residuals tell you how well your model fits the data. For this example, I used the PlantGrowth dataset in R : Results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions. This first plot shows you the grand mean across all three treatment levels: The red lines are the residuals . Now by squaring and adding the length of those individual lines, you will get a value that tells you how well the mean (our model) describes the data. A small number, tells you the mean describes your data points well, a bigger number tells you the mean describes your data not so well. This number is called the Total Sums of Squares : $SS_{total}=\sum(x_i-\bar{x}_{grand})^2$, where $x_{i}$ represents the individual data point and $\bar{x}_{grand}$ the grand mean across the dataset. Now you do the same thing for the residuals in your treatment ( Residual Sums of Squares , which is also known as the noise in the treatment levels): And the formula: $SS_{residuals}=\sum(x_{ik}-\bar{x}_{k})^2$, where $x_{ik}$ are the individual data points $i$ in the $k$ number of levels and $\bar{x}_{k}$ the mean across the treatment levels. Lastly, we need to determine the signal in the data, which is known as the Model Sums of Squares , which will later be used to calculate whether the treatment means are any different from the grand mean: And the formula: $SS_{model}=\sum n_{k}(\bar{x}_k-\bar{x}_{grand})^2$, where $n_{k}$ is the sample size $n$ in your $k$ number of levels, and $\bar{x}_k$ as well as $\bar{x}_{grand}$ the mean within and across the treatment levels, respectively. Now the disadvantage with the sums of squares is that they get bigger as the sample size increase. To express those sums of squares relative to the number of observation in the data set, you divide them by their degrees of freedom turning them into variances. So after squaring and adding your data points you are now averaging them using their degrees of freedom: $df_{total}=(n-1)$ $df_{residual}=(n-k)$ $df_{model}=(k-1)$ where $n$ is the total number of observations and $k$ the number of treatment levels. This results in the Model Mean Square and the Residual Mean Square (both are variances), or the signal to noise ratio, which is known as the F-value: $MS_{model}=\frac{SS_{model}}{df_{model}}$ $MS_{residual}=\frac{SS_{residual}}{df_{residual}}$ $F=\frac{MS_{model}}{MS_{residual}}$ The F-value describes the signal to noise ratio, or whether the treatment means are any different from the grand mean. The F-value is now used to calculate p-values and those will decide whether at least one of the treatment means will be significantly different from the grand mean or not. Now I hope you can see that the assumptions are based on calculations with residuals and why they are important. Since we adding , squaring and averaging residuals, we should make sure that before we are doing this, the data in those treatment groups behaves similar , or else the F-value may be biased to some degree and inferences drawn from this F-value may not be valid. Edit: I added two paragraphs to address the OP's question 2 and 1 more specifically . Normality assumption : The mean (or expected value) is often used in statistics to describe the center of a distribution, however it is not very robust and easily influenced by outliers. The mean is the simplest model we can fit to the data. Since in ANOVA we are using the mean to calculate the residuals and the sums of squares (see formulae above), the data should be roughly normally distributed (normality assumption). If this is not the case, the mean may not be the appropriate model for the data since it wouldnâ€™t give us a correct location of the center of the sample distribution. Instead once could use the median for example (see non parametric testing procedures). Homogeneity of variance assumption : Later when we calculate the mean squares (model and residual), we are pooling the individual sums of squares from the treatment levels and averaging them (see formulae above). By pooling and averaging we are losing the information of the individual treatment level variances and their contribution to the mean squares. Therefore, we should have roughly the same variance among all treatment levels so that the contribution to the mean squares is similar. If the variances between those treatment levels were different, then the resulting mean squares and F-value would be biased and will influence the calculation of the p-values making inferences drawn from these p-values questionable (see also @whuber 's comment and @Glen_b 's answer). This is how I see it for myself. It may not be 100% accurate (I am not a statistician) but it helps me understanding why satisfying the assumptions for ANOVA is important.
