[site]: crossvalidated
[post_id]: 571286
[parent_id]: 
[tags]: 
Bootstrap to Statistically Compare Accuracy of Different Approaches

I am currently dealing with a multi-class classification problem. I have two different approaches (in terms of feature engineering) to this problem. Intuitively, the result is obvious. However, I want to show it statistically. I use random forest algorithm. The sample size is approximately 750, and the number of predictors is 200 and 150, respectively. The additional 50 variables are relevant, but coming from a different domain compared to the initial 150 I have. I plan to bootstrap on train sets, determine the best hyperparameters, and compare accuracy distributions for the best hyperparameters. Statistically, is there anything wrong? Should I train/test split before bootstrap? Is it ok to use all datasets in bootstrap? Do you have any recommendations to improve or make the process statistically more rigorous?
