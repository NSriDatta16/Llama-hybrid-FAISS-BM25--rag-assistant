[site]: crossvalidated
[post_id]: 157000
[parent_id]: 114573
[tags]: 
1) How many records of the 9000 should I use to train the network? 1a. Should I completely randomize the selection of this training data or be more involved and make sure I include a variety of output scores and a wide range of each of the input variables? I would recommend that you read about cross-validation here . The concept is not mathematically heavy. The below figure may be useful to illustrate: With regards to the selection of your subsets for cross-validation, I suspect the 9000 samples span quite a few years, over which the effects of different random variables on your outcome may have changed. I would suggest that you pay attention that each sub-set is homogeneously spread-out along the interval, rather than the first subset consisting exclusively of 1999 records, whereas the final one only contains 2014 records. 2) If I split the data into an even number, say 9Ã—1000 (or however many) and created a network for each one, then tested the results of each of these 9 on the other 8 sets to see which had the lowest MSE across the samples, would this be a valid way to "choose" the best network if I wanted to predict the scores for my incoming students (not included in this data at all)? After reading the cross-validation link above, I recommend that you have a look at @Dikran Marsupial's answer here . The summary is you should use the full data set to come up with the final model. 3) Since the scores on the tests that I am using as inputs vary in scale (some are on 1-100, and others 1-20 for example), should I normalize all of the inputs to their respective z-scores? When is this recommended vs not recommended? This is a good question and has been answered here . 4) I am predicting the actual score, but in reality, I'm NOT that concerned about the exact score but more of a range. Would my network be more accurate if I grouped the output scores into buckets and then tried to predict this number instead of the actual score? I do not see any added value in discretising your outcome variable in this way. One thing to be careful about though is that if you do this, your different ranges will be ordinal . So, category 3 will be closer to category 5 than category 6. I am not sure about neural nets, but if you were learning logistic regression, you may want to train an ordered logistic regression model instead of a general one, which assumes the outcome variable is nominal (e.g. pass/fail , blue/red/green ). 5b) What about -1(below 600), 0(exactly 600), 1(above 600), would this work? I would advise against this, you will see that exactly 600 almost never happens so the probability of getting a score of exactly 600 will be close to zero. Also, while training your model, you will not be able to find many examples with an exact score of 600 so your model most likely will not be reliable. If 600 is the pass/fail threshold, you can just train it as a binary classifier.
