[site]: crossvalidated
[post_id]: 285085
[parent_id]: 199151
[tags]: 
Based on my experience, just saying what i have seen, a few things could cause this: 1. learning rate. if it's too large, it could cause rising loss; just make it smaller, magnitudes smallers 2. in case of RNN, it could all blow up, weights, gradients, loss; there's paper published dealing with that.
