[site]: crossvalidated
[post_id]: 533405
[parent_id]: 533247
[tags]: 
This is just a continuation of my comments above. The short answer to Why do we really use cross-validation (CV)? is to get an estimate of the risk of an estimator . This could be the MSE in regression, misclassification rate (= 1 - accuracy) in classification, and so on. Splitting into the training and test sets, once, and computing the accuracy over the test set is a form of CV. So when you say ... fit and tested. Accuracy is 82%. You have done a form of CV to get this estimate 82%. This would be a very variable estimate if you compute it based on a single random split. If you split the data at random a second time, you get a different result. Now if you average these two values (a form of 2-fold CV), this would be a better estimate of the accuracy, i.e., will have a lower variance. Alternatively, you can use the standard 2-fold CV: Split into equal-sized portions (at random), train on one and test on the other and vice versa; then take the average of the two values. All this does is give you a less variable estimate of the accuracy than your original single-split approach. Part of the standard K-fold cross-validation procedure is shuffling the data at random . For each random permutation you get a different result. This variation tells you something about the variance of the estimate you obtain for the score/risk/etc. Alternatively, you can split into training and test K times at random, and take the average over K splits. The result would be very close to K-fold cross-validation. You can do this procedure multiple times and you get an estimate of the accuracy and a variance for your estimate of the accuracy. Here is some code: import numpy as np from sklearn.model_selection import train_test_split from sklearn import datasets from sklearn import svm from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt X, y = datasets.load_breast_cancer(return_X_y=True) n, _ = X.shape # flip some labels to make it harder idx = np.random.choice(n, int(np.floor(0.2*n)), replace=False) y[idx] = np.mod(y[idx]+1,2) # Pick only the first two features Xs = X[:,[0,1]] nperms = 100 nfolds = [1, 2, 5, 10] res = np.zeros((len(nfolds), nperms)) for fi, nfold in enumerate(nfolds): print(nfold, end=' ') for t in range(nperms): dat = np.hstack((Xs, y.reshape((-1,1)))) dat = np.random.permutation(dat) yp = dat[:,2] Xp = dat[:,0:2] clf = svm.SVC(kernel='linear', C=1) if (nfold == 1): X_train, X_test, y_train, y_test = train_test_split(Xp, yp, test_size=0.5) clff = clf.fit(X_train, y_train) res[fi, t] = clff.score(X_test, y_test) else: scores = cross_val_score(clf, Xp, yp, cv=nfold) res[fi, t] = scores.mean() plt.scatter(np.array(nfolds).repeat(nperms), res.flatten()) plt.savefig('cv_test.png') res.std(axis=1) This is what the result looks like: The standard deviations for the estimate of the accuracy, from the last line of code, are: array([0.0209317 , 0.00511349, 0.00426156, 0.00382858]) Since your task is classification, you are looking at the 0-1 loss. The resulting risk (= 1 - accuracy) depends on more than just the bias and variance of the estimator, i.e., knowing these is not enough to compute the accuracy. (The term "variance of the model" in the question is not clear. We have the variance of an estimator. A model can produce multiple estimators: an estimator of the parameters of the model, an estimator of the response, an estimator of the risk of the model, etc.) In regression with a quadratic risk, the story is different. This particular risk has the bias-variance decomposition. Also, it worth mentioning that you have two things here: An estimator of the parameters of the model. An estimator of the risk of the estimator (1). CV is a way to construct (2) and this estimator of the risk itself has variation as the above code demonstrates (sort of). You can lower this variation to some extend by using more folds (and you can see that there is a limit).
