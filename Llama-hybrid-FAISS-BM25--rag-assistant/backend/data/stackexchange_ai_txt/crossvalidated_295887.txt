[site]: crossvalidated
[post_id]: 295887
[parent_id]: 
[tags]: 
Time normalisation for time series using sliding window

As part of my research into eigenvalue dynamics of financial time series I am reading a paper which introduces the idea of time series normalisation between stocks before computing the zero-lag cross-correlation. The formula provide is as follows: where Ri(t) is the stock returns for 1...N stocks and Ri(t).bar is the time average of Ri over a time window of size T and sigma i is the standard deviation of stock i=1...N. How I am reading this formula is, at time t=10 I take the value of stock i and subtract the average value over the last 10 time periods and divide by the standard deviation for the entire time period, have I got this right? Should I expect a value between 1 and -1 or could my normalised value be greater than these ranges? I have seen in other literature I've see slightly different notation, rather than R's they use G's to denote gain. I've also read about range normalisation between 0 and 1 here but i am not sure if this is the same approach. I would be really gratefully if anyone could help me better understand this approach. And apologies for all the pics and poor syntax, I'm not familiar with inserting mathematical notation in these posts.
