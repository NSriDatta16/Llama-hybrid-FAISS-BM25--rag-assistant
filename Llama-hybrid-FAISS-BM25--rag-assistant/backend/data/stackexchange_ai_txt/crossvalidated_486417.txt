[site]: crossvalidated
[post_id]: 486417
[parent_id]: 486413
[tags]: 
One of the most common ways to check whether or not the model needs whitened data is to actually run it and see the results. However, sometimes that's not an option (at an interview, for example). First off, the reality check: most algorithms do need whitening. So, if no other option is present, say that it does. You will be right most of the time. Now, what about the small number of algorithms that don't need normalized data, what do they all have in common? These algorithms do not combine (by any mathematical operation) two distinct features. The best example here is Decision Trees (and Random Forests). At each leaf, the decision tree splits the dataset using only one feature at a time . Algorithms that themselves provide whitening, namely the Batch Normalization layers in Neural networks. These also don't benefit from pre-normalized data. Algorithms that assume that every feature is independent of others. Like naive Bayes classifier and it's derivatives. Does this help?
