[site]: datascience
[post_id]: 62676
[parent_id]: 
[tags]: 
Transfer learning training accuracy starts from a fixed value

I am training a transfer learning model using tensorflow.keras library for image classification. I am dealing with 7 classes. I have used several pre-trained models trained on 'imagenet'. For example, I am using Xception and unfreezing last 30 layers. When I replace the last layer with softmax layer, the training looks reasonable. The model summary looks like: Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= xception (Model) (None, 5, 5, 2048) 20861480 _________________________________________________________________ global_average_pooling2d (Gl (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 7) 14343 ================================================================= Total params: 20,875,823 Trainable params: 8,990,679 Non-trainable params: 11,885,144 _________________________________________________________________ Number of layers in the base model: 132 Number of trainable layers in the full model: 3 The training for the first two epochs looks like (The training accuracy starts from a low accuracy (10% or so)): Epoch 1/12 525/525 [==============================] - 190s 362ms/step - loss: 0.7438 - accuracy: 0.7314 - val_loss: 0.3813 - val_accuracy: 0.8648 Epoch 2/12 257/525 [=============>................] - ETA: 1:28 - loss: 0.4986 - accuracy: 0.8182 Problem The problem starts, when I add one or more new layers before the final softmax layer. The training accuracy starts from exactly 85.71 . It does not matter how many layers I add or whatever base model I use. The training accuracy starts exactly from 85.71, and after the first epoch, I get a very high accuracy (almost 94-95) for validation set . Within a few epoch, the validation accuracy get over 98%. But when I test it on a separate validation set, the performance is actually worse compared to model described earlier . For example, the model summary is: Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= xception (Model) (None, 5, 5, 2048) 20861480 _________________________________________________________________ global_average_pooling2d (Gl (None, 2048) 0 _________________________________________________________________ dense (Dense) (None, 1024) 2098176 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 7) 7175 ================================================================= Total params: 22,966,831 Trainable params: 11,081,687 Non-trainable params: 11,885,144 _________________________________________________________________ Number of layers in the base model: 132 Number of trainable layers in the full model: 5 The training epochs looks like: Epoch 1/12 525/525 [==============================] - 191s 363ms/step - loss: 0.1902 - accuracy: 0.9225 - val_loss: 0.0971 - val_accuracy: 0.9640 Epoch 2/12 58/525 [==>...........................] - ETA: 2:38 - loss: 0.1351 - accuracy: 0.9470 I change the base model with different models, add/remove layers before the softmax , but the training accuracy always starts from 85.71.
