[site]: crossvalidated
[post_id]: 600664
[parent_id]: 287137
[tags]: 
Ordinary least-squares regression yields the values of the regression coefficients that maximize $R^2$ . As more variables are introduced, $R^2$ will in general increase: the unconstrained model (with $\beta_{coal},\beta_{gas}\in\mathbb{R}$ ) can only have a maximum value of $R^2$ that is greater or equal to the maximum value of $R^2$ for the constrained model (with $\beta_{coal}=\beta_{gas}=0$ ), whether the effect is "significant" or not. Even if the association between the dependent variable and these regressors arose purely by chance (which may be the case if it is not statistically significant), ordinary least square regression will take advantage of this spurious association and maximize $R^2$ accordingly -- which is called overfitting. However, as more data are gathered, it is likely that the improvement of $R^2$ will fade away. This, is why alternative metrics of goodness-of-fit (e.g. adjusted $R^2$ ) are sometimes preferred to $R^2$ , which is spuriously inflated when there are relatively many predictors compared to the amount of data points. In order to avoid overfitting altogether, one may perform Regularized regression (e.g. Ridge regression) instead of Ordinary Least Square regression. Regularised regression will penalise values of the regression coefficients that strongly depart from 0, unless there is strong evidence for large values. This will reduce $R^2$ , especially for small datasets. Alternatively, Bayesian regression can extract the probability distribution of $\beta_{coal}$ in light of the data. Arguably, this probability distribution conveys more information about the magnitude of the effect of coal prices than a comparison of $R^2$ (or its adjusted counterpart) with and without including the variable as a predictor. Besides, Bayesian regression naturally implements some form of regularisation. Notwithstanding these general considerations about $R^2$ as a measure of goodness-of-it, an increase of $R^2$ from 0.527 to 0.528 is so small that it clearly suggests that the effect of these extra variables is not significant anyway.
