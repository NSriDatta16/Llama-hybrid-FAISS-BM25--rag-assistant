[site]: stackoverflow
[post_id]: 274395
[parent_id]: 274196
[tags]: 
"Zlib shrinks it by a factor of about 4x." means that a file of 100K now takes up negative 300K; that's pretty impressive by any definition :-). I assume you mean it shrinks it by 75%, i.e., to 1/4 its original size. One possibility for an optimized compression is as follows (it assumes a 32-bit integer and at most 3 bits changing from element to element). Output the first integer (32 bits). Output the number of bit changes (n=0-3, 2 bits). Output n bit specifiers (0-31, 5 bits each). Worst case for this compression is 3 bit changes in every integer (2+5+5+5 bits) which will tend towards 17/32 of original size (46.875% compression). I say "tends towards" since the first integer is always 32 bits but, for any decent sized array, that first integer would be negligable. Best case is a file of identical integers (no bit changes for every integer, just the 2 zero bits) - this will tend towards 2/32 of original size (93.75% compression). Where you average 2 bits different per consecutive integer (as you say is your common case), you'll get 2+5+5 bits per integer which will tend towards 12/32 or 62.5% compression. Your break-even point (if zlib gives 75% compression) is 8 bits per integer which would be single-bit changes (2+5 = 7 bits) : 80% of the transitions. double-bit changes (2+5+5 = 12 bits) : 20% of the transitions. This means your average would have to be 1.2 bit changes per integer to make this worthwhile. One thing I would suggest looking at is 7zip - this has a very liberal licence and you can link it with your code (I think the source is available as well). I notice (for my stuff anyway) it performs much better than WinZip on a Windows platform so it may also outperform zlib.
