[site]: datascience
[post_id]: 52857
[parent_id]: 
[tags]: 
Is the number of iterations in gradient tree boosting just the number of trees?

I have been searching for a while and I just can't find any indication. When people talk about iterations in algorithms like XGBoost or LightGBM, or Catboost, do they mean how many decision trees i.e. base learners will be built? I.e. XGboost m=100 means the algorithm will build a total of 100 base learners, each calculating and optimizing towards the residual value of the previous prediction? Or is it more like 1 epoch in deep learning?
