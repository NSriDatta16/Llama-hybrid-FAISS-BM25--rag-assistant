[site]: crossvalidated
[post_id]: 472290
[parent_id]: 
[tags]: 
Masking BERT subword tokens for NER?

I have the following named-entity recognition (NER) model: (BERT > LSTM > CRF) Extracting BERT vectors needs that we tokenize the input text using BERT tokenizer which tokenizes the text into subword tokens. Next, these subword tokens are passed through LSTM and finally classified using a final CRF layer. However, the final classification using the CRF layer requires that we mask these subword tokens because we do not want them to be a new state in the CRF layer. These subword tokens are also masked when calculating the loss because we do not want to predict on these subword tokens or make them contribute to the loss. My question is: Do these subword tokens require masking at the LSTM layer?
