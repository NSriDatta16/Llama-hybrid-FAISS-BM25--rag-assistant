[site]: crossvalidated
[post_id]: 422833
[parent_id]: 196686
[tags]: 
There is no reason to wonder whether a variable is a "bad control" anymore. We have simple graphical criteria for deciding whether a variable should be included in the regression equation given your target query and your model. If, for instance, you want to estimate the average causal effect via regression adjustment, "good controls" are characterized by the backdoor criterion . For instance, let me show an example where your case 2 would also be problematic. Consider the model below, where all disturbances $u$ are mutually independent standard gaussian random variables: $$ z = u_1 + u_2 + u_z\\ x = u_1 + u_x\\ y = x + u_2 + u_y $$ Note that $z$ is correlated both with $x$ and $y$ , and $z$ is not an "outcome" (it is a pretreatment variable). Yet, $z$ is "bad control" here, and adjusting for $z$ will bias your effect estimates. This happens because adjusting for $z$ opens a spurious colliding path $x \leftarrow u_1 \rightarrow z \leftarrow u_2 \rightarrow y$ . Here is a simple R code for you to see this in practice: n #> Call: #> lm(formula = y ~ x) #> #> Coefficients: #> (Intercept) x #> -0.002443 0.996894 lm(y ~ x + z) # biased, bad control #> #> Call: #> lm(formula = y ~ x + z) #> #> Coefficients: #> (Intercept) x z #> -0.0009577 1.3976798 -0.8012717 Another interesting example is the following: Again, here $z$ is a pre-treatment variable. But, if you naively "control" for $z$ this will amplify any existing bias. In this case, it turns out you can't obtain an unbiased estimate via adjustment, but you could recover the causal effect using instrumental variables. Here is some R code for you to see this in practice: n #> Call: #> lm(formula = y ~ x) #> #> Coefficients: #> (Intercept) x #> 0.00338 1.16838 lm(y ~ x + z) # even more biased #> #> Call: #> lm(formula = y ~ x + z) #> #> Coefficients: #> (Intercept) x z #> 0.002855 1.495812 -0.985012 This discussion may also be helpful.
