[site]: crossvalidated
[post_id]: 561507
[parent_id]: 
[tags]: 
EM derivation for mixture of t distributions

(Notation is at the bottom.) I'm currently self-studying the Expectation-Maximization algorithm and has learned to derive it for various mixture models including Gaussian mixture models, mixture of Bernoullis, mixture of experts and etc. However, I'm encountering some difficulty when working with mixture of Student distributions. As usual, my process is to derive the expression for the expected data log likelihood, and see which part(s) of that expression need to be estimated / bootstrapped (E step). After this, I can perform the M step easily, using equations similar to MLE. It's convenient to first derive the complete data log likelihood (for mixture of Students): where the log likelihood factors and I'm only showing the part for which I'm having difficulty. Constants have also been removed for brevity. Now, we can plug $\log L_{3c}(\mathbf{\theta})$ into the expectation: where I factored the joint probability to more easily push $z_i \sim p(z_i \mid y_i, \mathbf{x}_i, \theta)$ inside. Here's the problem: now, $\mathbb{I}(y_i = k)$ and $\mathbb{E}_{z_i \sim p(z_i \mid y_i, \mathbf{x}_i, \theta)}[z_i]$ are random variables that Both depend on $y_i$ Are multiplied together which makes it hard to elegantly factor the expectation of their product. So, my question is, What should I do for this step? Notation $\mathbf{x}_i$ is the $i$ -th observation. $y_i$ is the mixture component to which $\mathbf{x}_i$ belongs. $z_i$ is the sampled Gamma value corresponding to $\mathcal{x}_i$ - now, this treats the Student distribution as a Gaussian scale mixture, i.e., an infinite mixture of Gaussians weighted by a Gamma distribution, which is why you are seeing Mahalanobis distance in the derivation above. I'm not making this approach up - see references if you want to dig in this more. References The original paper deriving this algorithm can be found by searching the name "Robust mixture modelling using the t distribution" in Google. It has a derivation, but has been a bit vague about exactly how the expectations were pushed in and calculated. This notation is from an exercise in Murphy's "Machine Learning: A Probabilistic Perspective".
