[site]: datascience
[post_id]: 32152
[parent_id]: 32109
[tags]: 
Whenever we start with any dataset in machine learning, we often assume that all the data features are equally important with respect to the output and one feature should not dominate over other feature. Thatâ€™s GENERALLY the reason we choose to bring all the features to same scale. However, one may raise a doubt here that even if the features are not normalised then the weights assigned to it while learning may help the data set converge to expected output while training. The problem with this is that it will take really long to train and produce results. To choose specific number 0 as mean and variance 1 is just the ease to visualise and keeping such small numbers would help in faster training. Hence, it is suggested to bring all features to same scale smaller enough to train easily. Below link also discusses similar concept. https://stats.stackexchange.com/questions/41704/how-and-why-do-normalization-and-feature-scaling-work
