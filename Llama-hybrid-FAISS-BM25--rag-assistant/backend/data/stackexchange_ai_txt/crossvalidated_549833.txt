[site]: crossvalidated
[post_id]: 549833
[parent_id]: 549720
[tags]: 
I disagree with the consens that this is fine. I think it's not, because I can construct a better model than you did: a hashtable that remembers the entries . Performance estimation Having a model created from data is fine, it's the first step. But this itself is worthless: we need to assess the performance of the model (otherwise, a random model may be better). To assess the performance of the model (and distinguish it from random guessing), we need some data. Overfitting Now you could just use the same data as you already used to assess the performance. But this can lead to a bias: your model could have overfitted to the data and, in an extreme case, "remember" (hash table) the data. ( remark : this is indeed less of an issue with less powerful models such as logistic regression) Low statistics: cross validation As mentioned, ways out are to use resampling methods that do not really reduce the sample size used: This can be done by either using bootstrap methods or cross validation. They have their own advantages and disadvantages, however they tend to perform similar in most real-world cases. *I would suggest you to do Cross-validation to get an estimate, but then, any technique is fine. Why you need this For a paper, where you claim to have developed a model, it seems crucial to provide an unbiased estimate of the performance of the model ( for completeness: or a very strong theoretical motivation ). I understand that it means you would need to redo some stuff, but that should not be a lot actually. And it is also worth to maybe contact someone who understands more of it: if you say you just tried a bit around, there are many many more pitfalls (and possible improvements) that you can have. Data scientists are a thing these days ;)
