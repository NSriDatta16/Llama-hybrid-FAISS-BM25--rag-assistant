[site]: crossvalidated
[post_id]: 168666
[parent_id]: 
[tags]: 
Boosting: why is the learning rate called a regularization parameter?

The learning rate parameter ($\nu \in [0,1]$) in Gradient Boosting shrinks the contribution of each new base model -typically a shallow tree- that is added in the series. It was shown to dramatically increase test set accuracy, which is understandable as with smaller steps, the minimum of the loss function can be attained more precisely. I don't get why the learning rate is considered a regularization parameter ? Citing the Elements of Statistical Learning , section 10.12.1, p.364: Controlling the number of trees is not the only possible regularization strategy. As with ridge regression and neural networks, shrinkage techniques can be employed as well. Smaller values of $\nu$ (more shrinkage) result in larger training risk for the same number of iterations $M$. Thus, both $\nu$ and $M$ control prediction risk on the training data. Regularization means "way to avoid overfitting", so it is clear that the number of iterations $M$ is crucial in that respect (a $M$ that is too high leads to overfitting). But: Smaller values of $\nu$ (more shrinkage) result in larger training risk for the same number of iterations $M$. just means that with low learning rates, more iterations are needed to achieve the same accuracy on the training set. So how does that relate to overfitting?
