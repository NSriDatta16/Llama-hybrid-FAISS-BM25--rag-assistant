[site]: crossvalidated
[post_id]: 559990
[parent_id]: 559945
[tags]: 
If you create a Bayesian model for the standard deviation, you can do this quite easily. Let's start with some assumptions. Let draw $i$ from tube $j$ be modelled as $$ x_{i, j} \sim \mathcal{N}(\mu_j, \sigma(\mu_j)) \>. $$ Here, $\sigma$ is an unknown function of the true concentration. As the true concentration changes, so too does the standard deviation. Note that since we are using a normal distribution, some convenient stuff happens. Namely $\mu_j$ can be estimated by the arithmetic mean of the $x_{i,j}$ , and if $r_j = x_{1, j} - x_{2, j}$ is the residual then $E(x_{1, j} - x_{2, j}) = 0$ and $r_j \sim \mathcal{N}(0, \sigma(\mu_j))$ . So here is what I propose. Let's model the residual as coming from $\mathcal{N}(0, \sigma(\mu_j))$ . Once we estimate $\sigma(\mu_j)$ we can easily compute the CV as a function of the latent concentration. Let's begin with simulating your data and plotting out the residuals versus the estimated mean. library(tidyverse) library(tidybayes) library(cmdstanr) # Number of simulations N % mutate(resid = x1-x2, est_mu = (x1+x2)/2) data %>% ggplot(aes(est_mu, resid))+ geom_point() These data are simulated, but we can see the residuals are evenly distributed around 0 with a variance that changes as the estimated true concentration changes. Now, we'll write a Bayesian model to estimate the $\sigma(\mu_j)$ . There are a lot of ways to do this, but I will take a very easy way and consider $\log(\sigma) = \beta_0 + \beta_s \hat{\mu}$ . The full model is $$ r_j \sim \mathcal{N}(0, \sigma(\hat{\mu_j})) $$ $$ \log(\sigma) = \beta_0 + \beta_s \hat{\mu} $$ $$ \beta_0, \beta_1 \sim \mathcal{N}(0, 1)$$ Here is the Stan model data{ int n; vector[n] resid; vector[n] est_mu; } parameters{ real bs0; real bs1; } model{ bs0 ~ std_normal(); bs1 ~ std_normal(); resid ~ normal(0, exp(bs0 + bs1*est_mu)); } generated quantities{ real resid_ppc[n] = normal_rng(0, exp(bs0 + bs1*est_mu)); } and we can easily fit it. You'll need cmdstanr and tidybayes to run the remainder of the code. model_code = file = write_stan_file(model_code) model = cmdstan_model(file) model_data = compose_data(data) fit = model$sample(model_data) Model runs well. Now, we can use the draws from the model to estimate credible intervals for $cv = \sigma/\hat{\mu}$ . d = tibble(est_mu = seq(0.002, 0.2, 0.005)) fit %>% # Get the draws for the coefficients spread_draws(bs0, bs1) %>% # Peform a cross join so that each coefficient pair # can create a sigma for each est_mu in d left_join(d, by=character()) %>% # create the CV mutate(sigma = exp(bs0 + bs1*est_mu), cv = sigma/est_mu) %>% # Plot the results, one line per sample ggplot(aes(est_mu, log(cv), group = .draw))+ geom_line() The thick black line is comprised of 4000 individual lines, each considered a draw from the posterior. This doesn't look exactly like your plots, and with good reason -- we didn't use the exact form for the standard deviation you used in your simulation. That is always going to be the case. We can see the extent of the poor fit in this plot. Below I've plotted 95% prediction intervals conditioned on $\hat{mu}$ . If the model fit well, then 95% of the black dots would be in the red shaded region. It looks like it might be close, but there is certainly some misfit. The standard deviation is not large enough in most regions, again owing to the fact we misspecified the model (on purpose). You might might want to try using splines rather than modelling the standard deviation as I have, that is if you choose to go this route.
