[site]: crossvalidated
[post_id]: 428826
[parent_id]: 
[tags]: 
Can we extend the XGBoost algorithm to higher order steps?

In XGBoost, the idea is at every round of boosting we add an additional model (a decision tree in XGBoost for trees). This model is learned to optimize the second order Taylor expansion of the loss of the current ensemble model with respect to the current model's predictions. How XGBoost differs from gradient boosting is it minimizes the second order Taylor expansion rather than the first. But why stop there? Couldn't we also minimize the third order Taylor expansion? Is there a practical or theoretical reason this is not done?
