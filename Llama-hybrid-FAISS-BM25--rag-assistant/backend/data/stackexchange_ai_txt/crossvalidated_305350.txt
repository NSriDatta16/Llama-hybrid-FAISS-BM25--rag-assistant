[site]: crossvalidated
[post_id]: 305350
[parent_id]: 
[tags]: 
How does penalizing large weights (using the L2-norm) help prevent overfitting in neural networks?

The effect of applying the L2-norm regularization in neural networks is that it penalizes large weights in the model. How does this prevent overfitting? My assumption is that large weights means that the neurons are heavily reliant on the output from other neurons. For example, in a multilayer perceptron, if one of my neuron has a large weight, it means that it relies heavily on a neuron in the previous layer. Using the L2-norm and forcing the weights to be smaller makes this neuron use the output from the previous layer in a more balanced fashion. Is this reasoning correct?
