[site]: crossvalidated
[post_id]: 301604
[parent_id]: 
[tags]: 
Iterative Deletion Algorithm to Test Robustness?

Just had an idea, and was wondering if there were existing strategies to implement it: Let's say I run some well-specified linear regression and discover that a coefficient of interest (say a treatment variable in an RCT specification) is significant. Is there a strategy for discovering the fewest number of observations that would need to be deleted from your sample in order to negate the effect found for a particular dependent variable? The brute force approach of leaving an observation out, and then every pair-wise observations out, and then every triplet, and so on, is obviously computationally prohibitive. Is there a way to make the search more efficient, though? Looking for high leverage points, for example? This is more than just looking for outliers. Some combination of observations may collectively exert some kind of unique influence, but individually may blend in during visual inspections.
