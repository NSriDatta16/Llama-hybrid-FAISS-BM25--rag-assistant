[site]: crossvalidated
[post_id]: 178215
[parent_id]: 
[tags]: 
How do I optimize a cut-off through cross validation?

Imagine I want to tune a penalized logistic regression and I have imbalanced classes so I want to optimise a F-alpha-measure = 1/(alpha/P +(1-alpha)/R), with alpha=0.25 (arbitrary choice I make which is not to be discussed here). For a given model, let's say a ridge logistic regression (with a lambda shrinkage parameter), I will have as many F-alpha-measures as confusion matrices, so I will optimize the F-alpha-measure wrt the cut-off. To sum up, my modelisation is entirely defined by : one lambda and one cutoff* But this is not clear for me how I get the cutoff during the cross validation for tuning lambda wrt my F-alpha-measure Let's say I have 5 folds, usually I would do a loop : criteria This is not actually code but just the idea of the classic algorithm. But if I want to optimise the cutoff, is it right to do this ? criteria My problem is : what cutoff do I keep ? Does it really make sense to keep one cutoff corresponding to one measure taken on one fold ? Alternatively, I don't really think that the mean of the cutoffs for the lambda maximising the criteria would make sense either. My final guess would be to set cutoff* like any other hyperparameter : criteria_lambda_cutoff Please, I know there are plenty of alternatives for handling imbalanced classes, you might think that the approach is not the best, but my question is really cut-off specific. Thank you very much.
