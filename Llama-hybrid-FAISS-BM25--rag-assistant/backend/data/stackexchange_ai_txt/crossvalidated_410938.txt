[site]: crossvalidated
[post_id]: 410938
[parent_id]: 410934
[tags]: 
The way you are interpreting the coefficients is not quite right. The general interpretation of the coefficient on a dummy variable in a multiple regression is "the expected (or average) difference in the dependent variable between those with $1$ and those with $0$ values of that dummy variable, holding other independent variables constant . If you, for example, only have these two $A$ and $B$ variables as predictors, then the interpretation of the coefficient on variable $B$ is "the expected difference in the dependent variable between someone with the value of $1$ and someone with the value of $0$ of variable $B$ , if they both had the same value of variable $A$ . " That same value of variable $A$ can be anything, not just $0$ . Variable A can be present (i.e., 1) only when Variable B is present (1). I am wondering how I can interpret the estimated coefficient for variable B, because the coefficient for B represent the presence of B while A is 0, which logically does not make sense. I think in these two sentences you may have meant to say that you cannot interpret the effect of $A$ while $B$ is $0$ , because $A$ should be non-existent when $B$ is $0$ . But the regression model does not know that and the $\beta$ 's (marginal effects) it provides apply regardless of what the value of the second variable is. So you cannot just take interpret those coefficients as effects of the first variable for a specific value of the second variable. It is a pretty simple exercise to figure out what each coefficient represents in this kind of regression: $$ Y_i = \beta_0 + \beta_1A_i + \beta_2B_i + \epsilon_i $$ where both $A$ and $B$ are binary dummy variables. You can just plug in the possible values of those variables and see what you get, like this: If $A = 0$ and $B = 0$ : The expected value of $Y$ will be $\beta_0$ . If $A = 0$ and $B = 1$ : The expected value of $Y$ will be $\beta_0 + \beta_2$ . If $A = 1$ and $B = 0$ : The expected value of $Y$ will be $\beta_0 + \beta_1$ . If $A = 1$ and $B = 1$ : The expected value of $Y$ will be $\beta_0 + \beta_1 + \beta_2$ . So, we see that $\beta_2$ is the average difference in $Y$ between individuals with $B = 0$ and $B = 1$ , regardless of whether $A = 0$ or $A = 1$ . If you actually want to incorporate the knowledge that $A$ can be present only when $B$ is present into your model, to allow yourself to interpret results the way you want, you can modify the regression to something like this: $$ Y_i = \beta_0 + \beta_1A_i + \beta_2B_i + \beta_3A_iB_i + \epsilon_i $$ If you perform the same exercise as above with this equation, you'll see that different combinations of coefficients will represent the average difference in $Y$ between someone with the value of $1$ and someone with the value of $0$ of variable $B$ if they both had the value of $0$ for variable $A$ and the average difference in $Y$ between someone with the value of $1$ and someone with the value of $0$ of variable $B$ if they both had the value of $1$ for variable $A$ .
