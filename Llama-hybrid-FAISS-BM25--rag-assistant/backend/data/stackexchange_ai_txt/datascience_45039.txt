[site]: datascience
[post_id]: 45039
[parent_id]: 
[tags]: 
Why do most GAN (Generative Adversarial Network) implementations have symmetric discriminator and generator architectures?

For example, if the discriminator is a vanilla network of n layers, each with n(i) units, then, typically, the generator will also be a vanilla network of n layers, each with n(n-i) units (except the output of the discriminator, where n(n) = 1 whereas for the generator n(0) = NOISE_SIZE). If the discriminator is a CNN, the generator is typically a symmetric "deconvolution network" where the i-th layer is a transposed convolution layer, symmetric to the n-i-th layer of the discriminator. Virtually all implementations I've seen follow this pattern, although I can't see anything in the theory why it would have to be the case. And yet, I had a simple vanilla implementation of a digit drawing GAN trained on MNIST, which worked reasonably well. I tried to improve the discriminator by making it a CNN (with the same architecture which I had working well on recognizing MNIST digits) without changing the generator. The GAN no longer worked and converged to a state where the generator always produced the same gibberish drawing. Intuitively, a better discriminator should help the GAN but it was not the case. It seems that improving the discriminator without equally improving the generator does not work (and vice-versa probably). Is this the reason why people always choose symmetric architectures? To preserve a "balance of skills" between the adversaries? Or is there a deeper reason?
