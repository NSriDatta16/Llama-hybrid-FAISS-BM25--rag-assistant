[site]: datascience
[post_id]: 116318
[parent_id]: 116302
[tags]: 
The model does not need to know the feature names. Those exist only to help you understand what are the values you are seeing. Machine learning works with simple vectors. A vector is the numerical representation of an entity or an observation, and it consists of "features" that are the characteristics of the entity/observation that are quantified with those numeric values. And usually we have a lot of those vectors for training, that form a matrix (that's why tabular data structures like pandas.DataFrame are useful). It doesn't need to (and if you think about it, how would something like this be possible?) understand whether or not something is an age or a monetary cost, as long as the ages and the monetary costs and whatever else we use to characterize our data, exist in the same place in each observation vector. Everything is matrix operations. It works even with "nameless" features, such as vector representations of text (for example, in Word2Vec, each dimension of the output vector does not mean anything specific, and it can't be assigned a meaningful name other than the numeric value of its index). If what you think was the case, we would need completely different kinds of models for each different problem, as all problems use different variables to predict. The only time where feature names are somewhat important when passed into an algorithm is when we need to do some form of feature selection/feature importance evaluation, so it is needed by the library functions to tell us, humans, which features are more important. Also, I am aware that sklearn raises a warning when you try to predict on data that does not have column names when the model was fitted using them, in order to inform you of possible mistakes, but the predictions happen without any issues (because, again, the names of the features are not used in the process).
