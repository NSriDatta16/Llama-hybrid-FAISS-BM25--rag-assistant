[site]: datascience
[post_id]: 103350
[parent_id]: 103344
[tags]: 
T5 is in fact a sequence-to-sequence model, it has an encoder that generates some hidden states representing the input and a decoder that generates the output. When you fine-tune the model you can happily ignore how the model was pre-trained and only train for your specific task as schematically shown in the original Google blog post . For fine-tuning, you just get your supervised training data and feed them as the input and output and train as you would train any other model. There are some minimum examples in the Huggigface Transformers documentation .
