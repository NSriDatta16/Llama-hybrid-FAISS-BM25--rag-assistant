[site]: crossvalidated
[post_id]: 200590
[parent_id]: 
[tags]: 
Help in Expectation Maximization from paper :how to include prior distribution?

The Question is based on the paper titled : Image reconstruction in diffuse optical tomography using the coupled radiative transportâ€“diffusion model Download link The Authors apply EM algorithm with $l_1$ sparsity regularization of an unknown vector $\mu$ to estimate the pixels of an image. The model is given by $$y=A\mu + e \tag{1}$$ The estimate is given in Eq(8) as $$\hat{\mu} = \arg max {\ln p(y|\mu) + \gamma \ln p(\mu)} \tag{2}$$ In my case, I have considered $\mu$ to be a filter of length $L$ and $\mathbf{\mu}$ are $L \times 1$ vectors representing the filters. So, The model can be rewritten as $$y(n) = \mathbf{\mu^T}a(n) + v(n) \tag{3}$$ Question : Problem formulation : ${\mu(n)}$ (n by 1) is the unobserved input and $\{e(n)\}$ is the zero mean with unknown variance $\sigma^2_e$ additive noise. The MLE solution will based on Expectation Maximization (EM). In the paper Eq(19) is the $A$ function - the complete log-likelihood but for my case I do not understand how I can include the distribution of $A, \mu$ in the the complete log-likelihood expression. What will be the complete log-likelihood using EM of $y$ including the prior distribution?
