[site]: crossvalidated
[post_id]: 300917
[parent_id]: 220582
[tags]: 
your question intrigued me as I use the svm() function from this package from time to time. At first, I thought that you did run only one experiment per sample size, and were somehow unlucky with the splitting as suggested by the answer. So I re-run the experiment with your code and found the same pattern (and realized that you indeed did 100 repetitions). Then I increased the sample size to see whether the bias would disappear. Here is what I obtained with sample size of $n=10'000$ and 100 repetitions: Everything seems fins, variability decreases with sample size, you seem to be around the right level of 50. But looking closely, we still have a (relatively small) down bias on average (here $\text{mean}=49.68$) which seems persistent. Looking more closely to your code, I realized that when you generate labels, you force it to be exactly balanced, it is not a random variable anymore. By changing the code to provide balanced class on expectation, now I get this with average of 50.00 (rounded). So to answer your question, yes there is something wrong, your variable of interest is not Bernoulli random variable. You can replace the code to generate labels with the line labels = sample.int(2,n,replace=T) (and don't forget to set the seed for reproducibility).
