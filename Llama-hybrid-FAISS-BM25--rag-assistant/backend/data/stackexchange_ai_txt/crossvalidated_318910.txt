[site]: crossvalidated
[post_id]: 318910
[parent_id]: 
[tags]: 
What does word embedding weighted by tf-idf mean?

The paper that I am reading explains about how it implemented the feature vector used for a twitter sentiment classification task. The first is a simple combination, where each tweet is represented by the average of the word embedding vectors of the words that compose the tweet. The second approach also averages the word embedding vectors, but each embedding vector is now weighted (multiplied) by the tf-idf of the word it represents. I understand the first part which is just basically adding all word vectors of a tweet, but I am not quite sure how to get the second one which is a word vector multiplied by the tf-idf. To get this vector, do I just simply have to multiply the tf-idf vectorizer by the average of word embeddings? What kind of multiplication is it? I am also not sure if the multiplication will work since the shape won't match.
