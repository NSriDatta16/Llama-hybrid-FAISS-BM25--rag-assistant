[site]: crossvalidated
[post_id]: 459831
[parent_id]: 459815
[tags]: 
I see three separate questions here: Why do a sliding window validation? I like this article which addresses splitting data for evaluation of time series models. To quickly summarize, sequential observations are correlated in time series. A normal train/test split which assumes independence of samples doesn't make sense. Sliding window validation (also referred to as a walk forward validation) allows you to test predictive performance at a bunch of correlated time steps. This mimics a possible deployed model where you are retraining your model as you get more data and then predicting just a little ahead. Why do error metrics decline with a longer training window? The most likely reason for MAPE/RMSE declining as your training set gets long is that you have more data and therefor a more robust estimate of your model parameters. Some change in model performance based on training data length is expected, but if it is very pronounced and your data allows, you can up the minimum number of training samples in your training window. Also make sure you are predicting on the same number of test samples forward from each training window. And as always, be sure to considering overfitting/model complexity in addition to error metrics. Should I change model order during walk forward validation? The answer is no. You would use one model (like SARIMA(1,1,1)) and run walk forward validation. You would then choose another model (like SARIMA(1,1,2)) and another walk forward validation. This gives an apples to apples comparison of two models across what is hopefully a valid cross section of data to gauge performance. You can think of walk forward validation as an operation that asks how well did my model do? Changing model order mid way through would not answer that question.
