[site]: crossvalidated
[post_id]: 615230
[parent_id]: 237538
[tags]: 
There is a more model-based view that would be a nice complement to the top answer about SVM. The core insight is to assume a mixture model between positive and negative examples, then learn the mixture fraction separately from the densities. If features for positive examples follow a density $f$ and negative follow a density $g$ , you can learn $f$ from the positive examples. Then for the unlabeled examples you know they are a mixture of a known density $f$ with an unknown density $g$ , and there are established methods you can use in that situation. For example in FDR control, the density of the test stats is a mixture of a known density (for the null hypotheses) and an unknown density (for the alternative hypotheses). Here are a couple of useful references. https://link.springer.com/article/10.1007/s10994-020-05877-5 https://cseweb.ucsd.edu/~elkan/posonly.pdf EDIT: a very important assumption of this approach is that the labeled and unlabeled positive examples follow the same distribution. This may not be true in e.g. gene interaction network inference, where important genes may be better-studied and also produce larger effects. Some methods do allow for a biased selection mechanism but the type of bias must be understood in detail.
