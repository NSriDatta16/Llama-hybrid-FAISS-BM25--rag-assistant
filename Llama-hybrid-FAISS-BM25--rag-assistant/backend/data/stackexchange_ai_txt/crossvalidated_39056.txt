[site]: crossvalidated
[post_id]: 39056
[parent_id]: 39049
[tags]: 
I believe the phenomenon you're referring to is more common to hard-margin SVM classification. Using a vector of term occurrences (by which I assume you mean binary feature modeling), you essentially have an n-dimensional feature space (where n is the number of unique features found in your training data, or the number of features in your training data which passed feature selection) in which each dimension has two possible values--zero or one. In hard margin classification, the solution is the linearly-separating hyperplane which correctly divides the two classes in your data. When you have only two possible values the hyperplane can pass through, it is less likely that a solution will exist. Although this can still happen in the soft-margin setting (which allows for a less-than-perfect solution), I've found it to be less common. Don't take this to mean that binary feature modeling is bad! In my research, I'd say it's the most common type of modeling I use, but your mileage may vary according to the characteristics of your data. You might consider trying out kernel transformations (e.g., Radial Basis Function, or RBF) on your feature space, which can transform the space in which your SVM is classifying into one in which a solution may exist!
