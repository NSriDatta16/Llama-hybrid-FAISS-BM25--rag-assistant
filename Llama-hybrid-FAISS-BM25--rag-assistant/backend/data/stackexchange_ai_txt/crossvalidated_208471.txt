[site]: crossvalidated
[post_id]: 208471
[parent_id]: 208042
[tags]: 
Hypothesis 1: You have applied cross-validation incorrectly. The information encoded in position is somehow related to the outcome. To ameliorate this, you could try not selecting your sets to be adjacent but instead a random partition. That might be enough to "average out" the effect of position. Hypothesis 2: By ignoring the data in position, you're discarding information. But even better would be to leverage the information encoded in position as a feature of the model in some way. I don't know what form that would take here because I don't understand what problem you're trying to solve, but if you're prediction weather on the basis of day of the year, you would want to incorporate the knowledge that yesterday's and tomorrow's temperatures are good predictors of today's. In the weather analogy, your model is getting good marks for predicting tomorrow as similar to today, but failing at predicting three months ahead. Hypothesis 3: You actually want nested cross-validation. You've only one hold-out set, which is an interval of position. Instead, you want an additional $k$ randomly-selected partitions of position as hold-outs. This is the natural extension of hypothesis 1 (folds should be composed as random samples of observations, not convenient samples) to the notion of why cross-validation is desirable (use each observation to score the model, not just some observations).
