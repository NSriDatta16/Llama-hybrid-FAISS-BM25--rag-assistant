[site]: crossvalidated
[post_id]: 454168
[parent_id]: 454165
[tags]: 
From the outside, it's a little difficult to understand what the difference between the simulated likelihood and the "real" likelihood is in your example. In order to answer the question, I will assume that you can evaluate the likelihood function but you have some noise on top. In other words, you want to maximize $f(x)$ , but at best you are able to evaluate $f(x) + \epsilon$ , where $\epsilon$ is a random error term. This also implies you probably can't evaluate something like $f'(x)$ either so standard gradient-based methods are out. If that describes your problem relatively well, you're in luck! Bayesian optimization is a method for addressing exactly that. In a nutshell, we assume $f$ is a Gaussian-Process. This allows us to evaluate a few points of $f(x)$ or $f(x) + \epsilon$ and from this, evaluate the uncertainty of $f(x)$ over a grid of unevaluated points. We then pick the points that have promise (i.e., we are not certain they are not a global max), try those points, update uncertainty, repeat. One thing to note about this method is that the search for "points of promise" over our Gaussian process is particularly slow. This means Bayesian Optimization can be very slow compared to classical methods such as Newton's Method if the evaluation function (and derivatives for Newton's Method) is not too expensive. However, if your target function is expensive to compute anyways (such as cross-validation error) then this cost is irrelevant.
