[site]: crossvalidated
[post_id]: 492319
[parent_id]: 
[tags]: 
general rules of the variance of the posterior distribution

In Bayesian posterior inference, we have the following equation relating posterior to prior and likelihood: $$\pi(\theta|D)\propto \pi(\theta)l(D|\theta).$$ Is there any general rules that quantify the variance of the posterior vs the variance of the prior and variance of likelihood? Of course, without knowing the specific forms of the prior and likelihood, it is impossible to have any exact relations. But I am wondering if some approximate heuristics could be derived, potentially with some assumptions. In particular, if $\pi(\theta)$ is Gaussian and $D$ is also Gaussian around $\theta$ , then $\pi(\theta|D)$ is also Gaussian and we can work out its variance. But is such an assumption reasonable? Or are there better estimates? Having some estimate could be helpful, for example, to determine the approximate number of data points to acquire if a specific uncertainty level of the posterior is desired.
