[site]: crossvalidated
[post_id]: 611792
[parent_id]: 
[tags]: 
Uses of data standardization without subtracting mean

From what I've seen, it is common practice in Deep Reinforcement Learning to standardize certain data. By standardization, I refer to the process of subtracting the mean and dividing by the standard deviation. (Certain Reinforcement Learning projects refer to this as normalization, and I also encountered the term Z-standardization.) One example is the REINFORCE algorithm example code from the Pytorch examples: returns = (returns - returns.mean()) / (returns.std() + eps) The effects are clear: scaling data to have the mean of zero and a variance of one. However, in one project I encountered the mean subtraction being omitted. (It was long ago and therefore can not provide a link.) This is something I did not see anywhere else. Are there cases where omitting the mean subtraction - thus only dividing by the standard deviation - is beneficial in the field of machine learning? If yes, what are those? I would be particularly interested in Deep Learning.
