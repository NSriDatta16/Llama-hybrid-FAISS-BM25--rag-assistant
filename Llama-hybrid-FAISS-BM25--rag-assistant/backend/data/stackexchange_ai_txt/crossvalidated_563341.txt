[site]: crossvalidated
[post_id]: 563341
[parent_id]: 
[tags]: 
How to account for weights of a skewed dataset in a machine learning problem?

I, a novice, have a dataset which I would like to use for multiclass classification. I know that the data is skewed, but luckily, my dataset contains an observation weights column. The observation weights are continuous. Label Weight Feature 1 Feature 2 Class A 0.1 ... ... Class B 12 ... ... Class C 2.4 ... ... The weight distribution looks a bit like this (I am not permitted to disclose the data, so this is a plot of an F-distribution density, but it looks similar enough): For summary statistics and plots, it is quite easy to account for the weights, e.g. use the weighted mean instead of the mean. But I would like to go beyond that and use the data to train a classifier. I would like account for the observation weights to minimize the model's bias. I considered the following approaches, but I run into problems in each of them: Subsampling . I could take a weighted sample (without replacement) from my data, weighted by the provided observation weights. The problem is: How do I know which sample size to choose? I guess this must depend on the distribution of the weights. Oversampling . I could bin the observation weights and apply oversampling. But what method do I apply to bin the data? I have tried fixed-size intervals, but this leads to very large data set sizes because of the long tail of the weights distribution. The few observations with the highest weights are just repeated many times. I can take a weighted bootstrap sample . I am not sure that this is the right way to go. One one hand, I will lose some part of my data, even for large sample sizes. On the other hand, I should end up with an unskewed data set. Also, what sample size would I pick? Apply a weighted loss function when I train my machine learning models. While this is a solution for many machine learning algorithms, it is not helpful in all cases. For instance, a linear discriminant analysis classifier in sklearn does not accept a loss function at fit time. I would prefer a solution that works independently from the applied machine learning model. Any advice?
