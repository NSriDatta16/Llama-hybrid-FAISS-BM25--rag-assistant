[site]: crossvalidated
[post_id]: 436209
[parent_id]: 299920
[tags]: 
After having this question in the back of my head for some years, I think I came to some sort of agreement with myself as to what the " best " approach is: $$\begin{align*} v & \gets \mu v + (1 - \mu) \nabla_\theta J(\theta)\\ \theta & \gets \theta - \eta v\,, \end{align*}.$$ The answers to my questions below might give some arguments as to why I will probably stick to the formulation above: As already noted in the comments, the second version of the exponential average has the (what I would call) advantage that if the learning rate is decreased, we do not simply keep going in the direction we were going thus far (since we make smaller changes to $v$ ), but effectively make smaller updates. In none of the formulations, $\mu$ is really intuitive (to me), but in the last version, the learning rate is clearly the step size of the update . In the second formulation, the learning rate acts as some sort of dampening factor on the gradients rather than a step size. Therefore, I would argue that again the last formulation is more intuitive. By using a more traditional formulation of the exponential moving average (the formula above), $\mu$ is the degree of weighting decrease from the exponential moving average. The simple moving average seems to be little more than some heritage from the early days of momentum. Although it does allow for changing the direction more swiftly, I do not think that there is any compelling reason to prefer it over the exponential moving average. As to where the differences between the two versions listed above come from, we would probably need to ask some mathematical historian. I would be inclined to say that there is no particular reason . Addendum Note that for $t \geq 1$ $$\begin{cases} v_0 = 0 \\ v_t = \mu v_{t-1} + (1 - \mu) \nabla_\theta J(\theta_t) \end{cases} = (1 - \mu) \begin{cases} v_0 = \nabla_\theta J(\theta_1) \\ v_t = \mu v_{t-1} + (1 - \mu) \nabla_\theta J(\theta_t) \end{cases}$$ and $$\begin{cases} v_0 = \nabla_\theta J(\theta_1) \\ v_t = \mu v_{t-1} + (1 - \mu) \nabla_\theta J(\theta_t) \end{cases} = \begin{cases} v_0 = 0 \\ v_t = \mu v_{t-1} + (1 - \mu) \nabla_\theta J(\theta_t) \end{cases}.$$ As a result, the last exponential moving average in the question can be interpreted as a the convex combination formulation from the answer that uses the first gradient at prior instead of zero. This also shows that taking the prior at zero corresponds to scaling the learning rate with $(1 - \mu)$ .
