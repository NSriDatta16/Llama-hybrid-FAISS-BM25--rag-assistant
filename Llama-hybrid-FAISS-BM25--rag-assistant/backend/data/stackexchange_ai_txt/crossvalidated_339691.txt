[site]: crossvalidated
[post_id]: 339691
[parent_id]: 339676
[tags]: 
Imagine B is the current system with a clickthrough rate of 3%. You test A on 1 person and observe 100% clickthrough. Should you change the whole website to A? Why might we choose B even if A performs better in the data: We have prior beliefs that B is better (and our experimental results aren't strong enough to overturn those prior beliefs). Example: Let A be yellow, blinking text. I have strong priors that blinking text is worse in 999/1000 cases; it will take powerful evidence to convince me blinking text is better (rather than a spurious result). Similarly, if a disease is more rare than a test is accurate, most positive test results will be false positives. We are risk averse and measure B more precisely than A. Example: B is the extensively tested drug ibuprofen and A is a new, seemingly effective but potentially deadly painkiller. Even if we estimate A as better, we wouldn't deploy it widely until we're confident A is consistently safe. Applying Bayesian decision theory, point (1) treats the parameter as a random variable and brings prior beliefs into the analysis. Point (2) is that we may be risk averse (which can be formalized using expected utility ). Additional complications: Adjustment costs (eg. implementation costs or user learning costs) would raise the bar for making a change. For example if the change is irreversible, the hurdle to change may be especially high. In a multi-period setting, making the the change has an additional payoff in that we get to learn more about the change (which may have value).
