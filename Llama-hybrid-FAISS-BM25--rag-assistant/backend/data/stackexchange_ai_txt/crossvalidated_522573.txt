[site]: crossvalidated
[post_id]: 522573
[parent_id]: 
[tags]: 
What happens to the weights of a pretrained model on transfer learning?

I want to use a pretrained neural network for two similar (but not identical) classification problems. Let's say I want to use AlexNet for image classification, where in problem A I am interested in classifying images in two classes, and in problem B I am interested in classifying in six classes. I then create two instances of a pre-trained AlexNet and add a last layer with size according to the number of classes I want to classify. I then train these two NNs with a reduced, specific dataset. What happens to the weights of these NNs? I guess that the first few layers will detect more basic aspects, and thus the weights there won't change (therefore the two NNs will be equal in this part), and the last layers will have different weights because they are being fine-tuned for my applications (and therefore my two NNs would differ in this part). Is this correct? If so, how can I know where the point of divergence is between the two NNs that I have?
