[site]: datascience
[post_id]: 57287
[parent_id]: 57286
[tags]: 
There are very few resources that justify number of cells proportional to input. The intuition though is clear from colah's blog. The longer the sequence you want to model, the more number of cells you need to have in your layer. For e.g. if you are using the LSTM to model time series data with a window of 100 data points then using just 10 cells might not be optimal. The goal of any RNN (LSTM/GRU) is to be able to encode the entire sequence into a final hidden state which it can then pass on to the next layer. So even if you look at the older NMT models (sequence to seq for language translation), at a very basic level, it had a layer of an lstm encoding all info from the sequence that was passed to it and then using it to kick start the decoding. So basically, if the number of units are relatively lesser, the ability to encode all the info MIGHT not be optimal. Having said that, we regularly use a) multiples of 32 ( https://svail.github.io/rnn_perf/ ) so inspite of their explanation being vague at best, the way to look at it is that when u declare an output size of say, 100, the RNN will generate square matrices of 100 x 100 with weights in them (that will be adjusted during back prop to give you a final model) and that matrix multiplications of such a matrix will be unwieldy as opposed to a matrix thats a multiple of 32 ( this is totally my intuition again, please correct, if im mistaken) b) also if you use more than a certain number of hidden units, you will end up with the vanishing gradient problem (exploding gradients typically dont occur due to relu activation functions that keep activations between 0 and 1). Hence you are better off going "deeper" than "wider" , where you can stack 2-3 layers of 128 hidden cells "vertically", instead of using 512 hidden cells in one layer (this , i have actually experienced in almost all forms of experiments with text as well as image data) im hoping this clarified matters somewhat
