[site]: crossvalidated
[post_id]: 363389
[parent_id]: 348549
[tags]: 
For the sake of simplicity consider the case of CBOW with one-word context (this corresponds to section 1.1 in Rong's paper). Let us consider a single training sample and assess the time complexity of the updates. For naive softmax, one must compute all the $u_j={{v'}_{w_j}}^Th$ for $j\in \{1,\ldots,V\}$, this has complexity $O(NV)$ where $N$ is the size of the embeddings. Once this is done, store $\sum_{j=1}^V \exp(u_j)$ and all the $\exp(u_j)$ in memory. Then each of the $y_j$ can be computed in $O(1)$ time, yielding a complexity of $O(NV)+VO(1)=O(NV)$. For hierarchical softmax, one merely needs to compute the $\sigma({{v'}_{w_j}}^Th)$ where $j$ ranges in $\{1,\ldots,L(w_j)-1\}$. One property of the tree at hand is $L(w_j) = O(\log V)$, hence a complexity of $O(N\log V)$.
