[site]: crossvalidated
[post_id]: 618787
[parent_id]: 
[tags]: 
Proven approaches for labeling audio data for training a speech-to-text model

To train a deep learning model, for converting speech to text, we need labeled data . How should this data be arranged? I can think of several approaches, and I don't know which approach has already been tried in the world, and which has not yet, and which approach was successful and which was less so. Here are the approaches I thought of: Divide the speech into short and fixed periods of time, for example 40 milliseconds , hoping that each part will catch one letter, then listen to each part and give it the correct label. The advantage of this approach is that you don't have to label words but letters, and we have several dozens of letters in ABC and we gave each letter several examples and we were done. To divide the speech into longer, but fixed periods of time, for example of 1 second , in the hope that each part will capture one complete word, then listen to each part and give it the correct label. [It's a lot of work...] To divide the speech into varying periods of time, each part will be from silence to silence , and what is considered silence will be determined according to what average threshold. Then listen to each part and give it the correct notation. [It's also a lot of work...] Of course, these are all just my thoughts, and I'm interested to know what approaches have been taken by those who have already succeeded in developing deep learning models.
