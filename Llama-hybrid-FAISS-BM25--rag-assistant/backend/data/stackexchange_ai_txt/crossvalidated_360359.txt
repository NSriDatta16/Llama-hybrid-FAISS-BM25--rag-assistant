[site]: crossvalidated
[post_id]: 360359
[parent_id]: 360336
[tags]: 
This is a common problem in machine learning. In a very general scenario you have $M$ models that you want to compare on $D$ datasets using $k$-fold cross-validation. The performance measure doesn't really matter, as long it is a scalar value, accuracy, area under the ROC, F1-measure, etc. all would work. Naively, you would pool the results for each model across all datasets and all folds and compare the mean using e.g. ANOVA. However, this approach is flawed, because results from different folds for a single dataset are clearly not independent, because they have been derived from the same dataset. There are two important papers that deal with this kind of situation. Dem≈°ar proposed a set of frequentist tests that account for correlations and provide a measure termed critical difference to determine whether any set of methods actually perform different from each other. These methods have been implemented in the scmamp R package . However, there is one big problem with frequentist tests: the p-value is a function of the dataset size . You could just run 100 instead of 10 datasets and surely the differences will be significant eventually. In addition, the null hypothesis that all classifiers perform exactly the same is highly unlikely to begin with. Benavoli et al. developed a set of Bayesian tests that address this problem. These methods are preferred, because they allow to define your personal region of practical equivalence (ROPE) and avoids relying on unrealistic assumptions of the null hypothesis to determine significance. There is a notebook with code that illustrates this nicely.
