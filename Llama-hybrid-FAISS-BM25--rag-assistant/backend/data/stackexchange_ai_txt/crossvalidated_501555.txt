[site]: crossvalidated
[post_id]: 501555
[parent_id]: 498272
[tags]: 
First of all the equation is $MSE = Bias^2 + Variance$ and not Bias. Now in MVUE estimators, the Bias is zero and the variance is equal to the CRLB (Cramer-Rao Lower Bound) calculated as follows: CRLB( $\hat \theta$ ) = $\dfrac{1}{-E[\dfrac{\partial^2 \ln P(X;\theta)}{\partial \theta^2}]}$ The MSE then is equal to the variance. To define the Bayes estimators we need to define the cost function $C(\hat \theta,\theta)$ which is the cost of choosing $\hat \theta$ instead of $\theta$ . Then the Bayes estimator is an estimator in which minimizes the expected cost function with respect to the posterior distribution. Mathematically it is defined as follows: $E[C(\hat \theta,\theta)|x] = \int C(\hat \theta,\theta) P(\theta|x) d\theta$ Where $P(\theta|x)$ is the posterior distribution. Bayes estimator $\hat \theta$ minimizes the above expression. Now if you let the cost to be the quadratic cost defined as follows: $C(\hat \theta,\theta) = (\theta - \hat \theta)^2 $ Then the estimator becomes the MMSE (Minimum mean square estimator error) which minimizes the square of residuals or the MSE. The MMSE estimators have the lowest possible MSE among all the estimators since they are designed to minimize the MSE and they are Bayesian estimators. For more information and to better understand the difference between Bayesian (MMSE, LAE, and MAP) vs Frequentistis estimators (Maximum likelihood).please refer to this article Essential Parameter Estimation Techniques in Machine Learning
