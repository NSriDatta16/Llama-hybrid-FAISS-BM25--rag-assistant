[site]: datascience
[post_id]: 82235
[parent_id]: 31320
[tags]: 
In my experience, the two approaches can give quite different results. It doesn't really show in the example you provided because the sizes are similar. However, in some cases in object detection, you can have the same object appear with very different sizes in two images. Your first approach will weigh the IoUs equally but the second will give more weight to the larger object. The scripts I find online to calculate the average IoU seem to do the second approach but in my opinion, each instance of a class should be weighed the same regardless of its size. Edit : The first approach is: $\frac{1}{n}\sum\frac{i_i}{u_i}$ The second is: $\frac{\sum i_i}{\sum u_i}$ . Let's assume that we are detecting two objects the first object is very small and is well detected. It has an intersection of 20 pixels and a union of 21 pixels. The second object is very large and is poorly detected. It has an intersection of 20 pixels and union of 200. The first method will give an average IoU of 0.52 the whereas the second will give an IoU of 0.18. The first approach treats every object detection individually and averages the IoUs effectively giving each object detection the same weight. The second approach is biased toward the larger object.
