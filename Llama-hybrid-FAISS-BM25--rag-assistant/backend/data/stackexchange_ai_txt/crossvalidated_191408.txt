[site]: crossvalidated
[post_id]: 191408
[parent_id]: 191396
[tags]: 
Morten's answer correctly addresses the question in the title -- the figure is, indeed, a ROC curve. It's produced by plotting a sequence of false positive rates (FPR) against their corresponding true positive rates. However, I'd like to reply to the question that you ask in the body of your post. If a method is applied to a dataset, it has a certain FP rate and a certain FN rate. Doesn't that mean that each method should have a single point rather than a curve? Of course there's multiple ways to configure a method, producing multiple different points, but it's not clear to me how there is this continuum of rates or how it's generated. Many machine learning methods have adjustable parameters. For example, the output of a logistic regression is a predicted probability of class membership. A decision rule to classify all points with predicted probabilities above some threshold to one class, and the rest to another, can create a flexible range of classifiers, each with different TPR and FPR statistics. The same can be done in the case of random forest, where one is considering the trees' votes, or SVM, where you are considering the signed distance from the hyperplane. In the case where you are doing cross-validation to estimate out-of-sample performance, typical practice is to use the prediction values (votes, probabilities, signed distances) to generate a sequence of TPR and FPR. This usually looks like a step function, because typically there is just one point moving from TP to FN or FP to FN, at each predicted value (i.e. all the out-of-sample predicted values are unique). In this case, while there is a continuum of options for computing TPR and FPR, the TPR and FPR functions will not be continuous because there are only finitely many out-of-sample points, so the resulting curves will have a step-like appearance.
