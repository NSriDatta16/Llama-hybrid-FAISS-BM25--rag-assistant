[site]: crossvalidated
[post_id]: 389774
[parent_id]: 
[tags]: 
Disproportionate classification accurary between testset and entire dataset (Random Forest)

So I have a multiclass problem (16 classes, 58k samples), for which I decided to use the RandomForestClassifier. After some feature engineering and cross-validation I got a test set (13k samples) accuracy of ~33%, and an F1 score of 30%. Given a baseline of 6.25% (1/16), I am content with the results. While my model seems to be overfitting (91% accuaracy on the training set), I can't get the score any higher as I've done copious amounts of cross-validation already. However, I wanted to see how well the model works on the entire dataset, and I got 79% accuracy and an f1 score of 79%! This is what got me my initial scores: accuracy = forest.score(X_test, y_test) f1score = metrics.f1_score(y_test, forest.predict(X_test), pos_label=list(set(y_test)), average="micro") And this is what got me my surprising ones: accuracy = forest.score(dataset, labels) f1score = metrics.f1_score(labels, forest.predict(dataset), pos_label=list(set(y_test)), average="micro") Now, surely I must have done something wrong, but what?
