[site]: crossvalidated
[post_id]: 641224
[parent_id]: 
[tags]: 
Does the Law of Large Numbers work better for some Distributions?

Here are two popular principles in Statistics: 1) Law of Large Numbers: If $X$ is a random variable with a probability density function $f(x)$ and an expected value $E[X] = \mu$ . If we take a sample of $n$ independent and identically distributed observations from $X$ , denoted as $X_1, X_2, ..., X_n$ , and calculate the sample average $\bar{X_n} = \frac{1}{n}\sum_{i=1}^{n}X_i$ , then as $n$ becomes larger, $\bar{X_n}$ gets closer to $\mu$ . Mathematically, this is expressed as (Weak Law and Strong Law): For any $\epsilon > 0$ . $$\lim_{n \to \infty} P(|\bar{X_n} - \mu| > \epsilon) = 0$$ $$P(\lim_{n \to \infty} \bar{X_n} = \mu) = 1$$ 2) Bootstrap: Let $X = \{x_1, x_2, ..., x_n\}$ be a sample of $n$ independent observations from an unknown probability distribution $F$ with mean $\mu$ . Let $\bar{X} = \frac{1}{n}\sum_{i=1}^{n}x_i$ be the sample mean. We generate $B$ bootstrap samples $X^*_1, X^*_2, ..., X^*_B$ , each of size $n$ , by sampling with replacement from $X$ . For each bootstrap sample $X^*_b$ , we calculate its mean $\bar{X^*}_b$ . Then, the mean of these bootstrap sample means $\bar{X^*} = \frac{1}{B}\sum_{b=1}^{B}\bar{X^*}_b$ converges in probability to the actual mean $\mu$ as $B$ approaches infinity. Mathematically, this is expressed as: For any $\epsilon > 0$ : $$\lim_{B \to \infty} P(|\bar{X^*} - \mu| > \epsilon) = 0$$ Based on these principles, I have the following question: For certain types of distributions, does the Law of Large Numbers require a smaller value of $n$ to achieve the same result (e.g. "distance" to the true mean) compared to other distributions? For certain types of distribution, does the Bootstrap require smaller number of Bootstrap samples to achieve the same result (e.g. "distance" to the true mean) compared to other distributions? As an example, different types of distributions could include: Symmetric Distributions (e.g. Normal), Skewed Distributions (e.g. Exponential) and Multimodal Distributions (e.g. Mixture Distributions, Tweedie Distribution). We can also categorize distributions based on their relative variances (e.g. a Normal Distribution where $\frac{\sigma}{\mu} = 2$ vs $\frac{\sigma}{\mu} = 10$ ). We could also look at the dimensionality of the distributions (e.g. a univariate normal distribution, a bivariate normal distribution, a bivariate normal distribution with with a specific correlation structure, etc.) - does LLN/Bootstrap for lower dimension distribution require fewer samples to estimate the mean vector at the same level of precision compared to higher dimension distributions? Theoretically, I would think that the Law of Large Numbers and Bootstrap would apply to all distributions (provided iid sampling is done correctly) - but is it possible that they might work "better/faster" in some types of distributions compared to other types of distributions (e.g. I think they might work better in symmetric distributions compared to non-symmetric distributions)? Is there some way to quantify this bias-sample size trade off? (e.g. strength/speed of convergence)
