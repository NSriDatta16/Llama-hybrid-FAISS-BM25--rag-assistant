[site]: crossvalidated
[post_id]: 494886
[parent_id]: 332179
[tags]: 
Update on Dec. 6th 2020 : I made a blog post to explain this in details. I finally manage to figure out the reason of weighting KL divergence in VAE. It is more about the normalized constant of the distribution modeled the target variable. Here, I am going to present some output distributions we often use. Most of the notation will follow the book "Pattern recognitions and Machine learning" . Linear regression (unbounded regression): (section 3.1.1 on page 140) - This explains for the weighting KL divergence when using MSE loss The target variable $t$ is assumed to be the sum of the deterministic function $y(\mathbf{x}, \mathbf{w})$ and a Gaussian noise: \begin{equation} t = y(\mathbf{x}, \mathbf{w}) + \epsilon, \qquad\epsilon \sim \mathcal{N}\left(\epsilon | 0, \color{red}{\beta}^{-1}\right) \end{equation} The target variable is therefore modeled as a Gaussian random variable with the log-likelihood given as: \begin{equation} p(t | \mathbf{x}, \mathbf{w}, \color{red}{\beta}) = \mathcal{N} \left( t | y(\mathbf{x}, \mathbf{w}), \color{red}{\beta}^{-1} \right) \end{equation} Given this assumption, the log-likelihood at data points $\{\mathbf{x}_{n}, t_{n}\}_{n=1}^{N}$ is: \begin{equation} \ln p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \color{red}{\beta}) = \frac{N}{2} \ln \frac{\color{red}{\beta}}{2\pi} - \color{red}{\beta} E_{D}(\mathbf{w}), \end{equation} where: \begin{equation} E_{D}(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} [t_{n} - y(\mathbf{x}, \mathbf{w})]^{2}. \end{equation} We often optimize only $E_{D}(\mathbf{w})$ , not the whole log-likelihood $\ln p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta)$ , resulting in ignoring the precision $\color{red}{\beta}$ . This might be fine for conventional regression where the loss consists of only the negative log-likelihood (NLL) $-\ln p(\mathbf{t} | \mathbf{x}, \mathbf{w}, \beta)$ , and the prediction would be the mean of the target variable $t$ . However, the loss in VAE consists of the NLL (or reconstruction loss) and the regularization (KL loss). Therefore, if the weight factor of MSE term (or, $E_{D}(\mathbf{w})$ in this case) is 1, we need to weight the KL divergence with a factor $\beta_{KL} = 1/\color{red}{\beta}$ to be mathematically correct. In practice, people often find a good value of the precision $\beta_{KL}$ through hyper-parameter tuning. Another approach is to learn $\color{red}{\beta}$ by considering it as a learnable parameter which is obtained by minimizing the whole VAE loss function. Logistic regression - This explains the case of binary cross-entropy loss used for black-and-white images Let's consider the case of binary classification. The ground-truth is either 0 or 1, and the target variable $t = p(y = 1 | \mathbf{x})$ is assumed to follow a Bernoulli distribution: \begin{equation} p(t | \mathbf{x}, \mathbf{w}) = \mathcal{B}(t | y(\mathbf{x}, \mathbf{w})) = \left[y(\mathbf{x}, \mathbf{w})\right]^{t} \left[ 1 - y(\mathbf{x}, \mathbf{w}) \right)^{1 - t}. \end{equation} Hence, the NLL in this case is given by: \begin{equation} -\ln p(t | \mathbf{x}, \mathbf{w}) = -\left[ t \ln y(\mathbf{x}, \mathbf{w}) + (1 - t) \ln (1 - y(\mathbf{x}, \mathbf{w})) \right], \end{equation} which is the binary cross-entropy loss. (One can extend to softmax for multiclass classification by using a categorical distribution to lead to cross-entropy loss.) For MNIST (or black and white images) data set, each pixel is either 0 or 1, and therefore, we can use binary cross-entropy loss as the reconstruction loss in the VAE to predict the probability that the value of a pixel is 1. And since the mean of the Bernoulli distribution equals to $y(\mathbf{x}, \mathbf{w})$ , we often use $y(\mathbf{x}, \mathbf{w})$ as pixel intensity to plot the reconstructed images. Note that when using binary cross-entropy loss in a VAE for black and white images, we do not need to weight the KL divergence term, which has been seen in many implementations. Bounded regression (e.g. regression in [0, 1]) - This explains the case of weighting KL divergence when using binary cross-entropy loss for color images As explained in logistic regression, the support (or the label) of a Bernoulli distribution is $\{0, 1\}$ , not $[0, 1]$ , but in practice, it is still employed for color-image reconstruction, which requires a support in $[0, 1]$ , or $\{0, 1, \ldots, 255\}$ . Since our interest is the case for support in $[0, 1]$ , we could find some continuous distribution that has support in $[0, 1]$ to model our prediction. One simple one is the beta distribution. In that case, our prediction would be the 2 parameters $\alpha$ and $\beta$ . Seem complicated? Fortunately, a continuous version of Bernoulli distribution has been proposed recently, so that, we can still use the binary cross-entropy loss to predict the intensive of a pixel with some minimal modification. Please refer to the paper "The continuous Bernoulli distribution: fixing a pervasive error in VAE" or Wikipedia page for further details of the distribution. Under the assumption of the continuous Bernoulli distribution, the log-likelihood can be expressed as: \begin{equation} \ln p(t | \mathbf{x}, \mathbf{w}) = \mathcal{CB}(t | y(\mathbf{x}, \mathbf{w})) = C(y(\mathbf{x}, \mathbf{w})) (y(\mathbf{x}, \mathbf{w}))^{t} (1 - y(\mathbf{x}, \mathbf{w}))^{1-t}, \end{equation} where $C(y(\mathbf{x}, \mathbf{w}))$ is the normalized constant. Hence, when working with VAE involving binary cross-entropy loss, instead of tuning for a weight factor of the KL term - which might be mathematically incorrect, we simply add $- \ln C(y(\mathbf{x}, \mathbf{w}))$ into the loss, and then optimize.
