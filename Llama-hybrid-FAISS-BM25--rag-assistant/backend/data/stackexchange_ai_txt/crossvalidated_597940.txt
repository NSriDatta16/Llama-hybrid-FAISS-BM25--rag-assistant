[site]: crossvalidated
[post_id]: 597940
[parent_id]: 
[tags]: 
Can XGBoost handle dependency between outcomes?

Assume I have the following dataset: Race Id Athlete Id Athlete weight Athlete height Winner 1 1 50 180 0 1. 2 48 177 1 1. 3 52 182 0 2. 2 48 178 1 2. 1 48 178 0 2. 5 50 165 0 2. 7 52 188 0 We consider two races; the first race being ran by 3 athletes, while second by 4 athletes. Then, there is a winner flag associated with the runner that won the race. Assuming that the outcomes are the winner of the races, and we use a multiclass classifier that gives back the probabilities, is there a way to specify a variable number of classes using eXtreme Gradient Boosting (XGB)? If yes, how do I specify the number of classes based on each race, and how do you get the dependencies between observations (Note, the probability for each class in a particular race should sum up to 1). In this scenario we model the race outcome (i.e., which athlete will win) rather than the outcome for each independent athlete, hence the athletes can be considered dependent rather than independent. As pointed in the comments, the explanation above might be vague, so I can summarise with the following two questions: How can you model different choices per event (in this case of a race) using a boosting algorithm ? For example one event (a race in this scenario) can have athletes A, B, C while another race can have athletes B, C Z, P. Each athlete has a set of features. How can you model the dependence between choices (in this case athletes) competing in the same race ? In multinomial logit models, the dependency is respected and the outcome is a subset of probabilities for each athlete to win the race, probabilities which sum up to 1. I am not interested in renormalizing the probabilities of each athlete so their sum is 1, I want to consider the collection of athletes competing against each other as an input to the boosting model.
