[site]: crossvalidated
[post_id]: 595413
[parent_id]: 589824
[tags]: 
This is an extended comment on the good observation of @forecaster that the two extreme values in the data are causing numerical issues that double precision arithmetic can't handle. And in fact it appears that there is no maximum likelihood estimate for $c$ (or at best $c$ is extremely large). The log of the likelihood is given by $$\log(L)=-(k+1) \sum _{i=1}^n \log \left(\left(\frac{x_i}{\sigma }\right)^c+1\right)+(c-1) \sum _{i=1}^n \log (x_i)-c n \log (\sigma )+n \log (c)+n \log (k)$$ One can find the maximum likelihood estimate of $k$ in terms of $c$ and $\sigma$ : $$\hat{k}=\frac{n}{\sum _{i=1}^n \log \left(\left(\frac{x_i}{\hat{\sigma}}\right)^\hat{c}+1\right)}$$ So that makes it a maximization problem of just 2 parameters: $c$ and $\sigma$ . $$n \left(-\log \left(\sum _{i=1}^n \log \left(\left(\frac{x_i}{\sigma }\right)^c+1\right)\right)-c \log (\sigma )+\log (c)+\log (n)-1\right)-\sum _{i=1}^n \log \left(\left(\frac{x_i}{\sigma }\right)^c+1\right)+(c-1) \sum _{i=1}^n \log (x_i)$$ Using Mathematica where we can rationalize the input data (meaning converting the data to rationale numbers) we can avoid numerical round-off errors: x = {5.35382659682693, 4.74764035328555, 6.41688382297086, 4.70194010676229, 4.45113004935033, 4.50611840443683, 8.03060660585978, 4.5695082533874, 4.48052843925116, 5.05598541072842, 4.99368090343132, 4.63153147946491, 4.68443593804105, 5.02826922164524, 210.415754831859, 114.311867979766, 611.092749788117, 4.502751256927, 4.57870681149377, 5.11299025815068, 5.31665861351517, 14.8465590392403, 5.58683130567932, 15.9141068218137, 11.6050958855081, 10.175418658569, 109.160688760136, 8.28179845669206, 4.54159063565775, 22.8318960824204, 8.82622790142304, 4.88362864083956, 9.42213684934793, 84.7237613685393, 7.60704879618404, 46.393548985959, 4.86797703394291, 89.5695808475361, 8.43029961651323, 4.67889028482232, 75.3616899071995, 5.31730051442045, 18.729798449272, 9.9174137219138, 79.9573773033068, 15.069030596827, 66.7713319465588, 11.8175710845523, 7.87392052996755, 3072.60557504056, 14.3601404431787, 5.03732319328952}; (* Rationalize input data *) x = Rationalize[x, 0]; (* Construct log of likelihood *) n = Length[y] logL = n Log[c] - n c Log[\[Sigma]] + n Log[k] + (c - 1) Sum[Log[x[i]], {i, 1, n}] - (1 + k) Sum[Log[1 + (x[i]/\[Sigma])^c], {i, 1, n}] mlek = Solve[D[logL, k] == 0, k][[1]] (* mle for k in terms of c and sigma *) logLcsigma = logL //. mlek /. Log[u_/v_] -> Log[u] - Log[v] // Expand // Simplify A contour plot of the log of the likelihood for $c$ and $\sigma$ looks like the following: ContourPlot[logLcsigma, {c, 20, 450}, {\[Sigma], 4.4, 4.5}, Contours -> {-191.2, -191.3, -191.4, -191.5, -192, -193, -194, -105}, ContourShading -> None, ContourLabels -> (Text[#3, {#1, #2}, Background -> White] &)] This plot suggests that the value of $c$ that would maximize the log of the likelihood is much larger than 450. If we choose a value for $\sigma$ at 4.43 and plot the log of the likelihood (after subtracting off a constant to get a readable plot) for $\sigma$ , we see the following: base = logLcsigma /. {c -> 800, \[Sigma] -> Rationalize[4.43, 0]}; t = Table[{c, (logLcsigma /. \[Sigma] -> Rationalize[4.43, 0]) - base}, {c, 800, 1200}]; ListPlot[t, Frame -> True, FrameLabel -> (Style[#, Bold, 18] &) /@ {"c", "Log of likelihood\n-constant"}] We see that the outrageously large values of $c$ are resulting in larger and larger values of the log of the likelihood. My conclusion is that the maximum likelihood solution doesn't exist for fitting a Burr Type XII distribution (also known as the Singh-Maddala distribution) with this data.
