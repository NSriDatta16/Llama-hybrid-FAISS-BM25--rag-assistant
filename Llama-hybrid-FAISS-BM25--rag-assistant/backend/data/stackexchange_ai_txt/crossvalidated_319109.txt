[site]: crossvalidated
[post_id]: 319109
[parent_id]: 318952
[tags]: 
I don't know why this was downvoted, but I figured out the answer though it may be obvious. The training set is used to train one compression/encoder layer by learning to approximate itself using the training set. Once this is done, the weights / layer that is responsible for the encoding part is saved and paired with a classification layer (e.g. softmax layer) to learn a supervised classifier. This is done by using the same training set as before and fitting them with labels / classes of this training set that weren't used previously. After the classifier is trained, it can be used to make predictions or check performance using the test set. For example, if you already had an autoencoder trained and wanted to use the encoding layer with a softmax layer, you could do the following with keras: # For a single-input model with 10 classes (categorical classification): model = Sequential() model.add(autoencoder.layers[1]) model.add(Dense(10, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Convert labels to categorical one-hot encoding one_hot_labels = keras.utils.to_categorical(y_train, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.fit(x_train, one_hot_labels, epochs=10, batch_size=32) # Overall F1 score f1_score(y_test, np.argmax(model.predict(x_test), axis=1), average='macro') In the stacked autoencoder case, the procedure is the same except with more encoding layers. Discussion about this using keras here and here .
