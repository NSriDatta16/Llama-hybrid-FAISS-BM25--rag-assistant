[site]: crossvalidated
[post_id]: 617123
[parent_id]: 605071
[tags]: 
First, I'm just a graduate student who has just started reading about this topic either and has proven to get things wrong the first time. So may don't put to much on what I say. But since you ask for references, my answer can may can be of any help. I don't know where the quoted text is from, but I think we mixing two viewpoints here. The quoted text wants to use leverage scores to quantify the influence of each data point. If the score is high, the data point has a huge influence on the regression model and deleting or perturbating it can change the solution completely. So one can either (a) assume the the data point with high leverage score is an outlier which arised from wrong measurements and should be excluded from further computations (deterministic method). Or (b) one assumes that the measurements are right and especially this point should be considered, due to its great influence. This is important when applying randomized data reduction techniques like sampling. If you want to reduce the number of rows, say form $n$ to $k$ , you could just draw $k$ row indices from $\{1,\ldots,n\}$ with replacement (for the independence assumption besides w/o replacement would make more sense). This is uniform sampling , i.e. every row has probability $1/n$ to be drawn in one trial. This approach surely doesn't incorporate the different influence of each row. A more sophisticated approach is to calculate the statistical leverage scores and then choose $k$ times the $i$ -th row with the so-called importance sampling probability $p_i = \ell_i/n$ and then rescale the row by an appropriate factor. So an, in terms of leverage scores, influential row has a much higher probability to be included in the sample. This is what the matrix $S_L$ does in your linked lecture notes. They then proceed to show with Chernoff that $S_L$ is embedding the data in a, lets lay simplified, appropriate subspace which doesn't perturbuate the solution "too much". Unfortunately naively implemented the computation of the leverage scores for $A\in \mathbb{R}^{n \times m}$ takes $O(nm^2)$ and thus is not faster than solving for example a overconstrained least-square problem by classic methods like QR/ SVD/ normal equation CG. Thus one want to approximate the leverage scores or reduce the so-called coherence of the matrix. References: Introduction to importance sampling : https://arxiv.org/abs/1104.5557 approximate leverage scores: https://arxiv.org/abs/1109.3843 more randomized LS approaches : https://arxiv.org/abs/1712.08880 extensive intro: https://www.cs.ubc.ca/~nickhar/Book2.pdf I only included resources which I think are suited for a first reading at graduate level and not the important work of these who came up with the ideas. Just ask if you need more references.
