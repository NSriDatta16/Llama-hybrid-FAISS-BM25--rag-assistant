[site]: crossvalidated
[post_id]: 591130
[parent_id]: 
[tags]: 
Tic tac toe AI, general questions

Tl;dr - Very new to AIs and neural networks, trying to make tic tac toe. Trying to use a genetic algorithm that plays against other AIs, and the looser leaves. They seem to never learn even to not play where there already is a X or O. What to do then? Hi, I'm very new to anything related to AIs and neural networks. I have only done one project that was a self-driving car (I followed FreeCodeCamp tutorial), anyways, here is the deal: I want to make a tic tac toe AI, and have structured it like this: I have 27 inputs, the first 9 represent the AIs pieces (so if the AI is playing with X, the first 9 inputs will represent where there is an X on the board), the next 9 are the other players pieces (in this case, all the Os), and the last 9 represent any empty space. I have 2 hidden layers with 16 neurons each, and I guess my first question is this: is this enough/too much? should the hidden layers be bigger or smaller or less hidden layers, or is this enough? My output consists of 9 neurons, that represent each space on the board. An output closer to 1 means that playing there is probably good, and if closer to 0, the opposite. I'm having a lot of trouble trying to teach the AI to not play in an already occupied space, how should I approach this problem? What I'm doing now is that every time an AI does an illegal move, it looses, so I feel it should eventually learn not to, but it's not. I've read someone saying to just pick the largest result from the outputs that represent legal moves, but I think this would lead to an weird AI that doesn't really want to play what it's playing, so what is a better option? The training happens with "battles" between them, when one looses, it's out, and the battles continue until an arbitrary number of "rounds", or until there is only one left. I've also experimented making them "battle" an tic tac toe engine that uses minimax (and I added some randomness so it could loose somethimes), than after the rounds exceed the arbitrary number or only one is left, I have tried two different things: just taking a random AI from all still standing and mutating it for each new AI for the next generation, and also I tried combining a random number of the still standing AIs into one, and mutating that one for the next generation. I've read that you can also train the AI with just an opponent that plays random moves, but I don't see how that would be much different, is it? In all my experimentations I haven't gotten much, even if they get a little better it seems they go back after some time, and never get good enough. This is if I block the illegal moves they try to do, cause if I let them loose after making a random move, they just never learn, never. Should I just block the illegal moves and focus on making they play better from there, or do you have soggestions of what to do? I've also seen that Q-learning is a more normal way of doing this, but I really wanted to do this genetic algorithm. What should I do? any piece of advice is wellcome, also, sorry for the big question.
