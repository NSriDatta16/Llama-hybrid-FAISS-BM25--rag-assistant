[site]: datascience
[post_id]: 108075
[parent_id]: 
[tags]: 
How to evaluate the quality / trustworthiness of textual information?

I have a corpus of text (which can be used for learning). The text consists of proper names like street names: Bond Street Balmain Crescent Parkes Way Barrine Drive Gordon Street Marcuse Clarke Street I want to detect possible spelling errors (i.e. evaluate the quality / trustworthiness of some text such as street names). The idea is that a sequence of strings can be evaluated based on the likelyhood that certain strings occur in a certain order (I suppose sequence-to-sequence models and markov chain models strongly draw from this idea). Thus even if a sequence such as Marcuse Clarke Street may occur seldom, the observed sequences of strings such as Cla, lar, ark, rke may occur more often than (assuming a spelling error like Marcuse Cqarke Street ) Cqa, qar, ark, rke . Question: Are there models to estimate the probability that some sequence of strings is "true" given some training data? My objective would be: Input: Street -> Model output: very likely (high probability that it may occur) Input: Stxeet -> Model output: very unlikely (low probability...) Input: Balmain -> Model output: likely (OK probability that it may occur) Input: Buwmain -> Model output: unlikely (low probability...)
