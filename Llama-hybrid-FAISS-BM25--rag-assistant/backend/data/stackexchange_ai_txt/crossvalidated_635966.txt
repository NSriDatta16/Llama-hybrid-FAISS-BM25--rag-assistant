[site]: crossvalidated
[post_id]: 635966
[parent_id]: 
[tags]: 
Does back propagation with gradient descent update the weights and biases using the gradient of the loss with respect to the weights?

I am an absolute beginner in this sector. Currently learning supervised machine learning from Course era under the instruction of Andrew Ng. I am on my 2nd week in the 2nd course of that sequel. I am actually confused about the back propagation. I understood the theoretical stuff of back propagation which is basically chain rule of differentiation and stores intermediate derivatives to find prior derivatives which saves a lot less iterations and time compared to the conventional process. But I couldn't exactly understand how is back propagation integrated in gradient descent. let me elaborate my confusion then I hope you can get where I am stuck. Suppose there are 1 hidden layer in the network. So there are in total 3layers the initial one, the hidden one and the final one. I know that here we assign random weights and bias. But how do we assess the gradient descent? I mean in case of linear regression we have seen that weights and bias are changed with the association of learning rate and derivative of cost function with respect to weights/bias. $$w_1 = w_1-\alpha*\frac{d}{dw_1}J$$ where J is the cost function. Does the same thing happens for neural network as well? I mean $$w^3 = w^3 - \alpha*\frac{d}{dw^3}J$$ $$w^2_n = w^2_ n - \alpha*\frac{d}{dw^2_n}J$$ where $w^3=>$ weights of 3rd layer and $w^2_n=>$ weights of 2nd layer nth unit
