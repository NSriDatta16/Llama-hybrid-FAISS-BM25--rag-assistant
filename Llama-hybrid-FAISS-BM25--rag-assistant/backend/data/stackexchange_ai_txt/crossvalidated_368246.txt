[site]: crossvalidated
[post_id]: 368246
[parent_id]: 
[tags]: 
logistic regression formula gives inaccurate decision boundary

I have the following function: def binary_classify(X1, X2, Y, classA): b0 = 0.0 b1 = 0.0 b2 = 0.0 learning_rate = 0.3 total_epochs = 100 for _ in range(total_epochs): for x1, x2, y in zip(X1, X2, Y): if x1 != x1 or x2 != x2 or y != y: continue y = 1.0 if y == classA else 0.0 prediction = None try: prediction = 1.0 / (1 + math.exp(-(b0 + b1 * x1 + b2 * x2))) except OverflowError: prediction = 0.0 modifier = learning_rate * (y - prediction) b0 += modifier b1 += modifier * x1 b2 += modifier * x2 return (b0, b1, b2) X1 , X2 are 2 columns of variables, Y is a column of different classes each variable falls into, classA is the class we are testing against (one vs all) I have then plotted a graph displaying a scatter plot with each color being a different class. I also plotted a line for each color which represents the decision boundary. The desicion boundary is calculated via the formula: y = (-b1 / b2)x - (b0 / b2) The problem is that the red decision boundary didn't even fit on this plot, and the blue one cuts the blue dot cluster in half... so something seems wrong. additional notes: The values b0, b1, b2 actually seem to diverge when i increase the total_epochs value however the decision boundary line doesn't change. replacing this line: modifier = learning_rate * (y - prediction) with: modifier = learning_rate * (y - prediction) * prediction * (1 - prediction) gives a different plot with converging values of b0, b1, b2 , however the red decision boundary still looks wrong and is apparently bad for multi classification
