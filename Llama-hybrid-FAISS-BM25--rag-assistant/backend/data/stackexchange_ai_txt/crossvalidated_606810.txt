[site]: crossvalidated
[post_id]: 606810
[parent_id]: 606804
[tags]: 
It would certainly seem simpler to do so [take a ratio], because you wind up with one variable instead of two. However, you are invoking certain assumptions (which I'm not sure apply) based on your problem description, and the probability model of a ratio can be extremely poorly behaved whereas a regression model can handle two variables separately with improved precision. If you look into the literature on this problem, most discussion on ratios are against using them for one or both of these reasons. Conversely, papers that actually use ratios rarely if ever motivate or justify their use, except to say that they're replicating previous methods. Theoretically, ratios can be well behaved in large samples. But because dividing two quantities is a convex operation, the resulting variance will always be bigger, and the testing less inefficient. It can be infinitely less inefficient in fact. For instance, if $X_1, X_2$ are iid standard normal, their ratio is Cauchy and has non-existent variance (or mean)! Be cautious about what a "large" sample is, however. Even the simple logistic regression model fools some folks because, there may be a particular stratum that has few cases, and you will find that the stratum-specific odds ratio for the outcome has a highly irregular sampling distribution, and the resulting inference, without correction for sparse strata, is not well behaved. A relatable example: pizza. If I make 3 cheese pizzas and 1 "special" pizza at the same time. The cheese pizzas may initially sell more quickly because they are cheaper than the special, however, over time the cheese may split and look desiccated whereas the special pizza, because it's topped with delicious vegetables, retains appeal. This violates the property of independent interarrival times. Only if the sale process were truly IID exponential, and I model sales over a fixed time interval, will the number of sales over time be the MLE for the rate $\hat{\lambda} = \text{sales}/\text{time}$ according to a Poisson process, and in this case the ratio is well motivated and its error is known to be $\sqrt{\hat{\lambda}}/n$ . Once the intensity is non-constant, more advanced methods are required. For more reading: Kronmal's Fallacy of the Ratio describes the issue of accidentally taking as the denominator a value that has residual covariance with the regressors in a regression modeling context https://doi.org/10.2307/2983064 A reference to understand queueing theory and modeling non-independent processes is Sheldon Ross Probability Models or Alan Agresti Categorical Data Analysis.
