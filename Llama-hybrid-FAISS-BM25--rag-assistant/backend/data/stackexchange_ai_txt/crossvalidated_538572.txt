[site]: crossvalidated
[post_id]: 538572
[parent_id]: 538424
[tags]: 
Insofar as the problem is calculating a robust variance, the answer from AdamO (+1) solves the problem. You can calculate a robust variance estimate for a Cox model with only 1 data point per individual, if you specify robust = TRUE or include a cluster(id) term in the model with id values specified for the individuals. For a Cox model, Therneau and Grambsch explain in Chapter 7 how the matrix product $D'D$ provides a robust "sandwich"-type estimator of the coefficient covariance matrix, where $D$ is the matrix of "dfbeta" residuals (score residuals scaled by the covariance of coefficient estimates). The "robust" variance is an adjustment like that with generalized estimating equations (GEE). (As Thomas Lumley notes in a comment, that's different from a frailty(id) term, which is like a mixed model.) You can do that directly with your data set. The question is whether a robust variance estimate is what you should focus on here. Yes, as AdamO explains here , with a robust variance estimate you can do inference on the Cox model coefficients when the proportional hazards (PH) assumption is violated, and those coefficients (whose point estimates will be the same as in the usual Cox model) can be thought of as an average over all events--depending on the data, perhaps close to a time average. There might, however, be even more to learn from investigating the reason for the apparent failure of PH. First, it's possible that the massive scale of your data set gives a "statistically significant" violation of PH that doesn't really matter in practice. That's like the case with normality testing of large data sets --as hazards are never perfectly proportional and real data are never exactly distributed normally, large data sets can lead to "statistically significant violations" of model assumptions that have no practical significance. Look at the plots of scaled Schoenfeld residuals over time to see just how big you PH problem is. AdamO discusses the problem of PH in large data sets here . Second, if the PH violation is large enough to be practically important, that might be a sign that something is wrong with your model. Therneau and Grambsch say on page 148: There are several possible model failures that may appear as time-varying coefficients but would be dealt with more fruitfully by another approach. These include the omission of an important covariate, incorrect functional form for a covariate, and the use of proportional hazards when a different survival model is appropriate. Omission of an important covariate will lead to PH violation even if the omitted covariate is balanced between groups. As the at-risk population changes over time with respect to the omitted covariate, "the operative hazard is the average hazard of those at risk at each time point, a mixture of hazards" (Therneau and Grambsch, page 150). There is also a bias toward 0 in the coefficient estimates of the included predictors. On page 150, Therneau and Grambsch also say "a finding of lack of proportionality should lead one to check functional form diagnostics." Something as simple as a log transformation of a predictor might provide adequate PH. It's also possible that the underlying survival phenomenon doesn't follow PH. As Therneau and Grambsch note, an accelerated failure time (AFT) or an Aalen additive model might be called for instead. (A Weibull model assumes both AFT and PH, so use a different AFT distribution if you want to deal with a PH violation.) With this large a data set, you presumably have an opportunity to evaluate such possibilities in some detail. You might not have to limit yourself to accepting the violation of PH.
