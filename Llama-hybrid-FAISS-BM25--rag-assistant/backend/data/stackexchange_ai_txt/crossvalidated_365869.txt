[site]: crossvalidated
[post_id]: 365869
[parent_id]: 365269
[tags]: 
You choose vocab size for each language separately and simplest approach is just to preserve all words but usually you skip most common stop words ('a', 'an'...) and those very rare (e.g. occuring only once). So you end with two vocabs with different sizes and they don't need to be the same / similar. And effectively you have two vocabs, e.g. input, target, the architectures/solution are/should support that. When you create embeddings (for each language separately), they have features that are not one-hot encoded words - these are 'latent features' that encode meanings of specific words in a new space of selected dimensionality, much lower than size of vocab, e.g. 100 or 300. Have a look e.g. here: https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f
