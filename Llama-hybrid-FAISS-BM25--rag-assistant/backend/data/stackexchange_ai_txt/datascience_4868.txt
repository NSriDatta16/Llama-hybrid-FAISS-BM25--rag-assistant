[site]: datascience
[post_id]: 4868
[parent_id]: 4845
[tags]: 
The similarity function at the core of the method will define all the values for your distances $d_1,d_2, \ldots, d_n$. The initial query should have some words as a reference point to compare to the words in the document. Not knowing whether the query is a sentence or arbitrary list, you are restricted to a method that does some kind of histogram comparison of the frequency of the words matching in the documents. You can perform naive summations of keyword mappings counts, look at keyword likelihoods in the normalized distributions, or give a distribution of weighting based on the strongest matches. More exotic functions will be based on your prior belief of how the words should be compared. Working within a Bayesian Framework you can see your prior assumptions explicitly. Cosine similarity or any other vector based measure will be slightly arbitrary without knowing the desired nature of comparison between query and document. There is not much more you can do without looking at some type of features, or attempt to cross compare the documents together, or use the initial query's structure. In short, my answer is to use normalized frequency similarities of the document to the queries and produce a ranking, and with more specific goals in mind to apply measures like cosine similarity on test datasets to search for the best measure.
