[site]: datascience
[post_id]: 75186
[parent_id]: 
[tags]: 
Why N-pair Loss (NIPS 2016) stops minimizing in Image retrieval task?

Currently, I am using a Deep Learning model to build a search engine for retrieving images. With a dataset of pairs of (image, description) , I am using a joint model as shown in the figure below to generate embeddings for both image and description. In the training time, the model must learn to generate similar embeddings for images and descriptions that are in the same pair, and differents embeddings for images and descriptions of different pairs. This is done using the N-Pair Loss (NIPS 2016) described below: import torch from torch import nn class NPairsLoss(nn.Module): """ The N-Pairs Loss. It measures the loss given predicted tensors x1, x2 both with shape [batch_size, hidden_size], and target tensor y which is the identity matrix with shape [batch_size, batch_size]. """ def __init__(self): super(NPairsLoss, self).__init__() self.ce = nn.CrossEntropyLoss() def similarities(self, x1, x2): """ Calculates the cosine similarity matrix for every pair (i, j), where i is an embedding from x1 and j is another embedding from x2. :param x1: a tensors with shape [batch_size, hidden_size]. :param x2: a tensors with shape [batch_size, hidden_size]. :return: the cosine similarity matrix with shape [batch_size, batch_size]. """ x1 = x1 / torch.norm(x1, dim=1, keepdim=True) x2 = x2 / torch.norm(x2, p=2, dim=1, keepdim=True) return torch.matmul(x1, x2.t()) def forward(self, predict, target): """ Computes the N-Pairs Loss between the target and predictions. :param predict: the prediction of the model, Contains the batches x1 (image embeddings) and x2 (description embeddings). :param target: the identity matrix with shape [batch_size, batch_size]. :return: N-Pairs Loss value. """ x1, x2 = predict predict = self.similarities(x1, x2) # by construction, the probability distribution must be concentrated # on the diagonal of the similarities matrix. # so, Cross Entropy can be used to measure the loss. return self.ce(predict, target) During the training process, the loss starts to decrease as expected, but for some reason, it gets stuck at a value (would it be a local minimum?) where all image embeddings are equal to all description embeddings . That is, the similarity matrix has all entries equal to 1 . While what is desirable is that the diagonal value of the similarity matrix was always greater than the other values outside the diagonal. Could someone give me some clarification of what may be happening?
