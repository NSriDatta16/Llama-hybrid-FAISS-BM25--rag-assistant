[site]: datascience
[post_id]: 108161
[parent_id]: 108144
[tags]: 
Taking the average (or any other statistic combo) of the embedding vector for each word is NOT a good idea because the embedding dimensions are independent & you will loose a lot of nuance of the learned space. You could do a few things however. First would be to do PCA on the embedding matrix to reduce variance a little bit say to 45/50 dimensions. This is a quick hack so donâ€™t over do it. Ideally you would train another embedding of of this one into a smaller dimensionality space say of 10-30 dimensions that gives you good results for your task. You could remove unused or very rare words from your embedding altogether. You could also normalize your embedding vocabulary to exclude case sensitive and accented words. From an engineering standpoint if fitting in RAM is still an issue you could also load chunks of the embedding vocabulary into memory at a time via some caching mechanism like LRU.
