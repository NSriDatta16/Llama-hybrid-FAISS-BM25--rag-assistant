[site]: crossvalidated
[post_id]: 221619
[parent_id]: 221530
[tags]: 
As pointed out by others, logistic regression tries to estimate the probability of the outcome occurring so its outputs can be these probability estimates. If whatever software you are using is just outputting {0,1} for each case it may be splitting based on some cut off probability, say if it estimates that for record 1 the probability of future offending is more than 50% then it outputs 1, and if less than 50% it outputs 0. There may be a way to ask for the probability estimates explicitly, or the log odds. With 100 samples you're not going to be able to use 49 categorical variables. There's likely some states which don't have any records in so how could you possibly guess the impact of being from that state? The same is actually true for any state with very few records. If you had a state which had, for example, at least one sample with no future criminal activity and no samples with future criminal activity (I would be quite surprised if you didn't have this) then the model will predict that no-one from that state will ever commit a crime in the future. Obviously this isn't true and highlights just a particularly obvious failure to generalise from the sampled data to the population overall. It's (sort of*) an example of what we call overfitting, which you should definitely make sure you understand when trying to draw conclusions about the total population from your model. There is no simple answer to the question 'how to I determine which variables are relevant and which are not?' There are particular problems with using p-values on tests of 'coefficient = 0' and I would agree with what others have said that a method called the lasso is better (it will actually set coefficients to exactly 0 if it deems them not sufficiently predictive) if your goal is prediction. I don't know how well I can explain this distinction in a short answer but basically, if you're going to use the model to identify people at higher risk of future offending (prediction) then that should be fine. If you want to know what policies should be implemented to reduce crime rates (or, what causal influence do the things you've measured have), the model can't help you there. There are totally different techniques under the heading 'Causal Inference' which may be appropriate depending on what your analysis is going to be used for. Has been answered elsewhere. Splitting the data like this is (mostly) an attempt to avoid the problem of overfitting. If the predictions made on the test set (which the model didn't get to 'learn' from) are accurate, you can be more satisfied that they will continue to be accurate on the population at large. Again, your small sample size is going to make it very hard to be satisfied in this way (well, to even decide how satisfied you are). There are lots of more advanced options to help with this problem such as resampling techniques (cross-validation and/or bootstrapping), incorporating prior information via a bayesian model, or better yet, finding explicit generalisation error bounds from VC theory. Of course, probably none of these suggestions is helpful if it's going to take too long for you to understand how to use them and what they tell you. Take the thousands of journal pages filled with attempts to solve this problem satisfactorily as evidence that it's a very hard problem in general and in the face of that difficulty, the approach of virtually all statisticians is a great deal of humility and modesty in making claims about reality based on an analysis like this. Class imbalances (more 0s than 1s) won't affect your ROC curve directly, if it's pretty close to the diagonal that just shows that the inputs can't give a particularly accurate prediction of the outcome. Class imbalance can make your model worse and there are techniques like SMOTE, noisy PCA etc. Try googling 'class imbalance problem'. It's certainly much less of a problem when you use a measure of model 'goodness' which is insensitive to class imbalances (such as area under ROC, F1 score, cohen's kappa) instead of raw accuracy. I would recommend against spending effort here. *It could also be seen as failing to take into account sampling error, but I don't really think a discussion of inference vs induction is particularly useful here, and I'm trying to emphasise the dangers of pretending (or not realising that you are pretending) that 'statistics' has distilled pure (or at least 95%) certainty out of uncertainty.
