[site]: crossvalidated
[post_id]: 329230
[parent_id]: 329026
[tags]: 
If the activation used is an even function: $f(x)=−f(−x)$, then one could choose to "flip" a neuron by flipping the sign of all the input and the output weights associated with that neuron. The ability to do this or not to do this for each neuron gives $2^H$ total choices. Both the tanh function and the identity activation (no activation) are often used in neural networks and fit this criteria.
