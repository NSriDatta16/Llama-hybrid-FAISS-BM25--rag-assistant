[site]: crossvalidated
[post_id]: 333737
[parent_id]: 
[tags]: 
Maximum likelihood is not re-parametrization invariant. So how can one justify using it?

There is something that is confusing me about max-likelihood estimators. Suppose my I have some data and the likelihood under a parameter $\mu$ is $$ L(D|\mu) = e^{-(.7-\mu)^2} $$ which is recognizable as the likelihood of Gaussian upto scaling. Now my max likelihood estimator will give me $\mu=.7$. Now suppose I didn't know that and instead was working with a parameter $t$ such that $\mu=\sin(t)$. Also suppose all this were numerical and so I wouldn't immediately see how silly the following likelihood looks like $$ L(D|t) = e^{-(.7-\sin(t))^2} $$ Now I would solve for the max likelihood and get additional solutions. To help see this I plot it below. So from this point of view max-likelihood seems like a silly thing to do as it is not re-parametrization invariant . What am I missing? Note that a Bayesian analysis would naturally take care of this since the likelihoods would always come with a measure $$ L(D|\mu) P(\mu) d\mu = L(D|\mu(t)) P(\mu(t)) \frac{d\mu}{dt} dt $$ Added part after responses and comments (added on 3/16/2018) I realized later that my example above is not a good one because the two maxima in $t_1,t_2$ correspond to $.7=\sin(t_1)=\sin(t_2)$. So they are identifying the same point. I have kept the above for the discussion and responses below to make sense. However, I think the following is a better example of the issue I am trying to figure out. Take $$ L(D|\mu) = e^{-(a-\mu)^2} $$ Now suppose I reparameterize $\mu=\mu(t)$ then do a max-likelihood with respect to $t$ I get $$ \frac{\partial L}{\partial t} = \frac{\partial L}{\partial \mu} \frac{\partial \mu}{\partial t} $$ If I want a maxima at a location other than from the one I get from maximizing with respect to $\mu$ I require $$ \frac{\partial L}{\partial \mu} \ne 0 $$ and $$ \frac{\partial \mu}{\partial t} =0, \qquad \frac{\partial L}{\partial \mu} \frac{\partial^2 \mu}{\partial t^2} Thus, I can take a simple example $$ \mu = b - (a-b)t^2+t^3 $$ I plot the results below. We can clearly see that $\mu=a$ is the global maxima (and only one when maximizing with respect to $\mu$) but we also have another local maxima at $t=0$ when we maximizing with respect to $t$. Note the map $\mu(t)$ is not bijective but I do not see why it has to be. Also, at least in this example, the global maxima will always be the one at $\mu=a$ but from a frequentist point of view wouldn't I be oblidged to take some kind of weighted average of 1/1.6 of $\mu=a$ and .6/1.6 of $\mu=b$ (that corresponds to $t=0$) if I completely worked in the $t$ space?
