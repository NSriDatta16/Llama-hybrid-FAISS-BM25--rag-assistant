[site]: crossvalidated
[post_id]: 51874
[parent_id]: 42948
[tags]: 
I do not believe that optional "stopping rules" is a technical term in regards to optimal stopping. However, I doubt that you will find much in-depth discussion on the topic in intro-level psychology statistics textbooks. The cynical rationale for this is that all social-science students have weak math skills. The better answer, IMHO, is that simple t-Tests are not appropriate for most social science experiments. One has to look at the effect strength and figure out if that resolves the differences between groups. The former can indicate that the latter is possible but that's all it can do. Measures of welfare spending, state regulation, and urbanization all have statistically significant relationships with measures of religious behavior. However, just stating the p-value is framing the test in an all-or-nothing causal relationship. See the following: Results from both welfare spending and urbanization have statistically significant p-values but welfare spending is much more strongly correlated. That welfare spending shows such a strong relationship to other measures of religiosity ( non-religious rate as well as comfort in religion ) for which urbanization doesn't even attain a p-value of , suggesting that urbanization doesn't impact general religious beliefs. Note, however, that even welfare spending doesn't explain Ireland or the Philippines, showing that some other effect(s) are comparatively stronger than that of welfare spending . Relying on "stopping rules" can lead to false positives, especially in the small sample sizes of psychology. Psychology as a field is really being held back by these kind of statistical shenanigans. However, placing all of our faith on an arbitrary p-value is pretty stupid as well. Even if we all sent our sample sizes and hypothesis statements to a journal before conducting the experiment, we would still run into false positives as academia is collectively trolling for statistical significance. The right thing to do isn't to stop data mining, the right thing to do is to describe the results in relation to their effect . Theories are judged not just by the accuracy of their predictions but also by the utility of those predictions. No matter how good the research methodology, a drug that provides a 1% improvement in cold symptoms isn't worth the cost to pack into a capsule. Update To be clear, I totally agree that social scientists should be held to a higher standard: we need to improve education, give social scientists better tools, and up the significance levels to 3-sigma. I'm trying to emphasize an under represented point: the vast majority of psychology studies are worthless because the effect size is so small. But with Amazon Turk, I can properly compensate for running 10 parralel studies and maintain >3-sigma confidence level very cheaply. But if the effect strength is small, then there are significant threats to external validity. The effect of the manipulation might be due to a news story, or the ordering of the questions, or .... I don't have time for an essay, but the quality issues within the social sciences go far beyond crappy statistical methods.
