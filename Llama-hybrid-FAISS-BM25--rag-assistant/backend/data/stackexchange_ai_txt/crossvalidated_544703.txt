[site]: crossvalidated
[post_id]: 544703
[parent_id]: 544702
[tags]: 
Typically the posterior becomes less flat near the MLE as the amount of data increases. Numerically this might not happen if you are actually multiplying by the likelihood, because it will quickly underflow to zero. So don't do that! You should be summing log likelihoods instead of multiplying likelihoods. If you actually want the posterior function at the end, taking exp() of the sum of logs will typically underflow, just like the product did. But it's generally fine to add or subtract a constant to the sum of logs before exponentiating. Typically subtracting the value at the MLE does a good job: that will make the exp() evaluate to 1 there. You don't end up with a normalized posterior this way, but you end up with something you can plot and sample from using MCMC. To normalize it you would need to integrate over the whole range and divide by that value.
