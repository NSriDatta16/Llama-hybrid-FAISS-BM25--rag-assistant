[site]: crossvalidated
[post_id]: 301285
[parent_id]: 
[tags]: 
what is vanishing gradient?

I have seen the word "vanishing gradient" many times in deep learning literature. what is that? gradient respect to what variable? input variable or hidden units? Does that mean the gradient vector is all zero? Or the optimization stuck in local minima / saddle point?
