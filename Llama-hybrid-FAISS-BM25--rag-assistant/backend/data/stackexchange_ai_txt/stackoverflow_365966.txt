[site]: stackoverflow
[post_id]: 365966
[parent_id]: 365866
[tags]: 
I try not to use it more than I have to. It definitely has its place as a transmission protocol in an architecture where the client and the server do not know about each other and are implemented independently - or an API is being developed independently of any clients. It also has a place in persistence where the same reasoning applies, and I object to it far less in that domain. However, if the client and server are implemented by the same team then it makes little sense to translate back and forth between the two in a human readable form and there is almost always a faster, cheaper (in terms of processing) alternative, even if the client and server technologies are different. Concentrating my remarks on transmission protocols, back before XML arrived in the "bad" old client/server days when bandwidth and processing power were precious, it would be the job of the architects to design a protocol (normally binary) with the sole job of efficiency and speed where packet size would be minimized. The obvious limitation is that the handshake was very specific and the binary dialect was unintelligible unless it was published. The up-side was that it was extremely efficient and could be highly optimised for the application at hand. Very often the binary formats were published (have you seen the old Excel BIFF specification - not a protocol, but an example of publishing a binary format). XML in HTTP, i.e. SOAP, broke that. The rationale was very sane, have a universally understood protocol for the handshake, a sort of computer Esperanto , so that you could separate your client and server architectures and decide on their pace of development and internals completely separately. What's more, future-proof yourself against possible client requirements with the promise that switching clients was just a matter of implementing a new one. What's more, allow any Joe with an XML parser to consume your API. All great stuff and has led to a mushrooming of very well demarked architectures - which is wholly good. So to quite a large degree the power of this proposition has been manifested and there are clearly advantages, however I think that a) this requirement is often overstated and b) XML protocols are often implemented very sloppily and with scant regard for the processing cost they imply. What's more the originally sane reasoning has given way to cases of extremist religion (I bet I get voted down) and I have seen code passing XML between function calls within the same classes , using exactly the future-proofing rationale and functional separation arguments, which is clearly bonkers. So my mantra is to make the communication efficient and effective. If that means providing a generalised API and protocol for arbitrary and unknown consumers, then XML is a very good choice. If it means making lightning hot, scalable client/server (i.e. Web) architectures then I try and use a binary protocol, often rolling my own. The emergence of JSON is testimony to the fact that the XML bandwagon had a few too many layers. JSON is an attempt to shorten the structural elements while maintaining the generality and thereby get the benefits of smaller packets. Protocols like Adobe's AMF are generally much more compact, being almost entirely binary. And that's where I think the future probably lies. I am certain that it will be possible to keep all of the up-sides that XML represents for publication of interfaces, but be able to trim it dramatically and make it less processor and bandwidth intensive - at least that's my mission as developer and architect. Imagine if your average client/server request was 1/10th of the size and there was no text parsing at either end, but you retained the generality of the interface. I don't know any developer who wouldn't take that.
