[site]: stackoverflow
[post_id]: 4625014
[parent_id]: 4605178
[tags]: 
If the images overlap by a significant amount, and the stitching algorithm does a very good job of registering the overlap region, a very simple solution would be to blend the pixel values from the two images together in the overlap region, using a weighted average with weights going from 0-1 depending on the distance from the edge of the overlap region. blendedPixel = (imageApixel * weightA) + (imageBpixel * weightB) where weightA is approaches 1 as we get closer to the imageA side of the overlap region, weightB approaches 1 as we get closer to the imageB side of the overlap region, and the sum of weightA and weightB is always 1. The above solution is not particularly principled, and does depend on the stitching algorithm doing a very good job of image registration in the overlap region. Another, more principled solution to the problem would be to remove the source of the intensity difference, attempting to homogenize the response of the pixels across the image plane. The form of this solution will depend on the source of the intensity difference, which will depend on the optics and the scene lighting conditions. For example when dealing with photographs of outdoor scenes, taken at the same time from the same location, then the dominant effect will likely be "vignetting" effects, which can be due to a variety of different causes, including differences between the various paths taken by the light through camera optics. As another example, when dealing with photographs taken through a microscope of a sample illuminated at an oblique angle, the dominant effect will likely be due to the difference in illumination between those parts of the image closest to the light and those far away. Vignetting generally manifests itself as a radially symmetric function centred around the projection of the optical axis of the lens onto the image plane. To correct for vignetting, you should try to fit a suitable radially symmetric function. Lighting changes can take different functional forms, but fitting a straightforward linear approximation is sufficient in many cases. Depending upon the scene, and the number and variability of the images that have available, you may need to take calibration images to fit these functions properly. The above approaches make assumptions about the functional forms of the sources of the intensity differences, but not about the scene or it's statistics. Yet another approach might be to make some assumptions about the scene, for example, that all significant information is represented by spatial frequencies above some threshold. You can then remove all low image intensity spatial frequency components. This will "flatten" the image, removing much of the low-frequency vignetting and lighting issues. This approach might be applicable to microscopy images, sattelite images, or images of other scenes within which most of the interest lies in the detail, rather than in the drama of the composition. There are a number of papers that tackle this problem, many at a level of technical sophistication rather beyond the above discussion. For example, see D Goldman, "Vignette and Exposure Calibration and Compensation", IEEE Trans Pattern Analysis and Machine Intelligence , vol 32, no 12, pp2276-2288
