[site]: crossvalidated
[post_id]: 494455
[parent_id]: 
[tags]: 
Recurrent networks (RNN) and matrix definition of the input $x$ an weight matrix $W_{xh}$ for a time-series model?

I am playing with RNNs/LSTMs to do some time-series modelling. Now most of the tutorials on RNNs seem to focus on natural language processing tasks. Hence, most descriptions of RNNs begin with converting a sentence into a set of word vector, where each word is a one-hot encoded vector against a 10,000 or 30,000 word dictionary. Then as the model is trained, the weight vector is applied against these one-hot encoded word vectors to get the activation at that location in the sentence. So if I have a sentence with one-hot encoded vectors labeled as $x$ , then the activation or hidden state $h_t$ is computed as. $$ h_t = \tanh(W_{hh}\cdot h_{t-1} + W_{xh}\cdot x) $$ Now for a time-series model, we don't encode the $x$ as a one-hot encoded vector. And this is where I am confused. Instead of $x$ being a one-hot encoded vector, what is it encoded as? So I am just trying to understand mathematically what $x$ and what $W_{xh}$ looks like? Is $W_{xh}$ a discretized set of possible subsequent values or something? I am just not clear what that $x$ and $W_{xh}$ vector and matrix look like--what do the elements of those objects represent? Please let me know if anyone needs further clarification of what I am asking for. Thanks.
