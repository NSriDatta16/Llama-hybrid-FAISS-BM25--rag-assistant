[site]: datascience
[post_id]: 32794
[parent_id]: 32792
[tags]: 
Since you believe the output can be predicted by a linear combination of the inputs, a reasonable approach to try is Linear Regression , specifically Multiple Regression since you have more than one input variable. Linear regression will attempt to fit the best parameters $\beta_0$ and $\beta_1$ to model your output as a weighted sum of your inputs, ie $\beta_0*input_1 + \beta_1*input_2$. This is exactly the same as the expression you gave, but it's more standard to call the weights $\beta_i$s instead of $x$ and $y$. The most standard form of linear regression using Ordinary Least Squares will find $\beta_0$ and $\beta_1$ that minimize the sum of the squared errors over your dataset, which are the differences between the actual values of output and the predicted values generated by computing $\beta_0*input_1 + \beta_1*input_2$ for each row. EDIT: To answer your question in the comments: It is always reasonable to try linear model first since it is simple and efficient, and it will give you a good baseline. However, if you suspect there is a non-linear relationship between your inputs and outputs you can also try more flexible models such as gradient boosting regression trees or a neural network . You do not need to know what the exact relationship is to use these models - they will learn it for you. In theory a neural network can fit any function. As you use more complex models, however, you should be increasingly wary of overfitting .
