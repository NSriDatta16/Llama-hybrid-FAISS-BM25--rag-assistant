[site]: crossvalidated
[post_id]: 492423
[parent_id]: 492359
[tags]: 
For simplicity, let's say you have a (binary) logistic regression model where you regress the binary outcome variable $Y$ on the predictors $X_1$ and $X_2$ . $Y$ takes the value 1 for the event of interest (e.g., student is admitted into the program) and 0 otherwise (e.g., student is not admitted into the program). The (binary) logistic regression model is then formulated like this: $log(p/(1-p)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2$ where p is the conditional probability that Y = 1 given $X_1$ and $X_2$ . For the student admission program example, let's say that $X_1$ is the student age (in years) and $X_2$ is student gender (male/female); in this case, p represents the probability of admission given a students's age and their gender. As Demetri pointed out in his answer, the right-hand side (RHS) of the model is referred to as the linear predictor . Because $log(p/(1-p))$ represents the log odds of $Y = 1$ given $X_1$ and $X_2$ , the RHS of the model can also be referred to as a log odds value or, equivalently, a logit value (see https://en.m.wikipedia.org/wiki/Logit ). Note that you can re-express the model as: $p = exp(lp)/(1 + exp(lp))$ where $lp = \beta_0 + \beta_1 X_1 + \beta_2 X_2$ is the linear predictor. This allows you to compute a probability (which has a more natural interpretation) rather than a log odds as a function of input values for the predictors $X_1$ and $X_2$ .
