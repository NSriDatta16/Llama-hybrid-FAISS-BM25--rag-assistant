[site]: crossvalidated
[post_id]: 377398
[parent_id]: 
[tags]: 
KNN parameter tuning with cross validation: score draw

I'm trying to use the KNN method for binary classification. When trying to find the best 'k' parameter (the amount of neighbours that the algorithm looks at) I train a model on my training set and look at its accuracy on a seperate validation set I got with my data. This validation set only has 12 samples, which causes a draw in accuracy for 3 k's (1,3,5). Now I'm looking for a way to choose one of these 3 k's for the definitive model. I had the following approach in mind: for the 3 k's, I do K-fold cross validation for a certain K on the training set and then look which one has the best average accuracy here. Is this a decent approach, or are there better options? I also thought of just picking a random k (1, 3 or 5), because the 'validation procedure' tells me that I can choose any of the 3.
