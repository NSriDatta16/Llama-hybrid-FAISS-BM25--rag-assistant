[site]: crossvalidated
[post_id]: 374420
[parent_id]: 
[tags]: 
Backpropagation in a logarithmic layer of a regression NN

A "logarithmic neuron" is defined as follows [1] : Which for inputs $\left\{ {{x_1},...,{x_n}} \right\}$ yields an output of $z=\prod\limits_{i = 1..n} {x_i^{{w_i}}}$ (in MATLAB, the activation function is considered a separate layer, so I'm going to ignore it from now on). I'm trying to implement a layer of such neurons as a MATLAB class that inherits from nnet.layer.Layer (which is how custom layers should be defined so that they are compatible with the Deep Learning Toolbox), but my present difficulty is deriving expressions for the derivatives of the loss function through the layer . The loss function is defined as: $$ {L_{SPES}} = \frac{1}{2}\left( {\sum\limits_{i = 1..m} {{{\left( {\frac{{t - y}}{t}} \right)}^2}} } \right) \tag{1} $$ so to my understanding , the backward propagated derivative of the loss through the output (regression) layer is therefore: $$ \frac{{\partial L}}{{\partial y}} = \frac{{y - t}}{{{t^2}}} \tag{2} $$ Proceeding to the logarithmic layer - the forward pass can be defined like this, $$ z = \exp \left( {\sum\limits_{i = 1..n} {{w_i} \cdot \ln \left( {{x_i}} \right)} } \right) = \prod\limits_{i = 1..n} {x_i^{{w_i}}} \tag{3} $$ and the article mentions that the equation for weight updates in this layer is: $$ \frac{{\partial L}}{{\partial {w_{ji}}}} = \frac{{\partial L}}{{\partial {y_j}}}\frac{{\partial {y_j}}}{{\partial {v_j}}}\frac{{\partial {v_j}}}{{\partial {w_{ji}}}}\mathop = \limits^? {y_j}\sum\limits_k {{\delta _k}{w_{kj}}{y_i}} \tag{4} $$ My questions are: How can I express $\frac{{\partial L}}{{\partial {\boldsymbol{W}}}}$ as $f\left( {\frac{{\partial L}}{{\partial Z}},\boldsymbol{X,Z,W}} \right)$ ? What should be the expressions for $\frac{{\partial L}}{{\partial \boldsymbol{X}}} = g\left( {\frac{{\partial L}}{{\partial Z}},\boldsymbol{X,Z,W}} \right)$ ?
