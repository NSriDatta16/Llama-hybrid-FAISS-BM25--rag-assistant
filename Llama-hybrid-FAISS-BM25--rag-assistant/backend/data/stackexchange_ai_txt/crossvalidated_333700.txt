[site]: crossvalidated
[post_id]: 333700
[parent_id]: 
[tags]: 
To what exactly does the term "activations" refer in neural networks?

Does it refer to the input or the output of the activation function? The literature seems to be inconsistent. A few examples: Activations = Input of the activation function Deep Learning Book, Goodfellow et al. , Pages 208, 209 $a^{(k)} = b^{(k)} + W^{(k)}h^{(k-1)}$ [...] the activations $a^{(k)}$ Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Ioffe et al. We want to a preserve the information in the network, by normalizing the activations [...] Note that, since we normalize Wu+b, the bias b can be ignored ... http://cs231n.github.io/neural-networks-1/ (describing ReLU) this one is a common choice and simply thresholds all activations that are below zero to zero Activations = Output of the activation function Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or âˆ’1, Courbariaux et al speaks of pre-activations and activations http://cs231n.github.io/neural-networks-1/ h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations
