[site]: crossvalidated
[post_id]: 118320
[parent_id]: 117487
[tags]: 
It looks like you are referring to information criteria AIC and BIC in this example according to the tags you have provided. I would like to note that for fixed dimension parameters, these metrics "metrize the same topology" as maximum likelihood. KL divergence, total variance, and Hellinger distance are all examples of metrics that measure the probablistic distance between any two probability mass functions. For plain vanilla parametric estimation, nearly all of these give identical topologies. "Out of sample prediction accuracy": do you mean forecasting or external validation? None of these metrics are functions of the data per se , and you might not care about the consistency of parametric estimation, but how well the parameters (however poorly estimated) perform in prediction using external data. It might be useful to focus your attention on performance characteristics in a model validation setting. No, there is neither existence of uniqueness guaranteed for such parameters. For example, binary prediction with logistic regression and perfectly ordered outcomes. The odds ratio lives on the boundary of the parameter space. Hence the likelihood is singular. However, you have generated a valid prediction model. You predict risk 1 when risk falls beyond a certain threshold. In a lot of ways, this means that a variety of binary prediction models (decision trees, nearest neighbor, etc) are giving you the same answer. Refer to answer 2.
