[site]: crossvalidated
[post_id]: 153699
[parent_id]: 
[tags]: 
Prior predictive density given by $f(y) = {f(y\mid \lambda) g(\lambda)}\big/{g(\lambda | y)}$?

(I guess stats.SE is the right place for this) I'm reading Albert's book "Bayesian computation with R" . To get theprior predictive density, he extensively uses this formula $$f(y) = \frac{f(y\mid \lambda) g(\lambda)}{g(\lambda | y)}, \qquad (1)$$ where $f(y\mid \lambda)$ is the (whatever model we have; in the specific example 3.3, Poisson) sampling density on $y$, $g(\lambda)$ prior density on parameter $\lambda$, and $g(\lambda \mid y)$ posterior density on $\lambda$. Now, I'm thoroughly confused about this! Okay, the equation $$ f(y\mid \lambda) g(\lambda)) = g(\lambda \mid y) f(y) $$ is just the familiar Bayes' theorem rearranged, but I can't quite explain why we're using it, because don't we in the first place find the posterior by Bayes' theorem $$g(\lambda | y) = \frac{g(y\mid \lambda) g(\lambda)}{g(y)}$$ (or so, I'm not exactly sure about the notational logic behind $g$s and $f$s)? Why don't we end up with $f(y) = f(y)$? I thought predictive distributions (prior and posterior) were solely attained by 'marginalizing', you know, this thing: $$f(y) = \int_\Theta f(y\mid \theta) g(\theta) d \theta \qquad (2) $$ edit . (for prior predictive; and for posterior predictive something like $f(\tilde{y} \mid y) = \int_\Theta f(\tilde{y}\mid \theta) g(\theta \mid y) d \theta $)
