[site]: stackoverflow
[post_id]: 107252
[parent_id]: 107165
[tags]: 
This might be too mathematical, but here's my try. (I am a mathematician.) If something is O( f ( n )), then it's running time on n elements will be equal to A f ( n ) + B (measured in, say, clock cycles or CPU operations). It's key to understanding that you also have these constants A and B , which arise from the specific implementation. B represents essentially the "constant overhead" of your operation, for example some preprocessing that you do that doesn't depend on the size of the collection. A represents the speed of your actual item-processing algorithm. The key, though, is that you use big O notation to figure out how well something will scale . So those constants won't really matter: if you're trying to figure out how to scale from 10 to 10000 items, who cares about the constant overhead B ? Similarly, other concerns (see below) will certainly outweigh the weight of the multiplicative constant A . So the real deal is f ( n ). If f grows not at all with n , e.g. f ( n ) = 1, then you'll scale fantastically---your running time will always just be A + B . If f grows linearly with n , i.e. f ( n ) = n , your running time will scale pretty much as best as can be expected---if your users are waiting 10 ns for 10 elements, they'll wait 10000 ns for 10000 elements (ignoring the additive constant). But if it grows faster, like n 2 , then you're in trouble; things will start slowing down way too much when you get larger collections. f ( n ) = n log( n ) is a good compromise, usually: your operation can't be so simple as to give linear scaling, but you've managed to cut things down such that it'll scale much better than f ( n ) = n 2 . Practically, here are some good examples: O(1): retrieving an element from an array. We know exactly where it is in memory, so we just go get it. It doesn't matter if the collection has 10 items or 10000; it's still at index (say) 3, so we just jump to location 3 in memory. O( n ): retrieving an element from a linked list. Here, A = 0.5, because on average you''ll have to go through 1/2 of the linked list before you find the element you're looking for. O( n 2 ): various "dumb" sorting algorithms. Because generally their strategy involves, for each element ( n ), you look at all the other elements (so times another n , giving n 2 ), then position yourself in the right place. O( n log( n )): various "smart" sorting algorithms. It turns out that you only need to look at, say, 10 elements in a 10 10 -element collection to intelligently sort yourself relative to everyone else in the collection. Because everyone else is also going to look at 10 elements, and the emergent behavior is orchestrated just right so that this is enough to produce a sorted list. O( n !): an algorithm that "tries everything," since there are (proportional to) n ! possible combinations of n elements that might solve a given problem. So it just loops through all such combinations, tries them, then stops whenever it succeeds.
