[site]: crossvalidated
[post_id]: 397014
[parent_id]: 395197
[tags]: 
In a nutshell, overfiitting appears as a consequence of patterns that appear in your training dataset but are not present on the entire population (they appeared out of luck) If your use a simple model (think for linear regression for instance), risk of overfitting is low, as the number of possible patterns it can detect is small and therefore the chance of one of those randomly showing in the sample is not that big either. An example of this may occur if you try to study correlations 1,000,000 variables on a population taking a 100-individual sample. Some of the features may randomly present a huge sample correlation despite being completely independent from one another Another reason for overfitting is biased sampling (the "sample fake patterns" are there because the sample is not really random) For example, if you want to study the average size of a certain kind of mushroom by going out there and finding them in nature, you are likely to overestimate it (bigger mushrooms are easier to find) Underfitting is, on the other hand, a quite simpler phenomenon. It can mean two very basic things: A) We do not have enough data for the model to learn the population pattern or B) Our model is not powerful enough to reflect it. You can find a case of A if you have a phenomenon like $y = a*x + \epsilon$ where $\epsilon$ is a random variable with mean 0 and standard deviation 1000, and the actual value of a (the parameter you want to estimate) ia 1. If you don't take enough data, you may not even be able to distinguish a from 0 thus claiming y and x are uncorrelated/independent from one another. B could occur if your model is way to simple, for example, if $y = x^2 + \epsilon$ and you try linear regression, well.... Good luck!
