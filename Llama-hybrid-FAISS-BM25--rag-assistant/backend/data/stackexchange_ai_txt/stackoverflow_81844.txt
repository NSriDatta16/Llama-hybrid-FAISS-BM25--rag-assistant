[site]: stackoverflow
[post_id]: 81844
[parent_id]: 80141
[tags]: 
Mediawiki (the sotware for wikipedia) stores full text for all revision see the database schema . Each entry in the text table in Mediawiki has flags that tells if the content has been e.g. gziped, using a standard compression is often the sanest option. I can't tell you how to do the diffs algorithmically, but what ever algorithm you use you should do it from two full versions of the text. That is fetch the complete version of old and new object from database then do the diff. This makes it possible to easily change the diffing algorithm. Git is a great example of a Unix application that can do very cheap (storage and speedwise) delta storage. There are wikis that can use git e.g. ikiwiki , but I'm guessing you want to do it with a database.
