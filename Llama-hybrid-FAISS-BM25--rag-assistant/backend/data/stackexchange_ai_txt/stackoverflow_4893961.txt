[site]: stackoverflow
[post_id]: 4893961
[parent_id]: 
[tags]: 
Can anyone help me create a simple machine learning algorithm for a 'pet' application

I'm creating an organism simulator for Android, so I guess the algorithm would ideally be in Java. I realize that there is a whole Stanford course on Machine Learning available on youtube, but I simply don't have the time to sit through the whole thing, and I think for my purposes the solution could be very simple. The organism will be interacted with by the touchscreen primarily, or even if it's interacted with through the mic or accelerometer the inputs in the algorithm will mostly amount to coordinate positions for the different limbs. I think it will be inelegant to have a 'scolding' or 'rewarding' mechanism for random behaviors, so I would like to avoid that. So tracking general directions or patterns in movements and being able to repeat them when they have a high enough frequency would be the goal. To be honest I'm not really sure how hard this is to accomplish, but I'd like to hear any feedback to know how much more I have to research before I can implement it. EDIT: Is this a genetic algorithm? The problem is I have no idea how to measure a successful or non successful evolution. EDIT 2: Okay, I'll try to add as much detail as possible. The application is still in concept stage at the moment, but I just wanted to know how difficult the algorithm would be to put it. So I'm building it in Processing, which is really just Java. The organism would be comprised of limbs that have a fixed distance between them, but are allowed to move independently from the center piece. The limbs move around freely and would find random points periodically to ease to. The organism would have a center appendage that has x and y coordinates as well, and each of the outer limbs would move in relation to that. The user could interact with the organism by manually moving the appendages or the center piece with drags on the touch screen. When the organism is being interacted with is where the algorithm would be used, because there's no point in learning from just random numbers. So I guess the algorithm would take the x and y coordinates of the center piece into consideration, and each appendage would have its own version of the algorithm that learns independently from the others. For instance, if the user continually dragged the organism to the right side of the touch screen, it might be more attracted to that place when it isn't being interacted with. I hope that clarifies a little bit.
