[site]: crossvalidated
[post_id]: 178293
[parent_id]: 178289
[tags]: 
Yes, it is reasonable to have this boosting effect. This effect means that (to steal the words of C. Dyer) the learning algorithm " 'pays attention' to rare but informative features ". It is the direct consequence of the key insight behind adagrad that the dimension of the gradient vector are unequally informative. If you make the addition you propose you will dampen the learning in the case of rare feature updates. Aside the manuscript already linked, I found these notes by E. Fox quite helpful too. The comment on the geometric interpretation of the algorithm.
