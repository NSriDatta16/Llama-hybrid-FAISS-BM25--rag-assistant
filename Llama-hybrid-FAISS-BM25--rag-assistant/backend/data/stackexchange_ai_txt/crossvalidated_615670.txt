[site]: crossvalidated
[post_id]: 615670
[parent_id]: 
[tags]: 
Consistent very low (<0.5) AUC preformance of pipeline, cannot explain why

I have a 2-step ML pipeline (classification, binary, balanced dataset, about 300 samples and tens of thousands of features): step one I train 7 algorithms (XGB, SVM etc.) and step two I train a RFC on these predictions. I do that with 5x5 nested cross-validation, without repeat. Now, I have 23 different targets (0/1 labels) to predict, so I build 23 independent models. It turns out that some of these models have performances above 0.9 AUC, some are lower, some are close to 0.5, and one of them has a AUC of 0.29. Consistent across multiple runs with different subparts of the dataset, so a bug is excluded. I don't get how that is possible. The pipeline is the same for all targets so I am sure I do not invert labels or mix up samples, as it works very well for some targets. How can a model be consistently doing exactly the opposite than it is supposed to do?
