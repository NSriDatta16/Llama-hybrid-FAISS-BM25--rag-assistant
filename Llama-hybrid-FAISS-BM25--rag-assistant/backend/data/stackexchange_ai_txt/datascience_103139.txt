[site]: datascience
[post_id]: 103139
[parent_id]: 
[tags]: 
How to implement kfold and cv into Hybrid feature selection and evaluate the classification model performance?

I have been working on a Hybrid feature selection combined with hyperopt package for hyperparameter tuning and I am thinking about evaluating the performance of several model classifiers. I looked into the k-fold and cross-validation, so, I have been wondering the getting the average performance of classifiers, but, I am not sure of how should I approach this problem. I thought of two ways to solve this problem, Use cross-validation and k-fold to do the averaging do for loop and get the average of the accuracies Which way is the best? Find my approach of, for example, KNN classifier in Code section Side question: How can I do k-fold and CV into the permutation_importance? Also, do I get the feature ranking for each run and get the average to narrow down the features from the runs? Code: import winsound import os from sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV, RandomizedSearchCV, learning_curve, ShuffleSplit, cross_val_predict, StratifiedKFold, cross_val_predict, KFold, cross_val_score, GridSearchCV, train_test_split, chi2, RFECV, f_classif, VarianceThreshold, RFE, SelectKBest, SelectFromModel from sklearn.linear_model import LogisticRegressionCV, SGDClassifier from sklearn import ensemble, naive_bayes, svm, tree, discriminant_analysis, neighbors, feature_selection, model_selection import operator import xgboost as xgb from xgboost import XGBClassifier, XGBRFClassifier from matplotlib.colors import ListedColormap, to_hex from sklearn.multiclass import OneVsRestClassifier from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs, plot_confusion_matrix from mlxtend.feature_selection import SequentialFeatureSelector as SFS from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS from sklearn.inspection import permutation_importance import time from scipy.stats import skew, stats, norm, skewnorm, median_absolute_deviation, kurtosis from mlxtend.plotting import plot_learning_curves from scipy.stats import zscore from matplotlib import cm from sklearn.svm import SVR, SVC from sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB, ComplementNB, MultinomialNB from mlxtend.feature_extraction import PrincipalComponentAnalysis from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.linear_model import SGDClassifier, LogisticRegression, LassoCV, LinearRegression from sklearn.preprocessing import Normalizer, normalize, StandardScaler, LabelEncoder, MinMaxScaler, label_binarize from mlxtend.evaluate import confusion_matrix from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis from sklearn.pipeline import make_pipeline from sklearn import linear_model, metrics import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.cluster import KMeans from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, RandomForestClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier from sklearn import svm from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier from statistics import mean, median, stdev, variance import warnings from sklearn import preprocessing, neighbors from sklearn.metrics import auc, accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, roc_curve, roc_auc_score, r2_score, make_scorer, mean_squared_error, classification_report import matplotlib.pyplot as plt from hyperopt import hp, tpe, atpe, fmin, Trials, rand, STATUS_OK # from genetic_selection import GeneticSelectionCV import matplotlib.patches as mpatches warnings.filterwarnings('ignore') start_code = time.time() # Used to get full Feature Datasets: def set_pandas_options() -> None: pd.options.display.max_columns = 500 pd.options.display.max_rows = 500 pd.options.display.max_colwidth = 199 pd.options.display.width = None # pd.options.display.precision = 2 # set as needed models = {'knn' : KNeighborsClassifier, 'svc' : SVC, 'dtc' : DecisionTreeClassifier, 'rfc' : RandomForestClassifier, 'etc' : ExtraTreesClassifier, 'xgbc' : XGBClassifier, 'adabc' : AdaBoostClassifier } def search_space(model): # Initialising variables: model = model.lower() space = {} # Calling the models: if model == 'knn': space = {'n_neighbors' : hp.choice('n_neighbors', [1, 2, 3, 4, 5, 6]), 'leaf_size' : hp.choice('leaf_size', [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]), 'algorithm' : hp.choice('algorithm', ["auto", "ball_tree", "kd_tree", "brute"]), 'metric' : hp.choice('metric', ["euclidean", "minkowski", "chebyshev", "manhattan"]), 'weights' : hp.choice('weights', ["uniform", "distance"]) } elif model == 'svc': space = {'C' : hp.choice('C', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), 'kernel' : hp.choice('kernel', ['linear', 'sigmoid', 'poly', 'rbf']), 'gamma' : hp.choice('gamma', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) } elif model == 'dtc': space = {'criterion' : hp.choice('criterion', ['gini','entropy']), 'splitter' : hp.choice('splitter', ['best', 'random']), 'max_depth' : hp.choice('max_depth', [2, 4, 6, 8, 10, 12, 14, None]), 'min_samples_split': hp.choice('min_samples_split', [2, 4, 6, 8, 10, 12]), 'min_samples_leaf' : hp.choice('min_samples_leaf', [3, 5, 7, 9, 11, 13]), # If int, then consider min_samples_split as the minimum number. 'max_features' : hp.choice('max_features', ["auto", "sqrt", "log2"]), 'random_state' : hp.choice('random_state', [0]) } elif model == 'rfc': space = {'criterion' : hp.choice('criterion', ['gini', 'entropy']), 'n_estimators' : hp.choice('n_estimators', [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]), 'max_depth' : hp.choice('max_depth', [2, 4, 6, 8, 10, 12, 14, None]), 'min_samples_split': hp.choice('min_samples_split', [2, 4, 6, 8, 10, 12]), 'min_samples_leaf' : hp.choice('min_samples_leaf', [3, 5, 7, 9, 11, 13]), # If int, then consider min_samples_split as the minimum number. 'max_features' : hp.choice('max_features', ["auto", "sqrt", "log2"]), 'random_state' : hp.choice('random_state', [0]) } elif model == 'etc': space = {'criterion' : hp.choice('criterion', ['gini', 'entropy']), 'n_estimators' : hp.choice('n_estimators', [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]), 'max_depth' : hp.choice('max_depth', [2, 4, 6, 8, 10, 12, 14, None]), 'min_samples_split': hp.choice('min_samples_split', [2, 4, 6, 8, 10, 12]), 'min_samples_leaf' : hp.choice('min_samples_leaf', [3, 5, 7, 9, 11, 13]), # If int, then consider min_samples_split as the minimum number. 'max_features' : hp.choice('max_features', ["auto", "sqrt", "log2"]), 'random_state' : hp.choice('random_state', [0]) } elif model == 'xgbc': space = {'booster' : hp.choice('booster', ['gbtree', 'gblinear']), 'objective' : hp.choice('objective', ['reg:linear', 'binary:logistic', 'multi:softmax', 'multi:softprob']), 'max_depth' : hp.choice('max_depth', [2, 4, 6, 8, 10, 12, 14, None]), 'learning_rate' : hp.choice('learning_rate', [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]), 'min_child_weight' : hp.choice('min_child_weight', [1, 3, 5, 7]), 'gamma' : hp.choice('gamma', [0.0, 0.1, 0.2 , 0.3, 0.4]), 'colsample_bytree' : hp.choice('colsample_bytree', [0.3, 0.4, 0.5 , 0.7]) } elif model == 'adabc': space = {'n_estimators' : hp.choice('n_estimators', [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]), 'learning_rate' : hp.choice('learning_rate', [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]), 'algorithm' : hp.choice('algorithm', ["SAMME", "SAMME.R"]), 'random_state' : hp.choice('random_state', [0]) } space['model'] = model return space def get_acc_status(clf, X_train, y_train): acc = cross_val_score(clf, X_train, y_train).mean() return {'loss': -acc, 'status': STATUS_OK} def obj_fnc(params): model = params.get('model').lower() # X_ = scale_normalize(params, X[:]) del params['model'] clf = models[model](**params) return (get_acc_status(clf, X_train, y_train)) def get_acc_FS_status(clf, X, y_Cat): acc = cross_val_score(clf, X, y_Cat).mean() return {'loss': -acc, 'status': STATUS_OK} def obj_fnc_FS(params): model = params.get('model').lower() # X_ = scale_normalize(params, X[:]) del params['model'] clf = models[model](**params) return (get_acc_FS_status(clf, X, y_Cat)) def most_frequent(List): counter = 0 re_dist = List[0] for i in List: curr_frequency = List.count(i) if (curr_frequency > counter): counter = curr_frequency re_dist = i return re_dist #### Main program:- ## Time for running model: print('\n') start_model = time.time() ##### Hyperparameter optimisation: # Running Bayesian Optimisation to get the best parameters: start = time.time() # Create the algorithms tpe_algo = tpe.suggest # rand_algo = rand.suggest # atpe_algo = atpe.suggest # Assigning model: model = 'knn' # Creating the trial objects: hypopt_trials = Trials() # Getting the best parameters: best_params = fmin(obj_fnc, search_space(model), algo=tpe_algo, max_evals=500, trials=hypopt_trials) print("Best params: ", best_params) print('Best accuracy: ', hypopt_trials.best_trial['result']['loss']) print("[INFO] Baye. Opt. search took {:.2f} seconds".format(time.time() - start)) weights = ["uniform", "distance"] metric = ["euclidean", "minkowski", "chebyshev", "manhattan"] algorithm = ["auto", "ball_tree", "kd_tree", "brute"] n_neighbors = [1, 2, 3, 4, 5, 6] leaf_size = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99] # Creating the KNN model: knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors[best_params['n_neighbors']], weights='{0}'.format(weights[best_params['weights']]), metric='{0}'.format(metric[best_params['metric']])) # Test 1 cv = StratifiedKFold(n_splits=5, shuffle=True) # cv = KFold(n_splits=5, shuffle=True) ############################################# Hybrid Feature Selection Methodology ##################################### #################### Embedded Method ######################## # perform permutation importance results = permutation_importance(knn, X_train, y_train, scoring='accuracy') # get importance importance = results.importances_mean print(importance) # summarize feature importance for i,v in enumerate(importance): print('Feature: %0d, Score: %.5f' % (i,v)) # plot feature importance plt.bar([x for x in range(len(importance))], importance) plt.title('Permutation Feature Importance with KNN') plt.xlabel('Features') plt.ylabel('Feature Importance') plt.show() # Organising the feature importance in dictionary: # key_list = range(0, len(importance), 1) key_list = X_train.columns feature_importance_dict = dict(zip(key_list, importance)) sort_feature_importance_dict = dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)) print('Feature Importnace Dictionary (Sorted): ', sort_feature_importance_dict) # Removing the features that have value zero in feature importance: filtered_feature_importance_dict = {x:y for x,y in sort_feature_importance_dict.items() if y!=0} print('Filtered Feature Importnace Dictionary: ', filtered_feature_importance_dict) f_indices = list(filtered_feature_importance_dict.keys()) f_indices = np.asarray(f_indices) f_indices_knn = sort_feature_importance_dict print(f_indices) # Printintg out the important features invovled in model: X_train_knn = X_train.loc[:, f_indices] X_test_knn = X_test.loc[:, f_indices] print('X Train FI: ') print(X_train_knn) print('X Test FI: ') print(X_test_knn) #################### Wrapper Method ######################## # efs = EFS(knn, min_features=1, max_features=len(X_train_knn.columns), scoring='accuracy', print_progress=True, cv=2) efs = EFS(knn, min_features=1, max_features=len(X_train_knn.columns), scoring='accuracy', print_progress=True, cv=cv) # fit the object to the training data. efs = efs.fit(X_train_knn, y_train) print('\n') print('Best accuracy score: ', efs.best_score_ * 100) print('Best subset (indices):', efs.best_idx_) print('Best subset (corresponding names):', efs.best_feature_names_) # transform our data to the newly selected features. optimum_number_features = list(efs.best_idx_) optimum_number_features_knn = list(efs.best_feature_names_) X_train_sfs = X_train_knn.iloc[:, optimum_number_features] X_test_sfs = X_test_knn.iloc[:, optimum_number_features] print('X Train RFE: ') print(X_train_sfs) print('X Test RFE: ') print(X_test_sfs) importance_list = f_indices_knn.items() plt.figure() colors = ['b' for i in importance_list] # selected_list is what your wrapper function returns selected_list = optimum_number_features_knn for i, v in enumerate(importance_list): if v[0] in selected_list: colors[i] = 'r' plt.bar([i[0] for i in importance_list], [i[1] for i in importance_list], color=colors) plt.title('Selected features from feature selection') plt.xlabel('Features') plt.ylabel('Feature Importance') legend_feat = mpatches.Patch(color='red', label='Selected Features') plt.legend(handles=[legend_feat]) plt.show() cv_2 = StratifiedKFold(n_splits=len(selected_list), shuffle=True) # cv_2 = KFold(n_splits=len(selected_list), shuffle=True) ## KNN Classification: knn.fit(X_train_sfs, y_train) knn_accuracy = knn.score(X_test_sfs, y_test) print('\n') print('KNN Accuracy: ', (knn_accuracy) * 100) # Looking at the accuracy of the model: # y_predicted = knn.predict(X_test_sfs) y_predicted = cross_val_predict(knn, X_test_sfs, y_test, cv=cv_2) test_acc = accuracy_score(y_test, y_predicted) * 100 print('The test set accuracy is %4.2f%%' % test_acc) # Obtaining the KNN model accuracy: # pred_train_knn = knn.predict(X_train_sfs) # pred_test_knn = knn.predict(X_test_sfs) pred_train_knn = cross_val_predict(knn, X_train_sfs, y_train, cv=cv_2) pred_test_knn = cross_val_predict(knn, X_test_sfs, y_test, cv=cv_2) print('\nKNN Prediction accuracy for the training dataset') print('{:.2%}\n'.format(metrics.accuracy_score(y_train, pred_train_knn))) print('\nKNN Prediction accuracy for the testing dataset') print('{:.2%}\n'.format(metrics.accuracy_score(y_test, pred_test_knn))) # Obtaining the report of the model: print('Report of KNN: ') print(classification_report(y_true=y_test, y_pred=pred_test_knn)) targets = ['0', '1', '2', '3', '4'] cnf_matrix = confusion_matrix(y_true=y_test, y_pred=y_predicted) print('Confusion Matrix of KNN: ') print(cnf_matrix) # Obtaining number of labels: labels = list(set(y_test[:,0])) labels.sort() print("Total labels: %s -> %s" % (len(labels), labels)) # Obtaining the dataframe of the confusion matrix: df_conf = pd.DataFrame(data=confusion_matrix(y_test, y_predicted, labels=labels), columns=labels,index=labels) print('Confusion Matrix Dataframe:') print(df_conf) # Local (metrics per class) # tps = {} fps = {} fns = {} precision_local = {} recall_local = {} f1_local = {} accuracy_local = {} for label in labels: tps[label] = df_conf.loc[label, label] fps[label] = df_conf[label].sum() - tps[label] fns[label] = df_conf.loc[label].sum() - tps[label] tp, fp, fn = tps[label], fps[label], fns[label] precision_local[label] = tp / (tp + fp) if (tp + fp) > 0. else 0. recall_local[label] = tp / (tp + fn) if (tp + fp) > 0. else 0. p, r = precision_local[label], recall_local[label] f1_local[label] = 2. * p * r / (p + r) if (p + r) > 0. else 0. accuracy_local[label] = tp / (tp + fp + fn) if (tp + fp + fn) > 0. else 0. print('\n') print("#-- Local measures --#") print("True Positives:", tps) print("False Positives:", fps) print("False Negatives:", fns) print("Precision:", precision_local) print("Recall:", recall_local) print("F1-Score:", f1_local) print("Accuracy:", accuracy_local) # Global metrics # micro_averages = {} macro_averages = {} correct_predictions = sum(tps.values()) den = sum(list(tps.values()) + list(fps.values())) micro_averages["Precision"] = 1. * correct_predictions / den if den > 0. else 0. den = sum(list(tps.values()) + list(fns.values())) micro_averages["Recall"] = 1. * correct_predictions / den if den > 0. else 0. micro_avg_p, micro_avg_r = micro_averages["Precision"], micro_averages["Recall"] micro_averages["F1-score"] = 2. * micro_avg_p * micro_avg_r / (micro_avg_p + micro_avg_r) if (micro_avg_p + micro_avg_r) > 0. else 0. macro_averages["Precision"] = np.mean(list(precision_local.values())) macro_averages["Recall"] = np.mean(list(recall_local.values())) macro_avg_p, macro_avg_r = macro_averages["Precision"], macro_averages["Recall"] macro_averages["F1-Score"] = np.mean(list(f1_local.values())) total_predictions = df_conf.values.sum() accuracy_global = correct_predictions / total_predictions if total_predictions > 0. else 0. print('\n') print("#-- Global measures --#") print("Micro-Averages:", micro_averages) print("Macro-Averages:", macro_averages) print("Correct predictions:", correct_predictions) print("Total predictions:", total_predictions) print("Accuracy:", accuracy_global * 100) # TN (True Negative) # tns = {} for label in set(y_test[:,0]): tns[label] = len(y_test) - (tps[label] + fps[label] + fns[label]) print("True Negatives:", tns) accuracy_local_new = {} for label in labels: tp, fp, fn, tn = tps[label], fps[label], fns[label], tns[label] accuracy_local_new[label] = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0. else 0. total_true = sum(list(tps.values()) + list(tns.values())) total_predictions = sum(list(tps.values()) + list(tns.values()) + list(fps.values()) + list(fns.values())) accuracy_global_new = 1. * total_true / total_predictions if total_predictions > 0. else 0. print("Accuracy (per class), with TNs:", accuracy_local_new) print("Accuracy (per class), without TNs:", accuracy_local) print("Accuracy (global), with TNs:", accuracy_global_new) print("Accuracy (global), without TNs:", accuracy_global) print('\n') # Saving plot path: my_path = os.path.abspath('chosen path direction') # Figures out the absolute path for you in case your working directory moves around. fig_1, ax_1 = plot_confusion_matrix(conf_mat=cnf_matrix, colorbar=True, show_absolute=True, show_normed=False, class_names=targets) plt.title('Confusion matrix of KNN Model') my_file = 'Confusion matrix of KNN Model' # plt.savefig(os.path.join(my_path, my_file)) fig_2, ax_2 = plot_confusion_matrix(conf_mat=cnf_matrix, colorbar=True, show_absolute=False, show_normed=True, class_names=targets) plt.title('Normalized KNN confusion matrix') my_file = 'Normalized Confusion matrix of KNN Model' # plt.show() plt.show(block=False) plt.pause(1) # plt.savefig(os.path.join(my_path, my_file)) plt.close('all') # cv = StratifiedKFold(n_splits=5, shuffle=True) KNN_accuracy = cross_val_score(knn, X=X_train, y=y_train, scoring='accuracy', cv = cv) print(KNN_accuracy) # Get the mean of each fold print("Accuracy of Model with Cross Validation is:", KNN_accuracy.mean() * 100) print('KNN Accuracy: ', (knn_accuracy) * 100) print("[INFO] KNN model took {:.2f} seconds".format(time.time() - start_model))
