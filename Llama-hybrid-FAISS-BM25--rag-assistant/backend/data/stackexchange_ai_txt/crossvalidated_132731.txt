[site]: crossvalidated
[post_id]: 132731
[parent_id]: 14002
[tags]: 
Two types of metric MDS The task of metric multidimensional scaling (MDS) can be abstractly formulated as follows: given a $n\times n$ matrix $\mathbf D$ of pairwise distances between $n$ points, find a low-dimensional embedding of data points in $\mathbb R^k$ such that Euclidean distances between them approximate the given distances: $$\|\mathbf x_i - \mathbf x_j\|\approx D_{ij}.$$ If "approximate" here is understood in a the usual sense of reconstruction error, i.e. if the goal is to minimize the cost function called "stress": $$\text{Stress} \sim \Big\|\mathbf D - \|\mathbf x_i - \mathbf x_j\|\Big\|^2,$$ then the solution is not equivalent to PCA. The solution is not given by any closed formula, and must be computed by a dedicated iterative algorithm. "Classical MDS", also known as "Torgerson MDS", replaces this cost function by a related but not equivalent one, called "strain": $$\text{Strain} \sim \Big\|\mathbf K_c - \langle\mathbf x_i, \mathbf x_j\rangle\Big\|^2,$$ that seeks to minimize reconstruction error of centered scalar products instead of distances. It turns out that $\mathbf K_c$ can be computed from $\mathbf D$ (if $\mathbf D$ are Euclidean distances) and that minimizing reconstruction error of $\mathbf K_c$ is exactly what PCA does, as shown in the next section. Classical (Torgerson) MDS on Euclidean distances is equivalent to PCA Let the data be collected in matrix $\mathbf X$ of $n \times k$ size with observations in rows and features in columns. Let $\mathbf X_c$ be the centered matrix with subtracted column means. Then PCA amounts to doing singular value decomposition $\mathbf X_c = \mathbf {USV^\top}$, with columns of $\mathbf{US}$ being principal components. A common way to obtain them is via an eigendecomposition of the covariance matrix $\frac{1}{n}\mathbf X_c^\top \mathbf X^\vphantom{\top}_c$, but another possible way is to perform an eigendecomposition of the Gram matrix $\mathbf K_c = \mathbf X^\vphantom{\top}_c \mathbf X^\top_c=\mathbf U \mathbf S^2 \mathbf U^\top$: principal components are its eigenvectors scaled by the square roots of the respective eigenvalues. It is easy to see that $\mathbf X_c = (\mathbf I - \frac{1}{n}\mathbf 1_n)\mathbf X$, where $\mathbf 1_n$ is a $n \times n$ matrix of ones. From this we immediately get that $$\mathbf K_c = \left(\mathbf I - \frac{\mathbf 1_n}{n}\right)\mathbf K\left(\mathbf I - \frac{\mathbf 1_n}{n}\right) = \mathbf K - \frac{\mathbf 1_n}{n} \mathbf K - \mathbf K \frac{\mathbf 1_n}{n} + \frac{\mathbf 1_n}{n} \mathbf K \frac{\mathbf 1_n}{n},$$ where $\mathbf K = \mathbf X \mathbf X^\top$ is a Gram matrix of uncentered data. This is useful: if we have the Gram matrix of uncentered data we can center it directly, without getting back to $\mathbf X$ itself. This operation is sometimes called double-centering : notice that it amounts to subtracting row means and column means from $\mathbf K$ (and adding back the global mean that gets subtracted twice), so that both row means and column means of $\mathbf K_c$ are equal to zero. Now consider a $n \times n$ matrix $\mathbf D$ of pairwise Euclidean distances with $D_{ij} = \|\mathbf x_i - \mathbf x_j\|$. Can this matrix be converted into $\mathbf K_c$ in order to perform PCA? Turns out that the answer is yes. Indeed, by the law of cosines we see that \begin{align} D_{ij}^2 = \|\mathbf x_i - \mathbf x_j\|^2 &= \|\mathbf x_i - \bar{\mathbf x}\|^2 + \|\mathbf x_j - \bar{\mathbf x}\|^2 - 2\langle\mathbf x_i - \bar{\mathbf x}, \mathbf x_j - \bar{\mathbf x} \rangle \\ &= \|\mathbf x_i - \bar{\mathbf x}\|^2 + \|\mathbf x_j - \bar{\mathbf x}\|^2 - 2[K_c]_{ij}. \end{align} So $-\mathbf D^2/2$ differs from $\mathbf K_c$ only by some row and column constants (here $\mathbf D^2$ means element-wise square!). Meaning that if we double-center it, we will get $\mathbf K_c$: $$\mathbf K_c = -\left(\mathbf I - \frac{\mathbf 1_n}{n}\right)\frac{\mathbf D^2}{2}\left(\mathbf I - \frac{\mathbf 1_n}{n}\right).$$ Which means that starting from the matrix of pairwise Euclidean distances $\mathbf D$ we can perform PCA and get principal components. This is exactly what classical (Torgerson) MDS does: $\mathbf D \mapsto \mathbf K_c \mapsto \mathbf{US}$, so its outcome is equivalent to PCA. Of course, if any other distance measure is chosen instead of $\|\mathbf x_i - \mathbf x_j\|$, then classical MDS will result in something else. Reference: The Elements of Statistical Learning , section 18.5.2.
