[site]: crossvalidated
[post_id]: 363339
[parent_id]: 288474
[tags]: 
To add to previous answers, many of those frequentist statistics depend on the log-concavity of exponential family distributions to produce unique optima for the MLE/MAP problem. The non-convexity of most practical neural networks hinders frequentist analysis because the final model is not guaranteed to be globally optimal, and model size/complexity often hinders traditional Bayasian analysis. There is however variational Bayes. But even in this setting, the lack of separability of features makes it hard to evaluate the effect of any given input variables as you would have to look over a large (potentially infeasible) number of pathways effected by a single input, though you could definitely look at group means for categorical inputs. On second look, I suppose you wouldn't have to look over all paths, but rather at all parameters associated with an input. However, the non-linear, non-additive properties means you can't really use non-zero status as an indicator of significance. Surely if all associated parameters in the first layer were (statisically) zero, that would be highly suggestive. But I doubt that would happen very often. You could perhaps look at variability: presumably less variable weights signal importance of a particularly subtree for predictions. This only seems reasonable as a collective property on subgraphs though, and could maybe tell you about important interactions. I'm not familiar but I'm curious if the study of random graphs could be used here to identify clusters of associated inputs. Since the strength of a neural network is revealing non-linear interactions, this might be a much more fruitful type of interference.
