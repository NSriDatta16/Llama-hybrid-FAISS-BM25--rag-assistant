[site]: datascience
[post_id]: 46460
[parent_id]: 44768
[tags]: 
This plot is just a conceptual sketch. It is used to convey a message at page 10 of "2018 Machine Learning Yearning (Draft version) - Andrew Ng". I checked "2016 Deep Learning - Ian Goodfellow, et al." and many seminal papers on deep learning and did not find a real example. A hidden assumption: A key assumption of this sketch is the number of model parameters, if the number of parameters is kept constant and similar for deep and shallow models, this plot occurs. Otherwise, for arbitrarily large training data (right side of the plot), any universal learner, as simple as a neural net with only one hidden layer ( link ), can perform as well as any deep learner if the number of parameters is allowed to increase arbitrarily. Power of deep learning: The literature that justifies the power of deep learning is quite technical; for example this paper , and this paper . To proceed, "capacity" of a model must be understood first. Roughly speaking, capacity of a model is the most complex function that it can represent; one formalization of this concept is "VC dimension". In theory, if capacity of a shallow model with $m$ parameters is $O(c)$ , capacity of its deep counterpart with $m$ parameters that are spread across $d$ layers is roughly $O(c^{d})$ . This is an exponential growth in capacity with respect to depth $d$ . As a result, with more training data, a deep model does not reach its capacity as fast as a shallow model with the same amount of parameters. This is what the sketch tries to convey.
