[site]: datascience
[post_id]: 96996
[parent_id]: 96962
[tags]: 
My first thought was to calculate the probability of getting heads and the probability of getting tails (which would be $1-P(\text{heads})$ ). You then could use this to make your best guess about the next coin flip. If you get $P(\text{heads})>0.5$ , you might be tempted to guess that the next flip will land as a heads. In the absence of more information, this works, but for a more complicated scenario (say disease detection), you might find one type of mistake less acceptable than another, so you might set a different threshold. This gets into ideas about probabilistic predictions (not hard classifications), proper scoring rules, and decision theory. Then I looked at your attached image, and I have another idea. You have different coins being flipped. Yes, you could use the overall probability to make your guess, but the coins do not all have to have the same probability of landing with heads up. You could then use the outcome ( $0$ for heads, $1$ for tails) as your $Y$ variable, and the coin as your $X$ variable. This would be akin to ANOVA, granted for more of a prediction purpose than ANOVA tends to be used (and also some kind of generalized linear model instead of the linear model, but that discussion warrants a separate question). This can be fit with a logistic regression, shown in R code below. coin You will have poor predictive ability with so few flips, but this is a fine toy example.
