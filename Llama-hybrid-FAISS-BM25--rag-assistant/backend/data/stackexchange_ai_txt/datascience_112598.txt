[site]: datascience
[post_id]: 112598
[parent_id]: 
[tags]: 
How special tokens in BERT-Transformers work?

I was trying to understand how tokens work and all I understood is that tokens are the representation of the input in a more meaningful way (data preparation for the "encoder of transformer" or "BERT"). But when i see use of special tokens like this: https://arxiv.org/pdf/2005.01107v1.pdf , i realized that you can actually "specify" your purpose while training your data. For example, in an answer in StackOverflow , it says : "Just an example, in extractive conversational question-answering it is not unusual to add the question and answer of the previous dialog-turn to your input to provide some context for your model. Those previous dialog turns are separated with special tokens from the current question. Sometimes people use the separator token of the model or introduce new special tokens. The following is an example with a new special token [Q]" [CLS] previous question [Q] current question [SEP] text [EOS] But it doesnt explain how any NLP model can use and can be trained in these tokens. How is it being training such a way that it understands that " i should be aware of previous question to answer current question" ?
