[site]: crossvalidated
[post_id]: 461135
[parent_id]: 461123
[tags]: 
Linear Least-Squares is not robust to outliers, which are even harder to detect in large data sets. Per Wikipedia on Robust Regression , to quote: Certain widely used methods of regression, such as ordinary least-squares, have favourable properties if their underlying assumptions are true, but can give misleading results if those assumptions are not true; thus ordinary least squares is said to be not robust to violations of its assumptions.... In particular, least squares estimates for regression models are highly sensitive to outliers. A more robust and iterative methodology is, for example, Least-Absolute-Deviations (LAD). Parameters can be derived by iteratively re-weighted least-squares methodology to find the smallest average of the sum of the absolute difference between each fitted and observed point. LAD is not a mean perspective, but essentially a median focus (the one-parameter model solution is, in fact, the median). In practice, with a two-parameter LAD model (at least per my experience), one observes that the solution consists of a line that passes through at least two reported points. So, this does mirror the process of graphically looking for 'best' lines passing through points. A technical point per Wikipedia , to quote: The least absolute deviations estimate also arises as the maximum likelihood estimate if the errors have a Laplace distribution. If it is apparent (by testing, for example) that the error terms are not from the Laplace distribution (perhaps more like log-normal), I would suggest also exploring data transformations.
