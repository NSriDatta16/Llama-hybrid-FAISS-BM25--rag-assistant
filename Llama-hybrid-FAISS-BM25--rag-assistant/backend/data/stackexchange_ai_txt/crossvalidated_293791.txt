[site]: crossvalidated
[post_id]: 293791
[parent_id]: 292795
[tags]: 
You are exactly correct about using an unsupervised classifier. This is precisely what principal components analysis (PCA) is for, which happens to be a linear method. (there are many ways to tackle this, and PCA is like vanilla ice cream - no sprinkles or mix-ins). When explaining PCA to a colleague/student, you essentially say that: someone from a lab brings you data for 50 blood chemistry features for 400 patients. But the lab "does not know what the features are" -- they are just measurements." What PCA can do is determine the "loadings" or correlations between the 50 features and maybe 5-10 hidden dimensions (which have zero correlation between them -- i.e. "orthogonal") which were extracted from the correlation matrix of the 50 features. Before you tackle your problem, however, you need to think of scale. If you assume one of the (unknown) features was calories per day (1800-2200), and another was LDL (50-250), etc., the differences in scale for these two features is quite large, and you should not do unsupervised or supervised classification analysis when there are large differences in scale between features (min, max and range). Thus, normalize, or mean-zero standardize, or use percentiles of the input features when inputting into PCA. However, recall, PCA is typically based on the correlation matrix of the features, and Pearson correlation is scale invariant. After running PCA on your features (specify correlation as the default), there will be a 15-by-15 "loading" matrix, for which each value represents the correlation between your original 15 unknown features and 15 uncorrelated "PC" components extracted via eigendecomposition from the 15x15 correlation matrix. Because you don't know what the features are, look at the 15x15 loading matrix and take note of features which load heavily (corr>0.55) in the first column (PC). Then call these a specific group assigned to the "first PC." Then look down column 2 of the loading matrix. Any input feature with a loading (corr) > 0.55 can be assigned to the second group or PC. And so on and so forth... PCA can help you split up unknown features which tend to load heavily (correlate) with hidden orthogonal dimensions which have zero correlation. You can also use hierarchical cluster analysis (HCA) on the datset, and then look at the dendogram (tree branches) for the features and see how they agglomerate. Update(7/24/2017): Step 1: After running PCA (using correlation between the 15 features as the default), look at the loadings as a way to determine which original features "loaded heavily" on the PCs (principal components). Step 2: Next, fetch the PC scores for PCs whose eigenvalues>1. Using the PC scores from the PCs whose eigenvalues>1, input them into your classifiers. Recall, your input data has dimensions $n \times p$, where $n$ is the number of records, and $p$ is the number of features (i.e., $p=15$). The dimensions of the PC matrix will be $n \times m$, where $m The trick you are missing is that the PC score matrix has the same number of records as your input data, but has fewer dimensions, because the correlated original features were "collapsed" together by PCA. You now forget about the original data with 15 features, and assume each row in the PC score matrix represents an original object (record) --> with the same original true class, but with fewer dimensions required "to explain the variation in the original dataset." So, overall, you are running PCA, to reduce dimensions of the original dataset. After this point, do not be enamored with the original dataset -- it's over. You move on and use the PC score matrix as the newly transformed data, which you input into classifiers because the records in the PC score matrix represent your original records (they just have fewer dimensions). You need to read up on PCA as a dimension reduction technique. Loadings are only used in your case to explain which original variables loaded heavily (i.e., were collapsed together) on the fewer number of PCs. You use the PC score matrix as a replacement for your original dataset with 15 features.
