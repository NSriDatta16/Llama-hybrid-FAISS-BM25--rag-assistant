[site]: crossvalidated
[post_id]: 511961
[parent_id]: 511929
[tags]: 
I can only answer this question in a confirmatory way. Normally, many people in machine learning have huge datasets with lots of features and rows or only a few features from Kaggle with a moderate amount of rows. What is common to most people regardless of the dataset is that they do not derive a hypothesis or work out material. They see it as exploratory data analysis and want to confirm their opinion about something that could be in the data. Sometimes this can be appropriate, but normally you would derive a hypothesis. Thus, when you derive a theoretical construct about your use-case and what you are doing you should normally have considered multicollinearity of some features, especially in a case of regression where you first look at correlations. Thus, I would see it, from a confirmatory way, you should exclude the variables before doing your regression. I see the other regression methods more like a fallback. Imagine you have 5000 features in tabular data. You can not check for multicollinearity for all of them. You want your algorithm to deal with that by some sort of lowering the impact. But this can not be as good as excluding variables upfront. BTW. if the methods (Lasso, Linear) weren't executed on a ML pipeline, so that the cross validation samples are all the same for both regression tasks, and thus comparable, you cant compare both $R^2$ . Hope that helps you.
