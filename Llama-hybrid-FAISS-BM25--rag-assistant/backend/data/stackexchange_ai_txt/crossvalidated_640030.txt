[site]: crossvalidated
[post_id]: 640030
[parent_id]: 639989
[tags]: 
[I was in the middle of this post when you posted your answer. For future referential purpose, I am completing the draft.] Subadditivity of Mallows distance has been well-documented in many papers. The first thing is to note that the distance is indeed a metric and attains the infimum in $\mathcal F_p:$ this is possible by considering $X^*:=F_X^{-1}(U), ~Y^*:=F_Y^{-1}(U), ~U\sim\mathrm U(0, 1) .$ See $\rm[I,~lemma ~2.3]$ which uses the Fréchet-Hoeffding bound to deduce this. Now (cf. $\rm[II,~lemma ~8.6]$ ), using this, WLOG, we assume the the $B\times B$ -valued $(X_j, Y_j) $ are independent and $d_p(X_j, Y_j) =\mathbf E[\Vert X_j-Y_j\Vert^p]^{\frac 1p}.$ Using Minkowski's inequality, one can conclude $$ d_p\left(\sum_j X_j, \sum_j Y_j\right)\leq \sum_jd_p(X_j, Y_j).$$ When the space is equipped with orthogonality, one can improve the result above (cf. $\rm[II,~lemma ~8.7]$ ). Take $p=2, $ with $B$ being a Hilbert space with inner product $\langle\cdot, \cdot\rangle$ and assuming $\mathbf E[X_j]=\mathbf E[Y_j],$ one gets $$\mathbf E[\langle X_j-Y_j, X_k-Y_k\rangle]=\begin{cases}d_2(X_j,Y_j)^2&k=j\\0&k\ne j\end{cases}.$$ So \begin{align} d_2\left(\sum_j X_j, \sum_j Y_j\right)^2&\leq \mathbf E\left[\left\langle \sum_j(X_j-Y_j), \sum_j(X_j-Y_j)\right\rangle\right]\\&=\sum_jd_2(X_j,Y_j)^2.\end{align} References: $\rm[I]$ Central Limit Theorem and convergence to stable laws in Mallows distance, Oliver Johnson, Richard Samworth, $2005, $ url: https://arxiv.org/abs/math/0406218 . $\rm[II]$ Some Asymptotic Theory for the Bootstrap, Peter J. Bickel, David A. Freedman, Ann. Statist. $ 9 ~(6) ~1196$ – $ 1217, $ November, $1981. $ url: https://doi.org/10.1214/aos/1176345637 .
