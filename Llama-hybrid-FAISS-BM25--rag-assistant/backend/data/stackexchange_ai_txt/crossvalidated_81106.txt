[site]: crossvalidated
[post_id]: 81106
[parent_id]: 81079
[tags]: 
There are no useful rules of thumb for this as the number of patterns required strongly depends on the nature of the problem, in particular the more complex the form of underlying relationship between the variables, the more data you will need to estimate it, the more noise corrupting the data, the more data you are likely to need to average out the effects of the noise. There is also a fairly strong dependence on the nature of the machine learning algorithm as well, if the technique is well suited to the problem you will need less data, if it is not well suited, you will need more data. At the end of the day, there is no minimum amount of data that you need to perform inference, the more data you have the more confident you can be of your inference, but it is generally a smooth relationship. I would advise generating a learning curve - a plot of the test set error as a function of the number of training patterns. This will give you a good indication of the amount of data you will need to train your model, and whether you have already reached the point of diminishing returns.
