[site]: crossvalidated
[post_id]: 542022
[parent_id]: 541972
[tags]: 
is it possible to just use this optimal policy to train the agent, without using Q-learning or policy gradient at all? Yes it is, but you have to decide what it is you want to train the agent to do. In that case, what should my agent try to learn from my optimal policy? The two obvious choices are: Copy the optimal policy. This is a relatively straightforward supervised learning problem. Create a dataset of states and actions, and learn $\pi(a|s) = \mathbf{Pr}\{A_t = a | S_t = s\}$ Learn a value function. To do this you would need to use a reinforcement learning algorithm with a fixed target policy. Most value-based reinforcement learning algorithms have a variant for prediction instead of control that can learn value functions given a fixed policy. Usually that variant is the first one studied, as preeiction is a problem you need to solve before the more complex issue of modifying policies to solve control problems. Neither of these are required. You have to decide what it is you want the agent to learn, given that you already have the optimal policy. Does this idea also applies to a stochastic environment? Yes, provided you are certain that the policy you want to copy or learn value functions for is one of interest to you. It doesn't need to be an optimal policy for you to be interested in it, e.g. it might be a current policy in use and you have been asked to copy it or evaluate it.
