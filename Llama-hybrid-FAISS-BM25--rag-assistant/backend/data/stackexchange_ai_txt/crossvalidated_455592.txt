[site]: crossvalidated
[post_id]: 455592
[parent_id]: 
[tags]: 
Function for bayesian information criterion (BIC)

I am writing my own python function for the bayesian information criterion (BIC) calculation. What I want to do is to choose between two models that I fitted with a set of discrete xy data points. I followed the theory here and I wrote the solution for the Gaussian special case ( BIC = n*log(residual sum of squares/n) + k*log(n) ). The code is the following: n = len(y_data_tot) # number of data points k = 6 # model free parameters summ = 0 # summatory of (data - model)**2 for i in range(n): diffsqrt = (y_data_tot[i] - y_model_tot[i])**2 summ = summ + diffsqrt rsos = (1/n)*summ BIC = n*np.log(rsos/n) + k*np.log(n) Now, I have the following questions: is the Gaussian case a reasonable approximation? is k the number of free parameters or the total number of parameters (I have models with frozen/static parameters)?
