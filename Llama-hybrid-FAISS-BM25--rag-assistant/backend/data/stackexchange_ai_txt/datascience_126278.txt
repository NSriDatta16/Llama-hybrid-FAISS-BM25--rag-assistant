[site]: datascience
[post_id]: 126278
[parent_id]: 
[tags]: 
how to fix my increasing validation loss and decreasing training loss?

here is the code that got me this, please i need an advise on what to do to correct this. #import necessary libraries from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.models import Sequential from keras.layers import Embedding, LSTM, Dense import numpy as np # Split the data into training and testing sets train_df, test_df = train_test_split(bot_df, test_size=0.2, random_state=42) # Tokenize and pad the sequences max_words = total_unique_words max_len = 100 tokenizer = Tokenizer(num_words=max_words) tokenizer.fit_on_texts(train_df['cleaned_description']) X_train_seq = tokenizer.texts_to_sequences(train_df['cleaned_description']) X_test_seq = tokenizer.texts_to_sequences(test_df['cleaned_description']) X_train_pad = pad_sequences(X_train_seq, maxlen=max_len) X_test_pad = pad_sequences(X_test_seq, maxlen=max_len) # Initialize the label encoder label_encoder = LabelEncoder() # Retrieve class names class_names = label_encoder.classes_ # Fit and transform labels in the training set y_train_encoded = label_encoder.fit_transform(train_df['price_category']) # Transform labels in the test set y_test_encoded = label_encoder.transform(test_df['price_category']) # Build the model lstm_model = Sequential() lstm_model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len)) lstm_model.add(LSTM(100)) lstm_model.add(Dense(len(class_names), activation='softmax')) # Compile the model lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Train the model history = lstm_model.fit(X_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_split=0.2) # Evaluate the model loss, accuracy = lstm_model.evaluate(X_test_pad, y_test_encoded) print(f"Test Accuracy: {accuracy}") # Plot training and validation accuracy plt.subplot(1, 2, 2) plt.plot(history.history['accuracy'], label='Training Accuracy') plt.plot(history.history['val_accuracy'], label='Validation Accuracy') plt.title('Training and Validation Accuracy') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() # Plot training and validation loss curve for the model plt.subplot(1, 2, 1) plt.plot(history.history['loss'], label='Training Loss') plt.plot(history.history['val_loss'], label='Validation Loss') plt.title('Training and Validation Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.show()
