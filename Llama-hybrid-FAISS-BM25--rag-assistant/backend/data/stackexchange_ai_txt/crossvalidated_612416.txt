[site]: crossvalidated
[post_id]: 612416
[parent_id]: 
[tags]: 
In the Transformer, how do I get the Keys and the Values from the output of the top encoder (which go into Encoder-Decoder Attention)?

I am trying to implement Attention is All You Need paper in PyTorch without looking at any code. I'm struggling to understand how do I get the Keys and the Values from the output of the top encoder. Do I learn 2 linear projections which take as input the output of the top encoder (which is a single matrix) and output the keys (or values) matrix? I don't quite understand it from the original paper, nor from The Illustrated Transformer . I understand how to get the Queries; they are obtained the same way as in the encoder self-attention layer: via a learned linear projection (linear layer).
