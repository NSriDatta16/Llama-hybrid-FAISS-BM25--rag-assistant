[site]: crossvalidated
[post_id]: 502836
[parent_id]: 502834
[tags]: 
Yes, and this is what's happening when you tune the regularization hyperparameter in a regularized regression. In, say, LASSO regression, you optimize your $\hat{\beta}$ parameter with $\vert\vert y -X\hat{\beta}\vert\vert_2^2 + \lambda\vert\vert\hat{\beta}\vert\vert_1$ . This is a different loss function for every value of $\lambda$ . However, when you determine the best value of $\lambda$ for your production model, you evaluate your models on the same square loss function. The same idea applies to a neural network. In fact, in LeCun's list of MNIST performance , all models report accuracy $^{\dagger}$ as their performance metric, yet one uses Brier score as the training loss function while most use crossentropy. $^{\dagger}$ Set aside the issues with threshold-based metrics like accuracy .
