[site]: crossvalidated
[post_id]: 172191
[parent_id]: 
[tags]: 
why is Hoeffding's inequality correct in machine learning?

Suppose we are given some training set $D$ of elements $(x_{i}, y_{i})$ and a classifier $h: X \rightarrow Y$. Define the in sample error of $h$ to be $E_{in}(h) = \frac{1}{n} \sum_{i=1}^{n} 1_{h(x_{i})\neq y_{i}}$ Then Hoeffding's inequality says the following: $Pr(|E_{in} - E_{out}| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$ Now, suppose I choose $h$ to be the following function: $h(x) = 1$ if $x$ is in $D$ $h(x) = 0$ otherwise $E_{in} = 0$ then Hoeffding's inequality says that $Pr(|E_{out}| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$ however the out of sample error is expected to be very big while what this inequality says is that for large training set the probability that the out of sample is larger than 0 decreases exponentially!
