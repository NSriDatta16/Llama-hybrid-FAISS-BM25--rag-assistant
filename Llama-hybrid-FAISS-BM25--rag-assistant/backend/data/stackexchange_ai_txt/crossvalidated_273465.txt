[site]: crossvalidated
[post_id]: 273465
[parent_id]: 
[tags]: 
Neural network softmax activation

I'm trying to perform backpropagation on a neural network using Softmax activation on the output layer and a cross-entropy cost function. Here are the steps I take: Calculate the error gradient with respect to each output neuron's input : $$ \frac{\partial E} {\partial z_j} = {\frac{\partial E} {\partial o_j}}{\frac{\partial o_j} {\partial z_j}} $$ where $\frac{\partial E} {\partial o_j}$ is the derivative of the cost function with respect to the node's output and $\frac{\partial o_j} {\partial z_j}$ is the derivative of the activation function. Adjust the output layer's weights using the following formula: $$ w_{ij} = w'_{ij} - r{\frac{\partial E} {\partial z_j}} {o_i} $$ where $r$ is some learning rate constant and $o_i$ is the $i$th output from the previous layer. Adjust the hidden layer's weights using the following formula: $$ w_{ij} = w'_{ij} - r{\frac{\partial o_j} {\partial z_j}} {\sum_k (E_k w'_{jk})} {o_i} $$ where ${\frac{\partial o_j} {\partial z_j}}$ is the derivative of the hidden layer's activation function and $E$ is the vector of output layer error gradients computed in Step 1. Question: The internet has told me that when using Softmax combined with cross entropy, Step 1 simply becomes $$ \frac{\partial E} {\partial z_j} = o_j - t_j $$ where $t$ is a one-hot encoded target output vector. Is this correct? For some reason, each round of backpropagation is causing my network to adjust itself heavily toward the provided label - so much that the network's predictions are always whatever the most recent backpropagation label was, regardless of input. I don't understand why this is happening, or how it can even be possible. There must be something wrong with the method I'm using. Any ideas?
