[site]: datascience
[post_id]: 49360
[parent_id]: 
[tags]: 
Large action space for deep reinforcement learning

I know that in normal Deep Reinforcement Learning(DRL) scenario, we learn a deep neural network to map current states to Q values. The number of the Q values (# of outputs of the neural network) is the same as the number of possible actions so that we can choose actions according to the associated Q values. However, in this paper " Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads ", the author uses states and actions as inputs. The network only outputs a single Q value (see pictures below). The $s_t$ is the state at time $t$ and $a_t^i$ is the $i^{\text{th}}$ action at time $t$ (you can ignore this but each action is a combination of the vector $c_t^1$ to $c_t^3$ ). $Q_t$ is just the $Q$ at time $t$ for action $i$ . I am wondering why only a single Q value is learned? If this is the case, how do we determine the action for the next step?
