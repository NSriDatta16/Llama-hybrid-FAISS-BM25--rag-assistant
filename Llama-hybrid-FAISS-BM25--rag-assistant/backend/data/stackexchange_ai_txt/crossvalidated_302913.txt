[site]: crossvalidated
[post_id]: 302913
[parent_id]: 302893
[tags]: 
glmnet is very strong in this respect. Of course it depends on the situation but to give you an impression of its performance, I made an example with 3 mio lines and a sparse model matrix of dimension 40'001 (one factor with 40k levels and one normally distributed variable). It takes about half a minute to run on my normal laptop to find the regularization path of length 44. library(glmnet) library(Matrix) n Unfortunately, glmnet does not return standard errors etc. for its coefficients, even in the unregularized mode. (Using regularization invalidates quantities like this.) An alternative is the h2o R package that connects to h2o.ai backend, a small JAVA program designed for high performance machine learning. Their elastic net GLM implementation is able to provide p-values, standard errors etc. for your coefficients as long as you use the slow "IRWLS" optimizer and no regularization. If you are not interested in p values etc., it is as fast as glmnet . You can fit data sets of any size as long as it fits into memory. library(h2o) h2o.init(max_mem_size = "4G", nthreads = -1) # 20 seconds train
