[site]: crossvalidated
[post_id]: 625352
[parent_id]: 485084
[tags]: 
A high R-squared does not necessarily indicate that the model has a good fit. It depends on what is meant by having a good fit. Let’s look at a common way to calculate $R^2$ that is equivalent to many other calculations in simple settings. $$ R^2=1-\left(\dfrac{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\hat y_i \right)^2 }{ \overset{N}{\underset{i=1}{\sum}}\left( y_i-\bar y \right)^2 }\right) $$ The numerator of that fraction is the sum of squared model residuals, so a measure of by how much the predicted values miss the true observations. The denominator is the sum of squared residuals for a naïve model that always predicts $\bar y$ , which is a sensible guess of the conditional expectation of all you know is $y$ . Notice that the denominator only depends on the data, not on the model. You get the same denominator whether you use a simple linear regression, multiple linear regression, neural network, random forest, or support vector regression (as just four examples). Since only the numerator can change as you fit different models to the same data, the above $R^2$ calculation is equivalent to the sum of squared model residuals in that a higher $R^2$ will be achieved if and only if a lower sum of squared residuals is achieved. Since the sum of squared residuals is a measure of by how much the predictions miss the observations, $R^2$ absolutely has an interpretation as a measure of model fit, and this is true for both linear and nonlinear models . Where that quote has legitimacy and makes a very good point is in a situation like that presented in the question where a model is able to get very close to the true values, yet there is an obvious missing piece to the model. $R^2$ will not catch such a situation. Whether or not this matters is context-dependent. If you can make a trillion dollars using your model that has $R^2=0.985$ , you’ll probably be content to use it, despite its flaws. One issue worth mentioning is that your model seems to dip down toward the right, yet the data do not support that. If you try to predict for input (x-axis) values much greater than two, you may find your model performance to be terrible. $R^2$ on its own will not catch this.
