[site]: crossvalidated
[post_id]: 384226
[parent_id]: 
[tags]: 
weighted kappa for intra/inter-rater reliability in SPSS

I am working with ordinal scales for cerebral atrophy (ratings 0 to 4) and vascular burden (ratings 0 to 3) assessed by two raters. I have installed the extension bundle for weighted kappa from SPSS (version 25) but I keep getting the error message "some ratings are less than one this command is not executed". I have looked through the threads here and the Internet but as far as I can see nobody has come up with this problem. I know from the literature that intra- and inter-rater reliability are commonly calculated for these scales using weighted kappa. Also, weighted kappa seems to be applicable to ratings of zero in SPSS(e.g. http://www.spssusers.co.uk/Events/2015/ALDRIDGE2015.pdf ) The only solution I see is recoding the ratings to avoid the value of zero but, since I would have to keep in my dataset (which is very large!) the original ratings (which are the ones that are clinically meaningful), I would end up having so many variables that it would all get very confusing and it seems a pity, especially if the error message is due to me making some mistake I am unaware of. Any help would be greatly appreciated!! I also have a second question: in clinical practice average values of the ordinal scales are often used. For instance if atrophy is 1 for the left hemisphere and 2 for the right hemisphere a value of 1.5 is frequently used to summarize this information. I have read the threads on means of ordinal variables and I understand this issue is controversial. However I need to calculate intra and inter-rater variability for the means of the ordinal scale. I can't use weighted kappa (which obviously works only with integers) and I would not want to use Spearman's correlation coefficient because I take it that it tells me how much the ratings are correlated in terms of a linear relationship but not how much they are identical to each other (i.e if one of the two raters systematically over-rated by one point, the scores of the two raters would have a correlation of one but would not be in agreement with each other which is what I am interested in). I think I could maybe use Cohen's unweighted kappa but I wonder if there is a better way to go about this.. I really would be very grateful to anyone who could give me a hand.
