[site]: crossvalidated
[post_id]: 250183
[parent_id]: 250001
[tags]: 
From a pure implementation perspective, it should be straightforward: take your model code, replace every trainable Variable creation with ed.Normal(...) or sth similar, establish variational posteriors as well, zip them in a dict, feed it to some inference object from edward et voila. The problem is that variational training of RNNs, since based on sampling, is quite hard. The sampling noise will be of no fun as soon as it is amplified by the recurrent net's dynamics. To my knowledge, there is currently no "gold standard" on how to do this in general. The starting point is probably Alex Graves's paper [1]; some recent work has been done by Yarin Gal [2], where dropout is interpreted as variational inference. It will give you a predictive distribution by integrating out the dropout noise. The latter one will probably be the easiest to get to work, but I have no practical experience myself. [1] Graves, Alex. "Practical variational inference for neural networks." Advances in Neural Information Processing Systems. 2011. [2] Gal, Yarin. "A theoretically grounded application of dropout in recurrent neural networks." arXiv preprint arXiv:1512.05287 (2015).
