[site]: datascience
[post_id]: 122680
[parent_id]: 122679
[tags]: 
Your explanation is closer to how Random Forest works. It builds many weak parallel models, and then concatenates them. Gradient Boosting methods create the weak learners sequentially, and use gradient optimization to make each "generation" better, since it tries to also predict what the previous generation got wrong. Then, the final prediction is produced from the sum of those learners outputs, not each of them sequentially. In other words, Image B is closer to the truth (in some way), but only for the training part. When making a prediction, it is more similar to Image A. I am not that much familiar with LGBM specifically, so I'll leave it to someone else to explain its specifics.
