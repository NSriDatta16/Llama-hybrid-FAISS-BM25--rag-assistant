[site]: datascience
[post_id]: 117637
[parent_id]: 
[tags]: 
Does minimizing kl divergence (i.e. keep approximate posterior close to prior) contradict the goal of avoiding posterior collapse?

Posterior collapse means the variational distribution collapse towards the prior: $\exists i: s.t. \forall x: q_{\phi}(z_i|x) \approx p(z_i)$ . $z$ becomes independent of $x$ . We would like to avoid it when training VAE. When maximizing the variational lower bound on the marginal log-likelihood, we would like to minimize the kl-divergence: $KL(q_\phi(z|x)||p(z))$ . That is to keep the approximate posterior close to the prior. To have a tight bound, the $KL=0$ . Are these two not contradicting each other? In minimizing kl-divergence case, does $KL=0$ mean posterior collapse? (I feel I am mixing up some concepts here but not sure what exactly.)
