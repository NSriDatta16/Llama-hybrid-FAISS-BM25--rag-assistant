[site]: crossvalidated
[post_id]: 239484
[parent_id]: 239480
[tags]: 
The following is from Christopher M. Bishop's "Pattern Recognition and Machine Learning" page 294 where $M$ represents the feature dimension: "In the dual formulation, we determine the parameter vector a by inverting an $N ×N$ matrix, whereas in the original parameter space formulation we had to invert an $M × M$ matrix in order to determine w. Because N is typically much larger than M, the dual formulation does not seem to be particularly useful. However, theadvantage of the dual formulation, as we shall see, is that it is expressed entirely interms of the kernel function $k(x, x)$. We can therefore work directly in terms of kernels and avoid the explicit introduction of the feature vector $\phi(x)$, which allows us implicitly to use feature spaces of high, even infinite, dimensionality."
