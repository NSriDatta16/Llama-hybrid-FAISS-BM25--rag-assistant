[site]: crossvalidated
[post_id]: 369268
[parent_id]: 
[tags]: 
Is there a bug in sklearn's stratified sampling module?

I am following a book on basic machine learning and I come across the following example: # testing the model will be done on the remaining 10000 images X_train, X_test, y_train, y_test = X[:60000], X[60000:],y[:60000], y[60000:] import numpy as np shuffle_index = np.random.permutation(60000) X_train,y_train = X_train[shuffle_index], y_train[shuffle_index] y_train_5 = (y_train == 5) # returns numpy array of bools y_test_5 = (y_test == 5) from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train,y_train_5) from sklearn.model_selection import StratifiedKFold from sklearn.base import clone skfolds = StratifiedKFold(n_splits=3, random_state=42) ''' The following code splits the training set into n_splits sets and trains the model (each iteration) on the union of n_splits -1 of the sets and tests in on the remaining set. This is carried out n_splits times. ''' for train_index, test_index in skfolds.split(X_train,y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] print '\n' print "train_index " print train_index print "test_index " print test_index X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds,y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct/(1.0*len(y_pred))) This outputs train_index [19703 19718 19737 ... 59997 59998 59999] test_index [ 0 1 2 ... 20026 20027 20028] 0.96895 train_index [ 0 1 2 ... 59997 59998 59999] test_index [19703 19718 19737 ... 40040 40041 40042] 0.96385 train_index [ 0 1 2 ... 40040 40041 40042] test_index [39682 39692 39695 ... 59997 59998 59999] 0.89245 The second output is using the entire training set for training and then testing the hypothesis on a portion of the training set. Is this correct? If so, is this normal/fair/legitimate? I understand that the sets don't have size exactly 20000 because the method is stratified ( the data is heavily skewed ). No there is not a bug. If I actually calculate the lengths of these lists (although I think they are generators). The length returned as 40000 for all sub-training sets. So the second training set in the out put is [0,1,......,~20000,~40000,....,59999]
