[site]: datascience
[post_id]: 108570
[parent_id]: 108564
[tags]: 
I think that your method does make sense, it's indeed a kind of cross-validation and it would help obtaining a more reliable estimate of performance. Technically I think the process that you describe is bootstrap aggregation (or bagging): repeatedly sampling (usually with replacement) and calculating the average performance on the test set. It also offers several advantages: you can observe the subset of parameters selected every time, which gives you an indication about the stability of the subset. you can also calculate a confidence interval for the performance, instead of only the mean performance. However with this method I would recommend repeating the process more than 10 times, if possible try 100 or even 1000 times. Note that there are various cross-validation methods available, Wikipedia has a quite good list imho .
