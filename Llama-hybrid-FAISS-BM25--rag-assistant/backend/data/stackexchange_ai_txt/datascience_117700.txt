[site]: datascience
[post_id]: 117700
[parent_id]: 
[tags]: 
Memory Error when loading a txt file for an ML model

I am trying to run the Python code below: from transformers import pipeline question_answerer = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad') with open('output_file.txt', 'r', encoding='utf-8') as file: context = file.read() question = input("Type your question: ") print("Calculating answer, please wait.") result = question_answerer(question=question, context=context) print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}") but I get MemoryError , since the input text file is too large (>100GB). Are there any techniques to read such files? I can imagine there are others with the same issue. What is a standard approach in situations as such?
