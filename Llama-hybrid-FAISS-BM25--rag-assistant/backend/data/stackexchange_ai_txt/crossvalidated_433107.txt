[site]: crossvalidated
[post_id]: 433107
[parent_id]: 
[tags]: 
Is it reasonable to make an analogy between conjugate gradients and conjugate diameters of an ellipse?

Section 8.6.2 of "Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning" uses the term "conjugate directions" without any further explanation. Conjugate gradients is a method to efficiently avoid the calculation of the inverse Hessian by iteratively descending conjugate directions. The inspiration for this approach follows from a careful study of the weakness of the method of steepest descent ... this post gives this definition Thus the optimum direction is not along –∇f, but rather in a direction that preserves the minimization achieved in the previous step (and, in multi-dimensions, all previous steps) This is called the conjugate direction to get more intuitive understanding of this concept, I am trying to make an analogy between conjugate gradients and conjugate diameters of an ellipse , is it reasonable?
