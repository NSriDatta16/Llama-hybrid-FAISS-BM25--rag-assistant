[site]: crossvalidated
[post_id]: 575756
[parent_id]: 575607
[tags]: 
I could almost answer all my questions: The uncertainty in this example is about 0.16, which means that the resolution for the DummyClassifier is around 0, which makes sense because it only predicts one value. The refinement loss of the other classifier is shrinking because the calibration lowers the prediction values. uncertainty how unbalanced is the outcome? It is 0.25 (max value) for equally distributed outcomes and 0 if there is only one outcome. resolution how extreme are the probabilities? It is 0 if the probabilities are equal to the average (like in the DummyClassifier), and it is the same as the uncertainty if there are only 0 / 1 predictions. The comparison with the calibration plot is a display issue of how the graph is created and rendered.
