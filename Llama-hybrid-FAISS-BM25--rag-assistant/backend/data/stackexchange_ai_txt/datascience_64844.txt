[site]: datascience
[post_id]: 64844
[parent_id]: 
[tags]: 
Keras model evaluation accuracy vs. observation

I am a newbie here and trying to make sense out of the scores from model.evaluate from what I am actually seeing in model.predict I have a created a CNN model for the Google Audio Set data and achieved a 99%+ accuracy on training. Here is how I do the prediction model = load_model('model_audioset.h5') for x, y in unbal_generator: score = model.evaluate(x, y, verbose=0) pred_y = model.predict(normalized_x) Here is what I am seeing for one specific iteration of x and y from model.evaluate model.metrics_names = {list: 2} ['loss', 'acc'] 0 = {str} 'loss' 1 = {str} 'acc' score = {list: 2} [0.03851451724767685, 0.9905123114585876] 0 = {float64} 0.03851451724767685 1 = {float64} 0.9905123114585876 Here is a readable output from model.predict and comparing it to y Wondering how Keras came up with an accuracy score of 99.05 for this output? Clearly, the predicted classes are not the same as expected I am using a binary_crossentropy loss function and sigmoid activation in the predictions layer as classes are NOT mutually exclusive
