[site]: crossvalidated
[post_id]: 395279
[parent_id]: 395211
[tags]: 
We tune our models to decrease the chance of overfit or underfit, by measuring the performance over the validation set(s). Some use just one validation set, but if not costly, it's better to have k-fold validation. But of course, this mechanism doesn't explicitly detect overfit or underfit. For example, your best method's validation MSE and training MSE differences could be high, signalling the possibility of overfit. Especially in neural networks overfitting can be due to over-training, and to detect it you should look at your training/validation metrics at each epoch, as you said (and set some early-stop recipe). Specifically for Keras, use EarlyStopping , with parameters patience , min_delta for setting your stopping criteria. Not referring specifically to your situation, sometimes the problem lies in the data. In ML, we assume that test set comes from the same distribution as the training set, such that training set is a good representative of future samples. If your test set is very different, or if the training set is not enough to represent the space, you can't solve your problem even with the best algorithm.
