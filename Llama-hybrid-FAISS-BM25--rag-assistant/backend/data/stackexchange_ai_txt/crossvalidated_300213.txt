[site]: crossvalidated
[post_id]: 300213
[parent_id]: 300105
[tags]: 
First, remember what the curse of dimensionality is. As the dimensionality of the data increases, if the data are uniformly distributed throughout the space, then the distribution of the distances between all points converges towards a single value. So to check this, we can look at the distribution of pairwise distances, as illustrated in @hdx1011's answer . If all the distances are approximately the same, that is a problem for clustering algorithms like k-means, because it is trying to partition the data into sets of 'closer together' distinct from the other subsets of the data which are relatively 'further away'. But if they are all the same distance apart, this is logically impossible. One of the points I was trying to make in my answer there, was that data are often not uniformly distributed in the space such that the "effective dimensionality" is not the same as the "literal dimensionality" (note that these are not formal mathematical terms). In the comments, @ttnphns explains that "intrinsic dimensionality m is the number of positive eigenvalues", which is the rank of the matrix. This is true, and your example of duplicating variables will yield a matrix whose rank is less than the number of resulting variables, but this isn't quite what I was getting at. You can very well have a matrix of full rank in which the curse of dimensionality does not apply because the effective dimensionality is much lower. Below is an example, coded in R . I generate two datasets; the first has eleven variables, each of which is a manifestation of a single latent variable, with a small amount of noise, whereas the second is a set of roughly orthogonal uniform variables. Both matrices have rank $11$ (with eleven positive eigenvalues), but the first is essentially one-dimensional. The scatterplot matrices make this obvious. set.seed(9381) # this makes the example exactly reproducible lat.var = runif(500, min=0, max=10) # a single latent variable C.dat.mat = matrix(lat.var+rnorm(5500, mean=0, sd=.1), # 10 manifest vars w/ noise nrow=500, ncol=11, byrow=FALSE) qr(C.dat.mat)$rank # [1] 11 round(eigen(cor(C.dat.mat))$values, digits=3) # [1] 10.988 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 set.seed(9381) U.dat.mat = matrix(runif(5500, min=0, max=10), # 10 orthogonal, unif vars nrow=500, ncol=11, byrow=FALSE) qr(U.dat.mat)$rank # [1] 11 round(eigen(cor(U.dat.mat))$values, digits=3) # [1] 1.266 1.188 1.121 1.041 1.031 1.011 0.938 0.904 0.874 0.845 0.781 windows() pairs(C.dat.mat) windows() pairs(U.dat.mat) If we examine all pairwise distances, we see that the distances of the orthogonal, uniform variables are becoming compressed into a narrower range of similar values. Specifically, for the correlated data, the middle $50\%$ are within $(4.5, 16.6)$, but for the uniform data, the middle $50\%$ are only in $(11.7, 15)$. The curse of dimensionality just isn't 'biting' for the former set, despite the fact that there are eleven variables and the matrix is full rank. dist.vector = function(dat){ # this function computes all pairwise distances dists = as.matrix(dist(dat)) return(as.vector(dists[upper.tri(dists)])) } L.dists = dist.vector(lat.var) # for only the latent variable C.dists = dist.vector(dat.mat) # for the correlated dataset U.dists = dist.vector(U.dat.mat) # for the orthogonal uniform dataset summary(lat.var) # hinge points of the latent variable # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.02661 2.54400 4.93200 5.00200 7.39100 9.99900 summary(L.dists) # hinge points of the pairwise distances of the latent variable # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.000033 1.346000 2.931000 3.337000 5.013000 9.973000 summary(C.dists) # ... of the correlated matrix # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.1913 4.4820 9.7350 11.1000 16.6300 33.1000 summary(U.dists) # ... of the orthogonal, uniform matrix # Min. 1st Qu. Median Mean 3rd Qu. Max. # 3.126 11.660 13.370 13.330 15.030 22.970 As a footnote, you are generating matrices that are not full rank. To understand the effect of that on a principal components analysis, you may want to read this excellent CV thread: Should one remove highly correlated variables before doing PCA?
