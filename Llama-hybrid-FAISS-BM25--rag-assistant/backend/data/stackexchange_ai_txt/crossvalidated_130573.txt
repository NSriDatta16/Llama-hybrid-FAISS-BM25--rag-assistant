[site]: crossvalidated
[post_id]: 130573
[parent_id]: 130568
[tags]: 
You can add a quadratic term with logistic regression just as you can with regular old linear regression. That is a simple way to include a 'curve' in your model. Be sure you understand what that means. I suspect you want an R tutorial, which is off-topic on CV. The basic approach to adding a quadratic in R is to include I(x^2) in the formula. Here is a simple example: lo.to.p = function(lo){ # we need this function to generate the data odds = exp(lo) prob = odds/(1+odds) return(prob) } set.seed(4649) # this makes the example exactly reproducible x1 = runif(100, min=0, max=10) # you have 3, largely uncorrelated predictors x2 = runif(100, min=0, max=10) x3 = runif(100, min=0, max=10) lo = -78 + 35*x1 - 3.5*(x1^2) + .1*x2 # there is a quadratic relationship w/ x1, a p = lo.to.p(lo) # linear relationship w/ x2 & no relationship y = rbinom(100, size=1, prob=p) # w/ x3 model = glm(y~x1+I(x1^2)+x2+x3, family=binomial) summary(model) # Call: # glm(formula = y ~ x1 + I(x1^2) + x2 + x3, family = binomial) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.74280 -0.00387 0.00000 0.04145 1.74573 # # Coefficients: # Estimate Std. Error z value Pr(>|z|) # (Intercept) -53.65462 19.65288 -2.730 0.00633 ** # x1 24.78164 8.92910 2.775 0.00551 ** # I(x1^2) -2.49888 0.89344 -2.797 0.00516 ** # x2 0.03318 0.20198 0.164 0.86952 # x3 -0.09277 0.18650 -0.497 0.61890 # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 128.207 on 99 degrees of freedom # Residual deviance: 18.647 on 95 degrees of freedom # AIC: 28.647 # # Number of Fisher Scoring iterations: 10
