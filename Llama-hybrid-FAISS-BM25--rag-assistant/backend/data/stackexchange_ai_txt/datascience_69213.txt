[site]: datascience
[post_id]: 69213
[parent_id]: 69210
[tags]: 
A direct way would be to encode any binary input features as embedded vectors and add them together as the initial hidden state for the LSTM, and then you train it as a normal language model. The "little manual introduction" could be supplied to the language model (together with the initial hidden state created from the binary features) at inference time and then use it autoregressively to generate the description. Given that you are going to deal with proper nouns (i.e. the name of the game), you should use a vocabulary that does not lead to out-of-vocabulary words. I would suggest using a subword-level vocabulary (e.g. byte-pair encoding). For the implementation, you could do it in Pytorch , using as a starting point their language model example .
