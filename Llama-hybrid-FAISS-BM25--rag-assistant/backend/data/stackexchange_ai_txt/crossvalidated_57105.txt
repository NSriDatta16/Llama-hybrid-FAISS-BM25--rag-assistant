[site]: crossvalidated
[post_id]: 57105
[parent_id]: 57027
[tags]: 
The issue of the degrees of freedom on complicated statistical learning models has been discussed in Ye 1998 JASA . Basically, the idea is to see by how much the output of a complicated model, such as the neural network, responds to a unit change in inputs. For linear models, the relation is unsurpisingly one-to-one, so the degrees of freedom for a model of complexity $p$ (number of regressors) is $p$. For more complicated models (Ye considered regression trees), an ability to add an extra node provides way more flexibility, as the CART model will look for a good variable to split, and a good split point. That's way more than what adding a regressor to a linear model can do, and Ye found the regression trees to consume about 3.5-4 d.f.s per node. Neural networks may be somewhere in between, but the degrees of freedom is surely way larger than the number of units, and may be larger than the number of weights. I think something similar was provided by HTF Sec. 7.6 , although they surprisingly don't refer to Ye (1998). They do refer to Bishop as a special case, though.
