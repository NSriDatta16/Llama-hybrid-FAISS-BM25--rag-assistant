[site]: datascience
[post_id]: 72313
[parent_id]: 
[tags]: 
Validation Loss does not decrease but validation average precision improves

I am training three neural networks (2CNNs, 1LSTM) on an EHR dataset, which is to predict diabetes according to (100 labs * 360 days) data. However, when I trained them, there is a common phenomenon shown in the following pictures: training loss is decreasing, validation loss (seems) not decreasing at all, but both average precision score (PR-AUC) are increasing. I am using the scheduler in Pytorch to adjust learning rate, so the training loss is like steps. I know there is some overfitting happening here, so I used dropout (p=0.5) and weight decay (1e-3). I am wondering why the validation loss is not decreasing but the validation average precision score still goes up, whether I need to further do some thing with it to combat the overfitting until make the validation loss decreases like the training loss if not, what is the best way to select the best epoch to do the testing? Based on lowest loss function or best average precision score?
