[site]: crossvalidated
[post_id]: 491417
[parent_id]: 491413
[tags]: 
As you mention, probably one of the primary reasons to standardize variables are to provide some way of comparing across varying scales. There are other reasons that one might standardize variables as well, though it really does depend on what you're trying answer with your data analysis. At the end of the day, the standardization formula is a linear transformation of all your data points, so it will not change the substantive results of your data analysis. In other words, if you ran a regression on the standardized and unstandardized data points, then your model will have exactly the same performance and fit but the coefficients will just be different since the input data are on different scales. So, changing the scaling of your data usually depends on how you want to interpret the results. Thinking about your specific case, your three variables may all be on the same scale, but their ranges and central tendencies may all be different. This comes up fairly often in neuropsychology where a lot of our scores are standardized to have means of 50 and standard deviations of 10 in the population (we confusingly call scores on this metric T-scores). If I have three tests of cognition -- call them X, Y, and Z as you have -- and each is recorded in my database as T-scores, then yes they all have the same scale but this does not mean that their dispersion and central tendency are all the same. So, my sample descriptives for X, Y, and Z, respectively, may be 45(12), 60(5), and 50(8) [note: mean(standard deviation)]. In this case, sure all the variables are on similar scales, but they clearly all also have different distributions that make it hard to compare one test to the next. For example, in this sample, does a score of 50 for test X really convey the same information as a score of 50 for test Y? That comparison requires a little bit of nuance to make since the two tests have different means and standard deviations despite the two being on the same scale. Standardizing the three variables will change the scale to something more uniform even despite the fact that generally speaking the data are all on the same basic metric. In my field, it's usually fairly nice to be able to talk about what happens to an outcome when a predictor increases/decreases by one standard deviation (neuropsychologists love thinking about standard deviations for some reason). As a result, having everything scaled in terms of standard deviation makes that easy to talk about. Conversely, if my goal is to develop a regression model that will predict T-scores on a certain test, then I wouldn't want to change the scale of my predictors. The primary reason for that decision is that if someone else wanted to use my regression equation, then they would have to scale their data by the means and standard deviations of my sample first. Additionally, it's sometimes useful to describe what happens as scores improve by a single T-score since standard deviation unit changes are really only helpful for thinking about effect size. Aside from interpretability of your results, I sometimes find that rescaling variables helps with model convergence. That shouldn't be an issue if all of your variables are on similar scales, but if that's only true of your variables of interest and you have other covariates that you want to control for that are on very different scales, then sometimes convergence issues come up. In Bayesian frameworks, I often find it easier to specify priors if all the variables are standardized first, so maybe that would be a reason to standardize the variables. Primarily, though, I think the decision to scale a variable, regardless of what kind of transformation is applied, should ultimately facilitate the interpretation of the results.
