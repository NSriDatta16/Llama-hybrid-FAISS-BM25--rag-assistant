[site]: crossvalidated
[post_id]: 629286
[parent_id]: 320095
[tags]: 
One reason to prefer softmax is that it may be more efficient in terms of modelling resources, such as hidden units in a multilayer perceptron network. This is because it enforces a constraint that all of the probabilities must sum to one "for free" rather than this having to be modelled/learned by the network. The optimal solution of the cross-entropy loss, for multiple exclusive classes, requires this constraint to be met, so it is better if you can get it for free. It also means that if you have a hidden unit that encodes "class A is common in this area of the attribute space", it will automatically inhibit the outputs corresponding to the other classes, again without consuming modelling resources. I learned this while trying to construct some simple networks (with only one input) for illustrating how neural networks learn. Using softmax outputs I found it was more difficult to construct "pathological" examples for use in teaching.
