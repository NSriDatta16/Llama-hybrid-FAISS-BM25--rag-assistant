[site]: datascience
[post_id]: 58192
[parent_id]: 22
[tags]: 
I came across the very same problem and tried to work my head around it (without knowing k-prototypes existed). The rich literature I found myself encountered with originated from the idea of not measuring the variables with the same distance metric at all. Furthermore there may exist various sources of information, that may imply different structures or "views" of the data. This is a natural problem, whenever you face social relationships such as those on Twitter / websites etc. One of the possible solutions is to address each subset of variables (i.e. numerical & categorical) separately. It is easily comprehendable what a distance measure does on a numeric scale. Categorical data on its own can just as easily be understood: Consider having binary observation vectors: The contingency table on 0/1 between two observation vectors contains lots of information about the similarity between those two observations. There is rich literature upon the various customized similarity measures on binary vectors - most starting from the contingency table. Given both distance / similarity matrices, both describing the same observations, one can extract a graph on each of them (Multi-View-Graph-Clustering) or extract a single graph with multiple edges - each node (observation) with as many edges to another node, as there are information matrices (Multi-Edge-Clustering). Each edge being assigned the weight of the corresponding similarity / distance measure. Start here: Github listing of Graph Clustering Algorithms & their papers . As there are multiple information sets available on a single observation, these must be interweaved using e.g. descendants of spectral analysis or linked matrix factorization, the spectral analysis being the default method for finding highly connected or heavily weighted parts of single graphs. Having a spectral embedding of the interweaved data, any clustering algorithm on numerical data may easily work. Literature's default is k-means for the matter of simplicity, but far more advanced - and not as restrictive algorithms are out there which can be used interchangeably in this context. I liked the beauty and generality in this approach, as it is easily extendible to multiple information sets rather than mere dtypes, and further its respect for the specific "measure" on each data subset. This does not alleviate you from fine tuning the model with various distance & similarity metrics or scaling your variables (I found myself scaling the numerical variables to ratio-scales ones in the context of my analysis) From a scalability perspective, consider that there are mainly two problems: Eigen problem approximation (where a rich literature of algorithms exists as well) Distance matrix estimation (a purely combinatorial problem, that grows large very quickly - I haven't found an efficient way around it yet) Have fun with it!
