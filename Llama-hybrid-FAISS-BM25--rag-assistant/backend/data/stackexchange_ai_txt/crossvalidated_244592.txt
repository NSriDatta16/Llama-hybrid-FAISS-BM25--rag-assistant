[site]: crossvalidated
[post_id]: 244592
[parent_id]: 
[tags]: 
First principles assessment of Overfitting

I am looking at some machine learning algorithms at the moment, namely logistic regression and simple neural networks. It seems that the only way that people restrict this is to check for bias and variance in the errors and check this way if it is overfit. Or vary $\lambda$ (the regularisation parameter) to adjust overfitting/underfitting. Are there any methods which use first principles instead of merely observing the effects. I.e. looking at the number of roots that a polynomial can create and knowing that by the input amount of information that it is more probable than not that this model achieved low error by virtue of replicating information than and actually modelling the link.
