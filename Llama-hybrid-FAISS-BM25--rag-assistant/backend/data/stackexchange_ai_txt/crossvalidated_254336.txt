[site]: crossvalidated
[post_id]: 254336
[parent_id]: 
[tags]: 
Why model selection criteria gone perfect testing on the first few data and wrong on the last data?

I've got a time series program that creates various models and test each one at the first 10% of data and selects the most fitted one and the program runs incredibly well. If I change this selection criteria to test on the last 10% of data (which makes more sense because that means the model is "up to date", it gone well in the last days) the program is not so good in this case. I've discovered this by a mistake, because my intuition was to test on the last 10% but I accidentally program to test on first 10%. I don't know if this has something to do with over-fitting but I honestly want some opinions
