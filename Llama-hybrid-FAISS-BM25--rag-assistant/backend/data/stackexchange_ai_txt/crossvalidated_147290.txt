[site]: crossvalidated
[post_id]: 147290
[parent_id]: 47846
[tags]: 
The usual, uninspiring and uninformative answer to this question is that 'the dynamics of your system / the nature of the physical problem will indicate how many hidden states there are', which translates into: "Take a pick and get over it". However, I do not buy into this. I think most problems are more complex than that, hence your question is a very valid one. Let us assume you are using a Gaussian distribution to model your emission (observation) variables. When fitting an HMM, what you are doing is essentially temporal clustering. Hence, you could utilise a quick and simpler clustering method to form the initial guesstimates of your Gaussian distributions per state. This is also mentioned in Rabiner's paper - the main reference for HMM's. 1) You can run a (simple) k-means on your dataset to roughly estimate the optimal number of clusters and the locations of the centroids (mean vector) in your dataset. Have a look at here for a great coverage on the topic of how to determine the number of clusters. 2) Better yet, you can choose to fit a mixture of Gaussians (or any mixture model) on your dataset to see how many mixture components best explain your dataset and what the sufficient statistics (Mean vector, Covariances) are. You can use these outputs to form your initial parameters while training the HMM - which is computationally harder than training a Mixture model (and obviously a k-means routine) primarily due to the serial dependencies that go into the loglikelihood (and therefore Expectation-Maximisation) calculation. Now, the important thing here is that these clustering techniques will not take into account the serial dependencies (as a HMM does via its transition matrix) while calculating what point most likely belongs to what state. Nevertheless they will give you rough estimations as to what your starting points (e.g. Mean vector, Covariance vector, prior distributions) should be. You can, for instance, calculate the loglikelihood of your HMM trained in this way and then compare it to the loglikelihood of one that has been trained using randomised initial values to observe the difference yourself. This is in a way analogous to, for instance, starting-off the global optimisation for a non-convex problem with the optimal values to its simplified (approximate) convex version. You do steps 1 and/or 2 to reduce the search space. Needless to say, the final values you find are not in anyway guaranteed to be the global solution to your optimisation problem - but maybe sufficiently good for your purposes.
