[site]: crossvalidated
[post_id]: 135659
[parent_id]: 135527
[tags]: 
In modern expositions of PLS there is nothing "partial": PLS looks for linear combinations among variables in $X$ and among variables in $Y$ that have maximal covariance. It is an easy eigenvector problem. That's it. See The Elements of Statistical Learning , Section 3.5.2, or e.g. Rosipal & Kr√§mer, 2005, Overview and Recent Advances in Partial Least Squares . However, historically, as @Aleksandr nicely explains (+1), PLS was introduced by Wold who used his NIPALS algorithm to implement it; NIPALS stands for "nonlinear iterated partial least squares", so obviously P in PLS just got there from NIPALS. Moreover, NIPALS (as I remember reading elsewhere) was not initially developed for PLS; it was introduced for PCA. Now, NIPALS for PCA is a very simple algorithm. I can present it right here. Let $\newcommand{\X}{\mathbf X}\X$ be a centered data matrix with observation in rows. The goal is to find the first principal axis $\newcommand{\v}{\mathbf v}\v$ (eigenvector of the covariance matrix) and the first principal component $\newcommand{\p}{\mathbf p}\p$ (projection of the data onto $\v$). We initialize $\p$ randomly and then iterate the following steps until convergence: $\v = \X^\top \p (\p^\top \p)^{-1}$ Set $\|\v\|$ to $1$. $\p = \X \v (\v^\top \v)^{-1}$ That's it! So the real question is why did Wold call this algorithm "partial"? The answer (as I finally understood after @Aleksandr made his third update) is that Wold viewed $\v$ and $\p$ as two [sets of] parameters, together modeling the data matrix $\X$. The algorithm updates these parameters sequentially (steps #1 and #3), i.e. it updates only one part of the parameters at a time! Hence "partial". (Why he called it "nonlinear" I still don't understand though.) This term is remarkably misleading, because if this is "partial" then every expectation-maximization algorithm is "partial" too (in fact, NIPALS can be seen as a primitive form of EM, see Roweis 1998 ). I think PLS is a good candidate for The Most Misleading Term in Machine Learning contest. Alas, it is unlikely to change, despite the efforts of Wold Jr. (see @Momo's comment above).
