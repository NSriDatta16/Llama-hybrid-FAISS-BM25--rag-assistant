[site]: crossvalidated
[post_id]: 574045
[parent_id]: 
[tags]: 
Linear regression has good performance in validation set despite not meeting the linearity assumption

I have a dataset with about 8000 samples and 18 predictors (16 continuous, 2 categorical). I am trying fit a linear regression, but despite trying multiple transformations, I can't make it meet the linearity assumption by checking at the predicted vs actual plot. The best I can do is: Also the residuals looks normal to the naked eye but they don't pass any normality statistical test, so this assumption is not met neither. However, when testing the regression on other datasets as validation, it performs just as good as an XGBoost model fit on the same data (LR: R2=0.47, MAPE=13.72; XGB: R2=0.47, MAPE: 13.32). The validation data has approximately the same range of values as the test data, so extrapolation does not seem to be the issue. My question is: if linear regression is doing that good in MAPE, RMSE and R2 in external datasets, can you ignore that it does not meet the assumptions? Is it telling me that it could do even better if it met the assumptions? And how can it be that despite not meeting the linearity assumption, it still does as good as XGBoost, which should handle non-linear data better? PS: XGBoost has been hyper-optimized before comparing
