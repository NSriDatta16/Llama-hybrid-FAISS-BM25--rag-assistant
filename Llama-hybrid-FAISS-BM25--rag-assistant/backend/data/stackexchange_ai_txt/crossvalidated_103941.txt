[site]: crossvalidated
[post_id]: 103941
[parent_id]: 
[tags]: 
What are appropriate tests for goodness of fit on glm with a small sample size?

I've thought quite a lot on large sample size inference where the strong law of large numbers is easily validated. In my case however, I'm trying to infer the sign and magnitude of an outcome where the noise-to-signal is quite large in comparison to my available sample size. How do I properly test for goodness of fit? I possess a very small sample size ( n = 16). I cannot possibly get more data as it simply doesn't exist. I'm fitting a generalized linear model with Gaussian errors for which I obtained the following p-values with summary(my.glm.fit) using R's glm() function for fitting. intercept 0.34172 slope 0.00734 Using Cook's D gives one data point that is problematic. If anyone knows about its validity with small samples, please speak out! I don't think a Durbinâ€“Watson test statistic is appropriate for n = 16. Even if my data has almost surely no auto-correlation, I have very sound reasons to think it doesn't. What could I use instead? Normality tests for small sample size...is Lilliefors OK? or should I go with a Bayesian test? Anything else you can point out? Anything else missing in my maybe too-broad question? I could provide more details if that's of any interest to you guys. Partial answers are also welcome. Thanks.
