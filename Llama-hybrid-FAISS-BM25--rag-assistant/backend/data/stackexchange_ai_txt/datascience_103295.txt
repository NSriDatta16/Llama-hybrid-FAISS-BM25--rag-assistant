[site]: datascience
[post_id]: 103295
[parent_id]: 103287
[tags]: 
In principle, I do not see any problem with using the RFM score as dependent variable ( $y$ ) since it is just an aggregate or balanced score of R, F, M. I suggest using random forest or (tree based) boosting (like xgboost , lightgbm ), since these techniques are very robust and usually deliver relatively good results (compared to other methods). You can look at " feature importance " from random forest or boosting to see which variables are most important. The main aspect you need to look at in the moment (as it seems) is, if the independent variables have sufficient explanatory power to build a proper model. Random forest seems to be a good choice to start against this backdrop. I would not suggest using PCA since PCA is a technique for dimensionality reduction . In essence, PCA generates "new" orthogonal features based on the original ones, usually reducing the number of overall features. So PCA is useful when there are many highly correlated features or in case ther are "too many" features. You may also look at "causal" like linear regression models. These models can tell you "if $x$ changes by some amount, $y$ changes by $\beta$ amount. However, these models are relatively complex and you need to take care about a number of things. So probaly start with a standard predictive random forest and see how ot goes.
