[site]: crossvalidated
[post_id]: 188925
[parent_id]: 
[tags]: 
Feature standardization for convolutional network on sparse data

I am preprocessing input data for a convolutional network (ConvNet), which is trained with SGD. For instance, see this quote from Ilya Sutskever ( A brief overview of Deep Learning ). It is essential to center the data so that its mean is zero and so that the variance of each of its dimensions is one. Sometimes, when the input dimension varies by orders of magnitude, it is better to take the log(1 + x) of that dimension. Basically, itâ€™s important to find a faithful encoding of the input with zero mean and sensibly bounded dimensions. Doing so makes learning work much better. This makes sense. The issue is that my dataset is highly sparse, meaning that it becomes difficult to obtain both unit variance and sensibly bounded dimensions. In Andrew Ng's coursera course, Machine Learning , he states that such a sensibly bounded interval is $ \left[ -3, 3 \right] $. To obtain unit variance I must divide by the std. deviation, which is roughly 0.25 for each channel, making around 30-50% of my values in each channel fall outside of the "sensibly bounded interval". More importantly, due to the standardization I get some fairly high values in the interval $ \left[ 10, 30 \right] $. I believe these could cause the weights to move far in the wrong direction. My dataset consists of roughly ~1.4 mio. observations of non-negative data and it is trained using a ConvNet with 8 layers with weights (conv. layers and dense layers). I am not modeling images or text (typical uses of ConvNets). One can imagine that 80% of my values are 0 and the rest are uniformly distributed on $ \left]0,10 \right] $. I have the following questions: Based on a knowledge of SGD (and perhaps ConvNets) - is it most important to aim for $ \sigma = 1 $ or a sensible interval such as $ \left[ -3, 3 \right] $? What useful transformations could I do to fulfill both $ \sigma = 1$ and keeping my values in a sensible interval?
