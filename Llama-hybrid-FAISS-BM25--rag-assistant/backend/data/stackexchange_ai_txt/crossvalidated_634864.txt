[site]: crossvalidated
[post_id]: 634864
[parent_id]: 
[tags]: 
Why do we mask input tokens for the decoder in a transformer?

I am currently trying to understand how the Transformer Architecture created by Vaswani et al. in 2017 works. Regarding this I have problems understanding the training process of the decoder. If the output of the Decoder is sequentially input back into the Decoder, why is there a need to mask tokens that are in a position right of the token in the output of the decoder? Shouldn't these tokens not have been generated yet and therefore not be relevant to mask? Thanks
