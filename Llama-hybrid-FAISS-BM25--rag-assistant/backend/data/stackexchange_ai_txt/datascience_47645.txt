[site]: datascience
[post_id]: 47645
[parent_id]: 47623
[tags]: 
You should implement a generator and feed it to model.fit_generator() . Your generator may look like this: def batch_generator(X, Y, batch_size = BATCH_SIZE): indices = np.arange(len(X)) batch=[] while True: # it might be a good idea to shuffle your data before each epoch np.random.shuffle(indices) for i in indices: batch.append(i) if len(batch)==batch_size: yield X[batch], Y[batch] batch=[] And then, somewhere in your code: train_generator = batch_generator(trainX, trainY, batch_size = 64) model.fit_generator(train_generator , ....) UPD.: I order to avoid placing all your data into memory beforehand, you can modify the generator to consume only the identifiers of your data-set and then load your data on-demand: def batch_generator(ids, batch_size = BATCH_SIZE): batch=[] while True: np.random.shuffle(ids) for i in ids: batch.append(i) if len(batch)==batch_size: yield load_data(batch) batch=[] Your loader function may look like this: def load_data(ids): X = [] Y = [] for i in ids: # read one or more samples from your storage, do pre-processing, etc. # for example: x = imread(f'image_{i}.jpg') ... y = targets[i] X.append(x) Y.append(y) return np.array(X), np.array(Y)
