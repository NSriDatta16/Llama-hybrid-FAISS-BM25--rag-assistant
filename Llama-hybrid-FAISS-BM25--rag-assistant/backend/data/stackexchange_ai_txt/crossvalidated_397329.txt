[site]: crossvalidated
[post_id]: 397329
[parent_id]: 
[tags]: 
Principal component weights flipped after PCA

I am trying to extract the principal components from a dataset, but the eigenvectors and eigenvalues aren't aligned as I would expect them to be. Here's a simple example to illustrate. import matplotlib.pyplot as plt import numpy as np x = [1, 2, 3, 4, 5] y = [10, 7, 6, 4, 2] plt.scatter(x,y); First I center the data, then calculate the covariance matrix, then extract the eigenvalues and eigenvectors. centered_x = x - np.mean(x) centered_y = y - np.mean(y) X = np.array(list(zip(centered_x, centered_y))) cov_mat = np.cov(X.T) e_vals, e_vecs = np.linalg.eig(cov_mat) print(e_vecs) print(e_vals) Here is the result I get. [[-0.88779092 0.46024699] [-0.46024699 -0.88779092]] [ 0.03751344 11.66248656] This isn't what I was expecting so I think here's where my misunderstanding comes in. I am interpreting this as I have two principal components, -0.88779092 0.46024699 with a weight of the square root of 0.03751344, and a second one of -0.46024699 -0.88779092 a weight of the square root of 11.66248656. Although the vectors seem right to me, the scaling seems flipped. The first principal component should be -0.88779092 0.46024699 with a weight of 11.66248656. So why are they flipped? What am I missing?
