[site]: datascience
[post_id]: 122216
[parent_id]: 
[tags]: 
When doing forward propogation in DNN's for a $m\times n$ matrix, does $m$ or $n$ dictate the dimensionality of the output matrix?

I am tracing through matrix multiplication in forward propagation. For some context, I am thinking about a input shape of (1, 2, 100). I am curious how (2, 100) is propagated forward in a neural network. I noticed that for a $m\times n$ matrix, $m$ is preserved throughout. Reading column wise, this would be a collection of vectors in $m$ dimensions. However, I am curious what $n$ contributes to the output matrix (or the matrices in between hidden layers)?
