[site]: crossvalidated
[post_id]: 378900
[parent_id]: 378876
[tags]: 
I'm not an expert enough to give the "correct answer", but in writing this down I hope to clarify my own thoughts and provide a perspective for you. Part of the problem is that probabilities (propensities) don't behave like other measures. And this misbehaviour makes it harder for humans to diagnose overlap in propensity distributions. Diagnosing overlap of histograms of quantities like lengths, temperatures, etc. are valid, because an additional 0.01 length means the same thing regardless of the original length. This is not true for probabilities. Not at all. Consider the following two scenarios: a coin that is heads with 50% probability vs a coin that is heads with 50.1% probability. a coin that is heads with 99.8% probability vs a coin that is heads with 99.9% probability. I argue that in the second scenario, there are much more differences in the final outcome compared to the first scenario, even though the probability change was the same in both scenarios (0.01). If I can convince you that this assertion is true, then it should be clear that "binning" probabilities and comparing them is misleading¹. Focusing on scenario 2 for a moment. For the first coin with $p=0.998$ , it would take us, on average, 500 trials to see the first tails. If we increase the probability to $p=0.999$ , it would take us about 1000 trials to see the first tails. This is a 2x difference! If I then added another 0.009, then the number of trials explodes to 10 000! Clearly, adding probabilities here has a massive effect on outcomes. Is this true for scenario 1? In the coin with $p=0.5$ , it would take us 2 trials to see a heads (the absolute number doesn't matter - so don't compare this with 500 above). When we increase this to $p=0.501$ , the expected number of trials only increases to 2.004. That's not a 2x difference we observed above. I hope it's clear now when I say: quantities like lengths, temperatures, etc. are valid, because an additional 0.01 length means the same thing regardless of the length. This is not true for probabilities. This matters a lot when we bin our data in histograms (see footnote ¹). It also matters when we use the propensity scores for something like inverse-probability-of-treatment-weights. The difference in the analytic outcome between users with a propensity of 0.998 and 0.999 is very large (think of how this user will change the psuedo-population ). I've argued against probabilities, but do log-odds fix this? Yes, well odds do, but we use log-odds because they have nice additive properties than odds. I want to recommend a few articles too: Log-Odds Why are we so surprised? ¹ To make that more clear with an example: we have binned all the probabilities above 0.95 into a single bucket, and compared them between treatment and control. I argue that these probabilities in this bucket are radically different, and we should not compare them. For example, suppose all the treated probabilities are just above 0.95, and all the control probabilities are near 1.0. Then these are very different probabilities, but we have naively compared them on a "linear" scale by bucketing them. Edit: actually, you can kinda see this in your provided data: there seems to be "overlap" in the 0.05 bucket, but on the log-odds scale, there is a long tail of small control log-odds that has no matching treatment log-odds.
