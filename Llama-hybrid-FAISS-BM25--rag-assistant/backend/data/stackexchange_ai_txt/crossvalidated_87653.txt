[site]: crossvalidated
[post_id]: 87653
[parent_id]: 
[tags]: 
Yet another "Bayesian vs Maximum Likelihood" question

In the fully Bayesian approach, the predictive distribution is: $$ P( Y|X ) = \int P(\theta | X ) P( Y | \theta ) d\theta $$ When the integral is difficult to compute, we might resort to the Maximum Likelihood approach, and approximate the predictive distribution as follows: $$ P( Y|X ) \approx P( \hat \theta_{ML} | X ) P( Y | \hat \theta_{ML} ) $$ where $ \theta_{ML} $ is the MLE of the parameter $\theta$. When you look at this, the ML approach looks like an awfully bad approximation. Why does it work not so badly in practice?
