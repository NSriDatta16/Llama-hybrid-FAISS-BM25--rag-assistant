[site]: crossvalidated
[post_id]: 203738
[parent_id]: 104526
[tags]: 
In the Baron and Kenney framework, a variable $M$ is called a mediator if comparing the two analyses: $E[Y|X] = \beta_0 + \beta_1 X$ $E[Y|X, W] = \beta^*_0 + \beta^*_1 X + \beta_2^*W$ has $\beta_1 \ne \beta_1^*$. Both of these models are simply estimable from a linear regression using categorical effects for binary $X$ (one coefficient) and polytomous $W$ (five levels, four coefficients). You can obtain confidence intervals for the $\beta_1 - \beta_1^*$ by bootstrapping (you must estimate both models from the same bootstrapped dataset, however). Baron and Kenney had also found that, when $W$ is continuously valued, the equivalent test of mediation is obtained by fitting a third model: $E[W | X] = \gamma_0 + \gamma_1$ and multiplying the $\gamma_1 \beta_1$ gave a hypothesis test equivalent to $\beta_1 - \beta_1^*$, that is: $\gamma_1 \beta_1 = 0 \implies \beta_1 = \beta^*_1$. So the tests of hypothesis are identical. Of course, with $W$ categorical, you would need a log linear model, and a sort of complicated ANOVA of product effects. It is a conservative approach whereas the approximation is anticonservative (increases type I error). When faced with the problem, I favor a conservative approach. In either case, using the bootstrap to obtain an empirical error estimator would be highly preferential. There are variance approximations, but these are highly dependent upon the normality assumption. Unlike the t-test, I bet the approximations are not robust. A last note, the Baron and Kenny test of mediation has been criticized. I appreciate the article from Pearl which encourages the earnest statistician to expand their conceptual model to a large panel of causal factors, not just a few, including confounding variables and precision variables, to be sure the "mediator" is not a proxy for other possible confounders.
