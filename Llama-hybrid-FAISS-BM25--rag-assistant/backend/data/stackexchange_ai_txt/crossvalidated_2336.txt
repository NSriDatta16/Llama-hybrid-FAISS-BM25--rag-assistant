[site]: crossvalidated
[post_id]: 2336
[parent_id]: 2131
[tags]: 
I would advise using a different value of the regularisation parameter C for examples of the positive class and examples of the negative class (many SVM packages support this, and in any case it is easily implemented). Then use e.g. cross-validation to find good values of the two regularisation parameters. It can be shown that this is asypmtotically equivalent re-sampling the data in a ratio determined by C+ and C- (so there is no advantage in re-sampling rather than re-weighting, they come to the same thing in the end and weights can be continuous, rather than discrete, so it gives finer control). Don't simply choose C+ and C- to give a 50-50 weighting to positive and negative patterns though, as the stength of the effect of the "imbalances classes" problem will vary from dataset to dataset, so the strength of the optimal re-weighting cannot be determined a-priori. Also remember that false-positive and false-negative costs may be different, and the problem may resolve itself if these are included in determining C+ and C-. It is also worth bearing in mind, that for some problems the Bayes optimal decision rule will assign all patterns to a single class and ignore the other, so it isn't necessarily a bad thing - it may just mean that the density of patterns of one class is everywhere below the density of patterns of the other class.
