[site]: crossvalidated
[post_id]: 99716
[parent_id]: 99475
[tags]: 
I think that the examples that you find on blog or websites are examples where it is known that the common methods work well (even if, of course, they can be improved). My specialization is in features engineering and I can tell you that often the standard algorithms do not work well at all. (I have not any knowledge of the field but often I work with people who have it.). Here there is a real problem where I worked for 6 months : Given a matrix X with 100 samples and 10000 variables representing the genetic value of patients and an output y of size 100 x 1 that represents the density of the bones. Can you tell me which genes influence the density of the bones? Now I am working on another problem. I have a manufacturing production dataset with 2000 samples and 12000 variables. My boss would like to extract from this dataset non more than 30 variables in an unsupervised way. I have tried some algorithms but I can not choose less then 600 variables because these are very very correlated between them. (I am still working on this...) Another important think to consider is the speed performance of the various algorithms. In a lot of situations you can not wait 20 minutes waiting for a result. For example you need to know when to use NIPALS and when to use SVD to compute PCA. Hope this can give you an idea of the problems that are common in ml.
