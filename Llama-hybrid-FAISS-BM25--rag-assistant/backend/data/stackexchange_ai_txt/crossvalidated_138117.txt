[site]: crossvalidated
[post_id]: 138117
[parent_id]: 138069
[tags]: 
Your maximum likelihood estimate is correct. A more careful formalization may help your understanding of the concepts involved. In the following, take notice of how conditional independence is used. Let $X=0,1$ be the result of the experiment, with $X=0$ meaning "Black ball is drawn", and $X=1$ meaning "White ball is drawn". Introduce a parameter $\Theta\in[0,1]$, and a random variable $Y$ such that $Y\mid\Theta=\theta\sim\mathrm{Bernoulli}(\theta)$, with $Y=1$ standing for "Heads". The experiment can be formalized specifying the distribution of $X$ given $Y$ as $$ P(X=0\mid Y=0) = \frac{3}{8} \, , \qquad\qquad P(X=1\mid Y=0) = \frac{5}{8} \, , $$ $$ P(X=0\mid Y=1) = \frac{1}{4} \, ,\qquad\qquad P(X=0\mid Y=1) = \frac{3}{4} \, , $$ and postulating that $X$ and $\Theta$ are conditionally independent, given that $Y=y$. Using the law of total probability and the product rule, the likelihood is $$ L_x(\theta) = P(X=x\mid \Theta=\theta) = \sum_{y=0,1} P(X=x, Y=y\mid \Theta=\theta) $$ $$ = \sum_{y=0,1} P(X=x\mid Y=y ,\Theta=\theta)\,P(Y=y\mid \Theta=\theta) $$ $$ = \sum_{y=0,1} P(X=x\mid Y=y)\,P(Y=y\mid \Theta=\theta) \, , $$ in which the last equality follows from the postulated conditional independence. Hence, the likelihood for your data is $$ L_1(\theta) = \frac{5}{8} \cdot (1-\theta) + \frac{3}{4} \cdot \theta = \frac{\theta}{8} + \frac{5}{8} \, , $$ and $\hat{\theta}_{\text{ML}}=1$. The problem looks artificial because you are trying to estimate the parameter of the Bernoulli with just one observation. Following Huber's suggestion, you may do a Bayesian analysis using the prior $\Theta\sim\mathrm{Beta}(a,b)$. The posterior distribution is a mixture of two betas. Can you find the Bayes estimate with quadratic loss?
