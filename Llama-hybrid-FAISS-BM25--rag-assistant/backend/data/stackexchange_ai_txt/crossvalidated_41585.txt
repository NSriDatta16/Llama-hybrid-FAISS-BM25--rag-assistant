[site]: crossvalidated
[post_id]: 41585
[parent_id]: 41524
[tags]: 
The problem here is that the test set is small, so even if the performance estimate from the hold out set is unbiased, it will have a high variance, and hence be unreliable (to some degree). Repeated hold out, cross-validation or bootstrapping are all good methods to reduce the variance of the estimate, and I suspect there isn't a great deal to choose between them (I tend to use repeated hold-out and bootstrapping a fair bit - repeated hold-out when I just wan't a performance estimate and bootstrapping when I want to make a bagged model as an extra guard against over-fitting). The real issue is that the estimated variance of all three approaches will be optimistically biased as the estimates from each iteration of the re-sampling will not be independent. So take the confidence interval on the mean with a pinch of salt. There is a nice book on "Evaluating Learning Algorithms - A Classification Perspective" by Japkowicz and Shah, I've only skimmed it so far, but I think it is a book that ought to be on the shelves of machine learning researchers and practitioners everywhere.
