[site]: crossvalidated
[post_id]: 295617
[parent_id]: 
[tags]: 
What is the advantages of Wasserstein metric compared to Kullback-Leibler divergence?

What is the practical difference between Wasserstein metric and Kullback-Leibler divergence ? Wasserstein metric is also referred to as Earth mover's distance . From Wikipedia: Wasserstein (or Vaserstein) metric is a distance function defined between probability distributions on a given metric space M. and Kullbackâ€“Leibler divergence is a measure of how one probability distribution diverges from a second expected probability distribution. I've seen KL been used in machine learning implementations, but I recently came across the Wasserstein metric. Is there a good guideline on when to use one or the other? (I have insufficient reputation to create a new tag with Wasserstein or Earth mover's distance .)
