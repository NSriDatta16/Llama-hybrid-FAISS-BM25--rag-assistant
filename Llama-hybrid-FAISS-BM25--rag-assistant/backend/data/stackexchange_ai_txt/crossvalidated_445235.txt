[site]: crossvalidated
[post_id]: 445235
[parent_id]: 
[tags]: 
Tests for significant difference between prediction errors of machine learning regression models

Are there any recommended statistical tests for significant differences between cross-validation (CV) estimates of prediction error of machine learning regression models? I have 5 different random forest regression models that use different predictor sets. For each of those 5 predictor set ups 8 different target variables (numeric) were modeled. For model evaluation I applied a 5x10 CV and computed the normalized root-mean-square error (NRMSE) and RÂ² of all 5 models. Now I would like to test the models for significant differences and verify that there are no significant differences between the model errors. I want to test for significant differences of the NRMSEs of CV prediction errors in different settings between the 5 predictor set ups (5 groups): of each target separately: 5 single values (each is the mean of 50 CV NRMSE estimates) are tested against each other of each target separately: the sets of 50 CV NRMSEs of the 5 models against each other of all targets together: all 8 target mean CV NRMSEs are in one group and the 5 groups are tested against each other of all targets together: all 400 target CV NRMSEs of the 5 groups I consider the following statistical tests: Kruskal-Wallis test ANOVA Tukey's range test (Tukey's HSD) I excluded Wilcoxon signed rank tests and Student's t-test to avoid the multiplicity effect, because I have multiple groups to compare. Which statistical tests would you recommend (also those that I didn't list)?
