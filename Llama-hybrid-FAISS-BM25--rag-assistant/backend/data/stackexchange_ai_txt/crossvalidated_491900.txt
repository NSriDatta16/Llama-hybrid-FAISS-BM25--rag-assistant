[site]: crossvalidated
[post_id]: 491900
[parent_id]: 491662
[tags]: 
Cross validation provides a point estimator of the recognition (or error) rate, and thus does say something about a single model. The question remains, however, how good this estimator is or, more precisely, how a confidence interval can be estimated for a recognition rate estimated by cross validation. You can combine $n$ -fold cross validation with the jackknife estimator for the variance, which cyclically omits one sample i , estimates the observable on the remining samples as $\theta_{(i)}$ , and then computes the standard deviation as $$\sigma_{JK}(\hat{\theta}) = \sqrt{\frac{n-1}{n}\sum_{i=1}^n (\theta_{(i)}-\theta_{(.)})^2} \quad\mbox{ with } \quad \theta_{(.)}=\frac{1}{n}\sum_{i=1}^n\theta_{(i)}$$ Don't get confused that samples are cyclically ommitted twice : once in the jackknife procedure and then inside it in the LOO error rate estimation. Out of curiosity, I have tried this out on the Iris dataset with a multivariate Gaussian Bayes classifier (the R function is called " qda ", but mathematically this is equivalent): library(MASS) n $class == iris$ Species[-i]) } # compute mean and jackknife variance rate.m This yields: Jackknife LOO recognition rate: 0.973199 +/- 0.011573 I do not know, however, how good the coverage probability of confidence intervals based on $\sigma_{JK}$ is in this particular case, because leave-one-out has been used both for computing each recognition rate estimator rate[i] and for estimating the variance therefrom. I have a gut feeling that this looses some "degrees of freedom", and the variance estimator might be somewhat too small. Maybe someone knows theoretical results about the statistical properties of this approach?
