[site]: datascience
[post_id]: 27535
[parent_id]: 27533
[tags]: 
LSTM layers require data of a different shape. From your description, I understand the starting dataset to have 3125 rows and 1000 columns, where each row is one time-step. The target variable should then have 3125 rows and 1 column, where each value can be one of three possible values. So it sounds like you're doing a classification problem. To check this in code, I would do: >>> X.shape (3125, 1000) >>> y.shape (1000,) The LSTM class requires each single sample to consist of a 'block' of time. Let's say you want to have a block of 100 time-steps. This means X[0:100] is a single input sample, which corresponds to the target variable at y[100] . this means your window size (a.k.a number of time-steps or number of lags) is equal to 100. As stated above, you have 3125 samples, so N = 3125 . To form the first block, we unfortunately have to discard the first 100 samples of y , as we cannot form an entire block of 100 from the available data (we would end up needing the data points before X[0] ). Given all this, an LSTM requires you to deliver batches of shape (N - window_size, window_size, num_features) , which translates into (3125 - 100, 100, 1000) == (3025, 100, 1000) . Creating these time-blocks is a bit of a hassle, but create a good function once, then save it :) There is more work to be done, perhaps look at more in depth examples of my explanation above here ... or have a read of the LSTM documentation , (or better still, the source code! ). The final model would then be simple enough (based on your code): #Create model model = Sequential() model.add(LSTM(units=32, activation='relu', input_shape=(100, 1000)) # the batch size is neglected! model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) Have a look at the documentation regarding input shape for the Sequential model . It basically says that we don't need to specify the number of batches within input_shape . It can be done with e.g. batch_size=50 , if you require it to be a fixed number. I know the input_shape argument is not in the documentation for LSTM , but the class itself inherits from RNN , which in turn inherits from Layer - so it will be able to use the info you provide. One last tip: if you plan on adding several LSTM layers ('stacking' them), then you shall need to add one more argument to all but the last LSTM , namely, the return_sequences=True .
