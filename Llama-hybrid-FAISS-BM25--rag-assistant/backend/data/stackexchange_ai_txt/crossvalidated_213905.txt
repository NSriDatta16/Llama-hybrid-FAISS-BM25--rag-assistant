[site]: crossvalidated
[post_id]: 213905
[parent_id]: 
[tags]: 
Rule of Succession for Unfair Coin

Given the first n flip results from an unfair coin, we wish to estimate the probability that the next flip is a heads. I can take 2 approaches to this: Frequentist: P(Next is heads|previous sequence of flips) = p (independence), and the MVUE for p is p_hat_freq = #heads / #flips Bayesian: Assume a uniform(0,1) prior for p , then condition the calculate the posterior P(Next is heads|previous sequence, H) , where H is our prior hypothesis, ..., giving the well-known Laplace Rule of Succession p_hat_Bayes = (#heads + 1)/(#flips + 2) . For a complete derivation see https://en.wikipedia.org/wiki/Rule_of_succession If I place myself at that decision point, I can use either of the two estimators above. When simulating this situation, the frequentist approach always wins, even when using a uniform prior for selecting p. Let's say we draw p , flip 10 times, and use the two estimators to estimate the true p . The frequentist estimator is unbiased, while the Bayesian is only asymptotically unbiased, and is biased for finite samples. Here is a sample run, along with the code. My question is - faced with the decision after seeing the first 10 flips, as in the simulation, why would I ever use the Bayesian estimate? It is biased, performs worse empirically, and even has lower standard error. So it is biased, but more sure of itself. The only benefit I see is when the number of flips and number of trials are both very low, in the ranges of 1-10 for each. from __future__ import division import numpy as np import numpy.random as rnd import pandas as pd import matplotlib.pyplot as plt import pdb def calc_probs(p,num_flips,num_trials): freq_est = [] Bayes_est = [] for trial_num in np.arange(num_trials): devs = rnd.uniform(0,1,num_flips) flips = [1*(x
