[site]: crossvalidated
[post_id]: 77610
[parent_id]: 
[tags]: 
Why are confidence intervals computed by the delta method so uneven?

I plotted a treatment effect on some performance measure over the course of an experiment. Stata has a nice feature called margins and marginsplot to do this. So what I do is run a linear regression, where the time trend is a third degree polynomial, like reg depvar control##c.time1##c.time1##c.time1 ..., vce(cluster clustvar) $$y=\beta_1 control\times time+\beta_2 control\times time^2+\beta_3 control\times time^3+\beta_4 time+\beta_5 time^2+\beta_6 time^3+X'\gamma+\epsilon$$ where "control" is a dummy denoting the control group, and "c.time1" is the time variable in months (the ## denote the interactions in Stata). In other words, I fit a time trend for the treatment and one for the control group. The plot is obtained by first computing the average prediction over time via the margins command, and then plotting it: margins, at(time1==(1(1)39)) over(control) marginsplot The end result is this: As you can see, the width of the confidence intervals looks like a continuous function of the month (x axis). But for example in month 4 or 21 the CI is extremely narrow, whereas it is quite broad for, e.g., month 12. Why is that? It is weird looking, and I can't explain why width in one month is so much narrower than in the other. At first I though that maybe there are many more observations in month 21 that reduce standard errors (and hence narrow CIs), but this is not the case. Why else would the width of confidence intervals differ by so much? The default method to compute these CIs is the delta-method. If I use the option "vce(unconditional)" for margins, the confidence intervals are much broader, but roughly of equal width for each month. Which would be the better choice here?
