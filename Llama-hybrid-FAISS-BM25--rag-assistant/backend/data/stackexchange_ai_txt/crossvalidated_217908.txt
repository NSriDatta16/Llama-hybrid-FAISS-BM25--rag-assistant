[site]: crossvalidated
[post_id]: 217908
[parent_id]: 
[tags]: 
Combining probabilities with different amount of evidence in sequence learning

Let us think of a simple case of sequence prediction. Based on 20 observed items, b was observed 16 times, c 3 times, and d only 1 time. The sequence is as follows: bbbcbbbbbcbbdcbbbbbb We try to predict the next item. We do not know anything about the independence of the items. Based on observations, P(b) = 16/20 , P(c) = 3/20 , and P(d) = 1/20 . Also, P(c|d) = 1.0 . Where the distribution P(X) is based on 20 observations, the distribution P(X|d) is based on only 1 observation. The amount of evidence strongly differs. How should we weight them in prediction? For example, let us observe one more item, d : bbbcbbbbbcbbdcbbbbbbd How should we now predict the next item? By relying only on P(c|d) , we would predict c with probability 1.0. However, b is more probable apriori: P(b) = 16/21 = 0.76 , P(c) = 3/21 = 0.14 , and P(d) = 2/21 = 0.10 . This is related to this other question and seems to be somehow related to multinomial logistic regression . However, I cannot get my head around it, especially in the context of sequence learning. How should the weighting be done? Is logistic regression the correct approach? What mathematical concepts relate to it? Would there be any references that discuss the matter?
