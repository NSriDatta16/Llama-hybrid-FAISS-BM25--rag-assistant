[site]: datascience
[post_id]: 40552
[parent_id]: 
[tags]: 
Why is this convolution equation easier to apply than it's commutative counterpart?

The convolution is an operation on two functions of a real- valued argument. The convolution operation is typically denoted with an asterisk: s(t) = (x ∗ w)(t) It is a special kind of linear operation which is also commutative, so: s(t) = (x ∗ w)(t) is also equal to s(t) = (w ∗ x)(t) . In convolutional neural networks we usually use convolutions over multiple axis. In a two-dimensional image we want to use 2 convolutional operations over two axis at a time. We also use a 2-D kernel as follows: $S(i,j) =(I*K)(i,j)= \sum\limits_{m} \sum\limits_nI(m,n)K(i-m,j-n)$ where I is the input, and K is the Kernel. It's commutative equivalent is: $S(i,j) =(K*I)(i,j)= \sum\limits_{m} \sum\limits_nI(i-m,j-n)K(m,n)$ As per www.deeplearningbook.org , Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of m and n. I am struggling to comprehend this sentence and thus, my question is, why would there be less variation in the range of valid values of m and n when they are commutative equivalents ?
