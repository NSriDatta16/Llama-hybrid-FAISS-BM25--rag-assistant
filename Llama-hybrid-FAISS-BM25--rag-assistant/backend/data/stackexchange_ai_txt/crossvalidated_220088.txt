[site]: crossvalidated
[post_id]: 220088
[parent_id]: 22552
[tags]: 
If I understand you correctly, you are doing grid search for each test fold independently. If this is the case, your parameters might be (and probably are) overfitting to their respective test fold (let's call it the validation fold, since the 700 images are the test set), hence the big difference in your validation and test data. One way to learn a more generalized model could be doing c.v. inside grid search (as opposed to grid search inside c.v.). In other words, you take one set of parameters, train and test your K models, measure the average performance, and select the best set of parameters, i.e. the one with smallest mean of K errors. As you suggested in your comment, increasing K should make your model more accurate, but for the cost of increasing training time. Since in each set of parameters you will be doing K separate experiments. Usually 10 is considered a good compromise. Speeding up the training is depending on the computing platform you are using, and on the implementation of SVM. Maybe there are some parameters like an error threshold, or maximum number of iterations, needless to say using a coarser grid. Hope it helps,
