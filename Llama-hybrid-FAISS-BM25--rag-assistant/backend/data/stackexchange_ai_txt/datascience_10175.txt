[site]: datascience
[post_id]: 10175
[parent_id]: 10085
[tags]: 
Where the feature selection finds a place in your pipeline depends on the problem. If you know your data well, you can select features based on this knowledge manually. If you don't - the experimentation with the models using cross validation may be best. Reducing number of features a priory with some additional technique like chi2 or PCA may actually reduce model accuracy. In my experience with text classification with SGD classifier for example leaving all hundred thousands words encoded as binary features brought better results compared to reducing to a few thousands or hundreds. Training time is actually faster with all features as feature selection is rather slow with my toolset (sklearn) because it is not stochastic like SGD. Multicollinearity is something to watch out for, but the feature interpretability might equally be important. Then people report getting best result with ensembles of models. Each model capturing a particular part of information space better than the others. That would also preclude you from selecting the features before fitting all models you'd include into your ensemble.
