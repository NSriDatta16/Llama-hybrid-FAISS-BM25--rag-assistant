[site]: datascience
[post_id]: 115959
[parent_id]: 
[tags]: 
Understanding Syntactic divergence

I am trying to read a paper https://arxiv.org/abs/2004.14444 Section 6.1 of the paper describes Syntactic divergence. I have confusion regarding the distribution graphs and split of the dataset. If a paper proposes a new test set for the NLP domain under the distribution shift concept, should the Syntactic divergence distribution be similar to all proposed test sets? If the Syntactic divergence distribution of the proposed test set is different, what does it mean? If my understanding is correct, Syntactic divergence is a difficulty measure metric so if the Syntactic divergence distribution of new test sets differs a lot from the original test set, does it means that the proposed test set is more difficult? Is it a good indication? In summary, the distribution of new test sets should be similar, or can it be different? Could someone from the NLP field help me to clear my doubts? Thank you!
