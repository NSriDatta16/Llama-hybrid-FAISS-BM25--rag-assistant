[site]: crossvalidated
[post_id]: 256880
[parent_id]: 256875
[tags]: 
Interesting question. I think you're right that multicollinearity can be a problem in Random Forests. Let's say you have a variable that is completely correlated with another. Both copies then get elected to build trees with. What happens when they are both available is up to the implementation I guess, both in any case the underlying variable, if it's a reasonably important one, will be in many trees, leading the classification to be overexcited about this variable (overconfident and / or overdependent on this value). The normal regularization parameters of Random Forests are the number of trees and their complexity. I guess you can apply some kind of penalty for including a variable at all in any iteration, or, in the spirit of ridge regression, a penalty for the square of the number of trees that have included each variable. But then, even if that makes sense, it's probably quite hard to get a quick optimization algorithm for this new global constraint on the classifier.
