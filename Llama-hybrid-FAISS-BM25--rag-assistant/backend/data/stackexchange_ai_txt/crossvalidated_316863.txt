[site]: crossvalidated
[post_id]: 316863
[parent_id]: 316842
[tags]: 
Why can you manipulate images The only reason that dataset augmentation works in images is that we have some previous knowledge on the structure and contents of our data. For instance, I know that a picture of an 8 is still an 8 if I rotate it 5 degrees, or if I add gaussian noise. In fact, because it is an 8, I can also flip the image horizontally or vertically! I know these to be valid manipulations (augmentations) of this dataset because I know some prior information about the dataset. Manipulating health data I personally know little about patient health information and its relevance to cancer patient classification, but maybe you do know some information. Think about the BMI, is a BMI of 20 that different from a 21? Could you perturb the BMI column by adding some gaussian distributed noise to it without affecting the meaning of the data? Same with age, when you are young 2 years old is very different from 3 years old. 55 years old is not that different from 56. Can you take this into account to make perturbations as a function of age? As a data scientist, it is your job to either be an expert on your data or find someone who is. Use that knowledge to make reasonable decisions about transforming, manipulating, or augmenting your data. Make sure to use valid statistical methods to validate your results (cross-validation) and compare existing manipulations to not manipulating, and see for yourself what works. Why augment at all The main point that I would like to get across is: why would someone want to augment their dataset, and what would they gain? Dataset augmentation cannot make the neural network (correctly) learn patterns that are not there. It may learn (incorrect) patterns that you introduce with the perturbations, but this is not likely what you want. Instead, we augment the data to make the network robust to noise within the dataset. So to do this we introduce the noise ourselves as a form of regularization. If the noise we introduce is structured differently from real noise, we may overfit to the wrong type of noise and hurt our network. For instance, if the noise associated with BMI is gaussian but we apply a uniform noise, the network may fail on real data. It sounds like your data set is quite small, and I'm guessing you want to augment it so that deep learning is possible. It seems likely that you've seen that you need thousands (millions [billions]) of samples to train a deep model, but unfortunately you cannot create data that you don't already have. Unless you have a simulator. Then you can create data you don't have. But I don't think we have a reliable cancer patient simulator (but I probably wouldn't know even if it did exist). Concrete suggestions Test it empirically! The best way to know if you can do something is to do it with valid statistical testing. Don't augment your data and test on a train/test split. How well does it perform? Augment the data by only perturbing the image portion, copy the metadata exactly as is between augmentations. Split the data (make sure the test set has no perturbations), how well does it perform? Come up with a strategy for perturbing some or all of the metadata fields. Augment the data with perturbations on these and not on the images. Split the data, how well does it perform? Use the strategy to augment the data perturbing both the images and the metadata. Split the data. How well does it perform?
