[site]: crossvalidated
[post_id]: 337536
[parent_id]: 
[tags]: 
How can I overfit a fully-connected neural network to predict RGB values from (x,y) coordinates?

The problem is the following: Given a single 3-channel image (e.g. 200x150), I constructed a dataset where the features are the pairs of (x,y) coordinates and the targets are the (R,G,B) values. Each {(x,y) , (r,g,b)} is a training example. The aim is to overfit the training set (another way to see this is to be able to reconstruct the image pixel by pixel). I would like to achieve an almost perfect reconstruction, but even with a neural network with 4 hidden layers ReLU activation function in each layer, except the output layer 1.000.000 parameters normalizing features and targets between [0,1] training 300 epochs with rmsprop weights from a normal with mean 0 and std 0.05 and the biases at 0. I can only achieve 0.005 mean squared error (normalized). How can I improve this performance? Do I need better preprocessing, network architecture, ecc, ...? summary: The network is pretty useless, bu you can interpret the question this way: How can I overfit a dataset with 200x150=30k training examples, each with 2 features (x,y) and 3 targets (r,g,b), With range(x) = [0,Width) , range(y) = [0, Height) and range r,g,b = [0,255] , using a fully-connected neural network?
