[site]: crossvalidated
[post_id]: 271712
[parent_id]: 89863
[tags]: 
The derivation involves considerations for the fact that CNNs use weight sharing as opposed to feed-forward networks. During both forward and back-propagation you will use convolutions where the weights and the activations will be the functions in the convolution equation. The pooling layers do not do any learning themselves. Their function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. During forward propagation, a P by P pooling block is reduced to a single value i.e. value of the “winning unit”. To keep track of the “winning unit” its index noted during the forward pass and used for gradient routing during backpropagation. During backpropagation, the gradients in the convolutional layers are calculated and the backward pass to the pooling layer then involves assigning the “winning unit” the gradient value from the convolutional layer as the index was noted prior during the forward pass. Gradient routing is done in the following ways: Max-pooling - the error is just assigned to where it comes from - the “winning unit” because other units in the previous layer’s pooling blocks did not contribute to it hence all the other assigned values of zero Average pooling - the error is multiplied by 1 / ( P by P ) and assigned to the whole pooling block (all units get this same value). Read a more comprehensive breakdown on the whole backpropagation procedure here
