[site]: crossvalidated
[post_id]: 366138
[parent_id]: 
[tags]: 
Why can't we use backpropagation and gradient descent on a Restricted Boltzmann Machine

Can someone please explain why we cannot use the backpropagation algorithm and gradient descent to train a Restricted Boltzmann Machine. In other words, why can't we train an RBM in the same manner that we train a feedforward network? Whenever I have googled around for answers to this question, I keep seeing answers that say the partition function for the RBM is intractable and so you need to use something like Gibbs sampling and Constrastive Divergence to train the RBM. The problem with this answer is that no one really explains why you don't need to use the partition function when training a feedforward network? Whenever they teach feedforward networks in textbooks nowadays, they just start by showing the backpropagation algorithm. I have not seen anyone try to explain why you don't need to worry about a partition function for a feedforward network. Can someone explain this part? I have used Gibbs sampling and Metropolis-Hastings when training hierarchical Bayesian models. So I understand the reason why the partition function is so complicated in that circumstance because you are really trying to use MCMC to estimate the posterior distribution of the parameter. But aren't we trying to do the same thing in a feedforward network when we are trying to learn weights from the data? So that is where I am getting confused. Why MCMC with RBMs but not feedforward networks?
