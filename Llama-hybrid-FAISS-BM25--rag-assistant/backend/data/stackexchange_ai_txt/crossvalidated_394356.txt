[site]: crossvalidated
[post_id]: 394356
[parent_id]: 
[tags]: 
Probability that feature selection in elastic net regularisation is meaningful - evaluating the statistical significance of chosen features

I have a research question - can I use baseline clinical features to predict my binary clinical outcome in individual patients? I am interested if the performance of my model is greater than chance. I am also interested in the whether the features themselves are meaningful. My dataset is high dimensional whereby (clinical features) p approaches n (patients). Elastic net regularisation seems a good approach. My patient dataset is drawn from 10 different sites. In order to determine an honest measure of performance, I have performed a leave one site site out cross validation (internal-external validation) as suggested here . As such, I have trained an elastic net model by cross validation on 9 sites and then tested it on the left out 1 site. I have done this 10 times and taken an average performance. In order to confirm this performance is significant, I have done a permutation test against its null distribution. I then have 10 sparse models with a number of overlapping features. A clinical audience is interested in the features which contribute to the models' significant performance. Given the high dimensional data, it is possible that many hypotheses could fit the data well, each with a distinct set of features. I am aware that it is dangerous territory mixing predictive statistics with inferential statistics but I wondered if there was any way to work out the probability that the feature selection in my 10 models was meaningful and summarising it. The first approach I took was to undertake a stability assessment across the models. This paper here provides a simple approach for determining the stability of feature selection. It proposes that regularised models are built on many bootstrap resamples of a dataset. A matrix Z of M rows is generated, where each row represents one run of a feature selection algorithm. Each row should be a binary string of length d, the total number of features, with a 1 indicating the feature was chosen, and 0 not chosen. Based on this a stability metric is generated analogous to a correlation coefficient where 0 is completely unstable and 1 is completely stable. Thresholds are suggested for differing levels of stability. I am not sure to what degree this method can be applied to my problem because I do not do bootstrap resampling of my dataset to get runs of feature selection but perform a leave one site out cross validation which is not truly random. Another approach which I wanted to try is from a much earlier paper, here , which seeks to determine the probability that the feature selection occurred by chance. They "randomly partition their data into two disjoint subsets of equal size and train the algorithm on both sets. After training we find the number of features common to both hypotheses. Since it is straightforward to determine the probability of randomly having common features, we can therefore quantify the extent to which feature selection is meaningful. Let n be the maximum number of possible features, n1 the number of features in hypothesis 1, n2 the number of features in hypothesis 2 and let Nc represent the number of features common to both hypotheses. Futher let na = max( n1 , n2 ) and nb = min( n1 , n2 ). If n1 and n2 features are drawn from n features uniformly at random, then the conditional probability of having nc features in common is: where nb ≥ nc ≥ max (0, na + nb - n )." However, I am at a loss to expand this for my 10 leave one site out splits (not 2 described here). Basically I want to find Pr ( Nc = nc | n , n1 , n2 , n3 ... n10 ). Would na = max( n1 , n2 , n3 ... n10 ) and nb = min( n1 , n2 , n3 ... n10 ) but otherwise everything else hold in this equation? Also, would it matter that my splits are not of equal size and not random? Finally, can this conditional probability be interpreted as a p-value where the null hypothesis is that these shared features occurred by chance?
