[site]: datascience
[post_id]: 123715
[parent_id]: 123714
[tags]: 
1. Why Does Transformer Work Good with Positional Embeddings? One question often thought about is why adding positional encodings to token embeddings don't mess up their meanings. Let's explore: High-Dimensional Space : Transformers usually have high dimensions, like 768. But there are these mini sentence transformers that operate effectively with just half; 384 dimensions. 768 gives double degrees of freedom and you can package a lot more information there. So, the number of dimensions is maybe depending on the task. It's a balance. Training : During the training, the Transformer model is learning which features in the high-dimensional space are important. It figures out the right weight for both the token and the position information. Relative Importance Through Attention : Tokens and positions both go into the attention mechanisms. This allows the Transformer to adjust how much importance is given to position compared to token, depending on context. 2. Why Both Sine and Cosine in Positional Encodings? So, why use sine and cosine for positional encodings? There are several reasons: Different Frequencies and Fourier Stuff : Using sine and cosine, the model can capture different types of information across dimensions. This is similar to Fourier Transforms, which decompose complicated signals into simpler waves. Linear Transformations : Sine and cosine make it possible to perform linear transformations between different times. Even though this looks like it confuses the embeddings, the Transformer learns to manage this complexity. Orthogonality : Incorporating sine and cosine brings in a kind of orthogonality. This helps the model to separate out different types of information, similar again to Fourier Transforms. 3. Why Positional Encoding Gets Much Attention? The original "Attention Is All You Need" paper and later works focus a lot on positional encoding. This is because older models like RNNs and LSTMs naturally understand position. But Transformer doesn't. Positional encoding fills this gap and makes it understand sequences. The token embeddings ( x_t ) are important, for sure. But the positional ones ( v_t ) are a new addition that makes the Transformer work effectively with sequences.
