[site]: crossvalidated
[post_id]: 558115
[parent_id]: 
[tags]: 
Why estimates becomes biased after second pass (from deep learning book of Ian GoodFellow)

In Chapter 8 (Optimization for training deep models) page 278 of deep learning book (pdf: https://www.deeplearningbook.org/contents/optimization.html ), it is stated that An interesting motivation for minibatch stochastic gradient descent is that it follows the gradient of the true generalization error (equation 8.2) as long as no examples are repeated. Most implementations of minibatch stochastic gradient descent shuﬄe the dataset once and then pass through it multiple times. On the ﬁrst pass, each minibatch is used to compute an unbiased estimate of the true generalization error. On the second pass, the estimate becomes biased because it is formed by resampling values that have already been used, rather than obtaining new fair samples from the data-generating distribution. Why using the same data for more than one causes to have a biased estimate of the generalization error? What is the idea behind it? Do we assume that, in real life, it is not possible to have the same data point twice (or more), or there is something else?
