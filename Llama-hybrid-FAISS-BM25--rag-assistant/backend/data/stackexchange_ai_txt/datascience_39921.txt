[site]: datascience
[post_id]: 39921
[parent_id]: 39919
[tags]: 
I have tried something similar once. I used an approach which you would not expect, but which gave some surprisingly good results. I used the NMT (Neural Machine Translation) model in Tensorflow. There are some examples online for Vietnamese->English translation. I changed it to translate from "old job title" to "new job title", and trained it on my dataset. This is easy to do and you don't need to modify any code, except maybe tweak the model a little (e.g. length of document is now shorter than in the machine translation example). The drawback of this approach is that only the most recent job title is taken into account when using it for inference. However you could try concatenating all previous job titles and other data (schools etc) as the input text, to produce a single job title as output, and this way you could make sure you are making use of their entire career path. I would suggest tweaking aspects of the architecture (number of layers, number of previous jobs used, off-the-shelf word2vec vs domain specific trained word2vec etc) until you get the best performance against whatever evaluation metric you are using. The other approach you could take would be to take the doc2vec of each title, and train some kind of RNN/LSTM to predict the next vec given all previous vecs. Then you need a postprocessing stage on the output to convert the output vec to a text. Unfortunately I don't know of a shortcut here to avoid building your model architecture from scratch, so this approach is more work. Of course there are other ways of solving the problem but since no-one else has answered yet I thought I'd give my suggestion.
