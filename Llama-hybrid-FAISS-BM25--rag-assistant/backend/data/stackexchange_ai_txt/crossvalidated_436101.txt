[site]: crossvalidated
[post_id]: 436101
[parent_id]: 435863
[tags]: 
One problem with comparing full and partial models in general is the implicit assumption that the full model is better. If your full model is overfit, that won't be the case. (It seems that you are careful about such things, but answers here are ideally written for a general audience.) Another problem is just what the predict() and survest() functions are returning for making the comparisons. From the way you wrote the predict() code, for example, it looks like you would be looking at mean-square differences in linear predictor values; those aren't necessarily the easiest things for a wide audience to interpret. What you presumably care about the most is how well each of the models (whether full or reduced) works on the data. For that, your choice of bootstrap-validated C-index makes the most sense among the comparisons you suggest. It's not clear what you gain from trying to calculate the RMSE of the C-statistic, as the C-statistic itself seems to be of most interest and is easy to interpret: it's the fraction of pairs of cases whose actual order in time of events agrees with the order predicted by the model. That said, Harrell points out that the C-index is not a very sensitive way to compare models. Comparisons of models based on log-likelihood considerations are more sensitive. The validate() function in the rms package, with its bootstrapping, provides several such optimism-corrected indices of model quality: Nagelkerke's $R^2$ , a discrimination index $D$ , an unreliability index $U$ , and an overall quality index $Q = D - U$ , as described in Chapter 20 of Harrell's Regression Modeling Strategies, 2nd edition . You could consider choosing one of them to display differences among your full and reduced models. These indices, however, can depend on the censoring distribution of the data. Thus some question the reliability of these indices. A search on this site for Harrell and C-index provides links to extensive discussion; the discussions of comparisons among logistic regression models are generally applicable to Cox models, too. Comparing how well calibrated the models are, with the calibrate() function in the rms package, can also be useful. That function uses flexible adaptive hazard regression, not assuming linearity or proportional hazards, to test model predictions against the data at a fixed point in time. In addition to an overall bootstrap-adjusted calibration curve, the function reports the mean absolute error and the 0.9 quantile of absolute error* in predicted survival probabilities. Those estimates of precision might be the easiest for a wide audience to understand. *For the 0.9 quantile use the value displayed on a plot of the object returned by calibrate() ; last I looked there seemed to be an error in the corresponding value returned by the print() function for those objects.
