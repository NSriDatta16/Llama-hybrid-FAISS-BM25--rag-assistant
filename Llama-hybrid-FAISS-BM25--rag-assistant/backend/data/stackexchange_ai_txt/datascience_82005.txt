[site]: datascience
[post_id]: 82005
[parent_id]: 82000
[tags]: 
This is a form of continual learning / life-long learning , which tends to be very challenging because retraining on new data make traditional models "forget" what they had previously learned. Depending on the frequency by which you receive new data and the time it takes to train the model, it is often infeasible to retrain on all data every time you receive a new sample. w.r.t. LSTMs, you don't need to change your input size because you can partition your time series data according to a preset and constant input size. For example, if you have 100 ordered samples, you could have the LSTM receive input of 10 samples and predict/regress the following 11th sample. So you'd have 90 partitioned samples of constant size. If you had another ordered sample (101), then you'd have 91 partitioned samples of size 10. Continual learning is an active research field, so you can find many recent works such as this one from a couple months ago on adversarial CL. For a review of CL in robotics, you can see this paper .
