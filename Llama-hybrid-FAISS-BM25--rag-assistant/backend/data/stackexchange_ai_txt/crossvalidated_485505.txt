[site]: crossvalidated
[post_id]: 485505
[parent_id]: 
[tags]: 
ROC Curves for Regression Output

I am working on a broad machine learning-based problem, which can be approached in several different ways. Essentially, my training values are floats between 0.0 and 1.0, and I have approached this in two ways: A) treat it as a classification problem, calling values below 0.5 "class X" and above 0.5 "class Y", or B) treat it as a regression problem and try to predict the exact value between 0.0 and 1.0. I have started testing both approaches using TensorFlow, but I would like to have a way to evaluate both and compare them. In case A above, I have been using ROC curves/AUC to evaluate different parameters. In case B, I can calculate the correlation coefficient between the true and predicted values. My question is: is there a better approach to standardize the evaluation and compare both cases? For example, is there a standard way to generate a ROC curve for the regression output in case B? (I assume this would mean setting some kind of threshold for distinguishing between "true" and "false"?)
