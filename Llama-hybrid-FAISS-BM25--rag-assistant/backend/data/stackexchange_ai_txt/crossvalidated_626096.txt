[site]: crossvalidated
[post_id]: 626096
[parent_id]: 626028
[tags]: 
I am posting my answer based on what I learnt from ISO terms and definitions , GUM , a book Taylor : An introduction to error analysis. and a lengthy discussion on physics forums. What I was lagging with standard deviation and standard error. I didn't know the difference between Standard deviation of the mean (also know as standard error = SD/(N)½) and Standard deviation. Both are deviation but in different sense. Former describes that how close our mean is with respect to conventional true value (population mean) and the later one is based on dispersion of measurements. Taylor in his book (In chapter 4 and 5) described that Uncertainity reported in measurement must be standard deviation in mean not standard deviation (SD). Further in a example he tells us we must report a measurement washing off individual indications (measurements) as Mean ± Standard error (Standard deviation in mean). Now according to formula which is inversely proportional to number of measurements, we would have a negligible standard error associated with mean if we take a large number of measurements (Uncertainity ≈ 0) But that doesn't mean that standard deviation would be negligible! And as described above ( Uncertainity as SD/(N) ½ ) GUM description for uncertainty ( Uncertainity is simply an estimate of the likelihood of nearness to the best value that is consistent with presently available knowledge ) would be the best definition for Uncertainity (valid for small or large number of measurements) as sometimes we are not aware with some of bias components of error and in that condition range in mean as Uncertainity, definitely would not show true value but will describe the best estimate that can be taken in those conditions. There is One another important point that there are same terms(like mean, standard deviation etc.)in mathmatical and physical statistics but they are somewhat different based on in what sense they have used. For example In Math : Like in a classroom of 100 students if I report a measurement of their performance in test out of 10, we will not use the word Uncertainity in report because each student's mark is exact i.e no error and Uncertainty is associated with that so in that case we would report performance as Average Score ± standard deviation not like Average Score ± standard error and one more point standard deviation shows here that 68 student's marks lie in SD range In Physics : Suppose I have to measure time period of a pendulum and I take 100 measurements under similar conditions and I know that now unlike to student's test scores these 100 time periods are not correct , they have a random ( or may have systematic component also) error associated with them so I would report time period as Average ± Standard Error and this reporting will not be necessarily reflect to true value (as there may be bias included). Standard deviation here would simply represent that if someone under similar conditions measure time period 68% of his/her readings will fall in SD range. ( In classroom example we are using statistics for different students scores and each one's individual score is known perfectly but in time period calculation we would have only one true value and we are just trying to get close to that by repeating measurements as we don't know about some unknown errors that could get us take away from true value.) That's all I had to say. Correct me if I am wrong somewhere!
