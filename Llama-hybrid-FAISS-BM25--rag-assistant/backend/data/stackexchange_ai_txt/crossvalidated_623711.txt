[site]: crossvalidated
[post_id]: 623711
[parent_id]: 623709
[tags]: 
For linear regression models I have simulated data that when analyzed using screening procedures like you describe can both leave out very important predictors, and include predictors as "significant" that are actually just random noise with no relationship to the outcome. I have not simulated for logistic regression models, but I expect that similar problems can be shown. Like @shadowtalker suggests above, using something like lasso is probably going to be much better. When you ran glmnet, did you just use the defaults? You may need to specify your own lambda values instead of using the defaults, and you may need to experiment a bit to get a good set of lambda penalties to use. Even then, I am not sure what a standard error would be for a logistic model when penalized, and it may be that glmnet does not compute it for you, hence the '.'. Another approach may be to use a Bayesian model. A laplace prior, centered at 0, in a Bayesian model has similar properties to a lasso penalized model, The scale parameter of the laplace prior is related to the penalty in the lasso (you will probably want to standardize or otherwise scale your inputs). Unlike glmnet, most Bayesian tools will not fit the multiple lambda values in one step, you would need to fit each penalty/scale individually. You could also look at the horseshoe prior which pulls very small coefficients towards 0 while not making much change to the larger ones. The brms package has an implementation of the horseshoe prior (other packages do as well, but brms is the one that I remember specifically).
