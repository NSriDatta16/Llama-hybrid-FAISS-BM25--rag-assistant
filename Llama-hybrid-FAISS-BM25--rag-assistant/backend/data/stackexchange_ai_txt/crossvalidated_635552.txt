[site]: crossvalidated
[post_id]: 635552
[parent_id]: 
[tags]: 
Take into account training accuracy when inferring new data

In my research, I aim at predicting classes of biological material on many samples with variable sample sizes (say from 10 individuals to 200). The context is archaeological inference of the presence of certain types, using models trained on modern, observable, reliable material. I have trained some classifiers on data which have a certain accuracy, sometimes good (>95% on binary cases, sometimes bad eg 40% for a 5 class problem). I have 6 of such models on binary/multiple problems with various performances. Sample sizes were balanced between classes, I resampled training sets and use the good old linear discriminant analysis. Now I want to use these models to predict classes for each model on new, unknown, data. And more specifically test the "significance" of counts/proportions on each archaeological samples. The purpose is to detect the (likely) presence of certain type beyond the background noise (which is known to be huge, multiple and vicious). How can I possibly test the "significance" of each class count/proportion in each sample taking into account i) the sample size and ii) the accuracy of the model? My best guess so far would be to obtain for each class within each sample, the quantile of multinomial (or binomial for binary problems). But how can I account for the accuracy results in the training phase? I'm kind of stuck with it. Put differently, let's say I have two models: one good , one bad predicting at three classes each. good would predict A, B or C with 95% accuracy while bad would predict D, E, F with only 50% accuracy on the training sets. For a sample with say 100 individuals, when good predicts 80 A cases one can reasonably (if such a state of mind is possible in that field) think that 'A' was likely present in that sample (and get numbers for that). Now, if bad also predicts 80 'D', we have the same proportion but given that model is less accurate we can be less confident in the prediction. And yet testing 80 cases in a multinomial with 3 equiprobabilities (1/3) would lead to the same quantile. Am I missing something? Should I stop data science in archaeology :-) ? Any pointer would be very appreciated.
