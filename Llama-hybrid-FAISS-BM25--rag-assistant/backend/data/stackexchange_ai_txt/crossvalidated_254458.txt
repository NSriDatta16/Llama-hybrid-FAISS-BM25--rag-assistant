[site]: crossvalidated
[post_id]: 254458
[parent_id]: 
[tags]: 
Choice of classification loss function with unequal payoffs

Suppose that I'm building a binary classifier parameterized by $\theta \in \mathbb{R}^k$ that maps some observed features $x_i \in \mathbb{R}^l$ to a decision of whether or not to play a game with an uncertain payoff: $$ f(\centerdot;\theta): \mathbb{R}^l \to \{0,1\}.$$ The payoff of the $i$th game is $y_i \sim N(0,1)$ and my objective is to maximize my expected profit after $n$ games: $$\max_{\theta} \mathrm{E}\left[\sum_{i=1}^n y_i f(x_i;\theta)\right].$$ The classifier might be a linear model, neural network, etc. From reading https://en.wikipedia.org/wiki/Loss_functions_for_classification , my understanding is that I should construct a loss function that uses the output of a model before discretization. I imagine the reason is so that there is some sense of "how close was the prediction to reality" rather than a binary right/wrong. But if I use one of their suggested loss functions, I lose all of the information from the size of the payoff, i.e., misclassifying a tiny payoff is just as bad misclassifying a large payoff. So my question is why shouldn't I just use my objective function directly as my loss function? Why are squared loss, cross entropy loss, etc. suggested instead?
