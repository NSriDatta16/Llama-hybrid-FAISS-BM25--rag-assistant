[site]: crossvalidated
[post_id]: 588550
[parent_id]: 
[tags]: 
How to organize experimental data for A/B testing?

Context: We have a fleet of aircraft in service, the manufacturer regularly produces upgrade recommendations (SB) to be implemented on aircraft in order to increase its reliability. Reliability is measured as the count of "events" due to technical malfunction divided by the number of flights over the same period. Data: We collect data on events (when, what, which aircraft...) in a database. We also have a database with the list of upgrades (SB) and when they were installed on each aircraft. By merging this two information we now have a table that tells us for every technical incident if the events happened PRE upgrade or POST upgrade. From a third table we're able to get the amount flights to compute the reliability. Questions For each of this upgrade (SB), I want to assess whether there was a significant change in reliability, but I don't know how to organize the data. Since we're comparing rate (events / flights) for each SB I compute : Rate PRE SB Rate POST SB But to my knowledge, most statistical test requires variance input, but since this measure are already aggregate I'm stuck. So what who be the right approach there ? Instead of comparing events rate, would that make more sense to assume a Poisson distribution for the events distribution and compare whether the two distribution differ POST and PRE SB ? EDIT By considering a binomial distribution for the events, with a probability of success of "event rate", it seems to be similar to the Amazon reseller issue. For which the Beta distribution is used. Here's an illustration of how I used it to assess the significance of an upgrade on reliability. H0: event rate PRE = events rate Post Each of this table represent an upgrade (SB), with the 3 parameters required to compute the Beta probability: s : number of events (success) f : number of failure (# of flights - success) p : event rate (probability of success) On the diagonal (grey) I've computed the P(p_pre > p | s_pre, f_pre), so the probability of p_pre being greater than "p" from a sample with "s" and "f". (informative but useless for the task at hand). On the two other quadrant I've computed P(p_post > p| s_pre, f_pre) and P(p_pre > p| s_post, f_post), or the probability of the real event rate being greater than "p_pre" with a sample containing "s_post" success and "f_post failure". We would consider the difference in events rate significant if the value in both quadrant is greater than 100 - p_value or p_value. (Since the reliability can improve or deteriorate after upgrade, value outside this boundary would be consider unlikely). Does that look like a legitimate methodology ? Thank you
