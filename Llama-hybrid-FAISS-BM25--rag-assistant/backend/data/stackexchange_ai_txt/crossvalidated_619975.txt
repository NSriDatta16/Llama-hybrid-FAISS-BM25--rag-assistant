[site]: crossvalidated
[post_id]: 619975
[parent_id]: 619920
[tags]: 
This high an accuracy suggests one of several things: It could be a trivially easy prediction task. It being trivially easy does not exclude the possibility that a model for it could be useful. It could be a prediction task, where any approach will achieve a high accuracy (and perhaps accuracy is not so meaningful). Examples include cases where predicting the majority class would perform amazingly well, already. E.g. for predicting whether you will die tomorrow, the constant prediction "No" will achieve >>99% accuracy across the population. There is target leakage. Performing pre-processing before train-test-splitting is certainly potential suspect and I've seen real cases where it has (misleadingly) increased metrics like accuracy from Your cross-validation may not be set-up in an appropriate way for the problem. E.g. with time series data (which you seem to say you have), you may want to use the future as your test set based on training from the past, or predict for a completely new time series (based on training on several others - e.g. if you have time series of measurements from different people, you may want to predict things for a new person). This really depends on how the model would get used in practice and the (cross-)validation-splitting should match what would truly be available for the model. Otherwise you may end up answering (often) pointless questions like "Can I predict something for tomorrow, if I magically didn't just know the past up to now, but also what happens in the future after tomorrow?".
