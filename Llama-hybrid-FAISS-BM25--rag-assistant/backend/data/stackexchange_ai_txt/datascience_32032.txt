[site]: datascience
[post_id]: 32032
[parent_id]: 
[tags]: 
Precision and Recall if not binary

I have to calculate precision and recall for a university project to measure the quality of the classification output (with sklearn). Say this would be my results: y_true = [0, 1, 2, 1, 1] y_pred = [0, 2, 1, 2, 1] confusion matrix: [1 0 0] [0 1 2] [0 1 0] I have read about it and the definition makes sense for me in a binary setting but with 3 labels I find it hard to interpret precision/recall. If I use sklearn.metrics.precision/recall_score it gives me 0.4 for both (average = micro) Now for the precision this makes somewhat sense because 2 out of 5 are correctly classified. But I am having problems interpreting the 0.4 result for recall.
