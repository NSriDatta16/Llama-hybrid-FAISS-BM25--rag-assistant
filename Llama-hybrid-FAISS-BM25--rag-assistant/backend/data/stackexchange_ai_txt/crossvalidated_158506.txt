[site]: crossvalidated
[post_id]: 158506
[parent_id]: 134461
[tags]: 
"Not statistically significant" means that the outcome you observed would be likely to happen (typically meaning with a probability > 5%) under the hypothesis that the two methods are equally good (null hypothesis). So the problem is to figure out how likely it would be to observing the result under that hypothesis. In this case it could be due to: that particular dataset just favors the random forest algorithm you got lucky with the random processes in the algorithms For the second issue you can certainly run you experiment multiple times, and see if the random forest method consistently outperforms the other. If you have a large enough test dataset you could split it randomly and see if your results are consistent across the different subsets. However, as I hinted at in a comment above, what's important is that you ask yourself the question and indicate (in reporting the results) which steps you took to check for significance. Too many people brush these issues under the carpet or just claim significance with no further details. Note that some journals have banned certain forms of statistical testing because of misuse.
