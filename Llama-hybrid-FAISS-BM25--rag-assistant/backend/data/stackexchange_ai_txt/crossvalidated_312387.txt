[site]: crossvalidated
[post_id]: 312387
[parent_id]: 312313
[tags]: 
It is not possible to make blanket statements about RL approaches being better than all alternatives, in all possible problem spaces - even constrained to non-stochastic problems. However, RL techniques can be used successfully where state transitions are deterministic. A MDP includes deterministic state transitions as a special case. In part the MDP formulation adds generalisation, which you do not always need. A stochastic system typically will need more samples in order to learn optimal control. For physical control systems, such as robotics, transitions are often well understood, and can be modelled well in software, but tracking the true state is limited by measurement accuracy. In practice this can be treated as a stochastic effect, which is usually minor but can amplify in a non-linear environment without control. It does not hurt the application of RL here that the underlying theory allows for stochastic effects. Software simulations of deterministic physical systems are common toy problems for RL. Environments like Inverted Pendulum (Cart Pole) and Mountain Car are fully deterministic in simulation and can be used to explore different control systems. The former has workable solutions with many different techniques that compare favourably with RL (e.g. see this implementation using neural networks and genetic algorithms ). The Mountain Car problem however starts to show the strengths of RL (and related planning algorithms) due to the need to move away from goal state in order to get there more efficiently. Many computer games are essentially deterministic in terms of control and responses to control (non-deterministic events may seed the environment). For instance the game Breakout is deterministic once it starts a level - there are no opponents, no randomness in motion, the game runs on a simplified physics engine and rewards are consistent for achieving goals. This game was solved efficiently by a Q Learning algorithm - although the point of the research was to demonstrate a general learning that coped with multiple games, its success at this game again shows RL being highly competitive in a deterministic environment. Similarly, a subset of classic board games such as Go are entirely deterministic, and current state of the art AlphaGo Zero is based on RL techniques.
