[site]: crossvalidated
[post_id]: 86375
[parent_id]: 86351
[tags]: 
What you have done is logistic regression . This can be done in basically any statistical software, and the output will be similar (at least in content, albeit the presentation may differ). There is a guide to logistic regression with R on UCLA's excellent statistics help website. If you are unfamiliar with this, my answer here: difference between logit and probit models , may help you understand what LR is about (although it is written in a different context). You seem to have two models presented, I will primarily focus on the top one. In addition, there seems to have been an error in copying and pasting the model or output, so I will swap leaves.presence with Area in the output to make it consistent with the model. Here is the model I'm referring to (notice that I added (link="logit") , which is implied by family=binomial ; see ?glm and ?family ): glm(formula = leaves.presence ~ Area, family = binomial(link="logit"), data = n) Let's walk through this output (notice that I changed the name of the variable in the second line under Coefficients ): Deviance Residuals: Min 1Q Median 3Q Max -1.213 -1.044 -1.023 1.312 1.344 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.3877697 0.0282178 -13.742 Just as there are residuals in linear (OLS) regression, there can be residuals in logistic regression and other generalized linear models. They are more complicated when the response variable is not continuous, however. GLiMs can have five different types of residuals, but what comes listed standard are the deviance residuals. ( Deviance and deviance residuals are more advanced, so I'll be brief here; if this discussion is somewhat hard to follow, I wouldn't worry too much, you can skip it): Deviance Residuals: Min 1Q Median 3Q Max -1.213 -1.044 -1.023 1.312 1.344 For every data point used in your model, the deviance associated with that point is calculated. Having done this for each point, you have a set of such residuals, and the above output is simply a non-parametric description of their distribution. Next we see the information about the covariates, which is what people typically are primarily interested in: Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -0.3877697 0.0282178 -13.742 For a simple logistic regression model like this one, there is only one covariate ( Area here) and the intercept (also sometimes called the 'constant'). If you had a multiple logistic regression, there would be additional covariates listed below these, but the interpretation of the output would be the same. Under Estimate in the second row is the coefficient associated with the variable listed to the left. It is the estimated amount by which the log odds of leaves.presence would increase if Area were one unit higher. The log odds of leaves.presence when Area is $0$ is just above in the first row. (If you are not sufficiently familiar with log odds, it may help you to read my answer here: interpretation of simple predictions to odds ratios in logistic regression .) In the next column, we see the standard error associated with these estimates. That is, they are an estimate of how much, on average, these estimates would bounce around if the study were re-run identically, but with new data, over and over. (If you are not very familiar with the idea of a standard error, it may help you to read my answer here: how to interpret coefficient standard errors in linear regression .) If we were to divide the estimate by the standard error, we would get a quotient which is assumed to be normally distributed with large enough samples. This value is listed in under z value . Below Pr(>|z|) are listed the two-tailed p-values that correspond to those z-values in a standard normal distribution. Lastly, there are the traditional significance stars (and note the key below the coefficients table). The Dispersion line is printed by default with GLiMs, but doesn't add much information here (it is more important with count models, e.g.). We can ignore this. Lastly, we get information about the model and its goodness of fit: Null deviance: 16662 on 12237 degrees of freedom Residual deviance: 16651 on 12236 degrees of freedom (314 observations deleted due to missingness) AIC: 16655 Number of Fisher Scoring iterations: 4 The line about missingness is often, um, missing. It shows up here because you had 314 observations for which either leaves.presence , Area , or both were missing. Those partial observations were not used in fitting the model. The Residual deviance is a measure of the lack of fit of your model taken as a whole, whereas the Null deviance is such a measure for a reduced model that only includes the intercept. Notice that the degrees of freedom associated with these two differs by only one. Since your model has only one covariate, only one additional parameter has been estimated (the Estimate for Area ), and thus only one additional degree of freedom has been consumed. These two values can be used in conducting a test of the model as a whole, which would be analogous to the global $F$-test that comes with a multiple linear regression model. Since you have only one covariate, such a test would be uninteresting in this case. The AIC is another measure of goodness of fit that takes into account the ability of the model to fit the data. This is very useful when comparing two models where one may fit better but perhaps only by virtue of being more flexible and thus better able to fit any data. Since you have only one model, this is uninformative. The reference to Fisher scoring iterations has to do with how the model was estimated. A linear model can be fit by solving closed form equations. Unfortunately, that cannot be done with most GLiMs including logistic regression. Instead, an iterative approach (the Newton-Raphson algorithm by default) is used. Loosely, the model is fit based on a guess about what the estimates might be. The algorithm then looks around to see if the fit would be improved by using different estimates instead. If so, it moves in that direction (say, using a higher value for the estimate) and then fits the model again. The algorithm stops when it doesn't perceive that moving again would yield much additional improvement. This line tells you how many iterations there were before the process stopped and output the results. Regarding the second model and output you list, this is just a different way of displaying results. Specifically, these Coefficients: (Intercept) Areal -0.3877697 0.0008166 are the same kind of estimates discussed above (albeit from a different model and presented with less supplementary information).
