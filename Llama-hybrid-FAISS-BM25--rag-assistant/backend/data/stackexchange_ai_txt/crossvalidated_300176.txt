[site]: crossvalidated
[post_id]: 300176
[parent_id]: 300171
[tags]: 
An LSTM layer can combine multiple inputs. From this perspective it is not different than ordinary neural network layers. Ordinary neural network layers consists of multiple units. Each of these units gets input from all the (weighted) activations of the previous layer. Likewise, an LSTM unit also gets input from the (weighted) activations of the previous layer, in your case the two inputs. An LSTM unit is different in that it does some fancy stuff (the forget and output gates) with the input and that it has additinal recurrent connections. But that does not change the fact that they have arbitrarily many inputs from the previous layer. Your network could thus look like this: Inputs (input1; input2; etc.) LSTM 1 (receiving actions from all inputs) etc.
