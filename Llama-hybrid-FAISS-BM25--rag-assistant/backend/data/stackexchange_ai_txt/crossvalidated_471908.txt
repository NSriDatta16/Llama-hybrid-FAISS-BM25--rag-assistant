[site]: crossvalidated
[post_id]: 471908
[parent_id]: 471901
[tags]: 
This isn't really possible to do in sklearn. In order to do this, you need the variance-covariance matrix for the coefficients (this is the inverse of the Fisher information which is not made easy by sklearn). Somewhere on stackoverflow is a post which outlines how to get the variance covariance matrix for linear regression, but it that can't be done for logistic regression. I'll also note that you are actually using ridge logistic regression as sklearn induces a penalty on the log-likelihood by default. That being said, it is possible to do with statsmodels. import numpy as np import matplotlib.pyplot as plt from scipy.special import expit, logit from statsmodels.tools import add_constant import statsmodels.api as sm import pandas as pd x = np.linspace(-2,2) X = add_constant(x) eta = X@np.array([-2.0,0.8]) p = expit(eta) y = np.random.binomial(n=1,p=p) model = sm.Logit(y, X).fit() pred = model.predict(X) se = np.sqrt(np.array([xx@model.cov_params()@xx for xx in X])) df = pd.DataFrame({'x':x, 'pred':pred, 'ymin': expit(logit(pred) - 1.96*se), 'ymax': expit(logit(pred) + 1.96*se)}) ax = df.plot(x = 'x', y = 'pred') df.plot(x = 'x', y = 'ymin', linestyle = 'dashed', ax = ax, color = 'C0') df.plot(x = 'x', y = 'ymax', linestyle = 'dashed', ax = ax, color = 'C0') Here ymin and ymax are the 95% confidence interval limits. You may be able to subclass LogisticRegression to easily gain access to the log-likelihood in which case you can invert the Hessian of the log-likelihood yourself, but that seems like a lot of work.
