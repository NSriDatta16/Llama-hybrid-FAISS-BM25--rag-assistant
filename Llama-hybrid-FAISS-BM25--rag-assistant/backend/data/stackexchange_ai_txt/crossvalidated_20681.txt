[site]: crossvalidated
[post_id]: 20681
[parent_id]: 
[tags]: 
How do you interpret the variance of time series data using the average growth rates?

I have been recently given a piece of time series data for analysis. I have calculated the average growth rate for infant mortality per 1000 births and have achieved a variance of 60449.1376 and thus a standard deviation of 245.864, how would I interpret this? The data are annual infant mortality rates per thousand from 1951 through 1981 inclusive: 82 78 71 72 71 67 68 64 58 57 52 53 56 55 53 54 48 50 53 47 45 46 46 51 45 44 42 37 38 34 29.5
