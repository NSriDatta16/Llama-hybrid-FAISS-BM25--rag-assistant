[site]: crossvalidated
[post_id]: 506256
[parent_id]: 
[tags]: 
My KNN plot keeps creating parallel segments. Am I doing something wrong?

I am working on a project examining how counties recover following large storms. I suspect there is some degree of interaction in some of my variables, and want to use KNN to create a visual representation of this. For the purpose of fitting the KNN model and plotting the results, I created a five-level categorical variable from the recovery dependent variable, and fit a KNN model with each of the suspected independent variable pairings. That said, the results I keep getting look like a mess. For almost all of the models, the areas it creates are stripes or parallel lines. I have not been able to rule-out that my problems are the result of errors in my code--either in graphing or over-fitting. It is also probable that the relationship between the two independent variables and the dependent does not lend itself well to a KNN plot. I suspect getting parallel lines like these are the symptom of one or more cases of machine learning malpractice, but I am not practiced enough to figure out how. I am open to all suggestions. I am considering using the same code to instead examine composite variable + single variable, or composite variable + composite variable. I am hoping that adding additional dimensionality will be enough to see the dv groupings pull apart. (Sorry for the garish color choice, I was trying to keep the function flexible to the level of the dv) # listIndices is a list of test/train masks for CV def run_knn (dfY,dfX,listIndices,listParam=range(2,26)) : dictMSE = {} # track param + MSE results for CV nFidelity = 1000 # graphing fidelity listNames = list(dfX) # get names of two IVs # Begin CV # Iter over param net for param in listParam : listError = [] # Iter over train,test set for fold in listIndices : # Fit model on train sets knnClass = KNeighborsClassifier(n_neighbors=param, weights='distance', ).fit(take(dfX,fold[0],axis=0), take(dfY,fold[0],axis=0)) # Calculate error on test set. Save to list y_pred = knnClass.predict(take(dfX,fold[1],axis=0)) listError.append(metrics.accuracy_score(take(dfY,fold[1],axis=0) ,y_pred)) # end for fold # Calculate, save MSE for param dictMSE[param] = mean(listError) # end for param / end of CV # Get best param from CV, bestParam = min(dictMSE,key=dictMSE.get) bestMSE = dictMSE[bestParam] bestKNNClass = KNeighborsClassifier(n_neighbors=bestParam, weights='distance', ).fit(dfX,dfY) # Fit model with best params on full data # Begin graphing # First Column = X-axis / Get left and right boundaries x_min,x_max = dfX[listNames[0]].min()*.95,dfX[listNames[0]].max()*1.05 # Second Column = Y-axis / Get top and bottom boundaries y_min,y_max = dfX[listNames[1]].min()*.95,dfX[listNames[1]].max()*1.05 # Make grid xx,yy = meshgrid(arange(x_min,x_max,(x_max-x_min)/nFidelity), arange(y_min,y_max,(y_max-y_min)/nFidelity) ) Zs = bestKNNClass.predict(c_[xx.ravel(),yy.ravel()]) # apply fit model to grid nColors = len(dfY.unique()) # get number of levels of DV, used to gen list of colors = # of levels # Make plot Zs = Zs.reshape(xx.shape) fig = plt.figure() plt.contourf(xx,yy,Zs, cmap=ListedColormap(sns.color_palette(None,nColors))) for i in range(nColors) : sns.scatterplot(x=dfX[listNames[0]][dfY==i+1], y=dfX[listNames[1]][dfY==i+1], palette=sns.color_palette(None,nColors)[i], edgecolor='black') # endfor fig.text(.1,.03,'K = '+str(bestParam)) plt.title('Distribution of '+listNames[0]+' & '+listNames[1]) ####
