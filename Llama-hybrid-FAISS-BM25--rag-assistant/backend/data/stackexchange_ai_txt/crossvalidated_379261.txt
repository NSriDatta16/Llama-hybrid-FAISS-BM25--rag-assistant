[site]: crossvalidated
[post_id]: 379261
[parent_id]: 379257
[tags]: 
A neural network is defined by the weights on its connections, which are its parameters. It doesn't matter what data the network was trained upon, once you have a set of weights, you can throw away your training dataset without repercussion. If you want to classify a new sample, you can do it with only the parameterized network. Even if your training dataset is very large, you can still describe the network with exactly the same number of parameters as a small training set. A k-nearest neighbor classifier, as an example on the other hand, is nonparametric because it relies on the training data to make any predictions. You need every training point to describe your classifier, there's no way to abstract it into a parameterized model. In essence, the number of "parameters" in this model grows with the number of training points you have. If you want to classify a new sample, you need the training data itself, because it cannot be summarized by a smaller set of parameters. A kNN classifier training on a large dataset will have more effective parameters than a kNN classifier trained on a small dataset. The neural network is parametric because it uses a fixed number of parameters to build a model, independent of the size of the training data.
