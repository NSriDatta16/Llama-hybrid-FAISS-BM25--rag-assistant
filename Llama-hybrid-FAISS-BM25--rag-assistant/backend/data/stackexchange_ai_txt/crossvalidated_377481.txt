[site]: crossvalidated
[post_id]: 377481
[parent_id]: 376934
[tags]: 
I am going to have a go at answering this, but unfortunately it is quite difficult to follow the question due to the excessive amount of notation. Nevertheless, if I understand correctly, this is essentially a problem where you have observed values that are coming from a multivariate normal distribution (which gives the objective function you have specified) and you want to estimate the parameters that determine the expected value of the outcome, based on some complex non-linear function. Re-specifying the model: First I am going to try re-specifying your problem using less notation. In particular, I am going to refer to the parameter vector as $\boldsymbol{\alpha}$ to avoid adding another variable name, and I will assume that the variance is built into the covariance matrix $\mathbf{\Sigma}$ so that no additional parameter is needed. This will help to clarify the model. Suppose that you specify your problem as: $$\mathbf{Y}_i = \mathbf{f}(\boldsymbol{\alpha}) + \boldsymbol{\varepsilon}_i \quad \quad \quad \boldsymbol{\varepsilon}_i \sim \text{N}(\mathbf{0}, \mathbf{\Sigma}).$$ In this model form the values $\mathbf{y}_1,...,\mathbf{y}_n$ are your observed vectors (containing measurements of length, mass and age) and the parameters $\boldsymbol{\alpha}$ and $\boldsymbol{\Sigma}$ are the objects of interest in the inference. In this case you have log-likelihood function given by: $$\ell_\mathbf{y}(\boldsymbol{\alpha}, \boldsymbol{\Sigma}) = - \frac{n}{2} \det (2 \pi \boldsymbol{\Sigma}) -\frac{1}{2} \sum_{i=1}^n (\mathbf{y}_i - \mathbf{f}(\boldsymbol{\alpha}))^\text{T} \mathbf{\Sigma}^{-1} (\mathbf{y}_i - \mathbf{f}(\boldsymbol{\alpha})).$$ If $\boldsymbol{\Sigma}$ is known then this essentially reduces to the form you specified as your objective function, which you have denoted as chi-squared. (This form is where I am getting the idea that you are essentially asking about a multivariate normal model.) Now, it is quite simple to deal with this problem analytically, without using MCMC methods. Estimating the parameters: Minimising the log-likelihood function (and then adjusting the MLE for the covariance matrix to correct for bias) gives the estimators: $$\mathbf{f}(\hat{\boldsymbol{\alpha}}) = \frac{1}{n} \sum_{i=1}^n \mathbf{y}_i \quad \quad \quad \hat{\mathbf{\Sigma}} = \frac{1}{n-1} \sum_{i=1}^n \Big( \mathbf{y}_i - \frac{1}{n} \sum_{i=1}^n \mathbf{y}_i \Big) \Big( \mathbf{y}_i - \frac{1}{n} \sum_{i=1}^n \mathbf{y}_i \Big)^\text{T}.$$ (I note that you have asked whether you should minimise the log-likelihood or the original likelihood function. Since the logarithm is a monotonic transformation, these minimisation problems yield the same minimising values.) The estimator for the covariance matrix is the sample covariance, which is a simple closed-form estimator. The estimator $\hat{\boldsymbol{\alpha}}$ is potentially a bit more complicated, since it involves inversion of the vector function $\mathbf{f}$ (which you have stated is a complex non-linear function). Solving this part depends on how much you know about this function.
