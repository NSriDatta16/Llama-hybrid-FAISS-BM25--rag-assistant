[site]: crossvalidated
[post_id]: 258891
[parent_id]: 
[tags]: 
Does standardization compromises Principal Component Analysis?

I've been dealing with PCA over the past week and I ended up with a requirement for performing PCA (in Python Machine Learning Book by Sebastian Raschka): Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features. However, PCA can be defined as (in Python Machine Learning Book by Sebastian Raschka): In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. So, if we standardize (features rescaled so that they’ll have the properties of a standard normal distribution with μ=0 and σ=1) aren't we eliminating this variance characteristic and ending up with our features being the same?
