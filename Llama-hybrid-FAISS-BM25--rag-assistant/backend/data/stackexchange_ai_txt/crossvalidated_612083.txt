[site]: crossvalidated
[post_id]: 612083
[parent_id]: 592110
[tags]: 
Almost half a year late, but here goes anyway : Regarding your first and third question, all the answers are in the first paragraph of page 3 of the paper you refer to (I guess you just misread it ?) : you wrote The subset that is introduced by that paper is simply: $U_\alpha(r,c) = \{P\in\mathbb{R}^{k\times x} | h(P) \ge h(r) + h(c) - \alpha\}$ But that is not true. If you look again, the subset $U_\alpha(r,c)$ is rather defined as $$U_\alpha(r,c) := \{P\in\color{red}{U(r,c)}\mid KL(P||rc^T) \le \alpha\} \equiv U_\alpha^{KL}(r,c)$$ Which the author claims (and proves) is equal to : $$\{P\in\color{red}{U(r,c)}\mid h(P) \ge h(r) + h(c) - \alpha\}\equiv U_\alpha^h(r,c) $$ Hence, the answer to your first question is simply that the set $U_\alpha(r,c)$ is defined as a set of elements of $U(r,c)$ satisfying some extra condition, so necessarily $U_\alpha(r,c)\subseteq U(r,c)$ . You are right however that the inequality constraint on the entropy of $P$ does not imply anything on the marginals. You asked about the relationship between the KL inequality constraint and the $\gamma$ conditions : as we've seen, $U_\alpha^{KL}(r,c) = U_\alpha^{h}(r,c)$ , i.e. the KL inequality constraint is equivalent to the entropy inequality constraint (if you're not sure why, prove it !), so similarly it doesn't constrain the marginal distributions in any way. As you see, the goal of introducing these $\alpha$ -dependent constraints on $P$ is not to make sure that $P$ lies in $U(r,c)$ , but rather to add some regularization to the problem of interest (which is computing the Sinkhorn distance $d_{M,\alpha}(r,c) := \inf_{P\in U_\alpha(r,c)} \langle P,M\rangle$ ) and make it more computationally tractable. By letting $\alpha\to 0$ , the set $U_\alpha(r,c)$ becomes very small and the distance is easily computed, while letting $\alpha \to \infty$ recovers the set of all possible pairings $U(r,c)$ . There is thus some tradeoff between numerical efficiency and accuracy, as always. Lastly, you ask [...] so what does it mean to have an outer product of two distributions? In the setting of this paper, probability distributions are represented as "histogram" vectors, i.e. elements $\pi \in \mathbb R^d $ such that $\pi_1,\ldots,\pi_d \ge 0$ and $\sum_{1\le i \le d} \pi_i = 1$ . It is not hard to see that this indeed defines a probability distribution on $\{1,\ldots,d\} $ (for instance, the probability distribution "histogram" of a fair dice on $\{1,\ldots,6\} $ would be given by the vector $(1/6,\ldots,1/6)^T \in \mathbb R^6 $ ). It may seem overly restrictive to limit ourselves to finitely supported distributions, but this already covers a wide range of applications, as the paper's popularity suggests. Once you see that the vectors $r$ and $c$ respectively represent probability distributions on $\{1,\ldots,d\}$ , you should be able to see that their outer product $rc^T$ represents the joint distribution of the random vector $(X,Y)$ , where $X\sim r$ , $Y\sim c$ , and $X$ and $Y$ are independent (you should definitely prove it if it's not clear). In a sense, this is the most "trivial" joint distribution $(X,Y)$ could have given that its marginals are $r$ and $c$ . Notice here that due to the identity $h(rc^T) = h(r) + h(c)$ , $rc^T$ is an element of $U_\alpha(r,c)$ for all $\alpha \ge 0$ , hence these sets are never empty and the optimization problem is always well defined. Hope that helps !
