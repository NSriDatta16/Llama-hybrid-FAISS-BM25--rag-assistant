[site]: crossvalidated
[post_id]: 262811
[parent_id]: 
[tags]: 
Is my data a good fit for a logistic or other binomial regression?

I am trying to fit my data for a logistic regression for a decision to be made YES or NO or even 1 or 0 . I was wondering if my data is a good fit for this type of regression or should I use another tool. So far by trying the R regression tool I find the propabilites for all test data to be always YES. I might be doing something wrong. My question is: Is my data OK? The code that I use with R is: #Data SRAB= read.csv2("T1.csv", header=T, sep=";") #Testfile TF= read.csv2("ttf.csv", header=T, sep=";") sapply(SRAB,function(x) sum(is.na(x))) model =glm(Y~X1+X2, data=SRAB, family=binomial) summary(model) glm.probs = predict (model, newdata=TF, type ="response") #create a table to check the probs predictions =rep("NO", 70) predictions [glm.probs >.5]="YES" predictions[1:70] table(predictions[1:70],Y[1:70]) Below is the output with X1 as a factor: Call: glm(formula = Y ~ X1, family = binomial, data = SRAB) Deviance Residuals: Min 1Q Median 3Q Max -1.97277 -0.00005 0.00005 0.00005 1.79412 Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 8.044e-01 2.918e-01 2.757 0.00584 ** X10.001 -2.137e+01 1.773e+04 -0.001 0.99904 X10.002 1.119e-01 8.861e-01 0.126 0.89949 X10.003 -1.903e+00 8.671e-01 -2.195 0.02818 * X10.004 -6.220e-01 6.722e-01 -0.925 0.35473 X10.005 -1.092e+00 8.176e-01 -1.336 0.18165 X10.006 -1.112e-01 7.649e-01 -0.145 0.88439 omited.......................... ...................................... (Dispersion parameter for binomial family taken to be 1) Null deviance: 1714.50 on 1360 degrees of freedom Residual deviance: 473.08 on 365 degrees of freedom AIC: 2465.1 Number of Fisher Scoring iterations: 19 and below is the output with X1 as a numeric: > summary(model) Call: glm(formula = Y ~ X1, family = binomial, data = SRAB) Deviance Residuals: Min 1Q Median 3Q Max -1.5923 -1.4870 0.8932 0.8967 0.8967 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.7034728 0.0640458 10.984 A slightly different data show better results using X3 which is X3=X1/1000 Call: glm(formula = Y ~ X3, family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -1.5932 -1.4685 0.9074 0.9119 0.9120 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.6623080 0.0750333 8.827 > glm.probs = predict (model, newdata=test, type ="response") > > predictions =rep("NO", 70) > > predictions [glm.probs >.5]="YES" > predictions[1:70] [1] "NO" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" [13] "YES" "YES" "YES" "YES" "YES" "NO" "YES" "YES" "YES" "YES" "YES" "YES" [25] "YES" "NO" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" [37] "YES" "NO" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" [49] "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "NO" "YES" "YES" "YES" [61] "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" "YES" > table(predictions[1:70],Y[1:70]) NO YES NO 3 2 YES 20 45 > FT[1:70] [1] NO NO YES NO YES YES NO YES YES YES NO YES NO YES NO YES NO YES NO [20] YES YES YES YES NO YES NO NO YES NO YES YES YES YES YES YES YES NO YES [39] YES NO YES YES YES NO NO NO YES YES YES YES YES YES NO YES YES YES NO [58] YES YES YES YES YES YES YES YES NO YES NO YES NO Levels: NO YES data is below:
