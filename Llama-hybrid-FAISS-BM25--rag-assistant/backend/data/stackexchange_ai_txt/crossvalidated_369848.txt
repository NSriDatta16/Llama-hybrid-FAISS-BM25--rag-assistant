[site]: crossvalidated
[post_id]: 369848
[parent_id]: 
[tags]: 
Explanation of generalization of Newton's Method for multiple dimensions

I've been following the CS 229 lecture videos for machine learning, and in lecture 4 (~14:00), Ng explains Newton's Method for optimization to maximize an objective function ( $f$ ), but doesn't clearly explain the derivation of the higher dimension generalization: $$ \theta := H^{-1} \nabla_{\theta}\ f(\theta) $$ I've read that the Hessian matrix ( $H$ ) is a multiple dimension generalization of the second order derivative, but other than that I'm not sure I understand the formulation of the Hessian or why we multiply by its inverse; perhaps I need to brush up on linear algebra. What's an intuitive explanation of the Hessian and its inverse in this formula?
