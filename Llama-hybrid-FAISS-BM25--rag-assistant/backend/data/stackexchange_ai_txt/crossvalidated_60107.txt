[site]: crossvalidated
[post_id]: 60107
[parent_id]: 
[tags]: 
How to model sum of two values in JAGS?

Imagine, that we measured two values and we know, that in reality one measurement directly corresponds to the latent variable "s1", and the other measurement is in fact sum of two values: "s1" and unknown "s2". Given the measurements input 1 for "s1" and input 2 for "s1+s2" what is our belief about the distribution of "s1" and "s2" separately? We assume, that s1 and s2 are from normal distribution with the same variance. We measure input[i] together with its SE $=0.2$, that acts as a weight in this "fixed effects model". I did the following JAGS model: var input[2], #Input tau, s1, s2 model{ input[1] ~ dnorm(s1,25) input[2] ~ dnorm(s1+s2,25) s1 ~ dnorm(0,tau) s2 ~ dnorm(0,tau) tau~dgamma(0.001,0.001) } As I understand it, it states something like this: Model the variable s1 as mean of normal distribution so the input 1 is a probable outcome from it. Favor s1 and s2 such, that input[2] is a probable outcome from normal distribution with mean s1+s2 and SD = 0.2. Because SD for both normal distributions is the same, model should choose s1 and s2 such that input[1] and input[2] are (on average) equally plausible random samples from N(s1,0.2) and N(s1+s2,0.2). Condition 2. should make the sampler favoring adjusting s2 over s1, since it is the only place that puts any stochastic constraints on s2 (except from a vague prior), in contrast to s1, which is already constrained to make input[1] plausible result from N(s1,0.2) So I would expect the model to yield the mean(s1) $\approx$ input[1] , and mean(s2) $\approx$ input[2] - mean(s1) ... Now what I get is something completely different. What is the error in my understanding? For example, setting the input[1] and input[2] produces s1 $=0.485 \pm 0,254$ and s2 $=0,321 \pm 0,174$ (mean $\pm$ SD), where I would I expect s1 $\approx 0.3$ and s2 $\approx 0.6$ ! I used 120000 adaptation and burn-in steps. I sampled 4000000 (4 million) samples that were thinned with factor 400. Maybe I should use the dsum probability distribution (a unique feature of JAGS)? But then, this model does not run: var input[2], #Input tau, i1, i2 model{ input[1] ~ dnorm(i1,25) input[2] ~ dsum(i1,i2) i1 ~ dnorm(0,tau) i2 ~ dnorm(0,tau) tau~dgamma(0.001,0.001) } I've got the following error: RUNTIME ERROR: Inconsistent arguments for logDensity in distribution dsum And I am really not sure how to fix it, besides the fact that this model doesn't let me specify SE of the input[2] . Ultimately I am trying to model Bayesian Network Meta-Analysis (NMA) for Hazard Rate (HR) data (survival analysis, proportional hazard rates model). This type of NMA is different to the one in e.g. Gelman A.,Carlin J.B.,Stern H.S.,Rubin D.B. - Bayesian Data Analysis(2003) in that all data I have is the estimate of the difference of log Hazard Rate between groups (as opposed to estimates of each treatment group separately ). So what data tell me is the measured sum (actually the difference) between latent effects of treatment A vs B, (and B vs G, G vs X, A vs X and so on) together with reported SE of the estimate. The model above is a minimal model that illustrate the principle of the NMA with fixed effects for HR, that can be stated in simpler words: Knowing bunch of values with standard errors of differences between pair of latent (unknown) parameters describing different combinations of treatments, what is the best estimate of the difference between treatment X and Y? Is the difference greater than zero? *I cross posted this question to the J. K. Kruschke's blog and M. Plummer's JAGS forum in hope of reaching a broader audience. I'll update all the posts with any information I gather on this problem. *
