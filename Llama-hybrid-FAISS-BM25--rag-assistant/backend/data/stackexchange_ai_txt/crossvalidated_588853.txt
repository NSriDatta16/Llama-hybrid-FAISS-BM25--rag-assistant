[site]: crossvalidated
[post_id]: 588853
[parent_id]: 588843
[tags]: 
Your data is not continuous. It consists of natural integers (deaths). This means there are no 0.33 deaths or 0.56 deaths, and so on. So strictly speaking, it could be convenient for you to use a so-called "non-parametric" test, like the Mann Whitney U Test, the Wilcoxon Signed Rank Test or the Kruskal-Wallis Test. However, if you are not bothered by results as "an average of 2.5 deaths" and similar, and there is a big debate around such issues among social scientists, you can either use t-tests or analysis of variance (ANOVA), for each of the categories in the two groups. You have to pick one of these methods. The t-test is more common for smaller samples (i.e. under 30), and ANOVA is normally used when you are comparing the means of more than two groups. In both cases, you have to define your level of confidence for when your results will be significant (for example 90 % or 95 %, which means p-values have to be smaller than 0.1 or 0.05 respectively). And for reliable results, you have to make sure that the data fulfill the assumptions of the particular tests you are performing. For instance, t-tests can be one-sided, two sided, paired (for data that is not independent), your groups could have homogeneity of variances or not, for which you have to run a modified test that accounts for this. ANOVA can be one way, which means you just compare the means of two groups, or two-way, when you take into account factors. These factors could be your categories. ANOVA works for groups with homogenous variances. Other important assumptions for the tests are independence of groups and normal distribution of variables. If all this is fulfilled, t-tests and ANOVA are reliable. If not, you are probably better off with the non-parametric tests, which have less assumptions. Roughly speaking, for any of the tests, what you will be looking for is if the means (averages) in each of the groups are confidently different from each other or not. What this means is if you are mostly sure that the data points are around the observed means, to a degree of certainty that makes it possible for you to conclude that the possible differences in means confidently reflect differences between the observations in the groups. You determine this degree of confidence, by choosing your critical value and p-value. In the case of the parametric tests, which tend to be more common, both the t-test and ANOVA take in consideration how the data points are disseminated around the mean in each of the groups. If the data points are not neatly aligned around the mean, there is less confidence in the differences between the means of the groups. For example, consider a hypothetical Group A, with four observations: 15, 585 and 1400 and 4000, for an average of 1500, and another Group B, with four observations: 2900, 3000, 3100 and 3000, for an average of 3000. The average of Group B is double the average of Group A. However, is this difference "significant"? It is easy to see that Group A has a larger dissemination of data around the mean than Group B. In other words, Group A has a population variance of 2325612.5, whereas group B has a population variance of just 5000. If you consider another, Group C, with observations 1400, 1500, 1500 and 1600, which also has an average of 1500, like Group A, the difference in means between Group C and B would be more significant than the difference between Group A and B. Because Group C only has a variance of 5000, compared to the variance of 2325612.5 in Group A. This means that the observations in Group C are closer to the mean of 1500, so we can be more confident that the differences between most observations in Group B and C are reflected in the means, to a larger extent than the differences between observations in Group A and B. The differences in the means between Group A and B could be greatly explained due to individual values in Group A, but not most of the values for our confidence level. If we run two-sided t-tests, that assume homogeneity of variances, the p-value for the difference in means between Group B and A is 0.14, which is not significant for a confidence level of 95 % nor 90 %. But the difference between Group B and C (and remember that Group C has the same average as Group A) has a p-value of 2.14Eâˆ’07 (or much closer to zero), which would be significant at a confidence level of 95 %. If I interpret your table correctly, it contains averages. You want to run the tests on the individual observations that are the basis to calculate these averages. This means you have to have access to this information. It is the only way you will be able to account for variance or similar measures of how the data is organized around the mean.
