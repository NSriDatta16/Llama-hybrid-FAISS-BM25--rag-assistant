[site]: crossvalidated
[post_id]: 249746
[parent_id]: 
[tags]: 
Markov property in simple random walk

there is something about the random walk I don't understand. Specifically about "The first time when a state in Markov Chain" is reached. So in the simple random walk, there is the states of: ..., -3, -2, -1, 0, 1, 2, 3, ... (from negative infinity to positive infinity of all integers), so the states $S={...-4, -3, -2, -1,0, 1, 2, 3, ...}$. Let's $T_s = k$ denote that when we start at zero (i.e s=0), $k$ is the time that we First Reaches state $s$ (so it is the very first time we reaches the state (or in this case more like a position) of $k$ when we start from the position $0$). Let's denote $f_r(n)=P(T_r=n |X_0 =0)$ to be the probability that we are at state $r$ for the Very First time at time n given we first start at position 0. So I would like to find out $f_r(n)=P(T_r=n | X_0 = 0)$ where $r>1$. I was reading something like this, it says we can condition on $T_1$ and make use of the Markov property to have this: $f_r(n)=\sum_{k=0}^{\infty}P(T_r=n|T_1=k)f_1(k)$ So my question is how did the author make use of the Markov property in the above equation when he condition on $T_1=k$? I know that $T_1=k$ means that $X_k=1$ since it means that at time $k$, we are at position 1. And what Markov Property says is this: $P(X_{n+m} = s | X_0 = i_0 ,..., X_{n-1}=i_{n-1} ) = P(X_{n+m } = s | X_{n-1} = i_{n-1})$ (the $i_0$,$i_1$,$i_{n-1}$ are just some states in $S$). So Markov property is saying the distribution of the states at a future time point say in this case the time point $n+m$ only depends on the most recent past observation (i.e. only depends on the state at the time n-1 in this case). I am just wondering how is the Markov Property being implemented in the above equation? Thank you
