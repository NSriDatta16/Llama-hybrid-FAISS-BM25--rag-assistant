[site]: crossvalidated
[post_id]: 284182
[parent_id]: 
[tags]: 
Is feature independence needed when training a neural network with the Cross-Entropy Method?

I'm using the Cross-Entropy Method (CEM) [1] [2] to train a neural network (NN) for use in reinforcement learning. I get poor performance when my elite sample size gets larger than a couple samples, even if I increase the total number of samples as to keep the elite proportion low. I'm wondering if the problem is that my NN features are correlated/colinear. Wouldn't that cause my NN weights to be unstable? If my weights are unstable, wouldn't averaging over the elite sample be bad? For example, two sets of near-optimal NN weights could be far away from each other in parameter space, so the averaging that's done in the CEM would result in new weights being chosen in a region that's not close to either optimal region. [1] http://ai2-s2-pdfs.s3.amazonaws.com/0ff1/88986e48d17fc4b59e6c4c00681794c9fd08.pdf [2] https://arxiv.org/pdf/1503.01842.pdf
