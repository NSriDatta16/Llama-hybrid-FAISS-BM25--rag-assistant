[site]: crossvalidated
[post_id]: 500240
[parent_id]: 500110
[tags]: 
From a probabilistic perspective, a variational autoencoder is a latent variable model e.g. a generative model for $X$ using latent variables $Z$ . By standard rules of probability, we can write the true distribution over $X$ as $$ p(X) = \int p(X,Z) dZ$$ Generally we the goal is to describe high dimensional data $X$ as e.g. images with low dimensional latent variables $Z$ . From a Bayesian point of view it is clear how we get this if the generative model $p(X,Z)$ is given, just using Bayes theorem $p(Z|X) = \frac{p(X,Z)}{p(X)} = \frac{p(X|Z)p(Z)}{p(X)}$ . However sometimes it is hard to write down $p(X,Z)$ analytically, instead one may use deep generative models to capture the latent structure e.g. we can use $$ p_\theta (X) = \int p_\theta (X,Z) dZ = \int p_\theta (X|Z)p(Z) dZ$$ where the likelihood $p_\theta (X|Z)$ is parameterized by a neural network with parameters $\theta$ . The whole goal now is the find the parameters $\theta$ that minimize the KL-divergence between $p_\theta (x)$ and the observed data distribution $p_D(x)$ i.e. $$KL(p_D(x) || p_\theta(x)) = E[\log p_D(x)] - E[\log p_\theta (x)]$$ Note that the first expectation is constant w.r.t $\theta$ so we do not need it for minimization. Hence this problem is equivalent to maximize the expected log evidence $E[\log p_\theta(x)]$ . This, unfortunately, is intractable. However, we can maximize its lower bound the ELBO. Todo so we have to introduce a variation distribution $q_\phi (z|x) \approx p(z|x)$ parameterized by $\phi$ approximating the posterior over the latent variables. Note that this will be our "Encoder", hence typically also implemented by a neural network. We can write $$ E[\log p_\theta (x)] = \mathcal{L} (x,\theta, \phi) + KL(q_\phi (z|x) || p_\theta (z|x)) \geq \mathcal{L} (x,\theta, \phi)$$ Which uses the fact that the KL divergence is always positive. The ELBO is now given by $$ \mathcal{L} (x,\theta, \phi) = \sum_n E_q[\log p_\theta (x_n|z_n)] - KL(q_\phi (z_n|x_n) || p(z_n)) $$ So for Variational autoencoders (VAE) the goal is now to find both the parameters for the generative model $\theta$ a.k.a the decoder and the parameters for the variational distribution $\phi$ a.k.a the encoder! This is done by maximizing the ELBO. As we can see this will either maximize $E_q[\log p_\theta (x_n|z_n)]$ (likelihood, decoder performance) or minimize $KL(q_\phi (z_n|x_n) || p(z_n))$ (keeps the encoder as similar to the prior). This may be a confusing step because it does not include maximizing the "encoder" performance. But notice that the ELBO is an lower bound to $E[\log p_\theta(x)] = \mathcal{L} (x,\theta, \phi) + KL(q_\phi (z,x) || p_\theta (z|x))$ , hence maximizing $\mathcal{L}$ also minimizes $KL(q_\phi (z|x) || p_\theta (z|x))$ (the encoder is near the exact posterior)! For the "standard" VAE you typically choose the prior $p(z) = \mathcal{N}(0,1)$ and $q_\phi(z|x) = \mathcal{N}(\mu(x), \sigma(x))$ where $\mu(x), \sigma(x)$ are given by a neural network. Thus is because the the KL divergence becomes analytical as you notices $$KL(\mathcal{N}(\mu,\sigma)||\mathcal{N}(0,1)) = \frac{1}{2}(\sigma + \mu^2 - 1 - \log \sigma )$$ Hence we can easily calculate the ELBO. What now typically is done is to pass $x$ through the encoder and obtain $\mu, \sigma$ . Notice the ELBO still has intractable expectation which however can be approximated using MC methods by sampling $z = \mu + \sigma\epsilon$ with $\epsilon \sim N(0,1)$ . Here I also used the reparameterization trick which allows us backpropagation of gradient for $\mu, \sigma$ . Then we pass the latent variables through the decoder and compute gradient/update parameters... Hope this helps and there are not too many typos :)
