[site]: crossvalidated
[post_id]: 319656
[parent_id]: 318959
[tags]: 
So this is going to be a terribly long answer and there is an insanely simple answer at the end. Let's state our assumptions, up front. $$x_t=\text{temperature at time }t$$ $$\epsilon\sim\mathcal{N}(0,\sigma^2_t)$$ $$t\in[0,\tau]\in\mathbb{Z}$$ and $$x_{t+1}=\beta{x}_t+\alpha+\epsilon_{t+1}$$ Because of the discontinuity at $\beta=1$, there are only two possible solutions. The first solution is to use Bayesian methods. As they will be automatically admissible, their greatest difficulty will be their implementation. The alternative will be Theil's regression. It is known, ex-ante, that this is not an admissible procedure, but it is comparatively fast. It also is not clear that the information loss is large enough that most people would care about it. In the case of $\beta I do not believe quantile regression is advisable because if the real world is made up of a mixed model the quantiles may not be stable enough and Theil's method shouldn't have that difficulty as only the center needs to be stable. Nonetheless, methods like Theil's do not lend themselves to forecasts, although they do to projections. Working out probabilities might be difficult. To support a Bayesian method, we will need to create an additional parameter dimension, $M_1,$ and $M_2$, because of the discontinuity. In Model 1 the likelihood for $(\alpha,\beta)$ is a normal likelihood. In Model 2 the likelihood for $(\alpha,\beta)$ is a Cauchy likelihood. Just a note the distinction between a distribution and a likelihood is what is random. If your density function is $$\frac{1}{\pi}\frac{1}{1+(x-\mu)^2},\forall{x}\in\chi,$$ where $\chi$ is the sample space, then your likelihood function is $$\frac{1}{\pi}\frac{1}{1+(x-\mu)^2},\forall{\mu}\in\Theta,$$ where $\Theta$ is your parameter space. A likelihood function does not need to be able to integrate to one, whereas a density does. If you are using a Bayesian method, you will need to define a prior density. Unless you have actual additional information, some relatively simple prior densities make sense. The most rapid temperature change in world history was a 47-degree change in seven minutes in Great Falls, Montana. The greatest single day temperature change was 103 degrees nearby Great Falls in Loma, Montana. This gives you an idea of your bounding events that have to fit inside your prior. The prior is really two half priors because there will be two likelihoods as there are two models. Nonetheless, it is logical that one prior can cover all of $\mu$ and $\sigma$. In the absence of additional information, it is reasonable to believe that temperature changes are zero in any nearby period of time, so $\beta=1$ and $\alpha=0$ are good centers of location for the regression slopes. You will also need to create a prior mass for models one and two. Since I know nothing about weather and I do not know your time frame, I would let $\Pr(M_1)=\Pr(M_2)$. The scaling of your regression is rather simple. A 103-degree temperature change is an exceedingly rare event, but in order to overestimate the diffusion, I will treat it as a 1% or less event. Since half of my models are a Cauchy distribution and the other half normal, I will use a Cauchy prior for $\beta,\alpha,\sigma$. As no Cauchy distributed variable can be independent, though they are uncorrelated, I am imposing a bit of constraint on the model through the prior. A Cauchy distribution is also a Student's t-distribution with one degree of freedom. The double-sided 1% happens at $\mu+63.66\sigma$. So, since $103/63.66\approx{1.6}$, I can use this to scale my prior. While this satisfies the extrema for $\sigma$, it doesn't satisfy the center, which I do not know. Because I believe to actually be very small, I will set the center at zero as this should be diffuse enough to not overly influence the impact of the data, but at the same time restrict the search area. This said, this makes my overall prior density for $(\beta,\alpha,\sigma)$ to be $$\frac{1}{\pi^2}\frac{1.6^2}{\sqrt{1.6^3}\left((\beta-1)^2+\alpha^2+\sigma^2\right)^2}.$$ For simplicity, the prior will now be denoted $\pi(\beta,\alpha,\sigma)$. The likelihood for model 1 will be $$\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(\frac{(x_{t+1}-\beta{x}_t-\alpha)^2}{2\sigma^2}\right)}.$$ The likelihood for model 2 will be $$\frac{1}{\pi}\frac{\sigma}{\sigma^2+(x_{t+1}-\beta{x}_t-\alpha)^2}$$ Two adjustments need to be made to the prior that are model specific. In particular, for model 1, $\Pr(|\beta|\ge{1})=0$ and $\Pr(\sigma\le{0})=0$. For model 2, the prior needs to be adjusted to $\Pr(|\beta| The Likelihood for Model 1 will be denoted $\mathcal{L}_1$ and likewise for model 2 as $\mathcal{L}_2.$ The model-specific priors will be $\pi_1$ and $\pi_2$. Model-specific parameters will also carry subscripts. If the choice between the two models is open, then they will be denoted $j$. Overall, your posterior will be $$\Pr(\beta_1;\alpha_1;\sigma_1;M_1;\beta_2;\alpha_2;\sigma_2;M_2|x_1\dots{x}_\tau)=\frac{\prod_{i=1}^\tau\mathcal{L}(x_i,x_{i+1}|\beta_j,\alpha_j,\sigma_j,M_j)\pi_j(\beta_j;\alpha_j;\sigma_j|M_j)\Pr(M_j)}{\sum_{j=1}^2\int_0^\infty\int_{-\infty}^\infty\int_{-\infty}^\infty\prod_{i=1}^\tau\mathcal{L}(x_i,x_{i+1}|\beta_j,\alpha_j,\sigma_j,M_j)\pi_j(\beta_j;\alpha_j;\sigma_j|M_j)\Pr(M_j)\mathrm{d}\alpha_j\mathrm{d}\beta_j\mathrm{d}\sigma_j},\forall(\alpha,\beta,\sigma)\in\Theta.$$ This does not answer your question until you do one more thing. Bayesian methods have a predictive distribution. It is from the prediction that you would estimate the probability. If from the posterior you observe that $$\Pr(M_1)\gg\Pr(M_2)$$, or vice-versa, then you could discard the alternate model. If they are similar, then you would add the two models. The general Bayesian predictive distribution for any random variable $\tilde{y}$ is $$\Pr(\tilde{y}|\mathbf{Y})=\int_{\theta\in\Theta}\mathcal{L}(\tilde{y}|\theta)\Pr(\theta|\mathbf{Y})\mathrm{d}\theta,$$ where $\tilde{y}$ is your uncertain prediction and $\mathbf{Y}$ is your data. Your question is actually a prediction question. What is the probability that $x_{t+1}>x_t.$ Since $x_{\tau+1}$ is yet to be seen, you can include your entire data set. The density of predictions greater than $x_\tau$ is your probability that a future value will be greater than any specific present value, in the absence of new temperature data. I actually have an open question on this forum regarding this using Frequentist methods. As far as I can tell, nobody has developed the predictive density for the case of $\beta>1$ using Frequentist methods. If I were not that curious, then what I would do is use Theil's regression and do a Monte Carlo simulation over the data. Or, if I really wanted a simple solution, I would count the times $x_{t+1}>x_t$ and divide it by the total amount of data. Actually, that is exactly what I would do unless I needed to make predictions. Do note that you can extend your model out to any level of AR(n) process you would like by adding models. Bayesian model selection under simple assumptions is not really that different from the BIC or AIC.
