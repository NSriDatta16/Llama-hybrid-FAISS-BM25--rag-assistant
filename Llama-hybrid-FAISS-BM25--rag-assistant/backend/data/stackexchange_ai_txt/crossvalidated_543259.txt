[site]: crossvalidated
[post_id]: 543259
[parent_id]: 525085
[tags]: 
The origion of vector space model is as follows: The idea that the meaning of a word might be modeled as a point in a multi- dimensional semantic space came from psychologists like Charles E. Osgood, who had been studying how people responded to the meaning of words by assigning val- ues along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the meaning of a word in general could be modeled as a point in a multidimensional Euclidean space, and that the similarity of meaning between two words could be modeled as the distance between these points in the space. For the question how does embedding search work There are two methods: 1) you have embedding A and you compute the cosine distances between A and all the embeddings in a corpus and you rank the embeddings by the distances to find the nearest embeddings; or 2) you try the approximate nearest neighbor searching using FAISS or ScaNN . Why consine? Because it is the normalized dot product since dot product favors long vectors . Embedding is the result of one of the two vector semantic models: sparse vector models and dense vector models. Embeddings are obtained from dense vector models, and the sparse vector models include word-context and term-term matrix. We can also utilize distances between sparse vectors to measure semantic similarities/associations. Reference: Speech and Language Processing: An introduction to natural language processing
