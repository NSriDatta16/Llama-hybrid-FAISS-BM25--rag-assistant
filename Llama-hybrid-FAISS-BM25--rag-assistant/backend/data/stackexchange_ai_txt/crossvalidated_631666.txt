[site]: crossvalidated
[post_id]: 631666
[parent_id]: 
[tags]: 
Should we use train, validation, or test data when creating PR/AUC curves to optimize the decision threshold?

It makes sense to me that we can use the ROC-AUC and PR-AP scores of the validation sets during CV to tune our model hyperparameter selection. And when reporting the models final performance, it makes sense to create a PR or ROC curve on the test data set to illustrate performance on unseen data. But in terms of selecting a decision threshold for the model probabilities, does it make the most sense to create the PR/ROC curve(s) from validation data during CV, or from test data after the final model is already tuned via CV? We generally try to avoid that the test data affects how the model is created/tuned, but the decision boundary is not technically part of the model's hyper-parameters (as elaborated on in Is decision threshold a hyperparameter in logistic regression? ), so is it fine in this case and this would not count as test data leakage? This question is similar to threshold choice for binary classifier: on training, validation or test set? , but specifically about creating the PR/AUC curves where the suggested approach in that answer would not lead to the creation of a PR/AUC curve. It is also similar to the question Why ROC Curve on test set? , but that does not explicitly address why this is not considered test data leakage (which would lead to an overly optimistic evaluation)
