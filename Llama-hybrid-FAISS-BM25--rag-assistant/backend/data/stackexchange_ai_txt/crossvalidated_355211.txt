[site]: crossvalidated
[post_id]: 355211
[parent_id]: 354877
[tags]: 
EDIT: The following answers a different question than what was asked - it is framed as if $\mu$ is considered random, but does not work when $\mu$ is considered fixed, which is probably what the OP had in mind. If $\mu$ is fixed, I don't have a better answer than $\min(\hat\mu_1,...,\hat\mu_n)$ If we only consider estimates for mean and covariance, we can treat $(\mu_1, ..., \mu_n)$ as a single sample from multivariate normal distribution. A simple way to get an estimate of the minimum is then to draw a large number of samples from $MVN(\hat{\mu}, \Sigma)$, calculate the minimum of each sample and then take the mean of those minima. The above procedure and its limitations can be understood in Bayesian terms - taking the notation from Wikipedia on MVN , if $\Sigma$ is the known covariance of the estimators and we have one observation, the joint posterior distribution is $\mu \sim MVN(\frac{\hat{\mu} + m \lambda_0}{1 + m}, \frac{1}{n+m} \Sigma)$ where $\lambda_0$ and $m$ arise from the prior where, before observing any data we take the prior $\mu \sim MVN(\lambda_0, m^{-1} \Sigma$). Since you are probably not willing to put priors on $\mu$, we can take the limit as $m \rightarrow 0$, resulting in flat prior and the posterior becomes $\mu \sim MVN(\hat{\mu}, \Sigma)$. However, given the flat prior we are implicitly making the assumption that the elements of $\mu$ differ a lot (if all real numbers are equally likely, getting similar values is very unlikely). A quick simulation shows that the estimate with this procedure slightly overestimates $min(\mu)$ when the elements of $\mu$ differ a lot and underestimates $min(\mu)$ when the elements are similar. One could argue that without any prior knowledge this is correct behavior. If you are willing to state at least some prior information (e.g. $m = 0.1$), the results could become a bit better behaved for your use case. If you are willing to assume more structure, you might be able to choose a better distribution than multivariete normal. Also it might make sense to use Stan or other MCMC sampler to fit the estimates of $\mu$ in the first place. This will get you a set of samples of $(\mu_1, ..., \mu_n)$ that reflect the uncertainty in the estimators themselves, including their covariance structure (possibly richer than what MVN can provide). Once again you can than compute the minimum for each sample to get a posterior distribution over minima, and take the mean of this distribution if you need a point estimate.
