[site]: crossvalidated
[post_id]: 594431
[parent_id]: 
[tags]: 
How to prove the solution obtained from the SVM dual problem also solves the primal problem

Given observations $\textbf{x}_1,\dots,\textbf{x}_n$ with labels $y_1,\dots, y_n$ , the SVM optimization problem is \begin{align} \min_{\textbf{w},b,\xi} \ \ & \frac{1}{2}||\textbf{w}||^2 + C\textbf{1}^{\top}\xi \\ \text{s.t.}\ \ \ & \textbf{Y}(\textbf{X}\textbf{w} + b\textbf{1}) \geq \textbf{1} - \xi \\ &\xi \geq 0 \end{align} where $C > 0$ is fixed. Here, $\textbf{Y} = \text{diag}(y_1,\dots,y_n), \textbf{X} = [\textbf{x}_1^{\top}, \dots, \textbf{x}_n^{\top}]$ . The Lagrangian is given by \begin{align} L(\textbf{w},b,\xi,\alpha) = \frac{1}{2}||\textbf{w}^2|| + C\textbf{1}^{\top}\xi - \alpha^{\top}(\textbf{yXw} + b\textbf{Y1} - \textbf{1} + \xi) \end{align} The dual problem is given by \begin{align} \max_{\alpha}\ \ & \alpha^{\top}\textbf{1} - \frac{1}{2}\alpha^{\top}\textbf{Y}\textbf{X}\textbf{X}^{\top}\textbf{Y}\alpha \\ \text{s.t.}\ \ \ &\alpha^{\top}\textbf{Y}\textbf{1} = 0 \\ &0 \leq \alpha \leq C\textbf{1} \end{align} The usual approach to find the primal solution is as follows: we first find the dual solution $(\alpha^*)$ . Then we obtain the primal solution $(\textbf{w}^*, b^*, \xi^*)$ by solving \begin{align} \min_{\textbf{w}, b, \xi} \ \ &L(\textbf{w},b,\xi,\alpha^*) \end{align} In particular, this gives \begin{align*} \textbf{w}^* = \alpha^{\top} \textbf{YX} \end{align*} However, we know that in general, finding the solution to the dual problem and then finding the minimizer to the Lagrangian is not necessarily a solution to the primal problem (see Lemma 2 of this , for example). The solution must additionally be primal feasible and satisfy the complementary slackness condition. My question is, how do we prove that the solution to the SVM optimization problem obtained by solving the dual problem is actually optimal? Moreover, it's not even clear to me why the solution is even primal feasible.
