[site]: crossvalidated
[post_id]: 347698
[parent_id]: 347687
[tags]: 
Predictions are not data - they are functions of the observed data. As such, they add no information to your knowledge of the process beyond what is already in the observed data. If you make a prediction based on some observed values, and then observe the predicted value, the new information comes from the new observation , not the prediction you made using the original data. Hence, in the absence of observing your predicted value, you still just have your original set of observed data, plus a known function of that data. This general principle is easiest to understand in the context of Bayesian methodology (though the same applies in classical statistics). Suppose you observe data $\mathbf{x} = (x_1, ..., x_n)$ and you use this to make a prediction $\hat{x}_{n+1} = h(\mathbf{x})$ but you do not observe $x_{n+1}$. (It doesn't matter how you make this prediction - it is some function of the observed data.) For any later value $x_{n+k+1}$ after the predicted value (i.e., for $k \geqslant 1$), you have the predictive density: $$p(x_{n+k+1} | \mathbf{x}, \hat{x}_{n+1}) = p(x_{n+k+1} | \mathbf{x}, h(\mathbf{x})) = p(x_{n+k+1} | \mathbf{x}).$$ This shows that the prediction you have made has no effect on the predictive density for the new value $x_{n+k+1}$. All information in this predicted value is already in the observed data $\mathbf{x}$.
