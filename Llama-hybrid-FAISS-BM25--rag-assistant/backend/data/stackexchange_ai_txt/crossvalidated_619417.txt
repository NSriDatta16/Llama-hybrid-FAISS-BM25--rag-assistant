[site]: crossvalidated
[post_id]: 619417
[parent_id]: 
[tags]: 
How to decompose the scaled estimation error of the debiased machine learning estimator? [Chernozhukov et al 2018]

In page C4 of Chernozhukov et al (2018) the authors introduce a debiased ML estimator $\check{\theta}_{0}$ for scalar parameter $\theta_{0}$ : $$ Y = D\theta_{0} + g_{0}(X) + U, \quad \mathbb{E}[U|X,D] = 0 $$ $$ D = m_{0}(X) + V, \mathbb{E}[V|X] = 0 $$ $$. \hat{V} = D - \hat{m}_0(X) $$ $$\check{\theta}_{0} = \big(\frac{1}{n}\sum\hat{V}_{i}D_{i}\big)^{-1}\frac{1}{n}\sum{\hat{V}_{i}(Y_{i}-\hat{g}_{0}(X_{i})).}$$ According to the authors the scaled estimation error of $\check{\theta}_{0}$ can be decomposed into three components: $$\sqrt{n}(\check{\theta}_{0}-\theta_{0})=a^{*} + b^{*} + c^{*} $$ where $$ a^{*} = (\mathbb{E}[V^{2}])^{-1}\frac{1}{\sqrt{n}}\sum{V_{i}U_{i}}$$ $$ b^{*} = (\mathbb{E}[V^{2}])^{-1} \frac{1}{\sqrt{n}} \sum(\hat{m}_{0}(X_{i})-m_{0}(X_{i}))(\hat{g}_{0}(X_{i})-g_{0}(X_{i})) $$ and the remainder $c^{*}$ that contains terms such as $$ \frac{1}{\sqrt{n}}\sum V_{i}(\hat{g}_{0}(X_{i})-g_{0}(X_{i})).$$ I don't understand how to reach this decomposition. My progress so far: $$ \sqrt{n}(\check{\theta}_{0}-\theta_{0})= \big(\frac{1}{n}\sum\hat{V}_{i}D_{i}\big)^{-1}\frac{1}{\sqrt{n}}\sum{\hat{V}_{i}U_{i}} + \big(\frac{1}{n}\sum\hat{V}_{i}D_{i}\big)^{-1}\frac{1}{\sqrt{n}}\sum{\hat{V}_{i}(\hat{g}_{0}(X_{i})-g_{0}(X_{i}))}. $$
