[site]: datascience
[post_id]: 49566
[parent_id]: 40084
[tags]: 
Your search results are on point: without dropping or imputing data, there's no built-in way to do what you want with BernoulliNB . There is, however, a way out: train separate Bayesian models on filtered samples from your data, and then combine their predictions by stacking them. Filtering Filtering here means: Isolating samples from your original df , each having only a subset of df.columns . That way, you'd have a DataFrame only for content_2 , one for content_2, content_3 , in a sort of a factorial combination of columns. Making sure each sample is made only of rows that have no NaN s for any of the columns in the subset . This part is somewhat straightforward in your case, yet a bit lengthy: you'd have $n!$ (n factorial) combinations of columns, each of which would result in a separate sample. For example, you could have a sample named df_c2 containing only content_2 rows valued 0 or 1, df_c2_c3 with only content_2 and content_3 columns filled, and so on. These samples would make NaN values non-existent to every model you'd train. Implementing this in a smart way can be cumbersome, so I advise starting with the simplest of scenarios - e.g. two samples, two models; you'll improve gradually and reach a solid solution in code. Stacking Bayesian Models This is called Bayesian Model Averaging (BMA), and as a concept it's thoroughly addressed in this paper . There, weight attributed to a Bayesian model's predictions is its posterior probability. The content can be overwhelming to absorb in one go, be at ease if some of it doesn't stick with you. The main point here is that you'll multiply each model's predicted probabilities by a weight 0 and then sum (sum results shall be in $[0, 1]$ ). You can attribute weights empirically at first and see where it gets you. Edit: Due to the added complexity of my proposed solution, as stated in this (also useful) answer , you could opt to implement Naive Bayes in pure Python, since it's not complicated (and there are plenty tutorials to base upon). That'd make it a lot easier to bend the algorithm to your needs.
