[site]: crossvalidated
[post_id]: 298033
[parent_id]: 297125
[tags]: 
Loss function Random Forests (RF) andBBoosted Decision Trees (BDT) are techniques that optimize on a given loss function. In sklearn , the selection of loss function is done by configuring the ´criterion´ parameter. This should also very likely possible with the statistical tools available on Azure. There are loss functions that are more or less sensitive to outliers, for example the MSE is more sensitive to outliers than the Mean Absolute Error (equivalent to estimating the conditional 50 percentile, aka the conditional median). You can see a rich portfolio of loss functions here . I see the choice of loss function a convenient way to assign more or less weight to "outliers". Edit 1: I have been working on speeding up the quartile regression The estimation of quantiles can also be done by modifying the loss function so that, instead of estimating $E[Y>X]$ or the median (MAE) (2 quartile), it will generate a conditional estimation for an arbitrary quantile. This is done by minimizing a sum of asymmetrically weighted absolute residuals ( see ). While you say that you would not switch to a loss function which is less sensitive to measurement errors, you might have already done so. Outliers vs high leverage points vs influential points It is important to distinguish the concept of outlier from the concept of high leverage and influential points An outlier is a data point whose response y does not follow the general trend of the rest of the data. A data point has high leverage if it has "extreme" predictor x values. A data point is influential if it unduly influences any part of a regression analysis, such as the predicted responses, the estimated slope coefficients, or the hypothesis test results. Outliers and high leverage data points have the potential to be influential ( source ) As you mention: I could then remove all training samples with a residual greater than a few standard deviations. This method did not work. I thought it would be useful to point you to strategies and techniques to deal with outliers, leverage and ultimately influential points. Literature is focusing on classical linear regressions (OLS) but a lot of insights provided by those techniques do also provide "warning" signs when using RFs and BDTs. Have a look here : tools include Cook's distance - for measuring influence Studentized residuals - for measuring outlierness Leverage - for measuring "unusualness" of x's Edit 1 I do not think that Cook's distance nor Leverage generalizes to regression trees I'd say that Cook's distance and Leverage generalize to a tree ensemble if we assume that the ensemble should approximate a function whose behavior could also be represented with a desired level of precision by a Generalized Linear Model. Not sure if this is the case. What I did try was something like the studentized residual. The random forest gives a standard deviation so I can do this in principle. In practice it did not work well. What standard deviation are you referring to? For sure you get residuals; and btw, there's a connection bw leverage and studentized residuals ( wiki ) Bagging The OP mentions both Random Forests (RF) and Boosted Decision Trees (BDT). I just would like to stress that RFs and BDTs are not the same . Bagging, in the presence of, highly influential points, is an option: If an “unstable predictor” is defined as a predictor for which there are highly influential points, then, in most situations, our analysis is in accordance with Breiman’s: bagging stabilizes estimators and reduces variance ( source )
