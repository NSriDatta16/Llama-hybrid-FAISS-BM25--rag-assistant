[site]: crossvalidated
[post_id]: 602031
[parent_id]: 602020
[tags]: 
The problem with p-hacking in my view is not the hacking itself, but rather the interpretation of the outcome. Your professor is right saying that if you test and test and test, you will at some point find significance even if nothing is going on. This means that significances that you found in this way cannot be reliably interpreted as meaningful. However, if you get at things from an exploratory angle, this should not be the point. Whatever turns up from an exploratory analysis should not be interpreted as a meaningful final result (some people argue that significances from testing should never be interpreted in this way, but I leave this discussion out here). Rather it should be interpreted as a potential hint at something, possibly be investigated on new independent data. If you look at certain data, test one hypothesis and get $p=0.44$ and another and get $p=0.0002$ , the data that you have are more critical against the second null hypothesis than against the first, and there may be something going on. This kind of information is gradual and not dichotomous; If you run lots of tests, you should know that $p=0.04$ isn't particularly convincing, $p=0.012$ or $p=0.009$ may still occasionally happen accidentally, but $p=10^{-8}$ is still quite a strong indication (unless you have so much data that basically everything becomes significant; and also always look at effect sizes!). Digression: There are formal procedures for multiple testing such as Bonferroni, Benjamini-Hochberg etc. I think it is good to know about these, because they give some information about what kind of p-value to expect in situations where multiple testing goes on. However, I think in EDA their worth is that they provide some orientation, but they shouldn't be literally followed and "trusted". They are based on simplifying assumptions (e.g., Bonferroni can be very conservative), and they can't take into account all the informal, visual snooping around that is done in EDA. Keep in mind that the "p-hacking" problem does not only come from running multiple tests, but also from running tests conditionally on informal decisions ("I run test $T$ because this connection here looks nonlinear, so I try out adding a squared term and test it"), which runs counter to the standard assumptions of the tests. (End digression.) Ultimately I'd use p-values as exploratory indicators for things that may be worthwhile investigating without interpreting them as final meaningful result. Another issue is the automatic use of multiple significance tests inside a procedure that ultimately does something else, for example a variable selection/prediction routine. You are right that the success of these procedures can be tested and validated on independent data, and the procedure can be seen as good to the extent that it leads to a model with good prediction power, if that's your ultimate aim. The issue here is the same as before: The significance tests are not interpreted as individually meaningful; i.e., you shouldn't claim that just because a model with variable $X_4$ was selected in such a procedure you have significant evidence that variable $X_4$ is important in a meaningful way. The tests here are not used as tests in the usual sense, but rather as building blocks of a method that actually does something else, and has to be assessed on its own merits. Now of course one can ask whether it is a good idea to use significance tests in situations in which they cannot be interpreted in the standard way. My answer would be: It depends. You can hear often these days that for variable selection regularisation using ideas such as the Lasso is better than stepwise selection using significance tests, sometimes with the implicit suspicion that this is because the significance tests are used "wrongly". Well, my experience is, sometimes this is true and sometimes it isn't. I have seen data sets on which cross-validated prediction worked better using stepwise regression than the Lasso. If you don't have very many variables, this may not even be that exceptional. Also, when doing exploratory analysis, I like to look at p-values just to address the question as how "surprising" a find should actually be seen given a model in which nothing meaningful is going on behind it. However it is clear that this is just one bit of information, and the detailed visual impression and things such as effect sizes are still very relevant. Also of course I should not forget that "p-hacking" is going on, i.e., that the "model in which nothing meaningful is going on" is already violated to some extent by me snooping around. If I could formalise my "snooping" in advance, i.e., formulate a battery of things to test and maybe even some things to look at conditional on earlier results, I can generate data from a null model and in fact explore what kind of p-values I should expect to find in such a situation using the whole battery ( $10^{-8}$ not often, I suspect). A key to suitable interpretation may be to say goodbye to the idea that significance is something inherently good and meaningful. If you look at the data in order to find significances , you will find significances many of which will likely be meaningless. We should try to address the questions that are meaningful to us using the data, and then a (strong) significance is just something of a marker for further attention.
