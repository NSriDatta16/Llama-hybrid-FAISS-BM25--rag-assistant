[site]: stackoverflow
[post_id]: 4172336
[parent_id]: 
[tags]: 
Ajax Crawling: old way vs new way (#!)

Old way When I used to load page asynchronously in projects that required the content to be indexed by search engines I used a really simple technique, that is Page $('#example').click(function(){ $.ajax({ url: 'ajax/page.html', success: function(data){ $('#content').html(data); } }) }); edit: I used to implement the haschange event to support bookmarking for javascript users. New way Recently Google came up with the idea of ajax crawling, read about it here: http://code.google.com/web/ajaxcrawling/ http://www.asual.com/jquery/address/samples/crawling/ Basically they suggest to change "website.com/#page" to "website.com/#!page" and add a page that contains the fragment, like "website.com/?_escaped_fragment_=page" What's the benefit of using the new way? To me it seems that the new way adds a lot more work and complexity to something that before I did in a simple way: I designed the website to work without ajax and then I added ajax and hashchange event (to support back button and bookmarking) at a final stage. From an SEO perspective, what are the benefits of using the new way?
