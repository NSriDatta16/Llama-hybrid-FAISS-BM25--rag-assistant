[site]: crossvalidated
[post_id]: 294500
[parent_id]: 294455
[tags]: 
Unless you have a very simple model, such as linear regression, then it is very hard to construct something that extrapolates to unseen data where one or more features and/or the target variable are outside the bounds of the training data. Most non-linear models will extrapolate poorly in general, unless the underlying "true" model happens to fit nicely to how the function approximation works. One way to address this would be to construct a pipeline with simpler robust logic for counting objects. Instead of trying to regression the number of objects in scene, train an object detection model working with smaller image patches and scan it across larger images to count. There are likely to be smarter pipelines in the literature, based on generic object detection and classification, or rnn-based attention etc. Alternatively, just train your object counter on data that is representative of how it will be used. As an aside, you could try removing the first dense layer. I would expect that to reduce overall accuracy but be somewhat better at extrapolation. In addition you could try more regularisation such as dropout or L2 weight loss (maybe try max norm weights limit as the original paper on dropout suggested it was effective used alongside it). These will not fix the problem but might make a minor improvement. Another unverified approach would be to change your ground truth data to identify the centre of each object (i.e. output would be a feature map, maybe reduced in size from original, where each pixel was probability that it was the centre of a target object). Then a logical process could sum up any centres past a certain threshold. You would need to allow for fuzzy detection of centres.
