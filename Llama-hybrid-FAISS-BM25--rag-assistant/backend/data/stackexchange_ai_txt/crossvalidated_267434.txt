[site]: crossvalidated
[post_id]: 267434
[parent_id]: 267423
[tags]: 
Derivative-free optimization methods solve these type of problems where you can view your objective function as a black box. Bayesian optimization is one type of derivative-free method. It helps if you know any other structural information about your objective function. E.g., can you assume convexity? Surrogate models can used to estimate approximate gradient, e.g., take a few points, fit a quadratic function, and use the model to do a local search in the direction of its negative gradient. There is a vast amount of literature on derivative-free methods (for a review of algorithms, see http://thales.cheme.cmu.edu/dfo/comparison/dfo.pdf ). Another example of a derivative-free method is the Nelder-Mead https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method . Of course, because very little information is known about the objective function, it is difficult to solve problems with high input dimension (compared to regular optimization problems) -- you would need to consider the number of function evaluations you can afford and the size of your input variable you're optimizing.
