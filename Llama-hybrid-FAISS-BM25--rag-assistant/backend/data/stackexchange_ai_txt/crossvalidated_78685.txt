[site]: crossvalidated
[post_id]: 78685
[parent_id]: 4284
[tags]: 
I highly recommend having a look at Caltech ML course by Yaser Abu-Mostafa, Lecture 8 (Bias-Variance Tradeoff) . Here are the outlines: Say you are trying to learn the sine function: Our training set consists of only 2 data points. Let's try to do it with two models, $h_0(x)=b$ and $h_1(x)=ax+b$: For $h_0(x)=b$, when we try with many different training sets (i.e. we repeatedly select 2 data points and perform the learning on them), we obtain (left graph represents all the learnt models, right graph represent their mean g and their variance (grey area)): For $h_1(x)=ax+b$, when we try with many different training sets, we obtain: If we compare the learnt model with $h_0$ and $h_1$, we can see that $h_0$ yields more simple models than $h_1$, hence a lower variance when we consider all the models learnt with $h_0$, but the best model g (in red on the graph) learnt with $h_1$ is better than the best model learnt g with $h_0$, hence a lower bias with $h_1$: If you look at the evolution of the cost function with respect to the size of the training set (figures from Coursera - Machine Learning by Andrew Ng ): High bias: High variance:
