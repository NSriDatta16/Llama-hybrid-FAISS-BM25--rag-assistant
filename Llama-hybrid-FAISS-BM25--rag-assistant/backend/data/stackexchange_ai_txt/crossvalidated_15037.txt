[site]: crossvalidated
[post_id]: 15037
[parent_id]: 15033
[tags]: 
It helps to have data. Jakob Nielsen has measured reading times for web pages ( "How Little Do Users Read" , 2008), which gives some strong hints: The data show that the variation in reading time is directly proportional to the number of words on a page. The variation therefore should be expressed as a time relative to the page length, not as a fixed amount. The log of this ratio has an approximately normal distribution with a standard deviation of around 12%. This scatterplot from Nielsen's report is valuable for revealing the amount of relative variation in reading times, even though only a small portion (about 18%) of each page is actually read. Notice how the absolute variation increases with word length (page size); this complication is handled by using the log of the relative variation. Don't forget that book pages have varying amounts of words, too. The variation will depend on accumulated tiny differences related to word length, page width, paragraph length, amount of dialog, and so on. For a book with uniform looking pages we can therefore expect that variation to be normal, except for the ends and beginnings of chapters. The end pages will have approximately a uniform distribution of word lengths. The beginning pages will have approximately a normal distribution with a smaller mean than the typical (full) page, depending on the page design. This gives a complex distribution but it's relatively easy to simulate. The parameters should include The mean number of words per (full) page, $w$ . You can easily measure this for actual books you are trying to simulate. The standard deviation in words per (full) page, $s$ . Again, this is readily measured. The mean number of words per start page, $u$ , also easily measured. The log variation in reading times, $\sigma$ . Use a value around 12% to start, based on Nielsen's study; consider looking at other studies for other realistic values. The user's reading speed, $v$ , as words per minute, say. Values around 200-250 wpm are often used, depending on the kind of reader. The mean number of pages per chapter, $n$ , also easily measured. The mean page turning time, $t$ . You could do your own little study of readers, perhaps by spending an hour with a stopwatch in a library :-). Don't be too fussy about this number--it will depend on the book size, page material, and the reader--but it could contribute enough time to be of interest in the simulation. The simulation should comprise an entire chapter, simulated as a sequence consisting of one start page, $n-2$ normal pages, and one end page. Simulate the number of words, $m$ , as $$m = (n-2)w + u + zw + r$$ where $z$ has a uniform $(0,1)$ distribution and $r$ has a normal distribution with mean $0$ and standard deviation $s \sqrt{n}$ . Draw a value $x$ from a normal distribution with mean $0$ and standard deviation $\sigma$ . Multiply $m$ by $w \exp(x) / v$ to simulate the reading time. Add $n t$ minutes for the page turns. This process will capture the most important influences on reading time for a book with homogeneous text, uniform reading difficulty, and no illustrations. For more complex books, such as collections of readings, math or science, books with lots of dialog, illustrated books, and so on, the model may need to be more complex in order to be realistic. Edit It turns out we may be able to justify and flesh out the suggestion offered by @Jason, because it happens that this complex but realistic simulation can be extremely well approximated by a version of a Gamma distribution in most cases. We have to rescale and shift the Gamma, in addition to selecting its shape parameter. Here is a (typical) example based on $100,000$ iterations with $w=300$ words per page, $s=15$ words (SD per page), $u=100$ words per start page, $\sigma=0.12$ , $v=250$ words per minute, $n=8$ pages per chapter, and $t = 0.04$ minutes per page turn. The histogram gives the distribution of results while the solid red curve is the PDF for a Gamma distribution with shape parameter $27.416$ , scale parameter $0.2043$ , offset by $2.98$ minutes. This approximation breaks down only for extremely short chapter lengths, but is still decent even when $n=3$ : The potential advantage of this observation is that you can avoid estimating many of the parameters needed to model reading a simple, homogeneous book if you are willing to specify three independent parameters of the distribution, such as its mean, standard deviation, and skewness. For instance, if you have actual data about chapter reading times you could use the first three sample moments to fit a three-parameter Gamma distribution to the data, then perform the simulation via draws from that Gamma. Furthermore, if you assume the times to read the book chapters are independent, it is easy to add these Gammas (one per chapter) to obtain a distribution for the length of time to read the entire book (because the shape parameter for the sum of Gamma distributions having a common scale factor is the sum of their shape parameters). Even with minimal data (such as used here) you could run some simulations for a single chapter, fit a Gamma to those simulation results, and proceed to deduce (rather than simulate) the total book reading times. For instance, in this case the reading times for a book of $16$ chapters should follow a Gamma distribution with shape parameter $16 \times 27.4164$ , scale parameter $0.2043$ , offset by $16 \times 2.98$ minutes. For many books (having many chapters), the resulting distribution will be Normal for all practical purposes. The chance assigned to negative values by this distribution would be so astronomically small that it doesn't matter. The blue curve shows the distribution of book reading times. The dashed red curve superimposed on it is a Normal approximation. Neither distribution assigns any appreciable probability to times less than 240 minutes.
