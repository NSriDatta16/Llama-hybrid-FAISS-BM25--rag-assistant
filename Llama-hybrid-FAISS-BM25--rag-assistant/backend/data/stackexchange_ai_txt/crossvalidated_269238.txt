[site]: crossvalidated
[post_id]: 269238
[parent_id]: 
[tags]: 
Imputation: cosine similarity vs random forest

Long time lurker, first time question writer in CV, so please be kind. Currently, I'm exploring the benefits and drawback to performing imputation with cosine similarity and with random forest. Rather than using imputation for missing values, I'm using to use imputation for indexing a reference dataset. I think this would refer to multiple-imputation. So I've ran imputation using both cosine similarity and random forest, and I'm noticing some very peculiar results in the random forest imputation. Here I'm comparing the actual result to the imputed result from cosine similarity. I'm pretty darn happy with this result. It implies that using cosine similarity to index values in my reference data set does a good job of finding similar values. Hooray! Now I compare that with the results of the random forest imputation... I'm would judge that random forest is as well doing a good job identify similar values in my reference set, although maybe not as good as cosine similarity. But what's going on with that stair step pattern? Is this pattern an artifact of how random-forest imputation actually identifies similar targets? My thought is that this might be related to the depth of the tree used in RF, or... something related to how random forest decides which data to grab for imputation? Really just looking to understand the two methods here, so any discussion regarding imputation, random forest, or cosine similarity would be helpful.
