[site]: crossvalidated
[post_id]: 157336
[parent_id]: 
[tags]: 
Equal weight between prior probabilities

While constructing a model hierarchy for Bayesian analysis, I have two parameters: $\theta_0 \sim \textrm{Uniform}(80, 90)$ $\theta_1 \sim \textrm{Normal}(0.093, 0.002)$ I take the $\ln$ of the pdf for the parameter's value, sum them: $\sum \ln(\rm pdf)$, and obtain the prior probability. This is then also added log-likelihood of the output of the model and the likelihood of this model is obtained (as one iteration in my MCMC). However, do these priors need to be rescaled somehow? It seems wrong that $\ln(\textrm{pdf}(\theta_1)) > \ln(\textrm{pdf}(\theta_0))$ will always be true. As an example case, any value for $\theta_0$ will have a pdf of $0.1$ and $\ln(\rm pdf)$ of $-2.3$ while for $\theta_1$ a value close to the mean will have a pdf of $200$ and $\ln(\rm pdf)$ of $5.7$. I do not mean to presuppose that the importance of $\theta_1 > \theta_0$ in the model. Similarly, the likelihood values for the model seem under represented -- the $\ln(\rm pdf)$ for predicted values' errors falling under the t-distribution is much smaller than my priors.
