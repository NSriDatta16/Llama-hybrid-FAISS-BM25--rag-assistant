[site]: datascience
[post_id]: 19989
[parent_id]: 19951
[tags]: 
There is unlikely to be any useful pattern analysis for this problem. I cannot prove it, but I think it highly likely that the raindrops are being generated using a pseudo-random process. Even if they are not, you have been given more than enough information in what your controller can "sense" in this simulation, so that predicting future raindrops is not necessary. Instead, this is a problem of optimal control and planning. In machine learning, often reinforcement learning can be used to solve problems like this - potentially Q-Learning would work here, but it would be quite hard to implement it so that it learned quickly due to the large number of states. You would need to use a function approximator like a neural network, and although that can be made to work nicely here, it does not seem necessary. I don't think you need any Data Science technique here, this is closer to more traditional AI planning . However, there are strong links between planning and learning models, so what I am going to suggest is also something you might see in game-playing systems that use a combination of machine learning (reinforcement learning) and planning. A quick analysis shows that you have a large number of visible states ($2^{42}$) which would take a long time to produce optimal rules for using the simplest reinforcement learning algorithms - although it is feasible. However, you also have perfect knowledge of the dynamics and a fully deterministic system within what you can see. In addition, the branching factor of your action decisions is not high, just 3 (and sometimes 2) per step - so for instance looking 8 steps ahead will involve checking under 10,000 scenarios. That seems possible to do on every time step, which immediately suggests a simple search-based planning algorithm - similar to Monte Carlo Tree Search , except in your case you can just brute-force all combinations up to an arbitrary horizon. As long as the horizon is far enough ahead, you can achieve optimal results this way, because it only takes 3 steps to fully traverse the different positions the collector can be in. On each time step: Observe the current state and position of incoming raindrops One by one, generate feasible chains of actions, working through all possibilities, some number of steps ahead (I'm suggesting 8 steps, but wonder if you can get away with less, e.g. just 5). If the sequence of actions would take the controller outside of the allowed area, discard it. Score the sequence of actions by counting how many raindrops you predict it will collect, based on the information about raindrops at each timestep. Keep a record of the best sequence of actions so far. Using the best sequence of actions so far, choose the first action in that sequence, and take that action "for real" in the simulation, advancing by one time step. Discard the rest of the sequence, even though your controller really might take that path, ready for next phase of planning. In the simplest cases you don't revise the plan using clever tree pruning techniques, you just brute force another search after each update. A more advanced algorithm may be able to preserve the most likely sequences to go forward to next time step, and save computation (at the cost of complexity and memory used to store candidates).
