[site]: datascience
[post_id]: 61499
[parent_id]: 61491
[tags]: 
Most problems in NLP require the system to understand the semantic meaning of the text and not just the arrangement of specific words. Semantic understanding enables a system to say that, " I am happy " and " It's joyful ", have the same meaning. To incorporate this feature to a system, we present words of a particular language in form of vectors. Often called as embeddings , they help in establishing similarities between words and phrases. For instance, a vector representing the word "happy" will lie in the vicinity of the vectors representing the words "joy", "pleasure", "sad" etc. These vectors are high dimensional but using PCA or other dimensionality reduction techniques they are brought down to 3 dimensions where they could be visualized. That's why we encode words in the form of vectors. We often use cosine similarity to determine the closest vector to a given vector in analysing sematic similarity. For an intuition, the 3D space which contains vectors for all possible English words could be thought of our knowledge base . We tend to keep similar words together in our mind. If we are talking about fast food, for instance, our brain would capture the region of the knowledge base to retreive words related with fast food like "burgers", "chicken" etc.
