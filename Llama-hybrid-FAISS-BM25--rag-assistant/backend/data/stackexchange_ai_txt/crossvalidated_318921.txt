[site]: crossvalidated
[post_id]: 318921
[parent_id]: 135035
[tags]: 
You're effectively describing DenseNet-like architectures. The relevant paper is this one: Densely Connected Convolutional Networks - Huang, et. al. . The basic idea is to connect layer $i$ to all layers $k$ with $k>i$, by concatenating all incoming connections to layer $k$. While the complexity of the network grows quadratically with size, the paper demonstrates that the resulting architectures tend to be much more efficient than vanilla neural networks, requiring less overall neurons, and making considerable savings in terms of model size. In the above paper, comparable accuracy results on ImageNet are achieved with 10x less weights, compared to competing architectures like ResNet. This is because features from previous layers can now be recycled and reused in lower layers. Note that the above paper ends up using blocks of 4 layers, which are densely connected, so the resulting network isn't exactly fully dense.
