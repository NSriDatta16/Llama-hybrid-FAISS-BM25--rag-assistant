[site]: crossvalidated
[post_id]: 383362
[parent_id]: 
[tags]: 
Proof of NOT initialization of weights to zero

Can anyone provide me the mathematical details to why we shouldn't initialize a neural network (single layer ANN) weights to zero? In multi-layer we don't initialize it to zero because of loss of symmetry but why can't we initialize a single layer network weights to zero ?
