[site]: crossvalidated
[post_id]: 345967
[parent_id]: 
[tags]: 
Information leaks related to test or validation sets

Lately I've been reading about (indirect) information leaks, related to validation and tests sets in the context of hyperparameter tuning. For example, Prakhar Agarwal, in his answer Does my classification model suffer from information leak? says, that: "Every time you tune a hyperparameter of your model based on the modelâ€™s performance on the validation set, some information about the validation data leaks into the model." How can those "informations" about validation data "leak" in general? Does it happen in situations, when I tell myself for example: "I see that hyperparameter xy (e.g. learning rate) is doing really well on validation set, as all the combinations of hyperparamters where that hyperparameter had ceratain value were doing well, let's try to fix hyperparameter xy to afformentioned value and let's tune other hyperparameters." My thoughts were that in that case information that leaked was the knowledge about the best performing value of (in our case) learning rate. This is just my hypothesis, please correct me if I am wrong. Another question related is, where else in the machine learning process can direct or indirect information leaks (or "contamination" of datasets) occur?
