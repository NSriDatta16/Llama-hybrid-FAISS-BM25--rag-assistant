[site]: crossvalidated
[post_id]: 365587
[parent_id]: 
[tags]: 
Need help writing a neural network for a Pokemon battle

I'm trying to write a neural network that's able to select the optimal course of action in a Pokemon battle. In a battle, there are two different types of actions: use one of the four moves known by the Pokemon, or switch to another Pokemon in the party. My idea thus far has been to define two "scoring functions" $$ s_{move}(\text{game state}, \text{move}) \rightarrow \mathbb{R} \\ s_{switch}(\text{game state}, \text{target Pokemon}) \rightarrow \mathbb{R} $$ Above, $\text{game state}$, $\text{move}$ and $\text{target Pokemon}$ are actually long vectors that contain many features that describe the game/move/mon. For example, $\text{game state}$ might contain the type of the current Pokemon (one-hot encoded), the type of the opposing Pokemon, any status conditions/entry hazards on either side, etc. Likewise $\text{move}$ would contain info about the move's type, accuracy, base power, and so on. The outputs will correspond to probabilities after they are fed to softmax. My idea was to run $s_{move}$ on each move, and $s_{switch}$ for each Pokemon in the party, and select the action that resulted in the highest score. I wanted to use a neural network so the AI would be able to pick actions based on complex/subtle relationships (for example, if Knock Off is one of our moves the AI may want to take into account whether the target has a held item; it'd be too tedious to program for this and every other special move manually). Now I know what I want to do, I have no idea how to train this AI. Actually, that's a bit misleading-- I know the broad overview of how I want to go about training. I want to pit the AI in games against itself, starting with random weights for the NN and updating them so that it's able to win a higher percentage of games. For example, I would pit v1 of the AI (Player 1) against v1 of the AI (Player 2), then simulate 100 games; Player 1's weights would change after each game, while Player 2's weights would remain frozen. After 100 games I would flush the weight changes to Player 2, then repeat. The problem is I'm not sure about how to go about updating the weights-- how do I shift them in a direction that makes the AI more likely to win games? Is backpropogation even applicable here? See, with a traditional DNN problem you could train the NN by getting the softmax probs and running cross entropy against the "expected" probability distribution-- but here there is no "expected" distribution, because we don't know what the optimal move was supposed to be at the time of prediction. So how am I supposed to go about training this model?
