[site]: crossvalidated
[post_id]: 368746
[parent_id]: 368576
[tags]: 
An issue with recurrent neural networks is potentially exploding gradients given the repeated back-propagation mechanism. After the addition operator the absolute value of c(t) is potentially larger than 1. Passing it through a tanh operator ensures the values are scaled between -1 and 1 again, thus increasing stability during back-propagation over many timesteps.
