[site]: crossvalidated
[post_id]: 335218
[parent_id]: 
[tags]: 
Why no one talks about stochastic conjugate gradient descent?

As is known to all, stochastic gradient descent is a popular optimizer in machine learning. There have been many variants of SGD. However, it has come to my attention that no one talks about the conjugate version of SGD.(i.e. incorporate 'conjugate' ideas with stochastic settings) In general, it's impossible to maintain conjugacy of search directions in stochastic settings. But I think just keeping some of the conjugacy does not sound like a bad idea. I googled it and found only a few related papers with low citations. I'm wondering if there is something wrong with stochastic conjugate gradient descent. Anybody any ideas?
