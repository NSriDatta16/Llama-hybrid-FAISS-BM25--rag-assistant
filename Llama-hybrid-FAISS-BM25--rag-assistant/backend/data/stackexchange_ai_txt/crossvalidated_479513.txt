[site]: crossvalidated
[post_id]: 479513
[parent_id]: 479500
[tags]: 
This is a case where t-tests can go horribly wrong, see for example: Liddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong?. Journal of Experimental Social Psychology , 79, 328-348. ( https://www.sciencedirect.com/science/article/abs/pii/S0022103117307746 ) You should use an approach that respects the categorical nature of your data, for example your ignorance about whether a step up from "dissatisfied" to "neutral" means the same, in terms of satisfaction, as a step up from "satisfied" to "very satisfied". You can do this for example using ordered probit or logit models (also called proportional odds models), either in a frequentist or Bayesian framework. Above reference advocates for the latter and provides an example in R. For a frequentist example in R, see for example: https://stats.idre.ucla.edu/r/faq/ologit-coefficients/ . Useful R functions for ordinal regression are ordinal::clm2() and MASS::polr() . You inference on the strength of evidence for a difference in satisfaction and the size of that difference can then be based on the odds ratio of the binary predictor (industry type) and its associated uncertainty (e.g., confidence or credible intervals). Some obvious advantages over non-parametric methods like rank based tests: The approach explicitly estimates a parameter that describes how likely it is to give a higher category or lower category rating when the group category is changed (but beware of causal interpretation given the probably observational cross-sectional data), and allows you to condition this estimate on additional variables like gender and age, if available in your dataset. You can also predict an individual probability for each person for giving a certain response.
