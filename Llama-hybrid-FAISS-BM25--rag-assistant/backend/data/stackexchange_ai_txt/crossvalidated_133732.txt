[site]: crossvalidated
[post_id]: 133732
[parent_id]: 95038
[tags]: 
"Explaining covariance" vs. explaining variance Bishop actually means a very simple thing. Under the factor analysis model (eq. 12.64) $$p(\mathbf x|\mathbf z) = \mathcal N(\mathbf x | \mathbf W \mathbf z + \boldsymbol \mu, \boldsymbol \Psi)$$ the covariance matrix of $\mathbf x$ is going to be (eq. 12.65) $$\mathbf C = \mathbf W \mathbf W^\top + \boldsymbol \Psi.$$ This is essentially what factor analysis does : it finds a matrix of loadings and a diagonal matrix of uniquenesses such that the actually observed covariance matrix $\boldsymbol \Sigma$ is as well as possible approximated by $\mathbf C$: $$\boldsymbol \Sigma \approx \mathbf W \mathbf W^\top + \boldsymbol \Psi.$$ Notice that diagonal elements of $\mathbf C$ will be exactly equal to the diagonal elements of $\boldsymbol \Sigma$ because we can always choose the diagonal matrix $\boldsymbol \Psi$ such that the reconstruction error on the diagonal is zero. The real challenge is then to find loadings $\mathbf W$ that would well approximate the off-diagonal part of $\boldsymbol \Sigma$. The off-diagonal part of $\boldsymbol \Sigma$ consists of covariances between variables; hence Bishop's claim that factor loadings are capturing the covariances. The important bit here is that factor loadings do not care at all about individual variances (diagonal of $\boldsymbol \Sigma$). In contrast, PCA loadings $\widetilde {\mathbf W}$ are eigenvectors of the covariance matrix $\boldsymbol \Sigma$ scaled up by square roots of their eigenvalues. If only $m the whole covariance matrix (and not only its off-diagonal part as FA). This is the main difference between PCA and FA. Further comments I love the drawings in @ttnphns'es answer (+1), but I would like to stress that they deal with a very special situation of two variables. If there are only two variables under consideration, the covariance matrix is $2 \times 2$, has only one off-diagonal element and so one factor is always enough to reproduce it 100% (whereas PCA would need two components). However in general, if there are many variables (say, a dozen or more) then neither PCA nor FA with small number of components will be able to fully reproduce the covariance matrix; moreover, they will usually (even though not necessarily!) produce similar results. See my answer here for some simulations supporting this claim and for further explanations: Is there any good reason to use PCA instead of EFA? Also, can PCA be a substitute for factor analysis? So even though @ttnphns's drawings can make the impression that PCA and FA are very different, my opinion is that it is not the case, except with very few variables or in some other special situations. See also: Under which conditions do PCA and FA yield similar results? What do the first $k$ factors from factor analysis maximize? Finally: For example, let's take a look at the first loading vector $w_1$, for $1\le i,j,k\le p$, if $w_{1i}=10$, $w_{1j}=11$ and $w_{1k}=0.1$, then I'd say $x_i$ and $x_j$ are highly correlated, whereas $x_k$ seems uncorrelated with them, am I right? This is not necessarily correct. Yes, in this example $x_i$ and $x_j$ are likely to be correlated, but you are forgetting about other factors. Perhaps the loading vector $w_2$ of the second factor has large values for $x_i$ and $x_k$; this would mean that they are likely to be well correlated as well. You need to take all factors into account to make such conclusions.
