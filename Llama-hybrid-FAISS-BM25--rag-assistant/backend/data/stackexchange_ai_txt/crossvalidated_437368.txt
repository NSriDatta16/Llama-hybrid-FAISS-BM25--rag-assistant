[site]: crossvalidated
[post_id]: 437368
[parent_id]: 436985
[tags]: 
Please refer to http://ceur-ws.org/Vol-284/page107.pdf The neural network architecture adopted is shown in the figure. From the input layer to the only hidden layer, I have fixed the weights to be equal to 1 for both connections. In first neuron of the hidden layer I a squaring my weighted input, i.e., $(1*x)^2$ , and in the second neuron of the hidden layer I am taking cube of the weighted input, i.e., $(1*x)^3$ . The output layer has its inputs as stated in the question (also given below), $\alpha x_i + \beta x_i^2 + \gamma x_i^3 = y_i$ where $\alpha$ , $\beta$ , and $\gamma$ are the coefficients to be learnt. Below is the code import torch from torch.optim import Adam mat1 = torch.tensor([[1.,1.]],requires_grad=False) mat2 = torch.randn(2,1,requires_grad=True) mat3 = torch.randn(1,1,requires_grad=True) optim = Adam([ {'params': mat2}, {'params' :mat3} ], lr=1e-4) print("Mat1 size : {}".format(mat1.size())) print("Mat2 size : {}".format(mat2.size())) print("Mat3 size : {}".format(mat3.size())) def forward(x) : middle = x*mat1 print("Middle : {}".format(middle)) middle[0,0] = middle[0,0]**2 middle[0,1] = middle[0,1]**3 print("Middle : {}".format(middle)) final = torch.matmul(middle,mat2)+ mat3*x return final def loss(pred,truth) : return (pred-truth)**2 target = torch.tensor([30.]) #y is set equal to 30. x = torch.tensor([40.]) #x is set equal to 40. num_epochs =100000 while(num_epochs>0) : optim.zero_grad() prediction = forward(x) loss_val = loss(prediction,target) loss_val.backward() optim.step() print("Prediction : {}".format(prediction)) print("Alpha : {}".format(mat3[0,0])) print("Beta : {}".format(mat2[0,0])) print("Gamma : {}".format(mat2[1,0])) num_epochs -= 1 Please run the code for at least epochs>3000 to get reasonable values for the coefficients.
