[site]: crossvalidated
[post_id]: 364131
[parent_id]: 
[tags]: 
Is skip-gram model in word2vec an expanded version of N-Gram model? skip-gram vs. skip-grams?

The skip-gram model of word2vec uses a shallow neural network to learn the word embedding with (input-word, context-word) data. When I read the tutorials for the skip-gram model there was not any mentioning regarding the N-gram. However I came across several online discussions in which people claim --- skip-gram model in word2vec is an expanded version of N-Gram model. Also I don't really understand this " k-skip-n-gram " in the following Wikipedia page. Wikipedia cited a paper from 1992 for " skip-grams ", so I guess this is not the word2vec's skip-gram model, right? Another paper regarding this "skip-grams" is https://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf . This is very confusing. Why there's no one clear this up. The wikipedia source and the online discussion are as follows: https://en.wikipedia.org/wiki/N-gram#Skip-gram https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures https://www.quora.com/What-is-a-skip-gram-model-Why-is-it-better-than-other-language-models-And-how-does-it-work
