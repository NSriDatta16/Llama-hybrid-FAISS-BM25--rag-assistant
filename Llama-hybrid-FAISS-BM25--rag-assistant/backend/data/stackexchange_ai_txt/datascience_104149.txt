[site]: datascience
[post_id]: 104149
[parent_id]: 104087
[tags]: 
Your X contains representations of all the words in docs. Each doc(phrase) is represented as a 4x2 second order tensor(since you are padding with max length 4 and your word embeddings have 2 dimensions). you can obtain such matrix by using embeddings(encoded_docs).Use of padding is dependent on the downstream task you are try to do with these embeddings. PCA is usually done to map vectors $R^d \rightarrow R^c$ . Doc representations above are $R^{4*2}$ , to perform PCA on these you'll have to either flatten your doc representations or use some of the recent tensor PCA techniques. you can Flatten the representations similar to the way you've done above to get $R^8$ vectors for each doc, or you can additively combine each word in a phrase to get $R^2$ vectors. I would suggest the later option. First option requires you to pad the phrases while second option doesn't require the padding. Post this you can use regular PCA to reduce dimensions and visualize the data and identify clusters. But with the data that you are using in this example might not result in meaningful representations or visualizations as it is too small.
