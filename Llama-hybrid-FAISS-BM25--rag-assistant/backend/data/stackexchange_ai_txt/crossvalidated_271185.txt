[site]: crossvalidated
[post_id]: 271185
[parent_id]: 270391
[tags]: 
I would still love to hear other comments but I'll post my own answer for now, as I see it. While I was looking for a more "practical" answer, there are two theoretical "dis-advantages" to t-sne which are worth mentioning; the first one is less problematic, and the second should definitely be considered: t-sne cost function is not convex, so we are not guaranteed to reach a global optimum : Other dimensionality reduction techniques (Isomap, LLE) have a convex cost function. In t-sne this is not the case, hence there are some optimization parameters that need to be effectively tuned in order to reach a "good" solution. However, although a potential theoretical pitfall, it's worth mentioning that in practice this is hardly a downfall, since it seems that even the "local minimum" of the t-sne algorithm outperforms (creates better visualizations) then the global minimum of the other methods. curse of intrinstic dimensionality : One important thing to keep in mind when using t-sne is that it is essentially a manifold learning algorithm. Essentially, this means t-sne (and other such methods) are designed to work in situations in which the original high dimensional is only artificially high: there is an intrinsic lower dimension to the data. i.e, the data "sits" on a lower dimensional manifold. A nice example to have in mind is consecutive photos of the same person: While I might represent each image in the number of pixels (high-dimension), the intrinstic dimensionality of the data is actually bounded by the physical transformation of the points (in this case, the 3D rotation of the head). In such cases t-sne works well. But in cases where the intrinsic dimensionality is high, or the data points sit on a highly varying manifold, t-sne is expected to perform badly, since it's most basic assumption - local linearity on the manifold - is violated. For the practical user, I think this implies two useful suggestions to bear in mind: Before performing dimensionality reduction for visualization methods, always try to first figure out if there actually exists a lower intrinsic dimension to the data you're dealing with. If you're not sure about 1 (and also generally), it might be useful, as the original article suggests, to "perform t-sne on a data representation obtained from a model that represents the highly varying data manifold efficiently in a number of nonlinear layers, such as an auto-encoder". So the combination of auto-encoder + t-sne can be a good solution in such cases.
