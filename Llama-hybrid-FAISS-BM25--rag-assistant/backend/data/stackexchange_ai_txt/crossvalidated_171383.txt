[site]: crossvalidated
[post_id]: 171383
[parent_id]: 26764
[tags]: 
This is a deep philosophical question which is commonly addressed from the statistical as well as machine learning end. Some say, categorizing is better for discrete to categorical indicator, so that the packages can easily digest the model inputs. Others say, that binning can cause information loss, but however categorical variables can/must be converted to {1,0} indicator variables leaving out the last class for the model residuals. The book - Applied linear regression (Kutner et al. ) mentions about the logic of introducing indicator variables in the model in the first few chapters. There may be other similar text too. My take on this maybe a bit too far-fetched: If we imagine the categorical variables like blocks in an experimental design, the indicator variable is a natural extension to non-experiment based data analysis. With respect to data mining algorithms (decision tree families), categorization is inevitable (either manually or automated-binning) which has to be fed to the model. Hence, there may not be a model that is specialized for numerical as well as categorical variables in the same way (without binning-numerical or using indicators-categorical).
