[site]: crossvalidated
[post_id]: 388464
[parent_id]: 331986
[tags]: 
What I will say here is that the proofs that compression guarantees a better lower bound on generalization are accepted, but it's not widely accepted if this lower bound is practically relevant. For example, a model with better compression might increase the lower bound from 1.0 to 1.5, but it might not be relevant if all models are already performing from 2.0-2.5. Likewise, I think it's apparent that while compression is sufficient for some amount of guaranteed generalization, it's clearly not necessary (for example, invertible neural networks can get just fine generalization). Probably the right conclusion is that the theory and analysis are a useful direction but it's unclear if it says anything about real networks.
