[site]: crossvalidated
[post_id]: 508201
[parent_id]: 
[tags]: 
Equality of optimization problems of the soft-margin SVM (moving constraint to objective function)

I'm reading the derivation of the soft-margin SVM optimization problem in Elements of Statistical learning. In it, the authors claim that \begin{align} \min_{} \quad & \| \mathbf{w}\| \\ \text{s.t.} \quad & y_i(\mathbf{w}\cdot\mathbf{x_i}+b) \geq (1-\epsilon_i) \\ \quad & \epsilon_i \geq 0 \quad \forall i \\ \quad & \sum_{i=1}^m\epsilon_i \leq C\\ \ \end{align} (which is equation 12.7 in the book) is equal to this \begin{align} \min_{b, \mathbf{w}, \mathbf{\epsilon}} \quad & \frac{1}{2}\| \mathbf{w}\|^2 + C\sum_{i=1}^m\epsilon_i \\ \text{s.t.} \quad & y_i(\mathbf{w}\cdot\mathbf{x_i}+b) \geq 1-\epsilon_i \\ \quad & \epsilon_i \geq 0 \quad \forall i \end{align} (which is equation 12.8 in the book). My question is how can you move the constraint $\sum_{i=1}^m\epsilon_i \leq C$ to the objective function in this way?
