[site]: datascience
[post_id]: 67777
[parent_id]: 40013
[tags]: 
Andrew Ng's explanation actually covers the YOLOv2 which uses anchor boxes. YOLOv1, which is the paper you linked, does not use anchor boxes so its not exactly the same. They key to understanding how the bounding boxes are formed is to first understand how the output is encoded. To which, I'll recommend this link: https://hackernoon.com/understanding-yolo-f5a74bbc7967 Briefly speaking, and I'll be using the example from the paper, for S=7, B=2 and C=20, our output is a 7x7x30 tensor that encodes where (bounding box coordinates) and what the objects (probability of class) are. To achieve this, we construct a fully-connected layer at the end of our CNN that will give us 7x7x30 (rather forcefully). Hence on our first forward pass, each cell will have 2 random bounding boxes. A loss is calculated. The weights of the CNN will then be adjusted according to reduce that loss (opitimisation). Then the following passes will produce bounding boxes closer to the ground truth.
