[site]: crossvalidated
[post_id]: 130024
[parent_id]: 129999
[tags]: 
Is there a known bias with telephone polls in the U.S.? Yes, there is a known bias with telephone polls in the U.S.: cellphones. For a long time, the gold-standard of randomized telephone polling in the US was random digit dialing: machines generated random phone numbers and pollsters asked questions of whoever picked up. Assuming everyone had one and only one phone, the poll was a simple random sample of phone owners. However, the arrival of cellphones changed that: many people, especially the young, have no landline. Even within specific demographics (age bracket, gender, race), these cellphone-only populations have distinct political attitudes from populations with landlines or both types of phones. Since it's illegal for automated dialers to call cellphones, this wreaks havoc with polls. For pollsters, this means that they have to use human beings to call all cellphones, which is obviously much more expensive. So pollsters must, a priori , decide how many cellphones they need to call to control costs. The number of cellphone calls must be balanced against the number of automatic landline calls in a scientifically-sound way -- dialing enough cellphone-only houses, landline-only houses and houses with both in proportion to their incidence in the population, all while not exceeding their budget! The effect of cellphones probably isn't the source of potential bias. I suspect that pollsters that the Washington Times/ABC poll used is well aware of all of this, since how to construct a random sample in light of the "cellphone effect" it's pretty much the topic that all contemporary political polling is dealing with today. So it's likely that the pollsters took steps to mitigate the "cellphone effect" in this poll. However, perhaps the pollsters had a bad day and their random dialing of cellphone numbers did not yield enough actual responses. If there is bias, it could be the sample weights. Another potential source of bias in these polls is the sample weighting. The goal of weighting samples is to reduce the number of people you need to reach in various (combinations of) demographic groups by attributing different weights to different population segments. This, of course, leads to the question "how do you know you've weighted the sample correctly?" This is why some polling firms will have estimates that are consistently above/below those of other polling firms: they have different methods for weighting their sample, and that consistently pulls the result in a particular direction. This is an example of the bias-variance trade-off. The pollsters could use a small, simple random sample to achieve a high-variance estimate. Or they could use weights to reduce the variance of the estimate from the small sample, but at the cost of (statistical) bias. Pollsters (and statisticians generally) all have to grapple with the bias-variance trade-off in one way or another. It doesn't necessarily mean that they're malevolent or attempting to manipulate the public. Alternative explanations abound. There are lots of ways to mess up a poll, accidentally or deliberately. Rephrasing a question can yield different answers among different groups. Push-polling, in which questions are framed to elicit a particular response, is the most obvious and nefarious example of this. Would an organization interested in manipulating public opinion choose a telephone poll, over online polls, or interviewing in the street, if they were after a specific bias? This question isn't entirely statistical, as it calls for speculation about the motives and methods of some hypothetical, malevolent entity. I can only answer it partially. Interviewing people on the street isn't really random, and is probably biased (in the sense that it will tend to include some groups at greater proportion than others). The goal of random-digit dialing is to achieve a state where everyone has equal probability of selection. Going to some street corner and interviewing people doesn't quite achieve this, because you'll tend to include people who live and work nearby. If you go to a street in Washington, DC, you'll have nearly 0 chance of selecting a person who is not from DC, Maryland or Virginia. Moreover, consider which street you go to. Interviewing people on the street in, say, the Cleveland Park neighborhood of Washington, DC will be very different than interviewing people on the street in Anacostia: these places have very distinct demographic composition. Polling people online isn't really random, but it's unclear whether it's worse than phone polling. The population of people opting-in to an online poll is a self-selecting population, not necessarily randomly sampled from all potential respondents. Traditional pollsters who tend to favor telephone polling are very skeptical of online polls for this reason. However, people like Andrew Gelman are less certain that the end result of a randomly-dialed phone poll and a self-selected online poll is meaningfully different. This is very much an open area of research.
