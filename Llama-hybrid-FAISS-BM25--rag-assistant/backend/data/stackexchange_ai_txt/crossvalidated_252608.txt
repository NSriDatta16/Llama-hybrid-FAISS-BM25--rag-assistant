[site]: crossvalidated
[post_id]: 252608
[parent_id]: 252577
[tags]: 
The simple linear regression model $$ y_i = \alpha + \beta x_i + \varepsilon $$ can be written in terms of the probabilistic model behind it $$ \mu_i = \alpha + \beta x_i \\ y_i \sim \mathcal{N}(\mu_i, \sigma) $$ i.e. dependent variable $Y$ follows normal distribution parametrized by mean $\mu_i$ , that is a linear function of $X$ parametrized by $\alpha,\beta$ , and by standard deviation $\sigma$ . If you estimate such a model using ordinary least squares , you do not have to bother about the probabilistic formulation, because you are searching for optimal values of $\alpha,\beta$ parameters by minimizing the squared errors of fitted values to predicted values. On another hand, you could estimate such model using maximum likelihood estimation , where you would be looking for optimal values of parameters by maximizing the likelihood function $$ \DeclareMathOperator*{\argmax}{arg\,max} \argmax_{\alpha,\,\beta,\,\sigma} \prod_{i=1}^n \mathcal{N}(y_i; \alpha + \beta x_i, \sigma) $$ where $\mathcal{N}$ is a density function of normal distribution evaluated at $y_i$ points, parametrized by means $\alpha + \beta x_i$ and standard deviation $\sigma$ . In the Bayesian approach instead of maximizing the likelihood function alone, we would assume prior distributions for the parameters and use the Bayes theorem $$ \text{posterior} \propto \text{likelihood} \times \text{prior} $$ The likelihood function is the same as above, but what changes is that you assume some prior distributions for the estimated parameters $\alpha,\beta,\sigma$ and include them into the equation $$ \underbrace{f(\alpha,\beta,\sigma\mid Y,X)}_{\text{posterior}} \propto \underbrace{\prod_{i=1}^n \mathcal{N}(y_i\mid \alpha + \beta x_i, \sigma)}_{\text{likelihood}} \; \underbrace{f_{\alpha}(\alpha) \, f_{\beta}(\beta) \, f_{\sigma}(\sigma)}_{\text{priors}} $$ "What distributions?" is a different question, since there is an unlimited number of choices. For $\alpha,\beta$ parameters you could, for example, assume normal distributions parametrized by some hyperparameters , or $t$ -distribution if you want to assume heavier tails, or uniform distribution if you do not want to make many assumptions, but you want to assume that the parameters can be a priori "anything in the given range", etc. For $\sigma$ you need to assume some prior distribution that is bounded to be greater than zero since standard deviation needs to be positive. This may lead to the model formulation as illustrated below by John K. Kruschke. (source: http://www.indiana.edu/~kruschke/BMLR/ ) While in the maximum likelihood you were looking for a single optimal value for each of the parameters, in the Bayesian approach by applying the Bayes theorem you obtain the posterior distribution of the parameters. The final estimate will depend on the information that comes from your data and from your priors , but the more information is contained in your data, the less influential are priors . Notice that when using uniform priors, they take form $f(\theta) \propto 1$ after dropping the normalizing constants. This makes Bayes theorem proportional to the likelihood function alone, so the posterior distribution will reach its maximum at exactly the same point as the maximum likelihood estimate. What follows, the estimate under uniform priors will be the same as by using ordinary least squares since minimizing the squared errors corresponds to maximizing the normal likelihood . To estimate a model in the Bayesian approach in some cases you can use conjugate priors , so the posterior distribution is directly available (see example here ). However, in the vast majority of cases, posterior distribution will not be directly available and you will have to use Markov Chain Monte Carlo methods for estimating the model (check this example of using Metropolis-Hastings algorithm to estimate parameters of linear regression). Finally, if you are only interested in point estimates of parameters, you could use maximum a posteriori estimation , i.e. $$ \argmax_{\alpha,\,\beta,\,\sigma} f(\alpha,\beta,\sigma\mid Y,X) $$ For a more detailed description of logistic regression, you can check the Bayesian logit model - intuitive explanation? thread. For learning more you could check the following books: Kruschke, J. (2014). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press. Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2004). Bayesian data analysis. Chapman & Hall/CRC.
