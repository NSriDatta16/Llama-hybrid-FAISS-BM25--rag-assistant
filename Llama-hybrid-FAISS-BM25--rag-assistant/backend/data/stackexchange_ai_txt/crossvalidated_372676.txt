[site]: crossvalidated
[post_id]: 372676
[parent_id]: 
[tags]: 
Theoretically can gradient boosting achieve 100% of accuracy in an arbitrary dataset?

Consider gradient boosting like gbm or xgboost. I have a labelled dataset (X,y). If I don't care about over-fitting and I allow gbm or xgboost grow as much as needed, eventually can I reach the accuracy of 100% in predicting in training dataset?
