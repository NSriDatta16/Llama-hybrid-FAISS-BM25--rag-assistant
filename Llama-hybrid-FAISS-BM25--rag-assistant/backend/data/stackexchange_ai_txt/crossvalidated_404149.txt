[site]: crossvalidated
[post_id]: 404149
[parent_id]: 
[tags]: 
Gamma distribution parameters estimation

I have a set of samples taken from a population distributed with a Gamma distribution , so \begin{equation} f_X(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x} \end{equation} I should estimate $\alpha$ and $\beta$ , so knowing that \begin{equation} \alpha=\frac{E[X]^2}{\text{Var}[X]},\quad \beta=\frac{E[X]}{\text{Var}[X]} \end{equation} My idea is to compute $\hat \mu, \hat \sigma^2$ from the set as estimators of the mean and average of the population, then find $\hat \alpha,\hat \beta$ as follows \begin{equation} \hat \alpha=\frac{\hat \mu^2}{\hat \sigma^2},\quad \hat \beta=\frac{\hat \mu}{\sigma^2} \end{equation} I'm trying to apply this logic to a task with a set of 10000 samples, but approximations seems to be not good enough. Is there any flaw in this reasoning? EDIT: I've also tried the Newton-Raphson method to find alpha. This method leads to approximations very similar to the ones that I get from the method of moments. EDIT 2: The logic is correct. Problem was a wrong understanding of the results of the implementation of this logic: in my task there was just half of the data needed to get all the estimators required. That is true for any possible method. Also approximation was fine.
