[site]: crossvalidated
[post_id]: 413179
[parent_id]: 413112
[tags]: 
Over-fitting is not the problem here, the problem is that the estimate of generalisation performance will be optimistically biased as the test set has been used to tune some aspect of the model (in this case the hyper-parameters). Whether you will have encountered over-fitting in model selection depends on how many hyper-parameters you have to tune, the sizes of the training and test sets and the sensitivity of your hyper-parameter tuning criterion to over-fitting. Note that over-fitting in model selection can result in a model that over-fits the data or under-fits it. I wrote a paper to clarify a few of these issues: G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ). Note that there often isn't really a statistical distinction between parameters and hyper-parameters (just one set of parameters for which there is a computationally efficient procedure for finding the optimal values, given the values of a second set, which we call hyper-parameters - see my answer on a related question here ). So the problems with tuning parameters or hyper-parameters on the test data are actually very similar.
