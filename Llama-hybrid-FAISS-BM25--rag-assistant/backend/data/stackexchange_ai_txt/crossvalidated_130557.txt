[site]: crossvalidated
[post_id]: 130557
[parent_id]: 130076
[tags]: 
Excellent question. I think your confusion may result from some of the basic differences between the "frequentist" and "Bayesian" perspectives. I have a lot of experience with the former and am new to the later so attempting a few simple observations might help me too. I edited your question to make a few distinctions clear - at least, as I understand them. I hope you don't mind! If I got something wrong, you could re-edit your question or add a comment on this response. 1) At the risk of sounding somewhat too elementary: A model is any statement that attempts an explanation of reality like "If I had pancakes for breakfast, it must be Tuesday." As such, a model is an hypothesis. A famous quote by George Box: "All models are wrong, some models are useful." For a model to be useful there must be some way to test it. Enter the concept of competing hypotheses and the answer to one of your questions. I would suggest that "...in the context of statistical inference," an hypothesis is any model that may be useful and can be tested mathematically. So hypothesis testing is a means of making a decision about whether a model is useful of not. In summary, an hypothesis is a model under consideration. It could be different parameter values of the same function or different functions. I think your lecture notes are showing that different outcomes (measurements) in the sample space would make different hypotheses (Is the intercept parameter zero? Do I need a cube in that polynomial? Maybe it's really exponential?), more or less likely. 2) Your Kahn video is an example of what Bayesian's call the "Frequentist" approach to hypothesis testing so it may have confused you when trying to apply it to your lecture notes which are Bayesian. I have been trying to come up with a simple distinction between application of the two approaches (which may be dangerous). I think I understand the philosophical distinction reasonably well. From what I have seen, the "Frequentist" assumes a random component to the data and tests how likely the observed data are given non-random parameters. The "Bayesian" assumes the data are fixed and determines the most likely value of random parameters. This difference leads to different testing methods. In "Frequentist" hypothesis testing, a model that may be useful is one which explains some effect so it is compared with the "null hypothesis" - the model of no effect. The attempt is made to set up a useful model that is mutually exclusive to the model of no effect. The test is then on the probability of observing the data under the assumption of no effect. If that probability is found to be low, the null hypothesis is rejected and the alternative is all that's left. (Note that a purist would never "accept" the null hypothesis, only "fail to reject" one. It may sound like angels dancing on the head of a pin but the distinction is a fundamental philosophical one) Intro statistics usually starts with what may be the simplest example: "Two groups are different." The null hypothesis that they are not different is tested by calculating how likely it would be to observe differences as great or greater as measured by a random experiment given that they are not different. This is usually a t-test where the null hypothesis is that the difference of the means is zero. So the parameter is the mean at a fixed value of zero. The Bayesian says, "Hold on a minute, we made those measurements and they are different, so how likely is that?" They calculate the probability for every value of the (now) random parameter and pick the one that is highest as the most likely. So in a sense, every possible value of the parameter is a separate model. But now they need a way to make a decision about whether the model with the highest probability is different enough to matter. That's why your lecture notes introduced the cost function. To make a good decision, some assumption of the consequences of making the wrong decision is needed. 3) "What does it mean to assign a hypothesis to each data sample?" I don't think they are. Be careful with what is meant by "sample point." I believe they are referring to a particular sample vector and want to know how likely each hypothesis is for all sample vectors in the sample space. Equations (14) and (15) show how to compare two hypotheses for a particular sample vector. So they are simplifying a general argument of comparing multiple hypotheses by showing how to compare only two.
