[site]: crossvalidated
[post_id]: 533540
[parent_id]: 
[tags]: 
python: xgboost parameters overfitting

I am using the xgboost regression algorithm to predict a continuous variable. after splitting the data between train and test, I kept changing the xgb parameters to obtain the best possible predictive for both train and test, but it looks like that while the model has learned the train data very well, the same model applied to the test data shows a lower correlation, implying that the model overfits the data. the following parameters xgb.XGBRegressor( n_estimators= 200, max_depth=9, eta= 0.1, colsample_bytree= 0.8, subsample= 0.8) show a correlation in the training set of roughly 0.8 but the same model applied to the testing set shows an r^2 of 0.6 Any suggestion in how to handle these parameters? One more question: when calculating the mean squared error, I get for both train and test data a result that is very close to zero, which I assume is good, but judging by the scatterplot between predicted and actual, plenty of values do not follow the straight line. Any idea why ?
