[site]: crossvalidated
[post_id]: 443582
[parent_id]: 126238
[tags]: 
The main reason why ReLu is used is because it is simple, fast, and empirically it seems to work well. Empirically, early papers observed that training a deep network with ReLu tended to converge much more quickly and reliably than training a deep network with sigmoid activation. In the early days, people were able to train deep networks with ReLu but training deep networks with sigmoid flat-out failed. There are many hypotheses that have attempted to explain why this could be. First, with a standard sigmoid activation, the gradient of the sigmoid is typically some fraction between 0 and 1; if you have many layers, these multiply, and might give an overall gradient that is exponentially small, so each step of gradient descent will make only a tiny change to the weights, leading to slow convergence (the vanishing gradient problem). In contrast, with ReLu activation, the gradient of the ReLu is either 0 or 1, so after many layers often the gradient will include the product of a bunch of 1's, and thus the overall gradient is not too small or not too large. But this story might be too simplistic, because it doesn't take into account the way that we multiply by the weights and add up internal activations. Second, with sigmoid activation, the gradient goes to zero if the input is very large or very small. When the gradient goes to zero, gradient descent tends to have very slow convergence. In contrast, with ReLu activation, the gradient goes to zero if the input is negative but not if the input is large, so it might have only "half" of the problems of sigmoid. But this seems a bit naive too as it is clear that negative values still give a zero gradient. Since then, we've accumulated more experience and more tricks that can be used to train neural networks. For instance, batch normalization is very helpful. When you add in those tricks, the comparison becomes less clear. It is possible to successfully train a deep network with either sigmoid or ReLu, if you apply the right set of tricks. I suspect that ultimately there are several reasons for widespread use of ReLu today: Historical accident: we discovered ReLu in the early days before we knew about those tricks, so in the early days ReLu was the only choice that worked, and everyone had to use it. And now that everyone uses it, it is a safe choice and people keep using it. Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple. Fragility: empirically, ReLu seems to be a bit more forgiving (in terms of the tricks needed to make the network train successfully), whereas sigmoid is more fiddly (to train a deep network, you need more tricks, and it's more fragile). Good enough: empirically, in many domains, other activation functions are no better than ReLu, or if they are better, are better by only a tiny amount. So, if ReLu is simple, fast, and about as good as anything else in most settings, it makes a reasonable default.
