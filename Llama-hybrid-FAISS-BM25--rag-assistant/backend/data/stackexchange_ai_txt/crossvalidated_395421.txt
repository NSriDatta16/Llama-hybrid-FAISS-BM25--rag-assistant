[site]: crossvalidated
[post_id]: 395421
[parent_id]: 395028
[tags]: 
As I mentioned in the comment, it is not "required." Without it, you would be left with regular importance sampling, which, depending on your problem, might be an acceptable strategy. Your weighted average estimator of the expectation of some function will probably be consistent, and there will probably be a central limit theorem that will help you approximate confidence intervals. This paper has a few results that will help you understand which situations one can use particle filters easily. It covers the case of when they are used for static estimation problems (e.g. estimating the posterior of a non time series model), as well as the case of Bayesian filtering for state space models (e.g. recursive formulas for $p(x_t \mid y_{1:t}$ where $x_t$ is a state at time $t$ , and $y_t$ is the observation). If you're estimating the filtering distribution $p(x_t \mid y_{1:t})$ , and you aren't resampling, you might look at the top of page 2395 for the relevant central limit theorem. Note that they call the time $t$ state $\theta_t$ , here. Also note that that asymptotic variance term $V_t^{\text{sis}}(\phi)$ depends not just on the dimension of your samples, but also on that weight ratio. This weight ratio is the tricky part in practice. Skipping forward a little, on page 2397, they compare two particle weights, each of which is that ratio. They show that the log of this ratio is approximately normally distributed (under certain assumptions of stationarity). That means the weight ratio itself, the exponentiated version of the log ratio, is asymptotically lognormally distributed, and in particular, "the ratio of weights of the two particles either converges or diverges exponentially fast." The author goes on to say More generally, when $H$ particles are generated initially, very few of them will have a prominent weight after some iterations, thus leading to very unreliable estimates, whether for smoothing or filtering the states. The algorithm suffers from the curse of dimensionality, in that its degeneracy grows exponentially with the dimension of the space of interest $\Theta_t$ . So, if you filter through time long enough without resampling, you'll basically end up with, effectively, one sample, even though, computationally, you're keeping track of many. Resampling can be helpful, particularly when you are filtering, but keep in mind that there are many problems it doesnâ€™t solve. For more details and cases, read on!
