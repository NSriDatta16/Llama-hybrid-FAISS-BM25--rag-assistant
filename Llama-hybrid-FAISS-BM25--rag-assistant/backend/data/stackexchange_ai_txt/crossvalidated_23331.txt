[site]: crossvalidated
[post_id]: 23331
[parent_id]: 
[tags]: 
Why is there an asymmetry between the training step and evaluation step?

It is well-known, especially in natural language processing, that machine learning should proceed in two steps, a training step and an evaluation step, and they should use different data. Why is this? Intuitively, this process helps avoid overfitting the data, but I fail to see a (information-theoretic) reason this is the case. Relatedly, I've seen some numbers thrown around for how much of a data set should be used for training and how much for evaluation, like 2/3 and 1/3 respectively. Is there any theoretical basis for choosing a particular distribution?
