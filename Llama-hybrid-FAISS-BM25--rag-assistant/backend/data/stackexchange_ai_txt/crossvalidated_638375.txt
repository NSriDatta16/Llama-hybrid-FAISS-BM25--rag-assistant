[site]: crossvalidated
[post_id]: 638375
[parent_id]: 
[tags]: 
Why do we work with factor of likelihoods instead of e.g. a sum for a batch in the negative log likelihood loss function?

In a classification task, at a certain stage of the training process, we get a likelihood of sampling proper class Y for a particular data point X. For batch, we get many independent likelihoods. Let's say we have likelihoods l1, l2, l3. l1 is a likelihood of sampling "fun" given a sequence "Data Science is", with current model params. l2 is a likelihood of sampling "cool" given a sequence "Machine Learning is", with current model params. l3... Then, we get a product of them, which is the likelihood of all the "events" together. l_batch = l1 * l2 * l3 Finally, we can apply the negative logarithm and get the "negative log-likelihood" loss function. -log(l_batch) Theoretically, we could drop the negative logarithm and optimize our joint likelihood (l_batch = l1 * l2 * l3) directly. It would be maximization instead of minimization, because of missing minus sign. It's perfectly differentiable and backpropagation would work. The only problem would be our likelihoods product could be extremely small and cause arithmetic problems or at least be inconvenient to read and work with. But we could use the sum of likelihoods instead of the product, and maximize it. It would be differentiable too. l1 + l2 + l3 The question is a) if I reason correctly b) why engineers/scientists don't use the sum of likelihoods instead of product as a loss function for neural network classification then? (we could include a minus sign, to turn it into a minimization problem and be consistent with other loss functions)
