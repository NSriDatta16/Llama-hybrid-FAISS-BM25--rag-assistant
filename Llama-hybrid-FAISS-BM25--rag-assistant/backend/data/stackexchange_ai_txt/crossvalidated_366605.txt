[site]: crossvalidated
[post_id]: 366605
[parent_id]: 366587
[tags]: 
Don't worry about it. ML methods tend to be robust against that kind of thing in the training set - roughly speaking. They will cancel each other out, but perhaps not exactly. If the occur in the testing set they place an upper bound on your accuracy, since you can't guess both. But that is fine. It may be good to report a theoretical upper bound on the accuracy in these cases (though I've rarely seen it done.). If regressing to class probability (like in logistic regression), you can do well at minimize the loss function (e.g the cross entropy between your estimated distribution, and that observed in the test set). And thus get a useful probability estimate as your output. A great many problem are such that your input data does not fully capture enough information to predict the output. I've worked on problems where there dozens or hundreds of different label classes for identical features. Multiple different output values for same input values is ubiquitous when talking about using ML for regression.
