[site]: datascience
[post_id]: 54738
[parent_id]: 
[tags]: 
Having problem in back propagation part for dimension

I was trying to build a neural network with single hidden layer from scratch. In back propagation part some problems have raised. For calculating gradient of loss function with respect to weight in layer-1 the equation becomes: $$ \frac{dL}{dW1} = \frac{dL}{dA1}*\frac{dA1}{dZ1}*\frac{dZ1}{dW1}$$ where, $$ \frac{dL}{dA1} = \frac{dL}{dA2}*\frac{dA2}{dZ2}*\frac{dZ2}{dA1}$$ Should I calculate it with np.dot() or np.multiply() ? I was trying to do it with np.dot() and having problem with dimension. dL/dW1 dimenion doesn't fit when I go to update W1. Here is my code and pardon me as it is little bit messy in back propagation part. https://gist.github.com/ipritom/30fcad0c74ab59e5b31e1daac1c1d1e7
