[site]: crossvalidated
[post_id]: 635655
[parent_id]: 635642
[tags]: 
Many excellent answers already. I would add a few more aspects. You do not say what you mean by "outperform", as in "tree models often outperform linear models". You presumably mean something like "tree models yield higher out-of-sample prediction accuracy". However: The other answers allude to an issue that should be considered explicitly: accuracy is not the only performance dimension . There are at the very least aspects like: Interpretability - where linear models give you parameter estimates you can interpret Debuggability - which is easier for linear models than for more complex ones, related to interpretability as above - your users may ask you to improve "bad" predictions in specific cases, and it's much easier to do so for linear models Robustness - which may or may not be better for trees than for linear models, especially if you regularize - you may prefer lower accuracy on average as long as you get fewer "really bad" predictions Data requirements - as others noted, Kaggle usually has large datasets, but a real use case may have much less clean data, and simple methods may be surprisingly hard to beat especially in low data situations Other resource requirements - one key aspect here being expertise ; a complex model requiring a lot of different data may require a data engineer and a highly qualified data scientist to get to work (but other requirements may be computing resources or something as prosaic as electricity; Petropoulos et al., 2023 ) Business value - everybody likes higher accuracy, but higher accuracy does not necessarily directly translate into higher business value (e.g., Kolassa, 2023, Foresight ), and this needs to be compared to the higher resource costs per above Data scientists naturally have a predilection for higher accuracy, and rightly so. But we do not work in a vacuum, and our models always need to be justified in the larger context, which includes other users and budgets.
