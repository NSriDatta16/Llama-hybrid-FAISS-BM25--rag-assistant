[site]: crossvalidated
[post_id]: 522258
[parent_id]: 
[tags]: 
A2C Agent does not learn anything in a simple 2D grid world

I am trying to implement a simple Advantage Actor Critic algorithm on a simple self-defined environment. My complete code is shown below. It is a 10Ã—10 grid world environment where there is one end state with +100 reward, and other end states with 0 reward. The agent can take a maximum of 15 steps with the actions (up, down, left and right). If the agent runs out on one edge of the field, he comes out on the opposite side. For each step on the field he gets +1 reward. The code for my Policy Value Network and the training code is shown in full below. I would expect the agent to learn over time where the +100 field is and steer specifically there so that the average reward of a trajectory converges to 100. I have shown one run as a result below. As you can see, the Episode Reward hardly/not at all increases, but stagnates at about 15 (which is the maximum number of steps on the field, and he gets +1 reward for each step). Every now and then he hits the +100 field, but that seems more random instead of him learning over time where the field is. I would be very grateful for any hints as to why my agent is not learning. 2d-world.py import torch import pandas as pd import numpy as np import torch.nn as nn from torch.distributions.categorical import Categorical from model import PolicyValueNetwork from torch.utils.tensorboard import SummaryWriter from sklearn.preprocessing import StandardScaler scaler = StandardScaler() writer = SummaryWriter() lr = 0.01 gamma = 0.99 episodes = 1000000 normalize_targets = True n_steps = 15 class Environment(): def __init__(self): field = { "0": [0, 1, 1, 1, 0, 1, 1, 1, 1, 1], "1": [1, 0, 1, 1, 1, 1, 1, 1, 0, 1], "2": [1, 1, 1, 0, 1, 1, 0, 1, 1, 1], "3": [1, 1, 1, 1, 0, 1, 1, 1, 0, 0], "4": [1, 0, 1, 1, 1, 1, 1, 1, 1, 1], "5": [1, 1, 1, 0, 1, 1, 1, 0, 1, 1], "6": [1, 0, 1, 1, 0, 1, 0, 1, 1, 0], "7": [1, 1, 1, 0, 1, 1, 1, 1, 1, 1], "8": [1, 1, 0, 1, 1, 0, 1, 100, 1, 1], "9": [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]} env = pd.DataFrame(data=field).T self.rewards = env.values n_rows = self.rewards.shape[0] n_cols = self.rewards.shape[1] self.start_points = [] for row in range(0, n_rows): for col in range(0, n_cols): if self.rewards[row][col] == 1: self.start_points.append((col, row)) # (x, y) self.s = self.reset() def step(self, action): # action in [0, 1, 2, 3] means ["up", "down", "left", "right"] # First we calculate the possible new positions on the field x = self.s[0] y = self.s[1] nx = [x, x, 9 if x == 0 else x - 1, 0 if x == 9 else x + 1] # possible next x values ny = [9 if y == 0 else y - 1, 0 if y == 9 else y + 1, y, y] # possible next y values # Get next state and reward ns = (nx[action], ny[action]) self.s = ns reward = self.rewards[ns[1]][ns[0]] # indexing is done like [y][x] # Calculate done signal done = reward == 100 or reward == 0 return ns, reward, done def reset(self): i = np.random.choice(list(range(0, len(self.start_points)))) self.s = self.start_points[i] return self.s # Get cuda device device = torch.device(0) # Instantiate model model = PolicyValueNetwork() model.to(device) # Optimizer optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer = torch.optim.RMSprop(model.parameters()) # Loss function for value loss loss_fn = nn.MSELoss(reduction="sum") # Set environment env = Environment() for episode in range(0, episodes): state = env.reset() # Set up arrays for this episode values = [] log_probs = [] rewards = [] states = [] for i in range(0, n_steps): states.append(state) # Create tensor for current state state_tensor = torch.tensor(data=state).float().to(device) # Get policy and value from model probs, value = model(state_tensor) values.append(value) categorical = Categorical(probs=probs) # Sample an action a = categorical.sample() log_prob = categorical.log_prob(a) log_probs.append(log_prob) # Execute step in the environment state, reward, done = env.step(a.item()) rewards.append(reward) if done: break # Calculate discounted returns state_tensor = torch.tensor(data=state).float().to(device) _, value = model(state_tensor) R = 0.0 if done else value.item() T = len(values) targets = np.empty(T, dtype=np.float32) for i in reversed(range(T)): targets[i] = R = rewards[i] + gamma * R # Normalize discounted returns alias targets if normalize_targets: targets = scaler.fit_transform(targets.reshape(-1,1)) targets = torch.tensor(data=targets).to(device).squeeze() # Calculate advantages values = torch.stack(values).squeeze() advantages = targets - values # Calculate policy loss log_probs = torch.stack(log_probs).squeeze() policy_loss = torch.sum(-advantages.detach() * log_probs) # Value loss value_loss = loss_fn(values, targets) # Backpropagation optimizer.zero_grad() policy_loss.backward(retain_graph=True) value_loss.backward() optimizer.step() # Logging with tensorboard writer.add_scalar("Episode Reward", sum(rewards), episode) writer.add_scalar("Episode Policy Loss", policy_loss.item(), episode) writer.add_scalar("Episode Value Loss", value_loss.item(), episode) model.py import torch.nn as nn # Neural network class PolicyValueNetwork(nn.Module): def __init__(self): super(PolicyValueNetwork, self).__init__() self.lin1 = nn.Linear(in_features=2, out_features=128, bias=True) self.policy = nn.Linear(in_features=128, out_features=4, bias=True) self.value = nn.Linear(in_features=128, out_features=1, bias=True) self.relu = nn.ReLU() self.softmax = nn.Softmax(dim=0) def forward(self, inp): out1 = self.lin1(inp) relu1 = self.relu(out1) policy = self.policy(relu1) value = self.value(relu1) probs = self.softmax(policy) return probs, value Results (Returns for each episode, about 45k episodes)
