[site]: crossvalidated
[post_id]: 179166
[parent_id]: 179130
[tags]: 
Unfortunately you cannot assume that as your sample size $n \rightarrow \infty$ your prediction error will tend to zero. Your prediction error $(\hat{y} -y)$ will never tend to zero. This is true in any statistical model that uses data that might be noisy irrespective of the dataset's sample size. Your only change is that the data are truly noiseless and therefore you will correctly infer that the noise variability is of zero magnitude and then your prediction error will also become zero (assuming your model models the true underlying dynamics). What you can expect is, that with larger sample size, your (hyper)-parameters $\hat{\theta}$ will become closer and closer your true $\theta$. This is true for any consistent estimation procedure. As a standard reference on the subject I would suggest you look at Chapter 7 from the Gaussian Process for Machine Learning by Rasmussen and Williams. Section 7.2 on the asymptotic analysis of GPs is most probably what you need;the section's extensive reference list will point you to specific papers if you need to dig a particular matter further.
