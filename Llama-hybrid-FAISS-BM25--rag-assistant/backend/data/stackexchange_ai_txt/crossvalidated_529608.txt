[site]: crossvalidated
[post_id]: 529608
[parent_id]: 529597
[tags]: 
As a general rule, fitting data that has an underlying/true model with that same model will usually produce the best results. More complex models may require more data to avoid overfitting, but that's not usually an issue with OLS vs. other methods when OLS describes the "true" model. I think there should be clarification on what you are trying to ask, since the statements/assumptions in your OP aren't strictly true , although fitting a numeric response variable to OLS as a first model isn't necessarily a bad choice. If your response variable is just 1s and 0s, an OLS model is almost always a bad choice. The OLS model is a way of fitting points to a line using a linear combination of X variables plus an estimate of normal error . A linear model fits points to a line, although it might be transformed. In this case, a logistic regression with a logit/probit response function is considered a generalized linear model . Alternatively, you could have a model that looks like $y_i = \beta_0 + \beta_1 * x_1 + Laplace(0, b)$ where the error is described by the Laplacian distribution / double exponential function , or $e^{-\left|{\frac{(y_i - \hat{y}_i)}{b}}\right|}$ In this case OLS would not be a good fit, despite it being a linear model, since the solution to this type of equation would require minimizing the sum of the absolute values of the errors, not the sum of the squares of the errors. In most cases, though, you can get a decent idea of whether or not OLS is a good choice of models based on the shape of the distribution of the error residuals on a normal-normal plot. e.g., compare the plots of normal error (left) with Laplacian error (right) below:
