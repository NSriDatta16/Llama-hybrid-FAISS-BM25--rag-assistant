[site]: crossvalidated
[post_id]: 523736
[parent_id]: 508574
[tags]: 
I finally found the answer in this paper https://ieeexplore.ieee.org/document/374138 , also explained and referenced in a blogpost . It mentions clearly in the paper that NLL performs better then MSE because the loss function becomes: $$ NLL = \sum_i \frac{1}{2}\log(\sigma^2(x_i)) + \frac{(\mu(x_i) - y_i)^2}{2\sigma^2(x_i)} $$ Now if i assume $\sigma$ to be constant then my loss function becomes equivalent to MSE * const. which was the basis of my original comment ... nll loss with Gaussian priors are same as MSE ... However in current network $\sigma$ is a variable, and hence the network gives higher weight to datapoints with lower variance. Resulting in improved learning in current case. But the paper mentioned that if datasets are not large enough then NLL results in over fitting, which can again be explained on the same basis. I am including the relevant part of the paper below: Nix, D.A. and Weigend, A.S., 1994, June. Estimating the mean and variance of the target probability distribution. In Proceedings of 1994 ieee international conference on neural networks (ICNN'94) (Vol. 1, pp. 55-60). IEEE.
