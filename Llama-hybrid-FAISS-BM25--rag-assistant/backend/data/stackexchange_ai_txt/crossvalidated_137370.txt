[site]: crossvalidated
[post_id]: 137370
[parent_id]: 
[tags]: 
Clustering methods for unknown number of clusters

Matrix $X=[x_1,...,x_i,...,x_N]$ is a data-set containing $N$ data-points that each data-point $x_i$ is a vector of $D$ dimensions. Each dimension is a feature. The number of clusters ($K$) is unknown. There is no training data so all of the data-points are unlabeled. It is assumed that each cluster has Gaussian distribution with parameters mean and sigma: [ m , sigma ] which m =$[m_1,...,m_D]$. There is no information about the parameters (mean and sigma) of each cluster. The feature space of a cluster is modeled as a multi-variable Gaussian ($D$ dimension) and the total feature space is a Gaussian mixture model for unknown number of mixture components ($K$). I studied a model-based clustering method that has utilized for such a problem. It is nonparametric Bayesian classification (infinite mixture model). Since the number of mixture components is unknown, the nonparametric prior based on Dirichlet process (DP) and the Chinese restaurant process (CRP) for sampling from a DP and the collapsed Gibbs sampling for DP mixture model has used, reference 1 . which other clustering methods (unsupervised classification) can I try for this problem? In DPMM (Dirichlet process mixture model), it is assumed that each mixture component is Gaussian. Can non-Gaussian distribution be used for mixture components? In collapsed Gibbs sampling, the number of iterations for the algorithm convergence is assumed fixed. Is it possible the number of iterations is adaptive depend on the data and the number of components? I asked question 1 generally. I know there are many solutions for one problem. But I seek what methods are there that they are comparable to DPMM? Question 2 and 3 are in detail about DPMM. I just studied about Gibbs sampling and collapsed Gibbs sampling. I want to know about other methods. On Identifying Primary User Emulation Attacks in Cognitive Radio Systems Using Nonparametric Bayesian Classification
