[site]: crossvalidated
[post_id]: 432305
[parent_id]: 87182
[tags]: 
I don't think it is possible to give you a universal "intuitive" answer. I'll give you answer that is intuitive for some people, such as physicists. Logarithm is there to get the average energy of the system. Here's details. Shannon used a word " entropy " because he adapted the concept from statistical mechanics . In statistical mechanics there's a seminal distribution named after Boltzmann. Interestingly, it's an important distribution now in machine learning! The Boltzmann distribution can be written as $$P=e^{\frac{a-E} b}$$ where $a, b$ are constants, and $E$ is the energy of the system in a state $dV$ of the state space $V$ . In classical thermodynamics $dV=dpdx$ , where $x,p$ are a coordinate and momentum of the particle. It's a proper probability function when constants $a,b$ are selected properly, i.e. $\int_VPdV=1$ . Also, you may find it interesting that $b$ corresponds to a temperature of the system. Now, notice how $\ln P\sim E$ , i.e. a log of probability is linear (proportional) to energy. Now, you can see that the following expression is essentially an expected value of energy of the system: $$S\equiv -\int_VP\ln P dV= $$ This is what Gibbs did. So, Shannon took this thing and discretized as $$\eta=-\sum_i P_i\ln P_i$$ and called it "entropy," and we call this "Shannon entropy." There's no more energy concept here, but maybe you could anti-log the probability of a state $e^{-P_i}$ and call this an energy of the state? Is this intuitive enough for you? It is for me, but I was a theoretical physicist in past life. Also, you can go to a deeper level of intuition by linking to even older thermodynamics concepts such as temperature and works of Boltzmann and Clausius.
