[site]: crossvalidated
[post_id]: 521700
[parent_id]: 521373
[tags]: 
I don’t exactly have an answer for you. I have done things like you are trying to do as a demonstration of the properties of statistics, but not for any type of validation. It makes me wonder what you are doing and why. Let me begin with an observation as well. You are proposing using an uninformative prior distribution for this project. The first difficulty that may be created is that your posterior may not sum to unity. It may not end up being a probability distribution. That is only assured with proper priors. Improper priors are not guaranteed to result in proper probabilities. The second is that I am not sure what you are going to gain by doing that. Null hypothesis methods provide a guarantee to the performance of their estimators and do not include prior information. Bayesian methods have no performance guarantees. If you were willing to use the MAP estimator, why not the MLE? At least with the MLE, you will get guarantees on your inference, and you do not have to worry about the model exploding due to a failure to integrate to unity. With that said, if your posterior does integrate to unity or you use a proper prior, then your estimators cannot be first-order stochastically dominated by another estimator. That is to say, any other estimator will be as risky or riskier than the Bayesian one. On average, you will be as close or closer with a Bayesian estimator. With all of that said, if you are creating a random set of data to check performance, then you should look at two things. The first one is how do the estimators perform when the sample is reasonably representative. The second one is how do the estimators perform when the sample statistics are far away from the population parameters? Unbiased estimators minimize the maximum amount of risk you will be exposed to should you get an unrepresentative sample, while Bayesian estimators minimize your average loss. There is any number of potential measures against a known parameter. You could find the distribution of the MAP estimator or the posterior mean around the true value. You could find the posterior variance around the parameter set. Each of those scales well. I don’t think either 1 or 2 will do what you want. I would take the points around the parameters from my MCMC set and then calculate what I needed, depending on my goal. As long as your set is large enough and converges, it should do what you want. You should strongly consider using proper a proper prior distribution with real information in it, or consider using a non-Bayesian method.
