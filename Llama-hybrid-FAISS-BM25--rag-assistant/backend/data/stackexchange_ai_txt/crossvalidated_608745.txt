[site]: crossvalidated
[post_id]: 608745
[parent_id]: 
[tags]: 
Prediction interval of the future median based on a series of future samples

I am using R and have based my analysis on the book by Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts using the fpp3 package ( https://otexts.com/fpp3/ ) The aim is to estimate number of averted cases after the implementation of an intervention. I have fitted an Arima model to the following time series of yearly incidence data which after log-transformation is stationary based on KPSS test (function unitroot_kpss). Using the ARIMA function from the same package for automatically deciding on the best model based on AICc (using also stepwise=FALSE for a more extensive search) i get the model Arima(5,0,0) as the most fitting. A residual check shows that there is no residual autocorrelation left, the Ljung-Box test also verifies that the residuals are white noise. As it can be seen in the graph, the prediction intervals of the median yearly incidence are simply too wide making it pointless to estimate uncertainty around the predicted averted cases. The intervals are of course the same also for the mean. What i have considered and would like a second opinion is to estimate the median incidence for the whole post-intervention period, instead by year, and use the prediction interval around that to estimate uncertainty around the averted cases. In this way we do lose the yearly predictions but i can't think of an alternative when they are as non-informative as in this case. I have done this by simulating 10000 future paths from the fitted Arima(5,0,0) model using bootstrap residuals, estimating the median incidence for whole the post-intervention period for each of them and finally the median of all medians in the following way: futures generate(times = 10000, h = 49, set.seed(123),bootstrap = TRUE, point_forecast = list(.median = median)) |> as_tibble() |> group_by(.rep) |> summarise(.sim = median(.sim)) |> summarise(total = distributional::dist_sample(list(.sim))) future_median mutate( median = median(total), pi80 = hilo(total, 80), pi95 = hilo(total, 95) ) Inspiration for this came from the section "Prediction intervals from bootstrapped residuals" in ( https://otexts.com/fpp3/prediction-intervals.html ) and ( https://otexts.com/fpp3/aggregates.html ) where the setup in that example was to aggregate monthly data to get yearly estimates. My questions are: Is this approach statistically correct? If so, is it more correct to estimate the median of medians (median = median(total) in the code above) or the mean of medians instead? Is it simply a matter of choice? I haven't worked a lot with arima models and the order of the chosen model (5th order autoregressive model) was a bit surprising. I am more used to seeing lower order models. Is there something to be considered there? Models selected by the ARIMA function will not contain roots outside or close to the unit circle so that part is already taken care of. Finally, how well are Arima models suited for long forecasting as in this case. Should other models be preferred instead?
