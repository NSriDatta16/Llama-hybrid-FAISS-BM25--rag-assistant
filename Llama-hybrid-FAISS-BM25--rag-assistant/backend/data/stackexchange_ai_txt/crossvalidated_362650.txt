[site]: crossvalidated
[post_id]: 362650
[parent_id]: 
[tags]: 
Sklearn: MaxAbsScaler mandatory when your dataframe has a lot of dummy variables?

I have a dataset with the following variables: price (numerical, min 350, max 25400) model_age_days (numerical, min 423, max 3405) factor_1 (dummy) factor_2 (dummy) factor_3 (dummy) (...) factor_58 (dummy) So the model has 60 columns, 58 age of which are One Hot Encoded dummy variables (created using pandas its get_dummies feature). So for each row, most columns will have a value of 0 I am training a classifier to predict the factor (but will likely perform other types of analysis later) , using a 'One vs the Rest' approach for each factor and am now trying to decide how to preprocess the data. My question is as follows: Am I correct in assuming that this dataset could be considered sparse and that, following the Sklearn documentation the MaxAbsScaler is thus almost mandatory? MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this. I'm asking this since I would like to create a 2d graph (using PCA) later, but the tutorials / documentation seem to suggest that the StandardScaler is preferred for PCA.
