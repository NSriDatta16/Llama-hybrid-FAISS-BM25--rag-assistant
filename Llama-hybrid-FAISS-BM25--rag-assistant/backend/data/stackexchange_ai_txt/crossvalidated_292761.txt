[site]: crossvalidated
[post_id]: 292761
[parent_id]: 292729
[tags]: 
There's a problem with your arithmetic. You can either exponentiate the intercept to get the odds, exponentiate the coefficient for antagonism to get the odds ratio, and then multiply them to get the new odds (which you could convert into a probability). Or, you just solve for the linear predictor, and then convert that from a log odds to a probability. Either way, it gives you the same probability ( .33 ). cofs = c(-1.6609981, 0.9629119, -0.3141261, -0.0967504) plogis(cofs[1]) # [1] 0.1596281 odds.antag1 = exp(cofs[1])*exp(cofs[2]); odds.antag1 odds.antag1/(1+odds.antag1) # [1] 0.3322367 plogis(cofs[1] + cofs[2]*1 + cofs[3]*0 + cofs[4]*1*0) # [1] 0.3322367 As far as how to communicate this to laypeople, I mostly don't try in any direct sense. Clients usually want to know if X is associated with Y. The test of the relevant coefficient provides that information. If they want to see how this plays out, you would do best to plot. It's worth remembering that on the probability scale, all variables are in essence always interacting because of the nonlinear transformation at the heart of logistic regression. It may help to read some of my answers that relate to these issues: Interpretation of simple predictions to odds ratios in logistic regression Difference between logit and probit models Interaction in generalized linear model Interpretation of multiple logistic regression with interactions in R How to visualize a fitted multiple regression model? Graphing a Probability Curve for a Logit Model With Multiple Predictors
