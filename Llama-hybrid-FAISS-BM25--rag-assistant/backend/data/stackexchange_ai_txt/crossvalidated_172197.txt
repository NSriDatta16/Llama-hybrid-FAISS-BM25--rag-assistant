[site]: crossvalidated
[post_id]: 172197
[parent_id]: 172191
[tags]: 
As far as I'm concerned it's not valid since it deals with hypotheses and allows only one try. Think of it this way: You have a data set and you guess the hypothesis. You have only one try. If you have enough data and you want a guess as accurate as $\epsilon$ then your chance can be computed by the right hand side. If it does not satisfy you, solve for $n$ and you know how much data you'll need. This will be a huge number since the equation covers the worst case. It's also obvious that you still have a chance for a bad event. However if you want to generalize with machine learning you need to pick a lot of hypotheses since ML uses iterations to nudge the parameters in a certain way to achieve an lower in sample Error $E_{in}$ in hope that it will represent the never known out of sample error $E_{out}$. Since you guessing in each iteration a new hypothesis, your worst case probability for a bad event is the union bound. With this rigid assumption you won't be able to learn at all. To be able to learn you need to switch from hypotheses to dichotomies. This accounts for bad overlapping events. Unfortunately this results in a lot of proofs which need to be done. You need to know about vc dimensions, break points and make sure that your growth function is polynomial or at least bound by a polynomial. If you did the theoretical part and made a proof for every thing you'll get the VC Inequality , which looks very similar to Hoeffdings equation: $P\left(\sup_{f \in \mathcal{F}} \left |\hat{R}_n(f) - R(f) \right | >\varepsilon \right) \leq 8 S(\mathcal{F},n) e^{-n\varepsilon^2/32}$ In general it says that the probability that you can generalize, so that the difference between $E_{in}$ and $E_{out}$ is smaller than $\epsilon$ gets higher if you have enough data to learn from. The VC proof is elegant but nothing really new. Think about arithmetic mean of variables. If you have 9 and 11 the mean is 10 but it's not reliable since you just have two values. If your third measurement is 15, you're doomed. Now if you have a much bigger set, say of 100 or 1000 values your mean value is much more 'secure', reliable or representative, however you want to call it, and is less dependent from individual measurements and thus generalizes the data better. The right part basically says that for each parameter you want to learn, generalize or fit you need 'approximately' (rule of thumb) more than 10 observations. There is a good online course from Caltech about machine learning which covers the entire theory.
