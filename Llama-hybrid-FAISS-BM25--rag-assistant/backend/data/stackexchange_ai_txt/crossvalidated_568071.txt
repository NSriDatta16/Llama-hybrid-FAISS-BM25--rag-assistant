[site]: crossvalidated
[post_id]: 568071
[parent_id]: 
[tags]: 
How to handle problem of different random seeds giving drastically different test scores in machine learning model?

For a rigorous empirical analysis, I am training a model with three different seeds - 0, 1 and 2. In each case, I found that the model obtained through early stopping (lowest validation loss) had an F1 score on the validation set of a similar value (indicating convergence). However, for these models, the F1 score on the test set was highly variable. e.g. On Validation Set, the F1 scores are: $79.6400449943757, 79.41507311586052, 78.74015748031496$ (very similar). But on Test Set, the F1 scores are: $60.115606936416185, 76.30057803468208, 66.76300578034682$ (highly variable) I've tried multiple learning rates as well, but am observing a similar problem. Can anyone help me understand why there might be such high variance in test set scores? Note: I am fine-tuning a T5 transformer model with sentiment analysis data. My loss function is a cross-entropy loss function and my final evaluation metric is F1-score.
