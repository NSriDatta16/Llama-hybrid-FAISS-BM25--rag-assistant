[site]: datascience
[post_id]: 16246
[parent_id]: 
[tags]: 
Explain to a layperson why text-to-speech (TTS) has made little progress?

I am a layperson who has searched for years in vain for a human-sounding TTS program, but it seems that over the years, hardly any progress has been made (a purely unscientific evaluation using just my ear and also the ears of many others). It still sounds like the same robotic word-by-word reading-out that I believe was already accomplished decades ago. This is puzzling (for a layperson like me who doesn't know anything about data science, linguistics, machine learning, etc.). Especially when contrasted to say speech-to-text or driverless cars, which have noticeably improved over the years. Could you explain to a layperson (like myself) why, in early 2017, do most text-to-speech (TTS) programs still sound so robotic and non-human-like? And why so little progress has been made (especially in comparison to some other achievements)? Might it just be that there simply isn't much demand for TTS (as compared to speech-to-text or driverless cars) and so few resources have been devoted to this? (If this isn't the right StackExchange for the question, I apologize in advance. Please migrate this to a more suitable StackExchange site instead, thanks!)
