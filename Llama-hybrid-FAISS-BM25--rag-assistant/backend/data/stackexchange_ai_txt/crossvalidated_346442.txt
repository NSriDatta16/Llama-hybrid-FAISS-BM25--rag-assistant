[site]: crossvalidated
[post_id]: 346442
[parent_id]: 
[tags]: 
Normalization: different approach

Today I had a discussion about the right way to normalize data, especially image data. The standard approach, as found in many tutorials etc., seems to be following. For example, the data has a range of integer values from $k = 0, ..., 255$ (e.g. uint8 value for each channel or grayscale in image encoding) with $N=256$ values. We form the scaled values: $$x = \frac{k}{N-1} \quad \quad \rightarrow \quad \quad x = 0, \tfrac{1}{N-1}, ..., \tfrac{N-2}{N-1}, 1.$$ These value are on a scale between zero and one. After processing the $x$ values by a neural network for example, the processed values $\hat{x}$ get transformed back to $\hat{k} = \hat{x} \cdot (N-1)$ and then rounded to the nearest integer . This gives us processed values for the $k$ values. Problem with this procedure: I believe that this procedure leads to a systematic statistical distortion. For example, when $N=4$, the scaled values are $x = 0, \tfrac{1}{3}, \tfrac{2}{3}, 1$ for $k = 0, 1, 2, 3$. We process these scaled values to some processed values $\hat{x}$ and then we convert these back to processed values of $k$ via rounding, so that: $$\hat{k} = \hat{k}(\hat{x}) = \begin{cases} 0 & & \text{for }\hat{x} \in [0, \tfrac{1}{6}), \\ 1 & & \text{for }\hat{x} \in [\tfrac{1}{6}, \tfrac{3}{6}), \\ 2 & & \text{for }\hat{x} \in [\tfrac{3}{6}, \tfrac{5}{6}), \\ 3 & & \text{for }\hat{x} \in [\tfrac{5}{6}, 1]. \\ \end{cases}$$ Now, let's assume, that the processed values $\hat{x}$ are uniformly distributed. Then the probabilities of the corresponding values of $\hat{k}$ do not reflect the originally uniformly distributed x values. My proposed solution: Because of this problem, my proposal is to scale the values as: $$x = \frac{k + \tfrac{1}{2}}{N} \quad \quad \rightarrow \quad \quad x = \tfrac{1}{2N}, \tfrac{3}{2N}, ..., \tfrac{2N-3}{2N}, \tfrac{2N-1}{2N}.$$ By this method, the scaled values get placed in the midpoints of $N$ equal intervals, and we take $\hat{k} = \hat{x} \cdot N - \tfrac{1}{2}$ and then round to the nearest integer to get back the processed values of $k$. (Since $1 \cdot N - \tfrac{1}{2}$ gets rounded up to $N$, which is out of the range of allowable values, we process this case separately.) The transformation back to integer values of $\hat{k}$ then leads to equal probabilities for all values in the example above. My question: In my opinion, this seems to be much more consistent in a statistal sense. So my question is, what do you think about it, and why is the first approach (dividing by $N-1$) so common?
