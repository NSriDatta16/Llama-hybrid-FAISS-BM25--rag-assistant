[site]: crossvalidated
[post_id]: 258453
[parent_id]: 258447
[tags]: 
To give a naive assessment of the situation: generally: suppose you have two different system of basis functions $\{p_n\}_{n=1}^\infty$, as well as $\{\tilde{p}\}_{n=1}^\infty$ for some function (hilbert-) space, usual $L_2([a,b])$, i.e. the space of all square-integrable functions. This means that each of the two bases can be used to explain each element of $L_2([a,b])$, i.e. for $y \in L_2([a,b])$ you have for some coefficients $\theta_n$ and $\tilde{\theta}_n \in \mathbb{R}$, $n=1,2,\dots$ (in the $L_2$-sense): $$ \sum_{n=1}^\infty \tilde{\theta}_n \tilde{p}_n = y= \sum_{n=1}^\infty \theta_n p_n.$$ However, on the other hand, if you truncate both sets of basis functions at some number $k However, here in the special case where one basis, $\{\tilde{p}\}_{n=1}^\infty$, is just an orthogonalization of the other basis, $\{p_n\}_{n=1}^\infty$, the overall prediction of $y$ will be the same for each truncated model ($\{p\}_{n=1}^k$ and their orthogonalized counterpart will describe the same $k$-dimensional subspace of $L_2([a,b])$). But each individual basis function from the two "different" bases will yield a different contribution to this predcition (obviously as the functions/predictors are different!) resulting in different $p$-values and coefficients. Hence, in terms of prediction there is (in this case) no difference. From a computational point of view a model matrix consisting of orthogonal basis functions have nice numerical/computational properties for the least squares estimator. While at the same time from the statistical point of view, the orthogonalization results in uncorrelated estimates, since $var(\hat{\tilde{\theta}}) = I \sigmaÂ²$ under the standard assumptions. The natural question arises if there is a best truncated basis system. However the answer to the question is neither simple nor unique and depends for example on the definition of the word "best", i.e. what you are trying to archive.
