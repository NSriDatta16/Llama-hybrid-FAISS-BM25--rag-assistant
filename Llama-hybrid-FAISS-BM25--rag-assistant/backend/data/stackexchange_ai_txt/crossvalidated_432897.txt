[site]: crossvalidated
[post_id]: 432897
[parent_id]: 432896
[tags]: 
In most cases CNNs use a cross-entropy loss on the one-hot encoded output. For a single image the cross entropy loss looks like this: $$ - \sum_{c=1}^M{(y_c \cdot \log{\hat y_c})} $$ where $M$ is the number of classes (i.e. $1000$ in ImageNet) and $\hat y_c$ is the model's prediction for that class (i.e. the output of the softmax for class $c$ ). Due to the fact that the labels are one-hot encoded and $y$ is a $(1000 \times 1)$ vector of ones and zeroes, $y_c$ is either $1$ or $0$ . Thus, out of the whole sum only one term will actually be added: the one with $y_c=1$ .
