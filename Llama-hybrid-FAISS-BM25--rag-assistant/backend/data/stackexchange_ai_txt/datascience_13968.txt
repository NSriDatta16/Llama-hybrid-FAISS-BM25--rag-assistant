[site]: datascience
[post_id]: 13968
[parent_id]: 
[tags]: 
Do categorical features always need to be encoded?

I'm using Spark's Machine Learning Library, and features are categorical. The features are strings, and Spark's MLlib (like many other machine learning libraries) does not accept Strings as inputs. The normal procedure for overcoming this is to convert Strings to integers, and then encode these integers (using a onehotencoder for example), because converting to integers implies that there is an ordering between features. My question is - do categorical features always need to be encoded? In what situation could integers be used instead of encoding? I'm using Logistic Regression and Naive Bayes. When using integers as features I get an 84% accuracy, when these integers are encoded I get an 82% accuracy. Is it necessary to encode?
