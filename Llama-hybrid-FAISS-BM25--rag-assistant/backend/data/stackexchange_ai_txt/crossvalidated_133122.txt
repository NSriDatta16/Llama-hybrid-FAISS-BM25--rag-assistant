[site]: crossvalidated
[post_id]: 133122
[parent_id]: 133046
[tags]: 
I would be wary of that approach. If all of the IVs are orthogonal to one another (that is, they are independent of one another), then there'd be no difference between 10 univariate tests and 1 multiple regression test, except fewer residual df. However, in practice, I'd bet that among 10 IVs, at least 2 or 3 of them are correlated with one another. Multiple regression tells you what the influence of one IV is holding all others constant. If IVs are correlated, then this yields better inference about the particular effects of each IV. The first thing I would do is check for collinearity. Maybe some IVs are highly correlated with one another and therefore redundant. You can then exclude redundant IVs. Another thing I would try would be some kind of factor analysis on the IVs, usually PCA. You might be able to use PCA on the IVs to reduce the 10 IVs into 3-4 IVs, depending on how correlated the IVs are. This would take advantage of your full set of IVs and allow you to use multiple regression at the same time. Another approach would be to formulate specific a-priori hypotheses given particular combinations of IVs. You can then use model selection (AIC, AICc) to identify which sets of IVs are important and then examine parameter estimates and SEs from there. See Burnham and Anderson (2002) - Model selection and multimodel inference for more details. Either way, you should find an approach that allows you to carefully make use of all IVs, rather than some kind of stepwise approach. There are too many issues with data dredging and multiple comparisons. You might be missing something in the univariate tests (such as one variable unimportant on its own, but important in the presence of others).
