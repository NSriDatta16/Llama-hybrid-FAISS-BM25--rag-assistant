[site]: crossvalidated
[post_id]: 296723
[parent_id]: 
[tags]: 
How to best report code benchmarking results

Let's suppose that we run a experiment consistent on the time it takes to execute a fixed piece of code. We know that the environment in which this code will be executed (the machine) will fluctuate due to a high number of different factors that we will assume are impossible to directly control. So we define the result of each experiment as the average of $n$ repetitions of the code execution. To gather statistical data, we perform $r$ different experiments so we end with a collection of $r$ data points. The question is: Which is the best way to report the result of this experiments so we can extract the most information out of the report? Notice that a possible approach like "give the mean and the standard deviation" has the downside of the fact that the distribution of errors has always negative skewness due to the fact that perturbations can only add to the final execution times and therefore $min(r_i)$ is the best measure on how much time the "code instructions" will take to execute in the abscence of perturbations. As a second question: What is the best number of repetitions ($r$) to have a statistically significat report for a given experiment? This question was based on this discussion about how to propperly report microbenchmarking results in the CPython interpreter.
