[site]: crossvalidated
[post_id]: 460649
[parent_id]: 
[tags]: 
Fitting glmms to data with very unbalanced sampling effort - could subsampling help?

I have a data set comprising measurements of invertebrate species richness from grab samples of seabed sediment collected from a shallow coastal area over a period of 20 years. The sampling effort is highly unbalanced over space (if that's the correct terminology) - for the first 10 years, 5 samples were collected annually from each of three stations, following which single samples were collected annually from 15 stations. There are also 60 additional stations that were only sampled once or twice over this period. I have several candidate predictor variables. Grain size measurements were collected from each individual grab sample, and mean current velocity , salinity , seabed depth and fishing pressure were estimated for each station. I would like to test whether species richness is related fishing pressure (accounting for relevant covariates), and I think a linear mixed model (with station and sample date as crossed random effects) is the way to go. The problem : Approximately half the data come from 3 stations, whereas the majority of predictor variables are calculated at the station level and I'm interested in spatial rather than the inter-annual variation in species richness. Possible solutions : Average to a single value per station, but this would confound any random effect due to sample date and result in an improved species richness measurement accuracy at well-sampled stations that is not represented in the model. Subset the data such that a each station is represented by a single randomly sampled grab sample. Stations that were visited only once would always be represented by the same sample. Generate, for instance, 1000 random subsets and fit a generalised linear mixed model, with sample date as a random intercept, to each subset. Pool the model results and extract summaries of fitted parameters. Questions : Is the second solution a sensible strategy or is there a better way? If it is sensible, would model selection involve comparing the median values of a distribution of AIC from each candidate model? How could hypothesis testing (such as using likelihood ratio tests) be carried out? Thanks for any help you could give, and let me know if anything needs clarification!
