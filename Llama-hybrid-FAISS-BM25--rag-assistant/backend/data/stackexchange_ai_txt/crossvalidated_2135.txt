[site]: crossvalidated
[post_id]: 2135
[parent_id]: 2131
[tags]: 
A popular approach towards solving class imbalance problems is to bias the classifier so that it pays more attention to the positive instances. This can be done, for instance, by increasing the penalty associated with misclassifying the positive class relative to the negative class. Another approach is to preprocess the data by oversampling the majority class or undersampling the minority class in order to create a balanced dataset. However, in your case, class imbalancing don't seem to be a problem. Perhaps it is a matter of parameter tuning, since finding the optimal parameters for an SVM classifier can be a rather tedious process. There are two parameters for e.g. in an RBF kernel: $C$ and $\gamma$. It is not known beforehand which $C$ and $\gamma$ are best for a given problem; consequently some kind of model selection (parameter search) must be done. In the data preprocessing phase, remember that SVM requires that each data instance is represented as a vector of real numbers. Hence, if there are categorical attributes, it's recommended to convert them into numeric data, using m numbers to represent an m-category attribute (or replacing it with m new binary variables). Also, scaling the variables before applying SVM is crucial, in order to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Check out this paper . If you're working in R, check out the tune function (package e1071) to tune hyperparameters using a grid search over supplied parameter ranges. Then, using plot.tune , you can see visually which set of values gives the smaller error rate. There is a shortcut around the time-consuming parameter search. There is an R package called "svmpath" which computes the entire regularization path for a 2-class SVM classifier in one go. Here is a link to the paper that describes what it's doing. P.S. You may also find this paper interesting: Obtaining calibrated probability estimates
