[site]: crossvalidated
[post_id]: 27816
[parent_id]: 11609
[tags]: 
There are so many long explanations here that I don't have time to read them. But I think the answer to the basic question can be short and sweet. It is the difference between a probability that is unconditional on the data. The probability of 1-alpha before collecting the dats is the probability that the well-defined procedure will include the parameter. After you have collected the data and know the specific interval that you have generated the interval is fixed and so since the parameter is a constant this conditional probability is either 0 or 1. But since we don't know the actual value of the parameter even after collecting the data we don't know which value it is. Extension of the post by Michael Chernick copied form comments: there is a pathological exception to this which can be called perfect estimation. Suppose we have a first order autoregressive process given by X(n)=pX(n-1) + en. It is stationary so we know p is not 1 or -1 and is There is a pathological exception to this which can be called perfect estimation. Suppose we have a first order autoregressive process given by X(n)=pX(n-1) + en. It is stationary so we know p is not 1 or -1 and is Now the en are independent identically distributed with a mixed distribution there is a positive probability q that en=0 and with probability 1-q it has an absolutely continuous distribution (say that the density is non zero in an interval bounded away from 0. Then collect data from the time series sequentially and for each successive pair of values estimate p by X(i)/X(i-1). Now when ei = 0 the ratio will equal p exactly. Because q is greater than 0 eventually the ratio will repeat a value and that value has to be the exact value of the parameter p because if it is not the value of ei which is not 0 will repeat with probability 0 and ei/x(i-1) will not repeat. So the sequential stopping rule is to sample until the ratio repeats exactly then use the repeated value as the estimate of p. Since it is p exactly any interval you construct that is centered at this estimate has probability 1 of including the true parameter. Although this is a pathological example that is not practical there do exist stationary stochastic processes with the properties that we require for the error distribution
