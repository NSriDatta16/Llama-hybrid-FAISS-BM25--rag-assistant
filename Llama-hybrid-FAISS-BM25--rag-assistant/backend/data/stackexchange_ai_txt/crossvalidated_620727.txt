[site]: crossvalidated
[post_id]: 620727
[parent_id]: 620718
[tags]: 
I thought the output from the logistic model the outputs are probabilities. Good! If you believe this---and I mean really believe it, not just know to say this as the full-credit answer to an exam question---you are quite a bit in front of many people who use logistic regression for "classification" problems and (mistakenly) believe logistic regressions to make explicit classifications. (Perhaps refer to my answer here for more detail.) Yes, the output of a logistic regression is either the predicted probability or a predicted log-odds that can be converted to a probability. Since either can be converted into the other, it is not worth arguing about the true output of a logistic regression model. The important fact to know is what your software gives as its predictions. predict_proba in sklearn will predict probabilities, while glm in R gives log-odds. If you use a different software package, the output should be documented somewhere. The reason the classical logistic regression gives probabilities is because it aims to predict the probability parameter $p$ of a binomial distribution. A binomial distribution also has an $n$ parameter, meaning that it can be thought of like multiple flips of the same coin that lands on heads with probability $p$ and tails with probability $1-p$ . In a binary "classification" problem where it is common to use a logistic regression (especially for "classification" problems), that binomial distribution is taken to have an $n=1$ , so it is just one flip of that coin. You would consider a successful flip to take the value $1$ and an unsuccessful flip to take the value $0$ . This leads to the answer to your first question. You do not input the category names like "heads" and "tails" into the log loss. You code them as $0$ and $1$ . Then you have the predictions, which are the probabilities of falling in the category that is coded as $1$ . You now have values that you can input into the log loss equation, where $y$ is your $0/1$ -coded observations and $\hat p$ is your predicted probabilities. $$ \text{LogLoss}\left(y, \hat p\right) = -\dfrac{1}{N}\overset{N}{\underset{i=1}{\sum}}\bigg[ y_i\log(\hat p_i) + (1 - y_i)\log(1 - \hat p_i) \bigg]\\ y_i \in\{0, 1\}\\ \hat p_i \in (0, 1) $$ Regarding the second question, the log loss comes from maximum likelihood estimation of the logistic regression parameter (the regression coefficients) for a binomial distribution, not a distance. It happens that maximum likelihood estimation for a Gaussian distribution coincides with minimizing Euclidean distance between predicted and observed values, but this does not generalize so nicely to all distributions (such as binomial). In fact, the log loss does not even obey the axioms of a metric in the sense of a metric space, which is typically taken as the most general mathematical space where "distance" has a reasonable meaning. However, there is still something distance-like about the log loss, in that, as the predicted probability of event $1$ approaches $1$ when the observation really was event $1$ , the loss decreases toward $0$ . Likewise, as the predicted probability of event $1$ approaches $0$ when the observation really was event $0$ , the loss decreases toward $0$ . library(ggplot2) p
