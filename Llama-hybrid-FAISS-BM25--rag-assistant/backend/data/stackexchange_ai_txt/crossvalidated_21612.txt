[site]: crossvalidated
[post_id]: 21612
[parent_id]: 21581
[tags]: 
To clarify the Bayesian approach: You start by knowing nothing, except that P(Heads) is in [0,1] . So start with a maximum entropy prior -> uniform(0,1) . This can be represented as a beta distribution -> beta(1,1) . Each time you flip the coin do a Bayesian up-date of the coin's P(Heads) by multiplying each point in the distribution by it's likelihood (multiply by x if you roll heads, multiply by (1-x) if you get tails), and re-normalize the total probability to 1. This is what the beta distribution does, so if the first roll is heads you'll have beta(2,1) . In your case you have beta(490,510) . From there I would calculate the 95% probability interval, and if 0.5 isn't in that interval, I would start to get suspicious. The first time I went through this exercise I was really surprised at how long it took to converge... I started because someone said "if you flip a coin 100 times, you know P(Heads) to +/- 1%" this turns out to be totally wrong, you need magnitudes more than 100 flips.
