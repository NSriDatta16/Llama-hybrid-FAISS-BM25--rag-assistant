[site]: crossvalidated
[post_id]: 544231
[parent_id]: 
[tags]: 
Transformer model training takes longer and results in lower train and validation loss

I have been making tests with Transformer model provided on Keras.io page , training for classification and seq2seq tasks in several datasets and compare Transformer to GRU/LSTM with almost same number of parameters in both models. In all my experiments, everything but the model are controlled variable and they are constant. I conducted my tests with GPU on my desktop (GTX 1070), Google Colab GPU and AWS Tesla T4. What I witnessed are: Training Transformer model takes longer for same number of epochs Both training and validation loss are much higher for Transformer model, than GRU/LSTM model at the end of same number of epochs. I tried various number of "number_of_heads" and "number_of_transformer_blocks", result did not change really. Transformers were said to be superior to RNNs like GRU/LSTM but I have never witnessed that. Therefore my question is, am I missing something?
