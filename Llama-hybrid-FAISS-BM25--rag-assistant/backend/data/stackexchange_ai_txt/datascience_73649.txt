[site]: datascience
[post_id]: 73649
[parent_id]: 
[tags]: 
Is there a limit in the number of layers for neural network?

I heard the neural network has a problem with vanishing gradient problems even though the ReLU activation function is used. In ResNet(that has a connecting function for reducing the problem), there is limit of maximum layers of 120~190(I heard). For the complete AI performance(or general AI with strong intelligence) I believe that the limit of the number of layers must be solved. Is there is any possibility that we find a new activation function that does not limit the number of layers? (maybe we could use exhaustive search... checking the train performance in a neural network of 200~500 layers)
