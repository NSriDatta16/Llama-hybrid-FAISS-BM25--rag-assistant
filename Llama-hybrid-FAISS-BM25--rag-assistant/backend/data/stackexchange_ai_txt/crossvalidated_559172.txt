[site]: crossvalidated
[post_id]: 559172
[parent_id]: 
[tags]: 
Recurrent Neural Networks(RNNs): does Truncated Back-Propagation Through Time (TBPTT) make RNN less effective than their unfolded version?

RNNs are Turing-complete. However AFAIU, the usefulness of this feature (provided by the recurrent nature of RNNs) depends on the network weights. If the weights are shaped by TBPTT they should be effective on a relatively short term memory, determined by the length of the unfolded RNN used for TBPTT. So when such weights affect the RNN forecast using long past data, that is an unwanted behavior and could potentially give bad predictions. In this case (so when coupling RNN and TBPTT) would it be better just to replace the RNN with its unfolded version even when forward propagating? It wouldn't be a Turing-complete system of rules anymore but it wouldn't risk using the learned weights improperly in the sense above described.
