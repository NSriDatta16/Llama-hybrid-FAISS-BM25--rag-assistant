[site]: datascience
[post_id]: 63405
[parent_id]: 
[tags]: 
Neural Network is overfitting when using bigger dataset

I'm tring to train a model using CNN (supervised) to solve a binary classification problem. I have pretty big dataset containing 2 800 000 samples, each having 100+ features. Because training with such big dataset takes about 20 hours I've built my net with using only 25% of dataset (about 700 000 samples). And if model have promising resoults on smaller dataset, then use full dataset to train. Here is something strange. I get pretty good resoults when using 25% of dataset It looks realy good, so I decided to train a model with using full dataset, and i get this which is awful. Model is clearly overfitting. I don't have much experience with machine learning, but as far as I know, one way to solve overfit problem is to increase dataset. But here, problem with overfiting occures only when I use bigger dataset. Other way to solve it would be to decrease number of layers, or number of neurons, but the same CNN have trained a good model on smaller dataset. My CNN looks like this 2 x Linear Layer 1 x Convolution 2 x Linear 4 x Convolution 6 x Linear And before every linear layer there is a dropout 50%, and dropout 20% is on input. And after every Convolution there is also a Avg pooling layer. Because dataset is pretty big, batch size is 256 for now. One of my ideas is to increase a batch size to 512, or even 1024 when training with full dataset, or to add a batch normalization layers, but before I do that, I want to ask for your advices, because, as I said, training takes about 20 hours... EDIT. I collect data on every 1% of batches. So data is collected 100 times for each epoch. Model is trained on 100 epochs, so data are collected 10000 times, and this value is on x axis. Also, to bad but I unfortunetly can not share the code.
