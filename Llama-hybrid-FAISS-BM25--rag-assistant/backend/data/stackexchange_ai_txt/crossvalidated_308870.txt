[site]: crossvalidated
[post_id]: 308870
[parent_id]: 
[tags]: 
Recognising distributions with neural network

I am wondering what is the right NN structure when the aim is to classify distribution function. Let's for example consider two $p$-dimensional probability distributions $D_Z$, $D_X$ (for example $D_Z\sim N(0, 1)$ and $D_X\sim N(0,2)$). Let $X, Z\in\mathbb R^{n\times p}$ be the matrix composed by $n$ samples of the $D_X$ and $D_Z$ distribution respectively. The aim is to construct a $NN$ such that $NN(X_{new}) = 0 $ and $NN(Z_{new})=1$. The main point here is that a single row in $X$ is in general not enough to recognise the underling distribution $D_X$ while it should be possible using as input $k>>0$ samples of $X$. This can probably be estimated using as input of the network $k$ rows of $X$ as a sample for class 0 and $k$ rows of $Z$ as sample for class 1. I am wondering if better results can be obtained using a convolution or recurrent input layer. EDIT: The distribution does not have a closed form and the number of samples I'll get is quiet small. My aim is not to correctly classify each sample individually but a batch of samples $X^k\in\mathbb R^{k\times p}$. The data can be modified in such a way to use as input of the network $X^k$ rather than a single row of $X$. I'm wondering if in this a convolution could help.
