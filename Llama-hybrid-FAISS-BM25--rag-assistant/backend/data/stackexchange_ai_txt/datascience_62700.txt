[site]: datascience
[post_id]: 62700
[parent_id]: 54277
[tags]: 
The Clear alternative is definitely a Monte-Carlo Search, if you really have to choose some alternatives. But if you are only facing a problem to update the Q values after the episode then let me mention you can do that with a little variation into regular Q learning equation and algorithm. Infect a very good example is the applied into Tic-Tac-Toe problem by Dwi H. et. al. In this paper, Dwi H train the agent by playing a full game and only when game is finished the reward is updated with positive if wining and negative if loosing. Also Deep-Q could also work in some scenarios but as @Neil suggested in his answer you have to explore possibilities, and if you had already developed Q learning method then best way is to modify it according to your need and see if it satisfy your needs before going to other options.
