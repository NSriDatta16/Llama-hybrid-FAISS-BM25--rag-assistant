[site]: crossvalidated
[post_id]: 453150
[parent_id]: 
[tags]: 
Can an agent be trained in a completely random environment (the rules and actions stay the same)

I am specifically talking about the FrozenLake example from openAI gym to illustrate my question. https://gym.openai.com/envs/FrozenLake-v0/ The way I understand how Q-Learning or DQL works on the FrozenLake environment is that the agent plays the same map a lot of times until either the Q-table or the NN can approximate the value of a given state action tuple. However while doing some research, I found no one attempting to try to use the randomly generated map of this environment. I also dont understand how it would work... It seems to me that an agent only tries to learn the different states of a specific environment. However if this environment changes every time, the agent can not use its previously learned information (e.g. the agent can not remember where the holes are to avoid as they change every time the level is newly generated)... Is it impossible to train an agent on a changing environment if the observation space is only its current position (but not the whole level) * Is it possible to create an agent that can somehow generalise and learn to navigate every random level he is confronted with? If yes, is DQN the right algorithm to do so? If yes, is the solution to give the agent more information about the state (i.e. a bigger observation space) (if yes, what kind of information could be given additionally? the whole level would be too much because then other algorithms could be used, or not?) Am I understanding something else wrong? (*) Intuitively I would say yes, at least in the case of the FrozeLake example. If the only information about the state the algorithm receives is its current position in the state, there should be no way for an agent to predict where a hole could be, so he can only guess where to go and might fall in a hole.
