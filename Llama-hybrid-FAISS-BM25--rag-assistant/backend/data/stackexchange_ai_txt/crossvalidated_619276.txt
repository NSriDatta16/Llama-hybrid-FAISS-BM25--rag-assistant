[site]: crossvalidated
[post_id]: 619276
[parent_id]: 412580
[tags]: 
An answer to 'why no $R^2$ for glm?' may be found in questions like 'what is actually the use of $R^2$ ?'. I can see $R^2$ as A measure of correlation, a coefficient of determination. And this is the origin of the variable ( Who is the creator or inventor of coefficient of determination (R-squared)? ) with scientist like Sewall Wright using it to describe heradity. For distributions that are not normal, correlation coefficients start to loose interpertability. See for instance Correlations between continuous and categorical (nominal) variables or Bernoulli random variables and correlation coefficient A link with the likelihood of normal distributed data. Both cases are not an argument that it should also be used for GLM. Tricky issue with glm 1 If you want the coefficient of determination, then you can always compute it. However, for most glm models it makes little sense to compute it and also the $R^2$ is not even always equal. Say I have coin flips coin 1 HHTTH coin 2 HTTHT which I model with equal probability heads tails as null model or with p=0.4 and p=0.6, then my $R^2$ can be nearly zero $$1-\frac{6 \times 0.4^2+4 \times 0.6^2}{10 \times 0.5^2} = 0.04$$ and it can be one $$1-\frac{(2-2)^2+(3-3)^2}{(2.5-2)^2+(2.5-2)} = 1$$ One can argue that this is also true for normal distributed variables, if one takes sums or averages of multiple cases, then $R^2$ will be reduced as the noise levels reduce due to the averaging. But for the coefficient of determination one is often interested in the relationship/correlation between individual cases from the population. E.g. like the correlation between the height of mom+dad with their children. And not for averages of such variables. Problems described with glm models are not often of that type (comparing correlations). Tricky issue with glm 2 For cases that are tackled with a GLM the relationship of the residual variance is often of less importance, or at least complicated , as there is already an intrinsic variance (like the coin flips above). The correlation between the data might be less important when there is some intrinsic variance in the model that can not be eliminated. In GLM the variance is even explicitly modelled where the variance is a function of the expected value. When we apply GLM then we work with problems of a different nature. For example, if you have Poisson distributed data, then you won't be aiming for high $R^2$ as there is an intrinsic unexplained variance in the model. This is why people may rather look at something like a comparison with the deviance difference between the full model and the null model and the deviance of the tested model.
