[site]: datascience
[post_id]: 120893
[parent_id]: 45285
[tags]: 
In practice, cross entropy is better than mean squared error with sigmoid because cross entropy loss is convex w.r.t the last layer whereas MSE with sigmoid is not. This means that it may get stuck at local minimum and as a result the learned probability may never reach the target value. On the other hand, it's non-trivial to argue if cross entropy is distinctly better than MSE with clamp. They actually produce the same gradient values. You can refer to the discussion on Using MSE instead of log-loss in logistic regression , where the conclusion seems to be that the two are computationally similar but from a probabilistic view, cross entropy is more intuitive and provides more useful results than just the output. I'd also argue that with a neural network model, the output will be too unstable during extrapolation that a simple clamping into [0,1] would not produce helpful result. Regarding your concern that It's weird to me that the cross entropy loss when predicting .7 when the target is .7 (ie a perfect prediction) is .61 The value of loss function doesn't always have practical meaning. Especially in probabilistic contexts, the value of metrics such as cross entropy and KL-divergence are mostly relative than absolute, meaning that they only makes sense when used in comparison, not much when used standalone. What really matters is the gradient value induced by them, which has a decisive role in optimization and model learning.
