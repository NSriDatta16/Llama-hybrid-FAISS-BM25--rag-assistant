[site]: datascience
[post_id]: 44710
[parent_id]: 44699
[tags]: 
There is one answer to know: try both methods and take the one that gives the best result. I would say that in general pre-trained embeddings usually gives better results. You can also start with pre-treained embeddings as initial conditions and let the embeddings train maybe with a smaller learning rate. In any case, the current state of the art for text classification is ULMFIT ( https://arxiv.org/abs/1801.06146 ), which actually doesn't do any of this. It pre-trains embedding and RNN with a language model in the wikipedia and in the target text and then fine tunes the whole model with the target text.
