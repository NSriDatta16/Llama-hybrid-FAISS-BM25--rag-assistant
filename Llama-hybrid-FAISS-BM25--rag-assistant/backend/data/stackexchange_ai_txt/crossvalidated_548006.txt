[site]: crossvalidated
[post_id]: 548006
[parent_id]: 97001
[tags]: 
They are equivalent in the following sense. Now suppose you have an offline dataset consist of (s1,a1),(s2,a2),..,(sk,ak) with reward r1,r2,...,rk. Based on the data, you can estimate the MDP model with transition probability T(s,a,s') and R(s,a,s'). You can also estimate the MDP model to be T(s,a,s') and R(s,a). Solve these two MDP models theoretically, you should obtain the same results of policy and value. The above is model-based learning. You can also use the Q-learning method by using the offline dataset in an online fashion (assume that you observe sk,ak,rk sequentially). Obviously, the estimated Q-matrix Q^t(s,a) would be different during the learning as the two forms of reward are different. However, you should obtain the same results of Q(s,a) when it converges. The reason is because P(s,a,s') has already contained the information you need to obtain R(s,a) from R(s,a,s').
