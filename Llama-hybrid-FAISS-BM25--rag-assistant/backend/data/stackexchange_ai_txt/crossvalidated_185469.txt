[site]: crossvalidated
[post_id]: 185469
[parent_id]: 185464
[tags]: 
There isn't a single "best" answer to your question. There are many threads throughout CV that discuss these issues. Here's one ... Understanding how good a prediction is, in logistic regression Key points to note are that training or calibration data provides useful information regarding optimal fit (e.g., Harrell recommends using a nonparametric loess smoother for this) but is well known to be positively biased: The calibration curve is both a measure of goodness-of-fit and a great way to check the accuracy of probabilities estimated by the model. Regardless, the emphasis in machine learning is on using validation (out-of-sample) data to evaluate fit. Validation results provide useful information regarding the bias-variance tradeoff that can't be inferred from calibration information. If your validation error and training error are both high, you have underfitting (bias). If your validation error is high but training error is low, you have overfitting (variance). Here's one discussion of this: Question about bias-variance tradeoff
