[site]: crossvalidated
[post_id]: 554821
[parent_id]: 
[tags]: 
What the i.i.d. assumption of the errors in linear regression implies for the response variable y?

In the linear regression model we assume that the errors $ε_i$ are independent and identically distributed (i.i.d.) random variables. I am trying to understand what this assumption implies regarding the response variables $y_i$ . As far it concerns the identitally distributed assumption, since $y_i = X_ib+ε_i$ where $ε_i\sim \mathcal{N}(0,\,\sigma^{2})\,$ then $y_i\sim \mathcal{N}(Xb,\,\sigma^{2})\,$ . So, the response variables are not identically distributed random variables because they do not have the same mean. My questions are the following: Can we assume that $y|X=x$ , for example $y|X=5$ , are identically distributed since they follow the same distribution with the same mean and variance? Can we assume that $y$ are independent random variables or $y|X$ are independent random variables or neither of the two? In the machine learning context we assume that the data $(x_i,y_i)$ are i.i.d. What does this assumption implies about the random variable $y_i$ . How is this related with the i.i.d. assumption of the errors in linear regression? Finally, regarding my opening statements that the errors are i.i.d. and $ε_i\sim \mathcal{N}(0,\,\sigma^{2})\,$ are they correct or we assume that the errors conditional on $X$ are i.i.d. and $ε_i|X_i\sim \mathcal{N}(0,\,\sigma^{2})\,$ or is the same thing?
