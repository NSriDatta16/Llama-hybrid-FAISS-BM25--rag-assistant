[site]: crossvalidated
[post_id]: 461744
[parent_id]: 461136
[tags]: 
What you want is actually feature importance. The method scikit-learn uses for evaluating it is pretty basic: it sums up the training score gain of all the splits made in all trees using each variable, and then scales the results so that they sum to 1. The "score gain" of a split is how much the training MSE was reduced by the split, in statistics, that is called $SS_{between}$ . In simple words, it is a sum over all observations in that knot, of the squared distance between the prediction considering that split, and the prediction without considering it. It is, to put it simply, the amount impact of that split on predictions. And its sum over all splits that employ one variable, averaged over all trees, is the amount of impact that variable has on predictions. I understand that maybe a sum of squared distances is not what you were looking for, but you may choose it for its link to the score function, if not because it's so easily served up to you!
