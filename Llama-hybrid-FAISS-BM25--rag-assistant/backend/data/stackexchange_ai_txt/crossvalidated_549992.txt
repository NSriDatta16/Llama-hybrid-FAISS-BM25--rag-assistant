[site]: crossvalidated
[post_id]: 549992
[parent_id]: 
[tags]: 
Why does my loglikelihood dip during training?

I have a model that contains two neural networks: one is computing the mean of a function as a function over x and the other is computing the sd of that function as a function over x . The model computes the loglikelihood of a pair x and y (using Gaussian pdf of the mean and sd at that value of x ). I train the model using a gradient descent method that tries to maximise this loglikelihood at each step. Plotting epoch against mean loglikelihood shows something interesting. The loglikelihood initially increases rapidly and then plateaus as expected. However, beyond a certain point there are 'dips' in the mean loglikelihood. What is causing these dips? Each step aims to maximise log likelihood yet some of the steps are decreasing it. I have a similar model that uses a single value for sd rather than a function over x and it doesn't demonstrate these dips. Code: import torch import torch.nn as nn import pandas import numpy as np import matplotlib.pyplot as plt url = 'https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/xkcd.csv' xkcd = pandas.read_csv(url) x,y = torch.tensor(xkcd.x, dtype=torch.float)[:,None], torch.tensor(xkcd.y, dtype=torch.float)[:,None] class MyNN(nn.Module): def __init__(self): super().__init__() self.f = nn.Sequential( nn.Linear(1,4), nn.LeakyReLU(), nn.Linear(4,20), nn.LeakyReLU(), nn.Linear(20,20), nn.LeakyReLU(), nn.Linear(20,1) ) def forward(self, x): return self.f(x) class MyLL(nn.Module): def __init__(self): super().__init__() self.μ = MyNN() self.σpre = MyNN() def σ(self, x): return torch.nn.functional.softplus(self.σpre(x)) def forward(self, y, x): σ2 = torch.pow(self.σ(x), 2) return - 0.5*torch.log(2*np.pi*σ2) - torch.pow(y - self.μ(x), 2) / (2*σ2) torch.manual_seed(0) model = MyLL() optimizer = torch.optim.Adam(model.parameters()) logliks = [] for _ in range(10000): optimizer.zero_grad() loglik = torch.mean(model(y, x)) logliks.append(loglik.detach()) (-loglik).backward() optimizer.step() plt.plot(logliks) plt.show()
