[site]: stackoverflow
[post_id]: 4358379
[parent_id]: 4358348
[tags]: 
You can download the local page using curl and then validate it using a local validator or post again using curl to the W3 Validator or your online HTML validator of choice. Or you can write a simple web spider in some scripting language and crawl the local web, validating each page as crawled. Sample crawler class in Perl: package Test::Crawler; use Moose; use WWW::Mechanize; has client => ( is => 'ro', isa => 'WWW::Mechanize', default => sub { WWW::Mechanize->new }, ); has handler => ( is => 'ro', isa => 'CodeRef', default => sub {}, ); sub crawl { my ($self, $url, $visited) = (@_, {}); # Already seen that. return if $visited->{$url}++; # Not seen yet, get. $self->client->get($url); $self->handler->($url, $self->client); # Follow all links. my @uris = map { $_->URI } $self->client->links; for my $uri (@uris) { # Skip mailtos, ftp and such. next if $uri->scheme; # Skip external links. next if $uri->host; $self->crawl($uri->rel, $visited); } } And a sample sub to validate using validator.nu : sub is_valid { my ($code, $page_url) = @_; my $ua = LWP::UserAgent->new; my $url = 'http://validator.nu/?out=gnu'; my $response = $ua->post($url, Content_Type => 'text/html', Content => $code); return $response->as_string !~ /error/; }
