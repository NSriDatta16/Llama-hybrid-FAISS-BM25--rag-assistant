[site]: crossvalidated
[post_id]: 363234
[parent_id]: 363208
[tags]: 
I respectfully disagree with the answer by Frans ; there is nothing in the frequentist methodology that takes any position on whether data-generating processes, as modeled by statistics, are deterministic or not. (While Wikipedia is a useful source for some statistical material, this unreferenced sentence does not hold any weight for me.) Frequentism defines the "probability" of an event in the context of a repeatable sequence of trials as the limiting relative frequency of that event over a sequence of trials.$^\dagger$ Hence, within this framework, the application of probabilistic models implies only that the user is satisfied that the event can be placed in a sequence of theoretically repeatable trials. The occurrence of events in the sequence, and the existence of a limiting relative frequency, are not affected by whether the process is deterministic or not. In view of this, within the frequentist paradigm, one should not imbue references to "probability" or "stochastic" with any non-deterministic implications (in a metaphysical sense). Within this paradigm the notion of probability refers merely to limiting relative frequencies of events, and a "stochastic" model is just one that is described with the use of probability (i.e., with the appeal to limiting relative frequencies of events). A frequentist statistical model is only "non-deterministic" in a mathematical sense ---i.e., that the specification of the parameters does not logically imply the outcome of the individual random variable. (Or to put it another way, the limiting relative frequency of an event does not logically imply the occurrence or non-occurrence of the event at any particular point in the sequence.) One could believe in randomness in the universe, or determinism, and apply frequentist methods and interpretations. Under this paradigm the observable values are considered to be results from a repeatable experiment and thus they are considered to be contained within a (hypothetical) infinite sequence, with limiting empirical distribution described by one or more "parameters". These latter objects are treated as "unknown constants" even if the user adopts a non-deterministic aleatory view which holds that the parameter is non-deterministic. $^\dagger$ My view is that this frequentist definition is problematic, since it takes the concept of a repeatable experiment to be preliminary to probability, and it therefore has trouble explaining the conditions that constitute repeatability (since it cannot appeal to any probabilistic condition). This notion is actually well-described within Bayesian theory by the concept of an exchangeable sequence of values, where the condition of exchangeability corresponds to repeatability. Within the Bayesian framework the representation theorem of de Finetti then establishes that the probability corresponds to the limiting relative frequency as a mathematical consequence of exchangeability, rather than as a definition.
