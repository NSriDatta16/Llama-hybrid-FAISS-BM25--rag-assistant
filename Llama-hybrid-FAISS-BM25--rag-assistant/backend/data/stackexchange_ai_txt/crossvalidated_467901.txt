[site]: crossvalidated
[post_id]: 467901
[parent_id]: 467893
[tags]: 
This is not the tradeoff between Bayesian and frequentist statistics. The likelihood function describes the probability (density) of the observations given particular parameter values. $$\mathcal{L(\theta | x)} = f(x\vert\theta)$$ It is reversing the dependent and independent parameters in the function, but it remains the same function. Likelihood vs probability This reversal occurs because often the behavior of the observations as a function of the parameters is known, but in practice we do not know the parameters and we know the observations. see the German tank problem for example Common problems in probability theory refer to the probability of observations $x_1, x_2, ... , x_n$ given a certain model and given the parameters (let's call them $\theta$ ) involved. For instance the probabilities for specific situations in card games or dice games are often very straightforward. However, in many practical situations we are dealing with an inverse situation ( inferential statistics ). That is: the observation $x_1, x_2, ... , x_k$ is given and now the model is unknown , or at least we do not know certain parameters $\theta$ . The central limit theorem, or any simplification of the probability of the observations $x$ as a function of the parameters $\theta$ , $f(x \vert \theta)$ , applies to Bayesian and frequentist statistics in the same way. Both methods use the function $f(x \vert \theta)$ as starting point and the simplifications based on the CLT are applied to that function. See for instance this article 'Bayesian synthetic likelihood' by Price, Drovandi, Lee and Nott as an example where the CLT is applied in Bayesian statistics. The tradeoff The tradeoff between Bayesian and frequentist statistics is from Are there any examples where Bayesian credible intervals are obviously inferior to frequentist confidence intervals What is different? The confidence interval is restricted in the way that it draws the boundaries. The confidence interval places these boundaries by considering the conditional distribution $X_\theta$ and will cover $\alpha \%$ independent from what the true value of $\theta$ is ( this independence is both the strength and weakness of the confidence interval ). The credible interval makes an improvement by including information about the marginal distribution of $\theta$ and in this way it will be able to make smaller intervals without giving up on the average coverage which is still $\alpha \%$ . (But it becomes less reliable/fails when the additional assumption, about the prior, is not true) The Bayesian and Frequentists methods condition their intervals on different scales. See for instance the differences in the conditional coverage for credible intervals (in the sense of highest posterior density interval) and confidence intervals In the image below (from the example in this answer/question) the expression of conditional probability/chance of containing the parameter conditional on the true parameter $\theta$ (left image) and conditional on the observation $x$ (right image). This relates to Why does a 95% Confidence Interval (CI) not imply a 95% chance of containing the mean? The confidence interval is constructed in such a way that it has the same probability of containing the parameter, independent from the true parameter value . The credible interval is constructed in such a way that it has the same probability of containing the parameter, independent from the observation . The trade-off is that the credible (Bayesian) interval allows making predictions with smaller intervals (which is advantageous, by contrast, imagine making the prediction that the parameter value is between $-\infty$ and $\infty$ ). But... the credible interval depends on prior information.
