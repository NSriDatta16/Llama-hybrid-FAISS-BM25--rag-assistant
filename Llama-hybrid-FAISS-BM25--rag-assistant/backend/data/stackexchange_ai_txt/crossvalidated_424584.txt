[site]: crossvalidated
[post_id]: 424584
[parent_id]: 424567
[tags]: 
If I got your question correctly (and I am not sure), the solution is that you have to take the spectral decomposition $VDV^{T}$ where V is the orthonormal matrix of eigenvectors and D is the diagonal matrix of eigenvalues. Then you transform the set of dependent variables by multiplying the original matrix of predictors $X^{*}$ by the eigenvectors $X=X^{*}V$ . Then you can regress y against the new transformer predictors X that are now uncorrelated. Since they are uncorrelated, the total explained variance of y will be the sum of variances of $\beta x$ where each x is the vector representing each column of X (so each transformed variable) and $\beta$ denotes its corresponding beta estimated at previous step. The PC variable $x^{max}$ such that $\beta^{2}_{x^{max}} Var(x^{max})$ is maximum among all the pc transformed variables is the one explaining the highest portion of the variance of $y$ .
