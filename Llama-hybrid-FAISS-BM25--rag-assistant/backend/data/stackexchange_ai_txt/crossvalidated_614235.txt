[site]: crossvalidated
[post_id]: 614235
[parent_id]: 
[tags]: 
Understanding the ridge leverage scores sampling from an arXiv paper

I give a try to read the arXiv paper Distributed Adaptive Sampling for Kernel Matrix Approximation, Calandriello et al. 2017 . I got a code implementation where they compute ridge leverage scores sampling sequentially using this paper, $$\widetilde{\tau}_{t,i}=\frac{1-\epsilon}{\gamma}\left( k_{i,i}-\boldsymbol{k}_{t,i}^\top \overline{\boldsymbol{S}}(\overline{\boldsymbol{S}}\boldsymbol{K}_t\overline{\boldsymbol{S}}+\gamma\boldsymbol{I}_t)^{-1}\overline{\boldsymbol{S}}^\top\boldsymbol{k}_{t,i} \right)\tag1$$ Where, they define a column dictionary as a collection $I_t=\{(i,w_i)\}_{i=1}^t$ , where the first term denotes the index of the column and $w_i$ its weight , which is set to zero for all columns that are not retained. And $\boldsymbol{S}_t$ is the weighted selection matrix. For detail information in the section 3, Sequential RLS Sampling . There they redefine the dictionary, a collection $I=\{i,\widetilde{p}_i,q_i\}$ , where $i$ is the index of the column (consider as point $\boldsymbol{x}_i$ ), $\widetilde{p}_i$ tracks the probability used to sample it, and $q_i$ is the number of copies (multiplicity) of $i$ . Question: I didn't understand what they mean by the parameter $q_i$ ? The code implementation to compute $\widetilde{\tau}_{t,i}$ , def compute_tau(centers_dict: CentersDictionary, X: np.ndarray, similarity_func: callable, lam_new: float, force_cpu=False): """Given a previosuly computed (eps, lambda)-accurate dictionary, it computes estimates of all RLS using the estimator from Calandriello et al. 2017""" xp = __load_gpu_module(force_cpu) diag_norm = np.asarray(similarity_func.diag(X)) # (m x n) kernel matrix between samples in dictionary and dataset X K_DU = xp.asarray(similarity_func(centers_dict.X, X)) # the estimator proposed in Calandriello et al. 2017 is # diag(XX' - XX'S(SX'XS + lam*I)^(-1)SXX')/lam # here for efficiency we collect an S inside the inverse and compute # diag(XX' - XX'(X'X + lam*S^(-2))^(-1)XX')/lam # note that in the second term we take care of dropping the rows/columns of X associated # with 0 entries in S U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X) + lam_new * np.diag(centers_dict.probs))) U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD) E = xp.asarray(S_root_inv_DD * U_DD.T) # compute (X'X + lam*S^(-2))^(-1/2)XX' X_precond = E.dot(K_DU) # the diagonal entries of XX'(X'X + lam*S^(-2))^(-1)XX' are just the squared # ell-2 norm of the columns of (X'X + lam*S^(-2))^(-1/2)XX' tau = (diag_norm - xp.asnumpy(xp.square(X_precond, out=X_precond).sum(axis=0))) / lam_new return tau I failed to understand the link between the actual paper formula $(1)$ (which is mention inside compute_tau as diag(XX' - XX'S(SX'XS + lam*I)^(-1)SXX')/lam ) and the code implementation of compute_tau . Question: "what is X_precond here and what svd decomposition doing here?" I create a GitHub issue and also mailed to reach the author, but got no luck. It would be greatly appreciated if anyone could assist me in resolving this matter.
