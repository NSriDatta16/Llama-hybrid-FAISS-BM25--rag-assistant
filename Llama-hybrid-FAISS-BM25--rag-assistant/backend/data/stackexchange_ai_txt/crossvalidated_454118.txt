[site]: crossvalidated
[post_id]: 454118
[parent_id]: 
[tags]: 
MNIST: single layer NN with 784 neurons; is 90% error rate normal?

I am relatively new to neural networks. To understand the basics, I converted the MNIST database into a format that I liked, and I wrote a single layer NN with 784 neurons from scratch without using any library related to NN. I trained the NN over 600 samples, and I tested it on 10000 samples. (I accept that the reverse would be much better.) I can see that as the NN trains with more and more samples, the error decreases almost exponentially with the training sample size. However, in the end, the test error was 90%. Is this normal for such a simple NN? Where can I find the performance of different types of NNs with different parameters to compare with my own NN? Of course, this is a very simple NN, but before making it more complex, I would like to know whether the results I am getting are in agreement with others, and I would like to get an idea about what kind of different methods & schemes could be used in other situations.
