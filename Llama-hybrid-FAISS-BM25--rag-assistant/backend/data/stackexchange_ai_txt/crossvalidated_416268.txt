[site]: crossvalidated
[post_id]: 416268
[parent_id]: 416266
[tags]: 
You are missing the point that the definition of the standard deviation is the square root of the variance $V$ which is defined as $$V = \frac 1n \sum_{i=1}^n (x_i-\mu)^2.$$ So why is $V$ defined the way it is? Well, a standard model is that the $x_i$ are a random sample from a distribution with known mean $\mu$ and unknown variance $\sigma^2 = E[(X-\mu)^2]$ and so $$V = \frac 1n \sum_{i=1}^n (X_i-\mu)^2\\$$ is a random variable whose expected value $E[V]$ is just $\sigma^2$ . Today , the random variable $V$ happens to have value $\frac 1n \sum_{i=1}^n (x_i-\mu)^2$ and we are using this value as an estimate of $\sigma^2$ . We are also calling $V$ as the variance of the random sample. Be that as it may, you are averaging too late by dividing the square root of the sum $\sum_{i=1}^n (x_i-\mu)^2$ by $n$ ; you need to do the averaging right after the sum has been computed rather than after the square-rooting has been done.
