[site]: datascience
[post_id]: 9912
[parent_id]: 9909
[tags]: 
It is natural that RBM requires some time to converge, but if you see that the reconstruction is decent, then your algorithm works well. I don't understand the idea of optimizing the model by your objective function: cost = T.sum((data - vis)**2).mean() . I think you just cannot train RBM (or any other stochastic model) that way because vis is a nonlinear function of the data. If you omit sampling of the hidden vector you might use this formula and end up having model similar to the feed forward network that learns an identity transformation. It fact it can be used as a feature extractor. Also notice that your second network no longer will be a proper generative model, because its learning rule doesn't follow the gradient of RBM's cost function. Hence it won't properly model either an energy function or a joint probability P(v, h) of your visible and hidden vector. You may also be interested in reading this : Although it is convenient, the reconstruction error is actually a very poor measure of the progress of learning. It is not the function that CD learning is approximately optimizing, [...] and it systematically confounds two different quantities that are changing during the learning. The frst is the difference between the empirical distribution of the training data and the equilibrium distribution of the RBM. The second is the mixing rate of the alternating Gibbs Markov chain. If the mixing rate is very low, the reconstruction error will be very small even when the distributions of the data and the model are very different. As the weights increase the mixing rate falls, so decreases in reconstruction error do not necessarily mean that the model is improving and, conversely, small increases do not necessarily mean the model is getting worse. So what is the cost function that RBM tries to optimize? good description can be found here $$-\frac{\partial log(p(x))}{\partial \theta }=\frac{\partial F(x)}{\partial \theta}-\sum_{\tilde{x}}p(\tilde{x})\frac{\partial F(\tilde{x})}{\partial \theta}$$ On the left hand side we have minus log probability of the data which we must minimize in order to increase the probability of the data. On the right hand side we have two terms. The function $F(x)$ coresponds to the free energy of the network given the data $x$ and current weights $\theta$ so minimization of this function increases the probability of our data. On the other hand we must take into account the normalizing factor $Z$- its role is to give correct probability distribution of the input vectors and this sum in the equation corresponds to the normalizing factor. In other words we must maximize the free energy (decrease probability) over all set of inputs proportionally to its current probability $p(\tilde{x})$. If we omit to maximize this sum our model won't converge to the optimal solution. Contrastive Divergence is only crude approximation of this objective funtion but it works very well in practice. I recommend that you watch this film by Geoffrey Hinton
