[site]: crossvalidated
[post_id]: 615721
[parent_id]: 276067
[tags]: 
As others have pointed out, the - log of the loss = (probability of correct classification). So, for example, losses of -log(.9), -log(.8), -log(.7), -log(.6), and -log(.5), or .11, .22, .36, .51, and .69 corresponding to probabilities of correct classification of 90%, 80%, 70%, 60%, 50%. Thinking evaluatively, a random classifier will, on average, make correct predictions in a balanced classification problem 1/n_classes % of the time, so the loss of a random classifier would be -log(1/n_classes). However, by log laws, this is equal to log((1/n_classes)^-1) = log(n_classes). For n_classes on [2,10], random classifiers should produces losses [log(2), log(10)], or (0.69, 1.10, 1.39, 1.61, 1.79, 1.95, 2.08, 2.20, 2.30) respectively if you're looking for some concrete numerical benchmarks. For unbalanced classification refer to Fed Zee's answer, and for determining the significance of a better-than-random log loss look at significance testing with binomial distributions.
