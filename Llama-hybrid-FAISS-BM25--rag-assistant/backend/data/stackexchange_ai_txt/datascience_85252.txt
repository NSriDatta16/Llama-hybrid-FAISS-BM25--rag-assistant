[site]: datascience
[post_id]: 85252
[parent_id]: 85242
[tags]: 
Why would you expect them to be the same? In one hand Random Forest and Gradient boosting are two types of different ensembles. Even if their estimator is a decision tree and they both seem to measure in scikit learn impurity-based feature importance. The result will be different. Not much is my guess, but different. For Deep Neural Networks, you are calculating a different "feature importance" metric. So it makes sense that the calculated result is different. Sadly, feature importance is just aggregation statistics that does not tell you much about the explainability of the model. Different feature importance in different models is completely fine, no problem with that.
