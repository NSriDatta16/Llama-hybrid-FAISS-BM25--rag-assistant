[site]: crossvalidated
[post_id]: 71470
[parent_id]: 54975
[tags]: 
You can consider some kind of jackknife approach in which each reading is compared against the mean of all other runs except the reading being contrasted. For example, in one of your comments you mentioned a set of number: $62.970, 62.202, 61.791, 56.588, 59.155$ To contrast the first one with the rest: Calculate the means of $62.202, 61.791, 56.588, 59.155$, which is $59.934$. Find out the percent difference: $(62.970 - 59.934)/59.934 \times 100\%$, which is $5.07\%$ Repeat this for the rest, the whole data should be: $5.07\%, 3.45\%, 2.59\%, -8.03\%, -2.85\%$ Now, this tells a lot of problems. First your assumption that readings get more consistent is challenged here. The "odd-one-out" is actually the fourth run. Another worrying sign is the consistent autocorrelation. A consistent test shouldn't have all the positive error in the front, and negative error at the back, aka, the positive and negative should scatter. This brings back to your original question. I'd suggest that statistics isn't the core problem here, but the quality control of your device. If "unclean" or "non-stablized" is your concern, then please check the system manual and investigate how to properly rinse your coil, and how to use test run reagent to estimate the efficiency. If you come back to judge your readings in such a post-hoc manner, "weeding" out weird data, your results can invite substantial suspicion. In a nut shell, I'm not exactly a lab-technician, but from a general researcher's point of view all these data cleaning criteria should be set a prior , that is before you have gotten any data. Once you have the data, and the protocol was followed, and there wasn't detected problem, then the number should be analyzed.
