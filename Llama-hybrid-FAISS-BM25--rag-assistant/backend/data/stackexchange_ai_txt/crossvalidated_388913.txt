[site]: crossvalidated
[post_id]: 388913
[parent_id]: 388910
[tags]: 
SVM with linear kernel can do that because mean is sensitive to outliers as you said. Consider a simple 1D case, with two classes. One class is distributed around -1, and the other around +1 with very low variance, i.e. doesn't overlap. But, there is an outlier for positive class and negative class, which are around $-N$ and $N$ , where $N$ is the number of data points. such that both means are at $0$ . But, this is a thought experiment and there will be cases where SVM cannot succeed, just as in your example, where there seems to be large number of so called outliers. Edit : Removing my hinge-loss argument since it's not completely correct but also not needed, I could generate the following experiment for the above case. import numpy as np from sklearn.svm import LinearSVC N = 100 x1 = np.concatenate([np.ones(N), [-N]]) y1 = np.ones(N+1) x2 = np.concatenate([-np.ones(N),[N]]) print("Means: {}, {}".format(x1.mean(),x2.mean())) y2 = -np.ones(N+1) x = np.concatenate([x1,x2]) y = np.concatenate([y1,y2]) svc = LinearSVC() svc.fit(x.reshape(-1,1),y) print("Threshold: {:4f}".format((-svc.intercept_ / svc.coef_[0])[0])) yp = svc.predict(x.reshape(-1,1)) print("Accuracy: {:4f}".format(np.mean(y==yp))) Having the following output: Means: 0.0, 0.0 Threshold: -0.014788 Accuracy: 0.990099
