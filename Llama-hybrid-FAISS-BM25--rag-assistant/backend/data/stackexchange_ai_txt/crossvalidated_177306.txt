[site]: crossvalidated
[post_id]: 177306
[parent_id]: 177203
[tags]: 
This situation sounds fairly reasonable to me. Correlated features definitely result in unstable parameter estimates, as you're seeing, so don't rely on those inferences too much. But predictions can still be reasonable even if your parameter estimates are unstable: the problem is that a lot of parameters give similar predictions, so it's hard to distinguish between them, but the predictions are still fine. Assuming that your training-test split is sufficiently hygienic, that it does well on test data means that it's a pretty good predictor. The fact that individual features don't yield good predictors doesn't mean that their combination is bad. If you're interested in reducing the multicollinearity, you could try using the principal components, or perhaps a sparse PCA to increase interpretability.
