[site]: crossvalidated
[post_id]: 397589
[parent_id]: 267628
[tags]: 
It depends on how you use PCA in your analysis. If you are using it to remove variables/variance from the dataset/model, then the PCA-derived model will not retain the SAME predictive power as a fully dimensional model. That being said, different is not always worse. In fact, by squeezing more of your data's variance into fewer variables (i.e. forming principle components from the data) and removing some less informative PCs (ones capturing minuscule variance), there's a real possibility that the PCA-derived model's predictions become more accurate due to the fact that you're less likely to overfit (VARs tend to run into overfitting issues often due to model parameter count exploding...). If you keep all principle components after PCA, then the predictive power should remain constant across methods. The real tricky part can be making sense of the model in its principle component form. Evidently, there are ways to back-project pca-derived model coefficients to their full dimensional form ( http://scot-dev.github.io/scot-doc/vartransform.html#covbivar1 ) so long as you retain all PCs after PCA (which seems utterly useless in most cases). I'm not sure you you would backproject when you remove PCs, but would love to find an answer to that question if anyone has any pointers.
