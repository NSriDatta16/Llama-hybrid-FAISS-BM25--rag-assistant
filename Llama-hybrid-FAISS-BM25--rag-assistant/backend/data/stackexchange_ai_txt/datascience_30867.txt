[site]: datascience
[post_id]: 30867
[parent_id]: 
[tags]: 
Machine learning with stochastic labels

I'm sure this is a standard problem in machine learning so links to pages or books that discuss the problem are also welcome. The problem is: We have some input data points, $x_i$, and some output labels, $y_i$, that are drawn from a probability distribution whose parameters are somehow encoded in the input data. Now we want to learn a model, $f(x_i, \theta): x_i \rightarrow \lambda_i$ with some fit variables $\theta$, that outputs $\lambda_i$, which are the parameters of the distribution where $y_i$ are drawn from. The type of the distribution (e.g. normal, exponential, etc.) and number of its parameters is known. My first idea would be to draw a new random variable from the distribution produced by $f$ for each sample, do this many times in batches and for each batch minimize $\sum_i (y^*_i - y_i)^2$. I can't really prove it, but my intuition tells me that this way the distribution parameters $\lambda_i$ should converge to those of the actual distribution.
