[site]: crossvalidated
[post_id]: 283451
[parent_id]: 237158
[tags]: 
Well here we don't take the errors in encoder time steps. What we do is we transform all the time step information in to a vector representation where it can help to generate correct labels in encoder side. The gradient method is simple. It's same as how we take gradients in standard RNN. But here we start to propagate gradients from decoder side only. The method is called the back propagation through time. You can understand that clearly in Chis Olah's amazing blog . Also read about the this amazing article on seq2seq model by google
