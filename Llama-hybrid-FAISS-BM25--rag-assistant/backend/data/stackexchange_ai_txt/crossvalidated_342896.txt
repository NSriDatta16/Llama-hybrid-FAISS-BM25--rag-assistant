[site]: crossvalidated
[post_id]: 342896
[parent_id]: 
[tags]: 
Hyperparameter tuning in multiclass classification problem: scoring metric relevant?

I'm working with an imbalanced multi class dataset. I try to tune the parameters of a DecisionTreeClassifier , RandomForestClassifier and a GradientBoostingClassifier using a randomized search and a bayesian search. For now I used just accuracy for the scoring which is not really applicable for assessing my models performance (which I'm not doing). Is it also not suitable for the parameter tuning? I found that for example recall_micro yields the same results as accuracy . This should be the same for other metrics like f1_micro . So my question is : Is the scoring relevant? Can a different metric lead to different results? If yes, which metric should I use?
