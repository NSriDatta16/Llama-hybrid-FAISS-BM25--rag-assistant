[site]: datascience
[post_id]: 53504
[parent_id]: 53488
[tags]: 
The deep feedforward neural networks used for regression are nothing but multilayer perceptron architectures. Originally, perceptrons were used as binary classifiers i.e to classify binary labels ( 0 or 1 ). But, if no non-linear activation function is applied to the dot product of the features and weights, then it is simply a linear regressor. If the linear function is $f(x) = x$ and $N$ is the number of features then, $\Large y = \sum_{i=0}^N x_i w_i$ or using vector notation, $\Large y = \vec{x}.\vec{w} ...(1)$ For multiple regression, we use the below equation with mean-squared error loss function to optimize the $\beta$ parameters, $\Large y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 ... \beta_n X_n$ Let , $\Large \vec{W} = [ \beta_1 \ \beta_2 \ \beta_3 \ ... \ \beta_n ] \\ \vec{X} = [ X_1 \ X_2 \ X_3 \ ... \ X_n ] $ Where $N$ is the number of features, $\vec{X}$ is the feature vector and $\vec{W}$ is the weights vector. Then, $\Large y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 ... \beta_n X_n = \beta_0 + \vec{W}.\vec{X} ...(2) $ Where, $\beta_0$ is also called as the bias coefficient in Artificial Neural Networks. You can relate equations 1 and 2 and understand the concept. Hence, $\Large y = \vec{x}.\vec{w} + bias $ generally represents a hyperplane which is used in linear regression.
