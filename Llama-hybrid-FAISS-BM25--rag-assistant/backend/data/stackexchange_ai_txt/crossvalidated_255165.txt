[site]: crossvalidated
[post_id]: 255165
[parent_id]: 
[tags]: 
Computing Training Set Perplexity of a Neural Language Model: Too low values

I am implementing a Language Model based on a Deep Learning architecture (RNN+Softmax). The cost function I am using is the cross-entropy between the vector of probabilities at the softmax layer and the one-hot vector of the target word to predict. For every epoch, I am computing the perplexity as: where is the number of batches per-epoch. Knowing that The problem here that after a given number of epochs the total cross-entropy per-epoch starts dropping and dividing it by the number of batches per-epoch will lead to very low perplexity values with respect to state of the art perplexity values on the same Corpus I am using in my experiments. Am I computing the training set perplexity in a wrong way ? Any kind of help is hugely appreciated.
