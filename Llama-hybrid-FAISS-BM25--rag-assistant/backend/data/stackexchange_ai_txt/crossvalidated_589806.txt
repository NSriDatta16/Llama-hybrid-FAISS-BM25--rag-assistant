[site]: crossvalidated
[post_id]: 589806
[parent_id]: 589327
[tags]: 
The Expectation-Maximization algorithm is an optimization procedure that can be used to infer the parameters of a model, when observations depend on hidden latent variables that are not observed. For instance, if you have $T$ samples from a Weibull distribution (from which you want to infer the values of the parameters $\alpha$ and $\theta$ ), it would not make sense to use the EM algorithm, since observations do not depend on latent variables. A classical optimization algorithm (based either on the likelihood or on the profile likelihood) would be much simpler. On the other hand, if you have a Hidden Markov Model with hidden variable $x_t = f(x_{t-1})$ and where the observation is e.g. $y_t = x_t \cdot w$ (where $w$ is a random variable drawn from a Weibull distribution), then the EM is relevant to infer its $\alpha$ and $\theta$ . However, you are absolutely right to say that the EM algorithm can be interpreted as a form of coordinated ascent. It can be " seen as maximizing a joint function of the parameters and of the distribution over the unobserved variables [...] The E step maximizes this function with respect to the distribution over unobserved variables; the M step with respect to the parameters ". This is detailed in the following seminal paper: Neal, R. M., & Hinton, G. E. (1998). A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in graphical models (pp. 355-368). Springer, Dordrecht. Regarding your questions: No, since the likelihood and the profile likelihoods are different functions. As you explained, in profile likelihood optimization, you set the value of one the parameters, use it to optimize the other parameter, and then go back to the first parameter. Which is not the same as solving $\nabla_{\alpha,\theta} \mathcal{L}(\alpha,\theta) = 0$ for $\alpha$ and $\theta$ . However, there is no formal and general theoretical link between the results of likelihood optimization and profile likelihood optimization. How they differ (and how they differ from the ground-truth parameters that we are trying to infer) will be a function of the model, of the value of the ground-truth parameters, and of the number of observations. For instance, the classical way to compute the MLE for the parameters of a normal distribution $\mu$ and $\sigma$ implies to first compute the estimate of the mean $\hat{\mu}$ and then to use it to infer $\hat{\sigma}$ . This can be seen as a form of profile likelihood optimization. For a Weibull distribution, the effect of the profile likelihood approximation might depend on $T$ ; intuitively, for a very small number of observations, both methods should give results that are equally bad. In the absence of theoretical results, the best way to quantify the difference between likelihood and profile likelihood approaches is to use MCMC simulations (i.e. for a given $T$ and ground-truth $\alpha^*$ and $\theta^*$ , generate several independent sets of $T$ samples, and apply both methods to each of them to systematically compare their results).
