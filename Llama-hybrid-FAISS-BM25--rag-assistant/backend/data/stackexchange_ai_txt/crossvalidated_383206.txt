[site]: crossvalidated
[post_id]: 383206
[parent_id]: 101351
[tags]: 
So, would this imply that Max Entropy = minimum information? No, max entropy means the event you have observed conveys the max information. If an event happens with equal probability of each possibility you have no any idear what to expect, then the outcome of an event tells you the most information. But when you are very certain for something the outcome if it can only contain very limited information. And the average information you get from all the events is what the entropy measures.
