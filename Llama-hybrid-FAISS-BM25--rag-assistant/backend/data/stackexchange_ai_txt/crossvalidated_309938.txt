[site]: crossvalidated
[post_id]: 309938
[parent_id]: 309928
[tags]: 
I'd recommend getting an overview of the math that's currently used in deep learning architectures that are used for supervised settings (this does mean looking into approaches that involve "training sets"), before you dive deeper into other math. http://www.deeplearningbook.org/ has a very good overview of the math you'd need to understand what's going on in neural nets/deep nets. Once you're comfortable with the current approaches, you'd be able to understand the research in the field, and the directions it's heading in. (from ICML, NIPS papers, for instance) At that point, you will likely find open problems that seem to interest you, and you can begin to actively work on them. It's often useful to have a problem you want to solve in mind, and then explore all the work that's been done on that problem (prior approaches, the math involved, etc) - Sometimes, you'll find that there are some problems that interest you deeply, but the current approaches to solve them are unsatisfying - this is really the point when you might have to invent(discover?) the math needed to solve it, or "borrow" the math from a different field. The main benefit you'll have if you work on problems that are similar to what other researchers are interested in is that there's a community that's publishing work at a breakneck pace, and you'll be able to quickly get feedback on approaches that have been tried and haven't quite worked just yet. I'm not quite discounting the value of learning math by itself, but just saying that if you learn the math in the light of a problem (or ten), you'll learn how to apply the existing math well (+ how to do a good literature search), and you'll also learn to figure out when new math is required.
