[site]: crossvalidated
[post_id]: 380001
[parent_id]: 2213
[tags]: 
What is really interesting in asking this question? Instead of saying RNN and FNN is different in their name. So they are different. , I think what is more interesting is in terms of modeling dynamical system, does RNN differ much from FNN? Background There has been a debate for modeling dynamical system between Recurrent neural network and Feedforward neural network with additional features as previous time delays (FNN-TD). From my knowledge after reading those papers on 90's~2010's. The majority of the literature prefer that vanilla RNN is better than FNN in that RNN uses a dynamic memory while FNN-TD is a static memory. However, there isn't much numerical studies comparing those two. The one [1]on the early showed that for modeling dynamical system, FNN-TD shows comparable performance to vanilla RNN when it is noise-free while perform a bit worse when there is noise. In my experiences on modeling dynamical systems, I often see FNN-TD is good enough. What is the key difference in how to treat memory effects between RNN and FNN-TD? Unfortunately, I don't see anywhere and any publication theoretically showed the difference between these two. It is quite interesting. Let's consider a simple case, using a scalar sequence $X_n, X_{n-1},\ldots,X_{n-k}$ to predict $X_{n+1}$ . So it is a sequence-to-scalar task. FNN-TD is the most general, comprehensive way to treating the so called memory effects . Since it is brutal, it covers any kind, any sort, any memory effect theoretically. The only down side is that it just takes too much parameters in practice. The memory in RNN is nothing but represented as a general "convolution" of the previous information . We all know that convolution between two scalar sequence in general is not an reversible process and deconvolution is most often ill-posed. My conjecture is "degree of freedom" in such convolution process is determined by the number of hidden units in the RNN state $s$ . And it is important for some dynamical systems. Note that the "degree of freedom" can be extended by time delay embedding of states [2] while keeping the same number of hidden units. Therefore, RNN is actually compressing the previous memory information with loss by doing convolution, while FNN-TD is just exposing them in a sense with no loss of memory information. Note that you can reduce the information loss in convolution by increasing the number of hidden units or using more time delays than vanilla RNN. In this sense, RNN is more flexible than FNN-TD. RNN can achieve no memory loss as FNN-TD and it can be trivial to show the number of parameters are on the same order. I know someone might want to mention that RNN is carrying the long time effect while FNN-TD can not. For this, I just want to mention that for a continuous autonomous dynamical system, from Takens embedding theory it is a generic property for the embedding to exists for FNN-TD with the seemingly short time memory to achieve the same performance as the seemingly long time memory in RNN. It explains why RNN and FNN-TD does not differ a lot in continuous dynamical system example in the early 90's. Now I will mention the benefit of RNN. For the task of autonomous dynamical system, using more previous term, although effectively would be the same as using FNN-TD with less previous terms in theory, numerically it would be helpful in that it is more robust to noise. Result in [1] is consistent with this opinion. Reference [1]Gen√ßay, Ramazan, and Tung Liu. "Nonlinear modelling and prediction with feedforward and recurrent networks." Physica D: Nonlinear Phenomena 108.1-2 (1997): 119-134. [2]Pan, Shaowu, and Karthik Duraisamy. "Data-driven Discovery of Closure Models." arXiv preprint arXiv:1803.09318 (2018).
