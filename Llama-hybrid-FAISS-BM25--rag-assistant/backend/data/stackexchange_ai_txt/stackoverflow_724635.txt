[site]: stackoverflow
[post_id]: 724635
[parent_id]: 724499
[tags]: 
From the point of view of having had my work machines unexpectedly powered down last weekend (construction elsewhere in the building) I sympathise with the idea. Is there any value in partitioning the task? Could the input file be reworked into many smaller ones? Orders of magnitude smaller, I know, but I have a process that loads about 2 million rows across a few AR models each morning. To get around the appalling database latency issues that I suffer from (DB server in a different country - don't ask) I rewrite my input CSV files into 16 "fragments" each. Each fragment is recorded in the Fragment model, which helps me identify any completion failures for re-run. It works surprisingly well and restarts, when needed, are simple. Usual run time about 30 minutes. If your XML input is reasonably well-structured, it should be fairly straightforward to extract sub-structures (I'm sure there's a better term than that) into separate files. I don't know how fast a SAX parser would be able to do this - probably not too horrific, but it could be done without an XML library at all if it was still too slow. Consider adding a column to the target model to identify the fragment that it was loaded from - that way stripping out incomplete runs is simple. Beyond that, consider holding all the state in one class and using Marshal to save periodically?
