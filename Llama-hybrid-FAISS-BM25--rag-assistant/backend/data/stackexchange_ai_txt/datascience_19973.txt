[site]: datascience
[post_id]: 19973
[parent_id]: 19971
[tags]: 
The issue with linear regression in your case is not whether it can be used - it can nearly always be used to build a predictive regression model - it is whether the resulting model is useful, usually whether it is accurate enough for the intended use. The accuracy and utility of your model does not depend only on the nature of $\mathbf{x}$. Instead it depends on the true nature of the $\mathbf{x} \rightarrow y$ relationship. Your measurements and model are an approximation of that relationship. Linear models are fast and stable to compute, but can be limited if the true relationship being approximated is non-linear. Here are some basic thoughts/feedback on your questions: Can Linear Regression work for such datasets? Yes it can work. Will it be good enough? You will know after testing. What are other models I can use? Almost any regression model class could be applied to this problem. For a quick comparison, to see if a non-linear model will make more accurate predictions for you, then you could try an easy-to-apply model from an existing library, such as XGBoost (which has a stand-alone command-line version). Should I drop one feature (some $x_{i,d}$) from each data record as I can easily calculate the value of dropped feature by using 1 - summing prob(remainingfeatures). For a linear model, your intuition is correct that one feature is redundant, as it is itself a linear combination of the other features, so there is no impact to removing it. You could remove any single column. You should not need to though, and some model classes might work better if left in. As always, experiment if you are not sure, and use hold-out test data to verify your ideas (or even better a separate cross-validation set before final test stage) As I use Linear Regression all independent variables ($x_{i,0}$, $x_{i,d}$)'s are For linear regression this scaling is not very important. However, there are a few caveats: If you measure success by an error metric (such as mean square error), then to compare different ideas fairly, you should report the test error in the same way - e.g. you should scale back the $log(y)$-based predictions and calculate error on the same $(y - \hat{y})^2$ each time. For some model classes, they could work better if the target variable is within a certain range. You should check documentation for this. How useful the mapping is depends on the true nature of the relationship between $\mathbf{x}$ and $y$. Specifically for a linear model, you have completely changed the line you are trying to fit - it could as a result be a lot better or lot worse, but that is not dependent just on $\mathbf{x}$. So again you will have to give it a try and test the results.
