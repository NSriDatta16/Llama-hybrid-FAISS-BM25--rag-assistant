[site]: crossvalidated
[post_id]: 221787
[parent_id]: 221715
[tags]: 
If you are working with English text and want pre-trained word embeddings to begin with, then please see this: https://code.google.com/archive/p/word2vec/ This is the original C version of word2vec. Along with this release, they also released a model trained on 100 billion words taken from Google News articles (see subsection titled: "Pre-trained word and phrase vectors"). In my opinion and experience of working on word embeddings, for document classification, a model like doc2vec (with CBOW) works much better than bag of words. Since, you have a small corpus, I suggest, you initialize your word embedding matrix by the pre-trained embeddings mentioned above. Then train for the paragraph vector in the doc2vec code. If you are comfortable with python, you can checkout the gensim version of it, which is very easy to modify. Also check this paper that details the inner workings of word2vec/doc2vec: http://arxiv.org/abs/1411.2738 . This will make understanding the gensim code very easy.
