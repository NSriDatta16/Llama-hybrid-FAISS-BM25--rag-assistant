[site]: datascience
[post_id]: 63918
[parent_id]: 
[tags]: 
Customize loss function for Music Generation LSTM (?)

I have to carry out a Music Generation project for a Deep Learning course I have this semester and I am using Pytorch. The dataset is songs in midi format and I use the python library mido to extract the data out of every song. The data in every midi song are organized as a series of discrete event messages. I plan to have my input of the LSTM for each time-step as a vectorized event of three variables: x_t = [note, velocity, timestamp] note : categorical variable, since it can take only one integer value in the range [0:127] which corresponds to a pitch. velocity : I think it is categorical too, as it also takes one integer value in the range [0:127] which corresponds to the force with which the note is played. timestamp : the continuous variable that corresponds to the number of seconds (mostly milliseconds) or ticks that have passed between the current and previous events. My question is since I have a combination of categorical and continuous variables to predict: Is it ok to one-hot-encode note and velocity and keep timestamp as is? Should I further standardize timestamp? Can I use a customized loss function (sum of cross-entropy loss for note/velocity plus mean squared error for timestamp), or it would mess up my model? Please, I feel a bit lost and need your help/suggestions. Thank you!!!
