[site]: crossvalidated
[post_id]: 458145
[parent_id]: 
[tags]: 
Reinforcement Learning: In TD learning, why Forward equals to Backward Views

I am following this tutorial and try to understand why in TD( $\lambda $ ) learning, the forward and backward view equals to each other. I got stuck at the following equation: I understand how 7.9 gets into 7.10 but not the rest of the equations. It might be more of an algebra question but it bothers me too much not understanding the bath behind the scene. Any help would be appreciated!
