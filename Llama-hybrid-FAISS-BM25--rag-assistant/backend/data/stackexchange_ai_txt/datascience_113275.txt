[site]: datascience
[post_id]: 113275
[parent_id]: 
[tags]: 
How to train models when the goal is to maximize an objective function on only the unknown part of the features space with a high score?

We have input features and features space - X and output target - Y (continous or discrete). The connection between X and Y inside small part of features space (A) is stronger than inside rest of features space (B). For example, if we already know how to divide features space on A and B, we can train regression or classifier model on A and on B. For example if it is a regression, the MSE may be equal 0.1 on A and 1.1 on B. If it is a classifier, the accuracy may be equal 0.8 on A and 0.51 on B. Our objective is to maximize the score only on A and not on the whole features space. Our task is to learn how to divide the features space into A and B and train the final model only on A. I can invent a lot of "DIY" methods for solving this problem, but is there a mainstream approach and may be ready to use code or libraries? Of course A - is enough big to get good statistical quality of a model training only on it, but enough small to spoil the quality of the model if we train it on a whole features space. A real life example - is stocks trading. We have two features X1 and X2 for all stocks of the exchange. And the target - Y - stocks returns on the next day. We train a model on the whole data set, and make forecasts on the next day to buy top N stocks with highest forecast. But when we train a model only on samples inside i-th quantile (for example from 0.9 to 1.0) of the X1 and select top N stocks each day, we get much better results. I cannot find any generic machine learning approach for model construction or samples filtration to get better results than this manual samples filtration by quantile of X1 input.
