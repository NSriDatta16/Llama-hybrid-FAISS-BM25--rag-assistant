[site]: crossvalidated
[post_id]: 369611
[parent_id]: 
[tags]: 
Can an optimal weighted average ever have negative weights?

I've got a few measurements $\vec{x}$ for some real-world value $\hat{x}$ . These measurements have some uncertainty, and are correlated. Given these estimates, and their covariances, I want to take some kind of weighted average of the estimates $$\bar{x} = \sum_i w_i x_i = \vec{w} \cdot \vec{x}$$ Naturally, I want my weights to sum up to 1: $$\sum_i w_i = 1 = \vec{w} \cdot \vec{1}$$ I want to choose my weights such that my uncertainty around this estimate is as low as possible, so I want to minimize: $$\mathbb{E} \left[ \left( \bar{x} - \hat{x} \right)^2 \right] = \sum_i \sum_j w_i w_j C_{i,j} = \vec{w}^T C \vec{w}$$ where $C$ is the covariance matrix of the errors of my measurements: $C_{i,j}=\mathbb{E} \left[ x_i - \hat{x}, x_j - \hat{x} \right]$ . Minimizing this under the constraint that the weights sum up to 1 can be done using a Lagrange multiplier, and (assuming I did everything right) results in: $$ C \cdot \vec{w^*} = \vec{1}$$ $$ \vec{w} = \vec{w^*} / \sum \vec{w^*} $$ Note that the latter step is just rescaling $\vec{w^*}$ to sum up to 1, so I'm mostly interested in the solution of $\vec{w^*}$ : $$ \vec{w^*} = C^{-1} \cdot \vec{1}$$ This procedure results in negative weights. This doesn't make sense for me intuitively. (E.g. if I have just one estimate $x_1$ of 100, then obviously my total estimate $\bar{x}$ will also be 100. How could the introduction of another measurement, of 110, make it so my overall estimate $\bar{x}$ lowers to below 100?) So I suspect that I'm doing something wrong, like using an inconsistent covariance matrix. But is it definite that I am doing something wrong? Can the above procedure reasonably return negative weights? How would this make sense in terms of the actual implications of what I'm trying to do (take a weighted average of multiple estimates)?
