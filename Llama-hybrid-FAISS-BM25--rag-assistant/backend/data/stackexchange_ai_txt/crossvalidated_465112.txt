[site]: crossvalidated
[post_id]: 465112
[parent_id]: 
[tags]: 
Why is the outer loop of nested cross validation needed?

I have read several threads and articles about the need to use nested cross validation when, for example, choosing between different machine learning algorithms. Here are two describing the use of nested cross validation in algorithm selection: https://sebastianraschka.com/faq/docs/evaluate-a-model.html , https://towardsdatascience.com/a-short-introduction-to-model-selection-bb1bb9c73376 . I am aware that the process of nested cross validation in selecting between, for example different algorithms, is as follows (for the case of k fold CV): The inner loop CV uses the 'outer loop' training sets, to find the optimal hyperparameter. The optimal hyperparameter is chosen based on its test performance on the 'inner loop' test folds. Here we loop over a selection of different hyperparamter values. If we performed 5 fold CV in the inner loop, then each hyperparameter's 'score' is the average of the scores from testing it on the 5 'inner loop test' folds. The 'inner loop' gives us our best hyperparameter. We then train a model on the 'outer loop' training data (which we earlier split inside the 'inner loop') and calculate its test performance on the 'outer loop' test fold. This entire process is repeated for as many 'outer loop' folds we are using. So, if we were doing 5 fold 'outer loop' CV, we could (depending on model stability) end up with 5 different hyper parameters. Once this is repeated for these 'outer loop' folds, we take the average of all the 'outer loop' test fold scores to get the overall test performance of that particular algorithm. This is repeated for all the algorithms we are comparing. The one with the best score is taken forward. I know the purpose of separating the model selection (done in the inner loop) and performance estimation (done on the outer loop) is to avoid biasing the results somehow, but i don't understand why. I have looked at several threads on here but cant get my head around it (e.g. Feature selection and cross-validation , Training with the full dataset after cross-validation? ) Take the following example of where normal (say 5 fold) cross-validation is used. This would involve the following steps: Split data into 5 folds Select the first algorithm to test. Take a given value form the selection of hyperparameter values we wish to test, and train it on the 4 training folds. Calculate a test score using the remaining test fold. Repeat for all 5 folds. Average all 5 test scores to get the average test score for that hyperparameter. Repeat this for all values of the hyperparameter. The one with the best test score is carried forward. Repeat this for all the algorithms we wish to test. Compare the average test scores for each algorithm and the one with the best test score gets carried forward. I can't seem to understand why the above method, which doesn't require nested cross validation is not sufficient? I guess the question is, why do we need to separate the model selection and performance estimation steps into the inner and outer loops? I can't see the benefits of this compared to doing both at the same time, which is what the second set of steps described does. UPDATE I've come across a video which explains the reasoning very well: https://www.youtube.com/watch?v=DuDtXtKNpZs
