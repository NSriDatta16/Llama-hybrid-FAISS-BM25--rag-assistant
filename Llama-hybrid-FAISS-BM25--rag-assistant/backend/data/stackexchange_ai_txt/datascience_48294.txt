[site]: datascience
[post_id]: 48294
[parent_id]: 
[tags]: 
Regression vs Random Forest - Combination of features

I had a discussion with a friend and we were talking about the advantages of random forest over linear regression. At some point, my friend said that one of the advantages of the random forest over the linear regression is that it takes automatically into account the combination of features. By this he meant that if I have a model with Y as a target X, W, Z as the predictors then the random forests tests also the combinations of the features (e.g. X+W) whereas in linear regression you have to build these manually and insert them at the model. I am quite confused, is this true? Also if it true then is it about any kind of combination of features (e.g. X*W, X+W+Z etc) or only for some specific ones (e.g. X+W)?
