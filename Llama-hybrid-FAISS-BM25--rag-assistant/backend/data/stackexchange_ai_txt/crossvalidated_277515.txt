[site]: crossvalidated
[post_id]: 277515
[parent_id]: 
[tags]: 
Truncated backpropagation for text

I know truncated backprop is useful for training RNNs like LSTM. But in some datasets of corpus, there are many long sequences with thousands of tokens. However the average length of tokens is just some hundred numbers. If I only backprop 400 or 500 words for training, how do I deal with those long sequences? Only backprop the previous 400 words from the end token? I dont think it could well capture the semantic meaning of the training examples.
