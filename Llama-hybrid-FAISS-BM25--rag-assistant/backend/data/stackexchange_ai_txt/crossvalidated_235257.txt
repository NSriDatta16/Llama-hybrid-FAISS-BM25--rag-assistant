[site]: crossvalidated
[post_id]: 235257
[parent_id]: 235254
[tags]: 
You're interpreting PCA incorrectly, I believe. The output of PCA is an orthogonal basis set of all the data observations, where the first principal component contributes the most variance, the second PC the second most, and so on so forth. However, I'll take 'finding the column vectors most correlated with each PC' to mean 'I want to reduce my dimensionality/number of features. Selecting the variables most correlated with the PCs will not do this. But, there is a way! You can transform your observation matrix to the new basis found by your PCA, achieving just this. This is done by $T_{L} = X~W_{L}$, where $X$ is your observation matrix, $W$ is the loading matrix, $T$ is the transformed matrix, and $L$ is the number of principal components you want to keep. So, if you let $L=225$ (the maximum L, there cannot be more PCs than the original number of features), then nothing has changed. But, if you let $L=50$, then you're effectively projecting your data onto the 50th-dimension hyperplane, in the new principal component feature space. This is key! You are not SELECTING 50 features, but projecting your data onto the 50 most important PCs. So, your features will be different from the original ones, but they will be 50 features that are linear combinations (/projections) of the old ones. Of course, there's also the fact that you have no idea if your dataset is linearly separable; you might want to check that out first before pursuing PCA.
