[site]: datascience
[post_id]: 53999
[parent_id]: 
[tags]: 
wavenet structure explanation

I am a beginner in deep learning and recently I am trying to understand the structure of Wavenet. (for more information, please refer to the paper http://sergeiturukin.com/2017/03/02/wavenet.html ) Nevertheless, I had some questions over the overall architecture. As can be seen from the graph, the residual blocks are stacked together, and my first question is, why are they stacked together? Moreover, I am not so sure how the skip-connection works. From my understanding, in a very deep neural network, we use skip-connection to skip several layers to directly add to the result several layers later. In this case, we can see several layers skip-connects to the part that processes the result in order to produce the output. Does this imply that, without skip-connections, we have to go from the bottom layer of the neural network to the top layer in order to produce the output? If not, how does the skip-connection work in this case? Finally, I had a question about the top arrow in the graph (around the "Residual"). I am confused by what this arrow is trying to do, and how does it contribute to the overall process? Thanks in advance!
