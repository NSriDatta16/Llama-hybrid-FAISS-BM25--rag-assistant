[site]: crossvalidated
[post_id]: 523788
[parent_id]: 
[tags]: 
Confusion about maximum likelihood estimation notation

I'm trying to learn some machine learning theory, in particular maximum likelihood estimation . Let me preface my question by saying that I'm very familiar with the Naive Bayes classifier, Bayes' Rule, and related notation. Below, I understand that $y$ is a class label and $X$ is a feature representation. Now, my confusion stems from two different definitions of maximum likelihood estimation. Oddly, they are by the same author, Kevin P. Murphy. Definition 1 Here's what he wrote in his 2012 book " Machine Learning: a Probabilistic Perspective ", where he first describes $D$ (a training set of ( $X, y$ ) pairs), and $H$ , a hypothesis space, which I didn't understand. I was thinking that a hypothesis is a class label. He then describes maximum likelihood estimation to be: $$ \hat{h}^{MLE} \triangleq \underset{h}{\mathrm{argmax}}\, P(D|h) $$ Definition 2 Now this is what he wrote in his upcoming 2021 book " Probabilistic Machine Learning: An Introduction ". Here, MLE is applied to get the parameters $\theta$ that satisfy $\underset{\theta}{\mathrm{argmax}}\, P(D|\theta)$ . Specific questions Why are there two definitions of MLE, one to satisfy $\underset{h}{\mathrm{argmax}}\, P(D|h)$ and another to satisfy $\underset{\theta}{\mathrm{argmax}}\, P(D|\theta)$ ? Is "hypothesis" a class label or a learnable parameter in these contexts? In Bayes' Rule, we see the likelihood term as $P(X|y)$ with class label $y$ . How does it relate to Definition 2, where we see the likelihood term $ P(D|\theta)$ with parameters $\theta$ instead of label $y$ ?
