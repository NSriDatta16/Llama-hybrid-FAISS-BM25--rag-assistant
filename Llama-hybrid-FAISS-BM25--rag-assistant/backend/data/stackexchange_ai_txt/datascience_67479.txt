[site]: datascience
[post_id]: 67479
[parent_id]: 
[tags]: 
How to calculate joint feature contribution for XGBoost Classifier in python?

I referred to this beautiful document to research about joint feature contibutions. But this works only for RandomForest algorithms because of treeinterpreter (does not work with xgboost ). Is there a similar way out for XGBoost as well? Basically what I want to achieve is to find out the joint contributions of all the combination of features towards the prediction. For example if I have a , b and c as my features, I want to know what is the effect of ab , bc and ca towards the prediction result. It is very similar to shap and lime , but for combinations of features.
