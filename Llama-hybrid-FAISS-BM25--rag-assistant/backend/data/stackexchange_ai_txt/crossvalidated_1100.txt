[site]: crossvalidated
[post_id]: 1100
[parent_id]: 886
[tags]: 
I will give an itemized answer. Can provide more citations on demand, although this is not really controversial. Statistics is not all about maximizing (log)-likelihood. That's anathema to principled bayesians who just update their posteriors or propagate their beliefs through an appropriate model. A lot of statistics is about loss minimization. And so is a lot of Machine Learning. Empirical loss minimization has a different meaning in ML. For a clear, narrative view, check out Vapnik's "The nature of statistical learning" Machine Learning is not all about loss minimization. First, because there are a lot of bayesians in ML; second, because a number of applications in ML have to do with temporal learning and approximate DP. Sure, there is an objective function, but it has a very different meaning than in "statistical" learning. I don't think there is a gap between the fields, just many different approaches, all overlapping to some degree. I don't feel the need to make them into systematic disciplines with well-defined differences and similarities, and given the speed at which they evolve, I think it's a doomed enterprise anyway.
