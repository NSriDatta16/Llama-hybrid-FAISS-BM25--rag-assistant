[site]: datascience
[post_id]: 106284
[parent_id]: 106177
[tags]: 
In the comments you wrote: I can't exclude it because it is related on an other feature If you mean related as in a function of two other features, you could add columns to explicitly show those relations, then delete the original column. E.g. add a column called petal.width.plus.length and then remove petal.width . Taking a step back, I think your problem could be rephrased as decision trees end up in a local minima, and so fail to find the best solution? If so, consider another algorithm? E.g. random forest, GBMs (e.g. XGBoost) or even deep learning. If other algorithms also don't do what you expect, you should also consider that the data is not saying what you think it is. Maybe you need to collect more data, or oversample some rows, or look for bugs (e.g. some data has turned into NA s). Finally (and I suppose this is the answer to the question you are asking) you can manually partition the data on the field you believe is most important, and then build a decision tree for each partition. As you need to do that each time you want to do predict you definitely want to write your own wrapper class to store the partitions.
