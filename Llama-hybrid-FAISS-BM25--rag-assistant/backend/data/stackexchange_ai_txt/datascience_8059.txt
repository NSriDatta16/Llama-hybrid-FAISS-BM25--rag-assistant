[site]: datascience
[post_id]: 8059
[parent_id]: 8029
[tags]: 
My take: I agree with the issues raised in 1., so not much to add here - retraining and storage is indeed inefficient Vowpal Wabbit http://hunch.net/~vw/ would be my first choice stability of output between increments is really more of a data than algorithm feature - if you have plenty of variation on input, you won't have that much stability on output (at least not by default) hashing can take care of the variation - you can control by a combination of three parameters: the size of the hashing table and the l1/l2 regularization same for the new features / users (I think - most of the applications i used it had a ercord representing a user clicking or not, so new users / ads were sort of treated "the same") normally I use VW from the command line, but an example approach (not too elegant) for controlling from Python is given here: http://fastml.com/how-to-run-external-programs-from-python-and-capture-their-output/ if you prefer sth purely Python, then a version (without decomposition) of an online learner in the Criteo spirit can be found here: https://www.kaggle.com/c/tradeshift-text-classification/forums/t/10537/beat-the-benchmark-with-less-than-400mb-of-memory I am not sure how to handle the concept drift - haven't paid that much attention to it so far beyond rolling statistics: for the relevant variables of interest, keep track of mean / count over recent N periods. It is a crude approach, but does seem to get the job done it terms of capturing lack of "stationarity" helpful trick 1: single pass over data before first run to create per feature dictionary and flag certain values as rare (lump them into a single value) helpful trick 2: ensembling predictions from more than one model (varying interaction order, learning rate)
