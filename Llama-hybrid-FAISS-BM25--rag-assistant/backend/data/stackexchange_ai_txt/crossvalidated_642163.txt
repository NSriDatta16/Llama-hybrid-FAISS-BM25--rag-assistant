[site]: crossvalidated
[post_id]: 642163
[parent_id]: 642158
[tags]: 
The simple reason is that if probabilities are in a chain, for example, for it to snow, one needs both precipitation and low enough temperature, thus the probability of snow ( $p_s$ ) is the probability of precipitation ( $p_p$ ) TIMES the probability of low enough temperature ( $p_t$ ), or $p_s=p_p\cdot p_t$ . Now logarithms transform multiplication into addition such that we can write $\ln p_s=\ln p_p+\ln p_t$ . So we have transformed a product rule into a sum rule. Then, since we have a sum rule, we can find a maximum using least squares regularization, or L $_1$ optimization or whatever. Thus, transforming back into the original equation, we have found the maximum probability product of $p_s=p_p\cdot p_t$ . However, this is not the case for $p_?=p_p+ p_t$ . If we maximize that sum, we will have maximized $p_?$ , where $p_?$ is a sum of probabilities, but is not directly related to probabilities that follow a chain rule, i.e, conditional probabilities that say "How much of this times that times whatever..." So, one might produce answers that set $p_p=0$ and $p_t=\text{Max possible}$ or some such, but one would have to have a very special circumstance for that to have meaning. As I say below in a comment "Perhaps showing the log loss for logistic regression is what you are seeking?"
