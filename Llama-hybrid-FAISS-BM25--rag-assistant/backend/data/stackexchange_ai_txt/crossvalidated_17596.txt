[site]: crossvalidated
[post_id]: 17596
[parent_id]: 17581
[tags]: 
First, lets be explicit and put the question into the context of multiple linear regression where we regress a response variable, $y$, on several different variables $x_1, \ldots, x_p$ (correlated or not), with parameter vector $\beta = (\beta_0, \beta_1, \ldots, \beta_p)$ and regression function $$f(x_1, \ldots, x_p) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p,$$ which could be a model of the mean of the response variable for a given observation of $x_1, \ldots, x_p$. The question is how to select a subset of the $\beta_i$'s to be non-zero, and, in particular, a comparison of significance testing versus cross validation . To be crystal clear about the terminology, significance testing is a general concept, which is carried out differently in different contexts. It depends, for instance, on the choice of a test statistic. Cross validation is really an algorithm for estimation of the expected generalization error , which is the important general concept, and which depends on the choice of a loss function. The expected generalization error is a little technical to define formally, but in words it is the expected loss of a fitted model when used for prediction on an independent data set , where expectation is over the data used for the estimation as well as the independent data set used for prediction. To make a reasonable comparison lets focus on whether $\beta_1$ could be taken equal to 0 or not. For significance testing of the null hypothesis that $\beta_1 = 0$ the main procedure is to compute a $p$-value, which is the probability that the chosen test-statistic is larger than observed for our data set under the null hypothesis , that is, when assuming that $\beta_1 = 0$. The interpretation is that a small $p$-value is evidence against the null hypothesis. There are commonly used rules for what "small" means in an absolute sense such as the famous 0.05 or 0.01 significance levels. For the expected generalization error we compute, perhaps using cross-validation, an estimate of the expected generalization error under the assumption that $\beta_1 = 0$. This quantity tells us how well models fitted by the method we use, and with $\beta_1 = 0$, will perform on average when used for prediction on independent data. A large expected generalization error is bad, but there are no rules in terms of its absolute value on how large it needs to be to be bad. We will have to estimate the expected generalization error for the model where $\beta_1$ is allowed to be different from 0 as well, and then we can compare the two estimated errors. Whichever is the smallest corresponds to the model we choose. Using significance testing we are not directly concerned with the "performance" of the model under the null hypothesis versus other models, but we are concerned with documenting that the null is wrong. This makes most sense (to me) in a confirmatory setup where the main objective is to confirm and document an a priory well specified scientific hypothesis, which can be formulated as $\beta_1 \neq 0$. The expected generalization error is, on the other hand, only concerned with average "performance" in terms of expected prediction loss, and concluding that it is best to allow $\beta_1$ to be different from 0 in terms of prediction is not an attempt to document that $\beta_1$ is "really" different from 0 $-$ whatever that means. I have personally never worked on a problem where I formally needed significance testing, yet $p$-values find their way into my work and do provide sensible guides and first impressions for variable selection. I am, however, mostly using penalization methods like lasso in combination with the generalization error for any formal model selection, and I am slowly trying to suppress my inclination to even compute $p$-values. For exploratory analysis I see no argument in favor of significance testing and $p$-values, and I will definitely recommend focusing on a concept like expected generalization error for variable selection. In other contexts where one might consider using a $p$-value for documenting that $\beta_1$ is not 0, I would say that it is almost always a better idea to report an estimate of $\beta_1$ and a confidence interval instead.
