[site]: datascience
[post_id]: 32728
[parent_id]: 
[tags]: 
LSTM/RNN model fails on new test data - TFLearn

i'm fairly new to ML and at the moment i'm trying to develop a model that can classify spoken digits (0-9) by extracting mfcc features from audio files. I trained the model on a data set that consists of 15 speakers and 2400 training examples (240 audio examples for each digit). After 3000 epochs the model has achieved 97% accuracy. The problem is, when i record my own digit wavs, the model fails to classify them correctly. Why is this happening and what could i do to fix this issue? Is this an example of overfitting ? Extracting mfcc features wave, sr = librosa.load(wav_file, mono=True) mfcc = librosa.feature.mfcc(wave, sr) mfcc = np.pad(mfcc,((0,0),(0,80-len(mfcc[0]))), mode='constant', constant_values=0) Parameters learning_rate = 0.0001 training_iters = 150 batch_size = 64 width = 20 # mfcc features height = 80 # (max) length of utterance classes = 10 # digits Network building net = tflearn.input_data([None, width, height]) net = tflearn.lstm(net, 128, dropout=0.8) net = tflearn.fully_connected(net, classes, activation='softmax') net = tflearn.regression(net, optimizer='adam', learning_rate=learning_rate, loss='categorical_crossentropy') Training model = tflearn.DNN(net, tensorboard_verbose=0) EPOCHS = 20 for i in range(training_iters): model.fit(X_train, y_train, n_epoch=EPOCHS, validation_set=(X_val, y_val), show_metric=True, batch_size=batch_size)
