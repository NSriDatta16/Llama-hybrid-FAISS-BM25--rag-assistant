[site]: crossvalidated
[post_id]: 637798
[parent_id]: 
[tags]: 
Why the standard deviation of the BERT weight initialization is 0.02 by default

The purpose of weight initialization in the neural network is to keep the variance of calculation output in the layers to 1.0, and it depends on the calculations involved in the layers. Initializing weight W with Xavier initialization for Matrix Multiplication X@W.T at Self Attention in Transformer Architecture will use the standard deviation $\frac{1}{\sqrt{D}}$ to sample values from $N(\mu=0,\sigma=\frac{1}{\sqrt{D}})$ so that the product has variance 1.0 , providing the dimensions of X and W are both D and X follows the normal distribution. The dimension D of Transformer based BERT is 768 , so $\sigma$ is expected to be 0.036 . But BertConfig says it is using 0.02 . Where is 0.02 coming from? BertConfig initializer_range (float, optional, defaults to 0.02 ) â€” The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
