[site]: crossvalidated
[post_id]: 373181
[parent_id]: 373115
[tags]: 
A couple of points: When you want to work with missing not at random (MNAR) models, it is useful to use the notation $y_{ij}^o$ and $y_{ij}^m$ to denote the observed and missing components of the complete response outcome $y_{ij}$ . You wrote that you want to work with pattern mixture models, in which indeed the decomposition of the joint distribution of the outcome and missingness process is $$p(Y, M \mid X; \eta, \theta) = p(Y \mid M, X; \theta) \, p(M \mid X; \eta).$$ However, this is in contrast to the logistic regression you wrote, because there you postulate that the probability of a specific response $y_{ij}$ being missing depends on $y_{ij}$ . This means that this logistic regression model specifies the distribution $p(M \mid Y, X; \eta)$ , which is what you do in selection models not pattern mixture models. Pattern mixture models and selection models are more easy to conceptualize when you have dropout, and not general intermittent missing data. In pattern mixture models, you need to specify a model for the complete response vector $\{y_i^o, y_i^m\}$ per dropout pattern. In each pattern, you can only identify from the observed data the marginal distribution of $y_i^o$ . The conditional distribution $[y_i^m \mid y_i^o]$ is unidentifiable from the data, and you only proceed by making specific assumptions for this distribution. A general principle is that these assumptions should only define $[y_i^m \mid y_i^o]$ , but they should not affect the distribution $[y_i^o]$ . Even though there is some debate on whether this should always be the case. In selection models, the likelihood is: $$\ell(\theta, \eta) = \prod_{i = 1}^n \int p(y_i^o, y_i^m; \theta) \; p(M_i \mid y_i^o, y_i^m; \eta) \; dy_i^m.$$
