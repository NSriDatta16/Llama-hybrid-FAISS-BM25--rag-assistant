[site]: crossvalidated
[post_id]: 476293
[parent_id]: 476252
[tags]: 
There are several algorithms that are incremental like what you ask for, most of them are more or less based on stochastic gradient descent (SGD) (this is also the principle behind the use of batches in neural networks). Primarily SGD works on one example at a time but you can also use it for one batch at a time. SGD is an optimization algorithm that minimizes some loss that takes the form of an expectation (or a probability to be wrong), for example for linear models $E[(Y-\beta^T X)^2]$ in regression or $\log(1+\exp(-Y\beta^T X))$ in classification... For tree based models, you can use Boosting that can also be used in batches. The advantage is that you can train in batches as for neural networks but the disadvantage is that you must have a big database (this seems to be the case for you). If you try to use SGD, as in the case of neural network be careful of the hyperparameters you choose. If you want a peek at an algorithm that works well for what you want to do with linear models see scikit-learn SGDClassifier or SGDRegressor and particularly partial_fit if you choose to use mini-batches. For trees, you can also look at scikit-learn or at xgboost library.
