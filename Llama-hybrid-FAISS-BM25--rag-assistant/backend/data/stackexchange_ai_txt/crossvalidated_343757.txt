[site]: crossvalidated
[post_id]: 343757
[parent_id]: 343745
[tags]: 
As edited from the earlier version of the question, we have that $$\mathbb{E}[M]=\underbrace{\mathbb{E}[X]}_{=0}+\underbrace{\mathbb{E}[(Y-X)\mathbb{I}_{Y-X\ge 0}]}_{Y-X\sim{\cal N}(0,2)}=\frac{1}{\sqrt\pi}$$ implies $$\text{cov}(M,L)=\mathbb{E}[ML]-\mathbb{E}[M]\mathbb{E}[L]=\underbrace{\mathbb{E}[XY]}_{=0}+\underbrace{\mathbb{E}[M]^2}_{L\sim-M}=\frac{1}{\pi}$$ where $L\sim -M$ means that $L$ and $-M$ have the same (marginal) distribution since $$M\sim\max(X,Y)\sim\max(-X,-Y)=-\min(X,Y)\sim -L$$ Now, the derivation proposed in the second paragraph is that \begin{align} \text{var}(M+L)&=\text{var}(X+Y)\\&=\underbrace{\text{var}(M)+\text{var}(L)}_{\text{identical: }L\sim -M}+2\text{cov}(M,L)\\&=2\left\{\text{var}(M)+\text{cov}(M,L)\right\}\\ &=2\left\{\text{var}(M)+\frac{1}{\pi}\right\}=2\\ \end{align} Therefore $$\text{var}(M)=\dfrac{\pi-1}{\pi}$$ which somewhat surprisingly implies$$\mathbb{E}[M^2]=1=\mathbb{E}[X^2]$$ Except that$$2\mathbb{E}[M^2]=\mathbb{E}[M^2+L^2]=\mathbb{E}[X^2+Y^2]=2\mathbb{E}[X^2]=2$$explains the identity.
