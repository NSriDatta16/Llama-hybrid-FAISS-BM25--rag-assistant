[site]: stackoverflow
[post_id]: 500700
[parent_id]: 500607
[tags]: 
Here are a few: Suffix tries. Useful for almost all kinds of string searching ( http://en.wikipedia.org/wiki/Suffix_trie#Functionality ). See also suffix arrays; they're not quite as fast as suffix trees, but a whole lot smaller. Splay trees (as mentioned above). The reason they are cool is threefold: They are small: you only need the left and right pointers like you do in any binary tree (no node-color or size information needs to be stored) They are (comparatively) very easy to implement They offer optimal amortized complexity for a whole host of "measurement criteria" (log n lookup time being the one everybody knows). See http://en.wikipedia.org/wiki/Splay_tree#Performance_theorems Heap-ordered search trees: you store a bunch of (key, prio) pairs in a tree, such that it's a search tree with respect to the keys, and heap-ordered with respect to the priorities. One can show that such a tree has a unique shape (and it's not always fully packed up-and-to-the-left). With random priorities, it gives you expected O(log n) search time, IIRC. A niche one is adjacency lists for undirected planar graphs with O(1) neighbour queries. This is not so much a data structure as a particular way to organize an existing data structure. Here's how you do it: every planar graph has a node with degree at most 6. Pick such a node, put its neighbors in its neighbor list, remove it from the graph, and recurse until the graph is empty. When given a pair (u, v), look for u in v's neighbor list and for v in u's neighbor list. Both have size at most 6, so this is O(1). By the above algorithm, if u and v are neighbors, you won't have both u in v's list and v in u's list. If you need this, just add each node's missing neighbors to that node's neighbor list, but store how much of the neighbor list you need to look through for fast lookup.
