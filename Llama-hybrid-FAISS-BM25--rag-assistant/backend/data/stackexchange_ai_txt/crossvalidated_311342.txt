[site]: crossvalidated
[post_id]: 311342
[parent_id]: 
[tags]: 
Dummy coding categorical variables with lots of unique values using log2?

I'm trying to understand the logic behind this binary encoder. It automatically takes categorical variables and dummy codes them (similar to one-hot-encoding on sklearn), but reduces the number of output columns equal to the log2 of the length of unique values. Basically, when i used this library, i noticed that my dummy variables are limited to only a few of the unique values. Upon further investigation i noticed this @staticmethod, which take the log2 of the len of unique values in a categorical variable. My question is WHY!? I realize that this reduces the dimensionality of the output data, but what is the logic behind doing this? How does taking the log2 determine how many digits are needed to represent the data? def calc_required_digits(X, col): """ figure out how many digits we need to represent the classes present """ return int( np.ceil(np.log2(len(X[col].unique()))) ) Full source code: """Binary encoding""" import copy import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from category_encoders.ordinal import OrdinalEncoder from category_encoders.utils import get_obj_cols, convert_input __author__ = 'willmcginnis' [docs]class BinaryEncoder(BaseEstimator, TransformerMixin): """Binary encoding for categorical variables, similar to onehot, but stores categories as binary bitstrings. Parameters ---------- verbose: int integer indicating verbosity of output. 0 for none. cols: list a list of columns to encode, if None, all string columns will be encoded drop_invariant: bool boolean for whether or not to drop columns with 0 variance return_df: bool boolean for whether to return a pandas DataFrame from transform (otherwise it will be a numpy array) impute_missing: bool boolean for whether or not to apply the logic for handle_unknown, will be deprecated in the future. handle_unknown: str options are 'error', 'ignore' and 'impute', defaults to 'impute', which will impute the category -1. Warning: if impute is used, an extra column will be added in if the transform matrix has unknown categories. This can causes unexpected changes in dimension in some cases. Example ------- >>>from category_encoders import * >>>import pandas as pd >>>from sklearn.datasets import load_boston >>>bunch = load_boston() >>>y = bunch.target >>>X = pd.DataFrame(bunch.data, columns=bunch.feature_names) >>>enc = BinaryEncoder(cols=['CHAS', 'RAD']).fit(X, y) >>>numeric_dataset = enc.transform(X) >>>print(numeric_dataset.info()) RangeIndex: 506 entries, 0 to 505 Data columns (total 16 columns): CHAS_0 506 non-null int64 RAD_0 506 non-null int64 RAD_1 506 non-null int64 RAD_2 506 non-null int64 RAD_3 506 non-null int64 CRIM 506 non-null float64 ZN 506 non-null float64 INDUS 506 non-null float64 NOX 506 non-null float64 RM 506 non-null float64 AGE 506 non-null float64 DIS 506 non-null float64 TAX 506 non-null float64 PTRATIO 506 non-null float64 B 506 non-null float64 LSTAT 506 non-null float64 dtypes: float64(11), int64(5) memory usage: 63.3 KB None """ def __init__(self, verbose=0, cols=None, drop_invariant=False, return_df=True, impute_missing=True, handle_unknown='impute'): self.return_df = return_df self.drop_invariant = drop_invariant self.drop_cols = [] self.verbose = verbose self.impute_missing = impute_missing self.handle_unknown = handle_unknown self.cols = cols self.ordinal_encoder = None self._dim = None self.digits_per_col = {} [docs] def fit(self, X, y=None, **kwargs): """Fit encoder according to X and y. Parameters ---------- X : array-like, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : encoder Returns self. """ # if the input dataset isn't already a dataframe, convert it to one (using default column names) # first check the type X = convert_input(X) self._dim = X.shape[1] # if columns aren't passed, just use every string column if self.cols is None: self.cols = get_obj_cols(X) # train an ordinal pre-encoder self.ordinal_encoder = OrdinalEncoder( verbose=self.verbose, cols=self.cols, impute_missing=self.impute_missing, handle_unknown=self.handle_unknown ) self.ordinal_encoder = self.ordinal_encoder.fit(X) for col in self.cols: self.digits_per_col[col] = self.calc_required_digits(X, col) # drop all output columns with 0 variance. if self.drop_invariant: self.drop_cols = [] X_temp = self.transform(X) self.drop_cols = [x for x in X_temp.columns.values if X_temp[x].var()
