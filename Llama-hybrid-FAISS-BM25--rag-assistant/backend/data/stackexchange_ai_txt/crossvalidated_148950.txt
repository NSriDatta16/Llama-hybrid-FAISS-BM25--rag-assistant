[site]: crossvalidated
[post_id]: 148950
[parent_id]: 148942
[tags]: 
I'd recommend erring on the side of including more variables, since one of the selling points of random forests to begin with is that they can do a good job of using inputs variable according to how useful they actually are, heavily downweighting those which seem not to have much predictive value. It looks like the AUC values R produces for random forests are using the whole training set, not just out-of-bag samples: http://r.789695.n4.nabble.com/Random-Forest-AUC-td3006649.html Out-of-bag error is a lot like cross-validation error, so yes, it does estimate test error. That said, if you change the model or how it's fit after looking at out-of-bag error so as to minimize out-of-bag error, you may overfit for it and thus reduce performance given new test data. The same goes when doing cross-validation. The general principle is, the more aggressively you use some data for fitting, tuning, or selecting models, the more optimistically biased your estimates of test error will be using the same data, and the greater the danger of overfitting.
