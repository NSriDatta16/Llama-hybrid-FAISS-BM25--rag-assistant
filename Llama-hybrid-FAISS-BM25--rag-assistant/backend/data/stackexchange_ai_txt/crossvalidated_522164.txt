[site]: crossvalidated
[post_id]: 522164
[parent_id]: 522160
[tags]: 
Comparing the models and taking the highest probability is not a good idea as it doesn't correlate with accuracy . You should use one model which is the most accurate. This model can be one of the three or ensemble of models etc. You can compare their accuracy by calculating offline measures e.g. AUC & log loss on a common validation set. i.e. you can have a validation set with ground truth labels, have the prediction of each model, and then you calculate the AUC & log loss for each model and compare the results. Another approach is to compare them using ranking measure like: NDCG "could they be used in deciding which product to approach the customer" - Yes, if you have good offline measures for your model (e.g. high AUC) you can use it to recommend products. Though in machine learning it is never 100% guaranteed that good offline performance will lead to good online results (e.g. more money), it is usually correlated, and better models (better AUC) provide better results also online. Regarding calibration - this is a different topic from accuracy. You can have more accurate model which is less calibrated. At least at the beginning you should focus on accuracy and ranking and only then, ask yourself if you need your model to be also calibrated. The difference is that accuracy is pointwise measure, and calibration care about the model average being closer to prediction average. If you also care about calibration you can have additional model like isotonic regression that will calibrate the model results.
