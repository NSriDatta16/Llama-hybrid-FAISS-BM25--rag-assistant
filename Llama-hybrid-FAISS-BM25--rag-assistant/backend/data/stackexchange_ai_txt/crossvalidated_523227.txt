[site]: crossvalidated
[post_id]: 523227
[parent_id]: 
[tags]: 
Regression in high dimensions ($p>n$) with nonnegative outcome

I need to perform regression in a setting where my outcome is positive ( $Y > 0$ ) and there are more variables than rows ( $p > n$ ). The goal of the analysis is to obtain predictions, which, of course, must be nonnegative. Would it be as simple as performing LASSO (or other regularization) on the log-transformed outcome (e.g. $log(Y)$ regressed on $X$ ), and then exponentiating the predictions? Is this valid, or are there more complexities in obtaining that prediction that I have ignored here? It would also be helpful to know if there are possibly better ideas that generally work for high-dimensional data with nonnegative outcome (e.g. a transformed version of random forests that works for positive outcome?). Thank you! Update: I've read in certain places that if you take log(outcome) and want a mean prediction back on the original scale, you can't just exponentiate the mean on the log scale. However, I don't follow this, nor do I understand how to fix the issue (in the LASSO case, but also generally, with any type of model). Additional details: The reason for the positive outcome is that I am trying to perform a residual reweighting (because of heteroscedasticity), in which the denominator of the weights are the expected squared residuals . To obtain the expected squared residuals, I am regressing the true squared residuals on $X$ - hence the strictly positive outcome. In my question above, $Y$ directly refers to these observed squared residuals. I want it to be clear, though, that this explanation is secondary to my question. My primary goal is that I want to model $Y \sim X$ properly, given the nonnegative outcome and high dimensonality.
