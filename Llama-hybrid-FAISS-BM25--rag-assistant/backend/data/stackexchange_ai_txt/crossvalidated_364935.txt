[site]: crossvalidated
[post_id]: 364935
[parent_id]: 149709
[tags]: 
I find existing answer very misleading. The Word vector (aka word embedding ) is concept coming from probabilistic language models (see [1]). It describes contextual similarity between the words in the language model and came into existence several decades after the VSM was proposed and successfully applied for text categorization, document summarization and information retrieval. In the Vector Space Model (see [2]), it is not word/term being represented as vector in a n-dimensional space but document . The VSM is constructed to have separate dimension for each distinct unigram word/ term , existing in collection of terms aggregated from all BOWs in the document collection. In other words, in the VSM: distinct terms became dimensions , not word vectors. Documents are vectors in the VSM, located at associated term weights by each corresponding dimension. Bag-of-words (BOW), as approach of document representation in IR, does not allow multiple instances of same word - but represents an unordered list of distinct words, associated with their frequencies in the document (see [3]). [1] Y. Bengio, R. Ducharme, P. Vincent, C. Janvin, A Neural Probabilistic Language Model, J. Mach. Learn. Res. 3 (2003) 1137–1155. doi:10.1162/153244303322533223. [2] G. Salton, A. Wong, C. Yang S., A vector space model for automatic indexing, Commun. ACM. 18 (1975) 613–620. doi:10.1145/361219.361220. [3] G. SALTON, C.S. YANG, ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING, J. Doc. 29 (1973) 351–372. doi:10.1108/eb026562.
