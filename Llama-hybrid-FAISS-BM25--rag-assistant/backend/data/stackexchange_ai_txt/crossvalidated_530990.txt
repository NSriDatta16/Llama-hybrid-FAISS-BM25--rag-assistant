[site]: crossvalidated
[post_id]: 530990
[parent_id]: 
[tags]: 
Loss function of text generation in NLP

I have a small project in my mind and thinking to implement, to generate regular expressions from natural language. I found the following issue when reading researches and I had an idea in my mind and found other researchers outlining the same problem. Let's say my input is: 'Items with a numeral preceding a letter or small letter.' It matches the regular expression: ([0-9]). (([A-Za-z])&([a-z])). I know that the same input can have different correct outputs. But the sole idea here any small change in the output my model predicts such as: ([1-9]). (([A-Za-z])&([a-z])). with changing the '0' into a '1' will make it an incorrect prediction and it will be penalized harder than it should be. A way to tackle it is by counting how many sub-patterns it matches. I am thinking for the sake of easiness and understanding it to use the Edit Distance metric. Let's say after training first sample the edit distance = 3. How shall the model be penalized ? The model should be a seq2seq model, let's say an encoder-decoder LSTM. The decoder would output a pattern and get the loss function of it, and then ? Is that enough or there is something that should be considered ?
