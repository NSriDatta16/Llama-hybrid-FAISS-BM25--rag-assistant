[site]: crossvalidated
[post_id]: 118453
[parent_id]: 118442
[tags]: 
You tried to show detailed balance for the Markov chain that is obtained by considering one transition of the Markov chain to be the 'Gibbs sweep' where you sample each component in turn from its conditional distribution. For this chain, detailed balance is not satisfied. The point is rather that each sampling of a particular component from its conditional distribution is a transition that satisfies detailed balance. It would be more accurate to say that Gibbs sampling is a special case of a slightly generalized Metropolis-Hastings, where you alternate between multiple different proposals. More details follow. The sweeps do not satisfy detailed balance I construct a counterexample. Consider two Bernoulli variables ($X_1,X_2$), with probabilities as shown in the following table: \begin{equation} \begin{array}{ccc} & X_2 = 0 & X_2 = 1 \\ X_1 = 0 & \frac{1}{3} & \frac{1}{3} \\ X_1 = 1 & 0 & \frac{1}{3} \end{array} \end{equation} Assume the Gibbs sweep is ordered so that $X_1$ is sampled first. Moving from state $(0,0)$ to state $(1,1)$ in one move is impossible, since it would require going from $(0,0)$ to $(1,0)$. However, moving from $(1,1)$ to $(0,0)$ has positive probability, namely $\frac{1}{4}$. Hence we conclude that detailed balance is not satisfied. However, this chain still has a stationary distribution that is the correct one. Detailed balance is a sufficient, but not necessary, condition for converging to the target distribution. The component-wise moves satisfy detailed balance Consider a two-variate state where we sample the first variable from its conditional distribution. A move between $(x_1,x_2)$ and $(y_1,y_2)$ has zero probability in both directions if $x_2 \neq y_2$ and thus for these cases detailed balance clearly holds. Next, consider $x_2 = y_2$: \begin{equation} \pi(x_1,x_2) \mathrm{Prob}((x_1,x_2) \rightarrow (y_1,x_2)) = \pi(x_1,x_2)\,p(y_1 \mid X_2 = x_2) = \pi(x_1,x_2) \, \frac{\pi(y_1,x_2)}{\sum_z \pi(z,x_2)} \\ = \pi(y_1,x_2) \, \frac{\pi(x_1,x_2)}{\sum_z \pi(z,x_2)} = \pi(y_1,x_2) \,p(x_1 \mid X_2 = x_2) = \pi(y_1,x_2) \mathrm{Prob}((y_1,x_2) \rightarrow (x_1,x_2)). \end{equation} How the component-wise moves are Metropolis-Hastings moves? Sampling from the first component, our proposal distribution is the conditional distribution. (For all other components, we propose the current values with probability $1$). Considering a move from $(x_1, x_2)$ to $(y_1, y_2)$, the ratio of target probabilities is \begin{equation} \frac{\pi(y_1,x_2)}{\pi(x_1,x_2)}. \end{equation} But the ratio of proposal probabilities is \begin{equation} \frac{\mathrm{Prob}((y_1,x_2) \rightarrow (x_1,x_2))}{\mathrm{Prob}((x_1,x_2) \rightarrow (y_1,x_2))} = \frac{\frac{\pi(x_1,x_2)}{\sum_z \pi(z,x_2)}}{\frac{\pi(y_1,x_2)}{\sum_z \pi(z,x_2)}} = \frac{\pi(x_1,x_2)}{\pi(y_1,x_2)}. \end{equation} So, the ratio of target probabilities and the ratio of proposal probabilities are reciprocals, and thus the acceptance probability will be $1$. In this sense, each of the moves in the Gibbs sampler are special cases of Metropolis-Hastings moves. However, the overall algorithm viewed in this light is a slight generalization of the typically presented Metropolis-Hastings algorithm in that you have alternate between different proposal distributions (one for each component of the target variable).
