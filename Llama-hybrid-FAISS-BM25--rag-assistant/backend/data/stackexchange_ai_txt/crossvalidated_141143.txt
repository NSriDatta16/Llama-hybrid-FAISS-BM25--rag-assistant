[site]: crossvalidated
[post_id]: 141143
[parent_id]: 139129
[tags]: 
In NLP, where words are typically encoded as 1-of-k, the use of word embeddings has emerged recently. The wikipedia page with its references is a good start. The general idea is to learn a vectorial representation $x_i \in \mathbb{R}^n$ for each word $i$ where semantically similar words are close in that space. Consequently, the inputs are of size $n$ instead of the size of the vocabulary. Maybe you can transfer that idea to your setting.
