[site]: datascience
[post_id]: 6364
[parent_id]: 6350
[tags]: 
Strictly speaking, the k-means algorithm does not have a definition for "inside the cluster" and is therefore not a great candidate for anomaly detection. In k-means, every point is assigned to one of k clusters and then a new cluster centroid is calculated. But as previous uses have pointed out, you could construct some sort of ad-hoc system where you process a set of data and then define new data as anomalous when its extends beyond 2 standard deviations of the centroid location. DON'T DO THIS! K-Means will not work well for this ad-hoc method. If k is poorly chosen, then the distribution within a cluster will not be normally distributed. You very, very frequently see natural distributions of points which are split between two clusters. For instance, take a look at the ad-hoc segmentation of points in this location data for a cell phone user's location over a month: I suggest you use another clustering method. The first option that comes to mind is DBSCAN. This allows one to set a threshold for noise and the cluster numbers are not set a-priori. DBSCAN is therefor much more likely to return normal distributions within a cluster. Here is a single DBSCAN cluster of the same data: Finally, I'll point out that the method you are proposing is not as good as other novelty and anomaly detection methods. You should consider doing novelty detection using a single (or possibly even multiple) class support vector machine (SVM) with a nonlinear kernel. The nonlinear kernel will allow you to recover multiple "clusters" while the SVM will do much better at predicting which points are inside the class.
