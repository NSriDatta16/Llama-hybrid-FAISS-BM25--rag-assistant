[site]: crossvalidated
[post_id]: 263035
[parent_id]: 262930
[tags]: 
Can a decision tree recreate the exact same classification as a nearest neighbor classifier? Yes. Think about a trivial data set with only two points (0,0),(1,0) , each belonging to a different class. The decision boundary of a 1-NN will be the perpendicular bisector of the segment. Now, if you look for a splitter for this two points, using a decision tree, you will find a line (probably the parallel to the y -axis going through 1/2 (1)). I think any 1-dimensional data with perfect separation between two classes would produce the same "decision boundary" for 1-NN and decision trees. What are the odds that a decision tree recreates the exact same classification as a nearest neighbor classifier? In more than 1 dimension, almost certainly (2) no. The decision boundary for nearest neighbours are Voronoi diagrams : Whereas the decision boundary of a decision tree are lines parallel to the axis . More details can be found here : Questions on some data-mining algorithms (1) Most decision tree implementations look at the splitters defined by the average between two consecutive points. Therefore, 1/2 could be one of the decision boundary chosen by the algorithm, but this is implementation dependant. (2) If you assume a distribution over Voronoi diagrams, we can assume that the measure is rotation-invariant. Therefore making the probability of all the lines of a Voronoi diagram being parallel to the axis zero.
