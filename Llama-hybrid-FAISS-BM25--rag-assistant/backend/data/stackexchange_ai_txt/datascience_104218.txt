[site]: datascience
[post_id]: 104218
[parent_id]: 104143
[tags]: 
The general answer is the same as with everything in machine learning: it depends on the particular task. With Transformers, people tend to recommend larger batch sizes, typically thousands of tokens per batch. A highly cited paper on training tips for Transformers MT recommends getting the best results with 12k tokens per batch. For the number of epochs, the usual advice is: plot the learning curves, at some point, the validation loss starts to stagnate or grow, whereas the training loss will continue to decrease. This means that the model starts overfitting. It will also tell what number of epochs is meaningful for your task.
