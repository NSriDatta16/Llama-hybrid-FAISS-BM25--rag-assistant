[site]: crossvalidated
[post_id]: 396966
[parent_id]: 396044
[tags]: 
The cost function $$ C_\alpha(T) = \sum_{m=1}^{|T|}\sum_{x_i\in R_m}^{}(y_i-\bar{y}_m)^2 + \alpha |T| $$ balances the training error of a tree $T$ (first term) and its number of terminal nodes (second term). Notice that we're considering only the cost of trees $T$ that can be obtained by pruning the original (unpruned) tree $T_0$ (including $T_0$ itself, of course). Pruning is done because trees that are grown too high suffer from overfitting, meaning (informally) that a small change in the training data would produce a very different regression tree. The bias-variance trade-off tells us that, in general, overfitting gets in the way of good generalization (compare the public and private leaderboards at Kaggle competitions to have a live picture of this phenomenon). When $\alpha=0$ , the subtree that minimizes the cost is $T_0$ itself (formally, $T_0=\arg \min_T C_0(T)$ ), otherwise the tree growing algorithm would not have produced $T_0$ as the solution in the first place. When you raise the value of $\alpha$ , you're giving a chance for subtrees of smaller size, that have a larger training error, to be chosen, and this process may produce a tree that generalizes better (has a smaller test error). In practice, the value of $\alpha$ is chosen by some form of cross-validation. The more general idea behind this is that Machine Learning methods "like to overfit", and some sort of "regularization" is needed to stop this from happening. The regularizer in Bayesian Machine Learning methods is the prior distribution, which is kind of ironic, because for a very long time it was thought (not by everyone, of course) that we should always rely on diffuse priors and let the data speak for themselves (through the likelihood function). Well, if you're only interested in prediction performance, it seems that we shouldn't just let the data "sing" for themselves, because, like a group of sirens, the data will try to drive the learning method/model to the rocks of overfitting (students know this phenomenon very well: in general, memorization is "easier" than true learning, but not as useful when trying to extend what you already know to a larger domain). Finally, when we grow a forest of trees, we don't care about overfitting of each one of the trees (so we grow each of them high and don't prune), because mechanisms like bootstrap aggregation and random subspaces can take care of the variance of the whole ensemble. Random Forests do exactly this, with great success (thanks to Leo Breiman and Adele Cutler).
