[site]: crossvalidated
[post_id]: 555893
[parent_id]: 
[tags]: 
How the split points are chosen for continuous features in Decision Tree Classifiers

Using the Iris data set, where the feature variables used are sepal_width(x1) and petal_width(x2) , scikit learn Decision Tree Classifier outputs the following tree - clf = DecisionTreeClassifier(max_depth=6) clf.fit(X,y) Now my question is how is the split points determined for the continuous feature variables x1 and x2? I have found two type of answers(from different articles/blogs and here as well) - Some say that if suppose x1 = {2.0, 1.1, 5.3, 4.0} and to decide the split points for x1, sort x1 to get x1_sorted = {1.1, 2.0, 4.0, 5.3} and calculate the mean of two consecutive points in this set to get the split points, i.e. split_points = {1.55, 3, 4.65} While others say that the split points are all the distinct data points for the feature variable in question. So if x1 = {2.0, 1.1, 5.3, 4.0} then the split points would be, split_points = {1.1, 2.0, 4.0, 5.3} Once the split points are determined, we can check for the best split using gini index. But if one refers to the tree, it seems that its a combination of both, like X1 0.8 and X1 1.75 supports point 1 while X1 1.6 seems to support point 2 Distinct Values in the petal_width data set = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6 , 1.7, 1.8 , 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5] 1.6 is there in the data set, while average of 1.7 and 1.8 gives 1.75. Is there any specific algorithm in play here?
