[site]: crossvalidated
[post_id]: 577485
[parent_id]: 
[tags]: 
How to interpret a low level of classification?

There is a marked dataset of $n=2879$ objects on 7 classes. Each object (an object is a text of 1-3 sentences) was marked up by different users. Class 1 includes $n_1=790$ objects, Class 3 has the fewest objects: $n_3=192$ . On can see the dataset is not balanced because the ratio $n_1/n_3 = 790/192 =4.11$ is large. An expert analysed the dataset and concluded: objects were erroneously assigned to other classes during marking. The expert's decision based on human experience only: he read texts (remain the object for classification is a small text) and made decision: right or not right. In particular, there is a hypothesis that objects from class 3 were labeled with errors more often than others. We applied various classifiers (SVM, kNN, logistic regression) on the 1) original dataset and 2) balanced sample and obtained an accuracy within only 78-82%. Firstly, we divided the dataset on the train sample and test one in proportion 80/20. In experiments, we also tried to use balanced sample by taking 192 objects from all classes randomly. Question. Let's assume that the classifiers are implemented correctly. Can it be argued that a low percentage of classification confirms the presence of errors in the marking? Is it possible to statistically prove that class 3 really contains the most errors?
