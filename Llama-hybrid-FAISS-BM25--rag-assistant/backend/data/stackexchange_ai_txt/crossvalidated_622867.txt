[site]: crossvalidated
[post_id]: 622867
[parent_id]: 202879
[tags]: 
Statistical significance & p-values According to me, the most harmful words in the history of statistics are p-value and statistical significance . They should not only be corrected, but even deleted. The journal of Basic and Applied Social Psychology banned them ( Trafimow & Marks, 2015 ). "Statistical significance" doesn’t mean anything, for any definition that I have heard, even with definitions that didn’t include p-values. The only similar expressions that may be meaningful are "clinically significant", "mechanically significant", "biologically significant", " YOUR_APPLICATION significant", etc. Statistics are just numbers. A number becomes significant in light of an application IRL, it has no meaning by itself. So statistics, as a field, doesn’t provide significance; it provides numbers. If you obtain such and such statistic, it means that basing a decision on it, following a particular logic, will have a particular impact IRL. The significance of your statistic-based decision, and thus of the statistical value of interest, is the impact of that decision IRL. Thresholds 0.05, 0.01, 0.001, 0.0001, etc. are completely arbitrary and do not come from an impact analysis on real life. Impact analyses start with effect sizes ("The drug has this effect, which reduces symptoms by P%, which would have XYZ impact on patients’ health, etc.") or predictive values , for instance. A flawed framework Moreover, p-values and statistical significance are based upon a flawed framework. The null hypothesis significance testing (NHST) procedure has been thoroughly criticized through decades (see The ASA's Statement on p-Values: Context, Process, and Purpose and references). It’s not about Bayesians vs. frequentists, it’s just flawed. No Bayesian alternatives that I read convinced me, despite the fact that attaching probabilities to hypotheses can be meaningful in some contexts (e.g., when supposing that the drug that you’re studying is ineffective on some patients, or has a different effect). Fixing things Descriptive statistics and performance metrics are enough to evaluate the quality of a model/hypothesis. If you want to test the generalizability of the model/hypothesis, you need to repeat the experiment on a new population of interest. Studies after studies, experiments after experiments, you’ll likely find better estimates of the reality. This is systematic empiricism, or science in other words. For example, systematic empiricism is how our predecessors improved their understanding of gravity. The first humans proposed the simple model "object falls to the ground". Then, they started building catapults and formalized more and more the process of falling, taking into account the mass of the object, the distance, the time it would take to fall, etc. Centuries after, Newton did precise tests using more elaborate mathematical tools and created a model including Newton’s laws of motion. But Einstein wasn’t satisfied with it, so he proposed the theory of general relativity, etc. These theories, which have been being more and more precise, have never needed a p-value or "statistical significance" to be validated. You simply need to measure where objects fall, admit it’s the truth, and then build a model that predicts it. The challenge is to build a model that predicts well in different context. In any case, any fall you observe remains the truth. If you change the angle of your throw, it will fall somewhere else. You don’t need to run a "significance test" on a sample of falls to know whether the objects have fallen somewhere else, you’ve just observed where they have fallen. The only statistics you need are those who describe the falls (where, when, with what angle, etc.) and the difference between your prediction and the reality, whereupon you build a model that can reduce this difference. P-values and "significance tests" don’t tell you where object falls (what is the reality), they don’t tell you if your predictions were right, and they don’t tell you how to reduce the difference between the prediction and the reality. So ban them.
