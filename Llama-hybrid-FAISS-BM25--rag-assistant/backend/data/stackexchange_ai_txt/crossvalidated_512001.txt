[site]: crossvalidated
[post_id]: 512001
[parent_id]: 511726
[tags]: 
I will give you a run-down of the terminology used in statistics, which I think is sensible terminology. Cases (1) and (2) do refer to bias in the usual statistical sense, (4) refers to something closely related, and (3) is just misleading renaming of an object that already has a perfectly sensible name. Just to quickly dispose of (3), I note that the term $\beta_0$ in regression is called the "intercept" term, not the "bias". Unless there are good contextual reasons to refer to it as the "bias term", that language is highly misleading. (The link you give for this usage is just a CV.SE question that was closed for lack of clarity, so not really evidence of widespread usage. I have never seen this term in the regression referred to as the bias term.) Estimator bias and the "bias-variance trade-off": When we "fit" a statistical model we are essentially just estimating the unknown parameters in that model. As you note in case (2), the bias of an estimator is defined as the difference between the expected value of the estimator and the value of the parameter it is estimating: $$\text{Bias}(\hat{\theta}, \theta) = \mathbb{E}(\hat{\theta}) - \theta. \quad \ $$ When looking at the performance of estimators we often measure this by the mean squared error , which is the expected value of the squared deviation of the estimator from the value it is estimating: $$\text{MSE}(\hat{\theta}, \theta) = \mathbb{E} \Big( (\hat{\theta}-\theta)^2 \Big).$$ One of the properties of the mean squared error is that it can be decomposed as: $$\quad \quad \ \ \text{MSE}(\hat{\theta}, \theta) = \mathbb{V}(\hat{\theta}) + \text{Bias}(\hat{\theta}, \theta) ^2.$$ If we examine a model for an observable value $y = f(x, \theta) + \varepsilon$ composed of a regression term and an error term, we likewise have: $$\mathbb{E} \Big( (y-f(x, \hat{\theta}))^2 \Big) = \mathbb{V}(f(x,\hat{\theta})) + \text{Bias}(f(x,\hat{\theta}), f(x,\theta)) ^2 + \sigma_\varepsilon^2.$$ Now, if we examine the class of estimators with some fixed mean squared error , we can see that there must be a trade-off between the bias and variance in these estimators --- a lower bias corresponds to a higher variance and vice versa . Often when we have competing methods of estimating parameters (i.e., alternative ways to "fit" the model) we focus on those that have the optimal mean squared error, and within this class we see that there is a choice between methods that have higher bias but lower variance, and methods that have low (or no) bias but have higher variance. As you point out in your case (1), it is common in discussions of machine-learning to refer to a general "bias-variance trade-off" in the choice of model fitting methods and the use of training data. While this discussion is often quite broad and esoteric, ultimately it derives from the statistical decomposition shown here. It is therefore referring to "bias" in the standard statistical sense. Consequently, cases (1) and (2) both refer to bias in its usual statistical definition. Informative sampling (so-called "biased" data): In discussions of sampling you might sometimes run across references to "bias" in the data or sampling method. Statisticians generally do not use this language (except sometimes as shorthand) because they recognise that bias is a property of an estimator, and so it only occurs from the combination of a sampling method and an inference method . When data gives information in a non-standard way we say that it is an "informative" sampling method and we try to take this into account in our inferences. If the inference method does not properly take account of this information then this gives us a biased estimator, but the bias comes from the combination of the sampling method and the inference method --- it is not the data per se that is biased. This area of statistics is quite subtle, so I will describe it through an example. Suppose you have a small community with ten children, and you want to know the average number of children per family (only for the families with at least one child). Suppose that of these ten chidren, nine are from the same family (all siblings) and the one is an only child (no siblings), which means that the true average is five. Suppose you sample all the children and ask each of them how many siblings they have, then use this data to estimate the average number of children per family. A naive estimator would be to take the average number of siblings per child and then add one to get an estimate of the average number of children per family. If you use that inference method, you get a substantial overestimate: $$\hat{\theta} = \tfrac{1}{10} (9 \cdot 9 + 1 \cdot 0) + 1 = 8.1+1 = 9.1.$$ The problem here is that we are sampling over children rather than over families, and so we are more likely to select a child from a larger family; the average over all children gives proportionately more weight to families of larger size. This is an example of "informative" sampling method where the naive estimator leads to large bias in estimating the true quantity of interest. (The technical name for this type of sampling is probability proportional to size (PPS) sampling.) Note here that it is the use of a particular type of (erroneous) estimator that leads to the bias. If we take account of the fact that we are using PPS sampling, and account for this in our estimator, we can get rid of this bias. As you can see from this example, it is not quite right to say that the data themselves are biased --- the sampling method is unusual, and it leads us to substantial bias if we treat it as if it were giving us direct information on the quantity of interest, but the bias occurs from our failure to account for the nature of the sampling mechanism in our estimator. In statistical discussion we use the term "informative sampling" to describe this kind of sampling mechanism, but "bias" remains a property of the estimator. Consequently, whether or not we have bias is determined by the combination of the sampling method and the estimator.
