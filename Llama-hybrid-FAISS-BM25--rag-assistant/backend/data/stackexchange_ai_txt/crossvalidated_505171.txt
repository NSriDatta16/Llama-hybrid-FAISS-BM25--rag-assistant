[site]: crossvalidated
[post_id]: 505171
[parent_id]: 
[tags]: 
Learning prior p(z) in VAEs

The paper Ma et al. 2018 states the following statement about a VAE model: Usually, the prior $p_{\theta}(z)$ is standard normal, but we find that parameterizing it with a trainable mean vector $m$ and variance vector $s^2$ sometimes improves inference. A VAE has the objective function $$ \mathcal{L}^{B}=-K L[\overbrace{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}^{\text {Encoder }} \| \overbrace{p_{\theta}(\mathbf{z})}^{\text {Fixed }}]+\frac{1}{L} \sum_{l=1}^{L} \log \overbrace{p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}^{(l)}\right)}^{\text {Decoder }} $$ I was wondering how would the objective for such a VAE with learned $p_{\theta}(z)$ looks like. Also, how would the diagram of the model look like.
