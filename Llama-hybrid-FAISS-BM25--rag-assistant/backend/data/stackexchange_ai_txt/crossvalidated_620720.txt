[site]: crossvalidated
[post_id]: 620720
[parent_id]: 
[tags]: 
How should I use a larger dataset with fewer variables to help create a good model of a smaller dataset with more variables?

I have two data sets, DF1 and DF2. DF1 has millions of observations, and 20 variables. DF2 has ~100,000 observations, and 40 variables. The DF1 variables are a subset of the DF2 variables. I want to predict Y for observations with all of the DF2 variables, but I also need interpretable predictions. So I'm trying to create a model with predictive coefficients for the DF2 variables. Ideally I want to use the DF1 dataset to create the model as well, because Y has lots of random noise, and I want precise predictions for groups where DF2 alone doesn't have much data. My concern is that some of the DF2 variables covary with the DF1 variables, so the DF1 variable coefficients change with the inclusion of the DF2 variables. This answer outlines what I see as my two options. I can merge the datasets and build a single model; or I can build two models, and somehow incorporate the DF1 model estimations/ coefficients into the DF2 model. What's the best option? Merge datasets, one model: Including the DF2 variables changes some of the DF1 coefficients. Will the model's coefficients of DF1 variables be 'overpowered' by the larger number of DF1 observations? Bayesian updating: How do I go about doing this?
