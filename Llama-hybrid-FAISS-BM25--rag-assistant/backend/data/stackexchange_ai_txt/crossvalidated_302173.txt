[site]: crossvalidated
[post_id]: 302173
[parent_id]: 302169
[tags]: 
I am unfamiliar with using ensembles of CNNS, but you might be interested in Hinton, Geoffrey E., et al. "Improving neural networks by preventing co-adaptation of feature detectors." arXiv preprint arXiv:1207.0580 (2012). Dropout can be seen as an extreme form of bagging in which each model is trained on a single case and each parameter of the model is very strongly regularized by sharing it with the corresponding parameter in all the other models. So, if you equate dropout to bagging, then ensemble methods are used extensively for neural networks in the form of dropout. It is probably much easier (and less expensive) to tune the degree of dropout than to tune an actual ensemble of CNNs. If you do want to use an ensemble, the number of estimators would have to be chosen as a hyperparameter using a validation set or cross validation because the best number of estimators would be highly dependent on the data (although perhaps if there are networks trained on images you could use their choice as it seems that insights from working with one set of images tend to generalize, perhaps because processing pixels is kind of more general than processing structured attributes). Finding the number of estimators would be difficult because now the hyperparameters of each estimator would be conditioned on the number of estimators. This might not be so bad when they are decision trees, but when they are CNNs it might be really difficult to train. In terms of boosting, generally that is used to combine weak estimators while CNNs are high capacity models (although it might be a good idea to boost them, don't know).
