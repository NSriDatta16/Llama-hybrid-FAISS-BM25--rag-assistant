[site]: crossvalidated
[post_id]: 371684
[parent_id]: 
[tags]: 
When does knowing the causal structure of the data generating process improve supervised learning?

Consider a supervised learning prediction task where we have some real-valued feature vector X and wish to train a model that predicts discrete class label Y. When the model is deployed, Y will be unobserved. Suppose that in the true data generating process, causality flowed as follows: X ← Z → Y Z is a latent variable, and Y is essentially a proxy for Z. One might use some Bayesian latent variable and try to predict from the posterior P(Y|X) using exact or approximate inference. A naive selection of such a model would typically assumes an incorrect data generating process: Y → X In terms of predictive performance, this is typically fine, for example naive Bayes typically does OK. Under what circumstances, if any, would incorporating the true structure of the data generating process improve prediction of Y?
