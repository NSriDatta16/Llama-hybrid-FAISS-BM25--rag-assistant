[site]: crossvalidated
[post_id]: 207365
[parent_id]: 
[tags]: 
How does GibbsLDA++ ensure that we are sampling from a good posterior?

This is an extending of this question , which asked that whether we should do some estimating to ensure that we are really using a likely topic assignment instead of the one happened with low probability. And @Matt said that GibbsLDA++ was to do that, choose the last $T$ iterations instead of just the last one. I wonder is it related by the term $lag$ in MCMC? In addition, I downloaded the source code of GibbsLDA++, but I can't find that specific part to mark the parameter in last $T$ iterations and utilize it. Could you please help me with that?
