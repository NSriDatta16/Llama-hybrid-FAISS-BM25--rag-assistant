[site]: crossvalidated
[post_id]: 541612
[parent_id]: 541607
[tags]: 
1.Is my approach alright? The more usual approach would be to train some kind of text-to-image model, though the idea of using pre-trained image embeddings and text embeddings is interesting and a very practical approach. The part that I find troublesome is that you want to train a text encoder to produce the same embeddings as the ones created by an image encoder. If you think about it, the representation of an image and the representation of a sentence are likely completely different things. Images have colors, spatial features, edges, etc, while sentences have parts of speech, words, and their relations. Even the human brain has different regions for processing language and visual stimuli. This may be hard to achieve. Maybe it would be easier if you had a single model that tries encoding images and text at the same time with the constraint that the learned representations need to be the same, in such a case you would be forcing both representations (for text and images) to be alike, rather than forcing one of them to pretend to be the another. Alternatively, you could use two pre-trained models to create representations of images and text and train a model that would make a decision if the two representations are alike or not. It sounds like a simpler problem. You could use something like triplet loss to train it. Such a model could be simpler to train since instead of predicting 4096 targets, it would need to return just a single similarity score, e.g. probability that the representations are the same. Moreover, with such models, there is no need for the dimensions of both representations to be the same. Finally, you could go through the literature on GAN models that have the generator and discriminator components, since they seem to be trying to solve a similar problem. 2.flickr8k is the dataset. Should I have an even bigger dataset. Although i do not have much computation power. If I understand correctly, flickr8k dataset has 8000 samples. You want to train a model with 4096 features, so two samples per target. This sounds way too little. My guess is that you would need at least something like 100x more samples. All the deep learning models are very data-hungry, that's certainly the case for the image and text models. 3.Is there a pre-trained model that I should use or should I go with a new text encoder? If you stick to the idea of using pre-trained models for representing both images and text, you can use any NLP model. TensorFlow, Pytorch, HuggingFace, and lots of other software enable you to use such models.
