[site]: crossvalidated
[post_id]: 325336
[parent_id]: 325313
[tags]: 
GradientDescentOptimizer implements the most basic version of Gradient Descent while RMSPropOptimizer implements an adaptive version of Stochastic Gradient Descent called RMS Prop "Root Mean Squared Propagation . Basic gradient descent is the slowest of any of the neural network optimizers, you will want to run it for a much longer number of epochs if you want to achieve results similar to RMSProp. In fact just about anyone of the other methods: tf.train.AdagradOptimizer, tf.train.AdagradDAOptimizer, tf.train.MomentumOptimizer, tf.train.AdamOptimizer, tf.train.FtrlOptimizer,... will give you better results than gradient descent.
