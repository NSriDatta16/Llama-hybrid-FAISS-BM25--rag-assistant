[site]: crossvalidated
[post_id]: 337921
[parent_id]: 
[tags]: 
Statistics: orthogonality vs uncorrelatedness vs independence

In this post I would like someone to summarize and relate these 3 concepts of statistics (in the context of stats). 1) I remember that uncorrelated does NOT imply independence (e.g. the case where the RV $X=Y^2$, correlation=0 but they are dependent in a nonlinear way. (as mentioned here: https://en.wikipedia.org/wiki/Uncorrelated_random_variables ) 2)However can we say that independent implies uncorrelated, i.e. Pearson corr. coef. = 0 ? 3)For orthogonality: - Does orthogonal imply uncorrelated ? (i think yes) - Does uncorrelated imply orthogonal? 4) There are definitions of orthogonal w.r.t. Expected value as: $E[XY]$=0 A definition of uncorrelated is: $E[XY]=E[X]E[Y]$ how can we relate both (e.g. with an example) ? 5) Finally, what to say if X and Y are vector RVs ? Does $E[XY]$ goes down to a dot product of X and Y ? ( I m trying to refresh an reorganize my knowledge in my head so would be great to have like a "summary post" or "cheat sheet" here)
