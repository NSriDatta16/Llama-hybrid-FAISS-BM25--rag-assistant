[site]: crossvalidated
[post_id]: 346314
[parent_id]: 
[tags]: 
Normality and weighting scores by difficulty

I want to conduct a 2 x 3 x 2 mixed model ANOVA with Group ( malus/bonus contract ) as a between-subjects factor, time pressure (nine, 12 and 15 seconds) as a within-subjects factor and loss aversion (high loss aversion/low loss aversion) as a second between-subjects factor. Essentially participants had to count the right amount of zeros from tables containing randomly generated tables of zeros and ones. Performance is measured by omission error (number of wrongly counted zeros), answering speed , points (0 points for no answer at all, 1 point for wrong answer exceedind the time limit, 6 point for wrong answer in the time limit, 10 points right answer exceeding time limit, 15 points right answer in time limit). Time pressure, which also was varied randomly (9,12,15 seconds), was induced by giving participants extra money for answering in a certain time span. The diffrence between Bonus/malus contracts was essentially only wording in the instructions. Loss Aversion was measured with a lottery task and then dummy coded for high vs low loss aversion. So there is variable for points/errors/speed with a 9 second time limit, 12 second time limit and 15 second time limit. Now there are 4 groups in total with the following sample seizes: Unfortunatly normality is violated even after controlling for outliers (by winsorizing). Here is an example of subgroup Malus-High Loss Aversion. As you can see I have devided the skewness/kurtosis with its error (on the right side) but some still exceed the value of 1,96: Ok I then realized that since the sample seizes are rather small the difficulty of the counting task might have failed to be randomized and therefore would interfere with the performance measures. So I thought maybe If I could create weights for difficulty this might help. I now have prepared the data, which arguably defines difficulty, this includes amount of zeros , the amount of ones , and the averaged ratio of zeros and ones for each row . T-test for a median split for the amount of zeros revealed some diffrences but basically diffrent inbetween groups and performance measures. Does anyone have any ideas for these measures? I transformed the data (log10) but this didn't help and as I have read that diffrently skewed data between groups is tricky to normalize this way. I tested the residuels for diffrences in normality. I also looked into Mehmet Guven Gunver, 2018 paper Mehmet Guven using a online calculator , but I didn't find any cutoff values, but basically it is confirming my data is skewed. I thought about using relative weight analysis to account for difficulity but im not sure if this is the right method. Probabyly SPSS is not the best program for weighting the data either. The artificial structuring of the data could account for the skewed distributions. As I mentioned above they are structured by the time pressure varibale threefold but the order was actually randomized.
