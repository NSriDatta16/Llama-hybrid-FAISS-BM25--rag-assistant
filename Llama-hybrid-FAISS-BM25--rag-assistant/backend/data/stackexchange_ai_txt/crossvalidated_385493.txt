[site]: crossvalidated
[post_id]: 385493
[parent_id]: 311084
[tags]: 
You almost wholly and correctly answered your blog question. I created a few simulations to show you where the differences are. Also, I have a personal issue with the term of art, “overdispersed.” Predictive distributions are not overdispersed, they are correctly dispersed, but the true distribution in nature doesn’t match it. Instead, it is contained within it. The two types of predictions differ in a number of subtle ways. Which type of prediction to use should depend wholly on the actual problem you face and the losses you face from unfortunate sampling or other issues. The first difference comes from the existence of prior information. In the absence of prior information, Frequentist methods minimize the maximum possible loss you could face from a bad estimator. That is very advantageous when you have no framework to estimate an average loss instead. Bayesian methods minimize the average loss experienced, but the prior does matter. In the presence of prior information, all Frequentist solutions are inadmissible. Furthermore, Frequentist methods do not give rise to coherent probabilities and so should never be used in gambling situations such as estimating inventory needs, portfolio allocations or budgets. The question of whether you should use a Bayesian density or a Frequentist interval should depend only on the problem you are solving. I decided to exaggerate your example a little by expanding a metaphor I use a lot to teach the differences in prediction methods. I modified the game as well to fit the Reverand Bayes original example because it had both a Bayesian and a strong Frequentist interpretation. Although the Reverand Bayes used a billiards table to generate a uniformly distributed random number, I used the R language function runif(). Using seed 9817, I drew a value of the parameter of 0.5171191. Based on this parameter value eight Bernoulli trials were performed. There were four successes. The question is now to predict the number of successes, based on the eight observed trials, over the next 10,000 observations. Knowing the conditions of the generation of the parameter, I used a uniform prior density for the Bayesian solution. The Frequentist solution is an interval, but it does generate a density from which to create an interval. For a binomial likelihood, the Frequentist prediction interval is constructed from the hypergeometric distribution. For the Bayesian method, the prediction is a one-dimensional Polya distribution. They are very close, but they are importantly different. The Frequentist density from which to construct intervals is shown here, while the Bayesian density is shown here, if the true value of the parameter were known, then the prediction would be this one instead. To get a better feel, also consider the cases where the true value were 2/3, and there were five successes in eight trials. The Frequentist is, the Bayesian is, the true prediction is. Now as to the differences, imagine that the parameter space was discrete. Let us consider the case where the only three possibilities were a parameter space of $\{1/3,1/2,2/3\}.$ There isn’t a known way to construct predictive intervals on discrete parameter spaces. If the prior were uniform, then the Bayesian predictive distribution would be like this. Of course, this is not the only difference. There is only one way to construct a Bayesian predictive density, but there is an infinite number of predictive intervals because predictive intervals are built on top of confidence intervals. They depend entirely on the cost function chosen. As such, the Frequentist interval that is usually taught as “the” Frequentist interval is the one that minimizes the average loss under the Kullback-Leibler Divergence. It is not uniquely the Frequentist prediction interval method. Change the loss function, and you will change the boundaries. The reason this is usually used is that the Bayesian predictive density automatically minimizes the Kullback-Leibler Divergence. Note I did not say it minimized the average divergence, but the actual divergence. To illustrate the differences, I drew two samples from $$\frac{1}{\pi}\frac{1}{1+(x-\mu)^2}.$$ The samples were $\{-1.5,-1,-.5\}$ and $\{-6,-1,3\}.$ Because there are issues with using the maximum likelihood estimator on small samples from this distribution, I used the median instead. At this sample size, the information loss is trivial for the accuracy gained. One of the nice things about the Frequentist interval method is that it can be created from any statistic with a sampling distribution, although if you change the statistic, then you change the predictions. As it is known there is no analytic solution to the above problem, I created the prediction by drawing 10,000,000 samples of size four. I used the first three observations to create the statistic, and the fourth was to be predicted. Note that because both samples share the same median and because both samples have the same known scale parameter, the distribution from which to construct the Frequentist prediction intervals is the same for both. It is an averaging process — the Frequentist method controls for worst-case samples. It minimizes average loss, but not a specific loss as the Bayesian method would. There are a few other minor differences. A Frequentist prediction interval is uniformly distributed over the interval. There are no dense areas. Its interpretation would be that upon repetition, and $\alpha$ percent prediction interval will cover the prediction no less than $\alpha$ percent of the time. A Bayesian interval would be interpreted as having an $\alpha$ percent probability of containing the future sample. Of course, for both, it is contingent upon the prior sample actually seen. Furthermore, there is an infinite number of intervals from a Bayesian density because there is no restriction that you use the highest density region. You could choose the lowest density region if you wished. Any subset of the density that adds to $\alpha$ percent is a valid $\alpha$ prediction interval. The highest density region is usually used because it has other optimality properties.
