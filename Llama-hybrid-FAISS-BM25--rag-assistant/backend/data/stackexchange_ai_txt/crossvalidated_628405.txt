[site]: crossvalidated
[post_id]: 628405
[parent_id]: 
[tags]: 
Leverage and residuals - Show $\frac{e_i^2}{\| (I - H)Y \|^2} \le 1 - h_{ii}$ where $e_i$ is the $i$-th residual and $h_{ii}$ is leverage

Question : Suppose that $\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$ , and the errors have zero mean, and are uncorrelated with constant variance. Let $\hat{\boldsymbol{\beta}}$ be the least squares estimator of $\boldsymbol{\beta}$ . Let $\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^T \boldsymbol{X})^{-1}\boldsymbol{X}^T$ be the hat matrix, and $h_{ii}$ be the $i$ -th element on its diagonal. Show that $$ \frac{e_i^2}{\| (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{Y} \|^2} \le 1 - h_{ii}$$ where $e_i = Y_i - x_i^T \hat{\boldsymbol{\beta}}$ is the $i$ -th residual and $\boldsymbol{I}$ is the $n \times n$ identity matrix, where $n$ is the size of vector $\boldsymbol{Y}$ . Attempt : I got as far as writing the $e_i$ in terms of the matrices above. $$ e_i = \sum_j (\delta_{ij} - h_{ij})Y_j = Y_i - \sum_j h_{ij}Y_j,$$ where I use $\boldsymbol{e} = (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{Y}$ . I square to obtain $$ e_i^2 = Y_i^2 - 2 Y_i \sum_j h_{ij}Y_j + (\sum_j h_{ij}Y_j)^2,$$ which is then simplified to $$ e_i^2 = Y_i^2 - 2 Y_i \sum_j h_{ij}Y_j + \sum_j h_{ij}^2 Y_j^2 + \sum_j \sum_{k \neq j} h_{ij} h_{ik} Y_i Y_j.$$ This did not lead me anywhere further. Could anyone please offer me a hint?
