[site]: crossvalidated
[post_id]: 462272
[parent_id]: 462213
[tags]: 
Let me try to illustrate with the most simple regression example, that of a regression on a constant. The idea is similar in general regressions. We then know that the OLS estimator $\hat\beta=\bar{y}$ , the sample mean. The t-statistic is $$ t=\frac{\sqrt{n}\bar{y}}{\sqrt{Var(\bar{y})}}$$ In the denominator of a t-statistic, we therefore require an estimator of $Var(\bar{y})$ , related to the so-called long-run variance of $Y_i$ . As, for example, this answer demonstrates, this means the variance of a single $Y_i$ , call it $\sigma^2$ , plus two times autocovariances. When we have positive serial correlation, these autocovariances will generally be positive, meaning that a good estimator (in the sense of yielding t-ratios that, asymptotically at least, behave like $N(0,1)$ random variables if the null is true) of $Var(\bar{y})$ will be larger than the default estimator that takes the average of the squared residuals and thus only estimates $\sigma^2$ . Put differently, if we nevertheless use the default estimator which is inappropriate under serial correlation, t-ratios will on average be too large ("inflated"), meaning tests reject too often. For example, one can show that the long-run variance of an AR(1) process is $$ \sigma^2/(1-\phi)^2 $$ We see that this is larger than $\sigma^2$ when $\phi>0$ , but smaller if $\phi , so that "deflated" t-statistics are indeed also possible.
