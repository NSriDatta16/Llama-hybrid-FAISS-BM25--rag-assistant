[site]: crossvalidated
[post_id]: 66950
[parent_id]: 
[tags]: 
Intuition for recursive least squares

The least squares formula, $\beta = (X'X)^{-1}X'Y$ can be recursively formulated as \begin{align} \beta_t &= \beta_{t-1} +\frac{1}{t}R_t^{-1}x_t'(y_t-x_t\beta_{t-1}),\\ R_t &= R_{t-1}+\frac{1}{t}(x_t'x_t-R_{t-1}), \end{align} where $\beta_{t}$ denotes the least squares estimate using the observations $1,\ldots, t$, and $R_t$ denotes the matrix $\frac{1}{t}X_t'X_t$. This can be shown by induction, and is neat since all we need to know about previous observations $(x_i, y_i)_{i=1,\ldots, t-1}$ is captured by $\beta_{t-1}$ and $R_{t-1}$. The book I am reading suggests a proof by induction, which is not too hard and settles correctness. However, it seems to me that a clever projection argument and/or a clever Bayesian interpretation could help with getting a feel for the recursive formulation. After a quick googling session, I have not found either. So in short: What is your favorite intuition behind recursive least squares?
