[site]: crossvalidated
[post_id]: 585696
[parent_id]: 427600
[tags]: 
I'll substitute linear Gaussian state space model for Kalman filter here. Similarities they both model time series they both have a hidden/latent "state" or "layer" process for which there is no data the observed dependent sequence depends/conditions on the above process the hidden/latent state/layer can depend on independent predictors/inputs/covariates/exogenous variables, etc. they both have parameters that (usually) must be learned/estimated Differences RNN have nonlinear activation functions while linear Gaussian state space models have linear state equations and observation equations Linear Gaussian state space models have additive noise terms while RNN's do not Misconceptions RNNs are a class of models and Kalman filters are an algorithm. This makes comparison between the two misleading. Kalman filters assume the parameters of the time series model are known, but that does not mean the models they are used on--linear Gaussian state space models--cannot have their parameters estimated/learned--they usually are, and the Kalman filter's likelihood evaluations can be used in an optimization or sampling-based strategy. I have little experience with RNNs, but I tend to think that the last item in the "Differences" category is the most important. For example, RNNs such as long short term memory (LSTM) models and gated recurrent units (GRUs) have "forget gates" that describe how the hidden layer "forgets" its past values deterministically . On the other hand, a state space model's "forgetting" is random. In my very humble opinion, I think this probably explains why RNNs are used a lot for non-noisy data such as text data, while lgSSMs are used on data that is "more random" such as financial returns. To be completely truthful, though, again, I don't have much experience with RNNs, so I don't claim this with any certainty. The linear/nonlinear distinction is not so important. State space models can have nonlinear state and observation/emission equations and indeed frequently do. When this happens, though, you can't use the Kalman filter anymore, but there are many other approximate filtering techniques. Regarding the other answer supposing that RNNs are "arbitrarily complex"--that reminds me of a Tweet I read a while back: https://twitter.com/sirbayes/status/1537177495866327040?s=20&t=eJ8U-Az5Tn_P0vH1afKLAw I have a hard time engaging in this debate, myself, though.
