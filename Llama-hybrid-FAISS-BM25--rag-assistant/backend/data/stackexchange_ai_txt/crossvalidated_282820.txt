[site]: crossvalidated
[post_id]: 282820
[parent_id]: 282804
[tags]: 
I always like to think of logistic regression as what happens if you apply a binary decision to a linear model. That is, let's assume there is some underlying relationship that follows the linear model: $$ y = X\beta+\varepsilon $$ where $X$ is your independent variable and $\beta$ the coefficient (or slope) on that variable, and $\varepsilon$ is random noise. And then let's say we apply a function to the continuous variable $y$ that maps it onto a binary outcome: $$ f(y) = \left\{\begin{matrix}0, ~\operatorname{if}~ y \operatorname{\leqslant \theta} \\1, ~\operatorname{if}~ y \operatorname{>\theta} \end{matrix}\right. $$ where $\theta$ is a threshold. What is the probability that this function returns $1$, given a certain value of $X$? If we assume that $\varepsilon$ is Normally distributed with mean $0$ and variance $\sigma^2$, then we can calculate this probability as: $$ p(f(y)=1|X)=p(y>\theta|X)=\int_\theta^\infty N\left(y; X\beta, \sigma^2 \right)dy $$ In other words, this is computing the area under the Normal distribution that is to the right of the threshold. Note that this probability is essentially what a logistic regression model tries to describe. And indeed, if you plot this probability as a function of $X$, you get something pretty close in shape to the logistic function (in fact the logistic function is often used as a convenient approximation to the cumulative Normal distribution). For values of $X\beta$ near the threshold, the probability that $y$ will be above threshold is near $0.5$, because the noise $\varepsilon$ can sway the outcome either way. As you increase $X$, $X\beta$ will get further away from $\theta$ and $f(y)=1$ becomes more likely. Crucially, how quickly $p(f(y)=1|X)$ increases with $X$ depends on two things: the slope $\beta$ and the noise variance $\sigma^2$. More precisely, it depends on the ratio $\frac{\beta}{\sigma}$. It is this (signal-to-noise) ratio which determines the (expected) coefficient you get from a logistic regression. In other words, you can think of the coefficients in a logistic regression as controlling how much each independent variable needs to change relative to the noise in the data in order to increase the probability of a certain outcome by some amount. Now to come to your question: you're asking if it is possible to eliminate all randomness, i.e. to have no noise. This would mean that $\sigma$ equals $0$, and therefore $\frac{\beta}{\sigma}$ would be undefined (or "infinite"). This explains what you found, that you cannot estimate the coefficients when there is no noise. Indeed, you can think of the perfect separation you achieve without noise as corresponding to an infinite coefficient on your independent variable, since (for $X\beta$ near the threshold $\theta$) you only need to change $X$ an infinitesimal amount in order to go all the way from $p(y>\theta|X)=0$ to $p(y>\theta|X)=1$. Edit: actually one thing you could do is instead of drawing samples from a binomial distribution to simulate your data, replace these samples by their expectation, i.e. the probability predicted by the simulated logistic function. That way, you're removing the randomness that derives from simulating a limited sample (i.e. the sampling variability), and thus your coefficient estimates should then equal the ground truth (since there is one logistic function that exactly fits these values).
