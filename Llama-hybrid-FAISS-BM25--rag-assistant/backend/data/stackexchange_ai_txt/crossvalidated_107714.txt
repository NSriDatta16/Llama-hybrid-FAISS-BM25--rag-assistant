[site]: crossvalidated
[post_id]: 107714
[parent_id]: 107711
[tags]: 
Actually, the p-value does change but given that it rounds down to 0.000 in the first place you weren't able to see the difference. The bootstrap is mainly used for test statistics rather than regression coefficients. For a light introduction, have a look at the Wikipedia on bootstrapping or for a more thorough treatment see Horowitz (2001) . Also if you type help bootstrap in Stata you can click on [R] bootstrap on the top left of the help file which will open the Stata manual which very well documents the bootstrap (under the heading "remarks and examples"). Secondly, you have specified the bootstrap in the vce (variance-covariance estimator) option. With this you instructed Stata to bootstrap your standard errors and nothing else - that was the less technical explanation to your observation that the coefficients don't change. When you call this option, Stata takes $k$ bootstrap samples ($k = 1000$ in your case) from your original data. It then uses the formula $$\widehat{se} = \left( \frac{1}{k-1} \sum_{i=1}^{k} (\widehat{\beta}_i - \overline{\beta})^2 \right)^{\frac{1}{2}} $$ to calculate your standard error where $\widehat{\beta}_i$ is your coefficient estimate from each bootstrap sample and $$\overline{\beta} = \frac{1}{k}\sum_{i=1}^{k}\widehat{\beta}_i$$ is the average of the $k$ bootstrap estimates. So instead of calculating your standard error analytically via a specific formula (which sometimes may not even be available), you are obtaining the sampling distribution of your $\widehat{\beta}$ by repeatedly sampling from your data. From the above expression of the standard error you see that this is just the standard deviation of this sampling distribution. For instance, this sampling distribution may look something like this for which you can then use your standard error for inference like the 95% confidence region underneath the curve and the like. So your estimated coefficient doesn't change (what I called "mean" in the graph) but you obtain the standard error via re-sampling rather than calculating it in the usual way as $\widehat{se}(\widehat{\beta}) = \sqrt{s^2 (X'X)^{-1}}$. Some caution must be used with the bootstrap because it is easy to misapply it. For example, it will only provide inconsistent estimates of your standard errors when your observations are correlated in some sense and not independent (see again Horowitz (2001) for the different assumptions underlying the bootstrap). A good overview of this is also given in Cameron and Trivedi (2010) "Microeconometrics Using Stata".
