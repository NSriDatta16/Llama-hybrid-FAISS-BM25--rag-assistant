[site]: crossvalidated
[post_id]: 361250
[parent_id]: 
[tags]: 
Generalizing on-policy distribution of states

The box on page 199 of Reinforcement Learning: An Introduction, Second Edition talks about distribution of states in on-policy episodic settings. I want to generalize it for continuing tasks as is asked to do in some of the exercises in chapter 13. It seems to be a simple fix of adding {\gamma} to the second term (as mentioned at the bottom of the box), however, I am unable to see how that discounts the fraction of states in long term. Does someone have a more sound proof of this? EDIT: The question should be relevant for episodic discounted case rather than for continuing task.
