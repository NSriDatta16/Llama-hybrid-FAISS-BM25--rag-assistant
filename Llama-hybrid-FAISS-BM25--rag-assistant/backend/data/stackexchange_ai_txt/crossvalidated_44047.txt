[site]: crossvalidated
[post_id]: 44047
[parent_id]: 44034
[tags]: 
Maths part: If you write the PCA decomposition as $X = S \times L$ with scores $S$ and loadings $L$ reconstructing $X$ without the $i$th principal component becomes $X_{-i} = S_{\cdot, -i} \times L_{-i,\cdot} = X - S_{\cdot, i} \times L_{i,\cdot}$ (bit sloppy notation, but you get it; I use $-i$ for leaving out the $i$th component) The second is easier to calculate if just few components are to be left out (less multiplications needed). So you just have a normal PCA reconstruction that leaves out some components. Searching for PCA noise filter or the like should give you a wealth of papers. Does it make sense: ultimately this can only be known by intimate knowledge of the data I've seen similar things with PCA on spectroscopic data sets (which share the order of magnitude of variates with the microarrays). Sometimes some physical / chemical / biological process is a major source of variance. If I can identify this, and can thus reason why it is sensible to exclude this from further analysis, I'm fine excluding the respective source of variance. This doesn't have anything to do with the general noise level, i.e. I'd still expect that higher PCs carry measurement noise. That is, in the end, you may want to use PC scores $n$ to $m$ only. I often find that in such situtations, partial least squares regression (PLS) is a good alternative (if applicable, i.e. you do have an outcome to which you want to correlate your $X$). However if the artifact is known, you may want to correct/filter it on a "hard modeling" basis (i.e. start from the cause and develop a correction or filter) instead of the PCA, because in future data sets there may be only few measurements affected. Then this artifact does not end up in the top PC, but in some PC. I.e. you cannot rely on where in the PC this artifact will end up. To sum up : I'm OK with using such an approach for descriptive modelling, but I'd be very careful/suspicious if prediction is intended.
