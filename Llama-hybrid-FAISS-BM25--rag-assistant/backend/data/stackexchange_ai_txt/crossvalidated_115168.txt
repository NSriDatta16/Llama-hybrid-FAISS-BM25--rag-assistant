[site]: crossvalidated
[post_id]: 115168
[parent_id]: 
[tags]: 
How to model the sampling distribution of the sample sum

I'm stuck on a stats problem and am wondering if someone can point out the error in my logic. Imagine you are planning for a camping trip for 50 people. Each person consumes 2.0 lb of food per day on average, with a standard deviation of 0.7 lb. What is the least amount of food you need to bring to be 97.5% sure that you will have enough food? First line of reasoning: The standard deviation for the average amount consumed by one person is 0.7/sqrt(50) The standard deviation for the total amount consumed by all 50 people is sqrt(50)*0.7 The desired answer is thus 2.0*50 + 2*sqrt(50)*0.7. Second line of reasoning: SD(n*x) = n*SD(x). Thus, the standard deviation for the amount consumed by 50 people is 50*0.7 The desired solution is then 2.0*50 + 2*50*0.7 I know the second line of reasoning is wrong. Could someone explain why? Thanks! ** EDIT ** I think the error is that the standard deviation for the amount consumed by 50 people is not 50*0.7, because the people are independent. VAR(amount consumed by 50 people) != VAR(50 * amount consumed by one person)
