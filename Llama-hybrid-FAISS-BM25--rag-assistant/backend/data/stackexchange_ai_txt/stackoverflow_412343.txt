[site]: stackoverflow
[post_id]: 412343
[parent_id]: 412019
[tags]: 
Remember, that any changes in this activation function come at cost of different behavior . This even includes switching to float (and thus lowering the precision) or using activation substitutes. Only experimenting with your use case will show the right way. In addition to the simple code optimizations, I would also recommend to consider parallelization of the computations (i.e.: to leverage multiple cores of your machine or even machines at the Windows Azure Clouds) and improving the training algorithms. UPDATE: Post on lookup tables for ANN activation functions UPDATE2: I removed the point on LUTs since I've confused these with the complete hashing. Thanks go to Henrik Gustafsson for putting me back on the track. So the memory is not an issue, although the search space still gets messed up with local extrema a bit.
