[site]: datascience
[post_id]: 68718
[parent_id]: 68708
[tags]: 
A ReLU serves as a non-linear activation function . If a network had a linear activation function, then it wouldn't be able map any non-linear relationships between the input features and its targets. This would render all hidden layers redundant, as your model would just be a much more complex logistic regression. So I think a better question you should be asking is: Why do you need a non-linear activation function? As an intuitive example on what this means you can head on to TensorFlow playground and try this out for yourself. Just change the activation function; the rest of the default settings work well to illustrate the point. URL for trial with ReLU URL for the same function with a linear activation.
