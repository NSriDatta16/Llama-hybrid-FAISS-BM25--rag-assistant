[site]: datascience
[post_id]: 104110
[parent_id]: 104102
[tags]: 
Apparently what you've done so far is to apply various standard techniques by trial and error. I'd suggest you could investigate what happens with your models and your data by doing a manual error analysis. Take a random sample of instances which are incorrectly predicted and try to understand why this happens. Errors can can happen for various reasons: The gold label is wrong or questionable, sentiment can be subjective. A tweet considered neutral by A could be negative for B. If the same kind of tweet has different labels, the model cannot find good patterns to predict the label. If this kind of problem is common then the gold data is low quality and there's not much to do with it. The model doesn't catch some clues that are obvious to a human. Identify what are these clues and why the model fails to catch them. This way you can design features in a more informed way in order to improve performance (feature engineering). But first you should probably avoid resampling, and check for overfitting: the performance on the test set should not be much lower than on the training set. When you use complex features like all the n-grams, it's likely that you cause the model to overfit.
