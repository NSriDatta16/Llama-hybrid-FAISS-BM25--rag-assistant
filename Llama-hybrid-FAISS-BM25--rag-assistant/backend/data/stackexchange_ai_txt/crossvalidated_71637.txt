[site]: crossvalidated
[post_id]: 71637
[parent_id]: 71626
[tags]: 
Effects should be different across experiments, and so should variances. That's the nature of sampling. What you have is just different samples being different. There's no way to know which estimate of variance is closer to true value, or even guess at it with the information you've given and equal N's in the samples. So, while you'd like it to be the smaller one, that may not be correct. More than likely the average variance is best. Your general tactic here of searching for an effect may eventually bare fruit. You may combine all of the subjects into one experiment, run a few more, drop an outlier here or there, look for various analysis techniques, and voila, a significant effect. Maybe you won't do all of that but I'm trying to point out that you're thinking about it wrong. Use the data you have to make your best determination about the truth of the matter, not show an effect. An important thing to keep in mind is that an unstated assumption about any statistical test is that you're performing it because you want to know the answer to the test, not because you've previously done other tests and failed to find what you would like to find. So now, because you've already done the test, the rate of Type I error is no longer what you set it to be, alpha. You're increasing the probability of finding an effect whether there is one or not. That said, you could do something that's not a test. You could construct a confidence interval of the effect through a mega-analysis (just combine all of the data) and report that as a higher quality estimate of the effect than either experiment had alone. You will have to concede that what you've done is post hoc and describe the tests that you did do already. But this is probably the best way to report what you've done so far.
