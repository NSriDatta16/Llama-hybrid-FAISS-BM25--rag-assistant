[site]: datascience
[post_id]: 112140
[parent_id]: 112130
[tags]: 
There is some confusion in your question, since you are talking about an AE, but you are using softmax as output... which makes almost no sense First of all, the loss by itself, has no meaning : If I give you two models with two different losses (in your case, two AE), you cannot, in any way, guess which is the best one (therefore a loss that is 20 means nothing) About your questions: depends on the distribution... one hot encoding an output supposes a multinomial distribution, which you can approximate with both a softmax and sigmoid, but 99% of the times you should use a softmax, since encodes constraints of the distribution directly, and the NN don't have to learn them (like when approximating a probability, you can for sure use a linear output layer, but the NN has to learn to output values between 0 and 1, and there is no guarantee that it will always do that) an AE aims to reduce the dimensionality... you start with 4 features and you allow it to use 6 dimensions (in this case we are talking about overcomplete AE, but you need some constraint to make them work as expected) welcome in the world of non convex optimization... if the starting point is random (which is since the weights are initialized randomly), you will probably get to a different local minima an autoencoder aims to learn the identity function ( $decoder(encoder(x)) = x$ ) therefore obviously no, the input has to have the same shape as the output At this point, I'm pretty certain that an AE is not what you are looking for, but most likely a normal discriminative NN (since you are using cat-cross-ent as loss function)
