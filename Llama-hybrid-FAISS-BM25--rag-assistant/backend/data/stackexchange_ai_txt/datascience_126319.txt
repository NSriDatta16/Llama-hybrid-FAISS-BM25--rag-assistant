[site]: datascience
[post_id]: 126319
[parent_id]: 126317
[tags]: 
The usual approach to encode discrete stuff in NLP is to use an embedding layer, which saves you a huge matrix multiplication if you were using one-hot input representations. Once you have your discrete feature encoded with an embedding layer, you have two options to merge it with the rest of the data: concatenate it or add it. If you concatenate, you have to assign dimensionalities to each part a priori and the more space you allocate for a feature, the less space there is for the rest. This approach was used for "factored neural machine translation" (see the original scientific article ) to encode morphological features into the associated words (e.g. verbal tense for verbs, number and gender for nouns, etc). On the other hand, if you add your embedded feature to the rest of the data, it means that the embedding vector for the feature is of the same dimensionality as the rest of the data. This approach is used in transformers to add positional encodings to token encodings: I suggest you go with this approach. In my experience gives better results in general. Of course, either way the features can be ignored during training if they are not useful to improve the loss.
