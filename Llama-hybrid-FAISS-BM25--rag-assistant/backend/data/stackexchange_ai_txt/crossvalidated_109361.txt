[site]: crossvalidated
[post_id]: 109361
[parent_id]: 85869
[tags]: 
In principal it shouldn't affect the performance of an SVM very much, because the SVM uses regularisation to avoid over-fitting (c.f. ridge regression), which is also effective in making the optimisation problem well posed once more. However, the problem is then to find a good value for the regularisation parameter, $C$. Also the SVM is based on an approximation of a bound on generalisation performance that is independent of the dimensionality of the feature space, so at least for linear SVMs, it would be reasonable for it to have little effect. For non-linear SVMs, it will depend on the sensitivity of the kernel to the correlation. For the common RBF kernel, it will mean that the induced feature space will be a little more sensitive to a particular direction in the input space, but probably not enough for it to be problematical. In my experience, it is all too easy to make the performance of kernel methods worse by manually tinkering with the input representation in order to improve performance. Generally this will just end up over-fitting the test data, as the operator has become part of the model selection procedure in a way that is not properly taken into account in performance estimation. A good example of this is provided by machine learning challenges (e.g. kaggle) that have a leader board. Quite often those that work hard to move up the leaderboard find themselves near the bottom of the final rankings (which are determined on a separate sample of data). The reason is often "over-tuning".
