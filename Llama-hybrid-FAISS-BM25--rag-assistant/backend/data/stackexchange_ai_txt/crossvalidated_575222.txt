[site]: crossvalidated
[post_id]: 575222
[parent_id]: 
[tags]: 
Estimation of standard error in observables generated from time series data

Imagine that I have time series data which are time-correlated, non-scalar, and of unknown, but identical distribution From this time series I have a function that takes an subset of X as input to calculate a scalar quantity Q What I would like to know is how do you estimate the standard deviation of ? That is to say the standard deviation of the quantity calculated by passing the full dataset to function F. I have seen methods of dealing with time-correlated data before. For example if each random variable were a scalar and we wanted to calculate the standard error of the mean of the underlying distribution, we could use the Block Averaging method. Does an analogous technique exist for this case where I am trying to estimate the standard deviation of ? I should further note that for my case converges for large sample sizes (i.e. when N is large) and because of this I am using as an estimator for the quantity calculated if the entire population were passed to function F. Also, the range of the function is and has the property (the function applied to any single data point always gives a value equal to 1)
