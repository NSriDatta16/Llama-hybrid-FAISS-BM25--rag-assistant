[site]: crossvalidated
[post_id]: 355478
[parent_id]: 
[tags]: 
How should an agent learn about a good state?

I am trying to train an agent in an environment (let's say a grid environment) to learn to get to a specific location (let's call it G). The rewards the agent get from this environment is in this way: if the agent gets to G, it receives a reward of 1 and otherwise, it receives the reward of 0. My problem is that it is very difficult for this agent to get to G by choosing random actions at each location it is in because the probability of getting to G by choosing random actions is almost zero. Due to the reward structure of the environment, if the agent doesn't get to G, it cannot learn anything about the environment. Now, my question is that is there any solution for this issue? One idea that comes to mind is to change the reward structure of the environment. That is, instead of just having 0 and 1 as rewards, we assign reward of 0.5 to the neighbors of G, 0.25 to the neighbors of neighbors of G, etc. Is this a valid solution?
