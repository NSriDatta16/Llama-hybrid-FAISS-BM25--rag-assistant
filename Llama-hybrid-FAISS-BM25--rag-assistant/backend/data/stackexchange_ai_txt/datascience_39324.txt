[site]: datascience
[post_id]: 39324
[parent_id]: 39305
[tags]: 
Yes, this is common knowledge. Every time you add a parameter to a model, you will need to give it more data in order for it to learn as well as the simpler model. Every individual weight in a neural network is a parameter. The more weights, the more parameters, and the more data is needed. One of the fundamental tasks in model-building is finding a good tradeoff between having enough parameters to learn fine details and having enough data to train all those parameters. Too many parameters lead to overfitting in part because if you don't have enough data you end up memorizing individual cases, as opposed to learning about and averaging over a spread across several cases. This is one reason why class imbalance is a problem. If you don't have enough data on one of the classes, then that class will be poorly understood by the model.
