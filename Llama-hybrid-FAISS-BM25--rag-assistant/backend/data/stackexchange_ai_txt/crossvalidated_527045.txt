[site]: crossvalidated
[post_id]: 527045
[parent_id]: 
[tags]: 
Is this drop in training accuracy due to a statistical or programming error?

I am training a Feedforward NN on the MNIST dataset and hit a roadblock. My accuracy only goes to about 80% and the cost only about 0.32. This is the case for NNs that have a hidden layer with 10 neurons. Some tests with other example code (Michael Nielsen's Neural Networks and Deep Learning book) let me conclude that my network has too few nodes to sufficiently learn the classification (I could barely achieve 90% accuracy with the same model and his example code). However, as soon as I add one neuron, the training peaks at about 90% accuracy and then goes downhill. I suspected overfitting, so I logged the accuracy for the training set beside the accuracy for the validation set. According to this post, I had expected a growing discrepancy between those values, but turns out, both peaked and the decreased: (the X-Axis is in number of mini-batches / 100) I am running with a batch size of 10. The learning rates I've tried are 0.3, 0.1 and 0.02, all yielding the same result, just slower. As the network is a self made implementation, the cause might also be a bug, but I don't think because of the working example with 10 hidden neurons. What might also cause this drop? Is this overfitting? Or are there indicies to other mistakes? For anyone interested, you can view the code here (Rust). The entire PRNG is seed-based, so all scenarios are reproducable. To run it, just cargo run --release (in the network/ dir). Try to change the network dimensions and the learn factor in main.rs.
