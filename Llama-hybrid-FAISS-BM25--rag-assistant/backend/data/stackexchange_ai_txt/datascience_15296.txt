[site]: datascience
[post_id]: 15296
[parent_id]: 
[tags]: 
How can I run SVM on 500k rows with 81 columns?

I have about 500k rows of data. I'm new to data science and I'm trying to train a model utilizing support vector machines as part of my analysis. On my little macbook pro, it seems endless. Right now I'm using an RBF kernel and am not seeing an end to this computation. I do however have access to 100+ compute nodes with 16 cores a piece. My problem is, I don't know how to utilize this to my extent as I am lacking knowledge on how I should approach this SVM. Right now I am using scipy. SciPy Code def makeModelandPrediction(trainData, trainLabel, testData): model = svm.SVC( C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False, ) model.fit(trainData, trainLabel) prediction = model.predict(testData) return prediction I've already preprocessed the data and done a 70/30 train/test split on the data. Can someone point a beginner in the right direction?
