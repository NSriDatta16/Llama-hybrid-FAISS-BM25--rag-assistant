[site]: crossvalidated
[post_id]: 485603
[parent_id]: 485022
[tags]: 
TL;DR : ANOVA pools information among all observations to get the best estimates of fixed effects, random effects, and error variance. If you want to examine normality of ANOVA residuals, doing so after all fixed and random effects are taken into account thus makes the most sense. Reliable ANOVA estimates don't require normality of residuals; the issue is the distribution of the test statistics. In repeated-measures ANOVA, issues like imbalance or mis-specification of correlation structures might be even more substantial obstacles to reliable statistical tests. ANOVA is simply a particular type of a linear model, as described for example on this page of one of the sites that was linked from the question, and discussed extensively here . Like all linear models, ANOVA combines information from the combinations of predictor values to model the outcome values as a function of the predictors plus an error term. The error term is assumed to have a certain distribution shared among all cases, Gaussian with zero mean for standard ANOVA. Information about the distribution of the error terms is obtained by pooling across all the observations, smoothing out the vagaries that can happen just by chance within individual cells of the ANOVA design. A standard normal q-q diagnostic plot thus examines all the residual values, not those within individual cells. Despite the usual assumption of Gaussian errors in an ANOVA model, the significance tests don't necessarily require that assumption to be met. Significance tests in ANOVA are tests on regression coefficients. It's thus the sampling distributions of those regression coefficients that must adequately meet assumptions when one performs a standard parametric test. As @whuber put it in a crucially important comment: What you really want to know is whether the assumed distributions of the ANOVA test statistics are sufficiently accurate to compute the p-values in which you are interested. If the model assumptions are met and the shared error term has a Gaussian distribution then you know that tests on regression coefficients will be valid.* But strict normality of the error term isn't required for tests on the regression coefficients to be valid. Think about normally distributed error terms as sufficient but not always necessary for an adequately reliable significance test on linear model regression coefficients, including ANOVA. That's not to say that it's useless to examine the distribution of residuals around model predictions that incorporate information from all cases. For example, the R lme4 package provides a normal q-q plot as one of its diagnostic plots; see page 33 of the vignette . What you will often find, however, is that substantial deviations from normality in such a plot of residuals mean that the model itself is poorly specified. That might be the most useful information from such a plot. With a mixed ANOVA model having only fixed categorical predictors and including all interactions, you shouldn't have to worry about linearity in the fixed-effect predictors themselves. But there could be an incorrect handling of the outcome variable (e.g., if it's fundamentally log-normal rather than normal), omission of critical covariates associated both with outcome and with the included predictors, or mis-specification of the random-effects structure. Fix those problems exposed by the diagnostic plot rather than obsess about the normality per se. To evaluate the model all the diagnostic plots should be examined: not only the q-q plot for normality of residuals but also the fitted vs. residual plot and the scale-location plot and the various profile plots (see page 36 of the vignette) for mixed models and their random effects. Examine undue influence of particular observations, e.g. with the influence.ME package in R. This process, rather than a simple examination of normality, is critical to evaluating and improving the quality of the model specification. If the model is properly specified then the normality assumption on the sampling distribution of the regression coefficients can be reasonably reliable. With enough data the Central Limit Theorem can help with that despite non-normal residuals, although how much data is "enough" depends on the particular case. See this answer , for example. If you don't want to rely on that assumption, bootstrapping provides a way to get non-parametric confidence intervals. But that should be done only when the model itself is adequately specified. As an edit to the question notes, some diagnostic plots can be generated from repeated-measures data analyzed by aov , which according to its manual page fits "an analysis of variance model by a call to lm for each stratum." Each stratum is a portioning of the means of the observations by progressively complex models, starting with the overall mean. As Venables and Ripley say on page 283 with respect to a simpler split-plot design: Multistratum models may be fitted using aov , and are specified by a model formula of the form response ~ mean.formula + Error ( strata.formula ) In our example the strata.formula is B/V, specifying strata 2 and 3; the fourth stratum is included automatically as the "within" stratum, the residual stratum from the strata formula. For more complicated models, the last stratum is thus the automatically included "within" stratum. Continuing on page 284: "It is not possible to associate [fitted values and residuals from the last stratum] uniquely with the plots of the original experiment." You need the residuals from "the projections of the original data vector onto the subspaces defined by each line in the analysis of variance tables." The residuals can be examined for every stratum, but only the final stratum takes all aspects of the model into account. This answer shows the code for the Venables and Ripley example in which the fourth stratum is the "within" stratum. Before proceeding with aov , however, pay attention to the following quote from its help page: Note aov is designed for balanced designs, and the results can be hard to interpret without balance: beware that missing values in the response(s) will likely lose the balance. If there are two or more error strata, the methods used are statistically inefficient without balance, and it may be better to use lme in package nlme . *This is more complicated with mixed models, for which there is dispute about the number of degrees of freedom to use in the test. But that dispute won't be resolved by examining the distribution of residuals. Tests on mixed models can also involve assumptions about the covariance structure of correlated observations.
