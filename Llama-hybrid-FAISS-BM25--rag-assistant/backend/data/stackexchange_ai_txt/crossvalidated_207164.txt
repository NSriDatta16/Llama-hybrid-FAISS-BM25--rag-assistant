[site]: crossvalidated
[post_id]: 207164
[parent_id]: 191729
[tags]: 
There isn't much point in using it in SVM solvers or perceptrons because the cost function being minimized has properties that make other algorithms more attractive (e.g. interior points, SMO, normal equations). I'm not an expert on non-linear optimisation, but the equation looks rather like the Newton steps used in the line search in conjugate gradient descent (but in the direction of the gradient, rather than a conjugate direction). If that is the case, it is used quite often for statistical models (such as neural networks) where the cost function does not have convenient properties that admit more efficient optimisation schemes. Caveat lector: My own neural network library used conjugate gradients, but I wrote it so long ago I can no longer remember the details.
