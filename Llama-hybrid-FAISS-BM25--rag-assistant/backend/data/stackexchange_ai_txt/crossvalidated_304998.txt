[site]: crossvalidated
[post_id]: 304998
[parent_id]: 304988
[tags]: 
Assume in general that you decided to take a model of the form $$P(y=1|X=x) = h(x;\Theta)$$ for some parameter $\Theta$. Then you simply write down the likelihood for it, i.e. $$L(\Theta) = \prod_{i \in \{1, ..., N\}, y_i = 1} P(y=1|x=x;\Theta) \cdot \prod_{i \in \{1, ..., N\}, y_i = 0} P(y=0|x=x;\Theta)$$ which is the same as $$L(\Theta) = \prod_{i \in \{1, ..., N\}, y_i = 1} P(y=1|x=x;\Theta) \cdot \prod_{i \in \{1, ..., N\}, y_i = 0} (1-P(y=1|x=x;\Theta))$$ Now you have decided to 'assume' (model) $$P(y=1|X=x) = \sigma(\Theta_0 + \Theta_1 x)$$ where $$\sigma(z) = 1/(1+e^{-z})$$ so you just compute the formula for the likelihood and do some kind of optimization algorithm in order to find the $\text{argmax}_\Theta L(\Theta)$, for example, newtons method or any other gradient based method. Notce that sometimes, people say that when they are doing logistic regression they do not maximize a likelihood (as we/you did above) but rather they minimize a loss function $$l(\Theta) = -\sum_{i=1}^N{y_i\log(P(Y_i=1|X=x;\Theta)) + (1-y_i)\log(P(Y_i=0|X=x;\Theta))}$$ but notice that $-\log(L(\Theta)) = l(\Theta)$. This is a general pattern in Machine Learning: The practical side (minimizing loss functions that measure how 'wrong' a heuristic model is) is in fact equal to the 'theoretical side' (modelling explicitly with the $P$-symbol, maximizing statistical quantities like likelihoods) and in fact, many models that do not look like probabilistic ones (SVMs for example) can be reunderstood in a probabilistic context and are in fact maximizations of likelihoods.
