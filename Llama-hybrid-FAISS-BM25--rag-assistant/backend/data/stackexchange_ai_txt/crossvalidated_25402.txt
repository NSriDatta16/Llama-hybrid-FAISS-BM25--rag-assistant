[site]: crossvalidated
[post_id]: 25402
[parent_id]: 25392
[tags]: 
This is a complicated question. The simple nearest neighbor matching pairs each observation in the treatment group with a single person in control group who has a similar propensity score. Then you compute the difference in outcome $Y$ for each pair, and then calculate the mean difference across pairs. That's your treatment effect. However, it is also possible to match each treated person with multiple untreated folks. Matching using additional nearest neighbors increases the bias, as the next best matches are necessarily worse matches, but decreases the variance, because more information is being used to construct the counterfactual for each treated person. Different matching estimators differ in how they weight the neighbor(s) in calculating this difference. One important question is whether you can pair the same control group person with more than one treated person, essentially recycling them. Matching without replacement can yield very bad matches if the number of comparison observations comparable to the treated observations is small. It keeps variance low at the cost of potential bias, while matching with replacement keeps bias low at the cost of a larger variance since you are using the same info over and over. That is another trade-off. But I digress. Here are some ways to do propensity score matching, in increasing order of complexity: The simplest form of matching is using only one control dude who has the closest propensity score (with or without replacement), and calculating the mean difference for all pairs. Another strategy is divide the $ps(X)$ into $S$ buckets or intervals. For example, say you have some treated observations with $ps(X)$ between 0.3 and 0.4. Then you take all the control group folks with scores between 0.30 and 0.4 and then use their average $Y$ as the counterfactual. The total treatment effect is $\Sigma_{s}(\bar{Y}_{T=1}-\bar{Y}_{T=0})*w_{s}$, where $w_{s}$ is the the fraction of all treated folks in bucket $s$. For example, you might start with 10 $PS$ buckets and they don't need to have the same width. Note that some treated observations may not have any matches! This is known as the common support problem. Yet another way would be to grab all control group members within a fixed radius of treated unit $i$ and use them as the counterfactuals. Call them group $J_{i}$. The treatment effect is $\frac{1}{T}\Sigma_{i}(\bar{Y}_{i,T=1}-\bar{Y}_{J})*w_{s}$. The bandwidth problem here takes the form of picking the radius. Kernel matching. Here you weight the control group observations who are further away in PS less heavily, maybe not at all. How do you pick a method? All matching estimators are consistent, because as the sample gets arbitrarily large, the units being compared get arbitrarily close to one another in terms of their characteristics. In finite samples, which one you choose can make a difference. If comparison observations are few, single nearest neighbor matching without replacement is a bad idea. If comparison observations are many and are evenly distributed, multiple nearest neighbor matching will make use of the rich comparison group data. If comparison observations are many but unevenly distributed (check the PS kernel densities for the two groups), kernel matching is helpful because it will use the additional data where it exists, but not take bad matches where it does not exist. One complications is that standard errors don't take into account that you estimated the propensity score (since the real thing is not observed), so they are too small. People either ignore this or bootstrap, which may or may not be bad idea.
