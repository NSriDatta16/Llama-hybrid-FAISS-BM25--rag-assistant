[site]: crossvalidated
[post_id]: 597052
[parent_id]: 313163
[tags]: 
Your understanding seems to be based on Proposition 1 of Goodfellow et al (2014) https://arxiv.org/abs/1406.2661 , that is, the optimal discriminator result $$D_G^*(x)=\frac{{\rm p}_d(x)}{{\rm p}_d(x)+{\rm p}_g(x)}$$ where ${\rm p}_d(x)$ is the data PDF and ${\rm p}_g(x)$ is the generator output PDF. Clearly if ${\rm p}_d(x)={\rm p}_g(x)$ (almost everywhere in $x$ ), then $D_G^*(x)=\frac{1}{2}$ , which is where the 50% figure comes from. Recent research by myself and a collaborator has shown that the optimal discriminator result requires ${\rm dim}(z)\geq{\rm dim}(x)$ , that is, the dimension of the latent variable is at least equal to that of the data. This is however not the case that applies in practice: most of the time for GANs we have ${\rm dim}(z) . In this case it can be shown that the generator output PDF contains ${\rm dim}(x)-{\rm dim}(z)$ delta functions. This invalidates the application of variational calculus used in the proof of the optimal discriminator result. The proof of these claims is contained in a peer reviewed IEEE Access paper: Convergence & Optimality of low dimensional GANs . The same paper also has a quasi-analytical counter-example for the simplest case of ${\rm dim}(z)={\rm dim}(x)=1$ , i.e., a one-dimensional LSGAN, which uses a least-squares loss function. (Here quasi-analytical means that the results can be obtained to high accuracy via a simple 1-D Monte Carlo integration that does not require approximation of the expectations in the GAN loss function.) The convergence of this 1-D LSGAN generally does not achieve ${\rm p}_d(x)={\rm p}_g(x)$ with the parameters reaching a saddle point. Instead the parameters converge to a plateau where the gradients of the loss function are zero. The exact convergence point depends on initialisation, generator & data PDF and on the optimisation step size. For certain parameter values, divergence results even in this simple case. This paper has some interesting visualisations to illustrate these points. Note that none of this invalidates the utility of GANs, but the theoretical claims need further qualification.
