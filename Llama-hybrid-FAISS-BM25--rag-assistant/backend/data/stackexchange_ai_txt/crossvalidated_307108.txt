[site]: crossvalidated
[post_id]: 307108
[parent_id]: 231988
[tags]: 
This is kind of an old question, but since it asks essentially asks for 'best practices', rather than what is actually technically possible (ie, doesnt need too much research focus), current best practices is something like: RBMs are not normally used currently linear models (linear regression, logistic regression) are used where possible otherwise deep feed-forward networks with layers such as fully-connected layers, convolutional layers, and throwing in some kind of regularization layers, such as dropout, and lately batch-normalization of course with activation layers in between, typically ReLU, but tanh and sigmoid are used too and probably some max-poolings (not always: average poolings and others are used too) For generative usages, common techniques include: GAN, and its zillions of variants, http://www.cs.toronto.edu/~dtarlow/pos14/talks/goodfellow.pdf auto-encoders, but recently they tend to be being replaced by: variational auto-encoders, VAE, https://arxiv.org/abs/1312.6114 generative CNNs, wavenet: https://deepmind.com/blog/wavenet-generative-model-raw-audio/ RNNs, eg seq2seq https://arxiv.org/pdf/1409.3215v3.pdf
