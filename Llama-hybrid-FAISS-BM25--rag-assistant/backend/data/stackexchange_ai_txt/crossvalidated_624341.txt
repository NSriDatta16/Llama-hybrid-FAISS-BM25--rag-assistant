[site]: crossvalidated
[post_id]: 624341
[parent_id]: 
[tags]: 
Understanding the computation of sample bias and variance

I believe I am confused in some fundamental way about the bias-variance tradeoff and I am trying to clear up my confusion. Sorry for a bit of a preliminary rambling -- I wanted to put my understanding on the table. I was reading "An Introduction to Statistical Learning" and in particular I was learning about ridge regression. The following diagram is from the book to illustrate the affect of changing the tuning parameter $\lambda$ in ridge regression on the error of the model. The pink line represents the mean square error, the green line represents the variance, and the black line represents the squared bias. In the back of my mind is the case of estimators in statistical inference where we have $$ \mathbb{E}[(\hat{\theta} - \theta)^2] = \text{bias}^2(\hat{\theta}) + \text{Var}(\hat{\theta}) $$ When dealing with samples, an analogous equation holds where we apply the usual process of replacing expected values with averages over samples. I know that in the context of regression we have a model $Y = f(X) + \epsilon$ and we want to estimate $f$ with some $\hat{f}$ given several i.i.d. observations $X_1,...,X_n$ and $Y_1,...,Y_n$ each satisfying the same relation. We can then pick a particular pair $(x_0, y_0)$ in this setting we have $$ \mathbb{E}[(y_0 - \hat{f}(x_0))^2] = \text{bias}^2(\hat{f}(x_0)) + \text{Var}(\hat{f}(x_0)) + \text{Var}(\epsilon) $$ In this diagram, we are not seeing that the mean square error is equal to the sum of the squared bias and the variance due to this final $\text{Var}(\epsilon)$ term. My question: how where the (sample!) mean square error, square bias, and variance likely calculated in this example? Here is my guess: Some point $(x_0, y_0)$ was chosen and the data was broken up and $\hat{f}$ was computed using the different pieces. Then this was used to estimate the mean square error, squared bias, and variance terms. Maybe this was actually done for many points $(x_0, y_0)$ and the results averaged. Do you think this is a reasonable guess? Is there a standard way to do this?
