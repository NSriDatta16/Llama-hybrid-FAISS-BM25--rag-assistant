[site]: crossvalidated
[post_id]: 117974
[parent_id]: 117873
[tags]: 
In general you can define outliers differently, depending on what exactly you are trying to achieve. For example, a presence of observations with very high leverage won't necessarily indicate that they are effecting the regression at all. On the other hand, presence of values with high Cook Distance, can certainly do. It is also possible that some values will have both. High Studentized residuals can indicate Heteroscedasticity. Here's an illustration of how you can identify/inspect each when compared to your original data and fitted regression line Create some dummy data set and fit a linear regression model set.seed(11) df We will use influencePlot from car package in order to identify outliers and plot them, when x axis are hat values y axis are Studentized residuals Circles representing the observations proportional to Cooks distances library(car) (outs Now, we can get the corresponding row names of the, for example, 2 highest values in each n And plot them over the fitted regression line plot(df$x, df$y) abline(fit, col = "blue") points(df$x[Cooksdist], df$y[Cooksdist], col = "red", pch = 0, lwd = 15) points(df$x[Lev], df$y[Lev], col = "blue", pch = 25, lwd = 8) points(df$x[StdRes], df$y[StdRes], col = "green", pch = 20, lwd = 5) text(df$x[as.numeric(row.names(outs))], df$y[as.numeric(row.names(outs))], labels = round(df$y[as.numeric(row.names(outs))], 3), pos = 1) You can clearly see that some of the outliers are overlapping, when the leverage ones (the blue triangles) can sometimes affect the regression line while in other occasions be almost on it, while the red squares (Cook Distance) always have high effect on the regression line.
