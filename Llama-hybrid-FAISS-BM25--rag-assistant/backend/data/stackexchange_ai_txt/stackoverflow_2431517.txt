[site]: stackoverflow
[post_id]: 2431517
[parent_id]: 827557
[tags]: 
urlparse quite happily takes invalid URLs, it is more a string string-splitting library than any kind of validator. For example: from urlparse import urlparse urlparse('http://----') # returns: ParseResult(scheme='http', netloc='----', path='', params='', query='', fragment='') Depending on the situation, this might be fine.. If you mostly trust the data, and just want to verify the protocol is HTTP, then urlparse is perfect. If you want to make the URL is actually a legal URL, use the ridiculous regex If you want to make sure it's a real web address, import urllib try: urllib.urlopen(url) except IOError: print "Not a real URL"
