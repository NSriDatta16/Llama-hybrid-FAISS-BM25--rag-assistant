[site]: crossvalidated
[post_id]: 492904
[parent_id]: 
[tags]: 
Need learning curves explanation

I have a model which is trained for linear regression. The trained model works pretty well. The weird thing about it is the learning curves. During training I kept the loss/epoch graph. And for each epoch, the training loss is higher than the testing loss. I am asking myself but how? Because I created the dataset according to frequencies of words. The most frequent words are placed inside the training data and the less frequent ones are in testing . The data for both training and testing data sets are the same. My intuition tells me it should be vice versa. Even if they are not, the curves must be similar to each other. Am I wrong ? Then how is this possible ? Could someone please explain it? If you need extra info please write it to comments then I can update the post. B.R. Edit #1 The model tries to make word by word translation. Given a source language word it predicts a word in target language. The model itself uses word embeddings so it is not count based.
