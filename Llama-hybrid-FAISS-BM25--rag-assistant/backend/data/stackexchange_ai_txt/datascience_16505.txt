[site]: datascience
[post_id]: 16505
[parent_id]: 16346
[tags]: 
I decided turning my comment into an answer. If you want to go pro, use a framework such as scrapy. Personally, I find them overly-cumbersome and I have been successful using the following approach. I think your use case is simple enough for it to be of use to you as well. Assuming you are using Python3 as well, you can grab a webpage easily, and then get what you want using XPath notation. from lxml import html import urllib.request # keep running until there are no "next" pages for page in range(999): url = 'http://blablabla.com/?page=%d' % page text = urllib.request.urlopen(url).read() tree = html.fromstring(text) images = tree.xpath('//img[@class="car"]/href()') types = tree.xpath('//div[@class="type"]/text()') if not images: break for i, (cartype, image) in enumerate(zip(types, images)): urllib.request.urlretrieve(image, '%s-page%d-img%d.png' % (cartype, page, i)) (Purely illustrative example.) Now adjust as you may. XPath is an incredibly powerful notation of accessing XML nodes. Much more is possible than I am writing here. Take this tutorial to learn the full XPath syntax. Some web designers make it much harder to access whatever you want because they do not properly class -ify their HTML objects. In those cases, you may have to access a parent node and ask for their child. Or access a sibling and then get the siblings. Anyway, XPath and Python's lxml package make all this incredibly easy. Any modern browser like Chrome and Firefox also lets you easily explore the DOM of any webpage. Just right-click and press Inspect or go to Developer Tools in the Tools menu or some such. Note: some website like scholar.google.com disallow scrapers and are very good at detecting if that's what you're doing. You can specify an user-agent for urllib, but it might be futile. Even advanced frameworks may not be able to help you there. EDIT: I have made a blog post where I elaborate a little more.
