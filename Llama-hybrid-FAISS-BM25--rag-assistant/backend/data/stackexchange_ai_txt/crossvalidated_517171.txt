[site]: crossvalidated
[post_id]: 517171
[parent_id]: 207748
[tags]: 
There isn’t a division by zero because of two assumptions, all of which are standard procedures in n-gram language modeling. Your vocabulary $V$ is defined from your training corpus. (Consequence: every word $w \in V $ has been seen at least once. $c(w) \geq 1, \forall w \in V $ .) The vocabulary $V$ is a proper subset of the set of word types in the training corpus. Any word token not in $V$ is replaced with a special symbol $\textrm {OOV}$ . (This is colloquially called UNKing the corpus. This is necessary for any non-trivial, count-based language model.) As a reminder, we need a fixed vocabulary $V$ of allowable words. These are the values that $w_i$ and $w_{i-1}$ may take on. Typically, we take this to be some subset of the set of word types in the training set. We replace the rest with a special symbol to designate ‘out-of-vocabulary’ (OOV). Sometimes this is instead called ‘unknown’ (UNK). (I’ve been careless in my exposition about whether OOV is part of the vocabulary.) The final piece of the puzzle is that the denominator can be replaced with a unigram count. $\sum_{w’} c(w_{i-1}w’) = c(w_{i-1})$ . Because of our initial assumptions, this unigram count can never be zero. If the word is a normal member of the vocabulary, this is because it was seen at least once in the training set. If it is not, then we use the special OOV symbol. By assumption 2 and the pigeonhole principle, the count of OOV in the training set is also nonzero. Consequently, in practice we can never divide by zero.
