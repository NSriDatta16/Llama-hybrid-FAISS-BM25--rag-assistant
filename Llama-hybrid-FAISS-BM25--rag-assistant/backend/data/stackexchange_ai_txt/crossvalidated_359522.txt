[site]: crossvalidated
[post_id]: 359522
[parent_id]: 359459
[tags]: 
A 34-66 dataset with $N\approx 15000$ is far from being considered imbalanced. Any reasonable classification algorithm will work fine in this case. I would suggest focusing on making " probabilistic prediction or classification " where you focus on predicting a reasonable probability of instance $i$ being a member of particular class $A$. In that case, a suitable metric would be the Brier Score . You may be interested in exploring some of the highly voted questions using the tags scoring-rules and unbalanced-classes to get a better idea. Addition/Edit following comment : The general advice on choosing hyper-parameters applies here too: Use resampling (either bootstrap or repeated cross-validation) to get estimates of methods $A$ performance. This principal is ubiquitous in all machine learning methodologies. Particular to RusBoost, especially size is in direct analogy with the number of iterations in a most gradient boosting algorithms and cross-validation is the standard way we use to get iter . We can use the functionality of a package like caret to create relevant folds (see for example the function createFolds ) that will be used for training and validating the classifiers performance. Notice that the values of the hyper-parameters will be specific to the problem at hand and they will not be the same for different datasets/applications. (i.e. if we find that for this application 55 C50 learners with ir equals 1 give us the best Brier score we should not go ahead and use these numbers in every application of RusBoost we come across. We should use proper cross-validation and choose new values for our new problem at hand.)
