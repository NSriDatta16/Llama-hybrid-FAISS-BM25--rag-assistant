[site]: crossvalidated
[post_id]: 381693
[parent_id]: 169664
[tags]: 
I would like to provide some input from the statistics perspective. If Y~N(Xb, sigma2*In), then the mean square error of b^ is MSE(b^)=E(b^-b).T*(b^-b)=E(|b^-b|^2)=sigma2*trace(inv(X.T*X)) D(|b^-b|^2)=2*sigma4*trace((X.T*X)^(-2)) b^=inv(X.T*X)*X.T*Y If X.T X is approximately zero,then inv(X.T X) will be very big. So the parameter estimate of b is not stable and can have the following problem. some absolute value of the parameter estimate is very big b has opposite positive or negative sign than expected. adding or removing variables or observations will make the parameter estimates changes dramatically. In order to make the ordinal least square estimate of b stable, we introduce the ridge regression by estimating the b^(k)=inv(X.T*X+kI)*X.T*Y. And we can prove that there is always a k that make the mean square error of MSE(b^(k)) In machine learning, the ridge regression is called the L2 regularization and is to combat over-fitting problems caused by many features.
