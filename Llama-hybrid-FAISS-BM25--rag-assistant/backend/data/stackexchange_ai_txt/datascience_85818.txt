[site]: datascience
[post_id]: 85818
[parent_id]: 60946
[tags]: 
You would anyway start with word embeddings. Pre-trained vectors like Glove etc. Those are derived from DL algorithms. So whichever approach you now take, you already have an element of DL in your solution. Let us now look at the possible approaches. One way would be definitely to use RNNs like bi-directional GRU/LSTM etc which will embed the sentence nicely into one final vector. You could then bring 'Attention' into the scenario. 'Attention' help your model to focus on specific words when predicting the output (in this case selecting from one of the 10 answers) You would also need to use an LSTM on the answer and sort of 'sentence-embed' each of the 10 answers. So now you have a vector for the question, you have 10 answer vectors and you have attention logic. How should the final layer actually 'choose' the answer? This is somewhat different from the typical SQuAD type of scenario. Here one does not have to generate the answer from the given context, but find out which answer is most applicable to the question. We could have a Dense layer which takes the question-embedding and transforms it in such a way that the dot product of this transformation with each of the answer embeddings is an indicator of the actual answer. A softmax could be taken to determine the most probable answer. This is a new scenario from what we normally envisage when thinking of a QnA system, so I am sure the solution can be further improved. You can pick up some of the ideas from https://www.hindawi.com/journals/wcmc/2018/2678976/ . One can even try unsupervised methods like checking for K-nearest distance between the question and each of the 10 final answer, but while this may work for simple scenarios (where the answers are unrelated to each other), this approach may not work for complex ones
