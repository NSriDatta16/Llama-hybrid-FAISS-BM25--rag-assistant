[site]: crossvalidated
[post_id]: 90102
[parent_id]: 
[tags]: 
Recovering true data from many noisy samples with varying unknown amounts of noise

Input: $k$ vectors $x^1,\ldots,x^k \in \mathbb{R}^n$, where $x^i \sim \mathcal{N}(x,\mathbb{1} \cdot \sigma_i^2)$. Goal: approximate the vector $x$ as well as possible. The quality of approximation is measured in any reasonable metric such as $\mathbb{L}_1$ norm or Pearson correlation. Note that the values of $\sigma_i$ are not given as part of the input. In my input, $k \gg n$. What is a good solution to this problem? I'm pretty sure one can do better than simply returning the average of all the vectors $x^i$. One possibility I'm thinking of is performing a variant of canonical correlation analysis between $x^1$ and $x^2,\ldots,x^k$: the variant I'm thinking of would find the best linear combination only among those that have only positive coefficients . (I think this can be solved using semidefinite programming.)
