[site]: crossvalidated
[post_id]: 404311
[parent_id]: 403601
[tags]: 
Assuming the usual linear model with constant variance $\sigma^2$ . I will use notation (and some results) from Leverages and effect of leverage points . The linear model in matrix form is $$ Y= X\beta + \epsilon $$ where $\epsilon$ is a vector of $n$ iid error terms. Then the hat matrix is $H=X(X^TX)^{-1}X^T$ , and its diagonal terms are the leverages $h_{ii}$ . We can show that the variance of the residuals $e_i = y_i-\hat{y_i}$ is $\sigma^2 (1-h_{ii})$ (remember $0 .) So, under this model, to get constant-variance residuals we divide by $\sqrt{1-h_{ii}}$ : the standardized residuals defined by $r_i=\frac{y_i-\hat{y}_i}{\sqrt{1-h_{ii}}}$ have constant variance. So for many uses in residuals analysis we prefer this standardized residuals, for instance in checking the assumption of constant variance. EDIT In a comment the OP writes: As far as I know the formal assumption is not "homoscedasticity of standardized residuals", but only residuals by itself. This confuses errors with residuals . The errors are the unobserved $\epsilon_i$ in the regression equation $y_i =\beta_0 +\sum_i \beta_i x_i +\epsilon_i$ , while residuals is the observed difference between observation and model prediction. Homoskedastcity means that the the errors all have the same variance, not that the residuals have constant variance. If you want to use residuals to test/critizise the constant variance assumption, it is better to use a version of the residuals that do have constant variance (under the model.)
