[site]: crossvalidated
[post_id]: 94310
[parent_id]: 94303
[tags]: 
A similar problem is discussed in Gelman, Bayesian Data Analysis , (2nd ed, p. 128; 3rd edition p. 110). Gelman suggests a prior $p(a,b)\propto (a+b)^{-5/2}$, which effectively constrains the "prior sample size" $a+b$, and therefore the beta hyperprior is not likely to be highly informative on its own. (As the quantity $a+b$ grows, the variance of the beta distribution shrinks; in this case, smaller prior variance constrains the "weight" of the observed data in the posterior.) Additionally, this prior does not set whether $a>b$, or the opposite, so appropriate distributions of pairs of $(a,b)$ are inferred from all the data together, as you would prefer in this problem. Gelman also suggests reparameterizing the model in terms of the logit of the mean of $\theta$ and the "sample size" of the prior. So instead of doing inference on $(a,b)$ directly, the problem is about inference on the transformed quantities $\text{logit}\left(\frac{a}{a+b}\right)$ and $\log(a+b)$. This admits transformed prior values on the real plane, rather than untransformed prior values that must be strictly positive. Also, this accomplishes a posterior density that is more diffuse when plotted. This makes the accompanying graphs more legible, which I find helpful.
