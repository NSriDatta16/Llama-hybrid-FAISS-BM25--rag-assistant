[site]: crossvalidated
[post_id]: 280273
[parent_id]: 280244
[tags]: 
Imputation in small sample sizes If I understand correctly, your question is how to impute data in the case of a (very) small sample size. As a start, multiple imputation is affected in the same way by small sample sizes as as the models you actually use to find replacement values. This might seem an easy/lazy answer on my part, but it is important to realize your imputation models will have greater error and greater variability (both in the models and in the replacement values). You've even seen impossible values due to this variability. If you still think the imputation procedure is able to reduce bias and/or increase power, it might be interesting to look at specific imputation models aimed at maintaining the advantages of imputation, while accomodating small samples. See for example this reference . The authors describe how various replacement value generating models perform in the case of small samples. Each of the models shown (bayesian least square regression, predictive mean matching, local random residual) has its pros and cons, but according to their simulation study the regression models seem to create the most accurate replacement values. Preventing impossible values during imputation However, if your question is aimed at preventing impossible values, you have multiple options. As a reminder, 'impossible' values are created due to specific combinations of predictor values for a missing value, which according to the replacement value generating model should result in this impossible value. To prevent this, there are two options: Adjust model in such a way the expected values are always possible. If you initially used a linear regression model, do note that this model does not 'know' of any bounds on the continuous outcome. So the replacement values can theoretically be $[-Inf,+Inf]$. If however you use another model you can bound the possible replacement values. For example, in the case of predictive mean matching 'PMM' or local random residual 'LRR' , the replacement value is actually copied from a (random) similar 'donor' case without missing information (with some randomness added in the LRR case). The second option is to post-process replacement values. The R mice package and SPSS's imputation functions offer this option. Post processing occurs after replacement values are generated, but before the next iteration of imputation occurs. This way you can set bounds $[a,z]$ and set every value which falls below $a$ at $a$, and all above $b$ at $b$. Do note that when a lot of replacement values across imputation sets fall outside of your boundaries, you might need more information to obtain more reasonable estimates of the missing data. Further, different imputation software packages handle these bounds differently: mice does as described above, while SPSS's imputation 'redraws' replacement values until it finds one within the set bounds. Concluding, I'd recommend you to look at a package which let you pick the imputation/missing value generating model and use LRR as imputation model (as is recommended for small sample in the reference I've added), or use some kind of post-processing in order to set bounds.
