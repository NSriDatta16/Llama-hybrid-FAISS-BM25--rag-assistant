[site]: crossvalidated
[post_id]: 568518
[parent_id]: 568516
[tags]: 
That might make sense, if there's a hierarchy in the tasks. E.g. if there's a set of broad groups of tasks, one could first have a model that picks which group of classes is relevant and then a second set of models, from which you pick one based on that output. That's sometimes called model cascades and is closely related to "mixture of experts" approaches. However, the need for the first model (which I think you would need from what you describe, unless that can be determined by simple rules from some other information) means you'd have to train at least that model on all data. It's not obvious/intuitive to me that this would always be a good idea. E.g., if you do this via fine-tuning a (neural network) language model, then learning to do multiple tasks (e.g. training the model to simultaneously predict the group of tasks and the specific tasks) can do remarkably well. The main value of the joint training of all the tasks is that suitable representation are learnt in intermediate layers of the model that get informed by all tasks. The main reason not to do it (as far as I am aware) is model size/training speed (which is where interest in mixtures of experts has been huge). I'm less sure whether the considerations above also apply to other model classes (e.g. XGBoost/LightGBM/catboost or categorical logistic regression) that "just" use TF-IDF features. In short, it may be a good idea depending on what you want to prioritize (inference speed, training time, accuracy etc.), but whether it is in your specific case requires testing (e.g. via some suitable cross-validation).
