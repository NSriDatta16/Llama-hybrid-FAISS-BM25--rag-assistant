[site]: crossvalidated
[post_id]: 609876
[parent_id]: 609829
[tags]: 
PCA just comes down to using the eigendecomposition of the (empirical) covariance matrix of the data. The full eigendecomposition of the covariance matrix results in a set of eigenvectors and corresponding eigenvalues, which can be interpreted as the variance along these eigenvectors. Because these eigenvectors are orthogonal, they can be used to create a rotation matrix that rotates the data so that the basis aligns with the direction of maximum variance. For some people, this may be easier to understand with a few lines of code: import numpy as np data = np.random.randn(256, 2) # generate some random 2D data covariance = np.cov(data, rowvar=False) # compute covariance matrix variances, rotation = np.linalg.eigh(covariance) # eigendecomposition pcs = (data - data.mean(0)) @ rotation # compute principal components In other words, principal components are just a rotation version of the (centred) data. This also means that the (centred) data can simply be reconstructed by rotating back the principal components: pcs @ rotation.T . As a result, a "full" PCA (not sure if this is the correct term) is perfectly reversible. This is how PCA is typically used in the context of pre-processing data. After all, this rotation should typically make it easier to find important features. Typically, the principal components are additionally whitened (scaled to unit variance) using the eigenvalues of the decomposition ( variances in the code). It is also possible to whiten the data after the rotation and then rotate it back, which gives rise to ZCA pre-processing. I can highly recommend this answer to a related question for further reading (and some nice figures). When using PCA for dimensionality reduction, you would only use a subset of the columns in the rotation matrix. This obviously leads to loss of information. However, the columns of the rotation matrix can effectively be to transform the data back to the original space. After all, using only a few columns corresponds to setting principal components to zero (i.e. dropping information). If the total variance corresponding to these dropped dimensions is low enough, a reasonable reconstruction (of the centred data) is typically possible. Again, in code: data = np.random.randn(256, 784) # generate some random high-D data covariance = np.cov(data, rowvar=False) # compute covariance matrix _, rotation = np.linalg.eigh(covariance) # eigendecomposition reduced_pcs = (data - data.mean(0)) @ rotation[:, -70] # dimensionality reduction PCA reconstruction = reduced_pcs @ rotation[:, -70:].T # reconstruction of (centred) data Note that the reconstruction of the random data in this snippet of code is not going to work well. However, you should get some reasonable results if you plug in e.g. some MNIST data. TL;DR: PCA is often used to pre-process data (make it nicer to work with) and can actually often be transformed back to the original input space (i.e. is not necessarily abstract).
