[site]: crossvalidated
[post_id]: 458291
[parent_id]: 458273
[tags]: 
It really depends on the objective function of the optimization problem. Two approaches to hyperparameter optimization are: Grid search : for each parameter that needs to be tuned, assign a range of values and estimate the model for each combination of values within the specified ranges. Then evaluate your performance criterion on each calibrated model. In other words, you exhaustively run through all parameter values and come up with a large number of fitted models from which you can cherry-pick the most optimal set of parameters by graphing the effect on model performance of increasing/decreasing the parameters against one another Bayesian optimization : Instead of exhaustively running through a grid of models, you once again specify a range of values for each hyperparameter, but let a Bayesian toolbox like hyperopt in python intelligently iterate through a select few combinations of the parameters. This obviously can be much more efficient. It evaluates the objective function, by taking it to be random because it is unknown, placing a prior distribution on it, and uses the evaluations to update the prior to form a posterior distribution, which in turn is used to construct an acquisition function that directs the next query point. A less expensive way to define the prior/posterior distribution of the objective function is to use Parzen Tree Estimators which constructs two distributions for 'high' and 'low' points, and then finds the location that maximizes the expected improvement. Could also consider, Heuristics and annealing : Study the impact of slowly amping up certain parameters vice versa by feeling out what could be a right value for each parameter, and select a model that finds an optimal level of your performance criterion. More relevant and manageable for a uni-parameter problem. Analytical derivation : If the problem is a construct of linear equations, you can back out optimums for each hyperparameter using math.
