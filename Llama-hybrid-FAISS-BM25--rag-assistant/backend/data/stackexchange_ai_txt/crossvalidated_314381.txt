[site]: crossvalidated
[post_id]: 314381
[parent_id]: 
[tags]: 
Controlling and ignoring (revisiting this concept to make the difference clear)

(I'm aware there are other related questions.) I'm struggling with the concept of "controlling" in a regression analysis where one variable is binary (0,1) and a Professor of mine said in our stats class that the term control or ignoring is not right: "It is not ignoring Group, it is taking the average outcome for the not-GROUP and Groups into account while weighting for the N size of each of the groups (which I didn’t mention in class)." I think this sentence is not 100% correct but I want to figure out that. So, this question is: what´s the math formula for the explain the difference between two predicted outcomes (one using a binary group + continuous) another using only the continuous. All codes below could be load in R and i "attached" some figures to clarify this question. Let's assume 10 students (5 with disability (group=1) , 5 control (group=0)) were tested in a math and writing test. Now, I have a dataset like the code below: options(scipen = 999) # avoid scientific notation library(dplyr) # request library set.seed(123) # same numbers everytime dataset I can regress the math score on group and writing (model 1) and I can also regress math score on writing (model 2) to see the difference between "controlling" and "ignoring". # regression model full mod First model: lm(formula = math ~ reading + group, data = dataset) Residuals: Min 1Q Median 3Q Max -2.0430 -0.3655 0.1399 0.5040 1.3478 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 3.5242 1.6383 2.151 0.0685 . reading 0.6405 0.2116 3.027 0.0192 * group -3.3400 1.0080 -3.314 0.0129 * Second model: lm(formula = math ~ reading, data = dataset) Residuals: Min 1Q Median 3Q Max -3.5464 -0.8077 -0.0328 1.1261 2.2719 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -0.7441 1.5176 -0.490 0.63706 reading 1.0908 0.2431 4.487 0.00204 ** The last part of the syntax expresses the concept of "controlling", once it reports a predicted value for a model with a binary variable and a predicted value for a model without this binary variable: # get means descriptive % group_by(group) %>% summarise_all(funs(mean(.),n())) # insert predicted values for each model dataset$math_full As we can see, there´s nothing new in this output. But, if the professor is right, the difference is computed by taking the average outcome for the not-GROUP and Groups into account while weighting for the N size of each of the groups , what I don´t think it's right. The descriptive results, if someone want to do this calculation by hand (which I'd appreciate), are below: # A tibble: 2 x 11 group math_mean reading_mean math_full_mean math_reading_only_mean difference_mean math_n 1 0 8.2 7.30 8.2 7.218976 0.9810243 5 2 1 2.9 4.24 2.9 3.881024 1.3343249 5 # ... with 4 more variables: reading_n , math_full_n , math_reading_only_n , # difference_n The code here can be used in R.
