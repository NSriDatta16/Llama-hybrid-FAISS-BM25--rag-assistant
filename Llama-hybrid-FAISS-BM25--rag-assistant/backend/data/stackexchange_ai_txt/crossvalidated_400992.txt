[site]: crossvalidated
[post_id]: 400992
[parent_id]: 400988
[tags]: 
There are quite a few reasons this could happen. Probably the two most important to be aware of is: 1) Your accuracy didn't really drop. Every measure of model performance is subject to randomness. I.e., it's sensitive to the particular data you used to train and test your model. Its possible that you just randomly ended up with a "hard" test set or an "easy" training set. It's best to get multiple measures of model performance, and then average them, this gives you a much more stable look at your situation. The main tool for doing this is bootstrapping. 2) Your second chunk of data is not sampled from the same population as the first. It's possible that your second set of data is just describing a slightly different phenomena than the first. Like, if you first set was from twitter and your second set was from facebook. Or, more subtly, your first set was tweets from random days, but your second were tweets sent on christmas (or any other holiday).
