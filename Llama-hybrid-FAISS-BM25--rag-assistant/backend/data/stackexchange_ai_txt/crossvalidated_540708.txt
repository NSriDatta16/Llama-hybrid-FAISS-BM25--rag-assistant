[site]: crossvalidated
[post_id]: 540708
[parent_id]: 
[tags]: 
Why is testing accuracy consistently higher than training accuracy ? Possible interpretations?

I am trying to compare the outcome of machine learning algorithms given different types of simulated data, and no matter what dataset I use I am consistently getting a lower validation accuracy than testing accuracy, which to me seems strange. As far as I can tell, I am not falling into the common pitfalls: Train/Test set is 70/30 split, stratified, and on top of this I use many (1000) different splits to ensure results are not due to luck. I am using Leave One Out Cross Validation on the training set to assess the validation accuracy. Is there another possible explanation for this strange behavior? Attached is the GitHub repository where I have a walkthrough of what I have tried, and if anyone could help me understand what I am potentially doing wrong it would be greatly appreciated. If you need more information to answer my question, let me know! https://github.com/ScoobSharkason/sturdy-lamp Edit: I made some slight adjustments to the ipynb file to make it more plug and play friendly. Let me know if any errors pop up as it is possible I forgot something else.
