[site]: crossvalidated
[post_id]: 414919
[parent_id]: 349418
[tags]: 
Yes, you are correct that there is a lack of identifiability unless one of the coefficent vectors is fixed. There are some reasons that don't mention this. I can't speak to why they omit this detail, but here's an explanation of what it is and how to fix it. Description Say you have observations $y_i \in \{0, 1, 2, \ldots, K-1\}$ and predictors $\mathbf{x}_i^\intercal \in \mathbb{R}^p$ , where $i$ goes from $1$ to $n$ and denotes the observation number/index. You will need to estimate $K$ $p$ -dimensional coefficient vectors $\boldsymbol{\beta}^0, \boldsymbol{\beta}^1, \ldots, \boldsymbol{\beta}^{K-1}$ . The softmax function is indeed defined as $$ \text{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{l=0}^{K-1}\exp(z_l)}, $$ which has nice properties such as differentiability, it sums to $1$ , etc. Multinomial logistic regression uses the softmax function for each observation $i$ on the vector $$ \begin{bmatrix} \mathbf{x}_i^\intercal \boldsymbol{\beta}^0 \\ \mathbf{x}_i^\intercal \boldsymbol{\beta}^1 \\ \vdots \\ \mathbf{x}_i^\intercal \boldsymbol{\beta}^{K-1}, \end{bmatrix} $$ which means $$ \begin{bmatrix} P(y_i = 0) \\ P(y_i = 1) \\ \vdots \\ P(y_i = K-1) \end{bmatrix} = \begin{bmatrix} \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^0] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \\ \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^1] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \\ \vdots \\ \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^{K-1}] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \end{bmatrix}. $$ The problem However, the likelihood is not identifiable because multiple parameter collections will give the same likelihood. For example, shifting all the coefficient vectors by the same vector $\mathbf{c}$ will produce the same likelihood. This can be seen if you multiply each the numerator and denominator of each element of the vector by a constant $\exp[-\mathbf{x}_i^\intercal \mathbf{c}]$ , nothing changes: $$ \begin{bmatrix} \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^0] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \\ \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^1] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \\ \vdots \\ \frac{\exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^{K-1}] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal \boldsymbol{\beta}^k] } \end{bmatrix} = \begin{bmatrix} \frac{\exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^0-\mathbf{c})] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^k-\mathbf{c})] } \\ \frac{\exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^1-\mathbf{c})] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^k-\mathbf{c})] } \\ \vdots \\ \frac{\exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^{K-1} - \mathbf{c})] }{ \sum_{k=0}^{K-1} \exp[\mathbf{x}_i^\intercal (\boldsymbol{\beta}^k -\mathbf{c}) ] } \end{bmatrix}. $$ Fixing it The way to fix this is to constrain the parameters. Fixing one of them will lead to identifiability, because shifting all of them will no longer be permitted. There are two common choices: set $\mathbf{c} = \boldsymbol{\beta}^0$ , which means $\boldsymbol{\beta}^0 = \mathbf{0}$ (you mention this one), and set $\mathbf{c} = \boldsymbol{\beta}^{K-1}$ , which means $\boldsymbol{\beta}^{K-1} = \mathbf{0}$ . Ignoring it Sometimes the restriction isn't necessary, though. For instance, if you were interested in forming a confidence interval for the quantity $\beta^0_1 - \beta^2_1$ , then this is the same as $\beta^0_1 - c - [\beta^2_1-c]$ , so inference on relatively quantities doesn't really matter. Also, if your task is prediction instead of parameter inference, your predictions will be unaffected if all coefficient vectors are estimated (without constraining one).
