[site]: datascience
[post_id]: 94886
[parent_id]: 
[tags]: 
In Transformer's multi-headed attention, how attending "different representation subspaces at different positions" is achieved?

Question partially inspired by this post about the need of multi-head attention mechanism. For me though it is still not clear how we will be able to initialise those attention heads in a diverse way(so that they potentially can - as stated in the Attention is all you need paper - attend to information from different representation subspaces at different positions) and most importantly preserve this diversity during the training process.
