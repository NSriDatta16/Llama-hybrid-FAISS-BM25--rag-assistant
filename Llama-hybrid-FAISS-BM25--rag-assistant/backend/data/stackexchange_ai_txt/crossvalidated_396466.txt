[site]: crossvalidated
[post_id]: 396466
[parent_id]: 
[tags]: 
Why is there a need to find a variance and take the square root to get a standard deviation?

My question is why is the formula for finding the standard deviation of a given data (either grouped or non grouped) the way it is? so let me start from the definition of a standard deviation with my own words, which i believe is "what is the average dispersion between the mean and each data." Logically from what the definition says i believe the equation should have been sum of all the differences between the mean and each value divided by the total frequency. (am sorry i don't now how to right equations in this site so i would appreciate any editing). Well there is a catch here since by definition mean is the data that is by average at equal distance from each data point, the standard deviation calculated using the above formual (idea) will be zero which doesn't describe what we are looking for, and when we analyse where this problem may have come we can see that the definition of the standard deviation shouldn't depend on the direction of the data points relative to the mean, by that i mean smaller or greater in quantity we just need to now by how much they vary so i believe to alleviate this problem mathematician has came with the idea of variance which is squaring the difference, and then latter taking the square root in order to find the standard deviation, which is were i depart, i don't really get why we square the deviation between the mean and each data, i believe what we should do is take the absolute value of each difference and then divide the sum by the total frequency. so in general what i am saying is Shouldn't the formula of a standard deviation be the what i said when we come at the problem from the definition.
