[site]: crossvalidated
[post_id]: 629603
[parent_id]: 561460
[tags]: 
In my understanding, what you observed was overfitting stemming from data mispartitioning. Most likely, you have trained your stacking metamodel on the same data rows that were used for training base models. The correct approach would be to: split entire data into "base" and "ensemble" sets; use base set for training base models; issue predictions of base models using features from the ensemble set as inputs, persist results; train your meta-learner(s) using results produced in step 3 as inputs, and labels from the ensemble set as targets (feel free to split the ensemble set into train/val parts for early stopping) Note that the ensemble set must not be accessible for training base models in any way, even indirectly (ie, must not serve as a validation set for base models, etc - do not try to save on that and reserve separate validation set for early stopping of base models from within the base set). However, simpler ensembles like voting/averaging that do not have a target column and do not perform learning per se can be of course applied without reserving a separate set, often leading to better ML metrics due to reduced variance.
