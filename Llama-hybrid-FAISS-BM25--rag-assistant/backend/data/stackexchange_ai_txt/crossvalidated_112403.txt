[site]: crossvalidated
[post_id]: 112403
[parent_id]: 112211
[tags]: 
If it is a general approach, then it applies generally. A neural network is a universal approximator. Converging on a particular number of neurons means only that it is not universal anymore. So your equation results in the following plot. I am assuming a negative value for Neuron1 is meaningless. So it is a function that maps the continuous range of 0...1 to a discrete range of 4 to 24. "Best" does not exist without a measure of goodness. There are infinite numbers of candidate bests. For every candidate rubric there are infinite candidate anti-rubrics, and perpendicular rubriks. You have not specified a measure of goodness so there is no best to exist. It is likely that the underlying system is better fit by a higher number of neurons. There are networks with billions of neurons. Think about your own brain, for example. Assuming it is efficient, not a bad assumption, then it should take about a billion artificial neurons to make a good approximation of what the real neurons are doing. If you tried to approximate it using 24 neurons, it would fall stunningly short - for all currently used transfer functions. My suggestion: use something like a random forest to determine something about the actual complexity of your data. select neuron counts appropriate to that complexity, train properly, and cull as appropriate. Edit/Elaboration: A random forest subsets the data. It builds trees of random subsets of the data. When I build a random forest, of course I use CV, but then I look at total error and total complexity as a function of tree size to determine proper tree size. I look at variable importance to reduce dimensionality, and train using the important variables. Once you have a reasonable tree, you can iterate on training-validation using progressively smaller chunks of the data for training until you hit a "cliff" or "knee" in representation. At that point you are at minimum rows and minimum columns. You can double-check leaf size with an eye for minimization, but you are reasonably well situated to apply the fitting with a NN. You want to do a few reruns with random row-selection at every level of split, of course, to make sure that your mean error at the split-size is not an artifact. This leaves you several estimates of parameter: $ #rows x #colums = minimum data element count$ $ #leaves x #trees = tree parameter count $ Then you can look at the parameters count for your neurons and put a ceiling there. If you can't beat the trees with the NN you have a problem. You add up all the "weights" and "biases" in your ANN and that is your parameter count. You can also look at this to inform your training set, and to subsample your original data if training would take a prohibitively long time. So you start your ANN with the same number of parameters as the RF. You train until suitable representation occurs. Then you perform a cull operation, where you cull the least informative neuron, and update the training. You track the fully-trained error as a function of parameter count and make a good judgment call as to where you are comfortable leaving your NN. I would suggest evaluating performance from RF-base to at least 25% fewer parameters than RF-base as a domain to evaluate fully-trained performance. In my personal experience, many things can be approximated with Random Forests of less than 300 trees, otherwise built to the recommendations of Breiman and company . Sometimes fewer than 50 trees will do the job well. This means some ballpark complexity ceilings are between 300-1800 parameters. Your actual mileage is going to depend entirely on your data. aside: This would make a great (winning) science fair (ISEF) project. As a former judge - I would have liked to see some of this there, in the applied mathematics area. If someone wanted to apply this to a dozen or so textbook data sets and compare to textbook results. Given that "machine learning" and "data-mining" are pretty popular right now this could be good.
