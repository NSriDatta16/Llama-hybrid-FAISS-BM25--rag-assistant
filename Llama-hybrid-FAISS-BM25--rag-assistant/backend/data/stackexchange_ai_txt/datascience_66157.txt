[site]: datascience
[post_id]: 66157
[parent_id]: 54412
[tags]: 
BERT: import transformers import torch tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased') bert_model = transformers.BertModel.from_pretrained('bert-base-uncased') max_seq = 100 def tokenize_text(df, max_seq): return [ tokenizer.encode(text)[:max_seq] for text in df['text'] ] def pad_text(tokenized_text, max_seq): return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text]) def tokenize_and_pad_text(df, max_seq): tokenized_text = tokenize_text(df, max_seq) padded_text = pad_text(tokenized_text, max_seq) return torch.tensor(padded_text) def targets_to_tensor(df): return torch.tensor(df['label'].values, dtype=torch.float32) train_indices = tokenize_and_pad_text(small_train, max_seq) val_indices = tokenize_and_pad_text(small_valid, max_seq) test_indices = tokenize_and_pad_text(small_test, max_seq) with torch.no_grad(): x_train = bert_model(train_indices)[0] x_val = bert_model(val_indices)[0] x_test = bert_model(test_indices)[0] y_train = targets_to_tensor(small_train) y_val = targets_to_tensor(small_valid) y_test = targets_to_tensor(small_test) CNN: import time import torch.nn as nn import torch.nn.functional as F from sklearn.metrics import roc_auc_score from torch.autograd import Variable class KimCNN(nn.Module): def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static): super(KimCNN, self).__init__() V = embed_num D = embed_dim C = class_num Co = kernel_num Ks = kernel_sizes self.static = static self.embed = nn.Embedding(V, D) self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, D)) for K in Ks]) self.dropout = nn.Dropout(dropout) self.fc1 = nn.Linear(len(Ks) * Co, C) self.sigmoid = nn.Sigmoid() def forward(self, x): if self.static: x = Variable(x) x = x.unsqueeze(1) # (N, Ci, W, D) x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] # [(N, Co, W), ...]*len(Ks) x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # [(N, Co), ...]*len(Ks) x = torch.cat(x, 1) x = self.dropout(x) # (N, len(Ks)*Co) logit = self.fc1(x) # (N, C) output = self.sigmoid(logit) return output n_epochs = 50 batch_size = 10 lr = 0.01 optimizer = torch.optim.Adam(model.parameters(), lr=lr) loss_fn = nn.BCELoss() def generate_batch_data(x, y, batch_size): i, batch = 0, 0 for batch, i in enumerate(range(0, len(x) - batch_size, batch_size), 1): x_batch = x[i : i + batch_size] y_batch = y[i : i + batch_size] yield x_batch, y_batch, batch if i + batch_size This blog explains well how to use CNN with BERT in Pytorch.
