[site]: datascience
[post_id]: 68419
[parent_id]: 68327
[tags]: 
Mathematically, the weights that sit between two given rows of neurons collectively form a transformation matrix, and a row of neurons forms a vector. To use the network, we use the matrix to transform the vector, giving us a vector representing the next row of neurons. Then we apply an activation function to those neurons. Then we proceed to the next layer and repeat. So what happens when we don't have an activation function? Then we just have a series of matrix transformations, and we can use matrix multiplication to compute a single matrix that does the same thing. So in truth such a network has no hidden layers, and is incapable of deep learning.
