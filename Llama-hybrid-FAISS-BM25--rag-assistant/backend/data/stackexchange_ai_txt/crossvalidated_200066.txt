[site]: crossvalidated
[post_id]: 200066
[parent_id]: 200012
[tags]: 
[It's not just that the root-mean-square form is easier to work with; it has properties that make it attractive, such as the fact that the variances of sums of random variables have simple form. Though perhaps that's what you were getting at.] The second inequality doesn't compensate for the first; not even on average. If we define $s_n=\sqrt{ \frac{1}{n} \sum_i (x_i-\bar{x})^2}$, we can apply Jensen's inequality to see that the mean deviation will never be larger than $s_n$. That is, if $m_n$ is the mean deviation, $s_n\geq m_n$ (with equality only when they're both 0). Then we can see that the usual Bessel-corrected standard deviation $s_{n-1}=\sqrt{\frac{n}{n-1}}\,s_n\geq s_n$ (with equality only when they're both 0). Hence $s_{n-1}\geq m_n$, (again with equality only when they're both 0). The expected value of the ratio of the mean deviation to standard deviation - (while necessarily between 0 and 1) depends on the distribution you draw from. Indeed the ratio was proposed by Geary as a test of normality in 1935; it works well against symmetric heavy-tailed alternatives. [For normally distributed data, the expected value of the ratio in large samples is about 0.8]
