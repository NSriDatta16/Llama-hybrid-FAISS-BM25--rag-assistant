[site]: crossvalidated
[post_id]: 34801
[parent_id]: 34797
[tags]: 
I think @Macro is right; in fact, I'm pretty sure I've answered exactly this question. However, I can't find it, and I think the underlying reason for this happening is unrelated to the reason for the reverse situation. So I'll put down some quick information. One thing that I think is unfortunate is that the problem of multiple comparisons is always discussed the same / in only one way, namely the comparisons between multiple groups. But this issue occurs everywhere , not just in that situation. For example, if you run a multiple regression with 20 covariates where the null hypothesis obtains for each of them, you should expect that in the long run, on average one of them will appear 'significant' anyway in each model. There are various ways of addressing this issue (e.g., alpha correction techniques), but the most common is to use a simultaneous test , that is, a global $F$ test of the model. Thus, my first guess is that the model test is doing its job and protecting you from inflated family-wise type I error. That is, it is telling you to ignore any possible significant betas. (I apologize if this is bad news.) For the sake of completeness, there is of course, another possibility. The power of a simultaneous test can be quite week, especially in cases where there are many unrelated covariates, only a few (or one) actually related covariates that are weakly correlated with the response, and a large error term. Despite that fact, you should be cautious about concluding that the beta in question is 'significant'.
