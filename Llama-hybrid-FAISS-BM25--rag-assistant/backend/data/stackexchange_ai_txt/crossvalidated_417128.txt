[site]: crossvalidated
[post_id]: 417128
[parent_id]: 413986
[tags]: 
From the topology of this example it looks like the KNN should be good at distinguishing green and turquoise, because knn is strong when local relative density of a class is the best predictor and when a given distance has the same meaning at any point in space, and in any direction. SVM here should be strong at making the difference between points belonging to concentric circles. (e.g. distinguishing blue from red from green-ish). However here you are using SVM with a linear kernel, which means that you are making linear separators in the plane (straight lines). You want to be using what is called the kernel trick : using a transformation of the feature space equivalent to performing your fit into a space of higher dimension, that looks like a representation of the samples where the distance to the point of coordinates (0, 0) is reflected on a new third axis z. For example you could use a SVM with radial basis functions: M2_svc = svm.SVC(C=5, kernel='rbf') But the best kernel would be a custom kernel: def custom_kernel(x, y): return x**2 + y**2 M2_svc = svm.SVC(C=5, kernel=custom_kernel) I think that you are getting bad results with your stacking because the KNN already does a better job where your current SVM has OK performances, and everywhere else the linear SVM only introduces noise. The idea of stacking is to merge classifiers that have complementary strengths.
