[site]: crossvalidated
[post_id]: 390533
[parent_id]: 390469
[tags]: 
You should perform the steps above on centered data. You'll need to subtract a vector of each feature's sample mean from your observations. The steps are as follows. Compute the covariance matrix $\Sigma$ . I assume that your data matrix is observations x features, so this would be $\Sigma = \frac{1}{N}(A-\bar{A})^{T} (A-\bar{A})$ . Compute the eigenvalue decomposition of $\Sigma$ . Collect the eigenvectors as the columns of a matrix $V$ . Equivalently, you can find the singular value decomposition $\Sigma = V^{T} D V$ . Compute the projection of the (centered) data onto the eigenvectors. This gives the final reduction $\hat{A} = (A-\bar{A}) V$ . Here is my python code. I've used the eigenvector decomposition. import numpy as np from sklearn.decomposition import PCA #use sklearn pca A=np.array([[3, 0], [0, -2]]) pca = PCA() pca.fit(A) print("Sklearn PCA output:") print(pca.transform(A)) #And here is by hand. Cov=np.matmul((A-A.mean(axis=0)).T, (A-A.mean(axis=0)))/A.shape[0] #Get the eigenvectors from covariance matrix V = np.linalg.eig(Cov)[1] #perform the projection print("Manual PCA output:") print(np.matmul(A-A.mean(axis=0), V)) And the output. Sklearn PCA output: [[-1.80277564e+00 -1.11022302e-16] [ 1.80277564e+00 1.11022302e-16]] Manual PCA output: [[ 1.80277564 0. ] [-1.80277564 0. ]] Note that sklearn is doing the centering behind the scene. You can verify that fitting the PCA on the centered and original data give the same result.
