[site]: crossvalidated
[post_id]: 510615
[parent_id]: 510606
[tags]: 
You can create a decision tree with dependent features, and they can work very well in the sense that the resulting model is good at predicting new samples. Random forests and XGBoost have been widely used for many years in industry, and competitions too. I would argue this is in part because of their robustness to "data in practice", where the datasets aren't theoretical and features are correlated. What you should keep in mind however is that interpretations, like the often-used feature importance, are influenced by this (if they ever were valid, these things are a bit hand wavy). You can see that, when two features are highly correlated, splits can be made equally well on either of those and it may come down to chance which one ends up high on the feature importance metric.
