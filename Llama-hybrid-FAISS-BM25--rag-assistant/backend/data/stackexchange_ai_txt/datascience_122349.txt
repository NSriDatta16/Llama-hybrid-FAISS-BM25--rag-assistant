[site]: datascience
[post_id]: 122349
[parent_id]: 
[tags]: 
Effect of hyperparameters: the hidden size, layers, MLP size number of heads on Transformer

Is there any paper that explains the effect of hyperparameters: hidden size Number of layers MLP size number of heads on Transformer performance. I found some explanation on the web but I need papers.
