[site]: crossvalidated
[post_id]: 600180
[parent_id]: 600136
[tags]: 
TL;DR the $t_i$ do not have to do with adam specifically. Instead, they seem like class frequencies in a multi-class classification problem. During training you are finding the optimal parameters for the neural network. Those parameters are the coefficients of the links between the different nodes. The coefficients are updated by improving the cost function in small steps and each step a change to the coefficients is made according to the gradient of the cost function with respect to the coefficient values. The adam optimizer does this in a particular fashion by adapting the stepsizes during each step. The $t_i$ in your example seem to relate to a multi-class classification (with 3 classes). Given some predictors $x_k$ (4 of them) you wish to come up with an estimator $y_i$ for the frequencies $t_i$ of classes. The $t_i$ represents how often you observe class $i$ and $y_i$ represents your predicted probability for class $i$ . The derivatives should not be relative to the frequencies $t_i$ but relative to the coefficients in the NN. In the image these coefficients are not drawn, but it should probably be something like weights $w_{i,j}$ and $w_{j,k}$ (which are respectively 3x5 and 5x4 in number), and potentially there are more parameters then just those weights.
