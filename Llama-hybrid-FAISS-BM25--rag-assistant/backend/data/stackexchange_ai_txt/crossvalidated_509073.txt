[site]: crossvalidated
[post_id]: 509073
[parent_id]: 509058
[tags]: 
The MDP don't have to contain the probabilites of an action choice. The policy itself is not a part of the MDP. As an example you're Markov Decision Process would be fully described by the full table: S S' a p(S'|S,a) Reward $S_1$ $S_2$ $a_1$ 0.3 1 $S_1$ $S_3$ $a_1$ 0.7 1 $S_1$ $S_1$ $a_2$ 0.3 1 $S_1$ $S_4$ $a_2$ 0.7 1 .... .... .... .... .... Our goal in RL now is to find an optimal policy, whose aim is always to choose the best action in each step to maximize our return. That means the policy is our goal and not part of the initial MDP respectively the Markov Reward Process in this case. RL is all about methods to find the optimal policy. So regarding to your Q2 and Q3 a deterministic policy means, that your found policy selects always the same action in a specific state whereas a stochatic (non-deterministic) policy selects actions with a specific probability. But finding these probabilites is not trivial If you're interested in further information about RL I can strongly recommend this book: http://incompleteideas.net/book/RLbook2020.pdf
