[site]: crossvalidated
[post_id]: 630032
[parent_id]: 
[tags]: 
Is Using Adaptive Learning Rates Preferred?

I wonder what are optimizers with adaptive learning rates in deep learning? Are there any drawbacks for using them that we should be aware of? If there isnâ€™t any, why are we still using old-school optimizers with strict learning rate parameters? When to use an optimizer with adaptive learning rate like Adam or a classic optimizer with pre-defined learning rate like SGD? If optimizers with adaptive learning rate automates the parameter tuning process, is it true to assume that the model does not need an external fine tuning for its learning rate hyperparameter, if it is the case why are we still passing a learning rate value to those optimizers, does it make a difference at all?
