[site]: crossvalidated
[post_id]: 208156
[parent_id]: 168622
[tags]: 
There appears to be an underlying assumption here that not checking for collinearity is a reasonable or even best practice. This seems flawed. For example, checking for perfect collinearity in a dataset with many predictors will reveal whether two variables are actually the same thing e.g. birth date and age (example taken from Dormann et al. (2013), Ecography , 36 , 1, pp 27â€“46 ). I have also sometimes seen the issue of perfectly correlated predictors arise in Kaggle competitions where competitors on the forum attempt to eliminate potential predictors which have been anonymised (i.e. the predictor label is hidden, a common problem in Kaggle and Kaggle-like competitions). There is also still an activity in machine learning of selecting predictors - identifying highly correlated predictors may allow the worker to find predictors which are proxies for another underlying (hidden) variable and ultimately find one variable which does the best job of representing the latent variable or alternatively suggest variables which may be combined (e.g. via PCA). Hence, I would suggest that although machine learning methods have usually (or at least often) been designed to be robust in the face of correlated predictors, understanding the degree to which predictors are correlated is often a useful step in producing a robust and accurate model, and is a useful aid for obtaining an optimised model.
