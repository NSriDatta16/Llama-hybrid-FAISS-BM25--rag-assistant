[site]: crossvalidated
[post_id]: 162109
[parent_id]: 160870
[tags]: 
As I understand your application and available data, I believe it's better thought of as an anomaly-detection problem than a prediction one. That is, I have interpreted your question as, "Given a recent history of response times, how can I decide whether a long response time is an abnormality?" Incidentally, monitoring machines in a data center is one example given in the introduction to anomaly detection in Andrew Ng's ML course. The first few videos in this series illustrate an approach that can be applied to your problem. In this approach, you would use your data of response times to estimate a density function for non -anomalous examples, and then use cross-validation to set a threshold density that would indicate an anomaly. It would also require a little modification, in that a density threshold could indicate a response time that was anomalously fast, which you are not interested in. (I've linked to the first video, but the videos up through the fourth in the section are relevant.) If I've misinterpreted your question and you are interested in predicting actual response times, then regression is the choice. But since the observed data are non-negative, a gamma generalized linear model is better-suited. (Answers to this question describes when to use a gamma GLM.) I suspect that time since the beginning of (some arbitrary) period would fall short as a predictive feature, as it wouldn't account for things like daily traffic rhythm. To gain some intuition about this, recall the assumed model of a (simple univariate) linear regression: $$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$ $$\epsilon \sim N(0,\sigma)$$ (Note that this is very different from finding the slope of the line drawn between first and last points.) Applying this to your case, linear regression assumes that response time is the product of some number $\beta_1$ and time since the period began. Because time since is, by definition, increasing, this amounts to assuming that response times will grow or shrink over the course of the period. I'm no server expert, but this seems a blunt and unrealistic assumption. A model that accounts for heavy-traffic times of day seems more realistic, as it accounts for all of the people using their lunch break to share photos of cats. This is available from your data, you'd simply have to transform the time it was sent. But these are admitted hunches on my part, and I suggest that you test them, or any regression approach, with data regarding when servers actually needed an action. But again these strike me as very elaborate, and mostly tangential to your real aim of deciding when to act on a server. More, you don't have much data to work with: All you really know is response length and when the request was made. Given this, it seems much more straight-forward to cross-validate an anomaly detection method for each server, or use the statistical process control method referenced in the other answer.
