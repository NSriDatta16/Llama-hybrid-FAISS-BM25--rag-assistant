[site]: crossvalidated
[post_id]: 248772
[parent_id]: 
[tags]: 
Why does the L2 norm heuristic work in measuring uniformity of probability distributions?

To start off, please go through this question regarding measuring non-uniformity in probability distributions. Among several good answers, user495285 has suggested a heuristic of simply taking the L2 norm of a vector whose values add to 1. I've found in my experiments that it actually works very well for most practical applications and is usually performant. I am now in the process of writing a paper about a machine learning algorithm, which makes use of this heuristic. However, I need to provide at least some background explaining why this heuristic works reasonably well. Given that taking L2 norms for this purpose is probably just a heuristic, I understand that there might not be a solid theoretical basis, but I nevertheless need an intuition on what could be going on, so that I can at least explain it in the paper and I'm myself clear regarding what's going on. Ideally, if there's a proper explanation available that I can directly cite, kindly share it here. I looked up the web and could find some documents that talk about using L2 norms in context of measuring uniformity, but I'm not sure if they give an intuitive explanation of why it works and if they are citable. Here are the documents: Testing Uniformity of Distributions Ratio and Difference of l1 and l2 Norms and Sparse Representation with Coherent Dictionaries Sublinear time algorithms In addition, if you have any additional ideas on how to measure non-uniformity in distributions or you could tell why some measure is better than others, please let me know.
