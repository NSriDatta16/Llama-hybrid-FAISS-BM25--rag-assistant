[site]: datascience
[post_id]: 85345
[parent_id]: 85328
[tags]: 
The basic idea of most of the current question answering architectures is: get a common representation of the question and the input text (e.g., using BERT) get a representation of the answers do sort of attention over the answers: compute a scalar score for each of the answers (using a dot-product, linear layer, multilayer-perceptron) and normalize the scores using softmax The architectures are typically based on the Bidirectional Attention Flow Model , although it was designed for a slightly different task and although the pre-trained word embeddings and RNNs in the model are today usually replaced with BERT-like models. In 2018, there was a competition in question answering at SemEval where many interesting ideas on this problem were presented.
