[site]: crossvalidated
[post_id]: 473584
[parent_id]: 350695
[tags]: 
@Accumulation made good points (+1) that by minimizing loss you are already incorporating feedback and that you can always use weighted ensemble of different classifiers. Another thing that you could do is to use AdaBoost algorithm, where you train a model, then calculate errors, and train another model where misclassified samples are up-weighted during training, you repeat so a number of times and then ensemble the model. This is a well-known and pretty popular algorithm in machine learning, you could adapt your code to use this training schema for any model you want, the only requirement is to be able to use weighted loss.
