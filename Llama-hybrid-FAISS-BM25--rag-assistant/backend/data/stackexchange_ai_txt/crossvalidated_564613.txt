[site]: crossvalidated
[post_id]: 564613
[parent_id]: 564579
[tags]: 
A "traditional" transformation for $n$ variables that must add up to a constant (usually we use 1 corresponding to 100%), is to work with $n-1$ parameters $\alpha_i \in (-\infty, \infty)$ , to define the $n$ th one as $\alpha_n = 1 - \sum_{i=1}^n \alpha_i$ ("sum to zero constraint") and to use the softmax function to define $$w_i = \frac{e^{\alpha_i}}{\sum_{i=1}^n e^{\alpha_i}}.$$ We will then have $\sum_{i=1}^n w_i =1$ and all $w_i$ will be in $(0,1)$ . The $\alpha_i$ for $i=1,\ldots,n-1$ could then each have a set of linear regression terms behind them (or some other kind of model, e.g. if you like a neural network or whatever else).
