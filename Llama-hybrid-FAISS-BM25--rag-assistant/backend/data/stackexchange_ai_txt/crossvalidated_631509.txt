[site]: crossvalidated
[post_id]: 631509
[parent_id]: 
[tags]: 
Is it possible to have better results by PCA PCs in compare to Laplacian eigenmap

Suppouse I have a data set of the form $p = 200$ and $N = 35$ . I am interesting in the multiple linear regression model train, for this reason I need somehow simplify my data. I decided to use two methods: PCA and Laplacian Eigenmap. I tried to reduce the dimension to $p \in [1,\cdots 28]$ ( $28$ because $7$ other point I have used for the test of the model) and for any $p$ in this interval resulting MLR model has much better $R^2$ for the PCA PCs (twice better usually). How is it possible? And yes, all $200$ features highly pairwise correlated, could it be the reason?
