[site]: crossvalidated
[post_id]: 263246
[parent_id]: 
[tags]: 
Does it make sense to use auto-encoders to reconstruct GIST features?

I am trying to extract good low dimensional representation of CIFAR-10 images in an unsupervised way. It is a project requirement that I use 512-d GIST features, reduce the dimensions to 32 using PCA and reduce the dimensions using Auto encoder and compare. I am using image-classification accuracy as the final evaluation criteria for the extracted features. I used linear SVM, Medium Gaussian SVM and Medium KNN as classifiers on top of the extracted features. I was expecting the Auto-Encoder to do better than PCA. However, PCA is giving better results (higher by around 5%) as compared to the AE. My AE structure is as follows, 512-128-64-32-64-128-512. I used binary cross entropy as my loss to learn the AE. Should I use layer-by-layer pre-training strategy to train the AE. Also, does it make sense at all to use GIST as input to the AE? I believe that using an AE makes sense when the features are related to each other somehow to facilitate good compression. But I am not sure if any compression is possible on GIST features, or if it is meaningful to try to compress GIST. Any help will be greatly appreciated.
