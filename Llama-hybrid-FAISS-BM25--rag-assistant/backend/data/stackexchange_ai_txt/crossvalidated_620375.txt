[site]: crossvalidated
[post_id]: 620375
[parent_id]: 620349
[tags]: 
Inspecting the docs for CalibratedClassifierCV may offer more clarity (emphasis my own): This class uses cross-validation to both estimate the parameters of a classifier and subsequently calibrate a classifier. With default ensemble=True , for each cv split it fits a copy of the base estimator to the training subset, and calibrates it using the testing subset. For prediction, predicted probabilities are averaged across these individual calibrated classifiers. When ensemble=False , cross-validation is used to obtain unbiased predictions, via cross_val_predict , which are then used for calibration. For prediction, the base estimator, trained using all the data, is used. This is the method implemented when probabilities=True for sklearn.svm estimators. The "for calibration" part is presumably shorthand for your step 3. So your procedure is correct. Verifying this fact in the code is tricky. I could only verify that, for binary classification with probability=True , exactly two parameters are learned: one for the slope in logistic regression, and another for the intercept. See the properties probA_ and probB_ , which must be non-empty to run predict_proba . I agree that this part of the documentation you linked should be re-worded to the probabilities are calibrated using Platt scaling [9]: logistic regression on the SVM â€™s scores , fit by an additional cross-validation on the training data. obtained via cross_val_predict .
