[site]: crossvalidated
[post_id]: 123219
[parent_id]: 112856
[tags]: 
The Kruskal-Wallis test is not appropriate in this case, because it deals with a design that is similar in structure to a one-way analysis of variance (ANOVA). Also, even if it were appropriate, it does not really deal with the theoretical problems of the distribution of these data any better than ANOVA might be expected. Linear models can be thought of as a first-order approximation to the underlying phenomenon that you are measuring. Pragmatically, a primary question is "How well does the model fit the data?" The tests for statistical significance are reasonably robust under some deviations from the underlying assumptions. In this case, there could be an issue if answers to some of the scenarios were negatively correlated. Averaging (or summing) the answers from each subject kind of implicitly assumes that they all measure the same thing. That is very reasonable in this case, but it is worth checking out. A linear model is going to give you more bang for the buck in terms of being able to accurately model your design, quantify things, and slice and dice the results. So, if a linear model is a reasonable approximation then I would use it. There are other modeling methods that could be used here --- it would be comforting to see that they agree with the linear model chosen. Here are some approaches that you might use in R . First, I am going to set up some totally random fake data. # Make up some data. set.seed(123) Relationship Next, look at the pattern of results: # Plot the data to look for patterns in the response values. library(reshape2) Wide.Data Here the goal is to explore for obvious relationships among the answers. There are many other plots and summaries that would be good to generate. The principal components analysis could give you an idea of whether the dimensionality of the questions is going to be well represented by a one dimensional summary. If the loadings on the first component are roughly equal and the variance explained by the first component is high then an average is well supported. Next, look at linear model fits. A model that accurately represents the design would have errors in responses from each subject arbitrarily correlated with differing variabilities: # Fit an unstructured correlation matrix for measurements from the # same subject. (Note that this depends critically on the sort order # of the Situation items being the same for each subject. The safe way # is to define a "Time" variable that gives correct order.) library(nlme) fit This model is pretty rich in parameters and you could expect problems fitting it. Also, it is modeling at the granularity of the individual response value, so the linear model approximation might be tenuous. But, maybe with enough data it would not be bad. A simpler model would provide a simple random effect for each subject, implying all errors in response values from each subject are equally (and positively) correlated: # Fit a simpler model with a random effect for each subject. fit This model is pretty likely to work but again may be too granular to fit well. Finally, you can fit a model to some summary of the four response values (such as the average, or some other combination): # Compare with results from simply averaging the values. library(plyr) Mean.Data One of these last two approaches is likely to be pretty good, if you have a decent amount of data. Interpreting the results is going to be dependent on your population and sampling strategy. Other options for fitting the data: Carry out the analysis for each question separately (better to average if appropriate) Model the ranks using modern approaches such as nparLD or rlme Dichotomize the responses and fit binomial regressions Fit a multinomial mixed effect model (good luck, but see this post ) These approaches tend to lose information in varying degrees, which changes the type of conclusion you can muster after all is said and done.
