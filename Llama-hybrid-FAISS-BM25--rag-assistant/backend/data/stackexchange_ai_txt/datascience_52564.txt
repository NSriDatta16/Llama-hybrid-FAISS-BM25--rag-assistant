[site]: datascience
[post_id]: 52564
[parent_id]: 52515
[tags]: 
K-means minimizes sum of squared errors, and PCA finds the projection with maximum sum of squares. So they are a quite natural fit. Just run k-means and project it to 2d for visualization with PCA. They you are largely seeing the data the same way as k-means (if you only use the rotation, not the scaling!) What I'd be more concerned about is the input data, as it is not particularly well suited for neither k-means nor PCA. So I wouldn't be surprised if the results are barely interpretable. Both methods make most sense if the input variables are continuous and of the same scale.
