[site]: crossvalidated
[post_id]: 485091
[parent_id]: 
[tags]: 
Standard error of model residual SD from simulation

I'm futzing with getting the SE of model residual standard deviations from a linear regression, and keep getting narrower errors than I should - and I'd like to figure out why. The basic approach I'm taking is to fit a linear model. Draw simulated coefficients from a multivariate normal distribution. Calculate the RSS and from that, use sqrt(RSS/(n-2)) to calculate the model residual SD. Rinse and repeat 1K times, and then get the SD of the model residual SD. But... I keep finding that I'm off by an order of magnitude at least. Here's an example in R. First, the model. library(palmerpenguins) plot(bill_length_mm ~ flipper_length_mm, data = penguins) pen Then, the simulated coefficients. library(mvtnorm) coefTab Now, on to getting the residual SD. A function! get_resid_sd And let's apply it to our coefficients coefTab$sigma Now, the SE > sd(coefTab$sigma) [1] 0.01208458 OK, but.... to validate, let's use rstanarm as it naturally produces simulations with no extra work, and can produce equivalent results to lm() using stan_glm() with the appropriate optimizer and null priors. library(rstanarm) penStan Now, the SE of the model residual SD.... > sd(as.matrix(penStan)[,3]) [1] 0.1639105 Huh. You see why I'm a) glad I checked myself and b) why I'm worried I did something tragically wrong. Would love to know folks' thoughts, as if fixed, I think this is a killer example for students. Feel like I'm falling down on something obvious due to 2020 brain.
