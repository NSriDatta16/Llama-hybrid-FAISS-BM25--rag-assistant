[site]: crossvalidated
[post_id]: 463870
[parent_id]: 
[tags]: 
eval_set in xgboost and validation data

In my understanding, if I am picking from a set of models, each with a different set of hyper-parameters, the proper way to approach it is like this: First, split the data to train and val . Then fit the model with train , and score it with the val . Then pick the model and hyper-parameters with the highest score, and re-train it using the combined set of train and val . So for example if I am picking among knn models with k1, ..., k3 neighbours, and random forest models with a1, ..., a5 trees, I fit all 3 + 5 = 8 models with train and I score the result with val . I then pick the model with the best score, say knn with k2 neighbours, and re-train it with the train and val combined. To this point I think my understanding is correct. But when I got to xgboost, I got really confused. In xgboost fitting function, there are two parameters that confuse me: early_stopping_rounds and eval_set . Based on the documentation , eval_set is a validation set. If the score based on eval_set does not improve in early_stopping_rounds rounds, the training stops and returns the result. My question is how can the validation data be used in the fitting process? I am thinking about two possibilities, but I am not sure which is correct, or if neither is: possibility 1 : eval_set should be a subset of the train set. So continuing with the above example, I should split train to train_for_train and train_for_val , and set eval_set as train_for_val . I should then score the model returned with the with the same val as my knn and random forest models. possibility 2 : eval_set doesn't affect how the trees are build, so it isn't technically used to fit the model. Therefore, there is less concern on over-fitting. eval_set affects how many rounds there is, which is effectively how many trees to use in the ensemble. In the same way that it is okay to use val to determine the optimal number of trees in random forest, it is also fine to use val to determine the optimal number of rounds in xgboost. So it's okay to set eval_set as val during model fitting, score the returned model with val , and compare the score with the other knn and random forest models. If possibility 2 is correct, my next question is: If xgboost with the automatically detected optimal number of rounds turns out to have the highest score in all the models, how do I re-train it with train and val ? I don't see the option for this hyper-parameter in the documentation. Finally, I am also confused with how to use cross-validation with xgboost? If I let the model decide the best number of rounds using eval_set and early_stopping_rounds , the result would be different when I use different folds for train and validation. Or should I not use cross-validation if I am letting the model decide the hyper-parameters, and only use it when I am the one specifying the sets of hyper-parameters to choose from?
