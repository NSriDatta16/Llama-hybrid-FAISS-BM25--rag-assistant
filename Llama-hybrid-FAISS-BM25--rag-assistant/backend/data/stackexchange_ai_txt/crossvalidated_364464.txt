[site]: crossvalidated
[post_id]: 364464
[parent_id]: 
[tags]: 
Help with Exercise 9.11 from Bishop's PRML book

Exercise 9.11 from Bishop's "Pattern Recognition and Machine Learning" asks Show that in the limit $\epsilon\to0$, maximizing the expected complete-data log likelihood for the Gaussian mixture model in which all components have covariance $\epsilon I$, given by (9.40), $$ \mathbb{E}_Z[\ln p(X,Z|\mu,\Sigma,\pi)] = \sum_n \sum_k \gamma(z_{nk}) \bigl\{ \ln\pi_k + \ln\mathcal{N}(x_n|\mu_k,\Sigma_k) \bigr\} \tag{9.40} $$ is equivalent to minimizing the distortion measure $J$ for the $K$-means algorithm given by (9.1) $$ J = \sum_n \sum_k r_{nk} \|x_n-\mu_k\|^2. \tag{9.1} $$ I agree that $\gamma(z_{nk})\to r_{nk}$ as $\epsilon\to0$. But I could not derive the equation (9.43) $$ \mathbb{E}_Z[\ln p(X,Z|\mu,\Sigma,\pi)] \to -\frac{1}{2} \sum_n \sum_k r_{nk} \|x_n-\mu_k\|^2 + \text{const} \tag{9.43} $$ as $\epsilon\to0$. Any comment is welcome. I leave an answer here for a reference in future. Answer: (with the help of KDG ) The equation (9.43) in the book is not correct. The correct one is $$ \epsilon\cdot\mathbb{E}_Z[\ln p(X,Z|\mu,\Sigma,\pi)] \to -\frac{1}{2} \sum_n \sum_k r_{nk} \|x_n-\mu_k\|^2 $$ as $\epsilon\to0$. (See stats.stackexchange.com/a/129918/212274 for further discussion.)
