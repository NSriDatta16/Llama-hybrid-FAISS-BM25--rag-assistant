[site]: crossvalidated
[post_id]: 641692
[parent_id]: 641678
[tags]: 
Let us consider this problem from a Bayesian perspective. We start with a prior distribution on the changepoint, let us say, $d \sim U(\{1, 2, \dots, |D|\})$ . Given $d$ , the probability of seeing $x_1$ successes from the Bernoulli distribution for $i and $x_2$ successes from the Bernoulli distribution for $i \geq d$ is: $$p(x_1,x_2 | Q_1, Q_2, d, |D|) = p(x_1|Q_1, d-1)p(x_2|Q_2,|D|-d+1)$$ where the component distributions on the right-hand side are Binomial distributions. This gives us the likelihood function; given our uniform prior distribution, the posterior distribution of $d$ will be proportional to the likelihood function. An example in R follows. We set $Q_1 = 0.25$ , $Q_2 = 0.75$ , $|D| = 100$ and $d = 30$ . Q_1 1) { x1 A plot of the posterior distribution, with vertical lines at the cumulative 10th, 50th, and 90th percentiles: Note, however, that with this little data, this is a good result. If your probabilities are closer together and you don't have many observations, the randomness inherent in the Bernoulli distribution can result in posteriors that look like this (here $Q_1 = 0.35$ and $Q_2 = 0.65$ ): The posterior mean is 17.5, and the true value is within the posterior 10%-90% range. Still, the posterior has a mode at $d=1$ , thanks to the fact that the sum of the observations is 61, which is quite compatible with the data being drawn from a single Binomial $(100, 0.65)$ distribution. Of course, we aren't constrained to use a uniform prior on $d$ . We now model our belief that the changepoint is more likely to be in the middle of the sample than near the tails using a rescaled $Beta(2,2)$ distribution: which, applied to the same data that generated the previous posterior, results in:
