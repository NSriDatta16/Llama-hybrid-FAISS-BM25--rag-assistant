[site]: crossvalidated
[post_id]: 602062
[parent_id]: 
[tags]: 
How does inference work on a Transformer?

(Let's say I trained a transformer for translation.) In the training, the output sentences are given and fed into the decoder as a whole. However, with inference, only a start-of-sentence (SOS) token is given. How does the model know what word to choose to start the sentence? And how does it know what subsequent words to choose? I know that the decoder has a feedforward network after its attention heads. Is the choosing done in this part of the decoder (I know the output is a list of probabilities, but I am ignoring those for simplicity)?
