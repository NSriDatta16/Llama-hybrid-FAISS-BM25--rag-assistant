[site]: crossvalidated
[post_id]: 431202
[parent_id]: 
[tags]: 
Does ReLU produce the same effect as dropouts?

When we add dropouts to a densely connect layer, it randomly ignores nodes, by considering their output to always be zero. Though we may not observe the exact same effect in a CNN with ReLU as its activation, isn't what happens somewhat similar to using dropout? When training images with enough noise won't random nodes be turned on and off during the training process? The only difference is, it also extends to the model we use after deployment. Is there a proper way to establish this similarity or do both these methods solve completely different problems? Also, for a long time I have ignored the fact that ReLU is not actually differentiable at all points.
