[site]: crossvalidated
[post_id]: 39114
[parent_id]: 39105
[tags]: 
My experience with SVM has shown it to be fairly robust to uninformative features. Other classifiers, like Na√Øve Bayes, for example, tend to be more sensitive to such features, making feature selection a relatively more important part of the classification workflow. In terms of feature selection algorithms, I'm a fan of using information theory-based metrics, like mutual information . When I use this, I usually generate a graph of the distribution of the input feature's mutual information during cross-validation, and then manually select some cut-off.
