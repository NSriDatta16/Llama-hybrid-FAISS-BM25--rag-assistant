[site]: datascience
[post_id]: 8677
[parent_id]: 8264
[tags]: 
There is no one-size-fits-all Also, data dimensionality is not the same everywhere. Text data, which is inherently sparse, has a very different intrinsic dimensionality than e.g. random Gaussians. For text data, linear SVMs are known to work very well. RBF kernels do not work well with high-dimensional data, because they are distance-based at the core, and choosing the sigma parameter becomes next to impossible. If you can "fold" dimensions, you also get very different behavior. Im image recognition, you usually have thousands of pixels. However, you never look at all of them at once . Instead, you use convolutional kernels that move over the data space, and they may have only say 32x32 pixels. That is still 1024 dimensions, but not millions anymore.
