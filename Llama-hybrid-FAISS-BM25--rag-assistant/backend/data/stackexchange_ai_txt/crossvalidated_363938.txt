[site]: crossvalidated
[post_id]: 363938
[parent_id]: 363800
[tags]: 
Cross validation is used to either compare different models or to tune the parameter. For the case of the parameter tuning, you can consider the models with different parameters to be unique models that you want to test. For both cases, the $MSE_i$ are averaged out for each model, and then compared to $CV$ parameter in different models. These $CV$ parameters cannot be used in absolute sense, they only make sense when compared to $CV$ parameters from the same data, same kind of cross validation, and different model (think of this like you would think of AIC and BIC). Once you have the $CV$ parameters from different models, you can choose the appropriate model (or parameter) by picking the smallest number (smaller the average $MSE$ the better). Grid search cross validation is usually employed for parameter tuning, while K-fold is more general approach. Grid-search is a way to select the best of a family of models, parametrized by a grid of parameters.
