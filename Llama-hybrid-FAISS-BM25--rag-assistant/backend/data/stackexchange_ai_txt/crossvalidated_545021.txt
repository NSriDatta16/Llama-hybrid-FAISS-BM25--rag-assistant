[site]: crossvalidated
[post_id]: 545021
[parent_id]: 256013
[tags]: 
Obviously very late, but as Andrew's answer mentions, each $f^{(i)}$ is paired with the same label as $x^{(i)}$ . That is, if $x^{(i)}$ is paired with a corresponding label of $1$ , then $f^{(i)}$ will also be paired with $1$ . For this reason, if we only utilize the features $x^{(i)}$ with corresponding labels of $1$ (i.e. positive features), we'd end up with a transformed dataset in which each of the features $f^{(i)}$ have corresponding label of $1$ . Training an SVM on this transformed dataset would therefore yield an SVM that could constantly predict $1$ , and obtain a cost of $0$ each time, which obviously isn't very helpful at all. This is all to say we need to pick both positive and negative feature as landmarks, otherwise we could end up with a transformed dataset so skewed that it becomes impossible to train an SVM with a reasonable test set performance on said transformed dataset. Hope this helps.
