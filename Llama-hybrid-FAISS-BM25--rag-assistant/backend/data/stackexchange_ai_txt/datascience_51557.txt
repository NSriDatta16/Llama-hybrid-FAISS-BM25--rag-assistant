[site]: datascience
[post_id]: 51557
[parent_id]: 51404
[tags]: 
You might find this paper might be the closest thing to what you are looking for if you don't want to treat it as a regular hyperparameter: Towards Lower Bounds on Number of Dimensions for Word Embeddings The paper claims that there is a lower bound on the embedding based on the corpus. It also purposes a method for finding said lower bound which I will leave the paper to explain since I think I will not do it justice. Here is the most relevant section of the conclusion of the paper: We discussed the importance of deciding the number of dimensions for word embedding training by looking at the corpus. We motivated the idea using abstract examples and gave an algorithm for finding the lower bound. Our experiments showed that performance of word embeddings is poor, until the lower bound is reached. Thereafter, it stabilizes. Therefore, such bounds should be used to decide the number of dimensions, instead of trial and error. It has sourced and cited previous work regarding embedding dimension which also might be of interest to you. Unfortunately the conclusion seems to be the following: As is evident from the above discussion, the analysis of the number of dimensions have not received enough attention. This paper is a contribution towards that direction.
