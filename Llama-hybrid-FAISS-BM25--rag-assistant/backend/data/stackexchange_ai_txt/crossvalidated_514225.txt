[site]: crossvalidated
[post_id]: 514225
[parent_id]: 
[tags]: 
PCA versus other ways of feature selections?

I have been using many approaches like the random forest, ANOVA, chi^2, etc. to select the most important features for machine learning. After I read more on the feature selection and dimensionality reduction, I find that there is no one method that fits all cases. I find that many books about machine learning mention the principal component analysis. I find a dataset with 60 features (independent variables) and 1 dependent variable, I apply the PCA on it and find that 8 features' variance contributing to 95% of the total variance. As many books explain, PCA is an unsupervised method, while most ML-based feature selection methods are supervised approaches. In this sense, I do not see how PCA could help to reduce the dimensionality (by removing those low-variance variables). I find one thing very interesting. I apply a random forest method and PCA on the same datasets, which turns out that the most important features from the random forest are totally different from those given my PCA. I prepare two data sets, one with variables, suggested by random forest, removed and one with variables suggested by PCA, removed, and then fit them into the same regression model with the machine learning method. The one with the random forest has an accuracy of 20% higher than that on PCA. I am not sure if the result makes sense or not, but if it is true, how could one use PCA for features selection?
