[site]: crossvalidated
[post_id]: 106048
[parent_id]: 104397
[tags]: 
Any Bayesian classifier can be easily tweaked to incorporate knowledge about how often a particular class is expected. When you train a Bayesian classifier, two sets of parameters are learned: P(C=c), the probability that an observation belongs to class C (the class prior probabilities) P(F=f | C=c), the probability that an observation has the feature set F given that it belongs class C (i.e. its likelihood). The classification rule is to choose a c that maximizes P(C=c)*P(F=f|C=c). (See: http://en.wikipedia.org/wiki/Naive_Bayes_classifier#Constructing_a_classifier_from_the_probability_model ) You can modify P(C=c) according to the expected occurrences of positive and negative observations in your production environment. Then your classification criterion will be optimal. I wouldn't reduce the amount of positive observations in the training dataset. This will indeed change the prior probabilities to better match your test dataset. However, it will hurt the estimation of the likelihood parameters (since you won't use all available data). It's much better to use all available data and then modify the class prior probabilities according to your needs. When using discriminative classifiers such as SVM, the latter approach is less straightforward (since P(C=c) isn't explicitly modeled) and then the logic of keeping both the training and test datasets similarly (im)balanced makes sense.
