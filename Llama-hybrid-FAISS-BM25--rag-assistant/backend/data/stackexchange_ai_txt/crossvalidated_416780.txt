[site]: crossvalidated
[post_id]: 416780
[parent_id]: 
[tags]: 
Neural language model: Derivation of MLE

Recently, I studied NNLM and I saw the derivation of softmax using MLE: \begin{align} & \frac{\partial\log P(w_t\mid h)}{\partial\theta} \\[8pt] = {} & \frac{\partial \log \exp(s_\theta(w_t,h)) - \log\sum_{w'\in V} \exp(s_\theta (w', h))}{\partial\theta} \end{align} For the second, the log that inconveniently precedes the sum (known as a log-sum-exp) complicates the calculation of the derivative, but it can be approached using two inverse identities to rewrite the second term into a more conveinent form: \begin{align} & \frac{\partial}{\partial\theta}\log\sum_{w'\in V} \exp(s_\theta (w', h)) \\[8pt] = {} & \frac1{\sum_{w'\in V}\exp(s_\theta (w', h))}\sum_{w'\in V}\frac{\partial}{\partial\theta} \exp(s_\theta (w', h)) \\[8pt] = {} & \frac1{Z_\theta(h)}\sum_{w'\in V}\exp(s_\theta (w', h))\frac{\partial}{\partial\theta}\log \exp(s_\theta (w_t, h)) \\[8pt] = {} & \sum_{w'\in V} \frac{\exp(s_\theta (w', h))}{Z_\theta(h)}\, \frac{\partial}{\partial\theta}s_\theta(w',h) \end{align} where the $w_t$ is the target word, $h$ is history which means the contexts of the target word, and $V$ is vocabulary. In the derivation, I don't understand why $w'$ changed to $w_t$ . Can you please explain it to me? Thanks for reading.
