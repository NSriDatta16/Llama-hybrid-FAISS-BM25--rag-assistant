[site]: crossvalidated
[post_id]: 445078
[parent_id]: 445043
[tags]: 
I read a paper a while back that used an L2-norm to do sort of a "temporal smoothing" process across the predictions, which I thought was an interesting way to deal with time-varying components. I unfortunately can't find it but here's a similar paper I found Learning Time-Varying Graphs using Temporally Smoothed L1-Regularized Logistic Regression http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.480.550&rep=rep1&type=pdf I don't think this answers the question directly, but perhaps it can give you ideas of how to play around with penalty functions across the time points, to get a "stable" estimate of each factor's contribution to the neural activation? On the other hand, going back to your proposed idea of shuffling and permuting the values/matrices to determine each factor's estimate, then here's a page that talks about how this same procedure is done in random forests. Hope this helps!
