[site]: crossvalidated
[post_id]: 497100
[parent_id]: 
[tags]: 
Order of dropout and activation in 1D convolutional networks

I have a simple cnn-lstm network. There are two 1D convolutional layers after the input layer. Every 1D convolutional layer is followed by a dropout. What I observe is that when I have conv1D -> dropout -> activation, I get minimally better results (about 1%) compared with conv1D -> activation -> dropout (I use Relu as the activation function). I tried to find an example or some explanation of why this is the case, but couldn't find anything useful. Can you think of any reason why it works this way?
