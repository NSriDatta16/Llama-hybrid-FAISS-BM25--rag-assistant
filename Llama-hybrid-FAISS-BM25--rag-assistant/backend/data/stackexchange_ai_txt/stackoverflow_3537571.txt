[site]: stackoverflow
[post_id]: 3537571
[parent_id]: 
[tags]: 
Unit-testing coverage policy, test quality concerns

our company is trying to increase software quality by enforcing minimum function coverage on automated unit-tests. This is already a good starting point to get at least some tests written and to get them automated (although it would be better to go for branch or decision coverage). My main concern is the outcome of the tests which are going to be written after this policy is taken into use. I have seen too often such rules causing huge amount of null testing (i.e. nothing is asserted) or some maintenance nightmare kind of integration tests. I found following SO questions close to the subject but these concentrate more on coverage percentages: Pitfalls of code coverage What is a reasonable code coverage % for unit tests (and why)? Instead, I would like to get help or insight, how we could avoid awful quality of tests. So here's a couple of most worst unit-test no-nos and what I have already thought for avoiding them: 1) Null testing Code review for test code also As we use testing framework, assertation are done by using FW macros. Maybe we could write some tool which would calculate ratio for assertation per method call of class under testing. No idea yet, what would be good enough ratio. 2) Integration tests Again review Perhaps some code analysis tool to check dependencies of test code to production code. Test class should have very little amount of dependencies to other classes (except to the one under testing) of the tested system. There's plenty of teams and I am not totally convinced that team-internal reviews would be enough in all cases. Therefore I would be more interested in ways of automating quality ensurance. TIA, lutku
