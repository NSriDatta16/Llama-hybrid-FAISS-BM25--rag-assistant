[site]: crossvalidated
[post_id]: 479478
[parent_id]: 479475
[tags]: 
The description you give is basically what a sigmoid feed-forward neural network does in its hidden layers: find $a,b$ so that $\sigma(x|a,b)$ minimizes some loss, where $\sigma$ is any sigmoid function, for example you could choose $\sigma(x|a,b)=\tanh(ax+b)$ . Depending on the choice of $a,b$ , the function could be basically constant at a large value, basically constant at a small value, or approximately linear, or some kind of mix of all three. This is presented in terms of scalar-valued functions, but NNs with more than one unit uses matrix-vector products.
