[site]: crossvalidated
[post_id]: 55604
[parent_id]: 55597
[tags]: 
Support Vector Machines classify new vectors by comparing them against the set of support vectors. Depending on what parameters you used and the cost function, this set of support vectors might be large. For more than two classes, the number of SVMs needed increases as well, further reducing performance. For better runtime performance, you'll want something that does all of the training upfront. One such classifier is the neural network . It does all training upfront, leaving classifications as simple calculations. Another is a Bayesian classifier, which requires pdfs of the classes of your expected data. Only probabilities are calculated during classification, so its performance isn't affected by training set size. If you need your classifier to further minimize the number of false positives at the risk of increasing the number of false negatives , then consider implementing a loss function . With it, you can assign a cost to each type of error. In your example, that means classifying fewer negatives as positives while allowing more positives as negatives. A clear example of loss functions is a test for cancer, where it's assumed to be better to falsely diagnose someone who doesn't have cancer and they live than it is to not diagnose someone who does and they die. EDIT: Clarified SVM and Bayesian sections. Performance issue with SVMs is that there might be a large amount of SVs to check against new vectors. Generally, more SVs are used to increase fit to the training set (this is okay, but avoid overfitting). The Bayesian classifier simply requires that you know the distribution of your data. Also, forgot that SVMs are built to only distinguish between 2 classes. To support more classes, multiple SVMs using the one-vs-all approach are merged. This would also impact runtime performance.
