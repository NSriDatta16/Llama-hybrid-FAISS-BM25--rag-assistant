[site]: crossvalidated
[post_id]: 272409
[parent_id]: 
[tags]: 
Mixing shuffle order on training set for future epochs

Sometimes I like to train a neural network model on a number of epochs, save it, and load it again at a later date for more training (on the exact same training set). During the prior training I'll have the order of the training set randomly shuffled. When I later resume the saved model, the shuffled order of the training set is often lost. If I resume training with the same training set, but in a different shuffled order, will this lead to any problems for the future epochs? Again, this is the exact same training set (no data added to it, just shuffled differently for future epochs).
