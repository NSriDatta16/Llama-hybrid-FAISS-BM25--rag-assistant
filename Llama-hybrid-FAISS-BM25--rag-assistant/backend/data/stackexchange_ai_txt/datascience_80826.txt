[site]: datascience
[post_id]: 80826
[parent_id]: 
[tags]: 
Transformer masking during training or inference?

I'm working through Attention is All you Need , and I have a question about masking in the decoder. It's stated that masking is used to ensure the model doesn't attend to any tokens in the future (not yet predicted), so it can be used autoregressively during inference. I don't understand how masking is used during inference. When the encoder is given an unseen sample with no ground truth output or prediction, it seems to me that there is nothing to mask, since there aren't any output tokens beyond what the decoder has already produced. Is my understanding of masking correct? Thanks!
