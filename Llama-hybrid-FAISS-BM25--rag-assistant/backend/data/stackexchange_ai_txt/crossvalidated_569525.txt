[site]: crossvalidated
[post_id]: 569525
[parent_id]: 
[tags]: 
Do neural networks with similar weights necessarily compute similar functions?

Let $f_1(\cdot|\Theta_1)$ and $f_2(\cdot|\Theta_2)$ be two Feedforward Neural Networks with the same architectures (number of layers, width of each layer, activation function...) and parameter arrays $\Theta_1$ and $\Theta_2$ respectively. Since $f_1(\cdot|\Theta_1)$ and $f_2(\cdot|\Theta_2)$ are entirely parametrized by their respective parameters, I am interested in knowing whether a uniform bound on the distance between $\Theta_1$ and $\Theta_2$ implies a uniform bound on the distance between $f_1(\cdot|\Theta_1)$ and $f_2(\cdot|\Theta_2)$ as well. More precisely, my question can be formulated as follows : Does $\|\Theta_1-\Theta_2\|_\infty\le\epsilon\implies\|f_1(\cdot|\Theta_1)-f_2(\cdot|\Theta_2)\|_\infty\le g(\epsilon)$ ? If this is not true in general, under what assumptions can this be verified ? (Where $g(\epsilon)\to0$ when $\epsilon\to0$ ). Intuitively, I'd expect this relation to hold as long as the activation functions are well-behaved enough, but the answer might not be so simple, as suggested by this paper by Petersen, Raslan and Voigtlaender, in which the authors show that the converse of my desired identity is false. In fact, I would be happy with the probabilistic version of the implication, i.e. Does $\mathbb P\left(\|\Theta_1-\Theta_2\|_\infty>\epsilon\right)\le\delta\implies\mathbb P\left(\|f_1(\cdot|\Theta_1)-f_2(\cdot|\Theta_2)\|_\infty>g(\epsilon)\right)\le h(\delta)$ ? If this is not true in general, under what assumptions can this be verified ? (Where $g(\epsilon)\to0$ when $\epsilon\to0$ and $h(\delta)\to0$ when $\delta\to0$ ). I would also be grateful for references which might be helpful in proving the desired results. My thoughts : Feedforward Neural Networks can be mathematically written down as $$f(\cdot|\Theta) : x\mapsto W_L\sigma_{V_L}W_{L-1}\sigma_{V_{L-1}}\ldots W_1\sigma_{V_1}W_0 x,\quad x\in\mathbb R^d $$ Where the $W_i$ are the weight matrices, $\sigma$ are the bias activation functions between each layers and $V_i$ are biases such that for $x'\in\mathbb R^J$ , $\sigma_{V_i}(x') = (\sigma(x'^1-V_i^1,\ldots,\sigma(x'^J - V_i^J))^T$ . Here $\Theta$ would be the collection of all the $W_i$ and $V_i$ . The thing is that this expression is really awkward to work with in practice, and I don't see a straightforward way to translate the bound on $\|\Theta_1-\Theta_2\|_\infty$ to a bound on $\|f_1(\cdot|\Theta_1)-f_2(\cdot|\Theta_2)\|_\infty$ , so I haven't made much progress so far.
