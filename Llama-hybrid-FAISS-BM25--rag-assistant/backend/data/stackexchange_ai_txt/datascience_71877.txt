[site]: datascience
[post_id]: 71877
[parent_id]: 
[tags]: 
How do I discern document structure from differently-tagged XML documents?

I have a body of PDF documents of differing vintage. Our group had exported the documents as text to feed them into a natural-language parser (I think) to pull out subject-verb-predicate triples. This hasn't performed as well as hoped so I exported the documents as XML using Acrobat Pro, hoping to capture the semantic document structure in order to pass it in as a hint to the text parser. One document looked pretty good (something like this): ... ... ... ... ... ... ... ... ... ... Another one wasn't quite so nice semantically: ... (name of document) (title of document) 22 October 2013 ... ... ... ... PREFACE 1. Scope ... 2. Purpose ... 3. Application ... Intentionally Blank SUMMARY OF CHANGES ... ... Intentionally Blank TABLE OF CONTENTS ... ... ... ... ... ... ... CHAPTER IV ... (part of chapter 4 title) ... ... ... ... ... ... ... ... ... ... ... ... Intentionally Blank EXECUTIVE SUMMARY (section title) (a bullet item inside only a paragraph element) (a bullet item inside only a paragraph element) ... ... ... (some text inside only a paragraph element) ... ... ... ... ... ... (some text inside only a paragraph element) ... ... (some text inside only a paragraph element) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I'm relatively new to data science but handling the "normalization" of this kind of data set (XML documents largely in a chapter-section-subsection format) is a fairly tractable problem? Or maybe even a solved one? These are the only two documents I've looked at so far, but I suspect that each document will be snowflake enough that it could benefit from applying a machine learning algorithm to bring some consistency to the tags. I picture something very basic, like nested tags and using the PDF XML output structure as leverage whenever I can. Clarifiation: I'd like to gain some insight on algorithms (possibly existing ones) that can can "recognize" the semantic sections of a document that's structured inconsistently and loosely, like the second example I posted above. It would be straightforward for me to do this manually, but if there's an algorithm that does this (or even part of this) then it would be an improvement over doing it manually. The particular tag structure for the target document (I guess that's what it's called) I like to be more true to what the type of data is. A subset of the HTML5 tags would work: , , , . OpenDocument is probably too much. But something that is a step better than calling everything a . Any insights / pointing in the right direction?
