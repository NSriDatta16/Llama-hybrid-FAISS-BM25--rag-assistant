[site]: crossvalidated
[post_id]: 78756
[parent_id]: 78744
[tags]: 
Yes, it had been tried (including by myself - I tried it with neural nets, with rather mixed success). The Relevance Vector Machine (RVM) does pretty much exactly that, and the regularisation parameters are tuned by maximising the marginal likelihood. The advantage of this is that it leads to a sparse model where uninformative attributes end up with large regularisation parameters and hence small weights. The problem with this approach lies in the tuning of the regularisation parameters, which tends to result in over-fitting the model selection criterion (whether Bayesian or cross-validation based), simply because there are many degrees of freedom introduced by having many hyper-parameters to tune.
