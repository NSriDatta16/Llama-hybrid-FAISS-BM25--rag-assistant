[site]: crossvalidated
[post_id]: 461991
[parent_id]: 
[tags]: 
How do I resize output of a generative network to reconcile with the size of the real data?

I am designing a Generative Adversarial Network (GAN) trained on an image dataset. It has two components: the generator and the discriminator. The generative network outputs an artificial image. The discriminator takes that both real and artificial images and tells whether they are artificial or real. The real images have the size, say, 30x30. The output of the generative network is 29x29. How do I reconcile the two when feeding them to the discriminator? Do I pad one side with zeros or shave off a row and a column from the original images? Do I design a generative network that outputs exactly 30x30 images? This will constrain the architecture I can use. Do I resize the original dataset to 29x29? How do I reconcile different output sizes for loss computation and training when comparing generated and real examples?
