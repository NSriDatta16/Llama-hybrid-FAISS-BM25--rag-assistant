[site]: crossvalidated
[post_id]: 384966
[parent_id]: 
[tags]: 
How does nonlinearity in neural networks find meaningful features?

I'm currently reading through the book 'Neural Network Methods for Natural Language Processing' by Goldberg and I'm confused with the following statement: The nonlinearity of the classifier, as defined by the network structure, is expected to take care of finding the indicative feature combinations, alleviating the need for feature combination engineering. In my understanding, the activation function in each node in a nn brings in the non-linearity because the function itself is non-linear (e.g. sigmoid, relu, etc.) and the weights for each input dimension that are multiplied by the input of each node are changed based on the result they provide. But I don't understand how this does the 'feature engineering' for us, i.e. how it determines which feature combinations are meaningful for, e.g. a classification task?
