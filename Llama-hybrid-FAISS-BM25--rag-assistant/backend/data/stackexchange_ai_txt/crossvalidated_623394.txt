[site]: crossvalidated
[post_id]: 623394
[parent_id]: 
[tags]: 
Maximum simulated likelihood estimation in R - binary logit example

Very often the likelihood function involves an integral which does not have a closed-form solution (e.g., random coefficients, latent variables). In these cases, we can employ monte-carlo simulation to approximate the integral: For example, $\int t(x, e) f(e) de$ has no closed-form solution. Take a random draw from the error distribution $e_1$ . Plug $e_1$ into the $t(x, e)$ and evaluate. -> $t_1$ . Repeat 1. and 2. $R$ times. The average $1/R \sum_{r=1}^{R}t_r$ approximates the integral. I try to explore this algorithm in the most simple case, a binary logistic regression model. I want to compare the maximum simulated likelihood estimates to the closed-form solution. Below, the problem is illustrated. For a more complete example with the underlying formulas and reasoning see https://github.com/dheimgartner/DCM/blob/main/vignettes/full-simulation-binary.pdf Simulate some data for illustration simulate_binary 0) simulated_data$data $beta dat data beta head(dat) Using R's stats::glm fit Closed-form own implementation loglik $x1 + param["beta2"] * dat$ x2 exp Maximum simulated likelihood n_draws $x1 + param["beta2"] * dat$ x2 # 10000 x n_draws indicator_ 0) P_1n Clearly, our loglik function makes sense: Evaluated at the true parameters, we get the highest likelihood (which almost matches the non-simulated likelihood from the chapter before): loglik(p(c(0, 0, 0))) loglik(p(c(1, 1, 1))) loglik(p(c(0, 1, 2))) Remark: The line P_1n is needed because the simulated integral (depending on the parameter values passed to it) can yield P_1n = 0 which results in -Inf when taking the log. Similarly, P_1n = 1 would yield P_0n = 0 and thus the same problem. Therefore we add (or subtract) a very small value epsilon to 0 (from 1). Problem The above maximum simulated likelihood estimator does not converge (or at least is very sensitive to starting values and does not seem to update the parameters reasonably in each itteration). Is it because the approximation with random draws (or pmax ) introduces some non-smoothness into the loglikelihood function? Are there other strategies to write the simulated loglikelihood function (avoiding the problem of taking log of 0)? However, I've also tried to maximize the likelihood (without taking logs) and the convergence issues are still present... Update Plotting the surface of the simulated loglikelihood clearly shows the expected wiggles (the surface is not smooth) which is a problem for the numerical optimization. How does one deal with it?
