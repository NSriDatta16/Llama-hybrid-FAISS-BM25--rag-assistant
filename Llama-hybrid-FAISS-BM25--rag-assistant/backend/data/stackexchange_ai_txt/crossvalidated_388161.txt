[site]: crossvalidated
[post_id]: 388161
[parent_id]: 388153
[tags]: 
Perhaps I can help by explaining the reason cross-validation is used. Motivation: the situation for each model If $\tilde{y}$ is future/unobserved data and $y$ is data you have, a scoring rule/function $S(p,\tilde{y})$ is a function that takes the distribution you're using to forecast $p$ (estimated from $y$ ) , and a realized future/unobserved value $\tilde{y}$ and then gives you a real-valued number/score/utility. Higher is better, although this convention isn't always followed in the literature. This score is random because it's a transformation of a random future data point, so people are usually interested in its expected value. The expected value tells you the predictive capabilities of your model. We could write this as $$ E_f[S(p,\tilde{y})] $$ where $f$ is the unknown true distribution of $\tilde{y}$ . For example, a popular scoring rule is $S(p,\tilde{y}) = \log p(\tilde{y})$ . You take the future data point, and plug it into your log-likelihood. Approximation The trouble though is that the expected score is hard to get at because you don't know the true distribution of $\tilde{y}$ , and you don't want to wait around to get samples of this future data (if you did, you could the Monte-Carlo technique and take the average score to get something really close). Also, you can't just plug in $y$ for $\tilde{y}$ because this will overstate your models' predictive capability. This is why all of this data splitting comes into play. So here's the point: cross-validation is a way to estimate this expected score. You repeatedly partition the data set into different training-set-test-set pairs (aka folds ). For each training set, you estimate the model, predict, and then obtain the score by plugging the test data into the probabilistic prediction. This gives you one score for each fold. Then you take the average of these (or the sum, depending on what reference you're using). A few things about CV: The partitions are nonrandom, test sets are disjoint for each split/estimation/prediction, we never use a data point twice for each split/estimation/prediction, we lose parameter estimation accuracy because each training set is smaller than the full set however, we get to average over many prediction scores, which reduces variance the estimator is still biased, so people may or may not try to correct for this it can be computationally brutal to calculate for some models the logo of this site illustrates this splitting procedure! Many models And yes, you are right, after this whole thing is done for multiple models , you can see which model has the the best (estimated) predictive capability. You can pick that model, and then estimate it using the full data set. Before you implement this in practice, you should know that blindly doing CV for a large collection of models will rarely help you pick the correct one, but that's a question for another thread.
