[site]: datascience
[post_id]: 53356
[parent_id]: 
[tags]: 
Neural Network fails to train when a particular batch normalization layer is removed?

Background info: I built a baseline CNN model for the cifar10 dataset using batch normalization and then ReLu activation after each convolution layer. There are a couple of max pooling layers in between some of the convolution -- batchnorm -- relu layers, and then two fully connected layers at the end. The second to last fully connected layer also is followed by batchnorm and ReLu activation, whereas the last has a softmax activation. I am providing my jupyter notebooks code that links to google colab that you can just run so you can see what I'm talking about. If you try to run the code, make sure to hit the Edit tab, click notebook settings, and select TPU for hardware accelerator Here is the code for the baseline: https://github.com/valentinocc/Keras_cifar10/blob/master/cifar10_cnn_tpu.ipynb The problem: I want to replace all batchnorm and ReLu blocks with the ELu activation, which is supposed to reduce bias shifting and thus make batch normalization unnecessary. However, if I remove the batchnorm after the second to last fully connected layer (in the function "dense_elu_block" in the latter two programs linked to), the neural network does not train. Here is the code for the model with layers using ELu activations except for the fully connected layers, so the model still works: https://github.com/valentinocc/Keras_cifar10/blob/master/cifar10_ELUcnn_tpu.ipynb Here is the code for the model that doesn't seem to train itself at all: https://github.com/valentinocc/Keras_cifar10/blob/master/cifar10_ELUcnn_tpu_BROKEN.ipynb The only difference between the models in the above two links is that the first one uses batchnorm-ReLu instead of ELu for the second to last fully connected layer, which is in the function "dense_elu_block". I appreciate any advice, help, and input. Thank you for your time. If I did anything wrong in my post let me know so I can make a better one next time.
