[site]: crossvalidated
[post_id]: 490934
[parent_id]: 490781
[tags]: 
Adding three points to the answer by @SextusEmpiricus: First, Doob's Theorem says that the posterior (under correct model specification) converges to the truth except on a set of parameters $\theta$ with prior probability zero. In a finite-dimensional setting you would typically have a prior that puts some mass everywhere, so that a set with prior probability zero also has Lebesgue measure zero. Second, finite-dimensional misspecified models will typically also have (frequentist) posterior convergence to a point mass, at the $\theta_0$ which minimises the Kullback-Leibler divergence to the data-generating model. The arguments for this are analogous to the arguments for convergence of misspecified MLEs to the 'least false' model, and can be done along the lines of @SextusEmpiricus's answer. Third, this is all much more complicated for infinite-dimensional parameters, partly because sets of prior probability 1 can be quite small in infinite-dimensional spaces. For any specified $\epsilon>0$ , a probability distribution places at least $1-\epsilon$ of its mass on some compact set $K_\epsilon$ . In, eg, Hilbert or Banach spaces a compact set can't contain any open ball. In infinite-dimensional problems: Doob's Theorem is still true, but it's less useful. Whether or not the posterior converges to a point depends on how big (flexible, overfitting,..) the model is It's quite possible for a correctly specified model to have a prior converging to the wrong point mass. In fact, Freedman gave a reasonable-looking problem for which this is typical. So prior choice is more tricky than it is in finite-dimensional problems.
