[site]: crossvalidated
[post_id]: 574775
[parent_id]: 574752
[tags]: 
In case you are not sure whether a variable is being treated as categorical, you can manually one-hot-encode (=dummy coding) the categories to make sure you are using the variable as categorical. Then, run this model and see whether that changes the results. If so, the variable was not being treated as categorical / as a factor. Another idea (though I suspect that's not it, because it should not exactly result in what you described) is that there could be penalization going on. E.g. for 0 vs. 1 logistic regression, scikit-learn surprisingly defaults to having L2 penalization (aka ridge regression).
