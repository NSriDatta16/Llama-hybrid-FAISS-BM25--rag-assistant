[site]: crossvalidated
[post_id]: 132532
[parent_id]: 131138
[tags]: 
I will do my best to answer this question not because I'm an expert on the topic (quite the opposite), but because I'm curious about the field and the topic, combined with an idea that it could be a good educational experience. Anyway, here's the result of my brief amateur research on the subject. TL;DR : I would consider the following passage from the research paper "The connection between regularization operators and support vector kernels" as the short answer to this question: Gaussian kernels tend to yield good performance under general smoothness assumptions and should be considered especially if no additional knowledge of the data is available. Now, a detailed answer (to the best of my understanding; for math details, please use references). As we know, principal component analysis (PCA) is a highly popular approach to dimensionality reduction , alone and for subsequent classification of data: http://www.visiondummy.com/2014/05/feature-extraction-using-pca . However, in situations, when data carries non-linear dependencies (in other words, linearly inseparable ), traditional PCA is not applicable (does not perform well). For those cases, other approaches can be used, and non-linear PCA is one of them. Approaches, where PCA is based on using kernel function is usually referred to, using an umbrella term "kernel PCA" ( kPCA ). Using Gaussian radial-basis function (RBF) kernel is probably the most popular variation. This approach is described in detail in multiple sources, but I very much like an excellent explanation by Sebastian Raschka in this blog post . However, while mentioning the possibility of using kernel functions, other than Gaussian RBF, the post focuses on the latter due to its popularity. This nice blog post , introducing kernel approximations and kernel trick , mentions one more possible reason for Gaussian kernel popularity for PCA: infinite dimensionality . Additional insights can be found in several answers on Quora. In particular, reading this excellent discussion reveals several points on potential reasons of Gaussian kernel's popularity, as follows. Gaussian kernels are universal : Gaussian kernels are universal kernels i.e. their use with appropriate regularization guarantees a globally optimal predictor which minimizes both the estimation and approximation errors of a classifier. Gaussian kernels are circular (which leads to the above-mentioned infinite dimensionality?) Gaussian kernels can represent "highly varying terrains" The following point, supporting the main conclusion above, is better delivered by citing the author: The Gaussian RBF kernel is very popular and makes a good default kernel especially in absence of expert knowledge about data and domain because it kind of subsumes polynomial and linear kernel as well. Linear Kernels and Polynomial Kernels are a special case of Gaussian RBF kernel. Gaussian RBF kernels are non-parametric model which essentially means that the complexity of the model is potentially infinite because the number of analytic functions are infinite. Gaussian kernels are optimal (on smoothness , read more here - same author): A Gaussian Kernel is just a band pass filter; it selects the most smooth solution. [...] A Gaussian Kernel works best when the infinite sum of high order derivatives converges fastest--and that happens for the smoothest solutions. Finally, additional points from this nice answer : Gaussian kernels support infinitely complex models Gaussian kernels are more flexible NOTES: The above-referenced point about Gaussian kernel being optimal choice, especially when there is no prior knowledge about the data, is supported by the following sentence from this CV answer : In the absence of expert knowledge, the Radial Basis Function kernel makes a good default kernel (once you have established it is a problem requiring a non-linear model). For those curious about non-essential differences between Gaussian RBF kernel and standard Gaussian kernel, this answer might be of interest: https://stats.stackexchange.com/a/79193/31372 . For those, interested in implementing kPCA for pleasure or business, this nice blog post might be helpful. It is written by one of the authors (creators?) of Accord.NET - a very interesting .NET open source framework for statistical analysis, machine learning, signal processing and much more.
