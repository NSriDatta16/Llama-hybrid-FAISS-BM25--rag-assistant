[site]: crossvalidated
[post_id]: 198324
[parent_id]: 198316
[tags]: 
Interesting question. I don't really have an answer, only an extended comment, but it might be enlightening. Whenever I have a question like this, I start simulating. (Uwe's Maxim: Computers are cheap, and thinking hurts.) Specifically, I wondered whether you simply might not have sufficiently many observations for asymptotics to kick in. So I simulated $N(0,1)$ time series of lengths 100, 1,000 and 10,000 (100 times each) and fitted ARIMA models to each, with the initial values you specified. Below, I plot the coefficients - each dot corresponds to one out of 100 time series - for the two initial conditions against each other. The first row corresponds to having 100 data points, the second to 1,000 and the third to 10,000 data points. We would expect the points to cluster close to the diagonal. Contrary to what I'd expect, increasing the sample size (moving down columns) does not yield point clouds that lie closer to the diagonal. Quite to the contrary, they actually turn asymmetric, and the values for the intercept move off the diagonal altogether. I don't know what's happening (and I'd like to learn more), but it seems like your experience is not atypical. Of course, one might ask why you are modeling financial time series that appear to have a mean of zero with an ARMA(2,3) model, anyway. For financials, one would usually look to GARCH or similar, since we are usually more interested in modeling volatility. R code for the plot below. It takes a few minutes to run on my machine. I had to put in a couple of try() s to deal with nonstationary estimated models. nn
