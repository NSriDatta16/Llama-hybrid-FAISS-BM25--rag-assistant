[site]: datascience
[post_id]: 43855
[parent_id]: 43840
[tags]: 
Unfortunately, a neural network is only able to compute probabilities on labels that it has been trained to recognize. In your model, you only have three identified labels and presumably trained on a data set that only includes the three classed. So your model is evaluating everything in terms of those three labels. If you feed in images of a car, it is going to give you the probability of the car being a cat, dog, or wolf and the probabilities will add up to 100%. There are several approaches to try to deal with this problem. Increase Training Examples As Nga Dao suggested, add another class others and add a bunch of images that are not part of the target classes with the label others . This is probably the easiest option but it may not produce much better results. One-vs-Rest Modeling Create a binary classifier for each class and take the class with the highest probability over a threshold. For example, when classifying a rabbit using the cat binary-classifier, presumably, the probability of a cat will be lower than not a cat . If all probabilities are below a threshold you feel is significant, then label the image as other. If you have two significant probabilities, which might happen when classifying a dog and you have dog and wolf as classes for example, take the class with the highest probability. You may need to optimize the threshold to get the best performance out of your classifier.
