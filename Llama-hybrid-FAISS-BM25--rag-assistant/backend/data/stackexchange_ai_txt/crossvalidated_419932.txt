[site]: crossvalidated
[post_id]: 419932
[parent_id]: 419931
[tags]: 
This post suggests that the computational challenges of standard dominance analysis (or "Shapley Regression") are well known as computational time grows exponentially with the number of predictors. Instead, a popular alternative is to perform "relative weights" analysis given that simulations suggest that this generally produces very similar results, but is computationally tractable with many more predictors. https://www.displayr.com/shapley-vs-relative-weights/ Shapley regression and Relative Weights are two methods for estimating the importance of predictor variables in linear regression. Studies have shown that the two, despite being constructed in very different ways, provide surprisingly similar scores((Gr√∂mping, U. (2015). Variable importance in regression models, WIREs Comput Stat 7, 137-152.))((Lebreton, J.M., Ployhart, R.E., & Ladd, R.T. (2004). A Monte Carlo Comparison of Relative Importance Methodologies. Organizational Research Methods 7, 258 - 282.)). As the computational expense of Shapley regression by far outweighs that of Relative weights, and if the two yield essentially the same results, then you can consider Relative Weights a strong alternative to Shapley. In this blog post, I explore the relationship between the two methods. This post provides an example of conducting a Relative Weights analysis in R . It uses the flipRegression package on github: https://github.com/Displayr/flipRegression Warning: The package seems to have a lot of dependencies. The relaimpo package also has the genizi and car estimates of relative importance. These are alternative methods for decomposing variance explained across predictor variables. They can be requested using the type argument. relaimpo:::calc.relimp(fit, type = c("car", "genizi")) I've done a quick comparison of few linear models with 10 or so predictors and results were fairly similar for car and genizi to those obtained using "lmg" (note lmg is the default computationally demanding approach). E.g., coefficients correlating r = .9992. And on a model with 25 highly correlated predictors "car" and "genizi" obtained relative importance estimates that correlated r = .994. That said, it'd be good to identify some more thorough simulation studies.
