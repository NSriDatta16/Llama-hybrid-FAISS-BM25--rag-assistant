[site]: datascience
[post_id]: 46798
[parent_id]: 
[tags]: 
SGD vs SGD in mini batches

So I recently finished a mini batches algorithm for a library in building in java(artificial neural network lib). I then followed to train my network for an XOR problem in mini batches size of 2 or 3, for both I got worse accuracy to what I got from making it 1(which is basically just SGD). Now I understand that I need to train it on more epochs but I'm not noticing any speed up in runtime which from what I read should happen. Why is this? Here is my code(Java) public void SGD(double[][] inputs,double[][] expected_outputs,int mini_batch_size,int epochs, boolean verbose){ //Set verbose setVerbose(verbose); //Create training set TrainingSet trainingSet = new TrainingSet(inputs,expected_outputs); //Loop through Epochs for(int i = 0; i
