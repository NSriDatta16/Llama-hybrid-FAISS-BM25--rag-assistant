[site]: crossvalidated
[post_id]: 427521
[parent_id]: 
[tags]: 
Batch Normalization or just z-normalization as a Nonlinearity

It is already common to do something "like"**(see asterisks below) z-standardization of the outputs of one neural network layer before passing it to the next. z-standardization would transform the columns of $H_{\ell}W_{\ell} + \beta_{\ell}$ (where $\ell$ denotes a layer and $H$ denotes a "hidden" matrix containing the values of hidden neurons, or the input data) to have 0 mean and unit standard deviation. **In reality, batchnorm is used, which incorporates learnable weights to the standardization function to help the model "undo" or "modify" the deterministic nature of z-scaling. (1) I observe that z-scoring is a nonlinear function in $W_{\ell}$ , because we must compute the sample standard deviation of $H_{\ell}W_{\ell} + \beta_{\ell}$ , which involves a square root. It follows that batch-norm will be nonlinear in the previous layer's weights as well. (2) Thus, if we do batch norm, we do not need to use a common activation function such as ReLU or tanh to prevent the whole "stack" (composition) of affine layers from collapsing into one affine layer. Meanwhile, the community generally believes batch norm is "good". Thus, why not just use batch norm between layers and free ourselves from choosing between ReLU, ELU, etc. etc.? ^^ that is the purpose of my question what follows is an observation of why my question might also be useful: (3) Then, we may observe all the questions online about whether batch norm should be used before or after the activation function. But couldn't we just use batch norm and avoid this question? Insights and/or references on this specific topic would be much appreciated! Thanks
