[site]: datascience
[post_id]: 107080
[parent_id]: 107076
[tags]: 
About the first piece of code you posted: At least from the apparent behavior, I would say your code computes the average of all subword vectors in a sentence, not for each word. To compute word-level representations, you should average only the subwords belonging to a specific word, not all subwords in the sentence. As a side note, I would suggest not to reuse variable names, as it makes the code confusing. In your code, you reuse i . About the second piece of code you posted: It seems to add up the subword embeddings of each word (only the last BERT layer) and concatenate each resulting vector into a tensor for the whole sentence (whose length would be the number of words).
