[site]: crossvalidated
[post_id]: 627781
[parent_id]: 
[tags]: 
Do ROC curves require probabilities?

In the binary case, the implementation of ROC curve in torchmetrics automatically applies a sigmoid when it detects logit inputs (i.e. when the values of scores are not within [0,1]). I have several problems with this: Nominally, ROC requires a 1D list of scores, which then gets sorted and thresholded. Applying any monotonic bijective function to the scores won't change the ROC. However, if our model scores are not probabilities, but pairs of classification weights (appropriate for CrossEntropyLoss ), then the classifier is essentially invariant under an overall bias [w0,w1]->[w0+b,w1+b] because Softmax of them depends only on w1-w0 . This means that I could add a random bias b to each of my predictions and still get the same loss and the same accuracy. Meanwhile, the ROC could drastically change because if I were to extract the values of w1 and sort them, their order could be totally different after adding the random bias. Why is this not a problem for regular implementations of ROC that take logits? To me it seems like the only valid way of doing it without applying Softmax is to apply the threshold to w1-w0 , not just one of the weights. Simply using sigmoid the way torchmetrics does it can't possibly fix this issue. In the multi-class case things get even hairier. ROC is known to be invariant under transformations that preserve the ranking of the scores. Softmax is one such transformation. So you would think that if the binary ROC works fine with pre-softmax logits, multi-class one-vs-rest ROC should also work well with raw scores. However, in practice I found that the ROC changes drastically after applying Softmax to a multi-class output. What gives? My current theory is that due to how neural networks are trained, binary classification weights typically end up being close opposites of each other, w1 ~ -w0 . This means that it actually is enough to look at only w1 to infer the result of the classification, and thresholding on it without Softmax is safe (only approximately though). Meanwhile, in the multi-class case there is no such simple relationship, and you really do need Softmax to get rid of the inconsistent bias across different outputs. Even though Softmax doesn't change the rankings of the scores within each entry, it factors out the translational symmetry inherent in CrossEntropyLoss , which is necessary before you apply a threshold. Is my intuition in (3) correct? Is there a source that explains these subtleties? Should I actually always be using Softmax even in the binary case? Edit: as an example of how softmax affects the ROC (this is a binary classification case with 100% accuracy): import numpy as np from scipy.special import softmax, expit from sklearn.metrics import roc_auc_score scores=np.array([[0.5,1.0], [5.5,1.5]]) labels=[1,0] # Verify accuracy print("Acc =",np.mean((np.argmax(scores,axis=1)==labels))) # AUC using raw scores print("AUC =",roc_auc_score(labels, scores[:,1])) # AUC after applying a sigmoid (doesn't change ROC) print("AUC =",roc_auc_score(labels, expit(scores)[:,1])) # AUC after softmax (changes ranking of scores across entries) print("AUC =",roc_auc_score(labels, softmax(scores, axis=1)[:,1])) output: Acc = 1.0 AUC = 0.0 AUC = 0.0 AUC = 1.0
