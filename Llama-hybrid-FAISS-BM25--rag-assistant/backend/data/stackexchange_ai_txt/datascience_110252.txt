[site]: datascience
[post_id]: 110252
[parent_id]: 110180
[tags]: 
I understand that what you mean is the following: Not using positional encodings. Append some special tokens to the input sequence to represent the positions of the normal tokens within the sequence. For instance, the sequence ["my", "dog"] would be transformed into ["my", "dog", , ] . This would imply that the position of "my" is 0 and the position of p "dog"p is 1. This association would be given by the position of the special tokens and within the sequence. However, in a Transformer without the positional encodings, the model does not have any information about the token positions (which is precisely what gives meaning to our special tokens) and, therefore, it would not be able to establish the association between the positions of the special tokens and and their actual positions. This way, for the Transformer without positional encodings, the input ["my", "dog", , ] and the input ["1", "my", "dog", ] would be exactly the same.
