[site]: crossvalidated
[post_id]: 469662
[parent_id]: 469657
[tags]: 
In the case of time series forecasting, first of all, you need to understand that stationarity is important mostly in the context of ARMA and related models (AR: Auto-Regressive, MA: Moving Average). There are other types of time series forecasting models where stationarity is not a requirement, such as Holt-Winters or Facebook Prophet. Here are two intuitive, if not entirely mathematically rigorous, explanations of why mean stationarity is important in the ARMA case: The AR component of ARMA models, treats time series modeling as a supervised learning problem, $Y_t = a_1Y_{t-1}+...a_nY_{t-n}+c+\sigma(t)$ . A common rule of thumb in supervised learning is that the distribution of the training data and the distribution of the test data should be the same, otherwise your model will perform poorly on out-of-sample tests and on production data. Since for time series data, you train set is the past, and your test set is the future, the stationarity requirement is simply ensuring that the distribution stays the same over time. This way you avoid the problems that come with training your model on data that has a different distribution than the test/production distribution. And mean stationarity in particular is just saying that the mean of the train set and the mean of the test should stay the same. An even simpler consideration: take the most basic ARMA model possible, an $AR(1)$ model: $$Y_t = aY_{t-1}+c+ \sigma$$ so the recursive relationship for estimating on step based on the previous one is: $$\hat{Y}_t = a\hat{Y}_{t-1}+c$$ , $$\hat{Y}_t - c = a\hat{Y}_{t-1}$$ taking the expected value: $$E(\hat{Y}_t) - c = aE(\hat{Y}_{t-1})$$ meaning that: $$a = \frac{E(\hat{Y}_t) - c}{E(\hat{Y}_{t-1})}$$ so if we want $a$ to stay constant over time, which is the starting assumption of an $AR(1)$ model since we want it to be similar to a linear regression, then $E(\hat{Y}_t)$ has to stay the same for all $t$ , i.e. you series has to be mean stationary. The above considerations are applicable as well to the general ARMA case, with $AR(p)$ and $MA(q)$ terms, although the math is somewhat more complicated than what I describe, but intuitively, the idea is still the same. The 'I' in ARIMA stands for "Integrated" which refers to the differencing process that allows one to transform a more general time series into one that is stationary and can be modeled using ARMA processes. I disagree with @Alexis characterization that " that time series are stationary is more or less embodying the worldview that the past does not matter " - if anything it is the other way around: Transforming a time series into a stationary one for modeling purposes is exactly about seeing whether there are any causal/deterministic structures in the time series beyond just trend and seasonality . I.e. does the past impact the present or the future in ways more subtle ways than just the large scale variations? (But I might simply misinterpreting what she is trying to say).
