[site]: datascience
[post_id]: 87372
[parent_id]: 87361
[tags]: 
When you are fitting a supervised learning ML model (such as linear regression) you need to feed it both the features and labels for training. The features are your X_train , and the labels are your y_train . In your case: from sklearn.linear_model import LinearRegression LinReg = LinearRegression() LinReg.fit(X_train, y_train) If you are performing some unsupervised learning approach, or simply some data transformation (such as PolynomialFeatures) you simply fit on your feature space ( X_train ) since there are no labels required for such an approach. Like so: from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(2) poly.fit(X_train) X_train_transformed = poly.transform(X_train) For your second point - depending on your approach you might need to transform your X_train or your y_train . It's entirely dependent on what you're trying to do. As for your last point - never ever fit on testing data. It defeats the purpose of a train/test split. Usually what is done is your pipeline step is fit either with X_train and y_train or just X_train alone. This fit transformer can then be applied to your testing data ( X_test ) using the .transform() method but never use this data for .fit()
