[site]: crossvalidated
[post_id]: 196690
[parent_id]: 196681
[tags]: 
1 and 2 both describe techniques that would be valid in various circumstances. In (1), you're basically using a partially stacked model (holding out the "new data", by which I assume you mean additional variables not considered by the initial SVM model). In (2), you would be applying the SVM model to the entire dataset including the new variables. It's very difficult to say with no other contextual knowledge which of these approaches would produce "better" results for your model- it would, as always, be best to try both approaches (with proper cross-validation) to determine the "better" approach in terms of your goals. Assuming you already have the support vectors for the initial training of the model, (1) will be much cheaper (proportionally so to the number of independent variables in the initial model) in terms of memory than (2), as the number of dimensions (variables) passed to the algorithm is comparatively lower in (1).
