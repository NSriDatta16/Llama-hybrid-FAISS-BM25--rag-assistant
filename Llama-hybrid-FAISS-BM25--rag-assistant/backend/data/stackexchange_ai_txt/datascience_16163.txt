[site]: datascience
[post_id]: 16163
[parent_id]: 
[tags]: 
XGboost classification with very small data set

I have a general question regarding XGboost and especially the n_rounds parameter, regarding small datasets. Normally I tune the n_rounds parameters by cross-validation, but what if you have too less observations to do proper CV? For example if I have 30 variables and 4000 observations in my training data, how can I find a nice value for n_round which is not over/underfitting the training data? Are there any "best practices" for parameter tuning (also max_depth etc.) having small datasets?
