[site]: datascience
[post_id]: 12126
[parent_id]: 
[tags]: 
Looking for a rough explanation of additive hidden nodes and radial basis functions

I'm working on a neural networks project right now and for that I'm reading a bunch of scientific papers, in a few of those the terms additive hidden nodes and radial basis functions are thrown around, but I seem to have trouble to get a clear explanation of the terms there and anywhere else on the internet. I seem to have gathered that these are classifications for neuron types but I would love to get a more clear intuitive explanation of the terms. Preferably one that doesn't require me to be very mathy to understand. I'm fairly new to neural networks so beyond sigmoid neurons and backpropagation algorithms I'm still a bit lost when it comes to the common termonology.
