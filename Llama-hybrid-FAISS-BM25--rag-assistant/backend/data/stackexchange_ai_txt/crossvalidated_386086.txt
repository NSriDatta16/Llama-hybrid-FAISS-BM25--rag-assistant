[site]: crossvalidated
[post_id]: 386086
[parent_id]: 
[tags]: 
Using gradient descent to train dual formulation of Kernel SVM

I've seen other posts about using gradient descent for the primal form, but not the dual form. In this book, the author discusses using (projected) gradient descent for the dual form: http://ciml.info/ (Chapter 11, Kernel Methods) The problem is, it is unclear how to find the bias term, as it does not appear at all in the dual objective. The objective is: $L(\alpha)=\alpha^T 1 - \frac{1}{2}\alpha^TG\alpha $ Where: $ G_{nm} = y_n y_m K(x_n, x_m) $ The constraint on $\alpha$ is: $ C \ge \alpha_n \ge 0$ So, how can one find the bias term using this method?
