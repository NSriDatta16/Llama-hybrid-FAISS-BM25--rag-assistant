[site]: datascience
[post_id]: 52310
[parent_id]: 21734
[tags]: 
The output width and height of the output dimensions of the VGGnet are a fixed portion of the input width and height because the only layers that change those dimensions are the pooling layers. The number of channels in the output is fixed to the number of filters in the last convolutional layer. The flatten layer will reshape this to get one dimension with the shape: ((input_width * x) * (input_height * x) * channels) where x is some decimal The main point is that the shape of the input to the Dense layers is dependent on width and height of the input to the entire model. The shape input to the dense layer cannot change as this would mean adding or removing nodes from the neural network. One way to avoid this is to use a global pooling layer rather than a flatten layer (usually GlobalAveragePooling2D) this will find the average per channel causing the shape of the input to the Dense layers to just be (channels,) which is not dependant on the input shape to the whole model. Once this is done none of layers in the network are dependent on the width and height of the input so the input layer can be changed with something like input_layer = InputLayer(input_shape=(480, 720, 3), name="input_1") model.layers[0] = input_layer
