[site]: datascience
[post_id]: 86639
[parent_id]: 
[tags]: 
LSTM with return_sequences - "Training a model on multiple timesteps simultaneously"

So I'm following Tensorflow's LSTM/time series tutorial and there's something I don't understand. I do understand what happens with return_sequences on/off, however, it is stated, that with return_sequences on you allow Training a model on multiple timesteps simultaneously I don't quite understand what this means. If you keep it off, the input will go through all the previous timesteps, and make a prediction. Your loss will be the error of this prediction vs. the actual. You're still training on all the timesteps, right (although not simultaneously I guess?). Now if it's on, it's making a prediction on each timestep - what's the loss in this case? Is it the average error for each prediction in the sequence? Why is it better to switch return_sequences on, as it's done in the tutorial? Does the model learn more or faster in this case? How does it impact the learning? It is also stated that With return_sequences=True the model can be trained on 24h of data at a time. Isn't that still the case with return_sequences=False ? In the example they're still teaching the model to predict 1 timestep based on the previous 24.
