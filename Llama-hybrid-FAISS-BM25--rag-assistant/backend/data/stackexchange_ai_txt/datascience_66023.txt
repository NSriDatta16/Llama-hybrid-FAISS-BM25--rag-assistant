[site]: datascience
[post_id]: 66023
[parent_id]: 66016
[tags]: 
Three ideas come to my mind (from simple to complex) Include an additional category for anything which is not a number and train your network on these $k+1$ categories. Apply another predictor in the first place which has been trained to differentiate between "number" and "no number". Iff the input is classified as a number you then run your number recognition network. (this approach might make transfer learning easier, i.e. apply an existing model for the first step) Merge the two tasks into one network and make this a multi-task classification, i.e. your network includes layers not only to recognize numbers but also for the binary classification "number vs no number". Since these tasks are closely related the two tasks might benefit from sharing parameters (i.e. use the same features). The paper An Overview of Multi-Task Learning in Deep Neural Networks describes this approach in more detail. (Note that this approach is not identical with the first idea in this list since this one applies two separate classifications while the first one only does a single classification) However, as a disclaimer: I have not tried out the 3rd idea myself but used the second one for a similar problem.
