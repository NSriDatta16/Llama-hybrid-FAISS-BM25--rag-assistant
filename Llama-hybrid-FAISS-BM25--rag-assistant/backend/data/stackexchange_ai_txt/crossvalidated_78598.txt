[site]: crossvalidated
[post_id]: 78598
[parent_id]: 78469
[tags]: 
So what I can gather from your response you have a unbalanced data set with 100 1's and 900 0's. Since random forests are built on Classification and Regression Trees (CART) they are highly sensitive to unbalanced data. Let me explain a bit more, CARTs work by means of a divide a conquer approach. This means that they partition your feature space into subsets where your target variable behaves more homogeneously. For example, if you are looking at income as a target, maybe partitioning the population into people under 16 years old and people of 17 and over helps you obtain two subsets where income behaves in a more constant fashion. You can do this more thoroughly and with more variables until you have quite small subsets where a simple average helps you explain the behavior of the subset. In the classification scheme you replace an average with a majority vote, meaning if you introduce a new observation into the tree you just trained, it will follow certain rules (e.g. is older than 17 years old, is a female, is living in Milan) until it ends up in a certain subset with, for example, 6 observations labeled "high income" and 1 observation labeled "low income", 6 > 1; so you label it "high income" . If you have 100 1's and 900 0's then it is quite possible that every single subset created has more 0's than 1's so you will always classify things as 0. There are many techniques to tackle unbalanced data sets, luckily for you random forest has some quite straight forward approaches to do this. Note that a random forest is an ensemble of orthogonal trees, as en ensemble EACH tree will cast a vote and the majority of that will win (a majority vote of majority votes). Now, each tree is built on bootstrapped sample of your whole data. You may force the bootstrapped samples to be balanced themselves. To do this in R you would simply choose the sampsize in the random forest implementation accordingly: binaryclassifier These are based on out of bag samples. When you construct each tree you take a bootstrap sample which in turn means you left some of the observations out of that trees construction. These are called "out of bag" samples and can be used to test the model on the fly. You can also play around a bit with the sample size, a perfectly balanced data sample is not always what works best. Another thing that can work for you but is a bit more advanced is to use cross validation to find an optimal threshold for classifying new observations into 1's or 0's. Elaborating a bit you have a ratio of 0.1 1's on your whole data set. You could train a random forest as you have been doing now where all the predictions give you 0. But instead of taking the default random forest prediction (majority vote) you could obtain the probabilities: predictions And manually set all the observations which have a ratio of 1's to 0's larger than 0.1 to be labeled as 1. This 0.1 is just a guess, as I mentioned, you could find an optimal threshold through cross validation. Good luck!
