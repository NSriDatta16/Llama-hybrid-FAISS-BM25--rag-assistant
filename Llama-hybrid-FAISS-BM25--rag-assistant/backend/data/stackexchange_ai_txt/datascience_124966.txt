[site]: datascience
[post_id]: 124966
[parent_id]: 
[tags]: 
My machine learning model cannot classify data that it has never seen before

I am creating a spam identifier to detect wheter an email is malicous. The issue I have is my model using the RandomForest Classifier showed it was 99% accurate. csv: https://www.kaggle.com/datasets/mandygu/lingspam-dataset/code However when I enter in an email which is obviously spam: like Subject: give me money now and credit card Email: Send me your credit card number. The model would say that it is not spam.(same issue with Naive Bayes) This is disheartning as I would hope it would classify things that were obviously spam; however if i copy and paste answers from the csv file it would work. Model: X = df.sub_mssg Y = df.label X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.12, random_state=0, stratify=Y) tvec = TfidfVectorizer() mnb = MultinomialNB() model_5 = Pipeline([('vectorizer', tvec), ('classifier', mnb)]) model_5.fit(X_train, Y_train) y_pred = model_5.predict(X_test) print(confusion_matrix(Y_test, y_pred)) print("Accuracy:", accuracy_score(Y_test, y_pred)) print("Precision:", precision_score(Y_test, y_pred, average='weighted')) print("Recall:", recall_score(Y_test, y_pred, average='weighted')) code for saving model: import joblib joblib.dump(model_5, 'spam_model.joblib') Moreover the way i am making my test case is that I am adding the prompt data into a csv the same way I trained it and same way I cleanesd it, code: import csv import pandas as pd import joblib import numpy as np import re import nltk from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer from nltk.tokenize import word_tokenize # Function to decontract contractions def decontract(phrase): contractions = { "won't": "will not", "can't": "can not", "n't": " not", "'re": " are", "'s": " is", "'d": " would", "'ll": " will", "'t": " not", "'ve": " have", "'m": " am" } # Replace contractions with their expanded forms for contraction, expansion in contractions.items(): phrase = re.sub(contraction, expansion, phrase) return phrase # Download NLTK resources nltk.download('punkt') nltk.download('stopwords') subject = input('Please enter the subject of the message: ') message = input('Please enter the contents of the message: ') # Writing to CSV with label with open('data.csv', 'w', newline='') as file: writer = csv.writer(file) field = ["subject", "message", "label"] writer.writerow(field) writer.writerow([subject, message, 'your_label']) # Add a label here # Reading CSV oldf = pd.read_csv("data.csv") oldf['message'] = oldf['message'].str.lower() oldf.drop_duplicates(inplace=True) oldf.isnull().sum() # Preprocessing df = oldf.dropna() df['message'] = df['message'].str.lower() df['sub_mssg'] = df['subject'] + df['message'] # Decontract, remove stopwords, and lemmatize stopwords_set = set(stopwords.words('english')) lemmatizer = WordNetLemmatizer() df['sub_mssg'] = df['sub_mssg'].apply(decontract) df['sub_mssg'] = df['sub_mssg'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_set])) df['sub_mssg'] = df['sub_mssg'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])) # Remove non-ASCII characters, consecutive duplicates, and repeated characters df['sub_mssg'] = df['sub_mssg'].apply(lambda x: ''.join([char if ord(char) 2])) # Remove hyphens df['sub_mssg'] = df['sub_mssg'].str.replace(r'-', ' ') X_test = df['sub_mssg'] # Load the trained model loaded_model = joblib.load('spam_model.joblib') # Make predictions using the loaded model Y_pred = loaded_model.predict(X_test) # Display the predictions if Y_pred == 0: print("this is not spam") if Y_pred == 1: print("this is spam")
