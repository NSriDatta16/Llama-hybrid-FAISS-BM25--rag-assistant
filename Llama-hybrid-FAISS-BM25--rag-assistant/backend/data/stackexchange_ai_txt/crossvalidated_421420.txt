[site]: crossvalidated
[post_id]: 421420
[parent_id]: 418195
[tags]: 
Yes, what you describe is a bad idea because it biases the p-values strongly. As always, there's an xkcd for it , or at least an extreme version of it. However, it does happen and it was a factor in many big cases within the replication crisis in social science, including the work of Brian Wansink and Amy Cuddy. As such, it's also been a motivating factor in the effort to make pre-registration a standard practice in science. The topic itself has been discussed a tremendous amount in recent years, and you can find more through googling 'p-hacking', 'fishing for significance', or Andrew Gelman's favoured terms 'researcher degrees of freedom' and ' garden of forking paths ' (these encompass a great deal more than the behaviour you describe, though). Note that this does not mean that there's no place for the kinds of exploration you are describing! Exploratory analysis is important, but biases creep in when such exploration is conflated with hypothesis-testing. Both can be achieved in a valid way, however. Split your dataset beforehand and do your exploration/hypothesis generation on one subset of the data. Once you have developed & refined this into specific hypotheses, you can test them using a separate data subset (or subsets). Or equivalently, collect a new dataset to test the hypotheses you've generated through exploratory analysis of an existing dataset. Of course, another option is to ditch p-values entirely. Bayesian statistics has much to recommend it, and one advantage is that it's less susceptible to this kind of 'cheating' (loosely speaking).
