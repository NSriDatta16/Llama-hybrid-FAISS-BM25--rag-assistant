[site]: crossvalidated
[post_id]: 451771
[parent_id]: 451765
[tags]: 
A single designated test set is very common in the machine learning literature (see e.g. all these leaderboards on paperswithcode.com - I would guess they would [nearly?] all be a single test set). That's also what is done in kaggle competitions. There are some arguments for and against this. E.g. if you did cross-validation you should ensure that every method uses the same CV scheme (not easy across multiple papers) and that no information from one CV-fold is used in another to drive modeling decision for another fold. That is all a bit more complicated than when you simply have a single test set. You make people's lives easier by "allowing" them to do anything they want to do with the training data to drive just about any modeling decision. In fact, at my company we have occasionally really ring-fenced a test set so that a modeling team can look at data any way they want, but in the end we check performance on data nobody ever looked at. Additionally, if the training and test set are both huge, then for method comparison the extra precision you get in your comparison from doing CV (or perhaps even better repeated CV) do not matter. Since you trained on the same data and evaluate on the same data, you do not really need to use rank tests on summaries of the results, but can rather directly compare metrics (e.g. accuracy as a binary outcome) - again, if the data are only large enough statistical tests may be less and less relevant (i.e. when differences leading to a rejection of some null hypothesis are so tiny that they are in practice irrelevant - then you can rather only highlight differences that have some real relevance). For Kaggle competitions it is hard to see what else you could do, if you wish to avoid people knowing the test results (otherwise people can cheat and exploit this knowledge - as has been alleged to have happened in a recent competition , where someone is supposed to have reconstructed the test data). Problems with having a single test set are probably worst when we talk about small datasets, e.g. you can more easily overfit the test set, the training-test set is rather random and can easily favor one approach over another by chance and so on. Additionally, the evaluation is not very precise/efficient and there is of course a reason why people prefer CV or bootstrapping for hyper-parameter choice, model selection and other similar choices (and the existence of a test set keeps you "honest", because you know that not doing this properly will hurt you on the test set).
