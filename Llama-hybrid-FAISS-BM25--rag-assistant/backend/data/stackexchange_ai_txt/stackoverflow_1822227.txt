[site]: stackoverflow
[post_id]: 1822227
[parent_id]: 1822114
[tags]: 
You may have a look at std::tr1::unorderd_map. But if you have 32 bit unsigned integer keys (4294967296 possible values) and 130 million different keys, then you should write your own container optimized for this task. Especially if the 130 million key case is the usual case (and not only a rare maximum). 4294967296 / 130000000 = 33, so about each 33rd number in the whole space is used in your data. You could for example divide your key range into fixed size partitions. If the keys are rather evenly distributed, you should partition the key space into e.g. 256-sized buckets, or even 32-sized buckets, depends on how much storage you want to waste when only a few values are stored. Example, to give you an idea: #define BUCKET_SIZE 256 #define BUCKET_SIZE_SHIFT 8 struct Bucket { uint32_t key; float value; Bucket* pNext; }; Bucket data[ 4294967296 / BUCKET_SIZE ]; Bucket* find( uint32_t key ) { uint32_t bucket_index = key / BUCKET_SIZE; // or faster: uint32_t bucket_index = key >> BUCKET_SIZE_SHIFT; Bucket* pBucket = &data[ bucket_index ]; while( pBucket ) { if( pBucket->key == key ) return pBucket; pBucket = pBucket->pNext; } return NULL; }
