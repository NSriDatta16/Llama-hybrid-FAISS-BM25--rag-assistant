[site]: datascience
[post_id]: 12324
[parent_id]: 9869
[tags]: 
Kernel based methods work by constructing a feature vector based on some distance metric of an input vector to one or more example vectors, which are determined during training.These are the prototypes in your case (also known as regressors in some contexts). They are essentially vectors in the space of the independent variables. Radial Basis functions essentially "convert" the (Euclidean) distance between input vector and prototype to a similarity value, usually [0,1]. Some of them, such as the Gaussian, have parameters such as the bandwidth. In RBF Networks, there are a number of different options of determining the prototypes, for instance by Clustering at the space of independent variables, or by Orthogonal Least Squares (references at the bottom). Like you correctly assumed, the first sentence is explaining that in many cases of training algorithms (the ones mentioned above included), the bandwidth of the RBF is constant and determined a-priori What is subject to determining is the actual location of the prototypes. In this context, the prototype would refer to the value composition in the input space, but not to the radius (which is fixed anyway). I have not read the paper in question, but it seems that the authors are proposing some method to individually adjust the bandwidths of the RBFs. In this case, the radius may have been included as well into the notion of the prototype, but it is specific to the research in question. References J. Moody and C. J. Darken, “Fast Learning in Networks of Locally-Tuned Processing Units,” Neural Comput., vol. 1, no. 2, pp. 281–294, Jun. 1989. S. Chen, C. F. N. Cowan, and P. M. Grant, “Orthogonal least squares learning algorithm for radial basis function networks,” IEEE Trans. Neural Networks, vol. 2, no. 2, pp. 302–309, Mar. 1991.
