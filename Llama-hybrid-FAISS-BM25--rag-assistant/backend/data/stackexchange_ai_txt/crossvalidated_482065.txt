[site]: crossvalidated
[post_id]: 482065
[parent_id]: 
[tags]: 
Feeding Word Embeddings into Recurrent Neural Networks

I'm trying to understand how Recurrent Neural Networks use word embedding vectors as their inputs, and I've created the illustration below to reflect my current-state understanding. I understand that word embedding is its own learning process to reduce the dimensionality of the relationship between words in a corpus, and that the number of dimensions returned is user-defined (similar to PCA). But am I correct in understanding that once the resulting vectors are created, that each word's "embedding vector" in each statement observation is then fed into its own x t input of an RNN , when performing something like sentiment analysis? The application of the highlighted yellow boxes below is what I'm trying to confirm actuall happens.
