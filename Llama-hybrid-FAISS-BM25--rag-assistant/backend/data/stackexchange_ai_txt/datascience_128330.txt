[site]: datascience
[post_id]: 128330
[parent_id]: 128328
[tags]: 
Normal dropout does not remove whole tokens, but individual values within the vectors. Therefore, dropout does not remove 10% of the tokens in a sequence, but 10% of the values. There is a different type of dropout called "token dropout" that works at token level and therefore works as you described. Such type of dropout, however, is not the one described in the paragraph you highlighted. Update: Given that a user showed doubts about my answer, here is a piece of code to show the effects of nn.Dropout after an embedding layer. Feel free to run it in Google colab to check its effect by yourselves: import torch import torch.nn as nn # Set seed for reproducibility torch.manual_seed(42) # Define parameters vocab_size = 10 embedding_dim = 5 dropout_prob = 0.5 # Dropout probability # Create embedding layer embedding = nn.Embedding(vocab_size, embedding_dim) # Create dropout layer dropout = nn.Dropout(p=dropout_prob) # Input tensor input_tensor = torch.LongTensor([[1, 2, 3]]) # Apply embedding layer embedded_tensor = embedding(input_tensor) # Apply dropout layer dropout_tensor = dropout(embedded_tensor) # Print original and dropout tensor print("Original Tensor:") print(embedded_tensor) print("\nTensor after Dropout:") print(dropout_tensor) Here is the output (take into account that, apart from zeroing some positions, the outputs are scaled by a factor of $\frac{1}{1-p}$ ): Original Tensor: tensor([[[-1.2345, -0.0431, -1.6047, -0.7521, 1.6487], [-0.3925, -1.4036, -0.7279, -0.5594, -0.7688], [ 0.7624, 1.6423, -0.1596, -0.4974, 0.4396]]], grad_fn= ) Tensor after Dropout: tensor([[[-0.0000, -0.0861, -0.0000, -0.0000, 0.0000], [-0.7850, -0.0000, -0.0000, -0.0000, -0.0000], [ 0.0000, 3.2846, -0.3192, -0.9948, 0.0000]]], grad_fn= )
