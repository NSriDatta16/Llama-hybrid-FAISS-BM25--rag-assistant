[site]: crossvalidated
[post_id]: 493347
[parent_id]: 493346
[tags]: 
The vanishing gradient problem: [ 1 ] As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train. The reason that recurrent neural networks (RNN) suffer from the vanishing gradient problem is due to the nature of backpropagation through time. [ 2 ] An unrolled RNN tends to be a very deep network. In an unrolled RNN the gradient in an early layer is a product that (also) contains many instances of the same term. Long short-term memory (LSTM) neural networks solve the vanishing gradient problem by allowing network cells to forget part of their previously stored memory. For a concise mathematical summary, see How does LSTM prevent the vanishing gradient problem? . See also what is vanishing gradient? Why do RNNs have a tendency to suffer from vanishing/exploding gradient? The Vanishing Gradient Problem The Problem, Its Causes, Its Significance, and Its Solutions Vanishing gradient problem [1] Wang, Chi-Feng. "The Vanishing Gradient Problem: The Problem, Its Causes, Its Significance, and Its Solutions." towards data science. Available from: https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484 . [2] Milman, Oren . Answer to question: "Why do RNNs have a tendency to suffer from vanishing/exploding gradient?." Cross Validated (a Stack Exchange site). Available from: https://stats.stackexchange.com/a/370732/295223 .
