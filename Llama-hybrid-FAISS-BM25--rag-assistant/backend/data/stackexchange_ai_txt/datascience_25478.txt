[site]: datascience
[post_id]: 25478
[parent_id]: 11678
[tags]: 
Spark will automatically un-persist/clean the RDD or Dataframe if the RDD is not used any longer. To check if a RDD is cached, please check into the Spark UI and check the Storage tab and look into the Memory details. From the terminal, you can use rdd.unpersist() or sqlContext.uncacheTable("sparktable") to remove the RDD or tables from Memory. Spark made for Lazy Evaluation, unless and until you say any action, it does not load or process any data into the RDD or DataFrame.
