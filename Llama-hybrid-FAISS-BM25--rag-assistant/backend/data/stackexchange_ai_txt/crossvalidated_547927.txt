[site]: crossvalidated
[post_id]: 547927
[parent_id]: 547923
[tags]: 
There isn't an answer to your question. It is true that there are circumstances where a Bayesian solution is intrinsically preferable to a Frequentist solution. The reverse is also true. The main benefit of a Bayesian model is that it updates and improves your beliefs about the world. Other than that, the two systems are not comparable. They solve different questions. A posterior distribution, if you really are using your real priors, should become your new prior. It becomes your default understanding of the world with respect to the parameters and future data. If you are using it in analysis for a third party, then it should be specified by their prior distributions and not yours. You are not updating your beliefs. All three main methods of constructing estimators, the method of maximum likelihood, Frequency-based estimators such as the minimum variance unbiased estimator, and Bayesian estimators are optimal estimators. They are optimal under different criteria. If you woke up one morning and assuming that one or more categories of estimation were not foreclosed by the nature of the problem, and needed a point estimate, then the solution is to answer what you mean when you say a point is optimal. You should answer a variety of questions. Who needs the point? Why do they need the point? What happens to that third party if you choose the wrong point? Does it actually need to be a point? Could an interval or a distribution do as good or a better job? I think there is another difference that you might be missing. A Frequentist interval or point is working in the sample space. The distributions that are implicit or explicit in the process, such as the sampling distribution of Student's t statistic, are not distributions of beliefs. For statistics, they are the long-run distribution that you would expect to see while collecting samples over the sample space. They represent possible, but maybe forever unrealized, outcomes that could happen. The Bayesian prior and posterior distributions are distributions of beliefs about parameters. They are not distributions that can happen. They happen only in the mind. Change your priors and you change your posterior. Even the Bayesian predictive distribution, which also intrinsically minimizes the K-L divergence between the prediction and nature, can never happen. It is just the weighted sum of the possible distributions that could happen over the posterior or prior. The posterior is the Bayesian conclusion. Getting a point requires adding additional criteria that are then imposed on the posterior, prior, or predictive distributions. There are many good reasons to use a Bayesian solution. In some cases, it is the only permissible solution. The very same thing can be said about non-Bayesian tools too. If you look at the opportunity costs of your model, what school of estimation should you use?
