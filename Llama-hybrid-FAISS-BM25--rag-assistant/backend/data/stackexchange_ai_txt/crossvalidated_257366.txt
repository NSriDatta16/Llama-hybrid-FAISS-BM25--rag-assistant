[site]: crossvalidated
[post_id]: 257366
[parent_id]: 
[tags]: 
Where does problems analytically arise in logistic regression on singular data matrix?

If you do linear regression without regularization on close-to-singular data matrix $X$ (or it does not have enough data), the problem arises even in closed-form solution $w = (X^TX)^{-1}X^Ty$ when you do $(X^TX)^{-1}$. If you do logistic regression, your gradient would look like: $\frac{\partial L(w)}{\partial w} = X(y-\text{logit}(w^TX))$ and I don't see any particular reason why it should diverge if $X$ has not enough data or data is perfectly separable. I would expect gradient to skyrocket when problem is ill-posed, but that is not obviously follows from gradient. How one can see from analytical derivations that logistic regression would experience serious troubles if we don't have enough data? (same way as be observe inverse of singular matrix in linear regression)
