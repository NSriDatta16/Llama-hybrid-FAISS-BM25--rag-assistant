[site]: datascience
[post_id]: 32019
[parent_id]: 32009
[tags]: 
Let us formulate this problem in such a way that it can be understood from a machine learning perspective. You have a set of instances $X$ where each instance $x_i \in \mathbb{R}^m$ where $m$ is the dimensionality of the instance. In other words $m$ is the number of features that describe the instance. Your problem intends to go from a set of features to a class label good or bad . Thus, this is a mapping from $\mathbb{R}^m$ to $y \in \{0, 1\}$. How to achieve this mapping? This is when we will use the machine learning algorithms. We will train a model to effectively approximate the function which gives the output label from a set of inputs. It is evident that sparse features (low information entropy) will complicate the mapping function and will thus provide worse results. This is why feature engineering is of upmost importance for machine learning. It is probably the hardest part of the machine learning pipeline, however it is the lead factor in dictating your results. You can use some feature reduction techniques in order to remove features which are uninformative with respect to the output label. Some techniques that I use frequently are principle component analysis (PCA), linear discriminant analysis (LDA). Alternatively, you can use some projection methods to reduce the dimensionality of the data whilst maintaining separation between the classes. Such techniques are Isomap, MDS, Spectral Embeddings and TSNE. You can check to see which is best suited for your type of data. How to choose a model? Firstly, your problem is a supervised classification problem. This already narrows the types of models you can use. Furthermore, model selection is based on some key factors such as: the number of instances you have, the number of features per instance and the number of output nodes. You should also keep in consideration that the separability of the probability distribution between the output classes will impact the performance of the model directly. For example discriminating between cars and oranges is much easier than oranges and clementines. In your case, you have 1,000 instances and around 13 features. This means that deep learning based techniques are possible but discouraged. You do not have enough data. You can then attempt the following popular classification models Support Vector Classifier Naive Bayes K-Nearest Neighbors Decision Trees Random Forests To evaluate which model performs the best you will use the accuracy attained with a trained model on a test set. This set should be drawn independently from the training set as to catch overfitting. This is when the model cannot generalize to new data. In code Assuming matrix $X$ contains the data where rows are the instances and columns are the features, and matrix $Y$ contains the labels. First we split our data into a training and testing set X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33) from sklearn.svm import SVC clf = SVC() clf.fit(X_train, y_train) print('Score: ', clf.score(X_test, y_test)) from sklearn.neighbors import KNeighborsClassifier neigh = KNeighborsClassifier(n_neighbors=3) neigh.fit(X_train, y_train) print('Score: ', neigh.score(X_test, y_test)) from sklearn import tree clf = tree.DecisionTreeClassifier() clf.fit(X_train, y_train) print('Score: ', clf.score(X_test, y_test)) from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators = 100) forest.fit(X_train, y_train) print('Score: ', forest.score(X_test, y_test)) This should be a starting point. Let us know if you fall into any problems, and let us know what accuracy you are getting we can then look deeper into these models and better suit them to your data source.
