[site]: crossvalidated
[post_id]: 472377
[parent_id]: 27300
[tags]: 
I skim read through the comments above and I believe quite a few have pointed out that PCA is not a good approach to feature selection. PCA offers dimensionality reduction but it is often misconceived with feature selection (as both tend to reduce the feature space in a sense). I would like to point out the key differences (absolutely open to opinions on this) that I feel between the two: PCA is a actually a way of transforming your coordinate system to capture the variation in your data. This does not mean that the data is in any way more important than the other ones. It may be true in some cases while it may have no significance in some. PCA will only be relevant in the cases where the features having the most variation will actually be the ones most important to your problem statement and this must be known beforehand. You do normalize the data which tries to reduce this problem but PCA still is not a good method to be using for feature selection. I would list down some of the features that scikit-learn uses for feature selection to just give some direction: Remove highly correlated features (Using Pearson's correlation matrix) Recursive Feature Elimination (sklearn.feature_selection.RFE) SelectFromModel (sklearn.feature_selection.SelectFromBest) (1) above removes features(except 1) that are highly correlated amongst themselves. (2) and (3) run different algorithms to identify combination of features and checks which set gives the best accuracy while ranking the importance of features accordingly. I'm not sure which language you are looking to use but there might be similar libraries to these. Thanks!
