[site]: crossvalidated
[post_id]: 354191
[parent_id]: 
[tags]: 
why small weights are preferred in neural networks

I'm viewing the CS231n lectures and trying to understand some of the regularization concepts. I think I understand the rationale behind "spread out" weights that are preferred by L2 regularization, e.g. regularization penalty is higher for weights [1, 0] than for [0.5, 0.5]. Intuitively, the latter set of weights would use more of your input features. But why does the absolute value of weights matter, as opposed to only their relative values? I've heard that using L1 regularization helps with "feature selection" because in preferring small weights, some eventually shrink very close to zero. But wouldn't the same effect be observed if some weights were just relatively much smaller?
