[site]: datascience
[post_id]: 82793
[parent_id]: 
[tags]: 
Exploring variables to guide xgboost tuning

In short: How to think about the type and distribution of my variables when choosing parameter values for xgboost? Context: I have a dataset which I want to classify using the binary:logistic objective (I am using the R implementation). I wonder if visually inspecting variables in my dataset could inform selection of parameters values, and if so then how does one generally go about it. Example: A dataset with a binary outcome variable and a mixture of binary/continuous predictors. Outcome variable (one value per observation): A set of predictors called flavours (~150 variables) and textures (~80 variables) have binary values 0, 1. Each observation has one or multiple flavours and textures (number of f/t per observation on the horizontal axis): Some flavours/textures occur much more likely than others (frequency on horizontal axis): I added some quantitative variables to summarise flavour/texture for each observation. I calculated a frequency for each f/t (= what is the percentage of observations where this f/t occurs) and then calculated min (most unique f/t in the given observation) max (most common f/t) and mean (average frequency). Apart from that, there are three more continuous variables: What conclusions regarding model parameters one might draw from this, if any?
