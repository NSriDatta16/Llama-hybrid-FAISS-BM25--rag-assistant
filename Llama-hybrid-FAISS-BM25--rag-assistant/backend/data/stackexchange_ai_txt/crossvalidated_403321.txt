[site]: crossvalidated
[post_id]: 403321
[parent_id]: 353819
[tags]: 
This depends upon the domain that you want to use the word-embeddings for and the size of your training data . For example, for the Biomedical classification task that I had at hand, I tried 3 ways. Using pre-trained Google's word embeddings ( Google word2vec ) Using pre-trained Biomedical word embeddings from open-access subset from Medline database ( PubMed embeddings ) Training word embeddings from the "little" corpus I had available with me. Strangely, the word embeddings trained using the biomedical corpora did not perform any better than the google's general word-embeddings. Additionally, the embeddings I had trained using my own little corpus, performed even worse. Later, I found this study that experimented with the word embeddings from biomedical corpora and this too found that biomedical word embeddings did not perform any better on the downstream NLP tasks (like classification) compared to general word embeddings. Shockingly, tf-idf and Bag-of-words models performed better than the word2vec (With a 3-fold cross validation). tf-idf > Bag-of-words > word2vec
