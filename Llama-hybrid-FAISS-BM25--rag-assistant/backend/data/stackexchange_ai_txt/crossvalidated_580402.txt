[site]: crossvalidated
[post_id]: 580402
[parent_id]: 580399
[tags]: 
This is not about variance but rather about PCA and principal components. PCA searches for orthogonal principal components (a linear combination of variables) which maximize explained variability (or equivalently minimize the squared distance from the points to the line; think of it as $R^2$ - the higher it is the more variability we explained, which means the points are closer to the fitted line). When we do this we say that the first PC explains most of the data, i.e. it explains most of the variability, i.e. it contains most of the information about the data, because it's what we can explain best, given our data. However, this does not necessarily mean that this information is of any value to us, the model may be learning associations which are perfectly clear to us or which are not of interest. In conclusion, this is not about variance, but rather about "explained variance", which is distributed among the PC's and which is interpreted (by some people) as "how much information this particular PC holds given all the rest".
