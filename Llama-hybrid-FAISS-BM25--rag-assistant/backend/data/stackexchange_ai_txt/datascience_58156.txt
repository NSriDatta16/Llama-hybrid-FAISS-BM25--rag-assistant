[site]: datascience
[post_id]: 58156
[parent_id]: 57990
[tags]: 
SVMs are models to classify a dataset with a maximum-margin hyperplane. In fact, the optimization problem of a linear SVM is derived from the same assumption and is: $\mathcal{P}: max_{w,b} \frac{1}{\Vert w \Vert}$ subject to $y^{(i)}(w^Tx^{(i)}+b)\geq 1\forall i$ $\equiv min_{w,b} \Vert w \Vert$ subject to $y^{(i)}(w^Tx^{(i)}+b)\geq 1\forall i$ $\equiv min_{w,b} \frac{1}{2}w^Tw$ subject to $y^{(i)}(w^Tx^{(i)}+b)\geq 1\forall i$ The above case is of the hard SVM (when the data is linearly separable). However, when the dataset is not exactly linearly separable, we go for a soft SVM and use the penalty method. The primal problem $\mathcal{P}$ is reformed to: \begin{aligned} & \mathcal{P}: & & min_{w,b} \frac{1}{2}w^Tw+C\sum_{i=1}^N \xi_i \\ & \text{subject to} & & y^{(i)}(w^Tx^{(i)}+b)\geq 1-\xi_i\forall i\\ &&& \xi_i\geq 0\forall i\\ \end{aligned} The above constrained optimization problem can be solved as a series of unconstrained problems as: $$min_{w,b} \Big[\frac{1}{2}w^Tw+C\sum_{i=1}^N \max\big(0,1-y^{(i)}(w^Tx^{(i)}+b)\big)\Big]$$ ,which brings to the answer of your question. In a soft SVM, we are looking for parameters $(w,b)$ which minimizes the above cost function overall. As the data is not exactly linearly separable, for every set of parameters $(w,b)$ , there will be some non-zero penaltie for some $i$ . But the goal is to minimize the whole cost function in order to get the maximum margin and minimum sum of penalties.
