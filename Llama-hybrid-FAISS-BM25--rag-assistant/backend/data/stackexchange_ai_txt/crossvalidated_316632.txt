[site]: crossvalidated
[post_id]: 316632
[parent_id]: 
[tags]: 
How do CNNs handle scale invariance?

Even after googling and reading fitting articles and answers to fitting questions here on StackExchange, I don't understand how CNNs handle scale invariance. I found logical sounding answers saying that it is solely done by pooling, others, equally sounding answers said it is a combination of convolution and pooling; no one gave an explanation based on a simple example. Is scale invariance handling "located" in one layer or "spread" over multiple layers? If so, can the mechanism be easily described? Are images with patterns of different scales needed in the training phase in order to train scale invariance? If so, that would mean every possible size is needed and that cannot be the case, right? In consequence, that would mean, no real learning / "abstraction" would take place? A simple example: A very simple CNN should learn to identify one particular symbol in images, e.g. the digit '4'. Let's say I only have images of a fixed size, e.g. 12x12 pixel. What is needed as training data in order order to detect different-sized '4's? After training, how does the CNN detect that a 5x5 pixel '4' as compared to a 10x10 pixel '4'? What does it do in the different layers?
