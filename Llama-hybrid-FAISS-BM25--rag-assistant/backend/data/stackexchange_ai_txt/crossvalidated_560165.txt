[site]: crossvalidated
[post_id]: 560165
[parent_id]: 
[tags]: 
best way of dimensionality reduction with respect to model(classifier) performance

in my theoretical studies I faced the term Curse of Dimensionality so many times that when I started implementing those ideas in real world(in particular music genre classification) after getting about 5000 thousand features for only 1000 individual songs, first thing I did as a no-brainer was to perform PCA on it. However my model performed worst than the baseline(the classifier on the raw data). I then made some research and found out about blessing of non-uniformity and then a colleague told me about the superiority of wrapper methods(like sequential forward selection) to filter methods(like PCA). I then made some further research and found out about random forest dimensionality reduction and now my main question is: which method performs better with respect to classifier accuracy? edit 1 : I have 5 classes in this problem and I have 1000 features per class and only about 1000 data points
