[site]: datascience
[post_id]: 43202
[parent_id]: 43015
[tags]: 
Basically, what you have mentioned in your question is like that you have just one input feature and one output, and your task is regression. The point here is that you have to investigate your data first and find out whether it can be learnt or not. I will explain this later but before that, I have a suggestion about your consideration about separating models. Do not do that if you are not sure each training split has a same distribution as the real data distribution. It seems that you want to do ensemble learning in a wrong way. The main reason that you have not found a good precision is that if you change the distribution of your data, you will have poor performance on the test data. In other words, your model cannot generalise well due to poor training. On the other hand, changing the size of the training data does not reduce the size of your model if they are drawn iid and they are large enough to be learnt. First, you have to find the Bayes error of your training data. Because you have just one input feature, you may have high Bayes error, which in your case means your inputs and outputs do not satisfy the functionality condition, for each specific input you should just have one output. You have to investigate whether for each input there is just a single output or not. If you find out that you have a large Bayes error, you can be sure that you cannot have a good performance. On the contrary, if you see that you have low Bayes error, then you can employ linear and non-linear models. My solution is that, try to solve your problem using a two-layer neural network, find the correct architecture that can generalise well first, then, after finding a good model check the size of the network. Another solution can be using non-linear SVM if you are sure that your data is non-linear, although I guess a two-layer network can solve your problem if you don't have high Bayes error.
