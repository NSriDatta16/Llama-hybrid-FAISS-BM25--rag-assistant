[site]: crossvalidated
[post_id]: 43356
[parent_id]: 43339
[tags]: 
The greatest problem that I see is that there is no test statistic derived. $p$-value (with all the criticisms that Bayesian statisticians mount against it) for a value $t$ of a test statistic $T$ is defined as ${\rm Prob}[T \ge t| H_0]$ (assuming that the null is rejected for greater values of $T$, as would be a case with $\chi^2$ statistics, say). If you need to reach a decision of greater importance, you can increase the critical value and push the rejection region further up. Effectively, that's what multiple testing corrections like Bonferroni do, instructing you to use a much lower threshold for $p$-values. Instead, the frequentist statistician is stuck here with the tests of sizes on the grid of $0, 1/36, 2/36, \ldots$. Of course, this "frequentist" approach is unscientific, as the result will hardly be reproducible. Once Sun goes supernova, it stays supernova, so the detector should keep saying "Yes" again and again. However, a repeated running of this machine is unlikely to yield the "Yes" result again. This is recognized in areas that want to present themselves as rigorous and try to reproduce their experimental results... which, as far as I understand, happens with probability anywhere between 5% (publishing the original paper was a pure type I error) and somewhere around 30-40% in some medical fields. Meta-analysis folks can fill you in with better numbers, this is just the buzz that comes across me from time to time through the statistics grapevine. One other problem from the "proper" frequentist perspective is that rolling a die is the least powerful test, with power = significance level (if not lower; 2.7% power for the 5% significance level is nothing to boast about). Neyman-Pearson theory for t-tests agonizes over demonstrating that this is a UMPT, and a lot of high brow statistical theory (which I barely understand, I have to admit) is devoted to deriving the power curves and finding the conditions when a given test is the most powerful one in a given class. (Credits: @Dikran Marsupial mentioned the issue of power in one of the comments.) I don't know if this troubles you, but the Bayesian statistician is shown here as the guy who knows no math and has a gambling problem. A proper Bayesian statistician would postulate the prior, discuss its degree of objectivity, derive the posterior, and demonstrate how much they learned from the data. None of that was done, so Bayesian process has been oversimplified just as much as the frequentist one has been. This situation demonstrates the classical screening for cancer issue (and I am sure biostatisticians can describe it better than I could). When screening for a rare disease with an imperfect instrument, most of the positives come out to be false positives. Smart statisticians know that, and know better to follow up cheap and dirty screeners with more expensive and more accurate biopsies.
