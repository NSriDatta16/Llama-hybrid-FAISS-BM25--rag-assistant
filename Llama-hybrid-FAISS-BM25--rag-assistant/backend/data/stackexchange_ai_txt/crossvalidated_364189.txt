[site]: crossvalidated
[post_id]: 364189
[parent_id]: 263712
[tags]: 
On a very general note, this function is used to penalize the misclassifications. In the end, your aim is to classify the data in correct classes and to evaluate your results. To train the model you develop the loss functions and most frequently Mean Squared Error. But in MSE the accuracy may not reflect the true accuracy of the classifier. So we would like a loss function (like 0-1 loss function) which gives error as 1 if the class is wrong and 0 if the prediction is right. This is used in svm and called as Hinge Loss. But in broader terms if you look at the formula ∑max(0,1−y(i)(w⊺ x(i)+b)) It essentially applies the same. You may want to read more about how L1 and L2 regularizations come in the picture but intuitively that is what I understood.
