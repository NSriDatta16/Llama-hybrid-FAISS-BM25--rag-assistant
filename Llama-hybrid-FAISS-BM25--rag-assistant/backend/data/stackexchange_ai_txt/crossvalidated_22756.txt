[site]: crossvalidated
[post_id]: 22756
[parent_id]: 21254
[tags]: 
Given that what you are describing appears to be a binary classification problem, I don't see a point in having three different models for three different subsets of the data. The classical approach is to build a single model (e.g. logistic regression) trained on the whole data set (with cost-sensitive loss function in case of unbalanced classes) which typically produces decent results. More advanced methods such as generalized additive models that rely on bagging and/or boosting (e.g. Random Forests or Gradient Boosting Machine) may perform better if non-linearity and/or variable interactions are present. Having multiple classifiers trained on subsets of data often helps in case of multi-class classification where a dedicated classifier is trained on a labeled subset of the data for a particular class (as in this paper ) Having said that, if you really need to combine multiple special-purpose classifiers, I would start here .
