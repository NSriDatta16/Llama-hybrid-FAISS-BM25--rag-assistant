[site]: crossvalidated
[post_id]: 568914
[parent_id]: 
[tags]: 
Why $\gamma$ in regularization term of XGBoost is defined as minimum loss reduction (not minimum squared loss reduction) and not substracted?

From the source https://xgboost.readthedocs.io/en/stable/tutorials/model.html I guess that the mean-squared error is optimized subjected to a constraint of minimum loss reduction. It appears like optimization using Lagrange multiplier to bear solution $\Sigma w^2_j = \gamma T$ but why the $\gamma$ in https://xgboost.readthedocs.io/en/stable/parameter.html is mentioned as minimum loss reduction, not minimum squared loss reduction, and the regularization term $\Omega(f_t)$ written as $\Omega(f_t)=\frac{1}{2}\lambda\Sigma w^2_j+\gamma T$ , not $\Omega(f_t)=\frac{1}{2}\lambda(\Sigma w^2_j-\gamma T)$ ?
