[site]: crossvalidated
[post_id]: 613420
[parent_id]: 612952
[tags]: 
Your reference seems using the traditional accumulating eligibility trace, maybe the same type of eligibility trace equation (3) of this paper Replacing eligibility trace for action-value learning with function approximation by Framling is more clear to understand, that is, only when $s=s_t$ and $a=a_t$ , $e_t(s,a)=γλe_{t−1}(s,a)+1$ , otherwise $e_t(s,a)=γλe_{t−1}(s,a)$ . And indeed according to $Q$ -values update equation (1) there the same $\delta_t$ is used to update $Q$ -values for all $s,a$ at time step $t$ . Thus note in this context of your reference's pseudo algorithm it's clear that only for the tabular case of $s=s_t$ and $a=a_t$ at time step $t$ the accumulating eligibility trace is set at maximum value of $1$ since your reference simple overrides $e_t(s,a)=1$ rightly after choosing action $a$ at a specific state $s$ , while for all other state-action pairs it's simply set at $γλe_{t−1}(s,a)$ due to the one-step trace decay together with discount. So your reference algo actually simplifies the maximum of accumulating eligibility trace equation (3) via dropping the accumulating term. Finally to address your main concern of why the same delta applies to all $s,a$ at each time step, it's helpful to know that eligibility traces were inspired from the behavior of biological neurons that reach maximum eligibility for learning a short time after their activation via adjusting the synaptic weights between neurons. Thus if you follow the true online Sarsa(λ) algorithm using linear function approximation for action value of all state-action pairs from the book Reinforcement Learning, An Introduction by Sutton and Barto, each component of the eligibility trace with the common delta at every episodic time step will affect its corresponding component of the weight vector of the linear function form of the action value, thus the same delta affects all $Q$ -values in your tabular case at every time step.
