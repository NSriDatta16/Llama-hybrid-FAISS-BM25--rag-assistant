[site]: crossvalidated
[post_id]: 489157
[parent_id]: 488926
[tags]: 
Let's simplify your figure and understand at a more generalized level. This neural network has three layers: Input, Hidden and Output. Each layer has multiple neurons whose activations are determined by: Activations of neurons from previous layer Weights and bias Since this is a fully connected neural network or dense network, there are weights between each neuron between the layers. I have denoted weights by $\theta^{\text{layer number}}_{i, j}$ , and bias with $b^{\text{layer number}}_{j}$ where $i$ is the neuron from previous layer and $j$ is the current neuron. Now coming back to the figure you have provided, here are some example of weights and biases using my notation: $\theta^{2}_{1, 1}=0.35826$ $\theta^{2}_{2, 1}=-6.11547$ $\theta^{3}_{3, 1}=-2.24362$ $b^{2}_{1}=0.85928$ $b^{3}_{1}=0.01803$ and so on... Now that we know what each symbol means let's write some equations to get activation of each neuron. Let's assume a $tanh$ activation function. Second layer activations can be written as: $$ h_1 = tanh(\theta^{2}_{1, 1}\:x_1 + \theta^{2}_{2, 1}\:x_2 + \theta^{2}_{3, 1}\:x_3 + b^{2}_{1}) = tanh \bigg( \sum_{i=1:3}\theta^{2}_{i, 1}\:x_i + b^{2}_{1} \bigg) \\ h_2 = tanh \bigg( \sum_{i=1:3}\theta^{2}_{i, 2}\:x_i + b^{2}_{2} \bigg) \\ h_3 = tanh \bigg( \sum_{i=1:3}\theta^{3}_{i, 2}\:x_i + b^{2}_{3} \bigg) $$ And finally the output layer activations can be written as: $$ y = tanh \bigg( \sum_{i=1:3}\theta^{3}_{i, 1}\:h_i + b^{3}_{1} \bigg) $$
