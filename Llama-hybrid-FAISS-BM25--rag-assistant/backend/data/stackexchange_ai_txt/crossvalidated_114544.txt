[site]: crossvalidated
[post_id]: 114544
[parent_id]: 113746
[tags]: 
The terms auto-associative and auto-regressive aren't related. Auto-associative models learn their own inputs. The function mapping is approximating the identity function, $f(x) = x$. This is useful because the model has learned an internal representation of the input distribution. They are useful for things like the imputation of missing data, sampling, or (in the context of neural networks) pre-training/transfer learning. For instance, shown below is a random forest sampling from a toy dataset. The random forest was trained on the identity function then iterated 10,000 times, like so out = np.zeros((10000,2)) for i in xrange(10000): out[i] = model.predict(current + np.random.normal(0,0.1,2)) The only place I'm familiar with tree methods commonly having an auto-associative feel to them is for the imputation of missing data. They treat each column as regression problem, given the inputs of the other columns. When they encounter a missing feature it is imputed using the appropriate model. Auto-regressive models learn the function mapping $f(x_t) = x_{t+1}$. This is useful for forecasting time-series. To my knowledge, this almost exclusively refers to a specific model presented in Autoregressive Decision Trees . This model is a bag of trees, with each leaf reachable by some boolean conditions, filled with vanilla linear autoregressive models. Technically you could just use normal regression trees on autoregressive inputs, but this often results in modelling abrupt discontinuities and step-wise nonlinearities not commonly found in smooth time-series.
