[site]: stackoverflow
[post_id]: 3038049
[parent_id]: 3037440
[tags]: 
Reliably removing duplicates is pretty much as difficult as sorting the file. As another answer indicates, there is no guaranteed way of precisely detecting duplicates without keeping a full copy of each string in memory, which seems to be exactly what you're trying to avoid. You could keep an in-memory or on-disk index of hashcodes, and use these to retrieve actual strings from file storage for comparison, but this would essentially duplicate what a database would be able to do for you. An alternative is to post-process the file once it's complete. The UNIX sort command is pretty good at large files ( How could the UNIX sort command sort a very large file? ), so I'd expect the standard UNIX command-line approach to work reasonably: sort my-file-of-strings.txt | uniq > my-filtered-file-of-strings.txt (Note that files have to be sorted first before passing to uniq to remove duplicates). If you haven't got these tools (or equivalents) available, then you can always try implementing some variant of an external merge sort yourself.
