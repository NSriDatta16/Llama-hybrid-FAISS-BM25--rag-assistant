[site]: datascience
[post_id]: 124707
[parent_id]: 
[tags]: 
Holding batch size constant, will a bigger dataset consume more GPU memory?

If you hold (mini) batch size constant (as well as everything else) but increase the number of examples (and therefore the number of training iterations), should you expect a (significant) increase in GPU memory since? My instinct is to say no, because only a (mini) batch is uploaded on to the GPU at a time. However this is not what I am seeing in practice on a project I am currently working on. Moving from a very small dataset (1000 examples) to a large one (~3 million examples) caused me to have to decrease batch size 32-fold to avoid OOM errors (from 64 to 2). I don't understand why increasing the size of the dataset should cause such a spike in memory usage- shouldn't the dataset be held primarily in CPU memory? Since this might depend on the specific code packages being used, I am using the transformers library for training with pytorch models. If model architecture matters, I'm using OPT (open version of GPT). If modality matters, my dataset examples are text sentences, tokenized by whitespace. EDIT (2): AFAIK, the entire dataset should not be loaded on to the GPU at once. I am using the Trainer and DataCollatorForLanguageModeling classes from the transformers library to train and collate data, respectively; and the datasets library to hold the data (both libraries by HuggingFace)- if I am wrong and these libraries do load the entire dataset into GPU memory, please correct me. EDIT (1): To clarify, by "use more/less GPU memory," I mean a greater/lesser percentage of available GPU memory being in use at any given moment.
