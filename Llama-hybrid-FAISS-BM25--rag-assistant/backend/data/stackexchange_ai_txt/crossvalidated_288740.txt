[site]: crossvalidated
[post_id]: 288740
[parent_id]: 
[tags]: 
Feedback on approach for supervised code completion with tflearn

We got a machine learning task in a lecture regarding deep neural networks this semester. I already have an approach which works but I am 100% sure that there is a better one. Thus I request you to give me feedback/critic on my approach and comment how to improve/tweak it. Description The task is to create a neural network that is able to correctly predict a token X with a given prefix and suffix of tokens. The tokens are single parts of Javascript programs like "{", "if" or "var". The already existing stub we were supplied with, takes a sequence of tokens and puts a hole in it at a random place of the sequence. Afterwards the trained network is used to predict the missing token(s). In the easiest case this "hole-size" can be one token and in the worst case this goes up to three missing tokens. Approach My best working approach so far (~80% accuracy, with hole size of one) encodes the nearby tokens. This means I mark the two indices before and afterwards of the current index with 1,2,3 and 4 in a kind of one hot encoding (but because of those multiple values it isn't one hot any more, I think). So for example if the current index has a ";" and a "var" as prefix, those two values get the values 1 and 2 in the vector for all possible token values and the next two values in the suffix respectively the values 3 and 4. In the end it results in a vector that looks like this (shortened): [0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 2, 0, 4]. (The network is a LSTM and owns three hidden layers) Possible further work I thought of different other ways to go on, especially because the prediction accuracy decreases dramatically, when the hole-size is greater than 1. Other thoughts I have: Set other values, like negative one for the prefix and positive increasing values for the suffix "hits". Than a vector would be like [0, 0, 0, 1, 0, 0, -1, 0, 0, 0, -2, 0, 2] for example. But this rather results in worse accuracy... "Expand the view": i.e. the current index' vector gets more information about its neighbours via more prefix and suffix encoded values. This also results in lower accuracy... Train another neural network in reverse order of the tokens, so the prefix of the one is the suffix of the other. This also lead to poorer results (but nearly as good as the best approach with ~70% accuracy for holes of size one). Update: Actual state The project now uses an approach to also learn "holes" in the code with size greater than one. This is done by the help of an end of sequence token, which is added to the set of unique tokens and inserted in the case of holes with size 1 and 2. Holes of size 3 do not need this. The following visualization gives a hint on the idea by presenting the setting of x and y pairs: Example token list: [tok1 || tok2 || tok3 || tok4 || tok5 || tok6 || tok7 || tok8 || tok9] Hole Size 1 - with token tok5 missing : x = [tok3 || tok4 || tok6 || tok7] y = [t5 || EOS || EOS] Hole Size 2 - with token tok5 and tok6 missing : x = [tok3 || tok4 || tok6 || tok7] y = [t5 || tok6 || EOS] Hole Size 3 - with token tok5, tok6 and tok7 missing : x = [tok3 || tok4 || tok8 || tok9] y = [tok5 || tok6 || tok7] Then when querying the network, I get the next token as long as I do not get an end of sequence from the list best predictions. The actual problem I'm facing is, that the approach takes too much memory consumption due to the amount of tokens which are stored while training. I already tried to counter this by increasing the heap space of PyCharm, but this did not solve the problem. The source can be viewed here . Thanks for every valuable comment. Best regards, David PS: if the description or informations have a lack of anything important I forgot to mention, please let me know and I will provide the necessary aspects.
