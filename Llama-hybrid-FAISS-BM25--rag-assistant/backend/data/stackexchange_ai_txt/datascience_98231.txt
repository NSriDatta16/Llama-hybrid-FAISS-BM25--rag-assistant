[site]: datascience
[post_id]: 98231
[parent_id]: 98227
[tags]: 
When you train a neural network, you usually use 3 sets: one for training, one for development, one for testing. Your training set is here for (obviously) training your model: the performance of your model on your training set reflects how well your model learnt "by heart" what you showed it. Your development set is used jointly during training: your model does not see this data, but monitoring its accuracy on said set allows you to stop the training before overfitting. Overfitting is when your model learns on your training set so well it can no longer generalise what it learned to new data, but can only reproduce what it has seen strictly. In general, you do not want your models to have this behavior, and you therefore stop the training at the inflexion point where your training accuracy increases while your development accuracy starts decreasing. ( This is what you observe for your model 2: its training accuracy is higher, but dev accuracy lower ) Your test set is only used at the end of your training to check, on yet a new set of data, that your model can still generalize. You NEVER stop your model training based on the test set, because it would introduce a bias. TLDR . The only accuracy reported in the papers is the one on the test set. In most cases, how well the model learned the data "by heart" is not important (training accuracy) nor is how you chose to stop it before overfitting (dev accuracy). What is interesting is how well your model can generalize on completely new data, used neither for training nor for monitoring training (test accuracy).
