[site]: crossvalidated
[post_id]: 411632
[parent_id]: 411608
[tags]: 
This problem may sound complicated at first, but if you think about it, it's not that hard. First thing to realize is that what you have is binary time-series of events $Y_i \in \{\text{inactive},\, \text{active}\}$ . You say that those labels can be inaccurate, and that you can assume more false negatives then false positives among them. Unfortunately, you didn't tell us anything about what you know about particular labels being correct or not, so I assume that you don't know which labels are incorrect. If the above is true, then you don't have any data to determine the correctness of the labels, so you cannot do much about it and need to take them as-is. Additionally, you say that you have historical data, but if you care about "local" clusters within the data, then historical data does not sound to be useful in here, isn't it? As a side note : maybe your wording is imprecise, but you seem to be having data on "active" and "inactive" measurements, but ask about data being "real", and those seem to be rather unrelated things. Extreme case would be if the "active" and "inactive" labels were assigned completely at random, then they would tell you nothing about the "real" labels. What I am trying to say is that you need to ask yourself if your data is adequate for the problem of recognizing the "real" labels. Solution The simple solution that you can use, it to use moving average , with a window of size $k$ , so to predict $y_{t+1}$ you would take the average of the $y_{t-k},y_{t-k+1},\dots,y_t$ values. Alternatively, you can use something like exponentially weighted moving average (a.k.a. exponential smoothing ) , where as your prediction you would take $$ \hat y_{t+1} = s_t = \alpha s_{t-1} + (1-\alpha) y_t $$ where $\alpha \in (0, 1)$ , and for the starting value $s_0$ you can take average proportion of "active" cases from the historical data (or something similar). The nice thing about exponential smoothing is that you can control how much trust $(1-\alpha)$ do you put in new observation $y_t$ , vs the historical data $\alpha$ . If there is no cycles, seasonality, or other patterns, in your data, just the "clusters" over time, then this would be the kind of methods that would be usually used for solving such problems. Finally, you can try methods for forecasting binary time-series as described in Binary time series , Forecasting binary time series , and Modelling auto-correlated binary time series . Moreover, if you have additional data (e.g. there are seasonal cycles), you can use even more advanced models that take them into consideration. Bayesian updating As about Bayesian updating, you can use Bayesian updating to update your knowledge (prior) about probability of something happening, by incorporating some new data (likelihood), to obtain the estimate (posterior) given the Bayes theorem. So, for example, if you assumed that your time series is in fact a series of independent and identically distributed random variables (so no clusters, no autocorrelation over time) distributed as $$ Y_i \sim \mathsf{Bernoulli}(p) $$ with a beta prior parametrized with $\alpha$ = number of successes in historical data , and $\beta$ = number of failures in historical data $$ p \sim \mathsf{Beta}(\alpha, \beta) $$ then you can calculate the posterior distributions as $$ p | Y \sim \mathsf{Beta}(\alpha+\sum_{i=1}^n Y_i,\; \beta + n - \sum_{i=1}^n Y_i) $$ with expected value $$ E(p|Y) = \frac{\alpha+\sum_{i=1}^n Y_i}{\alpha+\beta+n} $$ But, as noticed before, this assumes that the probability is fixed, not changing over time. You can also notice that the expected value in here is basically a sum of successes in current and historical data divided by the total number of trials, so nothing fancy. This is how Bayesian updating would work. It wouldn't assume anything about data being "clustered", or that it changes in time. Bayesian updating is about updating probability about the same thing happening, given new data on this thing, while you explicitly assume that what you are trying to predict something that is changing over time , so you need a model that would be able to account for the nature of your data. Saying it differently, it gives same "weights" to all the observations, so it doesn't care about consecutive series of observations. To give an example, if you already observed millions of such events, then even a series of a thousand consecutive "active" measurements would not change it significantly. What you need is a time-series model , for example of the ones described in the links provided in the first part of the answer. If you wish, you can adapt those models to Bayesian framework, by assuming priors for their parameters, but it would be something much more complicated then directly applying the Bayesian updating.
