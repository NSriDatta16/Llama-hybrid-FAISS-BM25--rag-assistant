[site]: crossvalidated
[post_id]: 111918
[parent_id]: 111908
[tags]: 
A simple solution would be to transform your cost function into: $cost(\beta, x, y)=\sum_{i=1}^{N} \left|y - \hat y(\beta, x)\right| + \lambda \sum_{k=1}^{P} \left|\beta_k\right|^\alpha$ The second term is a so-called regularization term, the goal of which is to get small values of estimated parameters $\hat\beta$. Common choices of $\alpha$ are 1 and 2 (called the L1 and L2 regularizations, used respectively in Lasso and Ridge regression). A value of 1 will tend to set parameters to 0, a value of 2 tends to just shrink the larger parameters. The philosophy behind this can be viewed as Occam's razor: small values of $\beta$ yield simpler models, which is desirable (especially in your case). It also has Bayesian justifications (eg. the cost function of ridge regression arises from putting a Gaussian prior on $\beta$ in usual least-squares regression). You then have to find a "good" value of $\lambda$ (the parameter that trades off model complexity and goodness-of-fit), eg. by cross-validation.
