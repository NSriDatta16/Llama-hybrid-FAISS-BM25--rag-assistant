[site]: datascience
[post_id]: 110844
[parent_id]: 
[tags]: 
Random Forest significantly outperforms neural net for my regression task. What insight can I extract from this?

I'm working on a regression problem for a personal project, and I've noticed that random forest performs significantly better than the current neural net architectures I'm using. (about 0.7 R2 for a fairly deep RF on test-data vs 0.2-3 R2 for 2 hidden layer NN on even the training data). Does this mean that my problem needs a deeper, more complex model? more importantly, since according to my understanding RF uses a random combination of features at every decision node, does this mean I should be trying to extract features by combining some of my features, maybe something similar to self-attention? Or am I deeply misunderstanding my problem? I've considered that perhaps I need a very different loss function, but I think there is some insight i can extract from this large disparity. For context, I have 15 independent variables as features, in addition to about 500 linear or non-linear combinations of them that I know may have some significance based on papers I have read and domain knowledge.
