[site]: crossvalidated
[post_id]: 539841
[parent_id]: 539695
[tags]: 
The first paragraph of your post suggests you know the "timing" of the intervention. In your first DiD equation you set the post-treatment indicator equal to 1 in the first (i.e., immediate) adoption period and the model returned an insignificant, negative program effect. In your second DiD equation, however, you set the post-treatment indicator to 1 in a period(s) after the immediate adoption period and the model returned a significant, negative program effect. This suggests, to some degree, a delayed onset of treatment. It's often worthwhile to partition the temporal sequence into different 'timing epochs' to observe how effects evolve over time. If the post-treatment indicator simply 'turns on' (i.e., switches from 0 to 1) and 'stays on' in all periods after the start of the policy then it might obscure important intertemporal effects. As suggested in the comments, interact a treatment/control dummy with separate time indicators for all periods post-policy. This allows the treatment effect to vary with time since exposure. Now I will address your other inquiry regarding the reverse-engineering of a policy's effective date. I know that the DiD model is used to analyze treatment effects, but can the process be reversed? Yes, but it's inadvisable. For example, if I don't know when the treatment was administered and I find the time when the DiD coefficient is negative by trying different times, wouldn't this prove the effectiveness of the treatment and at the same time find the time when the treatment worked? It appears you want the results of a significance test to advise you on when a treatment was administered. There are two potential problems with this approach. First, it suggests you cannot reliably distinguish between the pre-/post-treatment epochs. If so, then how do you know the observed effect isn't occurring in a period before the exposure? It would be hard to rule this out if you don't know the precise timing of the policy. It also precludes any assessment of an anticipatory response to the policy. Second, this approach may ignore a relevant time dependency in your outcome's response to the intervention. For example, suppose the negative effects of your policy accumulate over time and the full effect is realized a few periods after the policy's effective date, say by the third lag. Since the start of the policy is largely unknown, the first emergence of a significant program effect (i.e., third lag) is taken as evidence of punctual change when, in fact, there was a growing policy influence materializing slowly over time. In practice, it may be inappropriate to assume units (e.g., individuals, families, firms, etc.) respond instantaneously to a stimulus (i.e., treatment). Investigating "how long" a policy takes to phase-in and effectuate meaningful change may be of substantive interest. The reverse-engineering approach is inherently a-theoretical. In essence, you're inferring a treatment's start date in the period (i.e., day/week/month) when the DiD coefficient is significantly bounded away from 0. Note how this implies that the policy will eventually produce the intended result. But say the policy doesn't yield a significant treatment effect in any period. Does this suggest the policy never took place? And how far into the future will you go to look for evidence of a treatment effect? If not obvious, the farther downstream you go to look for evidence of a policy effect the more and more your research endeavor becomes a mere data dredging exercise.
