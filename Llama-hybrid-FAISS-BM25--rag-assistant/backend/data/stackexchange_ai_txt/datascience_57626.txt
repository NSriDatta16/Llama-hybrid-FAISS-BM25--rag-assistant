[site]: datascience
[post_id]: 57626
[parent_id]: 
[tags]: 
How can I improve the performance of my DQN?

I created a deep Q network to play snake. The code works fine, except for the fact that performance doesn't really improve over the training cycle. At the end, it's pretty much indistinguishable from an agent that takes random actions. Here's the training code: def train(self): self.build_model() for episode in range(self.max_episodes): self.current_episode = episode env = SnakeEnv(self.screen) episode_reward = 0 for timestep in range(self.max_steps): env.render(self.screen) state = env.get_state() action = None epsilon = self.current_eps if epsilon > random.random(): action = np.random.choice(env.action_space) #explore else: values = self.policy_model.predict(env.get_state()) #exploit action = np.argmax(values) experience = env.step(action) if(experience['done'] == True): episode_reward += 5 * (len(env.snake.List) - 1) episode_reward += experience['reward'] break episode_reward += experience['reward'] if(len(self.memory) Here are the hyperparameters: learning_rate = 0.5 discount_rate = 0.99 eps_start = 1 eps_end = .01 eps_decay = .001 memory_size = 100000 batch_size = 256 max_episodes = 1000 max_steps = 5000 target_update = 10 Here's the network architecture: model = models.Sequential() model.add(Dense(500, activation = 'relu', kernel_initializer = 'random_uniform', bias_initializer = 'zeros', input_dim = 400)) model.add(Dense(500, activation = 'relu', kernel_initializer = 'random_uniform', bias_initializer = 'zeros')) model.add(Dense(5, activation = 'softmax', kernel_initializer = 'random_uniform', bias_initializer = 'zeros')) model.compile(loss='mean_squared_error', optimizer = 'adam') For reference, the network outputs 5 values because of the 4 directions the snake can move, and 1 extra for taking no action. Also, instead of being a screenshot of the game like in a traditional DQN, I pass in a 400 dimensional vector as representation of the 20 x 20 grid that the game takes place in, though I believe that it should still work fine with this. The agent receives a reward of 1 for moving closer to the food or eating it and receives a reward of -1 if it dies. How can I get the performance to improve?
