[site]: datascience
[post_id]: 126888
[parent_id]: 55901
[tags]: 
Why everyone compares RNN and Transformers, when you should actually compare Feedforward Neural Networks with Transformers? I am really sorry, I cannot comment @shepan6 answer, so I will post an answer. This means that, so far, transformers do not have any notion of word ordering. - @shepan6 This is totally wrong and misleading. Transformers are just FNNs. Order of input matter. Please stop spreading disinformation. I know two ablation studies about positional encoding - one in "Attention is all you need" [arxiv:1706.03762] and the other in "Convolutional Sequence to Sequence Learning" [arxiv:1705.03122] . Both authors conclude that there is no or negligible difference in performance of 1) different positional encoding; and 2) present/missing positional encoding. From paper "Attention is all you need": We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). From paper "Convolutional Sequence to Sequence Learning": Table 4 shows that position embeddings are helpful but that our model still performs well without them.
