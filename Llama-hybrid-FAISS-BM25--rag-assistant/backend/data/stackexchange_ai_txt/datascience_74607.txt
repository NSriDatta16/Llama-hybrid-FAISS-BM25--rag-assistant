[site]: datascience
[post_id]: 74607
[parent_id]: 
[tags]: 
When should I reverse normalizations to evaluate loss?

If I am training a neural network and have normalized the data before-hand, should I reverse the normalization to calculate the loss? This tutorial provides an example of this method. What if I'm using batch normalization? From this post it looks like there are ways to reverse it but my question is more along the lines of should I bother. My first thought was that it would depend on the problem but I could not think of a problem where I would not want to minimize the error in the actual units of the problem
