[site]: crossvalidated
[post_id]: 512933
[parent_id]: 512929
[tags]: 
As I read your question, the first thing that came to mind for me is that, in some sense, "randomness" is synonymous with, or encoded by, entropy . So, here is some person's blog post about the "entropy of images" which might give you a direction, as the author walks through the development. A similar idea that is for time series is approximate entropy , and there's even a Python implementation; however, you'd have to turn your 2-D image into a "1-D image" along the lines of a time series... In either case, the image with higher entropy has "more randomness" in the sense that entropy measures average level of "information", "surprise", or "uncertainty" inherent in the variable's possible outcomes from Wikipedia's entry on entropy in information theory. One final way I'll mention thinking about the "randomness" of your images is to think about how much either image can be compressed (from here ): you can imagine a simple image having little information (low entropy) can be encoded with fewer bytes of data while completely random images (like white noise) cannot be compressed much, if at all. I hope that this helps!
