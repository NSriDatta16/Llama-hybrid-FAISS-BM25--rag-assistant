[site]: crossvalidated
[post_id]: 427203
[parent_id]: 400122
[tags]: 
I struggled with this same problem--decomposing variance in high-dimensional prediction problems without limiting myself to fitting many, many linear regression models--and came up with the following solution: Shapley Decomposition of R-Squared in Machine Learning Models (with an R implementation). I would say that the trick in an applied setting is to first find classes of models that support fast Shapley value approximation calculations--e.g., neural networks and gradient boosted decision trees (see the shap Python package)--or use model-agnostic Monte Carlo Shapley value approximations like those implemented in the R package iml : Explaining prediction models and individual predictions with feature contributions . With the trained model, then run the package's predict() or feature importance function to get the resulting n_samples * n_features Shapley value matrix and decompose it according to the first link in my post. And be careful to take note of feature correlation as high correlations can make the feature-level attributions unstable. The authors of the shapr R package have come up with a few interesting approaches to adjusting Shapley values for feature correlations.
