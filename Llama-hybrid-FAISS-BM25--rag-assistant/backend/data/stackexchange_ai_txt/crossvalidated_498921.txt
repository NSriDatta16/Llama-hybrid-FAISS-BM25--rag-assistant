[site]: crossvalidated
[post_id]: 498921
[parent_id]: 81659
[tags]: 
Mutual Information (MI) uses the concept entropy to specify how much common certainty are there in two data samples $X$ and $Y$ with distribution functions $p_{x}(x)$ and $p_y(y)$ . Considering this interpretation of MI: $$I(X:Y) = H(X) + H(Y) - H(X,Y)$$ we see that the last part says about the dependency of variables. In case of independence the MI is zero and in case of a consistency between $X$ and $Y$ the MI is equal with the entropy of $X$ or $Y$ . Though, the covariance measures only the distance of every data sample $(x,y)$ from the average ( $\mu_X, \mu_Y)$ . Therefore, Cov is only one part of MI. Another difference is the extra information that Cov can deliver about the sign of Cov. This type of knowledge can not extracted from MI because of log-function.
