[site]: datascience
[post_id]: 10569
[parent_id]: 10557
[tags]: 
I assume you mean feature selection as feature engineering . The process I usually follow and I see some people do is Feature engineering Try a few algorithms, usually highly performant ones such as RandomForest, Gradient Boosted Trees, Neutral Networks, or SVM on the features. 2.1 Do simple parameter tuning such as grid search on a small range of parameters If the result of step 2 is not satisfactory, go back to step 1 to generate more features, or remove redundant features and keep the best ones, people usually call this feature selection . If running out of ideas for new features, try more algorithms. If the result is alright or close to what you want, then move to step 3 Extensive parameter tuning The reason for doing this is that classification is all about feature engineering , and unless you know some incredible powerful classifier such as deep learning customized for a particular problem, such as Computer Vision. Generating good features is the key. Choosing a classifier is important but not crucial. All the classifiers mentioned above are quite comparable in terms of performance, and most of the time, best classifier turns out to be one of them. Parameter tuning can boost performance, in some cases, quite a lot. But without good features, tuning doesn't help much. Keep in mind, you always have time for parameter tuning. Also, there's no point of tuning parameter extensively then you discover a new feature and redo the whole thing.
