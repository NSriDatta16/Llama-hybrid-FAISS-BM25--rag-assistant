[site]: crossvalidated
[post_id]: 471255
[parent_id]: 316632
[tags]: 
By themselves, convolutional filters do not handle scale invariance. Each learned filter will be sensitive to a given set of features only within a narrow range of scale. There are generally three techniques that can help approximate scale invariance and make the network resilient to scale changes: 1) Training with random zoom in/out on the input image. This synthetically provides examples at different scales. This technique works well in particular for zooming out, but zooming in is limited since one might end up zooming in too much on the wrong portion of the image. 2) Multiscale input: the input image is fed in at multiple scales, say 0.5, 1x and 2x; predictions are then averaged across the outputs. Another technique is to use feature pyramids . 3) Multisize or dilated convolutions: instead of using a fixed filter size (say, 3x3), you use multiple filter sizes (3x3, 4x4, 5x5) at each stage. One can also use dilated filters , for example a 3x3 convolution that covers a 5x5 receptive field by skipping super pixels in-between.
