[site]: datascience
[post_id]: 12101
[parent_id]: 
[tags]: 
Machine learning technique to calculate weighted average weights?

I'm just starting to investigate machine learning concepts, so I'm sorry if this question is very naive, but I'm hoping that it will be an easy one to answer! I have a document matching algorithm that individually calculates a match for each field (0-1, with 0 = no match, 1 = 100% match), and applies a separate weight to each field match to be used in calculating an overall weighted average relevance "score". E.g., given a document of 3 fields (d1-3) and an input query against each of the fields (q1-3), field matches are calculated for each pair (m1-3) and then weights (w1-3) are applied using a weighted average for a final relevance score: s = sum(mi x wi)/sum(wi). For this contrived example, perhaps we can simply say that a document is considered relevant if the score is above 0.5. I.e., there is either a "relevant" (0.5-1.0) or "not relevant" (0-0.5) outcome. But I don't want every field to have equal weight in determining the outcome. So, my question is simply: What type of machine learning technique is "best" used to calculate the appropriate weights (w1-n), based on past, known results? Is this even an appropriate use of machine learning? And secondly, if instead of a simple outcome of relevant and non-relevant, I actually want to rank the documents by relevancy, can this also be achieved using a machine learning technique?
