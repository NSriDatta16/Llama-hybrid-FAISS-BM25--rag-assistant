[site]: crossvalidated
[post_id]: 362931
[parent_id]: 
[tags]: 
Some general questions regarding an implementation of VAE

I'm a biology student, and on my spare time trying to learn a little bit about ML, DL and math.Right now I'm working on a project in which I need to learn how a Variational Auto-Encoder (VAE) works. While searching the web, I came across this implementation of a VAE, by Jan Hendrik Metzen. Even though it has great comments along the code that explains it, I hav some question to better understand the choices that was made. The loss function: def _create_loss_optimizer(self): # The loss is composed of two terms: # 1.) The reconstruction loss (the negative log probability # of the input under the reconstructed Bernoulli distribution # induced by the decoder in the data space). # This can be interpreted as the number of "nats" required # for reconstructing the input when the activation in latent # is given. # Adding 1e-10 to avoid evaluation of log(0.0) reconstr_loss = \ -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean) + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean), 1) # 2.) The latent loss, which is defined as the Kullback Leibler divergence ## between the distribution in latent space induced by the encoder on # the data and some prior. This acts as a kind of regularizer. # This can be interpreted as the number of "nats" required # for transmitting the the latent space distribution given # the prior. latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq - tf.square(self.z_mean) - tf.exp(self.z_log_sigma_sq), 1) self.cost = tf.reduce_mean(reconstr_loss + latent_loss) # average over batch # Use ADAM optimizer self.optimizer = \ tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost) a. What is the meaning of "the negative log probability of the input under the reconstructed Bernoulli distribution induced by the decoder in the data space" ? From what I know VAE is using a gaussian distribution, not bernoulli. Also, I thought this VAE reconstracts images, not distributions - so why "reconstructed Bernoulli distribution" ? b. Why not simply use the squared error between the original picture and the reconstructed one?
