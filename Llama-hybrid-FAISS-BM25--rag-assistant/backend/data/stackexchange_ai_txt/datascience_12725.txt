[site]: datascience
[post_id]: 12725
[parent_id]: 12724
[tags]: 
This is the expected behavior. Different learning rates should converge to the same minimum if you are starting at the same location. If you're optimizing a neural network and you want to explore the loss surface, randomize the starting parameters . If you always start your optimization algorithm from the same initial value, you will reach the same local extremum unless you really increase the step size and overshoot.
