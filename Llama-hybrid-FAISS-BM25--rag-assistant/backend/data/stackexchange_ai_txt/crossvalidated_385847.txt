[site]: crossvalidated
[post_id]: 385847
[parent_id]: 343481
[tags]: 
After thinking for a long time, my conclusion is that the author is not referring to the MSR version of variance but this on page 126: The Bias/Variance Tradeoff -------------------------- An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors: **Bias** This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10 **Variance** This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data. To answer your question more specifically: When we use bagging (with replacement), we are more likely to learn from the same training instances and have an underfit result in the end. Thus, the model would be less sensitive to small variances in the training data (since the chances are less likely to be selected by a particular predictor).
