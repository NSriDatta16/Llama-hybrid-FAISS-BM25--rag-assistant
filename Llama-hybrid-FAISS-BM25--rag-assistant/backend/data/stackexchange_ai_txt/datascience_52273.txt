[site]: datascience
[post_id]: 52273
[parent_id]: 
[tags]: 
What are some good machine learning techniques for developing rules that govern changes in data?

Say I read a set of data in and it looks like so: [ 1, 2, 3, 4, 5 ] Now, there's an unknown rule set by my data's source that says if the 3rd element in my list is set to 5, the fifth element must be greater than 10. So, subsequent data reads look like this: read #2: [ 2, 2, 3, 4, 5 ] read #3: [ 3, 2, 5, 4, 11 ] read #4: [ 4, 2, 5, 4, 11 ] If I'm only seeing the data, is there a way I can extrapolate that rule, so I end up with this: if (field3 == 5 and field5 I know it would be hard for a machine to differentiate the rules without far more data than this example, but in reality, I have thousands of these data records with hundreds of rules that govern their state. Is there an established method for doing this type of pattern recognition?
