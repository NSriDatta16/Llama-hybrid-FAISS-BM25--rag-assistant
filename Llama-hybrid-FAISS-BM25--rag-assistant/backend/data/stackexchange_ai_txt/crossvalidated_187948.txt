[site]: crossvalidated
[post_id]: 187948
[parent_id]: 
[tags]: 
How do you compare different datasets' performance in an unbiased way?

I am working with a Random Forest implementation and wanted to try out some parameter tweaking as well as some different dataset configurations. Basically my datasets look like this: I have a set A and B , which have different data from different sources. Then I have set C which has some samples from A and some from B but not all. Then I have a set D which has all samples from A and B combined. And what I want to compare is the performance of the classifier when trained with: Set A and X trees, Set A and Y trees, Set A and Z trees, Set B and X trees, Set B and Y trees, Set B and Z trees, etc... Where X, Y, Z equal some number of trees that I determine. Now what I had originally planned to do was separate a test set and then use cross-validation to compare the performance of the different tree count configurations and apply the best to the test set at the end. However, with these different datasets I am confused about how to accomplish this. If I do cross-validation on each dataset then I am only comparing the relative performance among the tree configurations for that particular dataset and not comparing the performance of the different datasets. One thought I have is to have 2 test sets, one that I use at the very end to get a more realistic estimate of my chosen training configuration's performance. And then another that I used as a sort of second validation set, wherein I basically take each cross-validation training set, and validate them on both the relevant k fold AND a similarly sized validation set that contains values from ALL datasets. So to outline this a little clearer here is an example, I partition dataset A for k-fold cross validation. I train on the first k-1 partitions in the cross validation cycle. I validate on the validation partition for that cross validation cycle. Then, I validate the classifier on a separate but similarly sized validation set that encompasses samples from D (i.e. A + B). Repeat steps 1-4 which each of the k partitions as is described in cross validation process. Repeat steps 1-5 with datasets B and C . Determine the optimum training configuration and try that and only that one on the final test set which was separated from D at the start and which has samples not included in any of the training or validation sets. Basically I'm looking for an answer to tell me either: Yes, that is a sensible approach. Or no, that is not a sensible approach and demonstrates a lack of understanding and here's why: ____ . Greatly appreciate any responses!
