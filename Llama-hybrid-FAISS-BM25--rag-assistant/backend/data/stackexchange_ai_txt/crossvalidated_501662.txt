[site]: crossvalidated
[post_id]: 501662
[parent_id]: 
[tags]: 
What is meant by "the number of examples is reduced", and why is this the case?

I am currently studying the paper Learning and Evaluating Classifiers under Sample Selection Bias by Bianca Zadrozny . In section 3.2. Logistic Regression , the author says the following: 3.2. Logistic regression In logistic regression, we use maximum likelihood to find the parameter vector $\beta$ of the following model: $$P(y = 1 \mid x) = \dfrac{1}{1 + \exp(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}$$ With sample selection bias we will instead fit: $$P(y = 1 \mid x, s = 1) = \dfrac{1}{1 + \exp(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}$$ However, because we are assuming that $y$ is independent of $s$ given $x$ we have that $P(y = 1 \mid x, s = 1) = P(y = 1 \mid x)$ . Thus, logistic regression is not affected by sample selection bias, except for the fact that the number of examples is reduced. Asymptotically, as long as $P(s = 1 \mid x)$ is greater than zero for all $x$ , the results on a selected sample approach the results on a random sample. In fact, this is true for any learner that models $P(y \mid x)$ directly. These are all local learners. This part is unclear to me: However, because we are assuming that $y$ is independent of $s$ given $x$ we have that $P(y = 1 \mid x, s = 1) = P(y = 1 \mid x)$ . Thus, logistic regression is not affected by sample selection bias, except for the fact that the number of examples is reduced. What is meant by "the number of examples is reduced", and why is this the case?
