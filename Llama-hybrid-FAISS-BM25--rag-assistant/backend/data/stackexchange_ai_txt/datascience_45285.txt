[site]: datascience
[post_id]: 45285
[parent_id]: 
[tags]: 
Loss Function for Probability Regression

I am trying to predict a probability with a neural network, but having trouble figuring out which loss function is best. Cross entropy was my first thought, but other resources always talk about it in the context of a binary classification problem where the labels are $\{0, 1\}$ , but in my case I have an actual probability as the target. Is one of these options clearly best, or maybe are they all valid with just minor differences around the extreme 0/1 regions? Assuming $x$ is the output of the final layer of my model. Cross Entropy: $\text{target} * -\log(\text{sigmoid}(x)) + (1 - \text{target}) * -\log(1 - \text{sigmoid}(x))$ Mean Squared Error with Sigmoid: $(\text{sigmoid}(x) - \text{target})^2$ Mean Squared Error with Clamp: $(x - \text{target})^2$ When I use the output I clamp the values between $[0, 1]$ .
