[site]: datascience
[post_id]: 39750
[parent_id]: 15062
[tags]: 
With the information provided is difficult to draw conclusions. But theoretically speaking this is absolutelly possible. There is what in Data Science is called the curse of dimensionality wich can explain these phenomena (I mean decreasing performance while increasing the number of features). In Machine Learning, the Hughes phenomenon shows that ...With a fixed design pattern sample, recognition accuracy can first increase as the number of measurements made on a pattern increases, but decay with measurement complexity higher than some optimum value. Maybe this will give you some intuitions or point some aspects of your experimental setting that might be linked to the lower results. Hope this helps!
