[site]: crossvalidated
[post_id]: 624979
[parent_id]: 605231
[tags]: 
In the reverse process of a diffusion model, we start by sampling $\mathbf{x}_T$ from the standard normal distribution $\mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$ then iteratively sample from our approximation to the "tractable" posterior distribution $p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)=\mathcal{N}(\mathbf{x}_{t-1};\mu_{\theta}(\mathbf{x}_t,t),\sigma^2_t\mathbf{I})$ , which is a normal distribution itself when $\beta_t$ s are small in the forward diffusion process. Using the re-parameterization trick for the normal distribution, we can write a sample $\mathbf{x}_{t-1}$ from $p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)$ as $\mathbf{x}_{t-1}=\mu_{\theta}(\mathbf{x}_t,t)+\sigma_tz$ . This is exactly what Algorithm 2 in the paper is doing. The first term in the equation is the mean of the tractable posterior distribution in terms of $\epsilon_{\theta}(\mathbf{x}_t, t)$ . As y.mazari has rightly pointed out. As to why the variance is $\sigma^2_t$ and not $\sigma^2_{t-1}$ look at the forward diffusion kernel $q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_{t};\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})$ . One could argue it should be $\beta_{t-1}$ instead of $\beta_{t}$ since we are starting from $\mathbf{x}_{t-1}$ . In one case they set $\sigma_{t}^2=\beta_t$ . Now, using $\sigma_{t-1}^2$ in the tractable posterior would lead to inconsistency in the $t$ 's. I think it is just semantics.
