[site]: crossvalidated
[post_id]: 252620
[parent_id]: 
[tags]: 
How does the scalarproduct look like, popular kernel functions compute

So far as I understood kernels, they compute a dot product of the vectors $k(x_i,x_j) = \langle\phi(x_i),\phi(x_j)\rangle$ where $\phi$ goes from original lower dimensional space to a higher dimensional space. The most popular ones are the polynomial Kernel, the radial basis function kernel and the sigmoid kernel. Regarding this, I have three questions: For each of these kernels, what is actually the function $\phi$ look like (in dependence of the hyperparameters)? Assuming there is a nonlinear decision boundary, that seperates the dataset perfectly with given features, does there always exist a kernel, so that we find this decision boundary with this kernel and SVM? What would be an example of an Dataset (which actually has a nonlinear decision boundary, sperating this set perfectly), where one of those 3 popular kernels fails?
