[site]: datascience
[post_id]: 124833
[parent_id]: 
[tags]: 
Any Interface/Library that can take the Python ML code and run on spark cluster without learning PySpark?

I have been working with Python for machine learning and have a fair amount of code written in Python using libraries such as scikit-learn, pandas, and numpy. Recently, Iâ€™ve been faced with larger datasets that require distributed computing to handle efficiently. I understand that Apache Spark and PySpark are often used for this purpose. However, I am looking for a way to leverage the power of Spark without having to rewrite my existing Python code in PySpark or learn PySpark from scratch. Is there an interface or library that allows me to run my existing Python ML code on a Spark cluster? Ideally, this would allow me to take advantage of distributed computing without needing to significantly modify my code or learn a new library in depth. Any suggestions or guidance would be greatly appreciated. Thank you!
