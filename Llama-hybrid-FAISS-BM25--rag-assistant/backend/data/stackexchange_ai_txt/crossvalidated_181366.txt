[site]: crossvalidated
[post_id]: 181366
[parent_id]: 177365
[tags]: 
We can construct a "kernel perceptron" by taking the standard perceptron and replacing the inner product $X^\intercal X=\left $ with the equivalent (due to the "kernel-trick") form K(X,X). This works since we have that the inner product is a map $ :\mathbb{R}^p\times\mathbb{R}^p\to\mathbb{R}$, which has identical properties to the kernel function $k:\mathbb{R}^p\times\mathbb{R}^p\to\mathbb{R}$. As in the case of the common Gaussian radial basis function kernel (RBF) : $$ K(x_i,x_j)=\exp\left(-\frac{{\left|\left|x_i-x_j\right|\right|}^2}{2\sigma^2}\right) $$ As mentioned in the Wikipedia page on the kernel perceptron , we select a subset of size $M$ of the inputs and use a linear combination of them to produce our output, $$ f(x) = \sum\limits_i^M \alpha_i y_i K(x,x_i) $$ If you've seen the support vector machine ( SVM ), you'll notice the identical dual. To select the subset of size $M$ to use, we optimize over $\alpha_i$, which represent whether sample $i$ is a support/basis vector of our solution. In the optimization of the $\alpha_i$ we include the weights $\omega_i$ of the original perceptron optimization. As to your question about not having to compute the projection, you're correct, your input data matrix $X$ is still 2-dimensional. In the computation of the output we replaced a dot product with the kernel function, and this is where the 'implicit' calculation in the feature space occurs.
