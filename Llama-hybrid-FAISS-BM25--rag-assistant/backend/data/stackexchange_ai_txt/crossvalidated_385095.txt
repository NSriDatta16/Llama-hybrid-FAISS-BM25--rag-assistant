[site]: crossvalidated
[post_id]: 385095
[parent_id]: 385088
[tags]: 
I'm familiar with two meanings of "meta-learning." Learning methods which allow a model to quickly adapt and fit new data. One example is MAML and related models. " Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks " by Chelsea Finn, Pieter Abbeel, Sergey Levine We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies. The second meaning of meta-learning is hyper-parameter tuning, such as using LIPO or Bayesian optimization to find the best parameters of a machine learning model (neural network, SVM, boosted tree ensemble). I don't have a reference at hand for this usage, since I've only seen it used this way on internet fora (comments on stats.SE posts, or threads in r/MachineLearning). I'm not familiar with a usage of "meta-learning" which includes bagging and boosting as examples. Bagging and boosting are typically used with ensemble methods (such as random forest or boosted trees).
