[site]: datascience
[post_id]: 33103
[parent_id]: 33100
[tags]: 
You can visualise the activation maps of the various layers, to show how a deep CNN decomposes your input into more and more abstract building blocks. There is a good discussion on Cross Validated SE here . This explains the main idea and intuition as to how CNNs work and normally gets layman motivated to listen and learn more :-) There is another great idea names saliency maps - [Simonyan et al.] . As well as reading that paper, I suggest watching Lecture 12 of CS231n ( slides here ). The general idea of 'how to do this' is, I believe, can really be applied to most deep neural networks. Here is a sample from the linked paper: The steps are something like this: take a trained network architecture choose the layer that you are interested in (be it the final layer or an intermediate layer set all weights in layers prior to your chosen layer to zero perform backpropagation rom your selected layer back to your input Now you will have a set of weights that match the dimensions of your input, but the values will be those of the weights at your selected layer, simply extrapolated backwards onto your input. You can then use these weights with an input image, perhaps simply add these value to one or all of the colour channels, and you will have an image which has certain regions highlighted. The highlighted regions signify where the corresponding neurons of your selected layer are activated - i.e. what they focus on to make their contribution towards the final prediction. Yet another approach is to systematically occlude (cover up) parts of your images, and track how the predictions fluctuate. The intuition is that, when you cover up a certain part of an image, and the prediction becomes terrible, you know your network really looks for that part, which you just covered up. The first time this was really published (as far as I can find) was in the 2013 paper from Zeiler and Fergus . In the image below (from that linked paper), you can see how the grey square was shifted over and image to create a final heatmap, which highlights the critical portions of the image to getting the correct image classification.
