[site]: crossvalidated
[post_id]: 617639
[parent_id]: 617637
[tags]: 
I think you've confused averaging, and summing. You're almost onto it with "because the noise components average to zero." Consider a series of independent variables $\{X_i\}$ (with $i$ running from $1$ to $n$ ). Let's say they all have expectation $\mu$ and variance $\sigma^2$ . Variance does sum. If we define sum $T=\sum_{i=1}^n X_i$ , then by some fairly simple properties of variances of sums, we find the expectation is $n\mu$ and variance $n \sigma^2$ . So that aligns with your "I have been told to expect that variance sums" comment. And it does - as $n$ increases, so does the variance of the sum. Your "the noise components average to zero" correctly reflects means . The mean is $\frac{1}{n} \sum_{i=1}^n X_i = T/n$ . Using some algebra, we find that the expectation of this function is $\mu$ and the variance is $\sigma^2/n$ . In this case, as $n$ increases, the variance decreases. In conclusion - there's nothing contradictory in these facts. You just need care and clarity in language.
