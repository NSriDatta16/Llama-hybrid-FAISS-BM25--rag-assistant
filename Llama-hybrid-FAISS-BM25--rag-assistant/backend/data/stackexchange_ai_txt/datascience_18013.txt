[site]: datascience
[post_id]: 18013
[parent_id]: 
[tags]: 
Tips and tricks for designing time-series variational autoencoders

I am new to VAEs but find them quite fascinating. I was wondering if anyone might have any tips or tricks regarding, how one should build the encoder and decoder layers w.r.t. to time-series data. Suppose my inputs are $x \in \mathbb{R}^{1\times1000}$ so they have a thousand dimensions. The data is highly-nonlinear and rather noisy. I have a couple of tens of thousand $x$:s. With that in mind, what are some general guidelines (because I cannot find any, owing to the VAE field being somewhat particular and very new): How deep should my encoder and decoder network be? Are there any good guidelines? Should one use fully connected dense networks, or stacked conv1Dnets? What activation functions are good choices? Can we say anything about the 'best' dimensionality of the latent dimension? Like everyone else I imagine, my loss function is the sum of a reconstruction term, and the KL divergence regularization term. Is there something else one should consider? Batch normalization? Currently, on my problem, it doesn't make a blind bit of difference. But it should. Should one always use batch-norm? In the decoder layer, is it better to up-sample before returning to the original input dimension? Any tips or tricks would be most welcome. I have naturally tried a few of these, but I cannot discern a trend at present.
