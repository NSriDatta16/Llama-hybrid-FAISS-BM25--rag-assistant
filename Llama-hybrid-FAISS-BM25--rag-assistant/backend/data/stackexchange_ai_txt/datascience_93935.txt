[site]: datascience
[post_id]: 93935
[parent_id]: 93931
[tags]: 
Why are positional embeddings learned? This was asked in the repo of the original implementation without an answer. It didn't get an answer either in the HuggingFace Transformers repo and in cross-validated , also without answer, or without much evidence. Given that in the original Transformer paper the sinusoidal embedding were the default ones, I understand that during preliminary hyperparameter tuning, the authors of BERT decided to go with learned embeddings, deciding not to duplicate all experiments with both types of embeddings. We can, nevertheless, see some comparisons between learned and sinusoidal positional embedding in the ICLR'21 article On Position Embeddings in BERT , where the authors observe that: The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction. How does the model handle multiple sentence segments? This is best understood with the figure of the original BERT paper: The two sentences are encoded into three sequences of the same length: Sequence of subword tokens: the sentence tokens are concatenated into a single sequence, separating them with a [SEP] token. This sequence is embedded with the subword token embedding table; you can see the tokens here . Sequence of positional embedding: sequentially increasing positions form the initial position of the [CLS] token to the position of the second [SEP] token. This sequence is embedded with the positional embedding table, which has 512 elements. Sequence of segment embeddings: as many EA tokens as the token length of the first sentence (with [CLS] and [SEP] ) followed by as many EB tokens as the token length of the second sentence (with the [SEP] ). This sequence is embedded with the segment embedding table, with has 2 elements. After embedding the three sequences with their respective embedding tables, we have 3 vector sequences, which are added together and used as input to the self-attention layers. Are the weights in these embedding layers adjusted when fine-tuning the model to a downstream task? Yes, they are. Normally, all parameters are fine-tuned when fine-tunine a BERT-based model. Nevertheless, it is also possible to simply use BERT's representations as input to a classification model, without fine-tuning BERT at all. In this article you can see how these two approaches compare. In general, for BERT, you obtain better results by fine-tuning the whole model.
