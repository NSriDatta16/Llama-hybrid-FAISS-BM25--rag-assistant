[site]: datascience
[post_id]: 47081
[parent_id]: 47061
[tags]: 
Yes, it is possible. You need to: Convert the bottleneck into a stochastic bottleneck . In VAE's the bottleneck are not the values deterministically generated by the encoder. Instead, the encoder generates the parameters defining some random variables. These random vars normally follow independent Gaussian distributions, and the encoder generates a vector with the means and the standard deviations of the Gaussians. In order to connect this output to the decoder, you first generate noise from a standard Normal distribution $\mathcal{N}(0, 1)$ , multiply by the standard deviation $\sigma$ and add the mean $\mu$ ; this is called the reparameterization trick . The results are then passed to the decoder, which remains the same as before. Add a KL divergence term to the reconstruction loss . With a VAE, you impose a prior distribution to your latent variables. This prior is normally a standard Normal distribution $\mathcal{N}(0, 1)$ . You need to add an extra term to the loss to make the stochastic bottleneck similar to the prior, normally with the KL divergence between both. The derivation of the KL divergence between two Gaussians can be found here . Taking into account that the prior has $\sigma=1$ and $\mu=0$ , the expression is $\sum_i \sigma_i^2 \mu_i^2 - \log(\sigma_i) - 1$ . Note: Given the logarithm of $\sigma$ in the KL divergence, it may be a good idea to have the stochastic bottleneck to generate the log standard deviation rather the deviation itself, in order to improve numerical stability.
