[site]: crossvalidated
[post_id]: 470858
[parent_id]: 470826
[tags]: 
How is the $q$ distribution (the proposal) related to the intractable posterior? I don't see how $q$ popped out of nowhere. The posterior is not intractable: $f(x)$ must be available (in a numerical sense) up to a multiplicative constant for the Metropolis-Hastings algorithm to apply. Otherwise, latent or auxiliary variables must be called in. Or else approximate methods such as ABC are needed. The density $q(\cdot|\cdot)$ is essentially arbitrary, provided it operates on the same "space" as $f$ . Meaning events measurable under $f$ should also be measurable under $q$ . It is chosen when running the algorithm with competing goals of (i) a manageable enough simulation of $y\sim q(y|x)$ (ii) a computable density function $q(y|x)$ [up to a multiplicative constant] (iii) a sufficient coverage of the neighbourhood of $x$ towards ensuring eventually (in the number of iterations) a likely exploration of the whole support of the density $f$ (which leads to irreducibility for the associated Markov chain). Why is the acceptance ratio calculated the way it is? It doesn't make intuitive sense to me. The acceptance probability $$\alpha(x,y)=1 \wedge \frac{f(y)q(x|y)}{f(x)q(y|x)}$$ is one of several choices that ensures $f$ is the stationary distribution density of the associated Markov chain. This means that (i) if $X_t\sim f(x)$ , then after one iteration of the algorithm $X_{t+1}\sim f(x)$ (invariance of $f$ ) (ii) whatever the choice (or distribution) of $X_1$ , the limiting distribution of $X_t$ as $t$ grows to $\infty$ is $f$ . One direct explanation for this stationarity is that $$f(x)q(y|x)\alpha(x,y) = f(y)q(x|y)\alpha(y,x)$$ which is called detailed balance . It shows that the flow of the Markov chain is the same looking toward the future and looking toward the past (this is called reversibility ). Any other function $\alpha$ that satisfies detailed balance works as well. Take for instance Barker's ratio $$\alpha(x,y)=\dfrac{1}{1+\frac{f(x)q(y|x)}{f(y)q(x|y)}}=\dfrac{f(y)q(x|y)}{f(x)q(y|x)+f(y)q(x|y)}$$ Another intuitive if informal explanation for this property is that, if $X_t\sim f(x)$ and $Y|X_t=x_t\sim q(y|x_t)$ , then accepting the value $Y=y$ with probability $\alpha(x_t,y)$ turns the distribution of the pair into $Y\sim f(y)$ and $X_t|Y=y\sim q(x_t|y)$ . In Step 3, we accept the X we sampled from the q distribution with some probability - why is that? How does that get me something closer to the intractable posterior, which is our goal? The algorithm produces a Markov chain $(X_t)_{t\ge 1}$ that is converging in distribution to the target distribution $f$ . Hence the distribution of $X_t$ is eventually getting close to $f$ and hence for $t$ large enough the marginal distribution of $X_t$ is approximately $f$ . But it does not make sense to consider that after each iteration $X_t$ is closer to the posterior distribution $f$ .
