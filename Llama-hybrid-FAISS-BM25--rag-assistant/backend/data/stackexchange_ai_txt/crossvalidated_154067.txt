[site]: crossvalidated
[post_id]: 154067
[parent_id]: 154027
[tags]: 
The short answer is that there is no straight forward answer to your question. First off, it is unclear what you mean by “significant linearity”. In OLS the R-squared is a measure of how well you estimates predict the data, it is not a useful measure for how linear your model actually is. Examining linearity is traditionally done with regression diagnostics. I have seen many models with R squared below 1% that appear very much linear (they are just really noisy), and models with R-squared of above 97% that are extremely non-linear (as in spurious time-series regressions). As far as how good the fit is… everything is relative, and much also depends on what exactly you're looking for. 43% is a huge improvement from 0%. Perhaps such a model could prove lucrative in the hands of a knowledgeable individual (in finance I have seen people save millions off of models with R-squares of 7 %). On the other hand, there could also be a very simple improvement to your model that obtains a much higher adjusted R square. In sum, getting an R squared of 43% with only one regressor would be an earth shattering accomplishment in certain contexts, while in others it would remain unimpressive or of average quality. It also depends on what exactly you’re looking for. Notice your coefficient is significant, which under the OLS assumptions, means that you’re independent has a significant effect on your dependent. If you are interested only in proving a significant relationship exists, than the R-squared does not matter nearly as much. However, if you are interested in making a model with high predictive precision, the R square matters much more whereas the coefficient significance is not as important.
