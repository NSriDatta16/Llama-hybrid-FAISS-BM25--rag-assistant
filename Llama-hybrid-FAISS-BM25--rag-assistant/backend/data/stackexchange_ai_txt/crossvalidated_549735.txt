[site]: crossvalidated
[post_id]: 549735
[parent_id]: 549732
[tags]: 
Training a classifier on data in which there is a large class imbalance can lead to problematic behaviour from the classifier. For example, using the popular classification rule of $p else 1 in logistic regression for an imbalanced classification can lead to all the classifications being 0/1 (depending on which class is most prevalent). In this case, it might be better to estimate the probability that each observation belongs to each class. This gives you the added flexibility of creating your own cutoff should you want it, or better yet using the probability directly in any downstream decisions.
