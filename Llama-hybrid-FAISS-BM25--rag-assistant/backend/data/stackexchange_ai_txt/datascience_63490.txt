[site]: datascience
[post_id]: 63490
[parent_id]: 63079
[tags]: 
I want to discuss some high-level intuition behind LSTM networks. Here are some questions to help explore the why aspects: Why/when would we use an LSTM over a feed forward neural network (FFNN)? What are the pros and cons of LSTMs and how do they compare to FFNN? How are they different from traditional recurrent neural networks (RNNs)? Feed Forward Neural Networks (FFNN) Let us first consider a standard FFNN with architecture: As you probably know, this FFNN takes three inputs, processes them using the hidden layer, and produces two outputs. We can expand this architecture to incorporate more hidden layers, but the basic concept still holds: inputs come in, they are processed in one direction, and they are outputted at the end. This concept is very well explained by other articles, so I will not go into much more detail. The key takeaway is: The primary condition that separates FFNN from recurrent architectures is that the inputs to a neuron must come from the layer before that neuron. FFNN Wikipedia Recurrent Neural Networks (RNN) Recurrent neural networks are mathematically quite similar to FFNN models. Their main difference is that the restriction placed on FFNN is no longer applied: Inputs to a neuron can come from any other layer. You will often see this architecture is often "rolled" into a recurrent unit such as the following: The "rolled" units you might see in architecture diagrams can therefore be deceptively small. When you unroll them, the network will often be quite deep! RNN Wikipedia Long-Short Term Memory (LSTM) LSTMs are a special type of RNN that are designed to tackle the vanishing/exploding gradient problem. When you train a traditional RNN, the network often suffers from vanishing/exploding gradients: unrolling a recurrent unit results in a very deep network! If you go through the backpropagation algorithm using traditional RNN architectures, the earlier layers will become less and less significant as we propagate through the network. This makes traditional RNNs prone to forgetting information, particularly data that appears many timesteps prior to the current time. An example LSTM cell is illustrated as such: This structure is similar to the traditional RNN unrolled unit, but the key difference with the LSTM are the gates: input gate, output gate, and forget gate. The function of these gates are well described by their names: input gate controls the data that enters the cell forget gate controls the extent to which data stays within the cell output gate controls the output of the cell via the activation function This github IO post is a great introduction to the basics of LSTMs. It also does an amazing job of explaining the intuition behind the math of an LSTM. LSTM Wikipedia Properties and an Example Use Case of RNN The feedback loops lend recurrent neural networks better to temporal challenges. Time is factored into their architecture! Let us explore an example: Perhaps you are using a network to predict the next word in a sentence. Say you are given the inputs: Starving, Alice drives to the nearest store to buy [prediction] A recurrent neural network might forget the first word "starving" whereas an LSTM would ideally propagate it. An LSTM would therefore use the context it heard previously in the sentence to guess "food" whereas an RNN might guess anything that is bought in a store, particularly given a long sentence or multiple sentences. The gating mechanisms that allow this type of memory is explained well by @StatsSorceress here: Forget Layer in a Recurrent Neural Network (RNN) - RNNs are designed to handle sequences. This can be used to analyze video (sequences of images), writing/speech (sequences of words), etc. LSTMs are designed to let important information persist over time. RNNs will often "forget" over time. FFNNs are memoryless systems; after processing some input, they forget everything about that input. Say, for example, we train an FFNN that takes 5 words as inputs and predicts the next output. This model would then receive the input from the above example: the nearest store to buy [prediction] . This is clearly losing context, and we would get a poor result. Now, you might ask, what if we made a FFNN that took many inputs so that it included the word "starving" in its inputs? In other words, could we not increase the number of inputs to an FFNN to sufficiently represent prior data? The answer is yes, but this method is both inefficient and often impractical. Say, for example, we need to remember context an entire paragraph back. We would need a very large FFNN! Moreover, how do we know how many words back we want as context? Would 20 be sufficient? 30? 100? The LSTM architecture eliminates these problems entirely by letting the network manage a transient memory. This problem is exacerbated by video processing challenges because each frame will require significantly more inputs than NLP tasks. Note, however, that LSTM units are inherently more computationally complex than units of a FFNN, so training them is often more difficult and takes more time. Despite these disadvantages, their success in temporal data challenges clearly warrants their use over FFNN.
