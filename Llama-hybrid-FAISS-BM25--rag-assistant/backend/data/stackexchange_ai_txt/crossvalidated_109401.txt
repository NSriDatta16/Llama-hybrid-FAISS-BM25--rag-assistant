[site]: crossvalidated
[post_id]: 109401
[parent_id]: 86955
[tags]: 
I'm thinking about the same passage and am also wondering when I would ever be interested in the conditional test error. What's more, as far as I can understand they should be the same asymptotically: for very large training and test sets the precise training/test set split should no longer result in different conditional test error estimates. As you can see in the Hastie et al. book their examples on conditional - expected differences are always based on relatively small number of observations, which if I understand this correctly is the reason for why conditional and expected test errors look different in the graphs. The book mentions that the expected test error averages over the randomness in the training set, while the (conditional) test error does not. Now when would I want to take the uncertainty associated with which particular training/test-set partition I make into account? My answer would be that I'm usually never interested in accomodating this kind of uncertainty as this is not what I'm interested in when I'm doing model assessment: In assessing a the predictive quality of a model I want to know how it would fare in let's say forecasting the weather tomorrow. The weather tomorrow is related to my overall data pretty much as my test data is related to my training data - so I calculate one conditional test error to assess my model. However, the weather tomorrow is related to my overall data not like one specific test set is related to the corresponding specific training set, but how the average test set is related to the average training set. So I obtain the next training/test- set partition and get another conditional test error. I do this many times (as e.g. in K-fold cross-validation) - the variation in the individual conditional test errors averages out - and I'm left with the expected test error; which, again, is all I can think of wanting to obtain. Put differently, in the test error/expected test error graphs in Hastie et al., we get an idea of the efficiency of the model estimator: if the conditional test errors are widely dispersed around the expected test error this is an indication of the estimator being inefficient, while less variation in the conditional test errors would indicate a more efficient estimator, given the amount of observations. Bottomline: I might be mistaken here, and I'd be happy to be corrected on this, but as I see it at the moment the concept of the conditional test error is a doubtful attempt at assessing external model validity through allowing oneself only one training/test-partitioning shot. For large samples this single shot should be equivalent to conditoinal test errors averaged over many training/test-partitioning shots, i.e. the expected test error. For small samples where a difference occurs the actual measure of interest appears to me to be the expected, and not the conditional test error.
