[site]: crossvalidated
[post_id]: 355095
[parent_id]: 
[tags]: 
Hidden dimension in LSTM

From https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html Sequence length is 5 ,batch size is 1 and both dimensions are 3. So we have the input as 5x1x3 . If we are processing 1 element at a time , input is 1x1x3 [thats why we are taking i.view(1,1,-1). What I am confused about is how do i decide what the hidden dimension is. Why is it specified like hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) Doc says: h_0 of shape (num_layers * num_directions, batch, hidden_size): 1x1x3 . But why is it concatenated with another 1x1x3 ?
