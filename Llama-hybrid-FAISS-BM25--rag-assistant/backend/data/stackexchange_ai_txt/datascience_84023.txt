[site]: datascience
[post_id]: 84023
[parent_id]: 74720
[tags]: 
A few ideas: As mentioned by @weareglenn in general there is no way to know if the performance obtained on some data is good or bad, unless we know the performance of other systems which have been applied to the same task and dataset. So yes, your results are "acceptable" (at least it does the minimum job of beating the random baseline). However given that that your approach is quite basic (no offense!), its reasonably likely that the performance could be improved. but that's just an educated guess, and there's no way to know by how much it can be improved. To me the level of imbalance is not that bad. Given the low recall on the minority class (fake news) you could try to oversample it if you want to increase recall, but be aware that this is likely to decrease precision (i.e. increase False Positive errors = class 0 predicted as 1). In my opinion you don't have to, unless for your task you must minimize False Negative errors. You could try a lot of things with the features, and I'm quite confident that there is room for improvement at this level: First as mentioned by @weareglenn you should try without removing punctuation, maybe even without removing stop words. Then you could play with the frequency: very often excluding the words with a low frequency in the global training vocabulary allows the model to generalize better (i.e. it avoids overfitting). Try with different minimum frequency threshold: 2,3,4,... (depends how large is your data). More advanced: use feature selection, preferably with a method such as genetic learning, but it might take time because it will redo the training+test process many times. Individual feature selection (e.g. with information gain or conditional entropy) might work, but it's rarely very good. If you want to go very advanced, you could even borrow methods from automatic stylometry, i.e. methods used to identify the style of a document/author (the PAN shared tasks is a good source of data/systems). Some use quite complex methods and features which could be relevant for identifying fake news. A simple thing I like to try is to use characters n-grams as features, it's sometimes surprisingly effective. You could also imagine using more advanced linguistic features: lemmas, Part-Of-Speech (POS) tags. You didn't mention Decision Trees in your methods, I would definitely give it a try (random forests for the ensemble method version).
