[site]: crossvalidated
[post_id]: 60559
[parent_id]: 
[tags]: 
Method to remove bad values in time series (bad values known to take on a particular value)

This sounds easy, but I don't know of a good statistical method for it. I have a time series that has (good) data points that range from ~3.5 to 30. The data are collected by an automated sensor. However, there are flawed measurements in the time series-- the sensor will sometimes read values that are typically at exactly 4.7. Example (in R): Data As you can see, almost every other point is 4.7. I don't want to simply use the rule of "throw out every value that is equal to 4.7" for two reasons: 1) sometimes there are real values that approach 4.7, which can become apparent as you see the slope of the time series change and enter the 4.7 region; 2) the "bad" values aren't always exactly 4.7 (they almost always are, but there is no guarantee). This seems like a fairly simple problem, and I'm hoping that someone has heard of a statistical approach that would be suitiable for this problem. The desired outcome would be a time series where the "bad" values were statistically flagged (e.g., with a probability of being an error). Suggestions? Thanks! EDIT: I'm still mulling over the first two responses. My original inclination was to define two states of the system: an erroneous state and a measured state. The measured state would reflect a modeled process (e.g., AR). The erroneous state would reflect my knowledge that bad values tend to be near 4.7. Both states could ~N(), with appropriate means and sd's. At each time step I could calculate the likelihood of either state, and define a threshold ratio for declaring an obs erroneous. This is similar to the 2 answers. For this approach, I'm stuck at finding a way to let the model for the measured state make estimates w/o being influenced by bad values. However, as stated above, I'm still considering the other two approaches.
