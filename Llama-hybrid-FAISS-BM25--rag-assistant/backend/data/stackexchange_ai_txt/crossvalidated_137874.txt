[site]: crossvalidated
[post_id]: 137874
[parent_id]: 137871
[tags]: 
Performance degradation due to growth of dimensionality comes from the fact that as the number of "visits" in the space of all possible feature sets increases, the probability to find an effect just by chance increases also. Of course, time to traverse all solution space also goes sky-high thus degrading the performance computation time-wise. Therefore, the only methods that would perform better in the case of many dimensions are the ones that make those "visits" in a smart way. I know two such methods: meta analysis, and smart heuristics such as Monte Carlo Markov Chains. Meta analysis is used successfully for example, in bioinformatics, where given that number of records (subjects in a study) is usually much less than number of features (say genetic polymorhpisms), any effect gets killed very often by the multiple comparison correction. So a lot of research groups use candidate gene approach, where the candidate genes (predictors) are being selected based on the overall understanding of modern genomics, including already published results by other groups. This reduces the feature space dramatically. Sampling methods such as MCMC reduce the feature space by traversing it in a smart way: roughly speaking searching for a maximum such as gradient while allowing to jump to a random point to avoid getting stuck in local maxima.
