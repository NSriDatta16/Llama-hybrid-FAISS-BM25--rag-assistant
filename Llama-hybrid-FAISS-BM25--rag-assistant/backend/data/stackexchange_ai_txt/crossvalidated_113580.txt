[site]: crossvalidated
[post_id]: 113580
[parent_id]: 97007
[tags]: 
1) Yes, in essence a language model is a collection of n-grams together with their probabilities as observed in the data set (modulo smoothing) 2) There are many toolkits for language modelling, so there's no need to reinvent the wheel. See for instance the IRSLM and IRSTLM toolkits, which allow you to easily estimate N-gram models of arbitrary size from a file of training data. 3) Your last question is impossible to answer without having a clearer idea of what you are trying to do in your application.
