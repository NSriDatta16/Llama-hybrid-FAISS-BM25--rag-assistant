[site]: crossvalidated
[post_id]: 20839
[parent_id]: 20729
[tags]: 
Optimisation is the root of all evil in statistics! ;o) Anytime you try to select a model based on a criterion that is evaluated on a finite sample of data, you introduce a risk of over-fitting the model selection criterion and end up with a worse model than you started with. Both cross-validation and marginal likelihood are sensible model selection criteria, but they are both dependent on a finite sample of data (as are AIC and BIC - the complexity penalty can help, but doesn't solve this problem). I have found this to be a substantial issue in machine learning, see G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ) From a Bayesian point of view, it is better to integrate over all model choices and parameters. If you don't optimise or choose anything then it becomes harder to over-fit. The downside is you end up with difficult integrals, which often need to be solved with MCMC. If you want best predictive performance, then I would suggest a fully Bayesian approach; if you want to understand the data then choosing a best model is often helpful. However, if you resample the data and end up with a different model each time, it means the fitting procedure is unstable and none of the models are reliable for understanding the data. Note that one important difference between cross-validation and evidence is that the value of the marginal likelihood assumes that the model is not misspecified (essentially the basic form of the model is appropriate) and can give misleading results if it is. Cross-validation makes no such assumption, which means it can be a little more robust.
