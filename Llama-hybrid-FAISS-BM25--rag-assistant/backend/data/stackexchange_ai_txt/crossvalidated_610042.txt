[site]: crossvalidated
[post_id]: 610042
[parent_id]: 357466
[tags]: 
Edit to summarize the following arguments and simulations: I propose that balancing by either over-/undersampling or class weights is an advantage during training of gradient descent models that use sampling procedures during training (i.e. subsampling, bootstraping, minibatches etc., as used in e.g. neural networks and gradient boosting). I propose that this is due to an improved signal to noise ratio of the gradient of the loss function which is explained by: Improved Signal (larger gradient of the loss function, as suggested by the first simulation) Reduced noise of the gradient due to sampling in a balanced setting vs. strongly unbalanced (as supported by the second simulation). Original answer: To make my point I have modified your code to include a "0" (or baseline) model for each run, where the first predictor column is removed, thus retaining only the remaining 9 predictors which have no relationship to the outcome (full code below). In the end I calculate the Brier scores for logistic and randomForest models and compare the differences with the full model. The full code is below. When I now compare the change in Brier score from the "0" models to the full original models (which include predictor 1) I observe: > round( quantile( (brier_score_logistic - brier_score_logistic_0)/brier_score_logistic_0), 3) 0% 25% 50% 75% 100% -0.048 -0.038 -0.035 -0.032 -0.020 > round( quantile( (brier_score_logistic_oversampled - brier_score_logistic_oversampled_0)/brier_score_logistic_oversampled_0),3) 0% 25% 50% 75% 100% -0.323 -0.258 -0.241 -0.216 -0.130 > round( quantile( (brier_score_randomForest - brier_score_randomForest_0)/brier_score_randomForest_0), 3) 0% 25% 50% 75% 100% -0.050 -0.037 -0.032 -0.026 -0.009 > round( quantile( (brier_score_randomForest_oversampled - brier_score_randomForest_oversampled_0)/brier_score_randomForest_oversampled_0), 3) 0% 25% 50% 75% 100% -0.306 -0.272 -0.255 -0.233 -0.152 What seems clear is that for the same predictor the relative change in the Brier score jumps from a median of around 0.035 in an imbalanced setting to a 0.241 in a balanced setting giving a roughly 7x higher gradient for a predictive model vs. a baseline. Additionally when you look at the absolute Brier scores, the baseline model in an unbalanced setting performs much better than the full model in the balanced setting: > round( quantile(brier_score_logistic_0), 5) 0% 25% 50% 75% 100% 0.02050 0.02363 0.02450 0.02545 0.02753 > round( quantile(brier_score_logistic_oversampled), 5) 0% 25% 50% 75% 100% 0.17576 0.18842 0.19294 0.19916 0.23089 Thus concluding that a smaller Brier is better per se will lead to wrong conclusions if say you are comparing datasets with different predictor or outcome prevalences. Overall to me there seem to be two advanteges/problems: Balancing the datasets seems to get you a higher gradient, which should be beneficial for training of gradient descent algorithms (xgboost, neural networks). In my experience without balancing the neural network might just learn to guess the class with the higher probability without learning any data features if the dataset is too unbalanced. Comparability between different studies/patient populations/biomarkers may benefit from measures which are less sensitive to changes in prevalence such as AUC or C-index or maybe a stratified Brier. As the example shows that a strong imbalance diminishes the difference between a baseline model and a predictive model. This works goes to a similar direction: ieeexplore.ieee.org/document/6413859 Edit: To follow up on the discussion in the comments, which partially concerns the error due to sampling for a model trained on an imbalanced vs. a balanced dataset I used a second small modification to the script (full version 2 of the new script below). In this modification the datasets for the testing of the original predictive models is performed on one test set, while the "0" models are tested on a separate "test_set_new", which is generated using the same code. This represents either a new sample from the same population or a new "batch" or "minibatch" or subset of the data as used for training models with gradient descent. Now the "gradient" of the Brier from a non-predictive to a predictive model seems quite revealing: > round( quantile( (brier_score_logistic - brier_score_logistic_0)/brier_score_logistic_0), 3) 0% 25% 50% 75% 100% -0.221 -0.100 -0.052 0.019 0.131 > round( quantile( (brier_score_logistic_oversampled - brier_score_logistic_oversampled_0)/brier_score_logistic_oversampled_0),3) 0% 25% 50% 75% 100% -0.318 -0.258 -0.242 -0.215 -0.135 > > round( quantile( (brier_score_randomForest - brier_score_randomForest_0)/brier_score_randomForest_0), 3) 0% 25% 50% 75% 100% -0.213 -0.092 -0.046 0.020 0.127 > round( quantile( (brier_score_randomForest_oversampled - brier_score_randomForest_oversampled_0)/brier_score_randomForest_oversampled_0), 3) 0% 25% 50% 75% 100% -0.304 -0.273 -0.255 -0.232 -0.155 > round( mean(brier_score_logistic>brier_score_logistic_0), 3) [1] 0.31 > round( mean(brier_score_randomForest>brier_score_randomForest_0), 3) [1] 0.33 So now in 31-33% of simulations for imbalanced models the Brier score of "0" model is "better" (smaller) than the score of the predictive model, despite a sample size of 10,000! While for models trained on balanced data the gradient of the Brier is consistently in the right direction (predictive models lower than "0" models). This seems to me to be quite clearly due to the sampling variability in the imbalanced setting, where even small variations (individual observation) result in a much stronger variability in performance (as observed above the overall Brier is more strongly affected by prevalence than by actual predictors when trained on an imbalanced dataset). As discussed below I expect that this may strongly affect any sampling approaches during gradient descent training (minibatch, subsampling, etc.), while when using the exactly same dataset during each epoch the effect may be less prominent. The modified version of OP's code: library(randomForest) library(beanplot) nn_train 0 ) break } dataset_train Version 2: library(randomForest) library(beanplot) nn_train 0 ) break } dataset_train
