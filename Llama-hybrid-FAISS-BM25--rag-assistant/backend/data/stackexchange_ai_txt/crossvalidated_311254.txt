[site]: crossvalidated
[post_id]: 311254
[parent_id]: 21592
[tags]: 
I wanted to draw attention to the fact, that the last 2 experiments are in fact using the SAME model on ALMOST THE SAME dataset. The difference in performance is not model difference, it is explained by different distributions of validation dataset and the properties of particular METRICS used - precision and recall, that depend highly on that distribution. To elaborate this point a bit more, if you took X distinct entries from your initial validation dataset and replicated the minority class for the upscaled dataset, your model will make the same predictions for those X entries, correct or incorrect, in both upscaled and unbalanced validation datasets. The only difference is that for each false positive there will be less true positives in the initial dataset (hence lower precision) and more true positives in the balanced dataset (simply due to the fact that there are more positive examples in the dataset in general). This is why Precision and Recall are said to be sensitive to skew. On the other hand, as your experiments illustrate as well, ROC does not change. This can be observed by looking at its definition as well. That's why ROC is said to not be sensitive to skew. I don't yet have good answers for points 2 and 3 as am looking for those myself :)
