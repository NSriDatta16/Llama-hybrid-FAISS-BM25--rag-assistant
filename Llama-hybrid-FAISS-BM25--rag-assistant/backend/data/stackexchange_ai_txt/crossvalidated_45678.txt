[site]: crossvalidated
[post_id]: 45678
[parent_id]: 38139
[tags]: 
I know this is a late answer to your question, but I've recently come across a similar situation in my own research. Just out of curiosity, I did a parameter optimization experiment for the c-parameter in a linear svm, in which I wanted to compare the performance of svmlight and the Weka svm implementation. For both approaches, I was classifying the same textual data in 5x2 cross-validation, using binary feature modeling, and bag-of-words unigrams. My findings surprised me: svmlight consistently performed at least 0.10 better (in terms of area under the curve), compared to Weka! I bring this up here, because I think your question raises an interesting point that people don't always think about: not all implementations of svm are the same, and perhaps they should not be treated as such! I think a lot of people will try out the Weka svm on their data (either because it Weka is commonly used, or because it's easier than using a specific implementation like svmlight or libsvm), and falsely conclude, "svms are not good for my data." As to why this is, I am as yet unclear. Perhaps there are differences in the quality of the code, or the optimization routines used in each? I'm going to look into this further!
