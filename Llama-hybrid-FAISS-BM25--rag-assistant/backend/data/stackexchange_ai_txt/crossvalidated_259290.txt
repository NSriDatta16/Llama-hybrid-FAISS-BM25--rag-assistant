[site]: crossvalidated
[post_id]: 259290
[parent_id]: 
[tags]: 
Theoretical: Minimum Number of Support Vectors

I wanted to check if I am understanding the concept of the support vectors correctly. Let's say I am not using any kind of kernel, and it is a hard-margin SVM. In this case, whatever the number of the dimensions is for our features, the minimum possible number of support vectors equals to 2 (1 for +, 1 for -). Am I correct? Do we still need more support vectors even in such an ideal case in $R^{d}$?
