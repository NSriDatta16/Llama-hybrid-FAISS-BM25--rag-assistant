[site]: crossvalidated
[post_id]: 299894
[parent_id]: 50807
[tags]: 
I suggest you, instead of using classic approaches for extracting hand-engineered features, utilise autoencoders . Autoencoders plays an important role in the feature extraction of deep learning architecture. The autoencoder tries to learn a function $f(X_T)≈X_T$. In other words, it is trying to learn an approximation to the identity function, so as to output $\hat X_T$ that is similar to $X_T$. The identity function seems a particularly trivial function to be trying to learn; but by placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data. In this way, your desired $ϕ(X_T)=v_1,…,v_D \in R$ will be equivalent to the output values of the middlemost layer in a deep autoencoder, If you limit the number of hidden units in the middlemost to $D$. Additionally, you can use many flavors of autoencoder for finding the best solution to your problem.
