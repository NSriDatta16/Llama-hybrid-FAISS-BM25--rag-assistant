[site]: crossvalidated
[post_id]: 623151
[parent_id]: 260721
[tags]: 
I understand why normality/homoskedasticity/independence are required (in order to keep alpha levels at 0.05) What is sometimes simply called linear regression in introductory courses has assumptions of normality, homoskedasticity, and independence. But I would caution narrowing our understanding of linear regression to that procedure. In the most general context for linear regression you only need the predicted variables to be linear combinations of parameters in the predictive model. A couple of popular categories of linear models that are more general than that introductory procedure are the general linear model and the generalized linear model , but even those are not as general as you can get! We just ran out of "general " names ;) [...] but why does regression assume interval or ratio data? As you might be able to predict from my previous paragraph, it doesn't in general. Of course specific procedures may take on such assumptions. And, most importantly for my curiosity, why? I have suspicions about why people have the opinion that the data should be naturally interval or ratio scaled. In Stevens 1946 he describes four different types of data that have a kind of structure to them. He named the types of data that follow such group structures nominal, ordinal, ratio, interval. The underlying structures are groups , which the four data types follow by definition. What 'following' the group structure actually means is certain functions of such data being symmetries of the group action . The group theory behind this taxonomy is not often taught in statistics courses, but the taxonomy has been far-reaching in its social influence on the sciences. While it has a bent toward applications in deep learning, I rather enjoyed Bronstein et al. 2021 for understanding the mathematical perspective. So why the fixation on ratio and interval scale variables for linear regression in particular? Let's consider the groups they represent by taking a peek at the table from the original paper. What is revealed is the similarity group and general linear group! I'll leave it to you to examine the math more closely. But do we have to respect group structures in order to perform linear regression? No. As always when stepping off the beaten path into new models, you need to validate your statistical method. This can sometimes be done with pure mathematics but more often with simulation methods. As long as you can show that the methodology does what it is supposed to do, you're fine. Just for fun, let me give you a really different example. It isn't linear regression, but illustrates the point. One can use autoencoders on natural language data to develop word embeddings . Turning words into numbers involves NLP , which is already a 'sin' by some standards (not mine). I've exempted being careful to show the mapping of converting between words and vectors of numbers, but it can be done in variety of ways. If you're not familiar with autoencoders, let's basically think of it as two (in our case real-valued) functions $$\text{encode}: \mathbb{R}^n \mapsto \mathbb{R}^m$$ and $$\text{decode}: \mathbb{R}^m \mapsto \mathbb{R}^n$$ which have parameters (that I have excluded from the notation) that are tuned such that the decoder is an approximate inverse of the encoder: $$(\text{decode} \circ \text{encode})(\vec x) \approx \vec x$$ So words are something like nominal, right? Well, arguably when composed into sentences they have more structure than that. But I could see someone declaring them as nominal. Many people I have met would not accept the notion of 'adding words', and they're not entirely wrong. But there is a kind of way to do it that gets something rather interesting right about how words relate. The following identity has been shown to hold for some word embedding models: $$\text{decode}(\text{encode}(\text{king}) - \text{encode}(\text{man}) + \text{encode}(\text{woman})) = \text{queen}$$ Of course we have much more sophisticated models in deep learning (e.g. transformers ) that better capture the structure of language. Similar things can be said for applications of deep neural networks to graphs and other structures. Enforcing models to obey group structures can be useful, which Bronstein and others convincingly argue lead to certain kinds of improvements, but don't make it a rigid rule.
