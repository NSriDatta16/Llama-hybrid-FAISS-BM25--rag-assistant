[site]: crossvalidated
[post_id]: 599085
[parent_id]: 
[tags]: 
Training Transformers: self attention weights vs embedding layer

I have been trying to wrap my head around transformers. While I have found many good resources that explain the self attention mechanism I've yet to find a good answer on how it really works with respect to training. With respect to the embedding layer my understanding is that an input of words or pixels is first tokenized and then projected using an learned linear transformation from the token space to a new embedding space [Commonly position is also projected]. From what I gather there are a few ways to actually train the embedding such as masking out words or image patches and trying to fill in the blank. My intuition is that it is this training that leads to an embedding which projects the token for Queen to a vector that is more "similar" to the vector transformed from the token for king than dog. I believe I understand the mechanism of self attention $Q=XW_Q$ $K=XW_k$ $V=XW_v$ $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$ The product of Q and K is a vectorized implementation of the dot product which measures the similarity between two vectors. I think this is like a distillation step where all information is weighted by relevance. I think the final step is to use the weight to the embedding. Where I get confused is that the weights $W_Q$ and $W_K$ also seem to serve a similar purpose to the embedding. In fact my first question is are $W_Q$ , $W_K$ the same weights as the initial embedding typically found at the beginning of transformers? It seems like $W_V$ must just be weights from the layer right before SA? So V is just the activations? If not why does the training in SA or MHSA result in weights encode words which often appear together similarly? I guess I just don't see what guarantees this property. Is it because pre-training tasks for language and image transformers back prop through all layers? Am I wrong to assume that $W_K$ and $W_V$ must be trained in the same manner as the embedding in order to learn transformations that would actually make the projection of vector king onto vector queen larger than the projection of vector king onto vector dog? If anyone could give some intuition on how training the weights for SA and MHSA actually works it would be greatly appreciated.
