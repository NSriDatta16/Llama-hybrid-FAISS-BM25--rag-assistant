[site]: datascience
[post_id]: 92250
[parent_id]: 
[tags]: 
Why is the optimal C chosen by GridSearchCV so small?

I'm trying to use GridSearchCV to select the optimal C value in this simple SVM problem with non-separable samples. The issue I'm having is that when I run the code the optimal C is chosen to be ridiculously small (~e-18) so that the margin is expanded to contain all samples. Even when I alter the samples so that they are easily separable, the optimal C is still on the scale of e-18. GridSearchCV selects a very small C however I try to alter the samples. Does anyone know why this is happening? import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets.samples_generator import make_blobs from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, confusion_matrix from sklearn.model_selection import GridSearchCV X, y = make_blobs(n_samples = 500, centers = 2, random_state = 6, cluster_std = 1.2) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(X[:,0], X[:,1], c = y, cmap = 'rainbow', s = 30, edgecolors = 'white') ax.set_xlabel(r' $x_1$ ', fontsize = 20) ax.set_ylabel(r' $x_2$ ', fontsize = 20) svc = SVC(kernel = 'linear') c_space = np.logspace(-20, 1, 50) param_grid = {'C': c_space} svc_cv = GridSearchCV(svc, param_grid, cv = 5) svc_cv.fit(X, y) c = svc_cv.best_params_['C'] svc.C = c svc.fit(X, y) support_vecs = svc.support_vectors_ x1_min = min(X[:,0]) x1_max = max(X[:,0]) x2_min = min(X[:,1]) x2_max = max(X[:,1]) x1 = np.linspace(x1_min, x1_max, 100) x2 = np.linspace(x2_min, x2_max, 100) X1, X2 = np.meshgrid(x1, x2) points = np.vstack([X1.ravel(), X2.ravel()]).T boundary = svc.decision_function(points).reshape(X1.shape) ax.contour(X1, X2, boundary, colors = 'k', levels = [-1, 0, 1], linestyles = ['--', '-', '--']) ax.scatter(support_vecs[:,0], support_vecs[:,1], s = 250, linewidth = 1, facecolors = 'none', edgecolors = 'k') ```
