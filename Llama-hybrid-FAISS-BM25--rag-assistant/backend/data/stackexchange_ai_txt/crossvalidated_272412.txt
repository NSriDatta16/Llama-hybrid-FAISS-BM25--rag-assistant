[site]: crossvalidated
[post_id]: 272412
[parent_id]: 272409
[tags]: 
The exact oppositie! It's very effective to shuffle your training data between iterations. This makes sure your neural network is not remembering a specific order. And when you don't shuffle training data, your network will actually view the last trained sets as more important. It is extremely important to shuffle the training data, so that you do not obtain entire minibatches of highly correlated examples. As long as the data has been shuffled, everything should work OK. Different random orderings will perform slightly differently from each other but this will be a small factor that does not matter much. @Ian Goodfellow, Lead author of the Deep Learning textbook: http://www.deeplearningbook.org Depending on the problem your neural network has to solve, I advise you to shuffle your training data every iteration or every X iterations. From here Check out this answer as well!
