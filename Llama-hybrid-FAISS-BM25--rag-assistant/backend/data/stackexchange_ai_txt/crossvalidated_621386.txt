[site]: crossvalidated
[post_id]: 621386
[parent_id]: 
[tags]: 
How do you deal with a high Feature/Sample ratio?

I am working on a machine learning task with a small dataset of 130 samples, each with 66 features. When I try to fit a model to this data, I encounter issues with either overfitting or underfitting. I have tried ElasticNet , and a PyTorch-based perceptron, but they encounter similar issues. I cannot collect more data due to cost and time constraints. How can I handle my high-dimensional data with limited samples? Extra I am particularly interested in feature selection/reduction techniques, regularization methods, and any other tips to prevent overfitting in such scenarios. Any help would be appreciated.
