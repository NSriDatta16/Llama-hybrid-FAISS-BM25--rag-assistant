[site]: datascience
[post_id]: 98337
[parent_id]: 
[tags]: 
K-Fold cross validation-How to calculate regular parameters/hyper-parameters of the algorithms

K-fold cross-validation divides the data into k bins and each time uses k-1 bins for training and 1 bin for testing. The performance is measured as the average across all the K runs err ← err + (y[i] − y_out)^2 as demonstrated in Wikipedia and the literature err ← 0 for i ← 1, ..., N do // define the cross-validation subsets x_in ← (x[1], ..., x[i − 1], x[i + 1], ..., x[N]) y_in ← (y[1], ..., y[i − 1], y[i + 1], ..., y[N]) x_out ← x[i] y_out ← interpolate(x_in, y_in, x_out) err ← err + (y[i] − y_out)^2 end for err ← err/N But what about the parameters that are obtained from the training? is it the average across all the training or does it to be picked from the best output in k-fold cross-validation? Do we need to run the same ML algorithm in k-fold cross-validation or each fold can have a different algorithm? I think we need to run only one algorithm for the k-fold and for each individual algorithms we need to run k-fold cross-validation.
