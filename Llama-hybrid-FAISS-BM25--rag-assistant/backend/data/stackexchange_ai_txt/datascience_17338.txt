[site]: datascience
[post_id]: 17338
[parent_id]: 17324
[tags]: 
The engineer in question that proposed traditional CV methods for your application simply did so out of habit. Using template matching is extremely outdated and has been shown to perform very poorly. However, I do think a CNN is overkill depending on your dataset's size. How does template matching work? Template matching slides a window across your image that will provide a percent match with the template. If the percent match is above a certain predefined threshold then it is assumed to be a match. For example if you have an image of a dog and you want to determine if there is a dog in the image, you would slide a dog template around the entire image area and see if there is a sufficiently large percent match. This will likely result in very poor performance because it requires the template to overlap the image identically . What is the likelihood of that in practice? Not very high. The only time template matching is a sufficient technique is if you know exactly what you are looking for and you are confident that it will appear almost identically in every example of a given class. Why use machine learning instead? Machine learning techniques are not rigid. Unlike what stmax said, CNNs are able to generalize a dataset very well . That is why they are so powerful. Using the dog example, the CNN does not need to see a picture of every dog in existence to understand what constitutes as a dog. You can show it maybe 1000 images from a Google search, and then the algorithm will be able to detect that your dog, is in fact a dog. The fact that machine learning algorithms generalize very well is the reason that they replaced all the ancient CV techniques. Now the problem is the amount of data that you need to train a CNN. They are extremely data intensive . I do not think that 100 data points is sufficient to train a robust CNN. Due to the deep complexity of the model in order to limit the bias you need to increase your number of examples. I usually suggest 100 examples for every feature for deep models and 10 examples for every feature for shallow models. It really all depends on your feature-space. What I suggest. What you are truly doing is anomaly detection . You have a lot of examples that will be presented of PCBs that are otherwise in good shape. You want to detect those which are broken. Thus I would attempt some anomaly detection methods instead. They are much simpler to implement and you can get good results using shallow models especially in skewed datasets (1 class is over represented).
