[site]: crossvalidated
[post_id]: 434750
[parent_id]: 
[tags]: 
MLE as an expectation over the empirical distribution

I am reading Ian Goodfellow "Deep Learning" book. At page 128, it writes the maximum log-likelihood estimator and then says it is equivalent to the expectation over the empirical distribution To obtain a more convenient but equivalent optimization problem, we observe that taking the logarithm of the likelihood does not change its arg max but does conveniently transform a product into a sum $$\mathbf{\theta}_{ML} = \arg\max_{\theta} \sum\limits_{i=1}^{m} \log p_{\text{model}}(\mathbf{x}^{(i)}; \mathbf{\theta}) $$ Because the arg max does not change when we rescale the cost function, we can divide by $m$ to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution $\hat{p}_{\text{data}}$ defined by the training data: $$\mathbf{\theta}_{ML} = \arg\max_{\theta} \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text{data}}} \log p_{\text{model}} (\mathbf{x}; \mathbf{\theta}) $$ Can you provide a little insight / a proof for that? I have read on Wikipedia that the "empirical distribution" is a cumulative distribution function and This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value. Is this the definition of "empirical distribution" used by the book? Or rather it uses the probability density instead of the cumulative distribution? How does it fit in the definition of log-likelihood as expectation over the empirical distribution, and why this definition follows from the definition of the first formula?
