[site]: datascience
[post_id]: 19581
[parent_id]: 19578
[tags]: 
Your weights have diverged during training, and the network as a result is essentially broken. As it consists of ReLUs, I expect the huge loss in the first epoch caused an update which has zeroed out most of the ReLU activations. This is known as the dying ReLU problem , although the issue here is not necessarily the choice of ReLU, you will probably get similar problems with other activations in the hidden layers. You need to tone down some of the numbers that might be causing such a large initial loss, and maybe also make the weight updates smaller: Normalise your input data. The autoencoder is trying to match the input, and if the numbers are large here, this multiplies up to a large loss. If the input can have negative values (either naturally or due to the normalisation) then you should not have ReLU activation in the output layer otherwise it is not possible for the autoencoder to match the input and output values - in that case just have a linear output layer. Reduce the learning rate - in Keras SGD has default lr=0.01 , try lower e.g. lr=0.0001 . Also consider a more sophisticated optimiser than plain SGD, maybe Adam, Adagrad or RMSProp. Add some conservative weight initialisations. In Keras you can set the weight initialiser - see https://keras.io/initializers/ - however, the default glorot_uniform should already be OK in your case, so maybe you will not need to do this.
