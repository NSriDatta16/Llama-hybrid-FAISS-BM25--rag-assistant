[site]: crossvalidated
[post_id]: 405454
[parent_id]: 
[tags]: 
Why are the additional set of parameters in discriminative models necessary(in Minka's 2005 paper)?

In a short paper titled Discriminative models, not discriminative training by Tom Minka, it says that the discriminative training might work better because it has two sets of independent parameters and hence is more flexible. Then I just wonder why we should model the $p(x_i|\theta')$ as stated in that paper? Or how we get it while training the discriminative models? I learned before that the $X$ is given and then it is unnecessary to encode a distribution over it, and in practice we often use only one set of parameters, like we train a logistic regression model and etc. More specifically, if we don't need to care about the $P(X)$ (as said in the paper "the best Î¸(in (7)) is the same as in (3)") that statement would not hold and what is the value of that paper? If we do how should we train the discriminative model, for instance logistic regression, using (7)? Any examples/references of discriminative models and the training according to (7)? I thought it just means that while we are training the discriminative models, like logistic regression, we are actually training the joint distribution the same as the generative models?
