[site]: crossvalidated
[post_id]: 486755
[parent_id]: 
[tags]: 
Using AIC weights to determine prediction intervals for a single model structure

I am working with fitting regression models to data, and producing prediction intervals would be useful. Unfortunately, the data often has few data points, and is reported as mean rather than individual data points, and so the typical delta-method and parametric bootstrap approaches are not reliably usable. Additionally, several parameter estimates have similar likelihood/AIC on these data sets. Would it be appropriate to use a AIC model averaging approach, where rather than comparing multiple model structures I compare multiple parameter sets? Procedure Using a simple 3-parameter sigmoid model y~sigmoid(x) for example, fitted to the mean of 4 different groups. Calibrate the find best fitting model with parameters (a*,b*,c*). Find large (N=10000) set of potential parameter sets Pi =(ai,bi,ci) where for all ai,bi,ci ai is between .5 x a* and 1.5 x a*, etc For all parameter sets Pi determine the likelihood and AIC of Pi with respect to the data. Use Akiake weights to determine the 'best' 95% of model parameter sets. Simulate these 'best' parameter sets for all independent (x) values. Use the lowest and highest prediction of these models for all independent (x) values as the bounds of the prediction interval. Also display the best fitting curve (a*,b*,c*) and the mean value of the 'best' parameter sets. This approach seems intuitively reasonable and has been effective on simulated data with a known underlying structure, but I haven't seen this approach used in any published work. Any guidance would be appreciated.
