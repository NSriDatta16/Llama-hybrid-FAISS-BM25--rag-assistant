[site]: crossvalidated
[post_id]: 437857
[parent_id]: 
[tags]: 
Need of value vector in transformers

I am reading the paper "Attention is all you need" ( https://arxiv.org/abs/1706.03762 ). In transformer architecture, we have 3 vectors(key,value and query) for each word. I don't understand the need of value vector. Why won't just the dot product of query and key vector suffice? Dot product of query and key vector would give us how closely are the words related to each other which is exactly what we need in self-attention layer. What is the need to scale this value with value vector? Thanks.
