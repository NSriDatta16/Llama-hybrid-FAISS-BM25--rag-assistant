[site]: crossvalidated
[post_id]: 276888
[parent_id]: 276831
[tags]: 
1. Normal distribution of residuals : The normality condition comes into play when you're trying to get confidence intervals and/or p-values. $\varepsilon\vert X\sim N (0,\sigma^2 I_n)$ is not a Gauss Markov condition . This plot tries to illustrate the distribution of points in the population in blue (with the population regression line as a solid cyan line), superimposed on a sample dataset in big yellow dots (with its estimated regression line plotted at as dashed yellow line). Evidently this is only for conceptual consumption, since there would be infinity points for each value of $X = x$) - so it is a graphical iconographic discretization of the concept of regression as the continuous distribution of values around a mean (corresponded to the predicted value of the "independent" variable) at each given value of the regressor, or explanatory variable. If we run diagnostic R plots on the simulated "population" data we'd get... The variance of the the residuals is constant along all values of $X.$ The typical plot would be: Conceptually, introducing multiple regressors or explanatory variables doesn't alter the idea. I find the hands-on tutorial of the package swirl() extremely helpful in understanding how multiple regression is really a process of regressing dependent variables against each other carrying forward the residual, unexplained variation in the model; or more simply, a vectorial form of simple linear regression : The general technique is to pick one regressor and to replace all other variables by the residuals of their regressions against that one. 2. The variability of the residuals is nearly constant (Homoskedasticity) : $E[ \varepsilon_i^2 \vert X ] = \sigma^2$ The problem with violating this condition is: Heteroskedasticity has serious consequences for the OLS estimator. Although the OLS estimator remains unbiased, the estimated SE is wrong. Because of this, confidence intervals and hypotheses tests cannot be relied on. In addition, the OLS estimator is no longer BLUE. In this plot the variance increases with the values of the regressor (explanatory variable), as opposed to staying constant. In this case the residuals are normally distributed, but the variance of this normal distribution changes (increases) with the explanatory variable. Notice that the "true" (population) regression line does not change with respect to the population regression line under homoskedasticity in the first plot (solid dark blue), but it is intuitively clear that estimates are going to be more uncertain. The diagnostic plots on the dataset are... which correspond to "heavy-tailed" distribution , which makes sense is we were to telescope all the "side-by-side" vertical Gaussian plots into a single one, which would retain its bell shape, but have very long tails. @Glen_b "... a complete coverage of the distinction between the two would also consider homoskedastic-but-not-normal." The residuals are highly skewed and the variance increases with the values of the explanatory variable. These would be the diagnostic plots... corresponding to marked right skewed-ness. To close the loop, we'd see also skewed-ness in a homoskedastic model with non-Gaussian distribution of errors: with diagnostic plots as...
