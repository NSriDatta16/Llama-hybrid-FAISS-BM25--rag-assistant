[site]: crossvalidated
[post_id]: 494348
[parent_id]: 348612
[tags]: 
A large subset of the space of interesting feature extractors can be represented as functions from fixed size lookback windows of the time series to elements of the set in which the time series takes values. For example, in real-valued time series, we have the rolling average over the last 100 observations, rolling standard deviation over the last 25 observations, rolling is-there-a-change-point-in the last 50 observations, etc. I assume you know this. A subset of the same class of functions can be applied to discrete, non-ordinal data. After all, you have a probability distribution on your hands, even though it's one most introductory textbooks don't treat very well. You've already hit upon one statistic (aka feature) you can calculate: the rolling mode. But there are plenty more features to compute over fixed lookback windows. How about: Rolling nth most frequent value (the 1st most frequent is the mode) Rolling percentage of each value in your set (A, B, C, etc.) Variance of the percentages of values in your set. E.g. if you had 15 As, 15 Bs, and 15 Cs, and your lookback window had size 45, then the value of this metric is zero) Various overlap metrics. These take two or more sets and compute how similar they are. You can run them over two or more lookback windows. E.g. you can apply the Sørensen–Dice coefficient , Jaccard index , Hamming distance , Damerau-Levenshtein distance , overlap coefficient , Tversky index , etc. to different lookback windows of your time series. E.g. a dramatic change in (any of the prior mentioned) distances between the [t-10,t] window and the [t-200,t] window might indicate that a change point has occurred. Computing this over several lags would be close to the concept of (a nonlinear version of) autocorrelation, but for discrete non-ordinal time series. You can also compute many new features on this new time series of distances, as it's real-valued (autocorrelation, mean, variance, etc.). Information theoretic metrics. These don't need real-valued data either. For a lookback window, you can compute the entropy, permutation entropy, Lempel-Ziv complexity, etc. For two windows, you can compute the mutual information, mutual Lempel-Ziv complexity, etc. Frequent pattern mining. This works as follows: at each new observation, you check if the pattern has appeared. If it has, the value of this new "pattern feature" is 1. If it hasn't, the value of this new "pattern feature" is 0. If the patterns are longish (say 6 or more items), and you can define a metric on these patterns (see any of the distances above), then this "pattern feature" time series can be the value of the metric instead. E.g. if the pattern is ABCABCABC and the last nine values of the time series is ABCABCABC, then any of those distances would be zero. If the last nine values are ABCABCABA, then those distances would spit out small numbers. If the last nine values are BCCBABBCA, then maybe the feature would have a rather large value, to indicate the dissimilarity. You get the picture. To find frequent patterns, you can just decide on a few window lengths (say, 6, 8, and 10) and just extract the most frequent 6-, 8-, and 10-length strings from your time series. There are several algorithms for extracting frequent patterns from a time series as well that I'm less familiar with (some googling around will yield results, though). Run lengths. Runs are contiguous observations, such as "AAAAA", which you might call "a run of 5 As". When your time series takes values in a small set (like {A,B,C}), run lengths can be very useful. For example, you can generate several new features by answering this question: over the past 50 observations, how many runs of A's of length 3/4/5/6/etc. were there? How many runs of 3/4/5/6/etc. B's? Etc. Statistics on the time series of one-hot encodings. You are probably familiar with one-hot encodings: if the time series at time t has the value "A", then the feature dummyA = 1; otherwise dummyA = 0. This is a simple feature, but it's a powerful feature, because you can compute any real-valued statistic on it! Mean, variance, standard deviation, etc. all make sense. In fact, since it's a nonstationary Bernoulli random variable, there are many, many things you can apply to it (if you've taken an introductory statistics class, then you know how amenable Bernoulli random variables are to analysis--even if they're nonstationary). I've mentioned this above, but once you compute a real-valued feature based on a discrete, non-ordinal valued time series, you can compute many features based on that real-valued time series. For more inspiration you can see feature generation libraries for time series like tsfresh or FeatureTools . The features truly are endless. In your specific case of classifying these time series, if you're interested in using any kind of kernel machine, you can treat the time series as a string, and use a string kernel over a long lookback window to directly classify it. You can also one-hot encode the time series (so, now it's a three-dimensional time series which takes binary values, like the comment to your question shows) and run it through any machine learning classifier or time series prediction algorithm (almost all of them take or can be modified to take binary data). If you have multiple discrete non-ordinal valued time series, then you can compute several of the above features on lookback windows from different time series, to get some measure of discrete dependence (like cross-correlation, though obviously nonlinear). Last thing to note: with all these options, you might end up with quite a large vector at each time index describing the values of all the features. Make sure to do some feature importance analysis or feature selection to prune the useless ones. You can find several guides about this on Medium or in the scikit-learn user guide.
