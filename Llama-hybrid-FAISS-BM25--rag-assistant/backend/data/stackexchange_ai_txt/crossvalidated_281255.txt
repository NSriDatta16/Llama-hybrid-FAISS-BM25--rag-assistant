[site]: crossvalidated
[post_id]: 281255
[parent_id]: 178504
[tags]: 
It depends on what kind of error you want to determine. Training data vs applying data differences A technique used to estimate the errors on the predictions is to train several algorithms using different random seeds. For most algorithms, this will lead to different predictions: the variation may gives you an estimate. Classification specific So in order to determine the classification error, there are roughly two methods: event by event : You can simply look at the predictions, create (for example) bins and divide label 1 by label 0. Because having 100 events of label 1 with a prediction between 0.6-0.65 and 50 with label 0 with a prediction in the same range simply yields a 2/3 chance for an event to be of class 1. Or, in other words, with a 1/3 change, your events in that bin are not class 1. Total efficiency : This approach is the one to use if it fits your case, it is more specific. You first determine where you apply your cut (meaning: what's the threshold on the predictions for an event to be class 1 or 0; this is usually not 0.5 but an optimized figure of merit). Let's say you cut on 0.9 (so class 0, else class 1). Then you can count: how many class 1 events are lost (lower then 0.9)? how many class 0 events are still in the sample? This gives you an estimation of the error on your classifiers output. Regression specific tag-and-probe : You can use known values, enter them and get their error. Then, you may assume that values in between two of those, roughly have the average error. Or in other words, you extrapolate the error from known values. Simple average : Simply take the average of the errors. If they are roughly equally distributed, this is a good way to go.
