[site]: crossvalidated
[post_id]: 157985
[parent_id]: 
[tags]: 
Neural network - binary vs discrete / continuous input

Are there any good reasons for preferring binary values (0/1) over discrete or continuous normalized values, e.g. (1;3), as inputs for a feedforward network for all input nodes (with or without backpropagation)? Of course, I'm only talking about inputs that could be transformed into either form; e.g., when you have a variable that can take several values, either directly feed them as a value of one input node, or form a binary node for each discrete value. And the assumption is that the range of possible values would be the same for all input nodes. See the pics for an example of both possibilities. While researching on this topic, I couldn't find any cold hard facts on this; it seems to me, that - more or less - it'll always be "trial and error" in the end. Of course, binary nodes for every discrete input value mean more input layer nodes (and thus more hidden layer nodes), but would it really produce a better output classification than having the same values in one node, with a well-fitting threshold function in the hidden layer? Would you agree that it's just "try and see", or do you have another opinion on this?
