[site]: datascience
[post_id]: 26659
[parent_id]: 26640
[tags]: 
It might be a problem of over-fitting, or that by just doing a single train / test split isn't giving a reliable estimate of the generalizable error of your SVM. I'd recommend using KFold validation to check. from sklearn.model_selection import KFold import numpy as np acc_score = [] kf = KFold(n_splits=5) for train_index, test_index in kf.split(X): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] svm_model.fit(X_train,y_train) predictions = svm_model.predict(X_test) acc_score.append(accuracy_score(predictions, y_test)) np.mean(acc_score) If the average is still 1.0 then you've done good, but my gut feeling is that your high score is dependent on the cut of the data you're looking at.
