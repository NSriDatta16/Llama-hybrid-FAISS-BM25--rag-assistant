[site]: stackoverflow
[post_id]: 3408733
[parent_id]: 3405805
[tags]: 
When you talk about scaling at the size of Facebook, is a whole different ball park. Latest estimates put Facebook datacenter at about 60000 servers (sixty thousand). Only the cache is estimated to be at about 30 TB (terabytes) ina a masive Memcached cluster . Although their back end is stil MySQL, is used as a pure key-value store, according to publicly available information : Facebook uses MySQL, but primarily as a key-value persistent storage, moving joins and logic onto the web servers since optimizations are easier to perform there (on the “other side” of the Memcached layer). There are various other technologies in use there: HipHop to compile PHP into native code Haystack for media (photo) storage BigPipe for HTTP delivery Cassandra for Inbox search You can also watch this year SIGMOD 2010 key address Building Facebook: Performance at big scale . They even present their basic internal API: cache_get ($ids, 'cache_function', $cache_params, 'db_function', $db_params); So if you connect the dots you'll see that at such scale you no longer talk about a 'big database'. You talk about huge clusters of services, key-value storage partitioned across thousands of servers, many technologies used together and so on and so forth. As a side note, you can also see a pretty good presentation of MySpace internals . Although the technology stack is completely different (Microsoft .Net and SQL Server based, with a huge emphasis on message passing via Service Broker ) there are similar points in how they approach storage. To sum up: application layer partitioning .
