[site]: crossvalidated
[post_id]: 544349
[parent_id]: 
[tags]: 
Cross-validation - does training loss matter?

Suppose you have two models $M$ and $N$ , with fixed hyperparameters. For example $M = \operatorname{Ridge}(\lambda = 20)$ and $N = \operatorname{Lasso}(\lambda =10)$ . You estimate their training and validation losses using k-fold cross validation on a data set. Let assume that the average validation losses are the same, but in training $M$ performs better than $N$ . In short, we have $$\mathcal{L}_\text{training} M My question is, is there any reason to prefer model $N$ in this case? $M$ has a larger train/validation gap and I feel this should be penalized. Is there any theoretical justification for this?
