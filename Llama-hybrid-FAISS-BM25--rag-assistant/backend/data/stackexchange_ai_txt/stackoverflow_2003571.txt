[site]: stackoverflow
[post_id]: 2003571
[parent_id]: 2003492
[tags]: 
I cannot comment on the write speed of arrays versus regular tables, but as far as i can tell, the design with 2 arrays will be pretty cumbersome for querying. I don't know about the performance of reading arrays either, but from what I can tell by looking at the documentation, all array acess is done through positional reference (indexes) so it will be a royal pain in the behind to find a particular measurement - you'd have to walk the name array to find the proper index, and then use that to find the value. I doubt it can be done in pure SQL, and it will probably require a user defined function. Now about the design with tables: you seem to be concerned about write speed. 24 mln components a day, that's 1 million rows per hour which is not that much. times 50, in a worst case, for the measurements, that's 51 million rows an hour, so less than 1 million rows per minute. I think this should be doable, although it would be advisable to batch inserts and avoid doing many single row inserts over many short lived transactions (better to insert them and commit in bunches of say 10.000 or 100.000). I do think you would need to also design an archiving and/or aggregation solution, because it doesn't seem very maintainable to keep inserting those volumes. I doubt it is useful too, but perhaps that s just me not understanding the purpose of this database. I mean, it seems unlikely to me that you want to be able to pinpoint an individial measurement of one component after say, 1 year aftre it was manufactured. Whereas it does seem useful to keep stats like average, min, max and stddev measurements over time. But perhaps you can explain a bit about this. Another thing that I thought of, is that it could help to store the raw measurement data first in a cheap and fast log (just text files say CSV format would do), and then use multiple readers to read them and insert them into the database. These readers could run in a fairly constant fashion. This would make the database less of a bottlenck, and make for a more robust system (assuming the chances that your log keeps working are higher than a database crash). Of course this approach is less suitable if you need realtime reporting from your db to monitor the process (although again, it seems very strange to me that you would need to do this on an individual component level)
