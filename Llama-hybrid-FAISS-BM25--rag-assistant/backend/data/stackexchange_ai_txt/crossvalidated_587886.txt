[site]: crossvalidated
[post_id]: 587886
[parent_id]: 587823
[tags]: 
EDIT: There is an elegant and useful way and a (at least) one clumsy one. The former is by whuber in his comment below which I copy here so it does not get overlooked in the comments. My own one follows below. Useful Rewrite the normal equations $$ X'X\hat\beta=X'y $$ as $$ X'X\hat\beta=X_1'y_1+X_2'y_2=X_1'X_1(X_1'X_1)^{-1}X_1'y_1+X_2'X_2(X_2'X_2)^{-1}X_2'y_2, $$ which equals $$ X'X\hat\beta=X_1'X_1\hat\beta_1+X_2'X_2\hat\beta_2, $$ Clumsy With $X:=\begin{pmatrix}X_1\\X_2\end{pmatrix}$ , $$ \hat\beta=(X_1'X_1+X_2'X_2)^{-1}(X_1'y_1+X_2'y_2) $$ Using https://math.stackexchange.com/questions/17776/inverse-of-the-sum-of-matrices , we may rewrite this as $$ \hat\beta=[(X_1'X_1)^{-1}-(X_1'X_1)^{-1}X_2'X_2(X'X)^{-1}](X_1'y_1+X_2'y_2) $$ Multiplying out gives $$ \hat\beta=\hat\beta_1+(X_1'X_1)^{-1}X_2'y_2-(X_1'X_1)^{-1}X_2'X_2\hat\beta $$ so that $$ \hat\beta=[I+(X_1'X_1)^{-1}X_2'X_2]^{-1}(\hat\beta_1+(X_1'X_1)^{-1}X_2'y_2). $$ If we want $\hat\beta_2$ to show up, we could rewrite $$ \begin{align*} \hat\beta&=[I+(X_1'X_1)^{-1}X_2'X_2]^{-1}(\hat\beta_1+(X_1'X_1)^{-1}(X_2'X_2)(X_2'X_2)^{-1}X_2'y_2)\\ &=[I+(X_1'X_1)^{-1}X_2'X_2]^{-1}(\hat\beta_1+(X_1'X_1)^{-1}(X_2'X_2)\hat\beta_2) \end{align*} $$ This is closed-form (unsurprisingly, since it just rewrites the closed-form full-sample OLSE), but probably not the type of result you had in mind?
