[site]: crossvalidated
[post_id]: 630528
[parent_id]: 625405
[tags]: 
"Despite this, data science seems to believe that ROC curves are problematic or illegitimate when the categories are imbalanced." This is because many in the data science community seem to think that class imbalance a an inherent problem, and ROC curves, and specifically the AUROC statistic "hide" the problem. The real problem is usually cost-sensitive learning. If your classifier classifies everything as belonging to the majority class, it may well be that is simply the optimal solution if the misclassification costs are equal. There is no class imbalance problem here, how can there be a problem if the classifier is behaving optimally for the question as posed? If this isn't acceptable for the practical application, it means that the minority class is "more important" in some sense than the majority class, so the practitioner should go and work out plausible values for the misclassification cost and build those into the classifier (preferably by using a probabilistic classifier and adjusting the threshold). ROC analysis can help with this (the slope of the tangent line to the curve gives the ratio of misclassification costs IIRC). The AUROC is a useful statistic where you are only interested in the ranking of patterns, perhaps because the misclassification costs are unknown or the operational class fequencies are unknown, and therefore you can't know the ideal threshold and hence can't use any statistic based on that threshold (such as accuracy or F1 or ...). We need to understand the problem we are trying to solve, and work out what we are really interested in, and then choose a suitable performance metric based on that (rather than focus on characteristics of the data, such as imbalance).
