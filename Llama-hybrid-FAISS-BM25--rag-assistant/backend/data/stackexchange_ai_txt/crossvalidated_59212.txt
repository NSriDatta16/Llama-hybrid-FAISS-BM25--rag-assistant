[site]: crossvalidated
[post_id]: 59212
[parent_id]: 
[tags]: 
When to aggregate for time series forecasting?

I have a some historical sales data for various product SKUs, including category information ("department" "category", "subcategory"). I want to use this to generate sales curve (a baseline forecast for future demand), no doubt using an appropriate exponential smoothing algorithm. The trouble is that the data for a particular SKU will often be too sparse to be able to infer a sales curve. I'm thinking that when that is the case, the sales data for the SKU could be aggregated with other SKUs for the same product and I could then use the sales curve for the product rather than the SKU (which would presumably be more reliable). If the product doesn't have enough sales history then I'd use the sales curve for the subcategory, and so on and so forthâ€¦ potentially even having to resort to a sales curve for the top level category (a department in this case) if insufficient sales data is available to generate reliable sales curves for lower level categories. What I'm wondering is, how do I determine when I have enough data to be able to trust a sales curve? How many sales "data points" do I need for a SKU or a subcategory in order to be able to trust its sales curve? Perhaps I'm approaching this the wrong way though. An alternative that I'd considered was to use data from all levels of the hierarchy in the forecast with appropriate weightings for each. So a product's future sales would be predicted by W1 * Forecast(Product) + W2 * Forecast(Subcategory) + W3 * Forecast(Category) + W4 * Forecast(Department) .... Working out the weightings might be a bit complex but not impossible (and might be done using a regression analysis). I'm sure this kind of problem has been solved before but I'm a bit of a stats newbie. Are there "off the shelf" algorithms/techniques that could use for this problem?
