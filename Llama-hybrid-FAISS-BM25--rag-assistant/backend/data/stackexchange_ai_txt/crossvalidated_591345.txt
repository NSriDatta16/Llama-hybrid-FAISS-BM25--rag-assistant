[site]: crossvalidated
[post_id]: 591345
[parent_id]: 438030
[tags]: 
One thing has to be said at the beginning RBF is not classification algorithm. It is used in SVM as a kernel, because it has properties of the kernel. SVM doeas all the work with finding hyperplane. I said that because you often mix them :"RBF - also tries to gues class". No, RBF only measures the similarity. Ad.1 Finding hyperplane in SVM is done by maximizing the margin between 'classes'. This is linear programming problem. As most problems are not separable in their original dimension, kernel trick is used. Kernel is projecting(automatically) data points to higher dimension where hyperplane can be found. RBF is used as a kernel function in SVM. Its great feature is that it is projecting data to infinite dimension. There you are finding the hyperplane separating classes, and project back to your dimension. Most importantly, to understand kernels, you have to be aware that kernel is measuring similarity between points. The greater the value, more similar points are. Gaussian RBF is doing great job in this. Ad.2 it is not cheating, as we are not using information from the target. Only data to build kernel is similarity between data points, but in higher dimension. Ad.3 You have great intuition. In fact SVM with Gaussian RBF can be called smooth KNN. It is because SVM+RBF is drawing Gaussian RBF around every data point, which is smooth bump with radius defined by gamma. While Knn is drawing sphere around every point with radius defined by distance to k nearest neighbors. This also clear when we look at math formula of SVM dual problem: and KNN: Ad.4 I don't understand this question. RBF is just a function. SVM uses it as a kernel, to 'send' points to infinite dimension(this is because Taylor expansion of this function is infinite, but can be easily calculated). Anyway, in your case, first thing you do is to calculate kernel matrix with GRBF: in python: K = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2: GRBF(x1, x2), 1, X), 1, X) When you plot the first row you will see that further points have less influence on the assigned label: If you recall the formula of SVM dual, you will see that those kerne values will be multiplied by target values y_i. That means, closer points have bigger impact on overall sign. And decision boundary will look like this:
