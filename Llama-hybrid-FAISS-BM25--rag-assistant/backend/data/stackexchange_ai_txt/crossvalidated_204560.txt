[site]: crossvalidated
[post_id]: 204560
[parent_id]: 142906
[tags]: 
Probably approximately correct (PAC) learning theory helps analyze whether and under what conditions a learner $L$ will probably output an approximately correct classifier. (You'll see some sources use $A$ in place of $L$.) First, let's define "approximate." A hypothesis $h \in H$ is approximately correct if its error over the distribution of inputs is bounded by some $\epsilon, 0 \le \epsilon \le \frac{1}{2}.$ I.e., $error_D(h)\lt \epsilon$, where $D$ is the distribution over inputs. Next, "probably." If $L$ will output such a classifier with probability $1 - \delta$, with $0 \le \delta \le \frac{1}{2}$, we call that classifier probably approximately correct. Knowing that a target concept is PAC-learnable allows you to bound the sample size necessary to probably learn an approximately correct classifier, which is what's shown in the formula you've reproduced: $$m \ge\frac{1}{\epsilon}(ln|H| + ln\frac{1}{\delta})$$ To gain some intuition about this, note the effects on $m$ when you alter variables in the right-hand side. As allowable error decreases , the necessary sample size grows. Likewise, it grows with the probability of an approximately correct learner, and with the size of the hypothesis space $H$. (Loosely, a hypothesis space is the set of classifiers your algorithm considers.) More plainly, as you consider more possible classifiers, or desire a lower error or higher probability of correctness, you need more data to distinguish between them. For more, this and other related videos may be helpful, as might this lengthy introduction or one of many machine learning texts, say Mitchell , for example.
