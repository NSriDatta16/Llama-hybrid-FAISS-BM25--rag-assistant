[site]: crossvalidated
[post_id]: 486351
[parent_id]: 
[tags]: 
Derivation of unbiased MLE for Gaussian variance

I'm currently studying ML basics with the book Introduction to Machine Learning (Ethem Alpaydin) and had a question regarding checking whether the maximum likelihood estimators (MLE's) for a Gaussian distribution's parameters are biased or not. We know that the MLE's for the mean $\mu$ and $\sigma^2$ are each: $$ \begin{align} m & = \frac{\sum_t x^t}{N} \\ s^2 & = \frac{\sum_t (x^t - m)^2}{N} \end{align} $$ for samples $\mathcal{X} = \left\{ x^t \right\}_{t=1}^N$ . We can check whether these estimators are biased or not by taking their expectations and seeing whether they equal the true values. This is easily verifiable for $m$ , but what I'm having trouble understanding is how the author goes about explaining the case for $s^2$ . More specifically: $$ \begin{align} s^2 & = \frac{\sum_t (x^t - m)^2}{N} = \frac{\sum_t (x^t)^2 - Nm^2}{N} \\ \mathbb{E}[s^2] & = \frac{\sum_t \mathbb{E}[(x^2)^2] - N \mathbb{E}[m^2]}{N} \\ & = \dfrac{N(\sigma^2 + \mu^2) - N\left(\dfrac{\sigma^2}{N} + \mu^2\right)}{N} \\ & = \left( \dfrac{N - 1}{N} \right)\sigma^2 \ne \sigma^2 \end{align} $$ What's confusing me is the first line. How does $\sum_t (x^t - m)^2 = \sum_t (x^t)^2 - Nm^2$ ? Please correct me if I'm mistaken, but I believe that: $$ \begin{align} \sum_t (x^t - m)^2 & = \sum_t ((x^t)^2 - 2mx^t + m^2) \\ & = \sum_t (x^t)^2 -2m\sum_t x^t + Nm^2 \end{align} $$ Not only is the sign for $Nm^2$ opposite, but I'm also wondering what happened to the second term $-2m\sum_t x^t$ . If someone could point out where I went wrong or explain why such a difference occurs that'd be greatly appreciated. Thanks!
