[site]: crossvalidated
[post_id]: 283007
[parent_id]: 
[tags]: 
If a neural network performs very well on training data, does that mean it is overfitting?

In a (including convolutional) neural network, suppose I get a fair accuracy (say, $\approx 80\%-90\%$) on my validation/test data, but extremely good accuracy ($\approx 100\%$) if I run it back on my training data. Does this necessarily mean that my network is overfitting, and I should apply some techniques (get more data, regularize, etc.) to handle it? In other words, can I conclude that my network is overfitting, or very highly likely overfitting? Are there other reasons why this can happen?
