[site]: crossvalidated
[post_id]: 120090
[parent_id]: 
[tags]: 
Best estimate of variation for duplicate data?

I'm simplifying a bit here, but my dataset contains 10 stations, each with measurements of growth at time increments. For each measurement, there are between 1-4 duplicates measured at the same day+station. The response data are roughly lognormal, and about 10% of the growth values are negative. There is a fair amount of error/high variability/heterogeneity in both the measurement technique and the natural system. The challenge is that for a subset of day+station (when there are at least duplicates), one of the individual measements appears to be an outlier when assessed graphically. What is then the best method to identify values (for each day+station) that have the highest (higher than normal) variation? Median average deviation (MAD) is the obvious candidate, but it is correlated with the number of duplicates - MAD decreases with the # of dupes. However, MAD does not 'respond' to outliers, so is not helpful in this case. Stdev of log+1 dataset has no real meaning for folks comparing two values. My goal is to identify the least repeatable measurements to flag as suspicious.
