[site]: datascience
[post_id]: 90284
[parent_id]: 90226
[tags]: 
BERT is a Transformer encoder, which is specifically based on attention layers, so you already have attention there, no need for extra attention. Actually, using just BERT is the typical approach for NER. No need for BiLSTMs on top, or CRFs. I suggest you start with standard approaches, like finetuning BERT. For that, there are libraries like tner , which is built on top of HuggingFace Transformers , that makes it really easy to finetune NER models.
