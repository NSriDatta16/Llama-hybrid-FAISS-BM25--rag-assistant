[site]: crossvalidated
[post_id]: 162798
[parent_id]: 144206
[tags]: 
Summary: the corrected parameters are for predicting as a function of the true predictor $x$. If $\tilde{x}$ is used in prediction, the original parameters perform better. Note that there are two different linear prediction models lurking around. First, $y$ given $x$, \begin{equation} \hat{y}_x = \beta\,x + \alpha, \end{equation} second, $y$ given $\tilde{x}$, \begin{equation} \hat{y}_{\tilde{x}} = \tilde{\beta}\,\tilde{x} + \tilde{\alpha}. \end{equation} Even if we had access to the true parameters, the optimal linear prediction as a functions of $x$ would be different than the optimal linear prediction as a function of $\tilde{x}$. The code in the question does the following Estimate the parameters $\hat{\tilde{\beta}},\hat{\tilde{\alpha}}$ Compute estimates $\hat{\beta},\hat{\alpha}$ Compare performance of $\hat{y}_1 = \hat{\beta}\,\tilde{x} + \hat{\alpha}$ and $\hat{y}_2 = \hat{\tilde{\beta}}\,\tilde{x} + \hat{\tilde{\alpha}}$ Since in step 3 we are predicting as a function of $\tilde{x}$, not as a function of $x$, using (estimated) coefficients of the second model works better. Indeed, if we had access to $\alpha$, $\beta$ and $\tilde{x}$ but not $x$, we might substitute a linear estimator of $x$ in the first model, \begin{equation} \hat{\hat{y_x}} = \beta\,\hat{x}(\tilde{x}) + \alpha = \beta\, (\mu_x + (\hat{x}-\mu_x)\,\frac{\sigma^2_{x}}{\sigma^2_{\tilde{x}}}) + \alpha = \frac{\sigma_x^2}{\sigma^2_{\tilde{x}}}\beta + \alpha - \beta(1-\frac{\sigma_x^2}{\sigma^2_{\tilde{x}}})\mu_x. \end{equation} If we first perform the transformation form $\tilde{\alpha},\tilde{\beta}$ to $\alpha,\beta$ and then do the computation in the latest equation, we get back the coefficients $\tilde{\alpha},\tilde{\beta}$. So, if the goal is to do linear prediction given the noisy version of the predictor, we should just fit a linear model to the noisy data. The corrected coefficients $\alpha,\beta$ are of interest if we are interested in the true phenomenon for other reasons than prediction. Testing I edited the code in OP to also evaluate MSEs for predictions using the non-noisy version of the prediction (code in the end of the answer). The results are Reg parameters, noisy predictor 1.3387 1.6696 2.1265 2.4806 2.5679 2.5062 2.5160 2.8684 Fixed parameters, noisy predictor 1.3981 2.0626 3.2971 5.0220 7.6490 10.2568 14.1139 20.7604 Reg parameters, true predictor 1.3354 1.6657 2.1329 2.4885 2.5688 2.5198 2.5085 2.8676 Fixed parameters, true predictor 1.1139 1.0078 1.0499 1.0212 1.0492 0.9925 1.0217 1.2528 That is, when using $x$ instead of $\tilde{x}$, the corrected parameters indeed beat the uncorrected parameters, as expected. Furthermore, the prediction with ($\alpha,\beta,x$), that is, fixed parameters and true predictor, is better than ($\tilde{\alpha},\tilde{\beta},\tilde{x}$), that is, reg parameters and noisy predictor, since obviously the noise harms the prediction accuract somewhat. The other two cases correspond to using the parameters of a wrong model and thus produce weaker results. Caveat about nonlinearity Actually, even if the relationship between $y,x$ is linear, the relationship between $y$ and $\tilde{x}$ might not be. This depends on the distribution of $x$. For example, in the present code, $x$ is drawn from the uniform distribution, thus no matter how high $\tilde{x}$ is, we know an upper bound for $x$ and thus the predicted $y$ as a function of $\tilde{x}$ should saturate. A possible Bayesian-style solution would be to posit a probability distribution for $x$ and then plug in $\mathbb{E}(x \mid \tilde{x})$ when deriving $\hat{\hat{y}}_x$ - instead of the linear prediction I used previously. However, if one is willing to posit a probability distribution for $x$, I suppose one should go for a full Bayesian solution instead of an approach based on correcting OLS estimates in the first place. MATLAB code for replicating the test result Note that this also contains my own implementations for evaluate and OLS_solver since they were not given in the question. rng(1) OLS_solver = @(X,Y) [X ones(size(X))]'*[X ones(size(X))] \ ([X ones(size(X))]' * Y); evaluate = @(b,x,y) mean(([x ones(size(x))]*b - y).^2); reg_mse_agg = []; fixed_mse_agg = []; reg_mse_orig_agg = []; fixed_mse_orig_agg = []; varMult = 1; numTests = 60; for dataNumber=1:8 reg_mses = []; fixed_mses = []; reg_mses_orig = []; fixed_mses_orig = []; X = rand(1000,1); X(:,1) = X(:,1) * 10; X(:,1) = X(:,1) + 5; varX = var(X); y = 0.5 * X(:,1) -10; y = y + normrnd(0,1,size(y)); origX = X; X = X + normrnd(0,dataNumber * varMult ,size(X)); train_size = floor(0.5 * length(y)); for t=1:numTests, idx = randperm(length(y)); train_idx = idx(1:train_size); test_idx = idx(train_size+1:end); Xtrain = X(train_idx,:); ytrain = y(train_idx); Xtest = X(test_idx,:); origXtest = origX(test_idx,:); ytest = y(test_idx); b = OLS_solver(Xtrain, ytrain); %first arg of evaluate returns MSE, working correctly. reg_mse = evaluate( b,Xtest,ytest); reg_mses = [reg_mses ; reg_mse]; varInd = var(Xtrain); varNoise = varInd - varX; bFixed = [0 0]'; bFixed(1) = b(1) * varInd / varX; bFixed(2) = mean(ytrain - bFixed(1)*Xtrain); fixed_mse = evaluate( bFixed,Xtest,ytest); fixed_mses = [fixed_mses ; fixed_mse]; reg_mse_orig = evaluate(b, origXtest, ytest); reg_mses_orig = [reg_mses; reg_mses_orig]; fixed_mse_orig = evaluate(bFixed, origXtest, ytest); fixed_mses_orig = [fixed_mses_orig; fixed_mse_orig]; end reg_mse_agg = [reg_mse_agg , reg_mses]; fixed_mse_agg = [fixed_mse_agg , fixed_mses]; reg_mse_orig_agg = [reg_mse_orig_agg , reg_mses_orig]; fixed_mse_orig_agg = [fixed_mse_orig_agg , fixed_mses_orig]; end disp('Reg parameters, noisy predictor') disp(mean(reg_mse_agg)) disp('Fixed parameters, noisy predictor') disp(mean(fixed_mse_agg)) disp('Reg parameters, true predictor') disp(mean(reg_mse_orig_agg)) disp('Fixed parameters, true predictor') disp(mean(fixed_mse_orig_agg))
