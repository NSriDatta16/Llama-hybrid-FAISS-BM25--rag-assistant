[site]: crossvalidated
[post_id]: 155997
[parent_id]: 155461
[tags]: 
First, I address why different random forest runs produce different results. The culprit is at least 1 of the following: Random forest, as the name suggests, is inherently random. At each tree, RF bootstraps your data. At each split, RF is taking a sample of $m$ candidate features and selects the best feature to split on from among those $m$. Given the same data, two separate runs of RF will construct different bootstrap samples, produce different splits and hence different results with high probability. Cross-validation is also random. Randomly allocating your data to $k$ different folds can have many different realizations, so there will be fluctuations in your results based purely on how the data were split. The solution to this is to not use an optimization algorithm which assumes a deterministic function. Standard optimization methods are sub-optimal when the output is a random variable. Instead, I suggest exhaustively trying weights over a grid. Alternatives do exist, however. Optunity is a software package that advertises improvements in hyperparameter tuning for some problems; I've never used it myself, so I don't know if it is well-suited to this problem. On the other hand, it may be possible to set the random seed to a specific value and therefore make repeatable the random component of the analysis. Caveat: I'm not familiar with Python. We can still find an optimal tuple of hyperparameters, however. In general, we first select some statistic that we're interested in optimizing, e.g. entropy. Then we compute out-of-sample entropy for your data, given a hyperparameter tuple. Record these results over several realizations of CV. Once we have these results, standard practice is to select the most regularized model within 1 standard error of the minimum entropy found for some tuple. In the case where one is simply optimizing mtry , it manifests as selecting the smallest mtry within one standard error of the minimum. In this context of weighted observations, I'm not sure what "most regularized" means for the weight component. It's possible that you will find a region of the hyperparameter surface which is relatively flat, so anything in that vacinity may be acceptable.
