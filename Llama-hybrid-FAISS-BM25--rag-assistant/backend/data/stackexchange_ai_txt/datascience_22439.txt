[site]: datascience
[post_id]: 22439
[parent_id]: 
[tags]: 
Why Restricted Boltzmann Machine(RBM) is better than random initiating?

To train a Deep neural network, why training Restricted Boltzmann Machine(RBM) layer-wise first and then apply BP algorithm through the network? Comparing to random initiating all the weights+bias in all layers and then apply BP algorithm, is there a quantitative reason(proof) that pretrained RBM is better than random weights assignment? I found a lot of case(datasets) studies about these two training methods saying RBM is better than random. (Some documents argued that RBM also could help avoiding local minimizers when running gradient descent, but didn't offer any proof)
