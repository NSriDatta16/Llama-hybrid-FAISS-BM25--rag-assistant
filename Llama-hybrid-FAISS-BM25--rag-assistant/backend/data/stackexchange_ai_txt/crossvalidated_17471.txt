[site]: crossvalidated
[post_id]: 17471
[parent_id]: 17461
[tags]: 
Simply said: there's more than one way to skin a cat. Logistic regression and regression trees are simply different algorithms that may pick up different variables as more or less important, because they are used in a different way. Simply said, in logistic regression, the influence of all variables is studied as they work simultaneously, while trees look at the influence of the variables one at a time (more or less). If a combination of, say, 2 variables is very indicative of the result, but each individual variable is not, this will typically not be picked up in regression tree type settings. The easiest example of this is: out pred1 pred2 1 1 1 1 0 0 0 1 0 0 0 1 You can easily see that both pred1 and pred2 are completely uninformative for out, but the combination of the two is completely informative (out is 1 if both variables are the same). I doubt this will get picked up in regression trees, but logistic regression (well, a penalized version of it, but that's another story) would. I'm confident that examples can be constructed where regression trees would have the benefit (probably where two variables are highly correlated). So in short: this probably mainly depends on the correlation structure of your predictors. The rule of thumb for these matters is: if you're looking for association (statistical significance), avoid correlation between your predictors. If you're looking for a model with high predictive value: never mind (but be aware that you may be overfitting).
