[site]: datascience
[post_id]: 108488
[parent_id]: 108472
[tags]: 
The input and output length are totally independent. Unlike sequence labeling problems, where we assign one label to each of the input states, encoder-decoder models do not have this limitation. Each decoder state (and there are as many decoder states as output tokens) queries the encoder states and no matter how many encoder states there are, the decoder receives one context vector (a weighted average of values + perhaps some projections). At training time, we know all the decoder states (i.e., queries) in advance, so we can query the encoder states in parallel and receive one context vector per decoder state. At inference time, this needs to be done step by step: the decoder generates a token, the token goes to the decoder input, it generates a new state, the state is used to query the encoder, encoder returns one context vector (multiple times in multilayer decoders), the decoder generates a new token. This is iterated until the decoder generates the end-of-sentence token.
