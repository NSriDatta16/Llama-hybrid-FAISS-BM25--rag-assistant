[site]: crossvalidated
[post_id]: 562257
[parent_id]: 
[tags]: 
Convergence rates of Quasi-Newton methods vs. Gradient Descent

I am interested in learning about if Optimization Algorithms that use both First Derivative Information and Second Derivative Information have any advantages (e.g. strength of Convergence) when compared to Optimization Algorithms that only use First Derivative Information. As a quick summary: Gradient Descent based algorithms try to find the minimum of the function they are optimizing by repeatedly using information obtained from the First Derivatives Quasi Newton Methods (e.g. BFGS) tries to find the minimum of the function by repeatedly using/approximating information obtained from the Second Derivatives Thus, it is almost a logical conclusion to believe that for the exact same (convex) function, Gradient Descent would on average require fewer calculations per iteration when compared to Quasi Newton Methods. But in the spirit of "tradeoffs" - I am interested in knowing if there are any theoretical results that show Quasi-Newton Methods gain stronger convergence rates compared to Gradient Descent based algorithms. For instance, since information about the second derivative gives the optimization algorithm additional knowledge about the function's local curvature - in theory, we would expect this additional knowledge would "better lead" the optimization algorithm to the minimum when compared to information only about the first derivative (especially in higher dimensional functions). Or is this additional second derivative information gained by Quasi-Newton Methods generally negligible compared to the first derivative information, and as a result, there is no general "strength in convergence" obtained by Quasi-Newton Methods when compared to Gradient Descent? Thus: Are there any theoretical results that compare the "strength of convergence" for Quasi-Newton Methods vs. Gradient Descent Based Algorithms? Can someone please comment on this? Thanks!
