[site]: crossvalidated
[post_id]: 543888
[parent_id]: 
[tags]: 
Categorical features driven perfect collinearity in logistic regression

In OLS regression, when one has a set of sum-to-one dummies one needs to take one out to avoid perfect collinearity (be able to invert X^TX), in logistic regression as far as I understand one does not need to invert X^TX, so not obvious to me that one needs to do the same, was able to setup an optimizer to solve for a toy example (y ~ intercept + dummy_1 + dummy_2), with the estimated parameters in hand could fit the probabilities and corroborate they match the event rates by dummy group, the parameters on dummy_1 and dummy_2 have the convenience that one group has a positive coefficient and one has negative (given the intercept) Question: Is it a bad idea to include a perfect collinear design matrix due to dummy variables in logistic regression? The main reason to set up the problem like this is coefficient interpretation and significance (real use case includes multiple sets of sum-to-one dummies with many categories) am not interested in testing significance relative to an arbitrarily chosen reference category
