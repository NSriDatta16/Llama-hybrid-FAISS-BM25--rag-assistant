[site]: crossvalidated
[post_id]: 472342
[parent_id]: 
[tags]: 
Use of the term 'dimension' in word-embeddings or various other tensors in AI

I've noticed that AI community refers to various tensors as 512-d, meaning 512 dimensional tensor, where the term 'dimension' seems to mean 512 different float values in the representation for a single datapoint. e.g. in 512-d word-embeddings means 512 length vector of floats used to represent 1 english-word e.g. https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6 But it isn't 512 different dimensions, it's only 1 dimensional vector? Why is the term dimension used in such a different manner than usual? When we use the term conv1d or conv2d which are convolutions over 1-dimension and 2-dimensions, a dimension is used in the typical way it's used in math/sciences but here a 1-d vector is said to be a 512-d vector, or am I missing something? Why is this overloaded use of the term dimension ? What context determines what dimension means in machine-learning as the term seems overloaded?
