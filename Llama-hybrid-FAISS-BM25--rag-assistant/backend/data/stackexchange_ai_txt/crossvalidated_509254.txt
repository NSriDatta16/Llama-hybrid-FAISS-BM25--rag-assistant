[site]: crossvalidated
[post_id]: 509254
[parent_id]: 
[tags]: 
What is the best way to eliminate neutral words in a text classifier?

I'm creating a news classifier using the reuters dataset. Right now I'm in the process of preparing the dataset for training. First I removed all punctuation, numbers and special characters and after a created a list with all the unique words in every piece of news. To improve training I want to remove neutral words that won't affect classification, like "the", "of", "a" , etc. What I tried doing is, get the ratio of the ocurrences of one word in one text with the total number of words in that particular text, then to that for every text and take the average of those ratios and then choose a certain threshold to remove a word from the vocabulary, but this way it takes too long. Is there a better way? This is a multi-class problem.
