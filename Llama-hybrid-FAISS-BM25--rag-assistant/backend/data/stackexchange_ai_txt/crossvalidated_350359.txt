[site]: crossvalidated
[post_id]: 350359
[parent_id]: 198463
[tags]: 
I've trained an all-cnn-c model. On CIFAR-10, the test accuracy is 91.97% (there was a checkpoint with over 92%), and test loss is 0.4654. The architecture is almost the same as described in the paper, except that they didn't mention whether or not batch normalization was applied. FYI: Batch Normalization : After the ReLUs of all convolutional layers. DropOut : drop_rate=0.2 after input layer drop_rate=0.5 after every convolutional layer with strides=2 Batch Size : 128 Initial Learning Rate : 0.01 Learning Rate Decay Scale : 0.1 , applied after [200, 250, 300] epochs. Initialization for convolutional layers : He Initialization. Weight Decay : L2-Regularization with scale=0.001 Optimizer : momentum optimizer with momentum=0.9 , use_nesterov=True Data Augmentation : generate augmented data randomly using keras.preprocessing.image.ImageDataGenerator(zoom_range=[0.8,1.2], rotation_range=15, width_shift_range=.17, height_shift_range=.17, horizontal_flip=True) Cross-Validation : Not in use.
