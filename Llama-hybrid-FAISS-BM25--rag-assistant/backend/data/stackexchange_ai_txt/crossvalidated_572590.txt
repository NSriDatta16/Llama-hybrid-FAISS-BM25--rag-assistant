[site]: crossvalidated
[post_id]: 572590
[parent_id]: 
[tags]: 
Hyperparameters chosen by CV on train dataset don't perform well on validation/test dataset

I've used the following objective function to assess best hyper-parameters using Hyperopt(): # Base optimizer with input hyperparameters on Tree based encoding and Logistic Regression def mxLogRegGiniComb(cb_depth = 1, max_depth=5, min_rr_delta=0.002, min_samples_MP=50, min_samples_leaf=0.03, check_rr_leaves=True, check_monotony=False, C=1, corr_threshold=0.9, **kwargs): from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.model_selection import cross_val_score # Tree based variable encoding vars_comb = tl.cxVarCombs(cb_depth, max_depth, min_rr_delta, min_samples_MP, min_samples_leaf, check_rr_leaves, check_monotony) vars_comb.fit(X_train, y_train, time_train) # WOE encoded tree into numeric variable X_train_enc_woe = pd.DataFrame(index=X_train.index) for key, clf in vars_comb.dic_clf.items(): if clf.dic_leaves is not None: X_train_enc_woe[str(key)+'_tree_enc'] = clf.encoding_class(X_train, 'woe') # Remove duplicated columns X_train_enc_nodup_woe = X_train_enc_woe.T.drop_duplicates().T # Remove very highly correlated variables high_cor_woe = mxHighCorr(X_train_enc_nodup_woe, corr_threshold) to_drop = [] for i in range(len(high_cor_woe)): if (i == 0) & (high_cor_woe.loc[i, 'corr'] > corr_threshold): to_drop.append(high_cor_woe.loc[i, 'var1']) #if variables that causes the high correlation has not already been deleted elif (high_cor_woe.loc[i, 'var2'] not in to_drop) & (high_cor_woe.loc[i, 'corr'] > corr_threshold): to_drop.append(high_cor_woe.loc[i, 'var1']) to_drop = list(set(to_drop)) to_keep = X_train_enc_nodup_woe[X_train_enc_nodup_woe.columns.difference(to_drop)].columns.to_list() # Logistic Regression model with LASSO feature selection log_reg = LogisticRegression(penalty='l1', solver='liblinear', C=C, fit_intercept=False) try: log_reg.fit(X=X_train_enc_nodup_woe[to_keep], y=(1-y_train['MP']).ravel()) except Exception: pass # CV Evaluation of performance with constraint of coefficients consistency score = 0 try: if all(log_reg.coef_[0] > 0): clf = LogisticRegression(penalty='l1', solver='liblinear', C=C, fit_intercept=False) roc_auc = cross_val_score(clf, X_train_enc_woe, (1-y_train['MP']).ravel(), scoring='roc_auc', cv=10).mean() score = 2*roc_auc-1 except Exception: pass # Send the current training score return score, vars_comb, log_reg, to_keep, X_train_enc_nodup_woe # Final optimizer def mxOptimizer(params): score, vars_comb, log_reg, to_keep, X_train_enc_nodup_woe = mxLogRegGiniComb(**params) tune.report(mean_gini=score) The search space is as follows: # Establish hyper-parameter search space import ray.tune as tune search_space = { "cb_depth": tune.choice([2]), "check_monotony": tune.choice([False]), "min_rr_delta": tune.quniform(0.001, 0.02, 0.0005), "C": tune.quniform(0.0001, 0.01, 0.00005), "corr_threshold": tune.quniform(0.5, 1, 0.05), } And the execution is done by the following function: search_alg = HyperOptSearch() analysis = tune.run( mxOptimizer, mode="max", metric="mean_gini", num_samples=50, search_alg=search_alg, scheduler=ASHAScheduler(), config=search_space, progress_reporter=tune.JupyterNotebookReporter(overwrite=True) ) The idea is to assure global model with all Logistic Regression coefficients being positive and maximizing global Gini. As a result I have a Gini of 0.55 with the following hyper-parameters: Current best trial: 4435e374 with mean_gini=0.552077644083361 and parameters={'cb_depth': 2, 'check_monotony': False, 'min_rr_delta': 0.0125, 'C': 0.00475, 'corr_threshold': 0.5} Though, when using the hyper-parameters to construct a model on global train data and evaluate on outside validation and test data I have very poor results of 0.47 and 0.48 Gini respectively. I suppose that there must be over-fitting. Any idea on how I can have similar results on train and validation/test will be very appreciated.
