[site]: datascience
[post_id]: 13457
[parent_id]: 12797
[tags]: 
We would need more information on the prediction problem and the features to be able to give something more precise. Anyhow, I am surprised no answer so far included all possible options since they aren't that many: get rid of incomplete observations or features --- obviously, only viable if there are few incomplete cases since you lose too much information otherwise replace NAs with some value like -1 --- this depends on the classifier you use; if your classifier supports categorical variables, you can create a new category for those NAs for example. In some continuous variables, sometimes there are some values that make sense (for instance, in text mining classification, if you have a title-length feature but you have no title, it might make sense to replace with title-length=0 ) fill up the missing data This last point encompasses too many things: replace NAs with the median (this is the usual lazy approach; sklearn has a class for this) if time series, replace with an average of the previous and following values -- in pandas, this can be done using DataFrame.resample() . use the $k$ closest neighbors. build a KNN model using the other variables and then do the average of those neighbors (if you use euclidean distance, you probably should normalize first). I never seen this done, but you probably could try predicting the missing NAs using another model as well. But all this depends very much on what you are doing. For instance, if you have performed clustering analysis and you know your data is made up of clusters, you could use the median within each cluster. Possibly other solutions could include things like multimodal or multiview models. These are recent techniques that can cope with missing modalities, and you can see a feature, or subset of features, as a modality. For instance, you could build a different classifier for various subsets of your features (using the complete cases in each of those subsets) and then build another classifier on top of that to merge those probabilities. I would only try these techniques if most of your data is missing. There are more advanced deep learning versions of this using autoencoders.
