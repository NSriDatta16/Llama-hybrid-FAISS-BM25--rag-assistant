[site]: crossvalidated
[post_id]: 604637
[parent_id]: 602925
[tags]: 
The OP is absolutely correct that assigning cluster labels without a way to interpret their real-world relevance is often a pointless exercise. The simplest technique to identify those differences (aside from doing EDA ) is to run a classifier where cluster labels are treated as class membership. Starting off, I would recommend a relatively "straight-forward" algorithm like logistic regression or a moderately-sized decision tree. In that way, the feature importance and interpretation are reasonably straightforward to estimate. Notably, if the result of this "classification" task is bad, then we have a good idea that the cluster assignment at hand most likely is not very interpretable or well-defined (cause if it was, the learning algorithms would have picked). There are some other metrics like cluster cardinality and cluster magnitude to assess the "quality of clustering" but I cannot recommend them, I have found them often useless and totally arbitrary. While not commented on in the answer, if the dimensionality of the points we are trying to cluster is "large" (e.g. 10+) the concept of a neighbourhood becomes somewhat ill-defined in which case we should re-work our problem formulation as well as the feature space we are working with. (See issues with curse of dimensionality ; CV.Se has some excellent threads on this too: eg. " Why is Euclidean distance not a good metric in high dimensions? " , " How do I know my k-means clustering algorithm is suffering from the curse of dimensionality? " )
