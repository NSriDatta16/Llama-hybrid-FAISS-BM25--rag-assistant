[site]: crossvalidated
[post_id]: 358047
[parent_id]: 
[tags]: 
Why does Off-Policy Monte Carlo Control only learn from the "Tails of Episodes"?

I was reading through section 5.7 of the second edition of Sutton and Barto's "Reinforcement Learning: An Introduction" when I came across this passage: where the "method" that the author is referring to is the incremental, every-visit implementation for Off-Policy MC Control with weighted importance sampling covered earlier in the section and provided here for reference: My question is this: why does the method above only learn from the tails of the episode, as the author suggests in the excerpt? After searching around a bit, the only relevant link I could find online was here: http://incompleteideas.net/sutton/book/first/5/node7.html which is an online version of the first edition of the text. An excerpt from this link clarifies that what the author means by "the tails of episodes" is in fact "after the last nongreedy action." However, I still fail to see why all of the remaining actions after some point in an episode are guaranteed to be greedy, or how these greedy actions belong to the only time steps from which the above method can learn.
