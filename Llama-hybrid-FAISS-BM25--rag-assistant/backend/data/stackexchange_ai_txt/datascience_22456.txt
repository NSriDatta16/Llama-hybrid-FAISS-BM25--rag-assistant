[site]: datascience
[post_id]: 22456
[parent_id]: 
[tags]: 
how can I solve label shape problem in tensorflow when using one-hot encoding?

I used tensorflow to recognize text from natural images by using convolutional neural network; there is no specific number of characters in the text. To make a successful training I should convert the categorical labels into binary using one-hot encoding. So, for each label, I used integer encoding for each character and stored them in one numpy array in order to create TFRecords. For example: alphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ' TrainLabel = ["CNN in Tensorflow"] # define a mapping of chars to integers char_to_int = dict((c, i) for i, c in enumerate(alphabet)) integer_encoded = [char_to_int[char] for char in TrainLabel[0]] if (len(TrainLabel[0])) 51 is the maximum number of character in the text, so if the text has less than 51 characters, pad it to 51 characters with spaces. If we print the label, it will be like this:: array([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 1.], [ 0., 0., 0., ..., 0., 0., 1.], [ 0., 0., 0., ..., 0., 0., 1.]], dtype=float32) after creating the batch queue, the label has shape [batch_size, 2703] . 2703 is come from 51*53 which 53 is the number of classes My problem is in loss function:: the label shape in tf.nn.sparse_softmax_cross_entropy_with_logits() must be [batch_size], but the label that I used here has this shape [batch_size, 53] because I used one-hot encoding ? How can I deal with that?? This is the problem:: (labels_static_shape.ndims, logits.get_shape().ndims)) ValueError: Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2).
