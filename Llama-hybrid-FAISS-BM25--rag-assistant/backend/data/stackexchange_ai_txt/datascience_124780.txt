[site]: datascience
[post_id]: 124780
[parent_id]: 
[tags]: 
Assign layers and weights in BERT

I print the weight names and shape of the BERT transformer. Now, I want to assign the printed weight to the layers in the transformers architecture: In the following, I can assign query, key and value: But, in the print there are attention.output.dense.weight and attention.output.LayerNorm.weight, where can I find it in the architecture of transformer/attention-head? Further there is an intermediate.dense.weight and there output.dense.weight and output.LayerNorm.weight. Are they parts of "Add & Norm" after the multi-head-attention blocks?
