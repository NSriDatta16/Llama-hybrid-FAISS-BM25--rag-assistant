[site]: crossvalidated
[post_id]: 505142
[parent_id]: 221513
[tags]: 
RNN is a time based neural network.. at the end of time steps ( length of the input) it forms a vector which represents a thought preserving sequence information across the time. Thinking of thought vector like some sort of figure or object might help, which gets it's proper shape (depending on the input sequence) through time steps depending on the inputs it see's each time. The weight matrices are initialized randomly first, If we take example as predicting the next letter using RNN, when we send the first letter and the network predicts the next letter by assigning probabilities to each possible letters. we can update the weights using the gradients in that timestep. same goes for all the letters until the word ends. At the end the weights gets updated in such a way that, it would increase the confidence ( probability ) of finding the right word which can be achieved through backpropagation. This Training process continues for large number of data, thus tuning the weight parameters in such a way that given the sequence seen so far and this is the current state then this particular letter/ word ( in case of machine translations ) has the high probability of occurring. Weight Sharing across the time stamps thus helps in understanding the sequence as well as in applied point of view reduces the training time.
