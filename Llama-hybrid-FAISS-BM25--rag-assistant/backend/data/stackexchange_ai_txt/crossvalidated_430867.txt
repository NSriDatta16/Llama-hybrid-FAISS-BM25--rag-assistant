[site]: crossvalidated
[post_id]: 430867
[parent_id]: 14588
[tags]: 
That depends on your data: Do the missing data occur randomly, or is "missing" related to some other variable? For example, in one of my problems I deal with student success. If a student is dismissed after the first semester, then data for the following semester are missing. However, it is very unlikely that the average grades of these students, had they been allowed to continue, in the following semesters would have been the same as the average grade of those students that passed first semester. Thus, the data are missing non-randomly and need extra careful handling in order to avoid bias. How much co-linearity is in your data, in other words, what is the rank of the data matrix? PCA will tell you that, and can also be used to reduce dimensionality if needed. For example, in a recent problem I had 170 variables, but only 24 significant eigenvalues. In such cases, it makes sense to do further analysis with the scores on these 24 components. PCA is in my experience totally unsuitable for nominal data, but works with ordinal and binary, and of course shines with cardinal. For cluster analysis, mixed data types can be handled using Gower 's universal similarity coefficient (DOI:10.2307/2528823). That works even with nominal data. Podani has published a modification of $S_g$ that uses the information in ordinal data better (DOI:10.2307/1224438). This coefficient also handles missing data (in effect by pairwise deletion, so that all available information is used). It has been shown that $S_g$ handles up to 25% missing data without too much bias ( https://www.raco.cat/index.php/Questiio/article/viewFile/143135/194807 ). Imputation by regression from existing data has also been tried with success ( https://www.researchgate.net/profile/Olivier_Gauthier2/publication/225554456_Missing_data_in_craniometrics_A_simulation_study/links/0c96052e276f03b78a000000.pdf )
