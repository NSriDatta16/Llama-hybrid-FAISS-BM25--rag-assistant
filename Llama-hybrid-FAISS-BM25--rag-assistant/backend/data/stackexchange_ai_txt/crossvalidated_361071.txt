[site]: crossvalidated
[post_id]: 361071
[parent_id]: 360876
[tags]: 
The standard 1 way to perform classification with neural networks is to use sigmoid activation function and binary cross-entropy loss for single binary output, and linear activation followed by exponential normalization (softmax) and multinomial cross-entropy for one-hot binary output. There are good reasons why people use these. If you want to use ReLU and mean squared error, you have to be prepared that things are probably not going to work optimally. But in principle it does not matter how you encode your values, your network should learn to predict them. It could be 0 and 1, or 0 and 34, or anything else. You cannot, however, expect the outputs of the network to be bounded within this range for arbitrary input. If that is among your requirements, use bounded activation function . 1 Understand as meaningful, accepted, working.
