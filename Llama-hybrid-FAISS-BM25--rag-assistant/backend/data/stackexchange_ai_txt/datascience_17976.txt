[site]: datascience
[post_id]: 17976
[parent_id]: 9731
[tags]: 
In the simplest view, maybe this will suffice: the backward() method is used for training a neural network with backpropagation; compute the output y given input x using your network's forward() method, then find out the error of the output from your target using the criterion you defined (e.g. negative log likelihood etc.). Now, if there were only one layer of network, then you could simply use those errors between the output layer and the target to update the weights in that single layer. When you have more than one layer (or more complex structure), you could update layers one at a time, each time computing the error in that layer (no longer the output layer), then using that error to update weights for a previous layer. That is backpropogation. For this, you obviously need some way to map the error in the output $\Delta y$ onto $\Delta x$ using the same state (a.k.a. model state and input $x$). Thus, the backward() method is essentially in the form: $$f : (x, \Delta y) \rightarrow \Delta x$$ For the same of completeness, a forward() method can be represented as: $$f:x \rightarrow y$$ Alternatively, if the previous state of y persists, then all you need to calculate is $\Delta y$, thus, equivalently speaking, a forward() can also be represented as: $$ f: (x, y) \rightarrow \Delta y$$ This form can be easily compared to the backward() method, and it is easy to see why it is called as such.
