[site]: datascience
[post_id]: 33490
[parent_id]: 33487
[tags]: 
Notice that random forests (and decision trees in general) do not assume that the given features are independent. On the contrary, a typical classification path from the root to a leaf in one particular tree/classifier in the random forest would be e.g. to apply a different rule on the successes feature, based on the value of the attempts feature. So as one comment suggests, the algorithm will be able to identify certain dependencies. However, you need to keep in mind that decision trees (and in certain sense random forests as a consequence) define only linear separations between classes. Thus, to enhance the domain space, you might want to try to "hint" to the algorithm some additional meta-features, and possible semantics among the features. For example, have you considered also introducing success ratio (successes divided by attempts) as an additional feature? Notice that additional features are not guaranteed to help, even if the easiest way to know is to try them. The reason for this is that the algorithm might already be able to learn the additional semantics you give it. Having said that, to me it is not obvious that a random forest would be able to "learn" a feature like success ratio.
