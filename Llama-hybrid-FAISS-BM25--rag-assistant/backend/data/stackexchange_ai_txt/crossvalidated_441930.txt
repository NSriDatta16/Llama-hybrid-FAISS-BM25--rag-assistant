[site]: crossvalidated
[post_id]: 441930
[parent_id]: 
[tags]: 
Principal Component Analysis on Numerical Predictors alone for Dimension Reduction

I'm trying to reduce the number of dimensions for this 'Network Anamoly Detection' dataset: https://www.kaggle.com/anushonkar/network-anamoly-detection The dataset has a total of 40 features out of which 32 are numeric and the rest are factors. I tried using Factor Analysis of Mixed Data (FAMD), Multiple Correspondence Analaysis (MCA), PCAmix (Package ‘PCAmixdata’) and princals (Package Gifi) to reduce the number of dimensions for this mix data. However, in all cases, I'm not able to reduce the dimensions by much. I tried doing principal component analysis using the prcomp function which uses singular value decomposition only on the 32 numeric features. This gave me much better results with two principal components explaining 99.99% of the variance. My question here is: Can I do what I did above, using pca on only the numeric predictors to reduce the numeric predictors from 32 to just 2? And then combine the 2 principal components with the other 8 non-numeric predictors and use them for detecting anomalies? I did all of this and I'm getting good accuracy in predictions using random forest and random tree. I'm just not sure if it is conceptually sound. Thanks.
