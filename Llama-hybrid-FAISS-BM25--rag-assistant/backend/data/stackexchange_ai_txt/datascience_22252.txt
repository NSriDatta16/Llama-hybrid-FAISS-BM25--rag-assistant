[site]: datascience
[post_id]: 22252
[parent_id]: 17804
[tags]: 
There are several ways of approaching improving that word2vec model. Try removing all the commas, creating a continuous block of text to train word2vec. There is also a concept called "fine tuning", it is a variation of transfer learning. Find a set of pre-trained word2vec weights and use those as the initial weights (word2vec weights are typically initialized as random). Then update the weights with your dataset. The pre-trained weights give the general patterns in words and better embed rare words. The fine tuning will improve the generic embeddings to model your particular dataset. Lastly when tokenizing your dataset make sure you create the correct collocations . For example, "Brad Pitt" should be "brad_pitt" (instead of "brad" and "pritt" separately). You can also recode all relevant mentions of "Brad" into "brad_pitt". Collocations carry much of the semantic meaning in proper nouns.
