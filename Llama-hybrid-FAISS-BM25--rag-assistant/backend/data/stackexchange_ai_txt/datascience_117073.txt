[site]: datascience
[post_id]: 117073
[parent_id]: 116963
[tags]: 
Depending at which stage you are in your project, I'd suggest using an experiment tracking system for machine learning, like MLflow, Metaflow, Weights & Biases, etc., in combination maybe with DVC. There you can log your experiments, including: algorithms chosen, their hyperparameters, evaluation metrics, you can log plots, artifacts, any other custom "thing" that's important to you. I personally use MLflow for this, there's a bit of an overhead in terms of writing code (you have to call something.log(..) a lot) but it has a nice dashboard functionality that enables you to visually inspect an experiment and compare different experiment runs. You have the concept of a project there, and you can specify multiple entrypoints , which you can logically split as: data pre-processing, hyperparemeter optimisation, algorithm selection, evaluation, etc. The actual algorithm selection step, the hyperparameter search space, the data pre-processing steps, etc. are written in whatever library you use ( sklearn , lightgbm , etc.) and its logic is defined by you. I use Optuna to do hyperparameter optiimsation, pandas for data manipulation, sklearn for data pre-processing and model evaluation, lightgbm for modeling, etc. If you have access to cloud, I think Sagemaker is like an AWS solution for this kind of problems, but not 100% sure, never worked with it. To sum up, my advice is, instead of writing one big function to handle all your steps, write many small experiments that handle one logical step within your pipeline/project, use something like MLflow to figure out what works best, and iterate, a lot.
