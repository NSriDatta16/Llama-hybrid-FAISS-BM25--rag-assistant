[site]: crossvalidated
[post_id]: 463703
[parent_id]: 
[tags]: 
OneHotEncoding and Scaler in Pipeline, avoid data leak?

so I have my data and split it in the beginning in test and train set. Then I apply following Pipelines on it: class DataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self,attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names].values num_pipeline = Pipeline([ ('selector',DataFrameSelector(numerical_attributes)), ('imputer',SimpleImputer(strategy="median")), ('std_scaler',StandardScaler()) ]) cat_pipeline = Pipeline([ ('selector', DataFrameSelector(categorical_attributes)), ('onehot',OneHotEncoder(handle_unknown='ignore')) ]) full_pipeline = FeatureUnion(transformer_list=[ ("nums", num_pipeline), ("cats", cat_pipeline), ]) To train it I use it prepared_data = full_pipeline.fit_transform(X_train) rf = RandomForestClassifier() rf.fit(prepared_data, y_train) So now I want to take my test set and predict the outcome with the trained rf classifier. First problem is now that the test set may contain new labels in some categories previously unseen in the trained data and thus it will not match in shape. Certainly one way to handle this would be to do prep_test_data = full_pipeline.transform(X_test) But I read up about data leakage with standard scaler and - correct me if I am wrong - this would lead to the effect, that the standard scaler is reusing its learned mean, variance to manipulate the X_test data and thus leaking it, correct? So what can I do now, part of the pipeline is the OneHotEncoder that I kind of have to reuse with the trained categories from X_train but I think to not have data leakage I cannot do that same thing for Standard Scaler which is a part of that pipeline. Any ideas, am I wrong with anything, please help me and push me in the right direction! Thanks!
