[site]: datascience
[post_id]: 123140
[parent_id]: 123139
[tags]: 
With the help of reddit I figured out the answer. We train the VQ-VAE, and then use it to encode our dataset into the latent space. This latent space, of course, will have a particular distribution (or prior). The original paper uses a autoregressive model (the PixelCNN) to learn this distribution. So we can sample by using another model to generate latent spaces that we then pass to the decoder to generate our images/ data. In the case of a latent diffusion model, we use a diffusion model to learn this prior, instead of an autoregressive one.
