[site]: crossvalidated
[post_id]: 22570
[parent_id]: 22566
[tags]: 
I believe that this is one of the most counter-intuitive aspects of statistics; it's just really difficult to wrap your head around. The key notion here is the idea of the bias-variance tradeoff . It has been discussed in several places on CV, and you may want to check out some of the other answers, for example here or here , and I discussed it before here . Setting mine aside, the other two are quite good and well worth your time. I will try to give a quick sense of the idea. Let me first define some terms. To start with, what Shmueli means by "true" model is the actual data generating process; the closer your estimated model is to the real data generating process, the truer it is. For instance, if $\beta_1=.5$, and one model fit yields $\hat{\beta}_1=.6$, that's truer than another fit that yields $\hat{\beta}_1=.7$. On the other hand, predicting better means getting your $\hat{y}$'s as close as possible to the actual $y$'s, especially for out-of-sample data. Notice the differences in goals here (because that's crucial to understanding the issue): getting $\hat\beta$'s as close as possible vs. getting $\hat{y}$'s as close as possible. So Shmueli's point is that sometimes your $\hat{y}$'s can be closer to the actual $y$'s when your $\hat\beta$'s are estimated by a process that, on average, yields values a little further from the true $\beta$'s. Now, how is that possible? The key is that there is variance associated with parameters estimated from sample data. For a given sample, sometimes the maximum likelihood estimate happens to be further from the true value and sometimes closer. It is quite possible to have a situation where the variance of the sampling distribution of a parameter estimate is so large that $\hat\beta$'s routinely bounce so far out around their true value that they are not worth much. The thing to remember here is that classical statistics are based on what are called the ' best linear unbiased estimators ', that is, the estimators that have the lowest variance of all the unbiased estimators . However, there can be other ways of attempting to get an estimate that are not unbiased . Typically, these have been developed within machine learning (a subfield of computer science). It is possible in some cases to have an estimate that doesn't tend to bounce as far from the true value, even though the sampling distribution of that estimate is not centered on the true value (i.e., it is biased ). Given all of this, what matters for the accuracy of your predictions is how the inaccuracy due to the induced bias trades off vis-a-vis the inaccuracy induced by the high variance of the BLUE parameter estimate (hence the name). Specifically, if the inaccuracy due to higher variance is greater than the inaccuracy due to the bias, the less true model will give the better predictions.
