[site]: stackoverflow
[post_id]: 2052911
[parent_id]: 2052684
[tags]: 
I built an alternate C solution that doesn't require any modulo or division operations: #include #include int main(int argc, char *argv[]) { int v[1100000]; int j1, j2, j3, j4, j5, j6, s, n=0; memset(v, 0, sizeof(v)); for (j6=0; j6 It generates 97786 self numbers including 1 and 1000000. With output, it takes real 0m1.419s user 0m0.060s sys 0m0.152s When I redirect output to /dev/null, it takes real 0m0.030s user 0m0.024s sys 0m0.004s on my 3 Ghz quad core rig. For comparison, your version produces the same number of numbers, so I assume we're either both correct or equally wrong; but your version chews up real 0m0.064s user 0m0.060s sys 0m0.000s under the same conditions, or about 2x as much. That, or the fact that you're using long s, which is unnecessary on my machine. Here, int goes up to 2 billion. Maybe you should check INT_MAX on yours? Update I had a hunch that it may be better to calculate the sum piecewise. Here's my new code: #include #include int main(int argc, char *argv[]) { char v[1100000]; int j1, j2, j3, j4, j5, j6, s, n=0; int s1, s2, s3, s4, s5; memset(v, 0, sizeof(v)); for (j6=0; j6 ...and what do you know, that brought down the time for the top loop from 12 ms to 4 ms. Or maybe 8, my clock seems to be getting a bit jittery way down there. State of affairs, Summary The actual finding of self numbers up to 1M is now taking roughly 4 ms, and I'm having trouble measuring any further improvements. On the other hand, as long as output is to the console, it will continue to take about 1.4 seconds, my best efforts to leverage buffering notwithstanding. The I/O time so drastically dwarfs computation time that any further optimization would be essentially futile. Thus, although inspired by further comments, I've decided to leave well enough alone. All times cited are on my (pretty fast) machine and are for comparison purposes with each other only. Your mileage may vary.
