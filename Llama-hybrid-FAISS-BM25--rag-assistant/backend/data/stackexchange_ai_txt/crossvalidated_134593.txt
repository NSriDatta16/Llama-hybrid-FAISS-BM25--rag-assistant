[site]: crossvalidated
[post_id]: 134593
[parent_id]: 
[tags]: 
How does the number of components in a GMM relate to the information content?

Say you fit a Gaussian Mixture Model (GMM) to your data using a Bayesian technique, which should tell you the number of components needed to fit your data. Does this also give insight into the information complexity of your data? I'm not sure the correct measure of complexity here, maybe more components higher entropy? In an intuitive sense, a signal with more features should require more components to describe it. Conversely, a signal that has been low-pass-filtered (reducing high-frequency signal) has less complexity and should require fewer Gaussian components to describe it. Is there a formal relationship between these concepts?
