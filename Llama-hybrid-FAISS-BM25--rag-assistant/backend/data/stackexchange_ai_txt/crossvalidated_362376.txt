[site]: crossvalidated
[post_id]: 362376
[parent_id]: 272607
[tags]: 
I gave this a shot today and was consistently able to hit near 75-80% in test accuracy. The total number of parameters used was: 183,242 You can do better by adding maybe a few more layers, but you don't need to be excessive. More complex networks do not always result in better results. Suggestions My suggestion to you is that you keep your architecture simple. Follow Occam's Razor , simple is better. Scale your data Don't use a random seed Use an appropriate optimizer; I used Adadelta as is from Keras. CNNs don't need to be convoluted; keep it simple Deeper skinnier networks sometimes work better than wider ones Use regularization (e.g. Dropout) Below is my code (using Keras) # Define the model model = Sequential() model.add(Convolution2D(64, (4, 4), padding='same', input_shape=(3, 32, 32))) model.add(MaxPooling2D(pool_size=(2, 2), strides=2)) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(Convolution2D(64, (2, 2), padding='same')) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(Convolution2D(32, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(Convolution2D(32, (3, 3), padding='same')) model.add(Activation('relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=2)) model.add(Dropout(0.15)) model.add(Flatten()) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(Dense(64)) model.add(Activation('tanh')) model.add(Dropout(0.25)) model.add(Dense(num_classes, activation='softmax')) # Compile the model
