[site]: crossvalidated
[post_id]: 437431
[parent_id]: 437308
[tags]: 
I cant see any parameter reduction in the second conv layer in the googlenet paper. There is no 1x1 conv layer and it makes no sense to have the same output dimension as input dimension. You wouldnt do: 64 filters -> 1x1 conv -> 64 filters. You use the 1x1 conv layers to reduce the amount of feature maps. This is done after the 2nd layer. Let's take the 3x3 part of the inception 3a module for example. The input is 28x28x192 and gets reduced with a 1x1 filter to 28x28x96 and then 128 3x3 filters are used to create 28x28x128. This 3x3 path has 192*1*1*96 + 96*3*3*128 = 129024 parameters. Without the bottleneck 1x1 layer you would have 192*3*3*128 = 221184 parameters. The 2.7k parameters are confusing, and your equation is correct if you use a standard CNN layer. Here is the same question at stackoverflow. It is either wrong or they used some sort of asymetric layer.
