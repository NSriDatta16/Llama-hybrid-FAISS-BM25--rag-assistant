[site]: crossvalidated
[post_id]: 466480
[parent_id]: 466204
[tags]: 
There are many (interrelated) questions here and not enough space to pursue all their implications. Let's therefore focus on a central idea, which I wish to state rigorously and generally, so I will begin with some definitions that cover the examples in the question (and much more). First, we need to capture the idea of a "distribution" in some interval like $[0,1]\subset \mathbb R$ or a ball in $\mathbb{R}^n$ or even a finite set like $\{1,2,\ldots,n\}.$ We need to relate this to some kind of distance on these sets and we will need to focus attention on small neighborhoods of points within these sets. Here's my attempt, which generalizes the usual concept of a real-valued random variable: Let $(S,\mathfrak{F},\mathbb P)$ be a probability space and $(T,\delta)$ a metric space. When $X:S\to T$ is a random variable, let us say that $t\in T$ is a support point of $X$ when there is positive probability that $X$ lies in any non-trivial closed ball around $t:$ that is, for any number $\rho \gt 0,$ $\mathbb{P}(\delta(X,t) \le \rho) \gt 0.$ "The" support of $X$ is the union of all its support points. Next, we need to create a framework to describe a sampling process that can become arbitrarily large. This is standard, but I will take the opportunity to count random points within neighborhoods in $T:$ Suppose $(X_i),$ $i=1,2,3,\ldots,$ is a sequence of iid $T$ -valued variables on $S.$ For any $t\in T,$ $\rho \gt 0,$ and integer $n,$ let $N_n(t,\rho)$ count how many of the first $n$ of the $X_i$ lie within distance $\rho$ of $t.$ For a given $t$ and $\rho,$ the sequence $N_1(t,\rho), N_2(t,\rho),\ldots$ is a sequence of integer-valued random variables on $S.$ Let's call such an iid sequence a "sampling process." These simple definitions are enough to prove a far-reaching claim: Claim: the sequence $N_i(t,\rho)$ almost surely diverges. Before proving this claim, let's apply it to the questions. There doesn't have to a any "regular pattern." Indeed, there's nothing in the general definitions and analysis that can be used even to define or characterize a "pattern." When $T$ is the unit interval $[0,1]\subset\mathbb R,$ and $\delta$ is the usual distance ( $\delta(x,y) = |y-x|$ ), the claim implies distances between neighboring samples must go to zero. For if not, let $t$ lie in one of the gaps and let $\rho$ be less than the distance from $t$ to the nearest sample points. The claim shows this can't happen because eventually there will be a large number of sample points within distance $\rho$ of $t.$ When $T$ is $\mathbb R$ with its usual distance and $X$ has a Normal distribution, it's easy to show the support of $X$ is $\mathbb R.$ (Proof: the chance that $X$ lies within $\rho$ of $t\in\mathbb R$ is the integral over the interval $[t-\rho,t+\rho]$ of a strictly positive continuous density function. That function therefore attains a strictly positive minimum value, say $q,$ on the interval, whence the probability is at least $2\rho q,$ which is nonzero.) The same analysis as $(2)$ proves that around any number $t$ eventually there will be an arbitrarily large number of sample points close to $t.$ (What it does not reveal, though, is that when $t$ is far from the mean of $X,$ the sample size need to be astronomically large before a cluster of sample points is likely to appear near $t.$ ) Proof of the claim. The claim is proven if we can show that for any integer $M$ and real number $\epsilon \gt 0,$ the chance that all the $N_i(t,\rho)$ in this sequence are bounded by $M$ is no greater than $\epsilon.$ Let's do some preliminary analysis before addressing this issue. Because $t$ is in the support of each $X_i,$ the number $$q = \mathbb{P}(\delta(X,t)\le \rho)$$ is nonzero. Define the random variables $I_i(t,\rho) $ to be the indicators of this event: $$I_i(t,\rho) = \left\{\matrix{1 & \text{if } \delta(X_i,t)\le \rho \\ 0 & \text{otherwise.}}\right.$$ Because the $I_i(t,\rho) $ are functions of the independent variables $X_i,$ the $I_i(t,\rho) $ are independent; and because the $X_i$ are identically distributed, so are the $I_i(t,\rho) .$ The common distribution of the $I_i(t,\rho) $ is Bernoulli $(q),$ as we have already computed. Since $$N_n(t,\rho) = \sum_{i=1}^n I_i(t,\rho),$$ the variable $N_n(t,\rho)$ has a Binomial $(n,q)$ distribution. Its expectation is $nq,$ its variance is $nq(1-q),$ and Chebyshev's inequality asserts that for any $\kappa \ge 1,$ $$\mathbb{P}\left(|N_n(t,\rho) - nq| \ge \kappa \sqrt{nq(1-q)}\right) \le \frac{1}{\kappa^2}.\tag{1}$$ Return, now, to arbitrary $M$ and $\epsilon.$ By choosing any $n$ so large that $$ n \gt \frac{1}{q}\left(2M + \frac{1-q}{\epsilon} + \frac{M^2}{q}\right),$$ we deduce $$nq(1-q) \le (M-nq)^2\epsilon.$$ In these terms, the inequality $(1)$ can be rewritten $$\mathbb{P}\left(N_n(t,\rho)\le M\right) \le \epsilon.$$ Although this applies only to any sufficiently large $n,$ it is enough for the proof, because the sequence $N_i(t,\rho)$ has independent increments. This means (among other things) that for integral $a\ge 1,$ $N_{an}(t,\rho)$ is the sum of $a$ iid variables having the same distribution as $N_n(t,\rho):$ namely, the count of the first $n$ of the $X_i$ plus the count of the next $n$ of the $X_i$ plus etc. The chance that $N_{an}(t,\rho)$ does not exceed $M$ exceeds the chance that all $a$ of these variables do not exceed $M,$ which (by independence) equals $(1-\epsilon)^a.$ The limit of this value, as $a$ grows large, is zero. Consequently, it is almost certain that at least one of these variables exceeds $M.$ But then all subsequent values of the $N_i(t,\rho),$ which can never be any less than the preceding counts, must all exceed $M,$ too. We have shown that no matter what $t\in T$ and $\rho\gt 0$ may be, there is zero chance that only finitely many of the $X_i$ are within distance $\rho$ of $t,$ QED. In this rigorous sense we have the right to say Every support point of a random variable $X$ is an accumulation point of the sampling process of $X.$
