[site]: datascience
[post_id]: 8521
[parent_id]: 
[tags]: 
Reduce dimension, then apply SVM

Just out of curiousity, is it generally a good idea to reduce the dimension of training set before using it to train SVM classifier? I have a collection of documents, each of them is represented by a vector with tf-idf weight calculated by scikit-learn's tfidf_transformer. The number of terms (feature?) is close to 60k, and with my training set that consist of about 2.5mil of documents, it makes the training process to go on forever. Besides taking forever to train, the classification also wasn't accurate most probably due to the wrong model. Just to get an idea of what I am dealing with, I tried finding a way to visualize the data somehow. And I decomposed the document matrix into a (m, 2) matrix using SVD with scikit-learn (wanted to try other methods, but they all crashed halfway). So this is what the visualization looks like So is it generally a good practice to reduce the dimension, and then only proceed with SVM? Also in this case what can I do to improve the accuracy of the classifier? I am trying to use sklearn.svm.SVC and kernel='poly' , and degree=3 and it is taking a very long time to complete.
