[site]: crossvalidated
[post_id]: 397213
[parent_id]: 396918
[tags]: 
If you look carefully most of the diagonal attention patterns are actually offset a bit from diagonal, indicating most of the attention is on nearby tokens, which is expected. The authors do say in the paper that it might be possible to limit the attention neighborhood to save on computation. But the layer 4 attention maps do show that being able to attend across the entire sentence can be useful. As for layer 6, I'm not sure entirely. Is breaking up "secretly" into "secret" and "ly" intended or a mistake? I'm not a NLP expert but I've never heard of such a tokenization step. The tokenizer they use, spacy, doesn't break the word up either. If this was a mistake, then that would explain it. There are also some attention visualizations in the original transformer paper which you might find interesting. I don't think they "collapse" there unexpectedly. They also show more interesting structure rather than just connections on the diagonal.
