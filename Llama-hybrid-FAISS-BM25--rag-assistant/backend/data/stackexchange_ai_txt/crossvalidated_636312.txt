[site]: crossvalidated
[post_id]: 636312
[parent_id]: 
[tags]: 
One extreme outlier fitted value that is replaced by another when dropped

I have an OLS model with a very bad prediction score - when I decided to test for heteroskedasticity, it turned out my model's predictions include one incredibe outlier - it looks like my fitted values plotted against residuals are all clustered in one cloud of dots, except there is one dot that has a way higher fitted value and residual, so high that the entire cloud of other dots has to be confined in the corner of the screen for this one dot to fit in the other. What's odd is that when i look that outlier up, it has no special, crazy qualities - very average X's across the board. Odder yet, if I manually drop it from my spreadsheet, another one takes its place, always in the row before or after the one I just dropped, with almost the same exact proportion of rows below to rows above. This is particularly puzzling, because I am using train-test split in python: x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0) which is supposed to shuffle values by default, so there is literally no significance to the position of the outliers. Why does it do that? How can it be remedied? The data, for those who want to replicate it, is here: https://docs.google.com/spreadsheets/d/1Fa2fsgeRKy8Iodpt6FfDzf9oCxyNph5gIB-uD3T_ktQ/edit?usp=sharing My entire code is here: import pandas as pd import matplotlib.pyplot as plt import numpy as np import statsmodels.api as sm from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from statsmodels.stats.outliers_influence import variance_inflation_factor pd.set_option("display.max_columns", None) pd.set_option("display.max_rows", None) # Read the data df = pd.read_excel("final.xlsx") # Assuming df is your DataFrame y = df["rent"] x = df.drop( [ "rent", "Index", "pets_deposit", "Commute_nonrush", "Commute_rush", "school3", "walk-score", "bike-score", "lease_term", "transit-score", "cats_allowed", "pets_allowed", "deposit_missing", "application_fee_missing", "application_fee", "school2", "garbageincluded", "pets_allowed_deposit", "arrests", ], axis=1, ) matrix = x.corr() with open("OLS/matrix.txt", "w") as f: f.write(str(matrix)) # Calculate VIF for each variable vif = pd.DataFrame() vif["variables"] = x.columns vif["VIF"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])] with open("OLS/vif.txt", "w") as f: f.write(str(vif)) # Split the data into training and testing sets x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0) # Perform the backward elimination cols = list(x_train.columns) p = [] x_1 = x_train[cols] model = sm.OLS(y_train, x_1).fit() # Calculate residuals residuals = model.resid # Find the index of the observation with the largest absolute residual outlier_index = np.argmax(np.abs(residuals)) print(f"The index of the outlier is: {outlier_index}") # If you want to see the entire row of the outlier in the DataFrame print(df.iloc[outlier_index]) # print the prediction of the outlier print(model.predict(x_1.iloc[outlier_index])) # Create a scatterplot for residuals plt.scatter(model.fittedvalues, residuals) plt.xlabel("Fitted Values") plt.ylabel("Residuals") plt.title("Residuals vs Fitted Values") plt.show() selected_features_BE = cols summary_str = model.summary().as_text() with open("OLS/summary_OLS.txt", "w") as f: f.write(summary_str) # Subset the test data to include only the selected features x_test_BE = x_test[selected_features_BE] # Make predictions on the test data y_pred = model.predict(x_test_BE) # Calculate and print the Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_pred) print(mse) with open("OLS/mse.txt", "w") as f: f.write(str(mse)) # Calculate and print the R-squared score r2 = r2_score(y_test, y_pred) print(r2) with open("OLS/r2.txt", "w") as f: f.write(str(r2)) ```
