[site]: crossvalidated
[post_id]: 202858
[parent_id]: 
[tags]: 
XGBoost Loss function Approximation With Taylor Expansion

As an example, take the objective function of the XGBoost model on the $t$ 'th iteration: $$\mathcal{L}^{(t)}=\sum_{i=1}^n\ell(y_i,\hat{y}_i^{(t-1)}+f_t(\mathbf{x}_i))+\Omega(f_t)$$ where $\ell$ is the loss function, $f_t$ is the $t$ 'th tree output and $\Omega$ is the regularization. One of the (many) key steps for fast calculation is the approximation: $$\mathcal{L}^{(t)}\approx \sum_{i=1}^n\ell(y_i,\hat{y}_i^{(t-1)})+g_tf_t(\mathbf{x}_i)+\frac{1}{2}h_if_t^2(\mathbf{x}_i)+\Omega(f_t),$$ where $g_i$ and $h_i$ are the first and second derivatives of the loss function. What I'm asking for is convincing arguments to demystify why the above approximation works: 1) How does XGBoost with the above approximation compare to XGBoost with the full objective function? What potentially interesting, higher-order behavior is lost in the approximation? 2) It's a bit hard to visualize (and depends on the loss function) but, if the loss function has a large cubic component, then the approximation will likely fail. How is it that this doesn't cause problems for XGBoost?
