[site]: crossvalidated
[post_id]: 220529
[parent_id]: 220507
[tags]: 
There will be a LOT of answers to this question, but I still want to add one since you made some interesting points. For simplicity I only consider the simple linear model. It is my understanding that the linear regression model is predicted via a conditional expectation E(Y|X)=b+Xb+e The fundamental equation of a simple linear regression analysis is: $$\mathbb E(Y\,|\,X) = \beta_0 +\beta_1X,$$ This equation meaning is that the average value of $Y$ is linear on the values of $X$. One can also notice that the expected value is also linear on the parameters $\beta_0$ and $\beta_1$, which is why the model is called linear. This fundamental equation can be rewritten as: $$Y = \beta_0+\beta_1X+\epsilon,$$ where $\epsilon$ is a random variable with mean zero: $\mathbb E(\epsilon) = 0$ Do we assume that both X and Y are Random variables with some unknown probability distribution? ... If we don't assume the independent variables are themselves random The independent variable $X$ can be random or fixed. The dependent variable $Y$ is ALWAYS random. Usually one assumes that $\{X_1,...,X_n\}$ are fixed numbers. This is because regression analysis was developed and is vastly applied in the context of designed experiments, where the $X$'s values are previously fixed. The formulas for the least squares estimates of $\beta_0$ and $\beta_1$ are the same even if the $X$'s are assumed random, but the distribution of these estimates will generally not be the same compared to the situation with fixed $X$'s. if we take the conditional expectation E(Y|X=35) ... would we just take the average(arithmetic mean) of y for those observations where X=35? In the simple linear model you can build a estimate $\hat\varphi(x)$ of $\mathbb E(Y|X = x)$ based on the estimates of $\hat \beta_0$ and $\hat \beta_1$, namely: $$\hat\varphi(x) = \hat\beta_0+\hat\beta_1x$$ The conditional mean least squared estimator has expression equal to the one you described if your model treats the different weights as levels of a single factor. Those models are also known as one-way ANOVA, which is a particular case of (not simple) linear model.
