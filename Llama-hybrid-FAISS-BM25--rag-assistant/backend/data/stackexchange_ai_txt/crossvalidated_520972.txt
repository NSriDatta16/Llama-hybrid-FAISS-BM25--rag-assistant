[site]: crossvalidated
[post_id]: 520972
[parent_id]: 
[tags]: 
Why does does the first term of a simulated MA(1) model with low variance have much larger absolute value than the rest?

When doing some simulations in R to learn about moving average models I noticed something in the time series plots I could not explain. When I take the innovations/noise terms to be from a normal distribution of low variance, (less than 0.1) then the plots seem to start with a term which is much bigger than the rest in absolute value. After that, it has the behaviour I expected, i.e. some noise around the mean of $0$ with constant variance. I have included a few plots that show what I mean and the code I used to generate them. set.seed(5) x1 = arima.sim(model = list(ma = 0.2), n = 100, innov=rnorm(100, 0, 0.01^2)) x2 = arima.sim(model = list(ma = 0.8), n = 100, innov=rnorm(100, 0, 0.01^2)) x3 = arima.sim(model = list(ma = 0.2), n = 100, innov=rnorm(100, 0, 0.1^2)) x4 = arima.sim(model = list(ma = 0.8), n = 100, innov=rnorm(100, 0, 0.1^2)) plot.ts(cbind(x1,x2,x3,x4)) I believe the model I am using is $$ m_t = a_t + \alpha a_{t-1}$$ where $\{a_t\}$ are white noise terms with mean zero and variance $0.1$ or $0.01$ and $\alpha$ is $0.2$ or $0.8$ which is the weight given to previous noise. An idea I have is that there is something different about the first term because there is no previous term to use in the model, but I can't see how that explains what I see.
