[site]: crossvalidated
[post_id]: 141555
[parent_id]: 
[tags]: 
How does regularization reduce overfitting?

A common way to reduce overfitting in a machine learning algorithm is to use a regularization term that penalizes large weights (L2) or non-sparse weights (L1) etc. How can such regularization reduce overfitting, especially in a classification algorithm? Can one show this mathematically?
