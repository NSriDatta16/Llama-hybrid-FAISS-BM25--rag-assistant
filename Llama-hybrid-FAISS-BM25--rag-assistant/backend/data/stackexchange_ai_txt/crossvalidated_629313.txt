[site]: crossvalidated
[post_id]: 629313
[parent_id]: 435721
[tags]: 
First up, some random forest background to address the questions about what the parameters do: Forests are "trained" one tree at a time. Each tree starts as a single node with all the training data. For each node, a random subset of the features is selected, and the best (feature, value) pair for splitting the data in a "left" and "right" half is determined. "Best" is defined by the criterion . If that split is good enough and allowed by the parameters, then a left and right child are created with the left and right partitions of the data. Then the process continues with the children. If not good enough or allowed, then the node remains a leaf forever. Forests are an ensemble model, and the theory behind ensembles shows you want each estimator in the ensemble to have very low bias and low correlation with the other estimators (implies high bias). Low bias is good in general, while the low correlation means the ensemble average will be much lower variance than the individual trees. Most of the parameters you mention control the creation of the trees - min_samples_split and min_samples_leaf only allow a split if the current node or resulting nodes, respectively have at least that number of samples. The criterion is how the tree determines which split is best. And so on. Per (2), you usually want the trees to grow very deep (low bias). So focus on min_samples_* on the order of 1-10, have low or nonexistent criteria for the minimum required improvement, that sort of thing. So with that in mind, here are a couple of things to try: try many fewer features per split, order 3-10, much less than the minimum of 100 you've tried thus far. This is because of (2) - you want each tree to be uncorrelated. With 600 features though, my guess is that many of them are effectively duplicates, or at least highly correlated. Some of those features will be better than others at various stages of tree growth. So by taking 100+, the decisions made by each tree may be highly correlated. Many fewer features though will promote diversity of the trees. Don't worry about the top-level decisions being poor - per (2) again, you'll be training deep. So there will be many more opportunities for the tree to make good splits later. try ExtraTreesClassifier instead. It's similar to RandomForestClassifier , but the training process in (1) is kicked up a notch in terms of variance: only a single randomly selected pivot is considered for each feature. This adds extra randomness (hence the Extra in ExtraTrees !), which in turn generally improves performance a bit. As a bonus, only considering one pivot per feature makes training the model on continuous-valued features dramatically faster. Win-win. per (2) above, in your other features, always favor parameter values that will make the trees more random and let them train deeper. if the most important features in your model are axis aligned, be careful of what @Sycorax points out: trees make splits on a single feature at a time, and thus are axis aligned. So making e.g. 45 degree cuts or circles in GPS lat/lon data is impossible - only squares. Deep trees still do okay on this because they can split recursively down to only 1-2 samples per leaf which basically aliases the angled/round boundary into a stairstep pattern. If your data is 2D like lat/lon GPS coordinates, you can mitigate this by adding two new features: a 45 degree rotation of your original coordinates. This lets trees make octagonal boundaries which should be close enough since they're trained deep. feature selection: forests are generally robust to overfitting thanks to the randomness among trees and ensemble averaging, but not immune. You can run your favorite feature selection algorithm to cut down on the number of features I expect the original asker has long since moved on, but other people might find this helpful.
