[site]: datascience
[post_id]: 126606
[parent_id]: 126604
[tags]: 
Problems of batch normalization: Unstability with small batch size: with only a few samples on the batch, the presence of noise/outliers can pull the statistics away from the population values. Unstable training dynamics in general ( source ). Longer training times ( source ). Different behavior between training and inference times, making it more difficult to reason when finding unexpected values in the training and validation losses. Doesn't make sense in RNN architectures, where weights are shared for all timesteps, but the output of each one might have different statistics. Recurrent Batch Normalization was proposed for that. Coupling in the computation of different training samples in the minibatch. Can't think of an scenario where is was actually a problem, but it's something to consider.
