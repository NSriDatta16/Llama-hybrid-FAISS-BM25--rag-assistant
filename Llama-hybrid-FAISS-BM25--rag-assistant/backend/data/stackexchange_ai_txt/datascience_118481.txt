[site]: datascience
[post_id]: 118481
[parent_id]: 
[tags]: 
What loss function to use for predicting discrete output sequence given a discrete input sequence?

I am working on sequence-to-sequence tasks where the input is an n -length sequence of discrete values from a finite set S (say {x | x is a non-negative integer less than 10} ). An example input sequence of length 5 is: 1 8 3 5 2 . The output is supposed to be some length preserving transformation of the input sequence (say reverse, shift, etc.). To be explicit, the tokens of the output sequence also come from the same set as the input sequence. For example, for the input sequence above, the reverse transformation produces the output sequence: 2 5 3 8 1 . I want the model to predict the output tokens exactly, so the task is closer to classification than regression. However, since the output is a sequence, we need to predict multiple classes (as many as the input length) for each input sequence. I searched for references but could not find a similar setting. Please link some suitable references you are aware of that may be helpful. I have the following questions for my use case: What changes are needed such that the model works on discrete sequences as defined above? What loss function would be appropriate for my use case? For 1), one might change the input sequence such that each token is replaced by an embedding vector (learned or fixed) and input that to the model. For the prediction, I was thinking of ensuring that the model produces a n x k length output ( n = sequence length; k = |S| or the vocab size) and then using each of these n vectors to make a class prediction (from k classes). For 2), the loss could be a sum of n cross-entropy losses corresponding to the n classifications. Please help me with better answers to these two questions. Thank you. Edit: My setup is encoder-only (non-autoregressive prediction). Please account for this while answering the questions by suggesting approaches that are in line with the setup, if possible.
