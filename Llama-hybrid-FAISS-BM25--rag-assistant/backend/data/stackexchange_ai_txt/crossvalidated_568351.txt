[site]: crossvalidated
[post_id]: 568351
[parent_id]: 567814
[tags]: 
This is very like classical problems in experimental design. In the classical version you'd assume that the variance of completion times for fixed $y$ was constant, say $\sigma^2$ , and the variance for random $Y$ was $\sigma^2+\tau^2$ . If you do $M$ cases for each of $N$ values of $Y$ , the variance of the mean is $\sigma^2/MN+\tau^2/N$ . If it costs $B$ times as much to generate a $Y$ as to do a test, the cost is proportional to $N(M+B)$ . As long as you have some idea about $\sigma^2/\tau^2$ and $B$ , you can optimise. If $\sigma^2/\tau^2$ is small, you don't gain much by doing many cases per $Y$ , but if it's large you do. Note that this is the variance ; it could be that the mean time to generate a $Y$ is very large, but that the variance of that time is small. The mean is still estimated by the average. You can estimate $\sigma^2$ by the average of the $N$ variances for fixed $y$ . The variance of the $N$ means for fixed $Y$ is $\sigma^2/M+\tau^2$ , so you can estimate $\tau^2$ , and you can then estimate the variance of the mean. This approach will still work if $\sigma^2$ varies importantly with $y$ , but something more complicated might be better.
