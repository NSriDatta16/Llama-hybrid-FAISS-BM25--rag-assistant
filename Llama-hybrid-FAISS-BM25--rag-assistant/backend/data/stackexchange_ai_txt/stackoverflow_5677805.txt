[site]: stackoverflow
[post_id]: 5677805
[parent_id]: 5677671
[tags]: 
The distance between a point and a line is given by: d = |(x_2 - x_1)(y_1 - y_0) - (x_1 - x_0)(y_2 - y_1)| / sqrt((x_2 - x_1)^2 - (y_2 - y_1)^2), which is an expansion of the dot product, where (x_0, y_0) are the coordinates of the point, and (x_1, y_1) & (x_2, y_2) are the endpoints of the line. It would be pretty simple to calculate this for each set of points, and then just determine which one is the lowest. I'm not unconvinced there's not a more elegant way to do so, but I'm not aware of it. Though I'd love to see if someone here answers with one! Edit: Sorry that the math in here looks so messy without formatting. Here's an image of what this equation looks like, done nicely: (from MathWorld - A Wolfram Web Resource: wolfram.com ) Another edit: As Chris pointed out in his post, this doesn't work if the points are in-line, i.e., if the line is defined by (0,0)-(0,1) and the point by (0,10). Like he explains, you need to check to make sure that the point being looked at isn't actually on the "extended path" of line itself. If it is, then it's just the distance between the closer endpoint and the point. All credit to Chris!
