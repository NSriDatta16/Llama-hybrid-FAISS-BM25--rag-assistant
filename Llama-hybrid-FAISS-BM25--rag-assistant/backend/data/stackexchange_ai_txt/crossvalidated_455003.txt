[site]: crossvalidated
[post_id]: 455003
[parent_id]: 
[tags]: 
GAN loss doesnt ocsillate but converges relly quickly

I am trying to train a GAN and am getting a strange convergence issue. The losses seem to converge really quickly and dont ocsilate. The examples of the generator produces are noise. Both the networks are fullt convolutional and have around 8 million parameters each. I am using the SGD optimizer with lr=0.0001 for the generator and 0.0004 for the discriminator. I am wondering if the generator is too good and is able to fool the discriminator early on, so need to make it less powerful, but i find it hard to diagnose GAN training failures. Does anyone have any ideas?
