[site]: crossvalidated
[post_id]: 10446
[parent_id]: 10423
[tags]: 
You are probably over impression from the classical modelling, which is vulnerable to the Runge paradox -like problems and thus require some parsimony tuning in post-processing. However, in case of machine learning, the idea of including robustness as an aim of model optimization is just the core of the whole domain (often expressed as accuracy on unseen data). So, well, as long as you know your model works good (for instance from CV) there is probably no point to bother. The real problem with $p\gg n$ in case of ML are the irrelevant attributes -- mostly because some set of them may become more usable for regenerating decision than the truly relevant ones due to some random fluctuations. Obviously this issue has nothing to do with parsimony, but, same as in classical case, ends up in terrible loss of generalization power. How to solve it is a different story, called feature selection -- but the general idea is to pre-process the data to kick out the noise rather than putting constrains on the model.
