[site]: crossvalidated
[post_id]: 402653
[parent_id]: 402633
[tags]: 
Simplifying a little bit in order to stay general (the math gets tedious once a lot more documents and topics are involved). Let's assume for a second there are only pure car and pure plane documents - those that load 100% onto their respective topics. Let's say your average "car" document has a car score of 10, i.e. an average vector length of 10 (because say, "car" happens to occur 3 times in the average document). Think of average documents like "There is one car, there is another car, there is yet another car..." What happens if you have a document that only has the word "car" in it once? This might happen if a document is much shorter than average, for example a sentence like "This is a car." Just looking at this, you know it is about cars. It should be in the "car" topic. Euclidian distance is a measure of magnitude. Euclidian distance between the average car document and the short document is 9 - in one dimension, its simply the arithmetic difference, and we are in one dimension, because we just assumed there are only pure topics so the vector does not have any value in "planes". Even though the short document is about cars, the euclidian distance is quite large, so you might judge them as not being very similar. You might notice in this case, but you will deal with much larger and complex documents, and you cant check everything by hand. To make things harder, your documents will likely never be 100% plane and 0% car or vice versa. Some might be a mix. There might be one that is absolutely about planes and flying, such as a pilot's diary, but contains the word car 3 times (pilot describing how he gets to the airport, etc). Is this document in the car topic? You would say no, but the math says "a little bit". By euclidian distance, the pilot document might end up being closer to the car topic than the short car document. This might lead you to misclassify it or misinterpret the topics. This issue gets more and more pronounced with the amount of topics and documents. There will be hundreds of words, dozens of topics and potentially millions of documents, all of different length. If you then want to do any sort of analysis as to how similar some documents are, you quickly run into trouble. Why use cosine similarity instead? Cosine similarity measures the difference in vector orientation Cosine similarity uses the cosine of the angle between two vectors. If there is no difference between vector orientation - assume pure topics again - as it would be between the average car document and the short "this is a car" document, no matter the vector length, then the angle is zero. The cosine of zero is 1 - their cosine similarity is high, therefore being a potentially better measure of document similarity than euclidian distance (and more easily interpreted difference as it is [1,-1] - good luck figuring out what a euclidian distance of 12453523 in a 245962-dimensional vector space means). If we go back to the pilot document, its vector has a 3 on the "cars" dimension, and a higher score of "planes". Its cosine similarity with the average car document will be lower than 1, since they are somewhat dissimilar. When it comes to measuring similarity, that is exactly the result you want. You can check out this tutorial on cosine similarity for a deeper dive with python examples. There are other distance, i.e. similarity, metrics - see this notebook by the gensim team .
