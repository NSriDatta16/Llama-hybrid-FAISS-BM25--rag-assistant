[site]: crossvalidated
[post_id]: 231171
[parent_id]: 
[tags]: 
Machine learning- Calculating predictive accuracy - cross validation vs accuracy on unseen data?

Which is the best/most reliable representation of a model's predictive accuracy, the accuracy based on the n-fold cross validation or the model's accuracy on an unseen dataset? At a glance, I would definitely have thought that predicting the unseen dataset would be a better measure of predictive accuracy but on second thoughts it seems likely that this will be smaller than the training dataset and hence more prone to anomalies causing mis-representations in the model's predictive power. Any references in the answer would be greatly appreciated. EDIT: Describing current setup - With a dataset of 100k , I randomly selected 10% and removed these building the model on the remaining dataset. I used the 10 % to generate predictions and calculate the accuracy, which of course differs from the accuracy of predictions from cross validation. Which of these realistically represents the model's predictive accuracy more? Thanks
