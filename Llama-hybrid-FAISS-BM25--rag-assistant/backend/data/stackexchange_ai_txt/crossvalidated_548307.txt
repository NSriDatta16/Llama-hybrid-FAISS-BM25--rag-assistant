[site]: crossvalidated
[post_id]: 548307
[parent_id]: 548269
[tags]: 
The way to test a machine learning model, to determine the expected accuracy on different data, is to use a third set of data that has not been used for training and validating. What is the difference between test set and validation set? If you use the test set as some sort of second layer of cross validation, to filter different algorithms according to their performance, then it is not a valid test but acts effectively as a validation set. This happens indeed with Kaggle competitions or p-values in research, they are susceptible to publication bias . There is no real formal way to deal with this. The rigorous way to solve it is re-test it with a fourth test. A simple way is to set stricter standards (it is one of the reasons for the $5\sigma$ standard: Origin of "5$\sigma$" threshold for accepting evidence in particle physics? ). If this happens in your own research. For selecting algorithms you could save data by selecting the algorithms along with the hyper-parameter tuning. The choice between algorithms could be seen as some sort of hyper-parameter tuning (the choice of model being the parameter).
