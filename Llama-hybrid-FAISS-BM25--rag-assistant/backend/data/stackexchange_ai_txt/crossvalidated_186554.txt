[site]: crossvalidated
[post_id]: 186554
[parent_id]: 
[tags]: 
Single lagged PACF spike: Why select MA term in model and not AR term?

I'm trying to understand the specific choice of model in an example in the book "Applied Econometric Time Series" by Enders, chapter 2 ("Stationary time-series models"), section 7 ("Sample autocorrelations of stationary series -- Estimation of an AR(1) Model", around page 90 in my copy). In this section the sample ACF and PACF of a time series are presented (figure 2.3, top two panels). The ACF is relatively rapidly and steadily decaying, and the PACF has a large spike at lag 1. Larger lags in the PACF are not significant (against sample variance of PACF $T^{-1}$), except for a significant spike at lag 12. To me, this seems to be consistent with an AR(1) process, except for the lag 12 in the PACF, which indicates a seasonality effect. So far so good. Now, Enders selects and comparatively assesses two models: Model 1. An AR(1) model: $y_t = a_1 y_{t-1} + \epsilon_t$ Model 2. An AR(1) model with added moving-average term at lag 12: $y_t = a_1 y_{t-1} + \epsilon_t + \beta_{12} \epsilon_{t-12}$ My questions: Given that the spike at lag 12 is in the PACF, not in the ACF, shouldn't the PACF spike at lag 12 be accounted for with an autoregressive term at lag 12, i.e., model 3: $y_t = a_1 y_{t-1} + a_{12} y_{t-12} + \epsilon_t$? Can you please explain why model 2 would be preferred over model 3 with the ACF/PACF pattern described above? I always thought spikes in the ACF can be explained by forcing (moving average) terms $\beta_j \epsilon_{t-j}$, and spikes in the PACF with autoregressive terms $a_j y_{t-j}$. Is this notion false?
