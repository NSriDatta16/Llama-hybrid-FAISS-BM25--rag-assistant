[site]: datascience
[post_id]: 23521
[parent_id]: 
[tags]: 
Does Kernels always map data points to higher dimensions

When we use kernels in SVM to linearly sperate non linear data points by mapping it to 'another dimensions', does this suitable 'another dimensions' always be a higher dimension with respect to original dimension of the data points? And is it true that we can always find a higher dimensions that can linearly seperate data points in a training set?
