[site]: crossvalidated
[post_id]: 429311
[parent_id]: 
[tags]: 
Preventing information leakage when scaling a time series?

I have a time series $S_i$ that I want to train a regressor on to predict the next point in the time series. I want to split the data into training and validation sets, and also scale the data in the range $[0, 1]$ . The order of processing + training I have is the following Split $S_i$ into training and validation sets. The first 80% of the data is training, while the last 15% of the data is validation. The 5% left out acts as a "buffer" to ensure the two data sets are not too "close" to each other, chronologically speaking. This should help mitigate information leaking from the validation set into the training set Learn a MinMax scaler on the training data only, and apply this scaler to the training data and validation data Train a model on the training data, validate on the validation data The issue with the process above, is that even though we only learn a scaler on the training data in step 2), I feel there is some information leakage within the training data; if the model is set up so that it iterates through every point $S_i$ in the scaled training data and attempts to predict the next point $S_{i+1}$ , then some information from the scaler (which has used all data points in the training data) has leaked into the entire training data set. One way that I have thought to get around this is to re-learn a MinMax scaler at every iteration of training using only the past data points that the model has already "seen". But this seems inefficient. Is there a better way to prevent information leakage when scaling? Is it necessary to prevent information leakage of this kind?
