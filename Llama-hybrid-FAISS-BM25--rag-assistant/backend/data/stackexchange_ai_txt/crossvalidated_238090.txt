[site]: crossvalidated
[post_id]: 238090
[parent_id]: 
[tags]: 
Why are my VAR models working better with nonstationary data than stationary data?

I'm using python's statsmodels VAR library to model financial time series data and some results have me puzzled. I know that VAR models assume the time series data is stationary. I inadvertently fit a non-stationary series of log prices for two different securities and surprisingly the fitted values and in-sample forecasts were very accurate with relatively insignificant, stationary residuals. The $R^2$ on the in-sample forecast was 99% and the standard deviation of forecast residual series was about 10% of the forecast values. However, when I difference the log prices and fit that time series to the VAR model, the fitted and forecast values are far off the mark, bouncing in a tight range around the mean. As a result, the residuals do a better job forecasting the log returns than the fitted values, with the standard deviation of the forecast residuals 15X larger than the fitted data series a .007 $R^2$ value for the forecast series. Am I misinterpreting fitted vs. residuals on the VAR model or making some other error? Why would a non-stationary time series result in more accurate predictions than a stationary one based on the same underlying data? Iâ€™ve worked a good bit with ARMA models from the same python library and saw nothing like this modeling single series data.
