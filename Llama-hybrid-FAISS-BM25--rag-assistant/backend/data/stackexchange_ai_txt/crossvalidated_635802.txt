[site]: crossvalidated
[post_id]: 635802
[parent_id]: 41436
[tags]: 
From a theoretical perspective, the cutoff points for a Likert scale item are completely arbitrary. The tradition in psychology for scales like yours is to use something between 5 to 7 items , but there is no saying that a 3-point scale or a 5-point scale can have complete superiority over one another (indeed, my field uses a lot of binary tests simply because literacy measures often don't require that much flexibility). One does have to make certain tradeoffs between: The item having too few levels that it doesn't capture the construct of interest well (e.g. "Are you happy?" with just a binary response probably isn't effective). Having too many levels that the the levels effectively become redundant (e.g. the same question rated on a scale from 1 to 200 is similarly weird to do). On the data side of things, there are certain features about the number of items which lend itself to nice properties. If a scale consists of binary items, then answering them becomes simpler and you can use them quite easily for things like by-trial logistic regressions. However, because of the lack of variance in such an item (it can only be equal to $0$ or $1$ ), increasing the number of items allows one to increase the level of estimable variance per item, and it may force choices which the user does not meaningfully report (they can also be a pain if you want to estimate factor-analysis based reliability with several items). At least one study has shown that using more than six items generally doesn't improve responses and a smaller number of response levels may have a negative affect on responses as well. However, this isn't a law and would vary by research context. The real answer becomes whether or not the levels properly account for the construct of interest both per-item and as a whole (if there are many items like it). Much of this can be answered by either pilot testing the data with multiple forms, then see which has better bandwidth-fidelity, or using well-validated tests, which have across-study validity and reliability. Following up testing with questionnaires about the measures for the subjects to discuss can also be a helpful option to get directly into the minds of the users. McDonald's book on test theory has some great parts in the initial three chapters about the rules of item construction, when levels become too simple/redundant, sample size effects, and how to think of the items in terms of a composite, so I put that book as a reference below for educational purposes. Above all, I must stress that reducing down levels post-hoc, after the study has already been conducted, is a terrible practice and should never be done. I note that because your question/comments seems to indicate that this is what you will be doing, which makes little sense to do. This necessarily reduces the amount of variation in response after it has already been recorded. You also can't change what people have dictated in that moment to be their choice. You're effectively "reading their minds for them" by falsely changing their responses to something more/less strong than what it was. The fact that this was for a qualitative study makes the purpose of this even more uncertain. References McDonald, R. P. (1999). Test theory: A unified treatment. Routledge. Simms, L. J., Zelazny, K., Williams, T. F., & Bernstein, L. (2019). Does the number of response options matter? Psychometric perspectives using personality questionnaire data. Psychological Assessment, 31(4), 557–566. https://doi.org/10.1037/pas0000648 Sullivan, G. M., & Artino, A. R. (2013). Analyzing and interpreting data from likert-type scales. Journal of Graduate Medical Education, 5(4), 541–542. https://doi.org/10.4300/JGME-5-4-18
