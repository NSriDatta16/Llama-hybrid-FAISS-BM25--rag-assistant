[site]: crossvalidated
[post_id]: 2059
[parent_id]: 
[tags]: 
Learning parameters of a mixture of Gaussian using MLE

It seems that MLE (via EM) is widely used in machine learning / statistics to learn the parameters of a mixture of Gaussians. I'm assuming we're given random samples from the mixture. My question is: Are there any proven quantitative bounds on the error in terms of the number of samples (and perhaps the parameters of the Gaussian)? For example, what is the runtime required to estimate the parameters up to a certain error? Ideally, these bounds would not assume that we start in a local neighborhood of the optimal solution or any such thing. (If EM is not the method of choice and there is a better way of doing it, please point this out, as well.)
