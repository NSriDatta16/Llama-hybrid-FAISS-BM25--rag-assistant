[site]: crossvalidated
[post_id]: 503477
[parent_id]: 
[tags]: 
Online learning problem formulation

I have been wondering about the usual formulation of online machine learning problems, as written in Wikipedia or in other papers I've read. What bugs me is the fact that this problem is written as the following expectation/integral minimization problem (in opposition to offline/batch learning which is written as a finite sum minimization problem) : $$\min_{\theta} \quad \mathbb{E}[V(f(x),y)] = \int V(f(x),y)dp(x,y)$$ I am not sure of the reason why the actual computations made in the online learning algorithm (let's say SGD for instance) do indeed correspond to the minimization of the expected risk as written above. I am guessing that the reason why this problem is formulated as such is because of the Law of Large Numbers, but even if all of the data points are sampled according to the same distribution $p(x,y)$ , the fact that the estimated function $\widehat f$ is updated every step mean that the pairs $(\widehat f(x), y)$ do not follow the same distribution and thus the LLN hypotheses are not met. Sorry if I'm not being clear enough, but what I'm asking is essentially : what theoretical guarantees do we have that online learning algorithms minimize the expected risk as written above ? It makes sense intuitively, but if you could give me resources that give more precisions on that result I'd be grateful.
