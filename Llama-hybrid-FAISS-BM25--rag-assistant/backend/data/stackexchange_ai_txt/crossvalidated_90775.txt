[site]: crossvalidated
[post_id]: 90775
[parent_id]: 90659
[tags]: 
Why is the AUC for A better than B, when B "seems" to outperform A with respect to accuracy? First, although the cut-off (0.5) is the same, it is not comparable at all between A and B. In fact, it looks pretty different from your histograms! Look at B: all your predictions are Second, why is B so accurate? Because of class imbalance. In test B, you have 19138 negative examples, and 6687 positives (why the numbers are different in A is unclear to me: missing values maybe?). This means that by simply saying that everything is negative, I can already achieve a pretty good accuracy: precisely 19138 / (19138 + 6687) = 74%. Note that this requires absolutely no knowledge at all beyond the fact that there is an imbalance between the classes: even the dumbest model can do that! And this is exactly what test B does at the 0.5 threshold... you get (nearly) only negative predictions. A is more of a mixed bag with. Although it has a slightly lower accuracy, note that its sensitivity is much higher at this cut-off... Finally, you cannot compare the accuracy (a performance at one threshold) with the AUC (an average performance on all possible thresholds). As these metrics measure different things, it is not surprising that they are different. So, how do I really judge/compare the classification performances of A and B? i mean, do i use the AUC value? do i use the acc value? and why? Furthermore, when I apply proper scoring rules to A and B, B outperforms A in terms of log loss, quadratic loss, and spherical loss (p You have to think: what is it you really want to do? What is important? Ultimately, only you can answer this question based on your knowledge of the question. Maybe AUC makes sense (it rarely really does when you really think about it, except when you don't want to make a decision youself but let others do so - that's most likely if you are making a tool for others to use), maybe the accuracy (if you need a binary, go-no go answer), but maybe at different thresholds, maybe some other more continuous measures, maybe one of the measures suggested by Frank Harrell... as already stated, there is no universal question here. The ROC graph for A looks very smooth (it is a curved arc), but the ROC graph for B looks like a set of connected lines. Why is this? Back to the predictions that you showed on the histograms. A gives you a continuous, or nearly-continuous prediction. To the contrary, B returns mostly only a few different values (as you can see by the "spiky" histogram). In a ROC curve, each point correspond to a threshold. In A, you have a lot of thresholds (because the predictions are continuous), so the curve is smooth. In B, you have only a few thresholds, so the curve looks "jumps" from a SN/SP to an other. You see vertical jumps when the sensitivity only changes (the threshold makes differences only for positive cases), horizontal jumps when the specificity only changes (the threshold makes differences only for negative examples), and diagonal jumps when the change of threshold affects both classes.
