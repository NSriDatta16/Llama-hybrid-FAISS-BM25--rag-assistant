[site]: datascience
[post_id]: 123831
[parent_id]: 
[tags]: 
Which Frameworks/Libs Best Support Integer-Based Features, Scaling, Training, etc?

Papers such as Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference have interested me in exploring integer-based data science. In particular, I'm thinking of econometric analysis and deep learning for time series data on modern CPUs (with AVX vectorization, etc), using mainly integer-based math for feature engineering (including scaling/normalization), weights, training, and inference. My main areas of application would be classification, anomaly detection, and forecasting. There would likely be a tradeoff of accuracy for runtime performance, which is specifically the area I want to explore. I am aware of practices such as training with floating-point values then exporting to integer-based inference (as mentioned in the paper), but I am looking rather for avenues which would allow me to utilize common frameworks to perform the entire ML lifecycle (or most of it) with integers. That said, I don't mind dropping down to python or c++ to override some pluggable steps in the process, if 80-90% of the job is still handled by the framework. I understand that TensorFlow utilizes the eigen library for linear algebra and that (on its own) this lib supports integer-based arrays & structs. However, I couldn't find any resources online showing (for example) the creation and training of a RNN or a 1d CNN on tabular data that didn't assume the use of floating-point numbers. Perhaps it is as easy as declaring integer-based primary structures and then proceeding from there with framework-provided methods? Or, maybe the community is aware of other frameworks with better support for this venture. Which frameworks/libs best support integer-based data science?
