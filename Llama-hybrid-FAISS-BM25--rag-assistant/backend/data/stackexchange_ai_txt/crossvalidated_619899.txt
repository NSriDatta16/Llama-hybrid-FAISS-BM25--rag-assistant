[site]: crossvalidated
[post_id]: 619899
[parent_id]: 
[tags]: 
Marginalization of latents in SEM

As I understand it, there are tricks to marginalize-out latent quantities in structural equation models, but I'm having trouble finding explanations of how this is achieved that aligns with both my minimal math background and typical use case. So I thought I'd post a couple examples here showing how I code things without marginalization and see if folks could show how marginalization would be achieved. Generally I've come to like an approach to SEM identification whereby I model all latents as standard normals, weights as correlations, and positive constrain one weight coming from each latent. First up is a model with three levels of latents: with Stan code: data{ int n_id ; int n_obs ; matrix[n_obs,n_id] a_obs ; matrix[n_obs,n_id] b_obs ; matrix[n_obs,n_id] c_obs ; matrix[n_obs,n_id] d_obs ; } parameters{ // common latent vector[n_id] abcd ; // positive-constrained correlations real r_abcd_ab ; real r_ab_a ; real r_cd_c ; // unconstrained correlations (using unit-vector reduces compute) unit_vector[2] r_abcd_cd ; unit_vector[2] r_ab_b ; unit_vector[2] r_cd_d ; // helpers (residuals on unit-scale) vector[n_id] ab_ ; vector[n_id] cd_ ; vector[n_id] a_ ; vector[n_id] b_ ; vector[n_id] c_ ; vector[n_id] d_ ; // across-id SDs of the observed variables real a_sd ; real b_sd ; real c_sd ; real d_sd ; // common across-observation measurement noise real noise ; } model{ // priors abcd ~ std_normal() ; ab_ ~ std_normal() ; cd_ ~ std_normal() ; a_ ~ std_normal() ; b_ ~ std_normal() ; c_ ~ std_normal() ; d_ ~ std_normal() ; a_sd ~ weibull(2,1) ; b_sd ~ weibull(2,1) ; c_sd ~ weibull(2,1) ; d_sd ~ weibull(2,1) ; noise ~ weibull(2,1) ; // abcd to ab & cd vector[n_id] ab = ( abcd * r_abcd_ab + ab_ * sqrt(1-square(r_abcd_ab)) // for contrained correlations, need to compute this explicitly ) ; vector[n_id] cd = ( abcd * r_abcd_cd[1] + cd_ * r_abcd_cd[2] // for unconstrained correlations, computed automatically ) ; // ab to a & b vector[n_id] a = ( ( ab * r_ab_a + a_ * sqrt(1-square(r_ab_a)) ) * a_sd ) ; vector[n_id] b = ( ( ab * r_ab_b[1] + b_ * r_ab_b[2] ) * b_sd ) ; // cd to c & d vector[n_id] c = ( ( cd * r_cd_c + c_ * sqrt(1-square(r_cd_c)) ) * c_sd ) ; vector[n_id] d = ( ( cd * r_cd_d[1] + d_ * r_cd_d[2] ) * d_sd ) ; // likelihood for(i in 1:n_id){ a_obs[,n_id] ~ normal(a[n_id],noise) ; b_obs[,n_id] ~ normal(b[n_id],noise) ; c_obs[,n_id] ~ normal(c[n_id],noise) ; d_obs[,n_id] ~ normal(d[n_id],noise) ; } } And the second has similar latent structure as the first, with the twist that multiple latents are connected to a given observed variable in a simple well-identified manner: Stan code: data{ ... // same as first model matrix[n_obs,n_id] a_minus_b_obs ; matrix[n_obs,n_id] a_plus_b_obs ; matrix[n_obs,n_id] c_minus_d_obs ; matrix[n_obs,n_id] c_plus_d_obs ; } ... // same as first model model{ ... // same as first model // likelihood for(i in 1:n_id){ a_minus_b_obs[,n_id] ~ normal(a[n_id]-b[n_id],noise) ; a_plus_b_obs[,n_id] ~ normal(a[n_id]+b[n_id],noise) ; c_minus_d_obs[,n_id] ~ normal(c[n_id]-d[n_id],noise) ; c_plus_d_obs[,n_id] ~ normal(c[n_id]+[n_id],noise) ; } } I feel like if I could see how a marginalized version of each of these is achieved, that would give me the much better leverage to understand marginalization in SEM. Thanks in advance!
