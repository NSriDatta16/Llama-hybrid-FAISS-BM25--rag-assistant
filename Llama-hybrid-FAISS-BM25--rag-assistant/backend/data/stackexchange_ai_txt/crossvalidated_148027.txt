[site]: crossvalidated
[post_id]: 148027
[parent_id]: 147982
[tags]: 
This problem could presumably be solved many different ways. Side-stepping the naive Bayes component of the question for a moment, it seems to me you need to do "feature selection" rather than dimensionality reduction, as you seem interested in knowing how relevant each of your features are. If you want to assess how important features are for your task, I think you should do that in a supervised setting rather than an unsupervised setting (PCA is unsupervised). A super-simple approach would be to use logistic regression with an L1 penalty on the weights (or coefficients). Used in this setting, you could do feature selection this way. The L1 penalty on the weights would force some of the coefficients to be zero and only the relevant features would end up with non-zero coefficients. Perhaps this logistic regression approach would yield a good classifier, or you could take the features that had non-zero weights here and use them in another approach (such as naive bayes). At this point, many of the standard software tools seem to let you use a version of the "lasso" function with binary output data (for matlab, see lassoglm ). If you want proper background on this approach, there are some references here (the 2008 paper basically covers the logistic regression setting).
