[site]: crossvalidated
[post_id]: 190311
[parent_id]: 
[tags]: 
How to combine perceptron weights and probabilities

I am working on machine transliteration using structured perceptron to learn the model parameters at word level. I am using beam-search to extract the top $n$ transliterations of a word and would like to decode the best sequence at sentence level using noisy channel model. My noisy channel model is a combination of two scores: $p(f|e)$, which is a perceptron score in my case, and $p(f)$, which is a language model score. Since perceptron scores do not belong to a probability distribution, I tried to use softmax. However softmax assigns $\sim1$ probability to the first best almost always and assigns very low probability to other words. In this case my best sequence at sentence level will mostly comprise first best words suggested by structured perceptron. In this regard, I want to know if there is a way to combine perceptron scores and language model scores which can help me to decode the best sequence at sentence level?
