[site]: crossvalidated
[post_id]: 563531
[parent_id]: 
[tags]: 
When is it okay to accept overfit model for production?

I am working on a binary classification problem using random forests (75:25 class proportion).m label 0 is minority class. I am following the below approach a) execute RF with default hyperparameters b) execute RF with best hyperparameters (GridsearchCV with stratified K fold and scoring was F1 ) While with default hyperparameters, my train data was overfit as can be seen from the results below. But, I went ahead and tried the default parameters in test data as well (results below) default hyperparameters - Test data confusion matrix and classification report Later, I used GridesearchCV to get the best hyperparameters based on 10 splits and stratified Kfold. However, the performance is poor for test data (using best hyperparameters) Best hyperparameters - Test confusion matrix and classification report So, my question is The results makes me feel like it is okay to stick with the overfit model as it provides me relatively good performance on test data (when compared to best parameter model because it performs poorly). In this case, should I go ahead with the overfit model with default parameters? Because when my business looks at the results, they would definitely wish for overfit model (of course, I didn;t show them yet). But looking at the results of ML model, it's a no brainer that overfit model is better and seems to help them better than non-overfit model b) As my dataset is imbalanced, I chose a scoring= f1. But looks like the model works to maximize the **f1-score only for majority class (Label 1) **. How can I input it to the model to indicate that it should maximize the metrics like recall and precision for label 0 (minority class) or should I invert my labels? Meaning, make 0's as 1 and 1's as 0? c) Am I doing any mistake with model building because this sort of result is impossible to obtain? update - code for best hyparameters from sklearn.model_selection import GridSearchCV param_grid = { 'n_estimators': [100,200,300,500], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth' : [4,5,6,7,8], 'criterion' :['gini', 'entropy'] } skf = StratifiedKFold(n_splits=10, shuffle=False) model = GridSearchCV(rfc,param_grid=None,cv = skf, scoring='f1') model.fit(ord_train_t, y_train) print(model.best_params_) print(model.best_score_) rfc = RandomForestClassifier(random_state=42, max_features='sqrt', n_estimators= 500, max_depth=8, criterion='gini') rfc.fit(ord_train_t, y_train) y_train_pred = rfc.predict(ord_train_t) y_test_pred = rfc.predict(ord_test_t) y_train_proba = rfc.predict_proba(ord_train_t) y_test_proba = rfc.predict_proba(ord_test_t) code for default hyparameters rfc = RandomForestClassifier() rfc.fit(ord_train_t, y_train) y_train_pred = rfc.predict(ord_train_t) y_test_pred = rfc.predict(ord_test_t) y_train_proba = rfc.predict_proba(ord_train_t) y_test_proba = rfc.predict_proba(ord_test_t)
