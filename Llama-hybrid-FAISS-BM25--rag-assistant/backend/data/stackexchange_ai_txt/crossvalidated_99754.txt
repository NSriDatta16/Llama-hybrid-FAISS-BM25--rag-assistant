[site]: crossvalidated
[post_id]: 99754
[parent_id]: 
[tags]: 
Supervised classification with different numbers of predictors per observation

I would like to build a supervised classifier on data that contains a different number of predictors depending on the observation, and I'm looking for the best approach to use. For example (which isn't quite my real dataset - mine is biology), imagine I want to build a learner for people's road accident risk based on the features of their cars. Clearly people can have more than one car (and the number of cars can by itself be a predictor), and I'm wondering what approaches exist for using the features of each of their cars as predictors. I can of course just use some kind of pooling strategy for the features (for example, counting the total number of air fresheners and having a binary variable describing whether or not at least one car has a spoiler), but what if, say, the least accident-prone people have one car that's a convertible and one that's a family van, with a different set of features in each? Pooling strategies won't describe this situation precisely, leading to a 'dilution' of the best set of predictors. Another strategy that comes to mind is models with missing data - in which case I could have a variable for each feature of each type of car, which could be set to NA for people who don't own a certain type. But what if somebody has two cars of the same type and this, in turn, is different from having just one with respect to their accident risk? In addition, random forests, for example, while allowing for missing data, lose the ability to assess variable importance when it is present - which isn't great in my situation. Surely people across disciplines - including actuarial science I suppose - have come across this type of problems? I would greatly appreciate if you could point me to some relevant papers/books.
