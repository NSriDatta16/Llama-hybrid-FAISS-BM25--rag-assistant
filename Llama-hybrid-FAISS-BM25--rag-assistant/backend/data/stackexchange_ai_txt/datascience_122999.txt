[site]: datascience
[post_id]: 122999
[parent_id]: 122995
[tags]: 
The sequence length is usually marked by a special token called "EOS" (end-of-sequence) appearing just after the last token in the sentence and before any padding tokens. In your autoencoder, the loss (presumably the categorical cross-entropy) will make the model learn to generate the same output as the input, including the EOS token. This way, the model learns to generate EOS tokens like any other token in the sentence. At inference time, you stop decoding once the model generates an EOS token.
