[site]: crossvalidated
[post_id]: 300083
[parent_id]: 300079
[tags]: 
Your question is difficult to understand without the context. It is about using the Caravan data set to predict purchasers of caravan insurance What the book says is that using KNN with $K=1$ gives the following results > table(knn.pred,test.Y) test.Y knn.pred No Yes No 873 50 Yes 68 9 which means that, after basing the model on $4882$ individuals in the training set and applying that model to the $1000$ other individuals in the test set, it predicts $68+9=77$ people will purchase insurance, and it gets $9$ of these correct, a proportion $\frac9{77}=11.7\%$ correct If it had been guessing insurance purchasers at random, it would have got an expected proportion $\frac{59}{1000}=5.9\%$ of those predicted to buy correct, about half the KNN figure. So it says KNN did better than random guessing It goes on to say using KNN with $K=3$ would have led to $\frac{5}{26}=19.2\%$ while using $K=5$ would have led to $\frac{4}{15}=26.7\%$ correct It does not say that with the same problem logistic regression would have a success rate of around $50\%$. Rather the opposite: As a comparison, we can also fit a logistic regression model to the data. If we use $0.5$ as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of $0.5$. If we instead predict a purchase any time the predicted probability of purchase exceeds $0.25$, we get much better results: we predict that $33$ people will purchase insurance, and we are correct for about $33\%$ of these people. This is over five times better than random guessing! The moral of the story illustrated by this part of the book is that some questions are difficult, but with suitable methods and good parameters, some machine learning can still produce results which are substantially better than random guessing
