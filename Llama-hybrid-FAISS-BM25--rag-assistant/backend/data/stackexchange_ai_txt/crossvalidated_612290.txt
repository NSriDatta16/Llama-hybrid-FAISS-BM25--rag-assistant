[site]: crossvalidated
[post_id]: 612290
[parent_id]: 
[tags]: 
PCA via a Neural Network

In a simple neural network, having more nodes on an input layer that on the next layer performs a compression or dimension reduction similar to what PCA does. The fewer nodes encode in a combination some kind of information that is in the previous layer. While the forward computation is structurally similar to PCA , the weights form a matrix, it is not equivalent. That is, an autoencoder reduces dimensions as does PCA, but there is no gurantee of orthogonality or correspondence to eignevalues. Is there an activation function and loss function for one layer (or larger more complicated architecture and choice of activation and loss functions and backprop alternative) that does converge to the PCA coefficients? That is, is there some way to get a weight matrix that is orthogonal -and- the next level of nodes correspond to the eigensystem (sortable by eigenvalues)? The motivation is to 'do everything' with a neural network architecture rather than use processes outside of the NN model. This way one could remove collinearity for modeling non-linear subspaces.
