[site]: crossvalidated
[post_id]: 394833
[parent_id]: 257421
[tags]: 
I've had similar problems, have you tried checking: Are your layers really wired correctly? Is the output layer really getting the signal? As pointed out by @D.W., do check the loss function -- cross entropy is the gold standard indeed Have you checked how the inputs look like? Have you encoded the input vectors (words) correctly? Are you using too much regularization at some point? What about the vocabulary, do its dimensions match with the e.g., embedding layer? Reproduce a minimal $\textbf{working}$ example Check if weights change in time Does loss change? If not, this is the core problem and the architecture does not learn from the input at all.
