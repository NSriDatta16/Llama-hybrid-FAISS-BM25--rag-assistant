[site]: crossvalidated
[post_id]: 613532
[parent_id]: 
[tags]: 
Given an AR(1) process, find the density of the first observation

I'm studying ML estimation and I have a silly question that I'm not able to see its solution. Suppose I have an AR(1) process: $$y_t = c+ \phi y_{t-1}+ u_t, \quad u_t \overset{iid}{\sim}\hbox{Normal}(0,\sigma^2)$$ The first step is find the density of $y_1$ : $$y_1 = c+ \phi y_{0}+ u_1, \quad u_1 \sim \hbox{Normal} (0,\sigma^2)$$ According to this, I have that $y_1 \sim \hbox{Normal}(c+ \phi y_{0}, \sigma^2)$ , since a normal with mean zero plus a constant gives a mean translation by the constant and the variance remains unchanged. (This is my reasoning) But the Hamilton book (Time series Analysis) says that $y_1 \sim \hbox{Normal}\left( \frac{c}{1-\phi}, \frac{\sigma^2}{1-\phi^2}\right)$ . The justification that the book gives is that the AR(1) given above is such that its mean is $\frac{c}{1-\phi}$ and its variance is $\frac{\sigma^2}{1-\phi^2}$ , regardless of $t$ . Ok, this I understand by the stationarity of AR(1), but this is inconsistent with my reasoning. Well, I'm probably making a huge mistake in my reasoning and I'd like to know where I'm going wrong.
