[site]: datascience
[post_id]: 128321
[parent_id]: 128274
[tags]: 
The short answer: engineered prompts are deliberately similar to fragments from training data, and this similarity is sufficient to elicit desired responses. Prompts do not need to literally appear in the training data. Suppose we have a series of answers to some question, and we are training a question-answering language model. The model learns that facets of the answers are related to the question, and it also learns that facets of the answers are internally related to other facets of themselves. This latter learned behavior allows the start of an answer to causally influence the rest of that answer. Now, suppose that some answers are written with qualifications at the start. For example, answers might start with "in my experience doing computer science," or "as a computer scientist," or "I am a computer scientist, and". These starts are similar to each other, and we can make a probabilistic generalization about their continuations: answers which start this way are likely to continue a certain way. For the prompt engineer, this is usually enough to be able to infer that a prompted answer could start with something like, "I am a computer scientist." in order to get answers which are continued as if they were originally written with that starting prompt. Note that this engineered prompt may not yet return decent answers. It could be the case that anybody who self-identifies as a computer scientist also writes poor answers, in which case this prompt would decrease the quality of completions. Prompt engineers have to consider word choice , just like writers in other disciplines. Also note that part of your question is really about the usage of commands to the second-person "you". This is specific to models which have post-training (usually RLHF); they are trained to have an additional conversational behavior which doesn't necessarily exist in their training data. If I am writing on my own, then I can use a first-person perspective; I only need to talk to you in a conversational context. This is a non-trivial insight required to get conversations out of models which weren't trained for it; the model needs to be told who is "I" and "you", requiring additional prompting, framing, and parsing.
