[site]: crossvalidated
[post_id]: 450923
[parent_id]: 450911
[tags]: 
I cannot confirm your assertion. For instance, if you want to estimate a multinomial logit model using the the maximal likelihood estimator you will naturally arise to “an entropy based method”: For instance, suppose that there are $K$ classses and the dimension of the input is $d$ . We usually assume the following latent model $$z_{1\times K} = x_{1\times d} W_{d\times K} + b_{1\times K}.$$ We apply the softmax function to $z$ , i.e. , $$y_k=P(C_k/z)=softmax(z_k)=\frac{e^{z_k}}{\sum_{j=1}^{N}e^{z_j}}.$$ So, as usual in frequentist statistics, we want to maximize the likelihood of $$p(T/w_1,\cdots,w_K)=\prod_{n=1}^{N} \prod_{k=1}^{K} P(C_k/z)^{t_{nk}}=\prod_{n=1}^{N} \prod_{k=1}^{K} y_{nk}^{t_{nk}}.$$ As usual, if you apply the log to the above expression you get the cross-entropy. This is only a example. Off course, you can use this likelihood in a Bayesian method as well or in a machine learning setup (this setup is very common to train neural networks in machine learning or the same multinomial methods)... I believe that entropy methods arise naturally in several situations and it is difficulty to say surelly in which one it arises more. Other frequentist examples: 1) Fisher information is related to the relative entropy. 2) Kullback–Leibler divergence (relative entropy) is also used to compare two distributions. But this also can be used in the bayesian context to measure the learning process in Bayesian statistics. Machine learning tipically uses methods that come from frequentist statistics and Bayesian statistics. I guess that the ones that come from computer science prefer the first and the ones that come from statistics prefer the second.
