[site]: datascience
[post_id]: 29262
[parent_id]: 29127
[tags]: 
I don't necessarily think it's that batch normalization is necessarily stochastic, but rather just how batch normalization works when compared to the inference stage. As you may know, during the training phase batch normalization depends on the mini-batch. However, that dependency is undesirable for inference so instead a moving average over all mini-batches is taken. Doing this obviously can cause problems since what your model is inference on is different than what it was trained on. Furthermore, the smaller your mini-batch the worse the performance gets with more depth since the the inaccuracies computed for the smaller batch just get compounded more and more. If you insist on having a batch normalization phase, a way to help mitigate this might include implementing a batch re-normalization setup instead.
