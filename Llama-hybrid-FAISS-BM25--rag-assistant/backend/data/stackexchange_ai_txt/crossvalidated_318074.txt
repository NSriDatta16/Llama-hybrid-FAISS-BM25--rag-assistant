[site]: crossvalidated
[post_id]: 318074
[parent_id]: 
[tags]: 
Advice on applying Machine learning for high dimentional datasets

I am working with a data-set of around ~100000 observations(rows) and ~256 features(columns). Is there any recommendation for applying Machine Learning techniques on such a data-set efficiently ? Maybe by parallelization or similar approaches ? I am currently using Matlab for applying different Machine Learning, but have investigated Python's scikit-learn as well for applying: Regression Gaussian processes for regression GPR Classification Linear discriminant analysis LDA Support vector machine SVM Obviously dimentionality reduction comes to mind, however for this specific data-set removing some of the features or applying transformations will distort the information.
