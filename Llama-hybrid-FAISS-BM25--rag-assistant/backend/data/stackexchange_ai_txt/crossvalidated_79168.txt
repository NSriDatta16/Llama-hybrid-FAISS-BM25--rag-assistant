[site]: crossvalidated
[post_id]: 79168
[parent_id]: 
[tags]: 
How exactly is sparse PCA better than PCA?

I learnt about PCA a few lectures ago in class and by digging more about this fascinating concept, I got to know about sparse PCA. I wanted to ask, if I'm not wrong this is what sparse PCA is: In PCA, if you have $n$ data points with $p$ variables, you can represent each data point in $p$ dimensional space before applying PCA. After applying PCA, you can again represent it in the same dimensional space, but, this time, the first principal component will contain the most variance, the second will contain the second most variance direction and so on. So you can eliminate the last few principal components, as they will not cause a lot of loss of data, and you can compress the data. Right? Sparse PCA is selecting principal components such that these components contain less non-zero values in their vector coefficients. How is this supposed to help you interpret data better? Can anyone give an example?
