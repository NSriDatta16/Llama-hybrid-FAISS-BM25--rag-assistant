[site]: datascience
[post_id]: 60621
[parent_id]: 60606
[tags]: 
Here is my current understanding: Q-value is just a value for a particular action taken, and the Value function of a state sums over the Q-value of every action possible in that state. Would this description be accurate? Not quite. First the Q value is a type of value function, it is often called the action value . Both $Q(s,a)$ and the state value function $V(s)$ calculate the expected future return given their parameters, a known environment and a known policy $\pi$ that describes how an agent will select actions in that environment. In the case of action value $Q(s,a)$ , the expected future return is based on the agent taking the action $a$ in state $s$ first, and afterwards following $\pi$ . Whilst the state value $V(s)$ , the expected future return will depend on what action $\pi$ chooses in state $s$ . There a few ways to write the relationship between $V(s)$ and $Q(s,a)$ . If the policy function is deterministic, action in state $s$ given by $\pi(s)$ then: $$V(s) = Q(s, \pi(s))$$ If the policy function is stochastic, with probability of selecting action $a$ in state $s$ given by $\pi(a|s)$ : $$V(s) = \sum_a \pi(a|s)Q(s, a)$$ This is a sum, as you suggest, but weighted by the probability of taking each action.
