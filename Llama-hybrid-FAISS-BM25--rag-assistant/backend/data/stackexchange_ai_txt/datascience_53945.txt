[site]: datascience
[post_id]: 53945
[parent_id]: 53271
[tags]: 
Mahesh's answer is correct, but to be more specific: In the lexicon-based sentiment analysis you start with a lexicon of words (a dictionary, a set of words) for each "sentiment" and you usually do some sort of counting - how many times the words from a given set showed up in a given document/paragraph/sentence. That is a supervised approach, in which you need to define the lexicons first. LDA is an unsupervised approach - you start with just your data (and some additional elements such as priors and the number of topics you expect to find - but this can be data-driven as well) and the algorithm will try to identify the topics. Topics here are distributions over your vocabulary - if you're talking about cars you probably use a different distribution of words than if you're talking about quantum physics. Then each document is a mixture of topics - you have document specific distribution over topics that determines what topics the words in that document "come from". The trick here is that you could potentially end up with topics that have interpretation similar to sentiments - e.g. a "disappointed" topic, or "happy" topic (you look at the distribution over words and provide interpretation for a given topic). Such approach has the advantage of not having to define the lexicons by yourself, it's much more data-driven. That being said, you might end up with topics that do not fit what you're looking for. The solution here would be to use semi-supervised approach, such as in GuidedLDA (also called SeededLDA).
