[site]: crossvalidated
[post_id]: 341545
[parent_id]: 
[tags]: 
Can/when should a trained classifier with summary statistics stand-in for a traditional hypothesis test?

In a project I'm working on, we'd like to show that certain stereotyped anatomical areas (represented by shapes at certain locations) are reproducible enough in terms of shape and location to be "classified". I'd like to add that these areas are generated by a segmentation algorithm that is based on some biological assumptions and that each area is given a certain label according to its shape and location; hopefully this isn't a case of "double-dipping". Currently, we apply the Mann-Whitney U test separately on the distances between areas and again on a landmark-based area metric to each pairwise comparison (with Bonferroni correction) between all identified areas. Independently, I constructed and cross-validated an ensemble classifier that applies k-Nearest Neighbors and a convolutional neural network trained on the shapes; I tested the trained ensemble classifier against one on shuffled labels (the null hypothesis) as a permutation test. Of course, the construction of the ensemble classifier was informed by priors that would elicit good performance (that shape and location were important and thus I should choose kNN and a CNN) with some exploration (Naive Bayes vs LDA vs kNN). The ROC curves and associated statistics are made available for the classifier. Presumably a permutation test or a restricted permutation test can generate p-values for the classifier but I'm unsure how comparable this is to the pairwise comparison and traditional hypothesis testing approach. Is there a compelling reason to use one or the other irrespective of p-value/classifier performance? Is there some resource that describes a theoretical framework of how and when a classifier can/should be used in equivalent or superior capacity to a more traditional statistical test? I was able to find almost no resources on it although this comes close. It seems to me that the traditional Neyman-Pearson hypothesis testing framework only allows for fixed hypotheses to be tested while the ensemble classifier approach tests a huge amount of hypotheses moderated by cross-validation which is presumably sufficient to maintain the latter's validity? I'm still fairly new to this so apologies if I am going about this incorrectly and thanks in advance.
