[site]: crossvalidated
[post_id]: 307215
[parent_id]: 307205
[tags]: 
If you look carefully at the output, you should have read in the console Warning messages: 1: glm.fit: algorithm did not converge 2: glm.fit: fitted probabilities numerically 0 or 1 occurred The output of glm cannot be relied upon. The problem seems due to complete separation of variables. Running a penalized logistic regression with Jeffreys prior penalty yields MAP values: logistf::logistf(formula = Dependent.variable ~ . - Country - Year, data = dat) Model fitted by Penalized ML Confidence intervals and p-values by Profile Likelihood coef se(coef) lower 0.95 upper 0.95 Chisq p (Intercept) 3.369863624 4.46210240 -11.4364696 25.63004559 0.37821187 5.385618e-01 Inflation.CPI. 0.207995387 0.21203529 -0.6598256 1.19474042 0.73101619 3.925540e-01 Debt.GDP 0.004909774 0.01848469 -0.0494921 0.09541694 0.06350028 8.010468e-01 OfficialForexReserves.US.Bil.. -0.191903652 0.07689628 -0.8971011 -0.06585468 22.38454885 2.231622e-06 GrossInvestment.GDP 0.138778700 0.16639479 -0.2263430 1.44319711 0.51060932 4.748752e-01 Gov.Revenue.GDP -0.254483400 0.09539591 -0.6933168 -0.09166023 10.29617094 1.333065e-03 Net.FDI.GDP 0.021054572 0.09666093 -0.3040144 0.40133013 0.04559491 8.309139e-01 Likelihood ratio test=65.10526 on 6 df, p=4.105605e-12, n=69 and the P-values are not ones anymore. See the post How to deal with perfect separation in logistic regression? for more information
