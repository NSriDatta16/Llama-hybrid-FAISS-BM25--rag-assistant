[site]: datascience
[post_id]: 10725
[parent_id]: 10724
[tags]: 
I haven't read the paper you linked, but I have followed a lecture notes by Richard Socher. So, basically that mapping matrix is called weight matrices. There are two weight matrices W1 and W2 for input and output mapping. Thus for each word, vectors in both the matrices are updated via backpropagation. To answer your question, a word is represented by one-hot sparse vector which has the size of Vx1 (V is size of vocabulary), with a value of 1 in one of the position. and when this vector is multiplied with the weight matrix W1 with size NxV , the corresponding embedding vector of size Nx1 (N is size of the required embedding vector) is used in the hidden layer. Document has no one-hot represented vectors. So, basically there is a document matrix D of size Nxd (d is number of documents) where each column represents a document. In other words, the matrices W1 and W2 need the one-hot representation only for mathematical steps, other than that they are representing each word in each column just as the document matrix D .
