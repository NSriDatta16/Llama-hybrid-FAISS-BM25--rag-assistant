[site]: crossvalidated
[post_id]: 627945
[parent_id]: 333018
[tags]: 
As a complement to shimao's answer , which boils down to "the mixture of $q(z|x)$ is reasonably close to the $N(0,I)$ distribution", it is worth noting that nowadays, at least in in the context of using VAEs for generative modeling of images, it is common practice to train another model (after the VAE training is done) which learns the actual $q(z)$ distribution rather than supposing it's "close enough" to the fixed prior. This training is typically carried out via an autoregressive neural network, which allows in turn to easily sample from this "true" $q(z)$ once that network is trained. This is the process used in VQ-VAE , VQ-VAE-2 and DALL-E for example. To quote from the VQ-VAE-2 paper (section 3.2) : Fitting prior distributions using neural networks from training data has become common practice, as it can significantly improve the performance of latent variable models. This procedure also reduces the gap between the marginal posterior and the prior. Thus, latent variables sampled from the learned prior at test time are close to what the decoder network has observed during training which results in more coherent outputs.
