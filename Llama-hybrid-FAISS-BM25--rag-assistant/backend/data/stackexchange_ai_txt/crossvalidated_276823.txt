[site]: crossvalidated
[post_id]: 276823
[parent_id]: 276812
[tags]: 
If your neural network is working as it was trained, why would you alter how it works? There is no reason to change the activation function after training - this might even break the network. In your specific case, the network might be doing something useful with negative activation values. By switching to ReLU, all activations will be >0 , which would harm te network (because there are no negative activation values anymore). Why are you asking this? (i'm curious!)
