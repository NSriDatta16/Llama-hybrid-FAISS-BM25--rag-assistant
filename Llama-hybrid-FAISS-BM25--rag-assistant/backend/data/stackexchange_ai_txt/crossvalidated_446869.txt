[site]: crossvalidated
[post_id]: 446869
[parent_id]: 446867
[tags]: 
Tl;dr, I think you're confusing "policy" for "learning algorithm". Informally speaking, a policy is like your current strategy for playing a game. When you receive some feedback (from either winning or losing the game), then you might adjust and improve your strategy. That is called "learning" a policy or improving a policy. In reinforcement learning, we usually care about learning algorithms which will start with a suboptimal policy, and gradually head towards a good policy. It's understandable that you're confused by the two examples you gave, because they're kind of degenerate cases. You've basically described a multi-armed bandit problem , which is a special case of the MDP (markov decision process) framework when there is only a single state (the state space is a singleton set, not empty). Suppose your strategy (policy) is to always choose rock: $\pi(0) = \text{Rock}$ , where $0$ is the single state. And your opponent always chooses paper in response, so you decide to act on your negative reward signal and change to a new policy $\pi'(0) = \text{Scissors}$ . You could say you have some learning strategy $\pi' \leftarrow L(\pi, \tau)$ which takes the current policy, some trajectories from it, and produces a new and improved policy. However $\pi$ and $\pi'$ are still perfectly valid policies themselves, mapping state to (deterministic) action without relying on reward. Only your learning strategy made use of the reward signal. $\pi(s_t) = \text{argmax}\ Q(s_t,a_t)$ This is, in general, incorrect. Sure, this is one way to define a policy, but it is not the only way. but the function is defined as a function of the reward! To be precise, the $Q$ function is defined with respect to a policy and an environment. So one might write $Q_{\pi,E}$ (although typically the environment is always fixed). So even from a purely mathematical perspective, should be a function of both the state and the reward, as it simply just depends on . That's just composition of functions man! Consider the equation $x = E_y[x+y]$ , where $y$ is a normal random variable with mean $x$ . Then it must be that $x = 0$ . Somehow, the RHS was defined using $y$ , yet we ended up finding that $x$ does not depend on $y$ ! This is an example of why your reasoning doesn't quite work. Also to be precise, $Q$ is not defined in terms of $r$ -- it is defined in terms of the policy and the environment. The environment and policy together produce a distribution over rewards / return, not any single reward value.
