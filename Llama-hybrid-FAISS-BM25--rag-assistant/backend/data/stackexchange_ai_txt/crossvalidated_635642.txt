[site]: crossvalidated
[post_id]: 635642
[parent_id]: 
[tags]: 
Why do we use Linear Models when tree based models often work better than linear models?

In Supervised Machine Learning, and specifically on Kaggle, it is usually seen that tree models often outperform linear models. And even in the tree-based models, it is usually XGBoost that outperforms RandomForest, and RandomForest outperforms DecisionTrees. If this is not true, then please feel free to correct this assumption. These are just my observations and somehow, a bunch of people share this opinion. Why should we even use Linear models such as Linear Regression or Logistic Regression? Specifically, when they supposedly do not perform as well as tree-based models and have more requirements than tree-based models? A similar question can be raised for tree-based models about using DecisionTrees instead of RandomForest or XGBoost. Are there some cases where linear models should be preferred?
