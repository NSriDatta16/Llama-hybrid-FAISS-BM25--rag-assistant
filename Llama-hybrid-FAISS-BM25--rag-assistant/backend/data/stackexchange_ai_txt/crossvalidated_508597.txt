[site]: crossvalidated
[post_id]: 508597
[parent_id]: 449159
[tags]: 
Davidson and MacKinnon in ("Econometric Theory and Methods") address head on the flaw you point out in Wooldridge's presentation: "At this stage as long as we say nothing about the unobserved quantity $u_{t}$ , equation (1.01) [ $y_t=\beta_0+\beta_1x_t+u_t$ ] does not tell us anything. In fact, we can allow the parameters $\beta_0$ and $\beta_1$ to be quite arbitrary, since for any given $\beta_0$ and $\beta_1$ , the model can always be made to be true by defining $u_t$ suitably. If we wish to make sense of the regression model (1.01), then we must make some assumptions about the properties of the error term $u_t$ ... Most commonly it is assumed that, whatever the value of $x_t$ , the expectation of the random variable $u_t$ is zero. This assumption usually serves to identify the unknown parameters $\beta_0$ and $\beta_1$ in the sense that, under this assumption, equation (1.01) can be true only for specific values of those parameters". Only because it addresses the same issue, I'll mention that in his book "Extending the Linear Model with R" (page 7), Faraday claims "The construction of the least squares estimates does not require any assumptions about $\epsilon$ ". This statement is false. The above paragraph explains why. Both authors are world-class experts, so I would file these as typos and nothing more. Why that particular assumption? What is definitional and what do we need to assume? The CEF error $\epsilon_i$ in $Y_i=E[Y_i|X_i]+\epsilon_i$ is always mean independent of $X_i$ . Agrist and Pischke "Mostly Harmless Econometrics": $E[\epsilon_i|X_i]=E[Y_i-E[Y_i|X_i]|X_i]=E[Y_i|X_i]-E[Y_i|X_i]=0.$ That is definitional. But once we assume linearity for the CEF, the linear projection error $u_t$ in $y_t=\beta_0+\beta_1x_t+u_t$ by definition is only uncorrelated with $x_t$ , which is a weaker condition than mean-independence. (See Hansen "Econometrics" for the proof). And since regression is at its heart about the conditional mean function $E[y_t|x_t]=\beta_0+\beta_1x_t+E[u_t|x_t]$ we need to assume mean-independence $E[u_t|x_t]=E[u_t]=0$ in order to identify the conditional mean function. In the case when $x_t$ is a non-random/fixed regressor, the assumption $E[u_t|x_t]=E[u_t]$ is superfluous; it is definitional, hence we only need to assume that $E[u_t]=0$ . Most of the times we are looking to do something with our model and thus we'd want the estimators to have some minimal desirable properties. No other assumptions are necessary to derive the OLS estimators or to show that they are unbiased. To derive the variance of the OLS estimators we need to assume that the errors have a constant variance and are uncorrelated. To construct hypothesis tests or confidence intervals we need to either assume the errors are normally distributed or have a large enough sample so that the OLS estimators are approximately normally distributed.
