[site]: crossvalidated
[post_id]: 294508
[parent_id]: 
[tags]: 
Classification where we are more confident in some training samples than others

Let's say we have a supervised learning binary classification dataset. We are a lot more confident in some training examples than in others being accurate (e.g. some were labelled by several highly skilled humans who analyzed each example they labelled in depth, others were labelled by a low-skilled person who had to label a large number of examples in a short amount of time, likely leading to a lot of mistakes). Only about 15% of examples were labeled by "experts". To make matters worse, the samples were not randomly given to experts or non-experts - the distributions of samples given to experts and non-experts are different (e.g. of the examples judged by experts, 70% of them are labeled with a positive class, vs 10% for examples judged by non-experts). I want to use random forests or gradient boosted trees to learn a model on this data. How should I incorporate this prior knowledge into the model or into the dataset? One way I could think of is to fit a model to the original data, and then take a closer look at the misclassified examples that I am less confident about, and potentially re-label them in the dataset if they are found to have been originally labeled incorrectly. Is that the best way of doing it? Is there some way to, e.g., give more weight to training examples I am more confident about? Will that not cause a problem given that those examples were not randomly sampled from the universe?
