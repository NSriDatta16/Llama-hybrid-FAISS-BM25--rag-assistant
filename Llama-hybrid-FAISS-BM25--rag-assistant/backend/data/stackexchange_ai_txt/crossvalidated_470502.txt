[site]: crossvalidated
[post_id]: 470502
[parent_id]: 
[tags]: 
Why does my Gibbs sampler find two optimals?

[EDITED] I am using a Gibbs Sampler to find a Bayesian optimization to my multilevel (hierarchical) model (2 levels). However, when I run multiple chains (each chain having different starting values) my Gibbs sampler seems to convergence to two different values. Any thoughts on why this is happening and how to solve it? The trace plot below shows my problem. Here I considered four different chains and used 100,000 iterations. The picture are the last 400 iterations of one of the coefficients that I am trying to estimate. Some extra info about my model: The first stage contains about 47 variables (half of them are dummies for seasonality and the others are (continuous) marketing variables like the cost of a marketing media or a continuous control variable, as I am trying to estimate the effect marketing has on sales) and my second stage model contains only dummies (about 10) and estimates one of the coefficients of the first stage. That is, I am trying to estimate the coefficients of the following model: (first stage) \begin{equation} y_t = alpha + beta_{1,t}X_{1t} + beta_2 X_{2t} + beta_3 X_{3t} + ... + \epsilon_t \end{equation} (second stage) \begin{equation} beta_{1,t} = phi_1 z_{1t} + phi_2 z_{2t} + ... + \upsilon_t. \end{equation} I have set all the priors to uninformative as I wanted to check whether the results somewhat correspond with a MLE approach. EDIT: My initial thoughts where: (1) Could it be because of 'bad' starting points. However, you would then expect that the four chains would convergence to the same point when letting the Gibbs sampler run with 1,000,000 iterations and that is not happening unfortunately. (2) Could it be because the some chain(s) get 'stuck' in a local optima? (3) Is there maybe an endogeneity problem?
