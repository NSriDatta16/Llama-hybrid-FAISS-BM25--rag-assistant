[site]: crossvalidated
[post_id]: 529814
[parent_id]: 529813
[tags]: 
It is just a linear function with parameters $\beta_1 = 1$ and $\beta_2 = -1$ $$ \beta_1 x_1 + \beta_2 x_2 = 1 \times x_1 + (-1) \times x_2 = x_1 - x_2 $$ Neural networks use such linear functions commonly. If you have a simple multilayer feed-forward network, just one of the layers would need to learn the parameters above to calculate the new feature to be used by the next layer. Of course, the network wouldn't learn exactly this, as it is unlikely the learned weights would be exactly $1$ and $-1$ and the result would be passed through the non-linear activation function, but it will be able to learn about the kind of relationship you are describing and use it to learn higher-level patterns. That said, there are cases where engineering the features by hand makes life easier for the algorithm so it does not need to figure them out.
