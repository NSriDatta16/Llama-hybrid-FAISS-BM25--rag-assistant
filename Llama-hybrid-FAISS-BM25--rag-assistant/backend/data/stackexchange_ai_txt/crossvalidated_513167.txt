[site]: crossvalidated
[post_id]: 513167
[parent_id]: 46457
[tags]: 
We propose the use of the repesenter Theorem that, for SVM, states: $f(\vec{x}_i)= \sum_{j=1}^l \alpha_j K(\vec{x}_i, \vec{x}_j) + b$ where $\alpha_j \in \mathbb{R}$ .This way the Primal object function becomes: $P = \frac{1}{2} \sum_{i=1}^l \sum_{j=1}^l \alpha_i \alpha_j K(\vec{x}_i, \vec{x}_j) + C \sum_{i=1| \xi_i > 0}^l (1 - y_i f(\vec{x}_i))$ and the gradient is: $\frac{\partial{P}}{\partial{\alpha_r}} = \sum_{i=1}^l \alpha_i K(\vec{x}_i, \vec{x}_r) - C \sum_{i=1| \xi_i > 0}^l y_i K(\vec{x}_i, \vec{x}_r)$ $b$ can be calculated averanging for the corresponding values where $\xi_i = 0, i \in {1..l}$ Since we always use Gaussian kernels, the second derivative is always $K(\vec{x}_r, \vec{x}_r) = 1 $ . So, the proposed update rule is: $ \alpha_r^{n+1} \leftarrow \alpha_r^{n} - \frac{\partial{P}}{\partial{\alpha_r}} $ We stop the iteration $n$ whenever the Primal objective function doesn't improve within a given number of iterations (100) for our small UCI datasets. Perhaps this could be enhanced with the Adam optimizer.
