[site]: crossvalidated
[post_id]: 561384
[parent_id]: 
[tags]: 
Model ensembling when classifiers work with different classification thresholds

I have a 2-class classification problem at hand and trained three classifiers to tackle this task. In doing so, I determined for each classifier the optimal classification threshold. For example, classifier1 might predict class 1 for values > O.5 while classifier2 already predicts class 1 for values > 0.4. Of course I could easily obtain a majority vote from these different models but I would like to somehow take into account how certain each vote is. Since I have different thresholds, I cannot simply take the average value and compare it to a threshold. What could be possible measures?
