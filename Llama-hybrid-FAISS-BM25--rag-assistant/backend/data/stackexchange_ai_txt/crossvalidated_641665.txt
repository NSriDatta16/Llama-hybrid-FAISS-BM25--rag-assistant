[site]: crossvalidated
[post_id]: 641665
[parent_id]: 
[tags]: 
Standard Error of a Mean of Standard Errors

I'm looking at a panel of observations where, for each column (15 sets of Y) in time series, I'm running individual OLS's to estimate each slope against a common (1) X variable. I now have the 15 slopes and the corresponding 15 standard errors of those slopes. What I'd like to do is get an idea of how beneficial it was to use 15 sets of Y versus just 1 Y. One method that my supervisor proposed was to take the average standard error of the slope across the 15 sets and divide by sqrt of the count-1 (essentially, taking the standard error of the mean of the standard error of the slopes). I kind of get this approach, and it would demonstrate a benefit of having multiple samples since the standard errors are somewhat similar but dividing by the sqrt(count-1) definitely makes it smaller than any individual's standard error of the slope, but I have two questions: Is that approach statistically valid? The 15 sample sets are not independent (i.e., the underlying observations are correlated cross sectionally). Thus, the "i" in the i.i.d. is violated. In that case, even if point 1 above is valid, it would need to account for the cross sectional correlation. How can this be done? Thanks so much!
