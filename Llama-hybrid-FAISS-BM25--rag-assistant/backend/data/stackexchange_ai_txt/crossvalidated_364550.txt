[site]: crossvalidated
[post_id]: 364550
[parent_id]: 364522
[tags]: 
For you first question, I would think using CV would be a better choice. Say you have 10 sets of hyperparameters to try, and set the CV fold to 5. For each set of hyperparameter, you will train 5 models, and have 5 scores on the each of the 5 validation sets. Then you can take the average (I think?) of these 5 validation scores, tag it as the "final score" of this hyperparameter. In this way, you can pick the best hyperparameter set out from 10. Furthermore, you can use this best hyperparameter set to train on all the data (without test set), and evaluate this model on the test set, just to give you some idea. In this CV way, you avoid fitting a model that is best on a single validation set. In each CV-fold, you can still use early-stopping while training. For your second question, it should be partially illustrated above. Yes, there are computational cost involved at evaluating the validation scores, but you can probably specify only to check the validation scores every N rounds (or number of trees in the case of boosting trees algorithm)
