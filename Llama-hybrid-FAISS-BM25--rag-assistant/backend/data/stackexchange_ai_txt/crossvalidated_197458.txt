[site]: crossvalidated
[post_id]: 197458
[parent_id]: 177428
[tags]: 
You can keep an eye on a weight during training (just print its value) to get a feel for its magnitude. If you want a mechanism to keep your weights low, you can add L2 regularization . This simply means you add the sum of the squares of your weights to your loss function, which penalizes the network for high magnitude weights. That said, in my experience, neural nets tend to keep their weights small. If you have (more or less) zero centered input, and a typical output function, I would be very surprised if the network somehow learned particularily high weights. I would also be surprised if you saw in any noticeable decrease in accuracy from the decreased bit depth. Some people have formally investigated using very low bit depths, for example: Training deep neural networks with low precision multiplications "We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications." Deep Learning with Limited Numerical Precision "Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy."
