[site]: crossvalidated
[post_id]: 405529
[parent_id]: 282634
[tags]: 
Always fun to come back to these... :) Rademacher bounds are still useful in theoretical settings precisely because they're distribution-dependent! While it's unrealistic to know the data distribution ahead of time, you can still use additional context from your setting and the Rademacher bounds to get sharper results than you could otherwise. For instance: The tools from Rademacher complexity give us ways to show uniform convergence of empirical processes, which is useful for proving generalization in a variety of settings, like M-estimation, which require flexible function classes that VC dimension-based bounds wouldn't be able to support. We can use Rademacher-like sums to draw connections between learnability, stability, and uniform convergence . On a more practical note, this can inform procedures for training with SGD for generalization ( SGD with quick-decaying LR generalizes well ). Perhaps most practically and directly answering the question -- "Is Rademacher complexity still useful without knowledge of the data distribution?" -- is Bartlett and Mendelson 2002 . With a firm yes . In particular, Rademacher complexities observe structural regularities (Section 3.1), which enable us to express complexities of weird function classes (such as neural networks) in terms of simpler ones (such as perceptrons). More precisely, using a technique called symmetrization you can show that your generalization error (or indeed, uniform convergence error for empirical processes) is bounded by Rademacher averages. These, in turn, allow for control through covering numbers on our functional space (so you can come up with bounds for very general classes of functions, from arbitrary parametric classes with a Lipschitz condition to nonparametric bounded Lipschitz or monotonic classes through fat-shattering dimension). With VC dimension, parametric function classes will usually have similar bounds as those derived from the above, but you have to write a proof for each parametric function class you bound! With covering number bounds from Rademacher complexity, you can take advantage of structural results and know that as long as your function class has a certain type of regularity, you can use "building blocks" that let you build-a-complexity-bound, or even bound things you couldn't otherwise (like non-parametric classes with infinite VC dimension). Other approaches to bounding Rademacher averages exist as well, some of which give better bounds than covering numbers (e.g., bracketing).
