[site]: crossvalidated
[post_id]: 439956
[parent_id]: 
[tags]: 
Quantify the output variance of a neural network classifier

Lately at work we are dealing with a theoretical problem concerning the output variance of a neural network classifier. To set the scene, suppose you have an image classifier, which takes an image as input and produces a $k$ -way softmax output. Now, as is known from literature [Gal & Ghahramani (ICML 2016)], it is possible to quantify the uncertainty of the output predictions by Monte Carlo dropout sampling. That is: given a single input image, sample $N$ output distributions by resampling weights according to some dropout probability $p$ . Thus, for each of the $k$ outputs we obtain $N$ different samples. Using these samples, we can calculate all sorts of statistics, including the variance, thereby quantifying the output uncertainty. Ok, so far so good. But now we are wondering whether it is possible that, for a given trained classifier, the variances of some outputs $k_i$ can increase while the variances of the other outputs remain the same. I'll give an example: We have trained a 3-way classifier, and when performing Monte Carlo dropout sampling on a given input image, the output variance on all 3 classes is roughly the same, say $\sigma_1$ . We now train a different classifier (or create one without training, I have not made any assumptions about the training procedure or the underlying data distribution), and we now observe that for the same input image, the output variance of class 1 has remained around $\sigma_1$ , while the output variance of the other two classes has increased to $\sigma_2$ ( $\sigma_2 > \sigma_1$ ). It is like saying that the output probability of one class is always roughly the same, while the other two output probabilities are more scattered and "dancing around". My question is, is the latter situation even possible or will it occur under some circumstances? Or will it never occur in practical applications (with neural networks trained with backprop and e.g. cross-entropy loss)? Are there any references for this? In fact my question boils down to: is there any prior probability distribution that a neural network softmax classifier adheres to? Thanks!
