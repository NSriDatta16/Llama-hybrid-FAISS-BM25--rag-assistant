[site]: datascience
[post_id]: 64348
[parent_id]: 64345
[tags]: 
Batch size determines how often the weights are updated during training. The smaller the batch size, the more frequent the updates. On the other hand, when the batch size is small, updates are made without evaluating a large portion of the data at hand so at times, the gradient may be moving in a direction other than minimum if that particular batch is not a good representative of the entire dataset. So in a sense, a large batch size helps you get to a lower loss faster compared to a smaller one - it takes relatively larger steps with little noise. In practice, one rarely uses the entire dataset because it takes too much time per iteration. Of course for smaller dataset, this is not a problem. Here you can find a good discussion of this issue by Andrew Ng. Having said that, I also vaguely remember a paper discouraging from using too large batches (more than 1024). Smaller batches might wiggle the loss left and right too much before reaching a minimum, but they will get closer to it. And also some discussion about training with very large batches not performing as well on validation sets. So there is a trade-off and the optimal value should be picked by trial and error.
