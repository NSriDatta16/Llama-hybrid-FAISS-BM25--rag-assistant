[site]: crossvalidated
[post_id]: 330686
[parent_id]: 
[tags]: 
Data transformations to increase variance

Here's a brief snippet of my data (the first two columns are measured spectral data attributes, and the latter columns are indexes calculated from the first two): The issue I'm running into is that the variance of the two measured attributes are drastically different (first column StdDev is There are common techniques for reducing high variance in data (log or sqrt transformations...) but I'm not sure what's a legitimate approach to increasing variance in an attribute? This must be a common issue though, right? I thought about standardizing the attributes, but that results in nonsense when I calculate indexes, because if the denominator is an "average" data point, the Z-score is near zero, resulting in huge values. Other things I've considered, but am not sure about the mathematical legitimacy of: Using a negative log transformation, or a negative power transformation. This seems mathematically reasonable, but I haven't seen it done. Simply subtracting a constant from the column with low variance. For example, the attribute I'm concerned about basically has only values between 246 and 248.I could just subtract 240 from all the values, and that would dramatically increase the variance. But it also would be problematic from a mathematical perspective, right? This data indicates the strength of an electrical signal from a sensor instrument. It is quantitatively connected to a real phenomena in the real world (spectral properties of tree needles) so the values are meaningful, in the sense that a 1% increase in the value = a 1% increase in signal from the sensor. So, if I remove a constant to increase the variance, I think I'd be loosing important quantitative information? That's all I've come up with so far. Anybody have mathematical support (or criticism) for either of these suggestions? Or another approach for dealing with this issue?
