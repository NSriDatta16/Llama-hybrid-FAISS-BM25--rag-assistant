[site]: crossvalidated
[post_id]: 444105
[parent_id]: 104255
[tags]: 
Hangyu Tian makes a great point that k-NN regression will not do well when there isn't enough data and method like linear regression that make stronger assumptions may outperform k-NN. However, the amazing thing about k-NN is that you can encode all sorts of interesting assumptions by using different weights. For example, if you normalize the data and then use $k(x, x')=x \cdot x'$ as the weight between $x$ and $x'$ for all $x, x'$ in your data, that will actually approximate good old fashioned linear regression! Of course, it would be unnecessarily slow compared to other methods for linear regression but the point is that you actually have a lot of flexibility. For reference this is called the linear kernel. I played around with the notebook that generated the photos you attached. I don't see much reason to do k-NN regression with uniform or distance weights as that example shows. So I changed it to use RBF weights. This means that it will be like scipy.interpolate.Rbf except that we are only looking at the nearest neighbors. Obviously looking at k nearest neighbors doesn't improve accuracy but it can be essential for performance when you have a large dataset. I also upped the number of neighbors to 10. Also, I think you should be comparing to the true function rather than the noisy data. Our goal here is to approximate the true function and ignore all that noise. Also, to have a baseline I compared to CubicSpline. I also am using 80 examples rather than just 40. I played around with different axis bound on the time axis because that affects the density which is an important factor for the performance of any k-NN method. The k-NN interpolation works pretty well even as I change the bounds on the axis. It usually gets close to the original function than the original data was. So I that this particular example just needed a bit of work done on it. Anyways, the reason in general for using k-NN anything is that it's faster than looking at the entire dataset. So k-NN based regression will be more scalable than a Gaussian process. It is also just very simple, intuitive and predictable. You can see in the attached picture that cubic spline is showing some strange behavior in certain areas. Neural networks are also relatively unpredictable. Another huge difference between k-NN interpolation and methods like cubic spline is that k-NN interpolation doesn't try to fit the data perfectly because we know the data is noisy. We can also see that k-NN gets better with more data. Meanwhile cubic spline gets starts acting crazy and gaussian processes start getting too slow. Here we use twice the amount of data. I think the moral of the story is that k-NN can do very different things depending on how you define your distances and weights. Actually, you could even use k-NN with polynomial kernel to interpolate polynomials as we did with cubic spline. Or since the underlying function here is sin it would make the most sense to use the periodic kernel. For more info see the kernel cookbook https://www.cs.toronto.edu/~duvenaud/cookbook/
