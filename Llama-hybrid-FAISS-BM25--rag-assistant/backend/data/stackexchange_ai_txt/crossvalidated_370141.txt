[site]: crossvalidated
[post_id]: 370141
[parent_id]: 
[tags]: 
How can I determine how often an event occurs based on collected data how long one has to wait for an occurence

This is an experiment I can only observe, not design/change. I make the following observations: A police officer frequently monitors the same traffic location in the same manner. I see the officer arrive and I note the time. It then takes x minutes until an incident occurs, the officer chases that car, and the observation is over. At a later point, perhaps even another day, the officer returns, and the game starts again. I have now a series of observations, x1..xn [in minutes]. It is thus trivial to calculate the mean and say "on average, one has to wait ... minutes to witness an incident". However, the statement I would like to make is "on average, .. incidents occur per day". I seems that should be possible, but I don't know how, and I suspect that just the average of wait times does not equal how frequently an event occurs on average. Note: Key here is that the officer does not observe continually 24hrs. They arrive from time to time, and then it takes x minutes for them to witness something. (Also to keep it simple assume a "perfect" officer who is invisible, sees exactly each incident while present and chases every incident they see.) I don't have the statistical vocabulary, but what is special about this experiment is that observation is not continuous (observe around the clock), but rather like a sample, and the duration of that sample is not independent (observe for say 15min every day), but ends with an observation. Bonus question: The way I worded it, the officer always stays indefinitely until an incident occurs. Sometimes, however, the officer "gives up" and just leaves without having observed an incident, and I would have a separate list of values, wait-time and no occurence. I have a gut feeling including this would make it much harder, but should be included, since I have that information.
