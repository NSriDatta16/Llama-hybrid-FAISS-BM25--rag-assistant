[site]: datascience
[post_id]: 24273
[parent_id]: 24263
[tags]: 
Similarity of documents can be done with varied approaches. As your documents are based on domain based words, you could employ a tfidf representation for each document and compute similarity based on this. Previously, I have used word2vec representations of words and constructed document vectors by taking an average of all the vectors ( as one of the approaches ) and did a cosine similarity between these vectors. Basically an n*n similarity computation. When you do topic modelling, even though the idea is to find documents which have similar topic distribution, there is a prior to the approach i.e identifying number of topics. Just using randomly 100 topics without looking at the distribution of the words might lead you on a wrong path. A very interesting approach I have recently come across is combining topic modelling and word vectors, here is the link to a blog by stitchfix : http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/
