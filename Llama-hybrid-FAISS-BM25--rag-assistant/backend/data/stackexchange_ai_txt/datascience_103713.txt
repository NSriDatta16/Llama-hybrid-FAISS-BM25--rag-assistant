[site]: datascience
[post_id]: 103713
[parent_id]: 
[tags]: 
How to stop a text-classification model from depending on only couple of the words from input text instead of entire sentence?

I have a text classification deep-learning model, which takes in a text and outputs a softmax probability. I am using glove embeddings to represent my input text in numerical form for the DL model. the DL model is actually quite simple too. the embedding layer is trainable and no weights has been passed to it. And after the training, while I was making predictions with unseen text, I could realise that only one of the key-words had huge importance in predictions. To make sure that that is the case, I have changed input texts to only include those key-words and softmax-probabilites were more or less same. So, what changes do I make in my model, from embedding-technique to network, or in anyway to not have the model rely on some of the words and force it to consider entire sentence?
