[site]: crossvalidated
[post_id]: 207350
[parent_id]: 207348
[tags]: 
Yes, absolutely! Classically one might use the t-test but there are far more powerful tools developed fairly recently. Using Bayesian approach, one can reason not simply about failure of rejection/rejection at some p-value, but reason about the whole spectrum. At some confidence level, one might even accept the null hypothesis! The power lies in natural handling of uncertainty which is lacking in frequentist methods. I strongly suggest using the BEST method by Kruschke John [PDF] . The paper in a fairly extensive manner explains the downsides of the frequentist approach with the t-test and then discusses the proposed Bayesian approach. The implementation in R, I believe, can be found on authors website [WWW] . If you are more comfortable with Python, I suggest to look at PyMC which implements this method in the examples section [HERE] . You can plug in your data and start playing with it right away. I am fairly new to Probabilistic Programming but reading numerous tutorials and playing with data, can tell that Bayesian approach to medium and especially small sample sized datasets is very powerful. This is where the Bayesian approach to data analysis excels. If the number of data pairs is relatively small, say in order of hundreds, I would try to compute the average distances for randomly shuffled columns and compare it to your reference. Here I assume that your dataset comes originally with some ordering and you are wondering whether its current configuration is optimal in a sense you described it. If the number of pairs is low, you can try to compute the measure for all possible pairings. If the number of pairs is large, you can draw some number of random pairings. In the latter case, you could try to visualize the distribution of the measure and see whether there is some structure.
