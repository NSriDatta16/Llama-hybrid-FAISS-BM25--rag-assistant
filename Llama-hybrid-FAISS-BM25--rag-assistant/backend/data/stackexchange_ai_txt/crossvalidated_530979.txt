[site]: crossvalidated
[post_id]: 530979
[parent_id]: 
[tags]: 
Bayesian inference on a summary statistic

I'm trying to set up a problem I'm working on to weight the inferences by number of observations, and from my very rudimentary understanding I think this sounds like it'd be appropriate to use a Bayesian inference framework. The challenge is this. I'm trying to make inferences on the sum of all observations for a given sample. When we have more observations, we have a higher sum, but we also have more confidence that the sum represents the actual underlying 'Truth'. The real problem is that when we only have a small number of observations, the sum shouldn't really change our priors a whole lot because the observations we get are likely biased. But I don't how I'd set this up in a Bayesian context. Originally, I was thinking I would use an MCMC chain, where the inferred parameters are the two shape parameters from a beta distribution (the sum will only vary from 0 to 1). But then when I was writing it out I realized that when I'm evaluating the likelihood it still only seems like I have one data point (the sum) for each sample, where I actually used different numbers of observations within each sample. Any ideas are welcome! PS: I'm not sure what the best title for this question would even be, which is partly the issue, I don't know what to google!
