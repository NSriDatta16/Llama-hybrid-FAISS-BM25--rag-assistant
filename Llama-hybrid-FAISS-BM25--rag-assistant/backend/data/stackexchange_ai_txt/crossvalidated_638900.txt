[site]: crossvalidated
[post_id]: 638900
[parent_id]: 637914
[tags]: 
There are two sorts of problems that will arise: general ones and boring technical ones for specific cases. I will start with the general one, where we can simplify to the independence working model and the identity link -- linear regression. Suppose you have longitudinal data with $t=1,\dots,T_n$ observations on person $n$ , and people $n=1,\dots,N$ . You have an outcome $Y_{nt}$ on person $n$ at time $t$ and a single predictor $X_{nt}$ . You might fit a model A $$E[Y_{nt}|X_{nt}]=\alpha+\beta X_{nt}$$ In this model, $\beta$ is an average difference in $Y$ per unit difference in $X$ , averaged over times and people, and $\alpha$ is an intercept averaged over people. You could also fit a model B $$E[Y_{nt}|X_{nt}]=\alpha_n+\beta X_{nt}$$ that has a separate intercept for each person (still just linear regression; no random effects or anything). Or you could even fit a model C $$E[Y_{nt}|X_{nt}]=\alpha_n+\beta_n X_{nt}$$ that has separate slope and intercept for each person. As long as you have at least two distinct $X$ values for each person all these models can be fitted and will give unbiased estimates of the parameters they estimate (because it's just linear regression). However, the last model will give absolutely terrible unbiased estimates in most settings, because each $\beta_n$ is estimated only from $T_n$ points. Unless $T_n$ is large, the variance of $\hat\beta_n$ will be large and poorly estimated, so you won't learn much by fitting the model. Your estimates of $\beta_n$ won't improve as $N$ increases, only as $T_n$ increases If you want a good estimate of $\beta_n$ , and $T_n$ isn't big enough to get one from model C, you need to accept some bias in your estimates. A reasonable approach is to take a weighted combination of $\hat\beta$ from model A and $\hat\beta_n$ from model C. What weighted combination do we want? Well, one approach is to minimise the expected mean squared error, which depends on the bias of $\beta$ and on the variances of $\hat\beta$ and $\hat\beta_n$ . We can estimate the variances (it's just linear regression) and we can estimate $E[(\beta-\beta_n)^2]$ , so that's all feasible. We'll give more weight to $\hat\beta$ when $T_n$ is small (so $\hat\beta_n$ is bad) or when $E[(\beta-\beta_n)^2]$ is small, so that $\hat\beta$ is good. After all this we have good (though not unbiased) estimates of individual-level slopes, done with just linear regression and no random effects. Except, it turns out that the estimates we get this way are exactly the linear mixed model estimates! So, it works, but it doesn't change anything. The same is basically true for log links: Ulrike Gr√∂mping wrote a paper on using Poisson GEE to fit Poisson mixed models. However, the $\hat\beta_n$ are no longer exactly unbiased -- they can be infinite -- and the pooling approach doesn't work very well. Having unbiased estimating equations (as you still do) isn't enough on its own to guarantee unbiased estimators (you need information going to infinity to get consistent estimators, and even they aren't unbiased outside linear models) For logistic regression the whole thing goes horribly wrong. Even in model B, with separate intercepts and a common slope, $\hat\beta$ is badly biased for $\beta$ (conditional logistic regression was invented to work around this problem). Getting back to the original question: yes, you can fit individual parameters by ordinary regression or GEE. You probably won't have much information about each parameter, and this will be a problem. The type of problem it is varies depending on which model you're fitting. You're still probably going to need the regularisation that a random-effects model gives you if you want individual-level parameter estimates.
