[site]: crossvalidated
[post_id]: 563679
[parent_id]: 554320
[tags]: 
I am not sure about what the 2d arrays, that you are referring to, have as row and column labels, maybe event type versus user? Anyway, The standard approach (which might be what you are describing, I am not sure...) is to create for each user a set of m reasonable summary statistics and use them to assign to each user a point in an m-dimensional space. So your users become an m-dimensional point cloud, which you can update as you wish. Next, you look for isolated points or isolated small groups in this point cloud, which would be your outliers/anomalies. Some clustering algorithms can be used here, but methods like isolation forest, OCSVM, kNN, ... (see here for examples and implementations) are often more appropriate. Of course, decisive is the choice of summary statistics, which is a job for a domain expert like you. And you have already mentioned some like several count statistics per time interval. Furthermore, it also makes sense to use the time series character of your data, and to do time series based anomaly detection (AD). The idea here is to either compute probabilities over subsequences and to flag those that have very low probability, or to compute probabilities for the next event (prediction) and raise alarm if the actual next event is very unlikely w.r.t. those probabilities. For this time series approach, deep neural networks are performing sometimes quite well, but there are also many classical models you could use, from exponentially smoothing to hidden Markov models or other dynamic Bayesian networks.
