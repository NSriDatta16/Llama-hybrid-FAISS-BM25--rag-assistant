[site]: datascience
[post_id]: 112729
[parent_id]: 112559
[tags]: 
I figured it out a little with many experiments, here's some key rules needs to obey during building the model and training: Keep positive samples and nagetive samples balanced. You have to let the Agent win, then it can learn a path leads to win. You can set to low difficulty initally, or guide it(by setting milestones of the game, and if it reaches the goal, give it a big reward). Sometimes you have to decrease the batch size to include more positive samples within a batch. Do not torch.log(MSE(y_hat, y)) , I don't know why. Limit the Agent's max step, the random action will distrub Agent, pushing the game difficulty to higher, you don't want it to play a high difficulty game initially. My code is wrong: next_q = net(next_board).detach() next_q = next_q.max(1)[0] # notice: use next_q's max value next_q[reward == GAME_WIN_REWARD] = 0 # ignore next_q if already win q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1) distance = manhattan(next_board) desired_q = (reward - distance ** 2) + next_q * q_decay # I reinforce the effect of the distance Here's how I train: learning rate: 0.00005 (constant) MAX_STEP: 10 * DIFFICULTY BATCH_SIZE = 100 DIFFICULTY EXPLORE RATE EPISODE mark 5 0.8 2e4 initally low difficulty 5 0.7 1e4 5 0.6 1e4 5 0.5 1e4 5 0.4 1e4 5 0.2 1e4 5 0 1e4 reinforce memory 6 0.3 1e4 6 0.1 1e4 6 0 1e4 reinforce memory 6 0.1 4e4 I found the Agent has bad performace in difficulty of 6 6 0 1e4 7 0.3 1e4 7 0.1 1e4 7 0 1e4
