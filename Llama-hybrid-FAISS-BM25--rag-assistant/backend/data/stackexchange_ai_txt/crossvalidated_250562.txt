[site]: crossvalidated
[post_id]: 250562
[parent_id]: 250505
[tags]: 
Yes, that is not a necessary condition. Recall that all we know about the null distribution of the Dickey-Fuller test is its asymptotic representation (although the literature of course considers many refinements). As is often the case, we do not need distributional assumptions on the error terms when considering asymptotic distributions thanks to (in this case: functional) central limit theory arguments. Here is a screenshot from Phillips ( Biometrika 1987 ) stating assumptions on the errors - as you see, these are way broader than requiring normality. That said, the asymptotic distribution does not have a closed-form solution, so that you need to simulate from the distribution to get critical values (existing infinite series representations are not practical either to generate critical vales). To perform that simulation, you must draw erros from some distribution, and the conventional choice is to simulate normal errors. But, as Phillips shows, if you were to draw the errors from some other distribution satisfying the above requirements, you would asymptotically get the same distribution. You could replace the line with rnorm(T) with some such distribution in my answer here to verify. That said, the finite-sample distribution will of course be affected by the error distribution, so that the error distribution will play a role in shorter time series. (Indeed, I did replace rnorm(T) with rt(T, df=8) , and differences are still relevant for $T$ as large as 20.000.)
