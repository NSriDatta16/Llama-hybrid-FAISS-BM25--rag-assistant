[site]: datascience
[post_id]: 796
[parent_id]: 778
[tags]: 
There are couple of things you need to understand when dealing with Big data - What is Big data? You might be aware of famous V's of Big data - Volume, Velocity, Variety... So, Python may not be suitable for all. And it goes with all data science tools available. You need to know which tool is good for what purpose. If dealing with large Volume of data: Pig/Hive/Shark - Data cleaning and ETL work Hadoop/Spark - Distributed parallel computing Mahout/ML-Lib - Machine Learning Now, you can use R/Python in intermediate stages but you'll realize that they become bottleneck in your entire process. If dealing with Velocity of data: Kafka/Storm - High throughput system People are trying to R/Python here but again it depends on kind of parallelism you want and your model complexity. What sort of analysis you wish to do? If your model demands the entire data to be first brought into memory then your model should not be complex because if the intermediate data is large then the code will break. And if you think of writing it into disk then you'll face additional delay because disk read/write is slow as compared to RAM. Conclusion You can definitely use Python in Big data space (Definitely, since people are trying with R, why not Python) but know your data and business requirement first. There may be better tools available for same and always remember: Your tools shouldnâ€™t determine how you answer questions. Your questions should determine what tools you use.
