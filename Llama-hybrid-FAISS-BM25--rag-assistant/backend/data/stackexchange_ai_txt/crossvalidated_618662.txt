[site]: crossvalidated
[post_id]: 618662
[parent_id]: 578574
[tags]: 
I agree that the term $\log p(x^{(0)})$ represents the likelihood. However, the equation: $$L = \int d x^{(0)} q( x^{(0)}) \log p( x^{(0)}) $$ is actually minimising the cross entropy between $p(x_0)$ and $q(x_0)$ . Here, $q(x_0)$ represents the true distribution and $p_\theta(x_0)$ represents the approximated distribution. By minimizing $L_{CE}$ , we aim to make the approximated distribution $p_\theta(x_0)$ as close as possible to the true distribution $q(x_0)$ . The whole derivation is as follows: \begin{aligned} L_\text{CE} &= - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0) \\ &= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int p_\theta(\mathbf{x}_{0:T}) d\mathbf{x}_{1:T} \Big) \\ &= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \int q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} d\mathbf{x}_{1:T} \Big) \\ &= - \mathbb{E}_{q(\mathbf{x}_0)} \log \Big( \mathbb{E}_{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \Big) \\ &\leq - \mathbb{E}_{q(\mathbf{x}_{0:T})} \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})} \\ &= \mathbb{E}_{q(\mathbf{x}_{0:T})}\Big[\log \frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0:T})} \Big] = L_\text{VLB} \end{aligned} However, I believe minimising the likelihood would yield the same variational lower bound in the end. See this blog for a nice derivation for the loss in diffusion models. Regarding your second question, I'm not sure if I understand it correctly. As you mentioned, the reverse process follows a normal distribution, which is fully defined by its mean and covariance. Hence, we only need to learn the functions $f_{\mu}(x^{(t)}, t)$ and $f_{\Sigma}(x^{(t)}, t)$ . However, in order to learn these parameters, we do require a training set. The parameters are dependent on the data distribution $q(x_0)$ and, therefore, on the dataset we have. It's worth noting that we don't necessarily need labels for the training set. To perform conditioned generation, where we generate samples based on certain labels or conditions, we do need labels. This allows us to condition the generation process on specific information. You can find more details in the "Conditioned Generation" section of this blog
