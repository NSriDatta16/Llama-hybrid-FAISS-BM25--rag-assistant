[site]: crossvalidated
[post_id]: 48926
[parent_id]: 48918
[tags]: 
PCA gives you the directions in feature space along which the variance of the data is maximal, i.e. containing most information about your data (if you assume it to be Gaussian distributed). Another way to look at it, and the one that best applies here is to see them as the vectors which give you the best reconstruction of your date in terms of the Euclidean distance. There are a number of ways to derive it. There are many books and tutorials where you can find those derivations. I would recommend you reading this one . What you would do is to calculate the first $k$ components for each of the character classes. WHen presented a new sample, you calculate the projection of that sample onto each of the principal components (PC) of each class, and take as resulting class the one giving the highest projection, in other words, the one that the sample is closest to. Last but not least, PCA is rather employed as a dimensionality reduction procedure than as a classifier. For that there are much better approaches. Mixture of probabilistic PCAs, SVMs, and lot more.
