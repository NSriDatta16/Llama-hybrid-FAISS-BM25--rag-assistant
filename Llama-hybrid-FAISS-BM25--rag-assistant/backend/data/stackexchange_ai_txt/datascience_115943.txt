[site]: datascience
[post_id]: 115943
[parent_id]: 115937
[tags]: 
That is a really good question, one not asked often enough. In a generic context along with the accuracy you have mentioned and the efficiency in parsimony you should consider two nuances sort of lost within parsimony and accuracy. sensitive, specificity & positive predictive and other metrics values are often more useful than straight up accuracy because they give you a sense of when your model is right and wrong, informing the expected behavior when deployed. Make sure you choose a measure based on what your model is predicting, the end use AND the potential bias that unbalanced targets might introduce. Given two models that are similarly accurate (or sensitive or specific) always choose the simplest one to train, test, deploy and explain. Regarding this second bullet, ultimately the more universally useful and deployable the better. This means considering how the model is trained, what resources are required and how much maintenance it will require if you will be updating it frequently. And you should always think about the resource consumption and velocity of the deployed model. If you create an NLP model with say 4000 target categories, but the model requires 30 permutations per target and you need to be ready to receive 100 incoming observations per minute, you might not like how much it costs to keep that all live in memory on AWS firing away each day. Likewise, if you use 50-million observations to train a kNN, each new prediction will have to be compared to 50-million training values in deployment...is that feasible or affordable given your velocity and resources and the value provided by this model? Also, Sooner or later someone will ask why the model works as it does, so if you can explain how with one and not they other and they are approximately the same, use the one you can explain. My personal philosophy is very un-popular but it has solid practical underpinnings. If I can build a model with a linear or logistic regression which is say 87.12% (choose a metric, say sensitive) and a neural network that is 88.00% sensitive, short of some major catastrophe arising in that .88% I will go with a regression model whose deployment is compact (just plugging values into an equation), is scalable affordably, is explainable both pragmatically and statistically every time. That having been said, I cannot always use a regression model, and I do not force it. But I also would not avoid a perfectly useful model because it seemed banal or not whiz-bangy enough. There is a lot of hoopla around building ideologically sexy models. That is really, in my view, just occupational vanity. Also, I would not use a regression simply because it is easy if something else were more appropriate. It is always about the cost benefit trade offs you have to make to get that extra .xx% and what that translates into for your business.
