[site]: crossvalidated
[post_id]: 624094
[parent_id]: 618883
[tags]: 
Bayesian optimization can be considered as an infinite-armed bandit algorithm. My understanding for why we don't use the same term for both is the scope of their applications and different subfields they are typically used in. When we say BO we typically mean a method for optimizing hyperparameters, with GP updated through a Bayesian rule, and a distinct set of acquisition functions. The research focus of BO is specifically on how to improve this sequential decision process for obtaining the best configuration of parameters for some predefined model. Research on bandits is much more general. As you most likely know the most popular practical applications of bandits utilize a relatively small discrete number of arms. Depending on what your research is about you can be generic and say "BO can be considered as an infinite-armed bandit algorithm", or you can be arbitrarily specific, based on your problem at hand. There are "continuous bandits", "Lipschitz bandits", "contextual bandits", "GP-bandits", "correlated bandits", and more. All of these can be used to describe how BO works, but focusing on a specific aspect of the model. I wish I could find a reference that states these claims directly for you, but meanwhile, here are a couple related ones that I would use: Nando deFreitas says: "Recently, there has been growing interest in the best arm identification problem, which is more suitable for the model selection task [7], [30], [50], [51], [72], [104]. When using Bayesian surrogate models, this is equivalent to performing Bayesian optimization on a finite, discrete domain." https://ieeexplore.ieee.org/document/7352306 . Nando also introduces BO through bandits in his lectures: https://www.youtube.com/watch?v=vz3D36VXefI . Kandasamy, Shneider and Poczos refer to GP-bandit and BO interchangeably http://proceedings.mlr.press/v37/kandasamy15.pdf .
