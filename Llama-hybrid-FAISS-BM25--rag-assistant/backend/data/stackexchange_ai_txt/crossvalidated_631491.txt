[site]: crossvalidated
[post_id]: 631491
[parent_id]: 
[tags]: 
Neural network training: Early stopping vs. reducing the number of neurons/layers

Assuming we have a regression task (1D-output, values between 0.0 and 1.0 ) with 100 input dimensions and a classical MLP with one hidden layer. When given 10000 training examples, a network with, for example, 50 neurons in the hidden layer has around 5000 trainable parameters and might overfit. To avoid this, two (out of many) options are: apply early stopping (quit training when the validation loss no longer improves) reduce the model complexity (for example, only use 8 neurons in the hidden layer, thus having only around 800 trainable parameters). Assuming training duration and model size are irrelevant, is there a consensus about which method of these two is to be preferred? Does one method usually produce better results than the other?
