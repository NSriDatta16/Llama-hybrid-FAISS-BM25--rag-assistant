[site]: datascience
[post_id]: 21956
[parent_id]: 21955
[tags]: 
Your network design/logic is basically correct, but you are seeing some very common problems with neural network numerical stability. This results in your weights diverging and not training accurately. Here are the fixes, any one of them might help a little, but the first two should be used for nearly all neural network projects. 1. Inputs need to be scaled to work with neural networks. This is called input normalisation. Usually you would do this in data preprocessing, but for your simple network we can include the scaling at the input: x_normalised = x * 0.2 - 0.5 # Arbitrary scaling I just made up Out1 = feedForward(x_normalised,w1,b1) # output of first layer The most common scaling operation is to take all the training data and convert it so that it has mean $0$ and standard deviation $1$ (typically per feature, as opposed to global scaling for all data as I have done here) - store the values used to achieve that and apply them to all following data for training, tests etc. 2. Adjust learning rate until you get a working value. train_step = tf.train.GradientDescentOptimizer(0.001).minimize(J) A value that is too high will cause your training to fail. A value that is to low will take ages to learn anything. 3. For small training sets, use more iterations than you might think This is not a general thing, but specific to demos with tiny amounts of training data like your example, or the commonly-used "learning XOR function". for _ in range(10000): # performing learning process sess.run(train_step, feed_dict = {x:xs, M:Ms}) With your very simple network actually this may cause over-fitting to the training data, so you will have to play with a value that gives you "sensible" results. However, in general how to spot and minimise over-fitting is a whole broad subject in itself, based on how you test and measure generalisation. This will need other questions if you are not sure when you learn it. It should be high on your list of things to learn though . . . it is a critical skill in producing useful neural networks that solve real problems.
