[site]: crossvalidated
[post_id]: 147403
[parent_id]: 147287
[tags]: 
Since the posterior is calculated starting from our subjective prior, our risk becomes subjective as well. When you observe the data, and update the probabilities, the prior's impact becomes smaller and smaller. If you have a very large data set, you can start with any prior and will end up with the same posterior. So, the reconciliation is in the convergence of the posterior to what comes from the data. Here's a simple analogy. Let's say, you're computing exponential weighted average: $$\mu_t=(1-\gamma)x_t+\gamma\mu_{t-1}$$. At time $t=0$ when you have no observation, you guess that the mean must be one: $\mu_0=1$, then you start updating: $$\mu_1=(1-\gamma)x_1+\gamma \mu_0$$ $$\mu_2=(1-\gamma)x_2+\gamma \mu_1=(1-\gamma)x_2+\gamma(1-\gamma)x_1+\gamma^2 \mu_0$$ etc. You can see that at time $t$ the weight on the initial guess $\mu_0$ is $\gamma^t$, i.e. its impact is vanishing. It's a similar process with Bayesian risk, when your prior's influence on posterior vanishes as the sample grows. On a small sample, of course, the impact is huge. However, on a small sample you could argue that non-Bayesian (say frequentist) approach is also unreliable due to large sample variances. If you're in a camp which says "let the data speak for itself", then small sample is an issue, while in large sample you get Bayesian approach to render the same result. So, on a small sample Bayesian approach "adds" to the data its subjective opinion, and it might be just what is needed.
