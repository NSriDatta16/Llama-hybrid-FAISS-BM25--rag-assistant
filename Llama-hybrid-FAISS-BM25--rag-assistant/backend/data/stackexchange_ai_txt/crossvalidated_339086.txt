[site]: crossvalidated
[post_id]: 339086
[parent_id]: 338980
[tags]: 
The keys are finding meaningful clusters and what you value in the resulting clusters. Let me illustrate with a simple example. The example is two Gaussian clusters that are pretty well separated. Using k-means to divide the data into either 2 or 3 clusters we get these partitions: set.seed(1066) x = c(rnorm(200,0,1), rnorm(200,6,1)) y = rnorm(400,0,1) XY = data.frame(x,y) KM2 = kmeans(XY, 2) KM3 = kmeans(XY, 3) par(mfrow=c(1,2)) plot(XY, pch=20, col=KM2$cluster+1, asp=1) plot(XY, pch=20, col=KM3$cluster+1, asp=1) Silhouette says that you are better off with two clusters rather than three. library(cluster) plot(silhouette(KM2$cluster, dist(XY))) plot(silhouette(KM3$cluster, dist(XY))) It is useful to look at why the silhouette went down. First of all, it is easy to see that for the cluster on the right, the silhouette barely changed. The reason for the big drop in average silhouette is the cluster on the left that has been split in two. Why didn't silhouette like that? As I said, you need to look at what the metric favors. For each point, silhouette compares the average distance between the point and the other points in the same cluster with the average distance between that point and the nearest other cluster. When there were two clusters, points in each of the two clusters were well separated from the other cluster. Not so with three clusters. The points in the two clusters on the left are right up against each other. That is how the metric can go down. Silhouette not only rewards clusters where the points in a cluster are close together; it also punishes clusters that are not well separated from each other. So that gets to the "downstream purpose". There are times when having well separated clusters is not so important. For example, you can use k-means clustering on the colors in an image to group similar colors for image compression. In that case, as long as each cluster is reasonably consistent (compact) it does not matter if sometimes two clusters might be close to each other. However, often people use clustering as a way of understanding more fundamental structure in their data. For example, in the two Gaussians example above, two clusters shows the underlying structure better than three clusters. If you are looking for structure, you want the number of clusters that most closely represents natural groupings in your data. But these are two different goals: a grouping of points where points in the same cluster are near each other and a grouping that also separates different clusters Your argument that more clusters should always be better is OK as long as you only want points in the same cluster to be close. But that is not good if you are trying to discover underlying structure. The structure is what is in the data. Taking one cluster and calling it two is not an improvement.
