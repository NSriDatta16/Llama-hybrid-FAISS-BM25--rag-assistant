[site]: crossvalidated
[post_id]: 113498
[parent_id]: 113494
[tags]: 
Since you say that you have a large amount of data, it would be "fairly logical" (or at least easy to defend) the use of a diffuse prior, which in this case is as simple as placing a Beta(1,1) which conveniently is the conjugate prior for the binomial likelihood. In general, if we have that $X\sim \text{Bin}(n,\pi)$ then placing a $\text{Beta}(\alpha,\beta)$ prior on $\pi$ yields the following: \begin{align*} p(\pi|X)&=\frac{p(X|\pi)p(\pi)}{\int p(X|\pi)p(\pi)d\pi}\\ \,\\ &\propto p(X|\pi)p(\pi)\\ \,\\ &\propto \prod_{i=1}^N\text{Bin}(n,\pi)\text{Beta}(\alpha,\beta)\\ \,\\ &\propto \prod_{i=1}^N\pi^{X_i}(1-\pi)^{n_i-X_i}\pi^\alpha(1-\pi)^\beta\\ \,\\ &\propto \pi^{\alpha+\sum_{i=1}^NX_i}(1-\pi)^{\beta+\sum_{i=1}^Nn_i-\sum_{i=1}^NX_i} \end{align*} which is proportional to another beta distribution and so the posterior distribution for $\pi$ is $\pi|X\sim\text{Beta}(\alpha+\sum_{i=1}^NX_i, \beta+\sum_{i=1}^Nn_i-\sum_{i=1}^NX_i)$. You can verify these results here: http://en.wikipedia.org/wiki/Conjugate_prior . Now to make the above have an uninformative prior then you lets $\pi\sim\text{Beta}(\alpha=1,\beta=1)$ which is really saying $\pi\sim\text{Uniform}(0,1)$. Now, returning to your question, we have that independently the posterior distributions for $\pi_1$ and $\pi_2$ are $$\pi_1|X_1\sim\text{Beta}\left(1+X_1,\, 1+n_1-X_{1}\right)$$ and $$\pi_2|X_2\sim\text{Beta}\left(1+X_2,\, 1+n_2-X_{2}\right)$$ and so now, $\pi_1-\pi_2|X_1,X_2$ should just be another distribution with updated parameters (I don't know the distributional parameters of the top of my head but I am sure its easy enough to google for difference (or sum) of beta random variables). However, if you don't care about a closed form solution you can sample from the posterior distribution of $\pi_1$ and $\pi_2$ independently and then literally subtract the posterior sample from each other to obtain posterior samples of $\pi_1-\pi_2$. Algorithmically, what I mean by this is the following: Step 1) Sample $\pi_1^*$ from $\pi_1|X_1$ Step 2) Sample $ \pi_2^*$ from $\pi_2|X_2$ Step 3) Obtain posterior samples of $\pi_1-\pi_2$ by calculating $ \pi_1^*- \pi_2^*$ Not sure if this is helpful or not but here is some R code validating what I am proposing to you. The code is the following: #Both pi's are approximately 2% and their difference is 0.1% pi1 = .02 pi2 = .019 #Sample sizes of 1000 n1 = 1000 n2 = 1000 #X1 and X2 equal to their pi*n x1 = pi1*n1 x2 = pi2*n2 #Sampling 10,000 random variables from the posterior distributions of pi1 and pi2 post1 = rbeta(10000,1+x1,1+n1-x1) post2 = rbeta(10000,1+x2,1+n2-x2) #Calulating posterior samples from pi1 - pi2 posterior = post1 - post2 Now we know that the true difference between $\pi_1-\pi_2=0.02-0.019=.001$ (in my computer code example) so now taking the mean (average) of the posterior samples I obtain for $\pi_1-\pi_2|X_1,X_2$ we obtain > mean(posterior) [1] 0.001002589 which is extremely close to the 0.1% that we expected to see.
