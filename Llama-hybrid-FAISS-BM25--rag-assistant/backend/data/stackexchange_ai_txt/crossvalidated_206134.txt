[site]: crossvalidated
[post_id]: 206134
[parent_id]: 
[tags]: 
Interpolating/smoothing 8-bit data

(As a caveat, I think this belongs on this stack site, but I'm not 100% sure.) We have a time series that is physically sampled with only 8bit resolution, so we wind up with a "staircase" pattern, which in itself isn't an issue. The problem however, is that with any noise this flip-flops between the 2 states (as seen in the figure), and we really need a smoother signal. I was wondering if there was any "standard" approach of doing this, or if anyone had any ideas. Currently we're using a Gaussian convolution filter, but that tends to insert a phase shift in the signal. Thanks, Aaron
