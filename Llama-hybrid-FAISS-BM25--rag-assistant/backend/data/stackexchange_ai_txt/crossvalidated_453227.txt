[site]: crossvalidated
[post_id]: 453227
[parent_id]: 
[tags]: 
Computing the average of measurements with an associated uncertainty

I think this might be a silly (or a really basic) thing to ask, but I have not been able to see this clearly. Suppose I have $N$ measurements of a certain magnitude $x$ , each with an associated uncertainty, $\Delta x$ . How do I compute the mean of these measurements and also the uncertainty of the mean? I know one could use the standard error if the data didn't have any uncertainties: $$\Delta(\bar{x}) = \frac{\sigma}{\sqrt{N}}$$ But I am not very sure about the best way to proceed in the case of the raw data already having associated uncertainties. I feel like just performing an error propagation calculation is probably a wrong approach. And by this I mean: $$\Delta(\bar{x}) = \sqrt{\sum_{i=1}^N \Big{(}\frac{\partial \bar{x}}{\partial x_i}\Big{)}(\Delta x_i)^2}$$ In case this is relevant: I have computed a parameter by performing several least squares regressions over my available data, and every time the result has been very similar. So now I have six values, each one with an associated uncertainty, but I need to know their arithmetic mean as well as the uncertainty of their mean.
