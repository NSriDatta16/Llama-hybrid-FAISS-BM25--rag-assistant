[site]: crossvalidated
[post_id]: 60043
[parent_id]: 53164
[tags]: 
Using a frequentist approach, you can set a null that they are different to a certain degree, and then reject that null. If you do that and reject the null you know they aren't different to that degree. A two tailed test would tell if you they are larger than that degree or less (perhaps to the extent that it is going in another direction), but you can eyeball it to know if that is the case and select to put your degree on the direction of greatest concern (given your post-hoc observation). K-S tests seem like a good approach in principle. I would recommend that you use the raw K-S statistic from each pair of sims as the input into your analysis and then do something like a one sample t-test of the differences (technically a paired samples t-test, but putting it in the context of a one-sample t makes it more obvious how you can adjust your population mean to reflect the null you actually want to test). In this way you are doing something akin to a random effects meta-analysis of the K-S comparisons and your effects will generalize to the population of your K-S comparisons. Nulls all too frequently are set to 0. However, it is also reasonable to set them to a prior value that a skeptic might hold. For example here, a skeptic for your sims being the same might hold a null that that they are indeed different (to a given degree), and you want to reject that null and say that they aren't as different as the skeptic believes. Inferential statistics won't ever let you claim they are the same... the best they can do is say that if they are different they are likely only X different (out to some confidence interval). Given all of that... fretting over what the null is in this situation is probably not horribly useful. You probably want to be looking at the confidence interval of the differences between the simulations. The same framework I give in the previous paragraph can be used to this end.
