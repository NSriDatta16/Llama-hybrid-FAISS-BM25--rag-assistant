[site]: crossvalidated
[post_id]: 163407
[parent_id]: 
[tags]: 
SVM with non-negative weights

An SVM classifier can be obtained by solving the following, $\arg\min \frac{1}{2}\|W\|_2^2 + C\sum_i \max(0, 1-y_i (W^T\mathbf{x}_i + b))$ where $W$ is the hyperplane (or weights), $b$ is the bias, $y_i$ is the label and $\mathbf{x}_i$ is the feature of an instance $i$. For some reason I need to constaint all the elements in $W$ should be non-negative, i.e. $w_j \geq 0, \forall j$. Bias could take any values. Is this reasonable? If it is, are there any pakages (e.g. liblinear) I can use directly for this? How can I optimize $W$ with non-negative constraints?
