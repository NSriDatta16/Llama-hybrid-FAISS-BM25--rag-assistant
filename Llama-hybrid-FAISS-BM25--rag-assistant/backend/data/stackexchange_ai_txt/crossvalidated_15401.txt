[site]: crossvalidated
[post_id]: 15401
[parent_id]: 15392
[tags]: 
If the spatial positions of points in the two datasets are the same, you should compare them directly by subtracting one elevation from the other and mapping the differences. If the positions are not the same, things get trickier, but you might be OK interpolating the values of one of the datasets to the locations of the other and comparing those. (This requires a good interpolator and some caution to make sure you're not just evaluating the accuracy of the interpolator.) Some characteristic features of this problem that preclude standard statistical comparisons are The univariate distribution of the data reflects the actual set of altitudes but says little or nothing about the accuracy of the LIDAR data, either relative to each other or absolutely. Therefore measures of divergence of distributions would appear to be irrelevant or misleading. They certainly won't generalize to the sensors themselves, because the divergence depends so strongly on the particular elevation distribution. Without any ground truth or surveyed elevations, there is no standard for determining which dataset is better: you can only compare them to each other. Typically, errors occur in both the altitude measurements and the position determination. A change in position by a vector amount $(dx, dy)$ at a location with gradient $(u,v)$ causes a change in altitude by $u dx + v dy$ (the directional derivative of the surface in the $(dx, dy)$ direction). This change will, on average, be proportional to the tangent of the slope. This causes the positional errors to influence the altitude measurements much more in high-slope areas than in low-slope areas. Errors tend to have strong spatial correlations, especially the positional errors. Given the rich spatial structure of these data and their expected spatial correlations, simple correlation coefficients or kappa statistics will likely not reveal anything useful. (For instance, a correlation of 0.80 would be considered extremely bad for LIDAR elevations of terrain with large elevation variations, where correlations ought to be 99+%, but might be decent on very flat terrain.) One useful way of comparing any two digital elevation models , LIDAR or not, therefore consists of subtracting the values of one from those of the other, point by point, and comparing (with a scatterplot, for instance) that to the average tangent of the slope determined by the two DEMs. Fitting a line to this and mapping out the residuals can reveal locations where the two DEMs differ by unusual amounts, giving you a spatial picture of their relative consistency. From this you can estimate the expectation of the difference (and of the absolute difference) between the two as a function of the slope. This could be digested into a single number, but the expected variation of error with slope indicates you would be better off using the entire curve to characterize the relationship between the two datasets. Alternatively, you could attempt to separate out the two components of variation into (a) a function that depends on slope, reflecting positional error, and (b) the variance of the difference of elevations. Ideally, the function in (a) will be accurately described by a small number of parameters (perhaps only one, which would be related to the variance of the positional error). If these two components of relative error--spatial and elevation--are sufficiently small, you can conclude the two sensors have essentially the same accuracy. How much accuracy they have cannot be determined without additional information. If the components of error are large for your application, you only know that one (or perhaps both!) of the sensors is inadequate.
