[site]: crossvalidated
[post_id]: 495825
[parent_id]: 363885
[tags]: 
The gradient of the loss must be computed each step. Without knowing the gradient of the loss, backprop can't update the parameters. When you compute the value of the loss is up to you; most neural network libraries compute the loss value of the mini-batch "for free" when doing the forward pass of the network. As an aside, computing the value of the loss for the whole training set at the end of an epoch can be useful, because SGD is noisy.
