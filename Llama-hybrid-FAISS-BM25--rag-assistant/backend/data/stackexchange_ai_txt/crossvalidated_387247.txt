[site]: crossvalidated
[post_id]: 387247
[parent_id]: 387232
[tags]: 
AUC ROC is a measure which tells one how much a classifier separates those classes. One way to think of that is to consider for example predictive probability of an instance and to sort you observations ordered by the predicted probability to belong to one class. For a good discriminant you should see that in general that most instances of the first class have high predicted probabilities to belong to that class and low probability if belongs to the other class. Now we will take an fabricated example. Let's suppose that your features are height and average daily calories consumed and you want to predict if the subjects have university degree (A) versus non university degree (B). I suppose that this would be a hard problem, since university degree have low correlation with the input features. In fact I would be surprised if one would get a significant discriminant far from random classification. Now let's consider that we separate those with university degrees in two categories: male university degree (A1) and female university degree (A2). Because we involve sex I suppose height would be a very good discriminant for sex. Usually males are taller than females, so height acts as a good discriminator between sexes. What happens with ROC AUC in this case? If you predict A1,A2 vs B you have a low AUC because your features are not a good discriminant. If you predict A1 vs A2,B or A2 vs A1,B than height produces good classifiers which dominates the difference between any A subcategory and B. As a consequence AUC ROC will be higher. As you can see what you noticed is that sometimes (I would say many times if you have a lot of features) you can isolate well some subgroups but the learning problem you have to solve remains hard.
