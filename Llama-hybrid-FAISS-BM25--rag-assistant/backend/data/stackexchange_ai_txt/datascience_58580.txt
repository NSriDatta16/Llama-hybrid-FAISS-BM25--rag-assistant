[site]: datascience
[post_id]: 58580
[parent_id]: 58577
[tags]: 
Yes, using one-hot encoding on 24k features requires 24k input nodes. However this should not be a problem for Keras (or any other deep learning library). Natural language processing often uses one-hot encoding on words with a vocabulary size in the same ballpark. If you are using a "deep" model, one of your hidden layers should take care of reducing the dimensionality of your data. A separate pre-processing step is usually not needed. The training time should not be unreasonable.
