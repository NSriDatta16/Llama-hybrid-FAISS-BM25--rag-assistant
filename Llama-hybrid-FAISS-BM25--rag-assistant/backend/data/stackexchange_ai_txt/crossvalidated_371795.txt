[site]: crossvalidated
[post_id]: 371795
[parent_id]: 316464
[tags]: 
A too large batch size can prevent convergence at least when using SGD and training MLP using Keras. As for why, I am not 100% sure whether it has to do with averaging of the gradients or that smaller updates provides greater probability of escaping the local minima. See here .
