[site]: datascience
[post_id]: 106766
[parent_id]: 
[tags]: 
Random Forest significantly outperforms XGBoost - problem or possible?

I have dataset of around 180k observations of 13 variables (mix of numerical and categorical features). It is binary classification problem, but classes are imbalanced (25:1 for negative ones). I wanted to deployed XGBoost (in R) and reach the best possible Precision & Recall. For dealing with imbalances I tried upsampling of positive class, as well as XGB high weights for positive class. However, despite the fact Recall is pretty high, there is very poor Precision (around 0.10). My parameters tuning for XGB: Random search of parameters - 10 interations 5-folds CV Parameter's intervals: max_depth = 3-10 lambda = 0 - 50 gamma = 0 -10 min_child_weight = 1 -10 eta = 0.01-0.20 Then, I tried Random Forest with upsampled dataset and it performed suprisingly great with Recall 0.88 and Precision 0.73 (on test dataset). Could someone tell me please, if it is possible that RF outperforms XGB so much, or it is a sign I am doing something wrong? Thank you very much.
