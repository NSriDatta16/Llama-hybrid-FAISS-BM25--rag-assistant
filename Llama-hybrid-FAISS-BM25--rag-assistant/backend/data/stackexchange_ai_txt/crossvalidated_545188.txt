[site]: crossvalidated
[post_id]: 545188
[parent_id]: 
[tags]: 
How to circumvent the Error: vector memory exhausted (limit reached?) when saving large Tidymodels workflowsets tibble

I have a question related to the methodology of working with tibbles and tidymodels workflowsets. I have a large number of models fitted and tuned using several recipe pre-processing steps/feature engineering. The result is a LARGE tibble with my tuned models (50 rows, where each row is the unique combination of model x pre-processing). [1] "workflow_set" "tbl_df" "tbl" "data.frame" Because I don't want to run the whole workflowset tuning again I save the result in an .RDS file format, with no problem. However when I try to load the .RDS file into my environment again my RStudio complains about memory being exhausted Error: vector memory exhausted (limit reached?) So my question is: Is there a way to save and/or load a large tibble that can avoid this problem? I have already tried the following that gives me the same error: ( Error: vector memory exhausted (limit reached?) ): Unnesting the workflow_set tibble by using unnest() I have also tried saving and loading as an .Rdata file Disclaimer: I am aware that I don't have a reprex, but my question is not directly related to code â€“ hence also why I did ask it here on Cross-Validated and not Stack Overflow. Of course if you really find it difficult to help out, don't hesitate to contact me and I will see what I can do to add specific code to this question.
