[site]: crossvalidated
[post_id]: 307676
[parent_id]: 
[tags]: 
MLE of regression coefficients with non-constant variance

In the simplest case of the statistical view of linear regression, we have that $$ y = f(\mathbf{x};\mathbf{w}) + \nu, \text{ where }\nu \sim \mathcal{N}(0,\sigma^2) $$ where $\mathbf{x}$ is an input vecto, $\mathbf{w}$ the vector of the coefficients of the linear function $f$ and $\nu$ is so called "white noise". Finding the MLE for $\mathbf{w}$ is pretty much a standard procedure, no real tricks involved. What I am wondering about is what happens in the following scenario $$ y = f(\mathbf{x};\mathbf{w}) + \nu, \text{ where }\nu \sim \mathcal{N}(0,\sigma_{\mathbf{x}}^2) $$ namely, when the variance $\sigma^2$ is a function of the input vector $\mathbf{x}$. In the case of constant variance, we create the likelihood function $$ \displaystyle p(\mathbf{y}|\mathbf{x}_i; \mathbf{w}, \sigma^2) = \prod_{i}^{N}p(y_i|\mathbf{x}_i; \mathbf{w}, \sigma^2) $$ and maximize it using standard techniques. Something like this does not seem to be possible under the new model with non-constant variance, since the variance is not really a fixed parameter of the model but rather a function of the input. It is also important to stress that we do not know the value of $\sigma_{\mathbf{x}}^2$ on each $\mathbf{x}$, we only know that it varies with $\mathbf{x}$. Now, if we happen to know the variance on each input vector $\mathbf{x}_i$, we (I think) can use the exact same methodology as the case with constant variance, namely maximize: $$ \displaystyle p(\mathbf{y}|\mathbf{x}_i; \mathbf{w}, \sigma) = \prod_{i}^{N}p(y_i|\mathbf{x}_i; \mathbf{w}, \sigma^2_{\mathbf{x}_i}) $$ I would appreciate comments on the following two questions: Is the first problem solvable without using Bayesian methods or some very advanced estimation theory? Am I missing something in the proposed solution for the case where we know the variance for each input vector?
