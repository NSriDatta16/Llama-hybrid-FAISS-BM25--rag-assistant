[site]: datascience
[post_id]: 33067
[parent_id]: 33041
[tags]: 
An embedding layer is in fact a linear layer. It maps the input, using a matrix multiplication, to the output, without any activation function after the multiplication. Therefore, the backpropagation is exactly as you would do with linear layer. Why don't we just call it linear layer, then? At theory level, an embedding layer performs a matrix multiplication to the input. However, in practice, the coded implementation is slightly different. This is due to the fact that the input, as a category, is incoded in a one-hot way, and the matrix multiplication by a one-hot encoded vector is as easy as a look-up, so there is no need to multiply the whole matrix.
