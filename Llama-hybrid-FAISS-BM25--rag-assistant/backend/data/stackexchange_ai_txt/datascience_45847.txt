[site]: datascience
[post_id]: 45847
[parent_id]: 45416
[tags]: 
I faced such a problem using CNN in Keras. Even, I thought that the output is being processed in the such a way like the argmax() . But, after investigating the model, I found that the model are learned and generalised so well that the labels got binarized ( 0s and 1s ). Remember, that we give the label of an image as a one hot vector ( like [ 1 0 0 0 ] ). The NN learned it so well that the output, usually being a probability vector , was now a one hot vector on which it was trained. Another way to prove this is that you are using a softmax activation function at the output layer. A softmax function produces probabilities which sum up to 1. The sum of all class probabilities is 1. In the vector [ 1 0 0 0 0 ]. The sum of all numbers is 1. Hence the output is valid. This is just a different phase in training your model. You can try to increase the size of your dataset or lower the learning rate to 0.0001 or even smaller.
