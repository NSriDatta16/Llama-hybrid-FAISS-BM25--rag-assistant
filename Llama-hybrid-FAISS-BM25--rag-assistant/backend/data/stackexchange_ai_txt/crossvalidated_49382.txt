[site]: crossvalidated
[post_id]: 49382
[parent_id]: 49210
[tags]: 
I am not sure I understand completely what you are doing but I believe this could be it. Mathematically has already been explained above so just to dumb it down to prosa: You roll out a sample of 300, each roll associated with an outcome. Then, you can group those results as you like. You also have the expected values for a "fair" dice and compare it to those outcomes by calculating standard errors, right? What you are actually doing is assuming that the dice is fair and then doing a regression like experiment: You add up all results from the experiment (or a segment thereof), calculate the frequency and take this as an estimator - which you know should be 1/6. However you have a random part in your model. Either your dice may not be fair, or it may not fall to its expected value "by chance". In either case, this "error" can be expected to be zero on average. It is added to each result (which would be expected as 1/6 chance to be either side), making the results differ. Well using a lot of statistical results which you will learn, one can see that if the amount of data points increase, this error term tends to diminish if it has certain properties (which are probably fulfilled in an experiment such as this). In short: The expected value of that error term is zero and with more data points the variance of the regression residuals decrease. So if you segment your groupings, you have less information about this error term. Your "estimate" of the true parameter (which should be 1/6), is simply more influenced by the error. If you combine your data points, you have much more information in your sample and those random errors start to even out more and more. In fact, if you would have an infinite sample, you'd end up with no errors at all in your estimates. The point beeing that your estimators are probably unbiased. Their expected value (so the expected value of the estimators) are in fact 1/6 (mostly because you error is expected to be zero). However in reality you will be off. And this residual error decreases with more data points. Remember that you add your data points together which means that you estimators for each segments are not from the same data. Adding those together you have more information - or said differently the error points for each sample - which are centered around zero - even out, pinning your estimate closer to the actual probability of the fair dice you threw.
