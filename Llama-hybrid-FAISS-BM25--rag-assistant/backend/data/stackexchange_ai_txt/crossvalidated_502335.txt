[site]: crossvalidated
[post_id]: 502335
[parent_id]: 502333
[tags]: 
While the author mentions it as an "example", it is true that, generally, smoother functions are often preferred in modelling the characteristics of the "true" underlying function, and therefore may be assigned a higher "prior probability", as the author maintains. Why is this? You may learn more about it by reading this similar question here , but essentially, there is no real justification for it, just the conventional belief that most things occurring in nature tend to change gradually rather than in a non-continuous way. Practically, smoother functions are desired because they are more easily differentiated and may have convenient mathematical properties. More on that discussion here . However, though I would say that smooth functions are still widely anchored in statistical methods, in my experience over the years we have been working more and more with non-smooth functions. Examples I can think of include in the context of real-world optimization problems, interpolation problems, and many applications of deep neural networks (an easy one to see is the common ReLU activation function). In any case, while this question easily spurs debate, I think opportunities to ponder underlying principles are great!
