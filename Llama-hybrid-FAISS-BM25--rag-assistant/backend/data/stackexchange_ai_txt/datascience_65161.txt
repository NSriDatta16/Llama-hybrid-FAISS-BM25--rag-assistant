[site]: datascience
[post_id]: 65161
[parent_id]: 55151
[tags]: 
Let's start by just recalling what each of these means. Reduction 'none' means compute batch_size gradient updates independently for the loss with respect to each input in the batch and then apply (the composition of) them. Reduction 'mean' and 'sum' mean apply the respective operations and the take the gradient with respect to this one value. Now, let's look at the different comparisons. None vs. one-by-one These are the same because by doing one-by-one, you're taking the mean of a 1-item list which is just the value in the list. So the loss will always be identical. Put another way, you're basically just re-creating the 'none' reduction in your code by applying the average to one loss output at a time. Sum vs. average Let's next compare sum and average, since this will make it easier to explain none vs. the two of them. To start, let's think about how we expect the gradient updates to differ between 'sum' and 'avg' reduced loss outputs. To be explicit, we know that average is $$ \frac{1}{n} \sum_{i=1}^n \mathrm{loss} _i $$ whereas sum is $$ \sum_{i=1}^n \mathrm{loss}_i $$ So regardless of the actual formula for our gradients, we expect that the gradient of the 'sum' reduced loss will always equal $ n $ times the gradient of the 'mean' reduced loss. This means that all weights will be updated in the same proportion just with different overall magnitudes. However, you're seeing that the gradients are the same between these two reductions. Why is that? Well, it turns out, looking at your code, you're dividing the 'sum' reduced gradient by $ n $ , so actually it's exactly what we'd expect to see. As an aside, there's a related question of why both tend to mostly produce the same result. I believe the answer is that smart optimizers like Adam's learning rate tuning will account for the constant difference between the two and factor it into their learning rate choices. I will also note that one nice thing about the 'mean' reduction is that it keeps your gradients constant regardless of your batch size (for the same data). Sum/avg vs. one-by-one/none Now that we've explained why the two pairs give the same results, let's understand why the results differ between them. The best guess (hat tip: Ken Arnold in the comments) is that it's the result of your final batch having a different (smaller) size than the rest of your batches and this producing different results between mean and and sum.
