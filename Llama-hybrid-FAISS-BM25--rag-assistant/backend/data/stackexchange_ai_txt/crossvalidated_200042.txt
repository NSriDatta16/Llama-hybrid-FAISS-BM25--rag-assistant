[site]: crossvalidated
[post_id]: 200042
[parent_id]: 
[tags]: 
Why does my RNN perform well on long sequences, but not on the short, easy ones?

I have a problem that has left me perplexed and baffled. I am training an RNN (with LSTM cells) to generate a sequence from the output of a CNN for the purpose of annotating images. E.g., input: picture of an orange output -> "fruit" input: picture containing: apple fish banana broccoli output -> "fruit meat fruit veggie" The sequences vary in length between 0-20 words. My model seems to be performing very well when there are ~7+ inputs (60%+ accuracy), but almost completely incorrect for shorter sequences ( One note: There are very many more long examples in the training set (naturally, since there are exponentially more possible combinations). But rather than augmenting the training set, I'm curious as to why the easy, short sequences don't turn out to be easy at all. Any ideas?
