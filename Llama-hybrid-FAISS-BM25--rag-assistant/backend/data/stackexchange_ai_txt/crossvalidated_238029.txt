[site]: crossvalidated
[post_id]: 238029
[parent_id]: 
[tags]: 
bayesian analysis of frequentist methods

I was wondering if anyone had recommendations for papers or resources on bayesian analysis of frequentist hypothesis testing and use of p-values? Brad Efron has this quote One definition is says that a frequentist is a a Bayesian trying to do well, or at least not too badly, against any possible prior distribution It's clear to me how this makes sense in the context of decision theory, but often I read things similar to: Cosma Shalizi: If you find a small p-value, yay; you've got enough data, with precise enough measurement, to detect the effect you're looking for, or you're really unlucky What does this really mean (in a precise mathematical sense)? "Enough" data for what? For low enough errors if we take the estimate to be true? And what does "precise enough...to detect" mean? To me that only seems to make sense in the context of a bayesian posterior distribution around the estimate. In the simplest case I can think of, consider two hypotheses, $H_0$ and $H_A$ with $\alpha = 0.05$ and $1-\beta = 0.8$. When we "reject the null" , the probability that $H_0$ is true is: $\frac{P(H_0)*\alpha}{P(H_A)*(1-\beta) + P(H_0) * \alpha}$ where $P$ is a prior distribution. Using the fact that $P(H_0) = 1-P(H_A)$, we can see how the probability that $H_0$ is true changes given various parameters of $\alpha$ and $\beta$. The plot below, for example, shows the problem of low power testing (see the replication crisis). However, it's not as clear to me how to interpret use of p-values in the more common, complex practice (in which the parameter is continuous, p-values inform data decisions or decision to look for more data, etc...)
