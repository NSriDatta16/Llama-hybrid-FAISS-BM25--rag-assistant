[site]: crossvalidated
[post_id]: 568578
[parent_id]: 
[tags]: 
Can we do Deep Reinforcement Learning with Disjoint Action Sets?

I'm defining a construction you can apply to a Markov decision process*, and it involves extending an equivalence relation from the the state space of the MDP to an equivalence relation on the action set. It would be very convenient for me to assume that the sets of available actions from each state are disjoint (so that I can say two actions are equivalent if and only if they come from equivalent states and have equal transition probability distributions across equivalence classes). However, it seems to be standard in reinforcement learning to assume the opposite - that the set of actions available in each state is the same - and gloss over any differences. I think this is so that policies can be represented compactly as a neural network for deep RL. Of course, we can force the actions available from each state to be disjoint by labelling our actions with the state they come from. If we previously had some action $a$ , available in all states, now we have $a_{s_1}$ available from $s_1$ , $a_{s_2}$ available from $s_2$ , and so on. My question is: does requiring action sets to be disjoint ruin our ability to apply Deep RL algorithms? *The details are irrelevant, but if you're interested, it's to do with model checking: I'm taking a notion of abstraction for transition systems - bisimulation - and one for Markov chains - probabilistic bisimulation - and roughly combining them to get a new notion of bisimulation for MDPs. I'm then going to use this as part of an RL method.
