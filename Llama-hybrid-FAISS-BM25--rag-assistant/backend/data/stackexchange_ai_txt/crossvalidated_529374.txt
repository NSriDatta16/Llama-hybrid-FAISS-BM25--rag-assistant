[site]: crossvalidated
[post_id]: 529374
[parent_id]: 527258
[tags]: 
As Forrest mentioned embedding data into a higher dimension (sometimes called basis expansion) is a common method which allows a linear classifier to observe a non-linear input space. Examples are using the RBF kernel with an SVM or polynomial expansion with linear regression. However basis expansion isn't always beneficial. For a classification task if your data is already linearly separable the only thing basis expansion will do is increase model complexity and training time. For a regression task basis expansion can lead to overfitting the training data if regularization is not imposed, effectively fitting the model to noise in the training set. And certain models don't benefit from basis expansion at all. For example deep neural networks will have no noticeable benefit from embedding the inputs into a higher dimension space. This is because the (non-linear) hidden layers of a network can be seen as performing basis expansion where the specific higher dimension embedding is learnt by the network during training. So as a summary, the merit of embedding data in a higher dimension depends on what model you are using and properties of your data. Embedding data in a higher dimension prior to using a linear model is common to attempt to introduce linear separability. Embedding data in a higher dimension is also something that occurs implicitly in some models such as SVMs using the kernel trick or neural networks with non-linear activations.
