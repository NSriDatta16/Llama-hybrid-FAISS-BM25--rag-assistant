[site]: crossvalidated
[post_id]: 115175
[parent_id]: 
[tags]: 
Is it valid to reduce dimensionality of the data with PCA before running nonlinear dimensionality reduction?

I have some very high dimensional data, and performing Locally Linear Embedding (LLE) is very time consuming. I also have to perform several LLEs, with varying parameters, to compute the optimal number of neighbors via Spearman's Rho (a la Karbauskaite et al, see citation). However, the data are highly linearly dependent--less than 10% of the features are necessary to preserve 98% of the variance. This does not preclude the data still being embedded in a lower dimensional manifold. Thus, my question is, is it valid to perform principal component analysis (PCA) to preserve the vast majority of the variance, followed by LLE, or will that radically change the results of the LLE? This would significantly reduce my compute time--so I'm hoping that can be done. As an aside, the Spearman's Rho is computed based on pairwise distances--although this requires using the Geodesic distance across the manifold, NOT the Euclidean distances. Is there a way to compute Geodesic distances based on Scikit-Learn's implementation of PCA? Citation: KarbauskaitÄ—, Rasa, and O. Kurasova G. Dzemyda. "Selection of the number of neighbours of each data point for the locally linear embedding algorithm." Information Technology and Control 36.4 (2007): 359-364.
