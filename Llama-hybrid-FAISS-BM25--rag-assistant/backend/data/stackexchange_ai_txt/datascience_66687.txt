[site]: datascience
[post_id]: 66687
[parent_id]: 66657
[tags]: 
Yes, the performance can vary a lot using feature engineering. Example: suppose a dataset where the response variable $y$ is true if $x$ is odd. x y 346 F 13 T 178 F 64 F 987 T ... Most learning models will fail to identify the pattern and will perform poorly, usually falling back to always predicting the majority class. However simply adding a feature $x \% 2$ to the data will allow any model to perform perfectly. Of course this a toy example, but the point is that a single well chosen feature can drastically change the performance. Naturally the increase in performance totally depends on the data and the nature of the features added.
