[site]: crossvalidated
[post_id]: 224743
[parent_id]: 
[tags]: 
Classification trees - how to avoid "cherry picking"?

This post How to compare the performance of two classification methods? (logistic regression and classification trees) represents a very similar problem I am just trying to solve - I have a dataset based on survey (arguably with less then desirable design), with about 3000 observations, and 100+ categorical variables. The variable of interest is binary. Despite identifying over 30 variables as significant, logit regression does not provide very interesting increase in the correctly predicted cases (approx. in 80% y=1, 20% y=0), the overall increase in the number of correctly predicted cases is roughly from 80% (naive model) to 81.5%. Which brings me to classification trees. While very appealing as a method, they inherently do not provide very stable results (and the choice of parameters influences the output). Using several classification methods you might obtain somewhat similar (i.e. several variables can be in all trees, but what about the variables that show up only in one tree?) Which brings me to a question, if there are somewhat similar, but distinct outputs, how do you interpret the results? I used rpart, evtree, ctree and chaid (because chaid can only work with categorical variables, all variables in the example are categorical to allow direct comparison), but I am not much wiser in terms of what the data say. I would attach a real dataset, but for the sake of discussion, let's create a random one. The variable of interest is binary variable y Df1
