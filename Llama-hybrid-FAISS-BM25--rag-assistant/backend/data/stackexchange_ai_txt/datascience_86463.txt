[site]: datascience
[post_id]: 86463
[parent_id]: 76444
[tags]: 
There are at least a dozen major flavours of attention, most of them are minor variations over the first Attention model that came out - Bahdanau et al in 2014. Each of this flavour can be implemented in multiple ways, so this can be confusing to someone who wants to add a simple attention layer to her/his model. A simple custom implementation of Attention in Keras is shared here: https://stackoverflow.com/questions/63060083/create-an-lstm-layer-with-attention-in-keras-for-multi-label-text-classification/64853996#64853996 Couple of answers above have recommended using existing libraries for attention. My experience doing that is as follows: Usage of tf.keras.layers.Attention and AdditiveAttention: While analysing tf.keras.layers.Attention Github code to better understand how it works, the first line I could come across was - "This class is suitable for Dense or CNN networks, and not for RNN networks". So this is not recommended for your case. There is another open source version maintained by CyberZHG called keras-self-attention. To the best of my knowledge this is NOT a part of the Keras or TensorFlow library and seems to be an independent piece of code. This contains two classes - SeqWeightedAttention & SeqSelfAttention layer classes. former returns a 2D value and latter a 3D value. The former seems to be loosely based on Raffel et al and can be used for Seq classification, The latter seems to be a variation of Bahdanau. The repo QnA contains the relevant attention papers on which the code is based on. In general, I would suggest you to write your Attention layer. This can be done in less than half a dozen lines of code (bare-bones essence.. See for e.g.: https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e )...much less than the time you would spend in integrating or debugging or understanding the code in these external libraries.
