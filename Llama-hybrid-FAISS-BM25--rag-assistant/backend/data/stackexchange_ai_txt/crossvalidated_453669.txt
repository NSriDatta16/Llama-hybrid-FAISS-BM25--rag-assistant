[site]: crossvalidated
[post_id]: 453669
[parent_id]: 
[tags]: 
Doubt in bayes classifier error calculation

I have recently started machine learning on my own. I started reading Duda art and start book. That author says that Bayes classifier has a min error. He calculates $$\begin{equation} P(error|x)=\begin{cases} P(\omega_1 |x), & \text{if we decide as $\omega_2$}.\\ P(\omega_2|x), & \text{if we decide as $\omega_1$}. \end{cases} \end{equation} $$ How did he write this probability of error? I know we decide $\omega_1$ if $P(\omega1 |x)>P(\omega_2|x)$ and vice versa. From this he calculates $$P(error)=\int_X P(error |x)P(x) dx = \int_X min\{P(\omega_1 |x), P(\omega_2 |x) \} P(x) dx$$ and this will be minimum. This part I understood. But how $P(error|x)$ equation is writtten I did not understand. Please clarify. Another doubt: In case of multi-classes(instead of 2 classes), we are deciding to which class X(d-dimensional feature vector) belongs to based on for which class conditional risk $R(\alpha_i |x)$ is minimum instead of for which class posterior probability $P(\omega_i |x)$ is maximum. Why are we focussing on minimum condiitonal risk rather than maximum posterior probability(this is what is bayes classifier in real sense right?). Kindly explain
