[site]: datascience
[post_id]: 9372
[parent_id]: 9340
[tags]: 
First point is that convolutional neural networks would be incredibly expensive to train on images that large. The minimum size you could allow would be num_pixels_per_letter x num_letters_per_row_or_col . It sounds like you want to achieve two tasks: OCR then document classification. Given that you have predominantly text in these documents, you would almost certainly be better served by using a more a traditional method for OCR. If you want or need to use NNs for the OCR piece, you will almost certainly need to use a sliding window. For inspiration on how you might do that, you could look to Recurrent Neural Networks that process images in sequences Once you have the text there are again much simpler methods to deal with text than NNs, but they can achieve great results. To deal with different document lengths, you can use an RNN architecture like LSTM . Or you can use paragraph vectors, which do most of the heavy lifting for you and give you an N-dimensional representation of your text.
