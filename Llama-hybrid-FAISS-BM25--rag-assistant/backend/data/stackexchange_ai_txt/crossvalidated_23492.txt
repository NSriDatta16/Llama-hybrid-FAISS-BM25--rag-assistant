[site]: crossvalidated
[post_id]: 23492
[parent_id]: 23490
[tags]: 
Most Machine Learning problems are easy! See for example at John Langford's blog . What he's really saying is that ML makes problems easy, and this presents a problem for researchers in terms of whether they should try to apply methods to a wide range of simple problems or attack more difficult problems. However the by-product is that for many problems the data is Linearly Separable (or at least nearly), in which case any linear classifier will work well! It just so happens that the authors of the original spam filter paper chose to use Naive Bayes, but had they used a Perceptron, SVM, Fisher Discriminant Analysis, Logistic Regression, AdaBoost, or pretty much anything else it probably would have worked as well. The fact that it is relatively easy to code the algorithm helps. For example to code up the SVM you either need to have a QP Solver , or you need to code up the SMO algorithm which is not a trivial task. You could of course download libsvm but in the early days that option wasn't available. However there are many other simple algorithms (including the Perceptron mentioned above) that are just as easy to code (and allows incremental updates as the question mentions). For tough nonlinear problems methods that can deal with nonlinearites are needed of course. But even this can be a relatively simple task when Kernel Methods are employed. The question often then becomes "How do I design an effective kernel function for my data" rather than "Which classifier should I use".
