[site]: crossvalidated
[post_id]: 622079
[parent_id]: 
[tags]: 
Label encoding performing well despite data being non-ordinal

So I'm currently training a model where the dependent variable is continuous and 9/11 of the independent variables are categorical, some of these categorical variables have upwards of 10,000 classes so I've grouped any of the classes that have a frequency of less than 1 into a class called "other". Note that these classes are all non-ordinal (no natural ordering). So as I have learnt in my postgrad studies, when there the data is non-ordinal I should use one hot encoding to encode these categorical data. After one-hot encoding the categorical data my model had close to 6000 predictors; however, this was fine as the random forest still ran. The cross validated R2 score was around 0.27. So wondering how I could achieve a higher accuracy I scoured the net and came across a page saying that, because of how decision trees use entropy split criterion I could use label encoder (which would artificially introduce a order as 7 is greater than 5) but a decision tree would be able to handle it. This is the part that gets confusing, the cross validated R2 of my model now around 0.80! Would I be able to get an explanation? My current thoughts are that the curse of dimensionality was heavily affecting my results more than the artificial ordinality. Thank you!
