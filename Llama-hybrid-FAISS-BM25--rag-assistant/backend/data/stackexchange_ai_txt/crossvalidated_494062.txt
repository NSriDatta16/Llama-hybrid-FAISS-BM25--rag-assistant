[site]: crossvalidated
[post_id]: 494062
[parent_id]: 
[tags]: 
Estimating Irreducible error from kappa statistics for medical applications

In many medical applications that involve some form of classification the data is annotated by expert humans (e.g. histopathologists, radiologists). This is then taken as the ground-truth. However, it is known that these experts often disagree with each other, and in the absence of a definitive label, we can rarely know who is correct. This disagreement is often quantified by Kappa statistics. When assessing the performance of a machine learning classifier, it is useful to compare its performance (say using accuracy) to an expert human - this gives us an idea of the irreducible (Bayes) error, which can be a benchmark for classifier performance and give an idea of any degree of under-fitting. However, in the case where there is significant variation in experts' annotations, and with no recourse to the 'true' label, we do not have a direct measure for benchmark performance. Hence, my question is whether we can use Kappa statistics to say anything useful about the irreducible error of a dataset?
