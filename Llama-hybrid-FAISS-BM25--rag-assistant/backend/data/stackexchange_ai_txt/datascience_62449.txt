[site]: datascience
[post_id]: 62449
[parent_id]: 
[tags]: 
Structure of LSTM gates

It is my impression that a single layer LSTM architecture consists of $t$ LSTM cells that are identical duplicates, where $t$ is the number of time steps. Then there are gates within the LSTM cell. I have struggled to find a rigorous explanation of what each “gate” actually consists of. Is each gate simply a feed forward neural network who’s output is either squished through sigmoid or tanh depending on which gate it is? Thanks
