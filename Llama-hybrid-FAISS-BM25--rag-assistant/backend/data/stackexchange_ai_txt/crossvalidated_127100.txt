[site]: crossvalidated
[post_id]: 127100
[parent_id]: 64648
[tags]: 
Test set Log-Likelihood is an objective measure of model performance. It can be intractable or difficult to evaluate for some models. This is equivalent to minimizing the KL divergence between the test data distribution and the model (remember that KL is not symmetric). It is not necessarily a good way to evaluate a generative model in general, though. It is common in deep learning generative modeling of image datasets to visualize samples from the model as a qualitative way of evaluating performance. For large datasets, it can be difficult to tell how much generalization is occurring, however. One thing that can help is looking at nearest neighbors of the samples in the dataset. It would also be interesting to look at other divergences as a way of evaluating. There are things like Fischer Information metric, and these http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence to check out. But it is not possible to calculate these in a meaningful way when all we do not actually have the data distribution , only example data points (as far as I am aware).
