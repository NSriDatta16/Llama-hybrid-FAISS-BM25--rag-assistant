[site]: crossvalidated
[post_id]: 545787
[parent_id]: 
[tags]: 
power analysis for multiple logistic regression via simulation vs power.prop.test()

I am using simulation for a power analysis of an experiment tested with a multiple logistic regression model. I find substantial improvements in power for my model with covariate vs without covariate. # packages if (!require("pacman")) install.packages("pacman") pacman::p_load(broom, tidyverse) Simulating Data with Binary outcome set.seed(42) N $x_cov_mc, df$ y) # strong correlation of covariate with outcome Fitting 1000 models with covariate set.seed(42) N_SAMPLING_DIST % filter(term == 'x_cond') %>% pull(p.value) p_vector[i] 81% of simulated models with N = 1000 observed a significant effect for x_cond when controlling for covariates. Fitting 1000 models without covariate set.seed(42) p_vector % filter(term == 'x_cond') %>% pull(p.value) p_vector[i] 67% of simulated models with N = 1000 observed a significant effect for x_cond when NOT controlling for covariates. This improvement in power is in the expected direction, but I am surprised that I have higher reported power from power.prop.test() in base R (see below). power.prop.test() results group_means % group_by(x_cond) %>% summarise(mean = mean(y)) group_means Group means are 41% & 53%. power.prop.test(p1 = group_means$mean[1], p2 = group_means$mean[2], n = N/2) How can I have 97% power using just my treatment condition in power.prop.test() , when my logistic regression with covariates only achieved 81% power?
