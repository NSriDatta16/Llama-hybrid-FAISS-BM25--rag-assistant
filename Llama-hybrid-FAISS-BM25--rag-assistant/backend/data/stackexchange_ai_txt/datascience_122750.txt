[site]: datascience
[post_id]: 122750
[parent_id]: 122749
[tags]: 
The process you described is an example of supervised learning. In supervised learning, a model is trained using labeled data, where both the input (text in this case) and the corresponding output (labels 0 or 1) are provided. The pre-trained BERT model is used for encoding the text, and then the encoded dataset is used to train the BERT model from the Transformer library. Since the labels are provided during training, it falls under the category of supervised learning.
