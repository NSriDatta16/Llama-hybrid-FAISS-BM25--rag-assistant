[site]: crossvalidated
[post_id]: 553928
[parent_id]: 
[tags]: 
Reinforcement learning based on change in discounted reward?

In RL systems that I know about, such as Q-learning or A3C, the agent makes a determination of discounted future reward as a function of the current observations or current internal state, and the action it takes. Are there RL algorithms where what we calculate from the current observations or current action is an expected change in discounted future reward? For example, we might have an agent where every time the agent sees a blue light, its expected future discounted reward goes up by 1 unit. Perhaps a blue light signifies a certain reward after a delay, independently of any other rewards the agent might also have coming. The agent's observation on that time step is only the blue light, and its only "memory" is the amount of discounted reward it had been expecting on the previous time step.
