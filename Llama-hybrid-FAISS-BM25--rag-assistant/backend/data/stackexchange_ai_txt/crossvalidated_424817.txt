[site]: crossvalidated
[post_id]: 424817
[parent_id]: 333633
[tags]: 
There's an implicit assumption in all reasoning, Bayesian or otherwise, that we know everything that could happen and accounted for it. If something happens which is impossible under the model, it just means that that assumption is false. The principled thing to do is to go back and expand the model, and start over. At least in a Bayesian framework, this process is relatively easy to formalize -- instead of inference within a single model, one would do inference in a set of models. At some point, our human ability to nest models within models must run out. Even with automated help (i.e. computers or whatever), there must be an upper limit to the complexity of the "mother of all models". I don't have any idea what to do in that circumstance, but we are certainly very far away from that, when we're working with typical parametric models found in applications.
