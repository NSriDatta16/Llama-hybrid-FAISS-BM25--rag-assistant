[site]: crossvalidated
[post_id]: 428043
[parent_id]: 
[tags]: 
What's the big deal with normalization constants in Bayesian inference?

I read this sentence in a book: "... therefore this method is particularly useful for Bayesian inference since it doesn't require a normalization constant" The method is a computational algorithm that takes a density (possibly un-normalized) and does some stuff with it. I've seen this sentiment elsewhere too, that in Bayesian stats, we need methods that don't require normalization constants ... but I don't understand what the big deal is?? If we have a density $f$ on $R$ but we don't know the normalization constant ... give me two seconds to type this in R: integrate(f, -Inf, Inf) and out pops the (inverse of the) normalization constant. So what is the big deal? Why do we care whether or not a method needs normalization constants when it takes 2 seconds to calculate them in R?
