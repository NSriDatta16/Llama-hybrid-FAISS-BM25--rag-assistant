[site]: datascience
[post_id]: 62892
[parent_id]: 
[tags]: 
K fold cross validation reduces accuracy

I am working on a machine learning classifier and when I arrive at the moment of dividing my data into training set and test set Iwant to confron two different approches. In one approch I just split the dataset into training set and test set, while with the other approch I use k fold cross validation. The strange thing is that with the cross validation the accuracy decreases, so if I have 0.87 with the first approch, with cross validation I have 0.86. Shouldn't cross validation increase my accuracy? Thank's in advance.
