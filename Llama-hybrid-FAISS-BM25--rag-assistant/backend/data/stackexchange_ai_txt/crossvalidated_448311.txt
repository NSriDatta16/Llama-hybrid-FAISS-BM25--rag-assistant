[site]: crossvalidated
[post_id]: 448311
[parent_id]: 448097
[tags]: 
Yes. The generalization is most clear if you think of linear regression not with an error term, but as a model for the conditional distribution of $Y \mid X$ : $$ Y \mid X \sim Normal(X \beta, \sigma) $$ When using the parametric bootstrap, we can think of our new $y_i$ 's as samples from these conditional distributions, one for each $x_i$ . This generalizes directly to generalized linear models. For example, logistic regression: $$ Y \mid X \sim Bernoulli \left( p=\frac{1}{1 + e^{X \beta}} \right) $$ or Poisson regression: $$ Y \mid X \sim Poisson(\lambda = e^{X \beta}) $$ In each case the parametric bootstrap is the same, we sample a new $y_i$ from the estimated conditional distribution of $Y \mid x_i$ . In the case of linear regression, this is mathematically equivalent to sampling from an error distribution and then adding on the linear predictor, but this error term distribution fails to generalize past the linear regression case (and, I'd argue, this makes the error term description of linear regression somewhat inferior to the conditional distribution description).
