[site]: crossvalidated
[post_id]: 425094
[parent_id]: 425010
[tags]: 
I have taken a very simple example to explain the relationship of eigen vectors/ principal components w.r.t linear regression # The formula is y=2x1 + 5 import numpy as np import random x1=np.random.randint(50,size=50) y1=[2*x1[i] + np.random.randint(30) for i in list(range(50))] Now we will fit the linear regression from sklearn.linear_model import LinearRegression reg = LinearRegression().fit(x1.reshape(-1,1), y1) plt.figure() plt.scatter(x1,y1) plt.plot(x1,[reg.coef_[0]*x + reg.intercept_ for x in x1],color='red') plt.show() Now what we want to do is find out the vector that best finds the direction of maximum variance. This is indirectly the regression line %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns; sns.set() from sklearn.decomposition import PCA pca = PCA(n_components=2) X=np.array(list(zip(x1,y1))) pca.fit(X) def plot_vector_extensions(v0, v1,color): plt.arrow(v0[0], v0[2], v1[0]-v0[0], v1[2]-v0[2],color=color) plt.scatter(v0[0],v0[2],color='yellow') plt.scatter(v1[0],v1[2],color='yellow') # Plot the data plt.scatter(X[:, 0], X[:, 1], alpha=0.2,color='blue') # Plot the first component v = pca.components_[0] * np.sqrt(pca.explained_variance_[0]) plot_vector_extensions(pca.mean_, pca.mean_ + v,color='red') # Plot the first component v = pca.components_[2] * np.sqrt(pca.explained_variance_[2]) plot_vector_extensions(pca.mean_, pca.mean_ + v,color='black') plt.show() We can see that the first principal component ( marked in red ) in a way depicts the line close to the regression line. I hope this helps you understand the relationship
