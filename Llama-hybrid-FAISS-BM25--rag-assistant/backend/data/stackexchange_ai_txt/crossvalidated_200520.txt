[site]: crossvalidated
[post_id]: 200520
[parent_id]: 
[tags]: 
Properties of Average Multinomial Likelihood

I am trying to understand the Kullback-Leibler Information: I read in http://arxiv.org/pdf/1404.2000v1.pdf the following: Ideally, we want the probability to be invariant to the number of measurements - this is given by the average likelihood $\overline{L} = L^{1/n}$, a number between 0 and 1. Matching intuition, as we perform more measurements, if $c_i/n \rightarrow q_i$ , then the average likelihood would be perfect, or $\overline{L} \rightarrow 1$. Conversely, as $c_i/n $ diverges from the model $q_i$ , the average likelihood $\overline{L}$ decreases, approaching zero. 1) This is actually not intuitive for me, why $\overline{L}$ goes to 0 as the models diverge, and 1 as the models converge. Yes, I understand that the likelihood is smaller for a model diverging from reality, than a model better representing reality. But how this average log likelihood goes to 0 or 1, I cannot see. We now begin the derivation by remembering that independent observations constituting a histogram are multiplied together to recover the joint probability of all measurements. Thus, an invariant likelihood across histogram counts is the geometric mean of the multinomial likelihood $L(c|q)^{1/n}$. We term this quantity the average multinomial likelihood, or average likelihood for short. We start by defining the average log-likelihood as $\overline{L}=L(c|q)^{1/n}$ 2) Why are we interested in an invariant likelihood when deriving the KL divergence?
