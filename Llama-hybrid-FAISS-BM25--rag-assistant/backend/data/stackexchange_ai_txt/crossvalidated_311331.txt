[site]: crossvalidated
[post_id]: 311331
[parent_id]: 
[tags]: 
linear regression with gaussian distribution

I am reading the book Machine Learning - A Probabilistic Perspective by Kevin P. Murphy. In the chapter about linear regression he introduces a method where you estimate the parameters for the Gaussian distribution via maximum likelihood estimation: Instead of maximizing the maximum likelihood he minimizes the negative log-likelihood: This formula can be transformed to: Which I can still follow. However, afterwards he writes: First, we rewrite the objective in a form that is more amenable to differentiation: followed by this equation: I don't understand how this equation equals NLL(theta) (from above). Were are all the sigma, pi etc.? I understand that he derives the function in order to retrieve optimal parameters but I don't understand the transformation. I hope this is enough to answer the question, I don't think that I am allowed to post the whole chapter here so I hope that someone has read the book. Thanks in advance!
