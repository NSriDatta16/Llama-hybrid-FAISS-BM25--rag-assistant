[site]: crossvalidated
[post_id]: 437838
[parent_id]: 
[tags]: 
When computing MLE for linear regression, where does the uncertainty come from?

In my Machine Learning course, when computing MLE for a linear regression problem, we modeled the likelihood function as a Gaussian. I have trouble understanding why. Where does the uncertainty come from? In my opinion, if our model is of the form $w^T\phi(x)$ , then the likelihood should be $p(\mathcal{D}|w)=\prod_{x,y\in\mathcal{D}}\mathbb{I}\{y=w^T\phi(x)\}$ , where $\mathbb{I}$ is the indicator function. In other words, I do not understand where the uncertainty comes from. Although I understand that using a "soft" function instead of a hard indicator has advantages (it is differentiable, and also $\prod_{x,y\in\mathcal{D}}\mathbb{I}\{y=w^T\phi(x)\}=0$ if only a single element of the product is 0), I still don't get why it is legit to model $p(\mathcal{D}|w)$ that way, because again in my mind the process of determining $y$ from $w$ and $x$ involves no uncertainty. Furthermore, I understand there is some uncertainty within the parameter $w$ , but still from my perspective the probability of $(x,y)\in\mathcal{D}$ given a certain value of $w$ should be either 1 or 0: either $y=w^T\phi(x)$ or not. I understand that this misunderstanding probably stems from some fundamental concept I am not seeing or grasping. Can somebody help me understand where this uncertainty comes from?
