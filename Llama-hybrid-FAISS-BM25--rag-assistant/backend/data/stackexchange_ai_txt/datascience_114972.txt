[site]: datascience
[post_id]: 114972
[parent_id]: 114964
[tags]: 
I think the correct answer is no. It is not so often the metric scores that drive a selection of model. In my experience it has more to do with the shape of the data or the volume of the data relative to training or deployment times or how the model compares a wild type observation in deployment that informs selection. Every model built atop a distinct dataset has its own upper performance limitations both metrics and operationally. This is something that you are actually considering and discovering as you build models. There is no guaranty you will build something useful, informative or deployable. That is just a part of the risk in this game. This is why domain awareness can be so meaningful because it does help a data scientist/ML engineer estimate a little bit better what may or may not work based on real-world past experiences. There is not one right way to approach what your team is trying to do. But if it were me, I would look at what the most challenging aspects of the data are and how the model will be deployed then make a model decision based on the specifics of those two things. Challenging Data Examples: . data is wider than long (more variables than observations) data has lots of incomplete observations data has a correlated nature (serial time or geospatial adjacency) data has many categorical variables with many categories data has lots of variables with vastly different magnitudes data set is huge (long with many samples) data shows collinearity or correlations not due to temporal or spatial relationships Deployment Challenges vastness of word embeddings will take up server space new predictions will arrive 50 or 500 or 5000 per second predictions need to scale exponentially over time disproportional workloads across time our domain requires the ability to explain (with real-world quantifiers) how the model works (confidence & variable importance) Armed with this information, you narrow the model types, explore your data choose a model and experiment to build the most predictive model for your given deployment needs. But there is no magic way to predict when you run out of runway to improve your model before you build a model, unless you have a prior model built on that data which you are trying to improve in some way (could be operational not metric improvement).
