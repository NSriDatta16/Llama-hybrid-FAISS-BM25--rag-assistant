[site]: crossvalidated
[post_id]: 275476
[parent_id]: 270618
[tags]: 
In the nature paper they mention: The trained agents were evaluated by playing each game 30 times for up to 5 min each time with different initial random conditions (‘noop’;see Extended Data Table 1) and an e-greedy policy with epsilon 0.05. This procedure is adopted to minimize the possibility of overfitting during evaluation. I think what they mean is 'to nullify the negative effects of over / under fitting'. Using epsilon of 0 is a fully exploitative (as you point out) choice and makes a strong statement. For instance, consider a labyrinth game where the agent’s current Q-estimates are converged to the optimal policy except for one grid, where it greedily chooses to move toward a boundary that results in it remaining in the same grid. If the agent reaches any such state, and it is choosing the Max Q action, it will be stuck there for eternity. However, keeping a vaguely explorative / stochastic element in its policy (like a tiny amount of epsilon) allows it to get out of such states. Having said that, from the code implementations I have looked at (and coded myself) in practice performance is often times measured with greedy policy for the exact reasons you list in your question.
