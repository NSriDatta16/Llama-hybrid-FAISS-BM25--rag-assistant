[site]: crossvalidated
[post_id]: 333801
[parent_id]: 242082
[tags]: 
Assuming your model is correctly specified, the predictive distribution gives an estimate of the new data point that takes account of all the uncertainty in the unknown parameter $\theta$. In the second method, where you merely use a parameter-substitution using your estimator, you are effectively treating this as a perfect estimator of the unknown parameter, and so the resulting "predictive" distribution does not take account of the uncertainty in the unknown parameter $\theta$. For this reason, the latter distribution will tend to have lower variability than the former, and if your model is correctly specified, this means that it underestimates the variability of the new data point. So yes, the predictive distribution is generally regarded as being "better". Incidentally, this kind of comparison is not exclusive to Bayesian statistics. This methods you are comparing are very much like the analogous methods that occurs in frequentist methodology, where one can use a pivotal quantity to get a proper confidence interval for a new data point (analogous to a Bayesian predictive interval), or one can merely substitute the MLE as if it were a known parameter value and obtain an interval for a new data point from the sampling distribution (analogous to the Bayesian parameter-substitution method).
