[site]: crossvalidated
[post_id]: 377550
[parent_id]: 340119
[tags]: 
A common use case is multi-class classification. Using the sigmoid activation in the final layer produces a quantity in $[0,1]$ . When used element-wise, the output is a vector where each element is a probability. This is in contrast to the softmax case, where the entire vector is a probability distribution over the classes. A vector where each element is a probability can be helpful for tasks where the target has multiple, non-exclusive categories. Softmax activations also play a crucial role in Transformer networks, which are a recent neural network architecture that has strong performance on sequence transduction tasks. See: " Attention Is All You Need " by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Replacing softmax with ReLU would create some problems, because then the output of the activation would not have a sum-to-1 constraint. Another use case for tanh and sigmoid activations is in LSTM units, where the activations' constraints form a critical part of the so-called "constant error carousel".
