[site]: crossvalidated
[post_id]: 594668
[parent_id]: 587941
[tags]: 
Positional encoding as implemented in Vaswani et al. (2017) embeds order into the inputs. Once you do that, you can permute tokens in each input while retaining a notion of order. This means the transformer is permutation invariant, but the transformer still has access to order information. For example, a naive positional encoding of a sequence of length $n$ could be $p = [1, 2, 3, ..., n]$ . If you add this positional vector to your sequence (by concatenation or summation - the paper uses summation), token $i$ will include position $p_i$ , and permuting tokens will retain order information without the need for order.
