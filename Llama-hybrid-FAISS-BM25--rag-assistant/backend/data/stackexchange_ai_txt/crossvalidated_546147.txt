[site]: crossvalidated
[post_id]: 546147
[parent_id]: 
[tags]: 
Standard Notion of Limit for relative frequency to define probability

I am self studying probability theory and am reading the book: Understanding Probability: Chance Rules in Everyday Life by Henk Tijms. In second chapter he explains about what challenges relative frequency can have if used for defining probability. To Quote: Intuitively, we would like to define the probability of the occurrence of the event A in a single repetition of the experiment as the limiting number to which the relative frequency $f_n(A)$ converges as n increases. Introducing the notion of probability this way bypasses several rather serious obstacles. The most serious obstacle is that, for relative frequency, the standard meaning of the notion of a limit cannot be applied (because you cannot assume a priori that the limiting number will be the same each time). Now I don't seem to understand, why we cannot apply the standard meaning of the notion of a limit to relative frequency? The first line explains that we wish to use the value the function $f_n(A)$ will reach as a probability when the number of repetitions of the experiment grows. Then he says this was we bypass some issues. But then he says for relative frequency the most serious case is we cannot use limit's standard meaning. Why can't we use standard limit notion? After this paragraphs he goes on to explain about sample space, but there also we use relative frequency. If relative frequency has serious issues, why we use it? Also what's non standard way we use limit in defining probabilities? The book is freely available to read here: http://xn--webducation-dbb.com/wp-content/uploads/2019/01/Henk-Tijms-Understanding-Probability-3rd-Edition-Cambridge-University-Press-2012.pdf
