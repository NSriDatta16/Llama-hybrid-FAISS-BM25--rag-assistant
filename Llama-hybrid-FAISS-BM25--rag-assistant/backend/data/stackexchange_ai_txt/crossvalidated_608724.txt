[site]: crossvalidated
[post_id]: 608724
[parent_id]: 
[tags]: 
Dimensionality reduction that preserves non-trivial similarity

I have a set of vectors $x_i \in\mathbb{R}^m$ and a similarity function f that quantifies how similar $x_i,x_j$ are. Unfortunately, calculating f takes a lot of time. I want to use some neural architecture A s.t.: $A(x_i)\in\mathbb{R}^n$ where $n Euclidian Distance between $A(x_i),A(x_j)$ will output similar results to f. When i say similar results i mean that if $A(x_i),A(x_j), A(x_k)$ are embedded in the vector space in a way That $A(x_i),A(x_j)$ are very close to each other but $A(x_k)$ is very far from both, then $f(x_i,x_j) and $f(x_i,x_j) What kind of loss should I use for this task? Edit : The data I have is unlabeled
