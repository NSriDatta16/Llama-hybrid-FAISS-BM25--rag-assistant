[site]: crossvalidated
[post_id]: 97537
[parent_id]: 95683
[tags]: 
OK this is only a partial answer based on what I understand so far: It is easier to make the dataset separable (either linearly or not) by applying a feature extraction step (feature transformation or a preprocessing step as it is called sometimes) before classification than making the dataset's class-conditional probability distributions follow an assumption (Gaussian or Uniform ...). Why? not entirely sure but it seems to be that assuming a probability distribution is a stronger assumption than the linear separability in the case of logistic vs Bayesian. So, amusing that after the feature extraction these assumption is going to hold is even more ambitious than achieving linear separability by going to another possibly high dimensional space (like in Support Vector Machines). In the literature, feature functions are included as part of the logistic regression model sometimes (and sometimes not). But never for the Generative counterpart. So theoretical convenience is my answer here but I am not convinced deep inside that this matters. Happy to here some thoughts on this
