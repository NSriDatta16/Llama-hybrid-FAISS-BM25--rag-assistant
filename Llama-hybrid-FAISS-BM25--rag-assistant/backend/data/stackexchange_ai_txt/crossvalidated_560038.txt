[site]: crossvalidated
[post_id]: 560038
[parent_id]: 
[tags]: 
Does it make sense to get the best Kth-fold CV test result from an epoch where train result were bad?

I have been looking for some explanation that could convince me over the right way of thinking about CV. My challenge is related to the automation of the model configuration process due to same kind of problem but different cases (unfeasible joining). I believe the general CV concept in this article mentioned by @user0123456789 is clear. After reviewing some StackExchange's valuable posts, some concerns persist. I will post them separately (as much as possible) following the running answers. If we have 10-fold cross-validation for a regression model (before any kind of regularization, dropout, etc.): it is said one should get the average of best epoch's test result from each run/fold/sample and perhaps standard deviation as good metrics for selection between models. Now thinking of each isolated run: it's known that test results can have the best fit in some cases even though the opposite is more common. Why should someone skip train results when deciding each best fold's test result for the calculation of metrics? Does a good (not the best) test result followed by a closer train result make sense for the desired generalization? I.e.: based on the figure attached, should the model from epoch ~200 (good train and test results) be passed over by the best test result (epoch ~80, bad train result)? Does it make sense (a good test result taken from a bad for training model)?
