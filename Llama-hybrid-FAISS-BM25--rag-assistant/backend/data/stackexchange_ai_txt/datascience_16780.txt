[site]: datascience
[post_id]: 16780
[parent_id]: 11843
[tags]: 
Generally memory cost increases linearly while mini-batch size increases. If batch size 32 costs you 2GB memory, then batch size 64 will cost you 4GB memory. In practice, deep learning framework(for instance, keras ) will load the entire full-batch into your memory, thus you cannot observe apparent memory increment or decrement during mini-batch tuning. References https://github.com/fchollet/keras/issues/146
