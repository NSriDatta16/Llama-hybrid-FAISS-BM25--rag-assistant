[site]: crossvalidated
[post_id]: 625944
[parent_id]: 625693
[tags]: 
Speaking for GPT1-3, the models learn an embedding. These models follow the paper Attention Is All You Need which states that they "use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model". Skim the papers for GPT 1,2 and 3 to see that GPT3 follows GPT2, GPT2 follows GPT and GPT uses the architecture from Attention Is All You Need . Looking at the code for gpt2 may also help you understand what is going on under the hood.
