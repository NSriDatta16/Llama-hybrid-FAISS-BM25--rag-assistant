[site]: crossvalidated
[post_id]: 238383
[parent_id]: 
[tags]: 
In Bagging, how does one train the base learner?

In Bagging (the grouping of predictive variables), I can set a decision tree, a neural network, and so forth as the base learner, since I can get m datasets randomly, and in each dataset, I can train a base learner, and in the end, I have m base learners. questions Given m base learners, say $h_t(x),t=1,2,...,m$ , to ensemble ( sic , group) these learners, should I use some sort of weighting or set constraints to these learners? Or, is it all that can be done to average all the base learners as in $H(X)=\sum_{t=1}^{m}h_t(x)$ ? Is all the base learners should make $H(x)=argmax_{y\epsilon{Y}}^{}\sum_{t=1}^{m}1(h_t(x)=y)$ Given m base learners, can a prediction or classification be performed for a new data?
