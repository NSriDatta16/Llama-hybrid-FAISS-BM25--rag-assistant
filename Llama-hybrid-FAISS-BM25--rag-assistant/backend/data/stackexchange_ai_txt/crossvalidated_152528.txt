[site]: crossvalidated
[post_id]: 152528
[parent_id]: 
[tags]: 
How can using Logistic Regression without regularization be better?

I'm using this Java machine learning library: https://sites.google.com/site/qianmingjie/home/toolkits/laml From the library I'm using Logistic Regression: http://web.engr.illinois.edu/~mqian2/upload/projects/java/LAML/doc/ml/classification/LogisticRegression.html This class supports 4 types of regularizations: 0: No regularization 1: L1 regularization 2: L2^2 regularization 3: L2 regularization 4: Infinity norm regularization You basically create an object of Regular Regression using this code: int regularizationType = 1; double lambda = 0.1; Classifier logReg = new LogisticRegression(regularizationType, lambda); When I tried it I noticed this weird thing: As far as I know the idea of regularization is to have the weights as small as possible and so using lambda will penalize large weights. So one should use a large lambda to regularize. However, when I used L1 regularization with a lambda=1 the performance was worse than using lambda=0.0001 . Actually the best performance I got is when I used lambda=0 ! My questions: 1- How can logistic regression without regularization perform better than when using regularization? Isn't the idea of regularization after all is to make the performance better?! 2- Should I use large values for the regularization parameter?! 3- Is using regularization in general always good?
