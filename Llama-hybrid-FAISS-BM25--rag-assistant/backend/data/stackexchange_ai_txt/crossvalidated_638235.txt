[site]: crossvalidated
[post_id]: 638235
[parent_id]: 
[tags]: 
Comparing and selecting models, constructing objective function (complexity, prior knowledge on the distribution of parameter values)

We have a set of models that were derived using some fitting routine that optimizes parameter values utilizing $\chi^2$ for a given model. model1 has 100 parameters, model2 has 99 parameters, ... model10 has 10 parameters.. All of the parameters are estimated using regression techniques but for different model structures (e.g., going down from the largest complexity to the lowest). The data is always the same for all compared models. All estimated parameter values have some known distribution in real life. We have prior knowledge about this distribution, e.g., n.d. with some mean and variance (for each parameter). We want to use that information for 2 purposes: for comparison and selection of the best model (if for example we need to compare various models given the data but we don't know how those models were derived). in regression - to incorporate the information into the objective function. Use the prior knowledge about parameter distribution to prefer more likely parameters (maximize the likelihood of parameter values given the data and prior knowledge on the parameter distributions). The questions here are: Can we compare models of different complexity in the sense that we can calculate the chi2 value - which will be the goodness of fit, and maybe the likelihood or measure of the probability of having such parameters given the data? Or it's not a correct formulation from the point of view of calculation of the likelihood/probability function if each model will have a different number of parameters? E.g., AIC/BIC (c) are using $\chi^2$ value and penalizing for the number of parameters k. Still, it doesn't care about the values of the parameters (which can be very unlikely, physical constraints, etc.). In our case, we have the prior information. For example, adding something like: $$OF = \chi^2 + 2 * PL$$ where PL - is the term calculated based on some prior knowledge. For example, we can calculate some value of the probability of the model complexity for the given data and/or probability measure of having several parameter values... given parameters distributions. Note - all parameters are derived from the same data, but we don't care how they were obtained. We only care about their values and the fitting quality. So PL must be calculated using our knowledge about the distribution of the parameters. But is it correct to calculate PL and compare that measure for different model complexities? And what are the possible problems with this approach? It seems like it's closely connected with Bayesian inference..
