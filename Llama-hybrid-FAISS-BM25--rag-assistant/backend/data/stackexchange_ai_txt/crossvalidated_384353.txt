[site]: crossvalidated
[post_id]: 384353
[parent_id]: 
[tags]: 
How to perform batch training using L-BFGS?

I want to train a neural network for regression. The neural network is actually composed of 4 separate child neural networks, each child neural network has a layer structure of {input_layer: 92 nodes, hidden_layer_1: 60 nodes, hidden_layer_2: 60 nodes, output_layer: 1 node} As a result, the model has about $40000$ parameters to be adjusted. I have little experience in training a neural network, so I decide not to use stochastic gradient descent method (because I learned that I have to determine hyperparameters like learning rate). The optimizer I chose is fmin_l_bfgs_b coded in scipy.optimize . One data point in the training set takes 6.5MB dick space, so I can load at most $800$ data points as a training batch for the optimizer. There are several problems I encounter when I build the model for regression: Each time I load a batch of training examples, I need to find the max and min of each dimension of the input vector to normalize the training batch. Can I calculate the set of (max, min) over all the training examples in the first place, then use it to normalize each training batch? Does it make sense to perform such 'batch L-BFGS-B' optimization for my model? For each training batch, I need to wait for about $9$ hours till convergence ( $|\text{NN_output} - \text{real_value}| ). Given the number of data points I have (almost $30000$ ), it will take a ridiculous amount of time to train such a model. Any helpful suggestions will be greatly appreciated!
