[site]: crossvalidated
[post_id]: 435468
[parent_id]: 
[tags]: 
Why the prior distribution can be neglected compared with likelihood function in the posterior distribution when sample is large

Here is the theorem, Bayesian posterior estimation is same as MLE under the large samples and its prove in the book All of Statistics A Concise Course in Statistical Inference page 190 . However I cannot understand why: $f(\theta|X^n)\approx \mathcal{L}_n(\theta)$ when $n\rightarrow\infty.$ $\dfrac{1}{n}\sum-\mathcal{l}''_i(\hat{\theta}_n)\approx\mathbb{E}_{\theta}[-\mathcal{l}''_i(\hat{\theta}_n)].$ I already known this is from the CLT. Can any one give me more detail of prove? I am so confused for the last two steps First, do we regard $\mathcal{l}''_i(\hat{\theta}_n)$ as a constant independent of $\theta,$ then we have $\mathcal{l}''_i(\hat{\theta}_n) = \mathbb{E}_{\theta}[\mathcal{l}''_i(\hat{\theta}_n)]?$ Second, even though, $I(\hat{\theta}_n) = \mathbb{E}_{\theta}[\mathcal{l}''_i(\theta_n)]\Big|_{\theta_n = \hat{\theta}_n}\neq\mathbb{E}_{\theta}[\mathcal{l}''_i(\hat{\theta}_n)].$
