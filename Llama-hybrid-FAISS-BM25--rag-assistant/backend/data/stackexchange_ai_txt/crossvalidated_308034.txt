[site]: crossvalidated
[post_id]: 308034
[parent_id]: 
[tags]: 
What are the downsides of having a large number of convolutional filters

This question is purely about a single convolutional layer in a neural network - not about the number of layers. Aside from computational time what are the downsides of increasing the number of convolutional filters in a given layer? My intuition would be the risk of overfitting the data - for a fixed training data size, increasing the number of parameters means you increase the risk of overfitting. However if a method is used to combat this (a high percentage of dropout on the input for example) then it seems unlikely that overfitting would be a problem. So when I want to make a network... should I have as many filters as I can handle computationally?
