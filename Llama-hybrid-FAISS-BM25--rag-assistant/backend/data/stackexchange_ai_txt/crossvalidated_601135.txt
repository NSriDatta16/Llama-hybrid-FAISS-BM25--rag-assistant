[site]: crossvalidated
[post_id]: 601135
[parent_id]: 211010
[tags]: 
Comparing maximum likelihood estimation (MLE) and the jackknife is like comparing apples and oranges. These techniques accomplish different things. Maximum likelihood estimation is a method for estimating parameters and fitting models. Since it's based on maximizing the likelihood function, it's a model-based algorithm. The jackknife is a method for estimating the bias and variance of estimators. It doesn't make distributional assumptions about the data generating process, so it's nonparametric. Let's highlight the difference between estimating a parameter and estimating the variance of that estimator. I'll use @Repmat's example. Let $x_1,\ldots,x_n$ be independent draws from a population with mean $\mu$ . The sample average $\bar{x} = \sum_{i=1}^nx_i/n$ is an estimator of the mean $\mu$ . How accurately does $\bar{x}$ estimate $\mu$ ? In other words, what is the variance of $\bar{x}$ ? The jackknife's answer is: $$ \begin{aligned} \widehat{\operatorname{var}}_{\text{jack}} &= \frac{n-1}{n}\sum_{i=1}^n\left(\bar{x}_{(i)} - \bar{x}_{(.)}\right)^2 \end{aligned} $$ where the $\bar{x}_{(i)}$ are the jackknife values: $$ \begin{aligned} \bar{x}_{(i)} = \sum_{j\neq i}\frac{x_j}{n-1} \end{aligned} $$ and $\bar{x}_{(.)}$ is the average of the $\bar{x}_{(i)}$ s. Now let's assume further that the $x_i$ s are independent draws from a Normal distribution with mean $\mu$ and variance $\sigma^2$ . Since we've made a distributional assumption, we can use maximum likelihood to estimate $\mu$ and $\sigma^2$ . $$ \begin{aligned} \hat{\mu}_{\text{MLE}} = \bar{x}, \quad \hat{\sigma}^2_{\text{MLE}} = \sum_{i=1}^n\frac{(x_i - \bar{x})^2}{n} \end{aligned} $$ In the Normal model, how accurate is $\bar{x}$ as an estimate of $\mu$ ? We start by computing the variance of $\bar{x}$ in terms of $\sigma^2$ . \begin{align} \operatorname{var}(\bar{x}) = \sum_{i=1}^n\frac{\operatorname{var}(x_i)}{n^2} = \frac{\sigma^2}{n} \end{align} And since we don't actually know $\sigma^2$ , we use the plug-in principle and substitute $\hat{\sigma}^2$ for $\sigma^2$ . To sum up, we have an estimator $\bar{x}$ of the mean $\mu$ and two estimates of the variance of that estimator, one derived with the jackknife and the other with maximum likelihood. Let's calculate them on some data. set.seed(1234) # We simulate some normal data but the jackknife makes no distributional assumptions. n [1] 0.0009936879 Now use the jackknife implementation in the bootstrap package. library("bootstrap") # The jackknife estimate of the variance of the sample mean: (jackknife(x, mean)$jack.se)^2 #> [1] 0.0009946825 Okay. So the jackknife has a slightly bigger estimate for the variance of $\bar{x}$ . Does it mean that it's better? The MLE estimate of $\sigma^2$ is biased by a factor of $n/(n-1)$ : $\widehat{\sigma}^2_{\text{MLE}}$ is a little too small on average, so when we plug it in for $\sigma^2$ we underestimate the variance of $\bar{x}$ . The jackknife estimate is corrected for that bias, hence it's better in a certain sense. For large samples the difference is negligible. PS: With some algebra you can show that the difference between the MLE and the jackknife estimates of the variance of $\bar{x}$ is a factor of $n/(n-1)$ . PPS: What @Repmap calls MLE_se is the MLE estimate $\hat{\sigma}$ of the population standard deviation $\sigma$ . Not an estimate of the standard error $\widehat{\operatorname{se}}$ of the sample mean $\bar{x}$ .
