[site]: crossvalidated
[post_id]: 431379
[parent_id]: 
[tags]: 
Average expected reward vs expected reward for start-state

Suppose we are given a MDP and a policy $\pi$ , where for simplicity we have exactly one possible start state $s_0$ . Let $V^\pi(s)$ denote the expected return when being in a state s. Moreover, denote by $d^\pi(s)$ the occupancy frequency or on-policy distribution (see for instance http://incompleteideas.net/book/bookdraft2017nov5.pdf on page 163). My question is now: Does the following hold: $V^\pi(s_0) = \sum_s d^\pi(s) \cdot V^\pi(s)$ where the sum runs over all possible states s. I assume there are just finitely many here. I cannot prove it somehow and I think it does not hold but I do not know of any example. Do you have a proof for that or give some counterexample? Any help appreciated!
