[site]: crossvalidated
[post_id]: 553875
[parent_id]: 
[tags]: 
exp(log_softmax) vs softmax as neural network activation

I have read about log_softmax being more numerical stable than softmax, since it circumvents the division. I need to use softmax, probabilities between 0 and 1, for my neural network loss function. So I have been wondering: Should one use exp(log_softmax) or softmax as activation function for the output layer?
