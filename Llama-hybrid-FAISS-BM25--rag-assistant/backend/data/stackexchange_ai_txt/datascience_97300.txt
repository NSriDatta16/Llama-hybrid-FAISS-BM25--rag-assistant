[site]: datascience
[post_id]: 97300
[parent_id]: 
[tags]: 
Explainability and Autoencoders

suppose I have an autoencoder as a two-stack LSTM that takes in sequences of $n$ features of some length $m$ . Let's say that the dimension of my encoding vector is $k$ , so the architecture is of the form: $n \times m \to 1\times k \to n \times m$ . I'm looking into how to construct some explainability metrics on the encoding part of the autoencoder. More specifially, I'd like to know which features impact each of the $k$ encoding entries I have. Naively, one could vary one feature at a time, check the impact on the encoding and see where it is greatest. This is both computationally expensive and neglects combinations of features. Do you know of any research or methods that can assist?
