[site]: crossvalidated
[post_id]: 294633
[parent_id]: 
[tags]: 
How should I construct a binary classifier for thousands of positive data and millions of unlabeled data?

So far, I have stumbled upon many advices and papers on PU Learning and Unary classification. The simplest answer has been one-class SVM ( https://stackoverflow.com/questions/25700724/binary-semi-supervised-classification-with-positive-only-and-unlabeled-data-set ), but I have so many unlabeled examples compared to how many labeled ones I can find. And I am unsure if either the positive class or negative class are rare enough for anomaly detection. One of the other suggested methods is the two-step process where I can figure out a set of reliable negative class data, but I cannot really identify a set of data as reliably negative ( https://www.cs.uic.edu/~liub/publications/ICDM-03.pdf ). And another method suggests a weighted SVM ( http://users.csc.tntech.edu/~weberle/Spring2011/CSC6910/Papers/posonly.pdf ), but I am unsure if I can make the same assumption as the authors in that my positive data is a random subset of all the positive data, as I used a criteria to figure out which ones were positive, so I assume there is bias in the labeled data. Overall, I have a lot of labeled data of positive class, that is to say the data of what I am looking for, but then I have many more unlabeled data. (Though in a way, the labeled data could also be considered data of a negative class.) And I am unsure what proportion of positive data and negative data exists within the unlabeled data, as there could be an equal distribution between the two classes or one class might be more prevalent than the other. And eventually, I want to expand this method onto much bigger datasets. TLDR: Does anyone have suggestions for a semi-supervised binary classification method for unlabeled data?
