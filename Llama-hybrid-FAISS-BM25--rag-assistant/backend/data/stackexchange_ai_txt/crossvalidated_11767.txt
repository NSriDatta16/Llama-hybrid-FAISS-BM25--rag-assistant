[site]: crossvalidated
[post_id]: 11767
[parent_id]: 11764
[tags]: 
Estimate feature importance by randomly bumping every value of a single feature, and recording how your overall fitness function degrades. So if your first feature $x_{1,i}$ is continuously-valued and scaled to $[0,1]$, then you might add $rand(0,1)-0.5$ to each training example's value for the first feature. Then look for how much your $R^2$ decreases. This effectively excludes a feature from your training data, but deals with cross-interactions better than literally deleting the feature. Then rank your features by fitness function degradation, and make a pretty bar chart. At least some of the most important features should pass a gut-check, given your knowledge of the problem domain. And this also lets you be nicely surprised by informative features that you may not have expected. This sort of feature importance test works for all black-box models, including neural networks and large CART ensembles. In my experience, feature importance is the first step in understanding what a model is really doing.
