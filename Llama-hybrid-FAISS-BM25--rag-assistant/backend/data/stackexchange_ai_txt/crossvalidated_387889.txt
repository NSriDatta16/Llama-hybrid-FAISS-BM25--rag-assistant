[site]: crossvalidated
[post_id]: 387889
[parent_id]: 
[tags]: 
variance explained in measurement error model

I have a seemingly simple problem that I struggle to find a solid answer to. Any hints on how to solve this--or indeed of good sources containing a discussion of this issue--would be highly appreciated. I have individual-level data points, $y_i$ , with standard errors $\sigma_i$ , which have been obtained from a previous estimation. I would now like to explain my distribution of these values across individuals taking the measurement error into account. To this end, I built the following Bayesian measurement-error model (similar to what is done in meta-analysis): $y_i \sim N(\theta_i,\sigma_i^2)$ $\theta_i = \alpha + \beta*x_i + \epsilon_i$ , where $\theta_i$ are estimated parameters taking the measurement error into account. I would now like to know what proportion of variance that is explained by my regression model. Simply comparing the variance of the residuals $\epsilon_i$ to those obtained in a model empty of covariates does not seem to do the trick. It appears to widely over-estimate the variance explained. What is the correct way of doing this? Thank in advance for any help!
