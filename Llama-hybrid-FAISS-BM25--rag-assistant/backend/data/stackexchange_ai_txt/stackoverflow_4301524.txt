[site]: stackoverflow
[post_id]: 4301524
[parent_id]: 4301112
[tags]: 
Besides denormalization, there are other ways to improve query performance. The first step, of course, is to actually measure your code's performance. Just because a join query doesn't scale up very easily doesn't mean it's a bad idea to use them. Not every website has the request-load to require ultra-optimized request handling. In fact, Cal Henderson has calculated that all but 100 websites are not in the top 100 most visited websites. If it turns out your well thought out and very mantainable queries are actually a bottleneck (even if you're not necessarily in the top 100, maybe you are in the top 1,000,000), the first step is to take a really good look at the actual queries that are bottlenecks. Sometimes adding or removing indexes in the right place, or just keeping the database tidy ( VACUUM or ANALYZE commands) on a scehdule can make the difference. A next step is to look for caching opportunities. A significant percentage of typical sites perform the same queries over and over for every page and user, and they don't change very quickly. Stashing oft-used data in a cache (memcache, for instance) will take some burden off of the database server. While we're on the subject of database servers , make sure that the machine hosting the database is free to respond to queries, instead of performing any other work. Databases are among the hardest technology to scale out, so to get the most out of it, make sure your database is running on a machine with loads of ram, fast disks and no other responsibilities. Put the web-app server on its own machine, put the static files on another machine, let the firewall live on a separate machine. The query cache can go on its own machine. Just because databases are hard to scale out (though easy to scale up), it is not impossible. If two tables are rarely used in the same query, they can be split across two devices. If the join conditions of a certain set of tables are always the same (like UserID's), then rows can be split across multiple servers (this is called sharding). Once you've really exhausted all of these options, denormalization is probably the next step. It seems simple, but there's a few reasons why this is so late in the steps you want to take. The biggest is because it's actually hard to keep the duplicated data right. If no data is duplicated, then there can't be a conflict, but if data does get duplicated and one portion of the app doesn't carefully update both places, then different queries will return different, conflicting results. good choices when denromalizing your data can make out of sync conditions minor (just rebuild the denormalized data from the authoritative source), but it always increases the code maintenance work you will have to do. The second is because denormalization isn't neccesarily a speed-up. Much like adding indexes, it trades write performance for read performance. If you actually do a decent number of writes, it will actually make the average performance slower . Unlike adding indexes, databases can't tell you when you need denormalization; It is always a guess. Finally, when you do decide to go with denormalization, be aware of what your ORM (if you use one, since this is a django question you probably do) is really doing. Make sure you are designing your queries to give you only the data you actually use, and make sure your ORM isn't helpfully pulling more than that.
