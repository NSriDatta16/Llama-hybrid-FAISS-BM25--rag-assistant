[site]: crossvalidated
[post_id]: 401285
[parent_id]: 396679
[tags]: 
Is the distinction between model parameters and hyper-parameters only about reducing the complexity of the problem or are there implications for the 'model quality' to be considered? Any time an unobserved variable enters into a statistical model, you have the choice either to estimated this variable from the data when fitting the model, or select its value by other means, and then leave it fixed at that value for the purposes of model-fitting. This fact gives rise to the distinction between parameters and hyper-parameters, which is related to the use of the variable rather than a distinction that exists strictly in the dependencies in the actual statistical model. A "parameter" is estimated in the model-fitting process, whereas a "hyper-parameter" is fixed for the model-fitting process, but it may be varied for the purposes of tuning, model selection, robustness testing, or other statistical purposes outside of the model-fitting step. In the latter case, the hyper-parameter is chosen by some other method than estimation from the data, and this can include setting its value to a fixed constant, varying it over a given range, or any other procedure that does not use the data. Since both are unobserved variables in the model, you can switch to treating a parameter as a hyper-parameter (i.e., determine its value by other means instead of estimating it from the data), or you can switch from treating a hyper-parameter to a parameter (i.e., estimate its value from the data rather than determining it by outside means). These choices will depend on the desired generality of your model in the fitting step, and the other outside analysis you wish to do (e.g., robustness testing). When statisticians undertake statistical modelling, they generally want to pose a model for the fitting step that is sufficiently general to give a reasonable representation of the data in its fitted form. Modelling often involves using some "structure" via assumed distributional forms, and so on, and so it is also common to want to test the robustness of the model by seeing what happens when you vary an unobserved variable in the model over a fixed range of values. This means that statisticians sometimes want to designate variables as model parameters (which are estimated in the model-fitting step) and sometimes want to designate variables as hyper-parameters (used for some other purpose). An example of hyper-parameters in Bayesian statistics: In the context of Bayesian statistics this same distinction comes up, and this is a useful way to illustrate the distinction. For example, one might use a Bayesian model for normal data with unit variance and unknown mean: $$\begin{equation} \begin{aligned} x_1,...,x_n| \mu, \lambda &\sim \text{IID N}(\text{Mean} = \mu, \text{Variance} = 1) \\[6pt] \mu | \lambda &\sim \text{N}(\text{Mean} = \mu_0, \text{Variance} = 1/\lambda) \\[6pt] \lambda &\sim \text{Gamma}(\text{Shape} = \tfrac{\varphi}{2}, \text{Scale} = \tfrac{\varphi}{2}) \\[6pt] \end{aligned} \end{equation}$$ In this model, the analyst wishes to estimate the unknown population mean $\mu$ . Suppose the analyst wants both the unobserved variables $\mu$ and $\lambda$ to be model parameters (estimated from the data), and wants $\mu_0$ and $\varphi$ to be hyper-parameters, used for robustness testing. These latter variables are fixed in the estimation step, but varied over a range for the purposes of robustness-testing. Now, using the mixture representation of the T-distribution, it can be show that this model is equivalent to: $$\begin{equation} \begin{aligned} x_1,...,x_n| \mu &\sim \text{IID N}(\text{Mean} = \mu, \text{Variance} = 1) \\[6pt] \mu &\sim \text{Noncentral-T}(\text{Mean} = \mu_0, \text{df} = \varphi) \\[6pt] \end{aligned} \end{equation}$$ With this representation we remove reference to the parameter $\lambda$ , so that the only variables are the parameter of interest $\mu$ and the hyper-parameters. In this form we can see that varying $\varphi$ is essentially equivalent to varying the "fatness" of the tails of the prior distribution for the parameter of interest. The robustness-testing thus consists of varying the location of the prior and the fatness of its tails. Now, suppose that the analyst is regards this robustness-testing as insufficient, and instead wishes to undertake robustness-testing which alters the variance of the prior distribution over some reasonable range. In that case, the analyst would alter the model by now treating $\lambda$ as a hyper-parameter, so that $\varphi$ no longer appears at all in the model. We now have the alternative model form: $$\begin{equation} \begin{aligned} x_1,...,x_n| \mu &\sim \text{IID N}(\text{Mean} = \mu, \text{Variance} = 1) \\[6pt] \mu &\sim \text{N}(\text{Mean} = \mu_0, \text{Variance} = 1/\lambda) \\[6pt] \end{aligned} \end{equation}$$ You can see that this different choice of treatment of the variables in the model leads to different model form for the purposes of estimation. In the first model we estimate the parameter $\mu$ using a T-prior with hyper-parameters for its location and the fatness of its tails. In the second model we estimate the parameter $\mu$ using a normal prior with hyper-parameters for its location and scale. The choice of these competing models will be determined by the degree to which robustness is desired; in the second case there is broader robustness-testing.
