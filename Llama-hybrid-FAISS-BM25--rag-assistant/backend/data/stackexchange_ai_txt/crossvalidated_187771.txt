[site]: crossvalidated
[post_id]: 187771
[parent_id]: 149236
[tags]: 
According to chapter 9 "Planning and Learning" in the (overall recommended) book Reinforcement Learning: An Introduction by Sutton and Barto, both learning and planning try to estimate a value function and use it to improve the overall policy. The difference is, that planning uses simulated experience or knowledge from an environment model meanwhile learning uses actual (trial-and-error) experience to learn the value function. Since Value Iteration as part of Dynamic Programming requires full knowledge of the environment, it is indeed a planning algorithm . Value Iteration is presented in the context of Reinforcement Learning as theoretical pre-stage, where an environment model is available, before switching to heuristics (Monte-Carlo, Temporal-Difference-Learning) where it is not. However, as Sutton points out, it is not necessarily helpful to make such a distinction in practice. For example, one can learn the state-value-function $Q(s,a)$ and use that one during action selection to plan (e.g. by using Heuristic Search).
