[site]: crossvalidated
[post_id]: 381895
[parent_id]: 
[tags]: 
What causes the degeneration of correlation in a simulated time series?

I am trying to simulate an ARMA(1,1) process with the following characteristics: $\phi$ = 0.97 $\theta$ = 0.80 Standard deviation $s$ = 245 Mean $m$ = 1000 phi = 0.97 theta = 0.80 s = 245 m = 1000 set.seed(1000) require(FitARMA) test When I calculate the mean and standard deviation of my simulated series I get c(mean(serieteste),sd(serieteste)) #[1] 997.9931 297.6867 - which isn't bad at all However, when I calculate the ACF, I get values that aren't as high as $\phi$ . I'm sure I'm missing something here, after all, the autoregressive coefficient isn't the same as correlation. But I did think I'd see a higher $\phi$ for my first correlation in the simulated series because, usually, for high values of both $\phi$ and $\theta$ , the ACF will decrease slowly. Acf(serieteste, lag.max = 2, plot = F) #0 1 2 #1.000 0.431 0.418 What have I missed? P.S.: I want to simulate a time series with a standard deviation of 300. The number I inserted as the standard deviation for the innovation came from a derivation that use both $\phi$ and $\theta$ to find the "noise's" standard deviation. That's why I said the mean and standard deviation aren't bad at all.
