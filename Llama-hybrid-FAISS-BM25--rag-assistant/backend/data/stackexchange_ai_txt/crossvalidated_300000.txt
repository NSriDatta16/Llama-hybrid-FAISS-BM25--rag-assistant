[site]: crossvalidated
[post_id]: 300000
[parent_id]: 299992
[tags]: 
To report on the quality of your model in machine learning, you always split the data into two parts, the training dataset (typically 2/3 of the data) and the test dataset (typically 1/3 of the data). With the training dataset you fit the model and with the test dataset you evaluate your model. You report the prediction quality of your test dataset. Reading the Wikipedia page on "Cross-validation (statistics)", cross validation basically randomly draws a sample of, say 2/3, the data again and again as training dataset and always evaluates it with the remaining test dataset. You report the average prediction quality of test datasets over all iterations. Therewith multi-iteration cross validations makes a more effective use of the data than a single-iteration split into training and test dataset. This said, when applying your machine learning model for observations where you don't know the classification or dependent variable, you of course train your model beforehand on all data with known classification or dependent variable you have.
