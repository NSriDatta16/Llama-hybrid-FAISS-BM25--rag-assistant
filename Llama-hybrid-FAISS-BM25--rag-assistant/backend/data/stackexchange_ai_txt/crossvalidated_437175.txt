[site]: crossvalidated
[post_id]: 437175
[parent_id]: 
[tags]: 
Normalization of training and test set with data leakage

I have a time series data set for actual number of airport passengers. Within 15 years (2004 ~ 2019), just like having a trend, number of the passengers is increasing over time as the country is becoming more and more popular, which is good for my country. (more recent time a little steeper increase in the passengers then further past years) But when I now try to do time series analysis for future passenger prediction, it is not a good event I presume. Because, I did standardization (mean and SD) for training data set (2004 ~ 2015) then applied the coefficient to validation set (2016~ 2018) and test set (2019). The problem is, number of passengers were much smaller in training data set time (2004 - 2015), but during validation and test set data time (2016~2019), quite strong increase in number of passengers is shown. That being said, standardization on training data set worked nicely (mostly around 0 ~ 1), but in validation and test set, it is about 7 ~ 9 as the standardization coefficient is coming from far behind past (training set) which is small. I know data leakage is a problem so I should not violate but I think because of this big difference in the standardization result, my LSTM models are not working properly. So my fundamental question is, How can I solve training/validation/test set with normalization/standardization under this situation? should I still stick to the fitting only to the training set and transforming validation and test set with it? or should there be a different approach? I would really be glad if there are productive feedback. Thank you very much in advance.
