[site]: stackoverflow
[post_id]: 3197377
[parent_id]: 3195520
[tags]: 
I don't see where in the standard suggests an approach as complicated as you describe. Instead, from my, admittedly cursory, overview of this, I think you need to calculate the loudness in a sliding window by breaking the window into smaller time bins, and if any of the smaller time bins within this window fall below a threshold (-8LU), you leave these bins out of your calculation. Maybe you are doing this and just not calculating the average correctly. To find the average loudness correctly when you drop samples, you need to take the sum of the loudness levels that are not dropped ( i.e. the ones above your cutoff threshold), and divide this by the amount of time that the loudness is above threshold . That is, I assume that when you say, "loudness level to become smaller [than] it actually should", what you're doing is dividing by the total time, which would incorrectly bring down the value of the average. Instead you should divide by only the amount of time used in calculating the sum, i.e. N*(small time bin size in seconds) , where N is the number of bins above threshold. Maybe the algorithm is seeming more complicated than it really is because you're looking at an approach that tries to determine whether each new time bin is above threshold as it comes into the sliding window, and not recalculate it each shift of the sliding window? This is certainly possible, and is the way to do it efficiently, but the algorithm is somewhat more complex.
