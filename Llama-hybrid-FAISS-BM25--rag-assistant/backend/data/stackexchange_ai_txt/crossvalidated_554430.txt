[site]: crossvalidated
[post_id]: 554430
[parent_id]: 554426
[tags]: 
Bandits do not assume fixed payouts. Bernoulli bandits assume that the rewards are binary, but there are also bandit algorithms for non-binary rewards, for example, they can be continuous (see e.g. Bonald, 2018 ). In the generic formulation of the multi-armed bandit problem, you have a set of arms $a \in A$ that generate rewards $r \in \mathbb{R}$ with probabilities $p(r|a)$ . We can define expectation $$ q(a) = E[r|a] $$ in such a case, the best action is the one that maximizes the expected reward $$ a^\star = \underset{a \in A}{\operatorname{arg\max}} \; q(a) $$ where the expected reward per action is $$ q^\star = \underset{a \in A}{\max} \; q(a) $$ and we aim to maximize the cumulative reward over time $\sum_t r_t$ . For such a problem, you wouldn't use the Bernoulli bandit, but another formulation that allows for non-binary rewards. The common choice is using Gaussian distributions, because of mathematical convenience (easy to update thanks to conjugacy , easy to calculate expectations, etc). If variable payout rates are an issue for you, you can always play the arm for a fixed amount of time and then look at the total collected rewards in the time window. This would make the results collected for each arm would be comparable. Answering the comment: If you are not limited to pulling one arm at a time, it's still doable, however, it is not the classical multi-armed anymore, but a budget allocation problem . You can pull multiple arms, doing a Bayesian update of the rewards per each arm. To allocate the resources, you can do it proportionally to the expected rewards or have other strategies depending on the details of the problem. It's a slightly different problem, as you are optimizing the overall expected reward that needs to account for how much do you exploit each of the arms.
