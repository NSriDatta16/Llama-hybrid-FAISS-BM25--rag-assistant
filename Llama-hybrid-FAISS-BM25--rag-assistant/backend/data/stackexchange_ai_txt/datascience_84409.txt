[site]: datascience
[post_id]: 84409
[parent_id]: 82808
[tags]: 
In short: Cell state: Long term memory of the model, only part of LSTM models Hidden state: Working memory, part of LSTM and RNN models Additional Information RNN and vanishing/exploding gradients Traditional Recurrent Neural Networks (RNN) have the ability to model sequential events by propagating through time, i.e. forward and backward propagation. This is achieved by "connecting" these sequential events with the hidden state: $a_n = f(W_n, a_{n-1}, x_n)$ The hidden state $a_n$ carries past information by applying a linear combination over the previous step and the current input. Despite being a very successful architecture, RNN have the issue of vanishing/exploding gradients. This means that every previous step is essentially considered in the calculation of the backpropagation (how wrong my prediction has been), due to the chain rule engraved in $a_n$ : $a_n = f(W_n, a_{n-1}, x_n) = f(W_n, f(W_{n-1}, a_{n-2}, x_{n-1}), x_n)$ , since $ a_{n-1}=f(W_n, a_{n-2}, x_n)$ . To summarise: RNNs are great, but issues occur with the long term dependencies because of the chain rule in their hidden state. LSTM and the cell state To alleviate the issues above, LSTM architectures introduce the cell state, additional to the existing hidden state of RNNs. Cell states give the model longer memory of past events. This long term memory capability is enabled by the storage of useful beliefs from new inputs the loading of beliefs into the working memory (i.e. cell state) that are immediately useful. In case you wonder "how does it know what to store or what's immediately useful?": remember that this a trainable weight that learns with training, consider it as an additional piece of muscle that will learn this new activity storing and loading by training it on examples (i.e. labelled datapoints). To summarise: LSTMs are usually better at dealing with long term dependencies, because of their capacity to store and load beliefs that are important at different parts of the sequence. TLDR: hidden state: Working memory capability that carries information from immediately previous events and overwrites at every step uncontrollably -present at RNNs and LSTMs cell state: long term memory capability that stores and loads information of not necessarily immediately previous events present in LSTMs GRUs are also very relevant but are excluded from the response.
