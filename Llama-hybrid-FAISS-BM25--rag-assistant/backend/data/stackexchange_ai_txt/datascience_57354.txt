[site]: datascience
[post_id]: 57354
[parent_id]: 57341
[tags]: 
All regression methods have the same purpose, but some methods are better suited to a given problem than others. For example, algorithms based on decision trees, like random forest or XGBoost, do not extrapolate outside the area that is covered by the input features. On the other hand, they can handle categorical features. SVMs are not very popular because they are computationally expensive but if you don’t have too much data, then that’s not a problem. Some algorithms, like deep networks, require a lot of data to train, so you can’t use them if you have little data. Linear regression only models a linear relationship between the input features (x) and the output, but you could add x squared as an input feature to model also quadratic relations. To model complex function shapes though, that quickly becomes unmanageable. If a fairly simple function shape is fine, linear regression generally allows you to understand/explain your model, so that can be a plus. In this way, you need to understand the pros and cons of each to know in advance which algorithm is going to work well, but how do you find out, eh? It’s like saying, you need experience. So, first, be aware that choosing the best algorithm is normally far less important than making good use of the algorithm you’ve chosen. Second, try it out, get your hands dirty, and you will find what works for what. Finally, start with the simplest option that may work. Eventually, you’ll have “Experience”.
