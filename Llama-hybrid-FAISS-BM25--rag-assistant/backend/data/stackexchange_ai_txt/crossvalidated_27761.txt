[site]: crossvalidated
[post_id]: 27761
[parent_id]: 27750
[tags]: 
To add a slightly different and more general description of the problem: If you do any kind of data-driven pre-processing , e.g. parameter optimization guided by cross validation / out-of-bootstrap dimensionality reduction with techniques like PCA or PLS to produce input for the model (e.g. PLS-LDA, PCA-LDA) ... and want to use cross validation/out-of-bootstrap(/hold out) validation to estimate the final model's performance, the data-driven pre-processing needs to be done on the surrogate training data, i.e. separately for each surrogate model. If the data-driven pre-processing is of type 1., this leads to "double" or "nested" cross validation: the parameter estimation is done in a cross validation using only the training set of the "outer" cross validation. The ElemStatLearn have an illustration ( https://web.stanford.edu/~hastie/Papers/ESLII.pdf Page 222 of print 5). You may say that the pre-processing is really part of the building of the model. only pre-processing that is done independently for each case or independently of the actual data set can be taken out of the validation loop to save computations. So the other way round: if your model is completely built by knowledge external to the particular data set (e.g. you decide beforehand by your expert knowledge that measurement channels 63 - 79 cannot possibly help to solve the problem, you can of course exclude these channels, build the model and cross-validate it. The same, if you do a PLS regression and decide by your experience that 3 latent variables are a reasonable choice (but do not play around whether 2 or 5 lv give better results) then you can go ahead with a normal out-of-bootstrap/cross validation.
