[site]: crossvalidated
[post_id]: 443484
[parent_id]: 
[tags]: 
Discordance between various methods of multivariate outliers detection

Here is a small "toy example" dataset, with 15 individuals described by 6 variables (this is R language): dat Lets say that the variables are some osteometric measurements (bone lengths or widths, or whatever), and therefore express the global sizes of the individuals. In this small sample, there is no huge outlier, but some slightly unusual values. A PCA can be performed so that you can see the general structure of the data: FactoMineR::PCA(dat) I applied two methods of multivariate outliers detection: robust Mahalnobis distance , and isolation forests . The individuals detected as potential outliers are totally different from one method to the other, and I would like to know why (or which is the global "philosophy" of each method). 1. Isolation forests Here, I use the R package solitude that implements a fast algorithm for isolation forests: library(solitude) isofo $new(sample_size = nrow(dat), seed = 2020) isofo$ fit(dat) # Anomaly scores by increasing order: scores $scores$ anomaly_score names(scores) As you can see by running this R code, the individual #14, having the highest anomaly score, is the best candidate for being an outlier. It corresponds to the "smallest" individual in this dataset, with relatively low values for all variables (and was the leftmost individual on PC1 axis, which was related to size). 2. Robust Mahalanobis distance (a.k.a. MCD estimator) Here, I use the R package robustbase , and I compute the robust Mahalnobis distance for each individual, with a parameter $\alpha = 0.75$ as suggested in the original publication: library(robustbase) mcd According to those distances, the individuals #9 and #11 are from far the most credible candidates for being outliers. 3. A possible exaplanation? Here, isolation forests seem to be more sensitive to anomalies in sizes, and detect an individual one could describe as "small" compared to all other individuals. On the other hand, robust Mahalnobis distances primarily detect individuals showing anomalies of "shape", i.e. whose values do not respect the correlation pattern usually observed among the variables: individual #9 has a high value of V6 given the values of the other variables (i.e., its global "size"), and individual #11 has a high value of V2 and V4 given its global size. Is this correct? It seems that one should be really careful in choosing one method or the other, depending on his/her own definition of what is an "anomaly"/outlier in the dataset. Is the discordance observed on this small sample a simple and meaningless artifact, or a real difference of "philosophy" between the two methods?
