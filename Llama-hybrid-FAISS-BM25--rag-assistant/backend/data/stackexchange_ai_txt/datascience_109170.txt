[site]: datascience
[post_id]: 109170
[parent_id]: 107366
[tags]: 
I am the lead contributor to Cooper , a library focused on constrained optimization for Pytorch. The library employs a Lagrangian formulation of the constrained optimization problem, as you do in your example. In fact, we have used the Cooper "approach" to your question as the getting started snippet in our README -- :) thanks! One of our tutorials contains a fully-runnable answer to this question. While the answer below focuses on the discrete die example, Cooper is designed with "real world" neural net problems in mind. We really encourage you to check it out, and get in touch if you find it useful/would like to see some specific features! To keep this answer self-contained, here is a way to approach this problem using Cooper. (You can install Cooper using pip install git+https://github.com/cooper-org/cooper.git ) import torch import cooper class MaximumEntropy(cooper.ConstrainedMinimizationProblem): def __init__(self, mean_constraint): self.mean_constraint = mean_constraint super().__init__(is_constrained=True) def closure(self, probs): # Verify domain of definition of the functions assert torch.all(probs >= 0) # Negative sign removed since we want to *maximize* the entropy neg_entropy = torch.sum(probs * torch.log(probs)) # Entries of p >= 0 (equiv. -p This example uses a fancy version of SGD (these are the ExtraSGD optimizers ), but you could use almost any other Pytorch optimizer in Cooper instead. I used ExtraSGD here since it helped control the parameter oscillations better (see plots in tutorial ).
