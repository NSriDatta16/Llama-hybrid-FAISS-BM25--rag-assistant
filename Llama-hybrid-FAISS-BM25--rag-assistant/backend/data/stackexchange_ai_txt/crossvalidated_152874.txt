[site]: crossvalidated
[post_id]: 152874
[parent_id]: 
[tags]: 
When Bayesian and frequentist statistics give different answers, is there a way to empirically test which one corresponds more closely to reality?

For example for this problem: You have a coin that when flipped ends up head with probability p and ends up tail with probability 1âˆ’p. (The value of p is unknown.) Trying to estimate p, you flip the coin 14 times. It ends up head 10 times. Then you have to decide on the following event: "In the next two tosses we will get two heads in a row." Would you bet that the event will happen or that it will not happen? ( http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html ) Could one flip a bunch of different coins a million times and see what happens? (Or write a program to do so?) EDIT: Some people were asking, I think, how probability theory could possibly be relevant when empirical data was available. For the above problem, I wrote a script which would flip a coin with a 1% chance of landing heads, a 2% coin, a 3% coin, etc. fourteen times; and if it came up heads ten out of those fourteen times, then it would flip the coin twice more and see whether it came up heads both times. This "experiment" is repeated a bunch of times and the percent of the time you get two heads, given that you've gotten ten out of fourteen heads, is around 48%, which is exactly the Bayesian answer given on the website above. I tried not to make any assumptions about the nature of probability and just run the experiment the way it would happen "in the wild", if it were possible to run a million trials of this kind in real life, but I'm slightly worried that I made some inherently Bayesian assumption in the writing of the code. Thoughts?
