[site]: crossvalidated
[post_id]: 306162
[parent_id]: 
[tags]: 
Test for comparing ratios of single tail distributions

Suppose I have data that looks like this: The infected numbers usually average around 4-10 in both the control and test. Let's call these values A . The total numbers can be anything between 20-90 in both the control and test. Let's call these values B The null hypothesis is always there is no difference between the Control and Test. Initially, I used a t-test and my two columns of data were: (1) A for each sample in Control, (2) A for each sample in Test However, I deemed this test invalid for my data because, the A values average close to 0 and as there are no negative numbers. It is no longer a normal distribution. A forms more of a single tailed normal distribution. (Please correct me if I may still use t-test with a single tailed distribution) Then, I tried fisher's exact test with a 2x2 contingency table with the following parameters: (1st row) A, (2nd row) B - A, (1st column) control, (2nd column) test. However, the tables usually end up having numbers like 700-800 in the second row, and 5-15 in the first row. The data becomes skewed because of the large values in the 2nd row. The small difference in the first row come out as insignificant. I looked into Barnard's exact test, but that doesn't seem any better than Fisher's exact test. I'm having a hard time in wrapping my mind around what things to compare to prove this properly, let alone choose a test that does so. For instance, should I be comparing the ratios, or the raw infected numbers, or take averages of ratios, etc. Part of the problem is the large variance I see in B (can be anything from ~20-~90). Hence, can someone offer some guidance on this issue.
