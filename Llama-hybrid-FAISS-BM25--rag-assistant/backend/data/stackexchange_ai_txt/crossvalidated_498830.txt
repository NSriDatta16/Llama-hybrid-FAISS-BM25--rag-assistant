[site]: crossvalidated
[post_id]: 498830
[parent_id]: 
[tags]: 
Attention is all you need: from where does it get the encoder/decoder input embeddings?

In "Attention is all you need" paper, regarding encoder (and decoder) input embeddings: Do they use already pretrained such as off the shelf Word2vec or Glove embeddings ? or are they also trained starting from random initialization / One Hot Encoding ?
