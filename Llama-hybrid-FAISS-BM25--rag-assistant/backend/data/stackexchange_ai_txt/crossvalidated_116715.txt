[site]: crossvalidated
[post_id]: 116715
[parent_id]: 116709
[tags]: 
That it can't exceed 1 follows directly from the Cauchy-Schwarz inequality - which if you know the inequality establishes things pretty intuitively. There are a number of common ways to motivate the result, but I'm going to give one you might not have encountered before ... and it also motivates the second part of your question. The correlation can be rewritten in terms of standardized variables as follows: $$r_{xy}=1-\frac{1}{2} \frac{\sum{(z_{x}-z_{y})}^2}{n}$$ where $z_x = \frac{x-\bar{x}}{s_x}$ and $z_y = \frac{y-\bar{y}}{s_y}$ are the z-scores corresponding to $x$ and $y$. [This formula has a corresponding population form as well, so the same argument applies to $\rho$ as to $r$.] a) Since the second term is necessarily non-negative, correlation can't exceed 1. b) Further, the more different from equality the standardized scores for $x$ and $y$ become, the larger the second term becomes, so the correlation reduces. [Readers familiar with the $1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}$ form of Spearman's rho may have spotted the similarity in formulas. This is no coincidence.] In the plots below, we start with $z_x=z_y$ and then slowly increase the discrepancy between them, reducing the correlation: $\quad$ How does $\frac{1}{2} (z_x-z_y)^2$ measure deviation from lying on the $y=x$ line? Consider the area of a triangle formed by a point $(x,y)$ and the $y=x$ line: The further the point gets from the line, the larger the triangle, which has area $\frac{1}{2}(x-y)^2$. So the term $\frac{1}{2} \frac{\sum{(z_{x}-z_{y})}^2}{n}$ is the average area of all the triangles formed by $(z_x,z_y)$ points and the $y=x$ line. I believe there's a derivation of that deviation-from-linearity form of correlation in Rodgers, J.L., and Nicewander, W.A. (1988), "Thirteen Ways to Look at the Correlation Coefficient," The American Statistician , 42, 59-66. The many ways of looking at correlation there are a good source of various kinds of intuition about correlation. As Nick Cox points out in comments, correlation is also the cosine of an angle (specifically, it's the cosine of the angle between the centered $n$-dimensional data vectors). As such, it has the same bounds as the cosine. You may find this answer relating to covariance informative (since correlation is just the covariance of the standardized scores above). (Edit: oh, I see it's linked from the page you linked to, which seems to basically reproduce whuber's argument, but not present it nearly as nicely.)
