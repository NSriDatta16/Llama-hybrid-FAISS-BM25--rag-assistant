[site]: datascience
[post_id]: 116846
[parent_id]: 
[tags]: 
Hyperparameter Tuning vs Regularization

While designing the architecture of a Neural Network, should I consider adding regularization (like Dropout, L1/L2, etc.) even after optimizing the problem using Hyperparameter Tuning? What should be the main focus (tuning or regularization) to achieve more generalization, considering that both of these simplify the problem? P.S.: Links to some recent publications would be of great help.
