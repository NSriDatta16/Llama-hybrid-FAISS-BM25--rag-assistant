[site]: stackoverflow
[post_id]: 3596744
[parent_id]: 
[tags]: 
How can I randomize this text generator even further?

I'm working on a random text generator -without using Markov chains- and currently it works without too many problems -actually generates a good amount of random sentences by my criteria but I want to make it even more accurate to prevent as many sentence repeats as possible-. Firstly, here is my code flow: Enter a sentence as input -this is called trigger string, is assigned to a variable- Get longest word in trigger string Search all Project Gutenberg database for sentences that contain this word -regardless of uppercase lowercase- Return the longest sentence that has the word I spoke about in step 3 Append the sentence in Step 1 and Step4 together Assign the sentence in Step 4 as the new 'trigger' sentence and repeat the process. Note that I have to get the longest word in second sentence and continue like that and so on- And here is my code: import nltk from nltk.corpus import gutenberg from random import choice import smtplib #will be for send e-mail option later triggerSentence = raw_input("Please enter the trigger sentence: ")#get input str longestLength = 0 longestString = "" longestLen2 = 0 longestStr2 = "" listOfSents = gutenberg.sents() #all sentences of gutenberg are assigned -list of list format- listOfWords = gutenberg.words()# all words in gutenberg books -list format- while triggerSentence:#run the loop so long as there is a trigger sentence sets = [] sets2 = [] split_str = triggerSentence.split()#split the sentence into words #code to find the longest word in the trigger sentence input for piece in split_str: if len(piece) > longestLength: longestString = piece longestLength = len(piece) #code to get the sentences containing the longest word, then selecting #random one of these sentences that are longer than 40 characters for sentence in listOfSents: if sentence.count(longestString): sents= " ".join(sentence) if len(sents) > 40: sets.append(" ".join(sentence)) triggerSentence = choice(sets) print triggerSentence #the first sentence that comes up after I enter input- split_str = triggerSentence.split() for apiece in triggerSentence: #find the longest word in this new sentence if len(apiece) > longestLen2: longestStr2 = piece longestLen2 = len(apiece) if longestStr2 == longestString: second_longest = sorted(split_str, key=len)[-2]#this should return the second longest word in the sentence in case it's longest word is as same as the longest word of last sentence #print second_longest #now get second longest word if first is same #as longest word in previous sentence for sentence in listOfSents: if sentence.count(second_longest): sents = " ".join(sentence) if len(sents) > 40: sets2.append(" ".join(sentence)) triggerSentence = choice(sets2) else: for sentence in listOfSents: if sentence.count(longestStr2): sents = " ".join(sentence) if len(sents) > 40: sets.append(" ".join(sentence)) triggerSentence = choice(sets) print triggerSentence According to my code, once I enter a trigger sentence, I should get another one that contains the longest word of the trigger sentence I entered. Then this new sentence becomes the trigger sentence and it's longest word is picked. This is where the problem sometimes occurs. I observed that despite the code lines I placed - starting from line 47 to the end- , the algorithm still can pick the same longest word in the sentences that come along, not looking for the second longest word. For example: Trigger string = "Scotland is a nice place." Sentence 1 = -This is a random sentence with the word Scotland in it- Now, this is where the problem can occur in my code at times -doesn't matter whether it comes up in sentence 2 or 942 or zillion or whatever, but I give it in sent.2 for example's sake- Sentence 2 = Another sentence that has the word Scotland in it but not the second longest word in sentence 1. According to my code, this sentence should have been some sentence that contained the second longest word in sentence 1, not Scotland ! How can I solve this ? I'm trying to optimize the code as much as possible and any help is welcome.
