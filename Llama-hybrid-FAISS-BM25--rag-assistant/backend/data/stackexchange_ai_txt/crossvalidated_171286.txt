[site]: crossvalidated
[post_id]: 171286
[parent_id]: 169611
[tags]: 
Addressing the second question only: I guess that variance has been the dispersion measure of choice for most of the statistician mainly for historical reasons and then because of inertia for most of non statistician practitioners. Although I cannot cite by heart a specific reference with some rigorous definition of spread, I can offer heuristic for its mathematical characterization: central moments (i.e., $ E[(X-\mu)^k]$) are very useful for weighing deviations from the distribution's center and their probabilities/frequencies, but only if $k$ is integer and even. Why? Because that way deviations below the center (negative) will sum up with deviations above the center (positive), instead of partially canceling them, like average does, for instance. As you can think, absolute central moments (i.e., $E(|X-\mu|^k)$) can also do that job and, more, for any $k>0$ (ok, both moments are equal if $k$ is even). So a large amount of small deviations (both positive and negative) with few large deviations are characteristics of little dispersion, which will yield a relatively small even central moment. Lots of large deviations will yield a relatively large even central moment. Remember when I said about the historical reasons above? Before computational power became cheap and available, one needed to rely only on mathematical, analytical skills to deal with the development of statistical theories. Problems involving central moments were easier to tackle than ones involving absolute central moments. For instance, optimization problems involving central moments (e.g., least squares) require only calculus, while optimization involving absolute central moments with $k$ odd (for $k=1$ you get a simplex problem), which cannot be solved with calculus alone.
