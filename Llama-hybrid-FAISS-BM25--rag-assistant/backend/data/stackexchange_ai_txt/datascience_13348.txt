[site]: datascience
[post_id]: 13348
[parent_id]: 
[tags]: 
How do I convert a series of timestamps in seconds to milliseconds in order to distribute them smoothly

I have a script that collects memory usage data from a device running a Linux stack. I collect samples roughly every 200ms and write the measurement to a csv file with a timestamp. The problem is that the date binary on the target platform only reports the time in seconds, so I have multiple samples for each distinct timestamp. Some seconds contain more data points than others because the sample rate is affected by what the device is doing. The goal is to spread my data points smoothly over a millisecond scale. I could take a naive approach and just take each second in the data as a separate series and average across it based on the number of data points there. However, this would not really smooth the seriet. It might approximate it well enough. What I'd like to know is how to do this properly. In particular, is there an algorithm or open source library (javascript perfered)? It could be an optimization problem with the following constraints: The total deltas of successive timestamps should be minimized Only the millisecond parts of the timestamps should change (they shouldn't move to another second
