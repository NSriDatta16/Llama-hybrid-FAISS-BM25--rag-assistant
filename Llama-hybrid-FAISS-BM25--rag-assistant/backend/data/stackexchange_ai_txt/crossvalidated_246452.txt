[site]: crossvalidated
[post_id]: 246452
[parent_id]: 246444
[tags]: 
Cross validation implies that the data is compared with a subset of the data (or, less frequently, an external data source). In this regard, it is a "replication" within the data, but with more room for uncertainty. The original idea is to test for consistency - does the inference from the data hold also for smaller parts? In machine learning, the result is improved accuracy. Back propagation is originally a phenomenon observed in real neurons, due to the mechanics of the axon membrane. However, it has a very specific meaning in computational neuroscience: The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. Rumelhart, Hinton & Williams (1986). [Abstract] Learning representations by back-propagating errors, Nature, 323, 533-536. doi: 10.1038/323533a0 http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html
