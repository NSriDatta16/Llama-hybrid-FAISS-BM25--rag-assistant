[site]: crossvalidated
[post_id]: 43730
[parent_id]: 
[tags]: 
Random search for the optimal number of input features and optimal number of hidden layers for a MLP?

I've performed a random search in hypothesis space $$\{(c,h)| c \in U[1,256]; h\in U[1,100];c \in \mathrm{Z} \text{ and } h \in \mathrm{Z}\}$$ that defines the parameters of a standard multilayer perceptron (MLP) neural network. In each step of the random search, I draw two parameters $c$ and $h$. $c$ defines the number of input features and $h$ defines the number of hidden layer nodes. $c$ and $h$ are integers drawn from a uniform distribution defined above. I train a neural network defined by $(c,h)$ and calculate a misclassification rate and average squared error rate for each model. This is done with $10$-fold cross-validation to estimate the true error for each $(c,h)$. I therefore have an average misclassification rate and an average square error rate over the train sets and the left-out sets for each parameter pair. The question is, how do I chose the best pair of $(c,h)$ and is the method I use here sufficient? There is no reasonably clear point in the results as I'd have hoped. The results over the hypothesis space in the training data is: The results over the hypothesis space in the hold-out data is, This question relates to work I've done as part of my masters dissertation, and is related to the question here this
