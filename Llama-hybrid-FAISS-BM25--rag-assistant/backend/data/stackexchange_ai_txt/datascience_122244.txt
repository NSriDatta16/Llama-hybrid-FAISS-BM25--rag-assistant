[site]: datascience
[post_id]: 122244
[parent_id]: 122029
[tags]: 
Being able to generate the type of data you want without relying on a specific package is a useful skill for a lot of reasons. From first principles, 2 variables, A & B, are independent if the following relation holds true: P(A=a | B=b) = P(A=a)*P(B=b). If they are not independent, P(A=a | B=b) != P(A=a)*P(B=b). Numerically, independent variables have low covariance while dependent variables will have high covariance. (Note: we're only considering the association between continuous variables here). I work primarily in R, but the logic remains the same. I'll supply 2 toy examples that you can extend to any number of variables from any number of distributions. # +++++++++++++++++++++++++++++++++++++ # # Simulating dependence among variables # +++++++++++++++++++++++++++++++++++++ # # # Let X = Son Height, Y = Father Height # # X,Y ~ Normal(mu, var) # # Let Father Height have a mean of 70" with an sd of 4" # Let Son Height depend on Father height in the following manner: # if Father Prob(Son > Father) = 0.75 # if Father > mean(Father) => Prob(Son > Father) = 0.25 # # # Create a generating mechanism based on the father's height # if(F > mu.F) # do # S mu.F){ # if dad's taller than average # son is from the taller distribution with prob 0.25 sample(c(taller, shorter), 1, prob = c(0.25,0.75)) } else{ # if dad's shorter than average # son is from the taller distribution with prob 0.75 sample(c(taller, shorter), 1, prob = c(0.75, 0.25)) } }) cor(fathers,sons) ## works out to correlation of -0.4606312 # To extend this to more than 2 variables, you just need to define # the relationships between the levels of variable i and variable j # Since you're using random uniform variables, we can do the following: set.seed(42) x1
