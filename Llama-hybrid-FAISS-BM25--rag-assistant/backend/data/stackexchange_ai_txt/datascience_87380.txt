[site]: datascience
[post_id]: 87380
[parent_id]: 82793
[tags]: 
A tip: Dont A trick: Dont The reason? Machine learning scientific methodology is based on cross-validation. Almost all papers (and i put the almost because of yes) select everything based on cross-validation and not in previous knowledge. Xgboost is particularly more complicated because it has a lot of math involved. For a simpler case, lets say that you have a problem with 3 features and you want to use a Lasso regression. Which hyperparameter will you choose? Even in this really simple case you don't know. You will need to do CV and select them there. For xgboost the only tip that I can provide based on my experience is that the only important hyperparameter is the number of trees on the ensembling. The rest seems not to be so important. My recommendation: only tune number of trees in the xgboost and select the best in cross-validation. Unless you want to find the absolute minime, that then, you are going to explore the whooooole hyperspace. Good luck!
