[site]: crossvalidated
[post_id]: 370273
[parent_id]: 31238
[tags]: 
The likelihood is defined as $\mathcal{L}(\theta; x_1,...,x_n) = f(x_1,...,x_n; \theta)$ , where if f(x; θ) is a probability mass function, then the likelihood is always less than one, but if f(x; θ) is a probability density function, then the likelihood can be greater than one, since densities can be greater than one. Normally observations are treated iid, then: $\mathcal{L}(\theta; x_1,...,x_n) = f(x_1,...,x_n; \theta) = \prod_{j} f(x_j; \theta)$ Let's see its original form: According to the Bayesian inference, $f(x_1,...,x_n; \theta) = \frac{f(\theta; x_1,...,x_n) * f(x_1,...,x_n)}{f(\theta)}$ holds, that is $\hat{\mathcal{L}} = \frac{posterior * evidence}{prior}$ . Notice that the maximum likelihood estimate treats the ratio of evidence to prior as a constant(see answers of this question ), which omits the prior beliefs. The likelihood has a positive correlation with the posterior which is based on the estimated parameters. $\hat{\mathcal{L}}$ may be a pdf but $\mathcal{L}$ is not since $\mathcal{L}$ is just a part of $\hat{\mathcal{L}}$ which is intractable. For example, I don't know the mean and standard variance of a Gaussian distribution and want to get them by training using a lot of observation from that distribution. I first initialize the mean and standard variance randomly(which defines a Gaussian distribution), and then I take one case and fit into the estimated distribution and I can get a probability from the estimated distribution. Then I continue to put the case in and get many probabilities and then I multiply these probabilities and get a score. This kind of score is the likelihood. Hardly can it be a probability of a certain pdf.
