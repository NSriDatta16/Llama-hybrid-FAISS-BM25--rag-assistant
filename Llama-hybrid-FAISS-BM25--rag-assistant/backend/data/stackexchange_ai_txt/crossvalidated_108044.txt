[site]: crossvalidated
[post_id]: 108044
[parent_id]: 108032
[tags]: 
For probabilistic classifiers in general, not only Naive Bayes, one first starts with a model and then fits the model to the data. What does "fitting" mean?. You assume that the data is generated by the distribution (the model). All assumptions you make about your data are reflected in that model. You then look for the parameters which correspond to the particular distribution which is most likely to have generated the training data. See this chapter by Thom Mitchell which explains it in clear terms, and compares it to logistic regression. So what is the loss function you use?. The likelihood of the data for the given model. Please refer to the Wikipedia site on Naive Bayes for several examples. It is a measure of likely it is, that the data has been generated by the given model. The canonical form given in that site for the likelihood (for a given sample $(F_{1}, ... , F_{n})$) is, $$ L = P(C)\prod_{i}P(F_{i}|C) $$
