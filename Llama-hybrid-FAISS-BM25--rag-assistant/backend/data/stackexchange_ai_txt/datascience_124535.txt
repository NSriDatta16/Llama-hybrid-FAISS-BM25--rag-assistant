[site]: datascience
[post_id]: 124535
[parent_id]: 
[tags]: 
Can't overfit Transformer Encoder

In the below code I am trying to train a very simple Transformer Encoder model to basically do nothing with its input. Giving some arbitrary input vector x, the aim of the model is then to output that exact same vector. As my loss function I am using the Mean Squared Error, between my input vector x and the output vector y. So the optimal solution would be located at y=x. However apart from very simple input vector shapes where for example the batch size, sequence length and feature dimension are all one, the model always seems to get stuck at some non marginal loss. import torch import torch.nn as nn import torch.optim as optim class Transformer(nn.Module): def __init__(self, d_model, feedforward_expansion, num_layers): super().__init__() layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=1, dim_feedforward=d_model * feedforward_expansion, batch_first=True) self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers) def forward(self, x): y = self.encoder(x) return y n_batch, n_sequence, n_features = 10, 10, 10 lr = 1e-2 n_epochs = 1000 num_layers = 1 feedforward_expansion = 1 criterion = nn.MSELoss() model = Transformer(n_features, feedforward_expansion, num_layers) optimizer = optim.Adam(model.parameters(), lr=lr) x = torch.randn((n_batch, n_sequence, n_features)) for i in range(n_epochs): optimizer.zero_grad() y = model(x) loss = criterion(x, y) loss.backward() optimizer.step() print(f"{i}: {loss.item():.4e}") I assume this has something to do with the architecture of the Transformer. However I am wondering if someone could explain me in more detail why this model seems to fail at this rather simple task?
