[site]: crossvalidated
[post_id]: 618647
[parent_id]: 618317
[tags]: 
As a general rule, you should never let the testing set have an effect on the training set processing. For example, if you were going to run a PCA on your data, you would run the PCA using training data only . Then, you would use the PCA components found with the training set to transform both the training set and the testing set, but it's important that the testing set is not used to estimate the PCA components. Similarly, in your case what I would recommend is to get the normalization terms using the training set only, and then use them to normalize both the training and the testing set. I'm not sure how the deconvolution procedure works, but similar rule applies, don't let the testing set have any effect on how you process the training set. I would guess that you will probably see some effect of data source, and that may want to correct for that, which is what you mention with batch correction I think. What you do would probably depend on your data and problem, so I don't know what to suggest here. However, I think that the cleanest thing, keeping to the principle above, would be to separate training and testing in each batch, and do the processing so that within each batch, the correction parameters are obtained from training data only (although you'd apply the same parameters to the testing set). From what you say, there is some pre-processing that already comes from the data sources, where training and testing were both used to do the pre processing. This violates the principle above, I would think. But that doesn't necessarily mean that the analysis is not valid, there's only so much one can do with real data. One way to maybe circumvent this, though, is to use a whole batch of data (i.e. one data source) as testing only, and the rest as training.
