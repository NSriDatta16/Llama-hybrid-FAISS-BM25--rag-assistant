[site]: crossvalidated
[post_id]: 511660
[parent_id]: 508270
[tags]: 
Your question pertains to the "selection of variables" problem in regression, for which there is a large statistical literature that is quite a lot to summarise. There are a few issues to consider here, so I will go through them slowly. A preliminary note on training and testing: One of the difficulties that one encounters in conducting selection of variables is that many of the methods for doing this "optimise" over sets of available variables. One problem with this is that most testing methods constructed for regression analysis (e.g., T-tests, F-tests, etc.) assume that no optimisation of selected variables has occurred, which means that if you apply these tests then they will be biased towards showing relationships between the selected explanatory variables and the response variable. In order to avoid this, you can use one of two general approaches. One approach is to use a model form like LASSO regression that has a built-in selection mechanism as part of the model. These types of models build in a selection mechanism by imposing a penalty for non-zero regression coefficients in the model. The subsequent tests for the model take account of this in-built selection mechanism and so they are not biased by it. Alternatively, one can use regular regression methods (that do not have in-built selection mechanisms) but split your data into a training set used to fit the models and perform selection of variables, and then a testing set used to formally test the relationship between the selected explanatory variables and the response variable. In any case, whatever method you use, you will need to consider whether you are performing your selection of variables "within the model" and if not, you will need to consider using a separate training and testing phase, which generally involves some random partition of your data for this purpose. Use one multiple regression, not lots of simple regressions: It is not useful in these cases to construct simple regression models for the individual variables. That approach ignores the colinearity between the explanatory variables, and it does not assist in understanding their contribution to the multiple regression model. The ultimate goal in regression analysis is to model the response as a function of all relevant explanatory variables, so you should be using a multiple regression model for this. From what you have written, it appears that your goal in using the simple regression models was to construct a plot that will show the relationship between each explanatory variable and the response variable. Fortunately, this can be done in a multiple regression model by using added variable plots . This plot shows the relationship between these variables conditional on having removed the effect of all other explanatory variables in the model. Consequently, it shows you the conditional relationship of interest. (Note that the line-of-best-fit in the added variable plot uses the estimated slope coefficient from the multiple regression model, which is not true when you plot the marginal relationships using simplie regressions.) You can construct added variable plots using the avPlots function in the car package in R . This function takes an input model and automatically prints all its added variable plots. It is also possible to extract the information for these plots and plot them using ggplot2 if you prefer the latter graphical system (see instructions here ). Some methods for selection of variables: There are a few broad methods still in use for selection of variables. The simplest method is to avoid it all together, using the "kitchen sink" approach where you fit a regression model will all the available variables in it. Under this approach you eschew the selection of variables analysis and just use all the explanatory variables that are plausible predictors a priori . If you do this then you may end up including some explanatory variables that are not related to the response variable. For models constructed for predictive analysis, the only real cost of this is loss of a few degrees-of-freedom, which is usually not a big problem. For models constructed for expalantory or causal reasons, this method can be problematic. Some analysts do not like this approach, but I think it is quite reasonable in many cases. Another way to do selection of variables in your regression analysis is to use a model form with an in-built selection mechanism, such as LASSO regression. This is equivalent to a Bayesian analysis where you give a prior to your regression coefficients, with some non-zero probability that they are zero. Estimation in LASSO regression will estimate coefficients to be zero if there is sufficiently weak evidence of a relationship between the corresponding explanatory variable and the response variable. The model has an adjustable "penalty" function that allows you to set the appropriate bar for how much evidence you require for includion of a variable in the model. Finally, another common way to do selection of variables is to use standard regression models (i.e., those without an in-built selection mechanism) and then select variables to include based on optimisation of some kind of "goodness-of-fit" statistic. Commonly used statistics for this purpose include the Akaike information criterion, the Bayesian information criterion or the adjusted coefficient of variation. If your model space is not too large (i.e., you do not have too many available explanatory variables) it is usually possible to search over all possible models. Since this involves model selection using optimisation over a large number of models, it is important to separate training of the model from formal testing of relationships in the selected model.
