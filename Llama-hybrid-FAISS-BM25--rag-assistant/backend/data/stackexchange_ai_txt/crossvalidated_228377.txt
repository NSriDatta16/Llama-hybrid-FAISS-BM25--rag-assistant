[site]: crossvalidated
[post_id]: 228377
[parent_id]: 228339
[tags]: 
It's more about the ability to apply an algorithm which can discern the two clusters. Yes, you can see the two clusters, but typical statistical tests of e.g. means won't help since objects colored green are surrounded by red. Thus, try K-nearest neighbors (kNN), with $k=3$. This would essentially involve selecting each object one at a time, finding out the majority class (e.g. color) among the 3 closest neighbors of the object based on Euclidean distance, and then assigning the majority class to the object. (for example, if 2/3 of the 3 closest neighbors are green, then you assign the predicted class of green to the test object). Here, you are predicting class membership based on the majority class (green or red) among the 3 closest neighbors. Simply increment by 1 the elements in a confusion matrix to determine classification accuracy, call this $A$. Since you know the true class (colors shown), you can rerun the same steps but only this time randomly shuffling the true class feature (red/green) over all objects. Repeat this $B=10000$ times, calculate $A^*$ for each iteration, and the number of times the classification accuracy of objects after shuffling (the color feature) $A^*$ exceeds $A$ divided by $B$ is the statistical p-value. This is merely called a randomization test, or empirical p-value testing. FYI - kNN is the optimal first choice to identify a smaller cluster of objects buried inside of other objects in other clusters. Linear discriminant analysis commonly won't pick this up -- and your dataset is the classic picture for which kNN can do better than many others. Support vector machines (SVM) with a radial basis function (RBF) kernel can pick up the green cluster, but it's computationally more expensive than kNN, and therefore violates Occam's Razor - simpler is better.
