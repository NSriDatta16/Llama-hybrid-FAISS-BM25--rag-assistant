[site]: crossvalidated
[post_id]: 503945
[parent_id]: 
[tags]: 
Is Bias Affected By Dataset Size?

I am trying to understand the concept of asymptotic unbiasedness. I understand that an estimator is said to be asymptotically unbiased if, when the size of our data increases to infinity, the bias of the estimator approaches 0. However, this seems to conflict with what I have learned about bias in Machine Learning models. I have learned that increasing the number of examples used in training a machine learned model to infinity will not improve its prediction bias (see page three: https://www.cs.cmu.edu/~tom/10601_fall2012/exams/midterm_solutions.pdf ) However, the concept of asymptotic unbiasedness seems to be in conflict with this, since it seems to imply that increasing the amount of data can affect bias. Am I understanding this correctly?
