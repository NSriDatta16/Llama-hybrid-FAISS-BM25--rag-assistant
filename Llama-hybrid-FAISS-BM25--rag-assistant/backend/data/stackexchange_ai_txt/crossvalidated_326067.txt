[site]: crossvalidated
[post_id]: 326067
[parent_id]: 326063
[tags]: 
There are many, many examples. Way too many to list, and probably too many for anyone to know completely (besides possibly @whuber, who should never be underestimated). As you mention, in controlled experiments we avoid sampling bias by randomly partitioning subjects into treatment and control groups. In bootstrapping we approximate repeated sampling from a population by randomly sampling with replacement from a fixed sample. This lets us estimate the variance of our estimates, among other things. In cross validation we estimate the out of sample error of an estimate by randomly partitioning our data into slices and assembling random training and testing sets. In permutation testing we use random permutations to sample under the null hypothesis, allowing to perform nonparametric hypothesis tests in a wide variety of situations. In bagging we control the variance of an estimate by repeatedly performing estimation on bootstrap samples of training data, and then averaging results. In random forests we further control the variance of an estimate by also randomly sampling from the available predictors at every decision point. In simulation we ask a fit model to randomly generate new data sets which we can compare to training or testing data, helping validate the fit and assumptions in a model. In Markov chain Monte Carlo we sample from a distribution by exploring the space of possible outcomes using a Markov chain (thanks to @Ben Bolker for this example). Those are just the common, everyday applications that come to mind immediately. If I dug deep, I could probably double the length of that list. Randomness is both an important object of study, and an important tool to wield.
