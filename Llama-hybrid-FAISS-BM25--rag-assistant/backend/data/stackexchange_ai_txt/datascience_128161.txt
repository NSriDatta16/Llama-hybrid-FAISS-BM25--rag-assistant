[site]: datascience
[post_id]: 128161
[parent_id]: 128132
[tags]: 
From the annotated transformer link - https://nlp.seas.harvard.edu/annotated-transformer/#full-model For the positionwiseFeedForward network: D_model = 512, and the inner layer has dimensionality dff = 2048. It’s hard to be certain but perhaps the Gemma paper is using the dff for model size. I’ve always had the same understanding as you – n_heads * size_of_head = d_model, regardless of hidden dimension. This appears to agree with the hidden_size parameter on hugging_face for gemma_7b: • hidden_size (int, optional, defaults to 3072) — Dimension of the hidden representations. https://huggingface.co/docs/transformers/en/model_doc/gemma
