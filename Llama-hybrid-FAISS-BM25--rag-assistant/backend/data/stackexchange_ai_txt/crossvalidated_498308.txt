[site]: crossvalidated
[post_id]: 498308
[parent_id]: 498236
[tags]: 
Your formula 3 is correct: $$R_t = \sum_{i=0}^{T} \gamma^i r_{t+i+1}$$ I cannot find where Carnegie Mellon suggests formula 1, and the word "return" is not mentioned in the link you provided. That could be a misunderstanding on your part, from looking at other related formulae. I think McGill's formula is a typo, if you remove the $t$ from $\gamma^{t+k-1}$ (to make $\gamma^{k-1}$ ) it is correct because $k$ is 1-indexed as opposed to 0-indexed $i$ in the above formula. So it is essentially the same if the $t$ is removed from the exponent on $\gamma$ . You also made a typo (replacing 1 in McGill's version with 0 in yours) when you transcribed it here when writing option 2. There is a convention that varies between sources, whether the immediate reward following state $s_t$ and action $a_t$ on time step $t$ is considered to be part of current timestep and noted $r_t$ or the next time step and noted $r_{t+1}$ . I have used $r_{t+1}$ in the above equation because I consider it natural that the reward and next state are returned at the beginning of the next time step - a lot of people follow this convention too, but it is not universal. Therefore, under the convention that the immediate reward is $r_t$ the following version is also correct: $$R_t = \sum_{i=0}^{T} \gamma^i r_{t+i}$$ From your comments it looks like CMU follow this convention, and would use this variant of the formula for discounted return. As an aside, there are other ways to measure performance - and therefore set objectives - in reinforcement learning. For instance, you can consider average reward to cope with non-episodic problems. However, discounted return has a definition matching your formula 3.
