[site]: crossvalidated
[post_id]: 76969
[parent_id]: 
[tags]: 
Normalize vector-based integer data

I'll preface my question by saying I have a very limited knowledge of statistics, and while I've put some thought into this problem, I'm a bit stuck! Onwards... I have a collection of fixed-size vectors of the form $\overline{v} = \{ c_0,\ldots,c_n \}$, where each $c_i \in \mathbb{Z}^*$ and relates to an observation of some unique phenomenon $i$. Each $c_i$ are independent of each other. From a 'training' set $V$ of many such vectors, I would like to compute: The average (expected?) count and standard deviation (error? confidence?) for each phenomenon $i$ Given some new vector $\overline{v}$, some measure of 'distance' from what is 'expected', with respect to $V$. There's one troubling complication: the observation magnitudes $c_i$ for each vector may be scaled differently . I do not know what these scaling factors are for each vector. However, I do know that each $c_i$ in a vector have correct relative proportions to one another. I'm thinking that I need to: Normalize the $c_i$ values in each vector $\overline{v}$ so that all vectors in $V$ are directly comparable to each other. I'm currently thinking I should either normalize each vector by standardizing to z-scores or by converting to unit vectors using the L2-norm . Once normalized and comparable, take the average and standard deviation of all $c_i$ values for each $\overline{v} \in V$, giving the expected normalized average $u_i$ and standard deviation $\sigma_i$ for each phenomenon $i$. This gives rise to a vector of 'expected' values with standard deviations for each phenomenon, $\hat{v} = \{u_0 \pm \sigma_0, \ldots, u_n \pm \sigma_n \}$. Use $\hat{v}$ together with the notion of vector distance (in the 'normalized' space) to determine how close/far some new vector $\overline{v}$ is from that is expected. Am I on the right track?
