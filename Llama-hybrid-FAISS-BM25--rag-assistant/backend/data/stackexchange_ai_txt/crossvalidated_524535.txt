[site]: crossvalidated
[post_id]: 524535
[parent_id]: 104229
[tags]: 
χ^2 is a good test specifically because χ^2 is an easily-computable approximation of K-L divergence. An improvement to the χ^2 test is given by the G-Test , which uses the K-L divergence (or, equivalently, the likelihood ratio) directly. The G-test is harder to compute by hand since it involves logs instead of square roots, but with computers this really doesn't matter any more. However, the G-test (and the χ^2 test) are for testing goodness-of-fit to a prespecified model and determining whether or not you can reject it. Neither test is designed for model comparison , the task of picking the best model from a set of candidate models. While you can try to use the G-test for comparing the two models, AIC will do this job better, as you correctly pointed out. You can make bigger improvements if you move away from model selection, and instead look into Bayesian model averaging or model stacking. These can get significantly more complex, though.
