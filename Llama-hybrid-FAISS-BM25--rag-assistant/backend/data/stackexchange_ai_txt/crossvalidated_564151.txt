[site]: crossvalidated
[post_id]: 564151
[parent_id]: 563023
[tags]: 
The answer to your question depends on how you actually obtain the predictions of your model. If you know / have a good model for the distribution of the errors, and you use it for example for calculating maximum likelihood estimators for the model parameters, then the answer is no - MLE is invariant under such transformations, so it doesn't matter whether you transform the data or not. But if you just naively use some method that implicitly assume normality (such as least squares, which is equivalent to maximum likelihood with normal errors) then your model is effectively misspecified and that can have many undesired consequences. For example MLE's are known to be asymptotically unbiased and efficient, so if you use least squares with normal errors your predictions will have those properties, but with non normal errors they will generally not. To think of an intuitive example, imagine an error distribution with a long one-sided tail, and suppose you have correspondingly few data points far away from the bulk of the data. If you try to fit it with least squares, those 'outliers' will have a large contribution to the loss function so they will drag the entire fitted model away from the bulk of data, resulting in a less accurate model. On the other hand a transformation that normalizes the errors will squeeze the tail, pulling the outliers towards the rest of the data, so you will not suffer from this effect. (Of course this is under the assumption that the transformation does normalize the data, which may or may not be true for a given case). Quantifying the effects of model misspecification is generally hard, because there are obviously many ways in which a model can be misspecified. But if you google for example "consequences of model misspecification" you can find many references. This book might also be relevant.
