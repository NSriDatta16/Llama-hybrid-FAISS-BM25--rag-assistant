[site]: crossvalidated
[post_id]: 123894
[parent_id]: 86316
[tags]: 
I will offer a pragmatic solution that I believe has minimal potential for introducing bias in situations where the numbers of such measurements are modest relative to the overall numbers. I will assume that the instrument has a capacity for measurement which is understood and that there is some lower detection limit (LDL) below which accuracy is low. In some of my work with medical lab values of enzyme activity that value might be 2 or 3. What I do is take all the values below that limit and assign them to the midpoint between 0 and the LDL. I think using the word "null" may be misleading, because in some statistical packages it has a different meaning than "0.0". Admittedly, this crude practice is not endorsed by authorities (Helsel and Curie) and you should seek out more complete analyses if attempting to calibrate a method or have a substantial number of values in teh out-of-range regions. You can get a more carefully reasoned (and illustrated) answer by @cbeleites here: It may be helpful to separate the problems where calibration is not the underlying task from those where that is the scientific concern. I also see that @whuber cites Helsel as the authority and that Helsel maintains a very helpful website. I've been told by a statistician of good repute that a similar practice is sometimes needed when the upper values are very sparse if one is encountering numerical stability problems, and in some cases he has chosen to lump values above the 99.9th percentile at that value. One can also regress on order statistics which is one of the methods offered in the NADA package in R.
