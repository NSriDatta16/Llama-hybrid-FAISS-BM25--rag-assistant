[site]: crossvalidated
[post_id]: 46455
[parent_id]: 
[tags]: 
How to improve neural network sensitivity with a lopsided binary outcome?

I'm working on predicting (not explaining) a 0/1 outcome that generally has only about 10% "1"s (I'm not at liberty to name the variables). N ~40,000. Logistic regression proved unsatisfactory, both when using about 5-10 main effects and after I built in several interaction terms suggested by CHAID procedures. Sensitivity was ultimately only about 25%. I then turned to neural networks (radial basis function networks, in SPSS). I was pretty shocked to see the program fail to classify any cases as "1"s. That is, sensitivity was zero. First question : Is this a common or understandable NN result under these conditions? Next I tried randomly excluding a large number of "0" cases in the training set, bringing the fraction of "1"s up to about 40%. Now the program was able to correctly identify a halfway-decent number of cases in the training set, with sensitivity around 30%, but that dropped to 20% when the solution was applied to the test set, which once again consisted of only about 10% "1"s. 2nd question : How would you get around this problem?
