[site]: crossvalidated
[post_id]: 635300
[parent_id]: 
[tags]: 
Derivations of loss functions for learning loss attenuation in Bayesian DL

I'm fairly new to Bayesian deep learning, so sorry if this is a silly question. I'm trying to implement the work in this paper: What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? Throughout the paper, they give minimisation objectives that allow the model to learn aleatoric uncertainty, but they don't give any formal derivation of where they come from. They are not ad-hoc constructions so there must be a way to derive them. Where can I find the derivations? $$ \Large \mathcal{L}(\theta,p) = -\frac{1}{N} \sum_{i=1}^N \text{log} \hspace{1 mm} p(\text{y}_i|\text{f}^\widehat{\text{W}_i}(x_i)) + \frac{1-p}{2N}||\theta||^2 $$
