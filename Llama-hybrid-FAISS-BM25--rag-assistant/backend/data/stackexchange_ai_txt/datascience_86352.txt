[site]: datascience
[post_id]: 86352
[parent_id]: 66732
[tags]: 
You may duplicate some features in your dataframe. I certainly was looking for two things as you mention,one of them is putting some features closer to the root node, and also want some features to appear more on branches. I understand your concern is to make your trees more aware to some features. I may suggest something there. XGBoost samples each feature uniformly, which it would be nicer if we can say that some features are more important and should be used more. Short hack would be duplicating the columns while decreasing the colsample_bytree ratio. XGBoost for now doesn't support weighted features since it draws features uniformly. I've seen it on another place, there's no specific sampling technique for features(columns in XGBoost) in the documentation. Check colsample_bytree : https://xgboost.readthedocs.io/en/latest/parameter.html (Credits: Heard that technique from a webinar of https://www.kaggle.com/aerdem4 ) Generally, newbies like me starts to create a lot features via engineering, however also may discard that important features becomes minority. Although XGBoost parameters can deal with it while hyperparameter search, it seems really not beneficial creating trees with no rain data because of the domination of high number of engineered wind data when problem is predicting drought . If your concern is specifying domain knowledge, you may also define a domain knowledge via "feature interaction constrainst" in XGBoost, documentation. I hope I'll edit the answer when I become %100 sure what I'm doing with feature interactions: https://xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html#:~:text=Feature%20interaction%20constraints%20are%20expressed,but%20with%20no%20other%20variable . I'm aware these doesn't answer your primary question, it seems not possible for now. Also, I'm not sure that features put on the top are more important, this doesn't seem always true. Edit: XGBoost has added feature_weights to the DMatrix, in 1.3.0! However, 1.3.0 is not stable yet. I'll edit here when I find/apply feature weights. https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor.fit
