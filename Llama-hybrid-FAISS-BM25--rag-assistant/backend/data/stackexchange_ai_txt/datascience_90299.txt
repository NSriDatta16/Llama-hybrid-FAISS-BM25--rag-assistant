[site]: datascience
[post_id]: 90299
[parent_id]: 
[tags]: 
BUG: Tensorflow accuracy on training data different when training than when evaluating

I think I have made some sort of error because my model has a substantially different accuracy while training: Epoch 25/2000 19/19 [==============================] - 26s 1s/step - loss: 2.0174 - categorical_accuracy: 0.4253 than when it is evaluated: >>> model3.evaluate(train_dataset) 19/19 [==============================] - 4s 188ms/step - loss: 2.3323 - categorical_accuracy: 0.2987 even though these two accuracy scores are being evaluated on the same data . I think there might be something wrong with the batch normalisation layer? Like maybe it's not being applied properly outside of train time? If I look at a confusion matrix for the categorization problem on this data, it is predicting all examples as being from the most probable class. I'm trying to create the FCN model from Z. Wang, W. Yan and T. Oates, "Time series classification from scratch with deep neural networks: A strong baseline," 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 2017, pp. 1578-1585. The model is here: model3 = tf.keras.Sequential([ tf.keras.layers.LayerNormalization(axis=1), tf.keras.layers.Reshape((-1, 1)), tf.keras.layers.Conv1D(filters=128, kernel_size=8, strides=1, padding='same', input_shape=(None, 1)), tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(), tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding='same'), tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(), tf.keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='same'), tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(), tf.keras.layers.GlobalAvgPool1D(), tf.keras.layers.Dense(11, activation='sigmoid'), tf.keras.layers.Softmax() ]) model3.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['categorical_accuracy'])
