[site]: datascience
[post_id]: 113576
[parent_id]: 
[tags]: 
Bare minimum Transformer (Keras)

I'm experimenting with Transformers for a time-series prediction problem, where inputs are $k$ -length sequences and outputs are scalar values. Looking at the Keras Transformer example , my question is - what components of the example are actually required? Reproducing the Keras example below: def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0): # Normalization and Attention x = layers.LayerNormalization(epsilon=1e-6)(inputs) x = layers.MultiHeadAttention( key_dim=head_size, num_heads=num_heads, dropout=dropout )(x, x) x = layers.Dropout(dropout)(x) res = x + inputs # Feed Forward Part x = layers.LayerNormalization(epsilon=1e-6)(res) x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x) x = layers.Dropout(dropout)(x) x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x) return x + res def build_model( input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0, ): inputs = keras.Input(shape=input_shape) x = inputs for _ in range(num_transformer_blocks): x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout) x = layers.GlobalAveragePooling1D(data_format="channels_first")(x) for dim in mlp_units: x = layers.Dense(dim, activation="relu")(x) x = layers.Dropout(mlp_dropout)(x) outputs = layers.Dense(n_classes, activation="softmax")(x) return keras.Model(inputs, outputs) Looking at the code, what is the purpose of LayerNormalization ? [Edit] For anyone stumbling on this question, the version on Keras' tutorial page is different to if you google "keras transformer tutorial". The example above was based on the google result. You will notice that LayerNormalization is before multi-head attention which doesn't make much sense to me.[\Edit]
