[site]: crossvalidated
[post_id]: 616338
[parent_id]: 
[tags]: 
Expressive power of RNNs and deep RNNs

In class, my professor that a recurrent neural network is for problems where the state changes as a function of past state and new input, i.e. h_(t+1) = f(h_t, x_t), for any f. Then he said we would have, in each RNN cell, a universal function approximation (e.g. a deep neural network) to learn any f and thus solve all of these problems. But then he said that this actually won't work for many reasons (e.g. too slow and too unstable). Instead, usually only a single matrix multiplication is used, or some variant on this like LSTM/GRU. And to add complexity, multiple RNN layers can be used one after another. I am having a hard time seeing whether this is as powerful as using a deep NN in each cell (if we somehow got it to work). In other words, can a deep RNN learn the recurrence for any f? This seems much more non-trivial to show I am also not seeing what, if any, different inductive biases that this approach has compared with the theoretical deep NN approach.
