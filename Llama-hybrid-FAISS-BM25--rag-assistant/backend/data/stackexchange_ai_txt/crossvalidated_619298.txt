[site]: crossvalidated
[post_id]: 619298
[parent_id]: 
[tags]: 
How to avoid bias/avoid overfitting when choosing a machine learning model?

My typical workflow in the past, when creating machine learning models, has been to do the following: Decide on some candidate model families for the task at hand. Divide dataset into train and test sets. Set aside the test set, and pretend it doesn't exist. Do some EDA on the training set. For each model candidate, (depending on the results of the EDA), tune hyperparameters and estimate generalization error via nested cross validation. Choose whichever model family has the best performance in step 4. Check performance of the model chosen from step 5 on the holdout test set. I feel like this still introduces bias. In step 5, we're picking a model family based on performance in the nested cross validation. Is this safe to do? Also, how do we decide on the hyperparameters to try? I feel like one's choice is to get an idea for the ideal range and then do a grid search, or else do a wide grid search over hyperparameters which might leave some performance on the table. Finally, how do you actually choose a final model?
