[site]: crossvalidated
[post_id]: 515009
[parent_id]: 
[tags]: 
Why the Lasso cost function isn't differentiable at $\theta_i=0$ and what is the effect of $g$?

I'm reading Hands-On Machine Learning by Aurélien Géron. The author states that the Lasso cost function isn't differentiable at $\theta_i=0$ so we use a subgradient vector $g$ instead of the gradient vector. The cost function is: $$J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \alpha \sum_{i=1}^n |\theta_i|,$$ and the subgradient function is: $$g(\boldsymbol{\theta},J) = \nabla_\boldsymbol{\theta} \ \text{MSE}(\boldsymbol{\theta}) + \alpha \begin{bmatrix} \text{sgn}(\theta_1) \\ \text{sgn}(\theta_2) \\ \vdots \\ \text{sgn}(\theta_n) \\ \end{bmatrix}.$$ where $\text{sgn}$ is the sign function . Can someone please explain why the cost function $J$ isn't differentiable at $\theta_i = 0$ , and explain the effect of the function $g$ ?
