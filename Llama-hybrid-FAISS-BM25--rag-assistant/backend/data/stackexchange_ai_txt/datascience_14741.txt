[site]: datascience
[post_id]: 14741
[parent_id]: 
[tags]: 
Sample selection through clustering

I have a biased set of samples going into a binary classification sklearn pipeline, white and black samples. It is easy for me to fetch as many black samples as required, while whites are a bit harder to get. I suspect the black set is unbalanced and there for I have an incentive to preprocess as many black samples as my hardware permits, and pick the most representative samples. The feature set (after dropping completely invariant features) is about 28k features. The best approach I came up with is to cluster my black samples to a percentage of my white samples. For example say I have 500k white samples, I would then cluster the black samples to 100k clusters and pick five samples (randomly?) out of each cluster. My questions are the following: How can I have a transformer that will cluster the samples and mask out the unneeded samples in sklearn? Is there something ready for such tasks or should I build my own transformer based on the clustering algorithm? Can you point at any potential issue with what I plan to do? I think one potential risk here is not clustering the white samples, but since I'm not getting enough of those anyway... What clustering algorithms might be a good fit? I'm looking a different clustering algorithms that accept the number of clusters as parameter right now I plan to experiment with most and see which ones work best, but a direction will be appreciated. Additionally, I'd like to read suggestions about other approaches to sample subsampling in my case.
