[site]: crossvalidated
[post_id]: 434252
[parent_id]: 
[tags]: 
How to classify new data point for Kernel SVM?

I am working on implementing the kernel SVM using cvxopt quadratic programming, this is for a class so that why I'm not using something like SKLearn. Assume here no slack variable, only SMV but with the "kernel trick". Starting with linear SMV, once the model is trained, we calculate the $w$ and $b$ , and have a linear decision boundary defined by: $$x^Tw+b=0$$ When using a new (test) data point, I can directly interpret the class of the new by checking if it is above or below this line. My question is how to interpret the class of a test point when I am using a non-linear kernel for the SVM. So I have trained the SVM using the polynomial kernel, where the kernel function can be seen as a feature map: $$K=\langle \phi(x_i),\phi(x_j) \rangle=(x_i^Tx_j+1)^2$$ But again from the optimization I calculate a $w$ and $b$ , which just defines a linear separation hyperplane, so I can't use the decision boundary directly to interpret a new (test) data point. This is where I am unsure what to do. It seems I need to first transform a new data point and then use the linear boundary, like this: $$\phi(x)^Tw+b=0$$ Is this correct? If so, how can I calculate $\phi(x_i)$ for a single data point since I only used the kernel function and didn't transform the input data? I don't have an explicit function $\phi$ to use in this case. Thanks!
