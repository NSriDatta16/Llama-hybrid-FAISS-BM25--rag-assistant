[site]: crossvalidated
[post_id]: 576774
[parent_id]: 576771
[tags]: 
Your question is about feature selection/engineering, and this is a (the) major part of ML. While, for a very long time, people have put a lot of thinking into feature engineering, in recent years the trend has become to not worry too much about finding the best features and rather let the machine do this work. Models that are used with the latter approach are e.g. random forest, gradient boosting machines, and, of course, deep neural networks (DNNs). This works as long as you have enough data, and those "data-driven" approaches need quite a lot of data. So, since you are using LSTMs, i.e. DNNs, you should not have to put much effort into designing good features, just hand all your data to the DNN. Make sure it gets the maximum information. That means, definitely favor the second approach. In case the average (the feature that is used in the first approach) is indeed the best and only necessary feature to use, in the second approach the LSTM should figure that out itself. Of course, it probably is not much more work to also use the first approach and then compare it with the second. That could give you some insight into your data, and whether you should continue tuning your second approach. In case you don't have sufficient data, it is better to go for the classic models, like time series models (ARIMA, VAR, Kalman, ...) which also have the great advantage of being much more interpretable and often revealing new information about your data generating process.
