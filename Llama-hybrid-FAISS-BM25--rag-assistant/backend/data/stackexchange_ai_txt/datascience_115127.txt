[site]: datascience
[post_id]: 115127
[parent_id]: 
[tags]: 
How to solve mismatched code dependency issues between two or more different ML models/structures/frameworks?

I'm fairly new to machine learning. Currently I am trying to build a pipeline that uses two established NLP models. One is BERT that is fairly easy to load and quite structured. Another is FLAIR which has certain dependencies. I am trying to use one environment to load all packages from both, but they obviously use different versions of different packages. For example, different versions of numpy and pytorch. I cannot seem to combine them into one platform. I have faced this problem quite often with combining different models/frameworks (like tensorflow and pytorch) etc. My question: is there any standard way to deal with these kinds of dependency issues? Do I have to have separate environments to load the respective packages? How do developers commonly deal with this?
