[site]: crossvalidated
[post_id]: 95723
[parent_id]: 
[tags]: 
Mixing distributions to model parameter errors in Poisson

I'm trying to fit a complex model to count data from a detector. I have background and background+signal data. My goal is to obtain information from the signal by fitting a Poisson with $\lambda = \lambda_{back} + \lambda_{sign}$. The expected signal intensity $\lambda_{sign}$ at each pixel of the detector is calculated by a 7-dimensional integral which contains the parameters to be estimated. I'm using a Monte Carlo algorithm to compute these integrals together with their error estimate. On the other hand, I'm estimating $\lambda_{back}$ from the background data. Now, I would like to calculate background+signal data likelihood, but introducing some kind of mixing distribution to take into account both background an integration errors. I'd rather not to perform additional numerical integrals and it should be suitable for low (even 0 in some cases) counts. What would be an acceptable approach to this issue? Below is what I have in mind so far. I'd appreciate comments on the validity of this approach. If we let $n$ be Poisson-distributed with $$p(n|\lambda) = \frac{\lambda^ne^{-\lambda}}{n!}$$ and we let $\lambda$ be Gamma-distributed with $$p(\lambda|k,\theta) = \frac{\lambda^{k-1} e^{-\lambda/\theta}}{\theta^k \;\Gamma(k)},$$ the resulting marginal distribution for $n$ is the negative binomial distribution $$p(n|k,\theta) = \frac{\Gamma(n+k)}{n! \; \Gamma(k)}\left(\frac{\theta}{1+\theta}\right)^n \left( 1- \frac{\theta}{1+\theta}\right)^k.$$ In this way there is no need to perform additional integrals. Now the problem is how to choose $k$ and $\theta$, or even assess if such distribution is appropriate for modeling the errors. If we were dealing only with background data, a Bayesian treatment with a Gamma-distributed prior would lead to the above result for the new background counts. On the other hand, I don't know how I should interpret the error estimation returned by the Monte Carlo VEGAS integration. I've been thinking of trying some ad hoc distribution such that its convolution with the Gamma distribution may lead to something reasonable. Any suggestion? I considered dealing with background and signal counts separately and sum them, instead of trying a convolution of their respective distributions for the parameter $\lambda$. If both $\lambda_{back}$ and $\lambda_{sign}$ were Gamma distributed, then the distribution of expected counts for each $n_{back}$ and $n_{sign}$ would be like the above negative binomial (NB). Therefore, the likelihood of the total observed counts would be $$\mathcal{L}[n|(k,\theta)_{back},(k,\theta)_{sign}] = \sum_{n_{back}=0}^n p_{back}(n_{back}) \; p_{sign}(n_{sign}=n-n_{back}),$$ where $p_{sign}$ and $p_{back}$ are the NB from above, with their respective $k$ and $\theta$. This sum can be solved analytically in terms of hypergeometric functions which I could readily evaluate numerically using the corresponding implementation in the GSL library , but since counts are usually few, it seems to work better to sum it numerically instead of relying on the convergence of the hypergeometric function. In order for this idea to work, I would need to approximate or model VEGAS results as Gamma-distributed. I think I could choose $k$ and $\theta$ so that the mean matches the given result, and the SD matches the error: $$k = \frac{result^2}{error^2}, \; \theta = \frac{error^2}{result}.$$ For the background, I would choose $k$ and $\theta$ as if they resulted from a Bayesian analysis with prior values $k=1$ and $\theta=1$. Would this make sense? Is it too bad to use a Bayesian approach to model a distribution that will later be used to maximize a likelihood function? Any better ideas?
