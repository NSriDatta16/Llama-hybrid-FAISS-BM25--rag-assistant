[site]: crossvalidated
[post_id]: 280815
[parent_id]: 
[tags]: 
CNN Architecture

At first, this question is less about programming itself but about some logic behind the CNN architecture. I do understand how every layer works but my only question is: Does is make sense to separate the ReLU and Convolution-Layer? I mean, can a ConvLayer exist and work and update its weights by using backpropagation without having a ReLU behind it? I thought so. This is why I created the following independent layers: ConvLayer ReLU Fully Connected Pooling Transformation (transform the 3D output into one dimension) for ConvLayer -> Fully Connected. I am thinking about merging Layer 1 and 2 into one. What should I go for?
