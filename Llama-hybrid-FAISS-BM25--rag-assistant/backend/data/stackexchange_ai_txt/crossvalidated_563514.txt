[site]: crossvalidated
[post_id]: 563514
[parent_id]: 563396
[tags]: 
I think Ofer-a's suggestions are a bit confusing in this case (they seem geared more towards gradient boosting or a neural net), because RandomForest does not overfit "over time" or with more trees. This is because the trees are independent of one another - so running 500 trees will not increase overfitting any more than doing 50. Moving to directly answer your questions. Q1) You're correct. You can only tell that a model is overfitting by comparing train set performance to test set performance. By definition, "overfitting" is when a model fits itself to the noise of the training data and therefore doesn't generalize well to unseen data (because it fit the noise which, of course, is random/inconsistent). A small note here: You mention you want to build a baseline model with "default" parameters, without hyperparameter tuning. There isn't really such a thing as "default" parameters. Your software package will have defaults, yes, but these are arbitrary. I would recommend tuning your hyperparameters. RandomForest usually does a good job even without tuning, so improvement probably won't be dramatic, but hyperparameter tuning is pretty easy to implement, so why not? Q2) It is very typical to have amazing performance on training data. ML models are usually super complex, so it's rare that they don't manage to fit the training data super well. Again, this isn't a bad thing on its own. It's just if the training set performance is much better than test set, you know overfitting is occurring. I would highly recommend using cross validation to a) tune hyperparameters and b) use the performance on the out-of-sample folds in cross validation as error estimates (they're not perfect, but they're something. For any more advanced readers, please look into nested cross validation for real OOS error estimation), and compare your training set performance vs. validation performance, and then validation performance vs. test set performance. It will give a clearer picture of your situation. Additional note: I think it's fair to say that you can pretty much always expect some degree of overfitting with machine learning models. This intuitively makes sense. as you would expect this super sophisticated algorithm to find some kind of pattern within the training data (regardless of whether this pattern is noise or not). The question with overfitting is simply "how bad is it in my case?". Again - I highly recommend cross validation. Q3) I believe I answered this within my first two responses. Your measure of overfitting is how much better the model does in training than in test set. Q4) Cross validation. It's an absolutely essential tool and especially with a dataset that's pretty small, I cannot recommend it more. It doesn't stop all issues, but it helps you tune your hyperparameters and get better error estimation before running your model on your test set (and therefore biasing your whole experiment as you go back and forth between repeated runs on test set, correction, run on test set, correction etc.) Q5) Yes, that looks like it's also overfitting. However, decision trees are notorious for this (overfitting). I would recommend sticking with RandomForest, and using cross validation. I hope this helps understand overfitting! Next step: Generally speaking, overfitting is reduced by reducing the complexity of the model. For RandomForest, this means limiting the depth or number of nodes within each tree of the RF (so the algorithm is forced to make more simple trees).
