[site]: datascience
[post_id]: 86949
[parent_id]: 52723
[tags]: 
There are studies that explore the different cases where it is better fine-tuning versus directly using the vector representations. Maybe the most relevant study is To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks published at the ACL Conference in 2019. Their results are summarized in Table 1, which, for BERT tells us that in general you obtain better results by fine-tuning (fire emoji ) than with directly using the representations (ice emoji ❄️):
