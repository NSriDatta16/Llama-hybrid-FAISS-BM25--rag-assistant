[site]: datascience
[post_id]: 43677
[parent_id]: 
[tags]: 
Optimal proportion between the amount of Class = 1 and the amount of Class = 0?

I am quite new machine learning methods, so I may not write proper technical formulas. My question is about the optimal proportion between sample size in Class = 1 and Class = 0 in a binary classification problem. (I might use different models, but I will start with decision tree). I am trying to develop a risk model for a population of 300.000 by using some risk factors, and each risk factor has a risk-weight. So basically it is a accumulative model Risk_amount = Risk_factor_1 * weight_1 + Risk_factor_2 * weight_2 + Risk_factor_3 * weight_3 ... My big question is if a Risk_amount really belongs to Class = 1 or not. I want my ML-model to find the most optimal weights to those risk factors. I already know that 1000 out of 300.000 are risky in real. So the amount of target data with Class = 1 is 1000. So the total amount of target data with non-risky population Class = 0 is 299.000 The question is what is the optimal proportion between the amount of Class = 1 and the amount of Class = 0? 1000 Class = 1 vs. 1000 Class = 0 or 1000 Class = 1 vs. the whole Class = 0 as 299.000?
