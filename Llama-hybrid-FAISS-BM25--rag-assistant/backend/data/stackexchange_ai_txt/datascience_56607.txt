[site]: datascience
[post_id]: 56607
[parent_id]: 
[tags]: 
Using data agumentation for a frozen pre-trained model

I was following the following article with regards to doing transfer learning: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html In the section, Using the bottleneck features of a pre-trained network: 90% accuracy in a minute, the authors mentioned that: "Note that this prevents us from using data augmentation" I am not very clear about this; is there a rule that discourages the use of data augmentation when the pre-trained model is totally frozen?
