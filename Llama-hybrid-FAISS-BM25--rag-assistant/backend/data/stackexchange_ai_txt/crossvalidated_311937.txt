[site]: crossvalidated
[post_id]: 311937
[parent_id]: 311920
[tags]: 
Never mind, I think I proved it. Btw, the proof for Eq.3.64 provided by Bishop in the Machine learning and pattern recognition might not be correct. $\sum_{n=1}^Nk(x,x^n)=\sum_{n=1}^N\phi(x)^T(\Phi^T\Phi)^{-1}\phi(x_n)$ can be rewritten as $\sum_{n=1}^N\phi(x)^T(\Phi^T\Phi)^{-1}\phi(x_n)\times 1=\phi(x)^T(\Phi^T\Phi)^{-1}\Phi^T1_N$ let $\Phi^T1_N = a_1$. In fact $a_1$ is the first column of $A=\Phi^T\Phi$ Using the fact that $A^{-1}A=I=[e_1,...e_M]$, we have $A^{-1}a_1=e_1$ Therefore, $\sum_{n=1}^Nk(x,x_n)=\phi(x)^T(\Phi^T\Phi)^{-1}\Phi^T1_N=\phi(x)^Te_1=1$, according to the definition of $\Phi$, $\phi(x)^T=[1,\varphi_1(x),...,\varphi_{M-1}(x)]$ This proof only holds when there is always a bias term of 1 added to the Bayesian linear regression function
