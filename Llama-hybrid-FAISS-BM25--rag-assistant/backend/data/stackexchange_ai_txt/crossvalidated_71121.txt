[site]: crossvalidated
[post_id]: 71121
[parent_id]: 
[tags]: 
converting back to raw/original scale from time series tranformations and standard deviation

If I have a time series of Xt observations. I convert them to returns by: Rt = Ln(Pt) - Ln(Pt-1). I then calculate the 20 period moving average and subtract from the Return to find the de-trended return DRt: DRt = Rt- MA(Rt) Lastly I calculate the 20 period Moving Average of the de-trended return and calculate the standard deviation SD using: SDt = SQRT(SUM(MA(DRt)-DRt)^2/20) So basically I am doing a differencing and de-trending on time series, and then finding the standard deviation. My question is how do I convert back to the original scale the value MA(DRt) + SDt? I cant seem to get the transformations back correctly. I have done this: SDt + MA(DRt) + MA(Rt) = Rt, so exp(Rt) = Yt - Xt, where Yt is the value of SDt +MA(DRt) in terms of the original time series. therefore, Yt = exp(Rt) + Xt or in other words Yt = exp(SDt + MA(DRt) + MA(Rt)) + Xt Pretty sure this is not correct! Also is there a better math function toolkit I can use to write out more clearly what I am doing on stackexchange? Thanks for any help.
