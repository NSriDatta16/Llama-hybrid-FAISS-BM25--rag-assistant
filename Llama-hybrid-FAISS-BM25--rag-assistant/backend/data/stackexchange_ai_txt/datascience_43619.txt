[site]: datascience
[post_id]: 43619
[parent_id]: 
[tags]: 
How is y=mx+b different from hθ(x)=θ0+θ1x?

I could not quite comprehend the hypothesis represented by hθ(x)=θ0+θ1x To find out good values for the parameters θ0 and θ1 we want to minimize the difference between the calculated result and the actual result of our test data. So we subtract hθ(x(i))−y(i) for all i from 1 to m. Hence we calculate the sum over this difference and then calculate the average by multiplying the sum by 1/2m. This would result in: 1/2m∑mi=1(hθ(x(i))−y(i))2 So, I googled further and landed on a youtube video that talked about y=mx+b, where m is the slope and b is y intercept. This is called a Linear Model. Also, in the linear model, the following formula is used to determine m and b. Now my questions are: 1) Is the hypotheses and the linear model the same? 2) Is there a cost function for the linear model? 3) Why would anyone want to guess, choose and arrive at θ0 and θ1 when there is a straight forward formula (linear model)?
