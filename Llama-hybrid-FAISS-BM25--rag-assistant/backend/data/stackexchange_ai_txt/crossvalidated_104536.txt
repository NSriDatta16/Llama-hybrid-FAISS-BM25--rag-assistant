[site]: crossvalidated
[post_id]: 104536
[parent_id]: 104531
[tags]: 
Consider the following steps: Pick $\lambda$ using training set where it was the best in some sense (best average performance on validation folds, which are subsets of the training set). Fix this $\lambda$ and get a new regression model using the training set. Evaluate the model on a held out test set (which we hope was drawn from the same distribution as the training set). Think of what we did in steps 1 and 2. All we did was choose the coefficients of the ridge model (say $\beta$) and the regularization coefficient $\lambda$ using the training set only . In other words, Steps 1 and 2 can be seen as a black box which optimized all the parameters of our full model, a.k.a $(\beta,\lambda)$, using the training data. In the above point of view, there is no reason why this full model $(\beta,\lambda)$ has to perform better than a new full model $(\beta,10*\lambda)$ we come up with after getting feedback from the held out test set. Although we expect that the best performing model on the training set translates to great performance on the held out test set, it cannot beat a model which we come up with after looking at/taking into account the held out test set.
