[site]: crossvalidated
[post_id]: 439946
[parent_id]: 439839
[tags]: 
You aren't adding negative correlation correlation between observation and mean, you're taking out positive correlation between observation and mean. The whole problem with not doing cross-validation is that if you have n data points, then each time you do a prediction for one of the data points, 1/n of the prediction is coming from itself, so you're over estimating accuracy by an amount proportional to 1/n. If you take the point out, you're getting rid of that overestimation, and so the lower accuracy isn't overestimating error, it's getting a more valid estimate of error. Consider rolling a die ten times, and you're trying to predict the roll $x_i$ . Let $\bar x$ be the mean of all ten rolls, and $\bar{x'}$ be the mean of the nine rolls excluding $x_i$ . Clearly, there's correlation between $x_i$ and $\bar x$ . Try simulating 100 trials of rolling ten dice, and comparing the first roll to the overall mean for each trial. While the expected value of $\bar x$ is 35, there's going to be some variation, and the trials in which the average is higher than 35 are, more likely than not, trials in which the first roll is higher than 3. There is, however, no correlation between $x_i$ and $\bar{x'}$ . The other two rolls are independent of $x_i$ . While there is negative correlation between $x_i$ and the change between $\bar x$ versus $\bar {x'}$ , this is just reflecting the fact that $\bar x$ had a correlation that you're getting rid of.
