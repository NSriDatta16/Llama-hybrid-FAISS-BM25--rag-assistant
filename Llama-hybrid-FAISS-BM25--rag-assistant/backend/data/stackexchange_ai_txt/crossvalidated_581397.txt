[site]: crossvalidated
[post_id]: 581397
[parent_id]: 581376
[tags]: 
The operation of putting a value to zero, removes the gradients (see the ReLU activation function). You might want to consider that by putting a large part of your image to zero, you are giving the neural network a quite skewed input distribution. I would normally put this down as a comment, but I dont have enough reputation.
