[site]: crossvalidated
[post_id]: 216324
[parent_id]: 216307
[tags]: 
You can absolutely combine simple models into a more complex model. This class of techniques is called ensemble methods , and can be very successful. As to whether a specific form of this idea will help on a specific problem, you'll just have to try it and see. If the models are linear, and combined linearly, then it won't help. Some techniques to look up are bagging and boosting . These use different combinations/weightings of the data points to generate the different models, so they're slightly different than your question about using subsets of features. But, looking at the way the different models are combined and the principles behind it can give you some insight. Random forests use a combination of decision trees, where each tree is trained using a different subset of data points and features. Stacking uses a final 'meta model' to combine the predictions of lower-level models.
