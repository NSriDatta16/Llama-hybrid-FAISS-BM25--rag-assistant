[site]: crossvalidated
[post_id]: 560421
[parent_id]: 560077
[tags]: 
The model is based on a vector of responses $y$ associated with explanatory variables arranged into a parallel "model matrix" $X$ with one row per observation. Assuming, as is usual, that the responses differ from "true" values $X\beta$ by independent, identically distributed errors with zero mean and (unknown) variance $\sigma^2,$ the Ordinary Least Squares estimate of the parameters is $$\hat\beta = (X^\prime X)^{-1}X^\prime y\tag{*}$$ (at least when $X$ is of full rank). Similarly, assume the future responses $z$ are governed by the same model and will correspond to a matrix $Z,$ again with one row for each set of explanatory values of the future responses. To complete the specification of the problem, let $\gamma$ represent a linear combination of the future responses, which will be the (random) number $\gamma^\prime z.$ According to the OLS estimate $(*),$ the individual predictions (at the rows of $Z$ ) are the random variables $$\hat z = Z\hat\beta + e$$ where $e$ is the vector of (unobservable) errors. The expectation of the desired linear combination of these predictions is $$E[\gamma^\prime\hat z] = E[\gamma^\prime (Z\hat\beta + e)] = Z\gamma^\prime E[\hat\beta] + \gamma^\prime E[e] = \gamma^\prime Z\beta + \gamma^\prime 0 = \gamma^\prime Z\beta,$$ the $\gamma$ linear combination of the "true" responses $Z\beta.$ Its variance is $$\operatorname{Var}(\gamma^\prime\hat z) = \operatorname{Var}(\gamma^\prime( Z\hat\beta + e)).\tag{**}$$ Because $e$ is assumed independent of the $y,$ it is independent of $\gamma^\prime Z\hat \beta,$ permitting us to sum these variances separately. Since the components of $e$ are independent, $$\operatorname{Var}(\gamma^\prime e) = \gamma^\prime \operatorname{Var}(e) \gamma = \sigma^2\gamma^\prime \gamma = \sigma^2\,|\gamma|^2.$$ For instance, when $Z$ is $k$ additional values to be summed, $\gamma^\prime = (1,1,\ldots,1)$ (a vector with $k$ entries) this variance reduces to $k\sigma^2.$ Finally, the variance of the first term in $(**)$ is $$\operatorname{Var}(\gamma^\prime Z \hat \beta) = \gamma^\prime Z \operatorname{Var}(\hat\beta) Z^\prime \gamma.$$ The variance of the coefficient estimate $\hat\beta$ is obtained, as usual, from $(*)$ as $$\operatorname{Var}(\hat\beta) = (X^\prime X)^{-1}\sigma^2.$$ This gives a useful expression for the first variance term, $$\operatorname{Var}(\gamma^\prime Z \hat \beta) = \gamma^\prime Z (X^\prime X)^{-2} Z^\prime \gamma\, \sigma^2.$$ Whence the standard error of the predicted random variable $\hat z$ is $$\operatorname{se}(\hat z)^2 = (\gamma^\prime Z (X^\prime X)^{-2} Z^\prime \gamma + \gamma^\prime \gamma)\sigma^2.$$ An approximate $1-\alpha$ interval (exact when the errors are Normally distributed) is thereby obtained by setting limits at distances $t_{\alpha/2, \nu}\operatorname{se}(\hat z)$ on either side of the predicted value $\gamma^\prime Z\hat\beta,$ after substituting an estimate $\hat\sigma^2$ of $\sigma^2.$ (The usual OLS estimate is the unbiased one, but this method applies even to other estimates, such as the maximum likelihood estimate (the mean squared residual).) The term $\nu$ is the "degrees of freedom," equal to the number of rows of $X$ minus its rank, and $t$ represents the percentile of that Student t distribution. You can use the standard Normal distribution when $\nu$ is large; typically, $\nu \ge 20$ permits this. To interpret this interval, contemplate the entire procedure of observing $y,$ fitting the model, constructing the prediction interval from that, observing $z,$ and comparing $\gamma^\prime z$ to that interval. The chance this value lies in the interval (is "covered by" the interval) is approximately $1-\alpha.$ Thus, the risk that the interval fails to cover $\gamma^\prime z$ is only $\alpha$ -- and this risk doesn't depend on actually observing $z.$ Thus, if all you have available are the data $X$ and $y,$ in the sense just described you can have $100(1-\alpha)\%$ "confidence" that $\gamma^\prime z$ lies in your interval. Here is an example. Observations were made of the model shown at the left at ten equally spaced points $x_i$ between $0$ and $1.$ The "future" observations $z$ were made at the $k=5$ red points. The chosen linear combination was, as in the question, the sum of their values. In a simulation, 5000 datasets were created according to the OLS model (using Normal errors with $\sigma=1/4$ ) and, independently, 5000 sets of values of $z$ were also randomly created. In each iteration a $100(1-0.05) = 95\%$ prediction interval for the sum of the $z$ was computed and compared to the observed sum. The plot at the right shows (1) the true sum (horizontal dark line), which was constant throughout; (2) the lower (blue) and upper (red) prediction limits for the first 500 iterations, sorted in decreasing order; (3) the predicted values $\gamma^\prime\hat z$ (cyan); and (4) the observed values of the $\gamma^\prime z,$ displayed as gray dots. The ones that fell beyond their prediction interval are shown darker and larger than the others. In this simulation, 94.3% of the prediction intervals covered the values of $\gamma^\prime z.$ Much can be learned by varying the specifics of this simulation. Here, then, is the R code that produced it. It shows how all the formulas can be expressed in a matrix-oriented programming language. All the work is done in just three lines of code, following the comment "The prediction of the sum and its variance." (I set the number of iterations in the simulation to 5,000 because that takes less than a second to complete and provides a precise estimate of the coverage.) # # The model. # f $x, Z.df$ x)) x $x, f(X.df$ x), pch=21, bg=gray(0.5)) points(Z.df $x, f(Z.df$ x), pch=21, bg="Red") sim $x) fit x, fit $coefficients)) se x)) c(Covers = prod(PI - z.sum) 500) sim
