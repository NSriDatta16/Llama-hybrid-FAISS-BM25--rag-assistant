[site]: datascience
[post_id]: 4881
[parent_id]: 
[tags]: 
Probability distribution in input-output pairs

This question might sound silly. But I have been wondering why do we assume that there is a hidden probability distribution between input-output pairs in machine learning setup ? For example, if we want to learn a function $f: \mathcal{X} \rightarrow \mathcal{Y}$, we generally tend to assume a probability distribution $\rho(x,y)$ on $Z=\mathcal{X} \times \mathcal{Y} $ and try to minimize the error $$ \mathcal{E}(f) = \int (f(x)-y)^2 \ d\rho(x,y) $$ Is the probability distribtution $\rho$ inherent to the very nature of $Z$ or depends on $f$ ? Can anyone please provide a good intuitive explanation for this ?
