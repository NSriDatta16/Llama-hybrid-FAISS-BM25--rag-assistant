[site]: datascience
[post_id]: 115541
[parent_id]: 
[tags]: 
The role of policy optimization in model-based RL

So I have a simulation $M_{sim}$ that approximates a nonlinear dynamic robotic model $M_{real}$ by solving a set of nonlinear differential equations. Given $M_{sim}$ , I use an agent $A$ (namely, I use $PPO$ , which is a policy optimization method), to find an optimal policy $\pi^*$ w.r.t to some reward term $R$ . Though I did receive some good results (was able to increase the return over time), It crossed my mind that I'm actually mixing approaches from different worlds - if $M_{sim}$ is given, then we are dealing with model-based RL , and if so, how is that reasonable to use policy optimization, that by definition does not assume anything about the world? At least in terms of taxonomy, it would have been more reasonable if the learning process of $A$ would occur on $M_{real}$ , as this is considered model-free , but when $M_{sim}$ is used, all the notion of using policy optimization only with model-free approaches doesn't seem to work. am I missing something?
