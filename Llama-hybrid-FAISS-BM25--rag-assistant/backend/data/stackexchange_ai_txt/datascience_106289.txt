[site]: datascience
[post_id]: 106289
[parent_id]: 
[tags]: 
Shifting The Result of Regression Model By N-Value

I am doing a multivariable regression problem, which predicts the frequency of a failure mode in a production system. In this problem, I used XGBRF for Regression as my ML model. These are the results of walking forward predictions over 6 months compared to the actual data: From experiments on the hyperparameters and other datasets, It seems that my model will always over-predict. (no negative errors) So, my point is, is it a valid practice to just use this model, then straightly subtract a number (for example 1 or 2) to "improve" the result? For example, from the result above into this one below: I am new to the machine learning modeling world and statistics. And maybe this is a basic no-go. But I'd like to know if this practice is fine or not. Please enlighten me.
