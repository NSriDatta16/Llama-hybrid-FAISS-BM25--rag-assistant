[site]: crossvalidated
[post_id]: 32529
[parent_id]: 32527
[tags]: 
The $Y_i$s are dependent because some share the same noise terms. However you still have a well defined likelihood function for beta. The theory of maximum likelihood still applies. Determine the likelihood and obtain the mle for beta. A weighted average of the Yis would be unbiased and you could find the weights that minimize the bias amoung the linear estimators. The mle could be biased but I am reasonably sure it will be consistent, asymptotically normal and achieve the Cramer-Rao lower bound. This should be checked. For the best linear unbiased estimator of beta let that be $B= a_1Y_1+a_2Y_2+a_3Y_3+ a_4Y_4$. Then $${\rm Var}(B) =a_1^2 {\rm Var}(Y_1)+a_2^2{\rm Var}(Y_2) + a_3^2 {\rm Var}(Y_3) +a_4^2 {\rm Var}(Y_4) + 2a_1 a_2 {\rm Cov}(Y_1,Y_2)+2a_1 a_3 {\rm Cov}(Y_1, Y_3)+2a_1a_4 {\rm Cov}(Y_1,Y_4)+2a_2a_3 {\rm Cov}(Y_2,Y_3)+2a_2a_4 {\rm Cov}(Y_2,Y_4) +2a_3a_4 {\rm Cov}(Y_3, Y_4)$$. Now by the definition of the $Y_i$s, ${\rm Cov}(Y_1,Y_2)=0$. All the other covariance terms = sigma square because the Yi pairs each contain one common epsilon. So $${\rm Var}(B)= \sigma^2(2a_1^2 +2a_2^2+3 a_3^2+3a_4^2 + 2a_1 a_3 +2a_1 a_4 +2 a_2 a_3+2a_2 a_4 +2 a_3 a_4)$$ Now for $B$ to be unbiased we have the constraint $a_1+a_2+a_3+a_4=1$ So the solution does require the use of Lagrange multipliers.
