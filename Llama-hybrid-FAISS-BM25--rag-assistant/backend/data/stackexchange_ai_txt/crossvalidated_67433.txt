[site]: crossvalidated
[post_id]: 67433
[parent_id]: 
[tags]: 
Out-of-bag estimate biased by correlated features

I have a data set with a small number of samples (322) and a large number of features (318.976). My data consists of images, and I want to train a binary classifier. Since I have such a small amount of data available, I have employed the following strategy: Reduce dimensionality using a few hand-written rules (from 318.976 to 12.000 features) Train Sparse Filter to reduce dimensionality further (from 12.000 to 1000 features To train the sparse filter, I construct additional data by shifting my images horizontally which my problem is invariant under. This gives me 19.320 samples instead of 322. Now my data set has 19.320 samples and 1000 features In the training set, only around 2% of the samples are positives Now I train a random forest on the data set weighting the positive samples up so they in total achieves the same weight as the negative samples Here my problem arises. I get a quite good OOB-estimate from my random forest, but it is overly confident. The problem is that when I shifted my data to obtain more samples and then trained the sparse filter, I get some some sets of features which are just shifted versions of each other. When I then train my random forest, these features have a very high correlation, and the OOB-estimate does not take into account that some of my features are very highly correlated, consequently over-estimating the predictive power. Since I have so few samples originally, I have no good way to do cross-validation, especially since I have a highly imbalanced data set. Is there any way to get slightly more accurate error-estimates?
