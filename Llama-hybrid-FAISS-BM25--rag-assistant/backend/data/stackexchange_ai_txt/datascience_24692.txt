[site]: datascience
[post_id]: 24692
[parent_id]: 24583
[tags]: 
Tree-based classification models are generally insensitive to oversampling. That's because when you upsample the 'true' class, it affects leaf impurity metrics (such as Gini or entropy) only marginally. Imagine a leaf with $a$ 'true' and $b$ 'false' examples - the Gini would be $2\frac{ab}{a+b}$. When you upsample 'true' class by factor of $\lambda$, the Gini becomes $2\lambda\frac{ab}{a\lambda+b}$. If $a Thus my advice would be to give up upsampling attempts, and concentrate on feature engineering, parameter tuning, and maybe getting more real observations. If you still want resampling, then you can just set unequal class_weights when fitting your trees - it will affect trainging, but keep cross-validation metrics intact. If you are interested in top 10% conversion, you may well approximate this metric by ROC AUC and optimize the latter. AUC is not sensitive at all to over- and undersampling. If your model is precise enough, then most of the 15% 'true' examples will be somewhere around this top 10%. Thus, ranking performance measured by AUC and selection performance measured by 10% conversion would strongly correlate, and you can select your best model by AUC.
