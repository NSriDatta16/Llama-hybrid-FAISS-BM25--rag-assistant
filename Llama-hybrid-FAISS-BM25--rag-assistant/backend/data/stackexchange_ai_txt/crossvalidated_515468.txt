[site]: crossvalidated
[post_id]: 515468
[parent_id]: 515246
[tags]: 
It's kernel ridge regression with zero weight decay (zero regularisation, $\lambda=0$ ). Denote by $\textbf{k}$ the vector whose components are $k_i = k(x, x_i)$ . Then you can write your $f(\textbf{x})$ in a vector form: $$ f(\textbf{x}) = \alpha^T \textbf{k} = \textbf{y}^T \textbf{A}^{-1} \textbf{k} $$ For comparison, kernel ridge regression is given by $$ f(\textbf{x}) = \textbf{y}^T (\textbf{A} + \lambda \textbf{I}) ^{-1} \textbf{k} $$ For reference, see e.g. Slide 31 here , or p. 119 in Cristianini and Shawe-Taylor, " An Introduction to Support Vector Machines and other kernel-based machine learning methods ".
