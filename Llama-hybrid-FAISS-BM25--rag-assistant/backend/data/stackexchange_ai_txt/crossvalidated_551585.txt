[site]: crossvalidated
[post_id]: 551585
[parent_id]: 
[tags]: 
Would Support Vector Machines work on arbitrary Hilbert spaces?

I have a few questions, Has SVM ever been used to classify points in a Hilbert space other than $\mathbb{R}^n$ ? Say $\ell^2$ or $L^2$ ? The concepts involved in the derivation (like the margin) all seem to carry over just fine to arbitrary Hilbert spaces (*). As far as I understand, the kernel trick relies on the Mooreâ€“Aronszajn theorem to work. However, the theorem doesn't say the RKHS associated to a given kernel is one of the $\mathbb{R}^n$ . As far as I know, it could be one of the infinite-dimensional ones such as $\ell^2$ . Moreover, kernel-SVM consists in sending the data set to another Hilbert space and then applying linear-SVM. Wouldn't this require the justification that linear-SVM works on arbitrary Hilbert spaces? All the derivations I've seen seem to assume that (1) the data set consists of vectors on $\mathbb{R}^n$ and that (2) the inner product is the dot product. The last question may suggest that I don't understand how the kernel trick works. If that is the case, can someone give me a formal reason so as to why it is fine to change the dot products $\langle x_i, x_j \rangle$ in the optimization problem to kernels $K(x_i, x_j)$ ? My understanding is that one would be starting the SVM on a whole new space where the algorithm is still applicable, which means the derivation of the algorithm would be the same, just instead of working with $x_1, ..., x_n$ and $\mathbb{R^n}$ one would be working with $\phi(x_1), ..., \phi(x_n)$ and $H$ . (*) I don't know about the quadratic program; how would one differentiate with respect to a sequence?
