[site]: crossvalidated
[post_id]: 253759
[parent_id]: 194078
[tags]: 
Whenever backproportionate to an activation function, the operations become element-wise. Specifically, using your example, $\delta_2 =(\hat{y}-y)W_2^T$ is a backpropagation derivative and $a' = h \circ (1 -h)$ is an activation derivative, and their product are elementwise product, $\delta_2 \circ a'$. This because activation functions are defined as element-wise operations in neural network. See the cs224d lecture slides page 30, it might also help.
