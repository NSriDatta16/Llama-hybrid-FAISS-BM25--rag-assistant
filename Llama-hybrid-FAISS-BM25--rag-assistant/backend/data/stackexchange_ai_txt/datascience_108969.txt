[site]: datascience
[post_id]: 108969
[parent_id]: 108966
[tags]: 
By padding and this is not a necessarily a deep learning thing. In general if inputs are from variable lengths you may fix them to a specific size which let's you input all your data. Theoretically that size is the length of longest sequence in your data and any shorter sequence gets some zeros to become as long as the fixed size. Approach above is complicated as you may need to use the model on an even longer sequence e.g. an unseen longer input during test phase. That's why zero padding is combined with cutting sequences e.g. all pertained language model has a fixed input size. If your sequence is shorter than that, they add it with 0 in preprocessing/tokenization step and if input is longer than fixed size, they cut it. You may have a look at this question and its answers .
