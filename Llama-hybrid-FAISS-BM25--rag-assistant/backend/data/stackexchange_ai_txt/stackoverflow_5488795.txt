[site]: stackoverflow
[post_id]: 5488795
[parent_id]: 5488247
[tags]: 
You could use PHP (or another server-side script) to detect the user agent of webcrawlers you specifically want to target such as Googlebot. In the case of a webcrawler, you would have to use non-JavaScript-based techniques to pull down the database content and layout the page. I would recommended not paginating the search-engine targeted content - assuming that you are not paginating the "human" version. The URLs discovered by the webcrawler should be the same as those your (human) visitors will visit. In my opinion, the page should only deviate from the "human" version by having more content pulled from the DB in one go. A list of webcrawlers and their user agents (including Google's) is here: http://www.useragentstring.com/pages/Crawlerlist/ And yes, as stated by others, don't reply on JavaScript for content you want see in search engines. In fact, it is quite frequently use where a developer doesn't something to appear in search engines. All of this comes with the rider that it assumes you are not paginating at all. If you are, then you should use a server-side script to paginate you pages so that they are picked up by search engines. Also, remember to put sensible limits on the amout of your DB that you pull for the search engine. You don't want it to timeout before it gets the page.
