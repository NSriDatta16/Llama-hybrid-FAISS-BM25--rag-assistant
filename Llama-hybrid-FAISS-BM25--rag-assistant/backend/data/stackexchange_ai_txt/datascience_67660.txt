[site]: datascience
[post_id]: 67660
[parent_id]: 67614
[tags]: 
BERT generates contextualized word embeddings, which means that BERTprovides the most accurate embeddings when a word is in a sentence(context). For each of the words within the sentence, BERT will generate a vector of numbers. In your case, you will have a good representation of the word "bank". So if you have a sentence for all the other words that you are trying to match with "bank", all you need to do is to extract the embeddings for them, then compare them with a similarity matric (i.e. cosine similarity). this way you are ranking them to see which ones are most correlated to "bank".
