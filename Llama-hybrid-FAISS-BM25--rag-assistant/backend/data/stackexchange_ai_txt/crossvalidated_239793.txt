[site]: crossvalidated
[post_id]: 239793
[parent_id]: 
[tags]: 
Likelihood function and Fisher information of a coin experiment

I'm trying to understand the notions of likelihood function and Fisher information with a simple example. Suppose I have a coin that gives 0 with probability $p$ and 1 with probability $(1-p)$. I've thrown the coin $n$ times and observed $m$ 0s. My likelihood function is looking like $$L(p) = p^m(1-p)^{n-m}$$ I'm now trying to estimate p using maximum log-likelihood: $$\frac{d \ln L}{d p}(p)=0$$ That gives me $p=\frac{m}{n}$. Looks pretty good. Now I want to evaluate the Fisher information that these $n$ measurements gave me about $p$. The formula on Wikipedia says that it is the average of $(\frac{d \ln L}{d p})^2$. My question is: how should I take this average? As far as I can understand the likelihood associatec with the given sample is a function of the parameter $p$. If $p$ is fixed, $L(p)$ is just a number with average equal to this number.
