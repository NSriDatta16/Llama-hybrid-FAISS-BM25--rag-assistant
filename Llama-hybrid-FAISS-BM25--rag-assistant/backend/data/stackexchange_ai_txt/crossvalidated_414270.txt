[site]: crossvalidated
[post_id]: 414270
[parent_id]: 365778
[tags]: 
I feel like Djib2011, give great points about automated methods, but they don't really tackle the underlying issue of how do we know if the method employed to reduce overfitting did its job. So as an important footnote to DeltaIV answer, I wanted to include this based on recent research in the last 2 years. Overfitting for neural networks isn't just about the model over-memorizing, its also about the models inability to learn new things or deal with anomalies. Detecting Overfitting in Black Box Model: Interpretability of a model is directly tied to how well you can tell a models ability to generalize. Thus many interpretable plots are methods of detecting overfitting and can tell you how well any of the methods suggested above are working. Interpretability plots directly detect it especially if you compare the validation and test result plots. Chapters 5 and 6 of this unpublished book talk about recent advances in the field detection of overfitting: Interpretable Modeling Based on this book, I would like to mention three other methods of detecting and removing overfitting, that might be obvious to some, but I personally find that people forget these too often. So I would like to emphasize them if not one minds: Feature Selection Detection : The less number of parameters and less features your model has the better. So if you only include the important one's of the 100 million (maybe have 75 million instead), you will have a better generalizable model. The problem is many neural networks are not perfect in feature selection especially when # 2 is present. Bootstrap or Boosting fundamentally cannot fix both (only a version called wild bootstrap can). In simpler terms, If you give you neural network junk data then it's going to give you junk out. (L2 Normalization mentioned above is very good at helping with this) Detection and Dealing with Anomalies: The fewer "outliers" the more generalizable the model. By "outliers", we don't mean just outliers in the data. Outliers in the data (like the kind you see with a box plot) is a too narrow definition for neural networks. You need to consider also outliers in the error in a model, which is referred to as influence, as well as other anomalies. So detecting anomalies before you run your network is important. A neural net can be robust against one type of anomaly, but robust not against all other types. Counter Example methods, Criticism methods, and Adversarial example methods, and Influence plots are great at helping you discover outliers, and then figure out how to factor them in. (Ie. change the parameters or even remove some of the data) Stratified Sampling, Oversampling, and Undersampling based on statistical or ethical considerations : I wish i was an expert in under and oversampling, but I am not but I know about stratified sampling. Clustering important factors such as (race, sex, gender) and then doing stratified sampling by the cluster is vital to not overfit when one considers big data. When doing image detection, stratified sampling in combination with clustering is legally required in some fields to avoid racial discrimination. The book linked above briefly talks about a methods to do this. P.S. Should I include more links?
