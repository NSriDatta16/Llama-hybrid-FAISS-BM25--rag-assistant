[site]: crossvalidated
[post_id]: 136789
[parent_id]: 
[tags]: 
Predicting problem hardness based on feature vector

Given some optimization problem. For each instance of this problem I can obtain a number of features, say [p1, p2,..., pn]. Features could be for example: number of customers, number of vehicles, costs, etc. I have several different algorithms which can solve problem instances. After solving an instance, I obtain 2 important quality metrics: 1. t: Time (in seconds) it took to solve this instance. Typically, the higher this value, the harder the instance. 2. q: Quality score of the solution (value between 0 (best) and 100 (worst)). If the problem could be solved within the time limit, the score is always 0, otherwise the score is a value greater than 0. The closer the score is to 100, the harder the instance. Given [p1, p2,..., pn], t, and q for a large number of problem instances. I would like to identify which features (or subset of features) make the problem hard to solve? Trivially, larger problem instances are harder to solve, but what about instances of a fixed size? Are there features which make the problem particularly hard to solve? Q1: What kind of analysis should I perform to investigate this? I suppose there are some machine learning tricks? I'm not sure what to search for. Q2: Are there (academic) tools/software which can perform the desired analysis? This analysis is meant to improve the computations section of a paper, so it shouldn't be too much effort to compute, and certainly not a research project on its own. Q3: Is there a way to order the features from [explains hardness of instance well] to [does not affect the complexity of the instance] Q4: Is there a good way to support these findings visually? To support my question, a small example: Example data: Instance [p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13] q t A-2-20-3 0 1756 A-2-20-4 0 1943 A-3-5-3 0 113 A-3-10-0 0 293 A-3-10-1 0 556 A-3-10-3 0 339 A-3-10-4 0 127 A-3-15-0 0 2786 A-3-15-1 0 1652 This is a very small sample, where all q values equal zero, but in the real set there are many more instance with non-zero q. As you can see, the values of t are rather different. Which features (p1, .., p13) are responsible for this? Is it perhaps a higher value of p13 together with a low value for p5, or does the computation time increase the closer p9 gets to the value 0.5?
