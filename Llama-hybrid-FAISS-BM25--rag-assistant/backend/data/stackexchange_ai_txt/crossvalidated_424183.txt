[site]: crossvalidated
[post_id]: 424183
[parent_id]: 424175
[tags]: 
Then you multiply the matrix of original features (let’s say $X$ ) by the matrix whose columns are the k chosen eigenvectors (let’s say $V$ ) corresponding to the k largest eigenvalues (the 8 representing 95% of the total variance) to get to the reduced-set of transformed uncorrelated features: $$X^{*}=XV$$ Which is a matrix with 8 columns and a number of rows equal to the number of samples that you have, as this matrix represents the 8 PCA-transformed features observed for each sample (row of $X^{*} $ ). Now you can use this new set of features for multiple purposes, including (which is very common!) the estimation a model for the prediction of a certain dependent variable, with the desiderabile properties that, compared to the previous features, the new transformed ones are not correlated and they have reduced the number of predictors from 168 to 8 while retaining a large percentage (95%) of their original total variance.
