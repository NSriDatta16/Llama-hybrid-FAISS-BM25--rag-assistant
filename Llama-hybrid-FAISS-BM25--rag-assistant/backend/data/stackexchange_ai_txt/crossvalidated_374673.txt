[site]: crossvalidated
[post_id]: 374673
[parent_id]: 297658
[tags]: 
I determined the answer to my problem. It is sort of interesting so I thought I would sketch the ideas here. First the problem. In natural resource management one often fits models to age or length data where the errors in the data are highly positively autocorrelated. Assume that a subset of the data are being fitted with a number of multinomial log-likelihoods. To account for the positive autocorrelation one wants to include positively autcorrelated random effects. Let $p_i$ be the predicted proportion for the $i$ 'th bin. The random effects are included as $$q_i=\frac{p_i\exp(\epsilon_i)}{\sum_j p_j\exp(\epsilon_j)}$$ so thaty the $q_i$ are the new predicted proportions in the model. The denominator is included to that the $q_i$ will sum to 1. If $\bar \epsilon$ is the mean of the $\epsilon_i$ this can be written as $$q_i=\frac{p_i\exp(\epsilon_i-\bar\epsilon)}{\sum_j p_j\exp(\epsilon_j-\bar\epsilon)}$$ Subtracting off the sample mean reduces the autocorrelation. In fact if the autocorrelation dies off slowly it is impossible to create random effects with high enough lag 1 autocorrelation. So one want to use a method to parameterize the random effects so that the lag one autocorrelation is high, but dies off more quickly than those fo ar(1) process. It turns out that autoregressive-autoregressive processes are a convenient way to parameterize these random effects. Let $i$ index the components of a time series and for $k=1,\ldots$ let $k$ index the order of the autoregressive-autoregressive process so that $$x_{ki}=\frac{\alpha x_{k,i-1}+ \sqrt{1-\alpha^2} x_{k-1,i}}{\lambda_k} $$ where the $x_{0i}$ are standard normal and the $\lambda_k$ are chosen so that the variance of the time series is 1. By construction $\lambda_1=1$ . The autocorrelation is determined by the single parameter $\alpha$ . For $\alpha=0.9$ and a fourth order process the correlation matrix is 1 0.993956 0.976318 0.948365 0.911855 0.993956 1 0.993956 0.976318 0.948365 0.976318 0.993956 1 0.993956 0.976318 0.948365 0.976318 0.993956 1 0.993956 0.911855 0.948365 0.976318 0.993956 1 One sees how much more quickly it dies off compared to an ar(1) process with the same lag one autocorrelation. From the cholesky decomposition of the inverse of this matrix one can calculate the coefficients of the corresponding fourth order aurotregressive process. $$y_i = 2.96923y_{i-1} - 3.28944y_{i-2} + 1.6125y_{i-3} - 0.295267y_{i-4} + 0.0162462\epsilon_i$$ The element of the correlation matrix which are needed for parameter estimation can be calculated by solving a nonlinear system of equations. Anyway that is the idea. I have incorporated these calculation into a large nonlinear statistical model for analyzing commercial fisheries data.
