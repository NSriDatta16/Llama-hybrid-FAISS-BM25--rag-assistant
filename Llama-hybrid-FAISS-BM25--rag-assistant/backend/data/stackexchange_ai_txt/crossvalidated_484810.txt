[site]: crossvalidated
[post_id]: 484810
[parent_id]: 
[tags]: 
Test for bias in the residuals of regressions

Most of the answers I found say "look at the residual plots". Which is great! But I have a large set of curves, and would like to "distinguish" time series curves that fit a model without bias in the residuals. For example, testing whether curves are mostly "white noise" and fit a model like y = constant_mean + random_noise . Examples In the following example time series, a polynomial fit does not give biased residuals: And, as expected, a linear fit does have biased residuals: How can i test for bias in the residuals? Is there a correct way of doing this? Or any thoughts? Mine so far include: Looking for a test similar to heteroscedasticity, but for the mean of the residuals. Maybe some kind of sliding window scan, of the residuals of a linear fit with null slope. In that case perhaps the "distributions of means" over all time windows (continuous subsets) must be the same across all windows then there is no bias. For example, that should be the case in curves of this appearance: I've also seen some suggestions for using auto-correlation, "serial correlation" or a "white noise test" for the residuals may work (see comments). Use case I have many curves, and i would like to find which of them fit noisy (flat|straight) lines and which do not. Or at least rank my curves in some way. Thanks!
