[site]: crossvalidated
[post_id]: 524382
[parent_id]: 488722
[tags]: 
Yes, this produces a nested cross-validation. Think about how the code sees things: You fit a RandomizedSearchCV : the dataset X gets split into (say) 5 folds. For each of the 5 80% training sets, it calls fit for its estimator for each hyperparameter combination. The estimator here is a StackingClassifier . So on the 80% training set, it produces cross-val-predictions for each of the base models. That is, the 80% training set gets further split into folds, and the base models are trained on some folds and predict on the other. Not directly relevant to your question, but then Those out-of-fold predictions from the base models are used to train the meta-estimator of the StackingClassifier ; this occurs for each of the 80% outer training folds. The base estimators are also rebuilt on these 80% outer folds. Finally, the rebuilt base models and the meta-estimators predict on the 20% outer test folds, and those scores are averaged and compared across hyperparameters. By default, the best hyperparameters are used for retraining on the original dataset. This requires a cross-validation-prediction loop of the base estimators as in (2), but is no longer nested, and finally a version of step (3) now on the entire dataset is performed.
