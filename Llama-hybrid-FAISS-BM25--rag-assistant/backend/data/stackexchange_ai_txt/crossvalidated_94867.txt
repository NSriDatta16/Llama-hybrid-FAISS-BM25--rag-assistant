[site]: crossvalidated
[post_id]: 94867
[parent_id]: 
[tags]: 
Remove outliers and calculate average

I am trying to understand (by reverse engineer) the method a program is using to calculate an average value over a population. The program displays a (very very large) image consisting of tiles. Each tile has some overlap with it's neighbors, expressed in pixels. In total there are 8625 tiles, hence 8625 overlap values (considering the x-axis overlap only). The program calculates a value that looks like an average of all the overlap values, but in some cases it is not equal to the simple average. Here are some examples: If all the tiles have a x-overlap of 50 pixels , the average is of course 50 and the program also reports 50 as the average value. If 4312 tiles have an overlap of 1 pixel and the rest 4313 have an overlap of 100 , the average is 50.50573913 , but the program says 50.511583 . If 4312 tiles have an overlap of 10 pixels and the rest 4313 have an overlap of 100 , the average is 55.005217391 , but the program says 55.01053 . If 4312 tiles have an overlap of 50 pixels and the rest 4313 have an overlap of 100 , the average is 75.00289855 , but the program says 75.00585 . If 4312 tiles have an overlap of 99 pixels and the rest 4313 have an overlap of 100 , the average is 99,500058 and the program agrees (it also reports 99.500058 as the average). The program outputs a summary file which states the following: outlier limit='4' I don't know what that number means or what units it is expressed in though. Assuming that the difference in this results is not due to rounding errors (I am pretty sure it is not), what approach could I follow in order to understand it's logic of removing outliers, or make them weigh less somehow in the calculation of the average?
