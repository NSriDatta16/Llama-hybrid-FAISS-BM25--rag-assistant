[site]: crossvalidated
[post_id]: 587367
[parent_id]: 
[tags]: 
How to merge 2 losses in a reasonable ratio

my question is pretty basic, but I can't find many resources online about this. Say we have two losses for a model, for example a pix2pix GAN, which for those who are not familiar with it, for the generator, has 2 losses: $$ L_{gen} = L_{discriminator} + \lambda L_{rconstruction} $$ where $L_{discriminator}$ is for example mean squared error and $L_{rconstruction}$ is binary crossentropy Now, I would like to know if there is a way to balance those 2 losses, in other words, how to choose $\lambda$ I'm asking this because I quite sure that makes no sense to compare the actual value of the losses, but makes maybe sense to compare the gradient. However, at that point, i don't know which gradient (or, with respect to which weight/layer) I should consider. My question comes from the fact that I would like to know which $\lambda$ gives more importance to the reconstruction wrt the discriminator, and which value of $\lambda$ gives more importance to the discriminator than to the reconstruction
