[site]: datascience
[post_id]: 16621
[parent_id]: 16620
[tags]: 
In all the implementations for CNNs processing images that I have seen, the output in any layer is Width x Height x Channels or some permutation. This is the same number of dimensions as the input, no additional dimensions are added by the convolutional layers. Each feature map channel in the output of a CNN layer is a "flattened" 2D array created by adding the results of multiple 2D kernels (one for each channel in the input layer). Usually even greyscale input images are expected to be represented as Width x Height x 1 so that they fit the same pattern and the same layer model can be used. It is entirely feasible to build a layer design which converts a standard 2D+channels input layer into a 3D+channels layer. It is not something I have seen done before, but you can never rule out that it could be useful in a specific problem. You may also see 3D+channels convolutions in CNNs applied to video, but in that case, the structure will be some variation of Width x Height x Frames x Channels
