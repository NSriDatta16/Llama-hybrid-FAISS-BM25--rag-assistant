[site]: crossvalidated
[post_id]: 12117
[parent_id]: 12068
[tags]: 
See here: http://en.wikipedia.org/wiki/Support_vector_machine#Multiclass_SVM I am not familier with libSVM, but this is the way I would do it (Given c classes): one-vs-all : Train c classifiers, each classifier $i$ with the two classes $c_i$ and (all classes except $c_i$). Afterwards sum-normalize all single-class-scores, i.e. $finalscore(c_i)=\frac{score(c_i)}{\sum{score(c_i)}}$. one-vs-one : Train (c^2 - c)/2 classifiers $m_{i,j}$ (one for each pair $(c_i,c_j)$. Let $m_{(i,j)}(c_l)$ be the score for class $c_l$ of model $m_{(i,j)}$, which means that $m_{(i,j)}(c_l)>=0$ for $l \in \{i,j\}$, else 0. Afterwards calculate the average score for each class, i.e. $finalscore(c_i)=\frac{\sum_{m_{(l,k)}}m_{(l,k)}(c_i)}{c-1}$ and sum-normalize the finalscores afterwards (as in one-vs-all). Final remark: This paper (found on libsvm-page) might help, too: A comparison of methods for multi-class support vector machines
