[site]: crossvalidated
[post_id]: 278031
[parent_id]: 
[tags]: 
Confusion about backpropagation - Matrix dimensions

Following Andrew Ng's notation. Suppose I wanted to implement a 4 layer neural network with the following weights, $\Theta_1 \in\mathbb{M}_{5\times 4},\:\Theta_2 \in\mathbb{M}_{5\times 6},\:\Theta_3 \in\mathbb{M}_{4\times 6}$ Where the input $x\in\mathbb{R}^3$ and output $y\in\mathbb{R}^4$ My question is, when performing Backpropagation. One uses the formulae $\delta^{(L)}=a^{(4)}-y$ , $\delta^{(l)} = \Theta^{(l)T}\delta^{(l+1)}\circ \sigma^\prime(z^{l})$ to calculate the "error" of layer $l$ Following the formulae, we get $\delta^{(4)}\in\mathbb{R}^4 , \delta^{(3)} \in\mathbb{R}^6$ So far so good, but as we calculate $\delta^{(2)}$, the dimensions of $\Theta_2$ and $\delta^{(3)}$ don't match, namely $\Theta_2^T\in\mathbb{M}_{6\times5} , \delta^{(3)}\in\mathbb{R}^6$ Is there anything I have done wrong? Thanks!
