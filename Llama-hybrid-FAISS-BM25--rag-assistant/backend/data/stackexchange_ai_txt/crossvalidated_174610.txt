[site]: crossvalidated
[post_id]: 174610
[parent_id]: 65272
[tags]: 
As bayerj said it, there is no way to know a priori ! Random Forests are relatively easy to calibrate: default parameters of most implementations (R or Python, per example) achieve great results. On the other hand, GBMs are hard to tune (a too large number of tree leads to overfit, maximum depth is critical, the learning rate and the number of trees act together...) and longer to train (multithreaded implementations are scarce). A loosely performed tuning may lead to low performance. However, from my experience, if you spend enough time on GBMs, you are likely to achieve better performance than random forest. Edit. Why do GBMs outperform Random Forests? Antoine's answer is much more rigorous, this is just an intuitive explanation. They have more critical parameters. Just like the random forests, you can calibrate the number of trees and $m$ the number of variables on which trees are grown. But you can also calibrate the learning rate and the maximum depth. As you observe more different models than you would do with a random forest, you are more likely to find something better.
