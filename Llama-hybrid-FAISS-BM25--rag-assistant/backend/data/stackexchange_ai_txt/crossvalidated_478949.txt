[site]: crossvalidated
[post_id]: 478949
[parent_id]: 
[tags]: 
k fold cross validation with regularisation R code - unsure whether it is correct

I'm working through a book called A First Course In Machine Learning. The first chapter introduces K-fold Cross Validation as well as other validation methods, and then has a few paragraphs on regularisation. The explanations are very brief, and I am coming to these concepts for the first time. There are exercises but no solutions, so I am unsure whether I am completing them correctly. The book mainly focusses on the use of Matlab, but there is some sample code available for Python and R on GitHub . The final exercise of chapter 1 presents Olympic Men's data for the 100m and asks to produce a K-fold cross validation and select a regularisation parameter for a 1st order model, and a fourth order model; then compare the two regularisation parameters that give the best predictive performance. To do this I wrote an R Function that combined the K-fold cross validation to find model parameters, and a secant function to converge on a minimum value of lambda (for the regularisation parameter). See the function provided. KFCV_REG_lsq = 1){ # this ensures all data points are incorportated a = 1){ a 0.0001){ Number_Of_Iterations abs(x_new - x1)){ x2 = x_new }else{ x1 = x_new } } return(cbind(L = x_new, Iterations = Number_Of_Iterations)) } L1 When I run this with K = 5, for order of model = 1, I get Lambda = -6.152844e-05. And for K = 5, order of model = 4, I get Lambda = -1.439062e+15 Now I really do not know if this is correct, and if this is how you're supposed to calculate the best value of Lambda. I find many of the explanations in the book a bit too brief, and I struggle to apply the concepts covered in the book. Can the regularisation parameter, Lambda be negative, and are these values (x10^-5 and x10^15) too small and large, respectively? Am I overcomplicating the process, or have I misunderstood the technique? The data is as follows: x
