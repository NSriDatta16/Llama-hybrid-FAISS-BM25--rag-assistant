[site]: datascience
[post_id]: 115562
[parent_id]: 115501
[tags]: 
I think two factors are working against you. First, you are working with tabular data , for which neural networks not often yield optimal results. At least when we are not talking aboult very large datasets. I would approach this problem with tree-based algorithms since they are so much simpler to handle and regularly beat neural networks in (tabular) benchmark datasets (...). Second, what I already mentioned, you are working with a comparatively small dataset . Neural networks are generally hungry for data. 569 Instances minus your test set won't give much space for learning. I would go with RandomForest or XGBoost. In case you just want to experiment with neural networks on this dataset regardless you might be already at the possible optimum. A curcial part for training neural networks is the data preprocessing, which you did not talk about that. There might be room for improvement by applying feature scaling and general feature engineering. Although, I have to say I'm not familiar with this particular dataset.
