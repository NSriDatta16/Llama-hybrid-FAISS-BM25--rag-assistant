[site]: crossvalidated
[post_id]: 385330
[parent_id]: 385313
[tags]: 
I'm not sure if I understand your entire question, but my answer's too long for a comment, so i put it here anyway. Hope it helps. Sorry if I miss the point. So when does this causal interpretation of a PCA actually make sense? You use the term PCA and then the term 'factors', which is often used to refer to factor analysis. The two are similar but come from totally different points of views. They are not interchangeable. PCA, in stats anyway, is not regarded as causal at all (unless there's some really obscure way of using it as such). It is generally viewed as a dimension reduction procedure. The principal components are orthogonal and each successive one will account for less and less variance, so you just drop the last few and say that you've reduced the dimensions but kept most of the structure of the data in tact. Factor analysis can be viewed as causal, sort of. Or at least, that the correlation of observed variables are because of underlying unobserved latent variables--the factors. Factor analysis explicitly assumes that the loadings are correlations between the standardized observed variables and the underlying unobserved factor. PCA doesn't need this at all. Often you will assume factors are uncorrelated for simplicity but they don't have to be, whereas principal components are by definition orthogonal. After factor analysis, you will use Structural Equation Modelling to establish causation between latent factors and each other. Random variance not explained by any of the factors You had this comment in your code, PCA does have any unique variance that will not be accounted for by the principal components. If you have as many principal components as you have variables they will account for all variance in the original data. Factor analysis on the other hand, does have unique variance of each variable that will not load entirely onto the factors. Which indicates a common underlying cause for y1, y2, and y3 (which is not what I actually generated). It looks like you did input Y. From your code, it looks like you generated Y from G, and cbined them into a matrix. You then fed that matrix into the prcomp command. So, R is using the covariance matrix of Y's, not of G's. So the principal components are in terms of Y's. If you cbind(g1, g2, g3) and the feed that into prcomp you should get principal components in terms of G. My first guess, was that the gi may not be perfectly uncorrelated (in fact they are not) and hence the PCA is trying to account for this slight correlation. You definitely want your original variables to be correlated, not uncorrelated. The point is to rotate the matrix so that it maximizes the variance of the data onto the first PC, and then maximizes all the variance remaining onto the next PC and so on. If the data is not correlated then the correlation matrix is literally the identity matrix and all the eigenvalues would equal 1, and so it won't do anything. Here's another answer of mine on this exact topic. Also on a related note, are there exact conditions when a PCA might be able to find the original factors? I'm not sure if the following is what you mean, but if you don't drop any principal components then you can just unrotate the data and you have your original values again. In your case, you can rotate the data back to get Y and then solve for G again. Here's an awesome answer of how to do that using different codes.
