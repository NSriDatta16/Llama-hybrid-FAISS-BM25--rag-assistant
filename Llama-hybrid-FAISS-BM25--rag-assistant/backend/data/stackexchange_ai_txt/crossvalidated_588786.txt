[site]: crossvalidated
[post_id]: 588786
[parent_id]: 
[tags]: 
SHAP value transformation to probabilities

For my research project, I use SHAP values for plotting multiple diagrams to investigate the prediction process for a XGBoost and CatBoost model. I wonder how xgbc_explainer.expected_value (also known as expected_value , base_value and E[f(x)] ) is calculated. In my understanding, E[f(x)] is calculated by averaging the (probability) predictions, but I am not able to do it manually. A similar question has already been asked within the repo and marked by the repo owner as a bug: https://github.com/slundberg/shap/issues/1388 Since the bug is already more than two years old and has not yet been fixed, I decided to resolve the issue for my code: from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier import numpy as np import pandas as pd from catboost import CatBoostClassifier from xgboost import XGBClassifier import shap # print the JS visualization code to the notebook shap.initjs() iris = datasets.load_iris() data = iris.data label = iris.target df = pd.DataFrame(data, columns=["feature0", "feature1", "feature2", "feature3"]) df["labels"] = label df = df[(df["labels"] == 0) | (df["labels"] == 1)] data = df[["feature0", "feature1", "feature2", "feature3"]] label = df["labels"] X_train, X_test, y_train, y_test = train_test_split(data, label, random_state=42) xgbc = XGBClassifier(eval_metric='error', use_label_encoder = False) xgbc.fit(X_train, y_train) y_pred = xgbc.predict(X_test) y_pred_proba = xgbc.predict_proba(X_test) xgbc_explainer = shap.Explainer(xgbc) shap_values_xgbc_train = xgbc_explainer(X_train) shap_values_xgbc_test = xgbc_explainer(X_test) y_train_proba = xgbc.predict_proba(X_train) print(f"My base-line is: {xgbc_explainer.expected_value:.3f} " + f"and my average prediction is: {np.mean(y_train_proba[:,1]):.3f}") # My base-line is: 0.165 and my average prediction is: 0.520 # Since xgbc_explainer.expected_value is not correct, I calculate the value manually: shap_values_xgbc_test.base_values[:] = np.mean(y_train_proba[:,1]) xgbc_explainer.expected_value = np.mean(y_train_proba[:,1]) n = 5 display(shap.force_plot(shap_values_xgbc_test[n].base_values, shap_values_xgbc_test[n].values, X_test.iloc[[n]], link="identity")) display(shap.force_plot(shap_values_xgbc_test[n].base_values, shap_values_xgbc_test[n].values, X_test.iloc[[n]], link="logit")) Here are the force_plots for link="identity" and link="logit": I know how I can calculate f(x) for the "identity" and the "logit" case: def sigmoid(x): return 1 / (1 + np.exp(-x)) n = 5 f_x_identity = shap_values_xgbc_test[n].base_values + np.sum(shap_values_xgbc_test.values, axis=1) print(f_x_identity[n]) # -3.2665708 f_x_logit = sigmoid(f_x_identity) print(f_x_logit[n]) # 0.036735985 What is the correct way of transforming the SHAP values into probabilities? Is a linear transformation valid? shap_values_proba = (shap_values_xgbc_test.values / np.sum(shap_values_xgbc_test.values, axis=1)[:,None] * (f_x_logit - sigmoid(shap_values_xgbc_test.base_values))[:,None])
