[site]: datascience
[post_id]: 77156
[parent_id]: 77149
[tags]: 
This depends on your model type: Classical using ensemble/stacked models: If you are using classical machine learning, you could use your old model built on the previous 1 million records, and create a new model on the most recent 500k records and then combine the predictions in an ensemble or stacked approach. References for Ensemble and Stacking: https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/ Video Reference: https://www.youtube.com/watch?v=Un9zObFjBH0 AI/NN using transfer learning: If your are using a NN (neural network) model, you can use the idea of transfer learning. Save your model built on the first 1 million records, then add it as an initial layer to a new NN for analyzing the new data. You can then save the new NN and use it in the next round. Reference: https://machinelearningmastery.com/transfer-learning-for-deep-learning/ Video Reference: https://www.youtube.com/watch?v=yofjFQddwHE General guidelines: If you need to do this updating process many times, you can create a new model on n number of records, drop the oldest data/model off once your new dataset reaches a minimum, and predict only on the last x number of models. n and x are adjusted based on your data, flexibility and need for real-time predictions. If the data is changing over time, then it would be better to only use the latest data, or weight the older data lower and the newer data higher. Here is a good definition of transfer learning: "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task."
