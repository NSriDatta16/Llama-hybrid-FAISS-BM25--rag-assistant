[site]: crossvalidated
[post_id]: 617637
[parent_id]: 
[tags]: 
Noise cancels but variance sums - contradiction?

I have been told both things with regard to e.g. summing noisy time series, to justify opposing expectations. On the one hand, I have been told to expect that summing multiple noisy inputs should lead to noise reduction for the output (the sum), because the noise components average to zero. I imagine this explains why larger sample sizes increase sensitivity, or why brains can generate exquisitely patterned activity despite individual neurons being very noisy. It all cancels out in aggregate. On the other hand, I have been told to expect that variance sums, so the combination of multiple highly variable inputs should lead to a many times more variable output (N times). This too seems very intuitive, fitting with the idea that "error accumulates" like in dead reckoning. When faced with the sum of noisy inputs, when should we expect that the noise cancels versus accumulates in the output? What am I missing here that makes these two facts appear contradictory?
