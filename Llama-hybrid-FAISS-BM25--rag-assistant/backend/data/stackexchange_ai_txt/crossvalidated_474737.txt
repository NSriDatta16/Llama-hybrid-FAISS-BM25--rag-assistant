[site]: crossvalidated
[post_id]: 474737
[parent_id]: 474555
[tags]: 
There are two things to separate here: The metric The threshold You should choose the metric based on business goals. If you need a good balance between precision and recall, F1 is a good choice; though as I mention in my answer to this similar question I have found models that optimize logloss tend to be more robust when released into the wild. For the threshold, the tricky bit is assuming the best threshold for your chosen metric on your training data will be the best on your test data (or, more importantly, in production). You can plot the performance at every threshold to get a feel for how sensitive it is; the ideal is a broad flat top, as it means the choice of the threshold does not matter too much. For models based on a time series, where e.g. train data is the older 90%, and test data is the newest 10%, I have used the average of the best threshold for each of train and test, as the value when putting the model into production. The thinking being I want to over-weight the more recent data. For the last bit of your question, I'd treat the ensemble as a single model, when considering metric and threshold. (Though it can be tempting to make a small ensemble of models on top, that are each optimized for a different metric, or even threshold.)
