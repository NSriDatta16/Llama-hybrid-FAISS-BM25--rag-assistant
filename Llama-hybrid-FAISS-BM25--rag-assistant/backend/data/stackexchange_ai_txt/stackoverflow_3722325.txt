[site]: stackoverflow
[post_id]: 3722325
[parent_id]: 
[tags]: 
Sum image intensities in GPU

I have an application where I need take the average intensity of an image for around 1 million images. It "feels" like a job for a GPU fragment shader, but fragment shaders are for per-pixel local computations, while image averaging is a global operation. One approach I considered is loading the image into a texture, applying a 2x2 box-blur, load the result back into a N/2 x N/2 texture and repeating until the output is 1x1. However, this would take log n applications of the shader. Is there a way to do it in one pass? Or should I just break down and use CUDA/OpenCL?
