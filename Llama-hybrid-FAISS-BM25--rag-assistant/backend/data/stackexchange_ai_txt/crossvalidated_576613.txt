[site]: crossvalidated
[post_id]: 576613
[parent_id]: 
[tags]: 
loss function for neural network with matrix output

I want to train a neural network with $\mathbb{R}^{d\times d}$ symmetric and positive definit matrix output. Are there rules which loss function ( in dependence of the predicted matrix $A_{\text{pred}}$ and $A_{\text{true}}$ ) to choose? I guess MSE on the flattened matrices equales Frobenius norm of the difference $\|A_{\text{pred}}-A_{\text{true}}\|^2_{\text{Frob}}$ . Also, I think something like $$1-\frac{\|A_{\text{true}}^{-1}A_{\text{pred}}\|^2_{\text{Frob}}}{d}$$ might also be an idea. Or $$\frac{\|A_{\text{pred}}-A_{\text{true}}\|^2_{\text{Frob}}}{\|A\|_{\text{true}}}$$ to have better scaling than just the difference.
