[site]: crossvalidated
[post_id]: 358349
[parent_id]: 358311
[tags]: 
As the author of the linked blog post, I am happy to say that (with the correct choice of hyperparameters and a little luck) Fizz Buzz can be completely learned by a neural network with one hidden layer. I spent some time investigating why it works, and the reason is somewhat interesting. It hinges upon the binary representation of the input and the following observation: if two numbers differ by a multiple of 15, then they belong to the same fizzbuzz "class" (as-is / fizz / buzz / fizzbuzz) It turns out that there are a number of ways in which you can reverse two bits in a 10-digit binary number and get a number that differs by a multiple of 15. For example, if you start with some number x and turn on the 128 bit and turn off the 8 bit, you get x + 120. There are many other such examples. And if you have a linear function of the bits that puts the identical weights on those two bits, it will produce the same output for x and x + 128. As there are many such bit pairs, it turns out that the neural network basically learns a bunch of equivalence classes (for example, one would contain x, x + 120, and a few other numbers), and "memorizes" the correct answer for each equivalence class And so it turns out that when you train the model on the numbers 101 to 1023, you've got enough equivalence classes to predict correctly on 1 to 100. (This is not formal, of course, this is just a high-level summary of what I learned when I investigated the network.) -- As to your question about finding prime numbers, I'd be surprised if an approach like this worked. My sense is that the nice "same modulo 15" structure of this problem is what makes the neural net work, and it's hard to think of anything analogous for e.g. finding prime numbers.
