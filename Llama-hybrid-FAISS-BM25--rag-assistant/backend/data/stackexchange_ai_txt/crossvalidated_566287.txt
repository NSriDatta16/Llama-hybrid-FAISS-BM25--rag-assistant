[site]: crossvalidated
[post_id]: 566287
[parent_id]: 408549
[tags]: 
According to scipy documentation, alpha is the standard deviation of the sampling noise at your data point. This is useful for applications such as ones in experimental physics where you have instrument errors, in which case you would just calculate your expected noise and use it as alpha. If you are tuning hyperparameters for machine learning to optimize for some variable, however, you would have to estimate this. Monte Carlo may be a good way for this estimation. Kappa controls how much you value uncertainty. During the optimization, if you have found a good region but still have some parameter space unexplored, a larger Kappa means that you will want to explore the unknown parameter more and a smaller Kappa means that you want to try out points in this local area that you expect to have good values first. This parameter is slightly more tricky to tune. In theory, it does not depend too much on how volatile your function is and people seem to just use fixed values for this such as e=2.71828... Example of kappa people chose: Bayes Optimization in R
