[site]: datascience
[post_id]: 116993
[parent_id]: 
[tags]: 
I don't understand why the true error of this classifier is $1/2$

Forgive me. I am very new to machine learning. Let $S=((x_1,y_1),...,(x_m,y_m))$ be a finite sequence of pairs in $X \times Y$ . Assume that the probability distribution $D$ is such that instances are distributed uniformly within a square of area $2$ . Centred in this square is another of area $1$ . Assume that we have a correct labelling function $f$ . $f$ determines the label to be $1$ if the instance is within the smaller square and $0$ otherwise. Consider the following predictor: $$h_S(x) = \begin{cases} y_i & \text{if } \exists i \in[m] \text{ s.t } x_i = x \\ 0 & \text{otherwise} \end{cases}$$ Before I move on I need to define two types of loss. First the true error: $$L_{D,f}(h)= P_{x \sim D}[h(x)\neq f(x)].$$ Then there is the training error $$L_S(h) = \frac{|\{i \in [m] :h(x_i) \neq y_i \}|}{m}$$ Clearly $L_S(h_S) = 0$ . The following statement I do not understand (Up to this point I have tried to fit all the relevant data and definitions): The true error of any classifier that predicts the label $1$ only on a finite number of instances is, in this case $1/2$ . Thus $$L_{D,f}(h_S) = 1/2.$$ Please give me an opportunity to fix this post before down voting me please.
