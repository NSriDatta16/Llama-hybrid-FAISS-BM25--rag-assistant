[site]: crossvalidated
[post_id]: 505532
[parent_id]: 469074
[tags]: 
Applied statisticians should know conditional probability inside and out; this is the source of a great deal of misunderstandings about p-values and type I assertion probability $\alpha$ as well as holding back more usage of the Bayesian paradigm experimental design, sources of bias and variability measurement properties and how to optimize them how to translate subject matter knowledge into model specification which modeling assumptions matter the most, and which type of model flexibility should be prioritized (e.g., in many situations nonlinearity is more damaging than non-additivity) how to specify and interpret details of regression models (at least up to specification of nonlinear interaction terms) model uncertainty and how it damages inference, and understanding that trying multiple models can destroy inference understand that getting a Bayesian posterior probability of normality is better than trying to use the data to decide whether to assume normality or not how to specify flexible models so that you don't need to worry so much about model uncertainty instead of learning all the standard statistical tests and ANOVA, learn how to accomplish them through modeling (this includes standard nonparametric tests such as Wilcoxon and Kruskal-Wallis)
