[site]: crossvalidated
[post_id]: 521764
[parent_id]: 521515
[tags]: 
Solution. Using the results in here linked by StubbornAtom . The loss function you have specified can be interpreted as a generalisation of squared error loss, known as weighted squared error loss . That is, $$L(\tau, \hat{\tau}) = \frac{1}{\tau} \cdot (\hat{\tau} - \tau)^2,$$ where $w(\theta) = 1 / \tau(\theta)$ is a weight, and $L'(\tau, \hat{\tau}) = (\hat{\tau} - \tau)^2$ is squared error loss. In this case, the Bayes estimator $\hat{\tau}_B$ minimises posterior risk $r'(\hat{\tau} | x^n)$ under squared error loss $L'(\tau, \hat{\tau})$ . Where the weight $1 / \tau({\theta})$ has been absorbed into the original posterior $p(\theta | x^n)$ with renormalisation to form a new posterior $p'(\theta | x^n)$ : $$\hat{\tau}_B = \min_{\hat{\tau}} r'(\hat{\tau} | x^n) = \min_{\hat{\tau}} \int (\hat{\tau} - \tau)^2 \cdot p'(\theta | x^n) \space d\theta.$$ Using the result that the Bayes estimator under squared error loss is mean of the posterior $r'$ , and the lazy statistician rule, we have that $$\hat{\tau}_B = \mathbb{E}_{p'(\theta | x^n)}[\tau(\theta) | X^n = x^n].$$ Rewriting the right hand side in terms of our original posterior $p$ , the solution is: \begin{align*}\hat{\tau}_B &= \int \tau(\theta) \left( \frac{[1 / \tau(\theta)] \cdot p(\theta | x^n)}{\int [1 / \tau(\theta')] \cdot p(\theta' | x^n) \space d \theta'} \right) \space d \theta \\ &= \frac{\int p(\theta | x^n) \space d \theta}{\int [(1 / \tau(\theta')] \cdot p(\theta' | x^n) \space d \theta'} \\ &= \frac{1}{\int [(1 / \tau(\theta')] \cdot p(\theta' | x^n) \space d \theta'}. \end{align*} Where in the 2nd line the normalisation constant has been factored out, and in the 3rd line it has been assumed that $p(\theta | x^n)$ has been appropriately normalised. The above result is an instance of the more general solution (in this context) that $$\hat{\tau}_B = \frac{\mathbb{E}_{p(\theta | x^n)}[w(\theta) \tau(\theta) | X^n = x^n]}{\mathbb{E}_{p(\theta | x^n)}[w(\theta) | X^n = x^n]}.$$ A statement of this can be found in Corollary 2.5.2. of The Bayesian Choice by Robert (2003). Whilst you have in a sense raised some appropriate concerns about the validity of interchanging the order of differentiation and integration, also known in intermediate statistics reference textbooks as "differentiating under an integral sign", there are broader issues at stake when you "compute the derivative of the posterior risk with respect to an estimator". On differentiating under the integral sign. The technicality of differentiation under an integral sign amounts to inquiring, "under what conditions it is valid to interchange a limit and an integral?" This amounts to an appeal to Lebesgue's dominated convergence theorem . Without knowledge of formal tools in analysis and measure theory, this is difficult to treat, so the best one can hope to do in this situation is to supply some more easily verifiable conditions/corollaries of the theorem that should in principle allow one to assess if differentiating under the integral sign is appropriate. The relevant reference you require is Section 2.4: Differentiating under an integral sign in Statistical Inference by Casella and Berger (2002) : 1. In the case that range of definite integration $[a, b]$ is such that $a$ and $b$ are constants not depending on $\theta$ , and $f(x, \theta)$ is differentiable with respect to $\theta$ , then a special case of Leibniz rule means that $$\frac{d}{d \theta} \int^b_a f(x, \theta) \space dx = \int^b_a \frac{\partial}{\partial \theta} f(x, \theta) \space dx.$$ 2. In the case that the range of integration $[a(\theta), b(\theta)]$ depends on $\theta$ and all of $f(x, \theta)$ , $a(\theta)$ and $b(\theta)$ are differentiable with respect to $\theta$ , then Leibniz's integral rule states that \begin{align*} \frac{d}{d \theta} \int^{b(\theta)}_{a(\theta)} f(x, \theta) dx = & \space f(b(\theta), \theta) \frac{d}{d \theta} b(\theta) - f(a(\theta), \theta) \frac{d}{d \theta} a(\theta) \\ & \space + \int^{b(\theta)}_{a(\theta)} \frac{\partial}{\partial \theta} f(x, \theta) \space dx. \end{align*} 3. In the case that the range of definite integration is not finite. Supposing that $f(x, \theta)$ is differentiable and there exists a function $g(x, \theta)$ such that $$\left | \frac{\partial}{\partial \theta} f(x, \theta) \left. \right |_{\theta = \theta'} \right | \leq g(x, \theta)$$ for all $\theta'$ such that $\lvert \theta' - \theta \rvert \leq \delta_0$ . If additionally $$\int^{\infty}_{- \infty} g(x, \theta) \space dx then $$\frac{d}{d \theta} \int^{\infty}_{- \infty} f(x, \theta) \space dx = \int^{\infty}_{- \infty} \frac{\partial}{\partial \theta} f(x, \theta) dx.$$ That is, if $f$ is sufficiently 'smooth', in the sense that you can bound the variability in its partial derivative using a function $g(x, \theta)$ which has a finite integral, then the order of differentiation and integration may be interchanged. On why these results may not be applicable to your situation in a straightforward way. The arbitrary estimator $\hat{\tau} = \hat{\tau}(X_1, \dots, X_n)$ is a function of data. The posterior risk $r(\hat{\tau} | x^n)$ which can vary in choice of function $\hat{\tau}$ , is a functional . In order to compute derivatives with respect to functions, you need to define what differentiation means in this context, and to establish its properties. Given this additional complication, it is also somewhat unclear how differentiating under an integral sign would work in the context of functional derivatives. Perhaps that is why an alternative approach was suggested.
