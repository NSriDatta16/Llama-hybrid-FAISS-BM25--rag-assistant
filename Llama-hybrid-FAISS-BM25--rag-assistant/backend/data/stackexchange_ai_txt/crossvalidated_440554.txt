[site]: crossvalidated
[post_id]: 440554
[parent_id]: 440552
[tags]: 
The Cybenko Universal Approximation Theorem outlines the conditions under which a single-layer neural network with some number of sigmoid hidden activations can be used to approximate a function over a specific domain. Not all functions can be approximated in this manner; see the article for all caveats. The implication is that we can use neural networks for regression because Cybenko shows that it is at least theoretically possible to approximate some functions arbitrarily well under specific circumstances. For practical purposes, however, there are caveats. The optimization task of training a neural network is notoriously challenging. Cybenko merely shows that a network exists to approximate a class of functions; Cybenko's UAT does not comment on how easy or hard it is to train a given network to approximate the desired function to the desired level of precision in common real-world conditions, such as available data are finite, the data are subject to noise, the underlying true function which we wish to approximate is unknown, etc. Not all functions or settings match the theoretical conditions which Cybenko requires in his theorem. Nevertheless, despite these challenges, neural networks are used for regression tasks all the time , even with sigmoid hidden activations. However, it is more common these days to use ReLU functions and variants because they tend to do better than sigmoid functions. (There are also several universal approximation theorems pertaining to different settings for ReLU networks.) G. Cybenko, "Approximation by superpositions of a sigmoidal function." Mathematics of Control, Signals and Systems volume 2, pp. 303â€“314(1989)
