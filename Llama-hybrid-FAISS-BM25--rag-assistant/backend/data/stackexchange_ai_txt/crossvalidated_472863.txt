[site]: crossvalidated
[post_id]: 472863
[parent_id]: 472845
[tags]: 
Testing for independence of continuous variables is a non-trivial problem that is still a subject of active resarch. An old test was developed by Hoeffding in 1948: W. Hoeffding: "A non-parametric test of independence." The annals of mathematical statistics 19,4, pp. 546-557, 1948 Two random variables X and Y are called independent, if their joint distribution function factorizes: $$P(X It is thus natural to consider the difference between these two sides, i.e. $\Big(F_{12}(x,y) - F_1(x) F_2(y)\Big)^2$ . Hoeffding found an estimator D for the functional $$\Delta = \int dx\int dy\, \Big(F_{12}(x,y) - F_1(x) F_2(y)\Big)^2 f_1(x)\,f_2(y)$$ where $f_1$ and $f_2$ are the probability densities of X and Y , respectively. He found the approximate distribution of the estimator D under the null hypothesis that X and Y are independent. The test is thus used like any other test: if the computed p-value is less than a threshold (say 0.05), the observation is "unlikely" under the null hypothesis, and your variables are presumably not independent. Unfortunately a high value for p is not necessarily a sign for independence, but you can at least feel somewhat safer ;-) Example: > x y hoeffd(x, y) ... P x y x 0.9933 hoeffd(x, x+y) ... P x y x 0 Edit: Oh sorry, just saw that your data do not stem from two constant distributions, but are time series. In that case, other approaches are more appropriate, like computing the intra-class correlation ICC3 as a measure how similar the curve shapes are.
