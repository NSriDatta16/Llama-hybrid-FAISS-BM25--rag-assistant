[site]: crossvalidated
[post_id]: 626281
[parent_id]: 
[tags]: 
Negative ICC values for inter-rater reliability

I have conducted research which assessed the technical skills of surgeons. A surgeon will perform a simulated operation, and be assessed by two different assessors whilst completing the procedure. The assessment tool has several domains, which are averaged to give a final score, giving the surgeon two final ratings one from each assessor. We have multiple different procedures which are assessed, each with its own procedure specific assessment tool. The tool is interval scale data (rating 1-4) or discrete numerical (Rated 0-100 Visual Analogue Scale) depending on the tool. The 2 assessors are randomly selected from a pool of 10-20 assessors. For example if we assess 20 surgeons in one procedure, 5 would be assessed by assessor A + B, 5 would be assessed by different assessors C+D, 5 would be assessed by different assessors E+F, and 5 would be assessed by assessors G+H. This is simply due to time constraints and availability of assessors, we take whoever we can get on the day. We have data collected across 10 years, so each procedure generally has >7 sets of assessments. I am currently undergoing reliability statistics, on inter-rater reliability. I am undertaking the reliability assessment for each procedure per year. So for a single procedure, I am getting >7 ICC's as one for each year. I selected ICC(1,2) for our data. One-way random as we do not have constant raters for our trainees. k=2 as we want to look at average of 2 raters versus a single rater. Some of the ICC values are acceptable >0.5. However I'm getting multiple which are negative, and high negatives e.g. -1.2 or -2.7. I have triple checked the data and no mistakes in data. Can send examples of analysis and samples of data if needed. Have I chosen the right statistical test and is it normal to get negative values this high?
