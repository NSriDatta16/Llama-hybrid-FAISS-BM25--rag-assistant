[site]: crossvalidated
[post_id]: 318917
[parent_id]: 
[tags]: 
Maximizing (and derivating) log-likelihood of penalized logistic regression

I'm trying to solve Exercise 18.3 of "Elements of Statistical learning" by Hastie et al. and I'd be really grateful for any hints. Show that the fitted coefficients for the regularized multiclass logistic Regression Problem $$\max_{\{\beta_{0k},\beta_{k}\}_{1}^{K}}\left[\sum\limits_{i=1}^{N}logPr(g_{i}| x_i)-\frac{\lambda}{2}\sum\limits_{k=1}^{K}\|\beta_{k}\|_{2}^{2}\right]$$ satisfy $$\sum\limits_{k=1}^K\hat{\beta}_{kj}=0, j = 1,...,p.$$ $p$ is the number of Features, $K$ is the number of classes. EDIT: I actually think that $\max\limits_{\{\beta_{0k},\beta_{k}\}_{1}^{K}}\left[\sum\limits_{i=1}^{N}logPr(g_{i}| x_i)-\frac{\lambda}{2}\sum\limits_{k=1}^{K}\|\beta_{k}\|_{2}^{2}\right]$ is a Lagrange optimization Problem, so derivating with respect to $x$ and $\lambda$ and then Setting to Zero should do what I Need. But turns out that I'm even getting confused by derivating! $$Pr(g_{i}| x_i) = \dfrac{exp(\beta_{g_i0}+x_i^T\beta_{g_i})}{\sum\limits_{l=1}^Kexp(\beta_{l0}+x_i^T\beta_l)}$$ and I'm not quite sure if I should compute $$\dfrac{\partial f}{\partial x_i}$$ with $i=1,...,N$ or $$\dfrac{\partial f}{\partial x_{ij}}$$ with $i=1,...,N,j=1,...,p$, $f$ being the function inside the square brackets. Well I tried both and I guess it went wrong both times as I couldn't achieve what I'm trying to proove. I didn't forget $\dfrac{\partial f}{\partial \lambda}.$
