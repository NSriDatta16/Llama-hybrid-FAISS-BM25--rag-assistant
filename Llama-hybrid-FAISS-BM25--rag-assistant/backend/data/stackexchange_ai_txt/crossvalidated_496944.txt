[site]: crossvalidated
[post_id]: 496944
[parent_id]: 375348
[tags]: 
This is an interesting question. I was reading Doing Bayesian Data Analysis by John Kruschke and ran into this post. I might be wrong, but here are my thoughts. I am assuming you don't care about the exact order of your 0's and 1's in your data, rather only the counts. First, specify the data. We have 21 1's and 9 0's, which totals at N = 30. # data, z = 21, N = 30 data = c(1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1) Next, lets define the likelihood function. We calculate the number of 1's (z) and the total length of the data (N). Then, the uncommented lik specifies the probability for a set of outcomes y_i, where y_i specifies a single outcome in your data. The other lik (commented out) specifies a probability from a binomial distribution, where the data is z (21 1's), number of trials is N (30) and the probability is theta. This is now different from the previous lik, because it takes into account combinatorics, namely how many times you can get 21 1's in 30 trials with a given theta. These lik's differ only in the combinatoric term N over z which is a constant and thus doesn't affect the shape of the curve. likelihood You then specify everything basically the same as in the previous post. theta = seq(0, 1, 0.01) lls The first likelihood gives you And the second likelihood gives you As you can see, the shapes are identical and thus the inference is identical also. The only difference is the y-axis scale. The maximum is found at theta = 0.7, which is the proportion of 1's in your data.
