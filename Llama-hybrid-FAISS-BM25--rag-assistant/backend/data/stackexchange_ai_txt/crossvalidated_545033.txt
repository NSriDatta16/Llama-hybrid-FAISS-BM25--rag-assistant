[site]: crossvalidated
[post_id]: 545033
[parent_id]: 
[tags]: 
Random Forest Parameter Settings for Big Data

I have a big data set (with more than 9,000,000 rows) with 7 features and 1 label. The label is ordinal data. I would like to run a random forest regression. I'm fairly new to random forests so I have a few questions on parameter settings in my case: How many trees should I run based on my sample size? Shall I set a limit on the depth of my tree? How could I tell if a feature is important or not? I know there is a feature ranking. In my test run, none of the feature importance values is 0, but I wonder if there could be an importance value threshold to filter out the feature? I saw people visualize trees as part of their analysis output. However, if I end up training many trees (e.g., 1000 trees), shall I just randomly visualize 1 tree? I don't really get the point of visualizing one of the many trees as I doubt if that could really help people understand my result. Could anyone help rationalize the importance of visualizing trees in the random forests? I also wonder if XGBoost will be a better option than random forest in my case. Because my label is ordinal data and I will have to treat it as a continuous variable and use regression in random forest. It seems XGBoost can specifically analyze ranking data and it's faster than random forest. But I could be wrong. Any input would be appreciated. Other relevant info: I'm using Python sklearn for modeling on Google Colab (with 13 GB RAM limit). Please feel free to answer all or one of the questions. Thank you so much!
