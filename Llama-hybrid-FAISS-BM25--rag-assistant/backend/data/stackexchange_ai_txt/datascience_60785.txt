[site]: datascience
[post_id]: 60785
[parent_id]: 
[tags]: 
How are weight updates handled in Batch Gradient Descent vs SGD?

My current understanding is that in SGD, after each data sample, the loss is used to update each weight. Ex: With 1000 samples and a network with 10 weights, there will be 10,000 individual weight updates per epoch. In Gradient Descent and Batch Gradient Descent, how are these updates deferred over multiple data samples? What is being stored at each sample, that can be applied at the end of the batch? Is the loss at each sample averaged over the batch?
