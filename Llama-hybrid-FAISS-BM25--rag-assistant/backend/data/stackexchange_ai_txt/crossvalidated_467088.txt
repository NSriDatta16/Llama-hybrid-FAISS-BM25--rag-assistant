[site]: crossvalidated
[post_id]: 467088
[parent_id]: 453956
[tags]: 
I'm not a Bayesian expert and I'm happy to stand corrected, but to me the most straightforward & principled way to test this would be to define region of practical equivalence (ROPE) around c and then estimate how much posterior density falls inside this region. For example, let's say that, based on theory and domain knowledge, you know that for all practical purposes, if c deviates from exactly 1 by less than 0.01 then it might as well be 1 (outside of simulation, c is never going to be exactly 1 anyway and so you will always reject point null hypothesis with enough data). Anyway, using the deviation of 0.01 you define a ROPE of 0.99 - 1.01. After that, you run your model, and estimate how much density falls inside the ROPE region. If the proportion of density $k$ that falls inside the rope is smaller than whatever you decide your alpha is, then you should feel comfortable rejecting your model, with $k$ confidence. See this vignette: https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html PS: You'll probably want a large tail effective sample (ESS) size for this kind of testing. This is because Monte Carlo samplers tend to explore the typical set & give increasingly less precise estimates towards the tails of the distribution, which is where your ROPE might be. So you'll want to run your sampler with a lot of iterations.
