[site]: datascience
[post_id]: 49808
[parent_id]: 49720
[tags]: 
Repeating samples will probably only slow down the learning process. As long as the mini batches are small this will probably not have a big effect. However, if batches and the number of repetitions become large gradient based learning can run into difficulties. If you don't shuffle the batches, the repeated samples will prevent the model from taking in new information and may nudge it to "overfit" on the current samples. Since at every batch you only get one new sample (while all the others are repeated), the learning will either slow down or become stale and stuck at an unfavorable location. Since your approach only has a two-dimensional input, this is probably not a major concern since the loss function will probably not be as complex as in other neural network applications. Overall, in your case, you probably will not see a big impact on performance except for a longer learning time. You do not need to be concerned with combining predictions for repeated samples, since you can make predictions using the final parameters after the learning is completed.
