[site]: datascience
[post_id]: 94041
[parent_id]: 94036
[tags]: 
Setting dropout = 0.0 solves this issue: torch_model.encoder.layer[0].attention.self.dropout.p = 0.0 bert_self_attn.dropout.p = 0.0 I thought that dropout was only used during the training process. Is the dropout in BERT different from conventional dropout layers?
