[site]: crossvalidated
[post_id]: 543374
[parent_id]: 543361
[tags]: 
Looking at it the other way, if there were no true data generating process, how did the data get generated? The inability of standard estimating techniques to accurately approximate the true data-generating process doesn't mean that the data generating process doesn't exist, it just means that we don't have enough data to determine the parameters of the model (or more generally the correct form of the model). However, when we make a model, our goal is not to exactly capture the true data generating process, only to make a simplified representation or abstraction of the important features of the true data generating process (TDGP) that we can use to understand the TDGP or to make predictions/forecast of how it will behave in some situation we have not directly observed. Our brains are very limited, we can't understand the detail of the TDGP, so we need abstractions and simplified models to maximise what we are able to understand. Rather than say there is not TDGP, I would say there is no such thing as "randomness" (except perhaps at a quantum level, but even that might not be random either, although the Bell experiment suggests it probably is). We use the concept of "random" to explain the results of deterministic systems that we can't predict because of a lack of information. So the purpose of a statistical model is express our limited state of knowledge regarding the deterministic system. For example, flipping a coin isn't random, whether it comes down heads or tails is just physics, depending on the properties of the coin and the forces applied to it. It only seems random because we don't have full knowledge of those properties or forces. At the end of the day, the more data we have, in principle the more information we can extract from it (with diminishing returns), and the better our state of knowledge about the TDGP. The reason averaging helps is that the error of the model is composed of bias and variance, c.f. @Tim's answer (+1). If we don't have much data, the variance component will be high, but that variance will not be coherent for models trained on different samples, and so will partially cancel when model predictions are averaged. This is not telling you anything about the TDGP, it is telling you about the estimation of model parameters (and that you should get more data if you can).
