[site]: crossvalidated
[post_id]: 304150
[parent_id]: 303857
[tags]: 
The spikes are an unavoidable consequence of Mini-Batch Gradient Descent in Adam ( batch_size=32 ). Some mini-batches have 'by chance' unlucky data for the optimization, inducing those spikes you see in your cost function using Adam. If you try stochastic gradient descent (same as using batch_size=1 ) you will see that there are even more spikes in the cost function. The same doesnÂ´t happen in (Full) Batch GD because it uses all training data (i.e the batch size is equal to the cardinality of your training set) each optimization epoch. As in your first graphic the cost is monotonically decreasing smoothly it seems the title ( i) With SGD ) is wrong and you are using (Full) Batch Gradient Descent instead of SGD. On his great Deep Learning course at Coursera , Andrew Ng explains in great details this using the image below:
