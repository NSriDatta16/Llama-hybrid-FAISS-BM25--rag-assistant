[site]: crossvalidated
[post_id]: 545565
[parent_id]: 
[tags]: 
Existing limitations of solutions to the Vanishing Gradient Problem

In a feedforward neural network, the main causes for the VGP are saturation of activation functions and poor initialisation of weights. From what I have read, using non-saturating activation functions, batch normalisation and Xavier initialisation are effective solutions. I would like to know, do these methods solve the VGP? Are there any further difficulties for which the solutions are insufficient in mitigating the VGP? What I could think of is the very deep network structure, where intialisation is perhaps even more demanding? I am writing up a report concluding section on the VGP in training neural networks, however I am relatively a beginner in this topic. Thanks in advance!
