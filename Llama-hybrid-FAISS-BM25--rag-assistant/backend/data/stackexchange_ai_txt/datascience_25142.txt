[site]: datascience
[post_id]: 25142
[parent_id]: 25141
[tags]: 
You have (at least) two options. You can either: Use a bottleneck architecture where you use pooling layers to reduce the dimensions of your data and then upcast it again using deconvolutional/upsampling layers to go back to the original dimensions. The advantage of this approach is that you use a bigger receptive field for the final output, which means it can look at a bigger part of your input sequence. You could combine this with a highway connection like structure. A very similar concept is used in 2d with segmentation, here the spatial dimensions are preserved at the output. One of the best models is called a U-Net. Use no pooling and only padding-same convolutional layers with non-linearities. You can increase the depth but you will keep the (1000, 2) dimensions everywhere. The advantage is that it is very easy to implement, and if applicable to your problem only uses relatively local information.
