[site]: crossvalidated
[post_id]: 52798
[parent_id]: 52773
[tags]: 
Consider a simple case, lifted from a terrific and undervalued article "A Note on the Use of Principal Components in Regression " . Suppose you only have two (scaled and de-meaned) features, denote them $x_1$ and $x_2$ with positive correlation equal to 0.5, aligned in $X$, and a third response variable $Y$ you wish to classify. Suppose that the classification of $Y$ is fully determined by the sign of $x_1 - x_2$. Performing PCA on $X$ results in the new (ordered by variance) features $[x_1 + x_2, x_1 - x_2]$, since $\operatorname{Var}( x_1 + x_2 ) = 1 + 1 + 2\rho > \operatorname{Var}(x_1 - x_2 ) = 2 - 2\rho$. Therefore, if you reduce your dimension to 1 i.e. the first principal component, you are throwing away the exact solution to your classification! The problem occurs because PCA is agnostic to $Y$. Unfortunately, one cannot include $Y$ in the PCA either as this will result in data leakage. Data leakage is when your matrix $X$ is constructed using the target predictors in question, hence any predictions out-of-sample will be impossible. For example: in financial time series, trying to predict the European end-of-day close, which occurs at 11:00am EST, using American end-of-day closes, at 4:00pm EST, is data leakage since the American closes, which occur hours later, have incorporated the prices of European closes.
