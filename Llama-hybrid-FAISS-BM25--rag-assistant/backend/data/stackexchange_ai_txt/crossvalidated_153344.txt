[site]: crossvalidated
[post_id]: 153344
[parent_id]: 152823
[tags]: 
You can oversample your minority class examples by simply duplicating them, or you can use the SMOTE algorithm ( DMwR package in R, function SMOTE), that generates synthetic minority class examples while downsampling the majority category at the same time. Since you have a pretty high number of cases, downsampling should not lead to too much concept loss, but of course, you'll still be losing a bit of information, which is not ideal. Note that as already mentioned by Analyst, 1300 minority cases is relative rarity but not absolute rarity. That is, if the minority class is represented by strong concepts, your classifier should be able to pick that up (see this paper for a good discussion of absolute and relative rarity). So maybe your predictors are not that good at discriminating between classes in the first place, or maybe you have some concept overlap, which makes learning difficult. Also, which learning algorithm are you using? For instance, Stochastic Gradient Tree Boosting is a little bit less sensitive to relative class imbalance than Random Forests (since the focus is gradually put on misclassified cases). With Random Forests, two strategies have been developed to cope with class imbalance, they involve resampling and weighting. Some methods similar in spirit have also been introduced for Boosting ( e.g. ). EDIT: added references (in case links die in the future): Chawla, Nitesh V., et al. "SMOTE: synthetic minority over-sampling technique." Journal of artificial intelligence research (2002): 321-357. Weiss, G. M. (2004). Mining with rarity: a unifying framework. ACM SIGKDD Explorations Newsletter, 6(1), 7-19. Chen, Chao, Andy Liaw, and Leo Breiman. "Using random forest to learn imbalanced data." University of California, Berkeley (2004). Sun, Yanmin, et al. "Cost-sensitive boosting for classification of imbalanced data." Pattern Recognition 40.12 (2007): 3358-3378.
