[site]: datascience
[post_id]: 11081
[parent_id]: 
[tags]: 
Importance of feature selection for boosting methods

While it is obviously clear that features can be ranked on basis of importance and many machine learning books give examples of random forests on how to do so, its not very clear on which occasions one should do so. In particular, for boosting methods, is there any reason why one should do feature selection. Wouldn't the boosting methods themselves eliminate the low importance feature. Isn't is just always better to add more features (if one didn't have the practical problem of time limitations).
