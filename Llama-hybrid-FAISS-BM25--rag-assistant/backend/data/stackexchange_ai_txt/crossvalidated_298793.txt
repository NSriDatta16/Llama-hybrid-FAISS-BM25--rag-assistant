[site]: crossvalidated
[post_id]: 298793
[parent_id]: 126238
[tags]: 
Just complementing the other answers: Vanishing Gradients The other answers are right to point out that the bigger the input (in absolute value) the smaller the gradient of the sigmoid function. But, probably an even more important effect is that the derivative of the sigmoid function is ALWAYS smaller than one . In fact it is at most 0.25! The down side of this is that if you have many layers, you will multiply these gradients, and the product of many smaller than 1 values goes to zero very quickly. Since the state of the art of for Deep Learning has shown that more layers helps a lot, then this disadvantage of the Sigmoid function is a game killer. You just can't do Deep Learning with Sigmoid. On the other hand the gradient of the ReLu function is either $0$ for $a 0$. That means that you can put as many layers as you like, because multiplying the gradients will neither vanish nor explode.
