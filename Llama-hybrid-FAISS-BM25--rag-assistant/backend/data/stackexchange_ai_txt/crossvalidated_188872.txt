[site]: crossvalidated
[post_id]: 188872
[parent_id]: 188867
[tags]: 
Does this really happen or is it a theoretical issue? It happens, see any popular deeplearning model for computer vision. Say, alexnet has a dense connection between 2048 and 2048 units, that's 4 million coefficients. What's the point of analyzing a million IVs? Does it really give us that much increase in value of information gained as opposed to ignoring them? If you're analyzing highly categorical data (say, internet advertisement data), your model has to keep some meaningful 'descriptions' for each category (e.g. city, page id, sitename, advertisement id, user id, etc.), the actual size of 'description' depends on the selected ML model. Even simple logistic regression will have dozens of thousands of parameters to be fitted (one per category). More advanced models like factorization machines are going to have times more. Or is it because, initially we have no idea what is useful, so we just run the damn regression to see what is useful and go from there and possibly prune the set of IVs? Actually, most of fitted parameters in these models can be dropped, but you can not know that beforehand, so you leave the problem of defining which parameters are important to machine learning, and impose some regularizations to put 'soft limit' to the effective number of parameters to stay. ... and I think you'll find such examples later in your ML course.
