[site]: crossvalidated
[post_id]: 517254
[parent_id]: 
[tags]: 
Why GAN use adversarial MinMax formulation rather than Min formulation?

For generative adversarial neural network, originally Goodfellow used a MinMax formulation as $\text{Min}_D\text{Max}_G \mathbb{E}_{real}logD(x) dx+ \mathbb{E}_{fake}(1-D(G(z)))dz$ . As long as the generator $G$ is fixed, the optimal discriminator $D$ is explicit. My question is, as we have a clear understanding of $D$ , why not just minimize the Jenson-Shannon divergence as a Min formulation, which is equivalent to the MinMax formulation? BTW, other researchers raised a similar framework called f-GAN , which replaces the Jenson-Shannon divergence by other $f$ -divergence. They also adopted the MinMax formulation, rather than minimizing the divergence directly. Why MinMax formulation is more popular here than Min formulation in these works?
