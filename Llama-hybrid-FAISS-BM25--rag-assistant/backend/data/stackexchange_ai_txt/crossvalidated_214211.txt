[site]: crossvalidated
[post_id]: 214211
[parent_id]: 214196
[tags]: 
First, the way you have described $m$ and $n$ in your question is opposite of convention: we usually talk of $n$ observations (rows) with $m$ features (columns). To attempt to avoid further confusion, let's follow the convention of Introduction to Statistical Learning and call the samples/observations in the rows $n$ and the features in the columns $p$. Second, when there are as many features as there are observations ($n = p$), the matrix $X$ is positive semidefinite, and when there are fewer observations than features ($p > n$), the matrix is called singular or degenerate and cannot be inverted. Among other problems with this type of situation are the facts that (1) the least squares regression coefficient(s) $\beta = (X^T X)^{-1} X^T Y$ does not have a unique solution because $(X^T X)$ is not invertible, and (2) a solution can be found with zero variance, i.e. a perfect fit. The short answer to your question is that more observations than features is always desirable, at least in an idealized world. In the cases where there are more features than observations, particular care must be taken in building and interpreting the machine learning model employed. I recommend reading section 4 of chapter 6 of the above referenced book as a good introduction to the topic in more space than is appropriate for this context.
