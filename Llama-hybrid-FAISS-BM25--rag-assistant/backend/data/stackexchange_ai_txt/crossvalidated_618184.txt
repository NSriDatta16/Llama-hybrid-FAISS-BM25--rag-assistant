[site]: crossvalidated
[post_id]: 618184
[parent_id]: 618179
[tags]: 
In the train-validate-test paradigm, you start by setting the test data to one side. For a time series history that in a sense only happens once, this is usually the most recent data. The purpose of the (cross-)validation step is model selection and hyperparameter tuning, such as which variables to include with what transformations or what lags to consider or what kind of averaging or smoothing or shrinkage to use. In effect you choose your model by taking a selection of possible models, train each of them on the training set(s) without the corresponding validation set and choose the structure of model that in a sense performs best on the validation set(s). You end up with a single chosen model with determined hyperparameters; this does not stop you training it with new data. You want to assess how well your final model will perform on the as-yet unseen data in your test set, so you train your model on all the training and validation data to give predictions for the test set period, and then compare them to the real test data to estimate how well your model may have predicted the test data. By now it is now too late to change your model apart from training. You can then retrain your model (with the same structure and hyperparameters) on the combined training and validation and testing data for the actual future and hope it performs as well. Once the future has happened, this will provide new test data you will be able to use to compare to the predictions you are making now. So in this sense you are able to train using the most recent time-series data to make forecasts, but are unable to use it in this paradigm for model selection or hyperparameter tuning, since you needed to make those choices at an earlier stage. This is a price you pay to reduce overfitting.
