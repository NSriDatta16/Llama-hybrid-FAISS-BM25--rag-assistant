[site]: datascience
[post_id]: 121435
[parent_id]: 97341
[tags]: 
For an imbalanced dataset, you should report the base rate (5% anomaly, 95% regular observation), alongside your selected score; without that, the interpretation can lead to base-rate fallacy. The common metrics derived from the confusion matrix will do. Be careful with ROC AUC, it tends to be over-optimistic compared to accuracy and is generally less understandable by practitioners. For highly imbalanced datasets, ROC AUC will be very close to 1 most of the time. I personally like paired metrics, such as precision-recall, or specificity-sensitivity, which report separately type I and type II errors. There are also metrics, which weigh these two errors unequally. F1-score focuses on the lower, e.g., precision 0.1, and recall 0.9 yields F1 0.18. Check out the USENIX paper Do's and Don'ts of Machine Learning in Computer Security , which mentions metrics used for imbalanced datasets.
