[site]: datascience
[post_id]: 5607
[parent_id]: 5585
[tags]: 
I would go for dimensionality reduction. You can start with SVD (should be available in Weka). If SVD is too slow / too memory consuming, then there are still some options: CUR-decomposition: a variant of singular-value decomposition that keeps the matrices of the decomposition sparse if the original matrix is sparse (see: this chapter of Mining Massive Datasets book) Random projections: projecting the data onto a random lower-dimensional subspace (see: the Random projection in dimensionality reduction: Applications to image and text data paper) Coresets: Given a matrix A, a coreset C is defined as a weighted subset of rows of A such that the sum of squared distances from any given k-dimensional subspace to the rows of A is approximately the same as the sum of squared weighted distances to the rows in C see the Dimensionality Reduction of Massive Sparse Datasets Using Coresets paper) That's the tip of an iceberg. More approaches are there in the wild. The problem is that I doubt any of these solutions come with Weka (please, correct me if I am wrong on this). I would search for a usable Java implementation of any of these algorithms and try to port it to work with Weka's arff files.
