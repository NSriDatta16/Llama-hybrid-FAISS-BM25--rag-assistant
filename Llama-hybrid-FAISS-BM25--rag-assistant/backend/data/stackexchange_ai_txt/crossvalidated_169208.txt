[site]: crossvalidated
[post_id]: 169208
[parent_id]: 169168
[tags]: 
Use a logistic regression with a sparse (L1) regularizer . Logistic regression tries to predict the probability of winning given the items purchased: $$p(\mbox{win}=1|\mathbf x) = \sigma(\mathbf w^\top \mathbf x + b)$$ where $\sigma$ denotes the logistic function . Sometimes this algorithm is also called generalized linear model with a logit link function and a Bernoulli likelihood (you have to add the regularizer though). Represent your items as long vectors $\mathbf x$ of 0's and 1's. E.g. if you have 5 items in total, and the hero purchased item 2 and 3, the vector would be x = [0,1,1,0,0] This also helps you to deal with missing values, since this representation does not care how many items a hero purchased. The input dimension will be huge but that should not be a problem because of (a) the regularizer and (b) you have enough training data. Make sure to use proper model selection for the strength of the regularizer, using cross-validation, for instance. The sparse regularizer will try to push all those entries in your feature vector $\mathbf w$ to zero that are not relevant for predicting the success of the hero. Therefore, your relevant items will be the ones that have non-zero entries in $\mathbf w$. As a rule of thumb: The larger the absolute value of $w_i$ corresponding to an item $i$ is, the more important it is. I don't know what language you are using. If you are a Pythonista, sklearn has a logistic regression with regularization. However, I am sure that R or Matlab have similar implementations. Good luck.
