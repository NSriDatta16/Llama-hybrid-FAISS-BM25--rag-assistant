[site]: datascience
[post_id]: 89877
[parent_id]: 68220
[tags]: 
As Jind≈ôich has said, Q, K, V come from previous computations, they are not trained directly with backpropagation. However, the weights $W_i^Q, W_i^K, W_i^V$ are trained directly with backpropagation. Expanding on this, in the "Attention is all you need paper", in the self attention used by the encoder and decoder, Q, K, V are the same matrix . If we just look at the self attention in the encoder, in the first layer Q, K, V are the representation of the input sentence, after the embedding and positional encoding steps. The number of rows in these matrices is equal to the number of tokens in the input sequence, and the number of columns is based on the architecture (it's 64 in the paper). The outputs from the first encoder layer are then used as Q, K, V for the next layer (again these are all the same matrix). The decoders attention self attention layer is similar, however the decoder also contains attention layers for attending to the encoder. For this attention, the Q matrix comes the decoders self-attention, and K,V are the outputs of the final encoder layer.
