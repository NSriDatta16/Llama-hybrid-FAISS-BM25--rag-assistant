[site]: crossvalidated
[post_id]: 568102
[parent_id]: 568033
[tags]: 
You give very few details about your environment, but in general DQN can learn correct triggers for any sequence of actions provided the consequences of previous actions are summarised somehow in the state. If, in your case, the agent must learn "A before C" without any useful signal from the current observed state, then you have created a POMDP . One way to solve this whilst still using a DQN-like agent, is to use some form of RNN (e.g. LSTM) as the action value estimator. The RNN will learn to produce a summary of previous states and actions - in its cell state for the recurrent layer(s) - that is useful for predicting action values when they depend on history in addition to the current observable state. If you are in control of the state representation, and already know that specific action history is important, then you can add action history to the state features directly. You might choose not to do that if you are not sure what aspects of observation history are important, or if your goal is to create an agent that can learn to solve this problem (of determining what history impacts its decisions) without any developer assistance.
