[site]: crossvalidated
[post_id]: 14139
[parent_id]: 13858
[tags]: 
Econometricians have looked at similar issues within the framework of Granger causality . If you have two series, $Y_t$ and $Z_t$, you can run vector autoregressive models , which in the simplest form with a single lag look like $Y_t = a_0 + a_1 Y_{t-1} + a_2 Z_{t-1} + \epsilon_t$, $Z_t = b_0 + b_1 Y_{t-1} + b_2 Z_{t-1} + \delta_t$. If you see that say $a_2$ is significant, then you can claim that $Z$ (Granger-)causes $Y$: adding information about $Z$ improves the precision of your model for $Y$. Here, your time $t$ would be the post number, and the variables are obviously reputation and the score. Both are non-stationary, so a more serious fiddling with the data, like taking the increments $\Delta Y_t = Y_t - Y_{t-1}$ in place of $Y_t$ in the above equations will be called for. (Note that you may lose the normal and normal-based $F$ or $\chi^2$ distributions with non-stationary data, and the rate of convergence with trend variables, if you include them into analysis, may be $T^{-1}$ or even faster, rather than $T^{-1/2}$ that most of us are used to from the Central Limit Theorem. You need to be super-careful with these.) So I guess if $Y_t$ is the answer score, and $Z_t$ is reputation, then clearly $a_0$ is the average score, $a_1$ is how the person learns to write better answers, and $a_2$ is how their reputation precedes their word (provided the model assumptions are satisfied, etc.) On point 1: if you were doing fixed effects by hand, you should've centered both the response variable and the explanatory variables. The panel data regression package would've done this for you, but the official econometric way of looking at things is to subtract the "between" regression from the "pooled" regression (see Wooldridge's black book ; I have not checked the second edition, but I generally view the first edition as the best textbook-type description of econometric panel data). On your point 2: of course Eicker/White standard errors won't affect your point estimates; if they did, that would indicate an incorrect implementation! In the context of time-series, an even more appropriate estimator is due to Newey and West (1987) . Trying transformations might help. I am personally a big fan of the Box-Cox transformation , but in the context of the analysis that you are undertaking, it is difficult to do it cleanly. First, you would need a shift parameter on top of the shape parameter, and the shift parameters are notoriously difficult to identify in models like this. Second, you would probably need different shift/shape parameters for different people, and/or different posts, and/or... (all the hell breaking loose). Count data is an option, too, but in the context of mean modeling, a Poisson regression is just as good as the log transformation, yet it imposes an unwieldy assumption of variance = mean. P.S. You could probably tag this with "longitudinal-data" and "time-series".
