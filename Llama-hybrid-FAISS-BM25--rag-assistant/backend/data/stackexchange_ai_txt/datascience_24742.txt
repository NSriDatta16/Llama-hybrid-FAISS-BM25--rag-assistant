[site]: datascience
[post_id]: 24742
[parent_id]: 24739
[tags]: 
This is actually really easy to implement in any deep learning framework with automatic differentiation capability. You need two neural networks. One to approximate the $X$, another to approximate the $dX/dt$. The first step is easy. You have samples of input and target for $X$. Just follow the normal process to train you network to learn $X$ For the second step is that you don't have the target values (i.e. gradient) directly. The trick is that you already got an approximator for $X$ from the first step, so use that to get your target for the second network. I am most familiar with pytorch , so I will use it as an example. import torch from torch.nn import Sequential, Linear, ReLU from torch.autograd import Variable from torch.optim import Adam import numpy as np x = np.linspace(0, 10, 101).reshape(-1, 1) y = np.sin(x) # Using sin as an example x = Variable(torch.Tensor(x), requires_grad=True) y = Variable(torch.Tensor(y), requires_grad=False) f = Sequential(....) # Build f approximator f_optimizer = Adam(f.parameters()) for epoch in range(1000): loss= f(x) - y optimizer.zero_grad() loss.backward() optimizer.step() # Build grad approximator f_grad = Sequential(....) # need a new optimizer to train f_grad f_optimizer = Adam(f_grad.parameters()) for epoch in range(1000): # Directly backprop from the output to the gradient of the f model(x).backward() loss= f_grad(x) - x.grad # x.grad is the grad of f w.r.t x f_optimizer.zero_grad() loss.backward() f_optimizer.step() f_grad # this is the gradient function you are looking for (I don't have a machine with pytorch install at the moment, so don't expect the code to run with some debugging)
