[site]: crossvalidated
[post_id]: 362735
[parent_id]: 
[tags]: 
What method should I use for comparing the performance of two classifiers statistically?

1) Based on this paper: Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms by Remco R. Bouckaert and Eibe Frank, I conclude that "using 100 runs of random subsampling in conjunction with Nadeau and Bengioâ€™s corrected resampled t-test" would be a good approach for statistically comparing 2 classifiers. In this paper they have talked about accuracy as the performance metric. 2) I have also seen in literature, that many people use the ROCKIT software or the DeLong test for comparing the AUC of 2 methods to say whether one method is significantly better than the other or not. Now, I am confused. My two algorithms (say SVM and Naive Bayes) output probability scores. I can convert them to binary based on a specific threshold and use the first method (using 100 stratified shuffle split), or just use the second method using the probability scores (just run a 10 fold CV, concat the scores of the 10 folds and input to ROCKIT or DeLong). Which one should I use for my statistical test? Which one is more appropriate?
