[site]: datascience
[post_id]: 82583
[parent_id]: 
[tags]: 
Is fitting two RandomForestClassifiers 500 trees each and average their predicted probabilities on the test set more performant than one with 1000?

If I fit two RandomForestClassifiers 500 trees each and average their predicted probabilities on the test set, would it have better results than fitting a RandomForestClassifier with 1000 trees and use it to get test set probabilities? As these algorithms are random based I would say that their performance should be roughly the same? I am okay with some math to prove it, or any other way that might prove it.
