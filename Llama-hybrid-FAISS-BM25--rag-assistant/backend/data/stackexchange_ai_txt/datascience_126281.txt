[site]: datascience
[post_id]: 126281
[parent_id]: 
[tags]: 
Macro-average ROC curve not looking right

I am performing a 10-fold Cross-validation on imbalance datasets with small n examples and large p attributes. I am plotting ROC curves by merging predicted probabilities obtained by testing on each k fold. The code below performs a CV on a multi-class dataset. Where during each iteration, k fold is used as test and the k-1 folds as training. EDIT: I have created a GitHub_repository storing the whole code and a toy dataset (modified 'Iris' dataset) for simplicity. Have fun! Code below: def cross_validation(X,Y,n_splits,model,kernel): skf = StratifiedKFold(n_splits= n_splits, random_state=8, shuffle=True) skf.get_n_splits(X,Y) #X is (n_samples; n_features) y (target variable) list_predict_prob_KDE, list_predict_prob_origi, list_predict_SMOTE = [], [], [] # initilize empty lists for probabilities list_acc_ori, list_acc_smote, list_acc_kde = [], [], [] lst_y_test_labels = [] lst_X_test = [] for j,(train_index, test_index) in enumerate(skf.split(X,Y)): #iterate over the number of splits, returns indexes of the train and test set x_train_fold, x_test_fold = X[train_index], X[test_index] #selected data for train and test in j-fold y_train_fold, y_test_fold = Y[train_index], Y[test_index] #selected targets print('\n') print('ITERATION --> ',j) print('Raw training data: ',x_train_fold.shape) print('Raw training labels: ',y_train_fold.shape) print('Testing data: ',x_test_fold.shape) # 1) STEP #call the method OversampKDE on the training partitioning, which #- given a list of class examples, oversamples the minority classes x1_fold,y1_fold= oversamp_KDE_definitive(x_train_fold,y_train_fold,kernel) #call the method OversampSMOTE on the training partitioning x2_fold,y2_fold = oversamp_SMOTE_definitive(x_train_fold,y_train_fold) #------------------------------------------------------------------- # 2) STEP #fit model on augmented dataset KDE model.fit(x1_fold,y1_fold) # --> test model on test fold; append predict_proba y_proba_kde = model.predict_proba(x_test_fold) y_pred_kde = model.predict(x_test_fold) acc_kde = accuracy_score(y_test_fold,y_pred_kde) list_acc_kde.append(acc_kde) list_predict_prob_KDE.append(y_proba_kde) #---------------------------- #fit model on augmented training SMOTE model.fit(x2_fold,y2_fold) # test -- Since the multi-class scenario, I am comparing micro and macro averages methods from sklearn https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#roc-curve-using-the-ovr-macro-average . However, macro-averaged ROC curves are displaying strange behavior since are not starting from (0,0) This behavior is not shown in micro-averaged ROC curves, nor on ROC curves from binary datasets obtained from the same CV method above. What could be the reason? At the end of the Cross Validation, I concatenate lists of predicted probabilities for each model. d_probabilities = {'model_original':np.concatenate(list_predict_prob_origi,axis = 0),'model_smote':np.concatenate(list_predict_SMOTE,axis =0),'model_kde':np.concatenate(list_predict_prob_KDE,axis = 0)} y_test_labels = np.concatenate(lst_y_test_labels, axis = 0) A Vector of predicted probabilities from a Random Forest Classifier looks like this one: Predicted scores : [[0.82 0.09 0.05 0. 0.04] [0.94 0.01 0. 0.04 0.01] [0.98 0.01 0.01 0. 0. ] [0.81 0.12 0.04 0.01 0.02] [0.88 0.08 0.02 0.01 0.01] [0.1 0.74 0.06 0.04 0.06] [0.06 0.77 0.11 0.02 0.04] [0.04 0.79 0.04 0.02 0.11] [0.03 0.12 0.73 0.11 0.01] [0.06 0.14 0.7 0.09 0.01] [0.03 0.03 0.86 0.06 0.02] [0.06 0.1 0.05 0.72 0.07] [0.15 0.35 0.01 0.09 0.4 ] [0.9 0.04 0.02 0. 0.04] [0.94 0.05 0. 0.01 0. ] [0.9 0.06 0.04 0. 0. ] [0.67 0.16 0.1 0.03 0.04] [0.07 0.5 0.03 0.01 0.39]] n_classes = len(np.unique(y_test)) y_test_binarize = label_binarize(y_test, classes=np.arange(n_classes)) scores = {} for model_name, model_proba in d_probabilities.items(): y_pred = model_proba scores[model_name] = model_proba fpr ,tpr ,roc_auc ,thresholds = dict(), dict(), dict() ,dict() # micro-average for i in range(n_classes): fpr[i], tpr[i], thresholds[i] = roc_curve(y_test_binarize[:, i], y_pred[:, i], drop_intermediate=False) roc_auc[i] = metrics.auc(fpr[i], tpr[i]) # Compute micro-average ROC curve and ROC area fpr["micro"], tpr["micro"], _ = roc_curve(y_test_binarize.ravel(), y_pred.ravel()) roc_auc["micro"] = metrics.auc(fpr["micro"], tpr["micro"]) #aggregates all false positive rates all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) #fpr_grid = np.linspace(0.0, 1.0, 1000) # Then interpolate all ROC curves at this points mean_tpr = np.zeros_like(all_fpr) #mean_tpr = np.zeros_like(fpr_grid) for i in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[i], tpr[i]) # Finally average it and compute AUC mean_tpr /= n_classes fpr["macro"] = all_fpr tpr["macro"] = mean_tpr roc_auc["macro"] = metrics.auc(fpr["macro"], tpr["macro"]) # storing average-micro fpr, tpr, auc for each method (original,smote,kde) row_micro = {'Classifier': model_name, 'fpr': fpr['micro'],'tpr':tpr['micro'],'auc':roc_auc['micro']} #row_micro = {'Classifier': model_name, 'fpr': fpr['micro'],'tpr':tpr['micro'],'auc':roc_auc['micro']} table_multi_micro.loc[len(table_multi_micro)] = row_micro # storing average-macro fpr, tpr, auc for each method (original,smote,kde) row_macro = {'Classifier': model_name,'fpr':fpr['macro'],'tpr':tpr['macro'],'auc':roc_auc['macro']} #row_macro = {'Classifier': model_name,'fpr':fpr['macro'],'tpr':tpr['macro'],'auc':roc_auc['macro']} table_multi_macro.loc[len(table_multi_macro)] = row_macro
