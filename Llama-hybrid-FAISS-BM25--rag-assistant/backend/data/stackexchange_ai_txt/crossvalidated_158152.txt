[site]: crossvalidated
[post_id]: 158152
[parent_id]: 158149
[tags]: 
Adding on to Maxim's answer: How would you implement a single unit outputting catness, dogness, birdness, chairness, ...? It's not necessarily impossible, but doing separate classification nodes for each class is simple and effective. It's similar to the "one-vs-rest" multiclass classification scheme in more traditional classification methods, where you've trained one classifier for cats, another for dogs, and so on, and you take the most confident one. In the ANN case, you're doing exactly this but with a shared set of feature extractors beforehand (the earlier layers). In terms of the rounding approach that Maxim suggested: that scheme is never going to work, as he notes. The earlier layers would need to produce features such that a single linear combination produces "cat" in a certain range, "chair" in another range, "forest" in another; that's a much harder problem than producing features where different linear combinations can recognize "cat", "chair", and "forest." I've also never heard of this rule, but Maxim's thoughts seem reasonable. This is probably referring to the fact that shallow, wide neural networks are universal approximators: you can represent any continuous [or piecewise-continuous] function using only a single hidden layer . Though mathematically interesting, this is not really a useful statement beyond the basic sanity check that the representation isn't terrible. Such networks are nearly impossible to learn, and much be extremely wide in order to learn complicated functions, since they're just piecewise approximators. Deep structures can represent certain classes of functions with exponentially fewer parameters , and because they can generalize much more effectively than shallow networks may be learnable with fewer training examples when their architecture is chosen appropriately.
