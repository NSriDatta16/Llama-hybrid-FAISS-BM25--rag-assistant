[site]: datascience
[post_id]: 122756
[parent_id]: 
[tags]: 
Build a model with cross-validation on entire dataset to learn insights?

Goal : Use XGBoost regression to learn insights from data. Prediction or forecasting not needed. Hypothesis : If the model fits the entire dataset well, it can maybe capture its "physics" in the system. Technique: Define hyperparameter space. Use entire dataset (no training and testing split) Initialize model with hyper-parameters. Split the entire dataset to maybe 10 folds. Rather than just finding cross-val scores on each fold, I build the model using each fold. So I start a loop of 10 folds, and build the initialized model for each fold. Thus the final model will be built/trained on all the ten folds. I am also storing the RMSE of each fold. I use this fitted model to predict my entire dataset. Then I use RMSE of this prediction in the hyperparameter algorithm for it to be minimized. I can also add the variance of the RMSES in the folds to the algorithm as an uncertainity. Best parameters chosen based on the RMSE in step 5 (entire dataset). I know CV is not for building, but since my goal is not prediction, my goal is that the model fits the entire dataset well enough to understand its "physics". Is this a valid approach? I am using hyperopt for hyper-parameter tuning. Thanks.
