[site]: crossvalidated
[post_id]: 512409
[parent_id]: 512399
[tags]: 
Expanding upon what @gunes said, if you take a shallow network and explicitly trace out what the network's forward pass is. You can see the adding of additional nodes (within a layer) allows the network to minimise its prediction error. Let's assume we have some Feed-Forward Neural Network (FFNN) that takes in a single input and a single output and has a single hidden layer of N hidden nodes, and a non-linear activation function denoted, $\sigma$ , and the entire forward pass of the network can be described as $\hat{y} = f(x)$ , where $\hat{y}$ is the prediction of the true output $y$ . For N = 1, the network can be written as, $$ \hat{y} = w^{(2)}\sigma(w^{(1)}x_j + b^{(1)})$$ for a given number of hidden nodes, N, it's written as a sum of single neurons, i.e., $$ \hat{y} = \sum_i w^{(2)}_{i}\sigma(w^{(1)}_ix_j + b^{(1)}_i) $$ If we look at this from a linear regression standpoint we would minimise a loss function, for example the Mean Absolute Error (MAE), in order to tune the weights and biases such that our network accurately predicts our training data. Let's apply the Universal Approximation Theorem to see what adding more neurons does. Let's say we have a single hidden node in our network and we give our network a single example pair $(x_j, y_j)$ , this would give a prediction $\hat{y_j}$ which would have an error of $\vert y_j - \hat{y}_j \vert$ . We would then use some optimization algorithm (SGD, RMSprop, Adam etc...) to minimise this error. However, a single hidden node can only 'learn' so much and will eventually hit a local minimum where it'll learn no more. This would be written out like, $$\hat{y}_j = w^{(2)}_1\sigma(w^{(1)}_1 x_j + b^{(1)}_1)$$ and let's say (for the sake of argument) the lowest error we can get is 10. This error of 10 can be minimising by adding a new neuron. Let's say the target $y_j$ is 20 and our model predicts $\hat{y}_j$ = 10 (giving the error of 10 from above). We can add a new neuron to try and reduce this error by getting the new neuron to output a contribution to the total sum which minimising our error. And, let's say the best the 2nd neuron can do is an output of 5 - which would bring out total MAE to 5. $$\hat{y}_j = \underbrace{w^{(2)}_1\sigma(w^{(1)}_1 x_j + b^{(1)}_1)}_{\text{first neuron outputs 10}} + \underbrace{w^{(2)}_2\sigma(w^{(1)}_2 x_j + b^{(1)}_2)}_{\text{second neuron outputs 5}} $$ In this two neuron case, the best the model can do is 15 and in principle, we can keep adding neurons to the layer to reduce this error. Leading to the Univeral Approximation Theorem stating that a neural network can approximate any continuous function if given enough nodes to do so. In short, the neurons for a given layer allow the network to transform the input in a certain manner and having fewer neurons restricts the ability of the layer to transform the data in the 'correct' way. Which in the case of deeper networks this restricts the ability of higher layers in transforming the data because the initial transformation they get are 'further' away (in terms of a loss metric) from the exact transformation!
