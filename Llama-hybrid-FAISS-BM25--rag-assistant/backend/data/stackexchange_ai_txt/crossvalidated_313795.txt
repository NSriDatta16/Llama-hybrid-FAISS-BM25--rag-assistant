[site]: crossvalidated
[post_id]: 313795
[parent_id]: 270522
[tags]: 
They are simply max-pooling over time. You are right that the features are a a 1024 dimensional embedding of frames, which means you have a Kx1024 dimensional tensor representing the entire video, where K is the number of frames. For each feature, they just take the max feature activation from all of the frames in the video. Does it seem overly-simple? Maybe, but the comparison they make to bag-of-words models is appropriate. Sometimes you are only interested in whether or not a frame-level feature appears anywhere in a video, and you don't care exactly where it appears.
