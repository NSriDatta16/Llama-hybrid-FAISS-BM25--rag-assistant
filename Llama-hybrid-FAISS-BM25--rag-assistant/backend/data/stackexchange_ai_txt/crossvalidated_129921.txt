[site]: crossvalidated
[post_id]: 129921
[parent_id]: 129797
[tags]: 
I am not an expert on this, but I can give a vague answer with an overall strattegy. I too would take a sampling based approach. However, you are probably right that you actually have the possibility to do better than just isolating K for every observed W . I would model this in a bayesian framework using Gibbs sampling because (1) bayesian inference is all about computing with distributions and (2) because it is so easy for non-mathematicians like me :-) In your first model, everything but the parameters generating W would be deterministic nodes, including K. So the inference problem should be fairly simple and you will get your K-distribution. And you have data to narrow in the posterior distribution of W. The next step would then be to add guesses about the distribution of K. If you get any ideas about which parameters/distributions best describe the posterior, you can model that as well and compare the likelihood of these models by having a mixture model with an indicator variable choosing between each one. The relative frequencies that the indices are sampled is the posterior likelihood ratio between these models being true. Software-wise, many options exist for this but from the BUGS family you can have a look at JAGS, PyMC, OpenBUGS, stan etc. (written in the order of ease of use. JAGS is probably simplest). If bayesian inference sounds scary and you have a reasonable guess at the distribution-family for K you can compare alternative models using nonlinear frequentist approach. Look at R packages nls and nlm . Or nlmer if you want to do mixed model.
