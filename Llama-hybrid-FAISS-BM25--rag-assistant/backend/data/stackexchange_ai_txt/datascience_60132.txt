[site]: datascience
[post_id]: 60132
[parent_id]: 57027
[tags]: 
There are multiple algorithms that support monotonicity, i.e., can learn a monotonic decision rule. It is easy to enforce a monotonicity constraint when using linear regression and logistic regression; you simply enforce that the corresponding coefficient for that variable is non-negative during optimization. xgboost supports monotonicity constraints. However, beware that it is "best-effort" and does not guarantee that the model will be globally 100% monotonic for the entire space of data values. Neural networks can support monotonicity constraints, by enforcing that the weights are non-negative. I don't know how to enforce monotonicity with decision trees or random forest classifiers. See, e.g., https://cs.stackexchange.com/q/69220/755 for a counterexample showing that even if your training set is monotonic, the resulting decision tree might learn a non-monotonic rule. Some references to check out: https://stats.stackexchange.com/q/257049/2921 , https://stats.stackexchange.com/q/342651/2921 , https://stats.stackexchange.com/q/341422/2921 .
