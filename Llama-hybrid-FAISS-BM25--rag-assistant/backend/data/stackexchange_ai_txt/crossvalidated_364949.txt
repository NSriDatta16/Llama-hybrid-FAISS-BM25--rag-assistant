[site]: crossvalidated
[post_id]: 364949
[parent_id]: 
[tags]: 
RNN LSTM overfitting

I'm trying to build a dynamic RNN network for 2-class classification, and I just can't get rid of the overfitting. I have 5500 samples of class A, and 8000 for class B (total 13500). From that I take 70% for training, 15% validation, and 15% for test. I can get more data but I'm not sure this is what's causing the overfitting. Here are the graphs: Red: train Blue: validation The data is reddit posts (tokenized by nltk.word_tokenize() ), and the labels are the OP's ages. Since the posts are of different lengths, I'm using dynamic RNN. Here are the network parameters: BATCH_SIZE = 128 MIN_POST_LENGTH = 10 #in tokens MAX_POST_LENGTH = 50 #in tokens LEARNING_RATE = 0.1 NUM_OF_LAYERS = 3 LAYER_SIZE = 64 #features TRAIN_DROPOUT = 0.5 Optimizer: GradientDescentOptimizer Loss function: reduce_mean For building the vocabulary I sort the words by their popularity, use the rank as index, and then normalize to 0 mean and 1 std . I don't take all the words though, only top X, because most words' frequency is very low (I tried different X's...). What should I change in order to overcome this overfitting? I tried playing with the batch size, learning rate, changing the optimizer... Nothing helped.
