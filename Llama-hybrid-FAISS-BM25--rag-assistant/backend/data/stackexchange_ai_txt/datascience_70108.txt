[site]: datascience
[post_id]: 70108
[parent_id]: 
[tags]: 
Semi-Supervised Learning using NLP

I am working on a drug reaction problem in which I need to extract tweets and label the tweets (binary-reaction due to drug or not). But since I don't have domain knowledge, and clustering would also not help me in labelling them, I thought of using semi-supervised learning as one labelled small labelled dataset of around 400 tweets is available. So I will be training my model on those 400 tweets and then make predictions on the tweets that I extracted for labelling. I know since 400 tweets is very less so its predictions on my extracted tweets would be really poor. But ignoring that, I am facing another problem. In NLP, we map every word in the dataset during preprocessing. Suppose bag-of-words or create word embedding, word2vec or tf-idf vector. So during prediction when I run the preprocessing on my extracted tweets, it throws an error for the new words in this extracted tweets dataset. As in my extracted dataset there will be names of new medicines or some English word that was not present in the labelled dataset. And suppose I had created bag-of-words, the structure of bag-of-words changes with the input, so in that case model prediction would be wrong as inputs (bag-of-words) are not based on the same words. One thing that I have planned to do is combine extracted and labelled dataset and preprocess them together. Then train only on labelled and predict on extracted. This way maybe the predictions would be more accurate and it shouldn't throw any error. Still, is there any specific NLP technique that I can use in preprocessing to deal with this error and get better results?
