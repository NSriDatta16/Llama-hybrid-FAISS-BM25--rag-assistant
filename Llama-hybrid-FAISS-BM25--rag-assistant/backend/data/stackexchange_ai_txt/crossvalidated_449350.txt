[site]: crossvalidated
[post_id]: 449350
[parent_id]: 
[tags]: 
Mathematical meaning of minimizing JS divergence about GAN

Optimization of the loss function of GAN is equivalent to minimizing Jensen Shannon divergence, and minimization of cross-entropy loss, which is often used in classification problems such as image classification, is equal to minimization of Kullback-Leibler divergence. Do the two differences have mathematical or statistical meaning? And how can it be interpreted?
