[site]: crossvalidated
[post_id]: 99174
[parent_id]: 
[tags]: 
Can we make probabilistic statements with prediction intervals?

I've read through the many excellent discussions on the site regarding interpretation of confidence intervals and prediction intervals, but one concept is still a bit puzzling: Consider the OLS framework and we've obtained the fitted model $\hat y = X\hat\beta$. We're given a $x^*$ and asked to predict its response. We compute $x^{*T}\hat\beta$ and, as a bonus, we also provide a 95% prediction interval around our prediction, a la Obtaining a formula for prediction limits in a linear model . Let's call this prediction interval PI. Now, which of the following (or neither) is the correct interpretation of PI? For $x^*$ in particular, $y(x^*)$ lies within PI with 95% probability. If we're given a large number of $x$s, this procedure to compute PIs will cover the true responses 95% of the time. From @gung's wording in Linear regression prediction interval , it seems like the former is true (although I could very well be misinterpreting.) Interpretation 1 seems counterintuitive to me (in the sense that we're drawing Bayesian conclusions from frequentist analysis), but if it's correct, is it because we're predicting the realization of a random variable vs. estimating a parameter ? (Edit) Bonus question: Suppose we knew what the true $\beta$ is, i.e. the process generating the data, then would we be able talk about probabilities regarding any particular prediction, since we're just looking at $\epsilon$? My latest attempt at this: we can "conceptually decompose" (using the word very loosely) a prediction interval into two parts: (A) a confidence interval around the predicted mean response, and (B) a collection of intervals which are just quantile ranges of the error term. (B) we can make probabilistic statements on, conditional on knowing the true predicted mean, but as a whole, we can only treat prediction intervals as frequentist CIs around predicted values. Is this somewhat correct?
