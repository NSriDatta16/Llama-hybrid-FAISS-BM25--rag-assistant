[site]: crossvalidated
[post_id]: 453364
[parent_id]: 453350
[tags]: 
There may be, but I haven't seen it. I fear that there is a semantic confusion, as "non-parametric" means two different things ( https://en.wikipedia.org/wiki/Nonparametric_statistics ), depending on whether you look at the right-hand side of $y \sim f(x)$ , or the left-hand side: "Non-parametric" in the Bayesian machine-learning or Bayesian finite mixture context means that the model, i.e. the right-hand side, is non-parametric (e.g. https://blog.statsbot.co/bayesian-nonparametrics-9f2ce7074b97 ). In other words, the model structure $f(.)$ is not fixed and adapts to the data (think splines, Gaussian random fields). When your response is non-Gaussian, you are looking at a "non-parametric" response, which is addressed in Kruskal-Wallis and alike. Here "parametric" refers to the parameters of the distribution from which you assume your data derive. For such a model you could have priors for the response terms, making it potentially Bayesian, but you still need a likelihood (or approximation) for the "non-parametric" response. The idea of the rank-transformation (which, without ties, yields the Kruskal-Wallis when used in an ANOVA) is to get the response variable into something uniformly distributed. So if you are willing to assume a uniform distribution for your rank-transformed response, and present priors for your model terms, then you are in the land of "Bayesian non-parametric response". One point of Bayesian priors is to use previous knowledge. If you rank-transform your data, this knowledge is largely eliminated, or at least reduced to something like "group A is larger than group B". I wonder whether it is worth going Bayes in this case.
