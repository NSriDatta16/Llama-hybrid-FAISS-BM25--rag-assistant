[site]: datascience
[post_id]: 27768
[parent_id]: 27613
[tags]: 
To me, this describes conditional probabilities . And the first thing that springs to my mind when it comes to learning conditional probabilities is a Bayesian network (BN) , which is an example of a probabilistic graphical model. In fact, BNs can be fitted using Maximum Likelihood Estimation (MLE), which corresponds to counting occurrences (similar to "Naive Bayes", if you have used that). An excellent book on this is Barber . Another model that corresponds to your probabilistic problem, and which is more neural network-like, is a Restricted Boltzmann Machine (RBM) : ( source ) Once again, this involves learning conditional probabilities, like $p(v_0 | h_0, h_1, h2), p(v_1 | h_0, h_1, h2), p(v_2 | h_0, h_1, h2), \ldots$. This is a nice introduction to them. Unlike neural networks, which are trained using backpropagation and are not probabilistic, RBMs are indeed probabilistic and are trained using Contrastive Divergence.
