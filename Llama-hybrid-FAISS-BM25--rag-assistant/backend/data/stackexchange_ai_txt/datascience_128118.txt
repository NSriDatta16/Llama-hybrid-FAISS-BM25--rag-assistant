[site]: datascience
[post_id]: 128118
[parent_id]: 128113
[tags]: 
For this kind of setup, you should use the output at the first position and train a linear classifier over your 3 labels. BERT was trained with inputs that were prepended a special token [CLS] , and the output at that position (i.e. the first position) was used for a classification task. It is understood that, at that position, BERT outputs a representation for the whole input sentence. Therefore, this representation (i.e. the vector with dimensionality 768 outputted by the last layer in the first position) is what you should use as input for your sentiment classifier. You should train a new model that takes the 768-dimensionality vector as input and generates the label. For this, a linear model with a categorical cross-entropy loss would be the standard.
