[site]: crossvalidated
[post_id]: 505327
[parent_id]: 
[tags]: 
On quantifying the amount of information per example provided to the model in Supervised vs Self-supervised learning

I've seen Yann Lecun in his self-supervised learning talks talking about how traditional supervised learning (Classification setting) by attributing a class out of N classes to each example the information provided to the model per example is only logN (base 2), whereas in self-supervision this amount of information per example (potentially) explodes combinatorially to millions of bits of information per example depending on the dimensionality of the example and the pretext task chosen. I see the point in this statement but is it mathematically correct? like in traditional classification the model is learning only logN bits per example? isn't the model also implicitly leveraging correlations and covariances of example's dimensions to learn? if so how to quantify the amount of those bits? is there a way to quantify the amount of information provided to a model given M examples of (x,y) pairs where y is 1 out of N class? if yes how would it be possible to quantitatively compare it to contrastive learning or self-supervised learning in general?
