[site]: datascience
[post_id]: 28345
[parent_id]: 28344
[tags]: 
Take a few words you know are linked with e.g republicans and with democrats. Extract their word embedding. That is the vector representation of the word which will be high dimensional. This will assume you have trained word embedding, either pretrained, or even better trained them yourself on your data. The word embeddings could have been trained/extracted through e.g LSTM training or matrix factorisations. On the word embeddings, do dimension reduction(PCA as an example) to 2d. Plot! Those who are close together will be "similar" for the objective they are trained for. Pretrained word embeddings exist to download. However, they might not be the optimal embeddings for your objective, but better than nothing still.
