[site]: crossvalidated
[post_id]: 238184
[parent_id]: 238082
[tags]: 
The 87.5% calculation assumes that the outcomes of your prediction are statistically independent . But, thinking of your model as a fixed function $f$, your input as a fixed vector $x$, and your "slight modification" as an offset $\varepsilon$, it is certainly not the case that $f(x + \varepsilon_1)$ is independent of $f(x + \varepsilon_2)$, so the increase in probability of being correct is going to be tempered by the correlation between the two outputs. Usually, machine learning predictions are smooth in some sense in most directions; this is why we can do machine learning in the first place. So, in a classification problem, $f(x + \varepsilon_1)$ is going to be the same as $f(x + \varepsilon_2)$ most of the time. If you do this 1000 times and get the same answer every time, this definitely does not mean that your answer is going to be (nearly) 100%. On the other hand, if you do this 1000 times and get an answer of positive class 500 times and negative class 500 times, this means that your model is very uncertain about the inputs (assuming your $\varepsilon$ is on an appropriate scale), and you should therefore be less confident in your prediction for $x$.
