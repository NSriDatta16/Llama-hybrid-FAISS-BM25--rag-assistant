[site]: crossvalidated
[post_id]: 297747
[parent_id]: 16921
[tags]: 
Think of it this way. Variances are additive when independent. For example, suppose we are throwing darts at a board and we measure the standard deviations of the $x$ and $y$ displacements from the exact center of the board. Then $V_{x,y}=V_x+V_y$. But, $V_x=SD_x^2$ if we take the square root of the $V_{x,y}$ formula, we get the distance formula for orthogonal coordinates, $SD_{x,y}=\sqrt{SD_x^2+SD_y^2}$. Now all we have to show is that standard deviation is a representative measure of displacement away from the center of the dart board. Since $SD_x=\sqrt{\dfrac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}}$, we have a ready means of discussing df. Note that when $n=1$, then $x_1-\bar{x}=0$ and the ratio $\dfrac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}\rightarrow \dfrac{0}{0}$. In other words, there is no deviation to be had between one dart's $x$-coordinate and itself. The first time we have a deviation is for $n=2$ and there is only one of them, a duplicate. That duplicate deviation is the squared distance between $x_1$ or $x_2$ and $\bar{x}=\dfrac{x_1+x_2}{2}$ because $\bar{x}$ is the midpoint between or average of $x_1$ and $x_2$. In general, for $n$ distances we remove 1 because $\bar{x}$ is dependent on all $n$ of those distances. Now, $n-1$ represents the degrees of freedom because it normalizes for the number of unique outcomes to make an expected square distance. when divided into the sum of those square distances.
