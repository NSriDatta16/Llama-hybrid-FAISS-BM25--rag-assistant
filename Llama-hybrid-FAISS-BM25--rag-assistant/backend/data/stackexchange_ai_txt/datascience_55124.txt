[site]: datascience
[post_id]: 55124
[parent_id]: 55121
[tags]: 
This a good question. I'll post my own personal considerations that should not be taken as rigorous answers, I'd like to broaden the question a little bit [ Disclaimer: personal considerations, not rigorous answer ] The generator and discriminator are not strictly learning together, they are learning one against other. Indeed, when the discriminator is training, the generator is frozen and vice versa. At the very beginning of the training phase, the generated outputs of the generator are expected to be very far away from the real samples. It is also true that the discriminator is expected to not have a good feature space for the classification, but I think it gets to tell them apart faster than the generator begins to produce closer fake samples. Then, the minmax game evolves... I thougth once to use a pre-trained classifier to force the generator produce real good fake images, but I also got to the feeling that, in that way, the battle will not be fair and will be biased to the discriminator side, regarding this: Which input data should should be fed to the discriminator in its pre-training phase? I mean, to pre-train the discriminator, some pre-fake images are needed. One approach is to pre-train for a few steps the generator and produce pre-fake images which are far away from real images Since the discriminator is still learning, are they chances it becomes even better? The discriminator is so good it won't learn, is the game closer to nash equlibirum and so, closer to its end? Taking into account the enormous problems when training GANs, I've never tried it out. It would be fantastic if more experienced GANners offer more insight into these considerations
