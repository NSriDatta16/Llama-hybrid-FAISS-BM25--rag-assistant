[site]: crossvalidated
[post_id]: 626807
[parent_id]: 
[tags]: 
Shouldn't we consider the difference in variance between population and a sample while calculating confidence intervals?

To comprehend the concept of confidence intervals, I came up with an example. I want to share it here for your better understanding what my question is all about. Suppose, we want to figure out what is the average height in a town M. We randomly selected $n$ people and measured their height. We got a sample average $\bar{X}$ . A standard statistical question - what if the population mean $μ$ is higher or lower than our sample mean? Fortunately, we know that the distribution of heights looks similar to the normal distribution. If we got lucky and $\bar{X}$ = $μ$ , then our sample mean $\bar{X}$ will be right in the center of the distribution. But, most likely, it won't. Our $\bar{X}$ could be here: Or even here: In other words, our sample mean lies in some interval and because we know properties of normal distribution, it is possible to calculate that interval. We calculate the margin of error at a given α by formula $MOE_α = z_α \times \sqrt{\frac{\sigma^2}{n}}$ , so that we could "capture" possible left-or-right deviation from unknown population mean. Then we compute the confidence interval: $\bar{X}±MOE_α$ . As a result, we can say, for example, that there is a 95% probability that the 95% confidence interval will cover the true population mean. It seems for me that I understand the logic of confidence intervals, when there are equal variances of both population and sample. But what if, let's say, the population variance is higher than sample variance or reverse? In this case, the distribution will be wider or narrower and, if I think correctly, the probability will not be 95% . Do I understand the concept of confidence intervals correctly? Are there any flaws in my logic? If none, then how should we take into account the possible difference in sample and population variances?
