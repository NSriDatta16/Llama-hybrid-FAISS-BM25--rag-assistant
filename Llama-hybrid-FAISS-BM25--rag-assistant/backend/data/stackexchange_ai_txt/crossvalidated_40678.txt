[site]: crossvalidated
[post_id]: 40678
[parent_id]: 40576
[tags]: 
The simple answer is that you should do feature selection on a different dataset than you train on (you're doing this already, so don't change this)---the effect of not doing this is you will overfit your training data. You must also not do feature selection on your test set, as this will inflate estimates of your models' performance (I think you already realise this as well, but I found it a little hard to understand the question precisely). If you've divided your test set into training, validation and testing already, then there's no particular reason to do cross-validation unless you have so little data that your test set is too small to draw strong conclusions from. Many researchers have a dislike of cross validation because if used to drive model development (by which I mean, you tweak things, then run cross validation to see how they do, then tweak them some more etc.) you effectively have access to your test data and this can lead you to overestimate your performance on truly unseen data. If your data is so small that you have no choice but to do cross validation, the correct way to do this with training, dev and test sets is to explicitly split your data into three parts for each fold---the majority should be used for training, some for development (feature selection in your case, plus any other free parameters that need fitting) and finally you should test on the test portion. You can then average scores across these test portions to get an estimate of model performance: however, as I said, beware that if these scores are used to guide you to approaches you want to use for your problem, you shouldn't expect to get the same score on unseen data that you did from your cross validation.
