[site]: datascience
[post_id]: 56336
[parent_id]: 
[tags]: 
Improving recall in XGBoost algorithm

I have highly imbalanced dataset. I am using XGBoost and I got the following results without balancing the dataset out: Precision: 0.87 Recall: 0.79 F1: 0.83 My parameters are: xgb_model=xgb.XGBClassifier( objective = "binary:logistic", n_estimators=500, max_depth=8, verbosity=2, random_state=42) xgb_model.fit(X_train,y_train, eval_metric='aucpr', eval_set=eval_set) If I balance out my dataset via SMOTE, I get the following results: Precision: 0.81 Recall: 0.80 F1: 0.81 If I use an imbalanced dataset, how do I improve recall (via precision trade-off)?
