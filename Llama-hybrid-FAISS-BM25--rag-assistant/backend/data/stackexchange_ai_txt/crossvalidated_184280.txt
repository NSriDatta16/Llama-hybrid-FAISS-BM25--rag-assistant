[site]: crossvalidated
[post_id]: 184280
[parent_id]: 182734
[tags]: 
To expand on David Gasquez's answer, one of the main differences between deep neural networks and traditional neural networks is that we don't just use backpropagation for deep neural nets. Why? Because backpropagation trains later layers more efficiently than it trains earlier layers--as you go earlier and earlier in the network, the errors get smaller and more diffuse. So a ten-layer network will basically be seven layers of random weights followed by three layers of fitted weights, and do just as well as a three layer network. See here for more. So the conceptual breakthrough is treating the separate problems (the labeled layers) as separate problems--if we first try to solve the problem of building a generically good first layer, and then try to solve the problem of building a generically good second layer, eventually we'll have a deep feature space that we can feed in to our actual problem.
