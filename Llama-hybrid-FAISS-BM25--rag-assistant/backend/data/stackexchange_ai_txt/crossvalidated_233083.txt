[site]: crossvalidated
[post_id]: 233083
[parent_id]: 233050
[tags]: 
The weights assigned to each feature in a logistic regression model do not determine the importance of that feature, and neither does feature elimination help determine the order of importance. To begin understanding how to rank variables by importance for regression models, you can start with linear regression. A popular approach to rank a variable's importance in a linear regression model is to decompose $R^2$ into contributions attributed to each variable. But variable importance is not straightforward in linear regression due to correlations between variables. Refer to the document describing the PMD method (Feldman, 2005)[ 3 ]. Another popular approach is averaging over orderings (LMG, 1980)[ 2 ]. There isn't much consensus over how to rank variables for logistic regression. A good overview of this topic is given in [ 1 ], it describes adaptations of the linear regression relative importance techniques using Pseudo-$R^2$ for logistic regression. A list of the popular approaches to rank feature importance in logistic regression models are: Logistic pseudo partial correlation (using Pseudo-$R^2$) Adequacy: the proportion of the full model log‐likelihood that is explainable by each predictor individually Concordance: Indicates a model’s ability to differentiate between the positive and negative response variables. A separate model is constructed for each predictor and the importance score is the predicted probability of true positives based on that predictor alone. Information value: Information values quantify the amount of information about the outcome gained from a predictor. It is based on an analysis of each predictor in turn, without taking into account the other predictors. References: On Measuring the Relative Importance of Explanatory Variables in a Logistic Regression Relative importance of Linear Regressors in R Relative Importance and Value, Barry Feldman (PMD method)
