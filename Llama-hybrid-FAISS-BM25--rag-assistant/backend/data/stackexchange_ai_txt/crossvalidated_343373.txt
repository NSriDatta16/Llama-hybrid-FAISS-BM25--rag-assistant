[site]: crossvalidated
[post_id]: 343373
[parent_id]: 
[tags]: 
question about backpropagation with a softmax

When we have a neural network with a softmax layer, I'm a little confused how the weights get updated which are NOT associated with the correct answer. The issue is that the cost function will typically be a cross entropy loss like the following: $$L^{(i)} = -\sum_{k=o}^{n_y-1}Y_k^{(i)}log(a_k^{(i)})$$ This is the loss for a single example $(i)$, where both $Y$ and $a$ are one hot vectors, $a$ is the activation of the softmax, and $n_y$ is the number of entries in that vector. My confusion is that basically we only have a positive loss for the weights associated with the true label. So does this mean that for each example, the model will only learn better weights for that one label? This is confusing to me, because it means that we would only ever increase weights: If the model did not predict the true label, the weights would be changed to make that prediction more likely If the model did predict the true label, the weights would still be changed to make that prediction more likely, unless the model predicted the true label with absolute certainty What's missing here, I think, is a case where the model would reduce weights to reduce the likelihood of predicting a label.
