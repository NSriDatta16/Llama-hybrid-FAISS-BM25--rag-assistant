[site]: crossvalidated
[post_id]: 197429
[parent_id]: 
[tags]: 
Optimal prediction pools in practice

In their paper Optimal Prediction Pools , John Geweke and Gianni Amisano propose a rather intuitive approach to form linear opinion pools to combine different forecasts. As pointed out in this excellent answer from @Zachary Blumenfeld their method does not require that the true predictive Model is included, in my opinion a rather important property (if not, essential). However, I do struggle a bit when it comes to implementing the Theory into Practice: To make it simple, let's assume I have two model assumptions $A_1$ and $A_2$ defining Bayesian models, giving me the possibility to sample at time point $t$ from the posterior distribution $p(\Theta_i|Y_{t-1},A_i)$ for the set of parameters $\Theta$ of Model $A_i$ via MCMC where $Y_{t-1}=\{y_1,\ldots,y_{t-1}\}$. Therefore, I obtain two sequences $\{\Theta_1 ^{(m)}\}$ and $\{\Theta_2 ^{(m)}\}$. The optimal prediction pool corresponds to choosing the linear combination $$\omega_T ^0 =\arg\max_{\omega\in[0,1]}\sum\limits_{t=1}^{T}log[wp(y_t^o|Y_{t-1},\Theta_1,A_1)+(1-\omega)p(y_t^o|Y_{t-1},\Theta_2,A_2)]$$ where $y_t ^0$ is the observed value at time $t$. My question is how to compute the value of the one-step-ahead predictive likelihood $$p(y_t^o|Y_{t-1},A_i):=\int_{\Theta_i}p(y_t ^0|Y_{t-1},\Theta_i,A_i)p(\Theta_i|Y_{t-1},A_i)d\Theta_i$$ based on my MCMC samples. Is it correct to use Monte-Carlo Integration here by computing $p(y_t ^0|Y_{t-1},\Theta_i ^{(m)},A_i)$ for each of the $m$ draws and then just take the average to approximate the integral via $$\frac{1}{M}\sum\limits_{m=1}^Mp(y_t ^0|Y_{t-1},\Theta_i ^{(m)},A_i)$$ as pointed out on page 7 of this paper ?
