[site]: crossvalidated
[post_id]: 396155
[parent_id]: 
[tags]: 
Backpropagation - Assumption or Error in Bishop's "PRaML"

I am currently reading Bishop - "Pattern Recognition and Machine Learning" (2006) and I could not figure out if I missed an assumption he made or if it is just wrong. In the chapter about backpropagation on page 243 he computes the derivative of the error function with respect to a particular weight $\displaystyle \frac{\partial E_n}{\partial w_{ji}}=\frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial w_{ji}}$ (Equation 5.50) This is only correct if the output units are assumed to be the identity. Why does he not get the derivative of the activation function in the expression? It should look like this: $\displaystyle \frac{\partial E_n}{\partial w_{ji}}=\frac{\partial E_n}{\partial h}\frac{\partial h}{\partial a_{j}}\frac{\partial a_j}{\partial w_{ji}}$ where $h$ is the activation function.
