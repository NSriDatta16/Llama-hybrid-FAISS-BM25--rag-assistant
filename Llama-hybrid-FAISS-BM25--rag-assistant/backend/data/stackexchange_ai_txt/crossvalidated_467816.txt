[site]: crossvalidated
[post_id]: 467816
[parent_id]: 
[tags]: 
What is the advantage of shuffling data in train-test split?

I have been taking an online course on data science, and was recently introduced the ideas of overfitting and underfitting, that is splitting the dataset we have into two parts into training(80%-90%) and testing(10%-20%) dataset. The instructor says we prefer to shuffle our data before the split, what is the reason for this? Isnt the whole idea of the approach of Splitting into Testing/Training Set is based on the assumption, that the observations are Independent and identically distributed random variables. Then there is no meaning to shuffling our data. Did i get it correct?
