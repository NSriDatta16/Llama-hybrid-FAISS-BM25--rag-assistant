[site]: crossvalidated
[post_id]: 593634
[parent_id]: 593628
[tags]: 
This delves into the very complicated field of item response theory (IRT). IRT is the system of developing scoring algorithms from questionnaire data, addressing such issues as: is question A more discriminable than question B? or Is question C really necessary after asking questions A and B? or Does question D have the same sentiment as question A (reverse anchoring)? Based on your description of the problem, this may be too much into the weeds, but any approach you take needs to make some assumptions. The apparent flexibility of the mixed logistic model can be quickly convoluted by the difficulty in interpreting the response. 1: there's no guarantee that the GLMM will actually converge - the model's performance can be poorly behaved in small samples with additional adjustments. 2: the estimated group effect - the log odds ratio of responding "Yes" in Group 1 to responding "Yes" in Group 0, is not intuitive and easily misinterpreted. 3: calculating the p-value from a mixed model is complicated and there is no omnibus. To consult the documentation from whatever your software is to provide the appropriate description is a statistician's job. On the other hand, the simplicity of the ANCOVA can't be ignored - a simple sum-score is a very standard way to analyze multiple question questionnaires. If the particular question set administered is part of a standard instrument (like the CES-D for instance) you might already have scoring criteria for these questions. If not, you can still motivate the sum score as a pragmatic response variable. It should be obvious to the reader that a subject answering 5 yes questions has "more yes" than a subject answering 2 yes questions, regardless of whether the questions were equally difficult. Regarding missing questionnaire data, hopefully all the questions are scanned for adequacy, such as asking the household income from a non-working parent - a common source of informative missingness. One you can use a number of approaches: Standardize the sum score. For instance, a subject answering 9/9 questions affirmatively can be offset by 10/9 to reflect a 10 (i.e. 100%) "Yes" responses. Use multiple imputation Consider missing questions a negative. $^*$ As this is thesis work, the best decision may well be whatever your advisor says.
