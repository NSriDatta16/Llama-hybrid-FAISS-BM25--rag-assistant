[site]: datascience
[post_id]: 53881
[parent_id]: 53876
[tags]: 
In this kind of case, I would look into mean/median target encoding. Basically, you take the average/median target/response value of y (based on your training set only) per each category in your feature. This new statistic per group is now your encoding. For your validation/testing data, you simply join back the "learned" statistics from your training set as a lookup table. It tends to work extremely well for features with high cardinality. Also, suppose you do use one-hot encoding. How sparse are some of the resulting columns? If you have multiple sparse columns (can be detected easily by looking for low variance features or by simply counting how many 0's to 1's there are) then likely, these categories can be grouped. Finally, there are algorithms that have specifically been made to handle high cardinality situations internally; in fact, CatBoost (whose motivation in its creation was in part to create a gradient boosting framework that directly supported categorical variables) to my belief heavily popularized mean/median target encoding (what I explain above) and gives you many different options with respect to encoding categorical features.
