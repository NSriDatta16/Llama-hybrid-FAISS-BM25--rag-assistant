[site]: crossvalidated
[post_id]: 519805
[parent_id]: 519791
[tags]: 
BERT was trained with the special tokens, so it expects them to be on the input. Analyses of BERT's self-attention (e.g., Clark et al., 2019 ; Voita et al., 2019 ) show that the positions corresponding to special tokens are often used by the self-attention, probably having some technical function. It is certainly possible to finetune BERT not to do so (or trained BERT from scratch without them), but it would only make the fine-tuning more difficult with more new things to learn for the model.
