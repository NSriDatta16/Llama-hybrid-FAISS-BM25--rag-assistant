[site]: crossvalidated
[post_id]: 437634
[parent_id]: 
[tags]: 
Compact/Vectorized Multiclass Logistic Regression Hessian

I know that the Hessian of the categorical cross entropy w.r.t the weights is given by $$\frac{\partial^2 L}{\partial w^2} = \sum_{i=1}^{m} (Diag(\hat{y}_i)-\hat{y}_i^T \hat{y}_i) \otimes x_i^T x_i$$ where $\otimes$ is the Tensor Product The question is if there is some way to rewrite this statement in a Vectorized way that avoids using the sumation symbol, similar to the binary case where the Hessian is $\sum_{i=1}^{m} \hat{y_i}(1-\hat{y_i})x_i^T x_i$ and we Vectorize it as: $$X^T Diag(\hat{y}\cdot(1-\hat{y})) X$$ In the multiclass case this same formula does not work with the "Diagonalization" part because $Diag(\hat{y}_i)-\hat{y}_i^T \hat{y}_i$ is a $(C \times C)$ matrix for each example as oposed to a scalar in the binary case.
