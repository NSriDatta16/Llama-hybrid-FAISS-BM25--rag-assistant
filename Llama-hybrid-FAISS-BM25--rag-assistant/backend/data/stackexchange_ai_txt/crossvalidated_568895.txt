[site]: crossvalidated
[post_id]: 568895
[parent_id]: 
[tags]: 
Machine learning regularization parameter lambda proof

Consider the regularized empirical risk minimization problem L(x) + λ * r(x), where L(x) is the empirical risk, r(x) is the regularizer, and lambda is the regularization hyper-parameter. I have 2 cases (where λ1 L(x) + λ1* r(x) where x1 is the minimizer meaning that L(x1) + λ1 * r(x1) ≤ L(x) + λ1* r(x) for any x L(x) + λ2 * r(x) where x2 is the minimizer meaning that L(x2) + λ2* r(x2) ≤ L(x) + λ2* r(x) for any x. Intuition suggests that as λ increases, r(θ) decreases while L(θ) increases. How can I prove this mathematically? More specifically: a) How can I show that increasing λ will never make our regularization error larger, i.e. r(x1) ≥ r(x2) ? b) How can I show that increasing λ will never decrease our training error, i.e. L(x1) ≤ L(x2) ? Thanks!!
