[site]: crossvalidated
[post_id]: 592974
[parent_id]: 592936
[tags]: 
The number of trees is a key hyperparameters. Keeping all other hyperparameters constant, you usually have that too many trees = overfitting, too few trees = underfitting, and just the right number of trees = just the right level of regularization. We should not look at the training loss (which sounds like what you are doing), which will of course be able to decrease more and more as we overfit more and more. As long as no two records with different target values have the same exact features, you can eventually fit a XGBoost tree ensemble perfectly to the training data. However, that's not normally what we want to achieve, but we rather tend to be interested in the generalization ability of the model (i.e. prediction on new data). To assess something more like that loss on a validation set (or from cross-validation) tends to be a better thing to look at. For that, you should usually see a decreasing loss, then it should stabilize and eventually rise again (if you don't see that, yet, you just need to add even more trees). The way one normally tends to tune two of the key hyperparameters, namely, learning rate (aka eta) and number of trees is to set the learning rate to a low value (as low as one can computationally afford, because low is always better, but requires more trees), then do hyperparameter search of some kind over other hyperparameters using cross-validation, while determining the number of trees using early stopping. I.e. as you train, you look at the cross-validation performance as you add more and more trees, but you keep going until loss very clearly is just going up. At that point, you can then go back and see at what number of trees the loss was best across CV samples and that's a sensible number of trees.
