[site]: crossvalidated
[post_id]: 303646
[parent_id]: 98983
[tags]: 
I had a similar question myself and tried reasoning out the advantage of gradient descent than GA with a slightly different perspective than an intensive mathematical analysis like above - One of the best use-case of "Neural nets" is that they have an inherent advantage of learning from hierarchies. And this is true for MLPs, ConvNets or RNNs. This feature of neural nets enable us to do "Transfer learning" wherein we just plugin the trained weights from any layer in a trained network (vgg16 inception etc.,). The slow process of "back-propagation and gradient descent" in my opinion, helps the network, during training, to learn such optimal hierarchies and underlying patterns. This in-turn makes the underlying layers do feature extraction and enables us to apply transfer learning as needed. I relate this entire process to an "organic learning process" just like the way we humans typically learn. Now, assuming we arrive at the optimal weight coefficients using GA or (some other technique), we are interfering with the organic learning process and the optimal learning hierarchies may get disturbed in the process leading to an over-fitting on the trained data with high variance on unseen data.
