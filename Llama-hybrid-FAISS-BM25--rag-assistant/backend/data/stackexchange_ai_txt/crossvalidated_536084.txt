[site]: crossvalidated
[post_id]: 536084
[parent_id]: 
[tags]: 
Can I compare 2 models by t-testing on the f-score?

In the neural network project, I used hold-out validation, repeated the experiment 10 times on the test set, and got 10 f-scores. Then experimented the baseline method 10 times too, and get another 10 f-scores. Can I do the t-test on 2 groups of f-scores and say that my technique is statistically better? Additional description: I didn't use k-fold cross-validation, I just used hold-out. In order to eliminate the influence of the randomness of the neural network model, I repeated the experiment 10 times. I saw many papers that only report average f-score. Since the difference between the average f-score is not obvious, I did a t-test on them. The problem is that I donâ€™t know if this approach makes sense, or like other papers, just report the average value is OK? Additional description: I first divide the data set into train and test with a ratio of 80:20. After training on the training set, use the results of the test set to calculate the f-value. Then I repeated the training and testing 10 times and got 10 f-values (the same dataset for each experiment). Additional description: Since the initial weights of the neural network model may cause different results, so I retrain the model from scratch 10 times using different random weights, and then calculate the results on the test set.
