[site]: crossvalidated
[post_id]: 64411
[parent_id]: 2691
[tags]: 
I'll give a non-mathy response and a more detailed birds-eye view of the motivation-through-math in the second part. Non-Mathy: The non-math explanation is that PCA helps for high dimensional data by letting you see in which directions your data has the most variance. These directions are the principal components . Once you have this information you can then, in some cases, decide to use the principal components as the meaningful variables themselves, and vastly reduce the dimensionality of your data by only keeping the principal components with the most variance ( explanatory power ). For example, suppose you give out a political polling questionnaire with 30 questions, each can be given a response of 1 ( strongly disagree ) through 5 ( strongly agree ). You get tons of responses and now you have 30-dimensional data and you can't make heads or tails out of it. Then in desperation you think to run PCA and discover the 90% of your variance comes from one direction, and that direction does not correspond to any of your axis. After further inspection of the data you then conclude that this new hybrid axis corresponds to the political left-right spectrum i.e. democrat/republican spectrum, and go on to look at the more subtle aspects in the data. Mathy: It sometimes helps to zoom out and look at the mathematical motivation to shed some light on the meaning. There is a special family of matrices which can be transformed into diagonal matrices simply by changing your coordinate axis. Naturally, they are called the diagonalizeable matrices and elegantly enough, the new coordinate axis that are needed to do this are indeed the eigenvectors. As it turns out the covariance matrix are symmetric and will always be diagonalizeable ! In this case the eigenvectors are called the principal components and when you write out the covariance matrix in eigenvector coordinates, the diagonal entries (the only ones left) correspond to the variance in the direction of your eigenvectors. This allows us to know which directions have the most variance. Moreover since the covariance matrix is diagonal in these coordinates, you have cleverly eliminated all correlation between your variables. As is common in practical applications, we assume that our variables are normally distributed and so its quite natural to try and change our coordinates to see the simplest picture. By knowing your principal components and their respective eigenvalues (variance) you'll be able to reduce the dimensionality of your data if needed and also have a quick general summary of where the variation in your data lies. But at the end of the day, the root of all this desirability comes from the fact that diagonal matrices are way easier to deal with in comparison to their messier, more general cousins.
