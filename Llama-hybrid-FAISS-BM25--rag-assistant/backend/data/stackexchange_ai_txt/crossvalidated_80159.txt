[site]: crossvalidated
[post_id]: 80159
[parent_id]: 
[tags]: 
Asymptotic distribution for t-test with time series

I once read the following statement: Given the following equation $ Y_t= c + a Y_{t-1} + \theta_t$ If $c\neq 0$, then the t-ratio for testing $H_0 : a=1$ is asymptotically normal. If $c=0$, then the t-ratio for testing will converge to another nonstandard asymptotic distribution. But I do not know how to get the above two conclusions.
