[site]: datascience
[post_id]: 6461
[parent_id]: 6459
[tags]: 
I believe the question is, you want to learn from musical pieces and try to generate a tune from the trained instance. Lets see if I can set up a simple model to do this, and then you can extrapolate from there. So, MFCC is a good feature when working with sound. You can use that to extract the features from lets say 1-2 second windows of your song. You now have a fingerprint for the audio file. Take a look at Conditional Restricted Boltzmann Machines . They are Neural Networks which use multiple binary states to encode time series information. As you can see in the webpage, they trained on human-gait data and can now generate their own human gait. This is essentially what you want but for music files. So you can train CRBMs on the Audio MFCC vectors that you have. After the training is done, to generate an audio file you can either "seed" the CRBM with a few seconds of some melody or just randomly initialize it. Then just allow the CRBM to go nuts and record whatever it produces. This is your new audio file. To produce another sample use a different seed. This solves the question of how you can implement a "melody" generation scheme. There are of course variations. You can add other features to you vector apart from MFCC. You can also use other time series predictors like LSTM or Markov models. All of this being said, the problem of generating music might be much more nuanced than it looks at first glance. Machine Learning algorithms just apply previously learned patterns in the data. How does that correspond to "creating" new music , is a philosophical question. If we analyze the aforementioned algorithm, essentially the CRBM will generate a next output based on the probability distribution that it has learnt. It would be very interesting to see what kind of output it generates when the said distribution is that of musical notes.
