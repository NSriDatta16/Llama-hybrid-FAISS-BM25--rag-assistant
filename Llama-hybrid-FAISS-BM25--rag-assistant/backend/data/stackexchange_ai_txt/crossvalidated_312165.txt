[site]: crossvalidated
[post_id]: 312165
[parent_id]: 312013
[tags]: 
The prior appearing in the loss function is a general idea not limited to linear regression. It is simple to understand. Assume you do some Bayesian inference with a prior $p(\theta)$ and a likelihood $l(x,\theta)$. The the posterior is: $$P(\theta,x)\propto p(\theta)l(x,\theta)$$ Assume you want to find the mode (maximum) of the posterior (or equivalently its $\log$) as an estimator for $\theta$. You can just write: $$\log(P(\theta,x))= \log (p(\theta))+\log(l(x,\theta))+c$$ That's how you get a two part loss function. This works for any kind of model, not only linear regression. Another classical example is using a Laplace prior and this is known as $L^1$ regularization. Gaussian prior is $L^2$ regularization. With a Gaussian prior and Gaussian model (linear regression), the posterior is Gaussian and thus the mode happens to be the same as the mean. That's why this "mode estimator" is especially natural in Bayesian linear regression.
