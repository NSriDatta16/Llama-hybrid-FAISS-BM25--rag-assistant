[site]: crossvalidated
[post_id]: 149057
[parent_id]: 
[tags]: 
Implement a Bigram Latent Dirichlet Allocation (LDA) for Topic Modeling

I'm trying to implement Latent Dirichlet Allocation (LDA) on a bigram language model. This is described in Topic Modeling: Beyond Bag-of-Words by Hanna Wallach et al. I'm trying to easily implement this idea using the current LDA packages (for example python lda.lda). Here is the idea I thought of: Normally we introduce lda.fit(X) where X is a DxN bag of words matrix (D is number of documents, N is number of words in document, and each xij is the count for word j in document i). Instead we could introduce lda.fit(Y) where Y is a DxL bag of unigram and bigram words matrix (D is number of documents, L is addition of number of words and number of bigram options in document. Each yij is the count for word/bi-word j in document i). Will the rest of the algorithm work the same, and output a list of topics with a probability distribution of unigram and bigram words? Do you think this will work? Do you have any other idea for implementing bigram LDA?
