[site]: crossvalidated
[post_id]: 437183
[parent_id]: 
[tags]: 
Why would my random forest create a model that has a worse F1 than one of the features has on its own?

I am trying to detect a rare event (eventA) using a random forest in R. One of my features is itself an event (eventB). If eventB happens it should cause eventA, so it is a really good predictor. However, this is not the only way to cause eventA, and the eventB feature is actually incorrect about half of the time. If i predict that eventA happens every time eventB happens, I get about 30% recall and 50% precision. If I feed eventB into a random forest model with a few other predictors I end up with lower recall and precision from the model. Why does this happen? Is the only explanation that the trees where eventB isn't selected in the feature set are so much worse at predicting eventA that the model ends up worse overall?
