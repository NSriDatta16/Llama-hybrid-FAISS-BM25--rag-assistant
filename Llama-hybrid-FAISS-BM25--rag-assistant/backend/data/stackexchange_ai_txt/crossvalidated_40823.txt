[site]: crossvalidated
[post_id]: 40823
[parent_id]: 40792
[tags]: 
If your analyses are at this stage exploratory then I strongly advise that you do not do any 'corrections' for multiple comparisons at all. They are mostly important where hypotheses are being tested within the Neyman-Pearson error-decision framework which implicitly precludes the re-testing of hypotheses. (That aspect of N-P hypothesis testing is usually not emphasised, but it is exactly what 'inductive behaviour' requires.) If you are at the exploratory stage then there is no reason at all to decrease the sensitivity of your analyses by 'correcting' via a Bonferroni or other method. For hypothesis formation there is also no good reason to use a low cutoff for 'significance', which will lower the power of the tests. Instead, consider examining (and presenting, if appropriate) the actual P-values. The comparisons with the lowest P-values are the most convincing and should be focussed on in new experiments. (You cannot, of course, use the same data to form and test a hypothesis.) You might find a recent paper of mine in the British Journal of Pharmacology helpful for understanding the differences between N-P hypothesis testing and the more commonly useful Fisherian 'significance testing'. http://www.ncbi.nlm.nih.gov/pubmed/22394284 Another approach—probably better, but certainly more complex—is to use a multi-level Bayesian model that has the many comparisons built into it as model components. For many different types of problem such a model seems to do away with any reason to consider multiple comparison corrections. http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf This blog item might be relevant too: http://andrewgelman.com/2011/01/data_exploratio/ (I have to admit here that I do not completely understand the multi-level modelling, and I have never used it myself.)
