[site]: stackoverflow
[post_id]: 3428557
[parent_id]: 3428396
[tags]: 
I am attempting to read in a stream and save the read images to a zip file as this is going to be running over multiple days and would generate far too many individual files. I'm not convinced by this reasoning. The actual number of files should not really matter that much. You might lose on average 1/2 a (file system) disk block per file, but with terabyte disc drives available for a couple of hundred dollars, that's probably insignificant. But the more important problem is what happens if your application ... or the power goes off. If you write all of your images straight into a ZIP file, the chances are that you will end up with nothing but a corrupt ZIP file for a multi-day run. I expect that the ZIP file contents will be mostly recoverable, but only using some third party (non-Java) application. If file system resources (disk space, number of inodes, whatever) are a realistic concern, then maybe you should write a script to run (say) once an hour and ZIP up the files that were written in the last hour and (maybe) put the ZIP file somewhere else.
