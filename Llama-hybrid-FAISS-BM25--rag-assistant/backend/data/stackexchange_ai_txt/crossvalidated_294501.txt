[site]: crossvalidated
[post_id]: 294501
[parent_id]: 
[tags]: 
Training LSTM with and without resetting states

I'm quite new to deep learning and Keras and I want to know what is the difference between these two training methods of an LSTM RNN. 1: for i in range(10): #training model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=0, shuffle=False) model.reset_states() 2: model.fit(trainX, trainY, epochs=10, batch_size=batch_size, verbose=0, shuffle=False) In both cases, doesn't the network train 10 times over the whole dataset? I realize that in example one we can reset the state in each data batch iteration but even if I delete the reset instruction the results are quite different. I'm confused.
