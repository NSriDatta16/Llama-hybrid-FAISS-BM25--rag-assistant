[site]: crossvalidated
[post_id]: 18272
[parent_id]: 18271
[tags]: 
It sounds like you're carrying out a cluster analysis on a dataset that has a pretty large number of variables; having difficulty obtaining good results because of the large number of variables (the curse of dimensionality, as you mention); and you're considering using an optimization technique such as simulated annealing to carry out a search through your variables to discover whether you might be able to use just a subset - is that right? If so, that activity is typically called feature selection (sometimes feature extraction), and there's plenty of literature out there that describes how you might approach it. Feature selection involves selecting a subset of the original variables, and is not quite the same as dimensionality reduction, which typically involves creating a small number of linear combinations of the original variables that summarise them (this is what a technique such as PCA or SVD does). A suggestion I might give is to note that you're trying to search through what is a discrete space (the power set of your variables). Simulated annealing, as an optimization technique, is in my experience more easily applied to searching through continuous spaces. This is particularly true of the implementation in MATLAB Global Optimization Toolbox (since I note you added the MATLAB tag). If you're using MATLAB for this, I'd suggest that a genetic algorithm might be easier to adapt to searching through discrete spaces. I wrote an article for MATLAB Digest a while ago that applies genetic algorithms to a related problem (classification rather than cluster analysis), which comes with example code. You might find that it's possible to adapt that code to your needs. The article carries out feature selection on a classification problem though, so it's maximizing classification accuracy - you'd need to provide a clustering metric for the algorithm to optimize, such as separation, heterogeneity, or a gap statistic. Hope that helps!
