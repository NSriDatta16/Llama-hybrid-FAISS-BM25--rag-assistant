[site]: crossvalidated
[post_id]: 225021
[parent_id]: 224898
[tags]: 
I think it is correct to say that "principal components are linear combinations of the original features". You consider a $m\times n$ data matrix $A$ with $m$ data points in the $n$-dimensional space that has rank $r The problem in your argument is that in this case PCA will not extract $n$ principal components; it will only extract $r$ of them. The $n\times n$ covariance matrix $C$ will be of rank $r$, meaning that it will have $n-r$ zero eigenvalues. If we do its eigenvector decomposition $C=VSV^\top$, the diagonal matrix $S$ will have these zeros in it. I would only call "principal components" those eigenvectors (columns of $V$) that correspond to non-zero eigenvalues. The ones that correspond to zero eigenvalues do not deserve to be -- and are not -- called principal components. This resolves the contradiction. Note that this is not my personal interpretation, it is the standard terminology usage; see e.g. Why are there only $n-1$ principal components for $n$ data points if the number of dimensions is larger or equal than $n$? Edit in response to the comment: Regarding what exactly is called "principal component" please see my answer here What exactly is called "principal component" in PCA? . Whatever your personal preference is, there are only $r$ principal components if the rank of the data matrix is $r$.
