[site]: crossvalidated
[post_id]: 496010
[parent_id]: 
[tags]: 
Propagating uncertainty into integrated predictions using a confusion matrix

I've built a remote sensing-based land cover classifier using Random Forest. The model predicts whether a satellite pixel has been irrigated or not. While it has good overall accuracy, when I weight the cross validation data set according to training data distribution (about 2% irrigated), the imbalance leads to low precision, as can be seen in the confusion matrix: prediction irr not-irr reference irr 185 2 not-irr 136 9679 Our results imply that even with a low rate of false positives (predicting non-irrigated as irrigated), the extent of the non-irrigated area in our prediction domain will lead to a significant total area of false positive predictions. Further, the low rate of false negatives shows we're likely not under-predicting irrigation (good recall). I'd like to propagate this error into integrated area estimates at scale, and give some sort of 'error bar' representation that reflects the fact we are probably overestimating irrigation, but unlikely to greatly underestimate it. I've had trouble finding this type of analysis in the land cover classification literature, and post here in the hopes someone might be able to offer some advice. See our initial work: https://www.mdpi.com/2072-4292/12/14/2328
