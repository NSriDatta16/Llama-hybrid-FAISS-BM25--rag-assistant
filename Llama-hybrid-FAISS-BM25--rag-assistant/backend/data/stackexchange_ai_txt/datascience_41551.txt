[site]: datascience
[post_id]: 41551
[parent_id]: 
[tags]: 
Sparse Representation vs Dense Representation

The one of the benefits of ReLUs is sparsity. Sparsity arises when aâ‰¤0 (a = wX+b). The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations. Is it true that sparse representation is more beneficial that dense representation, especially for Neural Networks?
