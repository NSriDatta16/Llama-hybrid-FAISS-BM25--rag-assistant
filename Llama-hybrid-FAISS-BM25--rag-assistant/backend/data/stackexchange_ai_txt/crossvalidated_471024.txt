[site]: crossvalidated
[post_id]: 471024
[parent_id]: 
[tags]: 
Local Length Scale Kernel in Gaussian Processes (Nonstationary)

I've been working on Gaussian Processes and came across a rather nice local length scale kernel proposed in "Nonstationary Gaussian Process Regression Using Point Estimates of Local Smoothness" from ECML 2008. The idea is simple - to model non-stationarity you need varying lengthscales. The authors propose modelling the length scale itself as a GP over the input space, thus creating a hierarchical inference model, wherein you first learn the lengthscale as a function over your space and then you learn the target variable as a function over your space. It turns out that the predictive distribution is not analytically tractable and that the authors use some approximations for the same. My question is the following: a) If you are approximating the Log Likelihood with only the first level of inference, without even going into the second level, one could as well learn any ML algorithm, not specifically a GP to learn the lengthscale as a function of the space. Why model it as a GP afterall? PS. I am just a beginner in Bayesian Machine Learning and do not have any expertise. I happen to learn a little bit about GPs from Rasumussen and Williams and found them interesting.
