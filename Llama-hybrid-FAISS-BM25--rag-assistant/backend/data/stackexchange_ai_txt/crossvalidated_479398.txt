[site]: crossvalidated
[post_id]: 479398
[parent_id]: 
[tags]: 
Squared Error Loss for Bayesian estimator of Normal distribution

I'm following Brad Efron and Trevor Hastie book "Computer Age Statistical Inference" ( link ). In chapter 7, they begin to debate the James-Stein estimator by calculating the Bayes rule, or Bayes posterior mean: \begin{align} \mu &\sim N(M, A) \\ x|\mu &\sim N(\mu, 1) \\ \mu | x &\sim N(M + B(x-M), B) \\B&= \frac{A}{A+1}. \end{align} They state that: \begin{align} \mathbb E\left[\left(\hat\mu^\textrm{Bayes} - \mu\right)^2\right] &= B \\ \mathbb E\left[\left(\hat\mu^\textrm{MLE} - \mu\right)^2\right] &= 1. \end{align} My question is how exactly they calculated the expectation? I think for the MLE it's expectation with regards to $x|\mu,$ then it's simply the variance = 1; but I'm not sure how you get a $B$ in the Bayes case. You do get a $B$ if you take the expectation w.r.t. to $\mu|x.$
