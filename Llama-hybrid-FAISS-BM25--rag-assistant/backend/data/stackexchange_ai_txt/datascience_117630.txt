[site]: datascience
[post_id]: 117630
[parent_id]: 117609
[tags]: 
It is not true that most RL algorithms are to estimate the Q function. A famous RL algorithm is the policy-gradient method which does exactly what you mentioned. There is a neural net that takes in state and outputs distribution over what action should be taken next. The parameters of this network are then trained via policy gradient estimates, so this network is never explicitly modeling the Q-function or the predicted rewards at all. That said, the variance of these policy gradients tends to be extremely high, so in practice people use Actor-Critic training which optimizes both the policy network and a Q-network jointly to stabilize training.
