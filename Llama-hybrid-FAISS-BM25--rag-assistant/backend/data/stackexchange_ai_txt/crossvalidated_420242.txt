[site]: crossvalidated
[post_id]: 420242
[parent_id]: 420140
[tags]: 
The answer above really nails it. I'd just like to add that it is worth separating the idea of wanting "normality" vs. wanting to scale all features to be on the similar range (even if they have different distributions). Both of these transformations have their pros and cons, and sometimes are necessary to avoid numerical quirks in the optimization step or avoid systemic biases in these algorithms. Also, it depends what type of "machine learning" you're referring to (i.e., SVMs, tree-based models, neural nets, etc..), as these all behave differently and may have different numerical issues. As mentioned above, there are benefits in certain situations, but the idea that normalizing skewed data will lead to better performance is not a bullet-proof strategy. In general, justifying any "pre-processing" or "data manipulation/transformation" steps tends to be a more robust alternative.
