[site]: datascience
[post_id]: 80772
[parent_id]: 80530
[tags]: 
I'd like to confirm that this situation is not really something to worry about and I do not overfit the data. No, the situation is not worrying, you can consider it worrying when the test error starts increasing. Normally an optimal model is a bit overfitted. decision-tree based techniques always drill down the training data Yes, your intuition is right. The whole goal of bagging in Random Forest is to avoid the overfitting of an individual decision tree. I believe gradient boosting techniques are also known from drilling down the training dataset pretty deep and even with low learning rate we can't help it. A smaller learning rate will lead even more to overfitting after enough iterations. Same intuition as gradient descent. XGBoost stopped training around 600th epoch due to early stopping I am not sure if 'epoch' is the right term here (I might be wrong), but could be more iterations, as it is referring to the number of trees in the boosting ensemble. The figure What metric are you plotting? Are you sure it is a representative metric? What happens if you plot the rest of the binary classification metrics?
