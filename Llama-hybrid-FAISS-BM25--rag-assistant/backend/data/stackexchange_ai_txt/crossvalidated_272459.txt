[site]: crossvalidated
[post_id]: 272459
[parent_id]: 272411
[tags]: 
It's a pretty general question, I'll try to lay out the main ideas in a simple manner. There are a lot of good resources which you can use for further reading, one which I can recommend is Shai Shalev-Schwarz "Understanding Machine Learning" which focuses on the theoretical foundations for machine learning. Put very simply, the idea in machine learning is to be able to learn (for example, a classifier) given a set of labeled examples ("training set"), and then use that classifier to also classify new data ("test set"). The goal is to do well on the unseen test data - this is known as "generalization". Probably the most natural way to accomplish the above task is to choose a classifier that performs best on the training data. This is what is known as ERM (empirical risk minimization). But is that always a good strategy? As it turns out, the answer is no. Suppose we are given the following training data: points in blue belong to class 1, and points in red belong to class 2. Our goal is to learn a classifier that "separates" them (i.e can classify a new example to one of the classes). Then, if I were to follow the ERM rule, I would choose the classifier denoted in green: It achieves an accuracy of 100% on the training data (no example is mis-classified). But is this really what we wanted? We learned a very complex model, but most of the chances are that the data we got was a little noisy. With ERM, we essentially "learned" the noise, instead of ignoring it. If we were now to receive new test data from a similar distribution, we are likely to make mistakes. This is the phenomena of overfitting: We fitted out training data very well ( too well), at the cost of performing badly on test data. Essentially, we hurt our ability to generalize! If, on the other hand, we are willing to not perform perfectly on the training set, than we can actually do better on test data (see the black classifier in the above image). It's a little surprising when you encounter it for the first time. The transition from the green classifier to something that resembles the black classifier can be achieved by introducing regularization - you've probably encountered the R-ERM (regularized empirical risk minimization), but this is already another subject.
