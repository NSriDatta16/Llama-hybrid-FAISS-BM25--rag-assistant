[site]: datascience
[post_id]: 61733
[parent_id]: 
[tags]: 
Class Imbalance and Cost-Sensitive Learning XGBoost

I'm fairly new to data science and machine learning and have been trying to read a bit more on methods like boosting for one of the projects I am working on. The investigator on this project is interested in utilizing xgboost for this project, but there are a couple of main issues that we anticipate once we get the data. There will be class imbalance, as in way fewer 1's than 0's. I've seen that people handle this by using the scale_pos_weight parameter in xgboost , but from my understanding, xgboost allows for custom objective functions. I was wondering how people come up with custom objective functions and the basis behind their chosen objective function. I'm interested in a customized objective function that penalizes false negatives more and was wondering if there's anything out there on the web that's recommended. We are dealing with a classification problem, so by default we would be using a log-loss function. I am hoping there is some sort of custom log-loss function out there that is commonly used. I apologize if I am using the wrong terminology, as I'm still very new to this. Thanks so much!
