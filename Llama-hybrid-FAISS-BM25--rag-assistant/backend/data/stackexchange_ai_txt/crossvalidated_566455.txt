[site]: crossvalidated
[post_id]: 566455
[parent_id]: 
[tags]: 
What alternative techniques are available to perform variable selection with big data?

I have a 88x6500 dataset of people income, where 88 are time periods (quarters) and 6500 are people. My independent variable is X = income and my dependent variable is Y = average houses price in a country. I am studying how the income of the richest people of a country affects its average houses prices. I would like to perform variable selection and filter/select the richest people in my dataset: my intention is to employ principal components analysis, but since I am not a statistician I have some questions: Is there any specific rule (even of thumb) to pick the number of principal components? i.e. if I know that the first 1000 Are there any other similar or more accurate techniques to perform variable selection? It should be similar to the logic of PCA and other than ridge, lasso, elastic net regression or best subset selection; Does it make sense in this case to make a supervised principal component analysis including the dependent variable in this case? Since I have many missing values, excluding people with more than 10 yrs of missing values and using missing imputation techniques (such as linear interpolation or with a constant) for other people could invalidate the results? Is there any computational problem with a so huge dataset (I am working with MATLAB and R)? In particular regarding the number of predictors being extremely higher than the time series dimension. Since the unit of measurement of the income is the same across the dataset (thousand of dollars for people income people and for the average house prices) and since I am focused on the richest people in the sample (so demeaning maybe does not make much sense), can I skip data standardization without any complications in interpreting the results?
