[site]: crossvalidated
[post_id]: 444503
[parent_id]: 444464
[tags]: 
It depends on how you build your prediction model. For example, suppose you have a model that says: for region A the score is 1 and for all other regions the score is zero. Then your model will correctly predict the score for the first two people and incorrectly for the others. Or you can have a model that predicts for region A a score of 0.95, for region B a score of 0.75, etc (I am looking at the midpoint in the distribution of scores by region). There are several metrics that you can use to determine how well your predictions fit the actual data. For example, you can use mean squared error (MSE) $$MSE = \frac{1}{n}\sum_{i=1}^n(Y_i-\hat{Y_i})^2,$$ Where error $(Y_i-\hat{Y_i})$ is the difference between the actual $(Y_i)$ value and your prediction $(\hat{Y_i})$ . Since you might overpredict or underpredict the true scores, the errors could be either negative (the true value is lower than your prediction, for example, the true score is 0.8 while you predicted 0.9) or positive (the true value is higher than your prediction). The errors are squared because ultimately with MSE we do not care about the direction of the error (overpredict or underpredict), but whether it is close to the true value or not. As a final step you simply average over these squared errors. MSE and other metrics (for instance, $R^2$ ) can be automatically calculated by statistical software/package of your choice. You can also create your own metrics, it really depends on your research question and the nature of your data. Regarding your final question about hypothesis testing: you can do a linear regression with your data (where Y is the score and Xs are all your explanatory variables, such as region) and look at the p-values of the explanatory variables.
