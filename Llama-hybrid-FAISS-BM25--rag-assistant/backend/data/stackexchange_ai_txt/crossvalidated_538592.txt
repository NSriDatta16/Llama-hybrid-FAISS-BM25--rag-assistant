[site]: crossvalidated
[post_id]: 538592
[parent_id]: 538401
[tags]: 
This thread really concerns an answer in search of a question, which makes it especially difficult to formulate a response. I can, however, provide one derivation leading to statistical interpretations of (1). Suppose $X$ is a random variable and you want to analyze something like the expectation of $|X|^{-p}$ for $p \ge 1.$ As we know, this can be problematic when $X$ has some probability near $0,$ in the sense I described at https://stats.stackexchange.com/a/299765/919 , because values of $|X|^{-p}$ "blow up" near $0.$ Suppose, though, that (like Normally distributed random variables), $X$ is absolutely continuous in a neighborhood of $0$ with a cumulative distribution function $F.$ Let's explore the singularity at $0$ by "subtracting off infinity." Specifically, writing $\mathrm{d}F(x) = f(x)\,\mathrm{d}x$ for the density, consider the expectation-like integral $$H_f(\epsilon;p) = \int_{-\epsilon}^\epsilon |x|^{-p}\left(f(x) - f(0)\right)\,\mathrm{d}x.$$ This probes the behavior of the expectation within an arbitrarily small interval around its singularity. Ordinarily this integral would have to be evaluated by splitting it into a negative and positive part and taking limits at the singularity $x=0,$ because the factor $|x|^{-p}$ makes it look like the integrand blows up there. However, if it is the case that the integrand is bounded for sufficiently small $\epsilon$ (which will happen when $f(x)$ approaches $f(0)$ sufficiently rapidly), then no actual singularity appears and this integral makes sense and yields a definite value. In this case, we may fold the positive and negative parts together in a single integral to produce $$H_f(\epsilon;p) = \int_0^\epsilon x^{-2}\left(f(x) - 2f(0) + f(-x)\right)\,\mathrm{d}x.$$ In the question, $p=2.$ Assuming the density $f = F^\prime$ is differentiable at $0,$ one application of L'Hopital's Rule gives $$\lim_{x\to 0} |x|^{-2} (f(x) - 2f(0) + f(-x)) = \lim_{x\to 0} \frac{f(x)-2f(0)+f(-x)}{x^2} = \lim_{x\to 0} \frac{f^\prime(x)-f^\prime(-x)}{2x}.$$ When $f^\prime$ is differentiable at $0,$ the numerator approaches $0,$ permitting another application of L'Hopital's Rule, $$\lim_{x\to 0} \frac{f^\prime(x)-f^\prime(-x)}{2x} = \lim_{x\to 0} \frac{2f^{\prime\prime}(x)}{2}.$$ (Equivalently, you may use Taylor's Theorem with Remainder to obtain the same result.) The existence of this limit is thus tantamount to supposing $f^{\prime\prime}$ is continuous at $0$ and gives, finally, $$\lim_{x\to 0} |x|^{-2} (f(x) - 2f(0) + f(-x)) = f^{\prime\prime}(0).$$ This shows that the integrand does not blow up near zero: subtracting $f(0)$ from $f(x)$ in the integration has removed the singularity. Thus, when $f$ is thrice differentiable in a neighborhood of $0,$ $$H_f(\epsilon, 2) = \int_0^\epsilon \left(f^{\prime\prime}(0) + o(x)\right)\,\mathrm{d}x = \epsilon f^{\prime\prime}(0) + o(\epsilon^2)$$ where the error term $o(\epsilon^2)$ is proportional to the third derivative of $f$ somewhere near $0.$ (The generalization to other integral values of $p$ should now be clear.) Having removed the singularity, let's consider a version of the original (infinite) expectation where in the formula $E[X^{-2}]=\int x^{-2}\mathrm{d}F(x)$ the probability element $\mathrm{d}F(x)$ is replaced by $\mathrm{d}(F(x) - f(0)x)$ $= (f(x)-f(0))\mathrm{d}x:$ $$\begin{aligned} E^{*}[X^{-2}] &= \int_\mathbb{R} x^{-2}(f(x) - f(0))\,\mathrm{d}x \\ &= \int_{\infty}^{-\epsilon} + \int_{-\epsilon}^\epsilon + \int_{\epsilon}^\infty x^{-2}(f(x) - f(0))\,\mathrm{d}x \\ &= \int_{\infty}^{-\epsilon} + \int_{\epsilon}^\infty x^{-2}f(x)\,\mathrm{d}x\ -\ f(0)\left(\int_{\infty}^{-\epsilon} + \int_{\epsilon}^\infty x^{-2}\,\mathrm{d}x\right) + H_f(\epsilon, 2)\\ &= \int_{\infty}^{-\epsilon} + \int_{\epsilon}^\infty x^{-2}f(x)\,\mathrm{d}x - \frac{2f(0)}{\epsilon} + \left(\epsilon f^{\prime\prime}(0) + o(\epsilon^2)\right) \\ &= \int_{\infty}^{-\epsilon} + \int_{\epsilon}^\infty x^{-2}f(x)\,\mathrm{d}x - \frac{2f(0)}{\epsilon} + o(\epsilon). \end{aligned}$$ When taking the limit, by definition the last $o(\epsilon)$ term will vanish. Thus we may ignore it, permitting the first two re-interpretations of the expression in the question: Formula $(1)$ in the question is the integral of $(f(x) - f(0))/x^2.$ If you like, when $F$ is absolutely continuous everywhere with nonvanishing density (such as any Normal distribution), this can be written as an expectation: $$\int_{\infty}^{-\epsilon} + \int_{\epsilon}^\infty x^{-2}f(x)\,\mathrm{d}x - \frac{2f(0)}{\epsilon} = E\left[\frac{1 - f(0)/f(X)}{X^2}\right] + o(\epsilon) = E^{*}\left[X^{-2}\right].$$ The latter statement follows upon substituting $f(x)-f(0) = (1 - f(0)/f(x)) f(x)$ in the definition of $E^{*}.$ As a practical demonstration of this second interpretation, I performed $100$ simulations with $\mu=2,$ $\sigma=1$ of $10000$ trials each. I generated $x_i$ according to a Normal $(\mu,\sigma)$ distribution (with density $f_{\mu,\sigma}$ ) and averaged the values of $(1-f_{\mu,\sigma}(0)/f_{\mu,\sigma}(x_i))/x_i^2.$ This is a standard Monte-Carlo estimate of $E^{*}\left[X^{-2}\right].$ The figure is a "funnel plot" of those $100$ Monte-Carlo estimates and their standard errors. On it I have drawn the value of $E^{*}$ in red, obtained via numerical integration, and an approximate value based on formula $(1)$ of the question with $\epsilon = 1/10$ as a dashed black line: those lines coincide and they coincide with the funnel's tip, which is the best of the Monte-Carlo estimates. This example shows that the two re-interpretations of $(1)$ are practical and statistically meaningful. # For the expectation: f $value e.2 value e.1 + e.2 - 2 * dnorm(0, mu, sigma) / abs(eps) }, "eps") # The simulation: n
