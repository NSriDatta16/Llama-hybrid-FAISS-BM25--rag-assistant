[site]: crossvalidated
[post_id]: 624391
[parent_id]: 72613
[tags]: 
I will add to Aleco's answer by mentioning a few different notations which are common in the literature of statistics and machine learning. The most common interpretation seems to be the second one in Aleco's answer, i.e. we use the marginal pmf/pdf of the mentioned variable, and sum/integrate over all possible values of the mentioned variable. The only text I found which explicitly mentions this notation is the PRML book [1], which mentions: Sometimes we will be considering expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance $$ E_{X} [h(X,Y)] $$ denotes the average of the function $H(X, Y)$ with respect to the distribution of X. Note that $E_{X} [h(X,Y)] $ will be a function of Y. As I understand this: If X & Y are discrete: $$ g(Y) = E_{X} [h(X,Y)] = \sum_{x \in Range(X)} h(x, Y) p_{X}(x) $$ If X & Y are continuous $$ g(Y) = E_{X} [h(X,Y)] = \int_{-\infty}^\infty h(x, Y) f_{X}(x) \, dx $$ Notice that both of these are functions of $Y$ , since we are "averaging" over all possible values of $X$ and multiplying by their respective probabilities. Another common subscript is $X \sim D$ , which means the same thing. By using this subscript, the author is trying to tell us that the r.v. X is distributed according to some probability distribution $D$ (pmf or pdf), and that the distribution is an important part of the equation: If X & Y are discrete: $$ g(Y) = E_{X\sim D} [h(X,Y)] = \sum_{x \in Range(X)} h(x, Y) D(x) $$ where $D(x) = p_{X}(x)$ is the marginal pmf of X. If X & Y are continuous: $$ g(Y) = E_{X\sim D} [h(X,Y)] = \int_{-\infty}^\infty h(x, Y) D(x) \, dx $$ where $D(x) = f_{X}(x)$ is the marginal pdf of X. [1] Pattern Recognition and Machine Learning, Bishop, pg 20 (equation 1.36)
