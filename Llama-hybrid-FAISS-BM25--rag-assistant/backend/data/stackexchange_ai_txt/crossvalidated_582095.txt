[site]: crossvalidated
[post_id]: 582095
[parent_id]: 582087
[tags]: 
There's a yes and a no to this. For the yes , that is called overfitting, and it is the enemy of machine learning practitioners. For the no , if the distribution of features is exactly the same for all levels of the outcome, then nothing will give you an ability to separate the distributions, not even a little bit. You might be able to overfit, just because the samples from those distributions are not identical, but that's all you have done; nothing can separate $N(0,1)$ from $N(0,1)$ , for instance. Back to the yes , however, machine learning might be able to find patterns that are real and useful, perhaps even highly predictive, even if they are unexpected.
