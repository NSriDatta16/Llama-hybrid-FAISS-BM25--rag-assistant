[site]: crossvalidated
[post_id]: 544047
[parent_id]: 
[tags]: 
Is "probability calibration" intended to improve the performance of a statistical model?

I was watching this video over here: https://www.youtube.com/watch?v=AunotauS5yI This video brought up an interesting point that I never knew had a specific term for (i.e. probability calibration). If I have understood this correctly: in the case of a supervised binary classification problem, the goal of probability calibration is to attempt to make sure that when the statistical model predicts a new observation with a higher probabilities - on average, higher prediction probabilities should more likely to result in correct predictions (e.g. see @ 6:04 in the video) Question: It seems that probability calibration models the prediction probabilities of the trained model vs the true labels. Is this more for diagnostic purposes? Or can doing this actually improve the statistical model? Has anyone ever attempted to do this before? I found a way to do this using the R programming language: https://rdrr.io/cran/rfUtilities/man/probability.calibration.html
