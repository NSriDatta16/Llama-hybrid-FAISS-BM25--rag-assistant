[site]: crossvalidated
[post_id]: 325164
[parent_id]: 325136
[tags]: 
This is a kind of question that the whole 600-page statistics textbooks are written about. In this limited space let me give you some bullet points. 1] Everything begins with exploratory data analysis. You visualize the data, in 2D, 3D, run some simple (non-)parametric tests and form your opinion about what's going on. You build your intuition. 2] You look at the sample size. If the ratio of observations to estimated parameters is 10 or 20 forget about CART or boosting algorithms. "Data mining" is called so for a reason. It's when you get dealt a huge data set, you are swimming in information and you have to make sense of it somehow. 3] Non-linear relationships between $Y$ and $X_1$, $X_2$, ..., $X_p$ can be captured by linear regression or lasso. You just add $X_1^k$, $X_2^k$, ..., $X_p^k$ and/or $log(X_1)$, $log(X_2)$, ..., $log(X_p)$ as predictors. 4] Many modeling frameworks must be tried for each problem. Within each of them, the "optimal model" must be identified using statistical significance of parameters or an objective model selection criterion. 5] To identify the optimal modeling framework (e.g. regression vs CART vs neural network), you use an objective model selection criterion, e.g. cross-validation , Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) . 6] "Goodness of fit diagnostics" must be run after whatever modeling decision you have made.
