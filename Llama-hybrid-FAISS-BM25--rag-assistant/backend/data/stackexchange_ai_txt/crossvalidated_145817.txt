[site]: crossvalidated
[post_id]: 145817
[parent_id]: 145803
[tags]: 
A classical test for a null hypothesis $H_0$ relies on a statistic $T$, the distribution of which under $H_0$ is known. However, the choice of the statistic depends on what you imagine to be likely for $H_1$, that is when $H_0$ is not true, or more generally on the general underlying model you assume for the data -- not only for the null, for all kinds of data you may observe. Let's take the example of the association between a binary variable $Y = 0, 1$, and a variable with three different levels $X = 0, 1, 2$ -- this is a classical case in genetics when studying association between a disease and di-allelic genetic marker. The null hypothesis is independence of $Y$ and $X$. Using the logistic regression framework, there are different natural models that can be used to test this: $$\def\logit{\mathop{\text{logit}}}\def\P{\mathop{\mathbb P}} \logit \P(Y=1) = \alpha + \beta_1 \mathbf 1_{\{X = 1\}} + \beta_2 \mathbf 1_{\{X=2\}},$$ and $H_0$ is $\beta_1 = \beta_2= 0$ ; $$\logit \P(Y=1) = \alpha + \beta X$$ and $H_0$ is $\beta = 0$. These two models lead to valid tests (you can compute a $p$-value which is uniformly distributed under $H_0$), but the outcome may be discordant for some observations. For the record, the score test derived under the second model is equivalent to a test on $2\times 3$ contingency table called the Armitage Trend Test; it is the default choice in genetics. This is just an example: there are many other situations where you can propose different tests for the same null hypothesis. Another classical example would be parametric versus non-parametric tests, which most of the time means assuming normality or not.
