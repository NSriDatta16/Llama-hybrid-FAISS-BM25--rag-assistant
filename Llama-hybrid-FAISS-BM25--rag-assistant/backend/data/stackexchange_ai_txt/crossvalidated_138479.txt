[site]: crossvalidated
[post_id]: 138479
[parent_id]: 138477
[tags]: 
The more comparisons you do at some fixed significance level, the more chance of getting some significant results even if there's really nothing going on. Imagine we had 10 A groups being compared with each of 10 B groups. That's a total of 100 comparisons. Such a situation is sometimes referred to as multiple comparisons , which often come after rejection of some omnibus (i.e. overall) test. If we were doing tests at say the (say) 5% significance level and there were no differences between any of the pairs of populations, you'd expect on average around 5 to reach significance, just by chance . If the tests were independent, you could compute the chance of at least one rejection given that the null is true, however that gives an overestimate because the tests are not independent. Nevertheless the chance is still pretty high; it's not hard to compute via simulation (some simulations I did suggest something around 87% may be closer). Some people will attempt to guard against making type I errors by reducing the significance levels of the individual tests in one of a variety of possible ways (not always equally), in order that the overall rate of type I errors low. (Whether this makes sense depends on your situation; I think it's done more often than it really should be.) [If your type I error rate was chosen by minimizing a loss function -- by trading off the relative costs of the two kinds of errors, it may make more sense not to adjust at all, since you're already at the optimum - if you reduce your type I error rate it will be at the expense of "too many" type II errors by the criterion you just optimized] Other people attempt to control what's called the False Discovery Rate . That results in reducing the individual type I error rate less.
