[site]: stackoverflow
[post_id]: 4752315
[parent_id]: 
[tags]: 
Coercing float into unsigned char on ARM vs. Intel

When I run the following C code on an Intel machine... float f = -512; unsigned char c; while ( f %d\n", f, c ); f += 64; } ...the output is as follows: -512.000000 -> 0 -448.000000 -> 64 -384.000000 -> 128 -320.000000 -> 192 -256.000000 -> 0 -192.000000 -> 64 -128.000000 -> 128 -64.000000 -> 192 0.000000 -> 0 64.000000 -> 64 128.000000 -> 128 192.000000 -> 192 256.000000 -> 0 320.000000 -> 64 384.000000 -> 128 448.000000 -> 192 512.000000 -> 0 However, when I run the same code on an ARM device (in my case an iPad), the results are quite different: -512.000000 -> 0 -448.000000 -> 0 -384.000000 -> 0 -320.000000 -> 0 -256.000000 -> 0 -192.000000 -> 0 -128.000000 -> 0 -64.000000 -> 0 0.000000 -> 0 64.000000 -> 64 128.000000 -> 128 192.000000 -> 192 256.000000 -> 0 320.000000 -> 64 384.000000 -> 128 448.000000 -> 192 512.000000 -> 0 As you can imagine, this sort of difference can introduce horrible bugs in cross-platform projects. My questions are: Was I wrong to assume that coercing a float into an unsigned char would yield the same results on all platforms? Could his be a compiler issue? Is there an elegant workaround?
