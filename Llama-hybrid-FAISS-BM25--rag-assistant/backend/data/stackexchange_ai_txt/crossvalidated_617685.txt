[site]: crossvalidated
[post_id]: 617685
[parent_id]: 617668
[tags]: 
One big problem comes to mind: even without censored observations, there is no single "death rate" (e.g., deaths per person at risk per year) unless there's an exponential survival curve. The hazard function is the continuous death rate over time, and it's far from constant over time for a lognormal distribution; see the NIST page for example plots of lognormal hazard functions. Without an exponential survival curve, any "death rate" based on dividing a number of deaths by a number at risk will depend on the particular time window involved. What exactly do you mean by a "death rate" in this context? How would you apply it in practice? In your situation this problem is exacerbated by omitting right-censored observations from the calculations. That necessarily introduces bias into the estimate, in a way that depends heavily on the censoring pattern in the underlying data. It's hard to think of a scenario in which omitting censored event times leads to anything other than trouble. Survival analysis done properly allows for other than exponential survival curves while handling censored event times. It allows for predictions of survival at specific times of interest, so that if you are particularly concerned about, say, early events you can focus attention on them. In terms of the sampling variability you want to capture (variability in survival-curve estimates arising from random sampling of the population) the covariance matrix of the coefficient estimates should contain the information you need. If the data set is large enough and the form of the model is adequate, then the asymptotic multivariate normality of the estimates can be used directly without resampling. I showed that at the end of my answer to the question you linked , where resampling from the survival distribution gave essentially the same coefficient covariance as the original model's coefficient covariance matrix. If the form of the model isn't adequate, then modeling on resampled cases might give an estimate of a type of variability: the variability in coefficient estimates in a model that doesn't properly fit the data. What's the point? You are better served by developing an adequate model. If the data set isn't large enough, you have to consider whether to trust the model at all. Your statement about "paucity of data" is thus troubling. It might be more useful to present the actual scenario that you are interested in, as I fear that the "paucity of data" will limit what can be accomplished.
