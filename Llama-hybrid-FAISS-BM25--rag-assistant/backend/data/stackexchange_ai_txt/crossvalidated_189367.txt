[site]: crossvalidated
[post_id]: 189367
[parent_id]: 189331
[tags]: 
From an optimization perspective it has some nice properties in terms of differentiability. For a lot of machine learning problems it's a good fit for 1-of-N classification. From a deep learning perspective: One could also argue that in theory, using a deep network with a softmax classifier on top can represent any N-class probability function over the feature space as MLPs have the Universal Approximation property.
