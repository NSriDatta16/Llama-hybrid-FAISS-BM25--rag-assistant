[site]: crossvalidated
[post_id]: 178917
[parent_id]: 178913
[tags]: 
You are conducting a median regression ( tau=0.5 ) on asymmetrically distributed data. Here is a simpler example to show what is going on. Suppose your asymmetric data are lognormal: set.seed(1) xx Then what you are doing amounts to finding the median of your data. median(xx) [1] 1.121518 Now, the median minimizes the sum of absolute errors . It will not minimize the sum of "raw" errors: sum(xx-median(xx)) [1] 52.74494 If you want a value that minimizes the sum of "raw" errors, you need to take the mean: mean(xx) [1] 1.648967 sum(xx-mean(xx)) [1] -9.992007e-15 So: if it is important to you that your fit yields zero average error, you will need to run an ordinary OLS regression. Which will, of course, be sensitive to outliers. (You incidentally found that the conditional mean is equal to the conditional 47% quantile. But that of course won't minimize absolute deviations.) There is no way to have both minimal absolute deviation and balanced residuals if your distribution is asymmetric. You can of course find a tradeoff between median and OLS regression, perhaps by taking an average, or by regularizing in some other way (lasso, ridge regression, elastic net).
