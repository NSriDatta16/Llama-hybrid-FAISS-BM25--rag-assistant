[site]: stackoverflow
[post_id]: 879235
[parent_id]: 878866
[tags]: 
Different networks behave in different ways as you exceed their bandwidth. Most of them have a succession of badness along the lines: Jitter will begin to shoot through the roof as some packets have to be queued or retransmitted (e.g., collisions on half-duplex ethernet or wireless). Average latency will go up slightly. As oversaturation continues (or at higher oversaturation levels) average latency will go through the roof as pretty much all packets are being queued or retransmitted. This may be limited if queue sizes are small. Packet loss will increase as queues overflow. The higher you drive the bandwidth, the more packets will be lost. Depending on hardware, jitter and latency may or may not go back down. If some form of QoS is in use, different packet streams may see these effects independently. E.g., you may be pumping 3x bandwidth on your app connection and see relatively little change in ping time. So you must measure with your application's packets. (1) and (2) may not occur on a given network. (3) will always occur, no matter what. All three can, unfortunately, also occur even when you're nowhere near the bandwidth limit.
