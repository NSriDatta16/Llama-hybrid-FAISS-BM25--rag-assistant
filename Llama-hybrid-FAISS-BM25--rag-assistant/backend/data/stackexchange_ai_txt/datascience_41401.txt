[site]: datascience
[post_id]: 41401
[parent_id]: 37061
[tags]: 
There is nothing like pre-training on MNIST data . You can take any advanced CNN architecture and then fine-tune the network on your dataset. For performing the one-shot learning, the need for fine-tuning or training is must as this will tune your model to understand and emphasise only on the images that the model has been trained upon. To simplify, a network trained on faces will be able to recognise only the facial features over other objects in the images which is must for generating the embedding. In one-shot learning , the network tries to generate embedding by forward propagation of the image data through the network. The generated embedding must only emphasis on the required part of the image and not onto the useless data in the image data. A pre-trained model in this case is chosen that is capable of selecting the features in a image that are required to be emphasised. Let's try to understand it through the example Face Recognition system. In this case, we would require to have a model that has been trained with images of faces so that whenever we try to create a new embedding, the model must know that it has to concentrate its weights on face rather than the noise in the image. Thus when the embedding are generated, the part of the face is concentrated and thus the cosine distances between the embedding could be minimized to uniquely identify the closeness among the 2 images. You can get a better understanding of such a network in this video by Andrew Ng.
