[site]: crossvalidated
[post_id]: 568691
[parent_id]: 568684
[tags]: 
No, this does not change the order of the variables. It is a one-to-one transformation from any range of values (ideally when no values are ever exactly the same, but in practice as long as there are not too many ties) to $(-\infty, \infty)$ (but with 95% of values between -1.96 and 1.96). However, note that this may often not be something that you want to do. The author of the paper appears to be confused on why such an approach would be needed. Any assumption with linear regression models are about the residuals of the model (additionally, depending on the situation, methods may be quite robust to deviations from normality in residuals: see all the literature and previous questions about assessing normality of residuals). What the author proposes will not work on the residuals of a model, but rather on the raw variables. In fact, the approach could make residual normality worse, e.g. when normality of residuals is already perfect, but there are two explanatory variable levels that have very different means (then you would never ever want to do a transformation of the outcome variable). Oddly, the author also does not seem to say whether they intend this just for dependent or independent variables, when for the latter it would often not make much sense for a linear model. One setting, in which this approach applied to independent variables is known to be potentially helpful, is when training neural networks for prediction purposes or denoising autoencoders. In that setting, it has been used in Kaggle competitions (first in the Porto Seguro challenge ) and lead to improved model performance.
