[site]: crossvalidated
[post_id]: 524339
[parent_id]: 
[tags]: 
(How) can model parameters be learnt using MCMC?

I get stuck by part (b) of figure 4 in this paper: Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users . In my understanding, inference algorithms like MCMC are not for training because the posterior of a variable should be fixed before being sampled. In that figure, MC-Dropout is also grouped into the learning stage. But I thought it is only used in the validation step. Can inference algorithms like HMC and MC-Dropout be applied for parameter learning using data? Or am I missing something? It would be highly appreciated if anyone could help me out. Thanks in advance. I learned from this tutorial that No-U-Turn Sampler(NUTS) is used to sample and get the distributions of four variables, but I don't know how the sample function works, what is going on under the hood? I know the mechanism of MCMC, but what confused me is that: could that process be the same if the network is much more complex than linear regression? For instance, a deep neural network? with basic_model: # draw 500 posterior samples trace = pm.sample(500, return_inferencedata=False) I have a deep-learning-stochastic-gradient-descent mindset and cannot get my head around the sampling mechanism(sample to get the variable distribution?). How are the variables "trained" by data(no matter how expensive)? Or they can only be analytically calculated/derived and then be sampled to plot their distribution?
