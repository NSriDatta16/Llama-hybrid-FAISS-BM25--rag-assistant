[site]: crossvalidated
[post_id]: 470786
[parent_id]: 470626
[tags]: 
The first 5 answers fail to distinguish between estimation loss 1 and prediction loss 2 , something that is crucial in answering the question. A priori, there is no reason that the two should coincide. I will discuss both types of loss in the context of point prediction using linear regression. The discussion can be extended to models other than linear regression and tasks other than point prediction, but the essence remains the same. Setup Suppose you are facing a prediction problem where the model is $$ y=X\beta+\varepsilon $$ with $\varepsilon\sim D(0,\sigma)$ , $D$ being some probability distribution with location $0$ and scale $\sigma$ . You aim to predict $y_0$ given $x_0$ , and your point prediction will be $\hat y_0$ , a function of $x_0$ , the data sample, the model and the penalty (the negative of reward) function defined on the prediction error. The penalty function you are facing is $L_P(y-\hat y)$ . It has a minimum at zero (the value $L_P(0)$ can be set to zero without loss of generality) and is nondecreasing to both sides of zero; this is a typical characterization of a sensible prediction loss function. You can freely choose an estimation loss function $L_E(\cdot)$ and a point prediction function $y_hat_0$ . What are your optimal choices for each? This will depend on the error distribution $D$ and the prediction loss function $L_P(\cdot)$ . Estimation loss Estimation loss specifies how parameter estimates of a model are obtained from sample data. In our example of linear regression, it concern the estimation of $\beta$ and $\sigma$ . You can estimate them by minimizing the sum of squared residuals (OLS) between the actual $y$ and the corresponding fitted values, sum of absolute residuals (quantile regression at the median) or another function. The choice of the estimation loss can be determined by the distribution of model errors. The most accurate estimator in some technical sense* will be achieved by the estimation loss that makes the parameter estimator the maximum likelihood (ML) estimator. If the model errors are distributed normally ( $D$ is normal), this will be OLS; if they are distributed according to a Laplace distribution ( $D$ is Laplace), this will be quantile regression at the mean; etc. *To simplify, given a ML estimator, you may expect more accurate parameter estimates from your model than provided by alternative estimators. Prediction loss Prediction loss specifies how prediction errors are penalized. You do not choose it, it is given. (Usually, it is the client that specifies it. If the client is not capable of doing that mathematically, the analyst should strive to do that by listening carefully to the client's arguments.) If the prediction error causes the client's loss (e.g. financial loss) to grow quadratically and symmetrically about zero, you are facing square prediction loss. If the client's loss grows linearly and symmetrically about zero, you are facing absolute prediction loss. There are plenty of other possibilities for types of prediction loss you may be facing, too. Prediction Given the parameter estimates of the model and the values of the regressors of the point of interest, $x_0$ , you should choose the point prediction $\hat y_0$ based on the prediction loss. For square loss, you will choose the estimated mean of $y_0$ , as the true mean minimizes square loss on average (where the average is taken across random samples of $y_0$ subject to $x=x_0$ ). For absolute loss, you will choose the estimated median. For other loss function, you will choose other features of the distribution of $y_0$ that you have modelled. Back to your question Why do people frequently choose square error rather than absolute error, or correspondingly square loss rather than absolute loss, as estimation loss ? Because normal errors ( $D$ being normal) are common in applications, arguably more so than Laplace errors ( $D$ being Laplace). They also make the regression estimators analytically tractable. They are not much easier to compute, however. Computational complexity of OLS (corresponding to ML estimation under normal errors) vs. quantile regression at the median (corresponding to ML estimation under Laplace errors) are not vastly different. Thus there are some sound arguments for the choice of OLS over quantile regression at the median, or square error over absolute error. Why do people choose square error, or correspondingly square loss, as prediction loss ? Perhaps for simplicity. As some of the previous answers might have mentioned, you have to choose some baseline for a textbook exposition; you cannot discuss all possible cases in detail. However, the case for preferring square loss over absolute loss as prediction loss is less convincing than in the case of estimation loss. Actual prediction loss is likely to be asymmetric (as discussed in some previous answers) and not more likely to grow quadratically than linearly with prediction error. Of course, in practice you should follow the client's specification of prediction loss. Meanwhile, in casual examples and discussions where there is no concrete client around, I do not see a strong argument for preferring square error over absolute error. 1 Also known as estimation cost, fitting loss, fitting cost, training loss, training cost. 2 Also known as prediction cost, evaluation loss, evaluation cost.
