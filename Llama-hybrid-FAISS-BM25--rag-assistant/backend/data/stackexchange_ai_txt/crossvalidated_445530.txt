[site]: crossvalidated
[post_id]: 445530
[parent_id]: 445453
[tags]: 
For me, the notion of what i.i.d really is and why it is, in many cases, a necessary assumption makes more sense from the Bayesian perspective. Here, instead of data being thought of as i.i.d in an absolute sense, they are though of as conditionally i.i.d. given model parameters . For instance, consider a normal model from the Bayesian perspective. We specify how we think data were sampled given the parameters: $X_i|\mu, \sigma^2 \stackrel{iid}{\sim} N(\mu, \sigma^2)$ for $i \in \{1, \ldots, n\}$ , and express prior belief on those parameters: $\mu \sim P(\mu)$ ; $\sigma^2 \sim P(\sigma^2)$ (the exact prior used is unimportant). Conditional independence has to do with the fact that the likelihood factorizes: $P(X_1, \ldots, X_n|\mu, \sigma^2) = P(X_1|\mu, \sigma^2)\ldots P(X_n|\mu, \sigma^2)$ . But this is not the same thing as saying that the marginal distribution on the data implied by our model factorizes: $P(X_1, \ldots, X_n) \neq P(X_1)\ldots P(X_n)$ . And, indeed, in our specific case of the normal distribution, getting the marginal distribution on the data by integrating out the parameters indeed yields a joint distribution which is not independent in general, the form of which will depend on which priors you specified. That is to say: two observations $X_i$ and $X_j$ are not independent; they are only conditionally independent given the model parameters (in math notation, $X_i \perp \!\!\! \perp X_j | \mu, \sigma^2$ but $X_i \not\perp \!\!\! \perp X_j$ ). A useful way to think about what the independence of two random variables means is that they do not provide any information about each other. It would be completely absurd to say that two data points don't provide any information about each other: of course the data are related in some way. But by making data conditionally independent given some parameters, we are saying that our model encodes the whole of the relationship between the data: that there's "nothing missing" from our model. Effectively, an i.i.d. assumption is an assumption that our model is correct: if we are missing something from our model, data will contain information about one another beyond what is encoded in our model. If we know what that is, we should put it into our model and then make an i.i.d. assumption. If we don't know what it is, we are out of luck. But that we have mispecified the model is a constant and unavoidable risk. And finally, a short note: at first glance, this framework I've described wouldn't seem to fit models such as spatiotemporal models where we have explicit dependence between data hard coded into the model. However, in all cases like this that I am aware of, the model may be reparameterized as one with i.i.d. data and additional (possibly correlated) latent variables.
