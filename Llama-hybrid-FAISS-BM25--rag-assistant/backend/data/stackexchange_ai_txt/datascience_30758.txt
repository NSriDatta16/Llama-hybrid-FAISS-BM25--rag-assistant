[site]: datascience
[post_id]: 30758
[parent_id]: 30214
[tags]: 
Reading through the theoretical part of the tutorial, I understand that ideally, in the Word2Vec Kares architecture, for each target word, you would input all the available context words in order to make the network learn (through reinforcement) the words typically appearing in the same context as the target words (the "positive examples" in your question) and reduce the similarity between the target word and words not typically appearing in its context (the "negative examples" in your question). You typically go through all the "positive examples" (they are not many) so that the corresponding words end up being placed in the same region of the 300-D vector space. However, going through all the "negative examples" would be a big computational burden due to their sheer amount (they are too many of them). So you "randomly" select some of them (the "negative samples") and train only for them. So indeed there is some "imbalance" (you select "some" instead of "all" the "negative examples" for each target word) since the method makes sure that the target word is more "similar" to words of the same context in comparison to some (instead of all) the words of different contexts, but the same imbalance gives a substantial computational gain. After you have completed the training, you end up with a very nice 300-D representation of your words, readily available in the embedding layer, effectively solving the word embedding problem. See also this answer from Quora.
