[site]: crossvalidated
[post_id]: 383711
[parent_id]: 
[tags]: 
How to get the same output on each training iteration of neural network?

I'm a neural network noob and am trying to use LSTM(Keras,Tensorflow backend) for predicting time series data. Every time I train the network, I get a very different set of output values even though the percentage error is around the same. I'm using adam optimizer and mse loss. I believed that the inspite of the initial random weights the neural network was supposed to converge to around the same values every time it is trained. Is that belief wrong? Edit: I've fixed seeds using random.seed() , np.random.seed() and tf.random.seed and followed the instructions laid out here as well. Still different results. I'm going to try with the theano backend to see whether that works or not. But my main question is, shouldn't the models give similar results in spite of initial randomness.
