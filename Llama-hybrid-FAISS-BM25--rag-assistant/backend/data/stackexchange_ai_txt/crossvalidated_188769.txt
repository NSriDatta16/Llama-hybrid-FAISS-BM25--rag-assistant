[site]: crossvalidated
[post_id]: 188769
[parent_id]: 
[tags]: 
Initializing starting weights in Neural Nets with bounds?

I'm implementing a deep denoising autoencoder using Theano in Python. I know that you are supposed to initialise starting weights using random numbers and that's what Theano authors do in their tutorials, though they use weird upper and lower bounds without giving any explanation. So, here is he code: starting_weights = np.asarray(numpy_rng.uniform( low=-4 * np.sqrt(6. / (n_hidden + n_visible)), high=4 * np.sqrt(6. / (n_hidden + n_visible)), size=(n_visible, n_hidden)), dtype=theano.config.floatX ) Basically, they take a sample from the uniform distribution truncated at $ \pm 4\cdot \sqrt { \frac { 6 }{ n+m } } $, where $m$ and $n$ are the numbers of visible and hidden units. Can someone explain what is the rationale behind it?
