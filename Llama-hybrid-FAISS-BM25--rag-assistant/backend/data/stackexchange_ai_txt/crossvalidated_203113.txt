[site]: crossvalidated
[post_id]: 203113
[parent_id]: 
[tags]: 
Using control charts with very large subgroup size?

I am working with a very large data set -- time-series data from an on-line process monitor with a 10 second measurement interval. I am trying to develop control limits for the process using control charts theory. Here are the methods I have tried so far and the results: 1) Using an I-MR type chart: computing the control limits using an I-MR chart results in most of the data being out of control. I think the problem is that the time-scale over which the readings change significantly is hours but the range (and ultimately the control limit) is calculated from the differences between two consecutive data points, which are very small. 2) Subgrouping the data and using Xbar-R chart: If I sub-group the data into one-minute groups (i.e. n=6), and compute the control limits, I have the same problem as with the I-MR chart -- all the data is out of control. Same problem as above, the data doesn't change significantly from minute to minute. 3) Moving average over a longer time period: If I compute a moving average and range over several hours, the average range becomes much more realistic for the data set. However, I'm not sure how to actually calculate the real control limits because my subgroup size is so large (n=1440) and the tables of correction factors only go up to n=15. Is it appropriate to have such a large subgroup size, and if so, what is the correct method of calculating control limits? If it is not appropriate, what would be the correct way to calculate realistic control limits for this process?
