[site]: datascience
[post_id]: 82326
[parent_id]: 82317
[tags]: 
In addition to etiennedm's answer: Informally, this is the equivalent of making a student take a test while giving them all the answers. The goal of evaluating a model on an unseen test set is to measure the ability of the model to predict on different instances, in the same way that a school test is supposed to check if the student can solve new problems based on the knowledge they acquired. If the student is provided with all the answers, the result of the test will only show that they can read and copy the answer, and this is not very useful. Same thing for a program, it's pointless to prove that it can store and copy answers, and there's no need for Machine Learning to achieve that. Evaluating on an unseen test set is meant to measure how well the model generalizes (i.e. actually "learns something") from what it has seen in the training data to answer new problems. Using the training instances as test set completely defeats this purpose, since the model doesn't need to generalize anything to answer these instances.
