[site]: crossvalidated
[post_id]: 204017
[parent_id]: 203901
[tags]: 
I can't really read the code, but using a normal model to approximate a uniform should always result in exactly this result: a normal centered at the uniform mean with variance to match. If you want a better approximation, give the model more flexibility to adapt to the data. If you want the best outcome, though, you'll have to correctly specify the model. I think that the root of your understanding is in parsing the quotation My confusion comes from the oft repeated "Markov Chain Monte Carlo is a technique to solve the problem of sampling from a complicated distribution." Note the the quote says "complicated," not "unknown." In Bayesian statistics, we're perfectly capable of writing out what the probability distribution over the parameters given the data must be -- but it's usually completely intractable to evaluate. Typically, the math is only straightforward for the simplest models, and advanced calculus can take us a little further, but a Bayesian problem of even modest complexity requires simulation to resolve, particularly Bayesian hierarchical models. Your writing here also supports my interpretation of where the misunderstanding lies. So I tried to see if I could learn a stupidly simple distribution (uniform). I created some fake data drawn uniformly from (0-5). Again, I think that you've miscast the problem. PyMC isn't "learning" a distribution. You're just drawing samples from a specified functional form. The way of going about that is somewhat complicated, but it's just about drawing samples from the model that you've specified. The normal has support over the real line, so PyMC is drawing samples from the reals. The data more strongly support samples drawn from $[0,5]$ (can you see why?) but nothing about the model you've specified excludes $\pm6$ (for example) from having positive probability. I would recommend reading more about Bayesian statistics. Gelman's Bayesian Data Analysis is a great place to start.
