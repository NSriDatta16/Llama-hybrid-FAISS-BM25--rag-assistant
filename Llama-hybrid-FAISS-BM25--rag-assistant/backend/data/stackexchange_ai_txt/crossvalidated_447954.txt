[site]: crossvalidated
[post_id]: 447954
[parent_id]: 
[tags]: 
Expected value of the indicator variable Z , E.M. - PRML

I am having a hard time figuring out the equation 9.39 page 443 in Bishop's book : Pattern recognition and Machine Learning. A posterior distribution in the book is written as: $$ p(\textbf{Z} \mid \textbf{X}, \mu, \pi) \approx \Pi_n \Pi_k [\pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)]^{z_{nk}} $$ where $z$ is a 1-of-K binary rperesentation in which a particular element $z_k \in \{0,1 \}$ and $\sum_k z_k=1$ and $z_{nk}$ denotes the indicator for the $n$ th point. Then the author states that: $$ \mathbb{E}[z_{nk}] = \frac{\sum_{z_{nk}} z_{nk} [\pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)]^{z_{nk}}}{\sum_{z_{nj}} [\pi_j \mathcal{N}(x_n \mid \mu_j, \Sigma_j)]^{z_{nj}}} $$ I don't really understand where this expression comes from. This is not the classical definition of the expectation of a random variable.
