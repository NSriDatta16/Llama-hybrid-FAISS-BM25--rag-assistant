[site]: crossvalidated
[post_id]: 633112
[parent_id]: 633091
[tags]: 
Bottom line, linear regression is more readily interpreted, however it comes with more assumptions that need to be met when compared with SVR. Comparing Linear Regression (LR) and Support Vector Regression (SVR): Linear Regression (LR): Strengths : LR is a straightforward and efficient method. It's highly interpretable, providing clear coefficients for each feature variable, which can be interpreted as the change in the dependent variable for a one-unit change in that feature. This makes it a good choice when you need to explain the impact of individual variables. Weaknesses: LR assumes a linear relationship between the variables, which is not always the case in real-world data. It's sensitive to outliers, which can skew the regression line and affect the accuracy of predictions. LR can also be affected by multicollinearity, where two or more predictor variables are highly correlated. This can inflate the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. Weakness : The impact of failing to meet assumptions can be significant, and assumptions are often not met for real world data. Support Vector Regression (SVR) with a linear kernel: Strengths : Robustness : SVR with a linear kernel is more robust than Linear Regression as it doesn't make as many assumptions. For instance, it doesn't require the residuals to be normally distributed or homoscedastic (having the same variance at each level of the independent variables). This makes it more flexible in handling different types of data. Outlier Insensitivity : SVR is less sensitive to outliers compared to Linear Regression. This is because SVR tries to fit the best line within a certain threshold, focusing on the points that are close to the decision boundary (support vectors). This characteristic can be particularly useful when dealing with data that contains outliers. Flexibility : Even with a linear kernel, SVR does not strictly assume a linear relationship between the independent and dependent variables. This allows it to potentially capture more complex relationships within the data, even though the function it's trying to fit is linear. Weaknesses : It is less interpretable because it doesn't provide clear coefficients for each feature. As an example, look at this comparison of models fit by SVR with linear kernel and linear regression for noisy linear-ish data. $X$ is drawn from uniform distribution $[0,5)$ . $Y = \frac{1}{1+e^-X}$ with random uniform $[-1.5, 1.5]$ noise added to 20% of values. Note that we still have an inappropriate choice of model here, the intent here is to show what might happen if we dont meet assunptions for LR and how SVM might pull ahead in such a case.
