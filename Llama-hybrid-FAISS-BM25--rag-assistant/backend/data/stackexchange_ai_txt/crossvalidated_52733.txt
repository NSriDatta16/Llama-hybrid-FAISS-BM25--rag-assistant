[site]: crossvalidated
[post_id]: 52733
[parent_id]: 52730
[tags]: 
I am also dealing with imbalanced dataset. I try to say what I know about this. --will the classification algorithm fail to generalize when classifying instances from the target population? As I mentioned earlier, I'm more interested in accurately classifying the positive class instances, which my training set has in abundance. In general such difference in distributions has negative influence on the classifier's generalization ability/performance. But it also depends on the particular classifier used. You can try to run experiments to see how the performance goes. In some case, it might not be that bad. For the most naive classifier - the majority rule, that is we predict every item as the majority class in the training set. Then of course, in your case the test error rate will be 65%. For probability classifiers that estimate conditional distribution P(y|x), the performance might still be good, as long as the estimation of P(y|x) is good. Say for text classification, it might be the case that the words that appears in the two classes are drastically different, and the within-class distribution P(x|y) of the words stays the same in the training and test set, the classifier might still perform good, even though P(y) changed a lot. And similarly for discriminative classifiers like linear classifers as SVM. But in general, the difference in training set and test set might have some negative influence on classifer's performance. And you might need to note that the difference seems to be the class distribution p(y), but the p(x|y) might be the same. what do you mean by accurately classifying the positive class instances? Do you want high recall of the positive class? With positive class dominating the training set, the classifer might favor the positive class, so yielding high recall of positive class, but might have low precision for positive class, since it might also get lots of negative instances into positive class (false positive).
