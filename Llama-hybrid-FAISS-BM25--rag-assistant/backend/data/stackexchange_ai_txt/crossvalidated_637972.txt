[site]: crossvalidated
[post_id]: 637972
[parent_id]: 637889
[tags]: 
Could you provide a reference that discusses the use of autoencoders instead of nonlinear PCA? I ask this because I believe autoencoders are not an alternative to nonlinear PCA, but instead an alternative approach to fitting a nonlinear PCA model. For example, recently Urban & Bauer (2021) proposed the use of variational autoencoders (VAE) to fit exploratory item factor analysis (IFA) models (which are mathematically very similar to nonlinear PCA). In their article, they are not saying VAEs are preferable to exploratory IFA. Instead, they simply propose VAEs as an alternative estimation method. Also, see this website dedicated to nonlinear PCA, which includes the quote: Nonlinear principal component analysis (NLPCA) is commonly seen as a nonlinear generalization of standard principal component analysis (PCA). It generalizes the principal components from straight lines to curves (nonlinear). Thus, the subspace in the original data space which is described by all nonlinear components is also curved. Nonlinear PCA can be achieved by using a neural network with an autoassociative architecture also known as autoencoder, replicator network, bottleneck or sandglass type network. Such autoassociative neural network is a multi-layer perceptron that performs an identity mapping, meaning that the output of the network is required to be identical to the input. However, in the middle of the network is a layer that works as a bottleneck in which a reduction of the dimension of the data is enforced. This bottleneck-layer provides the desired component values (scores). on its home page. This description of nonlinear PCA makes it pretty clear that autoencoders are just one of many methods that may be used to fit nonlinear PCA models. References Urban, C. J., & Bauer, D. J. (2021). A deep learning algorithm for high-dimensional exploratory item factor analysis. Psychometrika, 86(1), 1-29.
