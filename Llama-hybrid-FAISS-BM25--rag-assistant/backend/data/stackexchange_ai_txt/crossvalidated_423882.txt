[site]: crossvalidated
[post_id]: 423882
[parent_id]: 
[tags]: 
How to measure the quality of fakes produced by a digital twin?

I have asked this on stackoverflow , but it was considered too broad, perhaps because there is no specific code involved. I hope this question is more appropriate for crossvalidated. I am developing a digital twin for a certain application. The digital twin has parameters that I tried to optimize using cross-entropy of a classifier trained to distinguish real from fake data. The whole setup runs similarly to GAN, except that the generative part is not a NN. Basically, for each iteration of the generative parameter optimization I construct and train the classifier NN, and use the trained classifier validation cross-entropy loss as the indicator of the generator's quality. I ran into the problem that the classifier cross-entropy loss doesn't seem to be monotone with respect to the quality of the fake. I proved that by freezing the generator and running the classifier training 100 times; the 100 classifiers, trained on the identical inputs, produced widely different validation cross-entropy and accuracy. That makes either cross-entropy or accuracy completely unsuitable for evaluating the quality of the fake: the noise-to-signal ratio is way too high to optimize generative parameters based on the classifier response. The question is, how do I overcome this problem? How does one produce a measure of fakeness that is sufficiently stable and nearly monotone with respect to the quality of the fake?
