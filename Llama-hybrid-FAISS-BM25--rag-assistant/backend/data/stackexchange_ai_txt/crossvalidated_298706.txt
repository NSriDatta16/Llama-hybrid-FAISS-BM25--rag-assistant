[site]: crossvalidated
[post_id]: 298706
[parent_id]: 298701
[tags]: 
Trees in random forests are very deep, and indeed typically grown until the terminal nodes are pure. A lot of these splits are overfit. The overfitting averages out when the predictions are averaged. You can illustrate this by growing a random forest with only random noise as predictors. The model will be forced to use the noise for splitting. By the way, a new subset of predictors is randomly selected for each split, not for each tree.
