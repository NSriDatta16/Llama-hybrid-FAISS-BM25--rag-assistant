[site]: datascience
[post_id]: 69656
[parent_id]: 
[tags]: 
How to deal with training set that overfits very easily

I have a dataset consisting of 72 one-hot encoded (thus binary) features and 2.5K training examples. With this I am trying to solve a 10-class classification problem. My main problem is that no matter which classification algorithm I use, the model overfits instantly to the train set while in the test all metrics seem pretty much bad, e.g. at best average f1-score around 0.18 and avg accuracy less than 20% average accuracy. To deal with this I tried to reduce the number of features to 30 and finally to 10, simplified the models I use (mostly tree based). Also note that the split is done using stratification. Would it be a good idea to oversample ? Note that I cannot reduce the number of target classes. Example reports: Random Forest precision recall f1-score support a 0.05 0.06 0.05 50 b 0.15 0.17 0.16 42 c 0.14 0.13 0.14 60 d 0.14 0.18 0.16 77 e 0.12 0.14 0.13 50 f 0.26 0.21 0.23 33 g 0.08 0.05 0.06 41 h 0.14 0.10 0.11 51 i 0.00 0.00 0.00 18 j 0.20 0.21 0.20 78 accuracy 0.14 500 macro avg 0.13 0.12 0.13 500 weighted avg 0.14 0.14 0.14 500 Gradient Boosted Trees with one-over-rest precision recall f1-score support a 0.00 0.00 0.00 50 b 0.08 0.02 0.04 42 c 0.23 0.10 0.14 60 d 0.13 0.42 0.20 77 e 0.25 0.02 0.04 50 f 0.00 0.00 0.00 33 g 0.20 0.02 0.04 41 h 0.00 0.00 0.00 51 i 0.00 0.00 0.00 18 j 0.17 0.44 0.24 78 accuracy 0.15 500 macro avg 0.11 0.10 0.07 500 weighted avg 0.12 0.15 0.10 500 Similar results I obtain for KNN and for Histogram Gradient boosting. The same statistics for the training set are all almost 1.0.
