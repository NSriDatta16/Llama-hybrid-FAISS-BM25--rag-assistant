[site]: crossvalidated
[post_id]: 385108
[parent_id]: 385102
[tags]: 
You have not done a very good job at reading Y. A. LeCun and L. Bottou, “Efficient backprop,” in Neural networks: Tricks of the trade, Springer, 2012, pp. 9–48. it is in Section 4.1 Stochastic learning also often results in better solutions because of the noise in the updates. Nonlinear networks usually have multiple local minima of differ- ing depths. The goal of training is to locate one of these minima. Batch learning will discover the minimum of whatever basin the weights are initially placed. In stochastic learning, the noise present in the updates can result in the weights jumping into the basin of another, possibly deeper, local minimum. This has been demonstrated in certain simplified cases [15,30] ... [15] T.M. Heskes and B. Kappen. On-line learning processes in artificial neural net- works. In J. G. Tayler, editor, Mathematical Approaches to Neural Networks, vol- ume 51, pages 199-233. Elsevier, Amsterdam, 1993. [30] G. B. Orr. Dynamics and Algorithms for Stochastic learning. PhD thesis, Oregon Graduate Institute, 1995. Note that earlier in this section LeCun and Bottou define stochastic as what you call a minibatch (giving example of using samples of 100 taken from 1000), and batch as what you call full batch
