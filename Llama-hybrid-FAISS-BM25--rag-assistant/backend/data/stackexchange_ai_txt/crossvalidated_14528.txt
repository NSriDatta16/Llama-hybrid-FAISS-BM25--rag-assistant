[site]: crossvalidated
[post_id]: 14528
[parent_id]: 3549
[tags]: 
It takes very little correlation among the independent variables to cause this. To see why, try the following: Draw 50 sets of ten vectors $(x_1, x_2, \ldots, x_{10})$ with coefficients iid standard normal. Compute $y_i = (x_i + x_{i+1})/\sqrt{2}$ for $i = 1, 2, \ldots, 9$. This makes the $y_i$ individually standard normal but with some correlations among them. Compute $w = x_1 + x_2 + \cdots + x_{10}$. Note that $w = \sqrt{2}(y_1 + y_3 + y_5 + y_7 + y_9)$. Add some independent normally distributed error to $w$. With a little experimentation I found that $z = w + \varepsilon$ with $\varepsilon \sim N(0, 6)$ works pretty well. Thus, $z$ is the sum of the $x_i$ plus some error. It is also the sum of some of the $y_i$ plus the same error. We will consider the $y_i$ to be the independent variables and $z$ the dependent variable. Here's a scatterplot matrix of one such dataset, with $z$ along the top and left and the $y_i$ proceeding in order. The expected correlations among $y_i$ and $y_j$ are $1/2$ when $|i-j|=1$ and $0$ otherwise. The realized correlations range up to 62%. They show up as tighter scatterplots next to the diagonal. Look at the regression of $z$ against the $y_i$: Source | SS df MS Number of obs = 50 -------------+------------------------------ F( 9, 40) = 4.57 Model | 1684.15999 9 187.128887 Prob > F = 0.0003 Residual | 1636.70545 40 40.9176363 R-squared = 0.5071 -------------+------------------------------ Adj R-squared = 0.3963 Total | 3320.86544 49 67.7727641 Root MSE = 6.3967 ------------------------------------------------------------------------------ z | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- y1 | 2.184007 1.264074 1.73 0.092 -.3707815 4.738795 y2 | 1.537829 1.809436 0.85 0.400 -2.119178 5.194837 y3 | 2.621185 2.140416 1.22 0.228 -1.704757 6.947127 y4 | .6024704 2.176045 0.28 0.783 -3.795481 5.000421 y5 | 1.692758 2.196725 0.77 0.445 -2.746989 6.132506 y6 | .0290429 2.094395 0.01 0.989 -4.203888 4.261974 y7 | .7794273 2.197227 0.35 0.725 -3.661333 5.220188 y8 | -2.485206 2.19327 -1.13 0.264 -6.91797 1.947558 y9 | 1.844671 1.744538 1.06 0.297 -1.681172 5.370514 _cons | .8498024 .9613522 0.88 0.382 -1.093163 2.792768 ------------------------------------------------------------------------------ The F statistic is highly significant but none of the independent variables is, even without any adjustment for all 9 of them. To see what's going on, consider the regression of $z$ against just the odd-numbered $y_i$: Source | SS df MS Number of obs = 50 -------------+------------------------------ F( 5, 44) = 7.77 Model | 1556.88498 5 311.376997 Prob > F = 0.0000 Residual | 1763.98046 44 40.0904649 R-squared = 0.4688 -------------+------------------------------ Adj R-squared = 0.4085 Total | 3320.86544 49 67.7727641 Root MSE = 6.3317 ------------------------------------------------------------------------------ z | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- y1 | 2.943948 .8138525 3.62 0.001 1.303736 4.58416 y3 | 3.403871 1.080173 3.15 0.003 1.226925 5.580818 y5 | 2.458887 .955118 2.57 0.013 .533973 4.383801 y7 | -.3859711 .9742503 -0.40 0.694 -2.349443 1.577501 y9 | .1298614 .9795983 0.13 0.895 -1.844389 2.104112 _cons | 1.118512 .9241601 1.21 0.233 -.7440107 2.981034 ------------------------------------------------------------------------------ Some of these variables are highly significant, even with a Bonferroni adjustment. (There's much more that can be said by looking at these results, but it would take us away from the main point.) The intuition behind this is that $z$ depends primarily on a subset of the variables (but not necessarily on a unique subset). The complement of this subset ($y_2, y_4, y_6, y_8$) adds essentially no information about $z$ due to correlations—however slight—with the subset itself. This sort of situation will arise in time series analysis . We can consider the subscripts to be times. The construction of the $y_i$ has induced a short-range serial correlation among them, much like many time series. Due to this, we lose little information by subsampling the series at regular intervals. One conclusion we can draw from this is that when too many variables are included in a model they can mask the truly significant ones. The first sign of this is the highly significant overall F statistic accompanied by not-so-significant t-tests for the individual coefficients. (Even when some of the variables are individually significant, this does not automatically mean the others are not. That's one of the basic defects of stepwise regression strategies: they fall victim to this masking problem.) Incidentally, the variance inflation factors in the first regression range from 2.55 to 6.09 with a mean of 4.79: just on the borderline of diagnosing some multicollinearity according to the most conservative rules of thumb; well below the threshold according to other rules (where 10 is an upper cutoff).
