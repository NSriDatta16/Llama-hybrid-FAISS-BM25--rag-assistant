[site]: crossvalidated
[post_id]: 586011
[parent_id]: 361066
[tags]: 
If you choose to use activation=None, you for example add a BatchNormalization layer before you actually use the activation. This is used often in convolutional neural networks, but is good for dense neural networks as well. z = tf.keras.layers.Dense(20, activation=None)(z) z = tf.keras.layers.BatchNormalization()(z) z = tf.keras.layers.Activation("relu")(z)
