[site]: crossvalidated
[post_id]: 23242
[parent_id]: 23235
[tags]: 
I haven't worked with R, so I can only give more general tips. Did you check whether the algorithm converged? One possible explanation might be that the different parameter sets are all somewhere half way to the same optimum. If the algorithm always converges but to a different local optimum, then there are many heuristics you could try to avoid those. One simple strategy when using stochastic gradient descent (SGD) would be to use smaller batches and larger momentum . The smaller batch sizes effectively introduce some noise into the training which can help escape some local optima. A much more sophisticated strategy would be to initialize the weights using autoencoders .
