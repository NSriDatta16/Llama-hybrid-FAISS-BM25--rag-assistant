[site]: crossvalidated
[post_id]: 479129
[parent_id]: 479122
[tags]: 
Just for clarity I will refer to the image of your question. From left to right, this Neural Network (NN) has one input layer of 5 neurons, one hidden layer of 2 neurons and one output layer of 5 neurons. how does the graphical representation work? Let's say you have a single new predictor value and you feed it into the NN below. How does NN know which coefficient is assigned to this single predictor value? What happens to the other coefficients? Would the vector of Xs values be equal to [X1, 0, 0, 0, 0]? Once we have already set up all the weights and biases the Neural Network has been already trained. Thereby the net is ready to receive data in its input layer and make the correct prediction at the output layer. In the NN of the image, we have an input layer formed by 5 neurons. This 5 neurons will ''receive'' the data whose values are known e.g. they could be a 5x1 vector of pixel gray scale values of an image formed by 5 pixels (the same as the number of neurons of the input layer). After this, the network will do all the mathematical operations needed and will give as a result, following the example of the image, a 5x1 vector at the output layer. This output could give some information of it e.g. if it contains 1 of a total of 5 objects, so if the 1st neuron of the output layer is asociated with object1, this neuron will be activated meaning that this object is present. Following this, if object 1 , object 3 and object 4 are present in the image, we will have at the output layer a vector like this: $[1, 0, 1, 1, 0]^T$ . Note: In practice, this will happen only ideally. If the network is well trained, the most likely thing to happen is that the vector will have elements close to $0$ for the non-activated neurons, and close to $1$ for the activated neurons. and how it relates to the matrix math behind it? In order to answer this question I am going to need some notation. As the Network is already trained, we know all the elements of the matrices of the image: Each weight $w_{jk}$ and each bias $b_{j}$ where $j$ represents the row of the weight matrix (or vector of biases) and it is also related to the neuron position in the layer from we want to calculate the vector of activations. On the other hand, $k$ represents the column of the weight matrix or the position of the neuron in the previous layer of the current one in wich we are calculating the activations. With this notation we can calculate the activations of each neuron $j$ in a layer as: $x_j = f(z_j)\,\,$ with $\,\,z_j = \sum_k w_{jk}\times x_k + b_j$ where $f$ represents the activation function, this can be of different kinds like Sigmoid or Softmax . Note that this is the same as calculating separately each $Z_i$ of the equation that appears in the image of the question. So this is what the matrix math do "behind the scenes". Edit: As Gunes has correctly say, the number of neurons at the input layer has nothing to do with the number of neurons at the output layer.
