[site]: crossvalidated
[post_id]: 367171
[parent_id]: 125261
[tags]: 
If you are able to use the same number of features for both test and training data, number of features has not anything to do with overfitting, unless this is a very exceptional case. If you have the possibility of using such a variable range of features (10 to 1000) then perhaps you should be more concerned about the correlation between your features. You maximize the interpretability and efficiency of your mode, use a "dimensionality-reduction" method to get rid of the features which are not contributing significantly to the classifications.As @While suggested, your model can be used to shrink the dimensionality. Correlation analysis is another approach which is more statistical and less machine learning approach. Overfitting usually comes with going too far with making your model more and more complex for obtaining higher accuracy on the training set, while resulting in losing the generality of your model, thus memorizing the training set instead of learning it. The number of features use in a model doesn't have anything to do with memorizing the training set.
