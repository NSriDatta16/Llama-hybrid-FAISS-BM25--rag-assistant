[site]: crossvalidated
[post_id]: 183440
[parent_id]: 180618
[tags]: 
I'm not sure if this is quite what you want, but here goes. You're LHS-sampling $n$ points from $[0,1)^d$, say. We'll argue very informally that, for any $\epsilon>0$, the expected number of empty (hyper)cuboids of size $\epsilon$ in each dimension goes to zero as $n\to\infty$. Let $m=\lceil 2/\epsilon \rceil$ so that if we divide $[0,1)^d$ uniformly into $m^d$ tiny cuboids -- microcuboids , say -- of width $1/m$ then every width-$\epsilon$ cuboid contains at least one microcuboid. So if we can show that the expected number of unsampled microcuboids is zero, in the limit as $n\to\infty$, then we're done. (Note that our microcuboids are arranged on a regular grid, but the $\epsilon$-cuboids can be in any position.) The chance of completely missing a given microcuboid with the first sample point is $1-m^{-d}$, independent of $n$, as the first set of $d$ sample coordinates (first sample point) can be chosen freely. Given that the first few sample points have all missed that microcuboid, subsequent sample points will find it harder to miss (on average), so the chance of all $n$ points missing it is less than $(1-m^{-d})^n$. There are $m^d$ microcuboids in $[0,1)^d$, so the expected number that are missed is bounded above by $m^d(1-m^{-d})^n$ -- because expectations add -- which is zero in the limit as $n\to\infty$. Updates ... (1) Here's a picture showing how, for given $\epsilon$, you can pick $m$ large enough so that an $m\times m$ grid of "microcuboids" (squares in this 2-dimensional illustration) is guaranteed to have at least one microcuboid within any $\epsilon\times\epsilon$ sized region. I've shown two "randomly"-chosen $\epsilon\times\epsilon$ regions and have coloured-in purple the two microcuboids that they contain. (2) Consider any particular microcuboid. It has volume $(1/m)^d$, a fraction $m^{-d}$ of the whole space. So the first LHS sample -- which is the only one chosen completely freely -- will miss it with probability $1-m^{-d}$. The only important fact is that this is a fixed value (we'll let $n\to\infty$, but keep $m$ constant) that's less than $1$. (3) Now think about the number of sample points $n>m$. I've illustrated $n=6m$ in the picture. LHS works in a fine mesh of these super-tiny $n^{-1}\times n^{-1}$ sized "nanocuboids" (if you will), not the larger $m^{-1}\times m^{-1}$ sized "microcuboids", but actually that's not important in the proof. The proof only needs the slightly hand-waving statement that it gets gradually harder, on average, to keep missing a given microcuboid as you throw down more points. So it was a probability of $1-m^{-d}$ for the first LHS point missing, but less than $(1-m^{-d})^n$ for all $n$ of them missing: that's zero in the limit as $n\to\infty$. (4) All these epsilons are fine for a proof but aren't great for your intuition. So here are a couple of pictures illustrating $n=10$ and $n=50$ sample points, with the largest empty rectangular area highlighted. (The grid is the LHS sampling grid -- the "nanocuboids" referred to earlier.) It should be "obvious" (in some vague intuitive sense) that the largest empty area will shrink to arbitrarily small size as the number of sample points $n\to\infty$.
