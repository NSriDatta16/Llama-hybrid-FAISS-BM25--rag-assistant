[site]: crossvalidated
[post_id]: 623571
[parent_id]: 330559
[tags]: 
Zero mean assumption relates to the curvature of the error surface. The whitening argument ( https://stats.stackexchange.com/a/330885/27556 ) is a standard issue with gradient descent. just consider linear regression with gradient descent - the curvature of the error surface is given by the covariance of the inputs (including bias term). [ for a nonlinear model such as a neural net, I would argue you have the same issues] Zero mean ensures you have no covariance with the bias term ( and the other non zero mean inputs). If you have covariance with a lot of inputs, then you create a narrow valley shape (in our case across the bias/constant input direction). This is hard for gradient descent to handle (because you would want a bigger learning rate along the valley and a shorter one across the valley), gradient descent has a single learning rate, so you are forced to have a very low learning rate (making slow progress along the valley) to avoid jumping from peak to peak of the valley. Having different learning rates in each direction is essentially what second order methods do, but they are too memory intensive for large parameter models such as neural networks.
