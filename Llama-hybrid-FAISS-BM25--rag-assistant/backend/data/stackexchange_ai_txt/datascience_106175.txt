[site]: datascience
[post_id]: 106175
[parent_id]: 
[tags]: 
Replicate the CNN arhitecture from a paper

In the paper it says: We train our model using CNN with the number of filters 128, stride of 1, and kernel size of 3, 4, and 5. We then apply the ReLu activation function with Max Pooling to the out of this layer; the output of this layer is often called feature maps. The feature maps are flattened and pass through three Fully-Connected layers (FC) with dropout between each layer. The first two FC layers use ReLu activation function with 256 and 64 of outputs. The last FC layer uses Softmax activation function which provides the output as complex (1) or non-complex (0). From what I understand, it looks like this: cnn = tf.keras.models.Sequential([ layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=(1,1), padding='same'), layers.Conv2D(filters=128, kernel_size=(4, 4), activation='relu', strides=(1,1), padding='same'), layers.Conv2D(filters=128, kernel_size=(5,5), activation='relu', strides=(1,1), padding='same'), layers.MaxPool2D(pool_size=2, strides=(2, 2), padding='same'), layers.Flatten(), layers.Dense(256, activation='relu'), layers.Dropout(0.25), layers.Dense(64, activation='relu'), layers.Dropout(0.25), layers.Dense(1, activation='softmax'), ]) But my epochs take a lot of time to run so I'm wondering if I'm doing something wrong.
