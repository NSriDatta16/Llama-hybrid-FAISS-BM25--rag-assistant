[site]: datascience
[post_id]: 128366
[parent_id]: 
[tags]: 
Fine-tuned Stable Diffusion not converging with custom encoder

I'm currently fine-tuning a stable diffusion model for the task of dataset augmentation. I am training the model on 80k images from hte CelebA-attributes dataset, replacing the text encoder with learnt embedding layers that encode the binary attribute vectors. I've trained a model, which generates images that look generally like faces, however, many of them are hit-or-miss, being low quality, and unrealistic. During training, the average loss from MSE loss of 0.18 in the first epoch down to 0.155, then stays there for the rest of training; I'm not sure what a good "loss trajectory" would look like for diffusion models, and if I should try something else. My main concerns are as follows: Is my method of encoding the "prompts" (the binary attribute vectors) valid? Is my setup adequate so they can be optimized? What would a good loss function trajectory be for this sort of model? What hyperparameters could lead to better results? Below is some code to show my setup. Keep in mind, although I use the term "prompt", it doesn't refer to text, but rather, a list of binary attributes that are used to condition the model to generate faces with certain qualities (i.e blonde, male, beard etc). A prompt would look like this: [-1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1] My models: class CombinedModel(nn.Module): def __init__(self, unet, prompt_encoder): super(CombinedModel, self).__init__() self.unet = unet self.prompt_encoder = prompt_encoder def forward(self, pixel_values, attributes, timesteps, ignore_prompt_embedding=False): # First, get the embedded attributes embedded_attributes = self.prompt_encoder(attributes, ignore_prompt_embedding=ignore_prompt_embedding) # Now, pass the necessary inputs along with the embedded attributes to the U-Net output = self.unet(pixel_values, timesteps, encoder_hidden_states=embedded_attributes, return_dict=False)[0] return output class AttributeEmbeddingModel(ModelMixin, ConfigMixin): @register_to_config def __init__(self, num_attributes, embedding_size, intermediate_size): super(AttributeEmbeddingModel, self).__init__() #assuming binary attributes, so num_embeddings is 2 self.embedding = nn.Embedding(num_embeddings=2, embedding_dim=embedding_size) self.num_attributes = num_attributes # Assuming intermediate_size is the target output size # Adjust the linear layer accordingly self.intermediate_layer = nn.Linear(embedding_size * num_attributes, intermediate_size) self.activation = nn.ReLU() def forward(self, attributes, ignore_prompt_embedding=False): if ignore_prompt_embedding: # Adjust the shape to [batch_size, 1, num_attributes] and convert to float resized_attributes = attributes.unsqueeze(1).float() # Convert to float return resized_attributes batch_size = attributes.shape[0] indices = (attributes + 1) // 2 embedded_attributes = self.embedding(indices) # Flatten the embeddings to combine them before the intermediate layer embedded_attributes = embedded_attributes.view(batch_size, -1) intermediate_output = self.intermediate_layer(embedded_attributes) activated_output = self.activation(intermediate_output) # No need to reshape if you want the output to be [batch_size, intermediate_size] return activated_output.unsqueeze(1) My training loop: for step, batch in enumerate(train_dataloader): num_steps +=1 # Use the accelerator context manager for the unet model with accelerator.accumulate(unet_prompt_model): # NOTE: batch["pixel_values"] is a list of tensors, one for each image in the batch # NOTE: batch["prompt"] is a list of tensors, one for each attribute row in the batch # Convert images to latent space latents = vae.encode(batch["pixel_values"].to(weight_dtype)).latent_dist.sample() latents = latents * vae.config.scaling_factor # Sample noise that we'll add to the latents noise = torch.randn_like(latents) bsz = latents.shape[0] # Sample a random timestep for each image timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device) timesteps = timesteps.long() noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps) noisy_latents.requires_grad_(True) #TODO fix this # Target: target = noise # target can be v_prediction too: # target = noise_scheduler.get_velocity(latents, noise, timesteps) # Encode the prompt. attributes = batch["prompt"].to(accelerator.device) # Directly move to device without stacking model_pred = unet_prompt_model( noisy_latents, attributes, timesteps, ignore_prompt_embedding=ignore_prompt_embedding ) # maybe snr loss here instead of mse loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean") # Gather the losses across all processes for logging (if we use distributed training). avg_loss = accelerator.gather(loss.repeat(conf.train_batch_size)).mean() train_loss += avg_loss.item() / conf.gradient_accumulation_steps loss_this_epoch += avg_loss.item() # Backpropagate accelerator.backward(loss) if accelerator.sync_gradients: accelerator.clip_grad_norm_(unet_prompt_model.parameters(), 1.0) optimizer.step() lr_scheduler.step() optimizer.zero_grad() And finally, my current hyperparams: @dataclass class TrainingConfig80k: # This model is 128x128, which is what we're aiming for pretrained_hf_model = "bguisard/stable-diffusion-nano-2-1" # the generated image resolution image_size = 128 train_batch_size = 256 dataloader_num_workers = 2 gradient_accumulation_steps = 1 num_epochs = 50 learning_rate = 1e-6 lr_warmup_steps = 200 use_ema = 0 # do not use ema, didn't seem to help num_inference_steps = 50 save_image_epochs = 2 save_model_epochs = 5 max_checkpoints_saved=5 mixed_precision = 'fp16' # `no` for float32, `fp16` for automatic mixed precision output_dir = 'LDA-CelebA-128-80k' # the model namy locally and on the HF Hub dataset_hf_dir = "tpremoli/CelebA-attrs-80k" hf_repo_id="tpremoli/LDA-CelebA-128-80k" # Choose between # ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]' lr_scheduler="cosine_with_restarts" prompt_embedding_first_size = 256 prompt_embedding_final_size = 128 ignore_prompt_embedding = False seed = None train_transforms = transforms.Compose( [ transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BILINEAR), transforms.CenterCrop(image_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ] ) def preprocess_train(self, examples): images = [image.convert("RGB") for image in examples["image"]] examples["pixel_values"] = [self.train_transforms(image) for image in images] examples["prompt_str"] = [ps for ps in examples["prompt_string"]] examples["prompt"] = tokenize_attributes_to_tensor_list(examples) return examples ```
