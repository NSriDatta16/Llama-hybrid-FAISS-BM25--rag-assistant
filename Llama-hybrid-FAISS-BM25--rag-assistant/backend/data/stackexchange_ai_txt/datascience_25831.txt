[site]: datascience
[post_id]: 25831
[parent_id]: 
[tags]: 
Some questions about feature hashing in the context of document classification

I'm trying to understand feature hashing, specifically in the context of document classification. I'm under the impression that it is useful because: it allows us to easily deal with 'new' words/features/predictors that we haven't seen before it is rather efficient because it allows us to exploit sparsity which is common in document/word data. Some questions: Say my word/feature set consists of 50,000 unique words and I use 16 hash bits $(2^{16}=65,536)$. In this setting, is it likely each word gets mapped to one of the 65,536 indices? How does this aid us in the situation where I build a model and try to make a prediction on a document that contains a word we haven't seen before? Say my feature set is 100,000 words and I still use 16 hash bits. Does this effectively mean I have (on average?) $100,000-65,536 = 34,464$ words mapped randomly on to other words? It seems I'm missing crucial point or idea here. Can anyone recommend some good introductory material?
