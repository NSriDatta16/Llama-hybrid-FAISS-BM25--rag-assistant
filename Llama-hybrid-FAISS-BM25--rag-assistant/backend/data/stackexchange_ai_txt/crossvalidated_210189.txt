[site]: crossvalidated
[post_id]: 210189
[parent_id]: 208614
[tags]: 
In page 2 of the paper you can read: "by also dropping out a random 20% of the pixels". Pixels are variables (or features) in CNN. So with this hint you might think that this is random variable masking. Also this make sense because this is what some other algorithms are successfully using to avoid overfitting, such as Random Forest. However, I read some papers on the subjects and most of them refers to dropout as a per sample random selection of weights. So each time a new sample is shown to the network a subset of weights will be deactivated. Thus actually creating a lot of networks, each seeing only 1 instance. Also this post ( https://www.quora.com/How-is-dropout-applied-to-mini-batches-in-dropout-neural-networks-with-stochastic-gradient-descent ) confirm that even for mini-batch training, each samples in the mini-batch actually goes through a different thinned network (network with some weight set to zero). So, to my understanding of the litterature on the topics I believe that dropout refers to random masking. You can find a lot papers on the subjects that will confirm what I said but here is one for instance, https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
