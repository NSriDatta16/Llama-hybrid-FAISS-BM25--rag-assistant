[site]: crossvalidated
[post_id]: 112060
[parent_id]: 112054
[tags]: 
I'm working (on my own) with a cousin of de-boosted trees. Boosting increases sensitivity and characterization, but I am wanting a robust fit. The approach that I currently have has results that appear similar to ridge regression, but in test sets I can get good results. Another reason that it isn't a proper GBT is that I replace the tree by making a weighted mix of the errors from the new "tree" and the "old". The weight works like a learning parameter and should be small. The result is a robustly fit Classification and Regression Tree (CART) model. In some senses it qualifies as optimally parsimonious. In the sense of a random forest it is optimally parsimonious because it is a forest comprised of the least nontrivial count of trees - one. There is an interesting connection between parsimony and robustness when it comes to CART models.
