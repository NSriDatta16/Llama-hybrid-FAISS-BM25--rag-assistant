[site]: crossvalidated
[post_id]: 441181
[parent_id]: 441169
[tags]: 
First, you can consider feature selection as a special case of dimensionality reduction (to be specific you can see it as a linear transformation). It basically eliminates the existence of such feature. How to perform this is usually using information from training e.g. feature importances, coefficient, statistical significance, etc. Commonly used dimensionality reduction algorithms in general attempts to map/project your feature vector to lower dimension. However this technique often lose us meaning of each variable or in other words the variable on the projected space are much less interpretable. Performing selection in this space does not make sense and it carries no meaning other than reducing the dimension further. Note there is a special case for PCA, you might observe that we did operation that resembles feature selection, but this only make sense if we take principal value that is associated with $k$ largest eigenvalue where $k$ here is arbitrary. So now hopefully you can see the issue if you do dimensionality reduction first then do. Note that I am not saying that doing dimensionality reduction first and then do feature selection. Feature selection also aims to limit/ select input of our model to few variables that are deemed important. This means it needs interpretable result which you could not get if you do it the other way around.
