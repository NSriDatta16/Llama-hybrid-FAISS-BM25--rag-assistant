[site]: stackoverflow
[post_id]: 4108011
[parent_id]: 4107531
[tags]: 
As others have pointed out, the array size is no big deal. Something else is going wrong, e.g. you tried to put this on the stack as an automatic variable. I'll add that when working with data sets of this size, cache efficiency plays a big role in performance. Your algorithm should try to do two things: 1) Keep array reads clustered near each other in both time and memory address. This means don't jump around in the high order dimensions, e.g. cube faces, until you are done processing the low order dimensions, e.g. individual vertices. Reorganize the array dimensions as necessary to help achieve this. 2) Try to touch the array in a predictable incremental pattern in terms of memory address. This allows the cache prefetchers to proactively bring data into cache ahead of time. You could have all kinds of fun dividing the work into parallel tasks too.
