[site]: crossvalidated
[post_id]: 609772
[parent_id]: 
[tags]: 
Is the iid assumption in Linear Regression necessary?

In linear or logistic regression, we have the following (adapted from Foundations of machine learning.): As in all supervised learning problems, the learner $\mathcal{A}$ receives a labeled sample dataset $\mathcal{S}$ containing $N$ i.i.d. samples $\left(\mathbf{x}^{(n)}, y^{(n)}\right)$ drawn from $\mathbb{P}_{\mathcal{D}}$ : $$ \mathcal{S} = \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \ldots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \subset \mathbb{R}^{D} \quad \overset{\small{\text{i.i.d.}}}{\sim} \quad \mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\beta}\right) $$ I am used to the iid assumption in machine learning, but in the case of conditional maximum likelihood, I have the following question. To use maximum likelihood for linear/logistic regression, it is required to have $y \mid x$ to be independent, in other words, $y$ is conditionally independent of $x$ . The question is, do we need the strong iid assumption mentioned above for us to invoke MLE?
