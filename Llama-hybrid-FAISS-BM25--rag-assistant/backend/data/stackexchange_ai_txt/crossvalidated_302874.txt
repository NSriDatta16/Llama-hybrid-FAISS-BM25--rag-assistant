[site]: crossvalidated
[post_id]: 302874
[parent_id]: 163304
[tags]: 
From {1}: While it is not theoretically clear what is the additional power gained by the deeper architecture, it was observed empirically that deep RNNs work better than shallower ones on some tasks. In particular, Sutskever et al (2014) report that a 4-layers deep architecture was crucial in achieving good machine-translation performance in an encoder-decoder framework. Irsoy and Cardie (2014) also report improved results from moving from a one-layer BI-RNN to an architecture with several layers. Many other works report result using layered RNN architectures, but do not explicitly compare to 1-layer RNNs. FYI: The same question on the data science Stack Exchange: Advantages of stacking LSTMs? Is anyone stacking LSTM and GRU cells together and why? References: {1} Goldberg, Yoav. "A Primer on Neural Network Models for Natural Language Processing." J. Artif. Intell. Res.(JAIR) 57 (2016): 345-420. https://scholar.google.com/scholar?cluster=3704132192758179278&hl=en&as_sdt=0,5 ; http://u.cs.biu.ac.il/~yogo/nnlp.pdf
