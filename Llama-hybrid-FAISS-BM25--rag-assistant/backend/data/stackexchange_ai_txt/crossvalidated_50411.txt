[site]: crossvalidated
[post_id]: 50411
[parent_id]: 50404
[tags]: 
Starting with your comment in part 2: $P(D|H)$ is not to be read as "the probability of getting this data given that I select hypothesis H". It is saying "probability of getting this given that hypothesis H is true". Clearly therefore $P(D|H) + P(D|¬H) = P(D)$ In other words, the partition can always in principle be between H is true and H is not true. By the law of the excluded middle, this must cover everything. However, you are right to observe that if you are summing up different models that could explain the data, then there is no guarantee (or even reason to believe) that you have got "everything" covered (i.e. in practice it is usually impossible to know $P(D|¬H)$). That is why typically Bayesian analysis doesn't try to talk about the absolute probability of the model being correct given the data but only the relative probabilities of competing models. So although I can in theory I can state: $P(H_1 | D) = \frac{P(D|H_1) P(H_1)}{P(D)}$ $P(H_2 | D) = \frac{P(D|H_2) P(H_2)}{P(D)}$ When I do my analsis there is an implicit conditioning variable that is $M =$ "One of my models is correct". Perhaps being strict one should write: $P(H_1 | D, M) = \frac{P(D|H_1) P(H_1|M)}{P(D|M)}$ $P(H_2 | D, M) = \frac{P(D|H_2) P(H_2|M)}{P(D|M)}$ Noting that $P(H) = P(H,M)$ since if H is true then M is true. Now, however, notice that defined like this my priors must logically add up to 1. Often people don't have priors that add up to one. We could incorporate this by saying $P(H_1) = P(H_1|M)P(M)$. Writing our priors like this would then give us: $P(H_1 | D, M) = \frac{P(D|H_1) P(H_1)}{P(D|M)P(M)}$ $P(H_2 | D, M) = \frac{P(D|H_2) P(H_2)}{P(D|M)P(M)}$ Of course none of this makes the slightest difference, because we are only going to compare the models, so we calculate: $\frac{P(H_1 | D, M)}{P(H_2 | D, M)} = \frac{P(D|H_1,M) P(H_1)}{P(D|M)P(M)} \cdot \frac{P(D|M)P(M)}{P(D|H_2,M) P(H_2)} = \frac{P(D|H_1) P(H_1)} {P(D|H_2) P(H_2)}$ And all that intricacy cancels out and we are back at the original expressions. Is it just laziness not writing the extra conditional variables that exist? Not really no. To me this is just another facet of the conditionality principle. Edit To take your example, suppose that $H_1$ is that $D \sim N(m,v)$ and $H_2$ is that $D \sim N(m', v)$ then clearly these two do not partition the space fully. Since we cannot evaluate $P(D|¬H_1 \cup H_2)$ we cannot work out the true $P(D)$ and so cannot ever get the absolute probability that either model is correct. What we can do though is establish the ratio of those probabilities, since the unpartitioned part of the space cancels out (see above). This could then tell us that say, that $P(H_1|D)$ is 10 times more likely than $P(H_2|D)$. In order to derive this, we had to write down expressions involving $P(D)$, even though we knew we'd never actually be able to calculate them.
