[site]: crossvalidated
[post_id]: 579685
[parent_id]: 
[tags]: 
Is there an empirical rule for selecting the value of label smoothing?

I am wondering if there is any emperical rule for selecting the value of label smoothing when training a neural network. Let's define smoothed prediction targets in relation to a value $\epsilon$ to be: $$y'_k = \begin{cases} 1-\epsilon, y_k = 1\\ \epsilon / K, y_k = 0 \end{cases}$$ Where $K$ is the number of classes. I am specifically wondering if $K$ should affect the value of $\epsilon$ . My intuition tells me that a wider selection should translate to increased uncertainty and therefore larger $K$ should mean larger $\epsilon$ . Is there any agreed upon best practice for selecting the value of $\epsilon$ or should it be adjusted experimentally?
