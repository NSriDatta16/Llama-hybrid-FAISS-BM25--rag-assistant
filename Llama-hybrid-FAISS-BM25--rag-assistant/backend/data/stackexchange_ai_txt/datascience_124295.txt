[site]: datascience
[post_id]: 124295
[parent_id]: 
[tags]: 
What is a proper activation function with simulated-annealing trainer for neural network?

I'm developing a gpu-accelerated simulated annealing based neural network trainer library. Currently its stuck on how to converge on "array sorting by neural network 3:10:20:10:3 topology". With swish activation function for hidden layers: approximation of square root between 0 and 1 has 1.5% error (with parameters confined to -1 to 1 range) and does not get any better but completes in 2-3 seconds. y = x or z problem is learned in 5-6 seconds sorting for 3 element array is very bad With tanh activation: sqrt approximation is learnt in 5-6 seconds but gets better accuracy but is not symmetrical for all range of inputs for example close to zero inputs are not learned well exlusive or is completed in 7-8 seconds sorting problem converges better but still far from a viable solution. So, is there a better activation function for automatically bounded parameters? Because simulated annealing simply returns random values between a fixed range as parameters. I'm not using backpropagation. Currently, tanh looks (converges) better if some of inputs can be ignored but swish function is much faster and generalizes better but fails worse in array-sorting. Since simulated annealing is the trainer, it should be possible to solve non-linear non-continuous problems. Also I'm not sure if there are enough neurons to sort 3 elements. Because 3 elements with all possible values would make trillions of possible combinations. Currently example code has only simple std::sort applied to output for simplicity. Example codes are here: https://github.com/tugrul512bit/FastSimpleNeuralNetworkTrainer Algorithm works in this way: neurons are bound densely, all to all between layers output neurons always have tanh hidden neurons had tanh before but now replaced by swish simply feed forward runs for each data pair until all data samples are completed total output error rms value is returned to simulated annealing simulated annealing runs 1000s of these in parallel, just with different parameters when temperature is lowest, it re-heats for annealing for n times latest greatest parameters are picked and returned to user by a callback during training and once again after algorithm is completed. during training, training data comes from gpu cache (~32MB for rtx4000 series) and parameters come from local in-chip memory(~64kB). Due to this, it can't train neural networks with more than 30-40kB of parameters.
