[site]: crossvalidated
[post_id]: 632405
[parent_id]: 284723
[tags]: 
Yes, PCA only makes sense for a real inner product space. (We typically only consider finite-dimensional spaces, for which all inner product spaces are automatically Hilbert spaces.) Given a random real vector $\vec{X}$ with mean $\vec{0}$ , its principal components $\hat{w}^{(k)}$ are defined iteratively: $\hat{w}^{(k)}$ is defined to be the unit vector that is orthogonal to all previous principal components $\hat{w}^{(i)},\ 1 \leq i , and maximizes the variance of the real random variable $\vec{X} \cdot \hat{w}^{(k)}$ subject to that constraint. This definition implicitly assumes an inner product within the phrases "unit vector" and "orthogonal" and within the dot product. Another way to see why we need an inner product is to note that the covariance matrix $K_{ij} := \mathbb{E}(X_i X_j)$ is naturally thought of as a symmetric bilinear form. For a general real vector space, symmetric bilinear forms cannot be diagonalized; they only have a signature by Sylvester's law of inertia. But the principal components are the eigenvectors of the "covariance matrix". What's really going on is that we are implicitly using the inner product to convert the covariance symmetric bilinear form to a covariance self-adjoint linear operator , which can indeed be diagonalized to yield the principal components. But we need to (implicitly) use the inner product in order to even meaningfully talk about the eigenvectors of the covariance matrix.
