[site]: datascience
[post_id]: 9779
[parent_id]: 9724
[tags]: 
Should I use Self Organising Map to reduce the dimensionality? Not the first choice. For dimensionality reduction you have more handy and basic methods like PCA and NMF (if you don't have negative values) or Archetypal Analysis which are recommended to be used first. and then K-means to classify them into normal and abnormal? Red alarm! Kmeans is not for classification but clustering. Be careful about the fundamental understanding of what you are about to do otherwise you get confused. If you have labels K-NN for instance is a supervised (thus proper) method. Or I can use just K-NN to classify them into normal - abnormal without any dimensionality reduction? Impossible. As you already know you have the problem called Curse of Dimensionality where the euclidean distances are highly distorted. You need to reduce the dimenstionality before K-NN. The only class of methods I know which may not need dimentionality reduction are Kernel Methods e.g. SVM . How many features can I use with K-NN? Once I examined the Curse of Dimentionality with K-Means in MATLAB and I saw after 7 dimensions everything was distorted. It of course depends on the nature of the data (distribution,etc) but this is what I got from my experiment. And if later I'd like to find why this person has included into the abnormal class how can I find that this happened because of these two features, his weight according to his height? This question is so fuzzy and about the goal of Machine Learning. In Machine Learning you get a set of answers (called labels) and try to learn them. You are not supposed to say if they are right or not. So if you are said a specific guy is abnormal you should ask the one who provided labels (in the terminology we call him expert ) about the reason. If a guy with a certain weight and hight is called abnormal you only know this weight and hight indicate abnormal people. Why? Well, who knows? If I did not get the last question correctly please drop me a line in comments. Would be glad to help. My Suggestion I recommend you to apply PCA to your data and take the first 5 PCs and feed them to K-NN or LDA . You can plot some of these PCs against each other and see how they discriminate your classes. It gives you a good impression of what is happening. You can also plot the variance or entropy of all features beforehand and drop those features whose variance or entropy is lower than a threshold. Good Luck :)
