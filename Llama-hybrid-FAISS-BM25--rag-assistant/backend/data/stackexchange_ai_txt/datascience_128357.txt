[site]: datascience
[post_id]: 128357
[parent_id]: 
[tags]: 
What is the best way to train a neural network with a variable number of inputs?

Suppose I have a neural network with 5 inputs: [A,B,C,D,E] There is only 1 output. The expected accuracy of the model should increase when all 5 inputs are available, but often not all 5 inputs are available. For example, I might have cases where I only have a variable number of the inputs, e.g. [A,B,C,-,-] , [A,-,-,-,E] , [-,B,-,D,-] , [A,-,-,-,-] , [-,-,C,-,-] , [A,-,C,D,E] , etc. In such a situation, what is the best way to train or build the neural network? Are there any specific approaches or architectures recommended for this type of problem? A couple ideas that come to mind include: Double the number of inputs to the neural network by including a second "binary input vector" that determines whether the input variable is present or not. For example, the binary input vector for the inputs [A,-,C,-,E] would simply correspond to [1,0,1,0,1] , which could be fed into the neural network as well. The outstanding question is how does one treat the undefined variables with "-" as placeholders in such an example...perhaps defaulting to 0 for "-" is one naive but simple way when coupling the binary vector. Build and train separate neural network for every combination of [A,B,C,D,E] â€” this could certainly be implemented, but would be a brute force approach that requires a lot of training and be rather inefficient. For 5 variables, this would require 31 separate neural networks , and would scale poorly as the number of potential input variables increase
