[site]: crossvalidated
[post_id]: 577425
[parent_id]: 
[tags]: 
Variable importance differs between lasso and random forest (R)

I am working with a dataset (~4000 subjects, 40 predictors, five continuous outcome variables) and I am interested in both feature selection and identifying the ordering of importance of various predictor variables across five regression models. I was interested in exploring linear and nonlinear ML algorithms. I have used LASSO ( glmnet for feature selection and lars for ordering of importance via point of entry at which variables enter model). I have also used random forests ( rfsrc package) and explored variable importance via VIMP (OOB estimate). My problem is that lasso and random forests are not always consistent in determining which predictors have the most explanatory relevance (aka which are most "important"). The lasso predictors make more intuitive sense, but this does not seem like good science. Why would there be considerable differences? I have considered two reasons but I am probably missing more: Lasso explores linear relationship to DVs, random forest looks at nonlinear predictive relationships; Overall variance explained is low across models (6-15%), so weak predictors are inconsistent across algorithms. I feel like I'm missing a third explanation. Grateful for feedback.
