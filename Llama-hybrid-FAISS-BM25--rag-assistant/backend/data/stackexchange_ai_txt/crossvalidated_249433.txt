[site]: crossvalidated
[post_id]: 249433
[parent_id]: 
[tags]: 
Why I'm having different result when applying momentum with zero in CNN?

I did apply momentum optimization method to test the function so I realized the cost is exploding. so I decided to check my method by applying momentum of zero and here I realized the cost result is different by the new method I applied. $v = momentum * v - learning_rate * dw$ $w += v$ by applying this code I have the same result as the one below: class sgd_momentum(): def update_param(self,grads, params, learning_rate, momentum): updates = [] for n,(param_i,grad_i) in enumerate(zip(params,grads)): updates.append((param_i, param_i -learning_rate * grad_i)) return updates as the original code updates = [ (param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(classifier.params, grads) ] So the first part of the code is working correctly. the issue when I add the memory to the code to apply the momentum even when it is 0 by this code class sgd_momentum(): def __init__(self,params): self.memory_ = [theano.shared(np.zeros_like(p.get_value())) for p in params] def update_param(self,grads, params, learning_rate, momentum): updates = [] for n,(param_i,grad_i) in enumerate(zip(params,grads)): memory = self.memory_[n] first_update = momentum * memory - learning_rate * grad_i updates.append((memory, first_update)) updates.append((param_i, param_i+ memory)) return updates the cost result is different. I'm not saying it is not converging. actually the cost is going down slowly but the initial cost is different then the first two codes which indicate a different or wrong implementation. first thougt of the problem, I think that the memory and param_i are updated in concurrence and that's why the first update to param_i is zero
