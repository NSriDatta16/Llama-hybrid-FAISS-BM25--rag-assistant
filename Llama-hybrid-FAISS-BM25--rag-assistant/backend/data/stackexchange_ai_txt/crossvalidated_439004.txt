[site]: crossvalidated
[post_id]: 439004
[parent_id]: 
[tags]: 
Need some help understanding the factorised posterior in semi-supervised generative modelling

I am having a bit of trouble with the derivation in Kingma's semi-supervised generative modelling paper for the M2-model. The M2 model assumes a probabilistic model where the data $x$ is generated by hidden latents $z$ in addition to class variables $y$ , which can also be modelled as latents when unobserved in the semi-supervised case. The approximate posterior takes the form $q(z, y|x)$ which one assumes to be factorised such that $q(z, y|x)=q(z|x)q(y|x)$ . So far so good. However, Equation (4) of the paper when specifying the variational family of each variable has: $q(z|x,y) = \mathcal{N}(z|\mu_x(x,y),\text{diag}\left(\sigma_x(x,y)\right)$ $q(y|x) = \text{Cat}\left(y|\pi_x(x)\right)$ This makes sense as by applying product rule, $q(z, y|x)=q(z|x,y)q(y|x)$ . In Algorithm 2 of the paper, $q(y|x)$ is used to get an estimate of $y_i$ in the unlabelled case $x_i$ and then used to get $q(z_i|x_i,y_i)$ . In various implementations I have found on the internet: e.g. here and here - $x$ and $y$ are both used to get $z$ as per $q(z|x,y)$ . My main question is such: why can one assume a factorised posterior in the form $q(z, y|x)=q(z|x)q(y|x)$ yet implement it as $q(z, y|x)=q(z|x,y)q(y|x)$ ? The latent space of $q(z|x)$ and $q(z|x, y)$ will be vastly different as the former is a vanilla VAE where one would expect clustering based on digit class whilst the latter is a conditional latent space . Is the factorised statement correct because we use two separate neural networks to encode $z$ and $y$ ? Can we assume $q(z|x, y) \rightarrow q(z|x)$ in Kingma's formulation as he effectively marginalises $y$ in the unlabelled case by performing $\sum_y q(y|x)\mathcal{L}(x,y)$ ? If so, how does this affect using a Gumbel-Softmax where we have access to a sample of $y$ and do not need to marginalise. In other work e.g. Flexible Fairness by Disentanglement , the factorised posterior $q(z,b|x)=q(z|x)q(b|x)$ is consistent with the implementation; one encoder $q(z|x)$ and separate encoder $q(b|x)$ . My gut feeling is that using $q(z|x,y)$ is due to the semi-supervised aspect of the work..but I'm not so sure. Thank you for any help - this may be trivial but I am trying to get to the bottom of this.
