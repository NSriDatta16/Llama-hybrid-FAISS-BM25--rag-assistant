[site]: datascience
[post_id]: 121015
[parent_id]: 
[tags]: 
What is purpose of stacking N=6 blocks of encoder and decoder in transformer?

I was trying to understand transformer architecture from "Attention is all you need" paper. What is purpose of stacking $N=6$ blocks of encoder and decoder? Does higher blocks represent longer phrases and learns what longer phrases attend to? While bottommost block represent single word and its attention; something like how first layer of CNN represent pixel and deeper layers represent edges and further deeper layers represents shapes (like nose, hand etc.)?
