[site]: crossvalidated
[post_id]: 522976
[parent_id]: 522972
[tags]: 
Yes, but you may need to have a different margin penalty for patterns of each class. There have been a fair few papers on this, this is mine: G. C. Cawley and N. L. C. Talbot, Manipulation of prior probabilities in support vector classification, In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks (IJCNN-2001), pp. 2433-2438, Washington, D.C., U.S.A., July 15-19 2001. ( pdf ) It is important to remember though that if you use assymetric margin penalties (or equivalently resample or reweight the data based on class), it will probably over-predict the minority class in operational use, see my answer to this related question here . However, if you have a large class imbalance, the I would recommend regularised logistic regression instead. This is because in class imbalance problems it often comes down to a matter of whether false-positive or false-negative costs are different (which is what motivates differential margin penalties), and the SVM aims to construct a hard decision boundary and doesn't care how well it models the data distribution away from the boundary. This means you need to sort out the misclassification cost ratio before you fit the model. Regularised logistic regression on the other hand aims to estimate the probabilities of class membership, which means you can accommodate different misclassification costs after fitting the model, which is usually easier and more flexible as sometimes you don't know the costs a-priori or they are variable.
