[site]: crossvalidated
[post_id]: 580101
[parent_id]: 580092
[tags]: 
I'll take a somewhat different track from Demetri . There are multiple aspects here. A best quantification of prediction accuracy is surprisingly tricky. An accuracy measure is, for practical purposes, a mapping $f$ that takes your point prediction $\hat{p}$ (the predicted proportion of blue marbles, say) and a random sample $x$ from the bag and returns an "accuracy" value. "For practical purposes", because this again relies on sampling from the bag, because we will usually not know the true proportions. This is Demetri's argument. So we now are faced with an $f(\hat{p},x)$ that is a random variable, because it depends on the sample $x$ . (Typically, we would sample multiple $x$ and take the average, $\frac{1}{n}\sum_{i=1}^n f(\hat{p},x_i)$ .) We can now take a look at the statistical properties of our random variable $f(\hat{p},x)$ as a function of $\hat{p}$ . For instance, we can require that the expected accuracy be maximized of the predicted proportion $\hat{p}$ is equal to the true proportion $p$ in expectation . This will quickly lead to proper scoring rules and the log loss (another term for the entropy Demetri notes). The log loss has this property: it will elicit an unbiased estimate $\hat{p}$ of $p$ . Alternatively, we could of course look at error measures we want to minimize (instead of maximizing accuracy - it's the same thing, of course). Here is a trap for the unwary. You might be tempted to use something like the Mean Absolute Error between $\hat{p}$ and the proportion of blue marbles you draw. This amounts to $$ f(\hat{p},x)=\begin{cases} 1-\hat{p} & \text{if $x$ is blue} \\ \hat{p} & \text{if $x$ is red} \end{cases} $$ This looks similar but simpler than the log loss. Instead of the log, we essentially have an absolute value. However, here is the problem: the MAE will not be minimized by the true proportion. Rather, if $p>0.5$ , the MAE will be minimized by setting $\hat{p}=1$ , regardless of the actual value of $p$ ! You can either calculate the expected value of the MAE for each given $\hat{p}$ analytically, or trust a little simulation: for a true $p=0.8$ , I looked at $\hat{p}=0, 0.01, 0.02, \dots, 1$ , simulated $10,000$ draws from the urn in each case, calculated the MAE, and plotted the MAE against the prediction $\hat{p}$ . While I was at it, I did the same using the Mean Squared Error (AKA the Brier Score) and the log loss instead of the MAE (with a minus sign, so we want to minimize the log loss), and added a red vertical line at the true proportion $p=0.8$ : As you see, using the MAE will reward us more (have lower errors) for a biased prediction $\hat{p}=1$ than for the unbiased estimate $\hat{p}=p=0.8$ . (If $p , the MAE will like $\hat{p}=0$ best.) Thus, if you want an unbiased estimate of $p$ , you should not use the MAE. Alternatives include the Mean Squared Error, or the log loss per Demetri. Well, now we have two error measures, the MSE and the log loss, that will at least not systematically mislead us (like the MAE). How do we choose between these two? (And of course, there are infinitely many other such error metrics which will elicit an unbiased prediction.) Now the question is how painful deviations from the true $p$ are for you. As you see in the simulated plots, as the predicted proportion $\hat{p}$ varies away from the true $p$ , the log loss and the MSE behave rather differently. And you can of course use transformations of these error metrics to change their behavior further. As Demetri writes, it comes down to whether a prediction of $\hat{p}=0.6$ or $\hat{p}=1$ is more painful if the true $p=0.8$ - where in practice you will not know the true $p$ . A few thoughts specifically on the comparison between the log loss and the MSE/Brier score can be found at Why is LogLoss preferred over other proper scoring rules? Finally, one last aspect. You ask about "standardizing" error metrics to the interval $[0,100\%]$ . As Demetri writes, I do not think this is truly possible. On the one hand, you would need to know when you have reached 100% accuracy, and in a practical situation, this simply won't happen. Similarly, it's hard to say whether you have reached 99% or 99.9% accuracy. It might be easier to assign 0% accuracy to a prediction - if you are certain that $p>0.5$ , then you could say that a prediction of $\hat{p}=0$ is "as wrong as it can be", and you could say that this prediction is 0% accurate. Same for $\hat{p}=1$ if you know that $p . The problem here is if you are unsure about the true $p$ . And of course this only works because we know (actually do know here!) that $0\leq p\leq 1$ - if you are predicting, say, temperatures, it's very hard to say what is the "worst possible prediction" that we would like to say is 0% accurate. R code for the plots: true_prop
