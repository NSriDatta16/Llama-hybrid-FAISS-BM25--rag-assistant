[site]: datascience
[post_id]: 37240
[parent_id]: 
[tags]: 
can I use public pretrained word2vec, and continue train it for domain specific text?

I have a set of reviews from apparel domain, about 100K reviews (2M words). And I want to train word2vec to do some cool NLP staff with it. However the size is not enough for creating adequate word2vec model, it requires billions of words. So the idea is to use public corpora (e.g. wikipedia), or even use some pretrained models (e.g. from gensim cool framework) and add my domain specific text. I assume the model will get aware of unseen-in-public words, and could correct vectors for common word. Does it make sense ? will the 2M words makes any effect at all ?
