[site]: datascience
[post_id]: 114875
[parent_id]: 104680
[tags]: 
Adding to the idea of checkpointing proposed by @thanatoz, a few more things you can do in such cases is to use gradient accumulation and mixed-precision training . This will reduce the amount of GPU memory necessary to run the model. I used these techniques to train a custom BERT-large model on long input sequences (512 tokens). Hopefully, it will help you too.
