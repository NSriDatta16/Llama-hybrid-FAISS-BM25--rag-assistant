[site]: crossvalidated
[post_id]: 172915
[parent_id]: 172848
[tags]: 
Your intuition is quite correct. In most situations, feature selection represents a desire for simple explanation that results from three misunderstandings: The analyst does not realize that the set of "selected" features is quite unstable, i.e., non-robust, and that the process of selection when done on another dataset will result in a quite different set of features. The data often do not possess the information content needed to select the "right" features. This problem gets worse if co-linearities are present. Pathways, mechanisms, and processes are complex in uncontrolled experiments; human behavior and nature are complex and not parsimoneous. Predictive accuracy is harmed by asking the data to tell you both what are the important features and what are the relationships with $Y$ for the "important" ones. It is better to "use a little bit of each variable" than to use all of some variables and none for others (i.e., to use shrinkage/penalization). Some ways to study this: Do more comparisons of predictive accuracy between the lasso , elastic net , and a standard quadratic penalty (ridge regression) Bootstrap variable importance measures from a random forest and check their stability Compute bootstrap confidence intervals on ranks of potential features, e.g., on the ranks of partial $\chi^2$ tests of association (or of things like univariate Spearman $\rho$ or Somers' $D_{xy}$) and see that these confidence intervals are extremely wide, directly informing you of the difficulty of the task. My course notes linked from http://biostat.mc.vanderbilt.edu/rms have an example of bootstrapping rank order of predictors using OLS. All of this applies to both classification and the more general and useful concept of prediction.
