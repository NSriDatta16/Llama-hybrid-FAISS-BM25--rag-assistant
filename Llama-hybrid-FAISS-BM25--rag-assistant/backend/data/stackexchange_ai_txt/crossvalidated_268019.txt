[site]: crossvalidated
[post_id]: 268019
[parent_id]: 
[tags]: 
Iteratively reweighted least squares in Machine Learning Probability perspective

I am studying Machine Learning Probability perspective and I have a question with Algorithm 8.2, which is $\mathbf{w} = 0_{D}$ $w_{0} = \log (\bar{y}/(1-\bar{y}))$ Repeat: $\eta_{i} = w_{0}+\mathbf{w}^{T} \mathbf{x}_{i}$ $\mu_{i} = \text{sigm}(\eta_{i})$ $s_{i} = \mu_{i}(1-\mu_{i})$ $z_{i} = \eta_{i}+\frac{y_{i}-\mu_{i}}{s_{i}}$ $\mathbf{S} = \text{diag}(s_{1:N})$ $\mathbf{w} = (\mathbf{X}^{T}\mathbf{S}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{S}\mathbf{z}$ until converged I think $w_{0}$ means the coefficient of intercept, but why it is not updated in the loop? It seems like $w_{0}$ is fixed like a constant?
