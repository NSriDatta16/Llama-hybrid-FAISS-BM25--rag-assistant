[site]: crossvalidated
[post_id]: 326899
[parent_id]: 326830
[tags]: 
Glen's list is good. I'm going to add 1 more application to complement his answer: deriving conjugate priors for Bayesian inference. A core part of Bayesian inference is deriving posterior distributions $p(\theta|y) \propto p(y|\theta) p(\theta)$. Having a prior $p(\theta)$ that is conjugate to the likelihood $p(y|\theta)$ means that the posterior $p(y|\theta)$ and prior $p(\theta)$ will belong to the same class of probability distributions. The useful property I'm referring to is that, for a likelihood of $n$ observations drawn from a one-parameter exponential family of the form $p(y_1,\ldots,y_n|\theta) = \prod p(y_i|\theta) \propto g(\theta)^n \exp \big[ h(\theta) \sum t(y_i) \big]$, we can simply write out a conjugate prior as $p(\theta) \propto g(\theta)^\nu \big[ h(\theta) \delta \big]$ and then the posterior works out as $p(\theta|y_1,\ldots,y_n) \propto g(\theta)^{n+\nu} \exp \big[ h(\theta) \big( \sum t(y_i) + \delta \big) \big]$ Why is this conjugacy useful? Because it simplifies both our interpretation and computation while performing Bayesian inference. It also means we can easily come up with analytical expressions for the posterior without having to do too much algebra.
