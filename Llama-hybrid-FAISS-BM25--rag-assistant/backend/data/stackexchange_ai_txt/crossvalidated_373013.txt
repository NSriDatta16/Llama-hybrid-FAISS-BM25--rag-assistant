[site]: crossvalidated
[post_id]: 373013
[parent_id]: 
[tags]: 
Residual learning approach + manifold

From: Wikipedia on Residual learning in ANNs "The intuition on why this works is that the neural network collapses into fewer layers in the initial phase, which makes it easier to learn, and thus gradually expands the layers as it learns more of the feature space. During later learning, when all layers are expanded, it will stay closer to the manifold and thus learn faster. A neural network without residual parts will explore more of the feature space. This makes it more vulnerable to small perturbations that cause it to leave the manifold altogether, and require extra training data to get back on track." I don't not understand The intuition on why this works is that the neural network collapses into fewer layers in the initial phase, which makes it easier to learn, and thus gradually expands the layers as it learns more of the feature space What is he talking about collapsing earlier and expanding later? This is a static network. Are the saying the learning is initially more towards simpler features so the skip connections carry most of the information and as the training goes on more features are learnt through the later actual layers? What exactly is the learning progression described here? During later learning, when all layers are expanded, it will stay closer to the manifold and thus learn faster. A neural network without residual parts will explore more of the feature space. This makes it more vulnerable to small perturbations that cause it to leave the manifold altogether, and require extra training data to get back on track." What is meant by 'manifold' in this context? What is he saying about feature space and perturbations?
