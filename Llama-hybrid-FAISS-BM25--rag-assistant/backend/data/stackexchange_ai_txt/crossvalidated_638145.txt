[site]: crossvalidated
[post_id]: 638145
[parent_id]: 
[tags]: 
Understanding "Understanding the difficulty of training deep feedforward neural networks"

I'm following up on this question with a slightly more specific clarification I'd like to have addressed. I'm well familiar with covariance matrices as a matrix-valued generalization to random vectors of (co)variance of random variables. However, in Understanding the difficulty of training deep feedforward neural networks , I'm a little unclear at what the authors mean by the variance of some of the terms, specifically in equation $(5)$ . It looks like the paragraph of assumptions before equation $(5)$ introduces that the weight distributions (in aggregate?) of the random matrices $W^{i'}$ are all independent of one another and of the inputs. Moreover, from context it seems they should be zero-centered (entrywise, I suppose). For scalar independent zero-mean r.v.'s $u$ , $v$ it is plain to me that $\mathbb{V}(uv) = \mathbb{V}(u) \mathbb{V}(v)$ , and from the linearity of $f$ (in fact, that $f(x) \approx x$ as a scalar function or as a coordinatewise-applied vector function) it's clear that $\mathbb{V}(\bf{z}^i) = \mathbb{V}(\bf{z}^{i-1}W^{i-1})$ - that is, assuming linearity of variance in this general context. How, from this (or from first principles if you don't care to read my work too closely), can we obtain equation $(5)$ ? The equation is below: $$ \mathbb{V}(\mathbf{z}^i) = \mathbb{V}(\mathbf{x}) \prod_{i' = 0}^{i-1} n_{i'}\mathbb{V}(\mathbf{W}^{i'})$$
