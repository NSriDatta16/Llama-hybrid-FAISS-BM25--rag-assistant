[site]: datascience
[post_id]: 126217
[parent_id]: 126197
[tags]: 
There are a few options there: Different models on HuggingFace GPT2 should fit your use case. If performance is really critical, I'd give distilGPT2 a try. They only generate English sentences but you should be able to translate things well using something like T5 . "Manual" generation NLTK provides a way to generate sentences using context-free grammar. It might be more time consuming, and might not yield sentences that make sense, but you would definitely do great on performance. from nltk import CFG from nltk.parse.generate import generate grammar = CFG.fromstring( """ S -> NP VP VP -> V NP | V NP PP PP -> P NP V -> "saw" | "ate" | "walked" NP -> "John" | "Mary" | "Bob" | Det N | Det N PP Det -> "a" | "an" | "the" | "my" N -> "man" | "dog" | "cat" | "telescope" | "park" P -> "in" | "on" | "by" | "with" """ ) # Generating sentences for sentence in generate(grammar, n=5): print(" ".join(sentence)) Build your own model Back in 2015, I saw this article: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ from Andrej Karpathy. I played around with it and realized that it was relatively easy to generate reasonable text using only a very small RNN. You'd need training data, but your model would be much smaller than a GPT2 or an mT5
