[site]: datascience
[post_id]: 66466
[parent_id]: 66426
[tags]: 
You could do that with a neural autoencoder using a custom loss function. Use a hidden layer, let's call it $l_{encoded}$ , with more nodes than the features of the input data. You have to code the custom loss: $loss = corr(o(l_{encoded}))+MSE(o(l_{output}),\ input)$ , where $corr(o(l_{encoded}))$ is the correlation of the output of the encoding layer and $MSE(o(l_{output}), input)$ is the mean squared error of the last layers output and the input instance. Using this loss your model will try to reduce the correlation of the hidden layer while still making sure that it is able to decode the training instance. I highly doubt this will be of any use though.
