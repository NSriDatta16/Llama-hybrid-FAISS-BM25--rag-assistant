[site]: crossvalidated
[post_id]: 326610
[parent_id]: 
[tags]: 
Importance of assumptions in linear regression

I have been in conflict with my study materials. I began learning about linear regression from a stats professor who emphasised on fulfilling the 4 assumptions on errors when performing linear regression. This also meant that in-sample performance measures such as AIC,BIC and R2 are mostly used to estimate model performance. As i begun to delve more into statistical learning and machine learning, I realised that the materials stopped focusing on all of these assumptions, because out-of-sample performance is the most important criteria of model performance. Ideas like regularisation and cross-validation further emphasis the importance of out-of-sample performance. I am now attempting to reconcile the need to even look at assumptions, and determine when exactly do they matter. As a data analyst, I am more concerned with model inference (which variables affect Y most), as compared to predictions. I hope to hear some advice on how I can possibly reconcile these two school of thoughts, or if there's a need to take a side.
