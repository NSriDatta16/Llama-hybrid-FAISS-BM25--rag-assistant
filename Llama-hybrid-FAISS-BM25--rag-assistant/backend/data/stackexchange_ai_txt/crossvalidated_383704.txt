[site]: crossvalidated
[post_id]: 383704
[parent_id]: 383702
[tags]: 
Why do we use multiple epochs? Researchers want to get good performance on non-training data (in practice this can be approximated with a hold-out set); usually (but not always) that takes more than one pass over the training data. Additionally, and independent of any out-of-sample considerations, it's typical that gradient descent doesn't reach a (global or local) minimum after the first epoch. For any problem, there's no reason to believe that the number of updates required to reach a minimum happens to coincide with the number of mini-batches in your data set. This observation isn't unique to neural networks, but is common to all iterative model estimation procedures. For example, logistic regression, SVM and gradient boosting all use multiple rounds of updates to estimate models. Why does this not lead to overfitting? One definition of overfitting is that performance on the training set improves while the performance on the hold-out data gets worse . Prolonged training can cause overfitting. Training a model enough just means that the model gets better. There's nothing magical about 1 pass over the data or 2, or 3, ... passes over the data. You can turn the question on its head and ask "why does the model not overfit after seeing 25% (or 1%, or 50%, or ...) of the training data?" Maybe 25% (or 1%, or 50%, or ...) of the training data does cause overfitting (for some problem, for some model, for some regularization strategy)! That's why people monitor performance against a hold-out and terminate when the hold-out starts getting worse.
