[site]: crossvalidated
[post_id]: 458316
[parent_id]: 
[tags]: 
How to add a $\beta$ and capacity term to a variational autoencoder?

Vanilla variational autoencoders add a Kullback–Leibler divergence (KL) term to the loss function, i.e. the loss is a combination of the reconstruction error (e.g. cross entropy or MSE) and the KL divergence, the latter of which assumes the following form: $$ -D_{KL} = \frac{1}{2} \sum (1 + log(\sigma^2) - \mu^2 - \sigma^2) $$ In practise, this typically assumes the following form in a keras custom loss function: # y_true and y_pred are 2d arrays of batch_samples x n_features def vae_loss(y_true, y_pred): # keras MSE returns per sample mean of MSE across features reconstruction_loss = losses.mse(y_true, y_pred) # weight reconstruction loss by dimensionality reconstruction_loss *= raw_dim # per sample and per latent dimension kl loss kl_loss = 1 + z_log_var - K.square(z_mu) - K.exp(z_log_var) # sum across the latent dimensions for a per-sample kl loss kl_loss = -0.5 * K.sum(kl_loss, axis=-1) # add the per sample reconstruction and kl_loss and return the batch mean return K.mean(reconstruction_loss + kl_loss) $\beta$ -VAE variants encourage better disentangling through use of the $\beta$ parameter, which can be used to increase the emphasis on the Kullback–Leibler divergence (KL) in the loss function, i.e. increased disentangling of the latent dimensions but generally worse reconstruction, i.e. the KL part of the loss function becomes: $$ -\beta(\frac{1}{2} \sum (1 + log(\sigma^2) - \mu^2 - \sigma^2)) $$ In the paper Understanding disentangling in $\beta$ -VAE the authors propose adding an annealed capacity term $C$ that increases the informational capacity of the autoencoder. This term is increased from $0$ to full strength of $C$ over successive epochs of training. $$ -\beta(|\frac{1}{2} \sum (1 + log(\sigma^2) - \mu^2 - \sigma^2) - C|) $$ As far as I understand this is a relatively straight-forward matter of adding the $\beta$ and capacity terms to the loss function. I've found two differing implementations: A) One approach first takes the mean across all samples per latent dimension: # per sample and per latent dimension kl loss kl_loss = 1 + z_log_var - K.square(z_mu) - K.exp(z_log_var) # take kl loss mean across all samples per latent dimension kl_loss = -0.5 * K.mean(kl_loss, axis=0) # then take the sum across latent dimensions kl_loss = K.sum(kl_loss) # apply beta and capacity kl_loss = beta * K.abs(kl_loss - capacity) B) Another approach first sums across the latent dimensions, then takes the mean across batch samples: # per sample and per latent dimension kl loss kl_loss = 1 + z_log_var - K.square(z_mu) - K.exp(z_log_var) # take kl loss sum across latent dimensions kl_loss = -0.5 * K.sum(kl_loss, axis=1) # then take the mean across latent dimensions kl_loss = K.mean(kl_loss) # apply beta and capacity kl_loss = beta * K.abs(kl_loss - capacity) (Not shown is a callback function which increases the capacity term from 0 to full strength over the duration of the training epochs.) I wish to confirm: Are the above interpretations of the annealed capacity $\beta$ -VAE loss function correct? The authors of the $\beta$ -VAE paper discuss normalising $\beta$ where $\beta_{norm} = \frac{\beta M}{N}$ is multiplied by the number of latent dimensions and dividing through the number of (batch) samples. What do people typically do in practise? Similarly, should reconstruction loss be scaled by the number of latent dimensions, or is it better to only scale the $\beta$ parameter? At the step casting the combined $(D_{KL} - C)$ term to an absolute number: my interpretation is that this step prevents this part of the loss function from approaching too close to zero (per strength of $C$ ). i.e. if the capacity of $C$ is greater than the $D_{KL}$ loss it would actually lead to an increase in the $D_{KL}$ loss, as if it encourages at least a certain amount of $D_{KL}$ loss in order to increase the information carrying capacity? Is this intuition correct and are there situations where casting negative values to positive could cause the loss function to behave oddly? Thanks.
