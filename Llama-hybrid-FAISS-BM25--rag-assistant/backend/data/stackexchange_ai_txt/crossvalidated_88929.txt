[site]: crossvalidated
[post_id]: 88929
[parent_id]: 
[tags]: 
Does Akaike information criterion penalize model complexity any more than is necessary to avoid overfitting

The AIC penalizes complex models. Obviously a certain penalty for complex models is necessary to avoid overfitting of statistical models: otherwise we would favour a model which is simply a copy of the data itself, and that would tell us nothing. Does the AIC penalize complexity any more than is strictly necessary to avoid this occurance, however? Edit: As an example of what I mean, wiki page for Occam's Razor references a textbook showing that a presumption in favour of simpler models is not required in Bayesian stats: There have also been other attempts to derive Occam's Razor from probability theory, including notable attempts made by Harold Jeffreys and E. T. Jaynes. The probabilistic (Bayesian) basis for Occam's Razor is elaborated by David J. C. MacKay in chapter 28 of his book Information Theory, Inference, and Learning Algorithms,[32] where he emphasises that a prior bias in favour of simpler models is not required. Is the AIC merely reflecting the frequentist equivalent of the same effect, or does it favour simplicity over and above that?
