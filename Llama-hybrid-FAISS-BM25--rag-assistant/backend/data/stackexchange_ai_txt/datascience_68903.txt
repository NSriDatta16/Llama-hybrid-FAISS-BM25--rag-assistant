[site]: datascience
[post_id]: 68903
[parent_id]: 
[tags]: 
Understanding Terminology in Goodfellow's paper on GANS

I am trying to understand Ian Goodfellow et al's paper Generative Adversarial Nets here . In section 3 the author's write: The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generatorâ€™s distribution $p_g$ over data $x$ , we define a prior on input noise variables $p_z(z)$ , then represent a mapping to data space as $G(z; $ $\theta_g)$ , where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\theta_g$ . I have a few questions. 1) From what I understand, the generator is a multi-layer perceptron input with a random vector sampled from a predefined latent space, (where wikipedia gives the example of a multivariate normal distribution). What does it mean to learn the generator's distribution $p_g$ over data $x$ ? 2) What does "we define a prior on input noise variables $p_z(z)$ " mean? I understand that in Bayesian statistical inference, that a prior probability distribution, called a prior for short, gives a probability distribution on outcomes before some evidence is collected. Any insights appreciated.
