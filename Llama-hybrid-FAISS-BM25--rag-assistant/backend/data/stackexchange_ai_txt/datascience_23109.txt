[site]: datascience
[post_id]: 23109
[parent_id]: 23075
[tags]: 
The firs solution sevo proposes is not feasible because of a third problem that was not mentioned. The first layer only learns a first representation of the input, which is used in later layers. Even if the absolute weights of $x_1$ might be very big, if the later layers have small weights connected to these neurons the importance goes down. This is exactly why neural networks are considered to be difficult to interpret. The rest of the answer is useful, I just wanted to add this.
