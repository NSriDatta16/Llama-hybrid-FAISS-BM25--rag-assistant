[site]: crossvalidated
[post_id]: 524061
[parent_id]: 524043
[tags]: 
Short answer: dimensions of $K$ and $V$ are $(seq\_len \times d_k)$ and $(seq\_len \times d_v)$ respectively. $d_k$ and $d_v$ are parameters, but they are often taken as $d_k=d\_model/nheads$ and $d_v=d_k$ Long answer: I think you are mixing up two concepts here: the dimensions of K/V in the attention calculation and the dimensions of the Decoder. Let's walk through the dimensions of the calculation of self-attention first. This is given by $$ softmax(\frac{Q^TK}{\sqrt{d_k}})\cdot V $$ Going a bit into the details, our input sequence $X$ has dimensions $seq\_len \times d\_model$ . $X$ is multiplied by weight matrices $W^Q$ , $W^K$ , and $W^V$ with dimensions $(d\_model\times d_k)$ , $(d\_model\times d_k)$ , and $(d\_model\times d_v)$ respectively. Therefore, the matrices $Q$ and $K$ have dimensions $(seq\_len \times d_k)$ and the V matrix has dimensions $(seq\_len \times d_v)$ . The overall dimensions from the attention calculation has dimensions $seq\_len \times d_v$ . However, we multiply this by a final weight matrix, $W^0$ with dimensions $(d_v \times d\_model)$ . This final step returns us to the original dimensions of $(seq\_len\times d\_model)$ . Now for the decoder, our input, $X$ is a sequence of another length, say $seq\_len2$ . However, our key and value matrices which we compare to are still from the encoder. Then our $Q$ , $K$ , and $V$ matrices now have dimensions $(seq\_len2 \times d_k)$ , $(seq\_len \times d_k)$ , $(seq\_len \times d_v)$ . Walking through the same calculations give the decoder output to have dimensions $(seq\_len2 \times d\_model)$ .
