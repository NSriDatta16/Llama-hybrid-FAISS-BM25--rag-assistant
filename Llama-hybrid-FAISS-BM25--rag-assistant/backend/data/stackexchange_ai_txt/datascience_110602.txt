[site]: datascience
[post_id]: 110602
[parent_id]: 79669
[tags]: 
A key issue in NLP is to encode text into a numerical representation. Embeddings are used for this purpose. word embeddings [...] is a method used when attempting to predict the next word in a text Not really. Embedding is a transformation of sparse data into a dense space. This is often used in NLP to take into account similarity among words. Predicting the next word (from previous words and other information) is a language model . how can [embeddings] be used to derive the weight/impact of each word on the target variable in my case Think about one alternative way to represent text numerically: the one hot encoding , where each word is a long vector (the size of your dictionary) made of zero values everywhere except at the index representing that word. The embedding representation, being dense, is much more informative: consider the case of "similar" words, (or more specifically, words used in a similar context): an embedding will consider "dog" and "wolf" as similar vectors, whereas in one hot encoding they will be considered as independent, as much as "dog" and "independence" (just dummy examples).
