[site]: crossvalidated
[post_id]: 342505
[parent_id]: 342466
[tags]: 
Different terminology suggests different conventions. The term "residual" implies that it's what's left over after all the explanatory variables have been taken into account, i.e. actual-predicted. "Prediction error" implies that it's how much the prediction deviates from actual, i.e. prediction-actual. One's conception of modeling also influences which convention is more natural. Suppose you have a dataframe with one or more feature columns $X = x_1,x_2...$, response column $y$, and prediction column $\hat y$. One conception is that $y$ is the "real" value, and $\hat y$ is simply a transformed version of $X$. In this conception, $y$ and $\hat y$ are both random variables ($\hat y$ being a derived one). Although $y$ is the one we're actually interested in, $\hat y$ is the one we can observe, so $\hat y$ is used as a proxy for $y$. The "error" is how much $\hat y$ deviates from this "true" value $y$. This suggests defining the error as following the direction of this deviation, i.e. $e = \hat y -y$. However, there's another conception that thinks of $\hat y$ as the "real" value. That is, y depends on $X$ through some deterministic process; a particular state of $X$ gives rise to a particular deterministic value. This value is then perturbed by some random process. So we have $x \rightarrow f(X)\rightarrow f(X)+error()$. In this conception, $\hat y$ is the "real" value of y. For example, suppose you're trying to calculate the value of g, the acceleration due to gravity. You drop a bunch of objects, you measure how far they fell ($X$) and how long it took them to fall ($y$). You then analyze the data with the model y = $\sqrt{\frac{2x}{g}}$. You find that there's no value of g that makes this equation work exactly. So you then model this as $\hat y = \sqrt{\frac{2x}{g}}$ $y = \hat y +error$. That is, you take the variable y and consider there to be a "real" value $\hat y$ that is actually being generated by physical laws, and then some other value $y$ that is $\hat y$ modified by something independent of $X$, such as measurement errors or wind gusts or whatever. In this conception, you're taking y = $\sqrt{\frac{2x}{g}}$ to be what reality "should" be doing, and if you get answers that don't agree with that, well, reality got the wrong answer. Now of course this can seems rather silly and arrogant when put this way, but there are good reasons for proceeding this conception, and it can be useful to think this way. And ultimately, it's just a model; statisticians don't necessarily think this is actually how the world works (although there probably are some who do). And given the equation $y = \hat y +error$, it follows that errors are actual minus predicted. Also, note that if you don't like the "reality got it wrong" aspect of the second conception, you can view it as being "We've identified some process f through which y depends on $X$, but we're not getting exactly the right answers, so there must be some other process g that's also influencing y." In this variation, $\hat y = f(X)$ $y = \hat y+g(?)$ $g = y-\hat y$.
