[site]: datascience
[post_id]: 107311
[parent_id]: 107278
[tags]: 
My assumption is that it used to be a necessary step when computing resources were scarce, but with today's resources it is basically irrelevant. You are entirely right. In the early days of computing, when resources were scarce, it was necessary just to keep important features and discard the rest. However, with the current abundance of resources, that is no longer necessary. The model will not overfit more with "useless" features included. Indeed, there is a branch of Machine Learning called "reservoir computing" where you purposely generate random features in the hope that a subset of them are still useful for the model. For instance, you can checkout the algorithm called "Rocket" for time series prediction.
