[site]: crossvalidated
[post_id]: 434459
[parent_id]: 433844
[tags]: 
In both a GAN model and a VAE model, there is a decoder network $g$ (in GAN literature, the decoder is usually called the generator instead). In both cases, we define a latent variable $z$ to be distributed normally, usually $z \sim \mathcal{N}(0, I)$ . A typical GAN or VAE model defines a density on images: $$p(x) = \int p(z)p(x|z) dz $$ where $p(x|z)$ is gaussian, with mean and variance determined by the generator network: $\mu, \sigma = g(z; \theta)$ . In the case of this GAN work, only $\mu$ was determined by $g$ , and $\sigma^2$ , the "conditional variance of the generator", was fixed as a hyperparameter. You can sample an image $x$ by first sampling some $z$ from the standard normal prior, then sample $x$ from $\mathcal{N}(\mu,\sigma=g(z;\theta))$ . In both the case of VAE and GAN, you'll eventually want to backpropagate some function of $x$ , and compute a value such as $\nabla_\theta f(x)$ . Notation overload warning: what is " $x$ " in the equation in OP is $\mu$ here, and what is $x$ here is " $x + \epsilon$ " in OP. This is problematic because sampling isn't a function you can differentiate through, so it's not clear how to compute the gradient. One approach taken by Kingma / Welling / Rezende is to "reparameterize" $x$ as $x = \mu + \sigma \epsilon$ where $\epsilon$ is a distributed as a standard normal. Basically this shifts the sampling operation over to $\epsilon$ , so that it doesn't "get in the way" of differentiation. Of course the resulting $x$ is still drawn from the same distribution as before. Note that you can backpropagate through both $\mu$ and $\sigma$ (which are functions of $\theta$ ) now. Goodfellow had a different solution to this problem: just substitute $\nabla_\theta f(x)$ with $\nabla_\theta f(\mu)$ . Equation 1 states that in the limit, the expectation of the former is the latter.
