[site]: crossvalidated
[post_id]: 197337
[parent_id]: 
[tags]: 
How to analyze temporal data in regression

I have monthly temperature averages for a weather station across 100 years. I am wondering how I should analyze this data in a regression. The data are set up in the following fashion: year month temp.avg 1900 11 9 1900 12 6 1901 01 5 1901 02 4 .... 2015 12 7 My question is: how do I go about accounting for the time and incorporating it into my model? Here are my 4 proposed methods: Should I add a "time" variable which essentially counts my months from 1 through n rows of data? Ex: Using the example above, I would have a "time" column of 1,2,3,4,etc. I essentially lose the actual year & month data, but does this matter? -- can I just add back, for example, "Dec 1900" for time 1? Given a month i in year j , should I create a continuous time by adding 0.0833 (1/12) to each year for each i-1 month? Ex. Again using the above example, I would have a "time" column consisting of values 1900, 1900.8333, 1900.9167, 1901.0000, 1901.0833, etc... The linear regression model for the prior two methods would essentially be: lm(temp.avg ~ time) Do I just incorporate year and month (or perhaps more usefully season ) in the model together? This would result in: lm(temp.avg ~ year + month) Or is 3 wrong and instead I'd have to create a dummy variable for each month (or season)? lm(temp.avg ~ year + jan + feb + mar + apr ...) So which is correct? I assume perhaps the questions I'm asking would dictate this to some degree. But perhaps someone could describe simply the validity of each method and when to apply each? Note: I understand that I will have to account for temporal autocorrelation, but I'm wondering how I incorporate time data prior to worrying about that. I will note that I perform my analyses in R .
