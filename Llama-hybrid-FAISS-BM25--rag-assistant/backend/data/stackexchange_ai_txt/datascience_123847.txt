[site]: datascience
[post_id]: 123847
[parent_id]: 84338
[tags]: 
First, your linear probability model is quite different from a logistic regression. The latter would have no problem separating these classes; the coefficients would blow up to infinity trying to push the predicted log-odds out to $\pm\infty$ , but wherever you stop the process you will have perfect separation. So this is really more of a regression problem than a classification one. And the two dimensions are really just distracting from the point, so here's a version with just one dimension: The blue line is the fitted linear regression, the orange what I imagine you think looks more natural. The problem is in the rather large number of dots near $x=\pm5$ (e.g., but of course taking all the points into consideration): under the orange model, the squared error there is significantly larger, and since there are so many more of them than the ones with $y=1$ , the total loss $J$ is better off with the blue line.
