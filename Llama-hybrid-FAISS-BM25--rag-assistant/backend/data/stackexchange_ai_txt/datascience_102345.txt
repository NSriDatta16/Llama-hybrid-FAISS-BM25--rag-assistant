[site]: datascience
[post_id]: 102345
[parent_id]: 
[tags]: 
Could I directly apply techniques for hyper-parameter tuning, and choose the best model?

I have noticed in some sources the author first trains the model (say a model from scikit-learn ) with the default hyper-parameters, and the model naturally gives a result. Then, they would try to optimize the hyper-parameters, even if the parameter grid is containing the same default parameters (for example with Exhaustive Grid Search), and then the optimal model is chosen with the best parameters. While I was practicing, I have done the same steps, but after I dissected the process, I realized that this is probably redundant. If Exhaustive Grid Search (or any other technique) involves training the model with various combinations of hyper-parameters, isn't it more reasonable to directly use these techniques, and directly obtain the best model for the problem, instead of trying to tune the model with the default hyper-parameters, which will almost always results in an improved performance. Like this piece of code, from the official site of scikit-learn . Fit the pre-processed training dataset with the best model, and move on from here with the project: parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} svc = svm.SVC() clf = GridSearchCV(svc, parameters) clf.fit(iris.data, iris.target) Additionally, are there cases where tuning the models would not be the wisest idea, or no matter what, always tuning the models is the best practice?
