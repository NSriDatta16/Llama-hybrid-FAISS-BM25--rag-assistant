[site]: datascience
[post_id]: 112674
[parent_id]: 
[tags]: 
Why are we training Segment Embedding in BERT?

In BERT we have segment embeddings that are used for "Segment Embeddings with shape (1, n, 768) which are vector representations to help BERT distinguish between paired input sequences." Yes, but why. There are just 2 sentences, why are we making it so complicated and using 768-sized vector representation for 0 or 1? And we are adding them to the token and positional embeddings. So it will be like : Segment Embeddings: [0.321,0.231,...,0.434,0.312,0.123] Position Embeddings: [0.123,0.6435,...,0.231,0.121,0.321] Even If we sum those embeddings, summation will be like any embedding. How this summation will make model distinguish between paired input sequences?
