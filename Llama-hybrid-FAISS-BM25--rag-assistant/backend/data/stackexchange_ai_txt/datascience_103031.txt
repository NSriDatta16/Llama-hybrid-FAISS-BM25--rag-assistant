[site]: datascience
[post_id]: 103031
[parent_id]: 
[tags]: 
Best platform to work with when having millions of rows in dataframe

I have table with around 20 features and millions of observations (rows). I need to create model base on this table, however, as it is huge, training models like random forest or XGB takes forever. I'm working mainly with scikit-learn and the XGBoost packages on Jupyter lab server, using python, and i'm struggling with this when the dataframes are very large. Also it is important to mention that I have windows (not Linux). My question is for people with more experience than I have: what way do you deal with huge dataframes? are there any better packages or platforms to work with when the data is so big?
