[site]: datascience
[post_id]: 9285
[parent_id]: 
[tags]: 
Term for calculated values that lose pertinence when changing scale

I'm trying to find the term for a type of calculations or values that cannot be simpply added or multiplied when zooming in or out from a temporal scale. I know it's not very clear, if it were I would just google the question. Here's an example: say you're comparing 2014 vs 2015 sales of ice cream and temperature at day's level. If you want to do an analysis by week, you can just add the sales for seven days, and compare weeks between them, the information still makes sense: there's a fact (amount of money deposited in the bank that week) that is grounded in reality that relate to the new figure. You could also calculate an average by day from the total by week. But, adding temperatures for a total would not make any sense in reality, you need to calculate an average for that kind of data. So is there a term for this type of values that can only make sense as average and never be added or multiplied (except as an intermediate step for calculating an average)?
