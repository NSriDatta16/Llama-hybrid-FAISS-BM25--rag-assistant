[site]: datascience
[post_id]: 121980
[parent_id]: 121979
[tags]: 
When you perform $k$ -fold cross-validation you obtain $k$ models, but each $M_k$ is independently trained and so you don't start training from the previous model $M_{k-1}$ : sequential training happens in boosting ensemble (like AdaBoost), which is unrelated to $k$ -fold cross-val. Training various models on different subset of the dataset allows to estimate the bias-variance of them: you get $k$ models which implicitly define a distribution over their performance. If you aggregare them (i.e. take the mean of their accuracy, for example) you get an estimate of the average performance of the class of models (e.g. SVM + chosen hyper-parameters) that you've used. Also, you can compute the variance in both predictions and accuracy: that provides you a means to estimate the uncertainty of your models. Indeed, you obtain $k$ models. So, you can either pick the best of them (e.g. the one that achieves the best accuracy) or aggregate them in an ensemble.
