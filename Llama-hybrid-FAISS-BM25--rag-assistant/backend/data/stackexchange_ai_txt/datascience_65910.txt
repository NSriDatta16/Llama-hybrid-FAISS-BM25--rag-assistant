[site]: datascience
[post_id]: 65910
[parent_id]: 65909
[tags]: 
Xgboost does the feature selection for you. If you want to report how valuable certain features are for prediction, print the feature importances. Those will, however, just tell you "feature $x_1$ is very important for predicting the outcome, feature $x_2$ is nearly useless for predicting the outcome, etc.". To get risk factors with p-values, you need a more interpretable model. You have to use linear classification, for example. Then you can make statements like "high $x_1$ correlates with a positive outcome". If you really want to only use a subset of features, train an Xgboost model on a validation dataset and drop features that have low importance. Then run an Xgboost model with the remaining features on the remaining training set. You need to think about what you want: Do you want to explain observations and extract explicit knowledge from data and try to find possible causal links? Use linear methods. Do you want to predict an outcome for new patient data? Use gradient boosting. You can obviously do both.
