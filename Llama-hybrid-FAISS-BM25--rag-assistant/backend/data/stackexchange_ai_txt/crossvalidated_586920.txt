[site]: crossvalidated
[post_id]: 586920
[parent_id]: 472820
[tags]: 
I presume you made sense out of it since then, but for those who may encounter this question: if $p_i$ is the dropout probability for layer $i$ , weight averaging is about multiplying the weights in layer $i$ by $1-p_i$ at test time. Indeed, dropout is activated during training time, but switched off at test time, so this ensures layer activations scale as expected.
