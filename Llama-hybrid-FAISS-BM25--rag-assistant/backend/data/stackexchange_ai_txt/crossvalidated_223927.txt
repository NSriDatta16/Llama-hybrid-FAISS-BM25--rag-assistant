[site]: crossvalidated
[post_id]: 223927
[parent_id]: 223912
[tags]: 
is k-means' performance a bottleneck anywhere? It is unclear for the definition of "bottleneck" and anywhere. So, it is hard to answer this question directly. For my experience (6 years with academia and industry) is that K-Means is still a widely used algorithm for its simplicity. When you want to apply K-Means, features used are very important. Because it is based on Euclidean distance, curse of dimensionality is a problem. Also see here for why Euclidean distance will not work well in high dimension case.( Why is Euclidean distance not a good metric in high dimensions? ) The computation / optimization process is not as critical as the dimensional problem. Another critical problem with Kmeans is that it can suffer from and local minima. Dimensional and local minima are more important things people care comparing to the execution speed. Do people really care about it? Too subjective to answer. People I worked thinks Kmeans is a reasonable approach as well as you can justify it. Do people need these faster versions of k-means? As mentioned earlier, computation time is not as critical as multidimensional problem. Do people run k-means in hadoop clusters? On GPUs and using libraries like tensorflow? Will k-means be used on google TPUs? I personally think there is not too need for this. Because the most computation intensive task would be "feature extraction" and "feature engineering". Those things can be done in a big server. The extracted and aggregated feature may not be too large and the algorithm can be executed in a desktop. In my work, the raw data is on Tb level, but aggregated data is just few Mb.
