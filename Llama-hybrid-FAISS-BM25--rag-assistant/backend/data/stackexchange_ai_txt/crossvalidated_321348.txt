[site]: crossvalidated
[post_id]: 321348
[parent_id]: 38797
[tags]: 
Hierarchical classification models frequently fail for different reasons. This is why flat classification methods based on one-vs-rest are usually preferred. One of the main reasons discussed in the literature is that once an error is made in the upper levels of the hierarchy the model has no way to recover. To analyze this type of problem in your case you should calculate the error in the first step P vs I binary problem. This will be highly indicative as if the accuracy of your model is low there, it will be even lower in the last level. Following that, the design of the hierarchy is also an issue. Things that are intuitive for humans do not necessarily yield the best performance. In your case for instance, it may be intuitive to split the problem as you describe ( P vs I ). From a machine learning perspective however, this may be sub-optimal due the the specificity of your data etc.. To better understand this, imagine that P vs I is difficult (not many examples of the I class, similar features...). There may be though another problem like I1 Vs Others that is simpler and better suited for the root of your hierarchy. This is a serious issue to be considered when designing your hierarchy. You simply need to start from the simpler (easier) problem (top of the hierarchy) and put more difficult problems to the bottom because you can not overcome from errors. By the way, there are several, arguably simpler ways to deal with imbalanced datasets like adding weights to the infrequent classes, subsampling the frequent class, oversampling the infrequent and others. You may want to try them first.
