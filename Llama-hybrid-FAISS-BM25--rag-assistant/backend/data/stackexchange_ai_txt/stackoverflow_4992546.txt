[site]: stackoverflow
[post_id]: 4992546
[parent_id]: 4962163
[tags]: 
Your use case sounds like a combination of Classification and Feature Selection. Statistics Toolbox offers a lot of good capabilities in this area. The toolbox provides access to a number of classification algorithms including Naive Bayes Classifiers Bagged Decision Trees (aka Random Forests) Binomial and Multinominal logistic regression Linear Discriminant analysis You also have a variety of options available for feature selection include sequentialfs (forwards and backwards feature selection) relifF "treebagger" also supports options for feature selection and estimating variable importance. Alternatively, you can use some of Optimization Toolbox's capabilities to write your own custom equations to estimate variable importance. A couple monthes back, I did a webinar for The MathWorks titled "Compuational Statistics: Getting Started with Classification using MTALAB". You can watch the Webinar at http://www.mathworks.com/company/events/webinars/wbnr51468.html?id=51468&p1=772996255&p2=772996273 The code and the data set for the examples is available at MATLAB Central http://www.mathworks.com/matlabcentral/fileexchange/28770 With all this said and done, many people using Principal Component Analysis as a pre-processing step before applying classification algorithms. PCA gets used alot When you need to extract features from images When you're worried about multicollinearity
