[site]: crossvalidated
[post_id]: 639494
[parent_id]: 637988
[tags]: 
I did some research on my own and came up with the following two advantages of BF16 mixed precision training: Less memory : Even though BF16 and FP16 both require 2 bytes per parameter, the memory requirements of BF16 training are actually lower than in FP16 training. Because of the low precision of BF16, the gradients in BF16 training are directly stored in FP32 and consequently do not require an extra copy [1] . No loss scaling : As pointed out in the comments above, BF16 requires no loss scaling as opposed to FP16 and may lead to a better accuracy. Faster : Since BF16 is basically the FP32 number's first 2 bytes, converting between BF16 and FP32 is as easy as discarding the last 2 bytes of the FP32 number and copying the BF16 number into the first 2 bytes of the FP32 number's storage location. Converting between FP16 and FP32 instead requires more logic because of the differently sized mantissa [2] . Furthermore, in FP16 mixed precision training, gradients are constructed in FP16, then converted to FP32 for the parameter update. This step isn't needed in BF16 training because gradients are directly stored in FP32. Lastly, the number of operations per iteration is reduced a bit because gradient/loss scaling isn't needed anymore. Silicon area of FMA units : Osorio et al. (2022) mention that the silicon area of fused multiply-add (FMA) units grows quadratically with the mantissa length, so specialized BF16 hardware can achieve a higher throughput than FP32 units with the same chip size. For now, I leave this question open and hope for a more authoritative answer. Above advantages are based on theoretical analysis and not entirely backed by experimental results. In my experiments, the throughput is consistently higher in BF16 training than FP16 training, but there are a few configurations in a distributed setting where the memory requirements of BF16 exceed those of FP16. Consequently, I believe the higher speed is the most compelling advantage of BF16, while no loss scaling and less memory requirement are either contributing factors or not generally true.
