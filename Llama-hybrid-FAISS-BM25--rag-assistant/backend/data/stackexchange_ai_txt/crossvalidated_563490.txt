[site]: crossvalidated
[post_id]: 563490
[parent_id]: 563484
[tags]: 
Your input has shape (channels, time). The output has shape (out, time), such that the output vectors are arranged sequentially in time. You haven't told us how your features relate to your label, but typically the prediction at the last time-step is the one that you use as the prediction of the label (e.g. predict the last word of a sentence given some, possibly all, previous words). This is the same for training and testing. A useful outline of different LSTM models can be found in Andrej's Karpathy's blog post " The Unreasonable Effectiveness of Recurrent Neural Networks ." Presumably you're using something like a sigmoid or softmax activation in the final layer to give a vector of probabilities. A predicted probability is not a label, but it does tell you about the model's estimate of the probability of each label given the input. If you truly need to dichotomize your predictions, then you'll need to use some appropriate rule to relate the probabilities to the outcomes, and ideally this rule will be informed by the relative costs of the different kinds of error associated (FN, FP). Some more elaboration: Reduce Classification Probability Threshold
