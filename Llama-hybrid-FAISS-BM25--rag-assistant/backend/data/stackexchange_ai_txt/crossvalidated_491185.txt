[site]: crossvalidated
[post_id]: 491185
[parent_id]: 327497
[tags]: 
I think the key part for these two loss functions is that they are unbounded due to their using logs. That is, with $x \in [0,1]$ , as $x \rightarrow 0$ , $\log{x} \rightarrow -\infty$ . Here, "brittleness" seems to be implying that certain records can unduly influence the loss, so that a (small) number of these in the training set (and not the test set) would overly influence the loss score and so the update of training parameters, causing overfitting. The cross-entropy loss function for classification for $n$ observations and some parameters $w$ , predictions $\hat{y}$ and actual values $y$ is: $$J(w) = -\frac{1}{n}\sum_{i=1}^{n}{(y_i \log\hat{y}_i + (1-y_i)\log{(1-\hat{y}_i)})} $$ You can see that if some prediction $\hat{y}_k$ is very small in this, then $\hat{y}_k$ will unduly influence the loss, especially if the true value $y_k$ is nearer to 1. Of course the function is trying to be expensive in these cases, but it's unbounded, so Langford is saying for some combinations of actual/model distribution, and some records, it's unusually expensive, and without already knowing the true distribution, this is always a risk. The same problem exists with mutual information. A similar problem still exists with other loss functions e.g. high-leverage points in OLS, but it is not unbounded for a finite input domain, and for OLS at least, can be tested for and handled in all cases, using e.g. an influence matrix.
