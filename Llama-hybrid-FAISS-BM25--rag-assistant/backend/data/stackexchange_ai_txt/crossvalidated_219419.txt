[site]: crossvalidated
[post_id]: 219419
[parent_id]: 218604
[tags]: 
@ttnphns' point about pairwise vs multivariate association is well taken. Related to that is the old saw about the importance of demonstrating association with simple metrics before leaping into a multivariate framework. In other words, if simple pairwise measures of association show no relationship then it becomes increasingly unlikely that multivariate relationships will show anything either. I say "increasingly unlikely" because of a reluctance to use the word "impossible." In addition, I am agnostic as to the metric employed whether it be a monotonic Spearman correlations for ordinal data, Somer's D , Kendall's Tau , polychoric correlation, the Reshef's MIC, Szelkey's distance correlation, whatever. The choice of metric is not important in this discussion. The original work done on finding latent structure in categorical information dates back to the early 50s and Paul Lazersfeld, the Columbia sociologist. Essentially, he invented a class of latent variable models that has seen extensive development and modification since. First, with the 60s work of James Coleman, the U of C political economist, on latent voter election propensities, followed by the contributions of the late Clifford Clogg, also a sociologist, whose MELISSA software was the first publicly available latent class freeware. In the 80s, latent class models were extended from purely categorical information to finite mixture models with development of tools such as Latent Gold from Statistical Innovations. In addition, Bill Dillon, a marketing scientist, developed a Gauss program for fitting latent discriminant finite mixture models. The literature on this approach to fitting mixtures of categorical and continuous information is actually quite extensive. It's just not as well known outside of the fields where it has been most widely applied, e.g., marketing science where these models are used for consumer segmentation and clustering. However, these finite mixture model approaches to latent clustering and contingency table analysis are considered old school in today's world of massive data. The state-of-the-art in finding association among a huge set of contingency tables are the decompositions available from deploying tensor models such as those developed by David Dunson and other Bayesians at Duke. Here is the abstract from one of their papers as well as a link: Contingency table analysis routinely relies on log linear models, with latent structure analysis providing a common alternative. Latent structure models lead to a low rank tensor factorization of the probability mass function for multivariate categorical data, while log linear models achieve dimensionality reduction through sparsity. Little is known about the relationship between these notions of dimensionality reduction in the two paradigms. We derive several results relating the support of a log-linear model to the nonnegative rank of the associated probability tensor. Motivated by these findings, we propose a new collapsed Tucker class of tensor decompositions, which bridge existing PARAFAC and Tucker decompositions, providing a more flexible framework for parsimoniously characterizing multivariate categorical data. Taking a Bayesian approach to inference, we illustrate advantages of the new decompositions in simulations and an application to functional disability data. https://arxiv.org/pdf/1404.0396.pdf
