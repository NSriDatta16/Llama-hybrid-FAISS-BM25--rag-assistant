[site]: crossvalidated
[post_id]: 431037
[parent_id]: 399950
[tags]: 
I am no expert, but I have been reading up a bit. I have read that binary/categorical variables will dominate the clustering when you use Gower (because they take the most extreme values, 0 and 1, while other variables are constrained to be between 0 and 1). It is possible to tweak Gower distance to use a different normalization and to deal with missing values, but that involves coding. In a high dimensional data set, the clustering can be mucked up by addition of irrelevant variables (I believe) so it can make sense to try to reduce the dimensionality. (One suggestion below. Others use PCA, I think, but that is going to be sensitive to outliers.) Regarding skewed data, I believe skewness is a big problem for most clustering, and I would advise using Verardi and Vermandele (Outlier identification for skewed and/or heavy-tailed unimodal multivariate distributions) to remove outliers, and then use Box-Cox to de-skew the data (if log doesn't work). (Not sure how to de-skew categorical variables though, hahaha.) Then you can use a clustering algorithm. Wang, Yabes and Chang (Hybrid Density- and Partition-based Clustering Algorithm for Data with Mixed-type variables) describes a way to go about clustering, including removal of variables, with mixed data. It is a multi-step process, and you could add the skewness step I mentioned above. I don't know about Ward's.
