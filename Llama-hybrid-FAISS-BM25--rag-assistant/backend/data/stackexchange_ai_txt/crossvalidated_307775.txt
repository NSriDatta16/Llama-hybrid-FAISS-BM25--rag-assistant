[site]: crossvalidated
[post_id]: 307775
[parent_id]: 307722
[tags]: 
Suppose you have a n-inputs 1-output useful predictor that predicts $P(Y=1|X)$. Logistic regression or random forest for example. One possibility to implement it for a p-output $Y=(Y_1,Y_2,...Y_p)$ is the following: Train predictor of $P(Y_1=1|X)$ Train predictor of $P(Y_2=1|X,Y_1)$ ... Train predictor of $P(Y_p=1|X,Y_1,Y_2...Y_{p-1})$ Now you want to estimate $P(Y=(y_1,y_2...y_p)|X)$. Just use conditional probabilities: $P(Y=(y_1,y_2...y_p)|X)=P(Y=y_1|X)P(Y_2=y_2|X,y_1)... P(Y_p=y_p|X,y_1,...,y_{p-1})$ This is just a possibility. The ordering is somehow arbitrary. Efficiency will obviously depend on the predictor you use. This could give better results than mere multinomial regression on all $2^p$ bins of $Y$, since a certain underlying independence of the correlation of the outputs with each other is assumed.
