[site]: crossvalidated
[post_id]: 333684
[parent_id]: 333676
[tags]: 
The "optimal" number of epochs for a neural network training is not very reliable (i.e. has high variance), so generally I would recommend against it. The benefit you get from using a few more samples is usually less than the benefit a validation set gives you (early stopping). See this answer for more details. Otherwise, you are correct. If you change the size of the dataset while keeping the batch size equal, an "epoch" corresponds to a different number of weight updates, which are the real measure of training length. You would have to work with the update count rather than the epoch count. See this answer for details.
