[site]: datascience
[post_id]: 101865
[parent_id]: 
[tags]: 
Generating unique points with an auto-encoder

I have been working on some research using a type of auto-encoder to generate new points with specific desirable properties. I trained my network and successfully generated some points, but when I compare them to my training data using nearest neighbor distance (NND), the resulting values are lower than I would like (although since I am somewhat new to machine learning I am not sure what an appropriate threshold value for uniqueness would be). I think this might be in part related to how large my training dataset is (it makes sense to me that with less training points to compare to, I would have larger NND's for my generated points), but because my current model is doing a very good job at generating points with specific desirable properties, I am hesitant to change it. I wanted to see what the common solutions for this sort of problem are before I start changing the number of data points I use in training. I was also wondering about whether it would be useful to add an NND-based term into the loss function when training, although I imagine this would slow things down significantly. So in total my questions are: 1.) What is a reasonable NND threshold for uniqueness 2.) What are some good ways to bring about higher NND values for generated points 3.) Is an NND-based loss term a reasonable thing to consider? Thank you!
