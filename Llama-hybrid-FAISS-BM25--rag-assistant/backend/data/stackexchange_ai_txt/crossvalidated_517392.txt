[site]: crossvalidated
[post_id]: 517392
[parent_id]: 
[tags]: 
Does marginal likelihood have closed-form solution for hyperparameters in Bayesian linear regression?

We know that marginal likelihood has the following form in Bayesian linear regression, $$ \mathbf{K} = \sigma_w^2XX^T + \sigma_n^2I\\ p(\mathbf{y}|X) \sim \mathcal{N}(0, K)\\ \log p(\mathbf{y}|X) = -\frac{1}{2} \mathbf{y}^{T} K^{-1} \mathbf{y} - \frac{1}{2}\log|K| - \frac{n}{2}\log (2 \pi) $$ Can we estimate $\sigma_n$ and $\sigma_w$ in closed form by maximizing log marginal likelihood or with any other way (other than numerical methods such as gradient descent)? I am also interested to know if this is possible for Gaussian process regression where $K$ could be RBF or Matern or any other kernel?
