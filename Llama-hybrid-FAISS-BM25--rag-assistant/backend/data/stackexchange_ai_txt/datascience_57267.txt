[site]: datascience
[post_id]: 57267
[parent_id]: 
[tags]: 
Why use regularization instead of decreasing the model

Regularization is used to decrease the capacity of a machine learning model to avoid overfitting. Why don't we just use a model with less capacity (e.g. decrease the number of layers). This would also benefit the computational time and memory. My guess would be that different regularization methods make different assumptions of the dataset. If so, what assumptions are made for the common regularizations (L1, L2, dropout, any other) Thanks in advance!
