[site]: crossvalidated
[post_id]: 636516
[parent_id]: 
[tags]: 
Bayesian optimization for solving least squares

Bayesian optimization with Gaussian processes (GPs) is an effective minimization methodology when the evaluation of the function to minimize, say $f(a)$ , is computationally expensive. Loosely speaking, the strategy requires a GP, chosen to emulate $f$ but much easier to compute. Let us assume that $f(a) = (g(a) - y)^2$ , where $y$ is a set of data points and $g(a)$ is the result of a compute-intensive simulation model. We wish to find the value $a_0$ of $a$ such that the compute-intensive model $g(a)$ ''reproduces'' the data $y$ . $f(a)$ has now some new properties compared to $g(a)$ (for example it is positive and convex around $a=a_0$ , while $g(a)$ might not be so) and a typical Gaussian process (which is centered at zero) is not a good surrogate for it. Does literature provide alternatives to GPs which incorporate those properties? Or did I miss something else?
