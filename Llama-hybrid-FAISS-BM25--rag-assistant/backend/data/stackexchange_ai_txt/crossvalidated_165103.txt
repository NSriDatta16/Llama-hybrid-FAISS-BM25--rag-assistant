[site]: crossvalidated
[post_id]: 165103
[parent_id]: 
[tags]: 
Why such a dissimilar solution to logistic regression and linear regression?

In machine learning, linear regression and logistic regression are very common. The solutions to them are to find a parameter to minimize the energy function. Specifically, given a training data samples: $\{x^{(i)},y^{(i)}\}$, for linear regression, we try to find a $\theta$ to minimize $\sum_i(y^{(i)}-\theta^Tx^{(i)})^2$. However, for logistic regression, although the model is $y\sim\frac{1}{1+\exp(-\theta^Tx)}$, when we solve it, we maximize the likelihood instead of finding a $\theta$ to minimize $\sum_i(y^{(i)}-\frac{1}{1+\exp(-\theta^Tx)})^2$, just like linear regression. Why the solutions are so different?
