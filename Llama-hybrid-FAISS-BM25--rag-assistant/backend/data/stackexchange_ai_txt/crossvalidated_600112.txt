[site]: crossvalidated
[post_id]: 600112
[parent_id]: 
[tags]: 
Gini Feature Importance for XGBoost Classification Not making sense

I am trying to look at the feature importance in my model. The most important feature according to the gini importance in my xgboost is %97 zero in my dataset. Meaning that 97% of all values in the column are 0, and it is the most important feature according to the model. Also, when I plot the SHAP values, that feature is ranked at the last place. How come that feature is the most important feature for my model? I would think that it would not be important for a model. Can anyone tell me why that might be? Edit: This is a multiclass classification model with 3 target classes. This feature takes values from 0 to 150, so it is not binary. It just so happens that 97% of the rows are equal to 0, the rest of the 3% of rows ranging from 1.45232 to 150.522 etc. There are 70 features in my model. Class 0 has 75% prevalence Class 1 has 12% prevalence Class 2 has 13% prevalence. Features are mostly highly correlated within groups, because it has mean,median, std, max, min values for 3 feature groups.
