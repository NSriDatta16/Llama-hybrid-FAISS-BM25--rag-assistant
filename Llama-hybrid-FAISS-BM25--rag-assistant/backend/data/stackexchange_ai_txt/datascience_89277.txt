[site]: datascience
[post_id]: 89277
[parent_id]: 89266
[tags]: 
I assume it will probably be more beneficial for you to focus on getting a sample of the different techniques in use rather than learning the theoretical frameworks, probability bounds, and so on. In that case, I'd say the following sections are most useful to you: Elements of Statistical Learning . Chapters 1â€“4 should give you a good flavour of 'traditional' classification and regression techniques such as nearest neighbours, linear regression, logistic regression, etc. Chapter 7 is also recommended by the authors as essential as it goes through the principles of selecting a model: these concepts will apply no matter what method you use. From there you could sample what you were interested in: chapter 11 on neural networks might be interesting, although you might find a more modern treatment preferable. I like Deep Learning by Goodfellow, Bengio and Courville, which you could browse through and choose relevant chapters. Support vector machines (chapter 12) are also pretty popular and worth being familiar with, and in the second edition you might read chapter 15 on random forests, too. Chapter 10 on boosting might be fun, too: XGBoost and similar often win machine learning competitions, so knowing the background would be helpful. Understanding Machine Learning . Part I has a rather theoretical slant; I suspect many practitioners don't know much about PAC learning and VC dimension, although they are interesting if you have the time. In Part II you could review any methods you don't see in ESL, and take a look at the unsupervised learning techniques in Part III. Part IV is again theoretical and probably not as relevant.
