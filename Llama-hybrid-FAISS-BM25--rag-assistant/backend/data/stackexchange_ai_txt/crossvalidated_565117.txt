[site]: crossvalidated
[post_id]: 565117
[parent_id]: 
[tags]: 
Gaussian closed form solution of marginal likelihood

i have tried some time now to understand a specific step in the derivation of what I think is a marginalization integral. I am still learning about these statistical things and I think I miss somethign to get behind it. The equation is from the paper "Sparse Bayesian Learning and the Relevance Vector Machine" by M.E. Tipping. So the integral is given as (Equation 15): $$\begin{aligned} p(\mathbf t|\mathbf \alpha, \sigma^2)&=\int{p(\mathbf t|\mathbf w, \sigma^2)p(\mathbf w| \mathbf \alpha)d\mathbf w}\\ &= \frac{1}{(2\pi)^{-N/2}|\sigma^2\mathbf I+\mathbf\Phi\mathbf A^{-1}\mathbf{\Phi}^T|^{1/2}}\exp{\{-\frac{1}{2}\mathbf t^T(\sigma^2\mathbf I+\mathbf \Phi\mathbf A^{-1}\mathbf \Phi^T)^{-1}\mathbf t\}} \end{aligned}$$ With $A=\text{diag}(\alpha_0, \dots, \alpha_N)$ . $\alpha_i$ are denote the precisions and $\Phi$ is a matrix of basis functions. I dont understand how this integral is solved to arrive at the given solution. The densities inside the integral are known Gaussian: $$ \begin{aligned} p(\mathbf t|\mathbf w, \sigma^2)&=\frac{1}{(2\pi \sigma^2)^{N/2}}\exp{\{-\frac{1}{2\sigma^2}\Vert\mathbf t-\mathbf \Phi \mathbf w\Vert^2 \}}\\ p(\mathbf w|\mathbf \alpha)&=\prod^N_{i=0}N(w_i|0,\alpha_i^{-1}) \end{aligned} $$ I think the second density can be written as $$ p(\mathbf w|\mathbf \alpha)=\frac{1}{(2\pi)^{N/2}|A|^{1/2}}\exp{\{-\frac{1}{2}\mathbf w^T\mathbf A\mathbf w \}} $$ Could someone give a little hint on the mathematical steps to arrive at the solution? I am keen to try it my self but my attempts so far where not very fortunate. I have tried to first multiply the two densities in the integral, but integration over $\mathbf w$ doesn't seem so trivial. Is there a "theoretical reasoning" step involved so that one can just "know" the result is Gaussian and then find the mean and vriance matrix somehow? EDIT: I have brought the exponential term $$ \frac{1}{2\sigma^2}\Vert\mathbf t-\mathbf \Phi \mathbf w\Vert^2 + \frac{1}{2}\mathbf w^T\mathbf A\mathbf w$$ into the form $$ \begin{aligned} &=\frac{1}{2}\mathbf{w}^T \mathbf{A}\mathbf{w}+\frac{1}{2\sigma^2}\mathbf{w}^T\mathbf{\Phi}^T\mathbf{\Phi}\mathbf{w}-\frac{1}{\sigma^2}\mathbf{t}^T\mathbf{\Phi}\mathbf{w}+\frac{1}{2\sigma^2}\mathbf{t}^T\mathbf{t}\\ &=\frac{1}{2}\mathbf{w}^T(\mathbf{A}+\frac{1}{\sigma^2}\mathbf{\Phi}^T\mathbf{\Phi})\mathbf{w}-\frac{1}{2}\mathbf{t}^T\mathbf{\Phi}\mathbf{w}+\frac{1}{2\sigma^2}\mathbf{t}^T\mathbf{t} \end{aligned} $$ As suggested by @Xi'an. I have been further searching in literature and I found a common step is "completing the squares", in this case with respect to $\mathbf w$ . So I guess I have to bring the above into a form: $$ (\mathbf{w}-\mathbf{x})^T\mathbf{C}(\mathbf{w}-\mathbf{x})+\mathbf{c}=\mathbf w^T\mathbf C \mathbf w - 2\mathbf x^T \mathbf C \mathbf w + \mathbf x^T \mathbf C \mathbf x + \mathbf c $$ Where $\mathbf C$ will be a symmetric matrix and $\mathbf c$ a term that is constant in $\mathbf w$ . Comparing the terms from the target form and my equation I could see: $$ \mathbf C=\frac{1}{2}(\mathbf A +\frac{1}{\sigma^2}\mathbf \Phi^T \mathbf \Phi) $$ and for $\mathbf x$ , $\mathbf c$ something along the lines of $$ \begin{aligned} \mathbf x^T &= \mathbf t^T\mathbf\Phi \mathbf C^{-1}\\ \mathbf x &= \mathbf C^{-1,T}\mathbf\Phi^T\mathbf t\\ \mathbf c &= \frac{1}{2\sigma^2}\mathbf t^T\mathbf t-\mathbf x^T\mathbf C\mathbf x \end{aligned} $$ Maybe I have done some awful mistakes here, I am still not super confident with this math. I don't see where I could go from here. Since the $\mathbf c$ term in the exponent does not depend on the integration variable $\mathbf w$ , I understand that I can pull it out of the integral. But the remaining quadratic form seems difficult to integrate. I also don't see how this will lead to the given solution.
