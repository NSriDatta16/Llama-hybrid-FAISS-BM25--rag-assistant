[site]: crossvalidated
[post_id]: 61303
[parent_id]: 
[tags]: 
Fisher for dummies?

Short version: is there an introduction to Ronald Fisher's writings (papers and books) on statistics that is aimed at those with little or no background in statistics? I'm thinking of something like an "annotated Fisher reader" aimed at non-statisticians. I spell out the motivation for this question below, but be warned that it's long-winded (I don't know how to explain it more succinctly), and moreover it's almost certainly controversial, possibly annoying, maybe even infuriating. So please, skip the remainder of this post unless you really think that the question (as given above) is too terse to be answered without further clarification. I've taught myself the basics of many areas that a lot of people would consider difficult (e.g. linear algebra, abstract algebra, real and complex analysis, general topology, measure theory, etc.) But all my efforts at teaching myself statistics have failed. The reason for this is not that I find statistics technically difficult (or any more so than other areas I've managed to find my way through), but rather that I find statistics persistently alien , if not downright weird , far more so than any other area I've taught myself. Slowly I began to suspect that the roots of this weirdness are mostly historical, and that, as someone who is learning this field from books, and not from a community of practitioners (as would have been the case if I had been formally trained in statistics), I would never get past this sense of alienation until I learned more about the history of statistics. So I've read several books on the history of statistics, and doing this has, in fact, gone a looong way in explaining what I perceive as the field's weirdness. But I have some ways to go in this direction still. One of the things that I have learned from my readings in the history of statistics is that the source of much of what I perceive as bizarre in statistics is one man, Ronald Fisher. In fact, the following quote 1 (which I found only recently) is very consonant both with my realization that only by delving into some history was I going to begin to make sense of this field, as well as my zeroing in on Fisher as my point of reference: Most statistical concepts and theories can be described separately from their historical origins. This is not feasible, without unnecessary mystification, for the case of "fiducial probability". Indeed, I think that my hunch here, albeit subjective (of course), is not entirely unfounded. Fisher not only contributed some of the most seminal ideas in statistics, he was notorious for his disregard of previous work, and for his reliance on intuition (either providing proofs that hardly anyone else could fathom, or omitting them altogether). Furthermore, he had lifelong feuds with many of the other important statisticians of the first half of the 20th century, feuds that seem to have sown much confusion and misunderstanding in the field. My conclusion from all this is that, yes, Fisher's contributions to modern statistics were indeed far-reaching, although not all of them were positive. I've also concluded that to really get at the bottom of my sense of alienation with statistics I will have to read at least some of Fisher's works, in their original form. But I've found that Fisher's writing lives up to its reputation for impenetrability. I've tried to find guides to this literature, but, unfortunately, everything I've found is intended for people trained in statistics, so it is as difficult for me to understand as what it purports to elucidate. Hence the question at the beginning of this post. 1 Stone, Mervyn (1983), "Fiducial probability", Encyclopedia of Statistical Sciences 3 81-86. Wiley, New York.
