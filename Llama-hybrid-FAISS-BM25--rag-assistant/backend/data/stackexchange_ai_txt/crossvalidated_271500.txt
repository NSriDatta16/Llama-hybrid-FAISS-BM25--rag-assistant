[site]: crossvalidated
[post_id]: 271500
[parent_id]: 
[tags]: 
starting off with same weights in neural networks

In a simple neural network with two neurons in series carrying weights w1 and w2 and just one input, what is limitation behind starting off with both weights being same? I am referring to patrick winston's MIT lectures and to quote him ".. By the way what would happen if you started off with all the weights being the same? Nothing, because it would always stay the same" Since the two neurons are in series i fail to see why they would necessarily remain same Thank you in advance
