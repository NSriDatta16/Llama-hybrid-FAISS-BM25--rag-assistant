[site]: stackoverflow
[post_id]: 4695678
[parent_id]: 4695391
[tags]: 
The question could use a little more context. In programming a real program, we are likely to measure the program's running time. There are multiple potential issues with this though 1. What hardware is the program running on? Comparing two programs running on different hardware really doesn't give a meaningful comparison. 2. What other software is running? If anything else running, it's going to steal CPU cycles (or whatever other resource your program is running on). 3. What is the input? As already said, for a small set, a solution might look very fast, but scalability goes out the door. Also, some inputs are easier than others. If as a person, you hand me a dictionary and ask me to sort, I'll hand it right back and say done. Giving me a set of 50 cards (much smaller than a dictionary) in random order will take me a lot longer to do. 4. What is the starting conditions? If your program runs for the first time, chances are, spinning it off the hard disk will take up the largest chunk of time on modern systems. Comparing two implementations with small inputs will likely have their differences masked by this. Big O notation covers a lot of these issues. 1. Hardware doesn't matter, as everything is normalized by the speed of 1 operation O(1). 2. Big O talks about the algorithm free of other algorithms around it. 3. Big O talks about how the input will change the running time, not how long one input takes. It tells you the worse the algorithm will perform, not how it performs on an average or easy input. 4. Again, Big O handles algorithms, not programs running in a physical system.
