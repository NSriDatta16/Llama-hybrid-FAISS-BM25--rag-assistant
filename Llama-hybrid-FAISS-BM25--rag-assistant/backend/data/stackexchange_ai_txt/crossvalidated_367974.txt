[site]: crossvalidated
[post_id]: 367974
[parent_id]: 367823
[tags]: 
The earth mover's distance (EMD) provides a way to do this. When computed between probability distributions, the EMD is equivalent to the 1st Wasserstein distance . Intuitively, each distribution can be imagined as a pile of dirt, consisting of a certain amount of dirt at each location. A pile can be transformed by moving the dirt from location to location. Work is measured as the amount of dirt moved times the distance moved. The EMD is defined as the minimum amount of work needed to transform one pile to match the other. In your problem, there are multiple classes, each corresponding to one of the 'discretized levels'. Distances between classes are the distances between the corresponding levels. The classifier outputs a predicted probability that the input is a member of each class. In the dirt pile analogy, each class corresponds to a location, and the predicted probability defines the amount of dirt. For each point in the training set, you have a target class. This corresponds to a probability distribution that takes the value one for the target class and zero for all others, i.e. all dirt is piled up at a single location. In general, computing the EMD requires solving an optimization problem, where we search over possible ways of transforming the dirt piles. But, the EMD has a convenient, closed form expression in your case, because there's only one transformation that makes sense: directly move all the dirt from wherever it was originally to a single target location. Suppose there are $l$ classes (represented as integers from 1 to $l$ ), and let $D_{ij}$ denote the distance between classes $i$ and $j$ . For a given data point with target class $c$ , let $p_i$ denote the classifier's predicted probability that the class is $i$ . The EMD is: $$\text{EMD}(p, c) = \sum_{i \ne c} p_i D_{ic}$$ Related references: Levina and Bickel (2001). The Earth Mover's Distance is the Mallows Distance: Some Insights from Statistics. Frogner et al. (2015). Learning with a Wasserstein Loss. Hou et al. (2017). Squared earth movers distance loss for training deep neural networks on ordered classes.
