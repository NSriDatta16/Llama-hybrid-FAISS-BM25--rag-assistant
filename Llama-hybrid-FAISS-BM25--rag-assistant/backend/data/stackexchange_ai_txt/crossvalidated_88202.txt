[site]: crossvalidated
[post_id]: 88202
[parent_id]: 88188
[tags]: 
Hover your mouse over any tag ($\leftarrow$ is a fake tag) appearing below to see a brief excerpt of its wiki. Please forgive the disruption of line spacing. I find it worthwhile because tag excerpts may help readers to check understanding of jargon while reading through. Some of these excerpts may deserve editing as well, so they also deserve a publicist, IMHO. $p>.05$ ordinarily implies one should not reject the null-hypothesis . Conversely, type-i-errors or false positives occur when one does reject the null due to sampling error or some other unusual incident that produces a sample that was otherwise unlikely (usually with $p population in which the null is true. A result with $p>.05$ that is called a false positive seems to reflect a misunderstanding of null hypothesis significance-test ing (NHST). Misunderstandings are not uncommon in published research literature, as NHST is notoriously counter-intuitive. This is one of the rallying cries of the bayesian invasion (which I support, but do not follow...yet). I have worked with mistaken impressions such as these myself until recently, so I sympathize most heartily. @DavidRobinson is correct in observing that $p$ is not the probability of the null being false in frequentist NHST. This is (at least) one of Goodman's (2008) "Dirty Dozen" misconceptions about $p$ values (see also Hurlbert & Lombardi, 2009 ) . In NHST, $p$ is the probability that one would draw any future random samples by the same means that would exhibit a relationship or difference (or whatever effect-size is being tested against the null, if other varieties of effect size exist...?) at least as different from the null hypothesis as the sample(s) from the same population(s) one has tested to arrive at a given $p$ value, if the null is true. That is, $p$ is the probability of obtaining a sample like yours given the null ; it does not reflect the probability of the null – at least, not directly. Conversely, Bayesian methods pride themselves on their formulation of statistical analyses as focused on estimating the evidence for or against a prior theory of an effect given the data , which they argue is a more intuitively appealing approach ( Wagenmakers, 2007 ) , among other advantages, and setting aside debatable disadvantages. (To be fair, see " What are the cons of Bayesian analysis? " You have also commented to cite articles that might offer some nice answers there: Moyé, 2008; Hurlbert & Lombardi, 2009 .) Arguably, the null hypothesis as literally stated is often more likely than not to be wrong, because null hypotheses are most commonly, literally hypotheses of zero effect. (For some handy counter-examples, see answers to: " Are large data sets inappropriate for hypothesis testing? ") Philosophical issues such as the butterfly effect threaten the literal validity of any such hypothesis; hence the null is useful most generally as a basis of comparison for an alternative hypothesis of some nonzero effect. Such an alternative hypothesis may remain more plausible than the null after data have been collected that would've been improbable if the null were true . Hence researchers typically infer support for an alternative hypothesis from evidence against the null, but that is not what p-values quantify directly ( Wagenmakers, 2007 ) . As you suspect, statistical-significance is a function of sample-size , as well as effect size and consistency. (See @gung's answer to the recent question, " How can a t-test be statistically significant if the mean difference is almost 0? ") The questions we often intend to ask of our data are, "What is the effect of x on y ?" For various reasons (including, IMO, misconceived and otherwise deficient educational programs in statistics, especially as taught by non-statisticians), we often find ourselves instead asking literally the loosely related question, "What is the probability of sampling data such as mine randomly from a population in which x does not affect y ?" This is the essential difference between effect size estimation and significance testing, respectively. A $p$ value answers only the latter question directly, but several professionals (@rpierce could probably give you a better list than I; forgive me for dragging you into this!) have argued that researchers misread $p$ as an answer to the former question of effect size all too often; I'm afraid I must agree. To respond more directly regarding the meaning of $.05 ...is between 5–95%. One may certainly argue this is a consequence of sample size, because increasing sample size improves one's ability to detect small and inconsistent effect sizes and differentiate them from a null of, say, zero effect with confidence exceeding 5%. However, small and inconsistent effect sizes may or may not be significant pragmatically ( $\ne$ significant statistically – another of Goodman's (2008) dirty dozen); this depends far more on the meaning of the data, with which statistical significance only concerns itself to a limited extent. See my answer to the above . Shouldn't it be correct to call a result definitely false (rather than simply unsupported) if...p > 0.95? Since data should usually represent empirically factual observations, they should not be false; only inferences about them should face this risk, ideally. (Measurement error occurs too of course, but that issue is outside this answer's scope somewhat, so aside from mentioning it here, I'll leave it alone otherwise.) Some risk always exists of making a false positive inference about the null being less useful than the alternative hypothesis, at least unless the inferrer knows the null is true. Only in the rather hard-to-conceive circumstance of knowledge that the null is literally true would an inference favoring an alternative hypothesis be definitely false...at least, as far as I can imagine at the moment. Clearly, widespread usage or convention is not the best authority on epistemic or inferential validity. Even published resources are fallible; see for instance Fallacy in p-value definition . Your reference ( Hurlbert & Lombardi, 2009 ) offers some interesting exposition of this principle too (page 322): StatSoft (2007) boasts on their website that their online manual “is the only internet resource on statistics recommended by Encyclopedia Brittanica.” Never has it been so important to ‘Distrust Authority,’ as the bumper sticker says. [Comically broken URL converted to hyperlinked text.] Another case in point: this phrase in a very recent Nature News article ( Nuzzo, 2014 ) : "P value, a common index for the strength of evidence..." See Wagenmakers' (2007, page 787) "Problem 3: $p$ Values Do Not Quantify Statistical Evidence"...However, @MichaelLew ( Lew, 2013 ) disagrees in a way you might find useful: he uses $p$ values to index likelihood functions. Yet in as much as these published sources contradict one another, at least one must be wrong! (On some level, I think...) Of course, this is not as bad as "untrustworthy" per se. I hope I can coax Michael into chiming in here by tagging him as I have (but I'm not sure user tags send notifications when edited in – I don't think yours in the OP did). He may be the only one who can save Nuzzo – even Nature itself! Help us Obi-Wan! (And forgive me if my answer here demonstrates that I've still failed to comprehend the implications of your work, which I'm sure I have in any case...) BTW, Nuzzo also offers some intriguing self-defense and refutation of Wagenmaakers' "Problem 3": see Nuzzo's "Probable cause" figure and supporting citations ( Goodman, 2001 , 1992; Gorroochurn, Hodge, Heiman, Durner, & Greenberg, 2007 ) . These just might contain the answer you're really looking for, but I doubt I could tell. Re: your multiple choice question, I select d . You may have misinterpreted some concepts here, but you're certainly not alone if so, and I'll leave the judgment to you, as only you know what you really believe. Misinterpretation implies some amount of certainty, whereas asking a question implies the opposite, and that impulse to question when uncertain is quite laudable and far from ubiquitous, unfortunately. This matter of human nature makes the incorrectness of our conventions sadly short of harmless, and deserving of complaints such as those referenced here. (Thanks in part to you!) However, your proposal is not completely correct either. Some interesting discussion of problems related to $p$ values in which I've participated appears in this question: Accommodating entrenched views of p-values . My answer lists a few references you may find useful for reading further into the interpretive problems and alternatives to $p$ values. Be forewarned: I still haven't hit the bottom of this particular rabbit hole myself, but I can at least tell you that it's very deep . I'm still learning about it myself (else I suspect I'd be writing from a more Bayesian perspective [edit]: or maybe the NFSA perspective! Hurlbert & Lombardi, 2009 ) , I am a weak authority at best, and I welcome any corrections or elaborations others may offer to what I've said here. All I can opine in conclusion is that there probably is a mathematically correct answer, and it may well be that most people get it wrong. The right answer certainly doesn't come easily, as the following references demonstrate... P.S. As requested (sort of...I admit I'm really just tacking this on instead of working it in), this question is a better reference for the sometimes uniform distribution of $p$ given the null: " Why are p-values uniformly distributed under the null hypothesis? " Of particular interest are @whuber's comments, which raise a class of exceptions. As is somewhat true with the discussion as a whole, I don't follow the arguments 100%, let alone their implications, so I'm not sure those problems with $p$ distribution uniformity are actually exceptional. Further cause for deep-seated statistical confusion, I'm afraid... References - Goodman, S. N. (1992). A comment on replication, P ‐values and evidence. Statistics in Medicine, 11 (7), 875–879. - Goodman, S. N. (2001). Of P -values and Bayes: A modest proposal. Epidemiology, 12 (3), 295–297. Retrieved from http://swfsc.noaa.gov/uploadedFiles/Divisions/PRD/Programs/ETP_Cetacean_Assessment/Of_P_Values_and_Bayes__A_Modest_Proposal.6.pdf . - Goodman, S. (2008). A dirty dozen: Twelve P -value misconceptions. Seminars in Hematology, 45 (3), 135–140. Retrieved from http://xa.yimg.com/kq/groups/18751725/636586767/name/twelve+P+value+misconceptions.pdf . - Gorroochurn, P., Hodge, S. E., Heiman, G. A., Durner, M., & Greenberg, D. A. (2007). Non-replication of association studies: “pseudo-failures” to replicate? Genetics in Medicine, 9 (6), 325–331. Retrieved from http://www.nature.com/gim/journal/v9/n6/full/gim200755a.html . - Hurlbert, S. H., & Lombardi, C. M. (2009). Final collapse of the Neyman–Pearson decision theoretic framework and rise of the neoFisherian. Annales Zoologici Fennici, 46 (5), 311–349. Retrieved from http://xa.yimg.com/kq/groups/1542294/508917937/name/HurlbertLombardi2009AZF.pdf . - Lew, M. J. (2013). To P or not to P: On the evidential nature of P-values and their place in scientific inference. arXiv:1311.0081 [stat.ME]. Retrieved from http://arxiv.org/abs/1311.0081 . - Moyé, L. A. (2008). Bayesians in clinical trials: Asleep at the switch. Statistics in Medicine, 27 (4), 469–482. - Nuzzo, R. (2014, February 12). Scientific method: Statistical errors. Nature News, 506 (7487). Retrieved from http://www.nature.com/news/scientific-method-statistical-errors-1.14700 . - Wagenmakers, E. J. (2007). A practical solution to the pervasive problems of p values. Psychonomic Bulletin & Review, 14 (5), 779–804. Retrieved from http://www.brainlife.org/reprint/2007/Wagenmakers_EJ071000.pdf .
