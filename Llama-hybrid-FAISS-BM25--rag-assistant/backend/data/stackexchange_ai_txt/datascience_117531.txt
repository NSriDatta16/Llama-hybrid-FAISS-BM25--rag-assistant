[site]: datascience
[post_id]: 117531
[parent_id]: 
[tags]: 
Can I have 0 loss in the validation set and still have bad accuracy?

I am starting in the world of deep neural networks and doing a series of tests with a convolutional model, I have found the following case: The accuracy in the training set is much better (around 0.85) than in the validation set (around 0.58), but, when looking at the loss results, it is observed that in both data sets, the loss is 0. My question is: How can it be that the loss in both sets is so low, and yet the accuracy is not perfect? Doesn't the loss function measure the difference between the prediction and the actual class and indicate how well the model is predicting? I am performing a multiclassing task and my loss function is categorical cross entropy I would be grateful if someone could clarify these concepts for me.
