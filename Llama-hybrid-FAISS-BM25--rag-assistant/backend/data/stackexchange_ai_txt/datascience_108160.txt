[site]: datascience
[post_id]: 108160
[parent_id]: 108075
[tags]: 
I don't know of any specific method/library, but I work a lot with large pre-trained transformer models and you can definitely calculate the probability that a token or series of tokens will occur according to a model that has trained on a lot of text. Here is a SO post showing how to do exactly that. Now that will only get you the probabilities from some pre-trained model of your choice, I imagine the next thing you would want to do is find some intelligent way of classifying whether or not it is a valid entry based on those probabilities. You might be able to just kind of eyeball it, but if you find that isn't working I would recommend adding a prompt to help it understand the task that it's trying to do. Or you could just label a bunch of examples and fine tune a model as well, I would recommend looking at Pattern Exploit Training (PET) as an option to avoid some large labeling task, here is the github . There is another version of PET called " ADAPET " that tries to improve upon PET, in my experience it's a bit finicky & prone to overfitting depending on the task, but I think the way the losses are setup it actually might be a really good fit for what you're trying to do, so also worth checking that out, here is the github .
