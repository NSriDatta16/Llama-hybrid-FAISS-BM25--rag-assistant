[site]: crossvalidated
[post_id]: 643981
[parent_id]: 
[tags]: 
How does memory usage does adam optimizer use given a transformer input's sequence length?

I am trying to fine tune a transformer LLM but I keep running out of memory during the optimize.step() part. I have around 20~GiB RAM being used up until the optimizer step and then I immediately get CUDA OOM even though my GPU has >40GiB RAM. I am trying to understand why. How does adam optimizer memory usage grow with respect to input sequence length?
