[site]: crossvalidated
[post_id]: 391271
[parent_id]: 
[tags]: 
Xgboost / Boosted decision trees: Representing categorical id numbers as continuous integer variable

I've been reading through some kernels at kaggle.com for a sales forecasting competition, and noticed that a lot of people using Xgboost are feeding it categorial ID variables, represented as continuous integer variables. See this link for an example: link . I know that it's often recommended to turn a categorical variable into a set of binary variables, but in this particular competition that would lead to a huge number of input dimensions (one dummy variable per store, and one per product). These people still seem to be getting decent results. I was wondering whether improvements could be made with respect to how the ID columns are utilized. They want to capture effects related to store and product, but there isn't necessarily any meaningful relationship between different ID numbers (eg. store 1 and store 2 are completely unrelated, even though their ID value are similar). Would training one model per store / product combination, instead of training one model for all combinations with ID columns included as features, be a reasonable approach? Another idea would be to replace the IDs with rankings. Product ID could be replaced with a ranking of most sold products, and Store ID could be the ranking of each store for a given product. By using ranks, we would get the same number of unique store / product combinations, and the columns would themselves hold some form of meaning to help Xgboost utilize the information. I was wondering whether someone with a good understanding of using boosted decision trees for regression problems could share their thoughts on all this: There are too many ID's to create binary input variables for each ID, what can be done instead?
