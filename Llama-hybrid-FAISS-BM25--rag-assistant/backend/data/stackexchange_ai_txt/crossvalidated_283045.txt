[site]: crossvalidated
[post_id]: 283045
[parent_id]: 283036
[tags]: 
A standard model of your campaigns $i$ would be that each campaign has a certain open rate $p_i$. What you can now hope to test is wether one campaign had a significantly higher $p_i$ than others to a certain significance threshold $\alpha$ wich you should set beforehand. The data generating process would be a scaled binomial random variable with $n_i$ trials where $n_i$ is not random (it's your second data column) Let $o_i$ be the number of recipients that opened the mail. We postulate $$o_i \sim B(n_i, p_i)$$ This model allows us to use hypothesis testing on the hypothesis $p_i \le p_j$. If the test rejects at confidence level of $\alpha$, we can assume that campaign $i$ is better than campaign $j$. Beware of multiple testing raising your actual significance level. In this case you test $5\cdot 4 = 20$ times so to make a false discovery with maximum probability $5\%$, you would have to set the individual FDR to $\alpha = \frac{5\%}{20} = 0.25\%$. Testing multiple campaigns in parallel to see if there is an "effect" (i.e. a difference in performance across the campaigns) can be tested with an F-Test using ANOVA . As you have thousands of rows in your dataset, you could also look into Bayesian inference. This requires you to formulate an a-priori distribution of the parameters $p_i$ of your campaigns. A possible choice for this would be the empirical distribution function of your measured open rates, maybe weightet by number of recipients to give more credibility to larger campaigns. Using this a-priori distribution of $p_i$ you can actually compute, assuming the a-priori distributions, the Probability that a certain campaign is better than all others given the observed open rates (a-posteriori distribution). This however is usually infeasible if the a-priori distribution is not "nice". The results can also be approximated using Markov-Chain Monte-Carlo. This class of algorithms uses a LOT of computational power to generate samples from a distribution close to the a-posteriori distribution. Using these samples you can then, for example, find intervals for each campaign in wich you are to some degree certain that the true open rate lies within.
