[site]: crossvalidated
[post_id]: 322765
[parent_id]: 
[tags]: 
Is it possible for a global attention pooling layer to have "channels"?

I'm trying to implement a neural model described in a paper, and I've encountered the following sentences: Due to smaller sizes no pooling is used in the encoder except for global pooling, for which we employ soft attention pooling of Li et al. (2015b). and The encoder has two convolutional layers (32 and 64 channels) with batchnorm and ReLU; followed by soft attention pooling (Li et al., 2015b) with 128 channels. The paper to which they refer is https://arxiv.org/abs/1511.05493 , but I cannot even understand why they cite it. Does anybody have any idea as to why they would mention " 128 channels" in a global pooling layer that takes as input 64 channels? Shouldn't it output 64 channels by definition? Thanks
