[site]: crossvalidated
[post_id]: 626469
[parent_id]: 
[tags]: 
Image processing: Inferre rotation angle of tilted rectangle on noisy background

I have many "grayscale" images i.e. 2d-arrays like the following: i.e. dark rectangles which are tilted by an angle $\alpha \in [-3^\circ, 3^\circ]$ and a bright but noisy background. I have a (rather slow) method to determine the angle based on Hough transform, thus I have sufficient training data. Question: Is there any machine learning based method to deduce the angle $\alpha$ efficiently and robustly? Precision should be at $\pm 0.3^\circ$ . Comment based update : Apart from solving the problem "somehow" analytically (e.g. by Fourier transform) I am mainly interested in how such a problem could in principle be solved by machine learning (e.g. which architecture to use).
