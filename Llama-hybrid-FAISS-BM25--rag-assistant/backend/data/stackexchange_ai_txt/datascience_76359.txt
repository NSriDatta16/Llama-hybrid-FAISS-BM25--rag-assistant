[site]: datascience
[post_id]: 76359
[parent_id]: 
[tags]: 
Calculating key and value vector in the Transformer's decoder block

I am implementing the transformer model in Pytorch by following Jay Alammar's post and the implementation here . My question is regarding the input to the decoder layer. As shown in the diagram above, the encoder_decoder_attention layer in the decoder get the queries vector from the self-attention layer below, and keys and values vector from the encoder. Here is the implementation of the decoder from above post: class DecoderLayer(nn.Module): # ... self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device) self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device) #... def forward(self, trg, enc_src, trg_mask, src_mask): #trg = [batch size, trg len, hid dim] #enc_src = [batch size, src len, hid dim] #trg_mask = [batch size, trg len] #src_mask = [batch size, src len] #self attention _trg, _ = self.self_attention(trg, trg, trg, trg_mask) #encoder decoder attention _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask) And the MultiHeadAttentionLayer looks like this: class MultiHeadAttentionLayer(nn.Module): def __init__(self, hid_dim, n_heads, dropout, device): # ... self.fc_q = nn.Linear(hid_dim, hid_dim) self.fc_k = nn.Linear(hid_dim, hid_dim) self.fc_v = nn.Linear(hid_dim, hid_dim) def forward(self, query, key, value, mask = None): batch_size = query.shape[0] #query = [batch size, query len, hid dim] #key = [batch size, key len, hid dim] #value = [batch size, value len, hid dim] Q = self.fc_q(query) K = self.fc_k(key) V = self.fc_v(value) #Q = [batch size, query len, hid dim] #K = [batch size, key len, hid dim] #V = [batch size, value len, hid dim] Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # ... What got me confused is as Jay mentions in his post: we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices(Wq, Wk and Wv) that we trained during the training process. Visually from the diagram, it looks like the key and the value vector themselves from the encoder is passed to the decoder. If so should't we be using the same weight matrices (Wq, Wk and Wv) from the encoder, in the decoder's encoder_decoder_attention layer as well ? The code implementation has it's own Linear layers (with own weights) to calculate these vectors, and obtains the vectors by multiplying the encoder_outputs.
