[site]: crossvalidated
[post_id]: 599889
[parent_id]: 27300
[tags]: 
From my perspective, I agree with Sidharth Gurbani's answer. "The pca is not suitable for variable selection." You can even construct a dataset in which linear model works well but the performs badly with respect to the first principle component: df Output: Call: lm(formula = y ~ x1 + x2, data = df) Coefficients: (Intercept) x1 x2 2.0000 0.5747 0.5747 Warning: essentially perfect fit: summary may be unreliable Call: lm(formula = y ~ x1 + x2, data = df) Residuals: 1 2 3 4 5 1.402e-16 -6.769e-17 -1.451e-16 -6.769e-17 1.402e-16 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 2.000e+00 8.340e-17 2.398e+16 |t|) (Intercept) 2.000e+00 3.651e-01 5.477 0.012 * pc1 2.503e-17 3.171e-01 0.000 1.000 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.8165 on 3 degrees of freedom Multiple R-squared: 2.465e-32, Adjusted R-squared: -0.3333 F-statistic: 7.396e-32 on 1 and 3 DF, p-value: 1 The R-square of fit is 1, of fit2 is 0! Since the value of y in the pc1 direction is always 1. Let me make it more clearly: The PCA is to reduce the dimension and retain the most variation simultaneously. She works well only if the variation is of interest. She doesn't guarantee that she won't hurt or break down the linear or other kind of relationship in the dataset. So, use PCA only if the variation is of interest.
