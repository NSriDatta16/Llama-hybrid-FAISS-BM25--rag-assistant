[site]: crossvalidated
[post_id]: 642655
[parent_id]: 641853
[tags]: 
The best paper I have seen on the subject of scaling laws for neural network memorization was published at ICLR last year: Provable Memorization Capacity of Transformers . Since you "don't care about the topology", I trust this paper's focus on transformers won't bother you. Indeed, the remarkably success of Transformer architectures in recent years across a broad variety of deep learning tasks makes the authors' focus on this now-dominant architecture seem entirely appropriate. Because your formulation was, it appears, chosen primarily to render the problem as simple and straightforward as possible, I trust you will also not be disappointed, given the impressive theoretical rigor of this paper, that the authors have cast their primary result in slightly different terms than you specified: We prove that Transformers are capable of memorizing N sequence-to-sequence mappings of length n with d -dimensional input tokens using $O(d + n + \sqrt{nN})$ parameters. I hope you find their proofs and results as impressive as I did.
