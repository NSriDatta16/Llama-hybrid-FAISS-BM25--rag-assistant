[site]: crossvalidated
[post_id]: 181577
[parent_id]: 181573
[tags]: 
There is a saturation point. Increasing the size of your training set can't help you surpass the assumptions of your modeling method. For example, if you use a linear model to classify data that is separable in a nonlinear way, you will never get perfect accuracy. As we almost never know the underlying process to its full extent, model mismatch is the norm. As George Box famously said " All models are wrong, but some are useful ". Powerful learning methods like neural networks (aka deep learning) or random forests can push the boundaries a little more than less flexible approaches (e.g. kernel methods), but even for them there is only so much that can be learned. Additionally, the amount of data and other resources you would need to gain worthwhile improvements become excessive at some point.
