[site]: crossvalidated
[post_id]: 288979
[parent_id]: 
[tags]: 
Does each node of every hidden layer have its own error gradient?

I understand that the forward pass of a feed-forward neural network can be thought of as a hidden layer-wise composite function, and backpropagation works by recursively applying the chain rule to that composite function to find local gradients. What exactly does each application of the chain rule during backpropagation represent? Is it applied to each node in each hidden layer? Does the error gradient have as many axes as there are nodes in the network?
