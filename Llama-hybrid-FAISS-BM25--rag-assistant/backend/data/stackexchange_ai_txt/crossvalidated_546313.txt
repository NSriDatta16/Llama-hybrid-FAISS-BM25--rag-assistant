[site]: crossvalidated
[post_id]: 546313
[parent_id]: 546299
[tags]: 
I do not follow your code 100%, but it looks like you are finding this in out-of-sample data. In that case, it means that you are adding features that do not contribute much. Therefore, your model overfits to those features and is tricked by them when it comes time to evaluate out-of-sample performance. However, this can happen with in-sample performance, too! set.seed(2021) N I get a mix of the intercept-only model having higher and lower AUC than the model with a predictor (which is not part of the true data-generating process). In contrast, the log loss is always higher for the intercept-only model, as we expect. What's going on is that the logistic regression fit is not optimizing AUC. The logistic regression fit is optimizing log loss, which is equivalent to maximum likelihood estimation in this case. $$ \text{Log Loss}\\ L(\hat p, y) = -\dfrac{1}{n} \sum_{i = 1}^n \bigg( y_i\log(\hat p_i) + (1 - y_i)\log(1 - \hat p_i) \bigg) $$ When we evaluate nested models on a different metric than the one that was optimized, it is possible that the more complex model will have inferior performance, even though it is guaranteed to outperform the smaller model on the metric for which it was optimized. This is akin to how with nested OLS models, the complex one will always have the smaller in-sample (training) SSE, but it might not have the smaller in-sample (training) MAE. Of interest: Does a logistic regression maximizing likelihood necessarily also maximize AUC over linear models?
