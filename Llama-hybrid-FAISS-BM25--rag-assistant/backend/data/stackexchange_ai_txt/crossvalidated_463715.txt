[site]: crossvalidated
[post_id]: 463715
[parent_id]: 
[tags]: 
Issue with training on classification metrics other than accuracy (using R and caret)

I have a binary classification problem with two classes 0 and 1. For training an XGBoost classification model, I apply a balanced data set (50% 0's, 50% 1's). In reality, 1's are much more abundant than 0's. After applying my newly generated model on some realistically distributed test data, I see very solid recall numbers but poor precision for the less abundant 0-class. To mitigate this effect, I tried to apply other optimization metrics. In particular, I was very much interested in optimizing precision, F1 or ROC. For ROC I used the following code: # load in training data (50/50) training When using F1 or precision, I replaced twoClassSummary with prSummary and changed the metric in the train()-function to either "F" or "Precision". Unfortunately, - while the absolute number vary a little bit in my confusion matrix, recall and precision values remain unchanged (when rounding to XX% without decimals). Did I do something wrong? When optimizing for ROC, F1 or precision is it better to use a realistically distributed training set to have an effect on the test data?
