[site]: crossvalidated
[post_id]: 481795
[parent_id]: 
[tags]: 
Correction of systematic multiplicative confounder effect

Dear community members, I have a very simple question. I have thousands of independent large samples (>1000 data points each) from complex 1D distributions. To imagine the problem better, I describe it as thousands of blood pressure studies, performed in different hospitals with different cohorts of people, and the sphygmomanometers used in different hospitals introduce multiplicative constant (within one hospital) bias to measures. So the shape of these distributions is exactly the same, but the "scale" is different. What would be the best way to normalize this data to one scale? The data is unpaired, so I can not just make a linear model to estimate the slope. I can not also use mean or median for comparisons - the distributions are complex and multi-modal, and mean/median become quite unstable so large sample size of each cohort does not actually help enough. I can proceed with mixture of Normals fitting, but the distributions are not exactly normals, so it may lead to inaccurate normalization. A simple example (in R) is (in reality the mixture contains up to 10 different components and some of them are quite extreme "outliers") - due to random sampling number of samples in small "extreme" components is different. Some small extreme components may be not even sampled in a particular set due to chance. Also the methods of robust statistics are inapplicable since the number of "outliers" is >50% since it is a mixture. Comparing many density quantiles should work fine on average, but I do not know how to do it. bias = 1.5 distrib1 One may say that the difference is not so crucial here - in my opinion the small right cluster is very different after such normalization. Also variance of the "big" component is different after z-scoring.
