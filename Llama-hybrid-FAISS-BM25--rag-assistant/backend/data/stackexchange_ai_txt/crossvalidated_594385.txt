[site]: crossvalidated
[post_id]: 594385
[parent_id]: 
[tags]: 
Are time-series models essentially difference equations with noise?

I've been studying some difference equations in my free time, and now I'm seeing them everywhere, especially in time-series models. Here are some models that seem to be difference equations to me. I'm wondering whether they actually are difference equations and whether it makes sense to think of them as difference equations. Moving averages: $$ y_t = a_1 x_{t-1} + a_2 x_{t-2} + a_3 x_{t-3} $$ Here, the $x_t$ 's are known values of the original time-series, so it's like a "purely non-autonomous" difference equation ( $y_t$ doesn't depend on its own lags). The $x_t$ 's come from some stochastic process, so they're a kind of noise we're trying to extract information from. Exponentially weighted moving averages: $$ y_t = (1-a) y_{t-1} + a x_t $$ This seems to be the linear difference equation $y_t = (1-a) y_{t-1}$ with additive noise $a x_t$ . Since we actually observe $x_t$ , I would't call it "noise", really, - maybe "additional state" or something? Autoregressive models: $$ x_t = a_0 + a_1 x_{t-1} + a_2 x_{t-2} + \varepsilon_t $$ Here, $\varepsilon_t$ is the noise. If we removed the noise, we'd get a purely deterministic 2nd-order difference equation. Note that we don't observe the noise $\varepsilon_t$ . ARMA models: same as AR, but with more lags of noise. Since we don't observe the noise $\varepsilon_t$ at all, we have to use conditional maximum likelihood and specify initial values for the first few lags of noise like $(\varepsilon_0, \varepsilon_1)$ . Or specify distributions for these initial values. EWMA for variance (RiskMetrics model): $$ \sigma_t^2 = (1-\lambda)\sigma_{t-1}^2 + \lambda x_t^2 $$ Again a difference equation in terms of $\sigma_t^2$ and its lag plus additive noise $\lambda x_t^2$ . Similar to ARMA, we don't observe $\sigma_0^2$ , so we have to supply an initial value. This seems extremely similar to difference (and differential) equations being called " initial value problems" because initial values are often supplied and they also arise in general solutions. ARCH: $$ \sigma_t^2 = \omega + a_1 x_{t-1}^2 + a_2 x_{t-2}^2 + \dots $$ Basically a moving average for $x_t^2$ . GARCH: $$ \sigma_t^2 = \omega + a_1 \sigma_{t-1}^2 + a_2 \sigma_{t-2}^2 + \dots + b_1 x_{t-1}^2 + b_2 x_{t-2}^2 + \dots $$ Here we have again, a difference equation in terms of the $\sigma_t^2$ 's plus additive noise in terms of the $x_t^2$ 's. We don't observe any of $\sigma_t^2$ 's, so have to supply a few initial values for its lags. Recurrent neural networks (RNNs) whose hidden state $h_t$ evolves over time like this: $$ h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b) $$ This seems to be a non-linear (because of $f(\cdot)$ ) difference equation for $h_t$ with either noise or some kind of "additional state" $x_t$ , which is the time-series we observe. Again, we don't observe the hidden state $h_0$ , so need initial value for it. Other RNNs, because their hidden states are also defined as difference equations (or so it seems to me). The statistical task for these models seems to be to fit the difference equations to data by imposing particular distributions for noise. We don't know anything about the $x_t$ process that we observe, so we include white noise to obtain a cost/merit function (the likelihood) that we can optimize by tuning the coefficients of these difference equations. The result of modelling is a difference equation fitted to the data in the sense that if you simulate the model (generate noise and let the system evolve according to the difference equation), you'd get time-series that look similar to the one that you observed. Is this correct? Are these time-series models difference equations? (Or recursive equations?) If so, what's the proper term? Stochastic DEs? DEs with noise? Are these in any sense similar to stochastic differential equations?
