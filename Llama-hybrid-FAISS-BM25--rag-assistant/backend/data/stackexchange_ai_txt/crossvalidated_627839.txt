[site]: crossvalidated
[post_id]: 627839
[parent_id]: 
[tags]: 
Keras Implementation of a neural network found on a paper

I am trying to implement a neural network I found on an open access paper as I have a similar problem and I am struggling with it. I am using tensorflow.keras for the implementation. Here is the paper: https://www.mdpi.com/2072-4292/13/19/3800/xml The authors work with data of size 100x100x100x2. My data are of size 128x128x128x1, so I only have one channel (real data, instead of complex like the authors which they divide into the two channels). The authors assign S1 = C1/2. In my case, for the first block I put both equal to 1 as my C1 is = 1. The authors use "max pooling layers of sizes 2 and 5 in turn". I choose sizes 2 and 4 as my input size is 128. In my case, as my output size is 128x128x128x1, I added a final convolution stage to go back to this image size. I have some issues with their description, and in particular figure 3 of their paper. If over they blocks they put the output size of the block as they write, I don't get how they go from size 25x25x25x12 to size 5x5x5x24. I assume a pool block is missing in the figure. I don't know what is the number of filters for the transpose convolutions in the decoder side. Is my implementation correct? Is the final convolution layer I added creating problems? Do you suggest to work with data with two channels (with the imaginary one only containing zeros) to avoid using this layer? def fire(x, S1, E, strides): x = layers.Conv3D(S1, (1, 1, 1), strides, padding="same")(x) x1 = layers.Conv3D(E, (1, 1, 1), strides=1, padding="same")(x) x2 = layers.Conv3D(E, (3, 3, 3), strides=1, padding="same")(x) x = tf.keras.layers.Concatenate(axis=-1)([x1, x2]) return x def fire_UNET(channels = 1): #down layer_conc = [] inputs = layers.Input(shape=(128, 128, 128, channels)) layer_conc.append(inputs) # A x = fire(inputs, 1, 3, strides = 1) print(tf.shape(x)) x = tf.keras.layers.MaxPooling3D(1, strides=2, padding="same")(x) print(tf.shape(x)) layer_conc.append(x) # B x = fire(x, 3, 6, strides = 1) print(tf.shape(x)) x = tf.keras.layers.MaxPooling3D(1, strides=2, padding="same")(x) print(tf.shape(x)) layer_conc.append(x) # C x = fire(x, 6, 12, strides = 1) print(tf.shape(x)) x = tf.keras.layers.MaxPooling3D(1, strides=4, padding="same")(x) print(tf.shape(x)) # D x = layers.Conv3DTranspose(12, (3,3,3), strides=4, padding="same")(x) print(tf.shape(x)) x = tf.keras.layers.Concatenate(axis=-1)([x, layer_conc[2]]) print(tf.shape(x)) x = fire(x, 12,6, strides = 1) print(tf.shape(x)) # E x = layers.Conv3DTranspose(6, (3,3,3), strides=2, padding="same")(x) print(tf.shape(x)) x = tf.keras.layers.Concatenate(axis=-1)([x, layer_conc[1]]) print(tf.shape(x)) x = fire(x, 6, 3, strides = 1) print(tf.shape(x)) # F x = layers.Conv3DTranspose(1, (3,3,3), strides=2, padding="same")(x) print(tf.shape(x)) x = tf.keras.layers.Concatenate(axis=-1)([x, layer_conc[0]]) print(tf.shape(x)) x = layers.Conv3D(1, (1, 1, 1), strides=1, padding="same")(x) print(tf.shape(x)) outputs = x return k.Model(inputs, outputs)
