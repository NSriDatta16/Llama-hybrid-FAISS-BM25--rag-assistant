[site]: crossvalidated
[post_id]: 331942
[parent_id]: 
[tags]: 
Why don't we use a symmetric cross-entropy loss?

Machine learning classifiers often use the cross-entropy $\mathbb{H}[p,q]$, where $p$ is the true distribution (often a delta) and $q$ is the predicted distribution over classes (or can at least be interpreted that way). Minimizing this is the same as minimizing the KL-divergence between the truth and the prediction, since $$ \mathbb{H}[p,q] = \mathcal{D}_\text{KL}[p||q] + \mathbb{H}[p] $$ where $\mathbb{H}[p]$ is the entropy of $p$ (zero for a delta, or constant wrt the model in any case). Question : why don't we use $$ \mathcal{L}(p,q) = \mathbb{H}[p,q] + \mathbb{H}[q,p] = \mathcal{S}_\text{KL}[p,q] + \mathbb{H}[p] + \mathbb{H}[q] $$ where $\mathcal{S}_\text{KL}=\mathcal{D}_\text{KL}[p||q]+\mathcal{D}_\text{KL}[q||p]$ is a symmetric KL-divergence. Notice that this also tries to minimizes the uncertainty in the prediction, which seems like a reasonable thing to me.
