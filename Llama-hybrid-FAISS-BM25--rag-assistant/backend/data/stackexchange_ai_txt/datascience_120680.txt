[site]: datascience
[post_id]: 120680
[parent_id]: 
[tags]: 
BERTopic: Is it okay to ignore the first two topics?

I used BERTopic to generate a topic model over a large dataset of texts. The result is very appealing and the modeled topics are mostly perfectly interpretable for a human, especially compared to other topic modeling approaches. According to the documentation (e.g. https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html ) the topic with number -1 refers to outliers and should be ignored. Topic -1 is in my case indeed a topic consisting of unrelated common words without a possible interpretation. However, in my topic model, as well the second topic with number 0 is just a mixture of unrelated words and not a "nice topic" in terms of human sense. All the following topics are very nice topics with a clear meaning. My question: Is it okay to ignore the first two topics ( -1 & 0 ) modeled by BERTopic and only start using the topics from topic 1 on? Or is this problematic and indicating an issue with the model? Is there a parameter that can be changed to change this behavior in order to ensure that only topic -1 will be an uninterpretable topic?
