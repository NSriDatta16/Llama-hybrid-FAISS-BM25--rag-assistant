[site]: crossvalidated
[post_id]: 375341
[parent_id]: 375220
[tags]: 
If you are using the R interface, there is xgboostExplainer . xgboostExplainer outputs the contributions of the features to the predictions on a per-instance basis. In a binary classification task, for example, the prediction for an input vector $\left( x_1, x_2, \dots, x_M \right)$ is usually made based on a predicted "success" probability, $\hat{p}$ : $$ \hat{p} = \mathrm{sigmoid} \left( f_{1}\left(x_1\right) + f_{2}\left(x_2\right) + \dots + f_{M}\left(x_M\right) \right) $$ For simple logistic regressions, $ f_{i}\left(x_i\right) $ is a linear function of $x_i$ , i.e. $ f_{i}\left(x_i\right) = w_i x_i$ and so we will be able to extract and interpret the estimates for $w_1$ . For boosting trees, however, the $f_i\left(x_i\right)$ is generally assumed to be non-linear, so there is no "coefficients" or "formula" like those found in generalised linear models to speak of; while the leaves of the trees do have scores/weights assigned to them, they are not very straightforward to interpret. Given a trained xgboost model and an input vector, xgboostExplainer will tell you the values of the individual $f_{1}\left(x_1\right),\ f_{2}\left(x_2\right),\ \dots,\ f_{M}\left(x_M\right)$ for the said input. These can be interpreted as the "contributions" of each feature towards its final prediction. You may then interrogate the explanations produced for your predictions to understand the (non-linear) relationships learnt by the model. See this article for a nice exposition of this idea and the package's capabilities. For the python API, there is the ELI5 package; I haven't used it myself but judging by its documentation it is similar to xgboostExplainer but covers many different types of models (including xgboost ).
