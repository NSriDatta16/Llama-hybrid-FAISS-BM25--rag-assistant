[site]: crossvalidated
[post_id]: 371094
[parent_id]: 
[tags]: 
Issues with XGBoost on H2O environment

I have a dataset from which I built lags at different levels to use as features in the XGBoost model. When I ran XGBoost models on H2O, the model is picking up the features which contain higher values as the most important features. When I tried GBM model on the same dataset, it's picking the correct features which are more correlated to the prediction column as important features. Does anyone know why it's happening? Eg: If I run XGBoost model on H2O on the below sample dataset to predict Y (categorical variable with levels 0,1), the model is giving A as the most important factor instead of B which is actually the most important factor according to GBM: A B C D E Y 10 5 6 1 6 0 20 9 7 3 8 1 15 8 5 1 2 1 13 9 2 2 7 1 18 5 7 1 8 0 I normalised the columns and tried XGBoost and it too gave the wrong columns as the most important columns
