[site]: crossvalidated
[post_id]: 550549
[parent_id]: 550547
[tags]: 
YES AND NO FIRST THE YES In theory, this is true. Start with logistic regression. That explicitly models the log-odds, which you can convert to the probability. A neural network with a sigmoid activation function on the final node is behaving the same as the inverse link function in a logistic regression. You're trying to get the probability. NOW THE NO Many machine learning models have poor calibration. The sklearn documentation has some nice discussion of this . I also have an open question about machine learning (particularly neural network) overconfidence. If your model has poor calibration, then it isn't really reasonable to claim that $\hat y_i= p$ means that $P(Y=1) = p$ , since the model is, in some sense, not telling the truth.
