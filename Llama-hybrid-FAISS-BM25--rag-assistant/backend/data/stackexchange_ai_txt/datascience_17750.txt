[site]: datascience
[post_id]: 17750
[parent_id]: 17737
[tags]: 
Yes, it makes sense to use CNNs with autoencoders or other unsupervised methods. Indeed, different ways of combining CNNs with unsupervised training have been tried for EEG data, including using (convolutional and/or stacked) autoencoders. Examples: Deep Feature Learning for EEG Recordings uses convolutional autoencoders with custom constraints to improve generalization across subjects and trials. EEG-based prediction of driver's cognitive performance by deep convolutional neural network uses convolutional deep belief networks on single electrodes and combines them with fully connected layers. A novel deep learning approach for classification of EEG motor imagery signals uses fully connected stacked autoencoders on the output of a supervisedly trained (fairly shallow) CNN. But also purely supervised CNNs have had success on EEG data, see for example: EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG (disclosure: I am the first author of this work, more related work see p. 44) Note that the EEGNet paper shows that also with a smaller number of trials, purely supervised training of their CNN can outperform their baselines (see Figure 3). Also in our experience on a dataset with only 288 training trials, purely supervised CNNs work fine, slightly outperforming a traditional filter bank common spatial patterns baseline.
