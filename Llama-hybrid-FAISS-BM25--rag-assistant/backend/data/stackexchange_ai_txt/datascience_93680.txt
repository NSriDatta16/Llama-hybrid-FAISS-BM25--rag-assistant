[site]: datascience
[post_id]: 93680
[parent_id]: 93200
[tags]: 
It is possible, actually. The answer is not too different than the one given by @10xAI, but it is not trying to exploit the order of the random seeds implicitly, since it would break for parallel training. So the answer above could maybe only work for trees not trained in parallel. But not sure. The actual working answer is simple, and it resides in using the random generator stored in each estimator and using it to redo the random sampling. So, for instance, assume rf is your trained random forest, then it is easy to get both sampled and unsampled indices by importing the appropriate functions and replicating the sampling using the seed in each rf.estimators[0].random_state . For example, to retrieve the lists of sampled and unsampled indices: import sklearn.ensemble._forest as forest_utils n_samples = len(Y) # number of training samples n_samples_bootstrap = forest_utils._get_n_samples_bootstrap( n_samples, rf.max_samples ) unsampled_indices_trees = [] sampled_indices_trees = [] for estimator in rf.estimators_: unsampled_indices = forest_utils._generate_unsampled_indices( estimator.random_state, n_samples, n_samples_bootstrap) unsampled_indices_trees.append(unsampled_indices) sampled_indices = forest_utils._generate_sample_indices( estimator.random_state, n_samples, n_samples_bootstrap) sampled_indices_trees.append(sampled_indices) estimator is a decision tree in this case, so one can use all the methods to compute custom oob_scores and whatnot. Hope this helps!
