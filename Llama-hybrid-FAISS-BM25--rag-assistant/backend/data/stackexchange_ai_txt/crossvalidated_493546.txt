[site]: crossvalidated
[post_id]: 493546
[parent_id]: 493519
[tags]: 
In the paper The Gauss-Markov Theorem and Random Regressors , Juliet Popper Shaffer writes: If attention is restricted to linear estimators ... that are conditionally unbiased, given $X$ , the Gauss-Markov theorem applies. If, however, the estimator is required only to be unconditionally unbiased, the Gauss-Markov theorem may or may not hold, depending on what is known about the distribution of $X$ . Therefore, in the assumptions of the Gauss-Markov theorem with random $X$ , it should be stated explicitly that $E(\tilde{\beta}\,|\,X) =\beta$ . There is an additional passage found in the “canonical proofs” that also bothers me, namely, that the equality $E(\tilde{\beta}\,|\,X) =\beta$ should hold for all $\beta\in\mathbb{R}^d$ , as usually unbiasedness (conditional or unconditional) is introduced with a given fixed probability measure in mind. Since this post refers to the method of proof, I've written a statement which explicitly asserts every assumption that is used in these proofs: Theorem Fix a measurable space $(\Omega,\mathscr{A})$ , a random $n\times d$ matrix ${X}$ and a $n\times 1$ random vector $v$ . Let $\mathfrak{M}$ denote the set of all probability measures $P$ satisfying the following $P(\text{rank}(X) = d) = 1$ $E_P\big(v'v\big) $E_P(v| X) = 0$ $E_P( v v'| X) = \mathbf{Id}$ Let moreover $\psi:M(n\times d)\to M(n\times d)$ be measurable (where $M(n\times d)$ denotes the vector space of $n\times d$ matrices), and put $ X_\psi = \psi\circ X$ . Suppose that, for all $P \in\mathfrak{M}$ , for all $\beta\in\mathbb R^{d}$ and all $\sigma>0$ , it holds that $$ E _P ( X_\psi'( X\beta + \sigma v)\,|\, X) = \beta,\qquad\text{$P $-a.s.} $$ Then, for any $P \in\mathfrak M$ , any $\beta\in\mathbb R^{d}$ and any $\sigma>0$ it holds that the $d\times d$ matrix $$ \text{var}_P ( X_\psi'( X\beta + \sigma v)\,|\, X) - \text{var}_P (( X' X)^{-1} X'( X \beta + \sigma v)\,|\, X) $$ is positive semidefinite, where $\text{var}_P $ is defined through $$ \text{var}_P ( z) := E _P ( z z') - E _P ( z)E _P ( z') $$ for all random vectors $ z$ such that $E _P ( z' z) .
