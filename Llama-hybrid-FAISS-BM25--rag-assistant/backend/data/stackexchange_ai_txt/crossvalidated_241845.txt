[site]: crossvalidated
[post_id]: 241845
[parent_id]: 
[tags]: 
What is keeping me from getting a reliable average of my NN's performance?

I have a 9000+ sample data set composed of 9 features and one binary output upon which I'm running a neural network. I coded Octave to go through all 500 or so combinations (of these 9 features) over 100 iterations each, such that for each iteration, I train a randomly selected subset for my training set, then apply the optimized parameters to a test set, where I will measure the percentage in which the model guessed the correct output. I thus end up with 100 results for each of these 500+ combinations, which I then average together in the hopes that these 100 accuracy measurements will be enough when averaged to give me a reliable approximation of the accuracy of that model/combination of features so that I can choose the best one. However, running this code twice has yielded me confusing results; the best performing models the first time were not the best performing models the second time, even though they used the same features. Sometimes, they weren't even in the same ballpark. Was 100 examples not enough to come up with a reliable mean? Should I run it again with more than 100 trials for each combination, though it will likely mean leaving my computer to run for a very very long time? Or is something else at play? Some technical notes: my data is normalized Many of the combinations that I tested anecdotally showed high bias (with 9000 training examples I would hope for bias over variance, at least!) The average accuracy of the 511 models observed was 90.290, with a standard dev of .047.
