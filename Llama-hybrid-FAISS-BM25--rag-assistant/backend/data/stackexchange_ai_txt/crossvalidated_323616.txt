[site]: crossvalidated
[post_id]: 323616
[parent_id]: 
[tags]: 
Finding a mixture of 1st and 0'th order Markov models that is closest to an empirical distribution

I am interested in finding the distribution "$p^*$" closest to an empirical distribution $\hat{p}$ where $p^*$ is a mixture of first and zeroth order Markov models. That is, I want to find $$ p^* = \arg\min_p \sum_{i,j} D\left(\,\hat{p}(j|i)\, \| \, p(j|i) \,\right) $$ subject to the following constraints $p(j|i) = \alpha \cdot r(j) + (1 - \alpha) \cdot q(j|i)$ $r(j) \ge 0$ for all $j$ $\sum_j r(j) = 1$ $q(j|i) \ge 0$ for all $i$ and $j$ $\sum_j q(j|i) = 1$ for all $i$ where $\alpha$ is a mixture parameter in $[0,1]$ that is given and fixed. (The model is not identifiable if $\alpha$ is also a parameter to be learned.) I know I can use EM here -- the missing data is the indicator variable for which Markov chain was invoked at a given time-step. But I am more interested in taking a direct optimization approach, particularly if this is a convex problem. How might one derive the updates for for optimizing this model? Any assistance in this endeavor is greatly appreciated.
