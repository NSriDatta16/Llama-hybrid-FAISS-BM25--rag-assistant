[site]: crossvalidated
[post_id]: 472284
[parent_id]: 
[tags]: 
Foundations behind Linear Regression / Statistical Modelling

I've always struggled with the foundations behind the concept of modelling (and specifically regression) - what is random, what is not, what we are modelling. I think I have a grasp of it - but I'd love if someone could please confirm if this matches with what they understand as well . Otherwise, if they have anything to add or correct - I would really appreciate this! We have a random variable $Y$ , for example the weather, that we want to understand. However, it is a little too variable if we know absolutely nothing else. If someone says "What's the weather like" with no other context about when/where etc, it's really hard to say anything so far. However, to shrink this problem, and to perhaps better understand $Y$ relative to some other variables that are easier to observe (if assuming random) or control (if assuming non-random controlled) some other variables $\mathbf{X}\in \mathbb R^p$ . For example, we might have predictors as the location and the month . This information would help us understand the season which is now something we can talk about - for example if it's January in Australia - you can start to imagine it the weather would probably be hot and sunny. This has a few benefits/aims: The variance of $Y$ given this new information $\mathbf{X}$ is significantly reduced. Before having infinite possibilities for the weather all with pretty even chance, now that we know something, we can start visualising what the nature (i.e. the distribution) of $Y$ might be like given $\mathbf{X}$ . We can understand the relationship between $Y$ and other variables $\mathbf{X}$ . The relationship with not be deterministic because $Y$ is random (intuitively, there are an uncountably infinite number of factors that come together to determine what $Y$ will be), so we cannot deterministically know what $Y$ might be just based on a finite (or even countably infinite) number of predictors $\mathbf{X}$ . But depending how relevant $\mathbf{X}$ is to the data-generating process for $Y$ , it might explain a good majority - leading to a visible trend when we observe data $\{(x_i, y_i): i=1,2,...,n\}$ . (I have used the terms "uncountably infinite" and "countably infinite" a little recklessly. They are not meant to be literally accurate - I don't have any sources for this. But this is how I intuitively understand what something purely random is in real life, and I wonder whether this analogy is suitable?) By making assumptions on the nature of the part of $Y$ unexplained by $\mathbf{X}$ (called the random error term $\epsilon$ - being collective influence of all other factors part from $\mathbf{X}$ on $Y$ ), we can say even more. If we assume that $\mathbb{E}(\epsilon)=0$ , then we can say that while there is no deterministic relationship between $\mathbf{X}$ and $Y$ , there is a deterministic relationship between $\mathbf{X}$ and $\mathbb{E}(Y|\mathbf{X})$ - i.e. there is a deterministic relationship between $\mathbf{X}$ and the average value of $Y$ . If we further assume a distribution for $\epsilon$ then we can formulate a probabilistic model (i.e. a model for the distribution) for $Y$ . For example, in simple linear regression, we assume that $\epsilon\sim \mathcal N(0,1)$ which leads to $Y\sim \mathcal N(\beta_0 + \beta_1 X, \sigma^2)$ . After contemplating, I also think the following interpretation is not accurate (do you agree?) There is some true deterministic underlying relationship between $Y$ and $\mathbf{X}$ but our data is noisy (for example due to measurement errors etc) and doesn't let us see this. I feel like this is inaccurate - it's not just the data that's noisy, but the relationship itself between $Y$ and $\mathbf{X}$ is noisy. This is because $\mathbf{X}$ does not completely determine $Y$ (for if it did, that would mean we could observe $Y$ when we have $\mathbf{X}$ , and prediction would not be necessary). We assume that the collective influence of uncollected information $\epsilon$ - all the other factors unobserved will symmetrically fault this relationship above and below. (however this assumption that $\mathbb E (\epsilon)=0$ is fairly arbitrary though.) Thanks in advance, I would love to hear your suggestions/modifications/corrections and any parts you feel are accurate :)
