[site]: datascience
[post_id]: 9718
[parent_id]: 9662
[tags]: 
There are many tools that are able to support such queries (as you mentioned Hive or Spark), and it is really up to your requirements in terms of number of queries, number of people who are going to query the data, what kind of BI or reporting tools you might want to use with it, etc. More than that, your requirements are probably going to change in the future. If you provide easier ways to query the data, the more people around you are going to use it. For example, if you choose to load that data into Amazon Redshift ( https://aws.amazon.com/redshift/ ), it will give you a full SQL flexibility, very fast performance and the ability to connect to several BI, visualization and reporting tools. When you COPY the data into Redshift it is being compressed (column based encoding) 5-8 times, and you can put all your data in a cluster of 6 nodes of DS2.8XL of Redshift, for a couple of weeks for your POC. The concept of compression can be applied in different solutions as well. For example, instead of using CSV format, you can convert it to Parquet ( https://parquet.apache.org/ ) format, and save on your storage and increase the performance as you need to move around and scan less bytes of data. Once you have your data in Parquet, you can query it for example with Presto, like Netflix are doing - http://techblog.netflix.com/2014/10/using-presto-in-our-big-data-platform.html Regarding Spark, it can help even if you don't fit all the data into memory. Spark can also help you with the usage of Zeppelin ( https://zeppelin.incubator.apache.org/ ) as an interactive notebook, with nice visualization features, as well as write code with Scala, Python or even R with SparkR.
