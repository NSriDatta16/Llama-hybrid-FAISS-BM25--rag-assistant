[site]: crossvalidated
[post_id]: 609942
[parent_id]: 609345
[tags]: 
Neural networks are universal approximators, so they could approximate any function. But for it to really work you possibly would need a huge number of weights, a lot of data, and training it for a really long time. All the new kinds of neural networks and other improvements in this area are to make them smaller, work with realistically big datasets, and be able to train in a reasonable time. So theoretically it could be possible, but not practically. The difference is between the model figuring everything by itself vs us giving it a reasonable starting point. We have different ways of making life easier for the machine learning algorithms: using feature engineering , initializing the weights cleverly, using informative priors for the parameters, or using architectures that already force some informed way of using the data as in this case. Machine learning to a great degree is about finding those shortcuts. As a side comment, notice that the same applies to the brains of humans and other animals: we do not start with a set of neurons and need to figure out how to use them all by ourselves. We start with pre-build architecture, and innate knowledge, with many of the reflexes and basic “functionalities” available to us at birth. Starting at tabula rasa would be really ambitious.
