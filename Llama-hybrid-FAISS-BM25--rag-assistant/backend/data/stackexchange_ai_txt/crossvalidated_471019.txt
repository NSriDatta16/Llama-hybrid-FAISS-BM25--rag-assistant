[site]: crossvalidated
[post_id]: 471019
[parent_id]: 471018
[tags]: 
The model initialization is random. Each update depends on the current position, which depends on the previous position and so to the initialization. Using well-tuned gradient descent on a strongly convex problem should remove this, but neural networks are not convex and gradient descent is hard to tune. Minibatch-based training is stochastic, because different minibatches are passed to the model. This is because minibatches are constructed by sampling without replacement within each epoch. Different minibatches imply different model predictions, which implies different losses and different gradients. So the updates for 1 epoch applied to the same data set can still be different, because at each step, the direction of the update will probably be different. If you need repeatable output for each call to fit , you'll need to take some additional steps. The documentation explains how to do this.
