[site]: crossvalidated
[post_id]: 449128
[parent_id]: 
[tags]: 
Time series DGP: No convergence to true parameter - Identification problem?

I set up a model, simulated some data and tried to infer the wanted parameter $\alpha$ . However it seems that there may be no convergence to the true parameter (result is either $-\alpha$ or $+\alpha$ ). Is the following model below actually identified, i.e. does my likelihood have a global optimum? The model looks as follows: Observed time series : $S_t$ , $U_t$ , $p_t$ Unobserved time series : $S_t^*$ , $U_t^*$ Parameter to infer : $\alpha$ Model : $S_t = S_t^* + \alpha * U_{t}^**p_t, p_t \in [0,1]$ where $S_t^* = \mu_{S} + S_{t-1}^* + \epsilon_{St}, \epsilon_{St} \sim N(0,\sigma_S)$ $U_t^* = \mu_{U} + U_{t-1}^* + \epsilon_{St},\epsilon_{Ut} \sim N(0,\sigma_U)$ with $Cov(\epsilon_S, \epsilon_U) = 0$ First differences of $S$ for $p=1$ : $\Delta S = \mu_{S} + \alpha * \mu_{U} + \epsilon_{St} + \alpha*\epsilon_{Ut}$ $\Delta S \sim N(\mu_{S} + \alpha * \mu_{U}, \sqrt{\sigma_S^2 + \alpha^2 * \sigma_U^2})$ (1) Inference: Step 1: Empirical moments of $\mu_S$ , $\mu_U$ , $\sigma_S$ , $\sigma_U$ based on observed time series with $p=0$ (first differencing and calculating mean and variance of resulting series). Step 2: Set up log likelihood based on (1), replace $\mu$ and $\sigma$ with their empirical moments and optimize over $\alpha$ . Result: Here comes the problem. I simulated K samples with a given $\alpha$ , however the optimizer converges to $-\alpha$ or $+\alpha$ , although the average over all simulations is closer to $\alpha$ . Depending on $\alpha$ I can mitigate the problem with choosing different optimization algorithms, however I wonder I actually may have an identification problem and the likelihood does not have a global optimum but two at -0.5 und 0.5?
