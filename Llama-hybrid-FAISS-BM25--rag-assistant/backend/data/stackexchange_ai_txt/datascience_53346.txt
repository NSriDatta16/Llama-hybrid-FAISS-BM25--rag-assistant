[site]: datascience
[post_id]: 53346
[parent_id]: 53345
[tags]: 
There's a great answer on the Mathematics stack exchange about the distinction between linear and affine functions. A linear function fixes the origin, whereas an affine function need not do so. An affine function is the composition of a linear function with a translation, so while the linear part fixes the origin, the translation can map it somewhere else. In section 5.1.4, the book introduces linear regression: $$\hat{y} = \pmb{\omega}^{T} \pmb{x} + b$$ In this example, the $\hat{y} = \pmb{\omega}^{T} \pmb{x}$ part is a linear function. It has to pass through the origin. The addition of a vertical translation $b$ causes $\hat{y}$ to become an affine transformation, since it no longer has to pass through the origin. I think the authors point out this distinction because it's technically incorrect from a purely mathematical point-of-view to describe $$\hat{y} = \pmb{\omega}^{T} \pmb{x} + b$$ as a linear function of the parameters $\pmb{x}$ . It's actually an affine function, so "linear regression" may appear to be a misnomer if you have a strong background in linear algebra. For what it's worth, I don't think this is very helpful information to include in the main text of an introductory chapter on machine learning. If you still don't understand, then just keep reading! Don't let this block you from progressing through the book.
