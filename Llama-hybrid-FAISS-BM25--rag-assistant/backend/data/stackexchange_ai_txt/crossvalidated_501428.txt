[site]: crossvalidated
[post_id]: 501428
[parent_id]: 
[tags]: 
Forcing the mean of predictions to converge to true value

I am training a neural network on a regression problem, where I have 12 long timeseries of data that correspond to unique target values. I chop up these timeseries in very short blocks, so that I have many samples for each target value. I am getting good convergence of the validation error, but one thing is bugging me: the mean of the predictions for a certain target value might not correspond to the actual target value. In the picture below, the dashed line is where I want my predictions to lie on. Sometimes I do get networks that have a very close mean: But not consistently. I would rather have a higher average error but a mean that converges to the true value, than a mean that does not converge but with a lower average error. Is there any way I can tinker with the loss function to enforce the mean of multiple predictions to be minimized as well? So that there is at least some kind of tradeoff that I can experiment with.
