[site]: crossvalidated
[post_id]: 314322
[parent_id]: 217519
[tags]: 
According to the book Introduction to Information Retrieval . Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch√ºtze: centroid clustering is not monotonic. So-called inversions can occur: Similarity can increase during clustering as in the example in Figure 17.12, where we define similarity as negative distance. this seems this is an typical behavior. Clearly, 1 and 2 are the closest points at distance 1. So that is the best possible merge here. The squared Euclidean distance of the centroids is $0^2+0.9^2=0.81$. With Lance-Williams: The squared Euclidean distance is $d(1,3)=d(2,3)=0.9^2+0.5^2 = 1.06$, while d(1,2)=1. So we get $$\frac121.06+\frac121.06-\frac141=0.81.$$ Therefore: The result is correct - if you use squared Euclidean distances. I don't think you can use the (efficient!) Lance-Williams approach with other distances. Plus, the centroid makes most sense with squared errors, too. Inversions in centroid linkage do occur. Even if you would use non-squared Euclidean distance. With regular Euclidean distance, you would still merge 1,2 at distance 1, and then merge 1,2,3 at $\sqrt{0.81}=0.9$; which is less than 1. But don't ask me for a proof that the Lance-Williams and "direct" definition are equivalent. Apparently the proof can only work for squared Euclidean? I guess it is similar to the derivation of Ward's, which appears to be the "properly weighted" version of centroid linkage.
