[site]: crossvalidated
[post_id]: 262565
[parent_id]: 262535
[tags]: 
Bayes theorem is $$ \mathrm{posterior} \propto \mathrm{likelihood} \times \mathrm{prior} $$ so posterior is proportional to likelihood times prior. For it to be equal we need to multiply the right-hand side of the equation by a normalizing constant, so that it integrates to unity, what makes posterior a proper probability distribution. Constant does not change anything about finding maximum of the function, since each possible output of the function is multiplied by the same constant, so if you are only interested in point estimate ( maximum a posteriori estimate), then you can ignore the normalizing constant. However if you want to obtain proper posterior distribution, then it is needed and we often use MCMC to find it and solve the equation. See also the Why Normalizing Factor is Required in Bayes Theorem? thread. Edit But, as noticed by Xi'an , what the slides that you refer to actually say is that the author by "semi-Bayesian" approach means using normal distribution as likelihood function and normal priors: This makes computation very easy since using conjugate priors, but it may not be the best approximation for all cases (recall that normal distribution is continuous, symmetric, and reaches from $-\infty$ to $\infty$ -- this is not true for many different kinds of data!).
