[site]: crossvalidated
[post_id]: 200419
[parent_id]: 200410
[tags]: 
We do not have to make any distributional assumptions in order to extract the Principal Component directions from a covariance or correlation matrix. To see this, recall the problem PCA solves. For a $p\times p $ covariance matrix $\mathbf{\Sigma}$ and all vectors $\mathbf{x} \in \mathbb{R}^p$ we would like to maximize $$\mathbf{x}^{\prime} \mathbf{\Sigma} \mathbf{x} \tag{1}$$ subject to the condition that $\mathbf{x}$ is a unit vector, i.e. $\mathbf{x}^\prime \mathbf{x} = 1$. The solution is the eigenvector of the covariance (or correlation) matrix corresponding to the largest eigenvalue. The second principal component is the vector that maximizes $(1)$ subject to the additional condition that is perpendicular to the first direction and likewise for the third, forth and millionth principal component (always $\leq p$) . All we require is that the maximizers are unit vectors and perpendicular to the previous directions. We do not need normality for the extraction but we definitely need the normality for hypothesis testing, e.g. to see how many directions are significant. It's worth noting, however, that with normality we have a pretty neat interpretation of PCA as the axes in ellipsoids of constant density (recall the exponent of a multivariate Normal distribution).
