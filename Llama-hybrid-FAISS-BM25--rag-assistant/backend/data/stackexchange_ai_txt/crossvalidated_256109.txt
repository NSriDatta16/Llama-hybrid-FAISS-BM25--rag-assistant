[site]: crossvalidated
[post_id]: 256109
[parent_id]: 252324
[tags]: 
1. Assuming $k=1,2,\ldots$ and $X_1 \sim \text{Normal}(0, \sigma^2/(1-A^2))$, and $A$ is constant for all time points (a homogeneous Markov chain): For the non-conditional expectations and variances you want to use the law of total expectation and the law of total variance. a) $E[X_{k}|X_{k-1}] = E[A X_{k-1} + v_{k-1}|X_{k-1}] = AX_{k-1} + 0$ (linearity of expectations) b) $E[X_k] = E[E[X_k|X_{k-1}]]= 0$ (since we're starting in the stationary distribution). Try this for $k=2$, then proceed inductively. c) \begin{align*} \text{Var}[X_k] &= \text{Var}[E(X_k|X_{k-1})] + E[\text{Var}(X_k|X_{k-1})] \\ &= \text{Var}[AX_{k-1}] + E[\sigma^2_v] \\ &= \text{Var}[AX_{k-1}] + \sigma^2_v \\ &= A^2 \frac{\sigma_V^2}{1-A^2} + \sigma^2_v \\ &= \sigma^2 \end{align*} The whole key is assuming that the first state is in the stationary distribution. Then, because the Markov Chain is stationary, it stays in that stationary distribution. b) shows the unconditional mean is constant at $0$, and $c$ shows that unconditional variance stays at $\sigma^2_v$. Then, by properties of normality, the marginal distributions are normal. If you do not start off in the stationary distribution, you'll still get closer to it after every time point, assuming $-1 d) For the covariance you want to use bilinearity. \begin{align*} \text{Cov}(X_k,X_{k-1}) &= \text{Cov}(AX_{k-1}+v_{k-1},X_{k-1}) \\ &= A\text{Cov}(X_{k-1},X_{k-1}) + 0 \\ &= A \text{Var}(X_{k-1}) \\ &= A \sigma^2_v. \end{align*} For the non-linear one, it's still the same math again. Just replace $Ax_{k-1}$ with $f(x_{k-1})$ everywhere. The answer will depend on whether or not you assume you start off in some stationary distribution, which depends on your specific $f$. I leave it to you to figure that out.
