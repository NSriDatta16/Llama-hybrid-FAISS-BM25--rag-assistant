[site]: crossvalidated
[post_id]: 597884
[parent_id]: 
[tags]: 
Calculating error metrics on log10(y) bayesian ridge regression model. Why does model perform better when trained on log10(y)?

I am using scikit learn's Bayesian ridge regression model and am training my model on log10(y), exponentiating (10 ** y_i) my predictions back to their original value, then calculating my error metrics to assess my model's performance. The error metrics I use are R^2 and MAE. I chose to do this because (1) I have a log tail in my dependent variable's distribution and logging it makes the distribution closer to normal and (2) there is an increase in the standard deviation of my residual plots as with y. However, I am not sure why logging my dependent variable, training my model on the log10(y), then exponentiating my predictions back seems to result in slightly increased model performance. Does anyone have an explanation for this? My first intuition is that linear models perform better when the dependent variable is normally distributed and, thus, logging the dependent variable to make it normal results in better training.... but I am not sure if this is right. P.S: Please let me know if I need to make this question clearer
