[site]: datascience
[post_id]: 115179
[parent_id]: 
[tags]: 
Finding the best data source query strategy for a ml model. Maximizing quality, minimizing cost

It is too expensive to query all data sources for each claim, so it is necessary to define a sourcing strategy to maximize the model quality score and minimize the cost of prediction (expressed in money terms) over the entire data distribution. I have several machine learning models that train on different data from different sources, and on all combinations of them. For example, if there are only 2 sources a and b, then there will be 4 models: no source, only a, only b, a and b together. Each source has a cost per 1 application. Model predictions are well calibrated, so the final predictions for the entire dataset can be considered as predictions of one meta model. The problem is that when predicting, we do not know in advance what this or that source will answer. If there is no data (no_hit), then its cost is 0, and the gain in model quality is 0 (for example, roc_auc). Otherwise, we need to pay for it (hit). It is also possible to determine other possible scenarios. For example, the prediction of the model by source is equal to 0.01 or 0.99 - it is obvious that there is no need to request other data (no_hit,score =0.01 and score =0.99) Example. There are 2 sources a and b. Their cost is 3 and 2. I know the statistical distribution of the scenarios on the training data. Let's say there are 100 samples, 10 have only a source, 20 have only b source, 30 have both sources, 40 have none of them. One possible strategy Request source a. If a is a hit, then predict the target by the model at source a. If a is not a hit, then request source b. If b hit, then predict target on sources a and b. If b no hit , then predict by the model on the available data (on source a). Thus, 10 samples will be predicted only by the model on a, 30 will be predicted by the ensemble model a and b, for the rest there will be no sources and rates. The total cost is 3 10+30 (3+2)=180. The final quality of the meta model is a metric on the combined score (10 only for a, 30 for a and b, the rest are dummy prediction). That is, the strategy can be represented as a tree. But with 2 kinds of sheets. 1 view - source (a, b, stop and predict with available data) 2 view - script (hit, no hit). And the strategies are all possible subtrees, where all the last leaves at the end have the option to stop and predict with the available data). This should include all scenarios if a source has been selected (hit and no hit ). Schematically, all possible options and the strategy described above in the figure: It is not possible to roughly sort through all possible strategies if there are many data sources (for example, 10). Please advise how this task can be optimized. Maybe there is a ready-made solution, or there is a mathematical description and algorithms for this problem, or you have experience in this problem.
