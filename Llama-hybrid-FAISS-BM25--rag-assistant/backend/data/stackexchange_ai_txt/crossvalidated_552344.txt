[site]: crossvalidated
[post_id]: 552344
[parent_id]: 
[tags]: 
T-test on two sets of observations from different distributions?

I'm comparing two implementations of an algorithm, and want to find out if one implementation (A) is better than the other (B). I'm mostly interested in the execution time. I have N test cases, each one generates a pair of execution times: ( $a_i, b_i$ ), where $a_i$ and $b_i$ are the time taken by A and B, respectively. Those N test cases are very different, some take less than 1 second, some take more than 5 minutes. Ideally, I would need to run the same test case multiple times and get the average time of this specific test case. However, due to practical reasons, I can only run each test case once. Is there any statistical test that I can use to determine if implement A is better than B? Since those N pairs of observations are from different distribution, I cannot directly run a t-test on them. Could I 'normalize' the difference to percentage error: $x_i = (a_i - b_i) / a_i$ , then do a single-sample t-test on the derived x_i samples? The assumption is, with the normalization, $x_i$ is of a normal distribution. Does this sound a reasonable approach? Or any other better way? Thank you a lot!
