[site]: datascience
[post_id]: 94687
[parent_id]: 94685
[tags]: 
In the original Transformer article , these linear layers are just matrix multiplications. As described in the paragraph you referred to in your question $W^Q$ is a matrix of dimensions $d_{model} \times d_k$ , that is, it is a fully connected layer with $d_k$ units. In practical implementations, these have the optional addition of a bias vector. You can see their actual definition in the fairseq code .
