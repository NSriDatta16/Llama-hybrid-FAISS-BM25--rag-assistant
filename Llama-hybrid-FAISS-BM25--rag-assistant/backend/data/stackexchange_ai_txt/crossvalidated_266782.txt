[site]: crossvalidated
[post_id]: 266782
[parent_id]: 
[tags]: 
Understanding Word2Vec

I am trying to understand the word2vec algorithm ( Mikolov et. al ) but there are a few thing which I do not understand. I get that the activation from the input layer ot the hidden layer is linear and that $\mathbf{h}$ is just the average of the linear combination of all $\mathbf{x}_{ik}$ vectors. Further I understand that the final output of each node $y_i$ of the output vector $\mathbf{y}$ is just the $\text{softmax}$ activation function from the hidden to the output layer. What I do not understand is what are my actual word vectors in the end? I currently see only one possibility and that is to take either the columns of $\mathbf{W_{V\times N}}$ or the rows of $\mathbf{W'_{N\times V}}$ since I assume these weights are now the "information carrying" entity of this algorithm. Am I correct with this assumption? Independent from this I would like to understand how Google managed to train their Google News word vector models. From the website: We are publishing pre-trained vectors trained on part of Google News dataset (about 100 billion words). Which is simply insane if this has been done with one-hot encoded vectors for training. That would mean each input matrix $\mathbf{W_{V\times N}}$ would be $(1 \times 10^{11}) \times 300$ in size!? This leads me to my final question: Shouldn't it be possible to use lower dimensional vectors? Somewhere in the back of my head I have the idea, that you can simply randomly initialize word vectors for all given words in a vocabulary and then apply word2vec on it. This however would also mean that not only the weights have to get updated, but also the word vectors of each input words too. Is something like this actually done or am I completely mistaken here?
