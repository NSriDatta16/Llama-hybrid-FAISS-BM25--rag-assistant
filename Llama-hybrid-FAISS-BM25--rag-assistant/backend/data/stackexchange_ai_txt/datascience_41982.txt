[site]: datascience
[post_id]: 41982
[parent_id]: 
[tags]: 
Programming a Neural Network in Python

In order to get some understanding of machine learning (i'm super new to this) I'm programming a neural network and try to train a sinefunction with it. The set-up is as follows: Backpropagation algorithm with stochastic gradient descent 1 input (x between [0,2*pi], sine function as target) plus optionally a biasinput. The sine function is scaled to [0,1] a variable no. of hidden layers with a variable no. of hidden units sigmoid activation functions weights initialized randomly between -0.05 and 0.05 I expected a network with for example 4 hidden layers and 25 hidden units in each hidden layer to be deep enough to predict a sine, but instead the 'learned' output sticks to roughly the average of the scaled sinefunction (i.e 0.5). I was wondering if somebody has had a similar experience and how it was fixed, or whether I am overlooking/misunderstanding something in my code. In this image ( https://i.stack.imgur.com/h6glp.png ) I have illustrated the setup and some nomenclature of my code shown below. Thank you for your time and help! import numpy as np import matplotlib.pyplot as plt class neural(object): def __init__(self,samples,n_in,n_hidden,n_hlayers,eta,bias): self.samples = samples self.n_in = n_in self.n_out = n_in self.n_hidden = n_hidden self.n_hlayers = n_hlayers self.eta = eta self.bias = bias # Prepare data with range [0,1) if self.bias is True: x = 2*np.pi*np.random.rand(self.samples,self.n_in) t = (np.sin(x)+1)/2 x = np.append(x,np.ones([self.samples,1]),axis=1) self.n_in += 1 else: x = 2*np.pi*np.random.rand(self.samples,self.n_in) t = (np.sin(x)+1)/2 # Partitioning self.x_train = x[0:self.samples/2] self.t_train = t[0:self.samples/2] self.x_test = x[self.samples/2:] self.t_test = t[self.samples/2:] # Create weight matrices self.get_weights() def get_weights(self): # Initialize weights with values [-b,+b) a = 0.1 b = 0.05 # Weigths from input layer to 1st hidden layer self.w_ih = a*np.random.rand(self.n_hidden, self.n_in)-b # Weigths between hidden layers if self.n_hlayers > 1: self.w_hh = np.zeros([self.n_hidden,self.n_hidden,self.n_hlayers-1]) for i in range(0,self.n_hlayers-1): self.w_hh[:,:,i] = a*np.random.rand(self.n_hidden, self.n_hidden)-b # Weigths from last hidden layer to output layer self.w_ho = a*np.random.rand(self.n_out,self.n_hidden)-b def train(self,xi,ti): # 1. Feedforward # Output z from the first hidden layer (right next to the input layer) z = np.zeros([self.n_hidden,1,self.n_hlayers]) z[:,:,0] = fsigmoid(np.dot(self.w_ih,xi)) # Output from other hidden layers if self.n_hlayers > 1: for i in range(0,self.n_hlayers-1): z[:,:,i+1] = fsigmoid(np.dot(self.w_hh[:,:,i],z[:,:,i])) # Output from output layer o = fsigmoid(np.dot(self.w_ho,z[:,:,-1])) # 2. Backpropagation delta_o = o*(np.ones([self.n_out,1])-o)*(ti-o) delta_who = self.eta*np.dot(delta_o,z[:,:,-1].T) if self.n_hlayers > 1: # Preallocate 3D array to contain all layers delta_h = np.zeros([self.n_hidden,1,self.n_hlayers]) delta_whh = np.zeros([self.n_hidden,self.n_hidden,self.n_hlayers-1]) # Backpropagation from outer layer to first hidden layer delta_h[:,:,-1] = z[:,:,-1]*(1-z[:,:,-1])*np.dot(self.w_ho.T,delta_o) delta_whh[:,:,-1] = self.eta*np.dot(delta_h[:,:,-1],z[:,:,-2].T) # Backpropagation between other hidden layers (walking from right # to left in the network) for i in range(-2,-1*self.n_hlayers,-1): delta_h[:,:,i] = z[:,:,i]*(1-z[:,:,i])*np.dot(self.w_hh[:,:,i+1].T,delta_h[:,:,i+1]) delta_whh[:,:,i] = self.eta*np.dot(delta_h[:,:,i],z[:,:,i-1].T) # To the input layer if self.n_hlayers > 1: i = -1*self.n_hlayers delta_h[:,:,i] = z[:,:,i]*(1-z[:,:,i])*np.dot(self.w_hh[:,:,i+1].T,delta_h[:,:,i+1]) delta_wih = self.eta*np.dot(delta_h[:,:,i],xi.T) else: delta_h = z[:,:,-1]*(1-z[:,:,-1])*np.dot(delta_who.T,delta_o) delta_wih = self.eta*np.dot(delta_h,xi.T) # 3. Update weights self.w_ho += delta_who self.w_ih += delta_wih if self.n_hlayers > 1: self.w_hh += delta_whh return o def fsigmoid(y): sigma = 1/(1+np.exp(-y)) return sigma def relu(y): z = (abs(y)+y)/2 return z if __name__ == '__main__': network = neural(samples=20000,n_in=1,n_hidden=25,n_hlayers=4,eta=0.01,bias=False) # For each training example pair err = [] out = [] for i in range(0,network.samples/2): print 'Iteration',i+1 xi = network.x_train[i].reshape([network.n_in,1]) ti = network.t_train[i].reshape([network.n_out,1]) output = network.train(xi,ti) # Error of training set RMSerr = np.sqrt(np.sum((output-ti)**2)/network.n_out) err.append(RMSerr) # To do: add error of validationset set (prevent overfitting) # Save trained output out.append(output[0]) plt.close('all') plt.plot(network.x_train,out,'.') plt.hold(True) plt.plot(network.x_train,network.t_train,'x')
