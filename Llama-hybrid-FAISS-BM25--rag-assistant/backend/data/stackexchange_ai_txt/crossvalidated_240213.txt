[site]: crossvalidated
[post_id]: 240213
[parent_id]: 240019
[tags]: 
To chime in, I assume that the application is to predict unknown subjects. That means (regardless of whether you have time series or inherently unordered repeated measurements) that the splitting needs to be done so that unknown subjects are tested => splitting a) Considering that you have only 38 subjects, you should put some thought into resampling validation, though. From my experience working with similarly small sample sizes (though more features), here are some recommendations in a nutshell: Go for subject-wise out-of-bootstrap or iterated cross validation. They allow to assess the stability of your models which is crucial in small sample size problems. The results can even be used for an aggregated model in case instability is an issue. Do not do leave-one-subject-out. It neither allows to measure model stability, nor to reduce it. In addition, there are situations where it is subject to a large pessimistic bias because of small sample size (as opposed to the minimal pessimistic bias that is expected). If you are using the typical classification figures of merit such as accuracy, sensitivity, specificity, etc. and the task is to correctly classify subjects: beware that the crucial problem is measuring the performance because the uncertainty of the test results depends on the absolute number of test cases. As an example, observing 17 correct predictions out of 17 truly positive subjects corresponds to a 95% confidence interval for sensitivity ranging from about 80% to 100%. In other words, you won't be able to do data-driven model optimization based on that. This also means that you do not need to set up a three-set splitting (nested cross validation), because you'd waste resources on statistically meaningless comparisons here - your optimization is almost guaranteed to fail (though you may not notice that if you don't check for stability). The same calculation for binomial confidence interval for the proposed 5 (3) test/optimization/validation subjects yields 95% c.i. for all correct ranging down to 50% (30%) so based on perfect test results your confidence interval would still include 50/50 guessing! Some papers we wrote about these subjects: Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. & Sowa, M. G. Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005). Beleites, C. & Salzer, R.: Assessing and improving the stability of chemometric models in small sample size situations, Anal Bioanal Chem, 390, 1261-1271 (2008). DOI: 10.1007/s00216-007-1818-6 Beleites, C. and Neugebauer, U. and Bocklitz, T. and Krafft, C. and Popp, J.: Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33. DOI: 10.1016/j.aca.2012.11.007 accepted manuscript on arXiv: 1211.1323
