[site]: crossvalidated
[post_id]: 366915
[parent_id]: 366745
[tags]: 
I'm not sure I understand what you propose doing with averaged data (by fold or by "point"). Each single prediction generates a point on the ROC/PR curve, so averaging cases in any way would throw away that information. However, you can calculate ROC/PR curves either by calculating one curve per fold (or per iteration) which emphasizes that you did in fact test 10 different surrogate models, or by merging the 10 lists into one long list (which corresponds to the underlying assumption of the cross validation that the k surrogate models are equvalent to each other and to the model built on the whole data set: they are used in place of the whole-data-model and that's the case for predictions of all folds). However, this is only sensible if the scores have the same meaning across the folds. This is the case e.g. for libsvm predicted probabilities. I often do plot both: The per fold (or per iteration/repetition) curves give a visual impression on the uncertainty of the test results, whereas the combined curve is the best estimate we have. What does make sense (and where "averaging" comes in) is to also plot the single working points given by a pre-determined cutoff for the scores. That can either be an externally pre-defined cutoff or a cutoff optimized in the inner CV (though with your setup that would probably not work as the optimization cannot account for the class imbalance due to your sampling!) Here's an example: thin lines give 10 different runs (in your case that would be folds) thick line gives ROC of long list (scores here were posterior probabilities so can be combined meaningfully) black dot gives sens/spec for prediction of class labels (pooled over all models, as usual for cross validation) by each surrogate model with its own working point which was automatically determined in inner optimization during training to yield sensitivity = specificity.
