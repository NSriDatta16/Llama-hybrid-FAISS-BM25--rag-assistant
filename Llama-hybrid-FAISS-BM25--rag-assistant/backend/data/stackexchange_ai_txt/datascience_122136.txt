[site]: datascience
[post_id]: 122136
[parent_id]: 
[tags]: 
Fine-tuning Pretrained Models for Web DOM Interaction Prediction Task

I am currently working on a side project that involves predicting changes to a webpage's DOM based on user interactions. The idea is to input the initial DOM state and a user interaction, and predict the subsequent DOM state. I've been considering using some form of a neural network for this task. I have explored both Seq2Seq models (with the possibility of using pre-trained models like GPT-2 or BERT) and Graph Neural Networks (GNNs). However, I am unsure about the feasibility and effectiveness of both approaches, and how to go about fine-tuning pre-trained models if they are to be used. For Seq2Seq, I can imagine encoding the initial DOM and the interaction as sequences, but I am unsure of how to encode the output DOM state. For GNNs, the situation seems even more complicated. I am considering representing each unique DOM state as a node, and each interaction causing a transition between states as an edge, but I'm unsure if this would capture all the necessary information and allow for accurate predictions. I would greatly appreciate any insights or advice on these questions: Are Seq2Seq or GNNs suitable for this kind of task, or would another type of model be more appropriate? If Seq2Seq models or GNNs are appropriate, how should I go about encoding the DOM and interactions for these models? Is it common and advisable to use pre-trained models for such tasks? If so, what are the practical steps to fine-tune these models for my task? Thanks for your time and looking forward to your inputs!
