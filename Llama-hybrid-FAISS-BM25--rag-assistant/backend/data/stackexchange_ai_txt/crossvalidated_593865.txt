[site]: crossvalidated
[post_id]: 593865
[parent_id]: 
[tags]: 
How to assess calibration of probability distribution for a multiclass model?

I have a multiclass classifier (boosting model), and my goal is to have a good approximation of the actual distribution to the classes given my feature values. I.e. suppose I have features $X$ , and three possible classes/target ( $A$ , $B$ and $C$ ). What I want is to have a good estimation of this discrete probability distribution conditioned on $X$ , that is, I want the model to accurately predict these values: $$P(target = A | X = x), P(target = B | X = x), P(target = C | X = x) $$ I have the predictions output of the model ( a vector of probabilities) and only the actual target for a given instance - In my use case I need to use the probability distribution itself (and not classify an instance to a single class), for example I want to use these predictions to calculate an expected value conditioned on these predicted probabilities (e.g. if the target is item bought on my store, each item has a price and I could have the expected revenue by averaging out the probabilities and the prices). Now, I researched a little bit how to assess calibration for a multiclass, discrete scenario and didn't found any clear way to achieve this. I found this question that mentions the Expected Calibration Error (ECE), but on my own research I found two types of metrics which seem to make sense as well: Brier Score Perplexity measure Brier score seems more intuitive to me, and perplexity score I couldn't find any use outside NLP application domains. Any tips or ideas?
