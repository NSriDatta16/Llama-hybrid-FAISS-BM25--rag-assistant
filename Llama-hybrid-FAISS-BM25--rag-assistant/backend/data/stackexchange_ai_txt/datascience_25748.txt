[site]: datascience
[post_id]: 25748
[parent_id]: 25731
[tags]: 
It depends on your task and the amount of data you have. If you have so much data but you can not find similar tasks to have appropriate architecture you should stack convolution and dense layers yourself. But if you have appropriate amount of data and there exist good architectures then you have to decide what you want and how is your situation. Suppose that you want to have recognition task, there are so many architectures that are applied to ImageNet data-set. You can use transfer learning but there is a point here. Suppose that you want to fine tune GoogleNet . This is a very large network and is capable for recognizing about a thousand distinct classes. If you have a recognition task with 5 classes and you have an agent that should be online, this is not logical to have such a big network. You may have similar performance by stacking a few layers and get better time complexity. If you don't have so much data, freezing the layers and applying transfer learning to the last layer maybe a typical solution.
