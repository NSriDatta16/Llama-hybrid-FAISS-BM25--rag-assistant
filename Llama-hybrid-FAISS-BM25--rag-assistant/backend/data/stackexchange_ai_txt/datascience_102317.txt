[site]: datascience
[post_id]: 102317
[parent_id]: 102084
[tags]: 
Unfortunately, there is little theoretical knowledge about what complex neural networks do. Transformers are known to be universal approximations , so in theory they can learn to do any function with the input sentence, unlike the other alternatives that you mention. Most of the time, the accuracy of the BERT-like model would be strictly better. In practice, however, everything depends on the data you have. Neural language models have very many parameters, which makes them often prone to overfitting and hard to train. Some classification problems might also be so easy that a stronger model would not help. There is also the question of computational efficiency, the accuracy gain might not be worth the slow-down from using a more complex model. BoW models might also offer better interpretability. To conclude, there might be many situations and many reasons why smaller and simpler models might be a better choice.
