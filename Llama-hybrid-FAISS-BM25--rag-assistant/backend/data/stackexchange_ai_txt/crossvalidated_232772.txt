[site]: crossvalidated
[post_id]: 232772
[parent_id]: 232771
[tags]: 
I'm going to answer the question as asked, as an abstract math problem. If you need advice on solving a real-world data-analysis problem, you should post a new question with full details . Since you want to make a probability statement about a population quantity (viz., the true number of marbles), a Bayesian approach is called for. Since we're sampling with replacement, we can treat the bag as having infinitely many marbles and merely concern ourselves with the probability of drawing white each time. Then we can state the problem like this: if $B_1$, $B_2$, … are IID Bernoulli conditional on the parameter $p$, and $Z$ is some constant in $[0, 1]$, then given $x$ observations of the $B_i$s, of which $y$ were equal to 1 (i.e., white), what is the posterior probability that $p > Z$? The conjugate prior of the Bernoulli distribution is the beta distribution, which has two (hyper)parameters $α$ and $β$. You can encode any prior beliefs you have about $p$ by setting $α$ and $β$ appropriately. If you have no precise prior belief, the Jeffreys prior, which sets each to 1/2, is a good default. (N.B. I've made a simplifying assumption by allowing $p$ to vary continuously in [0, 1] when in reality it can only take on $N + 1$ distinct values.) Given all this, the posterior distribution of $p$ is another beta distribution, with parameters $y + α$ and $x - y + β$. So calculating the posterior probability of $p > Z$ just means using the CDF of the beta distribution, which is the regularized incomplete beta function. An easy way to compute this is to run 1 - pbeta(Z, y + alpha, x - y + beta) in R. For your example with 10 draws 4 of which are white and $Z = .3$ and using the Jeffreys prior, this is 1 - pbeta(.3, 4 + 1/2, 10 - 4 + 1/2) , which is about .762. The case of sampling without replacement is similar, but using the hypergeometric distribution instead of the Bernoulli distribution, and the beta-binomial distribution instead of the beta distribution.
