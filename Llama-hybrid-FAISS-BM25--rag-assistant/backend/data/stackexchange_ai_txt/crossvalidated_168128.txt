[site]: crossvalidated
[post_id]: 168128
[parent_id]: 
[tags]: 
Testing the Validity of a Test Statistic (incorrectly)

In this post @Wolfgang shares a few lines of code to test in a simulation the ability of a particular test statistic to perform as expected, measured by the percentage of type I errors. He shows by simulation how the $chi \,square$ test works in the case presented in that original post , despite the small number of "successes" in comparison with the large number of observations. On a post I wrote to sort of piece together what I've been collecting from many other posts on comparison of proportions, I was trying to come up with a similar type of simulation using the $z$-test. I understand that the $\chi^2$ is mathematically equivalent in this situation, but it was one of these exercises in convincing oneself, and understand the $z \,test$ equations. Further, I wasn't sure the tests were going to be identical in every respect. So to the question... I road-tested the equation of the $z$ test function, and it did not produce the expected results. The same happened with a similar ad hoc formula in R-Bloggers , which I will use in the rest of the question. My bet is that my code is flawed (in which case, I'd like to fix it on my prior post), but the values it generates seem plausible; the only odd result being the proportion of type I errors. So I wonder if I could get some help, especially if the results are accurate, and the statistical interpretation is incorrect. This is the actual problem (more details here if needed): DATA: Medication Symptoms Drug A Drug B Totals Heartburn 64 92 156 Normal 114 98 212 Totals 178 190 368 TEST STATISTIC FUNCTION IN R ( from R-Bloggers ) : For reference: $ Z = \frac{\frac{x_1}{n_1}-\frac{x_2}{n_2}}{\sqrt{p\,(1-p)(1/n_1+1/n_2)}}$ with $ p = \frac{x_1\,+\,x_2}{n_1\,+\,n_2}$ z.prop = function(x1,x2,n1,n2){ numerator = (x1/n1) - (x2/n2) p.common = (x1+x2) / (n1+n2) denominator = sqrt(p.common * (1-p.common) * (1/n1 + 1/n2)) z.prop.ris = numerator / denominator return(z.prop.ris) } POPULATION SET UP WITH ONE SINGLE (AVERAGE) TRUE PROPORTION: set.seed(5) # Number of samples taken to obtain different proportions IF # we set it up so that the proportions are actually equal: samples SIMULATION: # Now we run the z-test in what we just did, but we repeat it 100000 times # and put the results in vector pval: pval To my surprise, the result is 0.09887 , and changing the set.seed ends up with larger proportions of falsely rejected null hypothesis.
