[site]: datascience
[post_id]: 27859
[parent_id]: 
[tags]: 
How to build a recurrent neural net in Keras where each input goes through a layer first?

I'm trying to build an neural net in Keras that would look like this: Where $x_1$, $x_2$, ... are input vectors that undergo the same transformation $f$. $f$ is itself a layer whose parameters must be learned. The sequence length $n$ is variable across instances. I'm having trouble understanding two things here: What should the input look like? I'm thinking of a 2D tensor with shape (number_of_x_inputs, x_dimension), where x_dimension is the length of a single vector $x$. Can such 2D tensor have a variable shape? I know tensors can have variable shapes for batch processing, but I don't know if that helps me here. How do I pass each input vector through the same transformation before feeding it to the RNN layer? Is there a way to sort of extend for example a GRU so that an $f$ layer is added before going through the actual GRU cell?
