[site]: crossvalidated
[post_id]: 12010
[parent_id]: 11955
[tags]: 
There are a number of statistical NLP projects out there, with NLTK one of the more active Open Source ones. However tracking word frequency over time and over a few hundred documents is probably a simple enough problem to code yourself. You will want to start with converting your documents to a format easy to process, like plain text ala your comment. Convert to lower case, drop punctuation, and then split each document into words. Start with a regular expression like /\b/, and then filter out numbers and obvious errors. Next you will probably want to drop stop words . Here is a decent stop word list for English language sources. Now count each occurrence of a word in each document. You will probably want to build a hashtable index, with (non-stop) word as the key and the value as an integer count. If you would like to get more sophisticated, you could pull out collocations by adding each preceding or succeeding $n-1$ words to your index. Or even run each word through a stemmer like this Ruby gem . Lastly sort your words by index count. Here are your steps for the plain text document: The quick brown fox jumps over the quick, brown lazy dog. Convert to lower case, and drop the puncutation: the quick brown fox jumps over the quick brown lazy dog Split into words with /\b/, drop the words that are all whitespace: the; quick; brown; fox; jumps; over; the; quick; brown; lazy; dog Now drop the stop words: quick; brown; fox; jumps; over; quick; brown; lazy; dog Build your count index: quick=2; brown=2; fox=1; jumps=1; over=1; lazy=1; dog=1 Add 2-gram collocations: quick-brown=2; quick=2; brown=2; brown-fox=1; fox=1; fox-jumps=1; jumps-over=1... Stem the words, so jumps and jumped become just jump .
