[site]: crossvalidated
[post_id]: 160600
[parent_id]: 160558
[tags]: 
Look into regularized regression such as elastic net implemented in the glmnet package for R. Regularized regression of this kind is capable of handling cases where the number of coefficients to estimate is much greater than the sample size. Use glmnet to select variables to retain. If you do any cross-validation on the elastic net regularization hyperprameters, I would use the bootstrap and only tune the parameter that guides how much regularization to do while leaving the parameter that determines the weight of the L1 vs L2 norm alone. I bet with 45 observations you'll end up with one or two non-zero coefficients. Then take your selected variables and fit a regularized Bayesian model with weakly informative prior over the coefficients. The bayesglm package for R is good for this. Fit as rich a model as possible, with interaction effects and everything. The bayesglm default prior will severely penalize the interactions and likely shrink them to zero, but at least you didn't assume the interaction effects are zero. Also actually look at your data with cross tabulation and plots once you have selected the candidate variables with glmnet . And do some posterior predictive checks on whether your data violates the assumptions made by the likelihood family you used in the regression. Finally, I would not recommend building some kind of predictive model based on 45 observations. This is statistical inference you are doing here, and it is inference to guide the collection of more data and the formulation of prior distributions to use in future analyses.
