[site]: crossvalidated
[post_id]: 588341
[parent_id]: 
[tags]: 
Can I use cross-validation to make predictions?

Hi I am a beginner and I am confused about whether I can make predictions with the models produced during kfold. For example in the code below, the model produced in each kfold was used to make predictions on the test set. auc = [] test_pred = [] # GroupKFold kf = GroupKFold(n_splits=5) for fold, (idx_train, idx_valid) in enumerate(kf.split(train, train['failure'], train['product_code'])): xtrain = train.iloc[idx_train][test.columns] xvalid = train.iloc[idx_valid][test.columns] ytrain = train.iloc[idx_train]['failure'] yvalid = train.iloc[idx_valid]['failure'] xtest = test.copy() features = [f for f in xtrain.columns if f != 'product_code'] model = make_pipeline(preprocessor, LogisticRegression(penalty='l1', random_state=1, solver = 'liblinear',C= 0.01,class_weight = 'balanced')) model.fit(xtrain[features], ytrain) preds_valid = model.predict_proba(xvalid[features])[:,1] score = roc_auc_score(yvalid, preds_valid) print(f"Fold {fold}: auc = {score:0.5f}") auc.append(score) test_pred.append(model.predict_proba(xtest[features])[:,1]) print(f"Average auc = {sum(auc) / 5:.5f}") submission = pd.DataFrame({'id': test.index, 'failure': sum(test_pred)/5}) This link talks about the cross_validate function, if I take each estimator generated by the function and use it to make the predictions in the test, wouldn't that be the same thing done in that code? I would like to know if this kind of approach is normal or if I am making a mistake.
