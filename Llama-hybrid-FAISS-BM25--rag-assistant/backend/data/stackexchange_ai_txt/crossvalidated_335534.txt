[site]: crossvalidated
[post_id]: 335534
[parent_id]: 335526
[tags]: 
It's easy to see that eigenvectors can be normalized to any number: $$Av=\lambda v$$ Now multiply both sides by any constant c: $$A(cv)=(cv)\lambda$$ Once you found an eigenvector $v$ you can multiply it by any c and get a vector $cv$ which satisfies the same equation. That's why it's customary to normalize them to 1. This is not the case with eigenvalues. They are unique. It's convenient in PCA in particular because you often use the operation $s=xv$ where $x$ is the data and $s$ is the score. Normalized to 1 eigenvectors will preserve the norm of $x$
