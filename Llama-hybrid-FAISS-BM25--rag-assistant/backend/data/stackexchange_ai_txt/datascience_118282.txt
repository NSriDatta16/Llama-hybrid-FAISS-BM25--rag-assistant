[site]: datascience
[post_id]: 118282
[parent_id]: 118271
[tags]: 
No idea about standard approaches but one option you have is: instead of fine-tuning the whole model, fine-tune only a part of it. For instance, you may fine-tune only the last layers few layers. This way, you can keep loaded the common part of the model, load just the small fine-tuned part and combine them to perform inference. This would reduce both storage space and decompression time, at the cost of more complex code logic. Of course, you should first determine what are the minimum fine-tuned parts of the model that let you get the desired output quality.
