[site]: datascience
[post_id]: 10309
[parent_id]: 9827
[tags]: 
What you are asking for is a gradient based hyperparameter optimization. To enable such a grid search one would have to define a metric on the space of all hyper parameter combinations for the current pipeline. I don't think this makes sense in your example because of the binary nature of the use_idf flag. Lets say your parameter combinations are (use_idf, alpha) . Then you would have to do decide what the distance between (True, 0.00015) and (True, 0.00010) is. In your case it should be smaller than the distance between (True, 0.00015) and (False, 0.00015) . So d((True, 0.00015), (True, 0.00010)) But what about the distance between (True, 0.00015) and (True, 0.05) ? Will it be still smaller than d((True, 0.00015), (False, 0.00015)) ? I think you get the point. By choosing the metric you will subjectively influence the search for the best hyper parameters. Probably a random search would be more suitable as suggested in the following paper @article{bergstra2012random, title={Random search for hyper-parameter optimization}, author={Bergstra, James and Bengio, Yoshua}, journal={The Journal of Machine Learning Research}, volume={13}, number={1}, pages={281--305}, year={2012}, publisher={JMLR. org} }
