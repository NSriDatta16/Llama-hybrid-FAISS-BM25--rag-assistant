[site]: crossvalidated
[post_id]: 214486
[parent_id]: 214455
[tags]: 
Note that before the line in question, it says: Sample random minibatch of transitions $(\phi_j,a_j,r_j,\phi_{j+1})$ from D Then it performs gradient descent on $(y_j - Q(\phi_j,a_j;\theta))^2$ It means that you will update the weights related only to the selected action $j$ from the experience replay memory. As you noted, there is one output for each action, but you can't update all values at once since only 1 action is taken at each step. This comes from the Q-learning update formula: $Q(s,a) = Q(s,a) + \alpha[r + \gamma max_aQ(s',a) - Q(s,a)]$ Note that it updates the value for only 1 action, which is the action taken in state $s$.
