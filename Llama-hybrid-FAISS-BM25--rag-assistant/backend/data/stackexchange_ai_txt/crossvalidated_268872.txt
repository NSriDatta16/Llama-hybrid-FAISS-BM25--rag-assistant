[site]: crossvalidated
[post_id]: 268872
[parent_id]: 268704
[tags]: 
PCA solution First, beware when using PCA for this purpose. As I wrote in response to a related question PCA does not necessarily lead to selection of features that are informative for the regression you intend to do (see also Jolliffe 1982 ). OP proposed solution Now consider the proposed alternative mechanism: reduce the dimension of your feature vector to k dimensions by just choosing k of your features at random and eliminating the rest. Now in the problem statement we were asked to suppose that dimension of your vector x is very large . Let's call this dimension $p$ There are $pCk$ ways to choose $k$ predictors from a group of $p$. To give an example if $p=1000$ and we choose $k=5$ predictors from the dataset there would be $\approx 8.25 \times 10^{12}$ different models we would have to fit. And that's supposing we knew that $k=5$, and not $k=6$ etc etc. Put simply, it's not a problem you'd want to brute force in a large $p$ setting. Suggested solution To cope with regressions where $p$ is large a number of penalised regression strategies have been proposed. In particular the LASSO method will do dimension reduction while constructing a regression model by zeroing out the contribution from predictors that do not contribute enough to the model. There is a very clever algroithm (LARS) to fit the model efficiently.
