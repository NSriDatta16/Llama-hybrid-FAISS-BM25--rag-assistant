[site]: crossvalidated
[post_id]: 119879
[parent_id]: 
[tags]: 
How to interpret autocorrelation plot in MCMC

I am getting familiar with Bayesian statistics by reading the book Doing Bayesian Data Analysis , by John K. Kruschke also known as the "puppy book". In chapter 9, hierarchical models are introduced with this simple example: \begin{align} y_{ji} &\sim {\rm Bernoulli}(\theta_j) \\ \theta_j &\sim {\rm Beta}(\mu\kappa, (1-\mu)\kappa) \\ \mu &\sim {\rm Beta}(A_\mu, B_\mu) \\ \kappa &\sim {\rm Gamma}(S_\kappa, R_\kappa) \end{align} and the Bernoulli observations are 3 coins, each 10 flips. One shows 9 heads, the other 5 heads and the other 1 head. I have used pymc to infer the hyperparamteres. with pm.Model() as model: # define the mu = pm.Beta('mu', 2, 2) kappa = pm.Gamma('kappa', 1, 0.1) # define the prior theta = pm.Beta('theta', mu * kappa, (1 - mu) * kappa, shape=len(N)) # define the likelihood y = pm.Bernoulli('y', p=theta[coin], observed=y) # Generate a MCMC chain step = pm.Metropolis() trace = pm.sample(5000, step, progressbar=True) trace = pm.sample(5000, step, progressbar=True) burnin = 2000 # posterior samples to discard thin = 10 # thinning pm.autocorrplot(trace[burnin::thin], vars =[mu, kappa]) My question is regarding the autocorrelation. How shall I interpret autocorrelation? Could you please help me to interpret the autocorrelation plot? It says as the samples gets further from each other the correlation between them reduces. right? Can we use this to plot to find the optimum thinning? Does the thinning affect the posterior samples? after all, what is the use of this plot?
