[site]: crossvalidated
[post_id]: 570416
[parent_id]: 570415
[tags]: 
This is absolutely not specific to time series analysis (so I took the liberty of editing your tags). Any model selection procedure will try to select predictors that make the model "better". But the mathematical theorems used in calculating p-values - specifically, theorems about the distribution of certain statistics under the null hypothesis do not make allowance for this kind of selection. Essentially, the null hypothesis never included a selection step. You can run a quick simulation of this effect yourself. Simulate $n$ observations data points, and a $n\times k$ matrix of predictors that are completely unrelated to the observations. Run a regression, and record the $p$ values of your coefficients' $t$ tests. Do this many times. The $p$ values should be approximately uniformly distributed. Now redo the experiment, but in each run do some model selection beforehand. Stepwise by significance or AIC, forward or backward, all subsets, whatever - it does not matter. Again record the $p$ values of the remaining predictors. You will find that they are biased low, and badly so - but since your original predictors were completely unrelated to your simulated outcome, this should not happen. The distribution of the $p$ values is not uniform any more. So you cannot use low $p$ values to reject the null hypothesis any more. The model selection step invalidated the $p$ values. (And note that there is nothing about time series analysis anywhere.) This is discussed at length at many places, e.g., Is p-value essentially useless and dangerous to use? (see also Gelman's pieces and the ASA statement on $p$ values cited there), or How much do we know about p-hacking "in the wild"?
