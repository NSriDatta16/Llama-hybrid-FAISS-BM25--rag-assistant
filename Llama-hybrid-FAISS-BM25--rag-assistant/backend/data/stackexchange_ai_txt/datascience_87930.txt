[site]: datascience
[post_id]: 87930
[parent_id]: 
[tags]: 
Best practice to select precision vs. recall threshold for a binary classifier

I have a logistic regression model in Scikit-Learn doing a binary classification. Looking at the Roc curve for the classifier I can see that it performs really well: The AUC score is 0.99 which is great and looking at a probably distribution for the classifier, it has no problems separating the classes: However, when I look at precision versus recall for different thresholds I see the following: I know that it is possible to write a function in Scikit-Learn to use a custom threshold, depending upon business needs. I am wondering if this is something I need to do here? I assume the default threshold is at 0 (I acknowledge that may be incorrect, but I am not sure) and looking at the Precision/Recall curve, the 'sweet spot' threshold to get the most out of Precision and Recall appears to be about 1.7. Can someone please let me know if that is correct or my process has errors in it, as I don't fully have a handle on best practice for this (yet). Thanks!
