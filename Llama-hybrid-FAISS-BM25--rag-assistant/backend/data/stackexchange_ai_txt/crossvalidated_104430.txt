[site]: crossvalidated
[post_id]: 104430
[parent_id]: 104402
[tags]: 
Edit : Because of the trend of other answers I'm seeing, a short disclaimer : my answer is motivated by a machine learning perspective, and not statistical modelling. Some models, such as Naive Bayes, do not function with continuous features. Discretizing the features can help use them perform (much) better. Generally, models which do not rely on the "numerical" character of the feature (decision trees come to mind) are not impacted too much as long as the discretization is not too brutal. Some other models however will underperform vastly if discritization is too important. For example, GLMs will gain absolutely no benefit from the process. In some cases, when memory / processing time become limiting factors, feature discretization allows to aggregate a dataset, reducing its size and its memory / computing time consumption. So the bottom line is that if you are not computationally limited, and if your model does not absolutely require discrete features, do not run feature discretization. Otherwise, by all means consider it.
