[site]: crossvalidated
[post_id]: 532073
[parent_id]: 
[tags]: 
Bayesian Optimization vs. gradient descent

I don't know if this is the right place to ask this question. If you think this question is better asked in another StackExchange, please point me to that. This question is about the sampling efficiency of Bayesian Optimization (BO) vs. Gradient descent (GD) (or an accelerated version of it like Nesterov's AGD). Says $f(x)$ is an unknown function (could be convex as a special case), differentiable. Its gradient $\nabla f(x)$ is available, however evaluating $f(x)$ and its gradient by an "oracle" is expensive. I want to minimize $f(x)$ with no constraints. Typically I will use GD/AGD to minimize it to a certain accuracy from an initial $x_0$ , resulting in a number $N_{GD}$ of queries to the oracle (the number of evaluations of $f$ and $\nabla f$ ). I can also use BO to achieve the same. Since the gradient is available, I assume that the derivative information can be incorporated into the GP for BO (GP with derivative observations / information) to improve the learning. The BO algorithm needs a number $N_{BO}$ of queries to the oracle to achieve the same accuracy from the same initial $x_0$ as GD. My question is: is there any result (like a theorem) in the literature regarding the sampling efficiency between BO and GD/AGD in this scenario or similar scenarios? By that I mean whether and in what conditions we have the guarantee that $N_{BO} , and how much smaller.
