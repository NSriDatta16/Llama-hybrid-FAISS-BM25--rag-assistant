[site]: crossvalidated
[post_id]: 140540
[parent_id]: 
[tags]: 
Bayesian Linear Model Posterior as Sum of Squares?

As part of a homework, I am asked to do the math from the Normal-Inverse Gamma linear regression model. Starting from priors $N(\beta_0, \sigma^2 A)$ and $IG(\alpha_0, \delta_0)$ and with the help of "Introduction to Bayesian Econometrics" by Edward Greenberg, I was able to find the posterior distributions for $\beta$ and $\sigma^2$, as $$N(\beta^*, \sigma^2 B)\text{ and }IG(\alpha_1, \delta_1)$$ respectively. The updated parameters are $$B = (A^{-1} + X^TX)^{-1},\ \ \alpha_1 = \alpha_0 + n,$$ $$\beta^* = B(A^{-1}\beta_0 + X^TX \hat{\beta})$$ and $$\delta_1 = \delta_0 + y^Ty + \beta_0^TA^{-1}\beta_0 - \beta^{*T}B^{-1}\beta^*$$ where $$\hat{\beta} = (X^TX)^{-1}X^Ty$$ is the Maximum Likelihood Estimate. From these expressions, I am asked to express $\delta_1$ as $$\delta_0 + (y-X\hat{\beta})'(y-X\hat{\beta}) + (\beta_0 - \hat{\beta})[(X^TX)^{-1} + A]^{-1}(\beta_0 - \hat{\beta}).$$ I have seen in a couple of books that they arrive to this expression but they never do the workout. Does anyone have any pointers on how I could arrive to the solution? Anything would be helpful at this point.
