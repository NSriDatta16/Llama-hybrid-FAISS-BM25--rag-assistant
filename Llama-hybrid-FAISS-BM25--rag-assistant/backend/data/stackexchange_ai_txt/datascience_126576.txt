[site]: datascience
[post_id]: 126576
[parent_id]: 
[tags]: 
Why is Precision-Recall AUC different from Average Precision score?

I have been calculating the area under the Precision-Recall curve (AUPRC) using the code snippet below: from sklearn import metrics precision, recall, threshold = metrics.precision_recall_curve(y_true, y_pred) prec_rec_auc_score = metrics.auc(recall, precision) and the Average Precision (AP) by using the code below: from sklearn import metrics avg_precision_score = metrics.average_precision_score(y_true, y_pred) The two scores have usually been exactly the same, however, I recently came across a situation where the area under the Precision-Recall curve (AUPRC) was significantly larger than the Average Precision (AP) score. Is this possible? And why would this happen?
