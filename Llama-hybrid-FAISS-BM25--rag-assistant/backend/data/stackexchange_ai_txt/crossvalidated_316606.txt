[site]: crossvalidated
[post_id]: 316606
[parent_id]: 130867
[tags]: 
I want to add to others' answers by expanding on the "inference" part. In the context of machine learning, an interesting aspect of inference is estimating uncertainty. It's generally tricky with ML algorithms: how do you put a standard deviation on the classification label a neural net or decision tree spits out? In traditional statistics, distributional assumptions allow us to do math and figure out how to assess uncertainty in the parameters. In ML, there may be no parameters, no distributional assumptions, or neither. There has been some progress made on these fronts, some of it very recent (more recent than the current answers). One option is, as others have mentioned, Bayesian analysis where your posterior gives you uncertainty estimates. Bootstrap type methods are nice. Stefan Wager and Susan Athey, at Stanford, have some work from the past couple years getting inference for random forests . Analagously, BART is a Bayesian tree ensemble method that yields a posterior from which inference can be drawn.
