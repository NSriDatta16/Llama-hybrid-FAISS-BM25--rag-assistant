[site]: crossvalidated
[post_id]: 357667
[parent_id]: 357146
[tags]: 
I can't speak about your model specifically, but in general, no . This is part of the reason why analytical (i.e. symbolic) derivatives are not used for large models in machine learning. (Instead, algorithmic or automatic differentiation is used). The main reason is that symbolic derivatives for large models (unless hand-optimized by a human... or sometimes because of hand-optimization by a human) tend to be unwieldly and inefficient (e.g. repeats of the same sub-expression within the larger expression). Using Mathematica or Maple for a little while will quickly show you why. They can also be numerically unstable, if one is not careful to avoid catastrophic cancellation, for example. On the other hand, numerical derivatives have problems with efficiency as the number of parameters increases, since one has to do several function evaluations per parameter to accurately get a numerical derivative. (though one can partly evade this by taking random direction perturbations instead I suppose). As the number of parameters increases, these will be too costly. You also have to choose a step-size. But if the number of parameters is small, it should be ok. One can imagine expressions that can be quickly evaluated twice (to get a finite difference), but their analytic derivative cannot be, meaning one should just take the numerical approximation. So really it can depend on many factors: function complexity and number of parameters (which can change independently of course), quality/availability of analytic expressions, etc...
