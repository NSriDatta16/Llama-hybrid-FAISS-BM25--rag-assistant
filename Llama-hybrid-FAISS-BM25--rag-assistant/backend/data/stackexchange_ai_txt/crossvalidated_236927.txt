[site]: crossvalidated
[post_id]: 236927
[parent_id]: 236564
[tags]: 
I would question whether rescaling the teacher ratings is appropriate in all cases because it's not entirely clear whether the teachers are, or more critically whether they're perceived to be, rescaling their responses. As an example, let's say I can feel supportive towards somebody on a scale of 0-100%. I can applaud at volumes ranging 0-100dBs. If I feel 0% support then I'll produce 0dB applause, feeling 50% support then 50db applause, etc. Now, let's say that I'm not allowed to produce more than 75dbs. I have two options. I could keep my scale the same so if I support someone 75% then they get 75dBs, but also if I support someone 75-100% then I give them 75dB applause. Let's call this the filter hypothesis . Alternatively, I rescale it so that 0% support gets 0dB applause, 100% support gets 75dB applause, and I rescale the 1-to-1 mapping so, say 50% support gets 37.5dB (half-way between 0 and 75dB). Let's call this the rescale hypothesis . I'm not sure that rescaling your data makes sense if the filter hypothesis is correct. Unless raters reliably believe that the rescale hypothesis is correct (and potentially, unless it actually is correct) I'm not sure that you should rescale their responses. The notion that an identical response to two questions (say 80/100) does not reflect the same amount of the latent construct (support for students) is not uncommon. In IRT , it is actually encouraged for questions to vary in difficulty and, subsequently, it's quite normal for responses to differ between questions whilst still providing a reliable measure of the latent construct. I'm not sure I believe that this will be particularly problematic in your case.
