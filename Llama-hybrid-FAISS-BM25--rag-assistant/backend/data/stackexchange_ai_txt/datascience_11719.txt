[site]: datascience
[post_id]: 11719
[parent_id]: 11591
[tags]: 
You're mostly correct! ReLU does have a problem with the gradient vanishing, but only on one side, so we call it something else: the 'dying ReLU problem'. See this stack overflow response for more information: What is the "dying ReLU" problem in neural networks? It's a small semantic difference. Lots of functions (tanh and logistic/sigmoid) have derivatives very close to zero when you're outside the standard operating range. This is the 'vanishing gradient' issue. The worse you get, the harder it is to get back into the good zone. ReLU doesn't get worse the farther you are in the positive direction, so no vanishing gradient problem (on that side). This asymmetry might be enough to justify calling it something different, but the ideas are quite similar.
