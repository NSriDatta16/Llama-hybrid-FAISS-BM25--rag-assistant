[site]: datascience
[post_id]: 85542
[parent_id]: 85534
[tags]: 
When to scale or normalize a column? When you are using an algorithm that assumes your features have a similar range, you should use feature scaling. The main example is distance algorithms like Euclidean distance. Thus, the algorithms that are distance-based algorithms like K-means, SVM, etc also require feature scaling. Gradient Descent Based Algorithms also require your data to be scaled to safely converge. From the article : The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. For more, check this . How to choose which feature/column in a dataset for scaling and normalization? If the ranges of your features differ much then you should use feature scaling. If the range does not vary a lot like one of them is between 0 and 2 and the other one is between -1 and 0.5 then you can leave them as it's. However, you should use feature scaling if the ranges are, for example, between -2 and 2 and between -100 and 100. Check this explanation for a better and detailed explanation. And how to decide on which normalization algorithm to choose for a column? I assume here you are asking which feature scaling method to use. The answer to this question also may vary due to context, however, the general answer is: Use Standardization when your data follows Gaussian distribution. Use Normalization when your data does not follow Gaussian distribution. However, some algorithms do not require feature scaling and they can handle it. For example, since Decision Tree builds the model by splitting the features, it is invariant to features' scale.
