[site]: datascience
[post_id]: 15825
[parent_id]: 
[tags]: 
An abstract idea for the performance diffs between SLP and MLP

Recently I am working on some predictive analytic which based on neural network. When I tried some tests on MLP with one hidden layer or multiple hidden layers, the results showed that: one hidden layer performance on prediction is always better than multiple hidden layers the tests were executed both on Knime and R models, they gave me the same trends Based on my knowledge, more hidden layers with more neurons should perform better? Is there a principle on this about for which kind of dataset turns out this kind of result? Or I may need some book / article / paper to read? Do you have any abstract idea (not mathematical algorithms) for me, thanks! UPDATE I am working on a fraud detection dataset, which includes 1000 observations. and all the dimensions are numericed and normalized... But I am actually asking for a general idea, for the general idea to choose algorithm for different dataset
