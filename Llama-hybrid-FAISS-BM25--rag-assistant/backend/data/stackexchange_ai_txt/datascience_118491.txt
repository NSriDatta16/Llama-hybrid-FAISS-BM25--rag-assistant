[site]: datascience
[post_id]: 118491
[parent_id]: 118481
[tags]: 
This is exactly the setting of neural machine translation. The typical architecture used nowadays for that is the Transformer model . It receives a discrete series of tokens, converts them to continuous vectors with an embedding layer, uses an encode-decoder structure and generates probabilities over the token space. The loss used for discrete outputs is categorical cross-entropy. You may also look into different decoding (i.e. token generation) strategies, like greedy decoding and beam search. You can find implementations, tutorials, and a vibrant community in the HuggingFace Transformers library .
