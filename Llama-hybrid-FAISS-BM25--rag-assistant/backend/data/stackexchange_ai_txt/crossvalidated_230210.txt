[site]: crossvalidated
[post_id]: 230210
[parent_id]: 
[tags]: 
Feature selection for Machine learning

Based on my limited understanding, there are in general two reasons why feature selection is used: Reducing the number of features, to reduce overfitting and improve the generalisation of models. To gain a better understanding of the features and their relationship to the response variables. In most of the articles about feature selection focus is on reducing noise and to find a balance between bias and variance. My question is - Are there any cases where higher noise(not so important features) in model will actually be helpful?
