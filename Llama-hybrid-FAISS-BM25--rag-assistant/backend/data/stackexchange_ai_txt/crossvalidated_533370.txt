[site]: crossvalidated
[post_id]: 533370
[parent_id]: 532499
[tags]: 
As answered by @Dave, if your goal is out-of-sample prediction, you should not care at all that the two algorithms/processes have different set of features. The one with the better performance is the best one. You may be interested in: "But do I really know that the second one is better than the first?" Then you are interested in statistical tests (or Bayesian analysis) if the two algorithms have different performances. In statistical testing, you need a set of measures/predictions quality for the first and for the second algorithms. You want to test if the prediction error of the second is statistically significantly different from that of the first. I would suggest a Wilcoxon rank sum if the two sets of measures were taken on different (new/out-of-sample) data, and Wilcoxon signed-rank if they were measured on the same new/out-of-sample data (even though the second algorithm used 50 new features of that data that the first did not use). If you can, use the signed-rank on the same data, it is a more powerful test). The usual is to conclude that the second algorithm is really "better" than the first if the p-value of the test is There is a third question, which is "Is it worth it to use the second algorithm?" If both have the same cost (they are both developed, none of them is in use, the 50 extra features of the second model are free to collect, and so on) then there is no reason not to use the 2nd model even if the difference between them is not statistically significant . If there is some cost difference, for example, one is in use and the other is a prototype, there are costs in collecting the extra features, there are costs in maintaining the two different pieces of software, and so on, then one has to balance the gains in the RMSE of the second model with these costs. Finally, your use of "fair" in the questions suggest to me (weakly) that you are interested also on things like "One model is a better explanation of the phenomenon that the other", because one does not use the extra 50 features. In this case, you are really in the in-sample domain, and you want some measure that balances performance quality with the complexity of the model. In this case AIC, BIC, Hannanâ€“Quinn information criterion, are usual metrics to use (I do not know enough to suggest which one to use)
