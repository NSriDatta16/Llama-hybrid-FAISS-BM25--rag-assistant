[site]: crossvalidated
[post_id]: 507802
[parent_id]: 295999
[tags]: 
However the cross-validation result is more representative because it represents the performance of the system on the 80% of the data instead of just the 20% of the training set. This is not the whole picture. Yes, the cross-validation error uses unseen ("out-of-bag") data. However, note that you are using the CV error in fitting your model and tuning (hyper-)parameters. And then the final model you are working with has seen these "unseen" data. Cross-validation is part of model training. CV errors are not indicative of out-of-sample performance. This would argue for using model A, which performs better out-of-sample. However... Note that now you are using your test set in selecting a model. Thus, for your final model, the test set is not unseen any more! Another thought experiment: assume you are fitting a huge amount of models to your data (maybe some of these models add random noise to your predictions?) and assess all of these models on your test set. Then one model will perform best on the test set. But if you then choose this model as your final model, its good performance on the test set may be due to chance alone. You may have overfit to the test set. Conclusion: test set performance is only then a guide to true out-of-sample performance if it is not used in selecting, tuning or "improving just a little bit" your final model. Moreover, if I change the split of my sets, the different test accuracies I get have a high variance but the average cross validation accuracy is more stable. High variance in test set performance is a red flag. It does seem like you are overfitting. You may simply have too little data for your model, and beyond some point, even cross-validation may not save you. Consider regularizing your model, or constraining it in some other way (e.g., using the one standard error rule ). Also, accuracy is not a good evaluation measure .
