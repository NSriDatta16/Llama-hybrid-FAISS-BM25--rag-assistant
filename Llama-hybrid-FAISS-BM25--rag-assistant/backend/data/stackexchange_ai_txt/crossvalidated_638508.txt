[site]: crossvalidated
[post_id]: 638508
[parent_id]: 
[tags]: 
Form of a joint distribution table of random vectors and missing vectors

I am trying to follow this lecture on variational autoencoders . When talking about random observed data $o$ with missing components $m$ (min 14:10) he states that to calculate the log-likelihood of your data $X$ you need to marginalize out the missing components $P(X) = P(o,m)$ . However, I can't quite visualize how you can build a joint pdf with vectors that are not constant in size? For example, let's say the probability of an observed vetor $o_1 = (1, 2, -)$ with missing component $m_3 = 3$ is 0.3. How would the entry of the joint pdf table $P(o,m)$ would look like? Is this correct? I think Im just getting stupidly confuse with the unimportant fact whether if the data is 1 or N-dimensional $o_1$ $o_2$ $m_1$ $m_2$ $m_3$ 0.3
