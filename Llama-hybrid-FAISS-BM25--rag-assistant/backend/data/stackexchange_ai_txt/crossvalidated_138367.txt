[site]: crossvalidated
[post_id]: 138367
[parent_id]: 138229
[tags]: 
Because gradient descent updates each parameter in a way that it reduces output error which must be continues function of all parameters. Threshold based activation is not differentiable that is why sigmoid or tanh activation is used. Here is a single-layer NN $\frac{dJ(w,b)}{d\omega_{kj}} =\frac{dJ(w,b)}{dz_k}\cdot \frac{dz_k}{d\omega_{kj}}$ $\frac{dJ(w,b)}{dz_k} = (a_k -y_k)(a_k(1-a_k))$ $\frac{dz_k}{d\omega_{kj}} = x_k $ $ J(w,b) = \frac{1}{2} (y_k - a_k)^2 $ $ a_k = sigm(z_k) = sigm(W_{kj}*x_k + b_k) $ if activation function were a basic step function (threshold), derivative of $J$ w.r.t $z_k$ would be non-differentiable. here is a link that explain it in general. Edit: Maybe, I misunderstood what you mean by perceptron. If I'm not mistaken, perceptron is threholded weighed sum of inputs. If you change threholding with logistic function it turns into logistic regression. Multi-layer NN with sigmoid (logistic) activation functions is cascaded layers composed of logistic regressions.
