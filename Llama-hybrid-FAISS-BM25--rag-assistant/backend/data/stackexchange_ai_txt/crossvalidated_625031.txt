[site]: crossvalidated
[post_id]: 625031
[parent_id]: 625008
[tags]: 
In the backpropagation algorithm used to train neural networks, the update is proportional to the derivative of the activation function. If a hidden layer neuron is saturated at +/-1, the derivative will be zero and the associated weights will no longer be updated. The reason for standardisation of the inputs is to help ensure that the input layer weights can be safely initialised to small random values such that none of the first hidden layer units are likely to be saturated and permanently stuck at +/-1 (and hence not contributing anything). The initialisation of weights in higher layers is less problematic as the range of their inputs is governed by the activation function used in the preceding layer. It also matters if regularisation (weight decay) is being used. Scaling the input by 2 will give the same functional behaviour if the corresponding input layer weight is scaled by 0.5. However that input will be less regularised because the natural value for the weight is lower for the same functional behaviour. So it also makes the effects of regularisation more consistent for different inputs if they are standardised. Effectively, if you don't standardise the inputs, some will be whispering at the network and others will be SHOUTING, just like LASSO and Elastic Net. BTW L1 regularisation (i.e. LASSO) was used quite early in neural networks, see Williams (1993) , perhaps earlier than it was in statistics (initially invented earlier still in geophysics)?
