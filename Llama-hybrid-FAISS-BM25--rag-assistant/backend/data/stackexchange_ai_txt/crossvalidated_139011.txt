[site]: crossvalidated
[post_id]: 139011
[parent_id]: 138725
[tags]: 
Ok, I'm not an expert in Inductive Logic Programming, so my undestanding can be a bit limited. I have read referenced paper and your clarifications (very helpful ones). Your problem don't have a readily avaliable solution that is guaranteed to work, so I can only give some directions to look. In my understanding your first problem is working with training samples, that contain different numbers of clauses. This can be dealt by two approaches. First, assuming you are working with small (maybe 1-4) number of clauses you can try fixed length representation with all nonexistent clauses padded with zeroes. This is the simplest thing possible and have some chances to work. If you want to work with large number of clauses or simple approach won't work, it might be worth to try convolutional neural network (CNN) approach. As I understand, exact order of clauses is irrelevant and CNN provides exactly that kind of positional invariance while also reducing number of parameters to be learned (unless you want to generate clauses that are composed from other clauses rather than from literals, in which case things will become quite more complex). As for bit vectors, note that in most cases compact representations are more difficult for NN to deal with and "bag of features" representations are best option in many cases. This is because you first concern should be to keep things understandable for NN. Regarding perfomance, vectors with length of 10000 (and more) are quite common and should not be major problem for properly optimized implementation (and since you probably will mostly be dealing with sparse vectors and good implementation knows how to multiply them efficiently). So you should probably try things as they are and see what happens, starting with the simplest setup possible. If you run into some difficult problems, come back here and describe them. Machine learning in general is very much an expiremental field where you learn by doing and it is generally considered good practice not to solve difficulities until they manifest themselves in practical settings. In other problem domains, such as language processing, one can usually learn compact represenation of such "bag of features" vectors by pre-training autoencoders or similar techinques, however, in you settings, every literal only have meaning in the context of specific problem. If NN goes through many examples in training phase, it still might be beneficial to put low-dimensional "embedding" layer on top of bit vector (or maybe on top of concatenation of bit vector and numeric vector). I very much doubt that 'offline' mode with learning from abstract (generated) datasets will lead to generalizations that benefit specific problems. I feel that most perfomance gain that was obtained by authors of that paper is due to NN learning problem-specific regularities rather then understanding anything about Logic Programming in general. But this is separate interesting (and very difficult) research problem itself. However, expirements with generated datasets can be very helpful in finding best NN topology. You might also find useful to read recent papers on application of NNs, in particular CNNs, autoencoders and recursive tree-based autoencoders to image processing and (especially) natural language processing tasks, because you can learn some useful ideas that could be transferable to your domain.
