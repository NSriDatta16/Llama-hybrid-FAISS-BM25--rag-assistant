[site]: crossvalidated
[post_id]: 385444
[parent_id]: 304962
[tags]: 
In the comment you ask for an example. You can find it here (links to most informative comment, but please read entire thread for clarity). In the above example, the most intriguing part for me is the value of -666 . It is the score on the 2nd tree (the one with variable V2). Note that score falls outside of assumed distribution of $Y$ , i.e. $[2000 - 20000]$ . I understand this could be because -666 from example above does not come from averaging as in simple regression tree / random forest, but from the fact that entire prediction comes from aggregation (chain-like summation) of results from different sub-trees. The summation involves weights $w$ that are assigned to each tree and the weights themselves come from: $w_j^\ast = -\frac{G_j}{H_j+\lambda}$ where $G_j$ and $H_j$ are within-leaf calculations of first and second order derivatives of loss function, therefore they do not depend on the lower or upper $Y$ boundaries. Please note that the linked example does not prove this is mathematically or empirically possible, because values in the example are arbitrarily selected and do not come from an actual model. Formulas above come from xgboost website / paper
