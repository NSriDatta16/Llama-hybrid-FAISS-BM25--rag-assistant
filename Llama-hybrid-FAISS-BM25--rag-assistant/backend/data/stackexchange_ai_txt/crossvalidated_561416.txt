[site]: crossvalidated
[post_id]: 561416
[parent_id]: 193195
[tags]: 
This question has yet to have a satisfactory answer so I will throw my hat in the ring. In the case where the outcome represents a fraction of successes from a finite set of trials then a logistic regression as user stan has mentioned seems like a good approach. In the case where the outcome is a real number between 0 and 1 (not including 0 or 1) then we can use most regression techniques if we simply take the logit of the outcome. The constraint on the outcome to be on the unit interval can be difficult to enforce for many off the shelf algorithms. However transforming the outcome into a space where the outocme is unconstrained and then fitting a model in that space can yield some useful results. Back transforming the predictions from the unconstrained space to the constrained space will force the predictions to respect the boundaries. Let me provide an example. I'll generate some data which are constrained to be between 0 and 1 exclusive. import numpy as np from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.kernel_approximation import RBFSampler from sklearn.compose import TransformedTargetRegressor from scipy.special import expit, logit from scipy.stats import beta import matplotlib.pyplot as plt x = abs(np.random.normal(0, 2, size = 1000)).reshape(-1, 1) eta = np.exp(-x/2)*np.sin(np.pi*x) mu = expit(eta) kappa = 200 y = beta(mu*kappa, (1-mu)*kappa).rvs() Sklearn in particular has a useful transformer for transforming targets to some space and then back transforming come prediction time. This is implemented with the TransformedTargetRegressor class. Here is one way to write such a model, complete with some additional features to add non-linearity model = make_pipeline(RBFSampler(n_components=10, gamma=0.05), TransformedTargetRegressor(LinearRegression(), func=logit, inverse_func=expit)) model.fit(x,y) When I call .fit , sklearn will transform y via the logit function (which takes an outcome on (0, 1) and maps it to the real line) and then train a linear regression model on that transformed data. Now, when I call .predict sklearn will take the predictions from the linear regression and apply the expit function (which takes a real input and maps it to the unit interval). The result is a model which is forced to predict inside the unit interval, as I demonstrate here Ignore the ill fit for a moment and notice how the red line (the prediction) flattens at $y=1$ without me telling it to or forcing it to. This is due to the expit transform in the pipeline. There is no need to use sklearn for this in particular, that library just makes things easy. The general workflow without the pipelines, feature engineering and transformers may look like... # Transform the outcome first logit_y = logit(y) # Now, train a model fit = LinearRegression().fit(x, logit_y) # Predict on new data xx = np.linspace(0, 10, 101).reshape(-1, 1) predicted_logit_y = fit.predict(xx) # Map the predictions back to the constrained space predicted_y = expit(predicted_logit_y) plt.plot(xx, predicted_y) Code: import numpy as np from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.kernel_approximation import RBFSampler from sklearn.compose import TransformedTargetRegressor from scipy.special import expit, logit from scipy.stats import beta import matplotlib.pyplot as plt x = abs(np.random.normal(0, 2, size = 1000)).reshape(-1, 1) eta = np.exp(-x/2)*np.sin(np.pi*x) mu = expit(eta) kappa = 200 y = beta(mu*kappa, (1-mu)*kappa).rvs() plt.scatter(x, y) model = make_pipeline(RBFSampler(n_components=10, gamma=0.05), TransformedTargetRegressor(LinearRegression(), func=logit, inverse_func=expit)) model.fit(x,y) xx = np.linspace(0, 10, 101).reshape(-1, 1) plt.plot(xx, model.predict(xx), color='red') ```
