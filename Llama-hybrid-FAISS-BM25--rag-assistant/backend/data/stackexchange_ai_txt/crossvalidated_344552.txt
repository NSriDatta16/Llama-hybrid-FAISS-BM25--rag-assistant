[site]: crossvalidated
[post_id]: 344552
[parent_id]: 344498
[tags]: 
The subject of my Ph.D dissertation was to reveal the black-box properties of neural networks, specifically feed-forward neural networks, with one or two hidden layers. I will take up the challenge to explain to everyone what the weights and bias terms mean, in a one-layer feed-forward neural network. Two different perspectives will be addressed: a parametric one and a probabilistic one. In the following, I assume that the input values provided to each input neuron have all been normalized to the interval (0,1), by linear scaling ($x_{input}=\alpha \cdot x + \beta$), where the two coefficients $\alpha$ and $\beta$ are chosen per input variable, such that $x_{input} \in (0,1)$. I make a distinction between real-numbered variables, and enumerated variables (with a boolean variable as a special case enumerated variable): A real-numbered variable is provided as a decimal number between $0$ and $1$, after linear scaling. An enumerated variable, take the days of the week (monday, tuesday, etc.) are represented by $v$ input nodes, with $v$, being the number of enurable outcomes, i.e. $7$ for the number of days in a week. Such a representation of your input data is required in order to be able to interpret the (absolute value) size of the weights in the input layer. Parametric meaning: the larger the absolute value of the weight is between an input neuron and a hidden neuron, the more important that variable is, for the 'fireing' of that particular hidden node. Weights close to $0$ indicate that an input value is as good as irelevant. the weight from a hidden node to an output node indicates that the weighted amplification of the input variables that are in absolute sense most amplified by that hidden neuron, that they promote or dampen the particular output node. The sign of the weight indicates promotion (positive) or inhibition (negative). the third part not explicitly represented in the parameters of the neural network is the multivariate distribution of the input variables. That is, how often does it occur that the value $1$ is provided to input node $3$ - with the really large weight to hidden node $2$ ? a bias term is just a translation constant that shifts the average of a hidden (or output) neuron. It acts like the shift $\beta$, presented above. Reasoning back from an output neuron : which hidden neurons have the highest absolute weight values, on their connections to the output neurons? How often does the activation of each hidden node become close to $1$ (assuming sigmoid activation functions). I'm talking about frequencies, measured over the training set. To be precise: what is the frequency with which the hidden nodes $i$ and $l$, with large weights to the input variables $t$ and $s$, that these hidden nodes $i$ and $l$ are close to $1$? Each hidden node propagates a weighted average of its input values, by definition. Which input variables does each hidden node primarily promote - or inhibit? Also the $\Delta_{j,k}=\mid w_{i,j} - w_{i,k}\mid$ explains much, the absolute difference in weights between the weights that fan out from hidden node $i$ to the two output nodes $j$ and $k$. The more important hidden nodes are for an output node (talking in frequencies, over the training set), which 'input weights times input frequencies' are most important? Then we close in on the significance of the parameters of feed-forward neural networks. Probabilistic interpretation: The probabilistic perspective means to regard a classification neural network as a Bayes classifier (the optimal classifier, with the theoretically defined lowest error-rate). Which input variables have influence on the outcome of the neural network - and how often? Regard this as a probabilistic sensitivithy analysis. How often can varying one input variable lead to a different classification? How often does input neuron $x_{input}$ have potential influence on which classification outcome becomes the most likely, implying that the corresponding output neuron achieves the highest value? Individual case - pattern When varying a real-numbered input neuron $x_{input}$ can cause the most likely classification to change, we say that this variable has potential influence . When varying the outcome of an enumerated variable (changing weekday from monday $[1,0,0,0,0,0,0]$ to tuesday $[0,1,0,0,0,0,0]$, or any other weekday), and the most likely outcome changes, then that enumerated variable has potential influence on the outcome of the classification. When we now take the likelihood of that change into account, then we talk out expected influence . What is the probability of observing a changing input variable $x_{input}$ such that a the input case changes outcome, given the values of all the other inputs ? Expected influence refers to expected value , of $x_{input}$, namely $E(x_{input} \mid {\bf x}_{-input})$. Here ${\bf x}_{-input}$ is the vector of all input values, except from input $x_{input}$. Keep in mind that an enumerated variable is represented by a number of input neurons. These possible outcomes are here regarded as one variable. Deep leaning - and the meaning of the NN parameters When applied to computer vision, neural networks have shown remarkable progress in the last decade. The convolutional neural networks introduced by LeCunn in 1989 have turned out to eventually perform really well in terms of image recognition. It has been reported that they can outperform most other computer-based recognition approaches. Interesting emergent properties appear when convolutional neural networks are being trained for object recognition. The first layer of hidden nodes represents low-level feature detectors, similar to the scale-space operators T. Lindeberg, Feature Detection with Automatic Scale Selection, 1998 . These scale-space operators detect lines, corners, T-junctions and some other basic image features. Even more interesting is the fact that perceptual neurons in mammal brains have been shown to resemble this way of working in the first steps of (biological) image processing. So with CNNs, the scientific community is closing in on what makes human perception so phenomenal. This makes it very worthwhile to pursue this line of research further.
