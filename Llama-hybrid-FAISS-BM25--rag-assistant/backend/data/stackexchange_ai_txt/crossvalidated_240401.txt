[site]: crossvalidated
[post_id]: 240401
[parent_id]: 240391
[tags]: 
One common approach in these cases is to plot the train and test errors as a function of the size of your dataset. This might give some insight as to how the specific bias variance tradeoff of the algorithm you're using, is working for the particular problem. For a specific value of n , generate several subsets of size n from your dataset. For each one, calculate the train error and test error (the latter, using the hold-out data), and record the average. Repeat this for several values of n , and plot the two curves: If the algorithm is suffering from high bias, then the test error will initially decrease, then settle at a relatively-fixed high value. If the algorithm is suffering from high variance, there will typically remain a large gap between the train and test error. In your case, you mention that you're using a linear predictor, so high bias is somewhat more of a suspect. If the graphs corroborate this, a good guess would be to switch to one of many higher-variance algorithms.
