[site]: datascience
[post_id]: 8788
[parent_id]: 1177
[tags]: 
Your problem is separable. You can 1) transform your vectors into a weighted combination of the "raw" vector dimensions and then 2) compute similarity or distance with things like correlation or Euclidean distance. Problem 1: Weights are typically linear, just a matrix multiplied by your feature vector to rotate and scale it however you like. A matrix multiplication can even reduce your vectors dimensions (all the way down to a scalar score, if you like). The state of the art for dimension reduction and optimal feature vector transformation is SVD/PCA/LSI/LDA/NMF. FYI, the "eigenvectors" that some of these approaches produce are simply the columns or rows of your transformation matrix, the eigenvalues are the relative importance/weight of each of your new vectors. Some eigenvalues will be zero if you're doing dimension reduction and there's redundancy in your features. Problem 2: Some common distance metrics based on vector norms are L_1 (Manhattan) L_2 (Euclidian) L_inf (Supremum) Cosine (vector product) Fractional (p-norm) The state-of-the-art distance metric for high-dimensional problems is fractional distance or p-norm which is like Euclidean distance/norm, but the exponent is set to a value between 0 and 1 rather than 2.
