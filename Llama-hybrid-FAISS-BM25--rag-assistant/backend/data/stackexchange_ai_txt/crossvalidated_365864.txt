[site]: crossvalidated
[post_id]: 365864
[parent_id]: 365762
[tags]: 
Since you asked "regardless of the paper", I would like to take a simpler example: Penalised linear regression (Ridge/Lasso). For those cases, I can think of two reasons why: But first, note that there are two functions here: (F1) The loss function, which is an analytic function of the hyper-parameter and the data (in the paper you linked, it's $\tilde{J}$ ; and (F2) an estimate of the generalisation error, which depends on the optimum solution to (F1) and the hyper-parameter you picked in (F1). Caveat: A cursory glance at the paper reveals that the authors train a neural network classifier for the MNIST dataset. It doesn't explicitly say how to pick the hyper-parameter $\alpha$ , but I would have picked one $\alpha$ that minimises the validation error of the best model. The objective function for optimising the hyper-parameter is an expression that is a proxy for generalisation error. This expression is hard to write down as a simple analytic function that can be differentiated, but it can be easily evaluated at some point by simply solving the underlying optimisation problem. Evaluating the function (F2) requires you to solve an optimisation problem, which could be expensive. So, even if you can approximate the gradient for F2 to do gradient descent, it would be expensive and slow. In such cases, doing a grid-search is often "good enough." Having said that, there are techniques to optimise black-box objective functions (such as F2) by assuming some smoothness structure due to their dependence on the hyper-parameter. As an example, you can see this post that shows how a Lasso model's performance varies with its hyper-parameter $\lambda$ : (Image taken from this post: https://stats.stackexchange.com/a/26607/54725 ) Some references: Workshop on Bayesian Optimisation for Black Box Functions: https://bayesopt.github.io/ Yelp's MOE: https://github.com/Yelp/MOE Google's Vizier: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf
