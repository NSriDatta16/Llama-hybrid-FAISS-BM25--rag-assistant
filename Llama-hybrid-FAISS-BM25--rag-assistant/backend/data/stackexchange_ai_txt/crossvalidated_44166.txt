[site]: crossvalidated
[post_id]: 44166
[parent_id]: 
[tags]: 
How can kernelization improve the K Nearest Neighbour algorithm?

I'm new to kernels and have hit a snag while trying to kernelise kNN. Preliminaries I'm using a polynomial kernel: $K(\mathbf{x},\mathbf{y}) = (1 + \langle \mathbf{x},\mathbf{y} \rangle)^d$ Your typical Euclidean kNN uses the following distance metric: $d(\mathbf{x}, \mathbf{y}) = \vert\vert \mathbf{x} - \mathbf{y} \vert\vert$ Let $f(\mathbf{x})$ map $\mathbf{x}$ into some higher-dimensional feature space. Then the square of the above distance metric in Hilbert space can be expressed by inner products: $d^2(f(x), f(y)) = K(\mathbf{x},\mathbf{x}) - 2K(\mathbf{x}, \mathbf{y}) + K(\mathbf{y} ,\mathbf{y})$ Note that if we let $d = 1$ the above will degenerate to your standard Euclidean distance. The Question The main problem I have is that I cannot see how kernelising kNN produces better results as experimentally shown by, e.g, this paper (warning, direct pdf link!).
