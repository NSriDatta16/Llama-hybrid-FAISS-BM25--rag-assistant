[site]: crossvalidated
[post_id]: 55608
[parent_id]: 55452
[tags]: 
I think a tree-based method, such as random forest or GBM would be work for this problem. This does NOT seem like a problem where an SVM would be immediately applicable. "Sharp Corners" just screams "trees" at me, as they excel at detecting thresholds and interactions. Maybe try a simple decision tree, and compare the speed and accuracy to your SVM. /edit: Here's a quick example dataset with an H: #Random sample of data rm(list = ls(all = TRUE)) set.seed(42) n .9-rnorm(n)*fuzz] .45-rnorm(n)*fuzz) & (y We can fit a few different binary classification models to this data using the caret package : library(caret) myControl A quick plot of the out-of-sample area under the ROC Curve for each model shows that you might want to try a GBM on your dataset (though the SVM does surprisingly well). If you set the fuzz parameter smaller, the tree-based methods will start to greatly outperform the SVM: all.models Finally, it appears the GBM fits faster than the SVM (on this dataset) > sort(sapply(all.models, function(x) x$times$final['elapsed'])) linear.elapsed tree.elapsed boost.elapsed svm.elapsed forest.elapsed 0.00 0.00 0.06 0.25 0.39
