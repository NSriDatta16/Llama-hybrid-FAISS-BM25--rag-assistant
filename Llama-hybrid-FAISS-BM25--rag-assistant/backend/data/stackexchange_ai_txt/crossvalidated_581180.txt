[site]: crossvalidated
[post_id]: 581180
[parent_id]: 560427
[tags]: 
The situation of underfitting refering to low training score, which means your model is not complex enough to capture the trend ,even in training set. Your training $R^2 $ is 0.94, whcih did not indicate any sign of underfitting. The true problem of your model is overfitting, where the difference between training score and testing score is large, which indicate your model works well on in-sample data but bad on unseen data. Your train $R^2 $ 0.94 vs test $ R^2 $ 0.69 indicate your model is overfitting. In order to prevent overfitting in random forest, you could tune the following hypermeter: Number of estimators Max feature per estimators Bootstrap sample ratio Min split per node, Min impurity decrease , Min sample per node Max number of leaf node Max depth per estimators see Sklearn for more information But your result seems quite werid, as it is impossible to having a lower training score after hypermeter tuning. In your case ( training $R^2 $ and MAE is worse after hypermeter tuning). The only cause of such situation is you tune your hypermeter using metrics other than MAE and $R^2 $ , which does make too much sense. If your target is to find a model with highest $R^2$ , it doesnot make too much sense to tune your model using other metrics.
