[site]: datascience
[post_id]: 63997
[parent_id]: 
[tags]: 
Why seq2seq models are superior to simple LSTMs?

It is common knowledge in the field of Deep Learning that the most powerful Recurrent architecture is the sequence-to-sequence, or seq2seq , for pretty much any task (to time series forecasts, to machine translation, to text generation). Why? What are the underlying mathematical reasons for which an LSTM encoder-decoder architecture would outperform more canonical RNNs? Is it in the generation of dense latent representations? Is it about the comparatively higher number of parameters? Any hint is appreciated.
