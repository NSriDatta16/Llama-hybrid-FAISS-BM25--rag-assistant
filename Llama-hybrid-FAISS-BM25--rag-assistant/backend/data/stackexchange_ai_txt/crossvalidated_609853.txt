[site]: crossvalidated
[post_id]: 609853
[parent_id]: 584685
[tags]: 
My answer is more of a comment... I am a bit surprised that Keras does not have this kind of dual use in the layers and, as the OP says, all the tutorials online, even when claiming to build LLM as GPT, do independent layers. The corresponding keras layers of the transformers library do indeed embedding-embedding tying, but at the cost of ad-hoc command in the call. A possible alternative could be an initializer that provides the same variable to the embedding an unembedding layers. Now, if one defines a metric to check the difference between embedding and embedding matrix, say print("Misalignment:", np.mean(np.square(np.transpose(unembedding.get_weights()[0])-embedding.get_weights()[0]))) it can be seen that it usually decreases during the training. So perhaps it is not provided because convergence is foreseen to happen in a natural way in a long training, and it only needs to be forced in a fine tuning or short training. Also, it does not need to be forced as hard as installing a single layer, it is possible to promote the above metric to an additional loss, and then we have a sort of "elastic parameter tying"
