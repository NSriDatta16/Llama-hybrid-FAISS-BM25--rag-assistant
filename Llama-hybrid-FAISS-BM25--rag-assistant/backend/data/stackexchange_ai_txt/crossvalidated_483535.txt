[site]: crossvalidated
[post_id]: 483535
[parent_id]: 
[tags]: 
How to include the observed values, not just their probabilities, in information entropy?

Shannon entropy measures the unpredictability in a random variable's outcome as the weighted average of the probabilities of that variable's outcomes or observed values. However, it discards the actual observed values that the probabilities were derived from, only using probabilities in its formula instead. This seems like a significant information loss since it is the observed values, not their probabilities, that contain details like magnitude and direction of the random variable's realizations. Aren't there any estimators of entropy that include the observed values alongside the probabilities, so that it is not being measured based on probabilities alone? For example, an adjusted entropy measure that is a weighted average of the probabilities and observed values somehow?
