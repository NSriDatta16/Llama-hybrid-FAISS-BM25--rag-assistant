[site]: crossvalidated
[post_id]: 297783
[parent_id]: 297749
[tags]: 
Neural nets don't necessarily give probabilities as outputs, but they can be designed to do this. To be interpreted as probabilities, a set of values must be nonnegative and sum to one. Designing a network to output probabilities typically amounts to choosing an output layer that imposes these constraints. For example, in a classification problem with $k$ classes, a common choice is a softmax output layer with $k$ units. The softmax function forces the outputs to be nonnegative and sum to one. The $j$th output unit gives the probability that the class is $j$. For binary classification problems, another popular choice is to use a single output unit with logistic activation function. The output of the logistic function is between zero and one, and gives the probability that the class is 1. The probability that the class is 0 is implicitly one minus this value. If the network contains no hidden layers, then these two examples are equivalent to multinomial logistic regression and logistic regression , respectively. Cross entropy $H(p, q)$ measures the difference between two probability distributions $p$ and $q$. When cross entropy is used as a loss function for discriminative classifiers, $p$ and $q$ are distributions over class labels, given the input (i.e. a particular data point). $p$ is the 'true' distribution and $q$ is the distribution predicted by the model. In typical classification problems, each input in the dataset is associated with an integer label representing the true class. In this case, we use the empirical distribution for $p$. This simply assigns probability 1 to the true class of a data point, and probability 0 to all other classes. $q$ is the distribution of class probabilities predicted by the network (e.g. as described above). Say the data are i.i.d., $p_i$ is the empirical distribution, and $q_i$ is the predicted distribution (for the $i$th data point). Then, minimizing the cross entropy loss (i.e. $H(p_i, q_i)$ averaged over data points) is equivalent to maximizing the likelihood of the data. The proof is relatively straightforward. The basic idea is to show that the cross entropy loss is proportional to a sum of negative log predicted probabilities of the data points. This falls out neatly because of the form of the empirical distribution. Cross entropy loss can also be applied more generally. For example, in 'soft classification' problems, we're given distributions over class labels rather than hard class labels (so we don't use the empirical distribution). I describe how to use cross entropy loss in that case here . To address some other specifics in your question: Different training and prediction probabilities It looks like you're finding the output unit with maximum activation and comparing this to the class label. This isn't done for training using the cross entropy loss. Instead, the probabilities output by the model are compared to the 'true' probabilities (typically taken to be the empirical distribution). Shanon entropy applies to a specific kind of encoding, which is not the one being used in training the network. Cross entropy $H(p,q)$ can be interpreted as the number of bits per message needed (on average) to encode events drawn from true distribution $p$, if using an optimal code for distribution $q$. Cross entropy takes a minimum value of $H(p)$ (the Shannon entropy of $p$) when $q = p$. The better the match between $q$ and $p$, the shorter the message length. Training a model to minimize the cross entropy can be seen as training it to better approximate the true distribution. In supervised learning problems like we've been discussing, the model gives a probability distribution over possible outputs, given the input. Explicitly finding optimal codes for the distribution isn't part of the process.
