[site]: crossvalidated
[post_id]: 420773
[parent_id]: 420771
[tags]: 
The F test is the test that you use to understand whether a model as whole is statistically significant in predicting y, and/or (in case the model as a whole is significant) to test whether a subset of the model variables may be not significant. For a simple description check this . Remember also that there are criteria like the R2 adjusted for the number of parameters to be estimated that allow to compare models with a different number of predictors and parameters, so that, given the same unadjusted R2, the models with additional parameters are penalized to reflect the fact that more parameters entail a higher risk of overfitting and, potentially, a lower performance out of the samples where they are estimated. For the intuitive part, look at the link provided and especially at “The low F-value graph shows a case where the group means are close together (low variability) relative to the variability within each group. The high F-value graph shows a case where the variability of group means is large relative to the within group variability. In order to reject the null hypothesis that the group means are equal, we need a high F-value.” Indeed the F statistic measures the capability of the model of producing groups in the dependent variable that must be as polarized as possible: so their mean must be as distant as possible and the observations must be as clustered as possible around such means/centroids. In this sense the f test is connected to Anova and difference in means. Good models with a high f statistic must produce a grouping in the dependent variable with different means (high variance between group means) and observations that have low variance around each group mean (low variance within each group, computed as average squared deviation from the respective mean).
