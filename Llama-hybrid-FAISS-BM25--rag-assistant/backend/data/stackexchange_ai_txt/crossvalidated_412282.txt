[site]: crossvalidated
[post_id]: 412282
[parent_id]: 412279
[tags]: 
Short answer: You can always "compare" them, but the devil is in the details --- i.e., how you plan to describe the meaning of the outcome of the comparison. Longer answer: Whether or not it is valid to "compare" the AIC across two models really depends on the nature of the comparison you are making, and what you intend to infer from this comparison. The Akaike Information Criterion (AIC) is really just a penalised maximised log-likelihood value, where the statistic is calculated so that the maximum log-likelihood is negated. Thus, a lower AIC represents a "better fit" in the sense of having a higher maximised log-likelihood subject to a penalty on the parameters. In principle, this statistic can be compared between models with different distributional families, since these all involve a likelihood function for the data, and some number of fitted parameters. While a comparison can certainly be made, the devil is in the details ---i.e., how you plan to describe the meaning of the outcome of the comparison. In classical statistics, for a comparison across nested models, one can invoke distributional results for the difference in the AIC under the null hypothesis of the restricted model, and this allows formal testing of the hypothesis that the restricted model is "better" than the larger model. (These results are very similar to the standard chi-squared tests for nested models.) It sounds like your analysis involves nested models (i.e., one model includes an additional term, while the other excludes it). This would mean that you can apply standard tests for goodness of fit. More broadly, for a comparison involving non-nested models, all you can really say is that one AIC is lower than the other, and thus has a better "goodness of fit" as measured by the penalised maximised log-likelihood. In this latter case, the differences in the AIC for two non-nested models does not necessarily have any useful distributional result, so the comparison is somewhat vague, insofar as you don't have a known null distribution under the hypothesis that one of these model forms is the true model. In Bayesian statistics, comparisons of AIC across models can be considered under Bayesian analysis, with the penalties on the parameters interpreted as a log-prior (see e.g., Forster and Sober 2011 ). This analysis means that you are essentially holding a prior belief that models with more parameters are less probably a priori , and this leads to an implicit prior penalty on those models. Interpretation of the AIC under this approach is discussed in the linked paper, which I would recommend reading.
