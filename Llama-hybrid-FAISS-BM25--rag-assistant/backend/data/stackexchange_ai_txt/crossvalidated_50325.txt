[site]: crossvalidated
[post_id]: 50325
[parent_id]: 49953
[tags]: 
Vladimir Vapnik (co-inventor of the Support Vector Machine and leading computational learning theorist) advocates always trying to solve the problem directly, rather than solving some more general problem and then discarding some of the information provided by the solution. I am generally in agreement with this, so I would suggest a classification approach for the problem as currently posed . The reason for this is that if we are only interested in classifying a project as profitable or non-profitable, then we are really only interested in the region where profitability is around zero. If we form a classification model, that is where we will be concentrating our modelling resources. If we take a regression approach, we may be wasting modelling resources to make small improvements in performance for projects that will either be very profitable or unprofitable, potentially at the expense of improving performance of borderline projects. Now the reason that I said "as currently posed", is that very few problems actually do involve simple, hard binary classification (optical character recognition would probably be one). Generally different kinds of misclassification have different costs, or operational class frequencies may be unknown, or variable etc. In such cases it is better to have a probabilistic classifier, such as logistic regression , rather than an SVM. If seems to me that for a financial application, we will do better if we know the probability of whether the project will be profitable, and how profitable or otherwise it is likely to be. We may well be willing to fund a project that has a small chance of being profitable, but massively profitable should it succeed, but not a project that is almost guaranteed to be successful, but which will have such a small profit margin that we would be better off just sticking the money in a savings account. So Frank and Omri374 are both right! (+1 ;o) EDIT: To clarify why regression might not always be a good approach for solving a classification problem, here is an example. Say we have three projects, with profitability $\vec{y} = (-\$1000,+\$1, +\$1000)$, and for each project, we have an explanatory variable that we hope is indicative of profitability, $\vec{x} = (1, 2, 10)$. If we take a regression approach (with offset), we get regression coefficients $\beta_0 = -800.8288$ and $\beta_1 = 184.8836$ (provided I have done the sums correctly!). The model then predicts the projects as yielding profits $\hat{y}_1 \approx -\$616$, $\hat{y}_2 \approx -\$431$ and $\hat{y}_3 \approx \$1048$. Note that the second project is incorrectly predicted as being unprofitable. If on the other hand, we take a classification approach, and regress instead on $\vec{t} = 2*(y >= 0) - 1$, we get regression coefficients $\beta_0 = -0.2603$ and $\beta_1 = 0.1370$, which scores the three projects as follows: $\hat{t}_1 = -0.1233$, $\hat{t}_2 = 0.0137$ and $\hat{t}_3 = 1.1096$. So a classification approach correctly classifies project 1 as unprofitable and the other two as being profitable. The reason why this happens is that a regression approach tries equally hard to minimise the sum of squared errors for each of the data points. In this case, a lower SSE is obtained by allowing project two to fall on the incorrect side of the decision boundary, in order to achieve lower errors on the other two points. So Frank is correct in saying that a regression approach is likely to be a good approach in practice, but if classification actually is the ultimate aim, there are situations where it can perform poorly and a classification approach will perform better.
