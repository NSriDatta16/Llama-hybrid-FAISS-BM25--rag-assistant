[site]: crossvalidated
[post_id]: 120108
[parent_id]: 119879
[tags]: 
First of all: if memory and computational time for handling the MCMC output are not limiting, thinning is never "optimal". At an equal number of MCMC iterations, thinning of the chain always leads (on average) to a loss precision of the MCMC approximation. Routinely thinning based on autocorrelation or any other diagnostics is therefore not advisable . See Link, W. A. & Eaton, M. J. (2012) On thinning of chains in MCMC. Methods in Ecology and Evolution, 3, 112-115. In every-day practice, however, there is the common case that you have to work with a model for which the sampler doesn't mix very well (high autocorrelation). In this case 1) Close chain elements are very similar, meaning that throwing one away doesn't loose a lot of information (that is what the autocorrelation plot shows) 2) You need a lot of repetitions to get convergence, meaning that you get very large chains if you don't thin. Because of that, working with the full chain can be very slow, costs a lot of storage, or even lead to memory problems when monitoring a lot of variables. 3) Additionally, I have the feeling (but that one I have never tested systematically) that thinning makes JAGS a bit faster as well, so might be able to get a few more iterations in the same time. So, my point is: the autocorrelation plot gives you a rough estimate about how much information you are loosing through thinning (note though that this is an average over the whole posterior, loss may be higher in particular regions). Whether this price is worth paying depends on what you gain by thinning in terms saving computing resources and time later. If MCMC iterations are cheap, you can always compensate the loss of thinning by running a few more iterations as well.
