[site]: crossvalidated
[post_id]: 336901
[parent_id]: 336842
[tags]: 
I have looked a little at the time series. Summary of conclusions: original series is not stationary, but differenced series is stationary. There is no real uncertainty. The autocorrelation function of the differenced series looks like an autogressive series, and a arima(1,1,0) model seems to fit well. For some reason automatic model selection prefers not to difference (based on KPSS test), but I doubt that! I did analysis in R and will give some code here. First reading the posted data: X I will plot the data, and to study possible nonstationarity I will overlay a simple smooth: plot(X) lines( lowess(time(X), X, 1/100), col="red") You can see the local mean (in red) moving up and down, the series is not stationary. No hypothesis test is necessary! This up-and down movements is what causes the slow decay of the autocorrelation function of the series (not shown). We do the same for the differenced series: plot(dX) lines( lowess(time(dX), dX, 1/100), col="red") and now the local mean is pretty constant, so this differened series is stationary. Its autocorrelation function is also quite simple: acf(dX, lag.max=20) and this looks like an ar(1) model could fit: > mod1 summary(mod1) Call: arima(x = X, order = c(1, 1, 0)) Coefficients: ar1 -0.2101 s.e. 0.0028 sigma^2 estimated as 0.2697: log likelihood = -95461.78, aic = 190927.6 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set -2.551876e-05 0.5193162 0.4142524 -164.122 394.1604 0.9777882 ACF1 Training set -0.0158746 Some diagnostics: tsdiag(mod1) The autocorrelation function is quite close to that of white noise, but the small deviation is significant due to huge sample size. If a more complex model will be useful is doubtful, but let us see hat auto.arima thinks: library(forecast) auto.arima(X, max.p=10, max.q=10, D=0, stepwise=FALSE, approximation=FALSE, parallel=TRUE, trace=TRUE) Series: X ARIMA(3,0,1) with zero mean Coefficients: ar1 ar2 ar3 ma1 1.1782 -0.1822 -0.0231 -0.4259 s.e. 0.0385 0.0287 0.0086 0.0384 sigma^2 estimated as 0.2629: log likelihood=-93877.74 AIC=187765.5 AICc=187765.5 BIC=187814.2 so this prefers not to difference, based on kpss.test (in package tseries ). Let us force differencing: auto.arima(X, max.p=10, max.q=10, d=1, D=0, stepwise=FALSE, approximation=FALSE, parallel=TRUE, trace=TRUE) Series: X ARIMA(0,1,5) Coefficients: ma1 ma2 ma3 ma4 ma5 -0.2322 -0.0369 -0.0237 -0.0200 -0.0165 s.e. 0.0028 0.0029 0.0030 0.0029 0.0029 sigma^2 estimated as 0.2672: log likelihood=-94878.14 AIC=189768.3 AICc=189768.3 BIC=189826.7 and---surprise--- this chooses an arima(0,1,5) model. But note that the estimated variances are very close. And, knowing that an ar(1) model can be represented as $\text{ma}(\infty)$, they should be pretty close. You could compare the residuals! I am not sure of the reason behind the discrepancy of my manual analysis and auto.arima. Before using the results I would compare models for differenced and original data with some sort of cross validation, maybe first estimating models on first half of data, then using them for prediction on second half. I think that would give a better ground for model choice, but I leave that to you.
