[site]: crossvalidated
[post_id]: 220542
[parent_id]: 220505
[tags]: 
In this case it seems that multicollinearity might substantially affect the way that you will be able to interpret your ultimate results of "utility values" determined from the predicted probabilities, via your utility function. I would recommend that you look for a different approach to the overall problem that you are trying to address. This is like an attempt to assess the relative importance among your independent variables (IVs), through their different effects on your utility values when you set the value of 1 IV to twice the values of the other 5, and perform this process for all 6 IVs. In general, assessing variable importance can be difficult, as discussed for example on this page . This particular way to evaluate your 6 IVs has some substantial problems. In response to an earlier question on this matter, @DJohnson said: Collinearity is a problem only if the model needs to enforce an assumption of the independence of predictors. It's known that the model coefficients usually aren't affected by collinearity, whereas std errors and t-values are ... Examples of where IV independence can be important include pricing models where the analyst needs to know what the impact of price changes are clear and free of the influence of the other predictors in the model. In your use of this logistic regression model, it seems that you do need to "enforce an assumption of the independence of predictors" in order to make reliable interpretations of the utility values returned by the 6 predictions, each based on a change in a single predictor variable, that you propose. If two IVs are highly correlated, it is unrealistic to expect in practice that you would ever find the value of one IV to double while its correlated partner IV stays unchanged. If your multicollinearity comes from more complicated relations among your IVs, then your setting the value of only one IV to double at a time is even more unrealistic. You do not know that the "impact" of any one IV is "clear and free of the influence of the other predictors in the model." In fact, you know the opposite: that some IVs are highly correlated. The way that you are constructing the 6 predictions from your logistic regression model is something like extrapolating regression results beyond the bounds of the data: your are asking it to make predictions for circumstances unlike those from which the model was built, in terms of the combinations of IV values. So even if you could "ignore" multicollinearity for some purposes, like making predictions based on new real-world cases, it would seem unwise to ignore it here, given the way that you propose to use your model. Furthermore, your proposed use of the logistic regression coefficients does not seem to take into account any of the uncertainties in the regression coefficient estimates, and it's not clear whether you are using techniques like cross validation or bootstrapping to evaluate the quality of your model building or its dependence on the peculiarities of your present data sample. There are probably better ways to accomplish your ultimate goal, if it is in some way related to evaluating the relative importance of your 6 IVs.
