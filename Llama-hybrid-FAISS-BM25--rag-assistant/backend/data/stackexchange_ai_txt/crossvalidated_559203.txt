[site]: crossvalidated
[post_id]: 559203
[parent_id]: 
[tags]: 
Why does precision_recall_curve() return similar but not equal values than confusion matrix?

INTRO : I wrote a very simple machine learning project which classifies numbers based on the minst dataset: from sklearn.datasets import fetch_openml import numpy as np from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.linear_model import SGDClassifier from sklearn.metrics import confusion_matrix from sklearn.preprocessing import StandardScaler # Load mnist = fetch_openml("mnist_784", version=1) # Prepare X, y = mnist["data"], mnist["target"] y = y.astype(np.uint8) X_train, X_test, y_train, y_test = X.iloc[:60000], X.iloc[60000:], y.iloc[:60000], y.iloc[60000:] num_pipeline = Pipeline([ ('std_scaler', StandardScaler()) ]) X_train_prepared = num_pipeline.fit_transform(X_train) # Predict sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train_prepared, y_train) y_train_pred = sgd_clf.predict(X_train_prepared) In order to investigate the results I displayed the confusion matrix report like so: print(classification_report(y_train, y_train_pred)) Which returns: precision recall f1-score support 0 0.98 0.94 0.96 5923 1 0.98 0.95 0.96 6742 2 0.94 0.89 0.91 5958 3 0.93 0.85 0.89 6131 4 0.95 0.90 0.92 5842 5 0.91 0.82 0.86 5421 6 0.96 0.94 0.95 5918 7 0.96 0.91 0.93 6265 8 0.63 0.94 0.75 5851 9 0.92 0.85 0.88 5949 accuracy 0.90 60000 macro avg 0.92 0.90 0.90 60000 weighted avg 0.92 0.90 0.91 60000 GOAL : My goal is to recalculate the precision and recall for each class and I did it by using the precision_recall_curve() function, where I pick the value when the threshold is closest to 0: def find_nearest(array, value): array = np.asarray(array) idx = (np.abs(array - value)).argmin() return idx y_score = sgd_clf.decision_function(X_train_prepared) precision = dict() recall = dict() threshold = dict() classes = sgd_clf.classes_ for i in range(0, len(classes)): c = classes[i] precision[c], recall[c], threshold[c] = precision_recall_curve(y_train==c, y_score[:, c]) th0 = find_nearest(threshold[c], 0)+1 print(c, round(precision[c][th0],2), round(recall[c][th0],2), threshold[c][th0]) However the result that I get is the following: class precision recall threshold 0 0.98 0.93 0.0490132 1 0.98 0.95 0.0270484 2 0.94 0.86 0.1177426 3 0.92 0.84 0.0451941 4 0.94 0.88 0.0716969 5 0.90 0.80 0.0486564 6 0.96 0.91 0.0712867 7 0.95 0.90 0.0052748 8 0.85 0.75 0.0049975 9 0.85 0.78 0.0060422 QUESTION : As you can see the classes are quite similar in some cases and very different in others. Would you be able to point out why this alternative way of calculating the same metric is not exactly the same offered by the classification_report() ? Would you be able to propose a solution using the precision_recall_curve() ? NOTE : My code seems to be working correctly on other datasets for example the iris one: https://stackoverflow.com/questions/70567206/why-does-precision-recall-curve-return-different-values-than-confusion-matrix/70568589#70568589
