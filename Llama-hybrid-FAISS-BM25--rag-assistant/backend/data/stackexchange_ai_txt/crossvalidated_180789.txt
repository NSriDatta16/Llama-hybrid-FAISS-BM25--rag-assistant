[site]: crossvalidated
[post_id]: 180789
[parent_id]: 179984
[tags]: 
Theoretically, MLP can approximate any function, to an arbitrary precision, therefore there is no need for RNN. However that doesn't mean it is usable in a wild. Assuming we are talking about time series input, textbook answer would be that you can feed your time series in feed forward network, by having a input layer containg also inputs from previous time points. Therefore effectively transforming time series problem, into feed forward problem. However you will have to choose length of your input beforehand, and you will not be able to learn functions that depends on the inputs happening long time ago. You can solve this problem by having a RNN, that can theoretically, store information from arbitrarily long time ago, in it;s context layer. In practice however, you will have gradient exploding/vanishing problem.
