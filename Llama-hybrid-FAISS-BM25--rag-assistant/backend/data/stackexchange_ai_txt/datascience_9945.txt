[site]: datascience
[post_id]: 9945
[parent_id]: 
[tags]: 
R: machine learning on GPU

Are there any machine learning packages for R that can make use of the GPU to improve training speed (something like theano from the python world)? I see that there is a package called gputools which allows execution of code on the gpu, but I'm looking for a more complete library for machine learning.
