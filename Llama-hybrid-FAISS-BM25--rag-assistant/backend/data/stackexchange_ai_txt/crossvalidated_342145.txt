[site]: crossvalidated
[post_id]: 342145
[parent_id]: 342074
[tags]: 
@GreggH's answer is excellent, but seems, in the penultimate paragraph, to hint at something fishy going on. In fact a formal definition of p-values takes this kind of situation into account. When the null hypothesis is composite, specifying a set of values $\Theta_0$ for the unknown parameter $\theta$, a valid p-value $p(x)=\alpha$ (where $x$ is the observed data) is one which has a distribution function at $\alpha$ that does not exceed $\alpha$ whatever the value of $\theta$ might be (within $\Theta_0$): $$\Pr_\theta \left[ p(X)\leq\alpha \right] \leq \alpha \quad \forall \theta \in \Theta_0, \ \forall\alpha\in[0,1]$$ One way of ensuring validity † is simply to construct the p-value as the supremum of the probability that a test statistic $T$ exceeds or equals its observed value $t$ over all values of $\theta$ within $\Theta_0$: $$p(x) = \sup_{\theta\in\Theta_0} \Pr_\theta\left[T\geq t\right]$$ In many cases the location of the supremum can easily be seen to be at the boundary with the alternative hypothesis, so there's no difference between testing $H_0:\theta=\theta_0$ or $H_0: \theta\leq\theta_0$ vs $H1:\theta>\theta_0$. † In general $\theta$ may be a vector, say $(\phi,\lambda$), with one component, say $\phi$, being the parameter of interest, & the other, say $\lambda$, being a nuisance parameter; another way to construct a valid p-value is to condition on a statistic that's sufficient for $\lambda$ when $\phi=\phi_0$.
