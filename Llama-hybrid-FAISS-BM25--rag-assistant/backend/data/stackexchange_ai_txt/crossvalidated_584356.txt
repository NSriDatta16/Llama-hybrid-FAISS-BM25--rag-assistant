[site]: crossvalidated
[post_id]: 584356
[parent_id]: 
[tags]: 
Computing the partition function using the forward-backward algorithm for linear chain CRFs

I'm trying to implement the forward-backward algorithm for a Linear Chain Conditional Random Field, as to compute the marginal distribution over labels for each time step in a sequence. I'm following Hugo Larochelle's (excellent) course on Neural Networks on YouTube . More specifically, he describes how to compute the $\alpha$ -table (forward pass) and the $\beta$ -table (backward pass) in video 3.4 on computing the partition function , with slides available here . In the video he states that it's possible to compute the partition function (i.e. the normalization constant of the likelihood function) $Z(\mathbf{X})$ , both from the $\alpha$ -table and the $\beta$ -table. He suggest making the sanity check of computing $Z(\mathbf{X})$ from both tables, as a test of ones implementation. This is exactly what I'm trying to do. But I get different values for $Z(\mathbf{X})$ when computing it using the $\alpha$ -table and the $\beta$ -table. My understanding of the math What Hugo calls the " unary factors ", $a_u(y_k)$ , is what other call "emissions". They represent some squashing function, combining (possibly) multiple feature vectors $...,\mathbf{x}_{k-1},\mathbf{x}_k, \mathbf{x}_{k+1},...$ from different time steps. I simply assume that each time step has a single feature vector. The feature matrix $\mathbf{X}$ is $C\times K$ , where $C$ is the size of the label space and $K$ is the sequence length, and I assume all $x_{i,k}\in(0,1)$ . So in my simplified case the unary factors are simply the entry in the feature matrix $\mathbf{X}$ , where the label value $y_k$ serves as the row index, and the timestep $k$ represents the column index, $$a_u(y_k) = x_{y_k, k}.$$ The pairwise factors $a_p(y_k, y_{k+1})$ are just entries in a $C\times C$ matrix of transition weights, $a_p(y_k, y_{k+1}) = V_{y_k, y_{k+1}}$ . Say $C=3$ , $$V=\left(\begin{array}\ \color{red}{V_{1,1}} & \color{red}{V_{1,2}} & \color{red}{V_{1,3}}\\ \color{blue}{V_{2,1}} & \color{blue}{V_{2,2}} & \color{blue}{V_{2,3}} \\ \color{green}{V_{3,1}} & \color{green}{V_{3,2}} & \color{green}{V_{3,3}} \\ \end{array}\right). $$ The $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ matrices are both $C\times (K-1)$ . In my code I simply assume they both are $C\times K$ and just keep the last and first columns, respectively, filled with NaN's. If we write out the $\alpha$ and $\beta$ table calculations for the case of $C=3$ we get the following. $\mathbf{X}$ is, $ \mathbf{X} = \left(\begin{array}\ x_{1,1} & x_{1,2} & \dots & x_{1,K} \\ x_{2,1} & x_{2,2} & \dots & x_{2,K} \\ x_{3,1} & x_{3,2} & \dots & x_{3,K} \\ \end{array}\ \right)$ . The forward algorithm The $\alpha$ -table is, $ \boldsymbol{\alpha} = \left(\ \begin{array}\ \boldsymbol{\alpha}_1 & \boldsymbol{\alpha}_2 & \dots & \boldsymbol{\alpha}_{K-1} \end{array}\ \right)\ =\left(\ \begin{array}\ \alpha_{1,1} & \alpha_{1,2} & \dots & \alpha_{1,K-1} \\ \alpha_{2,1} & \alpha_{2,2} & \dots & \alpha_{2,K-1} \\ \alpha_{3,1} & \alpha_{3,2} & \dots & \alpha_{3,K-1} \\ \end{array}\ \right)$ . Initialization : $\left(\ \begin{array}\ \alpha_{1,1}\\ \alpha_{2,1}\\ \alpha_{3,1}\\ \end{array}\ \right) = \left(\ \begin{array}\ \exp(x_{1,1} + \color{red}{V_{1,1}}) + \exp(x_{2,1} + \color{blue}{V_{2,1}}) + \exp(x_{3,1} + \color{green}{V_{3,1}})\\ \exp(x_{1,1} + \color{red}{V_{1,2}}) + \exp(x_{2,1} + \color{blue}{V_{2,2}}) + \exp(x_{3,1} + \color{green}{V_{3,2}})\\ \exp(x_{1,1} + \color{red}{V_{1,3}}) + \exp(x_{2,1} + \color{blue}{V_{2,3}}) + \exp(x_{3,1} + \color{green}{V_{3,3}})\\ \end{array}\ \right)$ Recursion for $k=2$ to $k=K-1$ : $\left(\ \begin{array}\ \alpha_{1,k}\\ \alpha_{2,k}\\ \alpha_{3,k}\\ \end{array}\ \right) = \left(\ \begin{array}\ \exp(x_{1,k} + \color{red}{V_{1,1}})\alpha_{1,k-1} + \exp(x_{2,k} + \color{blue}{V_{2,1}})\alpha_{2,k-1} + \exp(x_{3,k} + \color{green}{V_{3,1}})\alpha_{3,k-1}\\ \exp(x_{1,k} + \color{red}{V_{1,1}})\alpha_{1,k-1} + \exp(x_{2,k} + \color{blue}{V_{2,2}})\alpha_{2,k-1} + \exp(x_{3,k} + \color{green}{V_{3,2}})\alpha_{3,k-1}\\ \exp(x_{1,k} + \color{red}{V_{1,1}})\alpha_{1,k-1} + \exp(x_{2,k} + \color{blue}{V_{2,3}})\alpha_{2,k-1} + \exp(x_{3,k} + \color{green}{V_{3,3}})\alpha_{3,k-1}\\ \end{array}\ \right)$ The backward algorithm The $\beta$ -table is, $\boldsymbol{\beta} = \left(\ \begin{array}\ \boldsymbol{\beta}_2 & \boldsymbol{\beta}_3 & \dots & \boldsymbol{\beta}_{K} \end{array}\ \right)\ =\left(\ \begin{array}\ \beta_{1,2} & \beta_{1,3} & \dots & \beta_{1,K} \\ \beta_{2,2} & \beta_{2,3} & \dots & \beta_{2,K} \\ \beta_{3,2} & \beta_{3,3} & \dots & \beta_{3,K} \\ \end{array}\ \right)$ Notice the column index starts at 2, which ensures the $\beta$ -table is $C\times (K-1)$ . Initialization : $\left(\ \begin{array}\ \beta_{1,1}\\ \beta_{2,1}\\ \beta_{3,1}\\ \end{array}\ \right) =\left(\ \begin{array}\ \exp(x_{1,1} + \color{red}{V_{1,1}}) + \exp(x_{2,1} + \color{red}{V_{1,2}}) + \exp(x_{3,1} + \color{red}{V_{1,3}})\\ \exp(x_{1,1} + \color{blue}{V_{2,1}}) + \exp(x_{2,1} + \color{blue}{V_{2,2}}) + \exp(x_{3,1} + \color{blue}{V_{2,3}})\\ \exp(x_{1,1} + \color{green}{V_{3,1}}) + \exp(x_{2,1} + \color{green}{V_{3,2}}) + \exp(x_{3,1} + \color{green}{V_{3,3}})\\ \end{array}\ \right)$ Recursion for $k=K-1$ to $k=2$ : $\left(\ \begin{array}\ \beta_{1,k}\\ \beta_{2,k}\\ \beta_{3,k}\\ \end{array}\ \right) =\left(\ \begin{array}\ \exp(x_{1,k} + \color{red}{V_{1,1}})\beta_{1,k+1} + \exp(x_{2,k} + \color{red}{V_{1,2}})\beta_{2,k+1} + \exp(x_{3,k} + \color{red}{V_{1,3}})\beta_{3,k+1}\\ \exp(x_{1,k} + \color{blue}{V_{2,1}})\beta_{1,k+1} + \exp(x_{2,k} + \color{blue}{V_{2,2}})\beta_{2,k+1} + \exp(x_{3,k} + \color{blue}{V_{2,3}})\beta_{3,k+1}\\ \exp(x_{1,k} + \color{green}{V_{3,1}})\beta_{1,k+1} + \exp(x_{2,k} + \color{green}{V_{3,2}})\beta_{2,k+1} + \exp(x_{3,k} + \color{green}{V_{3,3}})\beta_{3,k+1}\\ \end{array}\ \right)$ According to Hugo Larochelle's slides the partition function can be computed using the $\alpha$ -table, $$Z(\mathbf{X})=\exp(x_{1,K})\alpha_{1,K-1}+\exp(x_{2,K})\alpha_{2,K-1}+\exp(x_{3,K})\alpha_{3,K-1}$$ or using the $\beta$ -table, $$Z(\mathbf{X})=\exp(x_{1,1})\beta_{1,2}+\exp(x_{2,1})\beta_{2,2}+\exp(x_{3,1})\beta_{3,2}.$$ My Python implementation Below is my implementation of the above equations in Python. Unfortunately the the compute_Z_from_alpha_table and compute_Z_from_beta_table functions do not produce the same value for the partition function (see output below). I cannot see where I'm going wrong, I need a pair of fresh eyes on the problem. I have three hypothesis: I have misinterpreted the math. There is an error in my implementation. Some detail is left out of Larochelle's slides. Any help is greatly appreciated. import numpy as np np.random.seed(1234) time_steps = 5 labels = 3 transitions = np.random.random(size=(labels, labels)) log_transitions = np.log(transitions) X = np.random.random(size=(labels, time_steps)) def compute_Z_from_alpha_table(X): labels, time_steps = X.shape last_time_idx = time_steps-1 alpha_table = np.empty((labels, time_steps)) alpha_table[:] = np.nan alpha_table[:, 0] = np.sum(np.exp(X[:,0] + log_transitions), axis=0) for t in range(1, time_steps-1): alpha_table[:, t] = np.sum(np.exp(X[:,t] + log_transitions)*alpha_table[:, t-1], axis=0) Z = np.sum(np.exp(X[:,last_time_idx])*alpha_table[:, time_steps-2]) return Z def compute_Z_from_beta_table(X): labels, time_steps = X.shape last_time_idx = time_steps-1 beta_table = np.empty((labels, time_steps)) beta_table[:] = np.nan beta_table[:, last_time_idx] = np.sum(np.exp(X[:,last_time_idx] + log_transitions.T), axis=0) for t in range(last_time_idx-1, 0, -1): beta_table[:, t] = np.sum(np.exp(X[:,t] + log_transitions.T)*beta_table[:, t+1], axis=0) Z = np.sum(np.exp(X[:,0])*beta_table[:,1]) return Z print(f"Z from alpha = {compute_Z_from_alpha_table(X)}") print(f"Z from beta = {compute_Z_from_beta_table(X)}") Output Z from alpha = 351.829397003545 Z from beta = 334.09770584094815
