[site]: crossvalidated
[post_id]: 139768
[parent_id]: 
[tags]: 
How to score predictions in test set taking into account the full predictive posterior distribution?

I have three predictive models (regressions) which parameters are estimated by Markov Chain Monte Carlo. Predictions are made over a test set of size $N$. Since I compare the models under different settings, I would like to summarize the results in a table. I have considered using Means Squared Error (MSE) or $R^2$ but I wonder if I could be fully bayesian and use all the posterior sampled by MCMC, and not just the means of the predictions. For instance, MSE would be something like: $$ \text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i^{true} - \mathbb{E}[y_i^{MC}])^2 $$ where $\mathbb{E}[y_i^{MC}]$ is the average of the $y_i$ samples. However, it seems reasonable to me that a tight posterior around the good prediction should have a better score than a flatter posterior. And yet, I don't find any reference to this kind of method. Questions: What is the/a correct bayesian way to do it? Does it have a name? What is the name for techniques that check the accuracy of posteriors-based predictions in test sets where we know the true outputs? [1] [1] : I say that because I've realized that a lot of bayesian literature talks about assessing the quality of a prediction (e.g.: the outcome of the elections) before we know the real outcome. Edit : What about taking the mean of the mean squared errors? That is averaging the squared errors for the samples at every test point, and then averaging the results through all the points. \begin{align} \frac{1}{N}\sum_{i=1}^N\left[\frac{1}{M}\sum_{j=1}^M\left(y_i^{true}- y_i^{(j)}\right)^2\right] \end{align} where $M$ is the number of samples from the MCMC and $y_i^{(j)}$ is the prediction, for the input $x_i$, using the $j$-th sample of the model parameters $\boldsymbol{\theta^{(j)}}$.
