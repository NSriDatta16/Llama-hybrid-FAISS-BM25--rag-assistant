[site]: crossvalidated
[post_id]: 531644
[parent_id]: 
[tags]: 
Univariate multi-step prediction model for missing value imputation

I have a dataframe with columns of timestamp and energy usage. The timestamp is taken for every min of the day i.e., a total of 1440 readings for each day. I have few missing values in the data frame. I want to impute those missing values with deep neural networks, and for this, I have to frame the dataset for developing the neural network. Here's a example of the data: timestamp mains_1 0 2013-01-03 00:00:00 155.0 1 2013-01-03 00:01:00 154.0 2 2013-01-03 00:02:00 155.0 3 2013-01-03 00:03:00 NaN 4 2013-01-03 00:04:00 153.0 5 2013-01-03 00:05:00 153.0 6 2013-01-03 00:06:00 153.0 7 2013-01-03 00:07:00 152.0 8 2013-01-03 00:08:00 152.0 9 2013-01-03 00:09:00 152.0 10 2013-01-03 00:10:00 152.0 10 2013-01-03 00:10:00 152.0 11 2013-01-03 00:11:00 NaN 12 2013-01-03 00:12:00 NaN 13 2013-01-03 00:13:00 NaN 14 2013-01-03 00:14:00 149.0 15 2013-01-03 00:15:00 150.0 For each NaN, I want to use the past five value as input for my model to predict the NaN Right now I have this line of code: def prp_data(df, ts, look_back=5, train_fraction=0.80): """ Create data for modelling """ # Convert an array of values into a dataset matrix def create_dataset(dataset, look_back=5): """ Create the dataset """ dataX, dataY = [], [] for i in range(len(dataset)-look_back-1): a = dataset[i-look_back:i, 0] dataX.append(a) dataY.append(dataset[i, 0]) return np.array(dataX), np.array(dataY) # Get dataset dataset = df[ts].values dataset = dataset.astype('float32').reshape(-1, 1) # Split into train and test sets train_size = int(len(dataset) * train_fraction) test_size = len(dataset) - train_size train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:] # Reshape into X=t and Y=t+1 look_back = look_back trainX, trainY = create_dataset(train, look_back) testX, testY = create_dataset(test, look_back) return trainX, trainY, testX, testY What this does is reframes the data into something like this: y-5 y-4 y-3 y-2 y-1 y 0 NaN NaN NaN NaN NaN 155.0 1 NaN NaN NaN NaN NaN 154.0 2 NaN NaN NaN NaN NaN 155.0 3 NaN NaN 155.0 154.0 155.0 NaN 4 NaN 155.0 154.0 155.0 NaN 153.0 5 155.0 154.0 155.0 NaN 153.0 153.0 6 154.0 155.0 NaN 153.0 153.0 153.0 7 155.0 NaN 153.0 153.0 153.0 152.0 8 NaN 153.0 153.0 153.0 152.0 152.0 9 153.0 153.0 153.0 152.0 152.0 152.0 10 153.0 153.0 152.0 152.0 152.0 152.0 11 153.0 152.0 152.0 152.0 152.0 NaN 12 152.0 152.0 152.0 152.0 NaN NaN 13 152.0 152.0 152.0 NaN NaN NaN 14 152.0 152.0 NaN NaN NaN 149.0 I want to train a model for predicting these missing values using only lagged values. As you can see in the data, I have continues missing gaps with different sizes. Now, I think I have to predict the first NaN (for example row 11) and then plug it in the next row and use it for predicting row 12 and so forth. For training and testing the model, I think I need to use rows where y is available as training data and rows where y is NaN as the testing dataset. Am I doing this correctly? If yes, what should the deep learning model look like? I have experience with Keras and Conv and LSTM layers and I prefer to develop a model that can handle multi-step prediction.
