[site]: crossvalidated
[post_id]: 373059
[parent_id]: 373055
[tags]: 
Gradient descent updates all parameters at each step. You can see this in the update rule: $$ w^{(t+1)}=w^{(t)} - \eta\nabla f\left(w^{(t)}\right). $$ Since the gradient of the loss function $\nabla f(w)$ is vector-valued with dimension matching that of $w$ , all parameters are updated at each iteration. The learning rate $\eta$ is a positive number that re-scales the gradient. Taking too large a step can endlessly bounce you across the loss surface with no improvement in your loss function; too small a step can mean tediously slow progress towards the optimum. Although you could estimate linear regression parameters using gradient descent, it's not a good idea. Likewise, there are better ways to estimate logistic regression coefficients.
