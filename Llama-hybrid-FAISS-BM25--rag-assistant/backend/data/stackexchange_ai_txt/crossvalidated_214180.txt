[site]: crossvalidated
[post_id]: 214180
[parent_id]: 213960
[tags]: 
Separable depends on how you intend to separate your data (e.g. linear VS polygon). With overfitting, you will be able to separate all classes/samples at some point, no matter how hard the problem is - but you will not have a model that generalizes, so it will perform bad on new data. Your assumption is correct that given some separation methods, feature transform/feature engineering might help in making the data "more separable" for those methods. But you cannot invent information by transforming it. This means that if your data is not separable because the required, underlying information is missing in the first place, then you will not find any way of separating it that still generalizes (this is the case if your data is e.g. random information by design). This is pretty much what the core idea of classification in machine learning is about: sometimes you find a suitable separation, but sometimes you just don't for e.g. this information not being present in the data, or you not being being able to find a suitable transformation/separation using this information. If data seems really "random" from my feature analysis and first runs, I would not immediately give up, but probably consider the following: try to understand what the information you have actually means - is there a reason it is so random? Does it fit your expectations from your domain specific knowledge? Could be that the data is just erroneous? (that happens...) try to get more samples if possible. Theoretically it could be that you have a very sparse representation of your samples in feature space, and that e.g. each of the samples you have actually represents a cluster of samples of the same class at this point in the feature space. Though this might be quite unlikely with most datasets. try out different feature transformations/feature derivations to make it "more separable" for the classifiers you use (just as you said), and evaluate different model types and model parametrizations using a thorough setup (e.g. using repeated cross validation with separate test set) to make sure the models' performance evaluations don't just give me good results caused by some unnoticed overfitting. If I still would get bad results I would probably think of getting more information for samples, e.g. recording more features, etc. But this of course might not be possible for all problems.
