[site]: crossvalidated
[post_id]: 204846
[parent_id]: 
[tags]: 
Gradient descent using Newtons method

I am implementing gradient descent for regression using newtons method as explained in the 8.3 section of the Machine Learning A Probabilistic Perspective (Murphy) book. I am working with two dimensional data in this implementation. I am using following notations. x = input data points m*2 y = labelled outputs(m) corresponding to input data H = Hessian matrix is defined as $$2*X^t X / m $$ gradient descent update $$ \theta = \theta - H^{-1}g(\theta) $$ where $ g(\theta) = 2* \sum_n (\theta^t x_{n} - y_{n})x $ is loss function. In my case $\theta$ theta is $2*1$ array and $H$ $2*2$ matrix. Now my question is how to calculate $\theta$ as $H^{-1}$ is matrix of $2*2 $ the product will of loss function and $H^{-1}$ will be $2*2$ but theta is $2*1$. Here is my python implementation of it with work around considering only diagonal values of $H$. However this is not working as cost is increasing in each iteration. def loss(x,y,theta): m,n = np.shape(x) predicted = np.dot(x, theta) error = predicted - y cost = np.sum(error ** 2) / m return cost def NewtonMethod(x,y,theta,maxIterations): m,n = np.shape(x) xTrans = x.transpose() H = 2 * np.dot(xTrans,x) / m Hinv = np.linalg.inv(H) thetaPrev = np.zeros_like(theta) best_iter = maxIterations for i in range(0,maxIterations): cost = loss(x,y,theta) theta = theta - np.diagonal(np.multiply(Hinv,cost)) if np.array_equal(theta,thetaPrev): break; else: thetaPrev = theta best_iter = i return theta Here are the sample values I used import numpy as np x = np.array([[-1.7, -1.5],[-1.0 , -0.3],[ 1.7 , 1.5],[-1.2, -0.7 ][ 0.6, 0.1]]) y = np.array([ 0.3 , 0.07, -0.2, 0.07, 0.03 ]) theta = np.zeros(2) NewtonMethod(x,y,theta,100) Need help / suggestions to fix this problem.
