[site]: datascience
[post_id]: 112721
[parent_id]: 112637
[tags]: 
For binary classification tasks, you do not require to use ReLU at the prediction node as you have said, the output from the ReLU activation doesn't match to your need. You may want to consider ReLU, Sigmoid, Tanh for the hidden layer(s) and Sigmoid or Softmax for the output node for binary classification. If you study the modern deep learning model architectures, you'll find ReLU often in the hidden layers but not in the prediction layer. Further, you may find this comprehensive blog useful.
