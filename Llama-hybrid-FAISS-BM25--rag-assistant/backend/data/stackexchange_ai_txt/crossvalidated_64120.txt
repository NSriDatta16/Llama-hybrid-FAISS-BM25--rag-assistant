[site]: crossvalidated
[post_id]: 64120
[parent_id]: 64117
[tags]: 
Short answer: an HMM is a generative, probabilistical model and a neural network on the contrary, is a deterministic, discriminative model. Longer one: Basically, a HMM is a generative, probabilistical model where you try to model the process generating the training sequences, or more precisely, the distribution over the sequences of observations. For the sake of clarity, let me refer to the case of text tagging . The goal is to learn to generate text: you give in one or more words, and the system should be able to generate a text on his own. In order to do that, one attempts to learn the grammatical structure of sentences (a verb comes after a noun) and how likely are given words to follow one another, given their lexical category . The corresponding graphical model for an HMM is the following: In the graph, $y(t)$ is the observation at time (or position) $t$. In our case, the word $y$ at position $t$. The hidden state, $x$, corresponds to its lexical category. It assumes that the observations (for example words) are generated by some hidden state. In addition, it assumes that those hidden states follow the markov property: the current state depends only on the previous state. I.e. whether the next word if a verb or an adjective only depends on the current lexical category. The way this model generates text is the following: given the current word and category, it first samples the probability distribution $P(x(t)|x(t-1))$. Given the resulting $x$, it samples the probability $P(y(t)|x(t))$. A neural network (specifically a multilayer perceptron) on the contrary, is a deterministic, discriminative model. A neural network leans a mapping from the input space into the output space. Once the network has been trained, i.e. the values of all weights and thresholds have been determined, the response of the network to every input is deterministic, i.e. for each input it always gives the same answer. You can learn to generate text with a neural network, but there is no explicit modelling of a grammatical structure (it may learn some on its own, but that is hidden from you). You would train your network, and then feed the network with the first words. But after that what comes next is fixed, it is always the same. With HMM even when you feed in the same words, the output text will likely vary more or less. Notice that for other models of stochastic neural networks is completely different.
