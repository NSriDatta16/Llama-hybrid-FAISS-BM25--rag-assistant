[site]: crossvalidated
[post_id]: 628542
[parent_id]: 
[tags]: 
Help with multimodal (hydra) CNN architecture

I am trying to learn hydra like CNN architecture based on publication using AFLW dataset. I replicated results from repository and its fine. Moving forward, I added one another head/regression task into this architecture with my own additional annotations. Dataloader is fine and annotations are correct, however I'm getting poor results when training the network and later getting inference visualization. What I did I changed default backbone which is alexnet in this case into something more complex ( as is in the publication ). loss curves seems to be decreasing, however are on different levels. I tried to weight the weights such as: loss = (a*loss_task_1 + b*loss_task_2 + c*loss_task_X) It only helps a little to position losses onto similar level, but when I set a,b,c values to hight I I'm getting NaN values.- Inference visualization is still bad. Again, dataloader, model, connections, I checked everything many times. I dont know where and what to look for. Could you share some tricks how can I approach it ? Some strategies of how to tackle multimodal learning issues ?
