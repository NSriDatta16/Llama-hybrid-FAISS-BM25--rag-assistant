[site]: stackoverflow
[post_id]: 5257614
[parent_id]: 5254870
[tags]: 
That is the problem with detailed test plan up front. You trying to guess what kind of errors, how many, and in what areas you will get. This may be tricky. Maybe you should have overall Master Test Plan specifying only test strategy, main tool set, risks, relative amount of how much testing you want to put in given areas (based on risk). Then when you starting to work on given functionality or iteration (I hope you are doing this in iterations not waterfall), you prepare detailed test plan for this set of work. You adjust your tools/estimates/test coverage based on experiences from previous parts. This way you can say at the beginning what is your general approach and priorities, but you let yourself adapt later as project progresses. Question about how much testing you need to put into testing COTS is the same as with any software: you need to evaluate the risk. If your software need to be Validated because of external regulations ( FDA , DoD ..) you will need to go deep with your tests, almost test entire app. One problem here may be ensuring external regulator, that tools you used for validation are validated (and that is a troublesome). If your application is mission-critical for your company, than you still need to do a lot of testing based on extensive risk analysis. If your application is not concerned with all above, you can go with lighter testing. Probably you can skip functionality that was tested by platform manufacturer, and focus on your customisations. On the other hand I would still write tests (at least happy paths ) for workflows you will be using in your business processes.
