[site]: crossvalidated
[post_id]: 267943
[parent_id]: 267925
[tags]: 
I'll describe two broad approaches: the Bayesian approach (where prior knowledge enters in the form of the prior distribution), and the optimization approach (where prior knowledge enters in the form of constraints and/or penalties). The regression methods you mentioned can indeed be cast in a Bayesian framework. Lasso and ridge regression correspond to the model: $$p(y \mid x, \beta, \sigma^2) = \mathcal{N}(x \beta, \sigma^2)$$ Each of these methods is equivalent to performing MAP estimation. Lasso places a zero-mean Laplacian prior on the weights, and ridge regression uses a zero-mean Gaussian prior. The width of the prior is controlled by the regularization parameter. The loss function you wrote would be equivalent to using a Gaussian prior with mean $\hat{\beta}$. A Bayesian approach is one example of the kind of 'universal' approach you're looking for. Do MAP estimation or go full Bayes and find the expected value of the posterior. Choose the prior based on the knowledge/assumptions you want to impose. For example, if you want to impose nonnegativity constraints, you can use a prior that's zero over negative weights. You can choose a different likelihood function if you don't think the errors are i.i.d. Gaussian, etc. Another 'universal' approach is to add penalty terms to the loss function or constraints to the optimization problem. For example, you can impose nonnegativity constraints as @Mortezaaa pointed out: $$\min_\beta \|y - x \beta\|^2 \quad \text{s.t. } \beta_i \ge 0 \enspace \forall i$$ It's also possible to encode more complicated kinds of assumptions. For example, the following problem imposes a smoothness penalty on the weights: $$\min_\beta \|y - x \beta\|^2 + \lambda \sum_{i=2}^d (\beta_i - \beta_{i-1})^2$$ Increasing the penalty parameter $\lambda$ forces neighboring weights to be more similar to each other, increasing the smoothness. You can choose the form of the the penalty/constraints to impose all kinds of structure. Many constraint forms have equivalent penalty/Lagrangian forms (and vice versa), and many of these have Bayesian equivalents (as in the case of lasso/ridge regression). When taking this approach, it's important to be mindful that some loss functions and constraints can result in a trickier (or even intractable) problem. In these cases, it may only be possible to obtain an approximate or locally optimal solution. Some problems also require more specialized optimization algorithms. For example, ridge regression can be solved using simple convex optimization techniques, but lasso requires a more specialized solver.
