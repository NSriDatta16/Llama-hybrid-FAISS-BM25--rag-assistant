[site]: crossvalidated
[post_id]: 526195
[parent_id]: 524624
[tags]: 
Have also been interested in this question in the past. Below are some thoughts. Assuming that the training data contain sufficiently many non-anomaly data points means the auto-encoder will compress the inputs (the encoder network) so as to “capture” the essence of the data in the topology and weightings of the encoder network. This in theory removes some of the noise, and keeps the signal for the decoder network to reconstruct. Advantages of autoencoders for anomaly-detection: The input and output size are equal allowing the reconstructed output to be directly compare with the input, this would be helpful for a human if the data were say images or sound as they can view or hear outputs flagged as an anomaly. Thus I would say for some data inputs such as images the model is an open model and not a closed model, that is it is easier for a human to interpret than some other methods. Because they are based on neural networks, presumably they are aligned to the advantages of neural networks. Hopefully someone can comment on this or provide a link about the benefits this may bring (for example what types of data this would be helpful for). The “code”, that is the output of the encoder and input of the decoder, if small may give an indication of the strength of the signal captured by the autoencoder, thus presumably a relatively small code would mean the autoencoder would be better at discerning anomaly data points. Thus just by looking at the code you can get an idea of how good the autoencoder might be for anomaly detection. Again, happy to have comments on the validity or otherwise of this thinking.
