[site]: datascience
[post_id]: 60904
[parent_id]: 
[tags]: 
My models performs better with the arbitrary random feature. How can I interpret this?

I am training 6 different classifiers 'Decision Tree', 'Random Forest', 'Logistic regression' and 'SVM' with different kernels. There are about 80 dependent variables including categorical and numerical variables. For my experiment, I added a 'random' column which is generated by any arbitrary random numbers, but all the model performs better on both validation set and test set. Is there any good explanation about this phenomenon?
