[site]: crossvalidated
[post_id]: 450990
[parent_id]: 
[tags]: 
How is the Cost Function of a neural network defined?

I was reading Michael Nielson's book on Neural Networks in which he writes that: The first assumption we need is that the cost function can be written as an average $C=\sum_x{C_x}$ over cost functions $C_x$ for individual training examples, $x$ . I am unable to understand this assumption. Do any functions exist for which the average over each training example won't exist? I am unable to grasp how this condition is supposed to be assumed since any function can be average over the training examples.
