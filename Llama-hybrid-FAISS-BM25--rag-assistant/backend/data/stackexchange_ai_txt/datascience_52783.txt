[site]: datascience
[post_id]: 52783
[parent_id]: 
[tags]: 
If a neural network is a universal function approximator, is the irreducible error 0?

How to marry the fact that (most) neural networks with a single hidden layer are universal function approximators with the fact that in the bias-variance decomposition we consider there to be an irreducible error? Can the irreducible error be 0? (Then it would be inappropriate named.)
