[site]: stackoverflow
[post_id]: 3340795
[parent_id]: 3315604
[tags]: 
Your suggested hardware optimization would not reduce the latency. Consider the operations at the lowest level: The old value at the location is loaded from the cache to the CPU (assuming it is already in the cache). The old and new values are compared. If the old and new values are different, the new value is written to the cache. Otherwise it is ignored. Step 1 may actually take longer time than steps 2 and 3. It is because steps 2 and 3 cannot start until the old value from step 1 has been brought into the CPU. The situation would be the same if it was implemented in software. Consider if we simply write the new values to the cache, without checking the old value. It is actually faster than the three-step process mentioned above, for two reasons. Firstly, there is no need to wait for the old value. Secondly, the CPU can simply schedule the write operation in an output buffer. The output buffer can perform the cache write simutaneously while the ALU can start working on something else. So far, the only latencies involved are that of between the CPU and the cache, not between the cache and the main memory. The situation is more complicated in modern-day microprocessors, because their cache is organized into cache-lines. When a byte value is written to a cache-line, the complete cache-line has to be loaded because the other part of the cache-line that is not rewritten has to keep its old values. http://blogs.amd.com/developer/tag/sse4a/ Read Cache hit: Data is read from the cache line to the target register Cache miss: Data is moved from memory to the cache, and read into the target register Write Cache hit: Data is moved from the register to the cache line Cache miss: The cache line is fetched into the cache, and the data from the register is moved to the cache line
