[site]: crossvalidated
[post_id]: 349062
[parent_id]: 349024
[tags]: 
$\newcommand{\cov}{\text{Cov}}$$\newcommand{\var}{\text{Var}}$$\newcommand{\E}{\text{E}}$This is quite slippery to pin down precisely; I'm going to outline my main thoughts but counterpoints are welcome. By the linearity of expectation $$ \cov(\alpha Y_{t-1} + t, Y_{t-1}) = \alpha \var(Y_{t-1}) + \cov(t, Y_{t-1}) $$ so the issue comes down to $\cov(t, Y_{t-1})$. Since this question ultimately is about what is and isn't random, I think we'll need probability spaces so I'll let $(\Omega, \mathscr F, P)$ be my probability space. I'll let $T : \Omega \to \mathbb N$ be the index when thought of as a random variable. I can think of two guiding questions to help determine how the index behaves: as a function of $\omega$ does the index change? I.e. if we sample $\omega \in \Omega$ are we handed a particular $Y_{T(\omega)}(\omega)$, or do we instead still have the collection $\{Y_t(\omega) : t \in \mathcal T\}$? does the index survive the covariance, or do we average over it? A covariance boils down to expectations which are averages over $\Omega$. If $t$ survives this averaging, i.e. we treat it as fixed/deterministic, that's no different than conditioning on $T=t$, and in this case it will be independent of the random variable in question. First, a more concrete example. Suppose $\Omega$ is the set of all people and take $X_1(\omega)$ to be $\omega$'s height, and $X_2(\omega)$ to be $\omega$'s age. Thus we have a collection $\{X_t : t\in\{1,2\}\}$. If the index depends on $\omega$ then this would mean that, depending on the person, we are told either their height or their age. If the index does not, then we just have $(X_1(\omega), X_2(\omega))$ for each person. So I could do something like $$ T(\omega) = \begin{cases}1 & \omega \text{ wears glasses} \\ 2 & \text{o.w.}\end{cases} $$ and then, when presented with a $\omega$, I return $X_{T(\omega)}(\omega)$, so I give the heights of the bespectacled people and the ages of those without glasses. Then when considering something like $$ \E (X_T)=\int_{\Omega} X_{T(\omega)}(\omega)\,\text d P(\omega) $$ I'm averaging this new random variable $X_T$ over all people, which is very different from averaging the heights or ages as in $\E X_1$ and $\E X_2$. Probably for this kind of thing I don't want the index to be random, as I'm interested in the sequence $(X_1(\omega), X_2(\omega))$ for each person. Thus the index will not be a function of $\omega$ and so $\cov(X_j, j) = 0$. Next, a more mathy example. Let's say we have a collection of random variables $\{X_j : j \in \mathbb N\}$ and $X_j \sim \mathcal N(j, 1)$. Furthermore let $J \sim \text{Pois}(\lambda)$. What is $\cov(X_j, j)$? Certainly as $j$ increases so does $\E X_j$ (as this just equals $j$), and similarly $j$ decreasing lowers $\E X_j$. But for a particular $j$, $j$ is just a number so the $\omega$ we observe doesn't change it and therefore $j$ survives the expectation in that $\E X_j = j$. This makes $\cov(X_j, j) = 0$. But instead if we allow the index to vary depending upon which sample space point $\omega$ we are considering, then we end up with something like $X_J$ where $\omega$ both determines the actual value of $X_j$ and also which $j \in \mathbb N$ we end up with. Now, as before, $$ \E (X_J) = \int_\Omega X_{J(\omega)}(\omega) \,\text dP(\omega) $$ so $\E X_J$ is just a number and doesn't depend on the index anymore. We've averaged over all the indices. So for your particular case, I think is more likely you don't want to average over the indices, but both are "legal" things to do. In many cases we've got a collection of RVs $\{Y_t : t \in \mathcal T\}$ and we're interested in the random vector/sequence corresponding to realizing all of them for a given $\omega$, while there are other situations where we have a pool of RVs and the one we get depends on $\omega$. In a time series setting I'm inclined to say you probably don't want to average over the indices, because you probably want to think about each point in time as a thing you look at, rather than rolling a die to decide which time point you see. Regarding the issue of the sample covariance between $(Y_1, Y_2,\dots)$ and $(1,2,\dots)$ I don't think this is doing the same thing. Everything up above is about the population covariance, where heree Finally, I think all of this is different from something like the sample covariance between realizations $(y_1,\dots,y_{10})$ and $(1,\dots,10)$ as these are not draws from the same distribution so this being non-zero doesn't say anything about a particular population covariance $\cov(Y_j,j)$ being zero or not. In my $\{X_j\}$ situation imagine in every case $x_j \approx j$, i.e. each draw was near its mean. Then certainly $\vec x$ and $\vec j$ will be highly correlated, but this is very different from the population correlation between one particular $X_j$ and its index $j$.
