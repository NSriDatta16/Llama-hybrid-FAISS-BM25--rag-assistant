[site]: datascience
[post_id]: 18152
[parent_id]: 
[tags]: 
Semi-gradient TD(0) Choosing an Action

I am trying to write an optimal control agent for a simple game that looks like this: The agent can only move along the x-axis, and has three actions available to it: left, right, and do nothing. A random number of falling rocks are spawned at arbitrary positions along the top row. The goal is to survive as long as possible by avoiding collision on each time step. Here's what I've done so far: 1) I use a feature vector $Φ$ with $Φ_0(s)$ being the current x-coordinate of the agent and $Φ_1(s)...Φ_n(s)$ taking on a 0 or 1 (1 indicating the presence of a rock). 2) The corresponding weight vector $θ$ is initialized to 0 for all weights. So I have the linear function approximation $$\hat v(s,θ)=\sum_{i=0}^nΦ_i(s)θ_i$$ 3) The reward on each time step is 1, and 0 upon collision. I'm actually trying to implement the algorithm below from Sutton and Barto's 2017 draft . Semi-gradient TD(0) for estimating $\hat{v} \approx v_{\pi}$ Input: the policy ${\pi}$ to be evaluated Input: a differentiable function $\hat{v} : \mathbf{S^+} \times \mathbb{R}^n \rightarrow \mathbb{R}$ such that $\hat{v}(terminal,·) = 0$ Initialize value-function weights $\theta$ arbitrarily (e.g., $\theta = 0$) Repeat (for each episode): $\qquad$Initialize $S$ $\qquad$Repeat (for each step of episode): $\qquad\qquad$Choose $A \sim \pi(·|S)\qquad$ # Not sure how to choose action here $\qquad\qquad$Take action $A$, observe $R, S'$ $\qquad\qquad\theta \leftarrow \theta + \alpha[R + \gamma \hat{v}(S',\theta) − \hat{v}(S,\theta)]\nabla\hat{v}(S,\theta)$ $\qquad\qquad S \leftarrow S'$ $\qquad$until $S'$ is terminal My problem is this: 1) Is this even an appropriate algorithm to apply to this sort of task? It feels like a policy gradient approach would be more suitable. 2) If yes to 1), how do I choose an action? In the algorithm above, this appears as "Choose $A \sim \pi(·|S)$" Since the policy is implicit, I compute the approximate value of the state as shown in 2) above and then greedify over the three actions (no ε-greedy, though), but the only thing that changes is $Φ_0(s)$. I'm definitely missing something.
