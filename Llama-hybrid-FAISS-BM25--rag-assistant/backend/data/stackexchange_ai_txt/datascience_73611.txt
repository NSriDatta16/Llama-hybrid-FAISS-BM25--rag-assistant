[site]: datascience
[post_id]: 73611
[parent_id]: 32200
[tags]: 
You should tune the meta-estimator using whatever data you want it to eventually predict with. This should definitely include the base model predictions (else you aren't actually ensembling), and may or may not include (some of) the original features. One important note though: you should not be training the meta-estimator using "predictions" of the base models on their own training data ; those are more accurately called estimations rather than predictions, because the base models already had access to the truth. A common approach is to train the meta-estimator on out-of-fold predictions from a cross-validation training of the base models. If the base models are quite good, then it's reasonable that the xgboost model might only use one tree; it just has to tweak the already-good predictions from the base models. But, consider dropping the learning rate or otherwise increasing regularization, to see if more trees can perform better.
