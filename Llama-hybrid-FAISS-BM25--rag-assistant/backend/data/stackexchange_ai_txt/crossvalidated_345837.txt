[site]: crossvalidated
[post_id]: 345837
[parent_id]: 345583
[tags]: 
You should be familiar with fully connected neural networks, where the weights between a layer of $N$ input nodes and $M$ hidden nodes are stored in a $N$ by $M$ matrix. With convolutional neural networks, the weights are stored in a $W$ by $H$ by $C$ by $D$ tensor (4d matrix) where $W$ is the width of the convolution window, $H$ is its height, and $C$ is its depth. The first 3 dimensions ($W$, $H$, $C$) are all input dimensions. So just imagine them as a very fancy $N$. The first hidden layer, (layer of filters), also has 3 dimensions, lets call them $W_2$, $H_2$ and $D$. $W_2$ and $H_2$ are calculated based off $W$ and $H$, so you cannot choose them, but you can choose $D$ which will be the depth of the first hidden layer of filters. So think of $W_2$, $H_2$ and $D$ as a fancy $M$. In your example, to get from 4 feature maps to 6 feature maps, $C=4$ and $D=6$.
