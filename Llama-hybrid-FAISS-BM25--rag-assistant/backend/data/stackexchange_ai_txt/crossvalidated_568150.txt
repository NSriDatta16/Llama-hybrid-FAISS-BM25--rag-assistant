[site]: crossvalidated
[post_id]: 568150
[parent_id]: 568137
[tags]: 
You used the tag cart ; CART trees always use binary splits. However, the Quinlan family of decision tree algorithms support multiple-arity splits for categorical features, and such a stump could be used as the base learner for AdaBoost. However, that doesn't say anything about multiclass targets . All trees support multiclass problems, by using a version of impurity measures that take into consideration multiple classes. Each node contains a subset of your training set, and we record the number in each class. The final class prediction is still just the majority class in a leaf, so a CART stump will predict at most 2 classes (but across the AdaBoost ensemble, probably all of the classes will eventually show up). For example, a stump for the iris dataset (using just two features to keep it from being too good a split to be instructive): The left leaf contains a large majority from the first class, so a row with first feature The trees can also produce "soft" predictions, the proportion of samples in a leaf for each class; this is done for random forests, but not in AdaBoost to my knowledge, since the reweighting uses information about the misclassified examples. (In the example above, the right leaf would predict probabilities of 5/98, 44/98, and 49/98.) All that said, you could produce multiple binary models in either a one-vs-one or one-vs-rest fashion if desired instead of relying on the trees' internal multiclass approach.
