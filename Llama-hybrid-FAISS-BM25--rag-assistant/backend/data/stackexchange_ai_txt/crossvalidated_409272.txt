[site]: crossvalidated
[post_id]: 409272
[parent_id]: 409054
[tags]: 
How do Bayesians verify that their methods define uncertainty properly (i.e., calculate valid credible intervals and posterior distributions) using Monte Carlo simulation methods, if probability is not defined as rates in the long run? I believe the confusion here is about the purpose of simulation methods in Bayesian statistics. The only purpose of Markov Chain Monte Carlo methods such as Gibbs Sampling or Hamiltonian Monte Carlo is to calculate the denominator of Bayes rule. Of course, there are often other methods available which would make MCMC needless. Some models can be expressed using conjugacy, others through applying a fine grid over the parameter space, yet others can be solved with acceptance-rejection testing. Where MCMC comes in handy is when the integral is ill-behaved. While I would love to avoid math, that really cannot be avoided. In looking at Bayes rule $$\pi(\theta|x)=\frac{f(X|\theta)\pi(\theta)}{\int_{\theta\in\Theta}f(X|\theta)\pi(\theta)\mathrm{d}\theta},$$ the numerator is made up of $f(X|\theta)$ and $\pi(\theta)$ . $f(X|\theta)$ is a likelihood and not a probability, so it doesn’t sum to one except by chance. The denominator assures us that $\pi(\theta|X)$ sums to one. The goal of MCMC is to determine the bottom number. Note that the bottom number is a constant. It is the expectated likelihood. The accuracy of that number does determine some but not all parameter estimates. If you were using the maximum a posteriori estimator, then MCMC is an unnecessary step. You should build a hill climbing algorithm instead. On the other hand, it is necessary to determine the posterior mean or an interval. That is because the 95% interval has to be 95% of something and the denominator determines what the scale of that something is. The goal of MCMC in Bayesian methodologies is to get the Markov chains to converge to the posterior density. That is it. It doesn’t test the validity of anything. It is just an attempt to determine a fixed point value. It is a form of numerical integration. As there is no way to know without letting the algorithm to run to infinity whether all dense regions have been covered, there is some human judgment. The algorithm will have a cutoff when it believes it is done, but that does not mean it is actually done. In Frequentist methodologies, MCMC is often used to test the reasonableness of a model or to numerically approximate a solution when an analytic one is not available. It serves no similar purpose here. If I were to write a custom model in Stan, how would I know that what I am doing is legit? How could I use simulation methods to verify that what I'm doing in Stan is actually going to tell me what I want to know? This question is far more difficult. Stan is a fast algorithm, which means it trades speed for an added risk of inaccuracy. Stan, by construction, will more often be correct than incorrect. There are other algorithms that are designed to search the parameter space widely for local maximums which may be more accurate, but which will be very slow. What you should do, before using a particular algorithm, is read the literature on that algorithm and look at its functional limitations. Unfortunately, that is usually mathematical work as the only real goal of any non-conjugate method is to estimate $$\int_{\theta\in\Theta}f(X|\theta)\pi(\theta)\mathrm{d}\theta.$$ The second thing you can do is to validate it with an alternative algorithm. The numbers will never match, but if you deem them close enough, then you are fine. Third, most of the prebuilt packages provide warnings that something may be amiss. If a warning comes up, use something else after investigating the source of the problem, so you do not recreate it in another algorithm. Fourth, look at your prior density. Imagine you had a prior density of $\Pr(\mu)=\mathcal{N}(7,2^2)$ with $\sigma^2$ known just to simplify it and a likelihood of $\mathcal{N}(25,.1^2)$ . At the least, you should be going wow, either I was wrong, the sample was bad, or there is something else going on that I should investigate. Fifth, and you should do this before you start Stan in the first place, graph out your marginal likelihoods in one or two dimensions. Are there surprises anywhere that may interfere with the algorithm? Since Bayesians don't define probability as what we see in the long run, how can I use simulation methods to verify than stan_glm is accurately capturing uncertainty? That is, how could I trust that these credible intervals are valid, using simulation methods? And right now, I'm not even defining a prior—how does the inclusion of priors come into play here, since that will affect our measures of uncertainty? If you do not define a prior, then your model is not valid. If you are not defining a reasonable prior density, then why would you use a Bayesian model? Frequentist models minimize the risk of the maximum loss that could happen from gathering a bad sample. They are very pessimistic and it often takes more information to produce the same result a Bayesian method would. Nonetheless, that is of no use without using a good prior density. The prior density allows the Bayesian method to minimize the average loss from choosing a bad sample. The information in the prior acts as a weighting scheme so that if some extreme sample is chosen by unfortunate chance, the prior weakens the role that the data plays. EDIT I realized I didn't provide one specific answer. It was to the question How could I use simulation methods to verify that what I'm doing in Stan is actually going to tell me what I want to know? What makes this question challenging is that in the Bayesian paradigm the fixed points are , $X$ , the sample. In Frequentist methods, the parameters are fixed and thousands of unseen samples are created. On the Bayesian side of the coin, it is the sample which is fixed. You need to simulate thousands of parallel universes. To see what that may be like, imagine all density functions of a coin toss with an unknown probability $p$ of being heads and $1-p$ of being tails. You observe six heads and two tails. Imagine a small parameter space where $p\in\{1/3,1/2,2/3\}$ . Your simulation would consider all the cases where six heads could be obtained over the three objective binomial distributions. The posterior would be the weighted average of each parameter being the true value. Your predictive distribution would be the sum of the weighted binomial distributions. Of importance to you, it is impossible for the Bayesian prediction to ever be the true distribution. One of the three distributions is the true distribution. The Bayesian methods weight their probability based on the observed value and the prior. The posterior can never be the true distribution, nor the predictive density. It is asking "what is the probability of seeing six heads and two tails over the set of all possible explanations (parameters, models, etc)." The Frequentist would assert one of the three choices was the true value by making it the null. Six heads and two tails would falsify $H_0:p=1/3,$ but not the others. If, by chance, you chose the correct one of the three distributions, then you are perfectly correct. Otherwise, you will be wrong. If you would use simulations to hold a sample fixed, you would find that Stan would perform admirably as Bayes theorem is a mathematical theorem. It is ex-post optimal. All you would find is that the algorithm correctly implemented Bayes theorem up to the natural error level in estimating the denominator. There are three things you can do. First, you can use model scoring methods for out-of-sample data. Second, you can use a Bayesian model selection or model averaging process. Third, you can treat it as a Frequentist problem and construct the sampling distribution of estimators. For the first, scoring methods are an entire literature unto itself. You should research them. Bayesian model selection and model averaging treat models as parameters. For model selection, the probability of the models being true is calculated. For model averaging the probability each model is true is calculated and that serves as weighting over the model space. Finally, you can treat it as a Frequentist model. The last one will be a problem in many standard cases because of the prior. For models with three or more dimensions and a normal distribution, the posterior density will not integrate to unity if the prior density isn't a proper density. In other words, you have to bite the bullet and choose a prior for any model with any real complexity. The presence of a correctly centered proper prior forces the case where the Bayesian method will be superior to the corresponding Frequentist method due to the improved information. The Bayesian method will win under any reasonable standard. That isn't due to a flaw in the Frequentist method, but the Bayesian method assumes exterior information. The Frequentist method, by only considering the information in the sample, will have less information in it if you have a real prior. Again, if you do not have a real prior, then why are you using a Bayesian method?
