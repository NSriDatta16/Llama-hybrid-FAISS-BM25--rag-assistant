[site]: crossvalidated
[post_id]: 87162
[parent_id]: 87039
[tags]: 
I don't know much, if anything, about the DeGroot model, but using an analogy to Markov chains, you get the long-run convergence through the matrix exponential. So there's two issues. 1) The existence of the eigenvector for an eigenvalue of 1 is just a necessary condition that the limit exists. 2) You shouldn't round off your probabilities. The first row doesn't sum to 1 (or close enough in floating point) and this matters. [~/] [1]: limT = np.array([(1/3.,1/3.,1/3.),(0.5,0.5,0.0),(0.0,0.25,0.75)]) [~/] [2]: np.linalg.matrix_power(limT, 10) [2]: array([[ 0.27272727, 0.36363636, 0.36363636], [ 0.27331321, 0.36383168, 0.36285511], [ 0.27214134, 0.36344105, 0.36441761]]) [~/] [3]: np.linalg.matrix_power(limT, 25) [3]: array([[ 0.27272727, 0.36363636, 0.36363636], [ 0.27272729, 0.36363637, 0.36363634], [ 0.27272725, 0.36363636, 0.36363639]]) [~/] [4]: np.linalg.matrix_power(limT, 50) [4]: array([[ 0.27272727, 0.36363636, 0.36363636], [ 0.27272727, 0.36363636, 0.36363636], [ 0.27272727, 0.36363636, 0.36363636]]) If you want the unique stationary distribution vector for a Markov chain, you can use the eigenvector associated with the unit eigenvalue. Just looking at the wiki for DeGroot Learning, I think this is analogous to consensus beliefs. [~/] [5]: from scipy import linalg [~/] [6]: eig, vl, vr = linalg.eig(limT, left=True) [~/] [7]: idx = np.argmin(np.abs(1 - eig)) [~/] [8]: s = vl[:,idx] [~/] [9]: s /= s.sum() [~/] [10]: s [10]: array([ 0.27272727, 0.36363636, 0.36363636]) [~/] [11]: np.dot(s, limT) [11]: array([ 0.27272727, 0.36363636, 0.36363636])
