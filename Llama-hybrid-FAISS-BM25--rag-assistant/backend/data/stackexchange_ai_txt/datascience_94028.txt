[site]: datascience
[post_id]: 94028
[parent_id]: 
[tags]: 
Multiple solutions with same minima in MLP with same weights

I came across an excercise on deep learning from here . It goes as follows: Consider a simple MLP with a single hidden layer of $d$ dimensions in the hidden layer and a single output. Show that for any local minimum there are at least $d!$ equivalent solutions that behave identically. As the network is a MLP with one hidden layer, the equation would be: $O = W^{(2)}(W^{(1)}x + b_1) + b_2$ Assuming I am correct, where do I need to go from here to get to the solution?
