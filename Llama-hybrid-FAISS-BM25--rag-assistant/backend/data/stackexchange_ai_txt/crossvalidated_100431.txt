[site]: crossvalidated
[post_id]: 100431
[parent_id]: 99828
[tags]: 
Stanford Professor Andrew Ng gave some guidelines for selecting a neural network architecture in his Machine Learning class on Coursera . I don't see the specific lecture videos on YouTube, but the course is free so it's no cost to access them on Coursera's site. Here's a summary of the relevant material. In lecture 9-7 Putting it all together , general guidelines are given on picking default values for your neural network architecture. Number of input units: Dimension of features x(i) Number of output units: Number of classes Reasonable default is one hidden layer, or if > 1 hidden layer, have the same number of hidden units in every layer (usually the more the better, anywhere from about 1X to 4X the number of input units). In lecture 10-7 Deciding what to do next revisited , Professor Ng goes in to more detail. Small neural networks: fewer parameters more prone to underfitting computationally cheaper Large neural networks: more parameters more prone to overfitting computationally more expensive use regularization to address overfitting Number of hidden layers : Split your data into training, cross validation, and test sets, and train neural networks with 1, 2, and 3 hidden layers, then see which one has the lowest cross validation error to choose an architecture.
