[site]: crossvalidated
[post_id]: 364508
[parent_id]: 298829
[tags]: 
I share your confusion about vanilla logistic regression. That is, when you are working with an unpenalised likelihood/objective function. This is where the iterations are not really about model selection, but rather about finding the maximum of a non-linear function. However....having said this...if you think of the starting point for the algorithm, which is often intercept (or bias) equal to log odds for the whole dataset, and everything else equal to zero. This could be a "simple" model, and you can think of your actual model as the "complex" model. As you do the iterations, the parameters move from the "simple" to the "complex" model. We can then imagine finding the MLE for a logistic model with "too many" predictors. The idea is that the iterations always start from a model with "too few" predictors. The "hand wavy" argument goes that somewhere in the middle iterations might be a nice "good fit". This is obviously dependent on how the iterations of the algorithm are done, such as how quickly it converges, and how much parameters are allowed to vary at each iteration. Hope this helps!
