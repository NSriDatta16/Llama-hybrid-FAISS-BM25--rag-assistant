[site]: crossvalidated
[post_id]: 304174
[parent_id]: 304113
[tags]: 
No, this is not bothersome! The short story: you have explicitly constructed a model that with very high probability will be a bad fit to data generated from the model, so the high spike close to zero in $P_0$ is artificial and not representative of the data. The way this works is that with high probability the very high spike will not contribute any observations, but the spike will dominate contributions to the log likelihood function so that the spike in the estimated distribution will be placed above the minimum value of the data. That will with high probability be the wrong place. I did some calculations to illustrate this, I will not give all details. I started with choosing $q=0.99$ , $N=15$ (but thinking of simulating a sample with $n=10$ , say). That gives $\gamma=0.9994$ and then distributing the missing probability equally gives $\alpha=\beta=0.0003$ . Then securing a dominating loglikelihood contribution from the spike, $\delta=10^{-13}$ will do. That will give an insanely high spike, in a region with no data. In short, as an approximate representation of the data (with $n=10$ observations) this model is nonsense. A more formal analysis is difficult, but hardly necessary. One reason is that computing the Fisher information for this model is "difficult": the derivative of the loglikelihood (with respect to the parameter $\theta$ ) is mostly zero, with delta function spikes at the borders between the three intervals defining the model. Maybe I come back with some simulations. If you replaced the three uniform parts of the model with three gaussian parts, you would get similar results, but a model easier to analyze. This is a general phenomenon, it is always possible to construct "models" which will misbehave. So in addition to inference principles like maximum likelihood, we need principles for helping model construction, to get sensible models. Tukey said that models should be "bland" or "hornfree", meaning that the model itself should not import information (or too much of it) into the analysis. The information in formal statistical analysis always come from two sources: the data, and the model. Bayesians are not alone in importing non-sample information into their models! A book-length treatment is this . It is the statisticians responsibility to make models which do not import too much information into the analysis. Your model decidedly has a very big horn!
