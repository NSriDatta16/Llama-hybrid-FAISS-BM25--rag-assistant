[site]: datascience
[post_id]: 31300
[parent_id]: 
[tags]: 
Need lesser memory consumption than doing sklearn.decomposition.TruncatedSVD on 99999 variables

Need lesser memory consumption than doing sklearn.decomposition.TruncatedSVD on 99999 variables. So I know that I can use TruncatedSVD to do the dimensionality reduction. However, my problem is about choosing the correct amount of n_components . One approach suggested doing TruncatedSVD to the maximum amount of variables first and then calculate on this, how many variables on needs to reach a "explained variance" goal. But doing the initial TSVD on 99999 variables seems to 1) take a long time (it hasn't finished it) and it 2) slowly seems to occupy all my memory (which I have 120GB).
