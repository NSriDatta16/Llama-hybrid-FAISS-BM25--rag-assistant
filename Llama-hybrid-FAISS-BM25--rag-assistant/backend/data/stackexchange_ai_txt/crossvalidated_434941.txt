[site]: crossvalidated
[post_id]: 434941
[parent_id]: 434224
[tags]: 
Yes, both approaches can be seen as doing the same as they are used to learn a representation of an input. But they differ in how the learning is performed. You can consider representation learning part of self-supervised learning (SSL) as an encoding step. In addition to encoding, autoencoders have a decoder too. VAEs, the most popular encoder assumes representations are distributed according to a prior (e.g., Gaussian) and does (approximate) likelihood maximization. The loss you're trying to minimize is different from usual supervised loss, which is used in SSL, but with self-supervised signals (e.g., rotation etc.).
