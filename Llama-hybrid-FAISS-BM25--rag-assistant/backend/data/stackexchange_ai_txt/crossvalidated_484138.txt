[site]: crossvalidated
[post_id]: 484138
[parent_id]: 394083
[tags]: 
This is actually starting to change as recent work are showing the benefit of second order methods specially for NLP problems. Some examples are: " ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning " Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, Michael W. Mahoney " Second Order Optimization Made Practical " Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, Yoram Singer There has been an incorrect notion that second order methods are impractical because the cost of forming Hessian is $O(N^2)$ or that the cost of Hessian inverse is $O(N^3)$ . In fact, you never do any of these explicitly. In practice, randomized linear algebra methods and matrix free approaches are used which can provide good approximations to Hessian in $O(N)$ complexity. There are actually multiple libraries now for this which enables calculating Hessian spectrum of large models (even billion parameter models) in reasonable times. Below is our implementation: https://github.com/amirgholami/PyHessian Update: There was a question about how Newton's method can be used without explicitly forming the Hessian ( $O(N^2)$ cost) and explicitly applying its inverse (O(N^3)) cost. The answer is that you can use Conjugate Gradient to compute the Newton step. Let's denote gradient with $g$ and Hessian with $H$ . In Newton's method the update to parameters $w$ is as follows: $w^{new} = w^{old} - H^{-1}g$ The solution to $H^{-1}g$ can be computed by solving: $H\Delta w = -g$ This is a system of linear equations that can be solved with Conjugate Gradient (for a PSD H). Thankfully, CG does not require $H$ to be explicitly formed to find $\Delta w=-H^{-1}g$ . It only requires the application of $H$ to a given vector which can be computed in $O(N)$ complexity. In theory, you will need r CG iterations to exactly solve the above equations (where r is the Hessian rank). However, in practice, a few iterations are often necessary unless machine precision accuracy is needed. This was elegantly explained in Perlmutter's 1994 paper (see section 5.2): " Fast Exact Multiplication by the Hessian " BA Perlmutter Also for a more in-depth analysis I suggest reading section 6 the following paper: " Optimization Methods for Large-Scale Machine Learning " Leon Bottou, Frank E. Curtis, Jorge Nocedal
