[site]: crossvalidated
[post_id]: 172353
[parent_id]: 172226
[tags]: 
Here's what I understand the situation is. You have one model, which you call your simulation, that you are confident generates a set of data that accurately represents what will actually happen in the epidemic. For some reason (presumably because it's expensive or slow to build and run, or there's theoretical interest in a simple equation that generates similar results to the complex model), you have an alternative model (the one you call a model) which can also generate a set of data, and you want to check if the version generated by this model is close to the version generated by the known-good model. I'm also presuming that each time either of the models generates data, it generates a similar and pretty regular trend to other times. Otherwise (for example, if there's a random "take off" moment where the series suddenly breaks) there's another big complication. First, the method of comparing parameters from an auto-fit ARIMA is a bad one (I'm guessing the reason that answer you linked to survived is that it is on Stack Overflow rather than Cross-Validated, where the statistical problems would have been picked up probably). The reason is that the same time series can get good fits with quite different combinations of auto-regressive and moving average values. There's no obvious way to look at the "similarity" of two different ARIMAs - ones that look very different may in fact be similar. As @IrishStat says in his answer to the second question you linked to, you could construct an F-test of a common set of parameters for both models, but that would require something quite a bit more complex than auto.arima() . And even then you might find that they don't have common parameters, but deliver similar predictions of the trend which is what you are actually interested in, rather than the details of the ARMA process that is generating some of the random noise around the trend. So what would I recommend instead? It sounds like you aren't worried about the small fluctuations but only the overall trend. I would compare a smoothed version of the trend of each dataset, and start by making a visual comparison. In the case you've got, this shows that they're definitely not the same time series; one of them hovers around 1478, the other around zero, and that's good enough for me. But if there were some ambiguity, I would probably sum the squares or absolute values of the difference between the two smoothed series and determine if that was close enough, for some arbitrarily chosen meaning of "close enough" which in the end will have to depend on your domain, and the costs of being wrong. Definitely I'd start with the graphic. If you want a more objective benchmark, I would try running both simulations multiple times and seeing how much difference (sum of squares or absolute differences) there is between different instances of the same simulation, and comparing that to the inter-simulation differences. If they're the same, that shows that you can't tell which model produced the simulation. If they're different, you still have to make a judgement call about how different is too much, but you'll have some numbers to help you. While fitting ARIMA models is a bad idea for identifying similarity in trends, it's a good way to let me generate some data, so pasted below is how I did that. I'm guessing something's wrong with the data - maybe you fit the ARIMA model to a transformed or differenced version of the data, in which case you might want to go the next step of quantifying the difference between the two trends. library(forecast) library(ggplot2) library(tidyr) library(dplyr) # generate some data good_model % gather(variable, value, -time) %>% mutate(value = as.numeric(value)) ggplot(combined, aes(x = time, colour = variable, y = value)) + geom_line(alpha = 0.5) + geom_smooth(se = FALSE, size = 2) + theme_minimal() Edit I blogged about this at http://ellisp.github.io/blog/2015/09/20/timeseries-differences , basically just exploring how you might use simulation brute force to determine if two models are similar. However, I reach the conclusion that you still need a (probably) subjective decision on a cost function - obviously you're two methods will be different, but how different are you prepared to put up with?
