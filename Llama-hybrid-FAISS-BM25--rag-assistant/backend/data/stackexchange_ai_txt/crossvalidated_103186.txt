[site]: crossvalidated
[post_id]: 103186
[parent_id]: 103175
[tags]: 
$Q$ is "wrong" distribution, while $P$ is the "right" distribution. The length of a code $i$ under the "wrong" coding is $-log(q_{i})$. So, the average message length under the "wrong" coding is: $-\sum p_{i} log(q_{i}) = H(P,Q)$ Under the "right" coding, the average message length is $H(P)$. Imagine that we are using entropy coding like arithmetic coding, but we have not estimated the probability distribution right. Then the average message length is a bit larger then the theoretical limit $H(P)$. The difference is Kullbackâ€“Leibler divergence.
