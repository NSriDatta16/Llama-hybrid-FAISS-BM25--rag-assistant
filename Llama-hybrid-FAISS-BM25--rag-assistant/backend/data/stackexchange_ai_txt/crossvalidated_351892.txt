[site]: crossvalidated
[post_id]: 351892
[parent_id]: 
[tags]: 
change hinge loss error function with cross-entropy

I'm trying to implement cross-entropy as an error function in RBF neural networks instead of hinge loss error function. I need to find cross-entropy error for each output neuron, like hinge loss error function as you can see in the formula below but when it comes to cross-entropy, it seems meaningless. because the formulation of cross-entropy returns 1 scaler. I look at some of the recent papers using cross-entropy and soft-max but can't find any relative information to my problem. Is there any way to finding cross-entropy value for each neuron of output?
