[site]: datascience
[post_id]: 27081
[parent_id]: 
[tags]: 
How to cross-validate a deep learning model for highly imbalanced datasets?

I am working with a multi-modality classification problem (with Keras ). I have 1000, 5000 and 10000 samples for three different classes. I would like to do a five fold cross-validation to select the best pre-trained deep learning model for deployment. I'm including class weights during model training to give more weightage for less-pronounced classes. For a given fold, I would be validating with 200, 1000 and 2000 samples from these three classes. Is accuracy a good prediction measure to be used in this case? Or do I have to measure the F1-score and Matthews correlation coefficients as well? Am I doing it right?
