[site]: datascience
[post_id]: 18907
[parent_id]: 
[tags]: 
What does images per second mean when benchmarking Deep Learning GPU?

I've been reviewing performance of several NVIDIA GPU's and I see that typically results are presented in terms of "images per second" that can be processed. Experiments are typically being performed on classical network architectures such as Alex Net or GoogLeNet. I'm wondering if a given number of images per second, say 15000, means that 15000 images can be processed by iteration or for fully learning the network with that amount of images?. I suppose that if I have 15000 images and want to calculate how fast will a given GPU train that network, I would have to multiply by certain values of my specific configuration (for example number of iterations). In case this is not true, is there a default configuration being used for this tests? Here an example of benchmark Deep Learning Inference on P40 GPUs ( mirror )
