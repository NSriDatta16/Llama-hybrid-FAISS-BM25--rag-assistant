[site]: crossvalidated
[post_id]: 12672
[parent_id]: 12641
[tags]: 
A "true" Bayesian would deal with model uncertainty by marginalising (integrating) over all plausble models. So for example in a linear ridge regression problem you would marginalise over the regression parameters (which would have a Gaussian posterior, so it could be done analytically), but then marginalise over the hyper-paremeters (noise level and regularisation parameter) via e.g. MCMC methods. A "lesser" Bayesian solution would be to marginalise over the model parameters, but to optimise the hyper-parameters by maximising the marginal likelihood (also known as the "Bayesian evidence") for the model. However, this can lead to more over-fitting than might be expected (see e.g. Cawley and Talbot ). See the work of David MacKay for information on evidence maximisation in machine learning. For comparison, see the work of Radford Neal on the "integrate everything out" approach to similar problems. Note that the evidence framework is very handy for situations where integrating out is too computationally expensive, so there is scope for both approaches. Effectively Bayesians integrate rather than optimise. Ideally, we would state our prior belief regarding the characteristics of the solution (e.g. smoothness) and make predictions notoionally without actually making a model. The Gaussian process "models" used in machine learning are an example of this idea, where the covariance function encodes our prior belief regarding the solution. See the excellent book by Rasmussen and Williams . For practical Bayesians, there is always cross-validation, it is hard to beat for most things!
