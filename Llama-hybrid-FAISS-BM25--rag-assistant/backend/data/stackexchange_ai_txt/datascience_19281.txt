[site]: datascience
[post_id]: 19281
[parent_id]: 
[tags]: 
Experimental design - hyperparameter optimization

I'm reading a paper by Bergstra and Bengio (2012) on random search for hyperparameter optimization. I'm confused by their graph and explanation in Section 2.2: "Figure 2 illustrates the results of a random experiment: an experiment of 256 trials training neural networks to classify the rectangles data set. Since the trials of a random experiment are i.i.d., a random search experiment involving $S$ i.i.d. trials can also be interpreted as N independent experiments of $s$ trials, as long as $sN \le S$." That part I think I understand. Here's my take on it: If I draw from a uniform(0, 1) random variable 100 times, then I have 100 i,.i.d. samples. That's the same as if I had drawn 10 times each from 10 separate uniform(0, 1) random variables. In their figure (below), they say they had 256 trials. I think they chose 256 possible values of a hyperparameter and trained a neural network for each value. In the caption for the figure, they say they had "256 trials of a random experiment...optimizing a neural network....At horizontal axis position 8, we consider 256 trials to be 32 experiments of 8 trials each". This implies that at horizontal axis position 1, they would have 1 experiment of 256 trials. My question is: did they train those 256 models and then break up the results into smaller chunks? Like this: Experiment 1: $[\lambda^1, \ldots, \lambda^{256}]$ Experiment 2: $[\lambda^1, \ldots, \lambda^{128}], [\lambda^{129}, \ldots, \lambda^{256}]$ ... with value $\lambda^{129}$ from Experiment 1 equal to value $\lambda^{129}$ in Experiment 2? Or did they do something else?
