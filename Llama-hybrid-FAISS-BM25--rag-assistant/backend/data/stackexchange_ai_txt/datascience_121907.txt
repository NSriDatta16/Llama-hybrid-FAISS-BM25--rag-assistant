[site]: datascience
[post_id]: 121907
[parent_id]: 
[tags]: 
Supervised time series anomaly detection

I have time series data. Dataset contains around 600.000 metrics. Each metric published daily and has three values, let's say 'count', 'number of something', 'length of something'. Looks this way: name cnt_2023_05_31 num_2023_05_31 len_2023_05_31 cnt_2023_06_01 num_2023_06_01 len_2023_06_01 m_1 100 1000 10000(bad) 99 1002 10003 .... 600.000 of such metrics So for each day metric gets three values. I have historical data where bad metrics are labeled. For example, length at 2023-05-31 was bad. I have quite naive approach to catch issues atm. Just set of rules with hand made thresholds. Rule example: m_1 (metric name) length (values name) - should not increase or decrease in more than 1% comparing to previous value - or - should not increase or decrease in more than 42 comparing to previous value Ofc, it's hard to maintain such rules since I have to periodically adjust threshold values and %. My thoughts: can I periodically train some sort of ML model using labeled data? I know exactly how bad data points look like. I'm pretty sure there is also a correlation between many metrics. Could it help? is there any statistical technique to automatically adjust thresholds in rules? Some sort of standard deviation calculated with sliding window (not good at math :( sorry if it sounds odd)? I've read people using z-score The ultimate business goals are: zero false positives zero rule thresholds maintenance.
