[site]: datascience
[post_id]: 8171
[parent_id]: 8168
[tags]: 
I can't check at the moment (no Matlab at hand), but I suppose the differences come from the different random seeds used to initialize the neural networks (at least this is the only part which i can think of that has a random component). I would suggest predicting class probabilities, averaging those and then viewing the resulting confusion matrix of the "averaged" prediction. This way you - to a degree - mitigating the effect of randomness resulting from different initializations of the weights.
