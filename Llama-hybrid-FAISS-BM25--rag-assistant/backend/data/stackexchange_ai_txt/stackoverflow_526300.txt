[site]: stackoverflow
[post_id]: 526300
[parent_id]: 526255
[tags]: 
This activestate recipe gives an easy-to-follow approach, specifically the version in the comments that doesn't require you to pre-normalize your weights: import random def weighted_choice(items): """items is a list of tuples in the form (item, weight)""" weight_total = sum((item[1] for item in items)) n = random.uniform(0, weight_total) for item, weight in items: if n This will be slow if you have a large list of items. A binary search would probably be better in that case... but would also be more complicated to write, for little gain if you have a small sample size. Here's an example of the binary search approach in python if you want to follow that route. (I'd recommend doing some quick performance testing of both methods on your dataset. The performance of different approaches to this sort of algorithm is often a bit unintuitive.) Edit: I took my own advice, since I was curious, and did a few tests. I compared four approaches: The weighted_choice function above. A binary-search choice function like so: def weighted_choice_bisect(items): added_weights = [] last_sum = 0 for item, weight in items: last_sum += weight added_weights.append(last_sum) return items[bisect.bisect(added_weights, random.random() * last_sum)][0] A compiling version of 1: def weighted_choice_compile(items): """returns a function that fetches a random item from items items is a list of tuples in the form (item, weight)""" weight_total = sum((item[1] for item in items)) def choice(uniform = random.uniform): n = uniform(0, weight_total) for item, weight in items: if n A compiling version of 2: def weighted_choice_bisect_compile(items): """Returns a function that makes a weighted random choice from items.""" added_weights = [] last_sum = 0 for item, weight in items: last_sum += weight added_weights.append(last_sum) def choice(rnd=random.random, bis=bisect.bisect): return items[bis(added_weights, rnd() * last_sum)][0] return choice I then built a big list of choices like so: choices = [(random.choice("abcdefg"), random.uniform(0,50)) for i in xrange(2500)] And an excessively simple profiling function: def profiler(f, n, *args, **kwargs): start = time.time() for i in xrange(n): f(*args, **kwargs) return time.time() - start The results: (Seconds taken for 1,000 calls to the function.) Simple uncompiled: 0.918624162674 Binary uncompiled: 1.01497793198 Simple compiled: 0.287325024605 Binary compiled: 0.00327413797379 The "compiled" results include the average time taken to compile the choice function once. (I timed 1,000 compiles, then divided that time by 1,000, and added the result to the choice function time.) So: if you have a list of items+weights which change very rarely, the binary compiled method is by far the fastest.
