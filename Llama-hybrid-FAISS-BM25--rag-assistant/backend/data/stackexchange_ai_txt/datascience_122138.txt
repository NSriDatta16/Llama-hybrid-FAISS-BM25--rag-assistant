[site]: datascience
[post_id]: 122138
[parent_id]: 
[tags]: 
bayesian neural network and the flipout estimator

i was reading the flipout paper and i stumbled over this passage. I will summarize the main point here: In bayesian neural networks the weights are random variables sampled from a distribution. W is the matrix of the realization of the weights and for each minibatch is decomposed as $W = \bar W + \widehat{\Delta W}$ , where the first term is the mean term of distribution and the second one is the perturbation. the issue is that for computational reasons the perturbation is sampled only once per minibatch and this lead to correlation between the gradients. given the assumptions that 1) the perturbations of different weights are independent and 2) that the perturbation distribution is symmetric around zero then why does he say that the loss gradients are identically distributed to those computed using $\widehat{\Delta W}$ ? if i randomly change the direction of the perturbation on every neurons, how can the gradients be identically distributed? I understand that $\widehat{\Delta W}$ and $\Delta W$ are equally symmetric around zero but the gradients?
