[site]: crossvalidated
[post_id]: 557624
[parent_id]: 
[tags]: 
How to deal with output transformation at inference/prediction time?

Suppose A machine learning model (e.g. RandomForest ) which uses $x$ as input and produces $y$ . Now as part of preprocessing and feature engineering, I applied some transformation on the inputs and target/output variables. As a result, now I feed $x^{'}$ and $y^{'}$ to RandomForest . During training time, I can easily transform the true targets $y_{t}$ to $y^{'}_{t}$ (before feeding them to RandomForest ) and then inverse transform $y^{'}_{p}$ to $y_{p}$ because I know what $y_{t}$ looked like. But at inference/prediction time, how can I transform the predicted values $y^{'}_{p}$ back to normal scale $y_{p}$ then? The problem is illustrated below For some transformations such as log , this is easy because I know, all I have to do is to take exp of predicted values $y^{'}_{p}$ to bring them to normal scale $y_{p}$ . But for many other transformations such as 'minmax' I can not transform back until I know properties of the original data. Please not that I am not talking about difficulty in interpreting the transformed output/target variable which has been discussed here in other posts but about, how to even transform back the predicted results.
