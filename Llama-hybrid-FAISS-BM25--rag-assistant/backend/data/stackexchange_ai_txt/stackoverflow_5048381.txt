[site]: stackoverflow
[post_id]: 5048381
[parent_id]: 
[tags]: 
Using genetic algorithms for neural networks

Currently I am working on a project that would use genetic algorithms to optimize neural networks. I do realize this is probably not the best way to optimize them, but I'm new to both so I just wanted to try using them. My plans are as follows (subject to change, a lot). My input neurons would be using a data set that could have just about any positive number (including decimals up to two places, so really they would be floating point numbers), but most likely between 0 to 20000. Because the importance is in how the numbers compare to each other in value rather than how big they are, they would first be divided by the highest number of all the values that would be inputted. They would them be multiplied by weights (any positive or negative float) before going to their hidden layer. Each neuron in the hidden layer would be summing all of it's inputs until they were done being calculated. They would then be run through a logistics function and be outputted. My environment is Visual studio C++ 2010 Express and I'm using the clr. My problem lies in the genetic algorithm and how it would work. It would be adjusting the weights. My problem is that when it randomly changes a bit in one of the weights (mutation rate), it could make the weights extraordinarily high or low, causing overflow or some other error when multiplied by the input and added with the others. I have no idea how I would organize my chromosomes either. So, would it just be better to perform randomization by selection weights rather than bits at random and changing them to a random number within a defined range? Basically I am looking for suggestions on how to organize this without causing errors with making values end up too big or too small while maintaining performance. Thanks, (and sorry if this should be in theoretical computer science, but I thought it wouldn't fit in there)
