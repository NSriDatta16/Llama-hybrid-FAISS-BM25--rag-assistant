[site]: crossvalidated
[post_id]: 428405
[parent_id]: 233658
[tags]: 
Adding to all the previous answers - I would like to mention the fact that any multi-class classification problem can be reduced to multiple binary classification problems using "one-vs-all" method, i.e. having C sigmoids (when C is the number of classes) and interpreting every sigmoid to be the probability of being in that specific class or not, and taking the max probability. So for example, in the MNIST digits example, you could either use a softmax, or ten sigmoids. In fact this is what Andrew Ng does in his Coursera ML course. You can check out here how Andrew Ng used 10 sigmoids for multiclass classification (adapted from Matlab to python by me), and here is my softmax adaptation in python. Also, it's worth noting that while the functions are equivalent (for the purpose of multiclass classification) they differ a bit in their implementation (especially with regards to their derivatives , and how to represent y). A big advantage of using multiple binary classifications (i.e. Sigmoids) over a single multiclass classification (i.e. Softmax) - is that if your softmax is too large (e.g. if you are using a one-hot word embedding of a dictionary size of 10K or more) - it can be inefficient to train it. What you can do instead is take a small part of your training-set and use it to train only a small part of your sigmoids. This is the main idea behind Negative Sampling .
