[site]: datascience
[post_id]: 66285
[parent_id]: 
[tags]: 
Use trained Tensorflow model to predict on different dataset

I have few datasets, I've trained model on the biggest one and now want to see how it will predict values for different set of data. For saving the model, I've used ModelCheckpoint callback with argument save_weights_only=True and later I've added line model.save(...) . Now I would like to use this model to predict on another dataset - how to do this properly? My biggest concern is that prior to the training, I've shifted my "reference" dataset for number of samples, so it's predicting X samples ahead and I'm not sure how it will behave now when I'll try to model.predict() for another set of data - will it predict for whole dataset or just for the number of shifted values? My second question is: would be it more reasonable to save full model with ModelCheckpoint with weights (meaning - delete save_weights_only=True argument) or my current approach is good? I'm using Tensorflow 1.14. I was following Hvass Labs tutorial on RRN time series predicting, my code looks like this: df =pd.read_json(f'dataset1.json') input_data = self.df.values[0:-10] output_data = self.df.values[:-10] num_data = len(input_data) num_train = int(0.9 * num_data) num_test = num_data - num_train input_train = x_data[0:num_train] input_test = x_data[num_train:] output_train = y_data[0:num_train] output_test = y_data[num_train:] num_input_signals = input_data.shape[1] num_output_signals = output_data.shape[1] Then I've declared the model: model = Sequential() model.add(LSTM(units=512, return_sequences=True, input_shape=(None, num_input_signals,))) model.add(Dense( num_output_signals, activation='relu')) def __batch_generator(self, batch_size, sequence_length, input_train, output_train): while True: x_shape = (batch_size, sequence_length,num_input_signals) x_batch = np.zeros(shape=x_shape, dtype=np.float16) y_shape = (batch_size, sequence_length,num_output_signals) y_batch = np.zeros(shape=y_shape, dtype=np.float16) for i in range(batch_size): idx = np.random.randint(num_train - sequence_length) x_batch[i] = input_train[idx:idx + sequence_length] y_batch[i] = output_train[idx:idx + sequence_length] yield (x_batch, y_batch) validation_data = np.expand_dims(input_test, axis=0), np.expand_dims(output_test, axis=0) optimizer = RMSprop(lr=0.001) generator = self.__batch_generator(256, 168, input_train, output_train) model.compile(loss='mean_squared_error', optimizer='adam') callback_checkpoint = ModelCheckpoint(filepath=f'model', monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True) model.fit_generator(generator=generator, epochs=100, steps_per_epoch=100, validation_data=validation_data, callbacks=[callback_checkpoint]) model.predict(input_train) model.save('dataset1-model.h5') And as you can see, I've shifted my data for 10 samples, so in the output dataframe last 10 rows show NaN and as far as I understand Tensorflow, following code should predict values for 10 samples and later I would like to use this trained model for prediction on another datasets - does it mean it will predict last 10 samples as well, or it will run the prediction for whole dataset? My "reference" dataset looks like this: [ { "timestamp": "2019-02-11 08:00:00", "sine": -0.5877852522924633 }, { "timestamp": "2019-02-11 09:00:00", "sine": -0.809016994374933 }, { "timestamp": "2019-02-11 10:00:00", "sine": -0.9510565162951421 } ] And the other one has less samples (100k vs 10k) of cosine function. When it comes to loading model, I want to use tensorflow.keras.models.load_model . Is that possible that prediction with model trained on 100k dataset, when given 10k dataset, will perform worse than model trained on 10k dataset? Since the first model was trained on bigger dataset, I assume that during prediction, it will expect similar amount of data to properly predict, am I right? Lastly, would I achieve better results if I would use more LSTM layers, but with less units? For example 4 layers, 128 units each. 512 is a limitation set by my hardware, if I go a few more, it will freeze during the training. I'm also taking into account using additional Dropout and Dense layers, but for model with only one LSTM layer I'm not sure if it will make any sense.
