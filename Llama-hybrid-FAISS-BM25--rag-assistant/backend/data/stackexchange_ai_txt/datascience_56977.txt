[site]: datascience
[post_id]: 56977
[parent_id]: 
[tags]: 
Are batch iteration and epochs different in reinforcement learning compared to supervised learning?

I'm following the Udacity "AWS DeepRacer" course about self driving cars with reinforcement learning. In one lesson, they says this: Batch size - This determines how many images, randomly sampled from the most recent episode, are used for training before updating the model. Epochs - Determines how many times you will loop through the batched data before updating the training weights. A larger number of epochs is likely needed if your model is still improving but training has otherwise ceased. Learning rate - Controls the speed at which your algorithm learns (it enlarges or shrinks the weight update after each epoch). Using Tensorflow and Keras with Adam, I learned that the weights are updated after each batch iteration. But in this course, they says that the weights are updated only after each epoch, and the batch size determine how much of the training data to loop through before updating the model. So, what's the difference between updating the model and updating the weights? In tensorflow with keras, the weights are updated after each batch iteration and also after each epoch? Is it different compared to RL?
