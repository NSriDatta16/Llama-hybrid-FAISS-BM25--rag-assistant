[site]: crossvalidated
[post_id]: 443648
[parent_id]: 443631
[tags]: 
Making them directly trainable would not work, since the training algorithm could reach a trivial solution and simply minimize the loss by setting them to zero (or make them negative and then let your actual loss terms grow till infinity). However, you could take the Bayesian approach and introduce a prior over themâ€”giving the model the freedom to find their optimal value while making it stick to some constraints that you define. In such case, your priors would be the hyperparameters that you would still have to set manually. Practically, you would add another loss terms, penalizing $a$ , $b$ , and $c$ for having some "unlikely" values.
