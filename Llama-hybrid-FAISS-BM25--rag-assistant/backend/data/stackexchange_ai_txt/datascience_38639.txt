[site]: datascience
[post_id]: 38639
[parent_id]: 38636
[tags]: 
I'm not really familiar with pytorch (I only know keras) so I'm not really sure. But here's some possible reasons for memory error: The garbage collector isn't working properly so the neural network models you've created while doing trial and error are just filling up in the memory and aren't being cleared. This can occur when you are using a notebook and doing modifications to the NN model there. In keras, you can add a few lines of codes to manually free up the GPU memory. Removing the maxpooling layer makes the model too large for the memory to handle. Basically, the function of the maxpooling layer is to pick only the maximum values produced by the previous convolution layers. By removing the maxpooling layer, you tell the model to use all the output produced by the previous convolution layers. It could be that these are in hundreds of millions VS only a few thousands when only the maximum values are used with the maxpooling layer.
