[site]: crossvalidated
[post_id]: 147072
[parent_id]: 
[tags]: 
Cross-Validation in binary classification using only 10 positive samples (SVM)

I have a binary classification problem for which only $10$ positive samples are available for training. Negatives are in general in abundance, but I choose to use solely $70$ ($7$ negatives per one positive). I am trying to learn a kernel SVM (using the RBF kernel), thus I want to optimize the pair of parameters $C$, $\gamma$. I conduct grid search and I am wondering which division of the training set is more appropriate. Should I use $3$-, $5$-, or $10$-fold cross-validation? Something else maybe? I am particularly interested in the case of $\mathbf{10}$-fold cross-validation , because I have only $10$ positive samples. Would that be a good approach?
