[site]: crossvalidated
[post_id]: 405705
[parent_id]: 405660
[tags]: 
You should calculate the eigenvectors corresponding the the $m$ Principal Components that you chose to keep based on the training dataset. And then use those eigenvectors to transform your new sample (in production) onto the new subspace. For example, let's assume you chose to keep the top two principal components, so $m=2$ . And your input data contains 100 rows and 5 columns, so $n=5$ . The set of eigenvectors is your "projection matrix" and it would look something like this: $$ W = \left[ \matrix { w_{1,1}&w_{1,2} \\ w_{2,1}&w_{2,2} \\ w_{3,1}&w_{3,2} \\ w_{4,1}&w_{4,2} \\ w_{5,1}&w_{5,2} } \right] $$ Notice that there are two eigenvactors because we decided to keep the top two principal components. And there are five total values for each eigenvector, each one corresponding to one of the five columns in the original dataset. Your input dataset would look something like this: $$ X = \left[ \matrix { x_{1,1}&x_{1,2}&x_{1,3}&x_{1,4}&x_{1,5} \\ x_{2,1}&...&...&...&x_{2,5} \\ ...&...&...&...&... \\ x_{100,1}&x_{100,2}&x_{100,3}&x_{100,4}&x_{100,5} } \right] $$ You can use the simple formula below to transform your input sample onto the new subspace: $$ Y = X \times W $$ $Y$ will be your $100 \times 2$ matrix of the transformed sample. Although there's no theoretical need for this, in most software the transformed data is centered (normalized to the mean). You can subtract the means of each column from $Y$ so that the results match with the results from a software function such as .fit_transform() in Python's scikit-learn package. Keep in mind that if you standardized your training data prior to performing PCA (e.g., mean=0 and standard deviation=1), then you'd have to do the same before applying the transformation formula mentioned above.
