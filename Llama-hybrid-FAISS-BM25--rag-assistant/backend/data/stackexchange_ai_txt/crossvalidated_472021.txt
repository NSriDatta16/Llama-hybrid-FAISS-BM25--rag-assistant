[site]: crossvalidated
[post_id]: 472021
[parent_id]: 
[tags]: 
In KNN, why does the number of training examples needed to learn a decision boundary increase (exponentially) as the number of dimensions increases?

In book I'm reading the following is said on k-nearest neighbour algorithms: "As the number of dimensions goes up, the number of training examples you need to locate the concept's frontiers goes up exponentially. With 20 Boolean attributes(features), there are roughly a million different possible examples" My questions: 1)why exactly does the number of training examples needed to learn a decision boundary increase (exponentially) as the number of dimensions increases? 2)the quoted paragraph says that we need to have a data point in our training data corresponding to each possible example, but do we really need an actually training data point for each and every possible example? (I can intuitively guess that performance is of course going to be better if we have training data for each possible example but I'd like to know exactly why?) 3)Does link to how in general statistics, as we introduce more parameters into a model, we need to collect more data?
