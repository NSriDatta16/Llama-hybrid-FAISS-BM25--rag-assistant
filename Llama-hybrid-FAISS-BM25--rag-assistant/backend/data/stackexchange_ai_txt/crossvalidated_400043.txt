[site]: crossvalidated
[post_id]: 400043
[parent_id]: 399848
[tags]: 
Your concern about having too many predictors for the number of cases is valid, but there are better ways to proceed than trying to combine models based on separate subsets of predictors within the same cohort. The problem is that survival models have an inherent omitted-variable bias . Unlike linear regression, where this is only a problem if the predictors omitted are correlated with those included, omitting any predictor associated with outcome from a survival or logistic regression model tends to bias the estimates of all the included predictor coefficients toward lower magnitudes. This answer has a nice analytical explanation for a conceptually similar situation with probit models. So the problem you face is trading off that omitted-variable bias against the danger of overfitting with too many predictors. Say that you have 450 events among your 873 patients. Then you could reasonably try to include 30 or 40 predictors in a standard unpenalized survival model without overfitting. So one approach would be to use clinical judgment to identify the 30 or 40 predictors among all the data types that are most likely to be related to outcome, and use them in a standard Cox regression. But you already are using penalized approaches in some of your modeling attempts. Your "penalized Cox regression" is presumably a ridge regression or LASSO, which cuts down the effective number of predictors and adjusts the coefficient values to reduce overfitting. A ridge regression using all of the predictors might work surprisingly well; there's no reason to do ridge on separate subsets of them. Random forests also tend to minimize the possibility of overfitting as they don't use all of the predictors at each decision point. Boosted trees don't typically overfit unless you use too many trees. These tree-based methods are essentially ensembles of models to start with. Different types of models have different strengths, and there might be something to be said for combining them. So you could consider the following approach: develop different types of models based on all of the predictors reasonably associated with outcome and then combine the models in some way. For example, Mark van der Laan and colleagues recommend what they call targeted maximum likelihood estimation in which they build large numbers of different types of models and then develop a parametric model combining the models' predictions near a particular point of interest (e.g., survival at 3 years) that is optimized by maximum likelihood techniques. That combination of information among several models can lessen any overfitting provided by any single model. That said, you could be just as well off in practice with one of a well chosen standard Cox model, ridge regression, or one of the tree-based approaches. Penalized approaches like ridge regression have the advantage that you can easily choose to include some critical predictors without penalization and include all other predictors with penalization. One important point here is to get away from analyzing separate subsets of predictors and to include together as many predictors as are reasonably related to outcome in the set of predictors that you evaluate for your model building. A second point, critical but not directly raised in your your question, is to evaluate your model building process with methods like bootstrapping to get an estimate of generalizability and bias. This page has an outline and links to further details.
