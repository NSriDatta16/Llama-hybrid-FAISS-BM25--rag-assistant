[site]: crossvalidated
[post_id]: 406691
[parent_id]: 
[tags]: 
Question about the location of regularization constant C in SVM

I've encountered very similiar but different functions in SVM optimization problem, the diffrence is in the location of regularization constant C. $\sum_{i=1}^n(1-(y_i(w^tx))_+ +\frac{1}{2C} \left\lVert w\right\rVert^2 \rightarrow min_w$ $C\sum_{i=1}^n(1-(y_i(w^tx))_+ +\frac{1}{2} \left\lVert w\right\rVert^2 \rightarrow min_w$ Is it right that technically they are diffrent functions, and if we graph them we will see the difference, but the location of minimum is the same for both of them? Also I would like to check whether my intuition about regularization is right. In the first function small values of $ะก$ makes second term of the summation large, that means with large $C$ we want $\left\lVert w\right\rVert$ to be small and thus miximize the separating margin. In the second equation with large values of $C$ we want to reduce the number of misclassifiactions and also penalise more those errors with lesser value of $y_i(w^tx)$ , since $y_i(w^tx)$ is negative for misclassified examples. These are two different ways to arive at the same optimal solution. Is it the right logic?
