[site]: datascience
[post_id]: 118245
[parent_id]: 115554
[tags]: 
I highly recommend you read Microsoft's recent paper about In Context Learning. Although the focus is on LLM I think it can be generalised to other models. The idea is to consider models as mesa|meta-optimisers (optimisers at inference time). They approximately show that the model performs implicit gradient descent (and thus implicit fine-tuning) at inference time. Obviously, the gradient descent doesn't modify the models' weights, but it modifies the attention mechanism (as would fine-tuning by modifying the weights).
