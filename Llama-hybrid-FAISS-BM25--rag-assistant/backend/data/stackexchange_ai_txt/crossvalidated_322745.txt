[site]: crossvalidated
[post_id]: 322745
[parent_id]: 322736
[tags]: 
For "modern-day" classifiers, I think you possibly mean generative vs. discriminative classifiers. Yes, DTCs (decision tree classifiers) can provide meaningful cutpoints of feature values along with class purity of the two child nodes (split off from each parent node). You commonly always need to run a DTC if you need to know useful feature cutpoints or decision rules which are informative for classification. However, you'd be better off using Random Forests (RF), which is a DTC "on steroids" for generating importance scores for each feature. Breiman's RF design can also provide the number of times a randomly drawn feature is used in first-node splits as well as all node splits over the trees used, so it breaks away from straightforward DTC. The advantage of RF is that the training data are bootstrapped for each tree, so RF follows the premise that "you don't believe the data," and need to bootstrap it in order to generate meaningful statistics. But every classifier has its own advantages -- and DTCs do have their own. Regarding terminology, DTCs are in the family of CART classifiers, "classification and regression trees." For more information, see the C4.5 classifier by Quinlan.
