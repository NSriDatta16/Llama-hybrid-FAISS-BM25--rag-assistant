[site]: crossvalidated
[post_id]: 283702
[parent_id]: 283674
[tags]: 
In addition to @DavidSmith's answer, some more formal terminology follows: Generalized linear models invoke a mean-variance relationship as a consequence of the link function. There are no residuals in a GLM because the variance is just a function of the mean. So when we write a GLM it is of the form: $$g(E[Y|X]) = \beta X$$ Where $g$ is a link function, the terms $\beta X$ are the linear predictors $\nu$ and the transformed values $g^{-1}(\beta X)$ are the fitted values. In general, the case is that $E[Y] = g^{-1}(\beta X)$ implies $var(Y) = \frac{\partial}{\partial \beta} g^{-1}(\beta X)$. For instance, with logistic regression, the inverse logit link $g^{-1}(x) = \log(\frac{X}{1-X})$ has $g^{ \prime -1}(X) = \log(\frac{1}{1-X}) = g^{-1}(X)(1-g^{-1}(X))$ with the second expression easily recognized as the binomial variance. When you write out the estimating equations for common probability models, like binomial, poisson, or exponential, you actually observe that the information (or variance) depends on the mean and nothing else. These one parameter models, as the name suggests, use only one parameter (like a log odds or log relative rate) to relate the expected outcome to a linear combination of predictors and a corresponding link function. The influence function (gradient or derivative) of the link relates the mean to the variance. Gaussian probability models differ from binomial (logistic) models in that they are 2 parameter models including a dispersion term (sigma, or the residual variance). A Gaussian model is also different from other 2 parameter models (like negative binomial or Gamma) because you can write the residual variance as a separate term in a model. Basically the ordinary least squares with normal, independent error is the only case I know of where we can actually write: $y = \beta X + \epsilon$ meaningfully. The bigger question of how you relate expected outcomes to observed outcomes is complicated. In a normal model, this is a simple difference of expected and observed to obtain a residual. In GLMs, the variance is heteroscedastic because the mean changes as a function of $X$, so you can standardize each residual by dividing by the expected standard error to obtain Pearsonized residuals.
