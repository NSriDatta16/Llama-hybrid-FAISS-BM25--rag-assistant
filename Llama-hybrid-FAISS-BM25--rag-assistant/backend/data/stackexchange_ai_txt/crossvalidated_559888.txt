[site]: crossvalidated
[post_id]: 559888
[parent_id]: 
[tags]: 
GANs training initially degrades pre-trained generator

I have an issue with the training of a GAN, which consists of a generator and two discriminators. The generator is used to generate waveforms. 1-The generator is independently pre-trained by regression. 2-The two randomly initialized discriminators are then activated, and GANs training takes place. However, step 2 initially degrades the output of the generator, by removing too much information. This is also reflected in all the validation losses, whose values get worse. You can see in the image below (orange curve), representing the l1-norm between input and output: the l1-norm steeply rises for the first few validation steps. I tried freezing the generator for 80k steps, to allow pre-training of the discriminators (blue curve), but the problems persists. The GAN is based or a variation of the code below: https://github.com/rishikksh20/hifigan-denoiser/blob/master/train.py Any suggestions?
