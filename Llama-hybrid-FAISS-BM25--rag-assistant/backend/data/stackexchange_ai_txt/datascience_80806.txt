[site]: datascience
[post_id]: 80806
[parent_id]: 
[tags]: 
Why does Transfer Learning works better on smaller datasets than on larger ones?

This question is not about the utility of Tranfer Learning compared with regular supervised learning. 1. Context I'm studying Health-Monitoring techniques, and I practice on the C-MAPSS dataset . The goal is to predict the Remaining Useful Life (RUL) of an engine given sensor measurements series. In health-monitoring, a major issue is the low amount of failure examples (one can't afford to perform thousands of run-to-failure tests on aircraft engines). This is why Transfer Learning has been studied to solve this, in Transfer Learning with Deep Recurrent Neural Networks for Remaining Useful Life Estimation , Zhang et al , 2018. My question is about the results presented in this article. 2. Question C-MAPSS dataset is composed of 4 subdatasets, each of which has different operational modes and failure modes. The article cited above performs transfer learning between these subdatasets. Particularly, when training a model on a target subdataset B using the weights of a trained model on a source dataset A, they don't train on all the B dataset. They conduct an experiment in which they test various sizes for the target dataset B : they try on 5%, 10%, ..., 50% of the total dataset B. The results are presented in page 11. A few cases excepted, the have better results on smaller target datasets. This seems counter intuitive to me : how could the model learn better on less examples ? Why does Transfer Learning works better on smaller datasets than on larger ones ?
