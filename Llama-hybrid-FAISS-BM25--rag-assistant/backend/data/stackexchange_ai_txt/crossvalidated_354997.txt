[site]: crossvalidated
[post_id]: 354997
[parent_id]: 
[tags]: 
Multiple hypothesis testing

Suppose I have time series data for $(X,Y_i)$, independent and dependent variables, respectively, which vary over time $t=1,...,1000$. I have data on $Y_i$ for 100 individuals/cities/etc $i=1,...100$. So, each of these 100 $Y$'s is a time series and has 1000 data points. $X$ is common across all individuals and also varies over time. I run 100 regressions of $Y_i$ on $X$, one for each $i$. I find that 1 one of those regressions is significant at, say, $\alpha=10\%$ level. Since the probability under a binomial with m=100 trials and probability of success ($\alpha = 10\%$) of having at least one success (one false positive) is almost 1, this does not mean much. The probability that there is at least one false positive in one of my 100 regressions is almost 1. A common way to go about this is to use a Bonferroni kind of correction that would demand the use of a $\alpha / m$ significance level. However, the probability of finding more than 15 false positives is $7.25\%$, which is actually lower than $10\%$. Therefore, if I would have found more than 15 significant coefficients across my 100 regressions, I could be relatively confident that I was not facing a chance result. Is this reasoning justified or not? In fact, in the sense of decreasing the type I error, my reasoning would suggest that I would be more confident that I am not getting false positives if I found 15/100 significant coefficients at the $10\%$ level compared to the situation in which I get 1/1 signifcant coefficient in one single regression at the $10\%$ level. In such a case, I would be inclined to say that a Bonferroni correction would not make sense (it would be unncessary). A bit more detail as requested: I run 100 time series regressions of the form (one for each $i$): $Y_{i,t} = \alpha + \beta_iX_t$, $i=1,2,3,...,100$ $Y_{i,t}$ is happiness of country $i$ in month $t$. $X_{t}$ is world GDP. Note that this is not a panel stacking countries. This is one time series regression per country. For instance, for China: $Y_{china,t} = \alpha + \beta_{china}X_t$ I reject the null that $\beta_i= 0$ at the 10% level for some countries $i$ and not others. The probability that in 100 trials I reject at least one by chance, is 99% under a binominal distribution with probability of success (false positive) of $10\%$ and 100 trials. Therefore, if I would find 1 out of 100 significant coefficients, the probability of type I error is almost 1: out of 100 tries, I'd expect almost surely at least one false positive anyways. So I should not reject the null that $\beta_i= 0$ for that country, even though it passes the individual significance test at the 10\% level. Now, suppose that instead of 1 rejection, I am actually able to reject the null for 15 countries. The probability that in 100 trials I reject at least 15 by chance, is $7.25%$ under a binominal distribution with probability of success (false positive) of 10% and 100 trials. Therefore, I am tempted to conclude that, given that I am willing to live with a type I error chance of 10% (just assume this), I can in fact reject the null $\beta_i= 0$ for those 15 countries and indeed say that I have evidence that x is associated with y. Therefore, what I am saying, is that I do not understand why I would ever apply a Bonferroni or FDR correction in this setting. If i I have only one significant coefficient out of 100, I cannot trust it because I'd expect it with probability 1. I would need some kind of correction (like Bonferroni)! But if I have 15 significant coefficients, I can actually trust it more since the probability of observing that by chance is less than 10\%. I do not need need any correction. Take the extreme case: instead of 15, all 100 coefficients are individually significant at the 10\% level. The prob that at least 100 coefficients out of 100 are significant (under binomial) is almost 0%. I couldn't possibly be more confident that my results are not driven by type I error. Why would I want to adjust or correct for anything? My question is simply: is this reasoning wrong or correct? I claim I do not need a correction. Yes, or no? If no, where is the mistake in my reasoning. Thanks! New add-on as reply to greg (no space in the comment): Thank you Greg! Awesome insightful explanation. I am just still confused on why it is such a big problem to assume that all null hypotheses are true. Isn't that what we do when we compute p-values: the probablity of observing a value more extreme than the one we estimated, given that the null is true? My idea is to basically derive a joint p-value by defining a probability of observing 15 values more extreme than the 15 ones I estimated across 100 regressions, given that the 100 null hypotheses are true (the worst case scenario in which everything is insignificant). If I understand your explanation correctly, the problem with my approach would be that, focusing on type I error only, for instance, maybe 7 null hypotheses are true and 8 are false. I reject 15, so I have 7 false positives. The probablity of having at least 7 false positives out of 92 coefficients, is 82\%, which is much larger than the desired 10\% level I would like. Is this correct?
