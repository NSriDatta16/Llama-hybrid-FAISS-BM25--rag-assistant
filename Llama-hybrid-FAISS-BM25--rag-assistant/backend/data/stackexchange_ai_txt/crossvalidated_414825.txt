[site]: crossvalidated
[post_id]: 414825
[parent_id]: 
[tags]: 
How exactly is the error backpropagated in backpropagation?

I am reading a book on neural networks, and am now doing a chapter on backpropagation. ( See chapter here ). In this chapter, the writer is presenting four equations, that together form the backbone of the Backpropagation algorithm. In the second equation: \begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \end{eqnarray} He states the following: This equation appears complicated, but each element has a nice interpretation. Suppose we know the error: \begin{eqnarray} \delta^{l+1} \end{eqnarray} At the l+1th layer. When we apply the transpose weight matrix, \begin{eqnarray} (w^{l+1})^T \end{eqnarray} We can think intuitively of this as moving the error backward through the network, giving us some sort of measure of the error at the output of the l'th layer. We then take the Hadamard product: \begin{eqnarray} \odot \sigma'(z^l) \end{eqnarray} This moves the error backward through the activation function in layer l, giving us the error Î´l in the weighted input to layer l. I don't understand how taking the transpose of a weight-matrix moves the error backwards; Since we multiply by the weight matrix to get to the next layer, some sort of division by the weight matrix would be more logical for me to get from the l+1 layer to the l'th layer. I also do not really get why taking a hadamard product takes us further back, to the weighted input. Can somebody explain me what is going on here?
