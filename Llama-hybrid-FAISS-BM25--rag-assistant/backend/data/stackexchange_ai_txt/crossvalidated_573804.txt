[site]: crossvalidated
[post_id]: 573804
[parent_id]: 573802
[tags]: 
There is no assumption of normality in logistic regression. Linear regression is often motivated as a Gaussian GLM (since solving a least squares problem is the same as assuming the likelihood for the model is normal), and this is where the normality assumption of the residuals comes from. In contrast, logistic regression makes the assumption that the likelihood is binomial $$ \operatorname{logit}(p_i) = x^T_i\beta $$ $$ y_i \sim \operatorname{Binomial}(p_i; n_i) $$ As a consequence, looking at the difference between observation and prediction is not as informative (especially if the outcome is a 1/0) and you're better off looking at deviance residuals (which require grouping of continuous covariates) or other types of residuals listed in Frank Harrell's Regression modelling strategies . Heteroskedasticity is actually an assumption of logistic regression. Since the variance of a binomial random variable is $np(1-p)$ and $p$ is a function of $x$ , then the conditional variance changes as a function of $x$ (hence is not constant; is heteroskedastic). You might want to use splines .
