[site]: crossvalidated
[post_id]: 328585
[parent_id]: 328562
[tags]: 
In a classical (frequentist/hypothesis testing) setting, this is IMHO a difficult tradeoff where neither of the solutions is better in all cases. Estimates with a random effect per person will be noisy, but calculating overall correlation looses information and may even be grossly misleading (e.g. Simpson's paradox). Which of the two is less wrong depends on your data and the decisions you want to use the data for. You basically want to take weighted average of the two, but I am not aware of any principled way to calculate the weights and determine p-values etc. (it definitely is possible, but I am not sure there are packages implementing it) In a fully Bayesian setting, hierarchical multilevel models can work well with little data per group. The "hierarchical" part implies something along the lines of: $X_i \sim N(\mu_i,\sigma)$ $\mu_i = \beta Y_i + \alpha_{g(i)} $ $\alpha \sim N(\nu, \tau)$ $\nu \sim N(0,1)$ $\tau \sim HalfNormal(0, 1)$ $\beta \sim N(0,1)$ Where $g(i)$ indicates the group (person) the i-th datapoint belongs to. This is actually a principled approach to find a good compromise between overall and per-person correlation. This means that the values for $\alpha$ are tied together via $\nu$ (overall trend for all persons) and $\tau$ (between person variance) and thus share information but are not forced to be equal. The scale parameters for priors (the ones) should be adjusted to reflect the scale of your data. There is a worked out example going from classical linear regression to a hierarchical model in the Radon case study for Stan+Python. Also you may check out the documentation for R package rstanarm (especially the relevant vignette ) that lets you fit these kind of models easily.
