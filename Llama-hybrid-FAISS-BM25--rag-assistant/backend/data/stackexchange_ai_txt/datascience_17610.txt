[site]: datascience
[post_id]: 17610
[parent_id]: 17567
[tags]: 
RNNs are not designed to do language modeling exclusively, they are designed to process time series data, and language happened to be representable as time series. There is plenty of papers demonstrating how to use RNNs to do classification and regression on time series ( awesome list of papers ). One-hot encoding is often used in cases where the input is discrete and not a number that can be directly fed into a model. However, one-hot encoding is actually not always the norm for language-modeling, some research actually map each character (or each word depending on how one wants to model the problem) to a unique numerical identifier (e.g. with $id\in \left\{ 0\ldots n \right\}$; with $n$ the size of the vocabulary) and the model map that identifier to a vector representation. This is particularly useful when the size of the vocabulary is huge and you want to avoid having to deal with big one-encoded vectors. Take a look at word2vec and word2vec Tensorflow for more details about this. In your case, you want to process sensor data. There is no need for one-hot encoded because your data are continuous and numerical. In other words, you can input the recorded EEG data directly; although it's usually better to clean them up and normalize them beforehand. There is actually plenty of papers about EEG data processing with RNNs . Regardless of the type of data, the idea to fed them into a RNN remains the same: provide a data sample $x$ for each timestep $t$ . For EEG data recorded for 5 timesteps with 1 electrode $a$, you would have a 1-dimensional vector with one sample per timestep: $a = [0.12, 0.44, 0.134, 0.39, 0.23]$ $inputvector = [0.12, 0.44, 0.134, 0.39, 0.23]$ For EEG data recorded for 5 timesteps with 3 electrodes $a, b, c$, you would have a 3-dimensional vector with one sample per timestep: $a = [0.12, 0.44, 0.134, 0.39, 0.23]$ $b = [0.43, 0.92, 0.3, 0.37, 0.4]$ $c = [0.13, 0.1, 0.4, 0.21, 0.14]$ $inputvector = [[0.12, 0.43, 0.13], [0.44, 0.92, 0.1], ...]$ I highly recommend you to read some of the literature I linked above.
