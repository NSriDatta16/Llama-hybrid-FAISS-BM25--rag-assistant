[site]: crossvalidated
[post_id]: 187430
[parent_id]: 
[tags]: 
simplification of variance formula about linear combination of order statistics

In the book APP. Math. Stat ( https://books.google.com/books?id=enUouJ4EHzQC&pg=PA266&lpg=PA266&dq=serfling+variance+of+L+estimate&source=bl&ots=ehRxuMmiQ5&sig=lDK209BhPb5chwbBrbMG-RMDFFA&hl=en&sa=X&ved=0ahUKEwiHpbfotubJAhUBGh4KHZprB0oQ6AEITTAI#v=snippet&q=He%20establishes%20several%20results&f=false ) by Robert Serfling at page 276, it pointed out that, for the type of estimator $S_{n}$=$\frac{1}{n}\sum_{i=1}^{n}J(\frac{i}{n+1})X_{i}$, where $X_{i}$'s are order statistics and $J()$ is a bounded function, Stigler(1974) showed that the variance of $S_{n}$ has the form $\sigma^2(S_{n})$=$\int_{-\infty }^{-\infty}\int_{-\infty }^{-\infty}J(F(x))J(F(y))[F(min(x,y)-F(x)F(y)]dxdy$ where capital $F()$ is the distribution function. I have two questions about the variance formula $\sigma^2(S_{n})$. First is that in practice if I have $n$ data points and a given functional form of $J$, can I approximate the variance formula in the following manner as $\sigma_{2}^2(S_{n})$= $\sum_{i=1 }^{n}\sum_{j=1 }^{n}J(\frac{i-1}{n})J(\frac{j-1}{n})[\frac{min(i-1,j-1)}{n}-\frac{(i-1)(j-1)}{n^2}](X_{i}-X_{i-1})(X_{j}-X_{j-1})$ ?? where I also use empirical distribution function instead of the unknown distribution $F$. The second question is that for the simple example of mean estimtaion, given $J()=1$, then $S_{n}$ is just simple mean, we already know that the variance formula looks like $\frac{1}{n^2}\sum_{i=1}^{n}(X_{i}-\bar{X})^2$ in a quadratic form. So I am thinking is there any way to rewrite $\sigma^2(S_{n})$ in more intuitively understandable quadratic form ?? thank you
