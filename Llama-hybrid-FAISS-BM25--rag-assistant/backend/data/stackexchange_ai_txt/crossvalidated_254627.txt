[site]: crossvalidated
[post_id]: 254627
[parent_id]: 21738
[tags]: 
In linear regression, you are fitting: $y = f(\beta, X) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots $ You fit $\beta$ given training data $(X, Y)$ Suppose you drop the $\beta_0$ and fit the model, will the error in the fit: $ \sum_i (y_i- f(\beta, X_i) )^2$ be larger than if you included it? In all (non-degenerate) cases you can prove the error will be the same or lower (on the training data) when you include $\beta_0$ since the model is free to use this parameter to reduce the error if it is present and helps, and will set it to zero if it doesn't help. Further, suppose you added a large constant to y (assume your output needed to be $+10000$ than in your original training data), and refit the model, then $\beta_0$ clearly becomes very important. Perhaps you're referring to regularized models when you say "suppressed". The L1 and L2 regularized, these methods prefer to keep coefficients close to zero (and you should have already mean and variance normalized your $X$ beforehand to make this step a sensible one. In regularization, you then have a choice whether to include the intercept term (should we prefer also to have a small $\beta_0$?). Again, in most cases (all cases?), you're better off not regularizing $\beta_0$, since its unlikely to reduce overfitting and shrinks the space of representable functions (by excluding those with high $\beta_0$) leading to higher error. Side note: scikit's logistic regression regularizes the intercept by default. Anyone know why: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html ? I don't think its a good idea .
