[site]: crossvalidated
[post_id]: 637888
[parent_id]: 637798
[tags]: 
According to the article Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention [Zhang et al., 2019] , the Transformer architecture suffers from poor convergence due to gradient vanishing caused by the interaction between residual connections and layer normalization, and adopting a much smaller standard deviation (0.02) for initialization helps mitigate this issue. From section 5 of the mentioned article (emphasis mine): The gradient norm is preserved better through the self-attention layer than the encoder-decoder attention, which offers insights on the successful training of the deep Transformer in BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), where encoder-decoder attention is not involved. However, results in Table 1 also suggests that the self-attention sublayer in the encoder is not strong enough to counteract the gradient loss in the feedforward sublayer. That is why BERT and GPT adopt a much smaller standard deviation (0.02) for initialization , [...]. P.S: this answer is the same as my answer to the cross-posted question on the Data Science SE.
