[site]: crossvalidated
[post_id]: 325162
[parent_id]: 324857
[tags]: 
I am scratching my head about this as well... In principal what follows is more of a comment than an answer. Everything I say is just guessing because I have neither read a formal description yet. I think that most applied statisticians have a very deep understanding what they are doing but the do not allow the rest of the world to understand it properly because, as you said: apparently they like it very much to confuse random variables, values, measures, distributions, densities and so on. Nevertheless: in all cases there is a clean, mathematical, unambigous interpretation behind the symbols: We start with a probability space $\Omega$ (with some $\sigma$-algebra and some measure $P$ on it). In Markov processes we have some easy 'input' that we are given and we need to interpret and understand it. The input is a (for now finite) set of possible state values $S = \{s_1, ..., s_n\}$ and a (for now finite) set of actions $A = \{a_1, ..., a_m\}$ and a deterministic transition function $\Delta : S \times A \to S$ that, when evaluated like so $\Delta(s,a)$ giving us a completely fixed description of what happens (of what is the next state) when we are in state $s$ and take action $a$. Nothing probability related so far. The first thing that people seem to have confusion about is the policy (is it random or not?). The input we are given is what they call $\pi : S \times A \to [0,1]$ such that for all $s$ we have $\sum_{a \in A} \pi(s,a) = 1$ The first mistake is to say that $\pi$ is some sort of random or random variable. Up to now we can read that as a matrix of size $|S| \times |A|$ such that the row sums are $1$. Absolutely deterministic. I think what they actually mean is the following: We are given a whole set of random variables $(\alpha_s)_{s \in S} : \Omega \to A$ that are responsible for sampling the action that we take. We will write this as $\alpha(s, \omega) = \alpha_s(\omega)$. Now what their $\pi$ means is nothing else but $$P[\alpha(s, \omega) = a] = \pi(s, a)$$ i.e. their $\pi$ gives the distribution of the variables $\alpha_s$. I think that you are absolutely right in saying that we must define the random variables $S_t$ and $A_t$ now recursively: $$ A_t(\omega) = \alpha(S_t(\omega), \omega) $$ i.e. the action we take at time $t$ is defined by what the policy "randomly" choses when inserting the current state $s_t = S_t(\omega)$ and the general information $\omega$. $S_t$ in turn is defined as $$S_t(\omega) = \Delta(S_{t-1}(\omega), A_{t-1}(\omega))$$ i.e. the state $s_t$ at time $t$ is the one that we get by the transition function $\Delta$ when evaluated at the state $s_{t-1} = S_{t-1}(\omega)$ and the action $a_{t-1} = A_{t-1}(\omega)$ that we took before. The mere fact that this definition is recursive is not too bad: On the one hand this seems to be the nature of the RL game: take the current state and do something with it in order to get to the next state. In mathematics in general there are many recursive things: the faculty function, the Fibonacci numbers, ... Now the only question that remains is how to define $S_0$! However, this is something that must come from the input: When considering a Markov process we are usually given a finite state graph. When unflating that and doing probability theory with it we always end up in terms like $$p(s_{t}|s_{t-1}) \cdot ... \cdot p(s_1|s_0) \cdot p(s_0)$$ i.e. that tiny little $p(s_0)$ stays there no matter what you do. I think it is something the user must specify (like a prior in the Bayesian setup). However, as we gain more and more information with many states and the current situation only depends on the very last state and not the thing we did initially, I guess that it is not so important how to define $S_0$ so I think it is fine to start defining $S_0$ to be just some uniformly distributed random variable over the states. Or maybe it is like this: The thing we want to do in the end is to compare policies (in order to get the best one). When comparing them I think that $p(s_0)$ has little influence. For example, if you get some KPI that somehow needs to divide the expressions above for different policies then the $p(s_0)$ cancels out... Does that make sense? edit : Now thinking about what you said about the deterministic policy and the "impossible" event in the comments...
