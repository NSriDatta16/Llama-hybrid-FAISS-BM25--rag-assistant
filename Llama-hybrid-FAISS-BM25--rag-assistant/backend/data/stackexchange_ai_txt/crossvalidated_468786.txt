[site]: crossvalidated
[post_id]: 468786
[parent_id]: 468782
[tags]: 
I think you are missing something, but it depends on your definition of robustness. I would have defined robustness as saying for any fixed $M_i$ , $P(error|M=M_i)$ is the same (or is controlled), regardless of whether $M$ is correctly specified. What you need to avoid post-selection problems is that $P(error|M=m)$ is correctly specified for random $m$ . This requires not only that $P(error|M=M_i)$ is the same for all $M_i$ , but also that the event $[error | M=M_i]$ is independent of the event $[m=M_i]$ . You can come up with settings where this is true, but it's a much stronger condition than just having correct errors under misspecification. You can often still control the post-selection error probability -- for example, if the null hypothesis is one that can be tested by permutation, do a permutation test over the entire model selection procedure. But that's stronger than robustness -- the reference distribution for the test depends on the entire set of models being selected In at least one post-selection inference problem there is a proof that no confidence interval procedure can have correct coverage (even asymptotically)
