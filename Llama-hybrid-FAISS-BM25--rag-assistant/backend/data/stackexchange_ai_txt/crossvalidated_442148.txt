[site]: crossvalidated
[post_id]: 442148
[parent_id]: 441100
[tags]: 
This depends on how big is the dataset and how much time and computational resources you have. If you can afford training a model many times I'd do option 5, i.e. For each vector of hyperparameters $h \in H$ perform a $k-$ fold Cross Validation to estimate the performance of the model. After some predefined number of iterations (e.g. 20) train the model again using the best found vector of hyperparameters $h^*$ with the whole dataset. However, since you've mentioned I have many samples in my data and if you're planning to train some deep neural network, then you're likely cannot afford using Cross Validation, since training is quite expensive. In such case I'd do option 1, i.e. Split your dataset $X$ into training $X_{train}$ and validiation $X_{val}$ sets (e.g. in proportions $80:20$ ) and then, for each vector of hyperparameters $h$ train a model and validate it using $X_{val}$ . At the end, select the best $h^*$ and retrain the model again using the whole $X$ . Note that if you're doing research and you want to explicitly measure and report the performance of your model, you will likely need to split your dataset $X$ into three sets: $X_{train}$ for training, $X_{val}$ for validation (hyperparameter tuning, i.e. Bayesian Optimisation) and $X_{test}$ which you can test your model on and report its performance.
