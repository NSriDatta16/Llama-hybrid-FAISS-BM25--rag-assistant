[site]: crossvalidated
[post_id]: 581648
[parent_id]: 
[tags]: 
Which optimizers are suitable for online learning for an agent?

An agent is exploring an unknown world. The agent uses a neural network to make predictions about the next time step. We want to use some form of gradient descent optimizer to tune the network parameters so that it makes more accurate predictions. Particular considerations: Observations are presented in a non-random order, the order in which the agent observes them. This order involves substantial correlations between subsequent observations, e.g. we might sometimes have the same or a very similar observation 10,000 steps in a row. But this should not cause the agent to forget how to predict other kinds of observations. The agent has a limited memory of past observations. We can't collect all the observations and present them in randomized batches, because the agent doesn't remember them. A specific constraint could be, the agent has only k*N memory, where N is the number of weights in the network and k is some small constant such as 10. The optimizer should be robust against large gradients - it should not blow up the weights to infinity even with bad inputs. We don't know beforehand the details of the environment, so it would be best if the optimizer has few or no parameters we have to specify beforehand. Which optimizers would be suitable for this setting?
