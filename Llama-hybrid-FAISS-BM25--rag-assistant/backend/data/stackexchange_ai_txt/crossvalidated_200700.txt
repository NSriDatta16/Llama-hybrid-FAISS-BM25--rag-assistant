[site]: crossvalidated
[post_id]: 200700
[parent_id]: 200500
[tags]: 
Here is my two cents. I think that at some point, many applied scientists stated the following "theorem": Theorem 1: $p\text{-value} and most of the bad practices come from here. The $p$ -value and scientific induction I used to work with people using statistics without really understanding it and here is some of the stuff I see: running many possible tests/reparametrisations (without looking once at the distribution of the data) until finding the "good" one: the one giving $p ; trying different preprocessing (e.g. in medical imaging) to get the data to analyse until getting the one giving $p ; reach $0.05$ by applying one-tailed t-test in the positive direction for the data with positive effect and in the negative direction for the data with negative effect (!!). All that is done by well-versed, honest scientists having no strong sensation of cheating. Why ? IMHO, because of Theorem 1. At a given moment, applied scientist may believe strongly in their hypothesis. I even suspect that they believe they known they are true and the fact is that in many situations they have seen data from years, have thought about them while working, walking, sleeping... and they are the best to say something about the answer to this question. The fact is, in their mind (sorry I think I look a bit arrogant here), by Theorem 1 if they hypothesis is true, the $p$ -value must be lower than $0.05$ ; no matter what the amount of data is, how they are distributed, the alternative hypothesis, the size effect, the quality of the data acquisition. If the $p$ -value is not $ and the hypothesis is true, then something is not correct: the preprocessing, the choice of test, the distribution, the acquisition protocol... so we change them... $p$ -value $ is just the ultimate key of scientific induction. To this point, I agree with the two previous answers that confidence intervals or credible intervals make the statistical answer more proper to the discussion and to the interpretation. While $p$ -value is difficult to interpret (IMHO) and ends the discussion, interval estimates can serve a scientific induction illustrated by objective statistics but lead by expert arguments. The $p$ -value and the alternative hypothesis Another consequence of Th.1 is that if $p$ -value $>0.05$ then the alternative hypothesis is false. Again this is something that I encounter many times : try to compare (just because we have the data) a hypothesis of the type $H_0: \mu_1 \ne \mu_2$ : take randomly 10 data-points for each of the two groups, compute the $p$ -value for $H_0$ . Find $p=0.2$ , notice in some part of the brain that there is no difference between the two groups. A main issue with the $p$ -value is that the alternative is never mentioned while I think in many cases this could help a lot. A typical example is point 4., where I proposed to my colleague to compute posterior ratio for $p(\mu_1>\mu_2|x)$ vs. $p(\mu_1 and get something like 3 (I know this figure is ridiculously low). The researcher asks me if it means that the probability that $\mu_1>\mu_2$ is 3 times stronger than those $\mu_2>\mu_1$ . I answered that this is a way to interpret it and she finds this amazing and that she should look at more data and write a paper... My point is not that this "3" helps her to understand that there is something in the data (again 3 is clearly anedoctic) but that it underlines that she misinterprets the p-value as "p-value>0.05 means nothing interesting/equivalent groups". So in my opinion, always at least discussing the alternative hypothesis (es!) is mandatory, allows to avoid simplification, gives element to debate. Another related case is when experts want to : test $\mu_1>\mu_2>\mu_3$ . For that they test and reject $\mu_1=\mu_2=\mu_3$ then conclude $\mu_1>\mu_2>\mu_3$ using the fact that the ML estimates are ordered. Mentioning the alternative hypothesis is the only solution to solve this case. So using posterior odds, Bayes factor or likelihood ratio conjointly with confidence/credible intervals seems to reduce the main involved issues. The common misinterpretation of $p$ -value / confidence intervals is a relatively minor flaw (in practice) While I am a Bayesian enthusiast, I really think that the common misinterpretation of $p$ -value and CI (i.e. the $p$ -value is not the probability that the null hypothesis is false and the CI is not the interval that contains the parameter value with 95% chance) is not the main concern for this question (while I am sure this is a major point from a philosophical point of view). The Bayesian/Frequentist view have both pertinent answers to help practitioner in this "crisis". My two cents conclusion Using credible interval and Bayes factor or posterior odds is what I try to do in my practice with experts (but am also enthusiast in CI+likelihood ratio). I came to statistics a few years ago mainly by self-studying from the web (so many thanks to Cross Validated !) and so grew up with the numerous agitations around $p$ -values. I do not know if my practice is a good one but it is what I pragmatically find as a good compromise between being efficient and making my job properly.
