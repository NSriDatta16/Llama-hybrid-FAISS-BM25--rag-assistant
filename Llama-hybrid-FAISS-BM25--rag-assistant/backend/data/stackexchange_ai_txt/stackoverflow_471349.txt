[site]: stackoverflow
[post_id]: 471349
[parent_id]: 471341
[tags]: 
Robots.txt should be your first port of call. The search bot should take note of these settings and stop hitting the pages that you deny access to. This is easily done by creating a file in the root of your website with the following syntax: User-agent: * Disallow: / That syntax essentially says: All search bots (the wildcard *), you are not allowed to index anything under /. More information at robotstxt.org If this doesn't work, the next step is to ban the IP address if possible.
