[site]: crossvalidated
[post_id]: 484379
[parent_id]: 484102
[tags]: 
As Arksakal has indicated in his comment, "perfect multicollinearity is not an issue in gradient based techniques of loss minimization." To explain this further, statisticians are often most concerned with uniquely estimating parameters in regression equations so they can be interpreted. When you have linearly dependent columns created with one-hot encoding the parameter estimates cannot be uniquely determined. The estimated coefficients have an infinite number of solutions. In machine learning, interpreation of parameters isn't really something of a concern. Instead predictive models are constructed and models are build to minimize loss functions such and the Means Squared Predictive Error. The value of these loss functions are identical whether one uses $c$ levels of a qualitative variable or $c-1$ . However, if you were were to use $c$ levels of this variable in a statistical model, there would be no unique estimate for $\hat{\beta}_2$ , for example, which makes interpretation of this parameter impossible.
