[site]: crossvalidated
[post_id]: 35786
[parent_id]: 35776
[tags]: 
The need mentioned in the first paragraph of the question relates to the output layer activation function, rather than the hidden layer activation function. Having outputs that range from 0 to 1 is convenient as that means they can directly represent probabilities. However, IIRC, a network with tanh output layer activation functions can be trivially transformed into a network with logistic output layer activation function, so it doesn't really matter much in practice. IIRC the reason for using tanh rather than logistic activation function in the hidden units, which is that change made to a weight using backpropagation depends on both the output of the hidden layer neuron and on the derivative of the activation function, so using the logistic activation function you can have both go to zero at the same time, which can end up with the hidden layer unit becoming frozen. In short, use tanh for hidden layer activation functions, chose the output layer activation function to enforce desired constraints on the output (common choices: linear - no constraints, logistic - output lies between 0 and 1 and exponential - output strictly positive).
