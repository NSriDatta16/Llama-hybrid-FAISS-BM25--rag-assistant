[site]: datascience
[post_id]: 32408
[parent_id]: 32407
[tags]: 
I suggest taking a look at this page for some more ideas: Feature Selection That being said a couple of ideas that come to mind quickly, is to: use a tree based method (like Random Forest) and look at your feature importances. Scikit Learn has a handy class for doing just that see the link above. Use some sort of regularization/penalty like L1 or L2 regularization. That will force non-useful features to have parameters close to zero. Recursively remove variables and see what the resulting output is and cross-validate. Again sklearn has a method for this. Generally, these methods will be "expensive" as you are fitting multiple models to get you where you need to go.
