[site]: stackoverflow
[post_id]: 1490553
[parent_id]: 1490061
[tags]: 
The main characteristics of the problem are: Externally defined categorization criteria (keyword list) Items to be classified (lines from the requirement document) are made of a relatively small number of attributes values, for effectively a single dimension: "keyword". As defined, no feedback/calibrarion (although it may be appropriate to suggest some of that) These characteristics bring both good and bad news: the implementation should be relatively straight forward, but a consistent level of accuracy of the categorization process may be hard to achieve. Also the small amounts of various quantities (number of possible categories, max/average number of words in a item etc.) should give us room to select solutions that may be CPU and/or Space intentsive, if need be. Yet, even with this license got "go fancy", I suggest to start with (and stay close to) to a simple algorithm and to expend on this basis with a few additions and considerations, while remaining vigilant of the ever present danger called overfitting. Basic algorithm (Conceptual, i.e. no focus on performance trick at this time) Parameters = CatKWs = an array/hash of lists of strings. The list contains the possible keywords, for a given category. usage: CatKWs[CustTx] = ('deposits', 'deposit', 'customer' ...) NbCats = integer number of pre-defined categories Variables: CatAccu = an array/hash of numeric values with one entry per each of the possible categories. usage: CatAccu[3] = 4 (if array) or CatAccu['CustTx'] += 1 (hash) TotalKwOccurences = counts the total number of keywords matches (counts multiple when a word is found in several pre-defined categories) Pseudo code: (for categorizing one input item) 1. for x in 1 to NbCats CatAccu[x] = 0 // reset the accumulators 2. for each word W in Item for each x in 1 to NbCats if W found in CatKWs[x] TotalKwOccurences++ CatAccu[x]++ 3. for each x in 1 to NbCats CatAccu[x] = CatAccu[x] / TotalKwOccurences // calculate rating 4. Sort CatAccu by value 5. Return the ordered list of (CategoryID, rating) for all corresponding CatAccu[x] values about a given threshold. Simple but plausible: we favor the categories that have the most matches, but we divide by the overall number of matches, as a way of lessening the confidence rating when many words were found. note that this division does not affect the relative ranking of a category selection for a given item, but it may be significant when comparing rating of different items. Now, several simple improvements come to mind: (I'd seriously consider the first two, and give thoughts to the other ones; deciding on each of these is very much tied to the scope of the project, the statistical profile of the data to be categorized and other factors...) We should normalize the keywords read from the input items and/or match them in a fashion that is tolerant of misspellings. Since we have so few words to work with, we need to ensure we do not loose a significant one because of a silly typo. We should give more importance to words found less frequently in CatKWs. For example the word 'Account' should could less than the word 'foo' or 'credit' We could (but maybe that won't be useful or even helpful) give more weight to the ratings of items that have fewer [non-noise] words. We could also include consideration based on digrams (two consecutive words), for with natural languages (and requirements documents are not quite natural :-) ) word proximity is often a stronger indicator that the words themselves. we could add a tiny bit of importance to the category assigned to the preceding (or even following, in a look-ahead logic) item. Item will likely come in related series and we can benefit from this regularity. Also, aside from the calculation of the rating per-se, we should also consider: some metrics that would be used to rate the algorithm outcome itself (tbd) some logic to collect the list of words associated with an assigned category and to eventually run statistic on these. This may allow the identification of words representative of a category and not initially listed in CatKWs. The question of metrics, should be considered early, but this would also require a reference set of input item: a "training set" of sort, even though we are working off a pre-defined dictionary category-keywords (typically training sets are used to determine this very list of category-keywords, along with a weight factor). Of course such reference/training set should be both statistically significant and statistically representative [of the whole set]. To summarize: stick to simple approaches , anyway the context doesn't leave room to be very fancy. Consider introducing a way of measuring the efficiency of particular algorithms (or of particular parameters within a given algorithm), but beware that such metrics may be flawed and prompt you to specialize the solution for a given set at the detriment of the other items ( overfitting ).
