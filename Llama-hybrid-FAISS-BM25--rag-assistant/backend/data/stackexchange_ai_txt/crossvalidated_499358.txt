[site]: crossvalidated
[post_id]: 499358
[parent_id]: 499347
[tags]: 
Is DL supervised or unsupervised? Although DL is usually applied to supervised learning (classification and regression), it can be used to unsupervised learning as well. You can read about it in this review . You mentioned the autoencoder in your question, and it is a nice example of unsupervised DL: it does not attempt to predict a target variable, it only seeks to find a low dimensional representation for your data, for purposes of dimensionality reduction, denoising, clustering, etc. Does splitting data make sense in unsupervised settings? I am not sure here. This answer on Stack Overflow discusses this a bit, but I did not find it very convincing. If you have some instances which you know to be anomalous, you could use these labels to test your model (in a kinda semisupervised way, since your model is unsupervised but the model evaluation is supervised). Even if you don't have labeled data, perhaps it would be helpful to check how many instances your model considers anomalous in a test/validation dataset (in case of overfitting, one would expect that your model considers too many test instances to be anomalous). How can we calculate the loss function (MSE, MAE, etc.) for the unsupervised setting? Is it common? If you are doing cluster analysis, perhaps you should focus on different metrics/loss functions such as the ones provided by Scikit-learn for clutering evaluation . For dimensionality reduction methods such as PCA and autoencoders, it makes sense to use metrics like MSE. If we denote by $f$ the function performing the dimensionality reduction and $g$ the function mapping the encoded representations back to the reconstructed instances, you could have something like: $$\text{MAE}=\frac 1n \sum_{i=1}^n ||x_i-g(f(x_i))||^2$$ Also, make sure you have a clear image of the question you want to answer. Clustering and anomaly detection algorithms will give you different information, and it's important to decide which is better for you. It seems that, in your application, the goal is to flag unusual traffic data. For this particular task, anomaly is a better fit than clustering. [Some algorithms, like DBscan or even GMM, can do both clustering and anomaly detection. Still, you have to be aware of the difference and the importance of each task in your application.] Hope it was helpful!
