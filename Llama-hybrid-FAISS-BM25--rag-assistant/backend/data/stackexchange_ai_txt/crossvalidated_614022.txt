[site]: crossvalidated
[post_id]: 614022
[parent_id]: 
[tags]: 
Is AIC scale invariant for problems concerning the number of data points in regression?

I am trying to use Akaike Information Criterion with the small sample correction (AICc) as method for determining how many data points to use in a linear approximation of a non-linear function; the goal is to make an extrapolation, with error bars, from the end of my data set. In working on this project I ran into a problem where the AICc does not seem to be invariant with respect to the scale of the y-variable, which I had expected it to be. By this, I mean that if I take some non-linear function, fit a series of linear regressions with different numbers of data points and plot the AICc scores for these regressions I get a different curve than if I take the same data set and multiply the values by say 10. Obviously, the magnitude of the two AICc curves should be different, but I was surprised that the minimums of the two curves are at different locations. My thought in using AICc to determine the ideal number of data points to use in the linear approximation is that it could balance the information added by including more data points with the information lost by fitting a linear regression to an increasingly non-linear set of data points. Typically, AICc is used to determine how many parameters to include in a model to avoid the risk of overfitting, but the un-simplified equation for AICc for least squares suggests that it can also be used to pick a number of data points to include in the model. Here I will define AICc as $$ AICc=2k +n\ln\left(2\pi\right) +n\ln\left(\widehat{\sigma}^2\right) +\frac{RSS}{\widehat{\sigma}^2} +2\frac{k^2+k}{n-k-1} $$ where $k$ is the number of parameters in the model, including the noise parameter (for 1st order OLS $k=3$ ); $n$ is the number of data points in the regression (for AICc $n\ge5$ ); $RSS$ is the residual sum of squares, $RSS=(\widehat{y}-y)^2$ ; and $\widehat{\sigma}^2$ is the reduced chi-squared statistic, $\widehat{\sigma}^2=RSS / (n-2)$ for 1st order OLS (I know some people just use $\widehat{\sigma}^2=RSS / n$ , which simplifies the AICc equation). In the code below, I fit the right half of a bell curve ( $y=\exp{(-x^2)}$ ) with a series of linear regressions using a different number of data points, starting at the right tail and progressively looking at more data points with smaller $x$ values; this is the same manner in which I would consider linear approximations to my actual data set. To simplify sharing this MWE I have used the linear regression library from statsmodels which I believe uses the full AIC equation but does not include the small sample correction (AIC vs AICc). The code has the ability to include noise in the data (the real data I am working with has noise), but I have set the noise level to 0 so that the fundamental behavior is more obvious. I ran the same fitting sequence twice, once calculating the y-values with the base function, and a second time where I multiply these y-values by some arbitrary coefficient, in this case 10. #preamble import numpy as np from numpy.random import default_rng import matplotlib.pyplot as plt import statsmodels.api as sm %matplotlib widget #%matplotlib inline #set x range and step size x_min=0 x_max=10 x_step=.01 #set y values for magnitude coefficient, steepness coefficient, and noise level #uses random range with fixed seed for consistent results y_coeff=0.1 steepness_coeff=.1 y_noise_level=0 rng = default_rng(0) #creates range of x-values and evaluates the function x=np.arange(x_min,x_max+np.spacing(x_max),x_step) y=y_coeff * np.exp(-steepness_coeff * x ** 2) + rng.normal(loc=0,scale=y_noise_level,size=len(x)) #fits the data set with 1st order OLS models with varying number of data points #y_mult_list is a multiplier that is put on the y-value right before the model is evaluated to it includes noise #store as a dictionary of lists of models where there is a list of models for each multiplier value #and each model in list has a different number of data points #use statsmodel because it is well constained and has AIC calculation built-in y_mult_list=[1,10] min_ols_points=5 #start at 5 b/c that is min for AICc max_ols_points=len(x) stats_models_dict={y_mult_list_val: [sm.OLS(endog=y_mult_list_val*y[-ols_points:], exog=sm.add_constant(x[-ols_points:])).fit() for ols_points in range(min_ols_points,max_ols_points+1)] for y_mult_list_val in y_mult_list} #computes AIC (not AICc) for the models in the dictionary #stores AIC values in same structure as model_aic_dict={key: [model.aic for model in model_list] for key, model_list in stats_models_dict.items()} #creates figure and labels axes fig1, ax1a = plt.subplots() ax1a.set_title('AIC score for models with different number of data point') ax1a.set_xlabel('number data points to look back at for OLS') ax1a.set_ylabel('AIC score (y multiplier = %d)'%(y_mult_list[0])) #creates secondary y-axis ax1b = ax1a.twinx() ax1b.set_ylabel('AIC score (y multiplier = %d)'%(y_mult_list[1])) #plots AIC values for both series of models on the two axes ax1a_obj=ax1a.scatter(range(min_ols_points,max_ols_points+1), model_aic_dict[y_mult_list[0]], color='b', label =y_mult_list[0]) ax1b_obj=ax1b.scatter(range(min_ols_points,max_ols_points+1), model_aic_dict[y_mult_list[1]], color='r', label =y_mult_list[1]) #merges legends for the different axes into a single legend #https://stackoverflow.com/questions/14344063/single-legend-for-multiple-axes axes_obj=[ax1a_obj, ax1b_obj] ax1a.legend(axes_obj, [axis_obj.get_label() for axis_obj in axes_obj], title='y multiplier value') #reverse x-axis so that including more data points #indicates you are looking back at more data ax1a.set_xlim(ax1a.get_xlim()[::-1]) The output of this function is a plot that shows the AIC curves for the two sets of values. To highlight the fact that including more data involves looking further back into the data set I have reversed the x-axis. I assumed that AIC was scale-invariant, and that simply putting a coefficient in front of the input data would not materially affect the AIC curve. I was expecting these two curves to have different magnitudes, but I was surprised that the minimizer was not at the same number of data points. The AIC equation suggests that you can just as easily use it for determining the number of data points to include, $n$ , as you can for the more traditional approach of ensuring your model is not overly complex, as measured by the number of fitted parameters $k$ . Is this interpretation wrong? Am I violating some fundamental assumption of the AICc? Am I missing some scaling factor that is typically ignored in the more common application, but that is needed in the way I am using it? I am also open to any other suggestions for an approach to selecting the number of data points to include in the linear approximation. I am working with projecting time series data, which an N-fold Cross-Validation approach is not typically suited for. I haven't tried a Generalized Cross-Validation (GCV) approach because I was not using regularization, but that would be fairly easy to implement (the Unbiased Predictive Risk Estimator (UPRE) would require an uncertainty estimate that I don't have). I could also try finding the minimum value of the reduced chi-squared statistic, but I haven't spent the time to consider if that is theoretically sound.
