[site]: crossvalidated
[post_id]: 304044
[parent_id]: 301357
[tags]: 
Your fundamentally misunderstood the concept of cross validation. $k$-fold cross validation: Have 1 loop Equally divide dataset into $k$ folds. For $i \in [1;k]$, test models on the $i$-th fold and train on the remaining folds. The final result is the average results of $k$ testing folds. nested $k$-fold cross validation: Have 2 loops. For every $i$ above, we have a $k$-fold cross validation nested inside. Holdout: literally hold a set of data out for testing. In your case, when you split the data into 2 folds and perform two different tasks on each fold , you are not doing cross validation. The key thing to remember that whatever you want to do, you have perform it independently for every $i \in [1;k]$. Thus, in your case, the correct way should be: In the inner loop, you select best hyper-parameters by subsequently using different hyper-params to train model on the inner training folds and test on the inner testing fold. Then use the best models selected in the inner loop to test on the outer loop. The average results of the outer loop is the estimated performance of your model.
