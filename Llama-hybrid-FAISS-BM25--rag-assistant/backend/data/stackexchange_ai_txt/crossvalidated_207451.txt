[site]: crossvalidated
[post_id]: 207451
[parent_id]: 207213
[tags]: 
I don't know why they don't teach certain things, but it doesn't mean that those things are not used. Machine learning research is at the moment taking two main avenues: a) Narrow AI b) General AI (aka AGI) The hot potato for narrow AI these days is Deep Learning. The advantage of Deep Learning is that it is able to learn "relatively" complex functions, and training is done with numerical optimization techniques which are also relatively fast. However you are limited to the functions that are simple in the Chomsky hierarchy and also limited to low time and space complexities. https://en.wikipedia.org/wiki/Chomsky_hierarchy https://en.wikipedia.org/wiki/Theory_of_computation Is this a big deal? For many problems not really, and there is the theoretical argument of growing the network size and make the claim that we are anyhow resource bounded. But AGI guys will disagree with you: "How can one understand the complexities of our universe by discarding complex functions (that take forever to compute)?" with a shame on you attitude. Now when you get into the realm of AGI you start building very complex models that require not only numerical optimization but also combinatorial optimization. It means you start trying different transformations with various different levels of complexity. And this is something horrendously slow, and that's why you need giant distributed computer farms. This is the automated equivalent of old-school statistics where you try transformations of variables. Probably the reason you are not seeing it is AGI is somewhat a smaller and more esoteric community.
