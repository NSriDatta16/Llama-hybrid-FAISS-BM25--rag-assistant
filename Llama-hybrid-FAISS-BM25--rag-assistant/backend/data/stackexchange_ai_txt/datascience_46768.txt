[site]: datascience
[post_id]: 46768
[parent_id]: 
[tags]: 
Which reinforcement learning methods can be trained off-policy?

My understanding is that: Value-Based methods such as DQN, C51, Rainbow DQN can naturally be trained off-policy using a Replay Buffer, without having to account for any kind of off-policy correction. Deterministic Policy Gradient methods, such as DDPG or TD3 can be trained off-policy, as they don't integrate over actions. Stochastic Policy Gradient methods can't , on principle, be trained off-policy, as they need to estimate the gradient using the current policy. However using importance sampling, you can correct that off-policiness, eg the way TRPO or ACER does. So in the end all the major methods: value function approximations, deterministic policy gradient, and stochastic policy gradient, can be trained using a replay buffer. Is my understanding correct?
