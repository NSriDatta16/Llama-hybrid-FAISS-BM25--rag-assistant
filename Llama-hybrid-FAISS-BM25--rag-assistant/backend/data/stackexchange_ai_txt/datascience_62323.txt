[site]: datascience
[post_id]: 62323
[parent_id]: 
[tags]: 
Grid search or gradient descent?

Assume we have a neural network and one of its activation functions is a function of parameter a . We want to find the weights and parameter a that leads to the minimum loss on the validation set which one is better?: Treat a as a hyperparameter. Do grid search for a : consider a range for a and evaluate the loss for a few points in the range. Pick the best a in the range along with the weights leading to the minimum loss. Treat a as a parameter. Since the loss of the network is a function of parameter a , use gradient descent to update weights as well as parameter a at each iteration. To me, the second option is better since it can lead to the optimal point but the grid cannot lead to the optimal point, it can lead to the points around the optimal point depending on your selection. But, is the second method right? Is there a better method? Lastly, in grid search, how do we pick the range?
