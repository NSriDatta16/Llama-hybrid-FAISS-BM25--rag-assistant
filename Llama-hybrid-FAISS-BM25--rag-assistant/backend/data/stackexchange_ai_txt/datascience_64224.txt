[site]: datascience
[post_id]: 64224
[parent_id]: 64218
[tags]: 
As a disclaimer, I would like to point out that the performance in this specific case might still be due to the specific dataset used. A likely explanation lies in the essence of what trees and boosting algorithms do. As @akvall pointed out in the comments, Boosting algorithms can often overfit since this is what they are designed to do! As a reminder, regardless of how fancy a boosting algorithm works, it follows the following logic: train on the training set evaluate to see which mistakes were made retrain and focus more on the previous mistakes repeat until satisfactory results Trees do not work the same way and are therefore less prone to overfitting. A Random Forest will simply compute independent trees and use majority voting to make a prediction. Every boosting algorithm will "boost" for a certain amount of iterations, it is probably wise to look at how many times your algorithms did "boost".
