[site]: crossvalidated
[post_id]: 27158
[parent_id]: 27146
[tags]: 
Being too certain of your model's results because you use a technique/model (such as OLS) that does not account for a time series' autocorrelation. I don't have a nice graph, but the book "Introductory Time Series with R" (2009, Cowpertwait, et al) gives a reasonable intuitive explanation: If there is a positive autocorrelation, values above or below the mean will tend to persist and be clustered together in time. This leads to a less efficient estimate of the mean, which means that you need more data to estimate the mean to the same accuracy than if the there were zero autocorrelation. You effectively have less data than you think you do. The OLS process (and therefore you) assume that there is no autocorrelation, so you are also assuming that the estimate of the mean is more accurate (for the amount of data you have) than it actually is. Thus, you end up being more confident of your results than you should be. (This can work the other way for negative autocorrelation: your estimate of the mean is actually more efficient than it would be otherwise. I have nothing to prove this, but I'd suggest that positive correlation is more common in most real-world time series than negative correlation.)
