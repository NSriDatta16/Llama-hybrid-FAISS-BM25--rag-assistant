[site]: crossvalidated
[post_id]: 622815
[parent_id]: 
[tags]: 
Does it make sense to perform Domain Adaptation before Transfer Learning?

Suppose I would like to do extractive question answering on scientific literature. I'm interested in using BERT which was pretrained on Wiki and Bookcorpus. I see two routes here: 1 . Fine-tune BERT on scientific literature (specifically abstracts) using the masked language modeling task, then replacing the head and fine-tuning with an extraction question answering scientific literature dataset (SQuAD like). 2 . Go straight to fine-tuning on the downstream task of extractive question answering. I know in both cases there would be transfer learning and domain-adaptation. But I believe I would get better results if I allowed the base model of BERT to learn and understand the vocabulary, patterns, etc. of scientific literature. In other words place more emphasis on domain-adaptation before doing anything else. Does this make sense or is this overkill?
