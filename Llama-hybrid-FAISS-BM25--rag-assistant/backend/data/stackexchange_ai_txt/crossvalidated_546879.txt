[site]: crossvalidated
[post_id]: 546879
[parent_id]: 
[tags]: 
Interpreting difference in logistic regression coefficients after data handling

I am building a multiple logistic regression model in which 10 variables (1 variable of interest and 9 covariates) are included. In total, I have 286 rows of data. 12 rows of data are unfortunately, missing values for 3 of the covariates. I therefore decided to exclude these rows prior to using the glm model to make things neater. To my surprise, I realised that the coefficients were different from if I had not done any exclusion, and let glm deal with the 'missingness'. Here, the R gives me a prompt that '12 observations deleted due to missingness' at the end of running the code. Is there an explanation for this difference? More importantly, should I be manually excluding the incomplete rows or leave it to glm to remove those? I have attached the code that I used below, along with the two differing outputs. ####Loading data, make subset of data with relevant parameters and col names### data $'Case' Case, levels=c(0,1)) analyse $'Education' Education, levels=c(0,1,2,3),labels=c("Never","1-6 years", "7-10 years",">10 years")) analyse $'Eyecolour' 'Eyecolour', levels=c(0,1),labels=c("Light brown","Black/dark brown")) analyse $'Skincolour' 'Skincolour', levels=c(1,3,4),labels=c("Very white/white","Light tan","Tan/dark brown/black")) analyse $'Sunburn' 'Sunburn', levels=c(4,3,2,1),labels=c("Never","Seldom", "Occasionally","Frequently")) analyse $Familyhx1 Familyhx1 %>% #family history in particular, required further manipulations due to categorizing # option 4 represented "unknown" cancer history and had to be omitted. na_if(4) %>% # option 3 and option 2 both represented past history of cancer in 1o relatives and had to be combined. replace(.==3, 2) %>% #putting correct labels on factor(levels=c(1,2),labels=c("No","Yes")) #subsetting only participants with completed covariates and micronutrient levels## ##This is the one line of code that results in different coefficients, even though the logistic regression model automatically subset the 274 rows of data for which all covariates are adjusted for## dt % drop_na(Age, Education, 'BMI','Familyhx1','all_trans_retinol') #logistic regression for retinol #1 all_trans_retinol (median analysis) w $retinolmedian all_trans_retinol $all_trans_retinol >w[2], "> Median", NA)) retinolmedian retinolmedian dt $retinolmedian retinolmedian) dt $retinolmedian retinolmedian, ref=" With the above codes I get the output: adjustedoddsretinolmedian ##THIS IS OUTPUT ONE WITH SUBSETTING ## all: glm(formula = prostaterisk ~ retinolmedian + age + education + familyhx + bmi + Eyecolour + Skincolour + Sunburn, family = binomial(link = "logit")) Coefficients: (Intercept) retinolmedian> Median age education1-6 years -9.2211 1.8047 0.1004 0.5553 education7-10 years education>10 years familyhxYes bmi 0.4106 1.6839 1.4142 -0.0905 EyecolourBlack/dark brown SkincolourLight tan SkincolourTan/dark brown/black SunburnSeldom 1.9385 0.9632 0.8592 0.1913 SunburnOccasionally SunburnFrequently 0.8926 2.2314 Degrees of Freedom: 273 Total (i.e. Null); 260 Residual Null Deviance: 374.6 Residual Deviance: 232.3 AIC: 260.3 However, after I remove 'Sunburn', 'Skincolour' and 'Eyecolour' during subsetting and let glm remove the rows with missingness instead, I get a different output. adjustedoddsretinolmedian ## THIS IS OUTPUT TWO, WITHOUT SUBSETTING FOR EYE COLOUR, SKINCOLOUR AND SUNBURN ## Call: glm(formula = prostaterisk ~ retinolmedian + age + education + familyhx + bmi + Eyecolour + Skincolour + Sunburn, family = binomial(link = "logit")) Coefficients: (Intercept) retinolmedian> Median age education1-6 years -9.48321 1.93527 0.10300 0.55615 education7-10 years education>10 years familyhxYes bmi 0.39554 1.58786 1.45186 -0.08927 EyecolourBlack/dark brown SkincolourLight tan SkincolourTan/dark brown/black SunburnSeldom 1.98861 0.89964 0.77817 0.17177 SunburnOccasionally SunburnFrequently 0.73251 2.26092 Degrees of Freedom: 273 Total (i.e. Null); 260 Residual (12 observations deleted due to missingness) Null Deviance: 374.6 Residual Deviance: 230.8 AIC: 258.8 I note that both the AIC and coefficients are also different, even though the degree of freedom is the same. I am struggling to understand why the two outputs should be any different. Thank you for any insight you could share on this.
