[site]: datascience
[post_id]: 96679
[parent_id]: 
[tags]: 
Does it make sense to translate text before vectorization?

I and new to NLP and I want to ask this question from knowledgeable people. I work on a Russian text document and I want to vectorize it using some pre-trained embeddings. There are much more pre-trained models in English compared to Russian. So I asked myself, I will lemmatize and tokenize my document so it should be pretty easy to translate it from Russian to English. Maybe instead of using Russian-trained embeddings, I will translate it and then use English embeddings? Does it even make sense?
