[site]: crossvalidated
[post_id]: 277700
[parent_id]: 
[tags]: 
ReLU gradient descent matrix dimensionality

If I'm backpropagating through a recurrent neural network, say my layer output is $$h_t = \text{ReLU}(U h_{t-1} + V x_t),$$ when calculating the gradient my dimensions don't seem to be coming out properly. I'm looking to take the gradient of h with respect to $h_{t-1}$ which I'm assuming is $$ \frac{\partial h_t}{\partial h_{t-1}} = \frac{\partial \text{ReLU}(A)}{\partial A}\frac{\partial A}{\partial h_{t-1}}$$ where $$\frac{\partial A}{\partial h_{t-1}} = \frac{\partial (U h_{t-1} + Vx_t)}{\partial h_{t-1}} = \frac{\partial (U h_{t-1})}{\partial h_{t-1}} = U^T$$ (in denominator notation). If $h \in \mathbb{R}^n$ I would expect $\partial h_t/\partial h_{t-1}$ to have dimensions $[n \times n]$, however I believe $\frac{\partial \text{ReLU}(A)}{\partial A}$ has dimensions $[n \times 1]$ and I know U^T has dimensions $[n \times n]$, so those two obviously can't be multiplied in any way to get a final dimension $[n \times n]$. Any suggestions?
