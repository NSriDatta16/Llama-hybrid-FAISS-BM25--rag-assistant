[site]: crossvalidated
[post_id]: 642161
[parent_id]: 642091
[tags]: 
First, don't think about the colliders situation. Both methods are equally ill-equipped to handle colliders and none are more robust to conditioning on colliders than others. That is not the sense in which propensity score methods are designed to be more robust. You might have heard that IPW is able to handle post-treatment colliders and regression can't, but that is only in the case of sequential treatment effect estimation with time-varying confounders, which are also colliders. In a single time point study like this, colliders are equally toxic for both methods and the difference you saw is likely due to the specific data-generating model and not to an inherent difference between the two methods that one would expect to hold more broadly. I'm not exactly sure what you mean by "a statistical regression method". When the estimand is the average dose-response function (ADRF), you can use regression in the form of g-computation to estimate it. You can also use propensity score methods to do so instead, or combine the methods. G-computation involves fitting a regression model for the outcome given the treatment and covariates. From that model, you generate predictions for outcome under every possible value of the treatment, and for each value, you compute the mean prediction across all units in your sample. That outcome model needs to be correct for the data-generating process in order for the ADRF to be consistently estimated. A problem with this approach is that not only do you need to allow flexibility in the relationship between the treatment and the outcome, but you also need to flexibly model the relationships between the covariates and the outcome and their interactions with treatments. That can lead to huge models that are impossible to fit, even with just a few covariates, and the method is not robust to misspecification of this model. For example, if the relationship between the outcome and the treatment is curvy, and the relationship between the outcome and a covariate is curvy, and the relationship between the outcome and treatment changes based on the covariate in a curvy way, then modeling any of these relationships as linear will yield a biased ADRF, even if you have correctly include all variables required to remove confounding. Weighting allows you to avoid some of these issues; importantly, you don't need to model the relationship between the covariates and the outcome or the interactions between the covariates and treatment in the outcome model. You do still need to get the relationship between the treatment and outcome correct, but that is much easier than getting the entire data-generating model correct. The tradeoff is that you still need to correctly estimate the weights and you may have a decrement in precision using weights. To model the weights, you need to correctly model the conditional density of the treatment given the covariates. For propensity score methods with a binary treatment, the density is determined by a single parameter, the mean (i.e., the probability of getting treated), so that is much easier to do. Here, you need to correctly model the whole density. Usually we just model the mean of the density and make a strong assumption about its shape (including that its shape is constant), but this can yield bias. An alternative is to use a weighting method that avoids modeling the treatment like distance covariance optimization weights (DCOWs), which directly estimate weights to minimize imbalance without a treatment model. Your toy examples could not hope to express the realistic situations in which these two approaches would differ. Below is a list of scenarios in which you would expect weighting to perform better than regression-based g-computation: The outcome model is highly complex and curvy in the covariates (it can be hard to specify a well-fitting outcome model that accounts for this) The individual dose-response functions vary a lot (weighting averages over the individual dose-repsonse functions without requiring modeling this heterogeneity) The outcome model involves many covariates (weighting is less sensitive to overfitting the treatment model and it is easier to prevent overfitting without impacting inference than it is to do so in the outcome model) The treatment is straightforward to model, but not linear (if it were linear, adjusting for linear terms in the outcome model would be enough even if the rest of the model were misspecified) You have a large sample (g-computation for a large sample can be computationally intensive, and the precision benefits one gets from g-computation are diminished with larger samples) I think it is noble to want to explore the differences in performance in toy examples that are easy to understand and control, but these methods were designed for more complicated cases. Simulations with simple data-generating models tend to prefer simple methods that make strong (but incidentally correct) assumptions. It is important to consider how these methods were designed to be used in practice, which often involves complex data-generating processes that aren't very revealing on their own. I recommend the simulations in Huling et al. (2023), which introduces the DCOWs and compares them to other weighting methods (and of which I am an author). While we don't compare regression and weighting, we do provide a potentially realistically complex data-generating model that is able to distinguish among many flexible weighting methods. References Huling, J. D., Greifer, N., & Chen, G. (2023). Independence Weights for Causal Inference with Continuous Treatments. Journal of the American Statistical Association, 0(0), 1â€“14. https://doi.org/10.1080/01621459.2023.2213485
