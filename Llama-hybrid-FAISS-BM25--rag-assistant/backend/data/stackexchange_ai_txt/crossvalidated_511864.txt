[site]: crossvalidated
[post_id]: 511864
[parent_id]: 
[tags]: 
Bayesian neural networks and out-of-distribution data?

In a Bayesian neural network (for classification) the posterior predictive distribution is $$ P(y=c \mid {\bf x}, \mathcal D_{train}) = \int P(y=c \mid {\bf x}, \theta) p(\theta \mid \mathcal D_{train}) d\theta $$ Let's assume that we have enough training data $\mathcal D_{train}$ such that if $\bf x$ is quite similar to the training data the uncertainty in the posterior (predictive) prediction is low. So, we assume that in this region we have low aleatoric uncertainty and due to the amount of training data the weights distribution is not quite different from a delta function. Can we say something about the posterior (predictive) prediction uncertainty if $\bf x$ is OOD (out-of-distribution)? In the literature it is often argued like this: " consistent in the region of training data, and becomes increasingly diverse when the input $\bf x$ is far from the training data" (Source: Predictive Uncertainty Estimation via Prior Networks ). However, is there a general theoretic argument (or proof) for arbitrary models (here Neural Networks) which underpins that? Gaussian processes for instance are designed that this nice property holds. As simple counter example, let's consider univariate linear regression with known slope. The only random variable of the model is the intersect. Here, the uncertainity of the predicted $y$ is the same for OOD and in-distribution $x$ . So, is there an argument that Bayesian neural networks can handle the uncertainty of $OOD$ appropriate?
