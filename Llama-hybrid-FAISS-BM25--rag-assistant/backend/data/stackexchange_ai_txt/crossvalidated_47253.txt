[site]: crossvalidated
[post_id]: 47253
[parent_id]: 
[tags]: 
Questions on parametric and non-parametric bootstrap

I am reading the chapter on Frequent Statistics from Kevin Murphy's book " Machine Learning - A Probabilistic Perspective ". The section on bootstrap reads: The bootstrap is a simple Monte Carlo technique to approximate the sampling distribution. This is particularly useful in cases where the estimator is a complex function of the true parameters. The idea is simple. If we knew the true parameters $θ^∗$ , we could generate many (say $S$) fake datasets, each of size $N$, from the true distribution, $x_i^s \sim p (·| θ^∗ )$, for $s = 1 : S, i = 1 : N$. We could then compute our estimator from each sample, $\hat{\theta^s}=f (x^s_{1:N})$ and use the empirical distribution of the resulting samples as our estimate of the sampling distribution. Since $\theta$ is unknown, the idea of the parametric bootstrap is to generate the samples using $\hat{\theta}(D)$ instead. An alternative, called the non-parametric bootstrap , is to sample the $x^s_i$ (with replacement) from the original data $D$ , and then compute the induced distribution as before. Some methods for speeding up the bootstrap when applied to massive data sets are discussed in (Kleiner et al. 2011). 1 . The text says: If we knew the true parameters $\theta^*$ ... we could compute our estimator from each sample, $\hat{\theta^s}$... but why would I use the estimator of each sample if I already know the true parameters $\theta^*$? 2 . Also, what is the difference here between the empirical distribution and the sampling distribution? 3 . Finally, I don't quite understand the difference between parametric and non-parametric bootstrap from this text. They both infer $\theta$ from the set of observations $D$, but what is exactly the difference?
