[site]: datascience
[post_id]: 74116
[parent_id]: 74084
[tags]: 
It depends on how sophisticated you want the system to be: the most basic way is to compare the user answer with the gold answer using a simple string similarity measure, such as the overlap coefficient. Basically it just counts the words in common, and there would be a minimum threshold to count the answer as correct (e.g. 80% words in common). It's not very good because a small typo is enough to make the score wrong and it gives the same importance to every word. The same idea but with TF-IDF weights, typically with cosine similarity. This requires a corpus on which to calculate the IDF weights (which reflect the importance of the words in general). Still based on string similarity measures but more advanced: a hybrid similarity measure which combines character-level similarity between words (e.g. Jaro, Levenshtein edit distance) and similarity across words. Soft-TFIDF is a common example. Disadvantage: can be tricky to properly adapt to the task. Beyond that there are a lot of fancy options: using semantic similarity with WordNet (synonyms), words embeddings, etc. Note: fyi this is not related to the task called Question Answering, which is about a computer-generated answer to a question.
