[site]: crossvalidated
[post_id]: 358508
[parent_id]: 358102
[tags]: 
Firstly, you do not necessarily need to decide on a single model. Bayesian model averaging or a single model with some kind of sparsity-inducing prior on the extra parameter are often very good options for prediction modeling. Secondly, tying a decision about a model choice to any particular threshold for Bayes factors, DIC or AIC can be quite problematic - except in extreme cases, when it is completely clear that the simple model is too simple. Thus, approaches that allow for uncertainty in this respect (see "Firstly, ..."). Just because one does this in a Bayesian context does not make the problems of model selection with hypothesis tests/AIC/whatever go away. Finally, there are further options to look at how well a model is performing. These include posterior predictive checks, evaluation on a hold-out test set, leave-one-out cross-validation (which can be very easy to implement in MCMC samplers with a lot of useful discussion available on Andrew Gelman's blog ).
