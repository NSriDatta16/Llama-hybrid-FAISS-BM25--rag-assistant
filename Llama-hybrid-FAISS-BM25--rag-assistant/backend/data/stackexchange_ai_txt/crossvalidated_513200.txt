[site]: crossvalidated
[post_id]: 513200
[parent_id]: 
[tags]: 
Is CPU utilization a predictable time series?

I've been wondering whether metrics about CPU and resource utilization is a time series which can be predicted or rather a random walk which I cannot learn from. Can recognizing a pattern in the data or using some moving average be useful for prediction in this case? I have seen some publications suggesting that it is a problem which we can apply ARIMA model on but it seems not logical to me because there are so many factors that affect the CPU usage in a way that the usage history becomes useless. Would love to hear your opinion, thanks!
