[site]: crossvalidated
[post_id]: 300618
[parent_id]: 
[tags]: 
How to implement L2 regularization towards an arbitrary point in space?

Here is something I read in Ian Goodfellow's book Deep Learning . In the context of neural networks, "the L2 parameter norm penalty is commonly known as weight decay. This regularization strategy drives the weights closer to the origin [...]. More generally, we could regularize the parameters to be near any specific point in space" but it is far more common to regularize the model parameters toward zero. (Deep Learning, Goodfellow et al.) I'm just curious. I understand that by simply adding a regularizing term to our cost function, and that by minimizing this total cost $J$ we can influence the model's parameters to remain small : $$J(\boldsymbol{\Theta}, \boldsymbol{X}, \boldsymbol{y}) = L(\boldsymbol{\Theta}, \boldsymbol{X}, \boldsymbol{y}) + \lambda||\boldsymbol{w}||_{2}^{2}$$ But how would one implement a version of this regularization strategy that would lead the parameters towards any arbitrary point? (say we want the norm to tend towards 5)
