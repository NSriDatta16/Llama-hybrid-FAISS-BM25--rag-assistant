[site]: crossvalidated
[post_id]: 273003
[parent_id]: 272881
[tags]: 
The only obvious tool based on the constraints is some form of Bayesian logistic regression. The reason is that your constraints would define the prior and the likelihood. For example, by assuming $\partial{f}/\partial{x_1}$ is positive, you are assuming that there is a zero probability that the $\hat{\beta}\le{0}$, in the linear analog problem. The bounding assumes the likelihood is some sigmoid function and the easiest way to express this would be through some function $g(h(x_1,x_2))$ where $g$ is the logistic likelihood. This just requires you to solve the prior for the relationship between $x_1$ and $x_2$ through $h$, and the relationships among the partitions as a prior. Your “smoothness” requirement can be helped through the rather ugly dropping of the “parallel lines” assumption used in Frequentist logistic regression. To see what I mean, lets drop $x_1$ and focus on the relationship between $y$ and $x_2$. Under the parallel lines assumption $y$ is mediated through a single parameter, say $\beta$, which is constant for all values. This implies that there is a direct relationship between how people behave when inside a frozen block of water, in cold water, in 90 or 100 degree water and in boiling water. This is obviously not the case since a person frozen in a block of water would have no activity and a person in boiling water might have brief frenetic activity followed by no activity. This is not at all like the behavior at 80-100 degrees. When you drop the parallel lines assumption you could map $\hat{\beta}_1\dots\hat{\beta}_{10}$ in such a way that it varies in a smoothly increasing or decreasing way. It would, however, create a nightmare of a prior distribution. You would have to restrict the posterior so that, for example, $\beta_{n+1}-\beta{n}\le{k}_n;k_n>0$ for increasing functions to reach “smoothness.” This may be unnecessary, but you should plan for it. I am also assuming no interaction effect between $x_1$ and $x_2$ and that you somehow can construct a clean, proper prior. You would first have to construct this as a series of probability statements, but you may be able to simplify this if you do drop the parallel lines assumption by treating $x_2$ as an ordered partition. You would need a lot of data, because the partitioning would consume a lot of explanatory power. One other method that may preserve your smoothness requirements is to not treat $x_2$ as a $1-10$ variable, but as a ten bit variable, where each bit has its own constant that is added to or multiplied with $x_1$. For example, if $x_2=3$ then it is coded as $0000000100$ and where that is present then $c_3$ is put in the function as either $g(c_3+h(x_1))$ or $g(c_3h(x_1))$. It could even be both a multiplication and additive constant together inside $g$ but outside $h$ such as $g(c_3+k_3h(x_1))$. The sticking point will be the probability statements to show the relationship between the variables prior to collecting the data. You could use Bayesian model selection to test competing models. I had to use something like this in a problem because of the weird restrictions on the variables that I faced.
