[site]: datascience
[post_id]: 87637
[parent_id]: 
[tags]: 
What is the difference between GPT blocks and BERT blocks

Nowadays many applications only use the Encoder and Decoder part of the Transformer respectively. I am having trouble understanding the difference though. If GPT uses Decoder only and BERT uses Encoder only does this mean that the only difference between the two is basically in the masking part? The cross attention layer in the Decoder is omitted since there is no Encoder within GPT right?
