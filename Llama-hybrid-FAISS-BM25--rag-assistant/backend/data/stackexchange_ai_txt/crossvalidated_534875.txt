[site]: crossvalidated
[post_id]: 534875
[parent_id]: 534866
[tags]: 
Unless you think that interactions among the features are critical, you could consider a LASSO logistic regression as a way to do your feature selection. Figure 6.4 of Statistical Learning with Sparsity shows how to display feature-selection stability based on multiple models built on bootstrap resampling of data.* Boxplots of regression coefficients among the models (left of figure) and probabilities of feature inclusion (right of figure) provide the type of information you seem to be looking for. (Two-dimensional displays of co- or counter-inclusion might be nice, although maybe unwieldy with 1000 candidate predictors.) If you think that yet-unknown interactions among the features are critical, then boosted trees with some tree depth to allow for interactions make sense over LASSO (provided that you use something related to a proper scoring rule as your criterion). Tree models, including boosted trees, can provide a quantitative relative importance for all features in a model on a scale of 0 to 100 (e.g., Figure 10.6 of The Elements of Statistical Learning ). With multiple models based on resamples of the data, displays of the distributions of importance values for features among the models, analogous to those described in the first paragraph, would be helpful. For reassurance, you could compare the average feature importance among the multiple models against the feature importance seen in the model on the full data set. The downside is that just how the important features interact in a boosted model can be hard to glean. A warning. You say: the primary aim here is not about prediction, but rather to identify important variables which we can then target more directly in future analyses. What you mean by "future analyses" is critical here. If those are on independently acquired data, fine. If you are to identify "important variables" from a data set and then use them for "future analyses" on the same data, however, then there will necessarily be some over-optimism in that later work. *Your selection of a 60% subsample each time, without replacement, ends up close to the fraction of cases included in a standard bootstrap sample with replacement. There's an argument that bootstrap sampling better mimics original sampling from the population. I suspect that the difference might not matter much with this scale of data, but you might want to check.
