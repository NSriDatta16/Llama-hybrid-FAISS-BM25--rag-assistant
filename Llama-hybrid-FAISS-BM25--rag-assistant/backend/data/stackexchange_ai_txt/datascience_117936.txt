[site]: datascience
[post_id]: 117936
[parent_id]: 117922
[tags]: 
First "linear algebra, statistics, complicated optimization" are not ML theories, they are mathematics toolkits that guides specific ML algorithms designs and improvement at operation level. An ML theory typically refers to a theory that tries to explain why certain ML design pattern works, and what an efficient design pattern looks like for a general category of ML tasks. Here's is an example Information Bottleneck Theory for Deep Learning Do you need to master those mathematics toolkits? It depends on how unique your "industrial applications of AI" is. If your application is a well researched and developed use case with a ton of open source algorithms and pre-trained model parameters, eg common object detection, common object segmentation, univariate time series forecasting, then NO you don't need be a master of those math stuff. Though have some basic math intuition will help you avoid mis use those open source algorithms. If part or the entirety of your application includes some unique uses cases. Either there isn't any openly available algorithms, or the use case requires substantial modification of an open source algorithms, then YES you need to have enough math knowledge of those toolkits for the task. For example my team developed a ML system to segment each individual tooth from a facial picture with opened mouth and then measure the whiteness of each tooth. Segmentation is a common ML task, but we couldn't find any open source ML algorithms pre-trained for teeth. Nor could we find any algorithms to identify the teeth's relative coordinates in the dentist standard. RGB Color measurements for selected pixels is a trivial function in OpenCV, but inferring the true color from a picture adjusted for the background environmental color tone and different smartphone camera's auto white balancing is a none trivial ML task. Without knowing statistics, geometry, high dimensional matrix calculation, etc... We could never come up with the ML experiential design to specify: What type of supervised labelling data do we need to collect? How much of those data? Should we train an end or end model or train different models for each sub task? How do we design model features? Which metrics should we use to measure our progress? etc etc ...
