[site]: datascience
[post_id]: 97241
[parent_id]: 
[tags]: 
Why autoencoders use binary_crossentropy loss and not mean squared error?

Keras autoencoders examples: ( https://blog.keras.io/building-autoencoders-in-keras.html ) use binary_crossentropy (BCE) as loss function. Why they use binary_crossentropy (BCE) and not mse ? According to keras example, the input to the autoencoders is a normalized image (each pixel has values in range [0..1]) The output of the autoencoders is the same. (predicted normalize image) I read some articles which shows that BCE use to evaluate the loss when the target is fixed value (0 or 1) and not range of values [0..1]. It seems that using BCE is incorrect when the target is not 0/1. Am I right ?
