[site]: crossvalidated
[post_id]: 545978
[parent_id]: 545905
[tags]: 
There are problems with using PCA for this type of analysis. First, even if your gene-expression data come from the same general platform (e.g., a particular RNAseq method), issues like correcting for batch effects within and between cohorts need to be handled carefully, as noted by @ReneBt in a comment. Second, if the gene-expression data come from different technologies it might be difficult to translate PCA models reliably from one technology to another. For example, microarray and RNAseq gene-expression data have different characteristics in terms of dynamic range and mapping raw or partially processed data to individual genes. Even within each of those broad technology classes there can be differences in implementation that pose difficulties in cross-cohort comparisons. Those fundamental problems must be addressed before trying to use a model from one cohort on data from a different cohort as you propose. This isn't impossible, but it does take careful attention. This recent paper explains one approach, with some references to other approaches in the literature. If you can address those issues, then what needs to be done to proceed with PCA is straightforward. The original PCA needs to be done on centered and scaled data if you want to put all genes onto an equal basis. (It's typically best to omit genes with little variance between treatments/groups first.) The centering and scaling, however, might differ in the weights placed on the same gene between your reference set and the validation set. I suspect that some combination of that difference in centering/scaling and the technical issues noted above have led to the differences you found in results. As presumably it's the gene-expression values that fundamentally matter, it makes sense to express the coefficients from your reference-set model back into the scale of gene-expression values and apply those back-transformed coefficients on your validation set. That is, don't work with centered/scaled data for the validation set but work directly in the scale of gene-expression values. That leads to another problem with this type of PCA for gene expression. Although PCA diminishes the dimensionality of the model in an important sense, the model you get could in principle still contain coefficients for all 20,000 or so genes. Is that what you really want? How useful will such a model be? I suspect that for this type of comparison, particularly if your interest is in prediction and you want to extend your results to a prognostic or diagnostic test, you would be better off using a method like LASSO that selects a restricted subset of genes. Although LASSO makes somewhat arbitrary selections of single genes from among a group of genes with correlated expression, that often works well in practice. Gene-expression studies provide many of the examples of LASSO in textbooks. If you're not familiar with LASSO, it's outlined nicely in An Introduction to Statistical Learning and covered in great detail in Statistical Learning with Sparsity . Finally, think carefully about whether this separation into reference and validation sets makes the most efficient use of the data. Although separate groups seems to have an intuitive appeal, it often is suboptimal. A combined model including all available data with rigorous internal validation and evaluation of cohort-specific effects typically provides more power unless there are tens of thousands of cases . Quoting from Frank Harrell on that page, in a paragraph that seems applicable to your study: Many investigators have been told that they must do an “external” validation, and they split the data by time or geographical location. They are sometimes surprised that the model developed in one country or time does not validate in another. They should not be; this is an indirect way of saying there are time or country effects. Far better would be to learn about and estimate time and location effects by including them in a unified model. Then rigorous internal validation using the bootstrap, accounting for time and location all along the way. The end result is a model that is useful for prediction at times and locations that were at least somewhat represented in the original dataset, but without assuming that time and location effects are nil.
