[site]: crossvalidated
[post_id]: 616610
[parent_id]: 
[tags]: 
T test when there are different raters? I am not sure if my procedure is correct

I have two sets of translated text. The first set contains poor translations, and the second set includes perfect translations. I asked three evaluators to rate each of the translations. My objective here is twofold. First, I aim to demonstrate agreement among the evaluators using an Intraclass Correlation Coefficient (ICC) measurement. Second, I intend to use a T-test to highlight significant differences between the two sets of translations. I am familiar with how to perform ICC and T-tests. However, I'm uncertain about conducting a T-test when multiple raters are involved. My current thought process is to take the average rating from the three evaluators for each translation, and then use these average ratings to perform the T-test. The exampple below is my throught process. Group A - poor translations Translation 1: Rater 1 : 2, Rater 2 : 3, Rater 3 : 2 Translation 2: Rater 1 : 3, Rater 2 : 4, Rater 3 : 3 Translation 3: Rater 1 : 2, Rater 2 : 2, Rater 3 : 3 Group B - perfect translations Translation 1: Rater 1 : 8, Rater 2 : 7, Rater 3 : 9 Translation 2: Rater 1 : 9, Rater 2 : 8, Rater 3 : 8 Translation 3: Rater 1 : 7, Rater 2 : 8, Rater 3 : 8 First, we compute the average rating for each Translation in both groups: Group A Translation 1: Average = (2+3+2)/3 = 2.33 Translation 2: Average = (3+4+3)/3 = 3.33 Translation 3: Average = (2+2+3)/3 = 2.33 Group B Translation 1: Average = (8+7+9)/3 = 8 Translation 2: Average = (9+8+8)/3 = 8.33 Translation 3: Average = (7+8+8)/3 = 7.67 Now, I use the t-test to compare the means of the two groups. from scipy import stats # These are your means for each Translation groupA_means = [2.33, 3.33, 2.33] groupB_means = [8, 8.33, 7.67] t_stat, p_val = stats.ttest_ind(groupA_means, groupB_means) print('t-statistic:', t_stat) print('p-value:', p_val) ```
