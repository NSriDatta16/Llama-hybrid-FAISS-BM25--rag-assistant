[site]: crossvalidated
[post_id]: 433261
[parent_id]: 
[tags]: 
Mixing probabilities and probability densities

I'm currently working on a Bayesian network designed to find the probabilities for various lung diseases. In the network there are, among others, a normally distributed random variable (body temperature) causally dependent on a binary one. In order to reach a diagnosis MCMC sampling is used (Metropolis in Gibbs). When generating a sample, random variables are assigned new values to generate a candidate for a sample and the probabilities are compared with the unaltered variant. ( $P_{new}/P_{old}$ ). The new values are accepted with probability $min(1, (P_{new}/P_{old}))$ When this comparison is made I multiply the probabilities of each value (given the values of it's dependencies) to get the probability of the current set of variables. However, I'm also multiplying these probabilities with a probability density value from the normally distributed random variable (from dnorm(x, mean, std) in R, where x in this case is an observed temperature). I figured this would be OK since I'm doing the same for both $P_{new}$ and $P_{old}$ , but my results leads me to suspect otherwise. Is this an okay way to compare the proposed values to the old ones? If not, what am I getting wrong and what should I do to get it right? EDIT: Here comes an attempt to make the question clearer as per Xi'an's request. I'm comparing $p_{old} = P(X=x_{old}, Y=y)$ and $p_{new} = P(X=x_{new}, Y=y)$ where $X$ is categorically distributed and is $Y\thicksim N(\mu , \sigma)$ and casually dependent on $X$ . I want to see if $(p_{new}/p_{old})> 1$ . To do this I calculate: $$ P(X=x_{old}, Y=y) = P(X=x_{old})\cdot P(Y=y |X=x_{old}) $$ and $$ P(X=x_{new}, Y=y) = P(X=x_{new})\cdot P(Y=y |X=x_{new}) $$ I can easily look up the value of $P(X=x_{old})$ and $P(X=x_{new})$ in my trained network. However, because $Y$ is normally distributed I've used R's dnorm(y, mean, std) R, where y is the $y$ (body temperature) in $P(Y=y |X=x_{new})$ and where mean and std are the appropriately trained values from my historical data. I know that dnorm(y, mean, std) returns a probability density value $f(y)$ , so what I'm actually calculating is $$ P(X=x_{old}, Y=y) \propto P(X=x_{old})\cdot f_{x_{old}}(y) $$ and $$ P(X=x_{new}, Y=y) \propto P(X=x_{new})\cdot f_{x_{new}}(y) $$ I added $\propto$ in the equations to symbolize how I've though about it. Since I'm doing the same for both $p_{old}$ and $p_{new}$ when checking if $(p_{new}/p_{old})> 1$ I have been thinking this was OK. Now, I'm doubting if that's the case. If not, what should I do instead?
