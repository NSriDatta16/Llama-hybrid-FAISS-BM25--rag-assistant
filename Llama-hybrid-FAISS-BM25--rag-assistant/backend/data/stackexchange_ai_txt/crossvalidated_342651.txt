[site]: crossvalidated
[post_id]: 342651
[parent_id]: 
[tags]: 
Constrain a Neural Network to be monotonic?

Is there a way or theorem that allows me to constrain a function $N(x)$, where $N(x)$ is a feedforward mlp such that it will always be a monotonic function. The simplest way is of course to use a penalty method, but this does not 100% guarantee, I am looking to see if there is a function or transformation that can be applied to $N(x)$ that constrains its form to being monotonic. The only way I can currently think of is by making all weights $w_{i,j,k} >0$ and that the activation function $f(a)$ are monotonic and non-negative (using $f(a) = \frac{tanh(a)+1}{2}$) and having a zero bias.
