[site]: crossvalidated
[post_id]: 225372
[parent_id]: 108797
[tags]: 
This question is rather simple if you are familiar with Bayes estimators, since it is the directly conclusion of Bayes estimator. In the Bayesian approach, parameters are considered to be a quantity whose variation can be described by a probability distribution(or prior distribution). So, if we view the procedure of picking up as multinomial distribution, then we can solve the question in few steps. First, define $$m = |V|, n = \sum n_i$$ If we assume the prior distribution of $p_i$ is uniform distribution, we can calculate it's conditional probability distribution as $$p(p_1,p_2,...,p_m|n_1,n_2,...,n_m) = \frac{\Gamma(n+m)}{\prod\limits_{i=1}^{m}\Gamma(n_i+1)}\prod\limits_{i=1}^{m}p_i^{n_i}$$ we can find it's in fact Dirichlet distribution, and expectation of $p_i$ is $$ E[p_i] = \frac{n_i+1}{n+m} $$ A natural estimate for $p_i$ is the mean of the posterior distribution. So we can give the Bayes estimator of $p_i$ : $$ \hat p_i = E[p_i] $$ You can see we just draw the same conclusion as Laplace Smoothing.
