[site]: crossvalidated
[post_id]: 202952
[parent_id]: 
[tags]: 
k folds cross validation for parameter tuning and model evaluation

I am faced with a regression problem which I am addressing using a Random Forest Regressor. I would like to use k folds cross validation to tune the parameters and estimate the out-of-set (i.e. test set) root mean squared error. However, I am a little confused as to what the right order of operations is. To do this would I simply: 1) split my training data using a train/test split (ex:80/20) 2) run GridSearchCV on the 80 to tune the parameters 3) refit my parameter tuned model on the entire 80 set 4) run k folds on the 20 set to find my out-of-set RMSE for my parameter tuned model? If so, how does this approach differ from nested k folds cross validation in terms of the bias variance tradeoff ?
