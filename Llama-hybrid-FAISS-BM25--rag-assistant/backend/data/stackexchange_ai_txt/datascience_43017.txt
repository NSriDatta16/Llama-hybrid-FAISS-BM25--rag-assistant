[site]: datascience
[post_id]: 43017
[parent_id]: 26411
[tags]: 
Some algorithms have feature importance calculations integrated in their models. In addition to performing statistical checks using p-values and covariance matrices you can rank features by importance. With xgboost you can rank features like this (Full explanation on MachineLearningMastery) : In addition to checking feature importance linearly, with xgboost and other tree based algorithms you can also check if features can be important when they interact with other features. With this in mind you can rank only old features first and evaluate if new features are better than some of them and which new features don't provide enough information or even worse add noise. You can also check other approaches on Interpretable Machine Learning and Stats Stack Exchange
