[site]: crossvalidated
[post_id]: 325012
[parent_id]: 
[tags]: 
Can WAIC be used to compare Bayesian linear regression models with different likelihoods?

I would like to use WAIC to help with model selection, where the models are simple linear regressions with Bayesian inference, non-flat priors and MCMC estimation. I am currently considering two such linear models, both of which have the same dependent variables and (Normal) priors for the regression coefficients, but one has a Normal likelihood and the other has a Student-T likelihood (i.e. for robust regression). I appreciate that information criteria such as WAIC are not the be-all-and-end-all of the model selection process, but I was intending to use WAIC as part of this analysis. However, when I turned to Chapter 9 of ‘Statistical Rethinking’ by Richard McElreath, he says that, “… it is tempting to use information criteria to compare models with different likelihood functions… Unfortunately, WAIC (or any other information criterion) cannot sort it out. The problem is that deviance is part normalising constant. The constant affects the absolute magnitude of the deviance, but it doesn’t affect fit to data. Since information criteria are all based on deviance, their magnitude also depends on these constants. That is fine, so long as all of the models you compare use the same outcome distribution type… In that case, the constants subtract out when you compare models by their differences. But if the two models have different outcome distributions, the constants don’t subtract out and you can misled by a difference in AIC/DIC/WAIC” My problem, is that I cannot see or derive this result (these constants), from the definition of the deviance given in the same book, $$ D(q) = -2 \sum_{i} log(q_i) $$ where i indexes each observation and $q_{i}$ is just the likelihood of case i. Now, I appreciate that the deviance is intended as an approximation to the cross-entropy term in the Kullback-Leibler (KL) divergence between two distributions - e.g. p for the ’true’ distribution of the data and q for the distribution implied by my model, $$ D_{KL} = E_{p}[log(p)] - E[log(q)] $$ where the cross-entropy, and thereby the deviance, represent an attempt to measure the additional entropy introduced (or the information lost) by using distribution q to describe distribution p. I can see that we cannot know $E_{p}[log(p)]$, which is a constant for all models being compared, but that this constant term disappears when making relative comparisons for some estimate of $E[log(q)]$ - i.e. when making relative comparisons of the deviance. I would expect this to be true even when we are comparing models using different likelihoods, as p is the ‘true’ distribution of the data and not related to the likelihood of the models in any way - is this correct? So, in summary, can I use WAIC to compare models with different likelihood functions, and if not, why not?
