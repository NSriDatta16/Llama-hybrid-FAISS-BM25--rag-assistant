[site]: crossvalidated
[post_id]: 190960
[parent_id]: 188799
[tags]: 
The basic approach is correct. There are many algorithms that would perform that. Logistic regression may do, with the advantage of relatively calibrated probabilities, even with large datasets. If your dataset is large, non-vanilla SVMs would not scale well, and you may want to take a look at Factorization Machines. Dato Graphlab provide several good implementations that I have had good experience with, but they are not alone: https://dato.com/products/create/docs/graphlab.toolkits.recommender.html kNN has issues with sparse data (unless your reduce it somehow) and is not straightforward with regard to probabilities. Random Forests would also need probabilistic calibration, and scaling them to large datasets is not always easy.
