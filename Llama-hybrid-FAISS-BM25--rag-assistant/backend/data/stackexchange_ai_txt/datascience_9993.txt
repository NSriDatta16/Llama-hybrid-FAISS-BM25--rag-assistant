[site]: datascience
[post_id]: 9993
[parent_id]: 6391
[tags]: 
As David states in the comments if you want to interpret a model you likely want to explore something besides neural nets. That said it you want to intuitively understand the network plot it is best to think of it with respect to images (something neural networks are very good at). The left-most nodes (i.e. input nodes) are your raw data variables. The arrows in black (and associated numbers) are the weights which you can think of as how much that variable contributes to the next node. The blue lines are the bias weights. You can find the purpose of these weights in the excellent answer here . The middle nodes (i.e. anything between the input and output nodes) are your hidden nodes. This is where the image analogy helps. Each of these nodes constitute a component that the network is learning to recognize. For example a nose, mouth, or eye. This is not easily determined and is far more abstract when you are dealing with non-image data. The far-right (output node(s)) node is the final output of your neural network. Note that this all is omitting the activation function that would be applied at each layer of the network as well.
