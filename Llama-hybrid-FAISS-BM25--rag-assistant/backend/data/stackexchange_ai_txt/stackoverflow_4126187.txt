[site]: stackoverflow
[post_id]: 4126187
[parent_id]: 4122940
[tags]: 
Caching Intermediate Representations It's pretty normal to cache the intermediate representations created by slower components in your document processing pipeline. For example, if you needed dependency parse trees for all the sentences in each document, it would be pretty crazy to do anything except parsing the documents once and then reusing the results. Slow Tokenization However, I'm surprise that tokenization is really slow for you, since the stuff downstream from tokenization is usually the real bottleneck. What package are you using to do the tokenization? If you're using Python and you wrote your own tokenization code, you might want to try one of the tokenizers included in NLTK (e.g., TreebankWordTokenizer ). Another good tokenizer, albeit one that is not written in Python, is the PTBTokenizer included with the Stanford Parser and the Stanford CoreNLP end-to-end NLP pipeline.
