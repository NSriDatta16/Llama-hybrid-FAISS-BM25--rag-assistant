[site]: crossvalidated
[post_id]: 252891
[parent_id]: 252869
[tags]: 
When I re-arranged the variables or simply swapped the position of some variables, my NN performance has changed. Why is that so? Most neural networks are sensitive to the ordering of the input variables. For example, if you consider this neural network: You'll see that the output when $(x,y)= (1,2)$ isn't equal to the the output when $(x,y)= (2,1)$. Note that some neural networks aren't sensitive to the ordering of the input variables, for example: Related: Why is the cost function of a neural network non-convex? : (written by Abhinav , user contributions licensed under cc by-sa 3.0) If you permute the neurons in the hidden layer and do the same permutation on the weights of the adjacent layers then the loss doesn't change. Hence if there is a non-zero global minima as a function of weights, then it can't be unique since the permutation of weights gives another minima. Hence the function is not convex.
