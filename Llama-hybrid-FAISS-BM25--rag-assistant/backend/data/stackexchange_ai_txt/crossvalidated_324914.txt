[site]: crossvalidated
[post_id]: 324914
[parent_id]: 
[tags]: 
Value of the keep probability when calculating loss with dropout

I'm training a small neural network (2 hidden layers) to classify the mnist images, and want to apply dropout regularization before my output layer. My first question : is it worth applying dropout to such a small network? My network is doing relatively well (around 98% accuracy consistently) without dropout. My second question : If I do apply dropout, and would like to periodically check on the value of my loss function, should I measure my loss without the dropout (i.e. use a keep probability of 1.0)? I am using a keep probability of 0.2 for each training step.
