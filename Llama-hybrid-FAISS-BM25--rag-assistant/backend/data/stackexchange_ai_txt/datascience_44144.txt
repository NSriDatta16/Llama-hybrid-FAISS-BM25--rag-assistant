[site]: datascience
[post_id]: 44144
[parent_id]: 44060
[tags]: 
please use early stopping. We cannot just choose some epochs and some hyperparameters in the beginning, and then change (transform) the data and wait for to see what happens. If your statement that normalizing the input data usually ends up in a faster fitting model is true (which is expected), the model should overfit faster as well. You should monitor your learning and catch the best epoch where your model fits . You can do this as follows: Define early stopping: from keras.callbacks import EarlyStopping es = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=0, mode='auto',restore_best_weights=True) Then add it to your training as: model.fit(X_train, Y_train, validation_split=0.3, epochs=500, batch_size=10,callbacks=[es]) You can set the parameters of early stopping yourself. Early stopping allows you to stop where your model performance starts to decrease due to overfitting. By choosing restore_best_weights=True you also get to restore the model from the epoch with the best performance when stopping. It will shorten your time of training as well since it will probably stop at somewhere. For more detail on early stopping, have a look at: https://stackoverflow.com/questions/43906048/keras-early-stopping Also you can use Batch Normalization for normalizing not just the input but the mid tensors between the hidden layers of your neural net. model = Sequential() model.add(Dense(100, input_dim=45, kernel_initializer='normal', activation='relu')) model.add(BatchNormalization()) model.add(Dense(50, kernel_initializer='normal', activation='relu')) model.add(BatchNormalization()) model.add(Dense(1, kernel_initializer='normal', activation='sigmoid')) This will accelarete your fitting speed, whereas it will also have a slight regularization effect. For a deeper insight(BatchNorm Paper): https://arxiv.org/pdf/1502.03167.pdf Lastly, for answering the specific core of your question, you need to normalize your data, not standardize it in your context. That usually depends on the activation functions, ReLU works well with normalization . Do not forget to separate training and test sets before normalization; you do them at once as I observe. That's a fine way to cause data leakage. You can use the configuration below: from sklearn.preprocessing import Normalizer transformer = Normalizer().fit(x_train) x_train = transformer.transform(x_train) transformer = Normalizer().fit(x_test) x_test = transformer.transform(x_test) Hope I could help, good luck!
