[site]: crossvalidated
[post_id]: 551194
[parent_id]: 
[tags]: 
What is the best way to feed IMU data to CNN?

I took the Introduction to Embedded Machine Learning course, which is provided by Shawn Hymel, on Coursera. While talking about sensor fusion, he made the following statement for the following diagram: In this example, you might need to calculate the quaternions from the sensors first before feeding them to the model. Then the neural network can be used to make a decision or classification, about what to do with that absolute orientation. Then, I had a look at quaternions . There is nice video on youtube which explains, what they are, how they work. We basically use them to represent 3D-rotation. However, I am confused how can I apply this to my data (or should I?). I will be using Arduino Nano 33 BLE Sense to detect anomalies on robotic arm. An example data are: Accelerometer: [1.44, -0.02, 0.99] Gyroscope: [-1.40, 0.49, -0.67] Magnetometer: [60.21, 21.18, -61.55]. I will be feeding IMU data to 1D-CNN, as there are several research( 1 , 2 ) that shows 1D-CNN works well with time-series data. I am using TensorFlow Lite for Micro , hence there are limited operations available. There is no 1D-Convolution function, but we can simulate them via using 2D-Convolution function as stated by Pete Warden in GitHub discussion: we recommend using Conv2D with a 1xSize kernel shape instead of Conv1D. Initially, I was thinking to feed raw IMU data to per sensor per dimension to CNN, then combine the results, to predict each data point. But, I am confused after taking that course. How should I proceed to get best results?
