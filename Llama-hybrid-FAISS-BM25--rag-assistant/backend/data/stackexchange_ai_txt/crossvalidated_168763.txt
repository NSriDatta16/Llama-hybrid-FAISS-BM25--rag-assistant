[site]: crossvalidated
[post_id]: 168763
[parent_id]: 
[tags]: 
Comprehensive list of misnomers in machine learning

Are there any reference document(s) that give a comprehensive list of misnomers in machine learning? I would like to have a list and simple explanation if needs be that I could go through easily (vs. a full encyclopedia) to make sure I know all the jargon pitfalls. Examples: logistic regression : actually not regression, but classification (using the ML terminology). multilayer perceptron : the model actually comprises multiple layers of logistic regression models (with continuous nonlinearities) rather than multiple perceptrons (with discontinuous nonlinearities). (see Christopher M. Bishop's Pattern Recognition and Machine Learning, page 226). non-parametric model : there are actually parameters, but potentially an infinite number of them. The difference between parametric model and non-parametric model is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data. The non-parametric model is not none-parametric: parameters are determined by the training data, not the model (see Wikipedia ). E.g. Bayesian nonparametric models are not parameter free, but have an infinite number of parameters. I'm aware that there is some dose of subjectivity when declaring a term as a misnomer. I prefer to have higher recall and lower precision, to a reasonable extent.
