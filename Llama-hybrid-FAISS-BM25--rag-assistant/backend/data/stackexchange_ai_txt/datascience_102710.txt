[site]: datascience
[post_id]: 102710
[parent_id]: 102679
[tags]: 
The predictive power of a model is highly contingent on the data generating process and it is ex ante hard to tell what will work best (especially with limited information about the data as in this case). Lasso, Ridge, and Principal Component Analysis are often used to reduce dimensionality or to select features which is not a real issue in your case. Linear regression will work well but will likely not deliver good predictive power as it is a parametric technique. Often random forest or boosting will work well for several reasons. First, you do not need to care about model parameterization. Second, you do not need to care about possible issues with multicollinearity in the data (a possible issue in linear regression). Third, you can investigate the feature importance from this models to see which features work well (and remove features with little or no predictive power). My suggestion: Start with a linear regression as baseline model, then try random forest and/or boosting (e.g. using xgboost, lightgbm) and compare results. Maybe have a look at Lasso/Ridge as well. Use 5-fold CV to assess model performance. Maybe also see if feature generation helps, e.g. by running a random forest with only few terminal nodes (3-4 splits) on single generated features like $y = x_1 - x_2$ , $y = x_1 / x_2$ etc. to see which feature interactions might have high predictive power. Add those generated features which perform best to the overall model and see if the model's predictive power is better then before.
