[site]: crossvalidated
[post_id]: 505028
[parent_id]: 505027
[tags]: 
With the MSE loss, you are welcome to use any activation function in your last layer, including the sigmoid function. However, since you are using the sigmoid activation function anyway, then it doesn't hurt to try both the MSE loss and the BCE loss. This is because the BCE loss enables faster convergence . Tips on training the VAE Since you are implementing a VAE, I would suggest you turn off any biases in your linear and convolutional layers, as this will avoid collapsing your reconstructions to the mean value of the input images. This is discussed in footnote 1 in section 3.3 (page 5) in the paper titled Deep One-Class Classification by Lukas Ruff et al. Also, consider keeping your batch size small at the beginning of training, to around 8 images per batch. I've observed that a smaller batch size greatly helps training.
