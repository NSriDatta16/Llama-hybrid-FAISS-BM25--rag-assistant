[site]: crossvalidated
[post_id]: 30611
[parent_id]: 30607
[tags]: 
One possibility is that the effects may be going in different directions in each group and are cancelled out when you aggregate them . This is also related to how, when you leave out an important interaction term in a regression model, the main effects can be misleading. For example, suppose in group $\rm A$, the true relationship between the response $y_i$ and the predictor $x_i$ is: $$ E(y_i|x_i, {\rm Group \ A}) = 1 + x_i $$ and in group $\rm B$, $$ E(y_i|x_i, {\rm Group \ B}) = 1 - x_i $$ Suppose group membership is distributed so that $$P({\rm Group \ A}) = 1-P( {\rm Group \ B}) = p$$ Then, if you marginalize over the group membership and calculate $E(y_i|x_i)$ by Law of Total Expectation you get \begin{align*} E(y_i | x_i) = E( E(y_i|x_i,{\rm Group}) ) &= p(1+ x_i) + (1-p)(1-x_i) \\ &= p + px_i + 1 - x_i - p + px_i \\ &= 1 - x_i(2p-1) \end{align*} Therefore, if $p = 1/2$, $E(y_i | x_i) = 1$ and does not depend on $x_i$ at all. So, there is a relationship within both groups but, when you aggregate them, there is no relationship. In other words, for a randomly selected individual in the population, whose group membership we don't know, there will, on average, be no relationship between $x_i$ and $y_i$. But, within each group there is. Any example where the value of $p$ perfectly balances the effect sizes within each group would also lead to this result - this was just this toy example to make the calculations easy :) Note: With normal errors, significance of a linear regression coefficient is equivalent to significance of the Pearson's correlation, so this example highlights one explanation for what you're seeing.
