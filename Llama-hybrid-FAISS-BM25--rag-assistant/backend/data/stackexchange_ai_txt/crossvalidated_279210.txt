[site]: crossvalidated
[post_id]: 279210
[parent_id]: 
[tags]: 
Standardizing vs Normalizing data for Support Vector Machines

I am fairly new to the topic of Support Vector Machines, and despite my readings, I still encounter difficulties in understanding the following: why do some people address the need to normalize data between -1 and 1, for instance, as useful to improve SVM prediction accuracy while others prefer to standardize data to zero mean and unit variance? The two philophies lead to different results. I have been able to observe the phenomenon myself, but I can't figure out why. Here is an example. I've been using both libSVM and python with scikit Learn on the same pairs of datasets (a train set and a labelled testing set). I've used sklearn in python first, using an RBF kernel for the SVM. I ran a grid search and got the best parameters C and gamma. I've tested my data and saved prediction accuracies. Later, I've tried on the same data to use libSVM, with an RBF kernel, using the parameters computed with the python grid search. Now, they bear different prediction accuracies. The only difference I've been able to find is that python was used scaling data to zero mean and unit variance, while libSVM is scaled with its built-in function, that does not guarantee zero mean and unit variance (I've checked this). In conclusion, I've assumed this is the reason behind the difference in behaviour. Can you help me understanding why this is happening?
