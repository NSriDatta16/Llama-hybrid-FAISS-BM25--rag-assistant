[site]: datascience
[post_id]: 94947
[parent_id]: 94935
[tags]: 
All 3 algorithms are giving very similar results. And looking at evaluation sample size I think training sample is not too large. It indicates me that if there is any opportunity, it's a) in feature engineering b) in not predicting for less confident cases c) getting more data to train more complex algo a) Feature Engineering - TFIDF or count vectorizer has a real world problem of having test words outside of training vocabulary. If you can use general language vocabulary for embedding both train and test sets, then results should improve. There are open source pretrained embeddings like USE, Glove etc. to do so. b) Prediction Confidence - Along with class prediction you can also get probability of classification. Then check - below which cutoff of probability your F1 score gets too low. Don't predict for those low probability cases. Most practical systems accept limitation of AI. c) More data will allow you to learn through more complex algorithms like boosting which may improve results. Hopefully you are already crossvalidating. Also, based on the cost of error, you can decide whether FP or FN should be prioritized. Accordingly optimize precision/ recall.
