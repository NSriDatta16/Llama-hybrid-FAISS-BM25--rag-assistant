[site]: crossvalidated
[post_id]: 583100
[parent_id]: 583080
[tags]: 
In the models that you name the error term is i.i.d. by assumption . Indeed, all time series models that I can now remember assume the error (appropriately defined*) to be i.i.d. or at least uncorrelated. However, the model need not approximate the data generating process (DGP) perfectly. E.g. if the DGP is ARMA(10,10) but we approximate it with a more parsimonious ARMA(2,2), the resulting error will be autocorrelated (serially correlated) and not i.i.d. Even so the reduction in estimation variance by replacing ARMA(10,10) by ARMA(2,2) may be so great that it will more than offset model bias, so that our ARMA(2,2) would do better in prediction than an ARMA(10,10) with estimated (rather than actual) coefficients. (The actual coefficients are of course not accessible to us.) Another case could be regression with ARMA errors such as \begin{aligned} y_t &= \beta_0+\beta_1 x_t+u_t \\ u_t &= \varphi_1 u_{t-1} + \varepsilon_t + \theta_1\varepsilon_{t-1} \end{aligned} with $\varepsilon_t\sim\text{i.i.d.}$ Here you can explicitly see that $u_t$ is autocorrelated, and so using just $y_t = \beta_0+\beta_1 x_t+u_t$ and assuming $u_t\sim\text{i.i.d.}$ would violate the i.i.d. assumption. (It is easy to notice the problem with the i.i.d. assumption on $u_t$ when the model is explicitly stated with an ARMA equation for $u_t$ , but it took some time to figure this out in the 20. century when people were working with simpler models without explicit equations for the error term.) For a more in-depth presentation of the model and an example (US personal consumption vs. income), see Hyndman "The ARIMAX model muddle" and section 9.2 from Hyndman & Athanasopoulos "Forecasting: Principles and Practice". *E.g. a GARCH model assumes the standardized error to be i.i.d. while the raw error is conditionally heteroskedastic, though not autocorrelated.
