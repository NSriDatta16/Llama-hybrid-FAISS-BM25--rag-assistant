[site]: crossvalidated
[post_id]: 404671
[parent_id]: 404095
[tags]: 
Let's review the generative process assumed to generate data using GMM with infinite (i.e. non-fixed) number of clusters. First we need to choose a cluster assignment. Using a Chinese restaurant process, we assume that there are currently $K$ clusters, but we also have a probability to assign an observation to a new cluster $K+1$ . This way we do not need to fix $K$ a priori , $$ \begin{equation}\tag{1}\label{eqn1} P(z_i=k\mid \alpha) = \begin{cases} \frac{N_k}{N+\alpha-1} & , k \in [1, K]\\ \frac{\alpha}{N+\alpha-1} & , k = k_{new}=K+1\\ \end{cases} \end{equation} $$ Given a cluster assignment, we can generate an observation from the corresponding Gaussian with parameters: $\mathcal{N}(\mu_k, \Sigma_k)$ For clustering, we need to determine the probability of assigning an observation to a cluster, which can be done using Bayes rule as follows, $$\tag{2}\label{eqn2} P(\textbf{z} \mid \textbf{X}, \alpha, \beta) \propto P(\textbf{X} \mid \textbf{z}, \beta) \times P(\textbf{z} \mid \alpha) $$ If we are doing MCMC sampling, the above equation can be written as, $$\tag{3}\label{eqn3} P(z_i=k \mid \textbf{z}_{-i}, \textbf{X}, \alpha, \beta) \propto P(x_i \mid \textbf{X}_{-i}, z_i=k, \textbf{z}_{-i}, \beta) \times P(z_i=k \mid \textbf{z}_{-i}, \alpha) $$ Answering your questions: Is $P(\textbf{z} \mid \textbf{X}, \alpha, \beta) = P(\textbf{z} \mid \alpha)$ ? No they are not. $P(\textbf{z} \mid \textbf{X}, \alpha, \beta)$ is the posterior evaluated using equation $(\ref{eqn2})$ and you get $P(\textbf{z} \mid \alpha)$ by integrating out $\pi$ as follows: $$ P(\textbf{z} \mid \alpha) = \int_{\pi}P(\textbf{z} \mid \mathbb{\pi}) \: p(\mathbb{\pi} \mid \alpha) \: d\pi $$ How should I interpret the expressions $P(\textbf{z} \mid \textbf{X})$ and $P(\textbf{X} \mid \textbf{z})$ ? The former term is the posterior distribution $[$ i.e. which is (\ref{eqn2}) ignoring $\alpha$ and $\beta$ $]$ of assigning clusters to the data, the latter is the likelihood of the data. The posterior is calculated using the two terms: $P(z_i=k \mid \alpha)$ is given by equation $(\ref{eqn1})$ $P(x_i \mid z_i=k, \beta) \sim \mathcal{N}(\mu_k, \Sigma_k)$ ... then is it possible to make the claim that this empirical distribution over $\alpha$ has a regularising effect over the amount of clusters forming ... Your analysis is correct, since the choice of $\alpha$ governs how many clusters will be allocated and whether the model will have a tendency to favor existing clusters or generate more clusters. References: Gibbs sampling for fitting finite and infinite Gaussian mixture models - by Herman Kamper A tutorial on Bayesian nonparametric models - by Gershman, Blei
