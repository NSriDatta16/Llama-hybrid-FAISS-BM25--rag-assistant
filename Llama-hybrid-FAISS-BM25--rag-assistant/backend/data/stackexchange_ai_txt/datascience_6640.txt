[site]: datascience
[post_id]: 6640
[parent_id]: 6635
[tags]: 
While I am not aware of software specifically for tuning trained word embeddings, perhaps the following open source software might be helpful, if you can figure out what parts can be modified for the fine-tuning part (just an idea off the top of my head - I'm not too familiar with the details): GloVe: Global Vectors for Word Representation (part of Stanford NLP Group software); SENNA (its use for word embeddings is mentioned in this blog post ); Code on GitHub for Deep Learning-based word embeddings training; Neural Probabilistic Language Model Toolkit , mentioned above (also Deep Learning-based).
