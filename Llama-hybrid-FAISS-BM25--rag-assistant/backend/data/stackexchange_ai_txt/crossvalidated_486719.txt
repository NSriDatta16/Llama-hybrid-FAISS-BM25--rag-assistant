[site]: crossvalidated
[post_id]: 486719
[parent_id]: 339894
[tags]: 
I'm not 100% sure, but I would guess that this is more referring to normalization like BatchNorm rather than skip connections. It's not like ResNets will not explode without any normalization and not like plain VGG-style network will explode if you properly place BatchNorms. Skip connections, I guess, only help make the function smoother and the logic of the function that neural networks compute less convoluted, but it's pretty unrelated to exploding gradients problem. I found that having an activation function after, for example, BatchNorm, may also be crucial to prevent exploding gradients. Sometimes, when I didn't have it follow BatchNorm, or when I had it precede BatchNorm loss was blowing up.
