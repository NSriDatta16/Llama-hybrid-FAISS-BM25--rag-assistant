[site]: datascience
[post_id]: 96442
[parent_id]: 
[tags]: 
Data preprocessing - Time Series data resets its values - Detection & Correction

1. Summarize the problem I currently trying to work with time series data from sensors which has some problems regarding resetting it values. For example some cumulative values gets reset and don't add up the latest value. | timestamp | col_01 | col_02 | col_03 | |-----------|--------|--------|--------| | 123456 | 0 | 0 | 0 | | 123457 | 12,8 | 0,14 | 0 | | 123458 | 85,7 | 3,87 | 1,5 | | 123459 | 140,6 | 41,2 | 1,5 | | 123460 | 210,8 | 78,1 | 5,7 | | 123461 | 14,9 | 9,42 | 0,8 | 2. Provide details and any research The dataset consists of multiple GB of sensor data and checking every row for a reset and summing it up isn't a very efficient method and consumes a lot of time. 3. When appropriate, describe what youâ€™ve tried I tried using some outlier detection with the Z- & IQR-Score Method, but these approaches detect the wrong values. Mostly the points after the sensor data rested itself. What's a suitable approach to solve this problem? Using a time series cycle detection to get the timestamps of the resets? The next step after detecting these outliers would be to correct or remove them from the dataset.
