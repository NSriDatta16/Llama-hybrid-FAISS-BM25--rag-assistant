[site]: crossvalidated
[post_id]: 553790
[parent_id]: 553784
[tags]: 
Prior is something you choose prior to looking at your data. So if you want to use a Bayesian model to find the posterior for $p$ given your data, you cannot use the same data to choose prior because you would be using this data twice. First, it wouldn't make much sense. If you would be able to estimate the (prior) distribution for $p$ from the data, then your problem would be already solved, since your aim was to calculate the distribution of $p$ . Second, if you used the data to get the prior and then used it for Bayesian inference, you would end up with incorrect (too narrow) uncertainty estimates. Consider simple example of beta-binomial model . In $n$ trials you observed $k$ successes, this corresponds to the maximum likelihhod estimate for the probability of success equal to $k/n$ . Let's say that you used this data to construct a Beta prior for the probability of success $p$ . $$ p \sim \mathcal{Beta}(\alpha, \beta) $$ with pseudocounts $\alpha = k$ and $\beta = n - k$ corresponding to $E[p] = \alpha / (\alpha + \beta) = k/n$ . Given that beta distribution is a conjugate prior for binomial likelihood, we instantly know that the posterior distribution for $p$ is $$ p|k,n \sim \mathcal{Beta}(\alpha + k, \beta + n - k) $$ It is the same distribution as the prior, but counting each datapoint twice. With "two times more data" the posterior would be narrower than the prior, since "more data" would make it more precise. You would be cheating yourself. Prior should be based on what you know about the parameter before collecting your data. So it can be based on theory, some other experiments described in the literature, your wild guess, etc, but not the data.
