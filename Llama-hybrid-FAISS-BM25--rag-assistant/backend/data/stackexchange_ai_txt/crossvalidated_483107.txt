[site]: crossvalidated
[post_id]: 483107
[parent_id]: 482934
[tags]: 
A basis function is a feature mapping. What an SVM does differently is that it introduces kernel functions to more easily work with basis functions (feature mappings) A kernel is a shortcut to compute the inner-product between basis functions (feature mappings). What a basis function does is to transform the space in a nonlinear way so as to capture more information about the data. In the following example, note how $x$ lives in the real numbers, but I can find some transformation of $x$ that makes it live in $\mathbb{R}^2$ . That's all a basis function does. To understand the role of basis functions , imagine we have a basis function $\phi:\mathbb{R} \to \mathbb{R}^L$ . For any $x,y\in\mathbb{R}$ that you give me, in order to compute the inner product $\phi(x)^T\phi(y)$ , I need to first compute $2L$ new values ( $L$ for $\phi(x)$ and $L$ for $\phi(y)$ ), and then multiply their entries one by one, thus giving me their inner product. But, what happens if $L$ is big. In fact, what happens if $L = \infty$ ? (making abuse of notation here). In this case, it does not matter how much computational power we have, we will never be able to to compute their inner product. Here is where kernels come into play. What we would like is to have a way to compute $\phi(x)^T\phi(y)$ without the need to explicitly transform the space and then get the inner product. And we can actually do so! Denoting $$ K(x,y) = \phi(x)^T\phi(y) $$ as a kernel function, we can try to find a function that returns $\phi(x)^T\phi(y)$ without the explicit steps mentioned above. Coincidentally, in an SVM you don't really care about who is $\phi(x)$ (your basis function; feature mapping), but rather who is $\phi(x)^T\phi(y) = K(x, y)$ .
