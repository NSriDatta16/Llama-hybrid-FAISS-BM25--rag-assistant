[site]: crossvalidated
[post_id]: 130651
[parent_id]: 
[tags]: 
Compare convergence of optimization methods

I need to quantify how 2 optimization methods differ in convergence. When training a neural network I get the following plots, which show an error function after each gradient update. I think the green one is preferable, because its variance reduces as the learning proceeds. I would like to make more meaningful plot to show that. I was thinking about sort of moving skewness, could you please tell me if it is a good idea or I need something else than skewness? Thank you very much!
