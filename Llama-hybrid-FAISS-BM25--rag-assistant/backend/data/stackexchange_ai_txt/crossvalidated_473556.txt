[site]: crossvalidated
[post_id]: 473556
[parent_id]: 473455
[tags]: 
The central limit theorem states that the mean of the data will become normally distributed as the sample size increases, it says nothing about the data itself. Another way to put it is the distribution of the parameter (the mean) is normal, but that is entirely separate from the distribution of the underlying data . Most of the value from the CLT comes from the fact that you can compare samples that are not normally distributed to one another (based solely on the fact that, due to the CLT, you know how their means should behave). I think where this gets confusing is that just because you can compare two sample means to each other based on some test that assumes normality (eg. t-test) doesn't mean that you should . (ie comparing the means of two exponential distributions might not tell you what you think it does, or two bi-modal distributions, or a bi-modal with a uni-modal distribution, ect). The question most people should ask is, "is the mean (or a difference in means) a useful metric given the distribution of my data". Only if the answer to this question is yes, should one proceed to compare means (thus relying on the CLT). By not asking this question, many people fall into the following (roughly stated) logical fallacy: The CLT applies, so I can compare means. And I can compare means because they are normally distributed. This comparison must be meaningful, because the CLT says I can do it (and the CLT is very powerful). The comparison/test I am using most intuitively (/only) makes sense when the data is normally distributed, and after all, the mean is normally distributed, so my data must be normally distributed too! To directly answer the question, you can: Show them the definition, point out that the CLT only makes a claim about the distribution of the mean approaching normality, emphasize the distribution of a parameter can be very different from the distribution of the data from which it is derived. Show them this video which provides a nice visual representation of how the CLT works using several different distributions for the underlying data. (its a bit quirky, but communicated very clearly) Addendum: I glossed over some technical details in my explanation in order to make it more understandable to someone who is less familiar with statistics. Several commenters have pointed this out and so I thought I would include their feedback here: A more accurate statement of the CLT would be: " The central limit theorem states that the mean of the data will become normally distributed (more specifically the difference between the mean of the data/sample and the true mean, multiplied by the square root of the sample size $\sqrt{n}$ is normal distributed) " I have also seen this explained as " the properly normalized sum tends toward a normal distribution " It is also worth pointing out that the data must be composed of independent and identically distributed random variables with finite variance in order for the CLT to apply. A more accurate and/or less Bayesian way to say " the distribution of the parameter (mean) " would be " the distribution of the parameter estimate by the regular sample mean "
