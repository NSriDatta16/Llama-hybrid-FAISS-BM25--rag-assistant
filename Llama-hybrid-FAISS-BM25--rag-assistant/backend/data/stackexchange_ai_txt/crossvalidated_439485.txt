[site]: crossvalidated
[post_id]: 439485
[parent_id]: 
[tags]: 
How to evaluate whether model is overfitting or underfitting when using cross_val_score and GridSearchCV?

This is something that has been written about extensively, but I'm just confused about a couple of particular things which I haven't found a clear explanation of. When cross validation is not used, data can be split into train and test, and trained on the train set. The model can then be evaluated on both sets and the goal is to have similar performance on either set, which means neither over/under-fit. As far as I understand, when cross-validation is used, this removes the need to split into train and test sets, since CV effectively performs this split a number of times (defined by the number of folds). However, averaging scores you get from cross validation returns just a single score. Should this be interpreted as the train or the test score from the previous case? or neither? How can we tell if the model is overfit or underfit? I am wondering how this fits in with GridSearchCV , since I have read that you are supposed to split your data into a train and validation set to confirm that your performance metric remains approximately the same. Is this necessary since we can just assume the model is not over/under-fit since we allow GridSearchCV to choose the best hyperparameters? Furthermore, I have read something confusing in "Introduction to Machine Learning with Python" which says that data should be split into 3: train, val and test. The model is trained on the training set, and evaluated on the validation set in order to choose the best hyperparameters, and then taking the best hyperparameters is trained on train+val, and evaluated on test.
