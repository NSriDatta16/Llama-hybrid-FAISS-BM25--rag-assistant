[site]: crossvalidated
[post_id]: 203938
[parent_id]: 203913
[tags]: 
The prediction function $f(\mathbf{z})$ for an SVM model is exactly the signed distance of $\mathbf{z}$ to the separating hyperplane. The separating hyperplane itself is the geometric place $f(\mathbf{z}) = 0$. For a linear SVM, the separating hyperplane's normal vector $\mathbf{w}$ can be written in input space, and we get: $$f(\mathbf{z}) = \langle \mathbf{w}, \mathbf{z} \rangle + \rho = \mathbf{w}^T\mathbf{z} + \rho,$$ with $\rho$ the model's bias term. If a kernel function $\kappa(\mathbf{u},\mathbf{v})=\langle \varphi(\mathbf{u}), \varphi(\mathbf{v})\rangle$ is used, $\mathbf{w}$ typically can no longer be expressed in input space, but only in the space spanned by the embedding function $\varphi(\cdot)$. Then we obtain the following: $$\begin{align} f(\mathbf{z}) &= \langle \mathbf{w}, \varphi(\mathbf{z})\rangle + \rho = \mathbf{w}^T\varphi(\mathbf{z}) + \rho, \\ &= \sum_{i\in SV} y_i\alpha_i \kappa(\mathbf{x}_i,\mathbf{z}) + \rho, \end{align}$$ with $y$ the vector of labels, $\alpha$ the vector of support values, $\mathbf{x}$'s the support vectors.
