[site]: crossvalidated
[post_id]: 482547
[parent_id]: 
[tags]: 
Addressing multi-collinearity issues with subset selection methods

I'm a long-time lurker and first-time poster to this forum... I am currently working my way through an Introduction to Statistical Learning, and I have a question regarding the algorithms presented for best and stepwise subset selection. Am I correct in assuming that none of these algorithms check for multi-collinearity between the predictor variables? Would the next step in the process be to look at the variables selected by these methods and then confirm that there is no multi-collinearity between the variables each method selects? (I have experience in detecting multi-collinearity using the VIF factor) I don't have a ton of experience in data science, but I have taken several graduate level business courses with an analytics focus, and I am having trouble understanding where one would use best and stepwise subset selection. Should these only be used when there are a lot of independent variables to explore and we are unsure what independent variables have a strong relationship with the dependent variable? I am about to study the Lasso and Ridge regression sections of the book, and I think they may address the issue of multi-collinearity to where it isn't as much of a concern, but I wanted to make sure I was thinking about best and forward and backward stepwise selection the correct way.
