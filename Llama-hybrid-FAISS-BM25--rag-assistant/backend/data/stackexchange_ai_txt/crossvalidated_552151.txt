[site]: crossvalidated
[post_id]: 552151
[parent_id]: 552146
[tags]: 
Calibration is not related to going from a probability prediction to a hard classification. Even in a well-calibrated model, you might not want your threshold to be $0.5$ . Imagine something like fraud detection. I don't want my bank alerting me willy-nilly, but if there's a $45\%$ chance that a transaction is fraudulent, that's worth my attention, even though it probably isn't fraud. Calibration aims to investigate if the model is predicting the correct probabilities of class membership and correct the predictions if they are off. Sure, a model might come back with a probability prediction of $0.6$ , but if almost all (say $95\%$ ) of the probability predictions of $0.6$ turn out to belong to class $1$ , a probability of $0.6$ really means that there is a probability of $0.95$ of being in class $1$ . We would say that the model is miscalibrated. The magic of a function like rms::calibrate in R is that it can do calibration even when none of the probability predictions are the same. I confess that I have yet to figure out exactly what Harrell does in that function, however. A potential resource is the sklearn documentation on model calibration, and I think this is a good statistics writeup, even for people who don't program in Python or do but not in sklearn .
