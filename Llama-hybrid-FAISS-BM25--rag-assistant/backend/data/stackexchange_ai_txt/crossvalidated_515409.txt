[site]: crossvalidated
[post_id]: 515409
[parent_id]: 515402
[tags]: 
If by shuffling you mean changing the order of rows in a dataframe or changing the order in which the model is given training data, then this primarily matters for models that are trained using batches of data. A typical example is neural networks for images. A typical approach is to run a batch of say 32 images (when there might be 50,000 in total) through the model, update the parameters a little bit, run the next batch trough etc. until all images have been used. A complete run through all data batch-by-batch is then typically called a training epoch. In that context it is useful to vary the order of the training data, because this shakes up the training a bit and hopefully gets the final fit to a better (global) minimum for the loss function. However, the order of the data within a batch does not matter. In contrast, many other ("traditional") algorithms/models (e.g. logistic regression, linear models, mixed models, gradient boosted decision trees, random forest etc.) are typically fit to all data at once (and the only subsetting of the data is for regularization purposes like bagging/using a subset of features for some trees etc.). I.e. in each iteration of the training process a single parameter update (or a single tree is constructed) based on all data at once without using its order. In that case, the order the data is in does not matter. Even with these types of algorithms you can run into the situation that the data are so big that training needs to proceed in batches (e.g. not enough memory to run the algorithm with all data at once). In that case, shuffling what data is in what batch does make sense.
