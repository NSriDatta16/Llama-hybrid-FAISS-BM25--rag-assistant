[site]: crossvalidated
[post_id]: 620429
[parent_id]: 620421
[tags]: 
Your example function does not really have multiple inputs in the same sense as max-out functions have multiple inputs. Your example function is in essence just a scalar function ( $\mathbb{R} \to \mathbb{R}$ ). The max-out non-linearity, on the other hand, is actually a vector function ( $\mathbb{R}^N \to \mathbb{R}^{N/k}$ ). Take the following six-dimensional vector (i.e. $N = 6$ ) as an example input: $\boldsymbol{x} = (1, 3, -2, -4, -1, 3)$ . Applying the max-out non-linearity with $k = 2$ to $\boldsymbol{x}$ would result in $\operatorname{max-out}(\boldsymbol{x}) = (3, -2, 3) = \big(\max(1, 3), \max(-2, -4), \max(-1, 3)\big).$ Similarly, using max-out non-linearity with $k = 3$ , we would obtain $\operatorname{max-out}(\boldsymbol{x}) = (3, 3).$ If $k$ does not divide $N$ , this just does not work. The only thing to make things work is to either crop the input vector (ignore the last few outputs) or pad the input vector with $p$ zeros such that $k$ divides $N + p$ . Finally, the max-out non-linearity should not be thought of as a function that models something useful. It was conceived as an activation function for hidden representations in neural networks. Typically, these hidden representations are hard to interpret. Therefore, it is not really important that these activation functions are easily interpretable. The only thing we care about is that the non-linearity is effectively non-linear.
