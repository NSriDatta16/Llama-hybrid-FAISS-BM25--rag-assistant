[site]: crossvalidated
[post_id]: 429318
[parent_id]: 
[tags]: 
Minimization of the asymptotic variance in MCMC

Suppose $(X_n)_{n\in\mathbb N_0}$ is a Markov chain generated by the Metropolis-Hastings algorithm. Assume $(X_n)_{n\in\mathbb N_0}$ is stationary and consider the ergodic averages $$A_n:=\frac1n\sum_{i=0}^{n-1}f(X_i)$$ for some a priori fixed square-integrable $f$ . Assume our goal is to minimize the asymptotic variance $$\sigma^2:=\lim_{n\to\infty}n\operatorname{Var}[A_n].$$ Tierney's theorem (see, for example, [1, Theorem 4] , [2, Theorem 2.7] and [3, Lemma 2] ) gives us a guideline how the transition kernel $(X_n)_{n\in\mathbb N_0}$ influences $\sigma^2$ - either in terms of nonnegative operators (in [1] and [2]) or the inverse Laplacian (in [3]). However, [1] and [2] give a condition under which the asymptotic variance of every $f$ would be minimized and the corresponding minimization problem is complicated, since it depends on this additional parameter. So, my question is: Is there any easier condition on the transition kernel, depending solely on our fixed $f$ , which ensures that $\sigma^2$ is minimized? [3] might be such a condition, but it involves the computation of the inverse Laplacian (which has no easy to handle closed form; does it?).
