[site]: crossvalidated
[post_id]: 503693
[parent_id]: 439905
[tags]: 
Coefficient change Let some there be some data distributed according to a quadratic curve: $$y \sim \mathcal{N}(\mu = a+bx+cx^2, \sigma^2 = 10^{-3})$$ For instance with $x \sim \mathcal{U}(0,1)$ and $a=0.2$ , $b=0$ and $c=1$ . Then a linear curve and a polynomial curve will have very different coefficients for the linear term. set.seed(1) x Correlation The reason is that the variables/regressors $x$ and $x^2$ correlate. The coefficient estimates computed with a linear regression are not a simple correlation (perpendicular projection onto each regressor seperately ): $$\hat{\beta} \neq \alpha = \mathbf{X^t} y$$ (this would give coefficients $\alpha_1$ and $\alpha_2$ in the image below, and these coordinates/coefficients/correlations do not change when you add or remove other regressors) Using the correlation/projection $\mathbf{X^t}y$ is wrong, because if there is a correlation between the vectors in $\mathbf{X}$ , then there will be an overlap between some vectors. This part that overlaps will be redundant and added too much. The predicted value $\hat{y} = \alpha \mathbf{X}$ would be too large. For this reason there is a correction with a term $(\mathbf{X^t}\mathbf{X})^{-1}$ that accounts for the overlap/correlation between the regressors. This might be clear in the image below which stems from this question: Intuition behind $(X^TX)^{-1}$ in closed form of w in Linear Regression Intuitive view So the regressors $x$ and $x^2$ both correlate with the data $y$ and they both will be able to express the variation in the dependent data. But when we use them together then we are not gonna add them according to their single independent effects (according to correlation with $y$ ) because that would be too much. If we use both $x$ and $x^2$ in the regression then obviously the coefficient for the linear term $x$ should be very small since this is the same in the true relation. However, when we are not using the quadratic term $x^2$ in the regression (or otherwise add a bias to the coefficient for the quadratic term), then the coefficient for $x$ which correlates somewhat with $x^2$ will partly take correct this (take over) and... the value of the estimate for the coefficient of the linear term will change. Standard error change (and confidence intervals and p-values) The errors of the variables may be correlated leading to very large errors in some coefficient when they strongly correlate with others. The matrix $(X^TX)^{âˆ’1}$ describes this correlation. Error in the regression line The image below shows intuitively how this changes when adding other regressors. The intercept is the point where a regression line crosses $x=0$ . On the left the error of the intercept is the error of the mean of the population. On the right the error of the intercept is the error of the regression line intercept. Confidence regions for correlated parameters The next image displays the confidence regions (contrasting with confidence intervals) of the above regression in a 2-D plot. Here it takes into account the correlation between the parameters. The ellipse shows the confidence region which is a based on a multivariate distribution of the slope and intercept which may be related via a correlation matrix. For illustration an alternative type of region is also show. This is depicted by the box which is based on two single variate distributions assuming independence (now the confidence for the single variables is $\sqrt{0.95}$ ). By changing the model from $y = a + bx$ to a shifted model $y = a + b(x-35.5)$ we see that the correlation between the slope and intercept changes. Now the "intercept" coincides with the standard error of the line around the point $x=35.5$ which you see in the image above is smaller. #used model and data set.seed(1) xt See also: why does the same variable have a different slope when incorporated into a linear model with multiple x variables regression with multiple independent variables vs multiple regressions with one independent variable Why is the intercept in multiple regression changing when including/excluding regressors? Why and how does adding an interaction term affects the confidence interval of a main effect? Why is the intercept changing in a logistic regression when all predictors are standardized? Intuition behind $(X^TX)^{-1}$ in closed form of w in Linear Regression Does adding more variables into a multivariable regression change coefficients of existing variables? Estimating $b_1 x_1+b_2 x_2$ instead of $b_1 x_1+b_2 x_2+b_3x_3$ Why do regression coefficients change when excluding variables? Does the order of explanatory variables matter when calculating their regression coefficients? Intercept changing after adding an interaction
