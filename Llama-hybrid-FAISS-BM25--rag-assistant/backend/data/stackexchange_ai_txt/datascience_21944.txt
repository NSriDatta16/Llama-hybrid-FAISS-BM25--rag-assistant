[site]: datascience
[post_id]: 21944
[parent_id]: 21828
[tags]: 
It's hard to give a very good answer to such a broad question. Model interpretability is a big topic, and usually depends on the model. Simpler models, such as logistic regressions, are easier to interpret than neural networks. It's easy to say things like "if I can increase feature $X_i$ by 1, then the odds of $y$ happening will increase by $\beta_i$". Likewise, individual decision trees are easier to interpret than random forests. Yet, some people try to interpret random forests by computing "feature importance", which can be computed several ways, one of which is the number of splits that include the feature relative to the number of samples it splits. You want a way to treat your model as a black box and be able to interpret any model? I can think of two ways: Manipulate your inputs and see what happens to the ouput, using your sample One typical way is to change the input and see the impact on the model performance. In images, you can black out parts of the image, and see which parts contributes most to the accuracy. This is widely used for convolutional neural networks, which are hard to interpret otherwise. For numerical variables, you can zero out or add some noise to each feature and see what the impact of that individual feature has on the result. I have seen these kind of things widely used. Train with and without the feature Similar to the previous one, except you train with and without the feature, and see the impact it has on the model accuracy. This has the added benefit that you don't have to think about hyperparameters such as how much noise you'll add to each feature like the previous approach. Furthermore, you can better understand the impact of several features in the output by trying with and without those. I haven't seen this approach being used, but apparently it's also being used, as another person replied. Anyway, I would avoid such hacks. Most models can be made somehow interpretable. I have seen people even making recurrent neural networks interpretable. And, if interpretability is a concern, just use a simpler model that is easier to interpret and explain.
