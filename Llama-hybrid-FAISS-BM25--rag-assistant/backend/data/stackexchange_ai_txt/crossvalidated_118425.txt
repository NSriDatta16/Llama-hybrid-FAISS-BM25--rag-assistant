[site]: crossvalidated
[post_id]: 118425
[parent_id]: 
[tags]: 
Using 2 sample T test in time series data

I have two data series (not stationary) and I would like to see if the mean of series 1 is significantly different when a certain condition (on the other series) is met. The theory is that when series 2 reaches a value greater than 100, the value of series 1 declines. I've done the following: I've broken the data down into chunks, where each chunk represents a time period during which all the data in series 2 were either below or above 100. I then compare the means of series 1 in each chunk with the mean in the next chunk using the two sample T test to see if the mean of series 1 is lower when series 2 is greater than 100, and higher when series 2 is less than 100. For example: Obs Series1 Series2 1 0.05 50 2 0.03 80 3 -0.4 30 4 0.1 110 5 0.03 105 6 0.12 90 7 -0.3 92 8 0.11 100 9 0.2 120 The first chunk would be the first 3 observations (all less than 100, and the second chunk the next 2 (both greater than 100). I would compare the means of series 1 in these two chunks to one another to see if they are different using the two sample t test. I would then compare the mean of series 1 from observations 6&7 with the mean from observations 8&9. Then, if I find that in the majority of the chunks the means are significantly different, I can conclude that series 1 is significantly lower when series 2 exceeds 100. Does this methodology even make any sense? I'm sure there is a much better way, if anyone has suggestions for me, I'm just fairly new to this kind of analysis. Thanks Mike
