[site]: crossvalidated
[post_id]: 444465
[parent_id]: 318882
[tags]: 
This means that embedding of all words are averaged, and thus we get a 1D vector of features corresponding to each tweet. This data format is what typical machine learning models expect, so in a sense it is convenient. However, this should be done very carefully because averaging does not take care of word order. For example: our president is a good leader he will not fail our president is not a good leader he will fail Have the same words, and therefore will have same average word embedding, but the tweets have very different meaning. Edit: To address this issue, one might look into: Sentence-BERT https://github.com/UKPLab/sentence-transformers and https://arxiv.org/abs/1908.10084 Here is quick illustration with the above example: from sentence_transformers import SentenceTransformer import numpy as np def cosine_similarity(sentence_embeddings, ind_a, ind_b): s = sentence_embeddings return np.dot(s[ind_a], s[ind_b]) / (np.linalg.norm(s[ind_a]) * np.linalg.norm(s[ind_b])) model = SentenceTransformer('bert-base-nli-mean-tokens') s0 = "our president is a good leader he will not fail" s1 = "our president is not a good leader he will fail" s2 = "our president is a good leader" s3 = "our president will succeed" sentences = [s0, s1, s2, s3] sentence_embeddings = model.encode(sentences) s = sentence_embeddings print(f"{s0} {s1}: {cosine_similarity(sentence_embeddings, 0, 1)}") print(f"{s0} {s2}: {cosine_similarity(sentence_embeddings, 0, 2)}") print(f"{s0} {s3}: {cosine_similarity(sentence_embeddings, 0, 3)}") Result: our president is a good leader he will not fail our president is not a good leader he will fail: 0.46340954303741455 our president is a good leader he will not fail our president is a good leader: 0.8822922110557556 our president is a good leader he will not fail our president will succeed: 0.7640182971954346
