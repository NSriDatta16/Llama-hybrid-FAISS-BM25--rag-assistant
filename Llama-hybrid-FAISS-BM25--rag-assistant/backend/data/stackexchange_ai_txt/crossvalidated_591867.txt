[site]: crossvalidated
[post_id]: 591867
[parent_id]: 591353
[tags]: 
What is Probability Theory? Probability theory is the theory of plausible conclusions. Since time immemorial, humans resort to plausible reasoning: a cop concluding a man in mask coming out of a broken window of a jewelry shop with a rag full of jewelries is a thief is not deductively reasoning but rather plausibly reasoning. More formally, probability theory is a system of unique, consistent rules necessary for plausible reasoning and inference: it's an extension of Aristotelian logic (cf. $\rm [I], $ section $\rm Preface, $ p. $\rm xxii;$ appendix $\rm A. 1,$ p. $652$ ). The rules emanate from common sense and the requirement that the the system has to be consistent. So, what are the desiderata needed for developing a formal system for conducting plausible reasoning? Well, they ought be the most primitive and universal: $\rm( D. I.) $ Degrees of plausibility to be represented by real numbers. $\rm (D. II.) $ Qualitative correspondence with common sense. $\rm (D. III.) $ Consistency. $\rm (D. I.) $ is obvious. $\rm (D. II.) $ says that if for a proposition $\rm A, $ updating from old information to new one $C\to C^\prime$ increases its plausibility $$(A|C^\prime)> (A|C) \tag 1$$ but the plausibility of $B$ given $A$ hasn't altered, $$ (B|AC^\prime) = (B|AC), \tag 2$$ then qualitatively $$ (AB|C^\prime) \geq (AB|C). \tag 3$$ $\rm(D.III.) $ simply dictates from the fact that if a conclusion can be drawn in more than one way, then all must lead to the same result (cf. $\rm [I], $ section $1.7, $ pp. $17-19$ ). There is one additional postulate of continuity: an infinitesimal change in $(A|C)$ would only lead to infinitesimal change in $(AB|C)$ and $\left(A^\complement|C\right), $ propositions formed by the actions of $\rm AND, ~NOT.$ Product Rule What should be the plausibility of logical product $AB$ given information $C? $ $\rm (D.II) $ tells that it should depend only on $(B|C) $ and $(A|BC) ;$ dependencies on any other form would violate $\rm (D. II.) :$ $A$ and $B$ might be very plausible given $C$ and yet $AB$ might be very plausible or implausible, and so it cannot depend on $(A|C) $ and $(B|C). $ Thus, there exists a relationship $$(AB|C) = F[(B|C),~ (A|BC) ].\tag 4\label 4$$ What could $F$ be? Based on $\rm (D.II.) $ and axiom of continuity, $F(\cdot~, ~\cdot) $ needs to be continuous monotonic increasing function of both arguments. For simplicity, assume differentiability of $F$ both w.r.t. both arguments. But this doesn't narrow down the possibilities of what $F$ can be. Let $\rm (D. III.) $ be invoked for imposing structural consistency: consider the plausibility of $(ABC|D) .$ Therefore, taking $(BC) $ as a single proposition and using $\eqref 4,$ \begin{align}(ABC|D) &= F[(BC|D), ~(A|BCD) ]\\ &= F\{F[(C|D), ~(B|CD)],~(A|BCD) \}.\tag{5.a}\label{5.a}\end{align} Again, taking $(AB) $ as a single proposition \begin{align}(ABC|D) &= F[(C|D), ~(AB|CD) ]\\ &= F\{(C|D), F[(B|CD),~(A|BCD) ]\}.\tag{5.b}\label{5.b}\end{align} Fir consistency to hold, $\eqref{5.a}$ and $\eqref{5.b}$ must be equal. That is $$ F[\underbrace{F(x, ~y)}_{:=~u}, ~z]= F[x, ~\underbrace{F(y, ~z)}_{:= ~v} ].\tag 6\label 6$$ Letting $F_i$ denoting differentiating $F$ with respect to the $i$ -th argument, differentiating $\eqref{6}$ with respect to $x$ and $y$ \begin{align}F_1(x,~v)&= F_1(u, ~z)F_1(x,~y)\tag{7.a}\label{7.a}\\F_2(x,~v)F_1(y,~z)&= F_1(u, ~z)F_2(x,~y);\tag{7.b}\label{7.b}\end{align} Dividing $\eqref{7.b}$ by $\eqref{7.a}$ and denoting $F_2(x, ~y) /F_1(x, ~y) := G(x, ~y), $ yields \begin{align}G(x, ~v) F_1(y, ~z) &= G(x, ~y) ,\tag{8.a}\label{8.a}\\ \implies G(x, ~v) F_2(y, ~z) &= G(x, ~y) G(y, ~z). \tag{8.b}\label{8.b}\end{align} RHS of $\eqref{8.a}$ must be independent of $z.$ Also, partial derivatives of LHS of $\eqref{8.b}$ w.r.t. $y$ and that of $\eqref{8.a}$ w.r.t. $z$ are equal, so $G(x, ~y) G(y, ~z)$ is independent of $y.$ This implies the generic form of $G(\cdot~, ~\cdot) $ would be $$G(x, ~y) = c\times \frac{H(x) }{H(y)}\tag 9\label 9$$ where $c$ is a constant and $H(\cdot) $ is arbitrary; however the monotonicity of $F$ would imply $c> 0$ and $H$ not changing sign in the domain of interest. Substituting $\eqref 9$ in $\eqref{8.a},~\eqref{8.b}$ leads to the differential equation: $$\frac{\mathrm dv}{H(v) }= \frac{\mathrm dy}{H(y) }+c\frac{\mathrm dz}{H(z) };\tag{10}\label{10}$$ defining $w(x) := \exp\left\{\displaystyle\int^x \frac{\mathrm dx}{H(x) }\right\},$ (so, $w$ is positive,continuous and monotonous) $\eqref{10}$ yields \begin{align}w(x)w^c(y)[w(z)]^{c^2} = w(x) w^c(y)w^c(z),\tag{11}\end{align} whence a non-trivial solution can be obtained only if $c=1$ which yields \begin{align} w[F(x, ~y) ] &= w(x) w(y) \\ \implies F(x, ~y) &= w^{-1}[w(x)w(y)].\tag{12}\end{align} Finally, since logical product is commutative and associative, $$ w(AB|C) = w(A|BC) w(B|C) = w(B|AC) w(A|C). \tag{PR}\label{13}$$ Now, consider that $A$ is certain given information $C.$ Then given $C,$ $AB$ and $B$ are same which means since their truth values are same, \begin{align} AB|C &= B|C\tag{14.1}\\ A|BC &= A|C\tag{14.2}\end{align} Therefore $\eqref{13}$ becomes $$w(B|C) = w(A|C) w(B|C) \tag{15}\label{15}$$ whence $$\textrm{certainty}\equiv 1.\tag{16}$$ As remarked earlier, $w$ is monotonic, but whether it is increasing or decreasing in nature, depends solely on the form of $H.$ Suppose now $A$ is impossible given $C.$ Then $AB$ is also impossible, so \begin{align} AB|C &= A|C\tag{17.1}\\ A|BC &= A|C\tag{17.2}\end{align} $\eqref{13}$ becomes $$w(A|C) = w(A|C) w(B|C) \tag{18}$$ This could only mean $$\textrm{impossibility}\equiv 0 ~\textrm{or}~+\infty.\tag{19}$$ Thus, if $w(\cdot) $ is monotonic increasing, then its range would be from zero for impossibility to one for certainty. If it is decreasing, it ranges from one for certainty to infinity for impossibility. However, the latter case is special; for then one can easily take the reciprocal of $w$ and that would bring back the case to the former one. So, $$\textrm{convention}: \textrm{choose}~w(\cdot) ~:~ 0\leq w(x) \leq 1.\tag{20}$$ This is consistent and in agreement with all the desiderata (cf. $\rm [I], $ section $2.1, $ pp. $24-30$ ). Kolmogorov Axioms Kolmogorov's axiomatic approach is based on set theory, and as Jaynes showed (cf. $\rm [I],$ appendix $\rm A. 1,$ pp. $651-653$ ), albeit apparently seeming to be unrelated, the Kolmogorovian axioms are but the consequences of the logical extension developed by Jaynes as outlined above in the discussion. Specifically, those axioms aren't arbitrary but rather were meant to pass the test of consistency, again as elucidated above. Coming to the bone of contention, that is normalization: is it really necessary? Is it just an arbitrary convention? As Jaynes put (emphasis mine): [...] but $\eqref{15}$ shows that if certainty is not represented by $p=1, $ then we must restate the sum and product rules, or we shall have inconsistency. ... The substantive result is not that one is obliged to use any particular scale; but rather that a theory of probability whose content differs from one in which there is a single scale that is normalized, non-negative, and additive, will contain inconsistencies. Reference: $\rm [I]$ Probability Theory: The Logic of Science, E. T. Jaynes, Cambridge University Press, $2003.$
