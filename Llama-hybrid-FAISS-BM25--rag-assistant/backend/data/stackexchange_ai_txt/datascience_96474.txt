[site]: datascience
[post_id]: 96474
[parent_id]: 96470
[tags]: 
That's a lot of questions you're asking there. I'll try to answer one question after another. Keep in mind this is only my point of view and some people may disagree with my answers. 1. Train, test and validation datasets First, I'll try to explain how I see the 3-splits of a dataset : train, test and validation sets. Train set is the one used to train the model, the loss function is computed with this set, and then the model improves via gradient descent. This is the set where the model will usually give the best performances. Test set is the set used to test the model and compute the metric (in your example, the metric is accuracy). This set is used to find the good architecture and hyperparameters of your model. Validation set is less common because it is only used at the very end of your work. When you have trained all your models, found the best hyperparameters and architecture, you finally just try your model on the validation dataset and see if it works properly or not. This set is only used to 'Validate' the fact that your model is working as expected. This is why we usually don't use consider it, and sklearn only advise you to use train and test sets. To give you an idea about how to split the datasets, we usually go for a split with : 80-90% on training set 15-5% on testing set on validation set The test size you chose in your code (30%) seems quite high and I would reduce it to 15 or 10%. I would not use a validation dataset as if your model works on your train and test sets, it is very likely to work on any other set. 2.About dimensionality reduction We usually do not use dimensionality reduction when training models because we usually want the model to have the best accuracy possible and computing time is not important in most cases(not always). The question you should ask yourself is 'Do i need my model to run quickly ?' : If you need it to run quickly, then using PCA or any dimensionality reduction technique is smart and will help reduce computation time. If run time is not an issue, then do not bother using dimensionality reduction technique as they are likely to decrease your model performances . I don't know much about all the algorithms used to decrease dimensions, so if I had to reduce dimensions, I would use PCA whatever my classifier is. 3.Classification algorithms They are some other algorithms you can use on your data to classify them. Here are the ones I would try (from least favorite to favorite one) : K-Nearest Neighbours Perceptron SVM Random Forest Multilayer perceptron Sorry for the long answer, I hope this helps.
