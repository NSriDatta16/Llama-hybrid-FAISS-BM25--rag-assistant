[site]: datascience
[post_id]: 14125
[parent_id]: 
[tags]: 
Storing large data sets for python machine learning algorithm consumption

I'm reading up on how to clean/munge/wrangle data sets in order to run machine learning algorithms on them. Lots of info on how to do the actual wrangling, but a practical detail seems to be glossed over: storage. My question is quite simple: which is the go-to technology to store/retrieve a large data set in order to run algorithms on it in the most convenient/efficient way possible? I'm guessing the language in which the algorithms are written is not all that relevant here.
