[site]: datascience
[post_id]: 74626
[parent_id]: 57157
[tags]: 
There are some options to run stuff on the GPU without having the user install CUDA: If the user has the nvidia driver installed, you could bundle the CUDA libraries with your application (along with any other indirect dependency, maybe cublas and stuff like that). This is what some deep learning libraries like PyTorch do. You could use tensorflow.js, which runs on the GPU via WebGL. According to their web site , running via WebGL can be 100x running on CPU. Use the driver nvidia driver API directly without CUDA. The decision depends on many factors (expertise of the team, deadlines, etc). Without any knowledge on those factors, I would recommend bundling the CUDA libraries, assuming that the GPUs are nvidia and that you meet the CUDA license . I would not go for docker. By default docker containers cannot access the underlying GPU. You need to install and configure the docker nvidia runtime and specify it when running the image. If your users cannot install CUDA, its probable they won't be able to deal with this either.
