[site]: crossvalidated
[post_id]: 298996
[parent_id]: 298147
[tags]: 
I have two very general ideas about what might be the reason based on the fact that it looks as though you're using Python's scikit-learn library: You say that you have lots of categorical predictors. Gaussian naive Bayes models these (and everything else) as following a normal distribution. But we know they are zeroes and ones, so that's not really a sensible model, hence you're probably throwing away some useful information. For some unfathomable reason the logistic regression model in sklearn is always regularized and there is no sane way to turn that off. This is of course hidden to the user unless she looks for it. Since you have about 100 predictors, the regularization may very well help shrink away some noise. I have used your code example and the provided data to test assertion 2 above, and it turns out that regularization isn't the source of the improved performance of logistic regression. My code is as follows: import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import AdaBoostClassifier from sklearn.model_selection import cross_val_score # load data df = pd.read_csv('census.csv', header=0, sep=',') response = len(df.columns) - 1 X = pd.get_dummies(df.iloc[:, :response]) Y = df.iloc[:, response] classifiers = [LogisticRegression(penalty='l2'), GaussianNB(), LogisticRegression(penalty='l2', C=5e10)] names = ["logistic", "naive bayes", "logistic, no regularization"] for i in range(len(names)): scores = cross_val_score(classifiers[i], X, Y, cv=10) print("Accuracy (%s): %0.2f (+/- %0.2f)" % (names[i], scores.mean(), scores.std() * 2)) As mentioned, the only way to not have regularization in sklearn s perplexing logistic regression is to use the regularization strength parameter. Running my code gives: Accuracy (logistic): 0.85 (+/- 0.01) Accuracy (naive bayes): 0.80 (+/- 0.01) Accuracy (logistic, no regularization): 0.85 (+/- 0.01) Hence my best guess is point 1 above that logistic regression makes better use of the 0/1 indicator variables and that you're throwing away useful information by modeling them as normal. You might experiment with combining GaussianNB() with BernoulliNB() , which is a better model for 0/1 variables.
