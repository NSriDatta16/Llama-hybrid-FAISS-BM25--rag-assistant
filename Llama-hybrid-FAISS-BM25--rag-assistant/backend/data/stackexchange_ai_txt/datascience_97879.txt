[site]: datascience
[post_id]: 97879
[parent_id]: 97867
[tags]: 
One way to understand how ELMo's character convolutions work is by directly inspecting the source code . There, in the forward method , you can see that the input to the network is a tensor of dimensions (batch_size, sequence_length, 50) , where 50 is the maximum number of characters per word. Therefore, before passing the text to the network, it is segmented in words, and each character is encoded as an integer value. This is what happens in the forward method before the highway layers: The tensor gets prepended and appended sentence boundary tokens (beginning-of-sentence (BOS), end-of-sentence (EOS)). The tensor goes through an embedding layer (this is somewhat similar to one-hot encoding and a matrix multiplication, see this answer ). This gets us a vector for each character. The tensor goes through different 1D convolutions of configurable kernel sizes. The resulting activation maps are concatenated and passed as input to the highway networks This architecture was proposed by kim et al. (2015) , and is summarized well in one of the figures of the paper:
