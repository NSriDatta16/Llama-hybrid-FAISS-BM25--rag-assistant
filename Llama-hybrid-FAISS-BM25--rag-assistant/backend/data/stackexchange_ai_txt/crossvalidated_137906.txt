[site]: crossvalidated
[post_id]: 137906
[parent_id]: 
[tags]: 
Bayesian regression with independent variable drawn from distribution

I'm trying to set up a bayesian regression of the form $y_i \sim f(\beta_0 + \beta_1 x_i)$ but rather than $x_i$ fixed, they themselves are drawn from a distribution of (known) mean $x_i \sim g(\mu_i)$. $f$ and $g$ could be any distribution suitable for regression, but will probably be poisson or negative binomial. Is this type of regression possible, or is the problem under-defined? I've tried a few MCMC runs but they typically fail to converge. I've so far only tried metropolis-hastings as the conditional distribution of $x_i$ given the data and $\beta$ doesn't seem to have an analytic solution for gibbs sampling. Any tips / pointers to discussions appreciated. Edit for clarity: Both $y_i$ and a set $x_i^{(1)},...,x_i^{(n)}$ are observed. $\{x_i\}$ are then used to fit $\mu_i$ (which could be performed in the same MCMC run) and then random samples $z_i$ are drawn from $g(\mu_i)$ to construct the mean $\beta_0 + \beta_1 z_i$ for the inference of $\beta$. This (slightly strange) modelling choice is chosen to get a full posterior on $\beta$ that takes into account the "true" $z$ generating the mean is hugely variable and that the $x$ observations are unreliable.
