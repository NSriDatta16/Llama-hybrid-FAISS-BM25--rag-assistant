[site]: crossvalidated
[post_id]: 286026
[parent_id]: 286016
[tags]: 
In practice, a lot of packages take care of the optimization and most of the math details for you. For example, TensorFlow can do backprop+stochastic gradient descent for training neural nets for you automatically (you just have to specify learning rate). scikit-learn's ML tools will generally not require you to actually know stuff about how the optimization actually occurs, but maybe just set some tuning parameters and it takes care of the rest (e.g. number of iterations the optimizer runs for). For example, you can train a SVM without knowing any math in scikit-learn-- just feed in the data, kernel type, and move on. That being said, knowing basic optimization (e.g. at the level of Boyd and Vandenberghe's Convex Optimization / Bertsekas' Nonlinear programming) can be helpful in algorithm / problem design and analysis, especially if you're working on theory stuff. Or, implementing the optimization algorithms yourself. Note that the textbook optimization methods often need tweaks to actually work in practice in modern settings; for example, you might not use classic Robbins-Munroe stochastic gradient descent, but a faster accelerated variant. Nevertheless, you can gain some insights from working with the optimization problems.
