[site]: datascience
[post_id]: 96980
[parent_id]: 96926
[tags]: 
The problem has been resolved. The datasets and the model were fine in my case. However, there was actually a bug in the code, more specifically in the evaluation cell. I had defined my InceptionV3 model inside a function that was being trained upon and validated upon as usual. However, during evaluation, the way that I had coded made it create a new model instance every time during testing which was why the testing accuracy was poor. After correcting my error, I managed to get the desired testing accuracy of 96%+. You can check out the notebook if you'd like after the update. I think this may be the issue in the case of others too, assuming their datasets are well made. Generally, in Deep Learning, if something is too odd or even too good to be true, it may have something to do with a bug in the code.
