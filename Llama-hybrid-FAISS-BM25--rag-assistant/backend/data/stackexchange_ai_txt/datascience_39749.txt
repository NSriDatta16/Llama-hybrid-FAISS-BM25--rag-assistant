[site]: datascience
[post_id]: 39749
[parent_id]: 22371
[tags]: 
I wouldn't consider DTW to be outdated at all. In 2006 Xi et al. showed that [...] many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. The results of this paper are summarized in the book "Temporal Data Mining" by Theophano Mitsa as follows: In [Che05a], a static minimization–maximization approach yields a maximum error of 7.2%. With 1NN-DTW, the error is 0.33% with the same dataset as in the original article. In [Che05b], a multiscale histogram approach yields a maximum error of 6%. With 1NN-DTW, the error (on the same data set) is 0.33%. In [Ead05], a grammar-guided feature extraction algorithm yields a maximum error of 13.22%. With 1NN-DTW, the error was 9.09%. In [Hay05], time series are embedded in a lower dimensional space using a Laplacian eigenmap and DTW distances. The authors achieved an impressive 100% accuracy; however, the 1NN-DTW also achieved 100% accuracy. In [Kim04], Hidden Markov Models achieve 98% accuracy, while 1NN-DTW achieves 100% accuracy. In [Nan01], a multilayer perceptron neural network achieves the best performance of 1.9% error rate. On the same data set, 1NN-DTW’s rate was 0.33%. In [Rod00], first-order logic with boosting gives an error rate of 3.6%. On the same dataset, 1NN-DTW’s error rate was 0.33%. In [Rod04], a DTW-based decision tree gives an error rate of 4.9%. On the same dataset, 1NN-DTW gives 0.0% error. • In [Wu04], a super-kernel fusion set gives an error rate of 0.79%, while on the same data set, 1NN-DTW gives 0.33%. Please see the original book for a list of the mentioned references. An important thing to note here is the fact that Xi et al. even managed to beat the performance of an MLP back in 2006. Even though the situation might look a bit different these days (as we have better and faster Deep learning algorithms at hand), I would still consider DTW a valid option to look into when it comes to signal classifications. Update I would also like to add a link to a more recent paper called "The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms" from 2016. In this paper, the authors "have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other)". The following quotes from the paper stress that DTW is (or at least was in 2016) indeed still relevant: Many of the algorithms are in fact no better than our two benchmark classifiers, 1-NN DTW and Rotation Forest. For those looking to build a predictive model for a new problem we would recommend starting with DTW, RandF and RotF as a basic sanity check and benchmark. Received wisdom is that DTW is hard to beat.
