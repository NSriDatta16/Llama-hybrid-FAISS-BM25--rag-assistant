[site]: crossvalidated
[post_id]: 641591
[parent_id]: 641587
[tags]: 
The simplest possibility would be to just take unweighted averages of your predicted probabilities. This corresponds to equal weights, and will give you bona fide predicted probabilities. Alternatively, you can of course use any kind of weighting in a weighted average. For instance, you could give higher weights to models that performed better in the past. (Ideally, on holdout data, not in-sample - in-sample performance is prone to overfitting.) You could map any performance measure to weights in a way that the weights sum to one and such that a better-performing model gets a higher weight. How exactly you would do this would depend on how better performance is operationalized in the first place. For instance, "smaller is better" AIC values can be turned into "Akaike weights" . However, do also try unweighted averaging. For one, it's easier. Also, unweighted averaging of predictions is quite often better than trying to find "optimal" weights for weighted combinations. This has been called the "forecast combination puzzle", and Claeskens et al. (2016) give an intuitively appealing explanation: estimating weights comes with uncertainty, and this uncertainty directly degrades the accuracy of the combination predictions. In addition, while this is a perfectly feasible approach to your final objective - predicting a total number of customers -, I would also recommend you try other methods, like predicting this number directly. Once you have both methods up and running, see whether taking the average of the two predictions yields better results than each one separately. Averaging predictions very often outperforms separate models, or trying to pick a "best" model. (Which is actually just taking your approach one step further.)
