[site]: crossvalidated
[post_id]: 518180
[parent_id]: 518125
[tags]: 
A critical statistical issue here is thow to handle your outcome measure. Unless it's a continuous measure, ANOVA would not be a good choice. For example, if you are marking recognitions as either "right" or "wrong," then you would want to use a generalized linear multiple regression model, e.g. logistic regression, to evaluate your results. If you are interested in which incorrect choices tend to be made, you could consider a multinomial output model with the "correct" emotion for each case as a predictor; that might require a lot of data, however. As an ANOVA model can be represented as a multiple regression model with continuous outcomes, the logic with a generalized model like logistic regression can be similar to ANOVA. There are similar tests for things like significance of interactions, based on model log-likelihood comparisons rather than variance-based ANOVA F-tests, often called "anova" tests even if they might not technically be considered such. The "interactions to look for" are more a matter of what questions you are trying to answer than of statistics per se. For example, when you ask: If there is a difference between each emotion on the emotion recognition task. do you care about potential differences between neurotypical and autistic Groups in terms of the between-emotions comparisons? If so, there needs to be an Emotion:Group interaction term. Do you care about differences of the between-emotions comparisons with respect to stimulation versus not? Then there needs to be an Emotion:Stimulation interaction. Do you care about whether the Emotion:Stimulation interaction depends on the neurotypical/autistic Group? Then you need the 3-way Emotion:Stimulation:Group interaction. Do you suspect that there might be a learning component, such that results of the second session would differ for an individual even if the Stimulation were the same? Then your model would have to include a marker for the actual session, too. Finally, I caution against your using the term "post-hoc tests" in this context. That term is generally used for tests specified after you see the initial results . What you are doing now is to pre-specify critical hypotheses of interest without seeing the data, the best practice and a very wise choice.
