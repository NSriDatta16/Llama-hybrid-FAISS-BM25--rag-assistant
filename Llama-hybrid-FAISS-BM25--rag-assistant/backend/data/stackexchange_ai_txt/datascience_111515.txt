[site]: datascience
[post_id]: 111515
[parent_id]: 
[tags]: 
Using BERT instead of word2vec to extract most similar words to a given word

I am fairly new to BERT, and I am willing to test two approaches to get "the most similar words" to a given word to use in Snorkel labeling functions for weak supervision. Fist approach was to use word2vec with pre-trained word embedding of "word2vec-google-news-300" to find the most similar words @labeling_function() def lf_find_good_synonyms(x): good_synonyms = word_vectors.most_similar("good", topn=25) ##Similar words are extracted here good_list = syn_list(good_synonyms) ##syn_list just returns the stemmed similar word return POSITIVE if any(word in x.stemmed for word in good_list) else ABSTAIN The function basically looks for the word "good" or any of it's similar words in a sentence (the sentences have been previously stemmed so are the words as the function syn_list returns the stem of each similar word) if found, the function will simply label the sentence as POSITIVE. The issue here is that my word vectors are based on word2vec, which does not respect context. I was wondering if I could use BERT instead, and since I am looking for similar words to a certain 'word', how do I include context? Will using BERT improve the performance much as labeling functions are allowed to be lousy?
