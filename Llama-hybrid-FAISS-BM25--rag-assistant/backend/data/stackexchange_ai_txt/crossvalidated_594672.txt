[site]: crossvalidated
[post_id]: 594672
[parent_id]: 347416
[tags]: 
The word "bridge" does not occur in the particular reference. But in other references it does occur. For instance equation 33 in Friedman, Jerome H. "An overview of predictive learning and function approximation." From statistics to neural networks (1994). Another approach is to approximate the discontinuous penalty (30) by a close continuous one, thereby enabling the use of numerical optimization. This is motivated by the observation that both (28) and (29) (30) can be viewed as two points on a continuum of penalties, such as $$\eta_q(\theta_1,\dots,\theta_p) = \sum_{j=1}^p |\theta_j|^q \quad\text{("bridge")} \tag{33} $$ (Frank and Friedman, 1993), or $$\eta_q(\theta_1,\dots,\theta_p) = \sum_{j=1}^p \frac{(\theta_j/w)^2}{1+(\theta_j/w)^2} \quad\text{("weight decay")} \tag{34}$$ (Wiegand, Huberman and Rumelhart, 1991). With the "bridge" penalty (33) $q=2$ yields the ridge penalty (28), whereas subset selection (29) (30) is approached in the limit as $q \to 0$ . Therefore if "bridge" is meant to be the figurative bridge between two points as Kjetil mentioned as possiblity in the comments, then it is a bridge between subset selection and ridge and not between Lasso and ridge. Lasso didn't exist yet when this "bridge" penalty was conceptualized.
