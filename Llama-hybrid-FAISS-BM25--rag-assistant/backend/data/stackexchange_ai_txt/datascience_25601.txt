[site]: datascience
[post_id]: 25601
[parent_id]: 
[tags]: 
How do I fix the misshape in a CNN?

I wrote a simple CNN with a maxpool, a dense layer and a drop layer. Unfortunately there is two messages why it doesn't work. In a normal session codes complaines about a misshape between y_conv and y_ nsorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [1400] vs. [100] [[Node: eval/Equal = Equal[T=DT_INT64, _device="/job:localhost/replica:0/task:0/device:CPU:0"](eval/ArgMax, eval/ArgMax_1)]] If I use the invoke stepper from the tensorflow debugger I get the message "Attempting to use uniniatialized variables". I am very confused. Does anyone have a clue? x = tf.placeholder(tf.float32, [None, 784], name="entry") #layer conv 1 x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1, 32], "Wconv1") # 5 x 5 b_conv1 = bias_variable([32], "bconv1") h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) #layer pool1 h_pool1 = max_pool_2x2(h_conv1) h_pool1 = tf.Print(h_pool1, [tf.shape(h_pool1)], 'shape=') # Dense Layer W_fc1 = weight_variable([ 7 * 64, 1024], "Wconv2") # 7 x 7 b_fc1 = bias_variable([1024], "bconv2") h_pool2_flat = tf.reshape(h_pool1, [-1, 7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) #drop out keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout (h_fc1, keep_prob) W_fc2 = weight_variable([1024, 10], "WconvD") b_fc2 = bias_variable([10], "bconv2") # softmax y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name="y_conv") with tf.name_scope('train'): y_ = tf.placeholder(tf.float32, shape=[None, 10], name="y_") cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)) train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) with tf.name_scope('eval'): correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) These are the definitions of some functions: def weight_variable(shape, name): initial = tf.truncated_normal(shape, stddev=0.1, name=name) #normalverteilung return tf.Variable(initial) def bias_variable(shape, name): initial = tf.constant(0.1, shape=shape, name=name) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name="maxpool")
