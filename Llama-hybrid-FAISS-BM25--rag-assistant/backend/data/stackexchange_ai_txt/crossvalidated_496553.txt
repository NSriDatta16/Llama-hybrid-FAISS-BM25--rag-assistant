[site]: crossvalidated
[post_id]: 496553
[parent_id]: 
[tags]: 
How does the author get ${P(T_{\text{new}}\leq 6|y_N) = \int P(Y_{new}\leq 6|r)p(r|y_N)dr}$

So this is probably a really stupid question, but I don't get where this particular formula comes from. I'm reading a Machine Learning book, and at one point the author states the following formula: $$ P(Y_{new}\leq 6|y_N) = \int P(Y_{new}\leq 6|r)p(r|y_N)dr $$ my first thought is that this comes from $$ P(Y) = \int p(Y,r)dr = \int p(Y|r)p(r)dr $$ but it doesn't really "match" the above form. If you let ${Y=Y_{new}\leq 6|y_N}$ you end up with $$ P(Y_{new}\leq 6|y_N) = \int p(Y_{new}\leq 6|y_N,r)p(r)dr $$ am I missing something stupid and obvious? If more context is needed I can provide it. I usually do lots of pure Mathematics so I'm finding statistics and applying it in this context a bit tricky. Thank you! Edit : to provide a bit more context, I'll setup the problem covered in the book. It's going over prior and posterior distributions, with the parameter we are interested in being $r$ (which is the probability of getting a heads with a coin toss). ${y_N}$ is the number of heads observed in $N$ throws and ${Y_{new}}$ is meant to be the number of heads observed in a batch of ${10}$ throws. Essentially the game is: if you get ${6}$ heads or less in ${10}$ throws, you win. Of course the game should be in your favour if we assume the coin is fair, but the idea I guess is to challenge this prior belief given evidence. He writes $$ p(r|y_N) = \frac{p(y_N|r)p(r)}{\int p(y_N|r)p(r)dr} $$ (which I know comes from Bayes formula). ${p(r)}$ is the prior and ${p(y_N|r)}$ is the likelihood, and lastly ${p(r|y_N)}$ is the posterior. He said given this, we can find the chance of getting ${6}$ heads in a new batch of throws by finding $$ P(Y_{new}\leq 6|y_N) = E[P(Y_{new}\leq 6|r)] = \int P(Y_{new}\leq 6|r)p(r|y_N)dr $$ It's this part I'm unsure where it comes from.
