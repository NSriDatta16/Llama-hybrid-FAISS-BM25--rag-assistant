[site]: crossvalidated
[post_id]: 324522
[parent_id]: 324468
[tags]: 
Basically, my question is how to make my model work with infinite vocabulary. In addition to the technique (subword features) that Jakub Bartczuk mentions in his answer to handle unknown words, seq2seq model may use a copy mechanism that copies a token from the input sequence to a token in the output sequence. Example of seq2seq models using the copy mechanism: {1} References: {1} Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, Nazli Goharian. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents . NAACL HLT 2018
