[site]: crossvalidated
[post_id]: 494748
[parent_id]: 
[tags]: 
What is the best practice to overcome small insufficient data

I have a small number of images (i.e. 108), and I wish to train a deep convolutional neural network with it. As I know - you need to have a large number of samples to be able to train a neural network, and one of the techniques that is used to extend the training data is augmentation, but in my case synthetically generating data may be harmful for the model, because it may present unrealistic examples. My question is - is there a way to extend the dataset without introducing augmentation? Thanks in advance. Ok, after searching a bit I've came across an idea to divide the large image into smaller images, and train the network on the small ones, providing a label to each one. Do you think it is reasonable?
