[site]: crossvalidated
[post_id]: 83337
[parent_id]: 
[tags]: 
Measure of dispersion for dataset with zero mean

I'm testing a machine learning algorithm which, until now used coefficient of variation in order to relative measure dispersion among different attributes in a given dataset. Now that I need to test it thoroughly on larger dataset, it's critical to do something about the possibility of a zero mean, which would immediately breaks down coefficient of variation (cv), and hence the algorithm. Please note that the distributions are not essentially Gaussian, and are often skewed when they are. Also, the measure of dispersion needs to be dimensionless. I searched the web a bit to find ways to 'normalize' cv in some manner, or to even find an alternative to cv. I tried inter-quartile range(iqr), but there was a significant drop in performance when I migrate from cv to iqr. Is there any other dispersion measure that returns similar values to that of cv? If not, is there any principled way to 'normalize' coefficient of variance?
