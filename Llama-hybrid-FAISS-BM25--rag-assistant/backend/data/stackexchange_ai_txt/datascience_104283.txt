[site]: datascience
[post_id]: 104283
[parent_id]: 104273
[tags]: 
You would need huge computing power and storage to do that. Unless you have access to resources such as those owned by Google or Facebook, it doesn't seem very realistic. Usually one doesn't need such a huge amount of data in order to train a lemmatizer, because natural languages have a limited number of morphological patterns. I'd suggest using the Universal Dependencies corpus , which contains annotated text for more than 100 languages.
