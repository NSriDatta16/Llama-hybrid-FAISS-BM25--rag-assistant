[site]: crossvalidated
[post_id]: 449805
[parent_id]: 
[tags]: 
What exactly is violated if your response variable exhibits autocorrelation in ordinary least squares regression?

I'm trying to understand the issues with using OLS regression when our data exhibits autocorrelation. Let's say you simulate a process where: t = [1, 2, 3, ..., 100] e = 100 random normal variables from N(0, 5) y = 2 * t + e Because of this set up, there is obvious autocorrelation of y. You nevertheless build a basic OLS model where y is the response and t is the predictor. What are the issues autocorrelation like this causes? In many of the regression books, we talk about issues if variance is non-constant or distribution of Y | t is non-normal. If I recall correctly, such model violations affect the standard errors of our estimates and thus make it tougher to estimate true parameters. But I don't recall any discussion on what happens when autocorrelation is present. Yet anywhere I look, it just says autocorrelatoin or non independence is bad. What exactly is the issue that prevents us from using OLS to model the process described above - and thus necessitates a time series technique.
