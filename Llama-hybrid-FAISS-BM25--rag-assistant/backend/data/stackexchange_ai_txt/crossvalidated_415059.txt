[site]: crossvalidated
[post_id]: 415059
[parent_id]: 415053
[tags]: 
You missed the fact that the optimization step is something like new_estimate = previous_estimate + learning_rate * change , so rather then oscillating, it would gradually average between different solutions. If learning_rate is small, then the increment towards new value will also be small, and there won't be "jumps". Oscillating between different solutions will happen if learning_rate is too big, so if you see this on the training history plot, this suggests that you should lower the learning_rate .
