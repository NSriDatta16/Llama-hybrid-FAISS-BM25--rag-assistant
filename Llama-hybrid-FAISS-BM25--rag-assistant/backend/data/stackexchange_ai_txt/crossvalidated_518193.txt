[site]: crossvalidated
[post_id]: 518193
[parent_id]: 518183
[tags]: 
As to the first critique, it could be a critique of any and all branches of the sciences. There are no perfectly repeatable experiments. It isn't really possible to completely control any experiment. A meteor could strike the location of the experiment, for example. Also, the ability to repeat an experiment is irrelevant. Most Frequentist inferences are of the form $\Pr(t(x)|\theta)$ . Frequencies in that framework are a limiting form. If the model is true, then a p-value has meaning. The better critique would be "what happens when the model is not true?" That is a good critique because your null is usually the opposite of what you really believe to be true. Frequentist frequencies are not probabilities in the colloquial sense. They are probabilities that provide guarantees. Except for exact tests, when you read that $p it really does not mean that $p=.05$ . It just guarantees that the false positive rate, if the null is true, will not exceed five percent over an infinite number of repetitions. However, as the number of repetitions becomes large enough, it will tend to converge. It is true that one cannot do an infinite number of repetitions and it may be 100% wrong if you only do one sample. Nonetheless, it does provide a sensible way to make inferences and decisions. It allows you a method to control how often you will be made a fool of. It does not allow you to say this time is not the time I will be a fool. The difficulty isn't in the math or the use but in the human need for there to be no false positives or negatives. The problem is in the human need for statistical significance to map perfectly to something being true and a lack of significance to something being false. The second critique is a valid critique of any probabilistic methodology. It is probably true that Bayesian methods handle this critique a bit better because of the logic behind the construction of Bayesian methods. If you need to be purist about that, then one could restrict the use of Frequentist methods to those cases where there truly is no prior knowledge or where the true null hypothesis of interest is a sharp null. Let me illustrate the point. You have a U.S. quarter that you are going to toss 50,000 times in a specially made vacuum chamber with a carefully constructed coin tossing machine. You want to determine if the coin is fair. Even if you believe the coins to be "roughly fair", it is reasonable to discard that belief unless there really has been a controlled study of the fairness of U.S. coins. As a side note, a group of engineering students has done such a study. The toss is totally deterministic and highly controlled. It is unclear how a Frequentist methodology would be disadvantaged here. Now let us redo the experiment a bit. Let us pretend that you and I are going to gamble money on the fairness of the coin. Indeed, I believe that the coin is so unfair that it will come up heads ten times in a row. You believe it is a fair coin. Prior to gambling any money, we will do a pilot study and have a third party toss the coin ten times. It comes up heads six out of ten times. So I ask that you ante up 500:1 odds. I will toss the coin ten times. Just before you do that a friend whispers in your ear that I apprenticed under my uncle who was a stage magician. Also, you are told that I was arrested, but not convicted, as working a number of street games like three-card Monte and coin games under the pseudonym Slick Eddy. Charges were dropped because, although I may have borne a striking resemblance to the alleged perpetrator, nobody was willing to come forward to identify me in a police lineup. Wouldn't you prefer to incorporate that information with a Bayesian prior? It is true that there is no such thing as a random coin toss. Any physicist, magician, or conman will tell you the same thing. The Frequentist method would tell you that the entire procedure was not fair, after the fact, but it wouldn't allow you to incorporate all outside information. The Frequentist method is perfectly accurate by construction, but intrinsically less precise in the resulting estimators in this case. The second argument is fittedness to purpose. Frequentist statistics are not a universal cure for all that ails mankind. They are a tool in a toolkit. Let us flip the above example upside down. Imagine that you truly do not possess any outside knowledge on something that you really must make a decision on. You do have the ability to collect a sample and you can use Bayesian or Frequentist statistics. Frequentist statistics minimize the maximum amount of risk that you will be required to take. Bayesian methods do not. Frequentist methods, despite having no background information, provide guaranteed performance levels. In a state of true ignorance, that is a valuable thing to have. The Bayesian method cannot do that.
