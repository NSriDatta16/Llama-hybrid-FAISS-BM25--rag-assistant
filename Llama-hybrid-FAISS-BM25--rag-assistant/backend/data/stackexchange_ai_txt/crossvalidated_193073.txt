[site]: crossvalidated
[post_id]: 193073
[parent_id]: 
[tags]: 
How do I calculate the derivative of kurtosis and entropy?

I'm using kurtosis and entropy as penalty terms in my neural network's cost function. Need to back-propagate the error. For that I need a quick way to estimate the derivative (the gradient eventually). Numerically calculating is too slow, I only use it to check if my implementation is correct or not. In Matlab: kurtosis = mean((mean(v)-v)^4) / (mean((mean(v)-v)^2)^2); equivalent with: kurtosis = (sum((v-mean(v))^4)/length(v)) / (var(v,1)^2) Then I also take: kurtosisPenalty = sqrt((kurtosis-3)^2) Because I don't care if it's sub-gaussian or super-gaussian. I've tried as derivative: mv = mean(v)-v; kurt = (mean(mv.^4) ./ (mean(mv.^2)^2))-3; kurtsqr = sqrt(kurt.^2); kurtosisDerivL(m) = 0.5./kurtsqr * 2*(kurt) * ... (mean(4*mv) * (mean(mv.^2)^2) - mean(mv.^4) * mean(2*mv.^2)*mean(2*mv)) ./ ((mean(mv.^2)^2).^2); But the gradient that this leads to is not even close to the numerically calculated gradient. With entropy I couldn't even think of how to derive.
