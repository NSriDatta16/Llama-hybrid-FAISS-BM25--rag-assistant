[site]: crossvalidated
[post_id]: 440899
[parent_id]: 440894
[tags]: 
Each node in each level of a decision tree uses a feature to split up the points. So, by the leaf, the number of nodes you've gone through is the number of features, if all have been used. So, it's not logarithmic in the number of features, especially useful ones. Note that one feature can be used couple of times in the path but it is just an approximation, and the complexity will still be linear on average (e.g. using each feature twice is still linear). However, each node/feature tries to split up the data points in the best way, e.g. by decreasing the entropy. So, while going down the tree, data points are split up nearly logarithmically (if it can be done) (e.g. 1/2 to the left, 1/2 to the right, but not $n-1$ to left, $1$ to right in general) so that each later node (in the worst case) will have lower number of data points.
