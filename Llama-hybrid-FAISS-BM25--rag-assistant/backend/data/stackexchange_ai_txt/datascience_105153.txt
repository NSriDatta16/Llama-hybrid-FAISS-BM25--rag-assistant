[site]: datascience
[post_id]: 105153
[parent_id]: 
[tags]: 
Why tfidf of one document is not zero?

I'm new to nlp. Recently I wanted to do little nlp tasks, and faced strange thing. That is I have run the following code from sklearn.feature_extraction.text import TfidfVectorizer docs = ["strange event"] tfIdf_vectorizer = TfidfVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords, ngram_range=(1, 2), use_idf=True, norm='l2') tfidf = tfIdf_vectorizer.fit_transform(docs) print(tfidf) and see the following result (0, 2) 0.5773502691896258 (0, 0) 0.5773502691896258 (0, 1) 0.5773502691896258 shouldn't the tfidf of one single document be zero? (Since the IDF=log(1/1)=0)
