[site]: datascience
[post_id]: 90078
[parent_id]: 90059
[tags]: 
The only thing that you need to do is to start your agent and the goal/end at a random (non-overlapping) location. You can try your setup initially with an empty grid (no walls). If DQN learns, your set up is good and you can start introducing obstacles into the grid. Gradually, the agent will start associating the end location inputs as something rewarding and will learn to navigate. Things to consider (thus my suggestion to run your algorithm first in a 10x10 empty grid with random initial locations): Input feature space: Should better be at the same scale. The Boltzmann's policy temperature should anneal throughout training in order to explore at the beginning (DQN will collect it in the memory buffer) and exploit later on. In the maze scenario, random locations especially ones that the agent and the goal are quite close in space, will help DQN to "realize" the rewarding target soon enough. Therefore, starting with 0/few obstacles with random initial locations for all entities on the grid will act as a form of curriculum learning for your agent.
