[site]: crossvalidated
[post_id]: 208285
[parent_id]: 208225
[tags]: 
It's not a preciseley defined term, so I'll go ahead and give you yet another definition that seems to be consistent with common usage. A hyperparameter is a quantity estimated in a machine learning algorithm that does not participate in the functional form of the final predictive function. Let me unwind that with an example, ridge regression. In ridge regression we solve the following optimization problem: $$ \beta^*(\lambda) = \text{argmin}_{\beta} \left( (y - X\beta)^t (y - X\beta) + \lambda \beta^t \beta \right)$$ $$ \beta^* = \text{argmin}_{\lambda} (y' - X'\beta(\lambda))^t (y' - X'\beta(\lambda)) $$ In the first problem $X, y$ is the training data, and in the second $X', y'$ is a hold out data set. The final functional form of the model, which I called above the predictive function is $$ f(X) = X \beta^* $$ in which $\lambda$ does not appear. This makes $\beta$ a parameter vector, and $\lambda$ a hyper parameter.
