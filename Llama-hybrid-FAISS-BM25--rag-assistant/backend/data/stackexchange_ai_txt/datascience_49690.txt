[site]: datascience
[post_id]: 49690
[parent_id]: 49668
[tags]: 
I'm not very sure what you mean by " 60% accuracy using AUC ". Accuracy and AUC are two different metrics... I'm going to answer as if you're referring to classification accuracy, since that's in your title and the first sentence of your post. First of all, don't use accuracy to evaluate performance on imbalanced data ! Your dataset has an imbalance ratio of 6843/159730 which is around 1/23. This means that if you make a dummy classifier that just predicts the majority class you'd get an accuracy of 96%. There are better options for imbalanced data such as the f1 score or any macro-averaged metric (you can read this post more information). Secondly, I'm not sure what you're doing it but, just in any case, you shouldn't evaluate on the oversampled dataset . As for ideas for improving performance, I don't have many because you are doing most things right. Tree-based algorithms (e.g. XGBoost) are good for dealing with imbalanced data. You are already oversampling the data, which helps a lot. Some other ideas are: Try different oversamplers, undersamplers or perhaps a combination of over and under-sampling techniques. Search to optimize the hyperparameters of your XGBoost. I can't tell by the information you gave, but maybe you're overfitting. Try different algorithms (catboost, lightgbm, etc.), or maybe ensembles of those models (stacked models, etc.).
