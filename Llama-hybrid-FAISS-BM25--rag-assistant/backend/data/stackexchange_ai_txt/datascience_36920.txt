[site]: datascience
[post_id]: 36920
[parent_id]: 
[tags]: 
two dimensional list takes huge amount of memory

I have a 2d list which is created from lung CT image data and a label (the first item is a 3d array(image data) and the second item is a label(0 or 1)), I need this to data to train CNN model, the list is created using the code below: def get3Dmatrix(ID_dist): print('preparing the 3d matrix') matrixlist=[] for Dist, xcoords, ycoords, zcoords, label in tqdm(ID_dist): # read the image imagearray,origin,spacing = load_itk_image(Dist) # resample in to 1mm*1mm*1mm imagearray = resample(imagearray,spacing,(1,1,1)) # transfer world coordinates to voxel-coordinates, divide new spacing 1mm z = int(round((float(zcoords)-float(origin[0]))/1)) y = int(round((float(ycoords)-float(origin[1]))/1)) x = int(round((float(xcoords)-float(origin[2]))/1)) # get the 3D array with shape 25*25*25 imagearray = imagearray[x-13:x+12,y-13:y+12,z-13:z+12] # or get the 3D array with shape 50*50*50 #imagearray = imagearray[x-25:x+25,y-25:y+25,z-25:z+25] matrixlist.append([imagearray,label])# 2d list consist of 3d array + label of all cases. return matrixlist def main(): start_time = time.time() # get ID_list from the csv and data dist. ID_list = getIDlist(candidates_V2_Dist, Data_Dist)# nested list - get file name with dist + x,y,z,class # Data_set[i][0] is the 3D array, Data_set[i][1] is the label Data_set = get3Dmatrix(ID_list) # 2d list consist of 3d array + label of all cases. print("Begin saving in numpy file") np.save(output_path+'numpy_dataset(300)-25-25-25.npy', Data_set) print("%s time takes in seconds" % (time.time() - start_time)) if __name__ == "__main__": main() I want to save the list in numpy file, my problem is, with approximately 400 images (400, 2) memory gets full, my laptop has 8gb RAM, can anyone tell me what is caused this huge memory usage? is there any way to solve this?
