[site]: crossvalidated
[post_id]: 282776
[parent_id]: 282524
[tags]: 
I selected 72 principal components Note: There was no selection here: 72 PCs cover your complete data set as having 72 rows implies that the rank cannot be more than 72. So your PCA is only a pure rotation of the data, and in that sense didn't change anything but the number of columns of the data matrix. But if your microarray data happens to be of a type where you expect only few variates to carry information at all, PCA is probably not the best preprocessing technique. (PCA is great if you have data where the information is spread out over many channels). You may find the chapter about regularization in the Elements of Statistical Learning helpful. can I use this dataset on SVM classifier and apply 10 cross validation ? is it right approach ? Contrary to the other answer, I'd say cross validation for testing is your best chance here unless the test set was collected as a separate experiment set up explicitly (and professionally!) to serve as a test set. Single split into train and test is highly inefficient in terms of making use of the few cases you have. Assuming that you'll report results as sensitivity and specificity and you have 17 positive cases and 17 controls in your test set, your point estimates and 95% confidence intervals for the possible outcomes of the testing are: Now the question for you is: if the outcome were 2 misclassified cases, i.e. a 95 % confidence interval for sensitivy ranging from 68% to 98% be of any use judging whether the model works or whether this is a promising technique? (Cross validation for validation using the whole data set may not be that much better, but at least you'd then have done what is possible with that data set) Within the training set using an inner cross validation for tuning of the SVM parameters, you pretty much have the same problem: the confidence interval widths I depicted above give the uncertainty due to having only few test cases. Data driven optimization of the model, i.e. multiple comparisons of models trained with varying hyperparameters will most likely give just spurious results (the hallucinating model) where in fact you choose a model for which the cross validation splitting was lucky. => Conclusion: either fix the hyperparameters yourself (e.g. based on previous experience with similar data) or go for a model that doesn't need hyperparameter tuning. With original 7000 variates and just 70 cases, you don't want to look for for particularly fancy models (and for sure not for nonlinear models), you look for very stable (aggregated or highly regularized) models. Models like random forest without tuning (but possibly considering only very few of the original data), or L0 regularization looking for the 3 best variates if you happen to think from literature that you'd expect 3 features to carry information.
