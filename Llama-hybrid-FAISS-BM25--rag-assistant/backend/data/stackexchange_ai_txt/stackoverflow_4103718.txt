[site]: stackoverflow
[post_id]: 4103718
[parent_id]: 4102409
[tags]: 
I question that your "inefficient" approach is actually inefficient. Consider this: There are 36^8 == 2,821,109,907,456 (2.8 Trillion) possible IDs. If you have N existing IDs, the chance of a new randomly generated ID colliding is N in ~2.8 trillion. Unless N is in the hundreds of billions, you "generate a unique ID and querying the DB to see if it already exists" algorithm will almost always terminate in one cycle. With careful design, you should be able to generate a guaranteed unique ID in one database request, almost all of the time ... unless you have an awfully large number of existing IDs. (And if you do, just add another couple of characters to the ID and the problem goes away again.) If you want to, you can reduce the average number of database operations to less than one per ID by generating the IDs in batches, but their are potential complications, especially if you need to record the number of IDs that are actually in use. But, if you have at most 150,000 IDs (I assume, generated over a long period of time) then creating the IDs in batches is not worth the effort ... unless you are doing a bulk upload operation.
