[site]: datascience
[post_id]: 90826
[parent_id]: 
[tags]: 
How do I aggregate cross-validation results for per-sample insights?

I'm trying to do feature engineering for point cloud data with three classes, and after having finished implementing the most obvious and simplest features with somewhat good results I'd like to use insights on the classification results to find difficult samples to look at to think of features that might distinguish those. I think a confusion matrix is really useful for this, especially because it also shows how the minority class is handled. It would be easy to compute metrics like a confusion matrix if I only had a training and test set, but I chose k-fold cross-validation for a robust estimation of the generalization error and because a single split for this imbalanced dataset would skew training and evaluation (because there are highly correlated groups of samples). This gives $k$ estimators, and I'm wondering how I can aggregate their predictions. I'm currently using two methods: Combining estimators predictions on their respective test splits Since k-fold CV puts each sample in exactly one test split, I can combine those prediction results to get a single prediction for each sample. With this I can easily construct a confusion matrix. This method of combining is equivalent to sklearn's cross_val_predict , where its documentation warns: Passing these predictions into an evaluation metric may not be a valid way to measure generalization performance. Results can differ from cross_validate and cross_val_score unless all tests sets have equal size and the metric decomposes over samples. As I understand it, the problem is that different estimators based on different test sets are used, but it's not really clear to me. Is it still OK to use it to find difficult samples or at least get an overview from the confusion matrix? Combining estimators predictions on the whole dataset each I can also use the $k$ estimators to predict each sample of the dataset, which gives me $k$ predicted classes per sample. It's not easy to construct a confusion matrix for this, however I can plot a histogram to show how often the estimators are in full agreement correct (5 correct predictions) or in full agreement wrong (0 correct predictions). Anything in between is at least partially caused by differing training splits, whereas the zeros indicate either wrong labeling or something the existing features haven't captured so far, which makes those samples really interesting to me. I'm planning to come up with features for these difficult samples and evaluate the performance difference with the confusion matrix of approach #1 without fiddling to much with parameters afterwards, and instead use a nested CV with GridSearch to generate the final feature + model selection. Is there something wrong with this approach? Can it cause overfitting or overly optimistic results?
