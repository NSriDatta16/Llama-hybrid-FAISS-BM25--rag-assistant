[site]: datascience
[post_id]: 87052
[parent_id]: 86618
[tags]: 
I can think of two approaches. You could use term-frequency/inverse-document-frequency (tf-idf) to cluster the vectors. Personally, I would start by first clustering the full text of the original job vacancies and then use this to assign clusters to the vectors. I have the feeling that it will outperform clustering directly the vectors. There are implementations both in R and in Python. I would use Python for clustering in a language other than English. You can use TfidfVectorizer from sklearn. I've a post on medium with an example of clustering wikipedia articles in Python if you are interested. You can start by a list of key/basic skills and then try to match your vectors to the items of this list. For defining a distance between words/phrases you can: a. use Jaccard index (ex. see this ) b. use SpaCy in Python to find descriptions that are similar. I believe there is a language model for Polish. I haven't tried it but I would use something that is described in "The Beginnerâ€™s Guide to Similarity Matching Using spaCy" . Since you are trying to find the similarity between phrases (not single words) you might also want to consider this stackoverflow discussion .
