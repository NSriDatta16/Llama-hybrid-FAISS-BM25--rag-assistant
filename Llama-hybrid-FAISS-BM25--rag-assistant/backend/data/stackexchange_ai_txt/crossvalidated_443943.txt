[site]: crossvalidated
[post_id]: 443943
[parent_id]: 298046
[tags]: 
Sorry for the late response. There are two places where this issue has an effect: training-time and evaluation time. I will try to cover both, below. I have a similar multi-label classification problem I am working on, also using MultiLabelBinarizer. I am currently using approach number 2 from your list where I fit_transform() once on the training set and then just fit() on the test set. I am training the classifier to output a catch-all class which should lesson the situation in your "problem A" of having some test samples with no class label identified by the classifier. But your case, I'm not sure those samples need to be excluded from your test. In fact, excluding them will make your metrics less accurate. What I do is compute an accuracy metric per class label such as precision, recall, and F-measure. My final, high-level evaluation metric is a macro-averaged F-measure, averaged over each class's F-measure. Those samples with missing labels would simply contribute false negatives to the accuracy metric -- not ideal, but still reasonably representative of the classifier's true abilities, which is the purpose of the evaluation. So, regarding the evaluation-time issue, I don't think there is a problem as long as you do not throw away samples. Regarding the training-time issue, this is probably unavoidable. The challenge of creating a training set is always in making it representative of future, unseen samples. The test set is designed to be different from the training set exactly for the purpose of uncovering problems like this before the model is put into production. I see two solutions: (1) increase your training set size so it is more likely to include all classes or (2) replace small classes with a catch-all class. You would use a count or frequency threshold that you adjust up or down in order to balance the trade off between removing too few and too many classes.
