[site]: crossvalidated
[post_id]: 523513
[parent_id]: 
[tags]: 
Comparing the accuracy of binary classifiers using iterated cross-validation

Let's say that you want to compare two binary classifiers (e.g., LDA and linear SVM) for a given research question, the question being "which one will probably perform best for the problem at hand, in future studies?". To that end, we have one reasonably large sample, and want to apply iterated/repeated cross-validation to compare the accuracy of these classifiers. The goal is really to have a generalizable statement that one classifier might be regarded as more accurate than the other one for this kind of data and research question. If you apply $K$ -fold cross-validation repeated $B$ times, you get $B \times K$ accuracies for each classifier. I can guess, from various answers on SE (e.g., here and here ), that using simple tests as t-tests or preferable Wilcoxon tests on those $B \times k$ accuracies should make sense. Is this a possible way, or even the canonical way, to compare the accuracy of two classifiers after such a design? If not, what would be a better way? Also, how to report extensively the results after an iterated cross-validation? The "global", mean accuracy, is the grand mean over the $B \times K$ accuracies. Should one also report their standard deviation, or any other measure of dispersion? Similarly, is it useful or usual to give a plot (boxplot, stripchart, ...) of those $B \times K$ accuracies for each classifier? Thanks!
