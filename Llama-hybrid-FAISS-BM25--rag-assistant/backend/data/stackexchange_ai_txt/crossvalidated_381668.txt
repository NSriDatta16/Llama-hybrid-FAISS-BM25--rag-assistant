[site]: crossvalidated
[post_id]: 381668
[parent_id]: 217652
[tags]: 
The Bias-Variance tradeoff is a theoretical tool for thinking about machine learning, not generally a practical tool for doing machine learning. This is an important distinction, we use this principle to guide our thinking in different scenarios, and anticipate how our models may behave in certain situations. To achieve this, we do conceive of our concrete training data set as one of a universe of possibilities. It's often very realistic to imagine that if you were to go through the process of collecting training data again, you would get a different particular data set, but it would still be informative of the same real world process. In this sense, while we only ever get one concrete training data set, it is one of many we could have received. We're not very interested in learning things about the particular training data set we happened to get, we're interested in learning things about the process that created that training data. So in that sense, we're much more interested in all of the training data sets we could have received, than the one we happened to recieve. That said, it's not quite true that you only ever get your hands on one estimate $\hat \theta$ . Think about the process of cross validation. In cross validation we are essentially simulating the situation where we have sampled multiple concrete training data sets from the same population, we fit our model once to each such training data set, and we receive multiple estimations $\hat \theta$ . We then often take the mean of the losses of these estimators on the test set, which is used to tune parameters or estimate the out of sample loss.
