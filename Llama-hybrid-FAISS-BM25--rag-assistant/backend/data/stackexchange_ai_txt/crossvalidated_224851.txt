[site]: crossvalidated
[post_id]: 224851
[parent_id]: 224848
[tags]: 
A Markov chain is defined by a probability distribution that is always conditional on your current position. So given you are at $x = 3$ the transition density will tell you what is the probability you go to $x_p = 4$. Every step the Markov chain takes, the distribution of the chain up until that point changes. Hopefully, the Markov chain will keep on progressing in such a way that it converges to a stationary distribution. At this point, every step the chain takes will not have an impact on the distribution of the variables. In this setup, the movement of the Markov chain is described by $\pi(x | x_p)$ and the distribution it converges to is $\pi(x)$. Don't get confused by the usage of $\pi$ to illustrate both distributions. These are completely different from each other. So, $\pi(x|x_p)$ means the probability of going to value $x$ if you are starting at $x_p$. In your example of $X \sim Bin(10, 0.3)$, this isn't really an ideal Markov chain, since the next step in the chain does not depend at all on the current step. However, consider this example where the Markov chain is defined by $X_t \sim Bin\left(10, \frac{X_{t-1}}{10} \right)$. So the next step in the Markov chain is based on the probability of the current step. Then $$\pi(x |x_p) = \binom{10}{x} \left(\dfrac{x_p}{10} \right)^x \left( 1- \dfrac{x_p}{10} \right)^{10-x}.$$ I just cooked up this example, but if this chain converges to a distribution that distribution will be denoted as $\pi(x)$.
