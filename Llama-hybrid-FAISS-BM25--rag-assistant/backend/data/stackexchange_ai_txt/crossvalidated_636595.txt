[site]: crossvalidated
[post_id]: 636595
[parent_id]: 636516
[tags]: 
Summarizing comments as the only possible answer to this question: Using Bayes Optimization to minimize an objective function is just repurposing Gibbs Sampling as an MCMC procedure to approximate maximum likelihood. In other words, if you sum the deviance, call it a likelihood, slap a completely non-informative prior on it, it's possible to use JAGS or WinBUGS as a very expensive and complicated non-linear minimization tool. Note, this is not a Bayes Optimal estimator which is the optimal solution for minimizing the MSE, compared to OLS, this swaps an unbiased/high variance estimator with a lightly biased, very low variance estimator. Minimizing an objective function in R is not an issue at all. The function nlm has great documentation. You can solve OLS with it. set.seed(123) x As noted, OLS has an analytic solution so it's the exact opposite of the type of problem requiring non-linear minimization. The OLS estimate is simply cov(x,y)/var(x) . But consider a more abstract objective function/metric $$ \phi(d) = d^2/(1+d^2) $$ which locally approximates a quadratic in a neighborhood of 0 but reduces weight on residuals. This solution is not analytic. Since the function is smooth it could be solved with Newton Raphson, but use Backward Gradient Descent instead: f
