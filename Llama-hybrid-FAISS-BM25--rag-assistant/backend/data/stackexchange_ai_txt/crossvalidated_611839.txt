[site]: crossvalidated
[post_id]: 611839
[parent_id]: 312562
[tags]: 
Autoencoders can be considered in the framework of [Variational Autoencoders][1] (VAEs), which effectively generalise deterministic autoencoders, where: each data sample $x$ is mapped to a distribution $q(z|x)$ over latent space, rather than to a unique value of $z$ (as given by a deterministic encoder function) similarly, each latent representation $z$ is mapped to a distribution over the data space $p(x|z)$ , e.g. a small Gaussian around a learned mean; the latent variables are fitted to a prior distribution $p(z)$ The point of considering VAEs is that they learn a proper latent variable model where terms of the loss function have a meaningful interpretation. The deterministic autoencoder can be seen as a special case of the VAE framework where: the variance of $q(z|x)$ is reduced towards 0, so that $q(z|x)$ in the limit tends/concentrates to a deterministic function of $x$ ; the mean square ( $L_2$ ) loss (mentioned in the question) relates to the reconstruction term of the VAE loss function and is equivalent to assuming $p(x|z)$ is Gaussian whose mean is learned as a function of $z$ ; and the second ( KL or regularisation ) term of the VAE loss, including the prior over $z$ , is dropped (so no assumed structure is imposed in the latent space). The distribution choice for $p(x|z)$ can be varied, which corresponds to different metrics in $x$ -space (e.g. $L_1$ equivalent to Laplacian, etc).
