[site]: datascience
[post_id]: 55816
[parent_id]: 
[tags]: 
Why does BatchNorm on FC layers increase my error?

I am training a deep CNN for multivariate regression, with three fully connected layers on top of the convolutions. I am using Sigmoid activation for FC layers. When adding BatchNormalization (BN) I observed an alidation error increase, see the picture. Orange: With BN on all FC layers Blue: Without BN It seems like without batch normalization the network needs very long to adjust, probably because the different layers are scaled differently and maybe also the gradient is very small. But I cannot figure out why at the end the network withous BN achieves a lower validation & testing error.
