[site]: datascience
[post_id]: 122937
[parent_id]: 
[tags]: 
How to increase model's validation accuracy?

I'm trying to build a model for text similarity problem using CNN, Bi-directional GRU and Bi-directional LSTM. I've tried changing several parameters but I'm getting the maximum validation accuracy as 0.77. I'm using MSE as loss function. Can anyone suggest what do I need to do to improve my model's validation accuracy? Details: Dataset: Quora Question Pairs (around 400K examples). Dataset split: 80% for training and 20% for testing. Pre-trained embeddings: Google News vectors (dimension: 300). Input text vector dimension: 300. Following is the code for my model: dropout = 0.5 def build_model_A(): inp1 = Input(shape = (max_length,)) embd1 = Embedding(len(embeddings), embd_dim, weights = [embeddings],\ input_length = max_length, trainable = False)(inp1) sp_drop1 = SpatialDropout1D(dropout)(embd1) # GRU rnn1 = GRU(128, return_sequences = True)(sp_drop1) rnn2 = GRU(128, return_sequences = True)(rnn1) bn1 = BatchNormalization()(rnn2) drop1 = Dropout(dropout)(bn1) # CNN conv1 = Conv1D(64, kernel_size = 4, activation = 'relu',\ kernel_initializer = 'he_uniform')(drop1) conv2 = Conv1D(64, kernel_size = 4, activation = 'relu',\ kernel_initializer = 'he_uniform')(conv1) max_pool1 = MaxPooling1D(2)(conv2) drop2 = Dropout(dropout)(max_pool1) gl_avg_pool1 = GlobalAveragePooling1D()(drop2) gl_max_pool1 = GlobalMaxPooling1D()(drop2) concat1 = concatenate([gl_avg_pool1, gl_max_pool1]) dense1 = Dense(1, activation = 'sigmoid')(concat1) model_A = Model(inputs = inp1, outputs = dense1, name = 'model_A') return model_A def build_model_B(): inp1 = Input(shape = (max_length,)) embd1 = Embedding(len(embeddings), embd_dim, weights = [embeddings],\ input_length = max_length, trainable = False)(inp1) sp_drop1 = SpatialDropout1D(dropout)(embd1) # LSTM rnn1 = LSTM(128, return_sequences = True)(sp_drop1) rnn2 = LSTM(128, return_sequences = True)(rnn1) bn1 = BatchNormalization()(rnn2) drop1 = Dropout(dropout)(bn1) # CNN conv1 = Conv1D(64, kernel_size = 4, activation = 'relu',\ kernel_initializer = 'he_uniform')(drop1) conv2 = Conv1D(64, kernel_size = 4, activation = 'relu',\ kernel_initializer = 'he_uniform')(conv1) max_pool1 = MaxPooling1D(2)(conv2) drop2 = Dropout(dropout)(max_pool1) gl_avg_pool1 = GlobalAveragePooling1D()(drop2) gl_max_pool1 = GlobalMaxPooling1D()(drop2) concat1 = concatenate([gl_avg_pool1, gl_max_pool1]) dense1 = Dense(1, activation = 'sigmoid')(concat1) model_B = Model(inputs = inp1, outputs = dense1, name = 'model_B') return model_B def build_full_model(): inp1 = Input(shape = (max_length,)) inp2 = Input(shape = (max_length,)) model_A = build_model_A() model_B = build_model_B() model_A_out = model_A(inp1) model_B_out = model_B(inp2) lambda1 = Lambda(euclidean_distance, name='euclidean_distance')\ ([model_A_out, model_B_out]) full_model = Model(inputs = [inp1, inp2], outputs = lambda1, name = 'full_model') full_model.summary() full_model.compile(loss = 'mse', optimizer = Adam(learning_rate = 1e-3),\ metrics = ['acc']) return full_model ```
