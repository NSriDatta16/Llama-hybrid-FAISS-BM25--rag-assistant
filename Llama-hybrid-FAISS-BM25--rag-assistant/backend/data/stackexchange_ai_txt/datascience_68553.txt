[site]: datascience
[post_id]: 68553
[parent_id]: 
[tags]: 
Why does the transformer positional encoding use both sine and cosine?

In the transformer architecture they use positional encoding (explained in this answer and I get how it is constructed. I am wondering why it needs to use both sine and cosine though instead of just one or the other?
