[site]: crossvalidated
[post_id]: 558378
[parent_id]: 277442
[tags]: 
It is based on the Optimizer's Curse (OC from now on). (And a lot of other math, which correlates the OC to Q-learning. Here is an article written by the original author of the DDQN algorithm covering this correlation). Normal Explanation: Essentially, the OC states, that if we constantly choose the maximum estimate of an outcome, on average our estimate will lie over the maximum of the actual outcome we're trying to predict. I.e We over-estimate. This correlates to the Q-value in the following way; The Q-value of the state-action pair (s,a) is actually an estimate of the maximum expected future rewards gained by following the optimal policy $\pi$ . The way we approximate this optimal policy - and therefore the Q-value of (s,a) - is with the following - well known - equation: $Q(s_t,a_t) Concluding; We try to approximate the optimal policy of the current state, which is itself is an expectation of the future rewards, by constantly taking the maximum of the expected reward at the next state. This falls under the OC, so therefore we overestimate in our max-term. Very short explanation: We have a "curse," which tells us, that constantly taking the maximum of our expectations/estimates will give us, on average, an estimate that is higher than the thing we're trying to estimate. I.e we overestimate. As Q-learning is the act of estimating the maximum future rewards, with its accompanying approximating and well-known equation, it too falls under the curse thanks to the max-term in this equation.
