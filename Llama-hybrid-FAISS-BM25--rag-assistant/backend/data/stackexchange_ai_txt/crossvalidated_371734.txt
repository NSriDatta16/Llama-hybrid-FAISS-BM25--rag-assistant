[site]: crossvalidated
[post_id]: 371734
[parent_id]: 233827
[tags]: 
Try to make your numeric features categorical by digitizing them. If you digitize the features, you can also one-hot-encode them. By that you will get rid of unnecessary floating point as well as having a sparse dataset which will enable you considerable efficiency. LightGBM is swifter by its natural depth-wise tree approach compared to XGBoost's level-wise approach and thanks to its booster: Gradient-One-Sided-Sampling[1]. Moreover, if you have a sparse dataset, its Effective-Feature-Bundling (EFB) skill will boost the computation efficiency sharply[1]. I recommend you to try LightGBM, it solves speed without losing accuracy[1]. Here is your evidence (also experimented but nevermind): https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree][1] Have fun.
