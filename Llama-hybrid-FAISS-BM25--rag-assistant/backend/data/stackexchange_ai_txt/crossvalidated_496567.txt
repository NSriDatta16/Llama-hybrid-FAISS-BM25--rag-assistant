[site]: crossvalidated
[post_id]: 496567
[parent_id]: 
[tags]: 
Why would fitted B spline coefficients be better features for logistic regression?

I had this HW assignment for my high dimensional analysis course where we had tabular data of 6000 features and n samples and then there was a target column which was a class. The assignment was to fit a B spline to each sample using 10 knots then take the b spline coefficients and use them as features for fitting a logistic regression with the target data. Then we took that fitted model and tried it on some test data and it actually did okay. I'm really curious if there's theory behind this method of running logistic regression. It is not obvious to me that the coefficients from B spline embeds useful information that would allow logistic regression to do well, because the other thing about this is B spline coefficients also don't really give you intuitions about things like degrees, knots and so on. In order for logistic regression to do well on a method like this I assume there has to be some sort of useful feature embedding that helps fit the log regressor, but why this is, is not obvious to me. Is there some sort of statistical theory or intuition that b spline coefficients is a good embedding of the data and therefore leads to being good predictors in regression?
