[site]: datascience
[post_id]: 50865
[parent_id]: 50816
[tags]: 
You are probably finding yourself in one of the most interesting problems in data science, the part which is more art than science. I will give you some ideas that can give you hints on how to solve this problem: Prices, Salaries and other variables who have information on "accumulables" many times have a distribution which is skewed to the left (many individuals have a little, a few have a lot), what is reccomended to do is to take logarithm of it. Your new variable should be $Ln(Y)$ , with this, you will close the gap between areas with greater avg_price and areas with lower avg_price. When that happens you find a less-skewed-normal-like distribution of your $Y$ variable. The idea of taking Logarithm also applies to the $X$ variables you have (because crimes also accumulate in certain areas). The Standard Scalling is not necessary when you run a linear regression, because the relativeness of the variable is not influencial in the regression: The regression $Y = \alpha_0 + \alpha_1X_1+...+\alpha_nX_n$ (no scalled) is mathematically equivalent to $Y = \beta_0 + \beta_1Z_1+...+\beta_nZ_n$ (scalled) If you want to use other models, you data seems suitable for it, maybe a regression tree or XGBoost might work well on your problem. I would bet that getting Logarithm in the avg_price, in some exogenous variables and not scalling would get better results for you. from sklearn.linear_model import LinearRegression from sklearn.preprocessing import StandardScaler import pandas as pd regDF = pd.read_csv('exampleDF') X_feats = regDF.drop(['Avg_Price_2012'], axis=1) y_label = regDF['Avg_Price_2012'].values X = log(X_feats) y = log(y_label.reshape(-1,1)).flatten() regModel = LinearRegression() regModel.fit(X, y) regModel.coef_
