[site]: datascience
[post_id]: 69219
[parent_id]: 53773
[tags]: 
Results can be different (even if in practice, it does not really maters, but only for some metrics!) Let's say you perform a 2-fold cross validation on a set with 11 observations. So you will have an iteration with a test set with 5 elements, and then another with 6 elements. If you compute the compute the accuracy globally, thanks to a global confusion matrix (which will have 5+6=11 elements), that could be different than computing the mean from the two folds. Because, with the mean procedure, you will put the same weight (here =0.5) to every folds, even if they do not have the exact same number of observations. While, with the global procedure, you will, kind of, put different weights to each observations. Of course, when you have more observations than 11 (...), the difference is insignificant (but is still present). And if you have the (exact) same number of observations in each fold, the mean procedure gives a result equals to the global one (for example for a 2-fold CV with 10 and 11 elements). But for other metrics, for example for the precision of a given class, that raises a more fundamental question, even if in practice it is still rare. Let's say you perform a 5-fold CV for a classification task, but one of the target class is not so much present. The model sometimes, for a certain iteration of the 5-fold but not for all, could learn not to predict that class at all. So, mathematically, you can't compute the precision for that given modality, since your model does not predict any test observation as belonging to that class. For example, for 3 of the 5 folds the models does, and for the other 2 it does not. If you compute a global confusion matrix, then compute, from it, the precision from that class, you, mathematically, can do it. But if you do the "mean/average procedure", you can't since, for some (2) of the folds, you were not able (mathematically) to compute it. So, and it could be another question :), which one (global or mean) should we choose for computation of metrics that sometimes can't be computed. Of course you could define, so decide, a value to allow every time the metrics can't be computed (let's say 0), but it is not really the debate here...) Here is a link to a certain article (not the only one) that discusses your question (if link is broken the article name is "Apples-to-Apples in Cross-Validation Studies: Pitfalls in Classifier Performance Measurement" from Forman and Scholz).
