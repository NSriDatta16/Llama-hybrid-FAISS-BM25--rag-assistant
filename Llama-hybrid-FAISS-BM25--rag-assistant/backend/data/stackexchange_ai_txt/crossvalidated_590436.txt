[site]: crossvalidated
[post_id]: 590436
[parent_id]: 589565
[tags]: 
To determine whether approximation $q$ or $r$ is closer to the target $p$ , we subtract their KL divergence from $p$ : $$ \begin{aligned} & D_{\operatorname{KL}}(p,q) - D_{\operatorname{KL}}(p,r) \\ &= \Big[\operatorname{E}_p\log(p) -\operatorname{E}_p\log(q) \Big] - \Big[\operatorname{E}_p\log(p) -\operatorname{E}_p\log(r) \Big] \\ & = - \operatorname{E}_p\log(q) + \operatorname{E}_p\log(r) \end{aligned} $$ Note : I use the p subscript to indicate that the expectations are with respect to p . If the difference $ D_{\operatorname{KL}}(p,q) - D_{\operatorname{KL}}(p,r)$ is positive , then $r$ is closer to $p$ that $q$ . (Or equivalently, $q$ is further from $p$ than $r$ , ie. $q$ a worse approximation of the truth.) Perhaps the phrasing is a bit imprecise. Here "we can estimate how far apart $q$ and $r$ are" refers to a very specific sense of distance between $q$ and $r$ in terms of expectations under $p$ . So the "distance" between $q$ and $r$ does tells us something about $p$ . The practical use of this theory is that we can estimate expectations under $p$ without knowing $p$ exactly, by averaging over a sample of observations drawn from $p$ . Understanding KL divergence requires that you know at least a bit of probability theory and the definition of expectations. I recommend Foundations of Probability Theory , from a book in preparation by Michael Betancourt. Follow-up : You substituted the expectation under the true distribution p with a summation over a sample drawn from the true distribution p (ie. the observed data). It's important to understand the role which p plays as it is present both in the expectation $\operatorname{E}$ and in the summation $\sum$ . More explicit notation might help. The expectation is with respect to p , so we can write: $$ \operatorname{E}\log(q) = \int p(x) \log q(x) dx $$ And the dataset of size n is drawn from p , so we can write the log-probability score $S(q)$ as: $$ \frac{1}{n}S(q) = \frac{1}{n}\sum_{x_i:x_i \sim p} \log q(x_i) $$ Let's illustrate the computation with the example you provide in a comment: # p, q and r are probability distributions over a binary space X = {1, 2}. X [1] -0.5610844 - ObsrvExpectation(log(q)) + ObsrvExpectation(log(r)) #> [1] -0.5600452 # r is "closer" to p p [1] 0.3168777 - ObsrvExpectation(log(q)) + ObsrvExpectation(log(r)) #> [1] 0.3180782
