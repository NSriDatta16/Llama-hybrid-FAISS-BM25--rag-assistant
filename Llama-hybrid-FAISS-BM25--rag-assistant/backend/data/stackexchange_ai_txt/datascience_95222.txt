[site]: datascience
[post_id]: 95222
[parent_id]: 
[tags]: 
Autoencoder: good loss values while fitting, but the actual performance is bad

I'm implementing a 1D convolutional autoencoder to reduce the dimensionality of an array. Here is my architecture: input_layer = Input(shape=(25088,1)) enc = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(input_layer) enc = MaxPooling1D(pool_size=2, padding='same')(enc) enc = BatchNormalization()(enc) enc = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(enc) enc = MaxPooling1D(pool_size=2, padding='same')(enc) enc = BatchNormalization()(enc) enc = Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(enc) enc = MaxPooling1D(pool_size=2, padding='same')(enc) enc = BatchNormalization()(enc) dec = Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(enc) dec = UpSampling1D(2)(dec) enc = BatchNormalization()(dec) dec = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(dec) dec = UpSampling1D(2)(dec) enc = BatchNormalization()(dec) dec = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(dec) dec = UpSampling1D(2)(dec) dec = BatchNormalization()(dec) dec = Conv1D(filters=1, kernel_size=2, padding='same', activation='sigmoid')(dec) mod = Model(input_layer, dec) mod.compile(optimizer='adam', loss='mean_squared_error') mod.summary() And this is the summary: Layer (type) Output Shape Param # ================================================================= input_5 (InputLayer) [(None, 25088, 1)] 0 _________________________________________________________________ conv1d_22 (Conv1D) (None, 25088, 32) 96 _________________________________________________________________ max_pooling1d_9 (MaxPooling1 (None, 12544, 32) 0 _________________________________________________________________ batch_normalization_18 (Batc (None, 12544, 32) 128 _________________________________________________________________ conv1d_23 (Conv1D) (None, 12544, 16) 1040 _________________________________________________________________ max_pooling1d_10 (MaxPooling (None, 6272, 16) 0 _________________________________________________________________ batch_normalization_19 (Batc (None, 6272, 16) 64 _________________________________________________________________ conv1d_24 (Conv1D) (None, 6272, 8) 264 _________________________________________________________________ max_pooling1d_11 (MaxPooling (None, 3136, 8) 0 _________________________________________________________________ batch_normalization_20 (Batc (None, 3136, 8) 32 _________________________________________________________________ conv1d_25 (Conv1D) (None, 3136, 8) 136 _________________________________________________________________ up_sampling1d_9 (UpSampling1 (None, 6272, 8) 0 _________________________________________________________________ conv1d_26 (Conv1D) (None, 6272, 16) 272 _________________________________________________________________ up_sampling1d_10 (UpSampling (None, 12544, 16) 0 _________________________________________________________________ conv1d_27 (Conv1D) (None, 12544, 32) 1056 _________________________________________________________________ up_sampling1d_11 (UpSampling (None, 25088, 32) 0 _________________________________________________________________ batch_normalization_23 (Batc (None, 25088, 32) 128 _________________________________________________________________ conv1d_28 (Conv1D) (None, 25088, 1) 65 ================================================================= Total params: 3,281 Trainable params: 3,105 Non-trainable params: 176 My 'data' matrix is a matrix 24 x 25088. I first normalize the values, dividing each row array (so each sample) by the maximum value of that sample's features, in order to have numbers in the [0, 1] range. Then I launch the fitting of the model: for i in range(data.shape[0]): data[i] = (data[i] / np.max(data[i])) mod.fit(data, data, epochs=200, batch_size=12, verbose=1, callbacks=[callback]) And at the final epoch the computed loss is equal to 0.0043, which seems to be a good value. I tried to compute the model's prediction for one of the arrays on which it was trained. More specifically, the array in the first row of the data matric. Let's call this array d . These are the values for the first 20 columns: d = data[:1] print(d[0, :20]) [[0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0. ] [0.01939528] [0. ] [0. ]] We now compute the model prediction for the d input and print the first 20 values: pred = mod.predict(d) print(pred[0, :20]) And this is the output, which should be similar to the input but this is not the case: [[0.04406765] [0.04403916] [0.04400587] [0.04404509] [0.04407492] [0.04407457] [0.04407048] [0.04407695] [0.04408097] [0.04407555] [0.04405355] [0.0440937 ] [0.04411143] [0.0441142 ] [0.04412091] [0.044112 ] [0.04410812] [0.0446077 ] [0.04569775] [0.04390234]] I've never used autoencoders before. Why do I obtain an acceptable loss value, but then the actual model performance is not good? What can I do to improve the performance?
