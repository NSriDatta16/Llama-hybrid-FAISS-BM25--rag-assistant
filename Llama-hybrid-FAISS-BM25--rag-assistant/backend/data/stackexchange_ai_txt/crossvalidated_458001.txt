[site]: crossvalidated
[post_id]: 458001
[parent_id]: 184657
[tags]: 
This is the recursive version of the Q-function (according to Bellman equation): $$Q_\pi(s_t,a_t)=\mathbb{E}_{\,r_t,\,s_{t+1}\,\sim\,E}\left[r(s_t,a_t)+\gamma\,\mathbb{E}_{\,a_{t+1}\,\sim\,\pi}\left[Q_\pi(s_{t+1}, a_{t+1})\right]\right]$$ Notice that the outer expectation exists because the current reward and the next state are sampled ( $\sim)$ from the environment ( $E$ ). The inner expectation exists because the Q-value for the next state depends on the next action. If you your policy is deterministic , there is no inner expectation, our $a_{t+1}$ is a known value that depends only on the next state, let's call it $A(s_{t+1})$ : $$Q_{det}(s_t,a_t)=\mathbb{E}_{\,r_t,\,s_{t+1}\,\sim\,E}\left[r(s_t,a_t)+\gamma\,Q_{det}(s_{t+1}, A(s_{t+1})\right]$$ This means the Q-value depends only on the environment for deterministic policies. The optimal policy is always deterministic (it always take the action that leads to higher expected reward) and Q-learning directly approximates the optimal policy. Therefore the Q-values of this greedy agent depends only on the environment. Well, if the Q-values depends only on the environment, it doesn't matter how I explore the environment, that is, I can use an exploratory behaviour policy .
