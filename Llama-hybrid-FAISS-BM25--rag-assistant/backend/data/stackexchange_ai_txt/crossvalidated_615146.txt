[site]: crossvalidated
[post_id]: 615146
[parent_id]: 
[tags]: 
multiple datasets train-val-test split for time series

Suppose that I have data with dimension $(N,H,F)$ , where $N$ represents the number of different datasets, $H$ is the history size and $F$ is the input size. how would you split it into a train-validation test? Here are my thoughts, with the relevant problems for each based on the dataset index (syntaxed as (dataset_range, :, :) ): we take in the first $[0,1,...,T]$ datasets as train, $[T+1,...,T+V]$ as validation, and $[T+V+1,...,N-1]$ as test. In theory, this is good as my goal is to collect a new dataset and test the model's performance on it. One big problem I see is that when extracting training data statistics for normalization, this should be ideally per every dataset. It is now impossible to normalize the validation dataset (for example), as the datasets have different shape (namely, I cannot normalize a validation set shaped as $(V,H,F)$ using mean and std statistics shaped as $(T,F)$ ) based on the history index (syntax as (:, history_range, :) ): each train-val-test split contains all datasets but for different ranges of history. Namely, we consider the timestamps $[0,1,...,T]$ as train, $[T+1,...,T+V]$ as validation and $[T+V+1,...,N-1]$ as test. This seems like a problem as the model is trained to provide future values for datasets already seen, whereas my goal is to provide it with an entirely new dataset PS - I currently use the first approach, but with a global normalization statistics, which is quite bad as the datasets are from possibly different underlying distributions
