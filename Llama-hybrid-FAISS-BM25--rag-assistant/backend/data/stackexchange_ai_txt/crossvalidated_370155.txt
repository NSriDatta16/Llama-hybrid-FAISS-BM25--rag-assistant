[site]: crossvalidated
[post_id]: 370155
[parent_id]: 370072
[tags]: 
It depends on what you mean by "encode". The likelihood is defined in Likelihood (Edwards, 1984) as a function in the parameters that is proportional to the probability of observing the data: $$ L(\theta; x , \mathcal{M}) \propto p_{\mathcal{M}}(x|\theta) $$ and this is the fundamental definition from which the likelihood given in statistics textbooks usually comes. Although they seldom state this explicitly, they usually define it in terms of the density functions: $$ L(\theta; x , \mathcal{M}) = p_{\mathcal{M}}(x|\theta) $$ which isn't wrong, it just ignores some information. Now for your question, what information can the likelihood "encode", other than that in the data? Since the likelihood is constructed using the probability, it will contain all the assumptions you make about the data. If you assume $$ x_i \sim \text{Normal}(\mu, \sigma^2), \text{ i.i.d} $$ for $i = 1 \ldots n$ , you will get a likelihood that contains your assumption about the Normal distribution $$ L(\theta| x ,\mathcal{M}) = \prod_{i=1}^{n}{ f_{\text{N}(\mu, \sigma)}(x_i) } $$ where $f_{\text{N}(\mu, \sigma)}(x_i)$ is the pdf of the normal distribution. What does this "encode"? Well, we constructed the likelihood using our assumptions of normality, and independence. If you want to "encode" some information about a sinusoid, you have to decide what this means for your probability assumptions. Perhaps you observe $x_t$ as a time series and you want it to follow a sinusoidal shape with an error: $$ x_t \sim \text{Normal}(\theta_1 sin(t + \theta_2), \sigma^2), \text{ i.i.d} $$ And then the likelihood becomes: $$ L(\theta| x , \mathcal{M}) = \prod_{t=1}^{n}{ f_{\text{N}( \theta_1 sin(t) + \theta_2 , \sigma^2)}(x_t) } $$ I think this is what you are alluding to. Perhaps you mean: $$ y_g = sin(y_f) + \varepsilon, \;\; \varepsilon \sim N(0, \sigma^2) $$ meaning $$ y_g \sim \text{Normal}( sin(y_f) , \sigma^2) \text{.} $$ Either way, don't try to construct the likelihood in such a way that it "encodes" your assumptions. Write out your assumptions probabilistically, and then derive the likelihood from that. The difficult bit is expressing your modelling assumptions probabilistically. You mention that some transformations don't lead to well definited probability distributions, and then you can't write down a likelihood. But you are observing some relationship between two GPs. So you will have to think of innovative ways to express this relationship in something that can be modeled. I will not attempt to do that here, since it might require a whole paper.
