[site]: crossvalidated
[post_id]: 370780
[parent_id]: 370769
[tags]: 
Short answer: It's really a shorthand way of writing things. If you have three distributions, $Y, \mu, \sigma^2$ , $p(y|\mu, \sigma^2)$ is essentially just $p_Y(y|\mu = \mu_0, \sigma^2 = \sigma_0^2)$ (i.e. you're evaluating the density of $Y$ , at the constant values of the distributions of $\mu, \sigma^2$ ) Long answer: Let's say that you've got some data $y_i$ - some i.i.d. samples from some distribution. Let's say (assume) that the distribution of these data points is a normal distribution. The normal distribution has some parameters associated with it that change the location and scale of the normal "bell" curve. You would want to find out what those parameters are, using your data. So, you've essentially got this model: $$Y_i \sim N(\mu, \sigma^2)$$ In Bayesian statistics, you treat the parameters $\mu, \sigma^2$ as random variables and place some prior distribution on them. The role of data here is to narrow down your uncertainty regarding your parameters. Here's where the posterior distribution comes in. As an example, let's pretend we know what $\sigma^2$ is, and just concentrate on $\mu$ . If we let the prior distribution to be $p_{\mu}(\mu_0)$ , i.e. the density of $\mu$ evaluated at constant $\mu_0$ , then the posterior is usually expressed as: $$p(\mu|y) \propto p(y|\mu) p(\mu)$$ ... (as you've got it) is just a shorthand way of writing: $$ p_{\mu}(\mu_0 | Y = y) \propto p_Y(y|\mu = \mu_0) * p_{\mu}(\mu_0) $$
