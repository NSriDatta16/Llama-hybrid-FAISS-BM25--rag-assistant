[site]: crossvalidated
[post_id]: 9998
[parent_id]: 9987
[tags]: 
"deviations in the data are the devil" is just not true I think - well I don't agree with it at least. I'd say its more like "chilli" than the "devil" - as much as you can reasonably handle is good, but it can get nasty if there is too much. The most general procedure I know of to "choose a statistic" to report your data is a combination of two things Bayesian inference (describing what is known) Decision theory (taking actions under uncertainty) However, both of these methods are only partially "algorithmic" so to speak. You have to supply the inputs though. Perhaps the most important part of this stage is that you have to ask a question that your procedure is going to answer. Naturally, different questions get different answers. As the saying goes "I have just derived a very elegant and beautiful answer. All I have to do now is figure out the question." This is a common problem that I have seen with many statistical procedures, is that there is not always a clear statement of the class of problems that it is the best procedure to use. Bayesian inference requires you to specify your prior information in a mathematical framework. This involves Specifying the hypothesis space - what possibilities am I going to consider? Assigning probabilities to each part of the space Using the rules of probability theory to manipulate the assigned probabilities this is basically an open-ended problem (you can always analyse a given English statement more deeply, to extract more or different information from it). Decision theory also requires you to specify a loss function - and there are basically no rules or principles by which to do this, at least as far as I know (computational simplicity is a key driver). One useful question to ask yourself though is "what information about the sample do I convey by presenting this statistic?" or "how much of the complete data set can I recover from using just this set of statistics?" One way you could use Bayesian statistics to help you here is to propose a hypothesis: $$\begin{array}{l l} H_{mean}:\text{The mean is the best statistic} \\H_{med}:\text{The median is the best statistic} \\H_{IQR}:\text{The IQR is the best statistic} \end{array} $$ Now these are not "mathematically well posed" hypothesis, but if we use them anyway, and see what parts of the maths are required to make it well posed. The first part is the prior probabilities, without any data, how likely is each hypothesis? The usual answer is equal probabilities (but not always - may have some theoretical reason to support one hypothesis being more likely - the CLT is perhaps one for $H_{mean}$ being higher than the others). So we use Bayes theorem to update each probability ($I$=prior information, $D$= data set): $$P(H_{i}|D,I)=P(H_{i}|I)\frac{P(D|H_{i},I)}{P(D|I)}\implies \frac{P(H_{i}|D,I)}{P(H_{j}|D,I)}=\frac{P(H_{i}|I)}{P(H_{j}|I)}\frac{P(D|H_{i},I)}{P(D|H_{j},I)}$$ So if the prior probabilities are equal, then the relative probabilities are given by the likelihood ratio. So you also need to specify a probability distribution for what type of data sets you would be likely to see if the mean was the best statistic, etc. Note that each hypothesis doesn't actually state what the specific value of the mean, median, or IQR actually is. Therefore, the probability cannot depend on the exact value of the mean. Hence in the likelihoods these must have been "integrated out" using the sum and product rules $$P(D|H_{i},I)=\int P(\theta_{i}|H_{i},I)P(D|\theta_{i},H_{i},I)d\theta_{i}$$ So you have the prior $P(\theta_{i}|H_{i},I)$ which can be interpreted for i=mean as "given that the mean is the best statistic, and prior to seeing the data, what values of the mean are we likely to see?" and the likelihood $P(D|\theta_{i},H_{i},I)$ can be similarly interpreted as "given the mean is best, and equal to $\theta_{mean}$ how likely is the data that was observed?". This may help you come up with some kinds of features that your distribution should have. This describes the inference - now it is time to apply decision theory. This is particularly simple because your decision doesn't influence the state of nature - the statistic won't change if you do or don't use it. So we can describe the decisions ($A$ for "action" because $D$ is already taken): $$\begin{array}{l l} A_{mean}:\text{The mean is the reported statistic} \\A_{med}:\text{The median is the reported statistic} \\A_{IQR}:\text{The IQR is the reported statistic} \end{array} $$ And now you need to specify a loss matrix $L_{ij}$ which relates the action/decision $A_{i}$ to the state of nature $H_{j}$ - what is the loss if I report the mean, but the median is actually the best statistic? In most cases the diagonal elements will be zero - taking the correct action means no loss. You may also have that all non-diagonal elements are equal - how you are wrong doesn't matter, only whether or not you are wrong. You then proceed by calculating the average loss for each action, weighted by their probabilities: $$L_{i}=\sum_{j}L_{ij}P(H_{j}|D,I)$$ And you then choose the action with the smallest average loss.
