[site]: datascience
[post_id]: 38784
[parent_id]: 38764
[tags]: 
You somehow misunderstood how mean_IOU is calculated. First, mean_IOU is not calculated per image and then averaging over all images, but calculated per class for images and then averaging over classes. That means for a single image, if some classes are missing (in both ground truth and your predicted labels), you don't need to worry about the 0/0 situation. You need to only update the confusion matrix, until you finished with all images, then calculate the mean_IOU. In the extreme case, some classes are missing in all images. In that case, you should adjust you num_class parameter. Even if you don't, the missing classes will be treated as IOU=0 instead of 1 (sad...) For example, suppose your image has 5 pixels and you have 3 classes. The ground truth vector is always [0,0,1,1,1] and you always predicted [1,1,0,0,0] (note that you predicted class 0 and 1 inverted, and class 2 is missing). In this case, the mean_IOU is 0, not 1/3. Therefore, you should understand that the IOU is already being normalized by the sum of the numbers of pixies in (the union of your predicted pixels and the ground truth pixels) across all you images. So I don't think it's really necessary to re-weight the IOU any more.
