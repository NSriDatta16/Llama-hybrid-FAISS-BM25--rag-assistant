[site]: crossvalidated
[post_id]: 396829
[parent_id]: 
[tags]: 
Variance/bias trade off regularisation penalty - why does it take this form?

In machine learning, if we estimate weights using a loss function $$L(W) = ||Y-F_W(X)||^2$$ (where $W$ is a weight matrix) we may add a "regularisation penalty" to control for the "variance/bias trade-off" so that $$L(W) = ||Y-F_W(X)||^2 + \lambda \phi(W).$$ Now, one of the penalties proposed frequently is of the form $$\phi(W) = \sum_{i,j} |W_{ij}|^2$$ but I cannot find a good explanation of why such a penalty is used, including in the papers I have come across it. Why does penalising larger values of $W$ affect the "bias/variance trade-off" of a model? Could someone give me an intuitive explanation for why we would prefer smaller values of $W_{ij}$ and why it's relevant to the bias/variance trade-off?
