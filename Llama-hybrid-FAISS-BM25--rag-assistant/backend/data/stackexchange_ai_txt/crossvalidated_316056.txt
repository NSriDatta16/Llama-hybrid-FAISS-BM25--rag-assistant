[site]: crossvalidated
[post_id]: 316056
[parent_id]: 
[tags]: 
How do you deal with changing time series forecasting outcomes from Neural Networks when measuring their accuracy?

I created forecasts using ARIMA, Multilayer Perceptron (MLP) and Extreme Learning Machine (ELM) and would like to compare their accuracies. It's straightforward with the ARIMA. However, the outcomes of MLPs and ELMs change every time I run them. So, how do I deal with those changing outcomes when measuring accuracy? Should I just use set.seed() (I am using R) and go with one specific outcome or use a mean of, say, ten forecasts? Moreover, I often read about forecasts with "accuracy of X %" and I am at a loss at how to calculate such absolute accuracy for forecasts. At the moment, the measures I use are MAPE, MASE, RMSE, MAE.
