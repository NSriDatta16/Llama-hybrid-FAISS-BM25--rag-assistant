[site]: crossvalidated
[post_id]: 234517
[parent_id]: 
[tags]: 
Optimizing Factor Analysis implementation in R - Maximum Likelihood

In order to learn more about Factor Analysis, I've tried to implement a common model in R by hand, using MLE. So I simulated data ( data ~ beta_1 + beta_2*x) . I employed PCA for generating starting values for X, and a normal distribution sample for betas. After defining a log likelihood, I started to run the iterations. However, it takes so long to converge (although it does after one hour). I'm unsure if it is related to some adjustment I overlooked. Anyway, my question is: is there some way of optimizing these iterations and estimating factor loadings in less time? If so, what are my mistakes here? # Generating data nbetas
