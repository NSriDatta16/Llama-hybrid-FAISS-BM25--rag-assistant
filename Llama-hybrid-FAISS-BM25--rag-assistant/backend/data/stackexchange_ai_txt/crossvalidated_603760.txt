[site]: crossvalidated
[post_id]: 603760
[parent_id]: 
[tags]: 
Linear Distance in Latent Feature Space of an AutoEncoder

I would like to perform a cluster analysis on a mixed data set containing continuous, categorical and binary data. As I have 93 features in total, I thought it might help to use an AutoEncoder to compress these 93 features in a latent space of e.g. 8 features. On top of the benefit of dimensionality reduction, this would also code all data types, i.e. continuous, categorical, binary data into a continuous representation, which I could subsequently use to calculate a linear distance matrix (e.g. euclidean) for clustering. I am wondering now: if I am planning to use a linear distance metric such as the euclidean distance on the AutoEncoder's latent space, may this latent space have a non-linear activation function to reconstruct the original dataset (i.e. may the output activation function of the hidden layer with the latent representation be non-linear such as ReLu)? My intution is No: The AutoEncoder then learnt to reconstruct the data using a non-linear function (from the perspective of the latent space). A linear distance measure would thus only be partially able to separate the data well based on the latent representation, if at all. In turn, this would mean data compression in combination with linear distance metrics are best used for AutoEncoders with a single hidden layer, as additional layers in deeper AutoEncoders (and in deep neural networks in general) need to involve non-linear activation functions for function aproximation. Does this make sense?
