[site]: datascience
[post_id]: 12158
[parent_id]: 12155
[tags]: 
One way could be to apply word-embeddings for semantic similarity checking. word2vec model generates feature vectors which could capture semantic similarity. For example, the closest vector to car will be honda, ferrari, vehicle, bike . Train a model using large amount of data from wikipedia dumps or the one released by google. It has fine quality vectors. Gensim has a nice implementation of word2vec For each blog articles, pre-process the data by removing the stop-words and stemming them. From the resulting words, collect the more frequent words. Do this for all the articles and check for the similarity among the frequent words in other articles as well. So that one article with frequent words car, race, tournament, ferrari, F1 will have more closer vectors in article with frequent words bike, honda, racer . Or other way is to look for similar vectors in tags itself. It is good to play with it for sometime, so that you get to know which features work better for the dataset you have.
