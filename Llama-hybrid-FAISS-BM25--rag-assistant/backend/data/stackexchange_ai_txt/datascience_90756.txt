[site]: datascience
[post_id]: 90756
[parent_id]: 90742
[tags]: 
The problem is that you are encoding the pieces of text as vectors and feeding those vectors to the model, but then the first layer of the model is again an embedding layer. You should only use the embedding once: either you embed the text out of the model ( train_embed=encoder(messages) ) or you pass integer inputs to the model and them inside the model ( X=embedding_layer(input) ), but not both. The problem is masked by the fact that the embedding layer does not raise an error when it receives an input of type float . Instead, it silently casts the input to integer. Apart from the problem itself, there are some extra things that you should note: depending on what you do, you may need to adjust the input to the LSTM to make it have the 3 dimensions it expects. Here, the universal sentence encoder creates a fixed size vector for each sequence, and then you are feeding such fixed-size vector to an LSTM. It doesn't make much sense to have LSTMs process non-sequential data. I think you could just apply an MLP (i.e. a couple of dense layers with ReLU activations plus a final dense with the softmax activation) to the vector. You are applying too much dropout for this case where there is almost no data.
