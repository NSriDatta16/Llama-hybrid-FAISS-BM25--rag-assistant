[site]: crossvalidated
[post_id]: 517305
[parent_id]: 517085
[tags]: 
A properly implemented cross-validation or train-test split provides an unbiased estimator of performance on unseen data. This of course assumes that your train/test data is indeed representative of the future unseen data. That's often a reasonable assumption so long as you have a moderately defensible method for collecting training data (i.e. you have "good" training data), but you can't truly know that a new test cohort is drawn from the same distribution unless you actually look at it. You couldn't, for example, train a well-tuned model of population growth in the US in 2020, and make a blanket statement that the model "will predict well on unseen data". It might do a great job of predicting population growth in the US in 2021, but probably won't do as well at predicting US population growth in 1821, or in predicting growth in Algeria in 2021. If you don't have any way to show or reasonably predict some level of similarity between your training data and the unseen test data, you can't predict your model's performance. A train/test split lets you evaluate model performance under specific circumstances, but you can't know how the model will perform under arbitrary, unknown circumstances. In fact, according to No Free Lunch, when applied to all possible circumstances, every algorithm will have the same average performance.
