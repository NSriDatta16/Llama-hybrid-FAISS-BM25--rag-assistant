[site]: crossvalidated
[post_id]: 494575
[parent_id]: 
[tags]: 
Bayesian fitting of a nonlinear model

Some years ago I developed a nonlinear model and java fitting engine that performs well enough to be useful, but is definitely a hack. I would like to modernize and publish an open-source tool (offers to collaborate invited), but it’s terribly frustrating that I cannot find discussion of some basic ideas and techniques that I am sure must have been better developed by others. Maybe all I need is a few keywords or links to get me started. FWIW, I earn my living as a cow doctor. I like M. Eppes explanation of Maximum Liklihood estimation of parameters for a nonlinear model (but I can’t follow all the notation and math). Next I want to incorporate prior knowledge of expected parameter values. To do that, I need a way to normalize the “power” of individual parameters. Then with expected values for mean and sd of each of the 4 parameters in my model, I’ll be able to fit even a data set with zero or one data point. Cool, but not good enough. We know to expect significanrt error in the data points (in a biological sense - it’s not measurement error but biological variability outside the model). With one data point the best estimate is not a curve that goes through that point, but somewhere between the mean curve and the best-fit curve. How do we calculate “somewhere” to incorporate expected error in our priors? I have a working kludge for this, and there is even a chance that it is mathematically correct, but I would like very much to understand it well enough to explain what I am doing.
