[site]: crossvalidated
[post_id]: 177365
[parent_id]: 
[tags]: 
How to kernelize a simple perceptron?

Classification problems with nonlinear boundaries cannot be solved by a simple perceptron . The following R code is for illustrative purposes and is based on this example in Python): nonlin Now the idea of a kernel and the so-called kernel trick is to project the input space into a higher dimensional space, like so ( sources of pics ): My question How do I make use of the kernel trick (e.g. with a simple quadratic kernel) so that I get a kernel perceptron , which is able to solve the given classification problem? Please note: This is mainly a conceptual question but if you could also give the necessary code modification this would be great What I tried so far I tried the following which works alright but I think that this is not the real deal because it becomes computationally too expensive for more complex problems (the "trick" behind the "kernel trick" is not just the idea of a kernel itself but that you don't have to calculate the projection for all instances): X Full Disclosure I posted this question a week ago on SO but it didn't get much attention. I suspect that here is a better place because it is more a conceptual question than a programming question.
