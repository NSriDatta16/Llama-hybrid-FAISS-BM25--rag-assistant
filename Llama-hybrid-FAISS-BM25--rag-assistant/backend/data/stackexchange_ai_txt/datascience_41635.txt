[site]: datascience
[post_id]: 41635
[parent_id]: 24511
[tags]: 
We need to shuffle only for minibatch/SGD, no need for batch gradient descent. If not shuffling data, the data can be sorted or similar data points will lie next to each other, which leads to slow convergence: Similar samples will produce similar surfaces (1 surface for the loss function for 1 sample) -> gradient will points to similar directions but this direction rarely points to the minimum-> it may drive the gradient very far from the minimum “Best direction”: the average of all gradient of all surfaces (batch gradient descent) which points directly to the minum “Minibatch direction”: average of a variety of directions will point closer to the minimum, although non of them points to the minimum “1-sample direction”: point farther to the minimum compared to the minibatch I drew the plot of the L-2 loss function for linear regression for y=2x here
