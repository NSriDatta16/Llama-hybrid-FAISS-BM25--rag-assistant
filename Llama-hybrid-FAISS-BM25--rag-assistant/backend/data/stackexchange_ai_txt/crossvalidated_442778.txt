[site]: crossvalidated
[post_id]: 442778
[parent_id]: 
[tags]: 
Training an agent to NOT act (sparse action optimal policy)

Consider a task with the following properties: Binary action space - Act or Don't Act Continuous, low dimensional state space Optimal policy probably involves not acting most of the time Rewards are sparse and binary (succeed/fail), and come randomly, with success determined by the agent's behavior surrounding the opportunity (both before and after). Basically, the goal of the agent should be: "be super prepared and set everything up properly in case an opportunity comes along in order to maximize our chance of winning, but it probably won't" Agent has a resource that it spends when it "acts". Resource replenishes according to a random self-exciting Hawkes process. Ideally, I'd like the agent to learn to be more conservative when reserves are low (and possibly sacrifice perfect "set-ups" since if it totally runs out, it definitely won't win anything) Goal is to maximize won opportunities over a fixed period of time where the resource levels vary So far, I am happy to report that I have miserably failed at creating an agent that performs well at this task. Here are a few of the issues I've run into: The largest problem seems to be that even with random action choice (during the exploration high epsilon phase), the agent never asks itself "hey I wonder what it'll be like to not do anything for like 100 time steps" since he chooses act/not act with a 50/50 probability. Meaning that during the exploration phase, the probability that he tries to not do anything for 100 time steps is $1/2^{100} \approx very \ small$ . I tried manually skewing this probability, but it didn't seem to affect learning at all I would imagine that the sparse rewards are also an issue here. One thing i tried doing was instead of allowing opportunities (which are very rare) to come along randomly, I would instead reward the agent every time he acts, with a reward proportional to the probability that he would have won had an opportunity come along. But this created all sorts of issues like the fact that I now have to calculate that probability (which is quite complex to do), and that reward hacking becomes an issue because the agent is ok with taking a bunch of small but frequent rewards, and thus he never figures out that if he were to just act less frequently, the magnitude of his wins would be much higher. And lastly, here are a few questions I have: Is RL even the right way to go in this scenario? The way I've framed the problem is by simulating the environment, and at every time-step the agent chooses whether to "act" or "not act". But i'm not even sure RL was the way to go. It's totally possibly that ML in general isn't the right solution here. The reason I gravitated towards it is because probability of winning given an opportunity involves the interaction of half a dozen distributions each tracking a single variable, and tbh hard coding a multivariate function $f$ that maps $state \to action$ seemed like something that a neural net would be good at. Is there a network structure that is particularly suited for this type of problem? I've been using a dueling DQN, but am happy to give policy gradients and anything else a shot. I think what i'm looking for here is the notion of "non-stupid, non-random curiosity". Is this a thing? I've read (lol, more like tried to read) a few papers on incentivizing curiosity in RL agents (goal based curiosity module, intrinsic curiosity module, random network distillation), and results seem promising, but implementing one of these seems like a lot of work, so I wanted to make sure I wasn't doing anything obviously silly first. Thanks for any help!
