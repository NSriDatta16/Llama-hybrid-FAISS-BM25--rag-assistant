[site]: crossvalidated
[post_id]: 77787
[parent_id]: 77743
[tags]: 
There's a few options. One is to search these hyper-parameters randomly then pick the one with the best cross-validation. I tend to run small experiments, record my results and then narrow the parameters. RandomizedSearchCV is a good starting point. After getting a decent estimate of good hyperparameters, with some modules, you can set the pass the argument, verbosity=9, then look at the output at each iteration. Learning_rate, epsilon, eta0, n_iters, and power_t are influenced by one another. It's even more important for tuning Gradient Boosting. Personally I like this approach because I've been too lazy to throw down some code for the next paragraph. I also like the fast iteration associated with it. I can tune a model close to real-time, and stay in the feedback loop. If you're willing to wade in undocumented water, then you can try to optimize over the hyperparameters with a Tree of Parzen Estimators, which has been shown in some cases to give a performance boost over Gaussian Process Confidence Bound methods but not in all. These are probably a much better approach, but their evaluation is time-consuming.
