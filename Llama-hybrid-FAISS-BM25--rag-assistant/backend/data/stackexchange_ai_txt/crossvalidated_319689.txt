[site]: crossvalidated
[post_id]: 319689
[parent_id]: 
[tags]: 
Why the words are mapped to integer before processing them to word2vec?

I'm reading CNN for text classification. According to this link, inside data and preprocessing step it says: Load positive and negative sentences from the raw data files. Clean the text data using the same code as the original paper. Pad each sentence to the maximum sentence length, which turns out to be 59. We append special tokens to all other sentences to make them 59 words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length. Build a vocabulary index and map each word to an integer between 0 and 18,765 (the vocabulary size). Each sentence becomes a vector of integers. I read the paper on word2vec , all it says is dealing with one hot vector. I'm not able to connect step 4 from above blog to paper.
