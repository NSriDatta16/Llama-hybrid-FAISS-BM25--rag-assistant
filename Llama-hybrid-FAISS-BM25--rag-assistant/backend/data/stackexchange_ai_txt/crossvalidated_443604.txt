[site]: crossvalidated
[post_id]: 443604
[parent_id]: 248709
[tags]: 
Here are some explicit ways that the model-based estimator can be biased Heteroskedasticity. Let X be binary and Y be continuous. We know that linear regression of Y on X reproduces Student's t-test (the equal-variance t-test), and we know that if the variance of Y is different between the X groups that the t-test has the wrong level . If the smaller group has larger variance, the t-test is anticonservative; if the smaller group has smaller variance, the t-test is conservative. That means the standard error is wrong: too small or too large depending on the group sizes. Some moderately tedious linear algebra shows that the Satterthwaite/Welch t-statistic is what you get by using the sandwich variance estimator in the regression of Y on X. We know the Welch t-test (in not-too-small samples) has correct size even when the variance of Y differs by X, so it must be using the correct standard error. Pseudoreplication. Suppose you have N observations of X and Y, and you take M identical copies of each one. The model-based variance is too small exactly by a factor of M. The sandwich variance is still correct: there's a factor of M^2 in the middle term and factors of 1/M in each outer term. Precision vs sampling weights. You can think of these as related to the difference between replication and pseudoreplication. The classical derivation of WLS is that an observation (X,Y) with a weight of W arises when you have W independent observations with the same value of X, and you take Y to be the average. It's real replication W times. A sampling weight of W is equivalent to pseudoreplication: you have one observation, but you replicate it W times to correspond to the W individuals in the population it represents. The correlations between residuals for pseudoreplicates are all 1; the correlations between residuals for true replicates are basically 0. The model-based variance estimator treats them the same, but the sandwich estimator doesn't.
