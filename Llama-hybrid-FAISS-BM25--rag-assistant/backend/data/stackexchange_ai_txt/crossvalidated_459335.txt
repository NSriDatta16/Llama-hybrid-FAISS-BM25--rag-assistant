[site]: crossvalidated
[post_id]: 459335
[parent_id]: 459298
[tags]: 
To answer my own question: I don't know if there is a canonical answer, but I came up with a working solution by sort of generalizing a $L_n$ penalty by penalizing the quantiles of the marginal priors. Here's the main function: def do_shrinkage(pos, shrinkage): densities = sps.beta.pdf(pos, a = shrinkage[0], b = shrinkage[1]) regularization_penalty = -np.sum(np.log(densities)) return regularization_penalty pos is a vector of quantiles, in the space of the prior, of the given MCMC iterate, for each variable. Shrinkage is a couple of beta parameters that I get from this function: from scipy.optimize import fmin from scipy.stats import gamma, beta import numpy as np def beta_from_q(l, u, quantiles_percent=0.95): def loss(params): a, b = params lq = (1 - quantiles_percent) / 2 uq = 1 - lq return ( (beta.cdf(l, a, b) - lq)**2 + (beta.cdf(u, a, b) - uq)**2 ) start_params = (1, 1) fit = fmin(loss, start_params, disp = 0) return fit I loop through a bunch of $\beta$ regularizers with 95% quantiles going from $[.05,.95]$ to $[.45, .55]$ , subtracting off the regularization_penalty from the posterior, and thereby making it less likely that MH will pick heavily-regularized cases. I pick the one that works best on the week-ahead holdout set. It works pretty well! Maybe this'll be useful to someone someday. I'd love to know of other approaches people have taken to this problem.
