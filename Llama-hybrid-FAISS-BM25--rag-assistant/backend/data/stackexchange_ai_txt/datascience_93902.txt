[site]: datascience
[post_id]: 93902
[parent_id]: 
[tags]: 
Why is the kernel of a Convolutional layer a 4D-tensor and not a 3D one?

I am doing my final degree project on Convolutional Networks and trying to understand the explanation shown in Deep Learning book by Ian Goodfellow et al. When defining convolution for 2D images, the expression is: $$S(i,j) = (K*I)(i,j) = \sum_{m}\sum_{n}I(i+m,j+n)K(m,n)$$ where $I$ is the image input (two dimensions for widht and height) and $K$ is the kernel. Later it states that the input is usually a 3D tensor. This is because usually the input image has multiple channels (say, red, green and blue channels). Furthermore, it says that it can also be a 4D-tensor when the input is seen as a batch of images, where the last dimension represents a different example, but that they will omit this last dimension for the sake of simplicity. I understand this. Let now $I_{i,j,k}$ be the input element with row $j$ , column $k$ (height and width) and channel $i$ . The output of the convolution is a similarly-structured 3D-tensor $S_{i,j,k}$ . Then, the generalized convolution expression for this 3D-tensor input is $$S_{i,j,k} = \sum_{m,n,l} I_{l,j+m-1,k+n-1}K_{i,l,m,n}$$ where "the 4-D kernel tensor $K$ with element $K_{i,j,k,l}$ giving the connection strength between a unit in channel $i$ of the output and a unit in channel $j$ of the input, with an oï¬€set of $k$ rows and $l$ columns between the output unit and the input unit". I am completely lost in this definition. Why is the Kernel 4-D and not 3-D (which is a logical generalization of the first formula)? What is the analogous to the sentence "giving the connection strength between a unit in channel $i$ of the output and a unit in channel $j$ of the input" in the initial 2D Kernel tensor? I think that is the main things that has to be understood to understand the 4D-kernel.
