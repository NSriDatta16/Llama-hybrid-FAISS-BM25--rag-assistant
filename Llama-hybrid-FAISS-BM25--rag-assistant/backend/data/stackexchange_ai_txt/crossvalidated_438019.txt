[site]: crossvalidated
[post_id]: 438019
[parent_id]: 
[tags]: 
Why does gradient descent outperform closed form solution

In Andrew Ng's machine learning course, in exercise 3 he uses gradient descent (or some other iterative algorithm?) to find coefficients for a problem of handwritten number classification. I repeated his approach, and got success rate ~95%, which is even little improving if I increase the number of iterations (50 -> 90, 0.7%). I also tried to find that coefficients by solving the system of equation: $X'X\, \theta = X'y$ , and success rate in this case is 89.3%. I'm quite surprised that closed form solution is outperformed by iterational approach so significantly. Things are even more weird for me because Andrew uses regularization, and if I add some to the closed form solution, my success rate appears as low as 78%. Could someone please help me get an idea of what happens there? Is it numerical stability issue, or I miss something completely? For my exercise, I use Octave 4.4.1 as: beta = linsolve(xx'*xx, xx'*yy) or beta = pinv(xx'*xx)*xx'*yy and get the same 89%. Thanks for any ideas. PS. Success rate is the accuracy of the algorithm on the training set. https://www.coursera.org/learn/machine-learning/home/welcome
