[site]: crossvalidated
[post_id]: 613291
[parent_id]: 
[tags]: 
Does PAC bound or MLE imply the distribution of empirical risk minimizer?

We know that PAC bounds tell us the empirical risk on training data cannot be too different from the true risk. My question is, does this result imply empirical risk minimizer also cannot be too far from the true minimizer in the parameter space? Personally, I don't think that is the case, as I don't see anything preventing the minimum on training loss landscape from having an arbitrary distance from the minimum on the loss of the entire data distribution while having a similar loss value. Edit: this paper in section 4.2 states that maximum likelihood estimate converges to normal distribution is a standard statistical result. So given convex loss, if we sample multiple training datasets from the entire dataset and train the neural network to find their respective empirical minimizers, then those empirical minimizers approximately follow a normal distribution? And in non-convex setting, the distribution is locally approximately normal centered around every minimizer on the entire data distribution?
