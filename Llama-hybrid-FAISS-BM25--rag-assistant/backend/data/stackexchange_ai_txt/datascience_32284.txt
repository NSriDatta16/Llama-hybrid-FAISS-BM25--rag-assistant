[site]: datascience
[post_id]: 32284
[parent_id]: 
[tags]: 
How to pre-process frequency of a series of signals?

I use a neural net to generate predictions based on a time series of signals. I use a sliding window to feed the data to an LSTM model. The input signals have a random frequency that - I believe - is a valuable input. Time measure is restricted to seconds. By random frequency I mean the time between the signals varies between 0 (delta not measurable in seconds but order is known) and an extreme of 1-2 hours of rough maximum. I struggle with the following questions: How to convert the frequency to an input feature for a neural net? My best idea is to measure the time elapsed since the last signal. This gives me a feature for each new signal in seconds. Approximate range is 0 through 7200 seconds. Is there any better way? Depending on the answer on point 1 how should I normalize the data? I can have anything between 0 seconds and 1-2 hours. Providing my assumption on point 1 is right then I can add: the median of the entire sliding window is sometimes well below 30 seconds, sometimes well over 300 seconds. Problems I have: I think I should normalize by each of the sliding window separately. If I apply Z-score exclusively on each sliding window scope then the overall magnitude is eliminated. It would be interesting to keep an indicator of the overall input signal frequency measured on the entire dataset not only on the given window. Shall I create a second feature for compensating this? If I apply a min-max approach on the given window then the small values will be too close to 0 due to having only a few very high values. If I apply a min-max or the Z-score on the entire dataset then the small values will be too close to 0 due to having only a few very high values. Any ideas about how to overcome these problems?
