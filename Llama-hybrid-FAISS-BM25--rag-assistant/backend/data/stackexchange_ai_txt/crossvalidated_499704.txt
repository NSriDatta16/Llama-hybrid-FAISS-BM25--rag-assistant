[site]: crossvalidated
[post_id]: 499704
[parent_id]: 498580
[tags]: 
As both approaches fall into the Machine Learning topic, the usual way to access the efficiency of such algorithms is to: Split your data set into $TRAINING$ and $TEST$ Fit your model into the $TRAINING$ , if needed use Cross-Validation to tune any hyperparameters Then report your accuracy on the unseen $TEST$ set. This can be done either with the calculation of the Mean Square Error $\frac{1}{n}\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^{2}$ or Root Mean Suare Error $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^{2}}$ , or one suitable evaluation metric for the problem that you are interested. Then you might say that you prefer the algorithm that generalized better on the unseen data, i.e the algorithm that predicted more accurately the unseen data $TEST$ . Lastly, for the asymptotic standard errors, I assume that you can calculate them as usual $\sqrt{\frac{\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^{2}}{n}}$ for each approach supervised or semisupervised. Where the standard error coincides with the Root Mean Square Error. Hence, a question could be what happens to the Root Mean Square Error as the number of $n$ increases. Intuitively, if the fit is good the Root Mean Square error as the limit of $n$ will go to zero because the difference between the predictions and actual values will be small (care should be taken in the cases where you have outliers in your data set and might falsely increase your Root Mean Square Error). Also, you can take a look at this https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e , where they give a discussion of the asymptotic behavior.
