[site]: datascience
[post_id]: 56830
[parent_id]: 
[tags]: 
What are the best activation functions for Binary text classification in neural networks?

I know that there are many activation functions like Relu, sigmoid, tanh ..etc, I just want to know the best one for my case - Binary text classification. I have heard that Relu is best for Binary classification (not sure if im correct) I have used keras to train a model, which is 2 layer , Dense 512, dropout 0.3, activation = "Relu" for these layers, But the predictions are not upto the mark. I have also changed the Dense units to 1024 , keeping others same, but still I got bad predictions. (Validation accuracy 50%) So, can i use other activations, or change my model layers (add few more layers) ?? What can be the best option ?
