[site]: datascience
[post_id]: 98174
[parent_id]: 98172
[tags]: 
To complete Archana David's answer : From what I encountered, the big advantage of sklearn.preprocessing.OneHotEncoder is that you can save it as an scikit-learn encoder, so you can train it on a train set, and apply it on your test based on what you train (you'll recreate the same columns). On the other hand, pandas.get_dummies only applies directly on your dataframe, so you won't be able to train it on a set and then apply it to another based on what you trained first. This then causes issues with machine learning problems. Basically always use one-hot. Example: Training set: Dog Cat Rabbit Train your one-hot encoder on that and apply it: Dog Cat Rabbit 1 0 0 0 1 0 0 0 1 Test Set: Dog Horse Cat The one-hot encoder trained from your training set applied to your test : Dog Cat Rabbit 1 0 0 0 0 0 0 1 0 If you used Pandas pandas.get_dummies , you could only apply it directly on your test: Dog Horse Cat 1 0 0 0 1 0 0 0 1 And you'll have a column mismatch between your train and test: ['Dog', 'Cat', 'Rabbit'] differs from ['Dog', 'Horse', 'Cat']
