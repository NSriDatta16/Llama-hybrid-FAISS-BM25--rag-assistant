[site]: crossvalidated
[post_id]: 352979
[parent_id]: 352977
[tags]: 
As I understand it: [Internal memory / internal state / C_t ] is a vector whose length is the same as the depth of the recurrence in the network (the number of green boxes in the image below). Your understanding is flawed. The RNN cell's internal memory is not the same length as the number of time steps. Instead, at each time step, the memory of the cell is updated. The values of the cell change to reflect the model's understanding of what is "important to remember" given the model weights, its memory state from the previous time-step, and the input. How do I define my LSTM to have enough internal memory to remember sequences of that length? This isn't how LSTMs work. Consider a very simple sequence, one that starts at 0 and adds 1 at each time step. Now I tell you that the value at $f(10)=10$. Do you need to know $f(9)=9, f(8)=8, \cdots, f(0)=0$ to predict $f(11)$? Of course not. LSTMs generalize this idea to more complex sequences by learning to approximate how the values change between time steps, and then applying that rule to make predictions. Is internal state a vector at all? print(state_h.shape) # (1, 5) print(state_c.shape) # (1, 5) It looks like each is a vector! What determines it's length? output_features_count = 5 lstm1 = LSTM(units = output_features_count, ...) Let's consider an example with a little more complex sub-sequences: [1,0,0,0]->1, [1,0,0,1]->2, [0,0,0,1]->3. Now in order to make a prediction (after the 4th input) we kind of have to remember the first and the second inputs (third is meaningless). I thought this information was stored in the internal state. Is that correct? It uses the memory cells to represent the information from the first 3 states. It doesn't need to explicitly store that information in the "raw" form of the original sequences.
