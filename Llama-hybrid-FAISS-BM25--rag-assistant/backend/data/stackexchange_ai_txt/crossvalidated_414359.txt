[site]: crossvalidated
[post_id]: 414359
[parent_id]: 414347
[tags]: 
This is an interesting question, let be just rephrase it a bit differently: Fully connected (FC) Neural Networks are known to be unifersal function approximators (i.e. they can approximate any function). If we had infinite computation power, would there be any reason to use Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs)? Even if we had enough "computing power" and we weren't at all interested in efficiency (i.e. solving the same task quicker with less parameters), there is still the issue that Fully Connected Neural Networks tend to overfit very easily. Actually I answered a similar question the other day on why "CNNs are less prone to overfitting than FC networks" . Besides that CNNs have some useful properties relating to images, the most notable is translation invariance (i.e. the network is invariant to translations in the image). This is very useful in image classification where the object that we want to classify can be anywhere in the image. A similar case can be made for RNNs: Because they can exploit the sequential nature of the data, they can solve problems that wouldn't be possible with FC networks. Through the context of its previous states it can extract information and enhance its performance. This gives it the ability to identify trends and seasonalities in the data, which are impossible to identify in a FC network. Even if we had an arbitrarily-large FC network and we passed it the full sequence in every time step (so that it doesn't miss out on any of the context), that would mean that the RNN requires less information (i.e. fewer features) about that given time step (because they can obtain this information from another time step). This makes less dimensions for the data so less room to overfit. As a rule of the thumb the more parameters in a model, the more data required to train it without overfitting! So, generally speaking, if we can get the same work done with fewer parameters , it is advantageous (faster and more efficient computation, less memory required, less prone to overfitting).
