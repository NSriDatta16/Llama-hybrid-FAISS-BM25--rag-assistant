[site]: datascience
[post_id]: 84123
[parent_id]: 84069
[tags]: 
This would not reduce the effect of the curse of dimensionality because you are not reducing any dimensions, simply the values of one dimension. A valid reason to do this would be if there are so few training examples above 20 that your neural network struggles to learn much about them. But as Erwan suggested, you should simply try clamping and not, and compare validation accuracies. I would suspect that a well designed neural network architecture could better use the information of all values from 0-20 and would not benefit from throwing away information by binning it.
