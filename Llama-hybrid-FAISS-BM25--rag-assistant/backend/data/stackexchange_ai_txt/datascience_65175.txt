[site]: datascience
[post_id]: 65175
[parent_id]: 
[tags]: 
How to handle performance degradation due to domain transfers in data

I am training a classification CNN on a labeled dataset $\langle x,y\rangle$ . The network reaches a 0.92% accuracy rate on testing and validation sets. After this process, I pre-process the data to simulate some reasonable conditions by potting it through some process $\hat{x}=\Psi\{x\}$ and getting a new dataset $\langle\hat{x},y\rangle$ with the same labels. An example of this process is a conversion between color domains, for example, $x\in RGB$ and $\hat{x}\in grayscale$ . In the example I gave, The process may reduce the performance of the network if it using color features that are lost. In my case, no data is lost though the data may be disfigured. After applying $\Psi$ I am training a new network with a degradation in performance with only 0.81% accuracy. I was guessing that I am experiencing a concept drift, though I am not sure how to show or visualize this. Also, I do not know how to mend concept drift in NN. Unfortunately, I cannot reveal the exact case I am working with. Is there a way to analyze or and show the data distribution has changed in the new domain? How do I address the case described and what can I do to try and improve the performance (regardless of the specific task)?
