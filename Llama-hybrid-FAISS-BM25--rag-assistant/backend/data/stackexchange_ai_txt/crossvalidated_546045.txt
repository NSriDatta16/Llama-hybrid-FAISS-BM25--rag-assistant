[site]: crossvalidated
[post_id]: 546045
[parent_id]: 545881
[tags]: 
From the sklearn User Guide (I know you're asking about an R package, but sklearn's user guide is pretty great about introducing topics IMO): The $\nu$ -SVC formulation [15] is a reparameterization of the $C$ -SVC and therefore mathematically equivalent. We introduce a new parameter $\nu$ (instead of $C$ ) which controls the number of support vectors and margin errors : $\nu\in(0,1]$ is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin. The linked reference (Schölkop et al) introduces $\nu$ -SVC in section 7, and I believe Proposition 6 is the result justifying the claim "reparameterization". So indeed, while varying $\nu$ and $C$ in the two problems has the same end effect, their interpretations and implementations are different. If you want to vary $C$ , you should use specifically $C$ -SVC; if instead you want to use $\nu$ -SVC, then you really do need to specify $\nu$ and not $C$ . From the kernlab documentation of the ksvm function: C : cost of constraints violation (default: 1) this is the ‘C’-constant of the regularization term in the Lagrange formulation. nu : parameter needed for nu-svc, one-svc, and nu-svr. The nu parameter sets the upper bound on the training error and the lower bound on the fraction of data points to become Support Vectors (default: 0.2).
