[site]: crossvalidated
[post_id]: 432683
[parent_id]: 
[tags]: 
How to plot cost function against iterations?

I am new to coding in machine learning. I am trying to plot a graph for the gradient descent of a univariate function. def linear_regression(x, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001): N = float(len(y)) for i in range(epochs): y_current = (m_current * x) + b_current cost = sum([data**2 for data in (y-y_current)]) / N m_gradient = -(2/N) * sum(x * (y - y_current)) b_gradient = -(2/N) * sum(y - y_current) m_current = m_current - (learning_rate * m_gradient) b_current = b_current - (learning_rate * b_gradient) return m_current, b_current, cost in order to plot the cost function with the iterations, i am using matlplotlib.pyplot. However, I am confused about how I should plot it against the iterations. Here 'i' is defined within the linear regression function, so I cannot use 'i' as an input for the plot function. How will I be able to plot the cost function with the iterations outside the linear regression function?
