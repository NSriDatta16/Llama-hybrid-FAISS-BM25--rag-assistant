[site]: crossvalidated
[post_id]: 449893
[parent_id]: 
[tags]: 
How to handle hyperparameter tuning, cross-validation and the train/val/test split?

I'm currently building a deep learning based model. To tune my hyperparameters I'm using crossvalidation on the train/validation datasets. Thereby I should get a better estimate of how well my model can generalise. Now, I'm left with the choice of how to train the final model that can be deployed. I thought of the following possibilities: Train on the full train/validation dataset and use the test set as "new" validation Train without test set validation – how can I know I'm not overfitting after n number of epochs? Use the best model from the initial (hyperparameter search) crossvalidation and train for longer Crossvalidation on the full train/validation/test dataset and select a model from there Other options? In normal machine learning I would select option 2. – but in comparison to deep learning most models don't have the problem of overfitting as much. I'm a bit confused on how to handle this problem and would love to get some of your input. Thanks in advance.
