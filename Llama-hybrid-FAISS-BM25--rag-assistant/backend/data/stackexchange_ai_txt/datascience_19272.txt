[site]: datascience
[post_id]: 19272
[parent_id]: 
[tags]: 
Deep Neural Network - Backpropogation with ReLU

I'm having some difficulty in deriving back propagation with ReLU, and I did some work, but I'm not sure if I'm on the right track. Cost Function: $\frac{1}{2}(y-\hat y)^2$ where $y$ is the real value, and $\hat y$ is a predicted value. Also assume that $x$ > 0 always. 1 Layer ReLU, where the weight at the 1st layer is $w_1$ $\frac{dC}{dw_1}=\frac{dC}{dR}\frac{dR}{dw_1}$ $\frac{dC}{w_1}=(y-ReLU(w_1x))(x)$ 2 Layer ReLU, where the weights at the 1st layer is $w_2$, and the 2nd layer is $w_1$ And I wanted to updated the 1st layer $w_2$ $\frac{dC}{dw_2}=\frac{dC}{dR}\frac{dR}{dw_2}$ $\frac{dC}{w_2}=(y-ReLU(w_1*ReLU(w_2x))(w_1x)$ Since $ReLU(w_1*ReLU(w_2x))=w_1w_2x$ 3 Layer ReLU, where the weights at the 1st layer is $w_3$, 2nd layer $w_2$ and 3rd layer $w_1$ $\frac{dC}{dw_3}=\frac{dC}{dR}\frac{dR}{dw_3}$ $\frac{dC}{w_3}=(y-ReLU(w_1*ReLU(w_2(*ReLU(w_3)))(w_1w_2x)$ Since $ReLU(w_1*ReLU(w_2(*ReLU(w_3))=w_1w_2w_3x$ Since the chain rule only lasts with 2 derivatives, compared to a sigmoid, which could be as long as $n$ number of layers. Say I wanted to updated all 3 layer weights, where $w_1$ is the 3rd layer, $w_2$ is the 2nd layer, $w_1$ is the 3rd layer $\frac{dC}{w_1}=(y-ReLU(w_1x))(x)$ $\frac{dC}{w_2}=(y-ReLU(w_1*ReLU(w_2x))(w_1x)$ $\frac{dC}{w_3}=(y-ReLU(w_1*ReLU(w_2(*ReLU(w_3)))(w_1w_2x)$ If this derivation is correct, how does this prevent vanishing? Compared to sigmoid, where we have a lot of multiply by 0.25 in the equation, whereas ReLU does not have any constant value multiplication. If there's thousands of layers, there would be a lot of multiplication due to weights, then wouldn't this cause vanishing or exploding gradient?
