[site]: crossvalidated
[post_id]: 283910
[parent_id]: 283508
[tags]: 
It's a bit too long to describe here, but Peter Norvig (of AI and Google fame) in the chapter called Natural Language Corpus Data from the book Beautiful Data by Segaran and Hammerbacher (2009) describes a statistical word segmentation algorithm for precisely this problem. The chapter is downloadable as PDF , and describes a working solution in Python on pages 1-9, so just take a look. As this is CrossValidated, not StackOverflow, to also point to a more statistically oriented discussion of the solution, there is also a blog post by Sanket Patil describing the solution purely from its statistical view-point only (instead of a "computer coded" one). In a nutshell, the approach is about finding the most likely segmentation by evaluating all possible segmentations as Markov chains with word probabilities taken from the Google n-gram corpus. And therefore this solution goes beyond the uni-gram approach described by the SO post/answer linked to in the question itself. Notice that the solution should work for "tricky" cases as hinted in the question, too - but only in theory: the particular example with "waterworks" is a poor one where I bet even most humans would not always agree on when to split and when not, either. For more common examples, you should get a probability that is higher for some "X AB" or "AB X" bi-gram than for the "A B" bigram, i.e., for "the waterworks" vs. "water works". So with Norvig's approach, as "the waterworks" has a much better probability than taking "the water" times "water works", the program always produces "the waterworks". Therefore, to give you some proof that Norvig's solution indeed works for your cases, I suggest a few more practical examples (adapted from his book chapter): >>> ngrams.segment2("insufficientnumbers") (-8.715317997754482, ['in', 'sufficient', 'numbers']) >>> ngrams.segment2("insufficientfunds") (-6.976606271979648, ['insufficient', 'funds']) >>> ngrams.segment2("choosespain") (-8.35482244334036, ['choose', 'spain']) >>> ngrams.segment2("choosespainkillers") (-12.023363792731391, ['chooses', 'painkillers']) Finally, in case its not obvious, Norvig's solution is only based on bi-grams; If you can get access to frequencies of higher n-grams (tri-grams, etc.), the approach would be able to take even more context into account and therefore could produce even better results. Maye it should also be remarked that "whatthehell" is used so frequently that it is considered a valid single word by this approach. Depending on your use-case that might be desirable or not - but for a text classification task I would assume this behavior should work in your favor.
