[site]: datascience
[post_id]: 68949
[parent_id]: 67974
[tags]: 
I would say the KMeans algo is doing exactly what it's supposed to do. I would be much more surprised if it DIDN'T do what you told it to do. I was also skeptical the first time I saw plots of my own KMeans calculations. Maybe plotting the data in a 3D chart would be more useful/practical. Here is some sample code that you should be able to adapt to your specific scenario. import pandas as pd pd.set_option('display.float_format', lambda x: '%.3f' % x) import numpy as np import statsmodels.api as sm import statsmodels.formula.api as smf import matplotlib.pyplot as plt import seaborn as sns import seaborn as sns; sns.set(color_codes=True) from sklearn.cluster import KMeans color = sns.color_palette() from IPython.core.display import display, HTML display(HTML(" .container { width:100% !important; } ")) df = pd.read_csv("https://raw.githubusercontent.com/noahgift/real_estate_ml/master/data/Zip_Zhvi_SingleFamilyResidence_2018.csv") df.head() for col in df.columns: print(col) import numpy as np import pandas as pd from sklearn.decomposition import PCA from sklearn.cluster import KMeans import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] newdf = df.select_dtypes(include=numerics) newdf = newdf.fillna(0) pca_ = PCA(n_components=3) X_Demo_fit_pca = pca_.fit_transform(newdf) kmeans_PCA = KMeans(n_clusters=4, init='k-means++', max_iter= 300, n_init= 10, random_state= 3) y_kmeans_PCA = kmeans_PCA.fit_predict(X_Demo_fit_pca) y_kmeans_PCA fig = plt.figure(figsize=(10,10)) ax = fig.add_subplot(111, projection='3d') ax.scatter(X_Demo_fit_pca[:,0],X_Demo_fit_pca[:,1],X_Demo_fit_pca[:,2], c=y_kmeans_PCA, cmap='viridis', edgecolor='k', s=40, alpha = 0.5) ax.set_title("First three PCA directions") ax.set_xlabel("Educational_Degree") ax.set_ylabel("Gross_Monthly_Salary") ax.set_zlabel("Claim_Rate") ax.dist = 10 In your case, it would be something like: `X[:,0],X[:,1],X[:,2],c=labels` Also, don't forget this line: ax = fig.add_subplot(111, projection='3d') Try that and see how you get along. FYI...if I did a generic 2D plot, like yours, with the data from the sample I showed above, I would get this as a result. plt.scatter(X_Demo_fit_pca[:, 0], X_Demo_fit_pca[:, 0], c=y_kmeans_PCA, s=50, cmap='rainbow');
