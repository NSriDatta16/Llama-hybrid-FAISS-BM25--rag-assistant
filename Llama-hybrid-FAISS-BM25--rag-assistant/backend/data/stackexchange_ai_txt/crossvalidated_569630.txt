[site]: crossvalidated
[post_id]: 569630
[parent_id]: 
[tags]: 
Is L2 normalization of rows followed by min/max scaling the same as mean-centering and unit variance?

I'm following this guide on detecting anomalies using autoencoders. The section titled "Normalising & Standardising" seems to be describing normalization in terms of scaling and shifting the features to be centered around 0 with a standard deviation of 1. But the implementation in the pipeline is using sklearn.preprocessing.Normalizer , which, if I understand correctly, is the l2 vector norm that scales your features to have a l2 norm of 1. Are these two somehow the same? Are there different "normalization" methods? If so, what should be used when?
