[site]: crossvalidated
[post_id]: 83844
[parent_id]: 
[tags]: 
Can we remove trees from a random forest with poor OOB error to improve generalisation?

My objective is to improve out of sample generalization of my random forest while holding the number of trees constant . Suppose that I am only allowed to use $n$ trees on the out of sample data but it is computationally feasible to estimate $z$ trees on my in sample data where $z \geq n$ (from which, the best $n$ is selected from the $z$ estimated trees based on OOB error). Is there any research on what we expect the generalisation error to be as $z/n$ increases from $1$ to larger (theoretical asymptotic result or an actual empirical result)? Will it offer better generalization than just choosing the trees randomly (equivalent to setting $n=z$)?
