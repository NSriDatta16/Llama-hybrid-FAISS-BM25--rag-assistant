[site]: crossvalidated
[post_id]: 548824
[parent_id]: 
[tags]: 
Frequentist Justification for using Student's t-Distribution to Construct Confidence Intervals

In my most recent statistics class, the general 1 frequentist approach to constructing confidence intervals 2 for an estimator (of any kind) was described as follows: Specify the estimator and the data generating process (i.e., the likelihood $\mathcal{L}(x; \theta)$ ). Apply the estimator to observed data to obtain the estimate. Calculate the distribution of the estimator, assuming the estimate obtained in Step 2 is the true value of the estimand, and output the appropriate cutoffs to give a (central) $(1 - \alpha)$ –confidence-interval. For instance, in the case of MLE estimators, one would proceed as follows: Construct the likelihood function $\mathcal{L}(x; \theta)$ for the data generating process. Given observed data $X$ , find the maximum likelihood estimate $\theta_{\text{MLE}} = \arg\min_{\theta} \mathcal{L}(X; \theta)$ . Calculate the distribution of the estimator $\min_{\theta} \mathcal{L}(\tilde X; \theta)$ assuming $\tilde X$ has the distribution implied by $\theta_{\text{MLE}}$ —e.g., the density of $\tilde X$ is given by $\mathcal{L}(x; \theta_{\text{MLE}})$ —and then find $b_{\text{lower}}$ and $b_{\text{upper}}$ such that $\Pr(\arg\min_{\theta} \mathcal{L}(\tilde X; \theta) b_{\text{upper}}) = \tfrac \alpha 2$ and output the interval $[b_{\text{lower}}, \, b_{\text{upper}}]$ . This process makes a lot of sense to me as a "theory" of frequentist CIs, but the most basic form of frequentist CI—viz., a CI for a sample mean—doesn't fit with this procedure. In particular, if we assume that $X = (X_1, \ldots, X_n)$ represents $n$ i.i.d. draws from a normal $\mathcal{N}(\mu, \sigma^2)$ , then we have that \begin{align} \mu_{\text{MLE}} &= \tfrac 1 n \sum_{i = 1}^n X_i \\ \sigma_{\text{MLE}} &= \sqrt{\tfrac 1 n \sum_{i = 1}^n (X_i - \mu_{\text{MLE}})^2} \end{align} Therefore, Step 3 suggests that we should look at the distribution of $\sum_{i = 1}^n Y_i$ where $Y_i \sim \mathcal{N}(\mu_{\text{MLE}}, \sigma_{\text{MLE}}^2)$ ; the distribution of this sample mean is, of course, $\mathcal{N}(\mu_{\text{MLE}}, \tfrac {\sigma_{\text{MLE}}^2} {n})$ . However, the standard advice is to construct the confidence interval as if the sample mean were drawn from the $t$ -distribution with $n-1$ degrees of freedom (appropriately scaled and recentered). This gives a slightly wider confidence interval which (based on a bunch of simulation I did and a century of received statistical wisdom) generally comes closer to nominal coverage. Is there any way to make sense of this standard advice in light of the "general theory of frequentist confidence intervals" laid out above? Is this "general theory of frequentist confidence intervals" not actually the general theory at all? 1 I realize that this is not actually the most general formulation mathematically. I just mean that it was the "most general" in the sense of, "This is how we think about constructing confidence intervals in a frequentist setting." 2 "Frequentist confidence intervals" is, perhaps, tautological if you distinguish between "confidence intervals" and "credible intervals"—I just mean error bars that are not Bayesian (or Fiducial or ...).
