[site]: crossvalidated
[post_id]: 524913
[parent_id]: 
[tags]: 
Scaled test data value range is much different than the scaled train data

I am trying to create an LSTM model for time series prediction. I am using MinMaxScaler (from the library sklearn for python) for scaling the data. At first, I didn't know that you shouldn't fit the test data to the scaler, so I fitted it with the whole dataset(train and test) and the results were decent, some of them pretty good. But then I realized that it is forbidden to fit the scaler with the test data so I only fitted it with the train data. The problem is that transforming the test data with the scaler gives values in a very different range than the train data that the model is trained on. For example: for one feature, the training data is between 0 and 1, but scaling the test data of this feature gives values between 8 and 9. As a result, the model cannot produce a result that makes sense and after I inverse transform the forecasts, they are in range that doesn't make sense. (40-60 when it should be around 200) And the test data isn't so different from the train data: train data: mean: 240.7 min: 225.8 max: 251.9 scaled range: 0-1 test data: mean: 230.4 min: 230.4 max: 251.6 scaled range(using the scaler fitted with the train data): 8.24-9 Does anyone knows what should I do in order to solve this problem? Edit: I found out the problem was my implementation for scaling each series. Make sure you don't use/fit the same scaler for different features of your data. For example: If you have a dataset of temperatures and humidity, make sure you created a scaler for each one of them. Then fit each scaler with the data of the corresponding feature. The code for scaling with sklearn's MinMaxScaler can look like this: temparture_scaler = MinMaxScaler() humidity_scaler = MinMaxScaler() temparture_scaler.fit(temparture_train_data) humidity_scaler.fit(humidity_train_data)
