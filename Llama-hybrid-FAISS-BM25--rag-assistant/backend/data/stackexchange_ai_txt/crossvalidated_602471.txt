[site]: crossvalidated
[post_id]: 602471
[parent_id]: 
[tags]: 
Prove that out-of-bag error converges to leave-one-out-error for random forests

I saw the wikipedia of out-of-bag (OOB) error states that the OOB error converges to the leave-one-out error. However, I was not able to find a formal proof of it. The wikipedia refers the book of " Elements of Statistical Learning " by Hastie, Trevor. But when I read the book, it leaves this proof as an exercise. I have a few questions about this statement. Do we need the bootstrap sample to be the same as training data size? Do we need the training data size to be infinity as well? Can anyone give a hint, or a reference to the full proof? Thanks!
