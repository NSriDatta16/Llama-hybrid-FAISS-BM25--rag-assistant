[site]: datascience
[post_id]: 55533
[parent_id]: 55503
[tags]: 
Why not do both? Like you mention, it might be worth first computing the percentage of all values that are values. Generally you might also have a percentage in mind that is acceptable, like up to 10% missing values, if they are scattered at random throughout your dataset. There are libraries built specifically for visualising missing data, such as missingno , which offers quite a few ideas. Here is an example heatmap of missing variables across features: "missing" normally implies you have a sequential dataset, for example time-series data. If you had discrete observations e.g. of people's height versus shoe size, there is no sequential causality (autocorrelation: dependency on previous values). In this case, imputing makes little sense. So assuming you do have sequential data, whether or not to impute or drop time steps with missing values will really depend on your use case. Also perhaps the frequency of the data. If all the missing vaiues appear in one chunk at either end of the time series, it is cokmmon to simply leave out that chunk. For example, if you have minute frequency data and you wish to predict a value once per day, then missing a few minutes here and there might be tolerable, and imputation of some kind (e.g. fill-forwad) wouldn't have a huge impact overall, but could help the model optimisation work more effectively. Some models cannot handle missing values, so imputation is necessary. In any case, it would always visualise the data before and after imputation. You can usually run the same visualisation anyway. Sure it costs a few extra minutes, but you might catch important issues. This can save a lot of time compared to only finding the issues later on while debugging your trained model.
