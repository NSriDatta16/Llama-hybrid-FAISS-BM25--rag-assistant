[site]: crossvalidated
[post_id]: 319089
[parent_id]: 318463
[tags]: 
Empiricism vs Theory You wrote: One of the biggest criticism of theory is that because its so hard to do, they usually end up studying some very restricted case or the assumptions that have to be brought essentially make the results useless. This I think demonstrates the main divide between the two views which we can call empirical and theoretical . From an empirical point of view, as you described as well, theorems are useless because they are never complex enough to model the real world. They talk about simplified ideal scenarios which don't apply anywhere in the real world. So what's the point in doing theory. However from a theoretical point of view the opposite is true. What can empiricism teach us beyond "I ran this method on this dataset and it was better than running that other method on this same dataset". This is useful for one instance but says little about the problem. What theory does is provides some guarantees. It also allows us to study simplified scenarios exactly so that we can start understanding what is going on. Example Imagine an actual example: you want to see how concept drift (when the data changes over time) affects your ability to learn. How would a pure empiricist approach this question? All he can do really is to start applying different methods and think about tricks he can do. The whole procedure might be similar to this: Take past 300 days and try to detect if the mean of that variable has changed. OK it somewhat worked. What if we try 200 days instead? OK better, let's try to change the algorithm once the drift occurs. Obtain more datasets and see which method developed so far works best. Results are not conclusive, maybe guess there are more than one type of concept drifts going on? Try simulations. What if we simulate some concept drift and then apply different methods using different number of days used to detect if change has occurred. What we have here is quite precise results on a few data sets. Maybe the data was so that updating the learning algorithm based on observations of 200 past days gave the highest accuracy. But will the same work for other data? How reliable is this 200 days estimate? Simulations help - but they don't reflect real world - same problem theory had. Now imagine the same from a theoretical standpoint: Simplify the scenario to an absurd level. Maybe use a 2-variate normal distribution with a mean suddenly changing over time. Choose your conditions clearly - pick the model that is optimal on normal data. Assume you know that the data is normal. All you don't know is when the shift in means occur. Device a method for detecting when the shift has occurred. Again can start with 200 past observations. Based on these setting we should be able to calculate the average error for the classifier, average time it takes for the algorithm do detect if change has occurred and update. Maybe worst case scenarios and guarantees within 95% chance level. Now this scenario is clearer - we were able to isolate the problem by fixing all the details. We know the average error of our classifiers. Can probably estimate the number of days it would take to detect that change has occurred. Deduce what parameters this depends on (like maybe the size of the change). And now based on something produce a practical solution. But most importantly of all: this result (if correctly calculated) is unchanging. It's here forever and anyone can learn from it. Like one of the fathers of modern machine learning - Jürgen Schmidhuber likes to say: Heuristics come and go – theorems are for eternity. Lessons from other fields Also briefly wanted to mention some parallels to physics. I think they used to have this dilemma as well. Physicists were studying frictionless objects of infinite mass moving inside infinite space. At first glance what can this tell us about reality where we want to know how snowflakes move in the wind. But it feels like theory carried them quite a long way.
