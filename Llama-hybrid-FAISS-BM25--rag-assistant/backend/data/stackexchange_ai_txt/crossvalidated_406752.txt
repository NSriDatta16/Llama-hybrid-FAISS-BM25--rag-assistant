[site]: crossvalidated
[post_id]: 406752
[parent_id]: 
[tags]: 
What mathematically enables one to Bayesian update multiple times?

I'm going through the MIT OCW notes on probability and statistics and came across an example in some class notes where the posterior is updated after two events: $x_1 = 1$ and $x_2=1$ , which correspond to a coin landing on heads twice. To explain how to calculate the posterior $p(\theta|x_1=1, x_2=1)$ , the notes say In life we are continually updating our beliefs with each new experience of the world. In Bayesian inference, after updating the prior to the posterior, we can take more data and update again! For the second update, the posterior from the first data becomes the prior for the second data. I intuitively how this could make sense, but I don't mathematically understand why we can calculate $$p(\theta|x_1=1, x_2=1) = \frac{p(x_2=1|\theta)p(x_1=1|\theta)p(\theta)}{\alpha}$$ where $\alpha$ is some normalizing factor so that the sum of probabilities equals 1. Thus, my question is: is there a proof for why can we update the posterior more than one time? Under what conditions can we do this?
