[site]: crossvalidated
[post_id]: 296869
[parent_id]: 
[tags]: 
Comparing models by averaging test errors

I have some data, which I split into training, cross-validation and test sets. I built two models, that I trained on the training set and optimised using the cross-validation set (e.g. finding the optimal polynomial degree and regularisation parameter). I then combined the training and the cross-validation sets into one and used this larger set test to train both algorithms and calculated the test error. However, my dataset is quite small (530 entries), and the test error varies quite a bit, so the range of $R^2$ seems to be 0.40-0.65 for both. My two questions are: Is it valid to combine the training and cross-validation sets after the algorithm has been optimised? Is it a good idea to calculate the test error multiple times (e.g. 100) for both algorithms (following random selection of the training, cross-validation and test sets) and look at the average to compare the two models? Sorry if the question is primitive, I'm very new to Machine Learning!
