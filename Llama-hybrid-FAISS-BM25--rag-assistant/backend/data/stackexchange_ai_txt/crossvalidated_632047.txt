[site]: crossvalidated
[post_id]: 632047
[parent_id]: 
[tags]: 
Regularization Problem and Reproducing Kernel Hilbert Space

The following shows part of the page 169 of The Element of Statistical Learning that I want to make clear. We have $$\min_{f \in \mathcal H_K}[\sum_{i = 1}^NL(y_i, f(x_i)) + \lambda\Vert f\Vert_{\mathcal H_K}^2].$$ It can be shown (see Exercise 5.15) that the solution is finite-dimensional, and has the form $$g(x) = \sum_{i = 1}^N\alpha_i K(x, x_i).$$ I have solved the given exercise (5.15 (a) to (d)). For reference, the exercise 5.15 (d) is as follows: Exercise 5.15(d) : Suppose $\tilde g(x) = g(x) + \rho(x)$ with $\rho \in \mathcal H_K$ and orthogonal in $\mathcal H_K$ to each of $K(x, x_i), \forall i=1,...,N$ . Show that $$\sum_{i = 1}^N L(y_i, \tilde g(x_i)) + J(\tilde g) \ge \sum_{i = 1}^N L(y_i, g(x_i)) + J(g)$$ with equality iff $\rho = 0$ . Now, I guess it remains to show that every $f \in \mathcal H_K$ can be (uniquely?) decomposed as $f = g + \rho$ , which is not sure for now. If this makes sense, then everything is clear. But, I'm stuck to whether my hypothesis is true and (if so) how I can show this. I first tried to show there exists a Hilbert space, say, $\mathcal G$ whose basis consists of $\{K(\cdot, x_i)\}_{i=1}^N$ so that $g \in \mathcal G$ and $\rho \bot K(\cdot, x_i)$ for all $i=1,...,N$ . But, I'm not certain about whether each of $\{K(\cdot, x_i)\}_{i=1}^N$ is linearly independent. Any comments about this problem (or any uncertainties) would be grateful.
