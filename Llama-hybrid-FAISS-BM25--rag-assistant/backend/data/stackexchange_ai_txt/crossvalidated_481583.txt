[site]: crossvalidated
[post_id]: 481583
[parent_id]: 226553
[tags]: 
Model Complexity (model flexibility) is about representing the structures hidden in the data. To take an example of polynomial curve fitting, a higher-order polynomial (say, parabola/quadratic) provides more flexibility to represent the hidden structures compared to a lower-order one (say, line/linear) if there is indeed a hidden parabolic structure (that we found using EDA). So, where does Regularization come in? The observations/outcomes of a random experiment are noisy (we assume gaussian noise as a good approximation). When we use a higher-order polynomial, the higher the polynomial, the more the training points that lie exactly on the fitted curve. But, this results in poor generalization and the test set results are disappointing. When we examine the coefficients of the higher order polynomials, they carry very high values. What has happened is that even though the model is flexible, it has tuned itself to the gaussian noise, so much so that the fitted curve oscillates rapidly near the ends of intervals between data points. So, during testing, a slight off-x results in a big off-y. Regularization helps in keeping these coefficients at lower values, hence, the curve is smooth. We now have less training points on the curve, more training error, but less test error, means, better generalization (less Overfitting ). The choice between higher order polynomial and regularization is not that of excluding one for the other , but that of striking a balance between how higher the polynomial can be without losing too much on generalization . When we talk about order of polynomial and regularization in the context of generalization/less-overfitting, there is a third lever that can reduce the overfitting caused by a higher-order polynomial. This lever is ' size of data '. More data helps in accommodating a higher-order polynomial. References: 1 Pattern Recognition and Machine Learning - Christopher Bishop
