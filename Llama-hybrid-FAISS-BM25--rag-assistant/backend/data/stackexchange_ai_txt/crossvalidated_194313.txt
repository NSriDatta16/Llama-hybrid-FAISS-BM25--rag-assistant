[site]: crossvalidated
[post_id]: 194313
[parent_id]: 
[tags]: 
The likelihood of response variables in variational Bayesian probit regression

I read the paper Explaining Variational Approximations (J.T. Ormerod & M.P. Wand) and there is a part where they explain variational probit regression with auxiliary variable since the posterior density isn't mathematically tractable. So $$y_{i}|\beta_{0}, \ldots , \beta_{k} \sim \operatorname{Ber}\left(\Phi\left(\beta_{0} + \beta_{1}x_{1i}+\cdots + \beta_{k}x_{ki}\right)\right), \qquad 1\le i \le n$$ where the coefficient vector has a normal prior $\beta \sim \mathcal{N}\left(\mu_{\beta}, \Sigma_{\beta}\right)$. Because the posterior $\pi\left(\beta|y\right)$ isn't available in closed form, they introduce auxiliary variables: $$ a_{i}|\beta \sim \mathcal{N}\left(x_{i}^{T}\beta, 1\right) $$ where $x_{i}$ is the $i^{\text{th}}$ row of the design matrix $X$. They now say the introduction of auxiliary variables allow them to write $$ p\left(y_{i}|a_{i}\right) = I\left(a_{i}\ge 0\right)^{y_{i}}I\left(a_{i} Albert and Chib (1993) but still don't understand why this is so. My question is $p\left(y_{i}|a_{i}\right)$ is a density function anyways but a product of two indicator variable is either 0 or 1. What is the reasoning behind this? 1 J. T. Ormerod & M. P. Wand (2010) Explaining Variational Approximations, The American Statistician, 64:2, 140-153, DOI: 10.1198/tast.2010.09058 ( link )
