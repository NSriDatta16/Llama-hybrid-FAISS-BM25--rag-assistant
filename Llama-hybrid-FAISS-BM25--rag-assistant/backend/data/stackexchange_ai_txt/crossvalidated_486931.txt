[site]: crossvalidated
[post_id]: 486931
[parent_id]: 486905
[tags]: 
Yes, if you allow it to perfectly learn from the previous model. But, for example with gradient boosting we utilize HEAVY regularization such as a learning rate and subsampling procedures. For something like trees the depth of each tree is usually fairly shallow (at least it used to, now we are able to build larger trees due to other regularization advances) so it is still a pretty high bias model. So we try to add bias to each weak learner so that the next iteration doesn't add too-too much value to our ensemble. Also it is typical to stop iterating when your train/test set begins displaying signs of overfitting. Even with this, we can use parameters which overfit badly, just like with a random forest or bagged tree.
