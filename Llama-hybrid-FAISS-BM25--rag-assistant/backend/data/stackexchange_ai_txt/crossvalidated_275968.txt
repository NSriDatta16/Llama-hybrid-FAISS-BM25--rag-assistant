[site]: crossvalidated
[post_id]: 275968
[parent_id]: 259573
[tags]: 
Actually you may have a look at chapter 8.4.2 in Murphys book 'Machine Learning: A Probabilistic Perspective', where the BIC is nicely derived from the marginal likelihood. A flat prior is assumed there, however under some specific assumption the presented method (Laplace approximation) may be applied to a nonuniform prior as well, which results in the likelihood in the BIC being replaced by the posterior evaluated at the MAP estimate (similar to what you have written in Question 1, but without the integration). I have some scanned, self written pdf about this that I may share (unfortunately it's in half english/half german, as I had intended it for private use only). Now, BICs rooting in the marginal likelihood also makes it clear that you were on the right track regarding question 1: If you can analytically compute the integral over $\theta_1$, then you do not have to refer to Laplaces approximation regarding $\theta_1$ and thus will end up having a better approximation of the marginal likelihood (which is what you are really interested in, BIC is just a substitute). The Laplace approximation may then be applied only to the resulting expression, i.e. $p(x|\theta_2,M)$. In other words, your approach is correct, you may swap the $\hat{L}$ of the original BIC with $\hat{L} =p(x|\theta_2,M)$. Now w.r.t. question 2, as Laplaces method is based on the maximum exponent of the integrant in question (which is the joint probability of $x,\theta_1,\theta_2$ in case of the marginal likelihood) you must use the MAP estimate as your "highest likelihood point", i.e. the point $$(\theta^*_1,\theta^*_2) = argmax_{\{\theta_1,\theta_2\}} p(\theta_1,\theta_2|x,M) $$ Hope this helps...
