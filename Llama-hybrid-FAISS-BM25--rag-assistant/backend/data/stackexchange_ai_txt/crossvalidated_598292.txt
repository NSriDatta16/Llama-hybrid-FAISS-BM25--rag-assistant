[site]: crossvalidated
[post_id]: 598292
[parent_id]: 598291
[tags]: 
A separable loss function computes the loss for each point individually & independently: $L(y_i, \hat{y_i})$ for all $i$ . XGBoost requires separable loss functions. The xgboost documentation says XGBoost is designed to be an extensible library. One way to extend it is by providing our own objective function for training and corresponding metric for performance monitoring. This document introduces implementing a customized elementwise evaluation metric and objective for XGBoost. I've bolded the part indicating that XGBoost requires separability. The Pearson correlation is computed as $$ r_{xy}=\frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum_{i=1}^n(x_i - \bar x)^2}\sqrt{{\sum_{i=1}^n(y_i - \bar y)^2}}} $$ which is not separable.
