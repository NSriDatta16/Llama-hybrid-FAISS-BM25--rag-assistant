[site]: crossvalidated
[post_id]: 164412
[parent_id]: 
[tags]: 
Fail to improve recall in classification

I have a large data set with over 700,000 examples and I tried to (binary) classify the data set with Naive Bayes and Random Forest . The task was carried out in Python and Scikit-learn data The data set has 3 categorical variables and 5 discrete (numeric) variables. I use One hot encoder to discretize the categorical variables and the data set consists of 18 features (13 dummy variables + 5 discrete variables). Overall, 20% examples are positive ( 1 ); 80% are negative ( 0 ). evaluation I used MultiNomialNB and RandomForestClassifier in Scikit-learn to classify the data set. I applied K-Fold (k=20) cross validation to the data set. The results of both classifier were of little variance in all metrics: precision, recall, F-measure, accuracy and AUC. AUC: both over 0.7 accuracy: both over 0.8 precision: both 0.5 recall: both 0.28 Since I am more interested in the positive, I worried about the precision and recall, which were really low. Consequently, I tweaked the hyperparameters of the classifiers. For Naive Bayes, the only parameter alpha had no effect at all since the size of examples were so big. For Random Forest, I changed the number of estimators from 5 to 30, and the number of max features. But the precision and recall did not exceed 0.28. question What is the probable reason for such low recall and precision? How could I improve recall?
