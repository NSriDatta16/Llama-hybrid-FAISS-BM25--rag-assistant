[site]: datascience
[post_id]: 27134
[parent_id]: 27105
[tags]: 
In general, with Policy Gradients (PG) you can have two 'flavors' of them: Stochastic and Deterministic . In your case, you can build a NN that outputs continuous actions or one that approximates the sufficient statistics of a probability distribution that you will sample the actions from. The main references are: Theoretical Proofs and NNs with DPG . There are no rules on which one to use. There are many examples in the papers I cite that DPG outperforms SPG but also the opposite. Sometimes you really want the output to be deterministic. For example, in pricing: If a client comes in a store and you use SGD for assigning prices, the client will encounter every single time different prices (small differences but still differences). In my personal experience I find it a bit hard to stabilize the DPG with NNs and you need also to come up with a good exploration strategy. However, once the network is stabilized you get the continuous action values that could control your vehicle. This is a detailed example of controlling a car with DPG using tensorflow and Keras. For general theoretical exploration about the two frameworks I suggest you this Master Thesis. It provides a concise overview of the two methods (no NN implementations). You can look up also the answer I gave here if you decide to get on board with PGs.
