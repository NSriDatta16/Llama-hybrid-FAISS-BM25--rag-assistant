[site]: crossvalidated
[post_id]: 509186
[parent_id]: 
[tags]: 
Do Gaussian Mixture Models monotonically decrease the sum of squared distances when number of clusters increases?

I am comparing the clustering performance of two closely related machine learning methods: K-means and Gaussian Mixture Models (GMM). Part of this research is selecting the best number of clusters K . One of the measures I am using for this purpose is the within-sum of squared distances. Now, I know that the K-means algorithm monotonically decreases the within-sum of squared distances as the number of clusters K increases and actually finds a minimum if each observation has its own cluster. Given their relatedness, does the same relation hold for GMM? What is the rationale behind the conclusion? Any thoughts and/or references would be very much appreciated!
