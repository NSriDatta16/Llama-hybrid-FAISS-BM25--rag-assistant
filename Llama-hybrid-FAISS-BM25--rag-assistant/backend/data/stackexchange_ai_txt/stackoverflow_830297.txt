[site]: stackoverflow
[post_id]: 830297
[parent_id]: 828764
[tags]: 
As Martin Fowler says, the only thing surprising about the requirements for software changing is that anyone is surprised by it. The requirements will change, new features will be requested. This is a good thing. Enhancement efforts succeed most of the time, and when they fail, they fail small, so there is budget to do more. Big up front design projects fail often (one statistics puts the failure rate at 66%), so avoid them. The way to avoid them is to design enough for the first version, and as enhancements are added, refactor to the point where it looks like the system intended to do that in the first place. The lifespan of a project that can do this (there are issues when you publish data formats or APIs - once you go live you can't always be pristine anymore) is indefinite. In response to the four points, I would say that a process that shuns refactoring demands: A static world where nothing changes so that the upfront design can hit a non-moving target perfectly. Will result in ugly hacks to work around design flaws that aren't being refactored. Will lead to dangerous code duplication as the fear of changing existing code sets in. Will waste resources over engineering the problem and building large design artifacts in anticipation of requirements that never end up getting built, causing large amounts of code and complication to drag the project down while not providing any value. One caveat, though. If you don't have the proper support, in an automated tool for simple cases, and thorough unit tests in the more complicated cases, it will hurt, there will be new bugs introduced, and you will develop a (quite rational) fear of doing it more. Refactoring is a great tool, but it requires safety equipment.
