[site]: datascience
[post_id]: 122347
[parent_id]: 
[tags]: 
Are there any studies on the actual loss of quality in a quantized GPT model? (eg GPTQ)

I've been trying to think of how different a GPT would perform if we swapped from floats to 8-bit. Given that we take say the top 10 words, how much fidelity is actually lost? How many tokens are actually in the top selection? Especially if something like beam selection is implemented, I would imagine there are few, but I really don't know. Has anyone looked into it?
