[site]: crossvalidated
[post_id]: 488004
[parent_id]: 
[tags]: 
Understanding the Q-learning loss function?

Perhaps this can be explained a little more to me. I understand what's in literature but I'm struggling to understand why this is the preferred loss. If we have an agent that can move ↑↓→← and for instance chooses to move in the direction that's correct. say the reward is two steps to the left and the highest observed Q values are both left, left then why have these states regress? The max Q value will never be reached. $Q(s,a) = r + \gamma \max_{a'\in A}Q(s',a')$ This is never true because the updated $Q(s,a)$ in the direction chosen will never equal but tend towards $r + \gamma \max_{a'\in A}Q(s',a')$ Since the method of getting our value $Q(s, a)$ is $$Q(s,a) = (1-\alpha)Q(s,a)-\alpha(r + \gamma \max_{a'\in A}Q(s',a'))$$ I also see an unfortunate consequence of using this as our loss. If we move in a direction of no reward and then say lose and score a negative reward, so death/failure is all that awaits our agent down the chosen path. Then won't this movement see no loss as $$Q(s,a) = r + \gamma \max_{a'\in A}Q(s',a')$$ since $$0 = 0 + 1 \times 0 \therefore Q(s,a) = r + \gamma \max_{a'\in A}Q(s',a')$$
