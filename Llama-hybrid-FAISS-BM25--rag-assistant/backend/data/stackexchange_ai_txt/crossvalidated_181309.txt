[site]: crossvalidated
[post_id]: 181309
[parent_id]: 181294
[tags]: 
PCA is a linear transformation, and if you are keeping every dimension, then your data should have the same distance function. Supposing that your original data was some matrix $X$ and your resulting data is some transformed matrix $Y = AX$, then the distance should be unchanged: $$ d(y_i, y_j) = d(A x_i, A x_j) $$ and if you are using the cosine distance, $$ d(A x_i, A x_j) = 1 - ((A x_i)^T (A x_j)) / (\|A x_i\| \| A x_j \|) $$ and since the $\| A x_i \| = \| x_i \|$, $$ = 1 - (x_i^T A^T A x_j) / (\| x_i \| \| x_j \|) $$ and since $A^T A = I$, $$ = 1 - (x_i^T x_j) / (\| x_i \| \| x_j \|) = d(x_i, x_j) $$ so all this was a very roundabout way to say that distance calculations should not be affected. Edit, once code appeared: in the calculation of centered , you are normalizing the features for unit variance. This might be a good idea in general, but it's going to change the distances between points since essentially you are weighting some dimensions to be more or less important than others. In that case, your resulting data is some transformed matrix $Y = ANX$ where $A$ is some orthogonal basis (so $A^T A = I$ as before), but $N$ is just a diagonal matrix that is not necessarily $I$. In that case you can't show that $d(y_i, y_j) = d(x_i, x_j)$. However, you can show that $d(y_i, y_j) = d(N x_i, N x_j)$. That means that if you were to transform your input data to have unit variance in each dimension, then the distance would be the same as the post-PCA data.
