[site]: crossvalidated
[post_id]: 316535
[parent_id]: 
[tags]: 
Pre-computing feature crosses when using XGBoost?

Currently my team is using a logistic regression model with pre-computed feature crosses, to predict a binary outcome. So, given data with columns A and B with values {A1, A2} and {B1, B2} , there is a step of creating new feature AxB: {(A1, B1), (A1, B2), (A2, B1), (A2, B2)} and using that in the LR model. One colleague wants to use XGBoost instead, and has made the statement that with XGBoost such pre-computation of crosses is not necessary. However, this is not my understanding of how tree-based methods work; I thought that the trees can only place cuts orthogonally on columns in the dataset, and are therefore bad at finding relationships such as AB , A/B , B/A , which may be computed with a prior feature engineering step. A second colleague who is more familiar with the algorithm has said that pre-computing these features can still be helpful when using XGBoost, since otherwise the trees must be grown very deeply and the run time becomes prohibitive. Still, I am confused by this because again, it's not in my understanding at all that a tree-based algorithm would find crossed features. Can anyone explain whether XGBoost can really do this, and if so, is the ability dependent on the hyperparameters used, such as tree depth? Are there any good references to read that cover this point?
