[site]: crossvalidated
[post_id]: 316193
[parent_id]: 316150
[tags]: 
1) Any prior that models your belief before seeing the data is an acceptable prior. If it is truly a Bernoulli likelihood, then a Cauchy prior is a very unusual choice. By the nature of your parameter, you are granting the greatest weight on 0 and the smallest weight on 1. Did you intend that? 2) You should be using combinatoric hypotheses. So, for example, if your largest model is $y=\beta_1x_1+\beta_2x_2+\beta_3x_3+\alpha$, then you should be testing each of the following hypotheses (unless scientific logic excludes some): $y=\beta_1x_1+\beta_2x_2+\beta_3x_3+\alpha$ $y=\alpha$ $y=\beta_1x_1+\alpha$ $y=\beta_2x_2+\alpha$ $y=\beta_3x_3+\alpha$ $y=\beta_1x_1+\beta_2x_2+\alpha$ $y=\beta_2x_2+\beta_3x_3+\alpha$ $y=\beta_1x_1+\beta_3x_3+\alpha$ You also need to assign a prior weight to each of these hypotheses. You should not use the default burn-in or iterations. I just did a regression that needed an over two million cycle burn-in. You need to look at the behavior of the model over the parameter space is. The question of "is $\beta_x=0$?" is a frequentist way of viewing the world. Remember that the null is that it is equal to zero. This does not matter, what if it is very close to zero, but not zero. You should be doing Bayesian model selection instead. If a variable does not matter, it will fall out of the selection process. What if $\beta_2\ne{0}$, but including $x_2$ results in a bad model? You can be statistically significant, but irrelevant.
