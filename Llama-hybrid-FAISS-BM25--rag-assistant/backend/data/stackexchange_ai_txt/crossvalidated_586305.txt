[site]: crossvalidated
[post_id]: 586305
[parent_id]: 586298
[tags]: 
It is not required. In many cases, it might be a good idea, for example when you have a rather small dataset. On the other hand, if you have a lot of good data, so the set used for tuning was already decent, it might not be necessary. It not always will be possible as well, for example, if training your model cost as much as a new car (some large deep learning models) you might not have enough resources to repeat it, same if the training takes very long and you don't have the time. If you decide to re-train, it is a good idea to keep the held-out test set to make sure that the re-trained model performs as well as the one found during cross-validation, as there's always a risk that something goes wrong (e.g. bug in the code, for a trivial example).
