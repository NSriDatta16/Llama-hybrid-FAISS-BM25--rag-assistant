[site]: crossvalidated
[post_id]: 641481
[parent_id]: 641474
[tags]: 
$\sigma^2 tr((X^TX)^{-1}$ is telling us something about how difficult the regression problem is. First, $tr((X^TX)^{-1})$ is equal to the sum of the reciprocals of the eigenvalues of $X^TX$ . So, if $X^TX$ is close to singular ( $1/\lambda_i\rightarrow \infty$ ), we might have problems. This can occur with collinear columns in $X$ . With this fixed design problem, we should choose a $X$ to avoid this. So, the difficulty of this problem is a function of the design of $X$ and the observation variance. $\sigma^2(n-p)$ is telling us how the problem is affected by the dimensionality of the task. Consider $n=p$ though, for each point, you may simply pass your regression through each point. This may minimize prediction error on your observations at the cost of overfitting. In a sense, we may always minimize the observed prediction error by increasing the number of predictors. Again, if $\sigma^2$ is large, then we will on average make more errors in our prediction. The theorem you have not discussed is used to derive the t-distribution for inference on $\hat{\beta}$ , when $\sigma^2$ is unknown.
