[site]: crossvalidated
[post_id]: 486914
[parent_id]: 486880
[tags]: 
The loss function depends on the generator output $G(z)$ and discriminators outputs $D(x)$ and $D(G(z))$ . Both generator and discriminator are usually neural networks parametrized by some parameter vectors $\theta_{G}$ and $\theta_{D}$ . Therefore, the GAN loss function is optimized with respect to $\theta_{G}$ and $\theta_{D}$ . Such loss function is not convex, and is usually solved by gradient methods (SGD, Adam etc.). According to this GAN tutorial https://arxiv.org/pdf/1701.00160.pdf (section 3.2.2 on page 22), the optimization problem would be convex, if we would solve the problem directly in the functional spaces of D and G instead of the parameter spaces of $\theta_{G}$ and $\theta_{D}$ .
