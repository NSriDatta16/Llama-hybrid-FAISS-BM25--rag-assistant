[site]: datascience
[post_id]: 109142
[parent_id]: 109100
[tags]: 
is that what sklearn does internally anyway, i.e. training two models with one fit call? That depends on the model. The (penalized) linear regression models that the notebook starts with do basically that: in a multivariate linear regression , the coefficients for each target are separate, so that essentially you are estimating individual linear models. In these cases, the rest of your questions aren't applicable. However, the random forest model does something more interesting. The tree chooses splits to decrease the impurity for all the targets at the same time. See sklearn's User Guide . The model won't be "confused" between two distinct labels: it's simultaneously estimating them both. However, the scale of the two targets may be important then, depending on the implementation details; if one target is on a larger scale than the other, its MSE will be larger, and so the tree will favor splits that reduce its error at the expense of the other target. See this example notebook that shows that effect in the sklearn implementation.
