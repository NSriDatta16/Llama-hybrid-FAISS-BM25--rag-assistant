[site]: crossvalidated
[post_id]: 403122
[parent_id]: 401366
[tags]: 
For this type of problem you may want to consider ULMFiT or a similar fine-tuning approach. The advantage of this approach is that you fine-tune a language model trained on another corpus to your smaller corpus (of ~70k) and then you create a classifier on-top of a whole language model on the small amount of labelled data. I.e. instead of just using a first-layer embedding like the ones you mentioned, you get to use structure in the whole neural network (that you can fine-tuning with appropriate careful unfreezing of layers). This is implemented e.g. in the fastai python library and described in the lectures of the excellent fast.ai deep-learning course. Additionally, you may consider some data augmentation during your training (e.g. round-trip translation or word/phrase- substitution ).
