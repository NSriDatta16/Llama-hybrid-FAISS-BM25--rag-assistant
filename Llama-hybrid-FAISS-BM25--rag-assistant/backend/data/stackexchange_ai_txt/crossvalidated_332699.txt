[site]: crossvalidated
[post_id]: 332699
[parent_id]: 65705
[tags]: 
This is the expanded with code my old answer moved here from another thread . I've been doing for a long time computation of a square symmetric matrix of pairwise Mahalanobis distances in SPSS via a hat matrix approach using solving of a system of linear equations (for it is faster than inverting of covariance matrix). I'm not R user so I've just tried to reproduce @ahfoss' this recipe here in SPSS along with "my" recipe, on a data of 1000 cases by 400 variables, and I've found my way considerably faster. A faster way to calculate the full matrix of pairwise Mahalanobis distances is through hat matrix $\bf H$. I mean, if you are using a high-level language (such as R) with quite fast matrix multiplication and inversion functions built in you will need no loops at all, and it will be faster than doing casewise loops. Definition . The double-centered matrix of squared pairwise Mahalanobis distances is equal to $\mathbf{H}(n-1)$, where the hat matrix is $\bf X(X'X)^{-1}X'$, computed from column-centered data $\bf X$. So, center columns of the data matrix, compute the hat matrix, multiply by (n-1), and perform operation opposite to double-centering. You get the matrix of squared Mahalanobis distances. "Double centering" is the geometrically correct conversion of squared distances (such as Euclidean and Mahalanobis) into scalar products defined from the geometric centroid of the data cloud. This operation is implicitly based on the cosine theorem . Imagine you have a matrix of squared euclidean distances between your multivariate data poits. You find the centroid (multivariate mean) of the cloud and replace each pairwise distance by the corresponding scalar product (dot product), it is based on the distances $h$s to centroid and the angle between those vectors, as shown in the link. The $h^2$s stand on the diagonal of that matrix of scalar products and $h_1h_2\cos$ are the off-diagonal entries. Then, using directly the cosine theorem formula you easily convert the "double-centrate" matrix back into the squared distance matrix. In our settings, the "double-centrate" matrix is specifically the hat matrix (multiplied by n-1), not euclidean scalar products, and the resultant squared distance matrix is thus the squared Mahalanobis distance matrix, not squared euclidean distance matrix. In matrix notation: Let $H$ be the diagonal of $\mathbf{H}(n-1)$, a column vector. Propagate the column into the square matrix: H= {H,H,...} ; then $\mathbf {D_{mahal}^2} = H+H'-2 \mathbf{H}(n-1)$. The code in SPSS and speed probe is below. This first code corresponds to @ahfoss function fastPwMahal of the cited answer . It is equivalent to it mathematically. But I'm computing the complete symmetric matrix of distances (via matrix operations) while @ahfoss computed a triangle of the symmetric matrix (element by element). matrix. /*Matrix session in SPSS; /*note: * operator means matrix multiplication, &* means usual, elementwise multiplication. get data. /*Dataset 1000 cases x 400 variables !cov(data%cov). /*compute usual covariances between variables [this is my own matrix function]. comp icov= inv(cov). /*invert it call svd(icov,u,s,v). /*svd comp isqrcov= u*sqrt(s)*t(v). /*COV^(-1/2) comp Q= data*isqrcov. /*Matrix Q (see ahfoss answer) !seuclid(Q%m). /*Compute 1000x1000 matrix of squared euclidean distances; /*computed here from Q "data" they are the squared Mahalanobis distances. /*print m. /*Done, print end matrix. Time elapsed: 3.25 sec The following is my modification of it to make it faster: matrix. get data. !cov(data%cov). /*comp icov= inv(cov). /*Don't invert. call eigen(cov,v,s2). /*Do sdv or eigen decomposition (eigen is faster), /*comp isqrcov= v * mdiag(1/sqrt(s2)) * t(v). /*compute 1/sqrt of the eigenvalues, and compose the matrix back, so we have COV^(-1/2). comp isqrcov= v &* (make(nrow(cov),1,1) * t(1/sqrt(s2))) * t(v). /*Or this way not doing matrix multiplication on a diagonal matrix: a bit faster . comp Q= data*isqrcov. !seuclid(Q%m). /*print m. end matrix. Time elapsed: 2.40 sec Finally, the "hat matrix approach". For speed, I'm computing the hat matrix (the data must be centered first) $\bf X(X'X)^{-1}X'$ via generalized inverse $\bf (X'X)^{-1}X'$ obtained in linear system solver solve(X'X,X') . matrix. get data. !center(data%data). /*Center variables (columns). comp hat= data*solve(sscp(data),t(data))*(nrow(data)-1). /*hat matrix, and multiply it by n-1 (i.e. by df of covariances). comp ss= diag(hat)*make(1,ncol(hat),1). /*Now using its diagonal, the leverages (as column propagated into matrix). comp m= ss+t(ss)-2*hat. /*compute matrix of squared Mahalanobis distances via "cosine rule". /*print m. end matrix. [Notice that if in "comp ss" and "comp m" lines you use "sscp(t(data))", that is, DATA*t(DATA), in place of "hat", you get usual sq. euclidean distances] Time elapsed: 0.95 sec
