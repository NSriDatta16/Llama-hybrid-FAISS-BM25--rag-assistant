[site]: crossvalidated
[post_id]: 618592
[parent_id]: 
[tags]: 
(Un)Calibrated Logistic Regression Fit

I'm fitting a logistic regression on a large dataset (n=89260, 17 predictors) with a class imbalance (1% positive class). I've tried to follow Dr. Harrell's teachings so I fit my full pre-specified model (I don't expect any interaction between predictors): Logistic Regression Model lrm(formula = label ~ rcs(ery, 3) + rcs(hgb, 3) + rcs(htc, 3) + rcs(mcv, 3) + rcs(mch, 3) + rcs(mchc, 3) + rcs(rdw, 3) + rcs(leu, 3) + rcs(neu, 3) + rcs(lin, 3) + rcs(mon, 3) + rcs(eos, 3) + rcs(bas, 3) + rcs(plq, 3) + rcs(pct, 3) + rcs(mpv, 3) + rcs(pdw, 3), data = df, x = T, y = T) Model Likelihood Discrimination Rank Discrim. Ratio Test Indexes Indexes Obs 89260 LR chi2 3879.88 R2 0.402 C 0.954 FALSE 88368 d.f. 34 R2(34,89260)0.042 Dxy 0.908 TRUE 892 Pr(> chi2) With (I believe) good results (very low Brier score, high AUC and Dxy). Validation gave low optimism: > validate(model, B=200) Divergence or singularity in 121 samples index.orig training test optimism index.corrected n Dxy 0.9083 0.9078 0.9060 0.0018 0.9065 79 R2 0.4017 0.4055 0.3964 0.0091 0.3925 79 Intercept 0.0000 0.0000 -0.0689 0.0689 -0.0689 79 Slope 1.0000 1.0000 0.9821 0.0179 0.9821 79 Emax 0.0000 0.0000 0.0185 0.0185 0.0185 79 D 0.0435 0.0440 0.0429 0.0012 0.0423 79 U 0.0000 0.0000 0.0000 0.0000 0.0000 79 Q 0.0435 0.0440 0.0429 0.0012 0.0423 79 B 0.0080 0.0080 0.0080 -0.0001 0.0081 79 g 1.0549 1.0569 1.0385 0.0183 1.0366 79 gp 0.0153 0.0154 0.0152 0.0002 0.0150 79 The calibration plot, however, looked like this: Is this due to the low prevalence of the positive class? Why does it look "uncalibrated" despite the otherwise good scores above? And finally, where can I learn more about this subject, I'm clearly lost. Thanks!
