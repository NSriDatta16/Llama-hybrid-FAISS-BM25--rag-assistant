[site]: crossvalidated
[post_id]: 133827
[parent_id]: 133776
[tags]: 
(First off, I'd consider kernel density estimation a form of a machine learning model, so that's a strange dichotomy to make. But anyway.) If you really do have enough samples to do good density estimation, then the Bayes classifier formed via KDE, or its regression analogue the Nadaraya-Watson model, converges to the optimal model. Any drawbacks of this approach are then purely computational. (Naive KDE requires comparing each test point with every single training point, though you can get much better than that if you're clever.) The other problem is the enormous issue of bandwidth selection, but with a good enough training set this is again only a computational issue. In practice, however, you rarely actually have a good enough sample to perform highly accurate density estimation. Some issues: As the dimension increases, KDE rapidly needs many more samples; vanilla KDE is rarely useful beyond the order of 10 dimensions. Even in low dimensions, a density estimation-based model has essentially no ability to generalize; if your test set has any examples outside the support of your training distribution, you're likely screwed. The reason for this drawbacks is that density estimation-type models assume only that the function being learned is fairly smooth (with respect to the kernel). Other models, by making stronger assumptions, can learn with many fewer training points when the assumptions are reasonably well-met. If you think it's likely that the function you're trying to learn is more or less a sparse linear function of its inputs, then LASSO will be much better at learning that model with a given number of samples than KDE. But if it turns out to be $f(x) = \begin{cases} 1 & \lVert x \rVert > 1\\0 & \text{otherwise}\end{cases}$, LASSO will do essentially nothing and KDE will learn more or less the right model pretty quickly.
