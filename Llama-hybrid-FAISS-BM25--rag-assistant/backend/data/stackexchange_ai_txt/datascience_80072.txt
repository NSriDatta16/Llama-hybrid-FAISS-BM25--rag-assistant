[site]: datascience
[post_id]: 80072
[parent_id]: 
[tags]: 
Is Flatten() layer in keras necessary?

In CNN transfer learning, after applying convolution and pooling,is Flatten() layer necessary? I have seen an example where after removing top layer of a vgg16 ,first applied layer was GlobalAveragePooling2D() and then Dense(). Is this specific to transfer learning? This is the example without Flatten(). base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer. x=base_model.output x=GlobalAveragePooling2D()(x) x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results. x=Dense(1024,activation='relu')(x) #dense layer 2 x=Dense(512,activation='relu')(x) #dense layer 3 preds=Dense(3,activation='softmax')(x) #final layer with softmax activation This example is with Flatten(). vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) # don't train existing weights for layer in vgg.layers: layer.trainable = False # useful for getting number of classes folders = glob('Datasets/Train/*') # our layers - you can add more if you want x = Flatten()(vgg.output) # x = Dense(1000, activation='relu')(x) prediction = Dense(len(folders), activation='softmax')(x) # create a model object model = Model(inputs=vgg.input, outputs=prediction) What is the difference if both can be applied?
