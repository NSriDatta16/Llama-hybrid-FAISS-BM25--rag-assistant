[site]: datascience
[post_id]: 47789
[parent_id]: 47787
[tags]: 
Neural networks are also called as the universal function approximation which is based in the universal function approximation theorem . It states that: In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function Meaning a ANN with a non linear activation function could map the function which relates the input with the output. The function $y = x^2$ could be easily approximated using regression ANN. You can find an excellent lesson here with a notebook example. Also, because of such ability ANN could map complex relationships for example between an image and its labels.
