[site]: crossvalidated
[post_id]: 337032
[parent_id]: 336903
[tags]: 
Usually, the same architecture and parameters would not be good for training both GAN and WGAN. In a typical GAN, you want to avoid making the discriminator more powerful than the generator, and you want to avoid training the discriminator so much that it "overpowers" the generator and always finds the fakes. In WGAN, you want to make the discriminator as powerful as possible, possibly by giving it a larger network, and you also want to train it for as long as computationally feasible -- several iterations for every one iteration the generator trains. The theory behind WGAN requires that the discriminator has converged to the optimal discriminating function, so this is important. If for some reason, you really need to fix one architecture, choose one where the generator is about the same size as the discriminator, and then make sure when you're training the WGAN that you really train the discriminator a lot -- maybe 10x more than the generator.
