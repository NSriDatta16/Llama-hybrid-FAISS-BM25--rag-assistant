[site]: datascience
[post_id]: 106628
[parent_id]: 106615
[tags]: 
This is evaluation and it's done experimentally: with a test set of fresh instances containing the true target value, apply the model and measure the error across all the instances (e.g. with MAE, MSE, RMSE...). Assuming that the test is a sufficiently large representative sample of the data, it's possible this way to estimate the quality of the model statistically . For example, we can say that an instance is in average predicted within range $x$ of the true value. But in general it's impossible to know how good a prediction is for a specific instance : by definition, a model gives its best prediction. If the model was able to know that its prediction is bad, logically it should give a different prediction. Note that if this was possible, it would also be possible to build a near-perfect model iteratively: as long as the prediction is bad, try again. For the record, there are some task-specific cases where one attempts to estimate the confidence of a supervised model (for example MT quality estimation ). This is done by building a new supervised model in order to predict a confidence score. This new model can also make errors, of course.
