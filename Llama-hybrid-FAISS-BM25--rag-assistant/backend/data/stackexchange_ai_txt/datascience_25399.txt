[site]: datascience
[post_id]: 25399
[parent_id]: 
[tags]: 
How can we use the current rewards as a system input in the RUN time when working with Deep Q learning?

When tuning the Deep Q net parameters we use the immediate rewards . Specially in action replay and regressing . But in the run time we don't care about the rewards at all. Because our neural network will take states as the input . I am explaining this relevant to the deep mind Atari game play paper . Any one thing it is wastage of input ? Is there a mechanism we can use something like Score even in the run time . p.s - I understand how they update the Neural Net parameters with regression . Sometimes with TD(Î») .
