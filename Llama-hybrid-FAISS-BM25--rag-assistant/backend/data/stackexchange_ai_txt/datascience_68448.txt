[site]: datascience
[post_id]: 68448
[parent_id]: 68327
[tags]: 
Ok, let's put it like that: "no activation" means "linear model", i.e. a model that can only learn linear associations between your variables. This is a very big limitation! The whole point of Neural Networks is: almost any regularity we observe in the world is non-linear, therefore we must make them flexible enough to learn all these complex patterns. That's where non-linear transformations come into play. A bit more technically, a layer is like a function: output = f(Weights * Data + Bias) if all your f() 's in your layers are linear, even the output of the stack of layers such as: ANN_output = f_3( f_2( f_1(Weights * Data + Bias) ) ) # yeah that's a Neural Net will be linear as well. Non-linear activation functions let us overcome this limitation, and allow a model to learn patterns that could be either linear or not. And the more you stack layers (i.e. the more you nest functions into one another) the more complex the final non-linear transformation becomes, making the model able to learn more complicated things.
