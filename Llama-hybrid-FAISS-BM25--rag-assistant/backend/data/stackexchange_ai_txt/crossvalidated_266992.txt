[site]: crossvalidated
[post_id]: 266992
[parent_id]: 266783
[tags]: 
This scheme is called the Brier loss . It is a proper scoring rule , and hence only the optimal classifier is correct, etc. It corresponds, of course, to the $L_2$ distance between the predictive label distribution and the true label distribution (which is a point mass). Deep learning types these days strongly prefer the cross-entropy loss, which corresponds to the KL divergence $KL( y \| \hat y)$. This will penalize giving very low probabilities to the correct class very harshly, perhaps encouraging a flattening out of predicted probabilities relative to the Brier loss. Consider a $K$-way classification problem, where your estimate of the probability of the $i$th class is $\hat p_i$. Let $y$ be the correct label for a given instance $x$, and $B = (\hat p_y(x) - 1)^2$ the Brier loss. Then $$\nabla B = 2 (\hat p_y(x) - 1) \nabla \hat p_y(x),$$ whereas if $C(x, y, w) = - \log \hat p_y(x)$ is the cross-entropy loss, then $$\nabla C = - \frac{1}{\hat p_y(x)} \nabla \hat p_y(x).$$ Plotting these: We can thus see that the cross-entropy really emphasizes wrong values, whereas Brier loss scales just linearly with the probability estimate. Another interesting property: suppose that there are three categories, with the first one being correct. Cross-entropy would value the predictions $(.8, .2, 0)$ and $(.8, .1, .1)$ equally, whereas Brier loss would prefer the second one. I don't know if that's of huge practical importance, but only caring about the true category seems like a reasonable criterion to me, and that leads to cross-entropy being the only proper scoring rule.
