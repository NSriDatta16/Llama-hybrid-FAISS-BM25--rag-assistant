[site]: datascience
[post_id]: 120915
[parent_id]: 120913
[tags]: 
Just to take a couple of examples, GPT-4 performance was evaluated on HumanEval : https://arxiv.org/abs/2303.08774 , LeetCode: https://leetcode.com/ , and CodeForces. "One such metric is pass rate on the HumanEval dataset [43], which measures the ability to synthesize Python functions of varying complexity. We successfully predicted the pass rate on a subset of the HumanEval dataset by extrapolating from models trained with at most 1, 000Ã— less compute" CodeT : https://arxiv.org/pdf/2207.10397.pdf used HumanEval, MBPP, APPS, and CodeContests for performance evaluation; The github for HumanEval is here: https://github.com/openai/human-eval . hth.
