[site]: crossvalidated
[post_id]: 429012
[parent_id]: 429010
[tags]: 
What you're describing is simply the split in training data and test data , where the test data is not used for training at all. You use only the training data to train your model. To avoid overfitting (on metrics like MSE), you could use ideas like cross-validation or bootstrapping. You can estimate the generalization error on unseen data (which you don't have yet) by comparing your prediction with the learned model on the test data to the actual outcomes of the test data. Sometimes you split your training data further into training data and validation data , where the validation data is not used to train your model, but to assess if/when the training is sufficiently good (e.g. in iterative procedures like neural networks).
