[site]: datascience
[post_id]: 40867
[parent_id]: 40853
[tags]: 
R(a) = V(s') - V(s) This is not correct. The value of a state is based on all its future rewards. Your formula would be correct for a value function $V$ that accumulated past rewards, but that is not directly useful for action selection in reinforcement learning. The agent needs to choose an action that makes it the most reward in future. It cannot make any action to change what happened in the past, so value functions are forward-looking only. For instance, the value of a state where an agent has completed its task successfully (or failed) is always $0$ , because the agent can no longer act, and has no chance of any future reward. Without discounting, then: $$V(S_t) = R_{t+1} + V(S_{t+1})$$ Therefore: $$R_{t+1} = V(S_t) - V(S_{t+1})$$ i.e. the opposite sign than you thought, but compatible with the TD update rule.
