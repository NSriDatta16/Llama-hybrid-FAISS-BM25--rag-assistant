[site]: crossvalidated
[post_id]: 414220
[parent_id]: 
[tags]: 
Why CNNs are less prone to overfitting?

I'm doing a course on CNN by Andrew Ng. and in one of the lectures he said that due to Parameter Sharing and Sparsity of Connections in CNN it has fewer parameters which enables it to be trained with smaller training sets and also makes it less prone to overfitting. As per the second part ie. makes it less prone to overfitting, I think it's because having less parameters makes the decision boundary less complex as compared to one with more parameters. My conclusion, 2 models with same number of layers, the one with more hidden units will make more complex Decision Boundary as it has more non-linear activation functions and hence will be more prone to overfitting. But I don't understand, why it can be trained better than a standard NN if both are trained on small datasets. Any help is highly appreciated.
