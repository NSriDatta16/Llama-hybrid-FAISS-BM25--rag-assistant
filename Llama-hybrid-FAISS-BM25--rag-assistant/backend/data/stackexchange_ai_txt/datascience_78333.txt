[site]: datascience
[post_id]: 78333
[parent_id]: 78306
[tags]: 
It seems they use a shared RNN which process each row sequentially on the sequence of concatenated channels of individual pixels. From the paper Implementation with channels last Let the output of the ConvNet be of size (batch_size, height, width, channels) . The RNN expects an input of size (batch_size, sequence_length, input_size)`. So you have to reshape it with the following correspondence. batch_size*height -> batch_size channels -> input_size width -> sequence_length And process each row (along height dimension) with the same RNN and concatenate the result. To do that, we simply reshape to merge the batch and height axis into one dimension so that our RNN will process columns independantly. rnn_input = keras.layers.Reshape((batch_size*height, width, channels))(convnet_output) rnn_output = keras.layers.RNN(hidden_dim, return_sequences=True)(rnn_input) rnn_output will have shape (batch_size*height, width, hidden_dim) . You can then combine this tensor into a context vector using a dense layer with tanh activation, as it is written in the paper. The paper also use trainable initial state for the RNN, you might be interested in this library to implement it. Implementation with channels first If you set your Conv2D layer with "channels_first" , the output convnet_output will be of size (batch_size, channels, height, width) . Therefore you need first to permute the dimensions before reshaping. convnet_output = keras.layers.Permute((0, 2, 3, 1))(convnet_output) After this step, convnet_output has dimension (batch_size, height, width, channels) . You can then proceed as previously, reshaping and feeding to the RNN. rnn_input = keras.layers.Reshape((batch_size*height, width, channels))(convnet_output) rnn_output = keras.layers.RNN(hidden_dim, return_sequences=True)(rnn_input)
