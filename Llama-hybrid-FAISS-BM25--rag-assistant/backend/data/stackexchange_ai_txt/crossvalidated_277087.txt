[site]: crossvalidated
[post_id]: 277087
[parent_id]: 276916
[tags]: 
From your question: I train the logistic regression model on the 2-d data from the PCA. I plot the decision boundary using the intercept and coefficient and it does linearly separate the data. Logistic regresssion is not a classifier. Its coefficients certainly do not represent a "decision boundary." You have a model that's $$ \log \frac{p}{1-p} = \beta_0 + \beta_1x_1 + \beta_2x_2 $$ Where $p$ is the probability of your outcome, and the $x$-s are your principal components. In the below code, from your notebook, you're using $\beta_0$ and $\beta_1$ as the coefficients to a line in the original predictor space, and it's just dumb luck that it's anywhere near the gap in your data points. new_model.fit(x_pca, target) y_intercept = new_model.intercept_ # A decision would be based on where $\log p = \log (1-p)$ or some other threshold like that. This is what's not making sense with your plot, and how much variance the PCs capture is a secondary concern. Assuming you want your decision to be at $\log p = \log (1-p)$, this translates to $$\beta_0 + \beta_1x_1 + \beta_2x_2 = 0.$$ Let's say $x_1$ is the first principal component, and $x_2$ is the second. Then $x_2$ is the $y$-axis in your original plot. Rewriting the above, the boundary should be $$x_2 = -\frac{\beta_0}{\beta_2} - \frac{\beta_1}{\beta_2}x_1.$$ That is to say that the intercept you want is $-\frac{\beta_0}{\beta_2}$ and the slope is $-\frac{\beta_1}{\beta_2}$. Note that a such a decision is subjective, and while using $\log p = \log (1-p)$ might fit with your particular view of risk, others might like the raw probability estimate to make their own decision.
