[site]: crossvalidated
[post_id]: 318837
[parent_id]: 
[tags]: 
Logarithmic sampling in Monte Carlo instead of linear

The common case for a Monte Carlo simulation is, if we want to run our simulation for $N$ steps, we define a delta $\Delta,$ such that $N/\Delta = n$ tells us the frequency with which we measure/evaluate quantities of interest during the MC run. For instance, say $N=1000,$ and we want to measure the average of an observable $f,$ after each $n=100$ steps (so here $\Delta=10$). The average is obtained by dividing $f$ by the number of times $f$ has been measured. This is linearly sampling the time or MC steps and the simulation looks schematically like (writing in python style): interval = N/delta f = 0 countmeasurement = 0 for mcstep in range(1,N): interval -= 1 . . . if interval == 0: f += computef() . . interval = N/delta countmeasurement += 1 #save or print print "steps ", mcstep, "average f ", f/countmeasurement Question: With this scheme as sketched above, if later we plot $f$ as a function of MC-steps, on a log-log scaled plot, we will not have sampled the same amount of datapoints for each time scale as we sampled the total steps only linearly. How do we alter our sampling scheme, thus redefining interval , such that we gather the same number of datapoints for $f$ for each part (time-scale) of the log-log plot? Is it common to sample the MC-steps logarithmically in Monte Carlo simulations?
