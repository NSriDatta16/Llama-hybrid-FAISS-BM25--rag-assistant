[site]: datascience
[post_id]: 126157
[parent_id]: 75616
[tags]: 
In ML research, we typically call a network with one hidden layer "shallow" and one with two or more hidden layers "deep". If by "better" you mean the networks are more expressive and have larger capacities, then the answer is positive. The previous answer mentioned the universal approximation theorem already. On top of that, there is a more subtle discussion of what width does not buy you when compared with depth. More explicitly, there are two ways to make neural nets more expressive: fix depth to be 1 (single hidden layer), enlarge width; (NTK regime) fix widths to be some constant, enlarge depth; universal approx thm applies to both. Which one converges to low generalization error faster, if the two networks have the same number of total nodes? It turns out that in certain settings, one can show that the second one is preferred. Tons of references are available, I would just point to some of the brilliant discussions Disentangling feature and lazy training in deep neural networks , Width is Less Important than Depth in ReLU Neural Networks . These provide an incentive to go deeper rather than wider. I hope the takeaway is clear that if you would like to argue that "deeper is better", then UAP alone does not explain why . If by "better" you mean the networks are able to achieve better test performance, this may not be true. I would like to push back this notion a bit. More expressive architecture unfortunately does not imply better test performance in practice. Overfitting may occur. Plus, the training dynamic may not lead you to the solution (due to gradient vanishing and many other issues): you may not be able to find a good set of parameters starting with some Gaussian initializations, even with the help of smoothing or skip connections. Welcomes to discuss ~
