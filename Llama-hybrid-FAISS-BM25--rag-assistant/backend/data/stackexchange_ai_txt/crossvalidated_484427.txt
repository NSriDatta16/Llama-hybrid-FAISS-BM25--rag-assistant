[site]: crossvalidated
[post_id]: 484427
[parent_id]: 483859
[tags]: 
There's a pretty sensible way of doing this if you centre all your predictors by subtracting the mean value. Using your example, say you have athletic shorts, shorts, and jeans, and your numeric predictors are a) brightness (defined for all categories), and b) short length (defined for shorts only, NA for trousers). Now, if you centre your both your numeric predictors, you get measures of a) whether the items are darker or lighter than average, and b) whether the shorts are shorter or longer than average. You can safely say that the trousers are $\pm0$ cm shorter or longer than average, and so impute a value of 0 for this predictor for all items that aren't shorts (they're neither shorter nor longer than average). You can then fit a multilevel as you normally would, allowing all predictors to vary across categories: lmer(sales ~ 1 + centred_colour + centred_leg_length + (1 + centred_colour + centred_leg_length | category), data=sales_data) Since this predictor only varies for the different kinds of shorts, only sales of those items will have an effect on this parameter. Since it's set to 0 for all other products, this predictor won't affect inferences or predictions about them. If it happens that you end up with only one category of shorts in your data, if won't be possible to include it as a random effect, and you'll have to change your model accordingly: lmer(sales ~ 1 + centred_colour + centred_leg_length + (1 + centred_colour | category), data=sales_data)
