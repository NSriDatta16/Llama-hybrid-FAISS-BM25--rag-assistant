[site]: crossvalidated
[post_id]: 522988
[parent_id]: 178479
[tags]: 
The important thing is that the entire model fitting procedure is performed independently in each fold, including all of the feature and hyper-parameter tuning, as that is what is needed to avoid bias due to over-fitting in model selection. I implement this by making a function, something like: model = train_model(x, t) where x is the input and t is the targets for the training data, and another function: y = generate_outputs(model, x) where x is the test inputs and y will be the test predictions from the model. Now train_model will encode how the hyper-parameters etc. are tuned. As @marc_claesen suggests, it is better to perform hyper-paremeter and feature selection jointly, especially for things like SVM* where the regularisation parameter depends on the feature set. I then use those two functions for implementing the outer cross-validation. I would probably perform all of the other steps jointly using a single inner-cross-validation. Note however the more you tune the model using cross-validation, the more you will tend to overfit the model, so exhaustively searching for the best possible model tends to end up giving a rather poor one. In many machine learning challenges (e.g. at conferences), the models at the top of the leader board tend to be towards the bottom of the final rankings because they have often been over-fitted to the leaderboard data by too much manual model selection (I call this "cyberML", which is a bit like "autoML" but where the operator has become part of the machine). *for SVMs I normally find feature selection make generalisation worse rather than better, at least for linear SVMs, as they are based on a learning bound that is independent of the size of the feature space, so regularisation tends to work well. The generalisation theory for feature selection is much less rigorous, and invalidates the generalisation bounds for the SVM, and tuning one continuous regularisation parameter tends to be less prone to over-fitting the model selection criterion than fitting one binary hyper-parameter for each feature (which is essentially what feature selection is doing). So for things like SVMs, I tend only to do feature selection if selecting the features is a primary goal for the analysis. I agree with @gung's suggestion of using e.g. LASSO penalty for feature selection (fewer degrees of freedom, and less pressure on CV to find the best model)
