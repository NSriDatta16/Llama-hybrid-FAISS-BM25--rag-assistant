[site]: datascience
[post_id]: 63089
[parent_id]: 63079
[tags]: 
Feed forward networks are networks where every node is connected with only nodes from the following layer. They don't have "circle" connections. Data can only travel from input to output without loops. Examples would be Simple Layer Perceptron or Multilayer Perceptrion. Convolutional Neural Networks also are purely feed forward networks In opposition to that are recurrent neural networks. LSTM is one of those. These RNN can also be connected "sideways". Meaning that your data can travel not only forward into the next layer but also to other nodes in the same layer or backwards. From an architecture standpoint that means that although you have a network with maybe only one hidden layer, you get "deepness" by adding loops to nodes in that layer. Usually these represent "time" in the data. This image shows what I mean by "deepness" through feedback loops: https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg Although its technically one node in your layer architecture, it get's deeper and deeper the more loops you add
