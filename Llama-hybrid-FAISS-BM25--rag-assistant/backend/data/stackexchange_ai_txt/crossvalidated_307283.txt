[site]: crossvalidated
[post_id]: 307283
[parent_id]: 307210
[tags]: 
If what you want to do is learn simple periodic functions like this, then you could look into using Gaussian Processes. GPs allow you to enforce your domain knowledge to an extent by specifying an appropriate covariance function; in this example, since you know the data is periodic, you can choose a periodic kernel, then the model will extrapolate this structure.You can see an example in the picture; here, I'm trying to fit tide height data, so I know that it has a periodic structure. Because I'm using a periodic structure, the model extrapolates this periodicity (more or less) correctly. OFC if you're trying to learn about neural networks this isn't really relevant, but this might be a slightly nicer approach than hand-engineering features. Incidentally, neural networks and gp's are closely related in theory, so in principle there is some activation function you could choose that would do the same thing for a neural network GPs aren't always useful because unlike neural nets, they are hard to scale to large datasets and deep networks, but if you're interested in low dimensional problems like this they will probably be faster and more reliable. (in the picture, the black dots are training data and the red are the targets; you can see that even though it doesn't get it exactly right, the model learns the periodicity approximately. The coloured bands are the confidence intervals of the model's prediction)
