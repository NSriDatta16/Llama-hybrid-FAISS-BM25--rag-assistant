[site]: datascience
[post_id]: 64768
[parent_id]: 29634
[tags]: 
I had been using the naive structure proposed by you for quite some time by now. In a well framed problem and with enough data, this type of architecture works quite well. However here are a few things which I learnt: Tree based algorithms (RF, XGB) generally performs well with mixed classes unless you have some specific output requirements or loss function which is easier to implement via neural networks. If using neural network is decided, then this architecture performs better compared to other type of string-encoding ways. This approach also works with mixed input time-series data - much better than any classical time series approaches. The key design would be the concatenation layer and where would you want to put it in the architecture. Also using embedding layers gives you additional benefit of using those learnt embeddings in some other tasks / visualizations. These type of architecture has been used in Kaggle competitions and is also taught in the Fast.ai course by Prof. Jeremy Howard .
