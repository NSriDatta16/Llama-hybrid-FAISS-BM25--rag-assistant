[site]: crossvalidated
[post_id]: 620635
[parent_id]: 620002
[tags]: 
Recall that transformer blocks apply forward at each time-step independently (allowing for parallelism). Therefore, each position in the sequence only has access to its own features, which explains why layer normalization in these architectures do not average across the whole sequence.
