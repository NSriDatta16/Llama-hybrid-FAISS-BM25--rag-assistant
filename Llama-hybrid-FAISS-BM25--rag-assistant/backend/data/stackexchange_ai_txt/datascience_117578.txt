[site]: datascience
[post_id]: 117578
[parent_id]: 62658
[tags]: 
In order to avoid confusion by anyone reading this page: pooled_output in the BERT model is NOT a pooling operation applied on the hidden states of all tokens in the sequence. The pooled_output is generated by applying an additional dense layer on top of the [CLS] token hidden state. This pooled_output is the basis on which classification tasks are done in the original BERT paper. From the TF implementation https://github.com/google-research/bert/blob/master/modeling.py line 227: first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) self.pooled_output = tf.layers.dense( first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range)) So to answer the main question: As other already responded both the [CLS] token and the pooled_output are a fixed-length vector of the whole sentence so you can use both to embed and compare sentences. The difference between them is that the pooled_output is a result of a dense vector applied on top of the [CLS] token representation.
