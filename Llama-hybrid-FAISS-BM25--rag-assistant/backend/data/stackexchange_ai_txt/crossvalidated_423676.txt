[site]: crossvalidated
[post_id]: 423676
[parent_id]: 423537
[tags]: 
The general workflow concerning nested cross validation is this: Perform nested cross validation, this gives you an estimate of your generalization error. As you have mentioned, this gives you K sets of hyper-parameters that may or may not be different. You do not simply want to pick one of these hyper-parameter sets or take the average or the mode or anything else, you will largely just disregard these (though it might be smart to check them for stability; if you get wildly different hyper-parameter sets across folds it may require some further research). Once you have your generalization error from step 1, you now perform cross validation on your entire dataset to tune your hyper-parameters. This is the set of hyper-parameters that will build your final model. Disregard the error rates you get during this cross-validation, they are not your error rate, the error rate from step 1 is your error rate. There is a common misconception that when you estimate an error rate you estimate it for a given set of hyper-parameters. Rather the error rate estimate is for the algorithm as a whole given your data not a specific instance of that algorithm if that makes sense. With regards to your second question, yes you could consider the mean of the inner loops to be your "validation performance". Nested cross-validation is analogous to your "regular" CV procedure done many times with different folds and averaged.
