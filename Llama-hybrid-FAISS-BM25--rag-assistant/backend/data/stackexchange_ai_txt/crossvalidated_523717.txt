[site]: crossvalidated
[post_id]: 523717
[parent_id]: 523708
[tags]: 
An explanation rather depends on what your background is. Suppose you have some so-called independent variables $x_1,x_2,\ldots, x_k$ (they do not have to be independent of each other) where each $x_i$ takes takes values $x_{i,1}, x_{i,2}\ldots, x_{i,n}$ and you want a regression for a dependent variable $y$ taking values $y_{1}, y_{2}\ldots, y_{n}$ . Then you are trying to find a function $f(x_{1,j}, x_{2,j},\ldots, x_{k,j})$ of the independent variables which in some sense minimises the loss from using that function as some measure across the observations comparing all the $y_j$ and their corresponding $f(x_{1,j}, x_{2,j},\ldots, x_{k,j})$ Linear regression restricts the possible $f$ to those of the form $f(x_{1,j}, x_{2,j},\ldots, x_{k,j})=\beta_0+\beta_1x_{1,j}+\beta_2x_{2,j}+\ldots+\beta_kx_{k,j}$ for real values $\beta_0,\beta_1,\beta_2, \ldots ,\beta_k$ . Least squares regression uses a loss function of the form $\sum\limits_{j=1}^n (y_j - f(x_{1,j}, x_{2,j},\ldots, x_{k,j}))^2$ which you want to minimise by choosing a suitable $f$ . Ordinary Least Squares Linear Regression combines the linear form of estimator and minimising the sum of the squares of the differences, so both requirements. But other forms of regression may only use one or even neither of them. For example, logistic regression can be seen as not being linear (it is not least-squares either, instead using maximum likelihood techniques), while robust regression typically is not a simple least squares calculation though may be linear
