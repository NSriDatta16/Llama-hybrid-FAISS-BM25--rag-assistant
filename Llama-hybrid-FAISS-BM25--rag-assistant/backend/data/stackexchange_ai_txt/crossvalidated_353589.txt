[site]: crossvalidated
[post_id]: 353589
[parent_id]: 135208
[tags]: 
As mentioned, with the LASSO approach it is common to use cross-validation to select the fixed lambda value. Most people default to selecting the value of lambda within 1 SE of the used criteria (e.g., AUC). This is referred to as the most regularized model. I believe it was initially recommended by Friedman. Your follow-up question, what about your small sample. Well you definitely have a small sample and potential threats of sparsity. A general rule (sorry I don't have a citation) is that the smaller the sample the more folds you should use in cross-validation. This is to ensure greater independence between folds. However, you have a fairly small set so switching from k= 5 to 10 would mean silly small sets. I haven't seen simulation studies examining this. We know LASSO does well in p > n scenarios. Though we would want to question whether a model based on a sample of say 2 observations could accurately represent the overall population of interest and Gaussian assumptions? At this point I would ask whether or not your a prior domain knowledge could not come into play in regards to the feature selection. Or perhaps the use of Bayesian modeling/regularization given the small sample. I would note that your final results are going to be conditional on the modeling processs and the more models you run and switching approaches, the greater the risk for selective type I errors. P.S., Per your above output (top pane), presented in the degrees of freedom allude to number of terms in model, % deviance, and the associated lambda value for that model. You can also plot these data for a better interpretation of what is going on.
