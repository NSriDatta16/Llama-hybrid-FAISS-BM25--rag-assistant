[site]: stackoverflow
[post_id]: 2953454
[parent_id]: 2950380
[tags]: 
On the surface, the numbers Gartner produces are akin to answering the question: How many angels can dance on the head of a pin? . Unless you obtain a full copy of their report (costing big bucks) you will never know how they came up with or justified the 200 billion lines of COBOL claim. Having said that, Gartner is a well respected information technology research and advisory firm so I would think they would not have made such a claim without justification or explanation. It is amazing how this study has been quoted over the years. A Google search for "200 billion lines of COBOL" got me about 19,500 hits. I sampled a bunch of them and most attribute the number directly to the 1997 the Gartner report. Clearly, this figure has captured the attention of many. The method that you have taken to "debunk" the claim has a few problems: 1) How many mainframes have been sold This is a big question in and of itself, probably just as difficult as answering the 200 billion lines of code question. But more importantly, I don't see how determining the number of mainframes could be used in constraing the number of lines of code running on them. 2) How large were the programs in lines of code? COBOL programs tend to be large. A modest program can run to a few thousand lines, a big one into tens of thousands. One of the jokes COBOL programmers often make is that only one COBOL program has ever been written, the rest are just modified copies of it. As with many jokes there is a grain of truth in it. Most shops have a large program inventory and a lot of those programs were built by cutting and pasting from each other. This really "fluffs" up the size of your code base. Your assumption that a program must fit into physical memory in order to run is wrong. The size problem was solved in several different ways (e.g. program overlays, virtual memory etc.). It was not unusual in the 60's and 70's to run large programs on machines with tiny physical memories. 3) Was there no standard software? A lot of COBOL is written from scratch or from templates. A number of financial packages were developed by software houses the 70's and 80's. Most of these were distributed as source code libraries. The customer then copied and modified the source to fit their particular business requirement. More "fluffing" of the code base - particularly given that large segments of that code was logically unexecutable once the package had been "configured". 4) How many programmers did we need to write 200 billion lines of code Not as many as you might think! Given that COBOL tends to be verbose and highly replicated, a programmer can have huge "productivity". Program generating systems were in vogue during the 70's and early 80's. I once worked with a product (now defunct fortunately) that let me write "business logic" and it generated all of the "boiler plate" code around it - producing a fully functional COBOL program. The code it generated was, to be polite, verbose in the extreme. I could produce a 15K line COBOL program from about 200 lines of input! We are taking serious fluff here! 5) What about the competition? COBOL has never really had much serious competition in the financial and insurance sectors. PL/1 was a major IBM initiative to produce the one programming language that met every possible computing need. Like all such initiatives it was too ambitious and has pretty much collapsed inward. I believe IBM still uses and supports it today. During the 70's several other languages were predicted to replace COBOL - ALGOL, ADA and C come to mind, but none have. Today I hear the same said of Java and .Net. I think the major reason COBOL is still with us is that it does what it is supposed to do very well and the huge multi billion lines of code legacy make moving to a "better" language both expensive and risky from a business point of view. Do I believe the 200 billion lines of code story? I find the number high but not impossibly high given the number of ways COBOL code tends to "fluff" itself up over time. I also think that getting too involved in analyzing these numbers quickly degrades into a "counting angels" exercise - something people can get really wound up over but has no significance in the real world. EDIT Let me address a few of the comments left below... Never seen a COBOL program used by an investment bank. Quite possible. Depends which application areas you were working in. Investment banks tend to have large computing infrastructures and employ a wide range of technologies. The shop I have been working in for the past 20 years (although not an investment bank) is one of the largest in the country and it has a significant COBOL investment. It also has significant Java, C and C++ investments as well as pockets of just about every other technology known to man. I have also met some fairly senior applications developers here that were completely unaware that COBOL was still in use. I did a rough line count through our source control system and found around 70 million lines of production COBOL. Quite a few people that have worked here for years are completely oblivious to it! I am also aware that COBOL is rapidly declining as a language of choice, but the fact is, there is still a lot of it around today. In 1997, the period to which this question relates, I believe COBOL would have been the dominant language in terms of LOC. The point of the question is: Could there have been 200 billion lines of it in 1997? Counting mainframes. Even if one were able to obtain the number of mainframes it would be difficult to assess the "compute" power they represented. Mainframes, like most other computers, come in a wide range of configurations and processing capacity. If we could say there were exactly "X" mainframes in use in 1997, you still need to estimate the processing capacity they represented, then you need to figure out what percentage of the work load was due to running COBOL programs and a bunch of other confounding factors. I just don't see how this line of reasoning would ever yield an answer with an acceptable margin of error. Multi-counting lines of code. That was exactly my point when referring to the COBOL "fluff" factor. Counting lines of COBOL can be a very misleading statistic simply because a significant amount of it was never written by programmers in the first place. Or if it was, quite a bit of it was done using the cut-paste-tinker "design pattern". Your observation that memory was a valuable commodity in 1997 and prior is true. One would think that this would have lead to using the most efficient coding techniques and languages available to maximize its use. However, there are other factors: The opportunity cost of having an application backlog was often perceived to outweigh the cost of bringing in more memory/cpu to deal with less than optimal code (which could be cranked out quite a bit faster). This thinking was further reinforced by the observation that Moore's Law leads to ever declining hardware costs whereas software development costs remain constant. The "logical" conclusion was to crank out less than optimal code, suffer for a while, then reap the benefits in the years to come (IMO, a lesson in bad planning and greed, still common today). The push to deliver applications during the 70's through 90's led to the rise of a host of code generators (today I see frameworks of various flavours fulfilling this role). Many of these code generators emitted tons of COBOL code. Why emit COBOL code? Why not emit assembler or p-code or something much more efficient? I believe the answer is one of risk mitigation. Most code generators are proprietary pieces of software owned by some third party who may or may not be in business or supporting their product 10 years from now. It is a very hard sell if you can't provide an iron-clad guarantee that the generated application can be supported into the distant future. The solution is to have the "generator" produce something familiar - COBOL for example! Even if the "generator" dies, the resulting application can be maintained by your existing staff of COBOL programmers. Problem solved ;) (today we see open source used as a risk mitigation argument). Returning to the LOC question. Counting lines of COBOL code is, in my opinion, open to a wide margin of error or at least misinterpretation. Here are a few statistics from an application I work on (quoted approximately). This application was built with and is maintained using Basset Frame Technology (frame-work) so we don't actually write COBOL but we generate COBOL from it. Lines of COBOL: 7,000,000 Non-Procedure Division: 3,000,000 Procedure Division: 3,500,000 Comment/blank : 500,000 Non-expanded COPY directives: 40,000 COBOL verbs: 2,000,000 Programmer written procedure Division: 900,000 Application frame generated: 270,000 Corporate infrastructure frame generated: 2,330,000 As you can see, almost half of our COBOL programs are non-procedure Division code (data declaration and the like). The ratio of LOC to Verbs (statement count) is about 7:2. Using our framework leverages code production by about a factor of 7:1. So what should you make of all this? I really don't know - except that there is a lot of room to fluff up the COBOL line counts. I have worked with other COBOL program generators in the past. Some of these had absolutely stupid inflation factors (e.g. the 200 lines to 15K line fluffing mentioned earlier). Given all these inflationary factors and the counting methodology used in by Gartner, it may very well have been possible to "fluff" up to 200 billion lines of COBOL in 1997 - but the question remains: Of what real use is this number? What could it really mean? I have no idea. Now lets get back to the counting angels problem!
