[site]: crossvalidated
[post_id]: 223869
[parent_id]: 
[tags]: 
How to propagate error from convolutional layer to previous layerï¼Ÿ

I've been trying to implement a simple convolutional neural network. But I stuck in some questions. To be specific, assume there are 3 layers in a convolutional pass, marked as l-1, l, l+1 -layer respectively. And the l-1 layer is the input layer with shape (num_channels, img_height, img_width) ,the l layer is the convolutional layer with kernel shape (num_kernels, num_channels, filter_height, filter_width) , and the l+1 layer is the pooling layer with pooling size (pool_height, pool_width) . In this setting, I encountered several problems in the back-propagation phase. I've been stuck at this problem for over a week. I've studied many tutorials in neural networks, however, it seems that all of them stop with back-propagating the error to l layer with the formular: \begin{align} \delta_k^{(l)} = \text{upsample}\left((W_k^{(l)})^T \delta_k^{(l+1)}\right) \bullet f'(z_k^{(l)}) \end{align} The question is, how should I back-propagate error from the l layer to l-1 layer, in order to chain several conv-pooling passes together? About up-sampling. Assume I take a simple mean pooling strategy with pooling size (2, 2). It is said the up-sampling is like: \begin{matrix} \delta_{11}^{l+1} / \beta & \delta_{11}^{l+1} / \beta & \delta_{12}^{l+1} / \beta & \delta_{12}^{l+1} / \beta &...\\ \delta_{11}^{l+1} / \beta & \delta_{11}^{l+1} / \beta & \delta_{12}^{l+1} / \beta & \delta_{12}^{l+1} / \beta &...\\ \delta_{21}^{l+1} / \beta & \delta_{21}^{l+1} / \beta & \delta_{22}^{l+1} / \beta & \delta_{22}^{l+1} / \beta &...\\ \delta_{21}^{l+1} / \beta & \delta_{21}^{l+1} / \beta & \delta_{22}^{l+1} / \beta & \delta_{22}^{l+1} / \beta &...\\ ... &...&...&... \end{matrix} What confuses me is, what is the value of $\beta$ , is it 1/4 or 1? What about L2 norm pooling? If I take max pooling, it seems that I have to keep track of the position of the maximum value in each patch of feature maps of l layer, which is quite complicate to do. Am I wrong? Is there any better way?
