[site]: crossvalidated
[post_id]: 263081
[parent_id]: 
[tags]: 
Why is the input that maximally activates a sparse autoencoder hidden unit of the following form?

This source on feature visualization asserts that the input vector $X$, such that $\|X\|_2 \leq 1$, which maximally activates a sparse autoencoder's $i$th hidden unit is of the form $$X_j = \frac{W^{(1)}_{ij}}{\sqrt{\sum_{j=1} (W^{(1)}_{ij})^2}},$$ where $W^{(1)}_{i}$ are the weights that the $i$th hidden unit depends on. It is assumed that the hidden units are using sigmoid activation functions. Now obviously the denominator is simply making sure the norm of $X$ is less than or equal to $1$. Also, as the weights can be positive or negative, keeping the inputs the same sign as the weights will result in higher activation values. After this, however, I am stuck. The source says it is not too hard to prove this, but I can't seem to find a way. An acceptable answer will most likely contain a formal proof, but some helpful intuition may also do the trick.
