[site]: datascience
[post_id]: 63322
[parent_id]: 
[tags]: 
Random Forest VS LightGBM

Random Forest VS LightGBM Can somebody explain in-detailed differences between Random Forest and LightGBM? And how the algorithms work under the hood? As per my understanding from the documentation: LightGBM and RF differ in the way the trees are built: the order and the way the results are combined. It has been shown that GBM performs better than RF if parameters tuned carefully. Random Forest: RFs train each tree independently, using a random sample of the data. This randomness helps to make the model more robust than a single decision tree, and less likely to overfit on the training data My questions are When would one use Random Forests over Gradient Boosted Machines? What are the advantages/disadvantages of using Gradient Boosting over Random Forests?
