[site]: datascience
[post_id]: 110894
[parent_id]: 62658
[tags]: 
This is an excellent guide on using sentence/text embedding for similarity measure. Important : BERT does not define sentence level - so basically anything between [CLS] and [SEP] is a piece of text for which you can use output embedding. https://github.com/VincentK1991/BERT_summarization_1/blob/master/notebook/Primer_to_BERT_extractive_summarization_March_25_2020.ipynb This approach uses [CLS] token value for 768 dimension or basically the cls_head in your question. As S-BERT is mentioned earlier , it contends taking [CLS] token's embedding does not work very well for text matching , natural language inference etc. They finetune BERT on a loss objective , such that sentences which entail one another has higher similarity score and they use mean pooling of token embedding rather than taking the [CLS]. In the end you will get the same result - a vector of [1,768]. Following link has an excellent tutorial for this - https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/
