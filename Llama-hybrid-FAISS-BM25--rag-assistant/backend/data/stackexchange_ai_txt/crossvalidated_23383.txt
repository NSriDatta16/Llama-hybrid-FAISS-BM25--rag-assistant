[site]: crossvalidated
[post_id]: 23383
[parent_id]: 23382
[tags]: 
UCB is indeed near optimal in the stochastic case (up to a log T factor for a T round game), and up to a gap in Pinsker's inequality in a more problem dependent sense. Recent paper of Audibert and Bubeck removes this log dependence in the worst case, but has a worse bound in the favorable case when different arms have well-separated rewards. In general, UCB is one candidate from a larger family of algorithms. At any point in the game, you can look at all arms that are not "disqualified", that is, whose upper confidence bound is not smaller than the lower confidence bound of some arm. Picking based on any distribution of such qualified arms constitutes a valid strategy and gets a similar regret up to constants. Empirically, I do not think there has been a significant evaluation of many different strategies, but I think UCB is often quite good. Most of the more recent research has focused on extending bandit problems beyond the simple K-armed setting with stochastic rewards, to very large (or infinite) action spaces, with or without side information, and under stochastic or adversarial feedback. There has also been work in scenarios where the performance criteria are different (such as the identification of best arm only).
