[site]: crossvalidated
[post_id]: 180600
[parent_id]: 180571
[tags]: 
It's to make the Langrange multiplier formulation a little less messy, specifically for the partial derivative to $\mathbf{w}$ as was hinted in the comment: $$ \begin{align} L_p &= \frac{1}{2}||\mathbf{w}||^2+C\sum_{i=1}^n\xi_i -\sum_{i=1}^n\alpha_i\Big[y_i\big(\langle\mathbf{w},\varphi(\mathbf{x}_i)\rangle+b\big)-(1-\xi_i)\Big]-\sum_{i=1}^n\mu_i\xi_i, \\ \frac{\partial L_p}{\partial \mathbf{w}}&=\mathbf{w}-\sum_{i=1}^n \alpha_i y_i \varphi(\mathbf{x}_i), \end{align} $$ where $\xi$ is the vector of slack variables, $\alpha$ the vector of support weights, $b$ the bias and $\varphi(\cdot)$ the embedding function. A direct result of this convention and the KKT conditions (which imply $\frac{\partial L_p}{\partial \mathbf{w}}=0$) is the well-known form of the separating hyperplane $\mathbf{w}$ in the feature space spanned by $\varphi(\cdot)$ : $$ \mathbf{w}=\sum_{i=1}^n \alpha_i y_i \varphi(\mathbf{x}_i), $$ and the decision function $f(\cdot)$: $$ f(\mathbf{z}) = \sum_{i=1}^n \alpha_i y_i \kappa(\mathbf{x}_i, \mathbf{z}) + b = \sum_{i=1}^n \alpha_i y_i \langle \varphi(\mathbf{x}_i), \varphi(\mathbf{z})\rangle + b, $$ with $\kappa(\cdot,\cdot)$ the kernel function.
