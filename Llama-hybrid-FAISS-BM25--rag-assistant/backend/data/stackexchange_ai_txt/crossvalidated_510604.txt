[site]: crossvalidated
[post_id]: 510604
[parent_id]: 509715
[tags]: 
It turns out this was only co-incidentally to do with the increase in the length of the time series. I was using a generator to iterate the training set. With the increase in the length of the time series, I tweaked the generator to use less RAM and introduced a bug. It turned out that the generator was progressively scrambling the training examples. On the first few epochs, the training samples were not so scrambled that the model couldn't find a pattern to fit, so the AUROC increased. As the scrambling was random, any pattern in the data progressively became noise and the model soon starts to under-estimate the amount of scrambling on the current epoch and so the loss increases each time.
