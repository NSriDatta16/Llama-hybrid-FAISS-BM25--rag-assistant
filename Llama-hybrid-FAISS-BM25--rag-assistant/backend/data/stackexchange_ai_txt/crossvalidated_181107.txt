[site]: crossvalidated
[post_id]: 181107
[parent_id]: 181068
[tags]: 
That you have daily aggregated sales information for only 1 or 2 months, even for thousands of products and their variations, limits the possible analyses. For instance, if your sales are strongly seasonal, e.g., as a function of the winter holidays or conversely the warm summer months, then you won't be able to integrate this potentially important information into the model. Per your question, as I see it, you have two broad options: turn the process over to one of the many vendors out there providing automated retail solutions or, alternatively, do it yourself. Regardless of which option you choose, you would be wise to do a whole lot of exploratory work on this dataset just so that you feel like you understand it. That way, if the solution used (whatever it is) returns nonsense, you will have a good sense of when that is occurring. There are plenty of vendors of turnkey, automated retail solutions. Here are a few of the big names. Others can provide additional names: IBM's Demandtec or omni-channel solutions http://www-01.ibm.com/software/info/demandtec/ McKinsey's Periscope for retailers http://www.periscope-solutions.com/ Planet Retail http://www1.planetretail.net/what-we-do Khi Metrics http://www.groceryretailonline.com/doc/khi-metrics-0001 And in terms of DIY, given the massive volume of information and the relatively short time frame (~30 days), I doubt that traditional, univariate, "Box-Jenkins," ARIMA, VAR-type approaches lend themselves that readily to turnkey solutions. First of all, the approaches rely on many more data points just to initialize the lags and moving averages than 30 days. Second, and to the best of my knowledge, they aren't fully multivariate in the sense that a pooled or multilevel model might but others can disagree. Regarding the suggestion made to use Hyndman's functional time series analysis, I can't evaluate the adequacy of that recommendation. I think you need to find a functional form for the model that is flexibly appropriate for: 1) The relatively short span of historic information 2) The massively categorical nature of the products 3) The hardware and software challenges of processing huge volumes of information 4) The need to update and produce compiled parameters on some regularly scheduled basis so that automated predictions can be made Presumably -- or better, hopefully -- you aren't doing this work on a single laptop or workstation but have access to some sort of multi-core platform such as AWS integrated with software such as Ufora . Ufora offers massively parallel analyses on AWS. There are workarounds to the limits, even in the cloud, of RAM or working memory for statistical modeling. These include the many variants of so-called "divide and conquer" algorithms which, essentially, amount to a greatly extended random forests approach. I've heard of shops where they will execute 3 or 4 million "mini-models" or random forest resampling iterations in a few hours on a 100 core Hadoop platform, then roll it up on the back end. A good reference for this is Chen and Xie's http://dimacs.rutgers.edu/TechnicalReports/TechReports/2012/2012-01.pdf In terms of the functional form, I think you're looking at a variation of multi-level modeling...whether that be pooled OLS, HLMs, GAMS, whatever, is a process that needs to be explored and cannot be determined in advance.
