[site]: crossvalidated
[post_id]: 41524
[parent_id]: 
[tags]: 
Evaluation method when using a large training set and a small test set

I am facing the evaluation of two text classifiers. I have a large training dataset (to be used for training only), and a separated small test set (to be used for testing only), both being balanced . Which one of the following methods should be most appropiated one and why? 1) Stratified Repeated held-out evaluation (repeated subsampling): Sample k times without repetition from the training dataset, each sample being balanced. For every sample, classifier is trained with the sample, and accuracy is tested with the full test dataset. Results are averaged. 2) Stratified Cross-validation: Divide the full training dataset in k slices with equal size, each slice being balanced. For every slice, classifier is trained with the slice, and accuracy is tested with the full test dataset. Results are averaged. 3) Stratified Bootstrapping: Sample k times with repetition from the large training dataset, each sample being balanced. For every sample, classifier is trained with the sample, and accuracy is tested with the full test dataset. Results are averaged.
