[site]: datascience
[post_id]: 82058
[parent_id]: 81914
[tags]: 
Remember that LinearSVM uses the full data and solve a convex optimization problem with respect to these data points. SGDClassifier can treat the data in batches and performs a gradient descent aiming to minimize expected loss with respect to the sample distribution, assuming that the examples are iid samples of that distribution. As a working example check the following and consider: Increasing the number of iterations Increase the number of iterations before the early stoping Both classifier should be using the same loss function, in this case "squared hinge" from sklearn.datasets import load_iris from sklearn.linear_model import SGDClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split X, y = load_iris(return_X_y= True) X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42) sgd = SGDClassifier(random_state= 42,loss = "squared_hinge", max_iter= 100000,n_iter_no_change=1000).fit(X_train, y_train) linearsvm = LinearSVC(random_state= 42,max_iter= 100000).fit(X_train, y_train) sgd.score(X_test, y_test) linearsvm.score(X_test,y_test) Before modifying these two parameters, SGDClassifier gave ~ 20% less accuracy After both performed the same on the test set
