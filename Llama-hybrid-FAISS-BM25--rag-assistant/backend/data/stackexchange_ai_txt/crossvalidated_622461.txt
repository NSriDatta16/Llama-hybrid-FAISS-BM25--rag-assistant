[site]: crossvalidated
[post_id]: 622461
[parent_id]: 622459
[tags]: 
What is true is the following: if $\epsilon \sim \mathcal{N}(0,\sigma^2)$ , then $\mathbb{E}[p(x+\epsilon)] = (p * \omega)(x)$ , if $\omega$ is the density of $\mathcal{N}(0,\sigma^2)$ . In fact, this formula is a special case of a more general formula, valid for any law! That is, if $E$ is a random variable of density $\phi$ , then $\mathbb{E}[p(x+E)] = (p * \widehat{\phi})(x)$ , where $\widehat{\phi}$ denotes the map $y \mapsto \phi(-y)$ . Here is the proof: $\mathbb{E}[p(x+E)] = \int p(x+y)\phi(y)dy = \int p(z)\phi(z-x)dy = \int p(y)\widehat{\phi}(x-y)dy = (p*\widehat{\phi})(x)$ . Note that since the density of the gaussian is symmetric, $\widehat{\omega} = \omega$ . So, the relation you quote is true « on average », that is, for all $x$ , the random variable $p(x+E)$ has expectation $(p*\widehat{\phi})(x)$ . However, if $p$ is not constant, then this random variable has no reason to be almost surely constant, hence should not be almost surely be equal to the value of the convolution. Stated otherwise, if you can generate some random $y$ values from a $\mathcal{N}(0,1)$ source and compute $p(x+y)$ for these $y$ 's, you will very likely get a bunch of different results. However, the average of these values is indeed $(p*\widehat{\phi})(x)$ .
