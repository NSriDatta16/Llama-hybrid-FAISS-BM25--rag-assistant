[site]: datascience
[post_id]: 82511
[parent_id]: 82478
[tags]: 
I think there are (at least) two parts to take into account in evaluating such a model: Whether the generated text correctly relate to the input topic Whether the generated text is grammatically and semantically acceptable In my opinion the first kind of evaluation could reasonably be done with an automatic method such as the one you propose. Note that cosine scores should not be interpreted absolutely: you should probably compute cosine similarity with a random sample of topics, and normally one expects the similarity to be much higher with the input topic than any other. You could also think of other variants, for instance training topic models on the generated text together with a sample of documents from various known topics, then check that the generated text belongs to the target topic (i.e. it should be grouped with the documents known to belong to this topic). For the second kind of evaluation, it would be difficult and unreliable to use an automatic method. As far as I know the only reliable way would be to ask human annotators to assess whether the text is grammatically correct and whether its content makes sense. If you're going to do that you might as well ask them to annotate to what extent the text is related to the topic. [added following comment] if you check whether the generated text is similar to the topic only by computing similarity with this target topic, what you obtain is for instance an average cosine score. Then you would probably select a threshold: for instance if the similarity is higher than 0.5 then consider that the text is indeed related to the topic. But there are two problems with this option: In some cases the average similarity will be lower than the threshold even though the text is correctly related to the topic. This could happen for example with a very "broad" topic which covers a large vocabulary. On the contrary you might have cases where the average similarity is higher than the threshold, but actually comparing to another topic would give an even higher similarity value. These issues are due to interpreting the similarity score "absolutely", as opposed to interpreting it relatively to other similarity scores. Instead you can calculate the similarity not only against the target topic but also against other topics, and then just check that the target topic is the most similar topic (or at least one of the top similar). This way : The target similarity score may be low, as long as it's higher than the other topics you can detect the case where another topic happens to have higher similarity than the target topic
