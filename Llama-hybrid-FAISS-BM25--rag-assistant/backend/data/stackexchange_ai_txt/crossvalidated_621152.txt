[site]: crossvalidated
[post_id]: 621152
[parent_id]: 
[tags]: 
Is the concept "statistical model" irrelevant in (supervised) machine learning?

In supervised machine learning, we are given a set of data $\{x_i\}_{i=1}^N$ where each data is associated with a label $\{y_i\}_{i = 1}^N$ . We would like to create/train functions $f$ to do several things. create a best-fit curve: $f: x_i \to y_i$ such that a metric (training error) is minimized. regression: assign a real value to a new data classification: assign a discrete label to a new data These are the most common tasks that a supervised machine learning do. The deep neural network is the most popularly used, general functional form $f$ that can do these tasks (which encapsulates linear, logistic, multi-class logistic, probit regression), among others. Observe one key thing in my description: I did not mention anything about the existence of a "statistical model" that so many machine learning authors discuss in their textbooks. By statistical model, I refer some prescribed relationship between the set of data and the labels as described in the first sentence. For example: according to https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf A linear model is: $y_i = \beta_0 + \beta_1^T x_i + \epsilon_i$ where $\epsilon_i$ is a noise term. A general linear model is: $y_i = \beta_0 + \beta_1^T x_{1i} + \ldots + \beta_p^T x_{pi} + \epsilon_i$ A logistic regression model is: $\log(y_i) = \beta_0 + \beta_1^T x_{i} + \epsilon_i$ There are also non-parametric, semi-parametric models, all following the following form $y_i = m(x_i) + \epsilon_i$ where $m$ is some function. I am confused why this discussion of statistical model is needed, especially in the context where the authors are trying to use these statistical model to essentially do the same thing as machine learning models. Here are some related concerns: Intuitively, in the real-world we do not know what is the relationship between label and data. So why would bother creating a relationship between them? In no part of modern machine learning training/validation/testing routine do we require the assumption of a clear relationship between $x_i$ and $y_i$ . It literally does not provide us with any additional information. In modern deep supervised machine learning, the concept of a model is not even remotely mentioned. For example, it doesn't seem to make sense to say things such as "what's the statistical model of GoogLeNet", or "what's the statistical model of LSTM". I mean, what is the functional form of $m$ and noise assumption on $\epsilon_i$ , $y_i = m(x_i) + \epsilon_i$ for things like DenseNet or U-Net? Can someone please help me understand this?
