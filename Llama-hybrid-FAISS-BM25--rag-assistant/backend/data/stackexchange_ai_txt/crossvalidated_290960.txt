[site]: crossvalidated
[post_id]: 290960
[parent_id]: 290958
[tags]: 
I was under the belief that scaling of features should not affect the result of logistic regression. However, in the example below, when I scale the second feature by uncommenting the commented line, the AUC changes substantially (from 0.970 to 0.520) ... I believe this has to do with regularization That is a good guess. If you look at the documentation for sklearn.linear_model.LogisticRegression , you can see the first parameter is: penalty : str, ‘l1’ or ‘l2’, default: ‘l2’ - Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. Regularization makes the predictor dependent on the scale of the features. If so, is there a best practice to normalize the features when doing logistic regression with regularization? Yes. The authors of Elements of Statistical Learning recommend doing so. In sklearn , use sklearn.preprocessing.StandardScaler .
