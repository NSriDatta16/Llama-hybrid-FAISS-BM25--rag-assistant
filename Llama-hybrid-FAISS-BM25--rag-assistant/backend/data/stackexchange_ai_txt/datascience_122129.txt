[site]: datascience
[post_id]: 122129
[parent_id]: 122123
[tags]: 
The metrics you talk are better understood for a two-class problem, they can still be used for more than 2 classes, let's see how checking values of precision and recall for a given class. Precision for class 3 Let's say we want to know how well the ML model is doing for class 3 (the forth class in your image), since we have more than 2 classes I have to evaluate how the model classifies samples for this class vs. all the other classes . Let's take class 3 , the model sees 21 samples (we count the values on the column: 5+2+13+1=21 ). 13 of those samples are correctly predicted (they are on the matrix diagonal), 8 are not (they are spread on multiple other classes), so the precision (PPV) ( wiki: sensitivity vs. specificity ) for class 3 is 13/(13+8) = 0.62 ; in this case 13 are TP (true positives) and 8 are FP (false positives: note that our model gives them class 3 but those 8 values are spread on the other classes). Recall (sensitivity) for class 3 For the same class 3 we know there are, in my dataset, 20 samples (here I count the row values: 2+3+13+2=20 ); with this in mind you can get the recall (TPR) for class 3 : TP/(TP+FN) = 13/(13+7) = 0.65 , where 13 is still TP , and 7 is FN (false negatives: note that we know they belong to class 3 but have been misclassified by our model). You can get the remaining values knowing that N=TN+FP (negatives) and P=TP+FN (positives). Metrics for confusion matrix If you check and example of classification with multiple classes you will notice the use of macro average and weighted average ( scikit-learn: Label Propagation digits active learning ). Those are useful to interpret how good your model is globally on every class.
