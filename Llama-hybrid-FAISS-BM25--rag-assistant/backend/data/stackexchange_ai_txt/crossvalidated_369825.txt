[site]: crossvalidated
[post_id]: 369825
[parent_id]: 
[tags]: 
Should I use other scaling methods for pre-processing the data rather than normalizing or MinMaxScaling?

I have a weather parameter (daily volume of inflow for a river in million cubic meters, MCM) time series data as follows: I want to scale this data and feed it to a LSTM network. I know that by choosing a practical and good method of scaling, performance of the LSTM highly increases. I think using tools like MinMaxScaler in python's sklearn.preprocessing would not be a good choice in this case, since all values are above zero and by scaling to a range between -1 and 1, data just become smaller and nothing changes. Also, when I try to normalize the data using this method: df_norm = (df - df.mean()) / (df.max() - df.min()) , the results become: So, I think this also is not a good choice. Here is the histogram results of the data: The goal is to be able to create a predictive model in which I can predict 7 days ahead using LSTM. What would be a suitable scaling technique for this kind of data?
