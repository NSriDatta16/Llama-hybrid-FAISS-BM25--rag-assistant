[site]: crossvalidated
[post_id]: 327627
[parent_id]: 327623
[tags]: 
Case I and Case II aren't so different as you might think: just consider the joint distribution of $(\mathbf x, y)$, which you've helpfully already notated as $\mathcal D$. Then any two-sample testing procedure would work; I'm partial to the maximum mean discrepancy test described by Gretton, Borgwardt, Rasch, Schölkopf, and Smola. A Kernel Two-Sample Test . JMLR 13(Mar):723−773, 2012. It's particularly good if you (warning: shameless self-promotion) optimize the kernel with the technique proposed by the following paper: Sutherland, Tung, Strathmann, De, Ramdas, Smola, and Gretton. Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy . ICLR 2017. ( code ) There are also methods that propose to automatically adapt for possible changes between the training and the test distribution, perhaps with a few labeled examples in the test distribution. This field in general is known as transfer learning , and there's a fair amount of recent work in it; often it attempts to make the two distributions look similar to a two-sample test. A reasonable starting point is this 2016 PhD thesis (by a former labmate of mine), though there is also surely a lot of very recent deep learning work that isn't discussed there (and that I'm not very familiar with). For your related question, I don't think it's the case that nonparametric models are in general more robust to distribution shift than parametric methods. You might be able to come up with examples where they are, but it's going to be highly dependent on the type of shift you assume; I'm sure you can also come up with examples where the nonparametric methods break faster than the parametric ones.
