[site]: crossvalidated
[post_id]: 371360
[parent_id]: 371342
[tags]: 
If your goal is as stated, then I would use neither. The problem appears more complex, and I would not compromise it by using simplistic statistics. you have repeated measures (multiple measurements for a given person) it is a longitudinal study there may be correlations across time series (e.g. number of work hours by end of December compared to number of work hours by end of January) the assumption that only the period directly leading up to the suicide is predictive might be incorrect, as depression (which often causes suicide) is not an acute state Given the last point, a t-test comparison between the two groups (no suicide vs suicide) seems a more reasonable approach, but you can't be sure whether the predictive power is hidden at earlier time points. What are your group sizes? How many months of data do you have for each individual? Moreover, before I would start doing anything with the data, I would remove a third of the samples and not touch them at all while you work on the remaining data set. Once you think you have figured out what your predictors are, you will use the test set to validate your findings, preferably in a blinded run. This is what we do in predicting disease outcomes. I strongly suggest that you do that; it will strengthen your conclusions immensely. POST SCRIPTUM: We have a paper in press with a somewhat similar setup. The readout was a disease (about 100 cases and a couple of hundred people who remained healthy) and the predictors were molecular data collected during two years leading up to the diagnosis (or end of follow-up for the controls). What we did was to use a machine learning approach (random forests), and treated each sample separately (i.e. sample from six months before was treated separately from sample 1 month before), and also tested the predictive power of different strata (for example, can we predict the onset based on samples collected more than half a year before the diagnosis?). However, this may be dangerous (overfitting), so we took utmost care to avoid it. Among other precautions (like never splitting samples from the same person when doing k-fold) we evaluated our predictions on a blinded, separate validation test set. Whatever else you plan, do yourself a favor and make this validation set, just make sure that it is stratified correctly (i.e. that there are no differences say in gender ratio or age between the training set and test set). You can always evaluate the full set (training + validation) post hoc.
