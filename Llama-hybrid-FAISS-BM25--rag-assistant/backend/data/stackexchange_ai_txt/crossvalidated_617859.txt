[site]: crossvalidated
[post_id]: 617859
[parent_id]: 
[tags]: 
Application of binomial probability to sequence matches

I'm searching for the number of matches of a three nucleotide sequence (GTG) in a window of 9 nucleotides located at a certain position in a large number of DNA fragments. I want to answer the question: What is the probability of seeing results as high as the results I'm seeing. Based on empirical computation (using 1 million random 9 nucleotide sequences), I get a proportion r (=.1044) for 1 or more pattern matches of the 3 nucleotides across these randomly generated windows. Failure is 0 matches, success is 1 or more matches. I want to use this proportion r in order to produce a null distribution from which I can determine a p value for the number of matches I find experimentally across a large number of actual DNA fragments (a ChIP-seq library, if anyone is interested). So for example, I get 9896 positive outcomes (1 or more matches in the defined 9 bp window) across 62084 tested fragments. In R, why can I not use a binomial distribution to determine the p value as 1 - pbinom(9895,62084, .1044) , i.e, the probability of seeing 9896 or more positive outcomes based on the null distribution? I think I'm making some conceptual error, but I can't figure out what it is. Update: The reason I thought I had a problem is that the p value was changing with the number of trials. I can now see why this is mathematically necessary from the distribution function. Is there an alternative approach whereby I can escape this dependence of p value or data likelihood on the number of fragments I am investigating. I am running this test on a number of ChIP-seq datasets with different library sizes, and having the result depend on size is not ideal. I could sub-sample, but changing the p value by altering the library size ad hoc is unbecoming.
