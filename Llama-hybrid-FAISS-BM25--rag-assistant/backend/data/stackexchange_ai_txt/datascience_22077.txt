[site]: datascience
[post_id]: 22077
[parent_id]: 22075
[tags]: 
In algorithm 1 of the original GAN article ( https://arxiv.org/pdf/1406.2661.pdf ), the discriminator is said to be updated by "ascending its stochastic gradient". This is referring to equation 1: $$ \min_G \max_D V(D, G)= \mathbb{E}_{x\sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z\sim p_z(z)}[\log(1 - D(G(z)))] $$ When we want to minimize something, we do grandient descent. When we want to maximize something, we do gradient ascent. In this context, we want to maximize $V(D, G)$ with respect to the discriminator $D$, that is, the $\max_D V(D, G)$ part from equation 1. I recommend you have a look at NIPS 2016 GAN tutorial video and text . They are very enlightening.
