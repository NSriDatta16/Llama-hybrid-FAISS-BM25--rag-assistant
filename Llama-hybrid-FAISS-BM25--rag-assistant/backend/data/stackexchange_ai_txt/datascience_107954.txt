[site]: datascience
[post_id]: 107954
[parent_id]: 32016
[tags]: 
There exists a wide-spread confusion (or misconception) about Multinomial Naive Bayes. Here is my understanding based on skimming through lecture notes linked in this comment (the file has been removed from the original location, but can still be found elsewhere). "Multinomial Naive Bayes" is a misnomer. The claims that Multinomial NB is simply a Naive Bayes model with multinomially distributed features (conditioned on a class) are wrong. The consensus definition of "Multinomial NB" on Wikipedia and scikit-learn documentation is different. Namely, $\mathbf X\mid(Y=y) \sim \mathrm{Mult}(\boldsymbol\theta_y)$ , where $\mathbf X = (X_1,\dots, X_D) $ and $Y$ are random variables for features and the class, respectively. Parameters $\boldsymbol\theta_y=(\theta_{y1},\dots,\theta_{yD}),\,\theta_{yi}\in[0,1], \sum_i \theta_{yi} = 1$ are estimated for each $y\in1,\dots ,K$ . Clearly, features $X_i$ are not independent (conditioned on $Y$ ), because $p(\mathbf{x}|y)=\frac{(\sum_{i}x_{i})!}{x_{1}!\dots x_{D}!}\prod_{i}\theta_{yi}^{x_{i}}$ cannot be represented as a product of terms that involve only $x_i$ . Thus, the Naive Bayes assumption breaks for $X_i$ : $$p(\mathbf x| y) \neq \prod_i p(x_i| y). $$ While the Naive Bayes assumption breaks when applied to $X_i$ , it may hold when applied to other features. Consider text classification. Let a sentence be represented as $\mathbf z$ , where $z_j\in1,\dots,D$ is a code of the $j$ th word in the given sentence. We use a vocabulary of size $D$ . Thus, a sentence is described by the random variables $Z_j|Y=y ~\sim\mathrm{Cat}(\boldsymbol\theta_y)$ , where $\theta_{yi}$ is the probability that a word with code $i$ occurs at position $j$ in a sentence of category $y$ . The assumption that words found in different parts of a sentence are independent (conditioned on the topic of the sentence) is somewhat reasonable. Thus, $$p(\mathbf{z}|y)={\prod_{j}p(z_{j}|y)},$$ which is precisely the Naive Bayes assumption applied to $\mathbf Z$ . Note that $z_j,j=1,\dots,N$ are word codes, not word counts. Because this model disregards the order of words in a sentence, we may forget about it too and only store word counts: $x_i, i=1,\dots,D$ is the number of times the word with code $i$ occurred in a given sentence. As far as I know, such models are called a "bag of words". The bottom line is as follows. We use features $\mathbf x=(x_i)_{i=1}^D$ , which are (jointly) multinomially distributed. But the Naive Bayes assumption applies to a different set of features, $\mathbf z=(z_j)_{j=1}^N$ . Sometimes this distinction is not explicitly made, and notation is abused (as in scikit-learn documentation ), which is why this question might be asked. Remark on the 2nd paragraph. Strictly speaking, we have a family of multinomial distribution $\mathbf X\mid(Y=y) \sim \mathrm{Mult}(\boldsymbol\theta_y,N)$ , where $N=\sum_i X_i$ is an unknown parameter which varies from data point to data point. E.g., two different observations $\mathbf x^{(1)}=(1, 2, 3)$ and $\mathbf x^{(2)}=(1, 2, 2)$ , are drawn from different multinomials, with $N=6$ and $N=5$ , respectively. In the context of text classification, $N$ is the length of a sentence or a text excerpt. Without specifying $N$ or a distribution over $N$ , it is impossible to generate data. For simplicity and because we are interested in discriminative modelling, I disregarded this detail.
