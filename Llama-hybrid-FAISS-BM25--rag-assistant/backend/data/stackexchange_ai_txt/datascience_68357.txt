[site]: datascience
[post_id]: 68357
[parent_id]: 
[tags]: 
Weka: Implementation of Random Forest

I am wondering how random forests are exactly implemented in Weka. This paper is very specific about RFs in Weka, but the description of its learning process in chapter 2 seems strange to me. They say: Bootstrap samples $B_i$ for every tree $t_i$ A random subset of features is selected for each $t_i$ Information gain is used to grow unpruned trees $t_i$ My questions: Shouldn't step 2 be repeated on all levels of the decision tree? Otherwise each tree will never see some of the features Whats the default when setting numFeatures=0 . I think this is the number of features that is available for each split. Is it the square root of the number of all features? Is really information gain used for determining the best split attribute? I am using Weka 3.8.3 - not sure if this matters. Thanks for all hints :)
