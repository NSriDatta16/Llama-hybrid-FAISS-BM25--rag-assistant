[site]: crossvalidated
[post_id]: 425101
[parent_id]: 345297
[tags]: 
Since you created a separate test set prior to feature selection / model training, any error estimates for that test set would still be valid. That said, the biggest positive change that you could make that comes to mind would be to use some variant of K-fold CV (10-fold, LOO, Repeated K-fold CV, or Stratified Repeated K-fold CV) to repeat your entire process on different splits of the data, to ensure that your results aren't overly specific for the particular test set you generated. Caret and scikit-learn both provide functionality to help you generate indices for all of the splits, so it just takes a little bit of time to use these to generate the various splits and repeat your entire process for each partitioning. Regarding the use of Boruta followed by RF, it's less obvious how necessary the feature selection step is, and most likely it will depend on the nature and scale (# features) of your data. Random forest seems to perform well even when the number of features is much larger than the number of samples you have, but at some point it will start to be affected by the disproportionate number of features. As such, I personally tend to combine the two approaches, using feature selection as a first-pass method to eliminate the least informative variables, and then training/optimizing a final model on the remaining subset. Ultimately though, the best way to find out is to try both approaches, using proper CV, and compare the impact of each on downstream model performance.
