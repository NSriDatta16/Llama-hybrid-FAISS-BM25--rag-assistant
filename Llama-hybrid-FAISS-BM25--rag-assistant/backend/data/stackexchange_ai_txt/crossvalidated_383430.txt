[site]: crossvalidated
[post_id]: 383430
[parent_id]: 381991
[tags]: 
I think the key here is to consider the column space of the $d\times k$ RP matrix $R$ as the subspace onto which we perform the projection. In general, regardless of whether the columns of $R$ are orthogonal, one can project a sample $x\in \mathbb R^d$ onto the column space of $R$ ussing the following equation [1]: $p = xR(R^TR)^{-1}R^T$ , where $p\in\mathbb R^d$ . If as in the older versions or RP the columns of matrix $R$ are restricted to be orthonormal, then $R^TR = I\in \mathbb R^{k\times k}$ , and therefore the projection of $x$ onto the column space of $R$ becomes: $p = xRR^T$ , with $p\in\mathbb R^d$ , and $RR^T\in\mathbb R^{d\times d}$ becomes a projection matrix , because it's square and $(RR^T)^2=RR^TRR^T=RR^T$ . Perhaps the claim that the older version of Random Projection (were the columns of $R$ were orthonormal) is in fact a projection refers to the fact that in that case the embedding down to $\mathbb R^k$ and posterior reconstruction back to $\mathbb R^d$ of a sample $x\in\mathbb R^d$ given by $xRR^T$ is indeed a projection onto the column space of $R$ , and $RR^T$ is a projection matrix . I would be grateful if you could confirm/correct my reasoning here. Reference: [1] http://www.dankalman.net/AUhome/classes/classesS17/linalg/projections.pdf
