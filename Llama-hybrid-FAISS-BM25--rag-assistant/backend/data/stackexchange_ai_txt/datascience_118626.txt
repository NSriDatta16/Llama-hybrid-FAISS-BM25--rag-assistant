[site]: datascience
[post_id]: 118626
[parent_id]: 
[tags]: 
how do I test if overfitting exists when I use cross_val_score method?

I got the following code form a book on xgboost. I wonder whether this is a correct way of analyzing cross validation score for overfitting purposes. mean accuracy is 81 which can be okay. but what if the training accuracy is 99% ? Shouldn't we also observe the training accuracy ? If yes, how can I do it since the model is fitted by the cross_val_score method with 5 difference cross validation-training sets ? model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2) scores = cross_val_score(model, X, y, cv=5) print('Accuracy:', np.round(scores, 2)) print('Accuracy mean: %0.2f' % (scores.mean())) Accuracy: [0.85 0.85 0.77 0.78 0.77] Accuracy mean: 0.81
