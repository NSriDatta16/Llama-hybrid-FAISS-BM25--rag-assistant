[site]: crossvalidated
[post_id]: 586739
[parent_id]: 
[tags]: 
Training accuracy decreases as training set gets larger

I have a classification task to predict a binary outcome on a balanced data set, N=138. To make sure nothing fishy was going on in my feature set (overfitting, high correlation), I ran a PCA on the features I generated, and chose the first 2 PCA components for all the modeling/graphing below. To make the graph: Starting at training set size N=20, in increments of 2, I increase the size of the training set (where the test set is all remaining data) (both sets are always perfectly balanced). I train a Ridge Classifer on each training set, and record the accuracy of the classifer on the training set and on the test set for that particular train/test split. To make sure I didn't pick some peculiar train/test split, I repeat this process 30 times, drawing a new train/test set each time. So for any particular point in the graph, the y-position of the line is mean train or test accuracy across 30 iterations. To be clear, the left most x-position on the graph is when the training set size is 20, and the test set size is 138-20 = 118. In normal circumstances, I would expect the training accuracy to increase as the training set size increases. And, as along as there is signal in the data, I would expect the test accuracy to rise as well. So why would the accuracy as evaluated on the training set decrease as the training set gets larger? I fear this might be happening because 1. there's no signal in my data or 2. my data is too small?
