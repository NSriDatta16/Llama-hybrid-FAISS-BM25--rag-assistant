[site]: crossvalidated
[post_id]: 624569
[parent_id]: 
[tags]: 
How to best perform early stopping and operation point choice when doing cross validation

I currently have my dataset split in three sets, training, validation, and a hold-out test set. I am training neural networks for a binary classification problem using the following protocol: Train (using the training set) for n epochs Validation loss is computed every epoch; if the validation loss does not improve for p epochs, stop and recover the model from the epochs (Early Stopping) Once training is finished, using the best trained model I evaluate on the validation set using the classification thresholds in range [0.0, 1.0], with step of 0.1 Select threshold that best optimizes some target metric I have Evaluate the trained model on the test set using the selected operation point Now, due to some restrictions on how I split my data, I ended up with a poor balance on my validation set, which is currently impact proper evaluation of some of the models I have been testing for my problem. I am considering switching to a cross-validation protocol, but still keeping the current test set separated for evaluation, that is, I would perform k-fold on the training+validation, and then evaluate on the test set. However, I am not sure about how to properly include the Early Stopping and Operation Point Selection on this protocol. Early Stopping: Initially, I thought about using the validation fold of the round for early stopping. Is this ok? Operation Point Selection: Now, I have no idea how to properly select the operation point for my model. The only option I can think about would be to perform k-fold using only my current training set, keeping my current validation set fixed, and then use it to select the operation point. My only worry is that my distribution issues will persist (albeit in a less impactful way). Is there any better alternative?
