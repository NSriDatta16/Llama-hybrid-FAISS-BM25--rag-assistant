[site]: stackoverflow
[post_id]: 1219671
[parent_id]: 
[tags]: 
C++ 'small' optimization behaving strangely

I'm trying to optimize 'in the small' on a project of mine. There's a series of array accesses that are individually tiny, but profiling has revealed that these array accesses are where the vast majority of my program is spending its time. So, time to make things faster, since the program takes about an hour to run. I've moved the following type of access: const float theValOld1 = image(x, y, z); const float theValOld2 = image(x, y+1, z); const float theValOld3 = image(x, y-1, z); const float theValOld4 = image(x-1, y, z); etc, for 28 accesses around the current pixel. where image thunks down to float image(const int x, const int y, const int z) const { return data[z*xsize*ysize + y*xsize + x]; } and I've replaced it with const int yindex = y*xsize; const int zindex = z*xsize*ysize; const float* thePtr = &(data[z*xsize*ysize + y*xsize + x]); const float theVal1 = *(thePtr); const float theVal2 = *(thePtr + yindex); const float theVal3 = *(thePtr - yindex); const float theVal4 = *(thePtr - 1); etc, for the same number of operations. I would expect that, if the compiler were totally awesome, that this change would do nothing to the speed. If the compiler is not awesome, then I'd say that the second version should be faster, if only because I'm avoiding the implict pointer addition that comes with the [] thunk, as well as removing the multiplications for the y and z indeces. To make it even more lopsided, I've moved the z operations into their own section that only gets hit if zindex != 0, so effectively, the second version only has 9 accesses. So by that metric, the second version should definitely be faster. To measure performance, I'm using QueryPerformanceCounter. What's odd to me is that the order of operations matters! If I leave the operations as described and compare the timings (as well as the results, to make sure that the same value is calculated after optimization), then the older code takes about 45 ticks per pixel and the new code takes 10 ticks per pixel. If I reverse the operations, then the old code takes about 14 ticks per pixel and the new code takes about 30 ticks per pixel (with lots of noise in there, these are averages over about 100 pixels). Why should the order matter? Is there caching or something happening? The variables are all named different things, so I wouldn't think that would matter. If there is some caching happening, is there any way I can take advantage of it from pixel to pixel? Corollary: To compare speed, I'm supposing that the right way is to run the two versions independently of one another, and then compare the results from different runs. I'd like to have the two comparisons next to each other make sense, but there's obviously something happening here that prevents that. Is there a way to salvage this side-by-side run to get a reasonable speed comparison from a single run, so I can make sure that the results are identical as well (easily)? EDIT: To clarify. I have both new and old code in the same function, so I can make sure that the results are identical. If I run old code and then new code, new code runs faster than old. If I run new code and then old code, old code runs faster than new. The z hit is required by the math, and the if statement cannot be removed, and is present in both. For the new code, I've just moved more z-specific code into the z section, and the test code I'm using is 100% 2D. When I move to 3D testing, then I'm sure I'll see more of the effect of branching.
