[site]: crossvalidated
[post_id]: 278935
[parent_id]: 278306
[tags]: 
Note that the VC dimension for the SVM is at most the dimension of the data in kernel space. The generalization capabilities of SVM arise through the margin, see for example here on page 13 where they use Vapnik's bound on classifiers having a specific margin $\gamma$ (these are actually for classifiers which don't make mistakes on the training data but the point is illustrative). Specifically: $$ VC(H_\gamma) \leq \min\left\{D, \left\lceil\frac{4R^2}{\gamma^2}\right\rceil\right\} $$ where $D$ is the dimension of the data, and $||x_i|| \leq R$ for all data (so for RBF, $D=\infty$ and $R=1$ )
