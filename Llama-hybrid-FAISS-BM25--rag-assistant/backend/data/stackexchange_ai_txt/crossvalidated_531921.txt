[site]: crossvalidated
[post_id]: 531921
[parent_id]: 480984
[tags]: 
The CV error for the best model is optimistic because the model was chosen precisely to minimise that error. If $\widehat{\text{Err}}_i$ is the CV error of the $i$ -th model, then $$ \mathbb{E}[\min\{\widehat{\text{Err}}_1, ..., \widehat{\text{Err}}_m\}] \le \mathbb{E}[\widehat{\text{Err}}_i] $$ for all $i$ . If you don't do model selection, then the problem is perhaps more subtle. Namely, the fact that the quantity that is approximated is not the error of the given model as trained on a fixed dataset, but the expected generalization error over all possible datasets of the given size. In other words, CV approximates $$ \text{Err} := \mathbb{E}_{D \sim \mathbb{P}^n_{X Y}} [ \mathbb{E}_{X Y}[\text{loss}(f_D(X),Y)|D]], $$ where the training set $D$ itself is random, instead of what one might want: $$ \text{Err}_{X Y} := \mathbb{E}_{X Y}[\text{loss}(f_D(X),Y)|D], $$ for a fixed dataset $D$ . But even for $\text{Err}$ , since CV splits in, say $K$ , folds, the actual expectation approximated is wrt to $D \sim \mathbb{P}^{\frac{K-1}{K} n }_{X Y}$ , so there is some bias wrt. $\text{Err}$ . Of course as $K/n$ decreases this can become irrelevant. Other sources of error in CV are the dependencies in the error terms for each test sample: because of the common training split and because of the dependence among different training splits there is variance which cannot be estimated using the empirical standard error of the individual losses at each test sample. This is the reason why common confidence intervals are bogus and have poor coverage. See this recent paper [1] for a nested procedure which provides good confidence intervals using a nested procedure. [1] Bates, Stephen, Trevor Hastie, and Robert Tibshirani. ‘Cross-Validation: What Does It Estimate and How Well Does It Do It?’, 1 April 2021.
