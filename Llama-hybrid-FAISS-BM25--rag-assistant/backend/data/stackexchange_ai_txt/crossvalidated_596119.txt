[site]: crossvalidated
[post_id]: 596119
[parent_id]: 328614
[tags]: 
I have not looked at the scikit learn documentation closely yet, but, from what I know about bayesian ridge, the learned alpha and lambda values (using scikit learned terminology for these) are hyperparameters of the bayesian ridge model. Alpha corresponds to the noise in your estimate of the target and lambda is the estimated precision of the weights. The equations above are good descriptions of these. These are learned in scikit learn by assigning a gamma priors to the hyperparameters and marginalizing over these hyperparameters. Then, because the resultant integral is analytically intractable, we need to use some approximation to find it (but I am not sure what they use... maybe Laplace approximation). After this, we have effectively solved for the hyperparameters without splitting the dataset and using a grid search method! We solve for these hyperparameters because they set our quadratic regularization term in the Bayesian ridge regression. Following Bishop's Pattern Recognition and Machine Learning, the posterior distribution w.r.t w (the weight vector) shown in equation (eq. 3.55): We can see that this look similar to the minimization of the sum-of-squares error function with a quadratic regularization term that is equivalent to alpha/beta or, in scikit learn's terminology, lambda/alpha. See below the minimization of the sum-of-squares error function with a quadratic regularization term (eq. 3.27) for comparison. Where Bishops alpha = scikit learn's lambda and Bishops beta = scikit learn's alpha: Another good source for understanding bayesian linear regression is: http://krasserm.github.io/2019/02/23/bayesian-linear-regression/
