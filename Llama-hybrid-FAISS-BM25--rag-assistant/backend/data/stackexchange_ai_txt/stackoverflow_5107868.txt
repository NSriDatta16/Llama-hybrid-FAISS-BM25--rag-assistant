[site]: stackoverflow
[post_id]: 5107868
[parent_id]: 5107806
[tags]: 
I must be missing something about the question. Given an array of partial sums, you should be able to get constant complexity -- the sum of elements from a to b is partial_sums[b] - partial_sums[a] (or if you can't assume a , partial_sums[max(a,b)] - partial_sums[min(a,b)] ). Perhaps you're talking about a and b as bounds on the values rather than the location? If so, then assuming your array is sorted, you can get O(log N) complexity by using a binary search for the locations of a and b , then subtracting as above. If the array isn't (and can't be) sorted, you can accomplish the same by creating an array of references to the original objects, sorting the references, and generating partial sums for those references. That adds work to the preprocessing, but keeps O(log N) for the queries. Edit: Making the array(s) dynamic should have no effect, at least in terms of computational complexity. If you only ever insert/delete at the end of the main array, you can insert/delete in constant time in the partial sums array as well. For an insertion, you do something like: N = N + 1 main_array[N] = new_value partial_sum[N] = partial_sum[N-1] + new_value To delete from the end, you just use N = N - 1 , and ignore the values previously at the ends of both arrays. If you need to support insertion/deletion in the middle of the main array, that takes linear time. Updating the partial sums array can be done in linear time as well. For example, to insert new_value at index i , you'd do something like: N = N + 1 for location = N downto i + 1 main_array[location] = main_array[location-1] partial_sums[location] = partial_sums[location-1] + new_value Deleting is similar, except that you work your way up from the deletion point to the end, and subtract the value being deleted. I did say "should" for a reason though -- there is a possible caveat. If your array is extremely dynamic and the contents are floating point, you can/will run into a problem: repeatedly adding and subtracting values as you insert/delete elements may (and eventually will ) lead to rounding errors. Under these circumstances, you have a couple of choices: one is to abandon the idea altogether. Another uses even more storage -- as you add/delete items, keep a running sum of the absolute values of the elements that have been added/subtracted. When/if this exceeds a chosen percentage of the partial sum for that point, you re-compute your partial sums (and zero the running sum).
