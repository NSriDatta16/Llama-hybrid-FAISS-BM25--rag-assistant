[site]: crossvalidated
[post_id]: 437326
[parent_id]: 
[tags]: 
Anomaly detection using vector autoregression

I want to detect anomalies in multivariate time series using statistical approaches. In particular. I want to use a vector autoregression model like VAR, VARMA or VARIMA, to predict a time stamp $x_t$ given $x_{t-1},...,x_{t-lag}$ . Here $x_t$ is multivariate timestamp having dimension $d$ . Then, I would use the deviation between $x_t$ and $\hat{x}_t$ ( $\hat{x}_t$ is the prediction of the VAR model) to assign an anomaly score to $x_t$ . One way would be to use the euclidean distance between $x_t$ and $\hat{x}_t$ . However this is critical, because it ignores the relationship between the $d$ variables. Some papers suggest to use the Mahalanobis distance: here . I wanted to ask if anyone has experience what is an appropriate way to compute the deviation between $x_t$ and $\hat{x}_t$ . I would appreciate any suggestion. It would also be very helpful if you could give a hint to the python function of it, if it does exist.
