[site]: crossvalidated
[post_id]: 627460
[parent_id]: 627335
[tags]: 
Thanks for the help of @Durden. Here is my answer. First, some relevant questions on this site: question 1 and question 2 are very relevant. @Sextus Empiricus ’s answer seems to utilize the idea of Bayesian synthetic likelihood: So we could use the likelihood function: $\mathcal{L}(\mu \vert \bar{X}) \approx \frac{1}{\sqrt{2 \pi \sigma^2/n}} \exp \left(\frac{(\bar X - \mu)^2}{2 \sigma^2/n} \right)$ The summary statistic $\bar{x}$ is observed and we can reasonably assume it follows a normal distribution per CLT. Then the posterior $\mu\mid x_{1:n}$ (here $\sigma^2$ is omitted because it’s known and fixed) becomes $\mu\mid\bar{x}$ . However, I have come up with a different explanation than replacing $x_{1:n}$ with $\bar{x}$ . Then, I provide an approach using Taylor expansion of log-likelihood. We konw in objective Bayes, the posterior distribution depends on the sum likelihood $p(x_{1:n}\mid\mu)$ . For notation conveniency, let $l(\mu)=\log p(x\mid\mu)$ , the population log-likelihood. $l_i(\mu)=\log p(x_i\mid\mu)$ , the individual log-likelihood. $\bar{l}(\mu)=\frac{1}{n}\log p(x_{1:n}\mid\mu)=\frac{1}{n}\sum_{i=1}^{n}l_i(\mu)$ , the average log-likelihood. This corresponds to our target sum of log-likelihood. $E[l(\mu)]=\mathrm{plim}_{n\to\infty}\bar{l}(\mu)$ , the expected log-likelihood. The asymptotics can be used in this way. Following this answer , the expected log-likelihood can be expanded in the parameter space (differing from the sample space) as a second-order Taylor series: $\bar{l}(\mu)=\bar{l}(\mu_0)+\bar{l}’(\mu_0)(\mu-\mu_0)+\frac{1}{2}\bar{l}’’(\mu_0)(\mu-\mu_0)^2+o^2.$ Let $\mu_0$ be the maximum likelihood estimator (MLE) point, i.e. $E[l’(\mu_0)]=0$ . Also, this answer said: Then one goes to show that the remainder converges uniformly in probability to zero... So asymptotically ( $n\to\infty$ ) we have $E[l(\mu)]\approx E[l(\mu_0)]+\frac{1}{2}E[l’’(\mu_0)](\mu-\mu_0)^2,$ which is essentially a quadratic approximation. This shows that asymptotically, the true average (also the expected) log-likelihood can be quadratically approximated. How do we do this quadratic approximation? A practical way is to model the individual log-likelihood $l_i(\mu)$ as Gaussian, whose highest order of Taylor expansion is 2. Then their sum and average will be just quadratic. So my conclusion: In a large-sample setting, the problems of modelling the unkonwn population distribution as Gaussian will be alleviated, which is a different perspective compared to frequentists’ CLT. However, I found that the Gaussian approximation error may not converge to zero at the second-order level even asymptotically, and the remaining error comes from the Fisher information differences of two models! The true asymptotic posterior and the Gaussian-approximted asymptotic postetior are both normal, but with different variances depending on their respective Fisher information matrices, as the Bernstein-von Mises theorem shows. This is a problem not specific to Bayesians. Frequentists too suffer from so, as the QMLE estimator does not reach the Cramer-Rao lower bound. So relying on asymptotic inference may be problematic! The case of unknown $\sigma^2$ would be a bit more tricky and I leave this unfinished.
