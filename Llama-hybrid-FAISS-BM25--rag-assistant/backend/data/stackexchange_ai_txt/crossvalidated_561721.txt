[site]: crossvalidated
[post_id]: 561721
[parent_id]: 561619
[tags]: 
No approach that distills the data into a single numerical summary of the 'evidence' will be resistant to misinterpretation. That's because the evidence has more than one dimension even for a simple system. The evidence will favour some values of the parameter(s) of interest and disfavour others and it will favour some strongly and others weakly. A single number can never capture that richness, and even adding the extra information of observed effect size and sample size helps only a little. A likelihood function is probably the best display of that favouring as it shows the strength of relative favouring and also the specificity of the favouring by way of its narrowness or peakiness. I discuss that here: https://arxiv.org/abs/1311.0081 Your suggestion (2) seems to be a one-tailed p-value. Is that right? One tailed p-values from simple significance tests have some important advantages over two-tailed p-values when they are treated as indices of evidence in the data against the specified null hypothesised parameter value according to the statistical model. Simplicity of interpretation is just one of them. (See that same paper.) A suggestion to use one-tailed p-values will often elicit howls of disapproval from those who like to interpret p-values as error rates, but p-values from significance tests are not error rates and such an interpretation is of little utility in most scientific programs. I have written about using one-tailed p-values in several places, but the most complete discussion is in the article linked above. The previous paragraph touches on two important distinctions that are not often enough respected. First, a 'neo-Fisherian' significance test yields a p-value and a Neymanâ€“Pearsonian hypothesis test yields a decision regarding the null hypothesis. An analyst might make a decision regarding the null hypothesis on the basis of a significance test-derived p-value, but no decision is encoded in the p-value. (That distinction has been dealt with several times on this site, links below.) The second distinction is between a statistical inference and the types of inferences that are routinely make as part of a scientific program. Statistical inferences are amenable to accounting and to theoretical and simulation-based validation because they are contained within a fully defined statistical model. In contrast, scientific inferences relate to the real word and are therefore not amenable to any simple process of validation. Furthermore, the statistical inferences are very often treated as being all or none (especially those that follow a hypothesis test) whereas the scientific thought process will often be a mixture of principled reasoning and intuition and can involve more than just a single analysis of one dataset. A scientist can (should) consider the evidence currently in hand along with previously available evidence along with biological or physical theory and speculations. I've written about that here: https://pubmed.ncbi.nlm.nih.gov/31897610/ Your "much cited result" is very misleadingly titled, and should not be cited without careful evaluation (including reading the adjacent article in the same issue of the journal!). It conflates the numerical discrepancies between p-values and the Bayesian posterior from an arguably ill-formed 'slab and spike' prior. If p-values were 'irreconcilable' with Bayesian evidence then they would have to be irreconcilable with likelihood functions. The opposite is the case, according to a little-cited result here: https://arxiv.org/abs/1311.0081 See these posts on this site: Why are lower p-values not more evidence against the null? Arguments from Johansson 2011 Accommodating entrenched views of p-values Is it fair to say p-values tell us nothing about the probability null hypotheses are true? Alternatives to the null hypothesis significance testing framework What is the difference between "testing of hypothesis" and "test of significance"?
