[site]: crossvalidated
[post_id]: 611575
[parent_id]: 
[tags]: 
Gaussian Process Regression with noisy gradient observation

I am trying to understand how to incorporate noisy gradient information in Gauss Process Regression using a Bayesian framework and I need your advice. I am following the following approach: Let $\mathcal{D} = (x,y)_i$ be noisy data and let $\mathcal{D}_g = (x_g,y')_j$ be noisy gradient observations. The joint prior should then be given by \begin{equation}\label{eq:EGP} p(\textbf{f},\textbf{f}'\mid \textbf{X},\textbf{X}_g) \propto \mathcal{N}\left (\begin{bmatrix} \textbf{f}\\ \textbf{f}' \end{bmatrix} \mid \textbf{0},\tilde{\textbf{K}}\right ) \end{equation} where $\tilde{\textbf{K}}$ is the block covariance matrix between $\textbf{X}$ and $\textbf{X}_g$ . The joint likelihood should look like this, regarding the noise model \begin{align} p(\textbf{y},\textbf{y}'\vert \textbf{f},\textbf{f}') &\propto \prod_{i}^{n}\exp\left (-\frac{1}{2} \left ( y(\textbf{x})_i-f_i \right )^2\sigma^{-2}_i \right ) \prod_{j}^{m\cdot d}\exp\left (-\frac{1}{2} \left ( y'_j(\textbf{x})-f_j' \right )^2\sigma^{-2}_{g,j} \right ) \nonumber \\ &= \prod_{i}^{n+m\cdot d}\exp\left (-\frac{1}{2} \left ( \tilde{y}_i(\textbf{x})-f_i \right )^2\tilde{\sigma}^{-2}_i \right ) \nonumber \\ &= \exp\left (-\frac{1}{2} \left ( \tilde{y}(\textbf{x})-f \right )^T\tilde{\textbf{E}}\left ( \tilde{y}(\textbf{x})-f \right ) \right ). \end{align} and the marginal likelihood should be \begin{align} p(\textbf{y},\textbf{y}'\vert \textbf{X},\textbf{X}_g) &= \iint p(\textbf{y},\textbf{y}'\vert \textbf{f},\textbf{f}')p(\textbf{f},\textbf{f}'\vert \textbf{X},\textbf{X}_g) d\textbf{f}d\textbf{f}' \nonumber \\ &= \iint \mathcal{N}(\tilde{\textbf{y}}\mid \textbf{f},\textbf{f}', \tilde{\textbf{E}}) \cdot \mathcal{N}(\textbf{0},\tilde{\textbf{K}}) d\textbf{f}d\textbf{f}' \nonumber \\ &\propto \mathcal{N}\left(\tilde{\textbf{y}}\vert\textbf{0} , \tilde{\textbf{K}}+\tilde{\textbf{E}} \right) \end{align} where $\tilde{\textbf{y}} = (\textbf{y},\textbf{y}')$ . Up to this point, is the approach correct ? I am not sure about the integral of the marginal likelihood. The joint posterior - using Bayse is then given by \begin{align} p(\textbf{f},\textbf{f}'\vert\textbf{X},\textbf{X}_g) &= \frac{p(\textbf{y},\textbf{y}'\mid \textbf{f},\textbf{f}')p(\textbf{f},\textbf{f}'\mid \textbf{X},\textbf{X}_g)}{p(\textbf{y},\textbf{y}' \mid \textbf{X},\textbf{X}_g)} \nonumber \\ &\propto\mathcal{N}(\textbf{y},\textbf{y}'\vert \textbf{f},\textbf{f}', \tilde{\textbf{E}}) \cdot \mathcal{N}(\textbf{0},\tilde{\textbf{K}}) \nonumber \\ &=\mathcal{N}\left (\textbf{f},\textbf{f}' \mid \left(\tilde{\textbf{K}}^{-1}+\tilde{\textbf{E}}^{-1}\right)^{-1}\tilde{\textbf{E}}^{-1}\tilde{\textbf{y}},\left(\tilde{\textbf{K}}^{-1}+\tilde{\textbf{E}}^{-1}\right)^{-1} \right )\end{align} where $\tilde{\textbf{E}}:= \mathrm{diag}\left ( \sigma_1^2,\dots,\sigma_n^2,\sigma_{11}^2,\dots,\sigma_{1d}^2,\dots,\sigma_{m1}^2,\dots,\sigma_{md}^2 \right )$ . I have more problems with the predictive distribution. Let $x*$ be an unseen data point , then I'm interested in $ p(y^* \vert\textbf{x}^*,\textbf{X},\textbf{X}_g) $ i.e. the distribution given the gradient data. \begin{align} p(y^* \vert\textbf{x}^*,\textbf{X},\textbf{X}_g) &= \iint p(y^*\vert \textbf{f},\textbf{f}',\textbf{x}^*,\textbf{X},\textbf{X}_g)p(\textbf{f},\textbf{f}'\mid \textbf{X},\textbf{X}_g) d\textbf{f} d\textbf{f}' \nonumber\\ &= \iint \mathcal{N}\left ( y^* \mid m^* + \tilde{\textbf{k}}^*\textbf{K}^{-1}\left ( \textbf{f},\textbf{f}' \right ),\textbf{k}_{y^*y^*}-\tilde{\textbf{k}}^{*T}\textbf{K}^{-1}\tilde{\textbf{k}}^* \right ) \nonumber\\ &\times \mathcal{N}\left (\left ( \textbf{f},\textbf{f}' \right ) \mid \left(\tilde{ \textbf{K}}^{-1}+\tilde{ \textbf{E}}^{-1}\right)^{-1}\tilde{ \textbf{E}}^{-1}\left ( \textbf{y},\textbf{y}' \right ),\left(\tilde{ \textbf{K}}^{-1}+\tilde{ \textbf{E}}^{-1}\right)^{-1} \right ) d\textbf{f} d\textbf{f}' \nonumber \\ &= \mathcal{N}\left(y^* \mid m^* + \textbf{k}^{*T}(\tilde{\textbf{K}}+\tilde{\textbf{E}})^{-1})\left ( \textbf{y},\textbf{y}' \right ),\textbf{k}^{**}-\tilde{\textbf{k}}^{*T}(\tilde{\textbf{K}}+\tilde{\textbf{E}}^{-1})^{-1}\tilde{\textbf{k}}^*\right). \end{align} where $p(y^*\vert \textbf{f},\textbf{f}',\textbf{x}^*,\textbf{X},\textbf{X}_g)$ is given by \begin{equation} \begin{bmatrix} f\\ f'\\ y^* \end{bmatrix} \sim \mathcal{N}\left ( \begin{bmatrix} 0\\ 0\\ m^* \end{bmatrix} \mid \begin{bmatrix} k_{ff} & k_{ff'} & k_{fy^*} \\ k_{f'f} & k_{f'f'} & k_{f'y^*} \\ k_{y^*f} & k_{f',y^*} & k_{y^*y^*} \end{bmatrix}\right ). \end{equation} Is this the right idea or do I have to calculate the joint predictive distribution $p(y^*,y'^{*}\vert\textbf{x}^*,\textbf{X},\textbf{X}_g)$ and if so, do I just marginalize the distribution in order to return $ p(y^* \vert\textbf{x}^*,\textbf{X},\textbf{X}_g)$ ? I'm really confused and would appreciate your help.
