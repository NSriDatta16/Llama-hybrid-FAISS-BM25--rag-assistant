[site]: crossvalidated
[post_id]: 436040
[parent_id]: 
[tags]: 
How can a neural net recognize it is out of its training domain?

In a recent kaggle competition with a huge overfitting potential the winning team first searched for features on the training data and after the feature engineering used a Kolmogorov-Smirnov Test for comparing the distribution of the features on the training and validation set. Features with a different distribution on the validation set were excluded. ( https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/94390#latest-632778 ) My question is how it is possible to replicate this for a neural network and to realize that the feed forward features of a layer in the end of the network are different from the domain it was trained on? Is there any research that is going into this direction?
