[site]: crossvalidated
[post_id]: 500057
[parent_id]: 252095
[tags]: 
For sequence classification, I would recommend an RNN like LSTM with an Attention layer added. Adding Attention significantly improves the output because now you are paying attention to all hidden states of the RNN layer and not just the last one. Each hidden state is assigned a attention weight and has a 'say' in determining the final label. A simple sequence classification implementation is explained here: https://stackoverflow.com/questions/63060083/create-an-lstm-layer-with-attention-in-keras-for-multi-label-text-classification/64853996#64853996
