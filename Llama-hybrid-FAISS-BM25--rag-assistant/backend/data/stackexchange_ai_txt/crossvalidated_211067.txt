[site]: crossvalidated
[post_id]: 211067
[parent_id]: 210760
[tags]: 
The nature publication described everything precisely about what deep learning really is, which I don't have to repeat here. And to my observation, "old school" neural network don't go "deep" for many reasons. And nowadays, people found new ways to overcome headaches brought by "old school" neural network frameworks, e.g. using GPU to improve the computational efficiency, using dropout to solve over-fitting problem etc. I think the answer is yes. It is those important heuristic tricks, along with the undisregardable GPU implementations that made deep learning really "deep". Generally, the lecture slides here answered this question clearly enough, see the last slide for a summary of learning methods for different problems. Recent research on deep recurrent attention models (DRAM) proved that using proper designed network models, which act more like a human brain, can improve computational efficiency (by decreasing number of parameters required to train with the help of recurrent networks), while increase recognition accuracy. This made the application of dropout less necessary for an implementation. EDIT : The above answer may contain much opinion of my personal perspective, but truth is when we look back a few year later that how deep learning grow popular among machine learning algorithms, we would see that it is the inspiration of human brain that push the development of deep learning forward. That is to say, we are just trying to mimic how brain solve the complicated problems, while "old school" neural networks are brains like a rat, and "deep" networks are brains of a monkey.
