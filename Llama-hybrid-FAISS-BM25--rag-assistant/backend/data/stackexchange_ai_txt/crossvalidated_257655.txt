[site]: crossvalidated
[post_id]: 257655
[parent_id]: 237944
[tags]: 
It helps to think of this process in pseudocode. Let generator(z) be a function that takes a uniformly sampled noise vector z and returns a vector of same size as input vector X ; let's call this length d . Let discriminator(x) be a function that takes a d dimensional vector and returns a scalar probability that x belongs to true data distribution. For training: G_sample = generator(Z) D_real = discriminator(X) D_fake = discriminator(G_sample) D_loss = maximize mean of (log(D_real) + log(1 - D_fake)) G_loss = maximize mean of log(D_fake) # Only update D(X)'s parameters D_solver = Optimizer().minimize(D_loss, theta_D) # Only update G(X)'s parameters G_solver = Optimizer().minimize(G_loss, theta_G) # theta_D and theta_G are the weights and biases of D and G respectively Repeat the above for a number of epochs So, yes, you are right that we essentially think of the generator and discriminator as one giant network for alternating minibatches as we use fake data. The generator's loss function takes care of the gradients for this half. If you think of this network training in isolation, then it is trained just as you would usually train a MLP with its input being the last layer's output of the generator network. You can follow a detailed explanation with code in Tensorflow here(among many places): http://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/ It should be easy to follow once you look at the code.
