[site]: crossvalidated
[post_id]: 49052
[parent_id]: 
[tags]: 
Are splines overfitting the data?

My problem : I recently met a statistician that informed me that splines are only useful for exploring data and are subjected to overfitting, thus not useful in prediction. He preferred exploring with simple polynomials ... As I’m a big fan of splines, and this goes against my intuition I’m interested in finding out how valid these arguments are, and if there is a large group of anti-spline-activists out there? Background : I try to follow Frank Harrell, Regression Modelling Strategies (1), when I create my models. He argues that restricted cubic splines are a valid tool for exploring continuous variables. He also argues that the polynomials are poor at modelling certain relationships such as thresholds, logarithmic (2). For testing the linearity of the model he suggests an ANOVA test for the spline: $H_0: \beta_2 = \beta_3 = \dotsm = \beta_{k-1} = 0 $ I’ve googled for overfitting with splines but not found that much useful (apart from general warnings about not using too many knots). In this forum there seems to be a preference for spline modelling, Kolassa , Harrell , gung . I found one blog post about polynomials, the devil of overfitting that talks about predicting polynomials. The post ends with these comments: To some extent the examples presented here are cheating — polynomial regression is known to be highly non-robust. Much better in practice is to use splines rather than polynomials. Now this prompted me to check how splines would perform in with the example: library(rms) p4 Gives the following image: In conclusion I have not found much that would convince me of reconsidering splines, what am I missing? F. E. Harrell, Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis, Softcover reprint of hardcover 1st ed. 2001. Springer, 2010. F. E. Harrell, K. L. Lee, and B. G. Pollock, “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response,” JNCI J Natl Cancer Inst, vol. 80, no. 15, pp. 1198–1202, Oct. 1988. Update The comments made me wonder what happens within the data span but with uncomfortable curves. In most of the situations I'm not going outside the data boundary, as the example above indicates. I'm not sure this qualifies as prediction ... Anyway here's an example where I create a more complex line that cannot be translated into a polynomial. Since most observations are in the center of the data I tried to simulate that as well: library(rms) cmplx_line $x) & sample >= min(ds$ x)] sample_ds $noise4 cmplx_line + rnorm(nrow(sample_ds), sd=2) reg.n4.4 $x, sample_ds$ noise4, col="#AAAAAA") lines(x=ds $x, y=ds$ cmplx_line, lwd=3, col="black", lty=4) nd $x) lines(ds$ x, predict(reg.n4.4, newdata=ds), col="orange", lwd=3) lines(ds$x, predict(reg.n4.4rcs_ols, newdata=ds), col="lightblue", lwd=3) legend("bottomright", fill=c("black", "orange","lightblue"), legend=c("True line", "Poly", "RCS - ols"), inset=.05) This gives the following plot: Update 2 Since this post I've published an article that looks into non-linearity for age on a large dataset. The supplement compares different methods and I've written a blog post about it .
