[site]: crossvalidated
[post_id]: 101491
[parent_id]: 101471
[tags]: 
If you would like to survey the literature your problem is often referred to as " class imbalance ". The main danger you face is that your model can declare 85% accuracy by always guessing "fully paid" and you should dissuade it from doing this. Ililasfl suggested oversampling the minority class which is something you should certainly try. However if you do not have sufficient information in the minority class to define this class then oversampling may not help. You should try first and see. If that fails modifying the cost to reflect the expected proportion is also a common approach and it is recommended within ililasfl's link. If you are using decision tree software there should be a way to do this. However again if there is insufficient information in the minority class then you might still get a dissatisfying number of false positives and false negatives. If you are still unhappy with your results you have other options. If there is insufficient information in the minority class then it is possible there is too much heterogeneity in that class thus it would need a larger data representation than what you have in your dataset. If that was the case I would consider a one class classifier . I've never tried a one-class decision tree (see here for a list) but I've had good results with the one-class classifier in LibSVM .
