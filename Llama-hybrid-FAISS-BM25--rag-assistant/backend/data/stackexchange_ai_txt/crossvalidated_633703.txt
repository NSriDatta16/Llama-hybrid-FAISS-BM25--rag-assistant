[site]: crossvalidated
[post_id]: 633703
[parent_id]: 512242
[tags]: 
The encoder-decoder architecture is not essential to transformers. The original paper used it because they designed it for a translation task, but this is not necessary. For classification tasks (sequence-to-value) you can use an encoder-only model like BERT , for text generation a decoder-only model (indeed this is what GPT is). These are architecturally almost the same, except in a decoder model you mask the top right half of the attention matrix, so that tokens can't attend to future tokens. It turns out that a decoder-only model can also learn to do seq2seq tasks like translation if you prompt it with the input sequence. The decoder is autoregressive in the same way as an autoregressive RNN: you feed it the input sequence and also all of the tokens it has already generated in the previous inferences. In this sense, each new token you generate is an additional inference, which adds to the cost. But this is also the case with RNNs. You could train a model to generate entire utterances on top of a transformer encoder, which would require only 1 inference per sentence, but this would probably not perform that well. In particular for a vocabulary size of $10^4$ and a sentence length of 10 tokens, you have $10^{40}$ possible output sentences. There's no way to train a distribution over this huge space that would generalize well without factorizing token by token. The key innovation of transformers that makes them perform better than autoregressive RNNs is the self-attention mechanism: this effectively means the model sees the entire input sequence at once . You compute the keys, queries and values of every token, then compute the attention. Therefore, the model can relate information across arbitrary distances in the context window. This is totally different from an RNN or LSTM model, where the information must flow through the hidden states of each unit. (see also this question ) At least, this is how people usually think about it. This paper claims that masked transformers with a linear variant of attention are effectively RNNs, so perhaps the divide is not as essential as everyone thinks. As for "why does Transformer have so complex architecture?" with respect to the transformer block and attention mechanism itself: I feel like this is not entirely understood and it's justified pragmatically based on its performance. Long-range dependencies certainly have something to do with it: for a context length $m$ , there are $O(m^2)$ possible relations between 2 words. The self-attention mechanism can use information from all of them. There is a lot of work trying to make the compute sub-quadratic using ideas like exploiting the sparsity of the attention matrix. But the fundamental idea of self-attention is kept, and it's about making it more efficient. I'm not aware of anything like the universal approximation theorem that specifically explains the expressive power of transformers, but here's some recent work in this direction. They look at how very small toy transformers perform and try to understand what's going on at the weight level. It turns out that even small 2-layer transformers (without any MLP layers) can learn a very general "induction schema" of the form [A][B]...[A]->[B] , which allows them to generalize previously unseen examples encountered in the prompt, even if they are completely out of distribution w.r.t. training. This schema is probably much more difficult to express without self-attention. They also propose an interesting description of transformers in terms of communication streams between attention heads that I haven't seen elsewhere.
