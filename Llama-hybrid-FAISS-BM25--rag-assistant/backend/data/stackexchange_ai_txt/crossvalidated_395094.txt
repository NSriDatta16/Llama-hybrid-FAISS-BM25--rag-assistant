[site]: crossvalidated
[post_id]: 395094
[parent_id]: 395018
[tags]: 
I absolutely do not understand why is activation function needed here. Activtion functions introduce nonlinearities into the network. In fact your first figure depicts this too. I absolutely do not understand why is activation function needed here. I also do not understand why we need to initialize "weights" using something like Xavier initialization. Are we initializing the weights of the filters that we use? Yes, you are initializing the weights of the filters. If so, why are we initializing it as if we are initializing the weights of edges of a neural network? Filter weights are edge weights. Finally, is a convolution layer considered a neural network all by itself (without the fully connected layer at the end)? Well, you can have a network with only one conv layer, although this wouldn't be useful for much. You can also have a network composed of only conv layers. You can even convert a fully connected layer into a mathematically equivalent conv layer. But conceptually, this is a bit like asking if a single rock is a collection of rocks. You might have a rock collection with only one rock in it, but there is a difference between "this rock" and "this collection of rocks holding just this one rock in it"
