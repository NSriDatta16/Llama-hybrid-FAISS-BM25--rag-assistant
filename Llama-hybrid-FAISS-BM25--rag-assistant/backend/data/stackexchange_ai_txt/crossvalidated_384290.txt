[site]: crossvalidated
[post_id]: 384290
[parent_id]: 384210
[tags]: 
Here is an indirect answer to the question that does not proceed by weighting coefficient estimates, but gradually computing the ingredients into the "global" estimate. I believe that biglm in R proceeds similarly. Note that $$ \hat\beta=(X'X)^{-1}X'y $$ where $X'X=\sum_{i=1}^Nx_ix_i'$ with $x_i'$ the $i$ th row of the regressor matrix. Likewise, $X'y=\sum_ix_iy_i$ . Now suppose you divide $N$ into $n$ manageable blocks of size $m$ , $N=n\cdot m$ . You may now load each block into memory consecutively. Then, with $x_{j,k}'$ the $k$ th observation of block $j$ , $$X'X=\sum_{j=1}^n\sum_{k=1}^mx_{j,k}x_{j,k}'$$ Each of the $\sum_{k=1}^mx_{j,k}x_{j,k}'$ yield an "unproblematic" matrix of size $k\times k$ ( $k$ being the number of regressors, which I assume is not huge, but that the number of observations $N$ is). A possible code snippet in R (sorry, I do not know python) might look like this - just for illustration, no claim to efficiency/elegance: N (beta.hat coef(lm(y~x-1)) # here, of course, everything fits into memory and we can check equality x -0.04175772
