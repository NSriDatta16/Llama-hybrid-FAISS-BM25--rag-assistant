[site]: crossvalidated
[post_id]: 212084
[parent_id]: 
[tags]: 
Predicting intensity of Poisson process, given event data

I have a dataset of events: each row is an event, and each column is a feature. There are millions of events and several dozen features. The features are mostly numerical (a few are categorical and I plan to one-hot encode them). One of the features is time, and I think there is a time trend. I know the events are not perfectly independent, but I think it's too hard to model that, so as a first approximation, I'm comfortable assuming that events are generated by Poisson process with the intensity that depends on the features. I want to predict the intensity, given the feature values. To clarify with a simple example: suppose I have the log which contains, for each accident, its time, weather data, location data, type of road, traffic conditions, etc. - in total, say 50 variables. I want to predict the intensity of car accidents given specific values for all those variables. I expect that the true relationship between the features and the intensity is relatively smooth but quite complicated (certainly non-linear and non-monotonic). I also expect that the intensity is never negligible (that is, for any feature values, the events might still happen once in a while). What machine learning or statistical models should I consider? I assume kernel density estimation and other local techniques are hopeless because there are too many dimensions. I thought Gaussian Mixed Models might help, but I haven't seen any papers or links that describe their use in a similar situation (I'm concerned that the "thin tails" of gaussian distributions would not do well given that in my data the intensity never gets too close to zero).
