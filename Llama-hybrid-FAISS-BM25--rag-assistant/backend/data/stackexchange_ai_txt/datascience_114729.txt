[site]: datascience
[post_id]: 114729
[parent_id]: 110967
[tags]: 
My intuition is that it is because tabular data does not necessarily form a manifold . There is some limited and indirect support in the literature for this hypothesis: According to [1], the manifold hypothesis [2] states that all natural data lies in a lower-dimensional space (a manifold) which is embedded in the higher dimensional feature space and locally behaves like an Euclidean space. And deep learning models can learn these manifolds which is the reason why they actually work so well. However, the author does not explicitly define what "natural data" is but he provides some examples like human faces, MNIST digits, natural language and human voices. In line with that, [3] provides images as an example for natural data and [4] states that it has been shown that some image and video data, in fact, form a manifold. ([4] also provides examples of other, non-neural, manifold learning algorithms.) In summary, I infer that these authors do not refer to tabular data when they speak about natural data . And, hence, neural nets might not work that well because tabular data does not form a manifold. At least this is the case for the way we usually encode tabular data, i.e. it might be possible to present tabular data in way that does embed a manifold. But that is just speculation. (There are a few examples which go into this direction already: Some applications of CNNs operate on data visualizations/plots and another example are transformers which are able to learn arithmetic operations from natural language to a limited extend.) Moreover, [1] raises the point that neural nets work well because the inductive biases of their architecture mirror the data (e.g. CNNs have a very special structures which works particularly well on image data). Again I am speculating, but maybe we will develop architectures going forward which provide inductive biases suitable for tabular data - or some special types of tabular data. But currently we do not have these which is another reason why neural nets lack performance on tabular data. References: [1] Chollet, Francois; "Deep Learning with Python"; Second Edition 2nd Edition; 2021 [2] https://en.wikipedia.org/wiki/Manifold_hypothesis [3] https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ [4] Cayton, Lawrence; "Algorithms for manifold learning"; 2005; https://www.lcayton.com/resexam.pdf
