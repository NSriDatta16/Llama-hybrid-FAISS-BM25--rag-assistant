[site]: datascience
[post_id]: 18985
[parent_id]: 18984
[tags]: 
Latent dirichlet allocation (LDA) would be a good one to try, as it is tailored for bag-of-words representations. LDA is a topic model , which basically means that it assigns $n$ topics, and figures out the words that associate with each topic. The output is a vector of $n$ continuous values where each value indicates membership to the corresponding topic. You should be able to find an implementation of LDA for whatever language you're using easily (but don't confuse it with linear discriminant analysis, also commonly referred to as LDA). You could also try principal components analysis or singular value decompositions, which are other standard dimensionality reduction techniques.
