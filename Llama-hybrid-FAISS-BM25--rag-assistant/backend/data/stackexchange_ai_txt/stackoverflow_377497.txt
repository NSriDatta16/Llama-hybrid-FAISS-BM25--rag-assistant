[site]: stackoverflow
[post_id]: 377497
[parent_id]: 375968
[tags]: 
With any trigger-based (or application code-based) solution you need to put in locking to prevent data corruption in a multi-user environment. Even if your trigger worked, or was re-written to avoid the mutating table issue, it would not prevent 2 users from simultaneously updating t1_appnt_evnt_id to the same value on rows where t1_appnt_evnt_id is not null: assume there are currenly no rows where t1_appnt_evnt_id=123 and t1_prnt_t1_pk is not null: Session 1> update tbl1 set t1_appnt_evnt_id=123 where t1_prnt_t1_pk =456; /* OK, trigger sees count of 0 */ Session 2> update tbl1 set t1_appnt_evnt_id=123 where t1_prnt_t1_pk =789; /* OK, trigger sees count of 0 because session 1 hasn't committed yet */ Session 1> commit; Session 2> commit; You now have a corrupted database! The way to avoid this (in trigger or application code) would be to lock the parent row in the table referenced by t1_appnt_evnt_id=123 before performing the check: select appe_id into v_app_id from parent_table where appe_id = :new.t1_appnt_evnt_id for update; Now session 2's trigger must wait for session 1 to commit or rollback before it performs the check. It would be much simpler and safer to implement Dave Costa's index! Finally, I'm glad no one has suggested adding PRAGMA AUTONOMOUS_TRANSACTION to your trigger: this is often suggested on forums and works in as much as the mutating table issue goes away - but it makes the data integrity problem even worse! So just don't...
