[site]: datascience
[post_id]: 5813
[parent_id]: 1074
[tags]: 
The offset parameter is sometimes called "bias" in classification tasks, and its intuitive understanding doesn't have to do with what kind of kernel is used. It is basically used to compensate for feature vectors that are not centered around 0. I will try to intuitively explain what the bias does with a toy example. Let's say you have some feature vector x who's parameters are always negative. The set of weights w you use in your SVM (let's say linear, just for clarity) will perhaps transform the features into the range [0, 1] - so they will always be negative. But, those elements that belong to class 1 fall in the range [0, 0.5], and the ones that are from class 2 fall into the range [0.5, 1]. To classify into a class, the SVM uses 0 as the threshold breakpoint - if greater than 0, it is an element of class 1, if less than 0, it is an element of class -1. But in this case, all the elements will be classified into class 2. However, with a bias of 0.5 (in this linear case), they will be classified correctly. This geometric interpretation doesn't quite work for more complex kernels, but the idea is the same: the bias term attempts to compensate for features that are not centered around zero. In practice, if the features are centered around zero the bias term isn't always needed. To get around the bias issue, you can either: center your features and forget about the bias term (doesn't always work) augment your feature data to include a 1 at the beginning. i.e.: [feat1 feat2 feat3] --> [feat1 feat2 feat3 1] for all of your feature vectors. Then the bias term will just be learned as another SVM weight parameter. Learn your weights, then calculate the bias based on the optimal prediction rule: That is, given all the support vectors $S$, the learned weights $\alpha_i$, the training class $y^{(i)}$, training features $x^{(i)}$, and features-to-use-for-prediction $x$, the predicted class will be $y(x)$. The optimal $w_0$ or bias will then be calculated as the average distance between the correct and calculated predicted class: Hope that helps. I think the easiest thing is to center your features ;)
