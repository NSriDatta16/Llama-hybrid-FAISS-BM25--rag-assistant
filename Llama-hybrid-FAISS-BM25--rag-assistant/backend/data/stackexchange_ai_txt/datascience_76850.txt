[site]: datascience
[post_id]: 76850
[parent_id]: 76847
[tags]: 
It sometimes is called " ensemble learning " where several "weak learners" make a prediction. These predictions are "combined" by some meta-model. A simplistic approach would be that you just use the majority vote. You can also use logistic regression. You can (and should!) of course check the performance of the stacked model(s) by predicting on test data. Related ensemble techniques are boosting and bagging. Simplified Example: Assume you want to predict a binary outcome. Say you have two different models, which do not perform extremely well, but perform better than random guessing. Also, assume the models are independent. Suppose each of your models makes a correct prediction with $0.8\%$ probability. Now when you have two models, the possible outcomes are: Correct, Correct: $0.8*0.8=0.64$ Wrong, Wrong: $0.2*0.2=0.04$ Correct, Wrong: $0.8*0.2=0.16$ Wrong, Correct: $0.2*0.8=0.16$ You see that the probability that both models are wrong is "only" $0.04$ . So if you manage to identify the remaining cases as "correct predictions" by some intermediate/meta (stacking) model, you end up with quite a good result. Essentially you would "boost" the expected accuracy of the model(s) from $0.8$ (single model) to $0.64+0.16+0.16=0.96$ (ensemble) here.
