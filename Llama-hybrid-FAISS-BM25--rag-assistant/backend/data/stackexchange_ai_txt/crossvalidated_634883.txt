[site]: crossvalidated
[post_id]: 634883
[parent_id]: 634864
[tags]: 
The sequence we feed to the decoder always has to be the same length. So when we feed the decoder a sequence the sequence must be "full-length". In this kind of transformer the generation of tokens, happens by the decoder always predicting the next token, based on the already generated tokens. However, because we have to use as input a complete sequence and not only what has been generated so far, the Decoder could "look" at the words that are right of the already generated tokens. In order to avoid this everything right f the token that is to be predicted will be masked. Here is an example Complete Sentence: [SOS] [I] [like] [hot] [chocolate] [EOS] [PAD] [PAD] Step One the Decoder aims to predict the [I] based on the [SOS] token and the output of the Encoder [SOS] [MASK] [MASK] [MASK] [MAKS] [MASK] [MASK] [MASK] In the second step, as he already has predicted the [I] , so now based on the first two tokens he aims to predict the 3rd. So the input for the decoder will be. [SOS] [I] [MASK] [MASK] [MAKS] [MASK] [MASK] [MASK] and so on: [SOS] [I] [like] [MASK] [MAKS] [MASK] [MASK] [MASK]
