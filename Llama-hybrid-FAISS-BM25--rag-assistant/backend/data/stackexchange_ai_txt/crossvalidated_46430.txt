[site]: crossvalidated
[post_id]: 46430
[parent_id]: 24904
[tags]: 
In ordinary linear regression maximizing the likelihood is equivalent to minimizing the sum of squared errors across the board (and consequently the estimated variance of errors) I In logistic regression, the errors are not expected to have the same variance: we should have high variance for p near .5, lower variance towards the extremes I Leads to (iteratively) (re)weighted least squares (IRWLS) method, where errors are penalized more where we expect less variance around p
