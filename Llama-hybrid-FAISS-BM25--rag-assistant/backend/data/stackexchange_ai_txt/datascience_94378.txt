[site]: datascience
[post_id]: 94378
[parent_id]: 25836
[tags]: 
The answer to your question is only possible through data analysis and the task. 1. Data Analysis: If you are trying to classify data with 38 samples as A and 2 samples as B, a Majority Vote Classifier (MVC) might be good enough to get 95% accuracy too. That's where you might want to check balanced accuracy or F-1 scores. Here, the MVC may give you 95% accuracy like your normal models, but its F-1 score will be 0. 2. Task: If you are trying to predict multi-class labels and are getting 95% accuracy, your results might be good. But if your task is like something described here , the model could be over-fitting and you might be missing the key output you are looking for, i.e. accurately predict the rare example. Suggestion I would suggest determining the loss function might be very useful. Ensemble methods usually help if they are trying to complement each other (as you might have seen in various Kaggle competitions). Also, you can look at Gradient Boosted Trees and XGBoost which create multiple small decision trees, with each new tree trying to predict the previous tree's incorrect result.
