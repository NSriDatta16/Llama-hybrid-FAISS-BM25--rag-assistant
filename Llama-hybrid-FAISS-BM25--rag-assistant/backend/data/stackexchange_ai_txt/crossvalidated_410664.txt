[site]: crossvalidated
[post_id]: 410664
[parent_id]: 167039
[tags]: 
Reading from here we confirm your understanding of unsupervised Breiman's Random Forests. Unsupervised learning In unsupervised learning the data consist of a set of x -vectors of the same dimension with no class labels or response variables. There is no figure of merit to optimize, leaving the field open to ambiguous conclusions. The usual goal is to cluster the data - to see if it falls into different piles, each of which can be assigned some meaning. The approach in random forests is to consider the original data as class 1 and to create a synthetic second class of the same size that will be labeled as class 2. The synthetic second class is created by sampling at random from the univariate distributions of the original data. Here is how a single member of class two is created - the first coordinate is sampled from the N values {x(1,n)}. The second coordinate is sampled independently from the N values {x(2,n)}, and so forth. Thus, class two has the distribution of independent random variables, each one having the same univariate distribution as the corresponding variable in the original data. Class 2 thus destroys the dependency structure in the original data. But now, there are two classes and this artificial two-class problem can be run through random forests. This allows all of the random forests options to be applied to the original unlabeled data set. If the oob misclassification rate in the two-class problem is, say, 40% or more, it implies that the x -variables look too much like independent variables to random forests. The dependencies do not have a large role and not much discrimination is taking place. If the misclassification rate is lower, then the dependencies are playing an important role. Formulating it as a two class problem has a number of payoffs. Missing values can be replaced effectively. Outliers can be found. Variable importance can be measured. Scaling can be performed (in this case, if the original data had labels, the unsupervised scaling often retains the structure of the original scaling). But the most important payoff is the possibility of clustering. The reason the total number of votes is the number of "true" samples (class 1) is simply due to the fact that there's no reason to return votes for "fake" samples (class 2). These are random and their probability density function is entirely known. Compare the distributions of vote in both yours PU and U models: png("unsupervisedRF.png") par(mfrow = c(1,2), mar = c(5.1, 3.6, 4.1, 1.6)) boxplot(rfPU $votes[1:150,], names = c("PU-1","PU-2"), col = c("green","red")) boxplot(rfU$ votes, names = c("U-1","U-2"), col = c("green","red")) dev.off()
