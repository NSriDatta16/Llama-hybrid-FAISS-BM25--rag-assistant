[site]: crossvalidated
[post_id]: 623487
[parent_id]: 
[tags]: 
Feature ranking during feature selection with cross-validation

I am learning about feature selection and studying the method given in Chapter 19 of the book "Applied Predictive Modelling" by Max Kuhn and Kjell Johnson. The algorithm to perform recursive feature elimination (RFE) is quoted as follows: for each resampling iteration: Partition data into train and test/hold-back set via resampling Train model on training set using all predictors Calculate model performance Calculate variable importance or rankings for each subset size, Si, i=1...S: Keep Si most importance variables Train model on training set using Si predictors Calculate model performance using held-back samples Calculate the performance profile over the Si using held-back samples Determine appropriate number of predictors Determine final ranks of each predictor Fit final model based on optimal Si on original training set An example is given where the above is performed for five repeats of a 10-fold CV. I don't fully understand how the final four lines can be performed. I would expect there to be some variability in the features being eliminated resulting in up to 50 different Si profiles. How can a final Si profile be computed from this? It seems like the final profile might be computed by averaging accross all 50 Si profiles. However, this seems to assume that the feature being eliminated is consistnet, or am I missing something?
