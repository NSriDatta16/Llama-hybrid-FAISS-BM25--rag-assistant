[site]: crossvalidated
[post_id]: 217840
[parent_id]: 
[tags]: 
What is the output of word embedding in LSTM tutorial?

I'm new to theano and found LSTM tutorial's embedding word so confusing as lack of print function. in build_model function we have: emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps, n_samples, options['dim_proj']]) I want just example, if our input x (one sentence with 2 word) is [1, 12], then each of this integers show a word in this sentence, in function above each word converted to one vector, right? but in which shape? if 1 converted to [0.1, 0.2, 0.3] and 12 converted to [0.4, 0.5, 0.6] then are these two vectors stand by each other like: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and fed into each LSTM block in hidden layer? The other question is the size (length) of converted vector decided by which variable? (i think it's decided by dim_proj but not sure). Thanks in advance for any help you can provide.
