[site]: crossvalidated
[post_id]: 58876
[parent_id]: 58874
[tags]: 
You can also perform a likelihood-ratio test (LRT) with mixed models. The command is also anova(model1, model2) . But there are a few things you have to consider. To use the anova command after lme that has been fitted by REML (Restricted maximum likelihood; the default), the models have to include the same fixed effects and both have to be fitted via REML. If you want to calculate a likelihood-ratio test for models with different fixed effects, you have to fit the models via maximum likelihood (ML). You can do that by specifying the option method="ML" within the lme command. After that, you can just type anova(lme.mod1, lme.mod2) to calculate the LR test. If the LR test is significant, you have evidence that the model including probCategorySame is an improvement over the model without that variable. As an alternative, you can also simulate the likelihood-ratio test with the following syntax (lm1 is the simpler model and lm2 is the alternative model): sim.lme This produces a graphic like that The key is that the simulated empirical $p$-values should be roughly the same as the nominal $p$-values (the blue line should be diagonal). If that is the case, you can use the nomial $p$-value from the likelihood-ratio test. In their book Mixed-Effects Models in S and S-Plus (page 87-92) , Pinheiro and Bates discourage the use of likelihood-ratio test for assessing the significance of fixed effects. They recommend the conditional $F$-test instead, which you can use with the anova command with a single model. In your case, that would be anova(lm2) .
