[site]: crossvalidated
[post_id]: 420282
[parent_id]: 
[tags]: 
Why we really need the concept of "Local" Rademacher complexity?

Recently, I have been studying High-Dimensional Statistics: A Non-Asymptotic Viewpoint written by Martin J. Wainwright. In this book, the author uses a special complexity measure which is called Local Rademacher complexity in order to show that non-parametric least square estimator matches the minimax risk of certain function class (family of distribution). I am very confused by why we need this slightly different localized version of Rademacher complexity. I did a little research and fount out it is proposed by Vladimir Koltchinskii and Peter L. Bartlett . In Bartlett's paper, for every $r>0$ , the Local Rademacher complexity is defined as $$E[\frac{1}{n}\sup_{f\in\mathcal F, P f^2\leq r}\sum_{i=1}^n\sigma_if(X_i)]$$ which is different from the usual Rademacher complexity by the restriction of $Pf^2\leq r$ where $Pf^2$ denotes the expectation of $f^2$ with respect to the unknown data distribution $P$ . In Bartlett's paper , they claimed that " One of the short comings of the Rademacher averages(complexities) is that they provide global estimates of the complexity of the function class, that is, they do not reflect the fact that the algorithm will likely pick functions that have a small error, and in particular, only a small subset of the function class will be used. As a result, the best error rate that can be obtained via the global Rademacher averages is at least of the order of $\frac{1}{\sqrt{n}}$ (where n is the sample size), which is suboptimal in some situations." I have a lot of questions of this statement. First, why does the lower bound of the Rademacher complexity is at least of order $\frac{1}{\sqrt{n}}$ ? In Statistical Learning, we often use Rademacher complexity as an upper bound of estimation error. We never actually calculated the lower bound of it given certain function class. I think the only lower bound we know comes from the fundamental theory of statistical learning which is only true for 0-1 loss and binary classification. ( See also this question for details ). However, Rademacher complexity is tightest in this setting. Furthermore, when the number of functions is bounded by $M$ , Massart's lemma suggests that Rademacher complexity is of order $O(\frac{\sqrt{\log M}}{n})$ . Second, in which non-trival but frequently visited learning model can we have function class assumptions that Rademacher complexity is $\Omega(\frac{1}{\sqrt{n}})$ but actually we can achieve fast rate of convergence by proper analysis using Local Rademacher complexity? (The existence of such function class justified the looseness of Rademacher complexity) Third, in such analysis, we often need to assume that for every member of the class $\mathcal F$ , it is possible to control its variance using its expectation. That is to say there exists a constant B such that $$ \forall f\in\mathcal F,\ Pf^2\leq B Pf$$ why we really need this variance assumption and its intuition and implication?
