[site]: stackoverflow
[post_id]: 5359880
[parent_id]: 
[tags]: 
hdf5 and ndarray append / time-efficient approach for large data-sets

Background I have a k n-dimensional time-series, each represented as m x (n+1) array holding float values (n columns plus one that represents the date). Example: k (around 4 million) time-series that look like 20100101 0.12 0.34 0.45 ... 20100105 0.45 0.43 0.21 ... ... ... ... ... Each day, I want to add for a subset of the data sets ( one hd5f file. Question What is the most time-efficient approach to append the rows to the data sets? Input is a CSV file that looks like key1, key2, key3, key4, date, value1, value2, ... whereby date is unique for the particular file and could be ignored. I have around 4 million data sets. The issue is that I have to look-up the key, get the complete numpy array, resize the array, add the row and store the array again. The total size of the hd5f file is around 100 GB. Any idea how to speed this up? I think we can agree that using SQLite or something similar doesn't work - as soon as I have all the data, an average data set will have over 1 million elements times 4 million data sets. Thanks!
