[site]: crossvalidated
[post_id]: 417652
[parent_id]: 413843
[tags]: 
There are really 3 cases one might consider: (1) the longer-term gradients vanish (2) they explode (3) they neither vanish nor explode somehow. The problem with case (1) is that shorter-term gradients, being added to longer-term gradients, then dominate exponentially the longer-term ones, making it difficult to learn long-term dependencies. Furthermore, in my 1994 paper on the subject, I showed that this condition was a consequence of an RNN which could store memories in a stable way (which is desirable in general). The problem with case (2) is that SGD breaks down (as it assumes that we make nearly infinitesimal steps, so large gradients tend to make optimization diverge). Researchers have been striving to achieve case (3), which is not easy, and may come with its own problems (because when you have limited memory, to DO want to forget things which are not relevant, and that implies some form of vanishing gradients). So indeed, fixing the vanishing gradient problem by obtaining exploding gradients is not a good solution. However, note that some heuristic approach (as we discussed in the Pascanu et al paper cited above) can greatly mitigate the explosion situation (which only happens in rare places during the optimization, at loss cliffs corresponding to boundaries between basins of attraction of the system's dynamics).
