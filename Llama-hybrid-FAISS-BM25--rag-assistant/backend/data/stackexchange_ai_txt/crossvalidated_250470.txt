[site]: crossvalidated
[post_id]: 250470
[parent_id]: 250409
[tags]: 
Logistic regression is working with a latent variable: the probability that a realization would take on $y_i = 1$. It does not predict the actual observed / realized $y_i$ values. So the sets of $(x_i,\ y_i)$ values that you have in your dataset are only indirectly informative about the function that logistic regression is trying to find. What would be ideal would be to have a dataset composed of sets of $(x_i,\ p_i(Y=1))$ instead. Since you don't have that, logistic regression models are fit using an iterative search algorithm. Candidate betas are plugged into the formula $$ \hat p_i(Y=1) = \frac{\exp(\hat\beta_0 + \hat\beta_1x_i)}{1+\exp(\hat\beta_0 + \hat\beta_1x_i)} $$ to compute the set of estimated $\hat p_i(Y=1)$s. Using those, you can compute the log likelihood and/or deviance associated with the model based on those candidate betas. The fitting algorithm searches for the best fitting (i.e., maximum log likelihood or minimum deviance) model by working through a series of progressively better fitting candidate betas. There are a number of different search algorithms that can be used, but none use $y_i$ as $p_i(x_i)$.
