[site]: datascience
[post_id]: 30135
[parent_id]: 30013
[tags]: 
If you choose your alternative to Tree based models, then you really have an upper edge here as compared to all other linear/logistic Regressions etc.. People generally use Co-relation, Co-variance and heat maps.. etc as a process which often generates tables of distances with even more numbers than the original data, but making Dendrograms in fact simplifies our understanding of the data. Distances between objects can be visualized in many simple and evocative ways, one of them is Hierarchial Clustering .. What is Hierarchical Clustering? Hierarchical clustering is where you build a cluster tree (a dendrogram) to represent data, where each group (or “node”) links to two or more successor groups. The groups are nested and organized as a tree, which ideally ends up as a meaningful classification scheme. So now, What is a Dendrogram? A dendrogram is a type of tree diagram showing hierarchical clustering — relationships between similar sets of data. They are frequently used in biology to show clustering between genes or samples, but they can represent any type of grouped data. The columns under the same split at the leaves are somewhat having relationships between them or have similar attributes.., that is what we try to Explore and deepen out understanding about in order to cut down redundant Features.... Sample Dendrogram looks like this one... So Let's try to interpret the Diagram Now, Removing redundant features One thing that makes it harder to interpret a variable is that there seem to be some variables with very similar meanings(redundant features...) Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy. def get_oob(df): m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True) x, _ = split_vals(df, n_trn) m.fit(x, y_train) return m.oob_score_ Here's our baseline. get_oob(df_keep) 0.88999425494301454 Now we try removing each variable one at a time.... for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'): print(c, get_oob(df_keep.drop(c, axis=1))) Output(s) saleYear 0.889037446375 saleElapsed 0.886210803445 fiModelDesc 0.888540591321 fiBaseModel 0.88893958239 Grouser_Tracks 0.890385236272 Coupler_System 0.889601052658 It looks like we can try one from each group for removal. Let's see what that does. to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks'] Looks good even after dropping some of the columns.... References -: Wiki Link Notebook Link - fast.ai(Jeremy is Just Awesome..) Blog Link Edit - (just to make the answer complete) We also have something known as Partial Dependencies while using RF's which is also a very helpful insight to explore even further...
