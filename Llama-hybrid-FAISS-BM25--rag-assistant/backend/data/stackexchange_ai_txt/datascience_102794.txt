[site]: datascience
[post_id]: 102794
[parent_id]: 102791
[tags]: 
In the linked answer , it is convenient to have the $1/2$ in the loss function so it cancels when we bring down the $2$ in the derivative, and this is okay since we just want to optimize the parameters. I do not see something that should cancel out in your equation, but there could be another reason to divide through. In your case, unless you pick a silly normalization factor like zero, your two loss functions have the same parameters that optimize them, so it does not matter which we optimize. Dividing by some factor can keep the numbers from getting too large, though, especially if you're adding up over thousands or billions of predictions. Additionally, if your normalization factor is the sample size, you get some sense of the average crossentropy loss for an observation, the same as the MSE gives some sense of the average squared deviation when we do linear regression.
