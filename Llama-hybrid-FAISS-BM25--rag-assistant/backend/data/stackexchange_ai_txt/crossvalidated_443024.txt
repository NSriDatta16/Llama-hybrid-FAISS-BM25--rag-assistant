[site]: crossvalidated
[post_id]: 443024
[parent_id]: 
[tags]: 
Relationship between Gaussian process and Normal-means regression with the same covariance function used

Question 1 I am trying to understand how the predictions made by a (mean zero) gaussian process at training points relates to the same training point predictions made by a Normal-Means model with a (mean zero) prior where the prior covariance is given as the GP covariance function. Question 2 More generally, I am interested in how the two models relate to one another. My current understanding is that although the predictions made at training points should be identical, the normal-means model is not a smoother and so would not be able to make predictions at other points y*. To be more specific, I am comparing: (1) A standard Gaussian process: $y = f($ x $) + e , \ \ f($ x $) \ \ $ ~ $ \ \ GP(0,k($ x $,$ x $')), \ \ e \ \ $ ~ $ \ \ N(0,\sigma^2)$ (2) A normal-means regression: y $ $ = $ (\theta_1...\theta_n)' $ + e , $(\theta_1...\theta_n)'$ ~ $N(0,K(X,X))$ , e ~ $N(0,\Sigma)$ x is a vector of inputs, at some point in $R^D$ , and x' the equivalent at some other point in $R^D$ y and e are N by 1 vectors X is an N by D matrix and K(X,X) is the covariance function evaluated over all training points 1..N Standard results show that the posterior predictive distribution at the training points y in each case are given as follows: (1) GP: y |X,f ~ $N(K(X,X)'(K(X,X)+\sigma^2I)^{-1} y,K(X,X)(I-(K(X,X)+\sigma^2I)^{-1}K(X,X))$ (2) Normal-means: y |X ~ $N((K(X,X)^{-1}+\sigma^{-2}I)^{-1}\sigma^{-2}y, (K(X,X)^{-1}+\sigma^{-2}I)^{-1})$ Now, I (think) I have derived that the predictive means are identical after some manipulations (by working through the case where N=2 and computing the inversions), however it's possible I have not done this correctly and so wanted to check whether it is in fact true that the two distributions are the same. Looking at the variance terms it is difficult to see how they could be identical, but as shown in C. E. Rasmussen & C. K. I. Williams; 2006, the form shown for the GP can also be derived from a standard Gaussian posterior predictive distribution (same form as the Normal-means case) where some basis functions are used as features (i.e. the "weight space" view of kernel regression), so intuitively it seems plausible that they are in fact the same. Question 3 If they are in fact the same, then could it be possible to combine a Gaussian linear model with a GP (e.g. as given in C. E. Rasmussen & C. K. I. Williams; 2006 (pp.27)) using MCMC-based inference to enable more complex priors on linear model coefficients that mean Marginal Likelihood results are not analytically available? EG, a simple model could look as follows (taking as given parameters not given distributions): $y|X,B,\theta,\sigma^2$ ~ $N(XB+I(\theta_1...\theta_n)',\sigma^2)$ $B|\Omega$ ~ $N(0,\Omega)$ $(\theta_1...\theta_n)'$ ~ $N(0,K(X,X))$ Any help would be greatly appreciated, and further comments around Gaussian processes that may aid my understanding (or correct any implied misunderstanding) would also be very much appreciated. As this is my first post on stackexchange, any comments on notation or question formatting are also welcome.
