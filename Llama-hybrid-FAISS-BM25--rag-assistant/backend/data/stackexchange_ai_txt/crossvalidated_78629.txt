[site]: crossvalidated
[post_id]: 78629
[parent_id]: 78606
[tags]: 
As stated in comment, the prior distribution represents prior beliefs about the distribution of the parameters. When prior beliefs are actually available, you can: convert them in terms of moments (e.g. mean and variance) to fit a common distribution to these moments (e.g. Gaussian if your parameter lies to the real line, Gamma if it lies to $R^+$ ). use your intuitive understanding of these beliefs to propose a given prior distribution and check if it really fits your purpose and that it is not to sensitive to arbitrary choices (performing a robustness or sensibility analysis) When no explicit prior beliefs are available, you can: derive (or simply use if already available, a great ressource is http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf ) a Jeffreys (e.g. uniform for a location parameter) or a reference prior (especially in case of multivariate parameters). sometimes such choices are impossible or quite difficult to derive and in this case, you can try to choose among one of the many "generic" weakly informative prior (e.g. uniform shrinkage distribution for scale parameters of hierarchical model or $g$ -prior for gaussian regression). Having said that, there is no restriction to use a joint or an independent prior ( $p(a,b)$ Vs $p(a) \cdot p(b)$ ). As a complement, I would say that in my humble opinion, there are three major things to take care when choosing a prior: take care that your posterior is integrable almost everywhere (or proper), which is always true if you use an integrable prior (see Does the Bayesian posterior need to be a proper distribution? for more details), limit the support of your prior only if you are highly confident on the support bounds (so avoid to do it). and last but not least, make sure (most of the time experimentally) that your choice of prior means what you want to express. In my opinion, this task is sometimes the most critical. Never forget that when doing inference, a prior means nothing by itself, you have to consider the posterior (which is the combination of prior and likelihood).
