[site]: crossvalidated
[post_id]: 553615
[parent_id]: 553583
[tags]: 
Yes, RNNs can work with arbitrary and variable sized inputs, and backpropagation can be done with these arbitrary and variable sized computation graphs. You still have to "unroll" the graph (and have enough memory to do so) -- in some sense, there is no such thing as a "rolled graph", that's just a nice abstraction for humans to look at, but it's not a mathematical object that you can perform backprop on. It's typically true that during training , you might want to compute the loss on a batch of inputs, and it's easier to parallelize this if all the inputs in your batch are of the same length, hence padding / trimming is common. During inference time (or if your batch size is 1), there's no need for this. Another, more esoteric reason for preferring "fixed sized" RNNs is that historically, some neural network libraries (tensorflow) would first require the user to specify a fixed computation graph, then perform some clever allocation / assignment of graph nodes to compute device (cpu/gpu), and then you finally you could run the graph both forward and backward. Of course, this makes a dynamic computation pattern very annoying, because you have to destroy and reconstruct the graph constantly. Newer libraries (and newer versions of tensorflow), don't have this limitation anymore.
