[site]: crossvalidated
[post_id]: 219914
[parent_id]: 
[tags]: 
RNNs: When to apply BPTT and/or update weights?

I am trying to understand the high-level application of RNNs to sequence labeling via (among others) Graves' 2005 paper on phoneme classification. To summarize the problem: We have a large training set consisting of (input) audio files of single sentences and (output) expert-labeled start times, stop times and labels for individual phonemes (including a few "special" phonemes such as silence, such that each sample in each audio file is labeled with some phoneme symbol.) The thrust of the paper is to apply an RNN with LSTM memory cells in the hidden layer to this problem. (He applies several variants and several other techniques as comparison. I am for the moment ONLY interested in the unidirectional LSTM, to keep things simple.) I believe I understand the architecture of the network: An input layer corresponding to 10 ms windows of the audio files, preprocessed in ways standard to audio work; a hidden layer of LSTM cells, and an output layer with a one-hot coding of all possible 61 phone symbols. I believe I understand the (intricate but straightforward) equations of the forward pass and backward pass through the LSTM units. They are just calculus and the chain rule. What I do not understand, after reading this paper and several similar ones several times, is when exactly to apply the backpropagation algorithm and when exactly to update the various weights in the neurons. Two plausible methods exist: 1) Frame-wise backprop and update Load a sentence. Divide into frames/timesteps. For each frame: - Apply forward step - Determine error function - Apply backpropagation to this frame's error - Update weights accordingly At end of sentence, reset memory load another sentence and continue. or, 2) Sentence-wise backprop and update: Load a sentence. Divide into frames/timesteps. For each frame: - Apply forward step - Determine error function At end of sentence: - Apply backprop to average of sentence error function - Update weights accordingly - Reset memory Load another sentence and continue. Note that this is a general question about RNN training using the Graves paper as a pointed (and personally relevant) example: When training RNNs on sequences, is backprop applied at every timestep? Are weights adjusted every timestep? Or, in a loose analogy to batch training on strictly feed-forward architectures, are errors accumulated and averaged over a particular sequence before backprop and weight updates are applied? Or am I even more confused than I think?
