[site]: crossvalidated
[post_id]: 217539
[parent_id]: 217512
[tags]: 
Different people use different notions of what it means to be ergodic. As far as I know the weakest form of ergodicity is requiring that there is exactly one invariant distribution. Let us call this weakly ergodic . For a finite state Markov chain, this is equivalent to requiring that the chain is irreducible, i.e. it is possible to reach any state from any other in a finite number of transitions. In your example, you have transition matrix $$ \begin{pmatrix} 0 & 1 & 0 \\ P_A & 0 & 1 - P_A \\ 1 & 0 & 0 \end{pmatrix} $$ Let $\pi = (\pi_A, \pi_B, \pi_C)$ denote any invariant distribution. The equations for invariance give $$\pi_B = \pi_A \quad \mbox{and} \quad \pi_C = (1-P_A) \pi_A,$$ or after normalization, $$ \pi = \frac{1}{3 - P_A} (1, 1, 1 - P_A).$$ So regardless of the value of $P_A$, there is a unique invariant distribution and the chain is weakly ergodic. For a weakly ergodic chain, time averages of functions converge to averages over the invariant distribution for every initial condition. Often, people have something stronger in mind when talking about ergodicity. Let us say that a Markov chain is strongly ergodic if it is irreducible and aperiodic. In the above Markov chain, if $P_A = 1$, the state $C$ is transient (and so the chain is not irreducible) and if $P_A = 0$, the chain is periodic. So to have strong ergodicity we require $0 I recommend Norris, Markov Chains for background reading.
