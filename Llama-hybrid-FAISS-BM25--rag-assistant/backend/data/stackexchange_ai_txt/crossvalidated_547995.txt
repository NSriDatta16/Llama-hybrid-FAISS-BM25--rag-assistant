[site]: crossvalidated
[post_id]: 547995
[parent_id]: 
[tags]: 
Increasing training set decreases accuracy

I've ~16000 labeled data. I split it in ~8000 for training and ~8000 for testing of my RBF neural network, find the best hyperparameters (RMSE from 1.2 to 1.4*) and finally train the model on the whole set of ~16000 labeled data. When I now run these 16000 samples through the final model, I get way worse result (RMSE from 2.0 to 2.3*). How is this possible? My RBF has 350 neurons in the input layer, 2 in the output one and two hidden layers with 50 and 30 neurons respectively. I use gaussian distribution for weight initialization, sigmoid as an activation functions in the hidden layers and RELU in the last one, Adam to optimize and L2 as loss function. * depending on the random seed used for train-test split and weight init EDIT: I use batches of 400 samples and 500 epochs. So I have 10 000 training iteration when I use 8k samples for training and 20 000 when I use the whole dataset.
