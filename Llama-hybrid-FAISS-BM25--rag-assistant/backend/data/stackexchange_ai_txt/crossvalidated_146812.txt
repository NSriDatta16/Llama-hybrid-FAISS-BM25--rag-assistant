[site]: crossvalidated
[post_id]: 146812
[parent_id]: 143999
[tags]: 
As to your estimator of the one-step forecast: it sounds like a "jack-knife" ; regarding the general problem, it sounds like one of time series filtering; the Kalman filter is one of the first techniques found in this (huge) topic, and valid in the limit of small time-steps and Gaussian-distributed realizations. One in general tests a model on the basis of the "evidence" (likelihood of the model parameters times prior over models, integrated over the parameters), as per Bayes' theorem . More to the point of your question: So my question is, does it make sense to compute the mean absolute error using the minimal mean squared error predictor? is this valid for all types of loss functions? It seems to me there should actually be an optimal predictor for every different loss function chosen? The mean absolute error is indeed different from the LS one; the absolute value function is not differentiable in the origin, for starters. The two estimators use respectively the L1 and L2 norms, which are nonetheless related. However the estimator quality depends on the knowledge of the noise distribution, and different penalizations lead to different weighting of the noise. See the discussion in e.g. Ch.1 of Pattern Recognition and Machine Learning by C.Bishop Hope this helps!
