[site]: datascience
[post_id]: 97324
[parent_id]: 
[tags]: 
Implementing computational graph and autograd for tensor and matrix

I am trying to implement a very simple deep learning framework like PyTorch in order to get a better understanding of computational graphs and automatic differentiation. I implemented an automatic differentiator for scalar values inspired by this and then was trying to implement a tensor automatic differentiator that you can see in my code below. import numpy as np class Tensor: def __init__(self,data,_children=()): self.data=data self.grad=np.zeros_like(self.data) self._prev=set(_children) self._backward=lambda :0 def __str__(self): return f"Tensor of shape {self.data.shape} with grad {self.grad}" def dag(self): topo=[] visited=set() def build_topo(v): visited.add(v) for i in v._prev: if(i not in visited): build_topo(i) else: pass topo.append(v) build_topo(self) topo.reverse() return topo def backward(self): topo=self.dag() self.grad=np.ones_like(self.data) for v in topo: v._backward() @staticmethod def sum(self,other): _tensor=Tensor(self.data+other.data,(self,other)) def _back(): self.grad=_tensor.grad other.grad=_tensor.grad _tensor._backward=_back return _tensor @staticmethod def dot(self,other): assert self.data.shape[1]==other.data.shape[0], \ f"can't multiply two Tensor with shape {self.data.shape} and {other.data.shape}" _tensor=Tensor(np.dot(self.data,other.data),(self,other)) def _back(): self.grad=(_tensor.grad*other.data.T) other.grad=(_tensor.grad*self.data) _tensor._backward=_back return _tensor My question is how am I suppose to implement an automatic differentiator for when we have a matrix of input data where each input is a column vector of the matrix (like what we do in a neural network for training)? I would appreciate it if you give me some study material or sample code so I can implement PyTorch like autograd for matrix input or tensors.
