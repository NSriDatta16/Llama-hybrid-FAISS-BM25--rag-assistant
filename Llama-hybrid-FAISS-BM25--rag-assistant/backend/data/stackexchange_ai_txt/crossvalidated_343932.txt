[site]: crossvalidated
[post_id]: 343932
[parent_id]: 343550
[tags]: 
The expression $\frac{\partial C}{\partial z_j^l}$ represents how much does the change of the weight $j$ in the layer $l$ influence the cost $C$. You are right that this is not equal to the absolute error caused by that weight 1 . However, in neural networks, empirically there seems to be a relationship between the magnitude of the derivative and the error: Around the (local) optimum, the surface will be rather flat and will be steeper further away. To answer the first part of your question: They are not equal, but it is a convenient approximation. To answer the second part: In a typical neural network, the neurons have no predefined meaning, so we have no information which of them should be important and which shouldn't. Moreover, the aim of the training is to get the weights into a state where all the derivatives are close to zero. That means we want any local change in the weights to have zero influence on the cost. Finally, for completeness, I am including the picture of the demon in question: 1 For example, take the $f(x) = |x|$: This function has the same derivative everywhere (1 or -1 on the positive and negative part, respectively), regardless of how far from the minimum you are.
