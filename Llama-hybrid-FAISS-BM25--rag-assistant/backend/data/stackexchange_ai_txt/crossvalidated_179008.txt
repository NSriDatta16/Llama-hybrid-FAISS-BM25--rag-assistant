[site]: crossvalidated
[post_id]: 179008
[parent_id]: 179000
[tags]: 
I agree that the issue with dispersion may be a red herring. I would want to suggest not taking this word too literally and consider the possibility that whoever is recommending a focus on dispersion may not have the clearest idea of what they mean. Having said that, some thought and reinterpretation based on the example you give suggests that the real issue seems to be sparsity in the response distributions, which is modelable. Here's another way to think about your example of 20 observations: 18= yes , 2= no , n -20= no response/don't know/not applicable, missing values and so on. In other words, most of your full survey sample (whatever that n is) didn't respond to this question. I would want to understand why this is so since, quite obviously, everyone with a mobile phone should have a yes/no response to a question about data plans. So, given the blocking and flow of the questions, are the nonresponses " don't knows ," " not applicables ," " missing values " or " structural zeros "? In survey terms, structural zeros are places in the survey flow where, due to question branching higher up in the survey item "tree," the question isn't relevant or a response is impossible -- like a null set. Understanding this can and does impact how you analzye the data. In this case, it could impact the denominator for your analysis, so instead of only 20 observations, you might have many more. I can't say for sure based on the information provided. Consider this, "yes/no" or 0,1 items are categorical or discrete variables which happen to be averagable. This average is directly translatable into a probability or likelihood. In your example, the 20 observations would average out to 90% probability of a "yes" (when "yes"=1). This example also illustrates the importance of having the correct denominator of 0s and 1s since the estimated probability would be dramatically different if more 0s are added to it. Other types of multi-level nominal or multinomially scaled categorical variables aren't "averagable" in this same way without converting them into a bunch of 0,1 dummy variables. There are a plethora of approaches to analyzing categorical data: contingency table analyses, log-linear models, finite mixture models, latent class models, correspondence analysis, classification trees, logistic regression, poisson regression, negative binomial models, zero-inflated poisson or negative binomial models and tensor regression are all approaches to modeling categorical data and their interactions. So, you might choose latent class models as a dimension reducing technique that combined modeling of some target or dependent variable in a supervised regression model with a residual-based partitioning of your sample into segments on the back side. Correspondence analysis would be another dimension reducing, PCA-like method specifically for discrete data. Tensor regression is appropriate for the case where you have a very large set of contingency tables to analyse, as in genome analysis. And so on (in the interest of space, I won't spell out how each of these methods could be used, but ask for more elaboration if curious). As noted, sparsity is modelable since sparsity is just another word for rare events. But it is not the only consideration depending on how your data is distributed: is your data normally distributed? dense? sparse? extreme valued? These are all considerations in choosing an approach. Logistic regression would model a categorical dependent variable under the assumption that the categories are well populated and not sparse since it's known that the logistic distribution does not provide a good fit to the extremes of its distribution. In the case of sparse categorical dependent variables, poisson (where the variance equals the mean) or negative binomial (where the data is overdispersed, i.e., the variance is greater than the mean) regression are the better choices for rare events information. And if it is the case that you have many zeros in your denominator and these other approaches are underpredicting those zeros, then zero-inflated models of some type are worth exploring. I'm writing all of this only too emphasize that there are more than a few considerations beyond dispersion or even sparsity in how you analyze categorical survey information. Which one you choose is a function of the strategic objectives for your analysis, which approach provides the best means to that end and the scale, type and distribution of the information you have available for analysis.
