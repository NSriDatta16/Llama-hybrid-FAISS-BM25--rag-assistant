[site]: crossvalidated
[post_id]: 199464
[parent_id]: 199400
[tags]: 
The rule of thumb for unpenalized logistic regression or binary classification schemes as you have been using is to examine no more than one feature (predictor variable) per 15 cases in the least frequent category. With only 30 cases in your least frequent class, that means you should be examining about 2 features, not 200,000. So yes, you almost certainly have been overfitting. Elastic net as proposed in another answer allows evaluation of more predictors because it penalizes the regression coefficients to values lower in magnitude than they would have in a standard regression. It also selects a subset of predictors while eliminating the others. But starting with 200,000 predictors makes this a difficult task. You must be very wary. At the standard p With so many predictors you may well have "perfect separation" where a small group of your 200,000 predictors gives perfect prediction--in this data set, but probably for no other sample from the same population. And if you use elastic net (or its limiting version, the LASSO) to select a subset of features, the particular features selected will be very sample-dependent. Try repeating the variable selection process on multiple bootstrap samples of your data to see that problem in action. Also, your reliance on AUC values to evaluate schemes has hidden assumptions of which you might not be aware. That approach essentially assumes that all types of misclassifications are equally important. That is seldom the case. You will typically be better off building a model that predicts probabilities of class membership well, as with a penalized logistic regression, then evaluate the costs of different misclassifications to choose a probability threshold for classification. But none of these approaches may work in a reliable way when you have a class of 30 members and 200,000 features.
