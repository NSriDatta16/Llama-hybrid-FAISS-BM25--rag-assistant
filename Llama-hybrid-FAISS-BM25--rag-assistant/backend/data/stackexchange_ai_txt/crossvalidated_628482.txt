[site]: crossvalidated
[post_id]: 628482
[parent_id]: 
[tags]: 
Estimation under model uncertainty that cannot be adjudicated empirically

Many well-known methods address specific forms of model uncertainty that can be adjudicated empirically. For example, if we are fitting a predictive model and there is uncertainty about the set of covariates that have nonzero coefficients, common approaches to variable selection will in principle find the "right" model given the data, based on various theoretical properties and asymptotics ( recent review ). However, in some problems, there is also model uncertainty due to identification assumptions that are not falsifiable. For example, suppose we are fitting a model to estimate a causal average treatment effect, and we have a covariate that could be either a confounder or a mediator. If the variable is a confounder, then we should adjust for it to obtain a consistent estimate, but if it is a mediator, then we should not adjust for it. As another example, suppose we have missing data, and we do not know the precise mechanism of missingness. Under one mechanism of missingness, a certain model would yield consistent estimates (say, a certain inverse probability-weighted model), but under another mechanism, only a different model would do so. In both examples, it may be impossible to adjudicate between the models empirically. My question: Are there are general approaches for estimation within a class of possible models, such that at least some of the uncertainty across the models cannot be adjudicated empirically? Intuitively, presumably one could do something like Bayesian model averaging (BMA) with a prior on the models, or even on identification assumptions themselves. This prior would reflect prior beliefs about non-falsifiable forms of model uncertainty (e.g., in the first example, scientific knowledge about whether the covariate is a confounder or a mediator). Has this approach or better ones been worked out in the literature?
