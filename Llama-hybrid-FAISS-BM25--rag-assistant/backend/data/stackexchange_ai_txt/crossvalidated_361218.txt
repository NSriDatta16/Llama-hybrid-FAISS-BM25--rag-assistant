[site]: crossvalidated
[post_id]: 361218
[parent_id]: 361198
[tags]: 
Sorry, my rep is too low to comment so will be posting as an answer. The benefit of conducting CV is that you can train your model over the entire data that you have and yet still be able to get a good estimate of the true error of your model. The more variables you include in your model, the lower the training error will get. However, doing so results in overfitting because your model becomes too specialized to its training data that when unseen data comes along it will instead perform worse. As Michael said, this is due to the model, in order to minimize training error, ends up fitting to the noise present in the data. When you then try to predict unseen data which will have a different noise signature using the model, you will end up getting a greater prediction error. CV simulates this environment by holding out data for test purposes. This plays the role of the unseen data. CV then does this K times and averages the error as the validation error. Hence the validation error increases if the model is overfitted.
