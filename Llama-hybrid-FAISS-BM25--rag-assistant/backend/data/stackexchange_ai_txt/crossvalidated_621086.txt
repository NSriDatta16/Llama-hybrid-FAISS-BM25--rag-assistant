[site]: crossvalidated
[post_id]: 621086
[parent_id]: 
[tags]: 
Input shapes for multiclass LTSM classification model in tensorflow

I am building a classifier for 7 classes where my array shapes are as follows: X_train data shape - (171812, 384) y_train data shape - (171812,) X_test data shape - (37715, 384) y_test data shape - (37715,) my model is like this: # parameters input_dim= 10000 max_length =384 output_dim =128 DENSE_DIM = 32 DENSE_DIM = 32 LSTM1_DIM = 32 LSTM2_DIM = 16 WD = 0.001 FILTERS = 64 model_lstm = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim, output_dim, input_length=max_length), # tf.keras.layers.LSTM(64, return_sequences=True, stateful=False, input_shape = ####(32,384,128)), tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM1_DIM, dropout=0.2, kernel_regularizer = regularizers.l2(WD), return_sequences=True)), tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM2_DIM, dropout=0.2, kernel_regularizer = regularizers.l2(WD))), tf.keras.layers.Dense(DENSE_DIM, activation='relu'), tf.keras.layers.Dense(7, activation='softmax') ]) # Set the training parameters model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.Accuracy()]) model_lstm.summary() Layer (type) Output Shape Param # ================================================================= embedding_31 (Embedding) (None, 384, 128) 1280000 bidirectional_67 (Bidirecti (None, 384, 64) 41216 onal) bidirectional_68 (Bidirecti (None, 32) 10368 onal) dense_108 (Dense) (None, 32) 1056 dense_109 (Dense) (None, 7) 231 ================================================================= I am having ValueError: Shapes (None, 1) and (None, 7) are incompatible when I run: epochs = 12 batch_size = 250 history = model_lstm.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=batch_size) What should I be changing? Also I am using an embedding layer for this model with 3 parameters (input_dim, output_dim, input_length) and I do not understand why using an LTSM layer instead tf.keras.layers.LSTM(64, return_sequences=True, stateful=False, input_shape = (32,384,128)) does not have a compatible input shape? I thought input_shape should contain batch_size, no_features (384) and timesteps, so I put 3 parameters in? it gives ValueError: Input 0 of layer "lstm_68" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 32, 384, 128). Thank you.
