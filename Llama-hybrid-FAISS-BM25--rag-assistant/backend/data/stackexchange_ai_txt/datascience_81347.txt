[site]: datascience
[post_id]: 81347
[parent_id]: 
[tags]: 
What is the difference between additive and multiplicative attention?

This paper ( https://arxiv.org/abs/1804.03999 ) implements additive addition. I think the attention module used in this paper ( https://arxiv.org/abs/1805.08318 ) is an example of multiplicative attention, but I am not entirely sure. Can anyone please elaborate on this matter? Also, the first paper mentions additive attention is more computationally expensive, but I am having trouble understanding how. Any insight on this would be highly appreciated. Thank you.
