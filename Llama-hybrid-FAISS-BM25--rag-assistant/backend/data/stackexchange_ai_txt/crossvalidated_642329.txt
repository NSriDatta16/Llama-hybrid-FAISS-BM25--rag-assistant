[site]: crossvalidated
[post_id]: 642329
[parent_id]: 
[tags]: 
Vision transformer overfitting, cannot figure the reason why as many experiments

I am training imaging data with ~1000 channels on a modified vision transformer model. Preprocessing I am limited in the number of samples as I only have 10 images (~200x200x1000) available to me which I have converted into patches yielding around 15k patches each with an associated label and balanced dataset through upsampling. I have also performed PCA on the channels to reduce the dimensionality. Since the model I am using won't accept patch sizes that are smaller than the number of channels (eg. 8x8x32), I interpolated the patches (eg. 8x8x32 to 32x32x32). The current set contains 6 training, 2 validation and 2 test. Training Currently, these are my training and validation curves of my best results so far. Patches generated for these results were sized 8x8x25 with 50% overlap trained for 100 epochs: training loss val accuracy val balanced accuracy val f1 val loss Problem & Attempts My problem is, understanding how to move forward with these results. It seems that the model is training and learning based on the validation metrics. However, when I increase the training time to 200 epochs, the model seems to consistently plateau at ~0.3-0.4 validation balanced accuracy while training is at ~0.9. What I have tried: Different patches sizes (4x4, 8x8, 16x16 etc.) Different channel sizes through PCA (8,16,32 etc...) Different overlaps when generating patches (20%, 50%, etc.) Lowering learning rate Lowering weight decay Increasing dropout Apply early stopping Balancing dataset Data augmentation Weight decay These are some ways I have tried increasing the performance. However, instead, it resulted in poorer performance such as plateauing early in training and more extreme fluctuations. One can definitely say my model is overfitting. Could this model be too complicated for the dataset I have as it has about 22M parameters?
