[site]: crossvalidated
[post_id]: 336662
[parent_id]: 
[tags]: 
Feedforward neural network for sinusoidal prediction

This is 100% curiosity, so I apologize if the question is under-constrained. If so, please comment with where my thinking is misguided! Can a feedforward neural network predict a sinusoidal relationship? In the simplest case, where y can be reliably predicted as sin(x) (that is, f(x) = sin(x)). Can this be accomplished without recurrent nodes? Apologies if the maths are obvious! Without an LSTM or other augmentation, can a standard neural network learn a sinusoidal relationship from back-propagation? I'm most curious about theoretical constraints. Are there provably lower bounds on the number of nodes or layers that would be required to model the relationship (if possible)? As a toy example, I put together a really, really simple example with the scikit MLP regressor: from sklearn.neural_network import MLPRegressor import matplotlib.pyplot as plt import numpy as np Fs = 8000 f = 5 sample = 8000 x = np.arange(sample) # CHOOSE Y y = np.sin(2 * np.pi * f * x / Fs) # sine y = x.copy() # linear sanity check of network performance model = MLPRegressor() num_sims = 100 with_noise = np.empty((num_sims, sample)) plt.figure() for i in range(num_sims): with_noise[i, :] = np.random.rand(1, sample) + y plt.plot(with_noise[i,:]) plt.show() # now with a neural network x_expanded = np.tile(x, (num_sims, 1)) model.fit(x_expanded, with_noise) predictions = model.predict(x.reshape(1, -1)) plt.figure() plt.plot(x, np.squeeze(predictions)) plt.show() In the linear example (i.e., the y = x.copy(), I'm paranoid about shallow copies...), the network, unsurprisingly, does a fine job. In the case of the sine, it falls down pretty hard. Again, just a curiosity! I don't typically work with periodic data, so this was interesting to me.
