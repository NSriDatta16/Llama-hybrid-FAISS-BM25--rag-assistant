[site]: datascience
[post_id]: 115015
[parent_id]: 84130
[tags]: 
In fact, dense layers are hard-coded into the Bahdanau attention. Let's break down the operation: The attention vector i.e. the amount of attention the output $y^t$ should pay to the hidden state $h^{t'}$ i.e. the fixed length representation of the position $t'$ , is calculated as $$a^{t,t'} = Softmax(e^{t,t'}),$$ where $e^{t,t'}$ is the similarity measure between the two entities. While in a range of attention layers this similarity is defined a-priori as the $cosine$ , in Badhanau the similarity function is being learnt by a small neural network . For this reason, you would combine the two (concatenate or add) to pass it through a simple fully connected neural network: $$e_{Bahndanau}^{t,t'}= fnn(y^t + h^{t'}, n_{out})$$ For a (t, t') pair, this would give you a "weight" to signify the attention that particular position of the input sequence must be given. You use this weight by multiplying it to your original hidden state $h^{t'}$ and you're good to go. [Meta point] Great, now you repeat the above over the rest of your sequence position to get their weight to amplify/reduce their hidden state emphasis. Can you see a problem here though? For every single $y^t$ output you now have $[0, ..., t-1]$ hidden states. To return to your original output size convention you generate the "context vector" as the sum of the weighted hidden states: $$ c_t = \sum_{i=0}^{T}a_{t,i}h_i$$ Hope it helps! References : - Neural Machine Translation by Jointly Learning to Align and Translate, 2014 - Effective Approaches to Attention-based Neural Machine Translation., 2015
