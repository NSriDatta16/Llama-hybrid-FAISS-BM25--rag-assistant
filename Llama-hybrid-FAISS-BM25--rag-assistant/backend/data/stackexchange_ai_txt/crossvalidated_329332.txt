[site]: crossvalidated
[post_id]: 329332
[parent_id]: 329308
[tags]: 
Momentum is a whole different method, that uses parameter that works as an average of previous gradients. Precisely in Gradient Descent (let's denote learning rate by $\eta$) $$w_{i+1} = w_i - \eta \nabla F(w)$$ Whereas in Momentum Method $$w_{i+1} = w_i - \gamma v_i$$ Where $$v_{i+1} = \beta v_i + (1 - \beta) \nabla F(w)$$ Note that this method has two hyperparameters, instead of one like in GD, so I can't be sure if your momentum means $\gamma$ or $\beta$. If you use some software though, it should have two parameters.
