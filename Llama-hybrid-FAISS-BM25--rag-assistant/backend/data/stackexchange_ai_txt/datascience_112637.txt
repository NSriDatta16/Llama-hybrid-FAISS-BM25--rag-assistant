[site]: datascience
[post_id]: 112637
[parent_id]: 
[tags]: 
Training deep neural networks with ReLU output layer for verification

Most algorithms for verification of deep neural network require ReLU activation functions in each layer (e.g. Reluplex). I have a binary classification task with classes 0 and 1. The main problem I see is that ReLU is not bound to [0, 1] like the Sigmoid or Softmax activation function. How can one use/train a DNN with ReLU at the last layer for binary classification tasks with one output unit? What loss function could work? Or do I have to reformulate my problem from classification to regression?
