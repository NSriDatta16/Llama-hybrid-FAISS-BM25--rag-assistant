[site]: crossvalidated
[post_id]: 172164
[parent_id]: 
[tags]: 
Error Surface / Cost Surface topology: gradient descent on saddle points vs. local minima

It isn't obvious what the error surface / cost surface of the weight space in neural networks looks like, except that it is high dimensional (could be hundreds of thousands or millions of dimensions) and very complicated / non-convex. It would be very useful to know for doing optimization in this space, e.g. using gradient descent to find minima (i.e. training your network). According to this June 2014 UMontreal / Yoshua Bengio paper: http://arxiv.org/pdf/1406.2572v1.pdf , in high dimensional spaces, there are actually many more saddle points than local minima. Is this a well established fact in the deep learning community? Have people given this an honest consideration and changed the way they optimize weights in their networks?
