[site]: crossvalidated
[post_id]: 535113
[parent_id]: 535106
[tags]: 
They work the same except for the last step, where $k$ NN regressor averages the target values among the $k$ neighbors, while $k$ NN classifier uses a majority vote to pick a class. As noticed in the comment by @Henry, for the classification you have additional issues to solve like ties, that's why it is recommended to have odd $k$ , otherwise, the algorithm would use some kind of heuristic for breaking the ties. Notice however that if you use something like predict_proba in scikit-learn , the result would be just an average, so the same thing as the regressor would give you. If we talk about the predicted probabilities, for them to be reliable, you would need to have large $k$ (imagine probabilities calculated with $k$ equal to 2 or 3), another reason why the majority vote makes more sense.
