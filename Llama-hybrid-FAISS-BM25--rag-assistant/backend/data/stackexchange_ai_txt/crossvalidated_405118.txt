[site]: crossvalidated
[post_id]: 405118
[parent_id]: 405103
[tags]: 
Quoting from the reference (Collobert et al., 2011) from the paper you mentioned The size of the output (6) depends on the number of words in the sentence fed to the network. Local feature vectors extracted by the convolutional layers have to be combined to obtain a global feature vector, with a fixed size independent of the sentence length, in order to apply subsequent standard affine layers. Traditional convolutional networks often apply an average (possibly weighted) or a max operation over the “time” t of the sequence (6). (Here, “time” just means the position in the sentence, this term stems from the use of convolutional layers in, for example, speech data where the sequence occurs over time.) The average operation does not make much sense in our case, as in general most words in the sentence do not have any influence on the semantic role of a given word to tag. Instead, we used a max approach, which forces the network to capture the most useful local features produced by the convolutional layers So the solution being suggested here is for the convolutional network to produce a matrix of dimensionality (number of words in sentence) x (number of features in the last convolution), and then to take the maximum over rows of this matrix to get a single vector of features for the whole sentence of fixed dimension.
