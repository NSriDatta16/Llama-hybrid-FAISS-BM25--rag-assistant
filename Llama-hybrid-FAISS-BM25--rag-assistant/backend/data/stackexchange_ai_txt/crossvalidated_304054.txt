[site]: crossvalidated
[post_id]: 304054
[parent_id]: 
[tags]: 
Fast multiplication by the Hessian in Neural networks

I have question about the $R\{.\}$ function in Bishop's book on page 254 (see snippet below). My questions are as follows: I assume $R\{\bf w\}$ in (5.97) is the premultiplication of $\bf{v}^{T}$ with gradient $\nabla\bf{w}$. However, I would expect that the argument of $R$ should be a multivariable function, rather than a vector $\bf{w}$? Does it mean that the gradient of a vector w.r.t a vector is being computed here? How do I interpret this? Any clarifications on where the result (5.97) comes from are appreciated! Thanks in advance for your time and clarifiations
