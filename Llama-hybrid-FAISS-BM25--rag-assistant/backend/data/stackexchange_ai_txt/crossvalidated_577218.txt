[site]: crossvalidated
[post_id]: 577218
[parent_id]: 576973
[tags]: 
Of course, it is not the same parameters that are used to parametrize the likelihood or the posterior for example, but we keep the same letter (here $\theta$ ) to refer to the parameters of the true (original) distributions. On the other hand, in VAEs, $\phi$ is often used to refer to the parameters to the variational distributions. Indeedn, this can sometimes be unclear. To address your second point, let us say $p_{\theta}(x|z)$ is a Gaussian distribution, classically it is parametrized by a scalar mean $m$ and scalar a variance $s^2$ : $p_{\theta}(x|z)=\mathcal{N}(x;m,s^2)$ , here $\theta=\{m,s^2\}$ . Now when we say that this distribution is parametrized by a neural network we mean that $m$ and $s^2$ will be the outputs of a neural network with input $z$ . Let us denote this neural network $f_{\theta}$ , $\theta$ is here the set of weights and biases of the neural network. We can write $p_{\theta}(x|z)=\mathcal{N}(x;f_{\theta}(z))$ .
