[site]: crossvalidated
[post_id]: 580735
[parent_id]: 506118
[tags]: 
I similarly found that most folks use either AveragePooling2D or MaxPooling2D because there doesn't appear to exist a layer in TensorFlow or Keras that precisely matches the Subsampling layer as described in the LeNet-5 paper, so I ended up implementing it myself. The number of trainable parameters matches the paper's description. Please let me know if you have any suggestions for how to improve it, or if you see any bugs. Full code for my Subsampling layer is below; if you want to see how I'm using it, please see my LeNet notebooks â€” be sure to select one of the versions that use this Subsampling layer; I am maintaining multiple implementations for comparison purposes, and some of them use MaxPooling2D instead. In my experience, MaxPooling2D actually seems to do a little bit better than the custom Subsampling layer, but there are a lot more details to that paper that most simple implementations overlook (custom activation function, pixel value scaling, training set augmentation, etc.), so it may actually be better to implement all of the details in the paper, but for a very simple implementation, MaxPooling2D seems to work just as well. # Copyright 2022 Google LLC. # SPDX-License-Identifier: Apache-2.0 """Subsampling layer as defined in the LeNet paper.""" from typing import Callable, List, Optional, Tuple, Union import numpy as np import tensorflow as tf from tensorflow import keras from keras.layers import Layer class SubsamplingArgumentError(ValueError): pass class Subsampling(Layer): pool_size: Tuple[int, int] strides: Tuple[int, int] padding: str activation: Callable[[float], float] w: np.array b: np.array def __init__( self, pool_size: Union[int, List[int], Tuple[int, int]] = (2, 2), strides: Optional[Union[int, List[int], Tuple[int, int]]] = None, padding: str = 'VALID', activation: Callable[[float], float] = None, **kwargs): """Subsampling layer as described in the LeNet paper. This layer is not a simple average or max pooling that are typically used to implement LeNet: neither average-pooling nor max-pooling have any trainable parameters, while the subsampling layer described in the LeNet paper *does* have trainable parameters. Note: assumes data format is `(batch_size, rows, cols, channels)`, i.e., what TensorFlow / Keras describe as "channels_last". Args: pool_size: int or 2-tuple specifying pool size (aka kernel size) strides: int or 2-tuple; if unspecified, will be copied from `pool_size` padding: """ super().__init__(**kwargs) if isinstance(pool_size, int): self.pool_size = (pool_size, pool_size) elif (isinstance(pool_size, list) or isinstance(pool_size, tuple)) and \ len(pool_size) == 2: self.pool_size = tuple(pool_size) else: raise SubsamplingArgumentError( f"`pool_size` must be an int or 2-tuple; received: {pool_size}") if strides is None: self.strides == self.pool_size elif isinstance(strides, int): self.strides = (strides, strides) elif (isinstance(strides, list) or isinstance(strides, tuple)) and \ len(strides) == 2: self.strides = tuple(strides) else: raise SubsamplingArgumentError( f"`strides` must be an int or 2-tuple; received: {strides}") assert padding is not None and padding.upper() in ('VALID', 'SAME'), ( f"`padding` must be either 'VALID' or 'SAME'; received: {padding}") self.padding = padding.upper() self.activation = activation or (lambda x: x) def build( self, input_shape: Union[List[Optional[int]], Tuple[Optional[int], int, int, int]]) -> None: """Builds internal structures to prepare for model training. Args: input_shape: length-4 list or tuple representing (batch_size, rows, cols, channels); where `batch_size` may be None; see docs above for `__init__()` for details. """ if len(input_shape) != 4: raise SubsamplingArgumentError( f"`len(input_shape)` != 4; received: {input_shape}") if input_shape[0] is not None: raise SubsamplingArgumentError( f"`input_shape[0] must be None; received: {input_shape}") in_chan = input_shape[3] self.w = self.add_weight(shape=(1, 1, in_chan), initializer="random_normal", trainable=True) self.b = self.add_weight(shape=(1, 1, in_chan), initializer="random_normal", trainable=True) def call(self, inputs: tf.Tensor) -> tf.Tensor: """Computes subsampling value: `w * (sum of window entries) + b`.""" # `scale` here undoes the average pooling by getting the original sum, # which is what we need, but there isn't a pooling mechanism that just # gets us the sum of products. scale = self.pool_size[0] * self.pool_size[1] tf_scale = tf.constant(scale, dtype='float32') avg = tf.nn.pool(inputs, window_shape=self.pool_size, pooling_type='AVG', strides=self.strides, padding=self.padding) return self.activation(self.w * tf_scale * avg + self.b) Here's the output of model.summary() for comparison: Model: "LeNet-5" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= C1 (Conv2D) (None, 28, 28, 6) 156 S2 (Subsampling) (None, 14, 14, 6) 12 C3 (Conv2D) (None, 10, 10, 16) 2416 S4 (Subsampling) (None, 5, 5, 16) 32 flatten (Flatten) (None, 400) 0 C5 (Dense) (None, 120) 48120 F6 (Dense) (None, 84) 10164 Output (Dense) (None, 10) 850 ================================================================= Total params: 61,750 Trainable params: 61,750 Non-trainable params: 0 _________________________________________________________________
