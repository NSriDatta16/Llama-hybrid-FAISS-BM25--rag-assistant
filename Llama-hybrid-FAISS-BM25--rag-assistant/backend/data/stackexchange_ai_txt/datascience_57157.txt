[site]: datascience
[post_id]: 57157
[parent_id]: 
[tags]: 
How to run a model in an application using gpu (without CUDA)

I am looking for a way to utilize a computer’s gpu without using cuda (or any installable software). The reason for this is I have an application where the neural network runs on a user’s computer and I can’t assume they will have the knowledge/ spend the time and effort to install CUDA. I see that it is possible to utilize the gpu using WebGL but from my reading, it sounds like due to other limitations with this approach it is not actually faster than using cpu. Does anyone know how to do this maybe using openCL or some other way to do it?
