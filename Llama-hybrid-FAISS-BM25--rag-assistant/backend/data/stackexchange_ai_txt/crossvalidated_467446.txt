[site]: crossvalidated
[post_id]: 467446
[parent_id]: 467350
[tags]: 
I have done simulations of linear regression with various levels of multicollinearity and to my great surprise, it seemed to have very little effect. I had a professor tell me that don't even worry about it unless you have > .8 or maybe even > .9 correlation. That turned out to be pretty true in general in my simulations. If I had > .9 I might pick one of the predictors and ignore the other, depending on the situation. But like some other people have said, VIF's are the best way to look at things. This is because the problem isn't actually correlation, it's linear combinations of the predictors. That is because in the math of regression we have to invert a matrix and you can't do that when there are linear combinations of predictors, if you remember your linear algebra. If you do have a lot of variables that have high VIF's and you just can't fathom whittling down the predictors, you have some options. You can do PCA of which is ubiquitous online, so I won't say it here. The other option is QR decomposition. I learned about it here first: https://mc-stan.org/users/documentation/case-studies/qr_regression.html That is written in Stan which has a crazy huge learning curve. I little softer is brms, which is a high level interface for Stan. https://www.rdocumentation.org/packages/brms/versions/2.12.0/topics/brmsformula Make the model formula and set decomp=TRUE. Then run the formula the command brm https://www.rdocumentation.org/packages/brms/versions/2.12.0/topics/brm But only try it if you have large numbers of predictors with high VIF's and you can't fathom reducing your predictors.
