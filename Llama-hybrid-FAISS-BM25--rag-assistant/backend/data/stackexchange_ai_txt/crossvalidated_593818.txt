[site]: crossvalidated
[post_id]: 593818
[parent_id]: 593815
[tags]: 
Linear regression is about estimating the correct coefficients from potentially noisy data. Any prior information you have about the coefficients will make that task easier. If you have a prior guess about where the coefficients are likely to be, that leads you to Bayesian linear regression/regularization. If all you know is that the coefficients should be positive (for physical reasons), that information is helpful too. It is possible that the best fit for the data you have requires negative coefficients, but as these are not the correct coefficients the model may not generalize well to new data. Constraining the coefficients to be positive will help you identify a more correct model in the presence of this noisy data. It is also possible that for all the data you are interested in, there are two models that give the same outputs, one which has only positive coefficients and other other which doesn't. In these settings, getting the "right" coefficients is still useful for interpretability. See the answers in Sextus Empiricus' link.
