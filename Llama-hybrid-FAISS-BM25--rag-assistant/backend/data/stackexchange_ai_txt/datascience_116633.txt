[site]: datascience
[post_id]: 116633
[parent_id]: 
[tags]: 
Why, in a classification task, with $n$ input variables, we can obtain all $2^n$ classiﬁcation functions for each possible set of missing inputs?

I don't get why, for a classification task with missing values, With $n$ input variables, we can obtain all $2^n$ diﬀerent classiﬁcation functions needed for each possible set of missing inputs, . Classiﬁcation becomes more challenging if the computer program is not guaranteed that every measurement inits input vector will always be provided. To solve the classiﬁcation task, thelearning algorithm only has to deﬁne a single function mapping from a vectorinput to a categorical output. When some of the inputs may be missing,rather than providing a single classiﬁcation function, the learning algorithm ˜must learn a set of functions. Each function corresponds to classifying with diﬀerent subset of its inputs missing. This kind of situation arises frequently in medical diagnosis because many kinds of medical tests are expensive or invasive. One way to eﬃciently deﬁne such a large set of functions is to learn a probability distribution over all the relevant variables, then solve the theclassiﬁcation task by marginalizing out the missing variables. With $n$ input variables, we can now obtain all $2^n$ diﬀerent classiﬁcation functions needed for each possible set of missing inputs, but the computer program needs to learn only a single function describing the joint probability distribution for an example of a deep probabilistic model applied to such a task in this way. - Goodfelloz et al., Deep Learning , chapter 5 Indded, I get that each variable offers either Classification A/Classification B output, and that we have n variables so that we have $\overbrace{2\times 2\times ... \times 2}^{n \ times}$ possible outpus. But that doesn't make functions, does it? I am a slow learner, don't hesitate to explain it to me with dummy examples or graphs :)
