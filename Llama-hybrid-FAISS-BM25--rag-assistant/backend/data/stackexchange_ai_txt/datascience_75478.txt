[site]: datascience
[post_id]: 75478
[parent_id]: 75464
[tags]: 
DON'T USE ACCURACY! USE PROPER SCORING RULES! What you propose is related to the area under the receiver operator curve, ROCAUC. ROCs plot the sensitivity and specificity (really 1-specificity) at all possible threshold cutoffs. It sounds like you would pick the model that has the highest accuracy value, regardless of that threshold. If the best accuracy comes from logistic regression with a threshold of $0.6$ , go with that model. If the best accuracy comes from KNN with a threshold of $0.07$ , go with that model. That sounds great, right, picking the most accurate model? THIS IS INCORRECT , tempting as it sounds. Here are a few blog posts on this topic by a professor at Vanderbilt University and an active member on Cross Validated (the statistics Stack). https://www.fharrell.com/post/class-damage/ https://www.fharrell.com/post/classification/ (Frank Harrell even has a post about how ROCAUC is flawed for model comparisons.) Accuracy is a flawed performance metric. Any performance metric based on a threshold has considerable flaws. Please refer to this excellent post on the topic. Shamelessly, I will link a question I posted on a similar topic that was answered by the same person with the same gist. Here is yet another post of his on this topic. (I plan to accept that answer but don't want to yet so others might post their thoughts.) An easy proper scoring rule to get you started is Brier score, basically square loss. Take the probability of being in class $1$ , subtract the true class ( $0$ or $1$ ), square that value, and add up those values for each prediction. $$Brier(y,\hat{p}) = \sum_{i=1}^N \big(y_i-\hat{p}_i \big)^2$$ $y_i$ is the true class, $0$ or $1$ , and $\hat{p}_i$ is the predicted probability (which will most likely be the predicted probability of being in class $1$ ). You can adjust Brier score if your software gives you the probability of being class $0$ .
