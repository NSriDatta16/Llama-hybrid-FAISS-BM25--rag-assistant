[site]: datascience
[post_id]: 32029
[parent_id]: 29470
[tags]: 
I tried using a CountVectorizer but it's giving me a dataframe with way too many columns and the model is having a hard time using it What model did you use? There are models that are well suited for such data, and some even can handle sparse data natively - in scikit-learn you can use : LASSO or other sparse linear models. Naive Bayes - Multinomial or Bernoulli one, make sure to try both, sometimes Bernoulli (which is simple, it only uses indicator, and not counts) performs better Other than that, Factorization Machines can be also used (FMs inventor, Steffan Rendle did even win a Kaggle competition which had really sparse data using them). Also another approach would be to just extract keywords and run BoW on them - for example Gensim library has methods to do that. For both of the above approaches the suggestion mentioned by Kasra Manshaei is still valid. Yet another approach would be to use neural networks - especially Recurrent NNs, but their usage is not that straightforward, and they might need serious computational resources to be trained reasonably (though they can use pretrained word embeddings, which makes it easier). BTW if you have problems with keeping all that data in memory, you might try to dump it to disk and then use a library that gets files as input - for example Vowpal Wabbit .
