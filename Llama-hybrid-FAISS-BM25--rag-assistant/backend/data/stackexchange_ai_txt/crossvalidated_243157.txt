[site]: crossvalidated
[post_id]: 243157
[parent_id]: 242864
[tags]: 
I do not recommend using princomp() because it is a source of constant confusion. Use prcomp() instead. Princomp() computes covariance matrix using the $1/n$ factor, instead of the more standard $1/(n-1)$ factor, which means that the eigenvalues returned by princomp and by any other R function such as e.g. prcomp or eigen(cov()) will, very confusingly, differ: Why do the R functions 'princomp' and 'prcomp' give different eigenvalues? Your issue is very related to that (as correctly identified by @whuber in the comments). When princomp is run with cor=FALSE input argument (which is the default), it computes eigenvectors of the covariance matrix, centers the data, and projects the data onto the eigenvectors. When run with cor=TRUE , the function computes eigenvectors of the correlation matrix, $z$-scores the data using variances computed with $1/n$ factor , and projects the data onto the eigenvectors. This means that PCA scores produced by the following two lines will be slightly different (because scale uses $n-1$): princomp(scale(data))$scores princomp(data, cor=T)$scores I find this counter-intuitive and confusing. As @ttnphns points out in the comments, using $n$ is not necessarily worse than using $n-1$. But princomp 's choice is inconsistent with almost all other R functions, leading to great confusion. If you supply already $z$-scored data and specify cor=TRUE , the function will $z$-score the data again, according to its own liking, by multiplying each column with $\sqrt{(n-1)/n}$. This is the difference that you observe. (See here about an unrelated algorithmic difference between princomp and prcomp .) Some comments on the source code Let us take a look at the code that you provided. The function starts with computing the covariance matrix and then multiplying it with (1 - 1/n.obs) . You are over-thinking this; it simply converts $1/(n-1)$ factor into $1/n$ factor, because $1-1/n=(n-1)/n$. So okay, covariance matrix is now scaled to be maximum likelihood (instead of unbiased). When cor=FALSE (which is the default), the scores are computed as scr = scale(z, center = cen, scale = sc) %*% edc$vectors which is centered data (non-scaled: sc is equal to 1 ) multiplied by the eigenvectors. When cor=TRUE , the scores are computed with the same formula, but now the sc variable is set to sqrt(diag(cv)) where cv is the ML covariance matrix. As you wrote yourself, in your case sc=.9486833 , which is $\sqrt{9/10}$. So even though you $z$-scored your data, princomp will re-$z$-score it for you because it prefers different denominator. Hence the confusion.
