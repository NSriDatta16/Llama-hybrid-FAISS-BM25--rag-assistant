[site]: datascience
[post_id]: 117465
[parent_id]: 
[tags]: 
Flatten layer description in "Hands-on Machine Learning"

I am currently studying from Hands-On Machine Learning with Scikit-Learn and TensorFlow . At some point in Chapter 10, the author demonstrates an MLP architecture to tackle the FashionMNIST problem: model = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation="relu")) model.add(keras.layers.Dense(100, activation="relu")) model.add(keras.layers.Dense(10, activation="softmax")) He then gives the following descripition for the flatten layer: Next, we build the first layer and add it to the model. It is a Flatten layer whose role is simply to convert each input image into a 1D array : if it receives input data X, it computes X.reshape(-1, 1) . This layer does not have any parameters, it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape: this does not include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting shape=[28,28]. At this point, I can't understand why X.reshape(-1,1) is the chosen reshape action. Assuming that Flatten is performed separately for each batch (is that true?), it should transform (28,28) to (784,), yet the above reshape yields (784, 1 ), which is not even a 1-D array. If that is correct, what is the purpose of this extra dimension with length 1? EDIT: Considering also batch_size, it should transform (batch_size,28,28) to (batch_size, 784). However, the reshape results to (batch_size*784,1), which does not seem correct either.
