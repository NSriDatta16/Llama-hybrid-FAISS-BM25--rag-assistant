[site]: crossvalidated
[post_id]: 410977
[parent_id]: 410975
[tags]: 
It could be considered as probability of output being 1 only in case if classifier is calibrated . For example, in weather forecasting it's important to have reliable rain probability estimation. Let's say your model says that there is 80% that there will be rain tomorrow. Your classifier will be considered calibrated only when you observe predictions with 80% for sometime and see that in nearly 8/10 predicted days there indeed was rain and 2/10 were just false positives. If your classifier is not calibrated than you cannot give direct interpretation to this probability. UPDATE: If your classifier is not calibrated than you cannot interpret probability from frequential point of view. It could be interpreted as some form of certainty/uncertainty. If you consider each class as hypothesis, probabilities might tell how certain you're about observation. Typically, network produces unbounded outputs which people call logits. Logits are then transformed into the probabilities using softmax function. Logits could be interpreted as activations that accumulate information about certain features. Logits will be larger for cases when there are more information present about some class. For example, let's say we want to classify digit 0. Imagine we try to classify the same image twice. Once with whole number 0 and once with only half of 0 (left or right). Logits will be larger for the first case since network can get more information from the image. Alternatively, logits could be interpreted as a value that proportional to log-posterior that has been calculated from the unnormalised probability distributions (since value could be greater than 0). $$ \ln[q(D|C_1) \cdot p(C_1)] = \ln[a \cdot p(D|C_1) \cdot p(C_1)] $$ where $q(\cdot)$ is unnormalised probability distributions (note that we can consider that one of the probability distributions is normalized) and $a$ is just some normalization constant. In that way, exponent helps network to transform logits into the unnormalised probability space (from log space) and averaging over all hypothesis helps us to eliminate common scaling factor $a$ $$ P(C_1|D) = \frac{a \cdot p(D|C_1) \cdot p(C_1)}{\sum_{k}{a \cdot p(D|C_k) \cdot }p(C_k)} = \frac{a \cdot p(D|C_1) \cdot p(C_1)}{a \cdot \sum_{k}{p(D|C_k) \cdot }p(C_k)} = \frac{p(D|C_1) \cdot p(C_1)}{\sum_{k}{p(D|C_k) \cdot }p(C_k)} $$
