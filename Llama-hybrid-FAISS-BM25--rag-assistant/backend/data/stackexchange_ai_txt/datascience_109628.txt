[site]: datascience
[post_id]: 109628
[parent_id]: 109414
[tags]: 
Welcome to datascience.stackexchange, There are 3 phases to your project: Ingesting , Storing and Querying . But as far as the different Tecnological Elements you might look at it as 2: Stream Processor and a NO-SQL DB . The Stream-Processing has many options, and in general might be chosen by: The publisher's (the "source") end-point technology (message queue service, cloud storage, etc..) The stream's Volume and Velocity (the rest of the big-data "Vs" are well defined here) with regard to your system's required SLA The targeted NO-SQL DB (can require / excempt a pre-processing stage, different cluster properties to handle the stream's "Vs", etc..) The NO-SQL DB has even more options, and in general might be chosen by: NO-SQL data family type (tabular, document, key-value, time-series, graph, etc..) Usage user-stories (query types, frequency of usage, etc..) Scale requirement (the read / write throughput requirement, mainly related to the "Vs" but also to the query usage) ===== The NO-SQL options can be better filtered in this early stage, since you provided a lot of details on the nature of your data. So I'd start with it: The nature of your data has 2 important elements: a purely json format, and a strong time-series like usage. This mainly suggest you'll need a NO-SQL DB that is: Document (json like) based Time Series based Scaleable I can think of 2 strong candidates: ElasticSearch vs MongoDB Both are: Scaleable document DBs from day 1, with elastic being a time-series based from day 1, and mongo adding time-series support since its v5. Both have a free open-source version vs a managed paid service you can opt for Elastic Strong built-in stats regarding its field - might be usefull for your per-attribute stats The ability to partition the DB to different indices by the time axis (each represent a time-sliced db). This contributes to a series performance boost when your queries are time sliced (like in your case). This could be further improved by slicing the indices further per-source, per-time-slice MongoDB Pretty much pioneered the BSON data type, strongly known for its efficiency handling stored binary json. ===== I recommend start testing one of the above (or other that fits the category, usage, etc..). See if you can find a larger common-ground for any of them to support the different sources' consumption technology, the "Vs" and SLA requirements. For ex: Elastic has the Logstash component that claims to support a variety of data sources plugins. This might help you filter the Stream-Processing options further. There are no definite options here, and you should also consider by the current personnel knowledge available at you disposal. (existing client-side languages, dev-ops server-side technology familiarity, dba's existing skills, etc..). BTW, the abilty to filter by "source" could be either adding a source attribute to your json, or excempting that in case you're going with the index-per-source solution.
