[site]: crossvalidated
[post_id]: 566023
[parent_id]: 566008
[tags]: 
A typical way to visualize in this way is t-distributed stochastic neighbor embedding (t-SNE). Briefly, t-SNE tries to create a low-dimension representation of high-dimensional data that respects, in some sense, the distance between points in the high-dimensional space. If the low dimension is $2$ , you can plot the points in a scatterplot, giving a unique color to each class. t-SNE is available in the R package Rtsne and the Python package sklearn . I have read some about how something called universal manifold approximation and projection (UMAP) is starting to be favored over t-SNE, though the math is harder, and the advantages UMAP has over t-SNE might not be worth it if you either do not understand the technique or have a customer who does not understand the technique. Offhand, I know of two advantages UMAP has over t-SNE. UMAP seems to be faster. While t-SNE does not create a function that can be applied to new data, UMAP can be applied to new data. I would consider the latter of those to be more important for someone interested in condensing many dimensions into fewer to have a low-dimension feature space for a regression or classification model that has some kind of out-of-sample validation (e.g., cross validation), rather than your objective of visualizing your data.
