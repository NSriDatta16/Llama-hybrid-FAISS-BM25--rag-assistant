[site]: crossvalidated
[post_id]: 337762
[parent_id]: 
[tags]: 
Why does $P(\theta_1\mid D, \theta_2) \propto P(D \mid \theta_1, \theta_2)P(\theta_1)$ hold?

Suppose that in a Bayesian framework we have observed data $D$, using independent prior distributions on the parameters of the model, denoted by $\theta_1, \theta_2$. Then, the joint posterior distribution of $\theta_1, \theta_2$ can be written as: $$ P(\theta_1, \theta_2 \mid D) \propto P(D \mid \theta_1, \theta_2)p(\theta_1)p(\theta_2) $$ One way to obtain estimates from the posterior is use Gibbs Sampling . This sampler requires simulating from the full conditional distributions for both $\theta_1$, $\theta_2$. One relation for the full conditional that caught my eye in a book is the following: $$ P(\theta_1\mid D, \theta_2) \propto P(D \mid \theta_1, \theta_2)p(\theta_1) $$ I am wondering how exactly this proportional relation is derived. Would anyone have any ideas? It seems that there are extra assumptions here that are implicit.
