[site]: crossvalidated
[post_id]: 634880
[parent_id]: 
[tags]: 
Estimating average processing time based on complexity

We calculate average processing time (y) to monitor the efficiency of a process. The process can vary in complexity and we observe some proxies to assess the complexity of the process (x1, x2, x3). Now, it has been argued that the complexity has increased over the last couple of years and that this is the reason for the increased processing time. Hence, I was wondering, if there I a way to measure the processing time for a "standardized" process (controlling for x1,x2,x3). My intuition was to set up a simple OLS estimation and include dummy variables for all but one year. I can then observe the sign of the year dummies and know whether it has increased or decreased over the years. Now to be able to communicate this to the important people, I was wondering about the following two topics: Is there a way to estimate the average time for a standardized process (controlling for x1,x2,x3)? Since I will rerun the regression next year, this will of course produce different estimators and hence likely change the average time for a standardized process in all years. Is there a way to keep past averages fixed? The time is not only influenced by the complexity, but also by the number of cases. How do I take this into account? I assume that I am not the first person facing this problem, hence any pointers on where to look are very welcome.
