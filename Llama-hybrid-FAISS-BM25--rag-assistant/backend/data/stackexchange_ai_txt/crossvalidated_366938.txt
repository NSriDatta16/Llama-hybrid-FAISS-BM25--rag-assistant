[site]: crossvalidated
[post_id]: 366938
[parent_id]: 366935
[tags]: 
First off, don't use the in-sample accuracy to choose a model. This will invariably lead to overfitting. In-sample accuracy is not a good guide to out-of-sample prediction. Instead, use a holdout sample. Regarding your main question: again, use a holdout sample to see how well your algorithm performs on truly new data. Thus, if you are interested in $h$-month-ahead forecasts: Fit your models to the data except for the last $2h$ months. Forecast all of them out to a horizon of $h$ months. Note the forecast error of each model, using rmse or whatever. Pick the model that performed best. Re-fit this model to the data except the last $h$ months. Forecast $h$ months ahead. Note the forecast error. Do this for all your time series. Check how well this algorithm worked, and compare it to the performance of a few very simple benchmark methods , like always forecasting the historical mean, or the last observation. Or taking the average of all your candidate models' forecasts - averages of forecasts often outperform choosing the "best" method by some criterion.
