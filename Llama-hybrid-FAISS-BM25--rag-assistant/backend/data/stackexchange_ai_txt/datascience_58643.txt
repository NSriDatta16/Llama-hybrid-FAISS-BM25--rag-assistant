[site]: datascience
[post_id]: 58643
[parent_id]: 32790
[tags]: 
max_features selects the number of features per split, not per tree. So, at every split of the tree, the algorithm randomly selects x% of your columns and selects the best one amongst this subset that leads to the most information gain/lowest loss. That being said, max_features and max_depth (even tuning max_depth might go slightly against the original idea for the algorithm which was to fit a bunch of overfit = high variance trees and then minimize the variance via. averaging, whilst mitigating correlation amongst trees by randomly selecting features at every split) are generally sufficient enough to control overfitting. Set n_estimators to be as large as computationally possible so that you can minimize variance as much as you can when averaging. However, there are no hard rules. I'd recommend starting with tuning max_features first, max_depth next, and all of the other min/max_xyz hyperparameters if results are still not satisfactory. In general, random forests should not require large amounts of tuning to be decent.
