[site]: crossvalidated
[post_id]: 474058
[parent_id]: 
[tags]: 
Number of parameters and neural networks

In basic statistics one often uses the rule-of-thumb that the number of parameters should not exceed the number of data points. There is obvious intuition behind it, grounded, e.g., in fitting data with polynomials, obtaining unbiased variance estimate, counting number of degrees-of-freedom for chi-squared test, etc. As I am learning about neural networks (and machine learning in general) I have yet to encounter any comparison between the parameter number and the number of samples. It strikes me, since there is obviously much talk about overfitting, underfitting, and regularization, and stressing that there are no simple rules, but only experience. I am wondering about the reasons for this omission: lack of contact between machine learning and statistics OR some deeper reason why this rule does not apply? Or something else? As a bonus question: how do information criteria fit into this picture?
