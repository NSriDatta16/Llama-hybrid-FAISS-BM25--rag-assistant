[site]: datascience
[post_id]: 65748
[parent_id]: 65745
[tags]: 
Cosine normalisation is result of the fact that we bound dot product and hence decrease the variance, when we use cosine similarity or centered cosine similarity instead of dot product in neural networks (which is quasi ground-stone in NN) Main benefit of cosine normalisation is Cosine normalization bounds the pre-activation of neuron within a narrower range, thus makes lower variance of neurons. Also it does not depend on any statistics on batch or mini-batch examples, and performs the same computation in forward propagation at training and inference times. In convolutional networks, it normalizes the neurons from the receptive fields rather than the same layer or batch size. Have a look at this paper showing emipirically comparison between normalisations you mentioned. C.N. comes on top.
