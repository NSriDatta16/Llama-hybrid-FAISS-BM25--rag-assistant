[site]: crossvalidated
[post_id]: 266874
[parent_id]: 266858
[tags]: 
It's certainly possible to improve prediction performance the way you suggest, although results on any particular problem aren't guaranteed. In general, methods that combine the predictions of multiple base models are called ensemble methods . There's a huge literature on them, and they tend to be very successful in practice. The method you're suggesting is form of stacking , where the outputs of the base models are fed as inputs to higher-level model, which uses them to produce the final prediction. The way you implement stacking matters. One issue concerns the use of held-out data. Stacking properly requires doing the following: Partition the data into disjoint subsets, as in K-fold cross validation. For each fold, you have a training set and a validation set. Train each of the base models on the training set, then use them to produce predictions on the validation set. After doing this for all K folds, you have predictions from every base model for every point when it was part of the validation set. Use these validation set outputs to learn the parameters of the higher level combiner/generalizer model. In your case, this will be the weights of a linear regression model. The details of the higher level model also matter. One issue is that the outputs of the base model will be correlated, because they're all trying to solve the same problem. When using linear regression as the higher level model, this is exactly the same issue as multicollinearity in standard regression problems. If unconstrained, the weights can be unstable and give poor predictive performance. Breiman (1996) discusses this issue. The solution is to constrain the weights to be nonnegative, which improved performance. A sum-to-one constraint makes sense, but turned out not to matter in practice. Penalizing the $\ell_2$ norm (i.e. ridge regression, which is often used to solve the multicollinearity problem in ordinary regression settings) worked better than unconstrained weights, but not as well as the nonnegativity constraint. Note that these results may not hold in all cases (e.g. there are other papers looking at classification problems where nonnegativity constraints weren't necessary). Ensemble methods in general (including stacking) typically perform better when there's greater diversity among the base models. Many methods take explicit steps to encourage diversity. This is also something you could look into. My answer here might also be relevant. References Wolpert (1992) . Stacked generalization. Breiman (1996). Stacked regressions.
