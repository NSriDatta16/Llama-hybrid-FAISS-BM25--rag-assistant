[site]: datascience
[post_id]: 120244
[parent_id]: 120243
[tags]: 
$$ \text{logit}=\hat\beta_0+\hat\beta_1x\\ \text{cor}(x, \text{logit})\\ =\text{cor}(x, \hat\beta_0+\hat\beta_1x)\\ =\text{cor}(x, \hat\beta_1x) $$ If the estimated slope coefficient $\hat\beta_1>0$ , then $\text{cor}(x,\hat\beta_1x)=\text{cor}(x,x)=1$ . Consequently, this does not test any assumptions: by definition, the linear prediction of your logistic regression model has a perfect (perhaps negative) correlation with the feature. If you understand why the feature in a simple linear regression is perfectly correlated (perhaps negatively) with the predictions, the same idea applies here.
