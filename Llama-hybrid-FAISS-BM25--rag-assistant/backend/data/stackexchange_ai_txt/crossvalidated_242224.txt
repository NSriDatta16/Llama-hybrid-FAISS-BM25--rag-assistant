[site]: crossvalidated
[post_id]: 242224
[parent_id]: 
[tags]: 
Problem with "Pattern Recognition and Machine Learning" (Bishop) Ch. 10

I'm working on putting together a variational solution to a somewhat "exotic" mixture model, and got stumped setting up the computation for the lower bound. This prompted me to go back to this wonderful ref. in an effort to see where I may be going wrong, and this has lead me to virtually the same problem I'm having with my 'real' problem. Specifically, the author obtains eq. 10.64: $$ \begin{align} \mathbb{E}_{\mu_k, \Lambda_k} &\left[ \left(x_n - \mu_k\right)^T \Lambda_k \left(x_n - \mu_k\right) \right] \\ & = D\beta_k^{-1} + \nu_k\left(x_n - m_k\right)^T W_k \left(x_n - m_k \right) \end{align} $$ Clearly, I'm doing something wrong, and I'm doing it consistently as I'm not able to obtain this result. Which is very frustrating considering the author's claim that the expectation value is "easily evaluated". Drats! EDIT: For the request to clarify the (relevant) notation, the author uses the following: The observed data is $X = \left\lbrace x_1, x_2, \ldots, x_N\right\rbrace$, where each $x_i \in \mathbb{R}^D$. Since it's a Gaussian mixture model he is attempting to fit, he introduces an independent Gaussian-Wishart prior governing the mean and precision of each Gaussian component as $$ \begin{align} p\left(\mu, \Lambda\right) &= p\left(\mu \vert \Lambda \right) p\left(\Lambda \right) \\ &= \prod_k \mathscr{N}\left(\mu_k \vert m_0, \left(\beta_0 \Lambda_k\right)^{-1}\right) \mathscr{W}\left(\Lambda_k \vert W_0, \nu_0 \right) \end{align} $$ so that the $\beta_k, \nu_k, m_k$ values are the updated values. MORE EDIT: I think I see it, not sure if my logic is sound though. My thinking has me going down the following path: $$ \begin{align} \mathbb{E}_{\mu_k, \Lambda_k} &\left[ \left(x_n - \mu_k\right)^T \Lambda_k \left(x_n - \mu_k\right) \right] \\ & = \mathbb{E}_{\Lambda_k}\left[ \text{tr}\left(\Lambda_k \text{var}\left( x_n - m_k \right)\right)\right] + \mathbb{E}_{\Lambda_k}\left[\left(x_n - m_k\right)^T \Lambda_k \left(x_n - m_k \right)\right]\\ &= \mathbb{E}_{\Lambda_k}\left[ \text{tr}\left(\Lambda_k \left(\beta_k \Lambda_k\right)^{-1} \right)\right] + \nu_k \left(x_n - m_k\right)^T W_k \left(x_n - m_k \right)\\ &= D\beta_k^{-1} + \nu_k\left(x_n - m_k\right)^T W_k \left(x_n - m_k \right) \end{align} $$ Look OK?
