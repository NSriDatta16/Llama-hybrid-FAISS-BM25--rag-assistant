[site]: crossvalidated
[post_id]: 44327
[parent_id]: 44313
[tags]: 
In fact their is a deep lemma about dimension reduction, the Johnson-Lindenstrauss lemma, which asserts that given a set $A = \{a_1,\dots,a_n \}$, a map $f : \mathbb{R}^D \rightarrow \mathbb{R}^d$ is an $\epsilon$-isometry if for every pair $a,a^{'} \in A$ we have $$ ( 1 - \epsilon) || a - a^{'} ||^2 \leq || f(a) - f(a^{'}) ||^2 \leq ( 1 + \epsilon) || a - a^{'} ||^2 $$ and the Johnson-Lindenstrauss lemma asserts that there exists a linear $\epsilon$-isometry whenever $d \geq k \epsilon^{-2} \log ( n )$ where $k$ is an absolute constant. You can realize such a map with an i.i.d. gaussian entries matrix. Edit : oh sorry i wasnt clear enough about how i think it relates to the question (maybe i am wrong) for dimension reduction (at least in a context where only the distance between points is important like clustering for example) i would calculate how much my mapping (PCA here i think) is changing the distance between points and compare the upper bound of that distorsion to the bound given by JL for example if you go to dimension 2 i would compare it to $\sqrt(\frac{\log(n)}{2})$ if i have n points (of course here 2 is much too low to get a non trivial bound). It gives me a way to assert that my algorithm has the best behavior possible (even if usually i guess you think the other way around setting first an $\epsilon$ and getting a $d$). Another thing the result gives me is a way to construct the best algorithm for dimension reduction which is to use a matrix whose entries are normal random variables as my "projection".
