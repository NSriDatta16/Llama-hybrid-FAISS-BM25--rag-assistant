[site]: crossvalidated
[post_id]: 285745
[parent_id]: 
[tags]: 
Convolutional neural networks backpropagation

My question is regarding the answer to this question: Training a convolution neural network It seems like the answer is saying to change all the weights in a given filter by the same amount in the same direction? Won't the CNN (convolutional neural network) be unable to learn appropriate filters this way? See the diagrams representing the weights here: http://jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/ The 2x2 delta is backpropagated onto a 2x2 filter which creates a 3x3 grid of the change in weights If you are supposed to add up those 9 values and then add them to the original 2x2 filter at all positions it will just change each value in the 2x2 filter by the same amount and in the same directions. Is that the correct way to update weights in a CNN? Or are you supposed to update each weight (each value in the 2x2 filter) individually by its own gradient like in a normal artificial neural network ANN?
