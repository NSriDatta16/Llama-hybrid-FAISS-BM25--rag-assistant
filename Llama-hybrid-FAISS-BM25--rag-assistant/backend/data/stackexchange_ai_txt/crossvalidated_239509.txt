[site]: crossvalidated
[post_id]: 239509
[parent_id]: 238496
[tags]: 
"Unfolding through time" is simply an application of the chain rule, $$\frac{dF(g(x), h(x), m(x))}{dx} = \frac{\partial F}{\partial g}\frac{dg}{dx} + \frac{\partial F}{\partial h}\frac{dh}{dx} + \frac{\partial F}{\partial m}\frac{dm}{dx}$$ The output of an RNN at time step $t$, $H_t$ is a function of the parameters $\theta$, the input $x_t$ and the previous state, $H_{t-1}$ (note that instead $H_t$ may be transformed again at time step $t$ to obtain the output, that is not important here). Remember the goal of gradient descent: given some error function $L$, let's look at our error for the current example (or examples), and then let's adjust $\theta$ in such a way, that given the same example again, our error would be reduced. How exactly did $\theta$ contribute to our current error? We took a weighted sum with our current input, $x_t$, so we'll need to backpropagate through the input to find $\nabla_\theta a(x_t, \theta)$, to work out how to adjust $\theta$. But our error was also the result of some contribution from $H_{t-1}$, which was also a function of $\theta$, right? So we need to find out $\nabla_\theta H_{t-1}$, which was a function of $x_{t-1}$, $\theta$ and $H_{t-2}$. But $H_{t-2}$ was also a function a function of $\theta$. And so on.
