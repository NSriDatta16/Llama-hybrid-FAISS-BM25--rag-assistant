[site]: crossvalidated
[post_id]: 620053
[parent_id]: 
[tags]: 
Bias-Variance tradeoff in prediction versus causal inference

In prediction, accepting a little more bias in exchange for a lot less variance is the very name of the game - we'll chose the model with minimal test MSE without regard for its composition (bias squared versus variance). In causal inference, we rarely - if ever - are willing to make this tradeoff. The emphasis/weight placed in textbooks preoccupied with causal inference (e.g. statistics, econometrics) on theorems such as Gauss-Markov or Cramer-Rao seems to support this point. Take for example this sentence from Peter Kennedy's A Guide to Econometrics - "In practice, the MSE criterion is not usually adopted unless the best unbiased criterion is unable to produce estimates with small variances. The problem of multicollinearity, discussed in chapter 11, is an example of such a situation." In causal inference, is an unbiased estimator with minimal variance the holy grail? Or is that just the starting point in our search? When are we willing to accept a little more bias in exchange for a lot less variance in causal inference? In machine learning a lot of time is spent on choosing amongst models (e.g. Nearest Neighbor Model versus Linear Regression Model) and less time on choosing the estimator given a model; in statistics/econometrics - causal inference - it seems that less time is spent on choosing the statistical model (e.g. Linear Regression Model) and more time is spent on choosing the best estimator (e.g. Least Squares versus Maximum Likelihood) given a statistical model and a causal model that we have in mind. That distinction seems very relevant here and I would love it if the answer to this question would address that directly. If I made some incorrect statements in asking this question, please correct me; there are clearly some gaps in my understanding of how these topics interrelate.
