[site]: stackoverflow
[post_id]: 3945464
[parent_id]: 3944556
[tags]: 
Your algorithm is a sort-based shuffle, as discussed in the Wikipedia article. Generally speaking, the computational complexity of sort-based shuffles is the same as the underlying sort algorithm (e.g. O( n log n ) average, O( n ²) worst case for a quicksort-based shuffle), and while the distribution is not perfectly uniform, it should approach uniform close enough for most practical purposes. Oleg Kiselyov provides the following article / discussion: Provably perfect random shuffling and its pure functional implementations which covers the limitations of sort-based shuffles in more detail, and also offers two adaptations of the Fischer–Yates strategy: a naive O( n ²) one, and a binary-tree-based O( n log n ) one. Sadly the functional programming world doesn't give you access to mutable state. This is not true: while purely functional programming avoids side effects , it supports access to mutable state with first-class effects, without requiring side effects. In this case, you can use Haskell's mutable arrays to implement the mutating Fischer–Yates algorithm as described in this tutorial: Haskell Shuffling (Brett Hall) Addendum The specific foundation of your shuffle sort is actually an infinite-key radix sort : as gasche points out, each partition corresponds to a digit grouping. The main disadvantage of this is the same as any other infinite-key sorting shuffle: there is no termination guarantee. Although the likelihood of termination increases as the comparison proceeds, there is never an upper bound: the worst-case complexity is O(∞).
