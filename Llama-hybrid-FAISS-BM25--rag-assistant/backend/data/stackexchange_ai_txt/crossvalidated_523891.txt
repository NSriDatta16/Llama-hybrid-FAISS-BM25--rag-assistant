[site]: crossvalidated
[post_id]: 523891
[parent_id]: 
[tags]: 
Why dual problem coefficients from svm.SVC contain zeros

My question is about the output in sklearn.svm.SVC function in Python. Apologies for a software context question but I believe a good number of those who have learnt SVM will have come across this function. In the sklearn documentation section 1.4.7.1, it is stated that 'These parameters can be accessed through the attributes dual_coef_ which holds the product $y_i \alpha_i$ . The array returned by dual_coef_ is of shape (n_classes-1, n_sv) . As I understand, the support vectors are those data points whose $\alpha_i > 0$ , thus they will make an impact on the decision boundary which has the form \begin{align*} f(x) = \sum_i \alpha_i K(x_i, x) \end{align*} Those non-support vectors have their $\alpha_i = 0$ . When I fit the following model however the output array in dual_coef_ does contain zeros. What is the explanation for that? Thanks in advance. svm= SVC(C= 10, kernel='rbf', gamma= 'scale', decision_function_shape= 'ovo', max_iter=-1, probability=True) svm_mod= svm.fit(xtr, ytr) print('yi * alpha_i: \n', svm_mod.dual_coef_) print('Number of support vectors of each class:', svm_mod.n_support_) # Output yi * alpha_i: [[ 4.58635603 -0. -0. -0. -0. -4.58635603 -0. -0. -0. -0. -0. -0. -0.39551576 -0. -0. -1.10882587 -0. -0. -0. ] [ 1.50434163 10. 10. 10. 3.00420504 0. 10. 10. 10. 10. -10. -3.00420504 -0. -10. -10. -10. -10. -10. -10. ]] Number of support vectors of each class: [1 9 9]
