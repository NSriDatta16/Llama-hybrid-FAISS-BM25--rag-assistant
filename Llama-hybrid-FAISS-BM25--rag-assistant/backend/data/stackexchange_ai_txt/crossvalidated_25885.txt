[site]: crossvalidated
[post_id]: 25885
[parent_id]: 
[tags]: 
Combining Deterministic and Random Unbiased Estimators

I am trying to compute an expectation $E[f(X;\theta,n)]$ where $\theta$ and $n$ are known parameters. I have an easy-to-compute deterministic function $\tilde{f}(\theta,n)$ that provides an approximation to $E[f(X;\theta,n)]$ that gets better as $n\rightarrow\infty$ (but always underestimates the true value). I can also sample directly from $f(X;\theta,n)$ and compute the Monte Carlo estimate $\hat{f}$, which is unbiased but can have a large variance for some $\theta$ and $n$. I am wondering if there are similar problems that might yield methods for estimating $E[f(X;\theta,n)]$ which combine these two estimates. Since we know $\tilde{f}(\theta,n)\rightarrow E[f(X;\theta,n)]$ as $n$ increases, it seems like it might also be possible to compute the estimators for a range of values of $n$ and use the asymptotic behavior to somehow constrain the fitted values. The only approach I can think of is to use a Bayesian estimate of $E[f(X;\theta,n)]$ by taking the sample mean of the Monte Carlo samples and incorporating a prior having mean $\tilde{f}$. This seems like an OK start, but at the very least I'd like to be able to incorporate the knowledge that $E[f]>\tilde{f}$. It also seems quite likely that a more sophisticated approach exists for a similar problem, but I'm just not sure where to start in searching the literature.
