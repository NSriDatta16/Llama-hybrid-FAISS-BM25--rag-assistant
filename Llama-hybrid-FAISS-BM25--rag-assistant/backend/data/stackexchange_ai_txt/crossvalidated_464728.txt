[site]: crossvalidated
[post_id]: 464728
[parent_id]: 464600
[tags]: 
If you're studying the Markov chain only by looking at simulations of its behavior, it is possible to miss an almost-absorbing class that is visited regularly over the very long run. Suppose the chain mainly alternates between states 1 and 2 at the beginning. It looks as if the chain is reaching a limiting distribution. Then suddenly it visits state 3 and begins to alternate between states 3 and 4, appearing to disrupt the convergence. Over the long run it 'mixes' among all four states. One reason that one must look at the history plot of a Gibbs Sampler is to make sure there are no 'sticking places' that will cause the limiting distribution to be something other than you first suppose. Consider the chain with the following transition matrix, and starting in state 1: P = matrix(c(.299, .7, .001, 0, .6, .4, 0, 0, 0, 0, .5, .5, .001, 0, .5, .499), byrow=T, nrow=4) set.seed(506) m = 3000; x = numeric(m); x[1]=1 for (i in 2:m) { x[i] = sample(1:4, 1, prob=P[x[i-1],]) } par(mfrow=c(2,1)) plot(x,ylim=c(1,4), type="l", ylab="State", main="History") plot(cumsum(x)/(1:m), type="l", ylab="Running Avg", main="Trace") par(mfrow=c(1,1)) Up until about step 1920 it looks as if the chain is alternating between states 1 and 2, and the trace is settling to a running average of about 1.54. But then the process "discovers" the other two states. (Over a sufficiently long run the average state will be about 2.5.) Note: The stationary vector of an ergodic chain (which is also the limiting distribution) is proportional to the left eigenvector of $P,$ with the largest modulus. Thus the exact mean of the limiting distribution is 2.480258: g=eigen(t(P))$vectors[,1] # largest modulus listed first sg = g/sum(g) # distribution must sum to 1 sum((1:4)*sg) # find mean [1] 2.480249
