[site]: datascience
[post_id]: 11508
[parent_id]: 10329
[tags]: 
[...] is it hopeless to get good results in reasonable time without a powerful computer/cluster/GPU? It's not hopeless and you can, without doubt, gain lots of relevant experience with deep learning using the computer spec you mentioned. It will come down to your neural network architecture (number of layers and neurons), size of the dataset (number of inputs), nature of the data (inherent patterns), and implementation. And although you may need to limit yourself with those regards it won't prevent you from acquiring intuition and knowledge you're referring to. You'll easily experience problems of overfitting, influence of regularization, effects of pre-training, impact of different neuron types and architectures to name a few. I'll give you a more concrete example. I've implemented a couple of deep learning algorithms (all CPU-based) in Julia and run them on a MacBook Air (similar to your spec). The code was not terribly optimised as neurons and layers were represented by actual data structures rather than a single giant matrix. So further performance improvements were possible. For a fully-connected network of 56x300x300x300x1 (56 inputs and approx 200k connections) and 250 training examples I was able to get 5k back propagation passes within a day. Often that was enough to overfit the data or perfectly fit the training set (but this will depend on your dataset and other aforementioned factors). If the data has strong patterns and less than 10k examples you often won't need that many iterations. It's not uncommon that few hundreds of pre-training and refinement iterations lead to good results. So yes, your laptop is good enough and you could run meaningful experiments that take several hours. [...] which deep architectures would you recommend that I try to implement with my hardware, such that the following goal is achieved: Acquiring intuition and knowledge about how and when to use techniques that were introduced in the past 10 years and were essential to the uprising of deep nets. I'd suggest to pick smaller datasets with strong patterns. And I'd recommend to look into pre-training techniques such as auto-encoders because they often require fewer iterations to reach better results. Start with back propagation and build from there, try different architectures, neuron types, use regularization, auto-encoders, dropout, ... Also make sure to pick a performant language or library for your experiments.
