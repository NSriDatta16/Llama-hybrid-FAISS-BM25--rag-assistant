[site]: datascience
[post_id]: 23056
[parent_id]: 
[tags]: 
Deep Learning models with top-down transfer

Are there Deep Learning models that utilize some sort of top-down information transfer (not only during training)? For example, if the model is used for speech recognition. The lower layers would listen to the samples and say: "The word 'one', the word 'two', I can't understand what this sample is". And then an upper layer would suggest: "Could it be the word 'three', that is not uncommon after 'one' and 'two'". And then the lower layers would be like: "Now that you said it! Of course it's 'three'" I'm pretty sure that an architecture like this is a requirement for reliable speech recognition, especially in noisy environments.
