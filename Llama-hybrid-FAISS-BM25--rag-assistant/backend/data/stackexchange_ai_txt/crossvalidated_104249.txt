[site]: crossvalidated
[post_id]: 104249
[parent_id]: 
[tags]: 
Why likelihood is not always a density function?

I try to self-learn Bayesian machine learning (mostly by studying Bishop and Kevin Murphy's books). While working with formulas I was puzzled by the quote that "Note that the likelihood function is not necessary a density function". The Bayes formula stated in terms of data $D$ and hypothesis $h$ is: $$ p(h|D) = \frac{p(D|h)p(h)}{p(D)} = \frac{likelihood \times prior}{evidence} \propto likelihood \times prior$$ I can see the previous formula simply as a formula between probabilities of some events. This looks not so useful since it does not incorporates the uncertainty, and I can agree with that. The second interpretation, which I believe it incorporates the uncertainty is to look at the posterior probability as a probability density. In this way we might take a look at "which is the shape of the uncertainty", if I am allowed to state that. Considering the chaining of learning which happens in Bayesian learning, where posteriors might be used as prior for further learning, a normal consequence is to have the prior also as a density. Now my question is what about the likelihood and evidence? If I am free to consider as likelihood any function of $D$ and $h$, than the evidence is simply a constant. The purpose of the evidence is diminished to a constant in order to be used for making the prior a density function. (I think so because a density function has to have its integral equal with 1). So working on this line the evidence is simply a constant which depends only on the integral of the product between likelihood and prior. If we g even further we might think that we can incorporate the evidence value in likelihood so we need no evidence there. Even the name evidence become meaningless. On the other side if we consider likelihood a density itself, I have difficulties with the math. Can you provide me with some insights regarding why that happens (likelihood is not necessary a density)?
