[site]: crossvalidated
[post_id]: 250556
[parent_id]: 
[tags]: 
Grid search with cross validation: should I keep a separate test set?

There is a lot of information on using cross validation and grid search, and there is also confusion about the test set in this situation. I have a labeled data set that I am using to build a predictive model. I will perform a model selection across a family of models (SVC, NB, random forests, boosting etc.) and then do parameter tuning using grid search. However, I could not find a definitive answer about whether I need to keep a separate test set for estimating out-of-sample performance. This is my intended plan: set aside the test set (and not touch it until the very end to avoid data contamination) on the remaining data: perform grid search for all models and estimate their performance using k-fold cross validation select the best model from each family of models and test their out-of-sample performance using the test set report results However, there are several challenges that I am not sure how to deal with: The data set is rather small: around 550 points for 30 variables The data set is noisy and I was told that I could get significantly different results for the same exact model with the same hyper-parameters on different random splits of data and that is a big challenge Given these two challenges, I have three questions : Does my 4-step procedure seem right? How do I deal with the second challenge of inconsistent performance? Due to the small size of data, what should be the size of the test set and how many fold is it better to have in the cross-validation step?
