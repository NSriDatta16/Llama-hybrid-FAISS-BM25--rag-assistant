[site]: crossvalidated
[post_id]: 179054
[parent_id]: 179049
[tags]: 
First, very simply, I don't think your call to solve looks right, this is what I would expect solve(t(X) %*% X + lambda.diag, t(X) %*% y) Your code seems to be explicitly calculating a matrix inverse and then multiplying. This is mathematically correct, but computationally incorrect. It is always better to solve the system of equations. I've gotten in the habit of reading equations like $y = X^{-1}z$ as "solve the system of equations $Xy = z$ for $y$." On a more mathematical note, you should not have to include an intercept term when fitting a ridge regression. It is very important, when applying penalized methods, to standardize your data (as you point out with your comment about scale . It's also important to realize that penalties are not generally applied to the intercept term, as this would cause the model to violate the attractive property that the average predictions equal the average response (on the training data). Together, these facts (centered data, no intercept penalty) imply that the intercept parameter estimate in a ridge regression is known a priori, it is zero. The coefficient vector from ridge regression is the solution to the penalized optimization problem $$ \beta = argmin \left( (y - X\beta)^t (y - X\beta) + \frac{1}{2}\sum_{j > 0} \beta_j^2 \right) $$ Taking a partial with respect to the intercept parameter $$ \frac{\partial L}{\partial \beta_0} = \sum_{i=1}^{n} \left( y - \sum_{j=0}^q \beta_j x_{ij} \right) x_{i0} $$ But $x_{0i}$ are the entries in the model matrix corresponding to the intercept, so $x_{0i} = 1$ always. So we get $$\sum_{i=1} y_i + \sum_{j=0}^q \beta_j \sum_{i=1}^n x_{ij} $$ The first term, with the sum over y, is zero because $y$ is centered (or not, a good check of understanding is to work out what happens if you don't center y). In the second term, each predictor is centered, so the sum over $i$ is zero for every predictor $j$ except the intercept. For the intercept, the second term $i$ sum comes out to $n$ (it's $1 + 1 + 1 + \cdots$). So this whole thing reduces to $$ n \beta_0 $$ Setting this partial equal to zero, $n\beta_0 = 0$, we recover $\beta_0 = 0$, as expected. So, you do not need to bind on an intercept term to your model matrix. Your function should either expect standardized data (and if you plan on making it public, it should check this is so), or standardize the data itself. Once this is done, the intercept is known to be zero. I'll leave it as an exersize to work out what the intercept should be when you translate the coefficients back to the un-normalized scale. I still don't get results equivalent to lm.ridge though, but it might just be a question of translating the formula back into the original scales. However, I can't seem to work out how to do this. I thought it would just entail multiplying by the standard deviation of the response and adding the mean, as usual for standard scores, but either my function is still wrong or rescaling is more complex than I realize. It's a bit more complicated, but not too bad if you are careful. Here's a place where I answered a very similar question: GLMnet - “Unstandardizing” Linear Regression Coefficients You may have to make a very simple change if you are not standardizing $y$.
