[site]: crossvalidated
[post_id]: 210817
[parent_id]: 210707
[tags]: 
There are different possible reasons why you observe (almost) the same performance for several parameter sets. Your data set is small, so testing variance is large, and you randomly observe the same performance with your test set even though the actual performance of the tested models does vary. This is of course not desirable, it means that you have too few cases to afford data-driven hyperparameter optimization. In this case, you need to look for other ways (without using your data) to fix the hyperparameters. Or far more data. Or even a different model where hyperparameters are easily determined. You found a parameter region where the model is rather insensitive to changes in the hyperparameters. I.e. the actual performance of the tested models is equal. In this case: congratulations, you can basically choose any parameter set in this region. I'd go for a central one. In order to get an idea of what is going on, estimate confidence intervals for your observed performance and compare the observed differences as well as the precision that is acceptable for your application to the confidence interval. Should all optimal parameters be used on the testing data set, and then the mean reported? Rather than averaging, comparing the predictions for all cases and you set of "optimal" models can indicate whether you really found a hyperparameter region that yield good and stable models: in that case, the predictions for the same test case of all the models should be equal. This would be a non-standard way of reporting model performance, but it would make sense. I feel that it is perhaps unfair to select the set of parameters that yield the lowest error on the testing data set as this will introduce information that is not available (i.e. I do not know beforehand which set will produce the minimum error on the testing data) Absolutely: the testing data should see the final model and treat it as a black box. No more fine tuning.
