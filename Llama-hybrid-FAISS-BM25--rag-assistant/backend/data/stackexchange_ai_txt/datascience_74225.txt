[site]: datascience
[post_id]: 74225
[parent_id]: 43263
[tags]: 
You definitely have an overfitting problem. I have few observations to look about your model: Reduce the number of LSTM layers drastically. RNNs are different from CNNs, and usually don't benefit from stacking several layers on top of each other (their "depth" is more in the sequentiality of the signal). Usually you never need more than one, two at most. I suggest you to take a look at this very good tutorial on time series forecasting on the official TensorFlow website. Do not use batchnorm after LSTM layers. They are sequential in nature, and I'd leave the output signal as intact as possible. Batchnorm subtracts the mean and scales (usually on the last axis of the tensor) and in that way you'd loose lots of information coming from the sequence of outputs. Do not use simple dropout. LSTM layers have recurrent dropout that was designed specifically for recurrent layers. IMHO, do no use dropout at all with RNNs. Once again, sequential information would be distorted/lost. If you want to add regularization, add dropout to the last Dense layers of the Network instead. In order to fight overfitting you can start simpler: from tensorflow.keras.models import Sequential from tensorflow.keras.layers import GRU, Dense RNN = Sequential([ GRU(n_lstm_units, input_shape), Dense(1) ]) Something like that can do the job better than a heavy network. (NB: I used GRU because they have less parameters (they are faster to train) and sometimes perform equally good.) Then you can try to progressively to build up fancier architectures. For example, add more Dense layers and regularization between them, change parameter initialization, batch size, learning rate, ... you name it. Also, please consider using vanilla Dense networks instead of RNNs. If the number of input time steps is small (5 if I understood correctly) then you probably won't need the complexity of Recurrent architectures, and Dense layers can do the job even better (since they are more fully connected with subsequent layers).
