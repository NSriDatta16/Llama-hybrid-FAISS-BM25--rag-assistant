[site]: crossvalidated
[post_id]: 583930
[parent_id]: 
[tags]: 
Ensemble classifiers trained using different sets of features

Background I have a dataset, each record in this set is represented by 2 different sets of features. Let's say feature set A and feature set B. I have trained classifiers using feature set A and feature set B respectively, and feature A outperformed. Now I would like to ensemble these classifiers to see if it is possible to get better performance. One of the solutions The first strategy I have tried is applying a data fusion strategy. For each prediction of the classifiers trained using the feature set A/B, I recorded the prediction probability. Then I calculated the mean value of the prediction probability for each prediction. After that, I get the prediction results of the data fusion strategy, which outperformed both features set A and B. Question Here's a problem in the solution above. I treated feature set A and B equally when calculating the average values, but what I would like to try is giving the outperformed feature set (feature set A) a higher weight. Here I find an answer from @ Bruno Lubascher ( https://datascience.stackexchange.com/questions/65973/differences-between-feature-weighting-and-feature-selection ). In which he mentioned a stacking-like solution: Example: You have an ensemble model, where all the feature coming into this model are actually predictions from other models. You might weight the predictions of these other models based on their individual performance. Then, your ensembler takes predictions from good performing models with more weight than from those with poorer individual performance. My question is if the weight would be "learned" during ensembling the classifiers. Or the weight was given manually according to the performance of the classifiers trained using different feature sets? Or are there any other ways to learn weight for feature A and feature B in my case?
