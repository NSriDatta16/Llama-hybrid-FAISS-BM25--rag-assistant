[site]: crossvalidated
[post_id]: 250005
[parent_id]: 249355
[tags]: 
After reviewing the equations a few more times. I think the correct loss is the following: $$\mathcal{L} = (11.1 - 4.3)^2$$ My reasoning is that the q-learning update rule for the general case is only updating the q-value for a specific $state,action$ pair. $$Q(s,a) = r + \gamma \max_{a*}Q(s',a*)$$ This equation means that the update happens only for one specific $state,action$ pair and for the neural q-network that means the loss is calculated only for one specific output unit which corresponds to a specific $action$. In the example provided $Q(s,a) = 4.3$ and the $target$ is $r + \gamma \max_{a*}Q(s',a*) = 11.1$.
