[site]: crossvalidated
[post_id]: 160933
[parent_id]: 
[tags]: 
Bayesian Inference and Conditional Probabilities

From Wikipedia , under "Formal description of Bayesian inference:" $\theta$, the parameter of the data point's distribution, i.e., $x \sim p(x|\theta)$ $\alpha$, the hyperparameter of the parameter, i.e., $\theta \sim p(\theta|\alpha)$. I have a really hard time understanding the $p(x|\theta)$ and $p(\theta|\alpha)$ as conditional disitributions, especially the latter. Let me assume these are actually conditional pdfs. Then aren't the conditional pdfs defined in terms of the joint and marginal pdfs? I.e. would we not have the following? $$ p(x|\theta) = \frac{p(x, \theta)}{p(\theta)}, \qquad p(\theta|\alpha) = \frac{p(\theta, \alpha)}{p(\alpha)}. $$ But, there is no mention of either joint pdfs, nor do I see a mention of a joint pdf between the data point and the parameter, $p(x, \theta)$. This part I might be okay with, since we can view both $x$ and $\theta$ as random variables and only specify the conditional pdf $p(x|\theta)$ and not need to assume a joint density...right? Furthermore, as I understand it, the hyperparameters are not random variables, so $p(\theta|\alpha)$, $p(\theta, \alpha)$ and $p(\alpha)$ are not even defined. Perhaps this is just bad notation on Wikipedia's part? Would better notation be $\theta \sim p(\theta;\alpha)$?
