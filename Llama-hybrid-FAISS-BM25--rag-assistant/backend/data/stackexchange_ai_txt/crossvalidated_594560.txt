[site]: crossvalidated
[post_id]: 594560
[parent_id]: 586813
[tags]: 
Positional encoding is typically a deterministic function, so it's always the same on all inputs. You are adding the same vector to all of your inputs. For example, if you were using a naive positional encoding, you would use positional_encoding = [1, 2, 3, ..., n] for all inputs. In Attention Is All You Need of course, they use a more sophisticated function, but it's still deterministic. It can seem strange to add both embeddings together, but the Transformer isn't confused since the variability it sees in embedded inputs isn't caused by a blend of word and positional embeddings, it's only caused by the word embedding. Positional encoding changes every input in the same way.
