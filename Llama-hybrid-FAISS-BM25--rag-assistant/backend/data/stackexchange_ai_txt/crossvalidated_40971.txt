[site]: crossvalidated
[post_id]: 40971
[parent_id]: 40818
[tags]: 
Thinking a little more about the part of the question: Does bigger sample size always increase testing power? If we are only talking about tests generally covered in an introductory statistics course and the conditions for those tests hold (e.g. simple random sample, central limit theorem gives an approximate normal, null hypothesis is false, etc.) then yes, increasing the sample size will increase the power. However, here are some cases where increasing the sample size may not increase the power: If the underlyng distribution is a Cauchy (undefined mean, infinite variance, CLT does not apply), then increasing the sample size may not increase the power (but I don't know what test you would be doing on such data, or even a realistic case that would follow a Cauchy). Excessive sampling causes subjects to loose interest and stop cooperating. I remember a presentation about an election in one of the Caribean island countries where the polling got so out of hand that all the registered voters were being surveyed on average every week and got so fed up that they stopped answering or just lied. The presentation showed that if they had used smaller samples for each survey then the population would not have been as frustrated and they probably would have received better results. Response rates and cost. If you plan a mail out survey and send the survey to 1,000 people but do no other follow up then you might only get 100 responses, but if you use the same money to only send out 200 surveys, but you also send follow-up letters and/or offer incentives then you may receive 150 responses, so the actual amount of data from the smaller planned study of 200 subjects will be more than that for the planned 1,000 subjects. This can also influence data quality, an in person interview of 50 people, or a telephone interview of 100 people may yield better quality data than a mail out survey of 1,000. The concept of power is only for the cases where the null hypothesis is false, so if the null is true then power will not be affected by sample size. When taking multiple measurements per subject then the concept of sample size is more complicated. Which gives more power, 10 measurements on each of 20 subjects (for a total of 200 measured values) or 2 measurement on each of 50 subjects (for a total of 100 measured values), often the 2nd will give more power even though the total number of measurements is smaller. If the parameter of interest changes over time (think of election polling) and getting the bigger sample will require more time in which things could change, then that could affect the power. Think of comparing a sample of 100 taken on a single day vs. a sample of 1,000 taken over a 2 week period (and what if there is a publicised debate, scandle, etc. during those 2 weeks). If you have a test whose type I error is not exactly alpha, and it depends on sample size, then increasing the sample size can actually decrease the power. Consider a binomial test with null that the probability is $0.5$, the alternative to test is that it is greater, and we want to test with $\alpha=0.05$. With a sample size of $n=5$ we can only reject if we see 5 successes (Type I error rate 0.03125), with a sample of $n=10$ we will reject if we see 9 or 10 sucesses (Type I error rate 0.01074, it would be 0.05469 if we rejected at 8 sucesses). If the true probabilty is $0.6$ then the probability of rejecting (power) with $n=5$ is $0.07776$ and with $n=10$ it is $0.04646$, so doubling the sample size decreased the power, but also decreased the type I error rate, so not really a fair comparison. Increasing the sample size to where the power is meaningful ($>80\%$) will make it much harder to find examples like this (though there are probably some where increasing n by 1 decreases the power slightly). If you are running the wrong test where assumptions are violated (e.g. using a test that assumes equal variances when they are quite different) then your power might not increase. If you first run a normality test, then run a different second test based on the results then larger sample sizes are more likely to reject the normal test (even when the difference does not matter) and if the test you run as a result is less powerful than if normality was not rejected then increasing the sample size could reduce the power (which is one argument against pre-testing the data for normality). There are probably other cases which like these are beyond the scope of what the wikipedia article was trying to cover.
