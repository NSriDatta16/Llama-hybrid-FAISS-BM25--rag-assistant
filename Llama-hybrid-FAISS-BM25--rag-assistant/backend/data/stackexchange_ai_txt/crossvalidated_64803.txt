[site]: crossvalidated
[post_id]: 64803
[parent_id]: 64788
[tags]: 
I would suggest that you use Frank Harrell's excellent rms package . It contains many useful functions to validate and calibrate your model. As far as I know, you cannot assess predictive performance solely based on the coefficients. Further, I would suggest that you use the bootstrap to validate the model. The AUC or concordance-index (c-index) is a useful measure of predictive performance. A c-index of $0.8$ is quite high but as in many predictive models, the fit of your model is likely overoptimistic (overfitting). This overoptimism can be assessed using bootstrap. But let me give an example: #----------------------------------------------------------------------------- # Load packages #----------------------------------------------------------------------------- library(rms) #----------------------------------------------------------------------------- # Load data #----------------------------------------------------------------------------- mydata $rank rank) #----------------------------------------------------------------------------- # Fit logistic regression model #----------------------------------------------------------------------------- mylogit chi2) |Z|) Intercept -3.9900 1.1400 -3.50 0.0005 gre 0.0023 0.0011 2.07 0.0385 gpa 0.8040 0.3318 2.42 0.0154 rank=2 -0.6754 0.3165 -2.13 0.0328 rank=3 -1.3402 0.3453 -3.88 0.0001 rank=4 -1.5515 0.4178 -3.71 0.0002 On the bottom you see the usual regression coefficients with corresponding $p$ -values. On the top right, you see several discrimination indices. The C denotes the c-index (AUC), and a c-index of $0.5$ denotes random splitting whereas a c-index of $1$ denotes perfect prediction. Dxy is Somers' $D_{xy}$ rank correlation between the predicted probabilities and the observed responses. $D_{xy}$ has simple relationship with the c-index: $D_{xy}=2(c-0.5)$ . A $D_{xy}$ of $0$ occurs when the model's predictions are random and when $D_{xy}=1$ , the model is perfectly discriminating. In this case, the c-index is $0.693$ which is slightly better than chance but a c-index of $>0.8$ is good enough for predicting the outcomes of individuals. As said above, the model is likely overoptimistic. We now use bootstrap to quantify the optimism: #----------------------------------------------------------------------------- # Validate model using bootstrap #----------------------------------------------------------------------------- my.valid Let's concentrate on the $D_{xy}$ which is at the top. The first column denotes the original index, which was $0.3857$ . The column called optimism denotes the amount of estimated overestimation by the model. The column index.corrected is the original estimate minus the optimism. In this case, the bias-corrected $D_{xy}$ is a bit smaller than the original. The bias-corrected c-index (AUC) is $c=\frac{1+ D_{xy}}{2}=0.6749$ . We can also calculate a calibration curve using resampling: #----------------------------------------------------------------------------- # Calibration curve using bootstrap #----------------------------------------------------------------------------- my.calib The plot provides some evidence that our models is overfitting: the model underestimates low probabilities and overestimates high probabilities. There is also a systematic overestimation around $0.3$ . Predictive model building is a big topic and I suggest reading Frank Harrell's course notes .
