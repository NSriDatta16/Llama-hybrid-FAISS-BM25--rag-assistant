[site]: crossvalidated
[post_id]: 123026
[parent_id]: 
[tags]: 
Rademacher complexity of logistic regression

Consider logistic regression. We have the logistic loss function, $\phi: R\rightarrow [0,1], \phi(u)=\log(1+\exp(-u))$, which is Lipschitz, and we have the linear function class $F=\{f_w:R^d \rightarrow R, f_w(x)=w^Tx, \forall x\in R^d\}$. Given a training set, $S=\{(x_1,y_1),...,(x_N,y_N)\}$, we do empirical risk minimization to estimate the parameter $\hat{w}$: $\hat{f}_{\hat{w}} = argmin_{f_w} \sum_{n=1}^N\phi(y_nf_w(x_n))$. I am interested in a bound on the generalisation error of of $\hat{f}_{\hat{w}}$. Can this be obtained using Rademacher complexities without introducing restrictions of boundedness on the parameter space and on the input space? Alternatively, I would also be interested in a VC bound (since no such boundedness assumptions would be needed), but as far as I understand, for that I would need to use hard-thresholded values of the values of $f$ and work with the 0/1 loss (which is different from what the ERM algorithm is optimising). The Rademacher approach that I was able to find unfortunately makes the following modification to the problem: It assumes that $\forall f\in F, \|f\|_\infty My problem is that this modification doesn't suit the problem that I am working on, and I wonder if there is any way to avoid it. I can see that it would be very convenient for the Rademacher complexity based bounding technique, since then by the Ledoux-Talagrand contraction inequality we get $\hat{R}_N(\phi\circ F_{trunc})\le L_{\phi}\hat{R}_N(F_{trunc})$ (where $\hat{R}_N(.)$ denotes the empirical Rademacher complexity, $N$ is the number of training points, and $L_{\phi}$ is the Lipschitz constant of $\phi$), and $\hat{R}_N(F_{trunc})$ can be estimated neatly as $\hat{R}_N(F_{trunc})\le B/\sqrt{N}$. However, I am interested in $\hat{R}_N(\phi\circ F)$.
