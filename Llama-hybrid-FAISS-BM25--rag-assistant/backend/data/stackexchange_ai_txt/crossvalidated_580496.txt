[site]: crossvalidated
[post_id]: 580496
[parent_id]: 
[tags]: 
Why is the sum of individual Spearman's rho squared less than 1 as opposed to Pearson's r in a synthetic example?

A relatively low number of iid random vectors of a relatively high dimension (10,000) are added up together element wise: $$\sum_{i=1}^{n}X_i=Y$$ where $dim(X_i)=dim(X_j)=dim(Y),\forall i,j$ and $dim(X_i)\gg n$ My modeling suggests that $$E \left[\sum_{i=1}^{n}\rho_{X_i,Y}^2\right] for the described case. More generally $$E \left[\sum_{i=1}^{n}\rho_{X_i,Y}^2\right] and $$E \left[\sum_{i=1}^{n}r_{X_i,Y}^2\right]\ge1$$ where $\rho$ is Spearman's rank correlation coefficient, $r$ is Pearson correlation coefficient. Computational assumptions For computing $\rho_{X,Y}^2$ and $r_{X,Y}^2$ in R I use cor(X, Y, method='spearman')^2 and cor(X, Y, method='pearson')^2 respectively. I also assume $X_i \sim \mathcal{N}(0, 1)$ but the following works for some other common distributions as well. Fixed number of variables, fixed dimension, varied correlation function Given just 3 of 10,000-vectors the distributions of Spearman's $$\rho_{X_1,Y}^2+\rho_{X_2,Y}^2+\rho_{X_3,Y}^2$$ and Pearson's $$r_{X_1,Y}^2+r_{X_2,Y}^2+r_{X_3,Y}^2$$ are as follows: Fixed number of variables, varied dimension Here's how the distributions of $\rho_{X_1,Y}^2+\rho_{X_2,Y}^2+\rho_{X_3,Y}^2$ and $r_{X_1,Y}^2+r_{X_2,Y}^2+r_{X_3,Y}^2$ look like next to each other for various dimensions. It appears as with the increase of the number of dimensions both distributions travel to the left but Pearson's stops at 1.0 while Spearman's continues to move below 1.0. Fixed dimension, varied number of variables When the number of variables increases, the Spearman's distribution seems to drift even further below 1.0 in the beginning and then comes back and exceeds 1.0 whereas Pearson's doesn't go below 1.0 at all and bounces back at the high number of variables just as Spearman's does: Varied dimension, varied number of variables A 3D plot to corroborate the above. Questions Is my modeling correct? Why is the Pearson sum of squares always greater than Spearman's on average? Why does the Pearson sum of squares never get below 1.0 on average whereas Spearman's does? Here's my code: library(ggplot2) library(dplyr) library(purrr) library(plotly) r_squared_sum $dimension method $dimension method $num_vars = as.character(num_vars_range[i]) data_frames[[i]]$ method $num_vars = as.character(num_vars_range[i]) data_frames[[i]]$ method % layout( scene=list( xaxis=list(title="vectors"), yaxis=list(title="dimensions"), zaxis=list(title="mean sum r_sq") ) ) fig % add_surface( z=data_spearman, name='spearman', colorscale=list(c(0, 1), c("#00BFC4","#00BFC4")), hovertemplate=paste(' vectors: %{x} ', 'dimensions: %{y} ', 'mean sum r_sq: %{z}') ) fig % add_surface( z=data_pearson, name='pearson', colorscale=list(c(0, 1), c("#F8776D","#F8776D")), hovertemplate=paste(' vectors: %{x} ', 'dimensions: %{y} ', 'mean sum r_sq: %{z}') ) fig % add_surface( z=matrix(data=1, nrow=nrow, ncol=ncol), name='level', colorscale=list(c(0, 1), c("#696969","#696969")), hovertemplate=paste(' vectors: %{x} ', 'dimensions: %{y} ', 'mean sum r_sq: %{z}') ) fig I've implemented similar modeling in Python (my main language) with the same results.
