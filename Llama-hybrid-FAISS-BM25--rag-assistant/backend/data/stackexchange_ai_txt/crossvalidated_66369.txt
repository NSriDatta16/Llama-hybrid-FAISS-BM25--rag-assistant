[site]: crossvalidated
[post_id]: 66369
[parent_id]: 
[tags]: 
Definition of autocorrelation time (for effective sample size)

I've found two definitions in the literature for the autocorrelation time of a weakly stationary time series: $$ \tau_a = 1+2\sum_{k=1}^\infty \rho_k \quad \text{versus} \quad \tau_b = 1+2\sum_{k=1}^\infty \left|\rho_k\right| $$ where $\rho_k = \frac{\text{Cov}[X_t,X_{t+h}]}{\text{Var}[X_t]}$ is the autocorrelation at lag $k$. One application of the autocorrelation time is to find the "effective sample size": if you have $n$ observations of a time series, and you know its autocorrelation time $\tau$, then you can pretend that you have $$ n_\text{eff} = \frac{n}{\tau} $$ independent samples instead of $n$ correlated ones for the purposes of finding the mean. Estimating $\tau$ from data is non-trivial, but there are a few ways of doing it (see Thompson 2010 ). The definition without absolute values, $\tau_a$, seems more common in the literature; but it admits the possibility of $\tau_a require(coda) ts.uncorr The "effectiveSize" function in "coda" uses a definition of the autocorrelation time equivalent to $\tau_a$, above. There are some other R packages out there that compute effective sample size or autocorrelation time, and all the ones I've tried give results consistent with this: that an AR(1) process with a negative AR coefficient has more effective samples than the correlated time series. This seems strange. Obviously, this can never happen in the $\tau_b$ definition of autocorrelation time. What is the correct definition of autocorrelation time? Is there something wrong with my understanding of effective sample sizes? The $n_\text{eff} > n$ result shown above seems like it must be wrong... what's going on?
