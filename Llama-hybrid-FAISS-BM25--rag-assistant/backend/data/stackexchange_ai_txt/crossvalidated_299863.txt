[site]: crossvalidated
[post_id]: 299863
[parent_id]: 299840
[tags]: 
Assuming you keep the size of the network fixed across your three proposed architectures (with #2 having k-times as many parameters overall), I expect #2 to give the most accurate results since all parameters in the network are being used to make a single prediction, however this comes at the cost of requiring a k-times larger memory footprint and k-times more training time, which is unlikely to be worth the marginal increase in accuracy. #3 is the most elegant, and the most likely to produce smooth, aesthetically-pleasing results, but as Nate Diamond points out below, this approach will compound prediction errors, eventually leading to unrealistic predictions for large values of k. If you make the network large enough, and use an appropriate loss function (see below), then #1 is likely to be your best bet. Your concern about the network being difficult to train due to the "large number of inputs and outputs" is largely unwarranted, as new techniques used in training such as ReLU's , batch-norm , and ADAM eliminate many of the problems previously encountered when training very large networks. As for your concern about the high-variance errors in the large-k predictions swamping the (more consistent) error signals coming from the small-k predictions, this can be mitigated by using a loss function that accounts for component-wise variance. For example, instead of the standard RMSE loss: $$\sqrt{\frac{\sum_{i=1}^m\sum_{j=1}^k (\hat{y}^{(i)}_j - y^{(i)}_j)^2}{m}}$$ you could use a variant of RMSE which weights the error in each component inversely proportional to the variance of its errors across the previous mini-batch: $$\sqrt{\frac{\sum_{i=1}^m\sum_{j=1}^k \frac{1}{\sigma^2_j}(\hat{y}^{(i)}_j - y^{(i)}_j)^2}{m}}$$ Where $$m = \text{size of the minibatch}$$ $$\sigma^2_j = \text{variance of the errors in the } j^{th} \text{ component over the previous minibatch}$$ $$\bullet_j^{(i)} = \text{value of the } j^{th} \text{ component of the } i^{th} \text{ sample}$$ There is also another option not on your list which you may want to consider, namely using a recurrent neural network architecture such as seq-2-seq which allows for variable-length inputs and outputs: https://github.com/guillaume-chevalier/seq2seq-signal-prediction
