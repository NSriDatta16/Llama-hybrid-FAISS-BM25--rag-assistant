[site]: stackoverflow
[post_id]: 2353003
[parent_id]: 2352516
[tags]: 
In my current project I'm using unittest , minimock , nose . In the past I've made heavy use of doctests, but in a large projects some tests can get kinda unwieldy, so I tend to reserve usage of doctests for simpler functions. If you are using setuptools or distribute (you should be switching to distribute), you can set up nose as the default test collector so that you can run your tests with "python setup.py test" setup(name='foo', ... test_suite='nose.collector', ... Now running "python setup.py test" will invoke nose, which will crawl your project for things that look like tests and run them, accumulating the results. If you also have doctests in your project, you can run nosetests with the --with-doctest option to enable the doctest plugin. nose also has integration with coverage nosetests --with-coverage. You can also use the --cover-html --cover-html-dir options to generate an HTML coverage report for each module, with each line of code that is not under test highlighted. I wouldn't get too obsessed with getting coverage to report 100% test coverage for all modules. Some code is better left for integration tests, which I'll cover at the end. I have become a huge fan of minimock, as it makes testing code with a lot of external dependencies really easy. While it works really well when paired with doctest, it can be used with any testing framework using the unittest.TraceTracker class. I would encourage you to avoid using it to test all of your code though, since you should still try to write your code so that each translation unit can be tested in isolation without mocking. Sometimes that's not possible though. Here is an (untested) example of such a test using minimock and unittest: # tests/test_foo.py import minimock import unittest import foo class FooTest(unittest2.TestCase): def setUp(self): # Track all calls into our mock objects. If we don't use a TraceTracker # then all output will go to stdout, but we want to capture it. self.tracker = minimock.TraceTracker() def tearDown(self): # Restore all objects in global module state that minimock had # replaced. minimock.restore() def test_bar(self): # foo.bar invokes urllib2.urlopen, and then calls read() on the # resultin file object, so we'll use minimock to create a mocked # urllib2. urlopen_result = minimock.Mock('urlobject', tracker=self.tracker) urlopen_result.read = minimock.Mock( 'urlobj.read', tracker=self.tracker, returns='OMG') foo.urllib2.urlopen = minimock.Mock( 'urllib2.urlopen', tracker=self.tracker, returns=urlopen_result) # Now when we call foo.bar(URL) and it invokes # *urllib2.urlopen(URL).read()*, it will not actually send a request # to URL, but will instead give us back the dummy response body 'OMG', # which it then returns. self.assertEquals(foo.bar('http://example.com/foo'), 'OMG') # Now we can get trace info from minimock to verify that our mocked # urllib2 was used as intended. self.tracker has traced our calls to # *urllib2.urlopen()* minimock.assert_same_trace(self.tracker, """\ Called urllib2.urlopen('http://example.com/foo) Called urlobj.read() Called urlobj.close()""") Unit tests shouldn't be the only kinds of tests you write though. They are certainly useful and IMO extremely important if you plan on maintaining this code for any extended period of time. They make refactoring easier and help catch regressions, but they don't really test the interaction between various components and how they interact (if you do it right). When I start getting to the point where I have a mostly finished product with decent test coverage that I intend to release, I like to write at least one integration test that runs the complete program in an isolated environment. I've had a lot of success with this on my current project. I had about 80% unit test coverage, and the rest of the code was stuff like argument parsing, command dispatch and top level application state, which is difficult to cover in unit tests. This program has a lot of external dependencies, hitting about a dozen different web services and interacting with about 6,000 machines in production, so running this in isolation proved kinda difficult. I ended up writing an integration test which spawns a WSGI server written with eventlet and webob that simulates all of the services my program interacts with in production. Then the integration test monkey patches our web service client library to intercept all HTTP requests and send them to the WSGI application. After doing that, it loads a state file that contains a serialized snapshot of the state of the cluster, and invokes the application by calling it's main() function. Now all of the external services my program interacts with are simulated, so that I can run my program as it would be run in production in a repeatable manner.
