[site]: crossvalidated
[post_id]: 504197
[parent_id]: 503823
[tags]: 
Independence is something that's hard to formulate mathematically. For example, un-correlatedness is not sufficient for independence (that's why PCA does not perform ICA). Spatial ICA takes the following formulation: $$X = A S$$ $X_{\tau\times\upsilon}$ is you data matrix of observed states. $A_{\tau\times\phi}$ is the mixing matrix, which translates the sources into the observed states. $S_{\phi\times\upsilon}$ is the source matrix of independent components. $\upsilon$ is the number of voxels, $\tau$ the number of timesteps and $\phi$ the number of components. $A$ and $S$ are jointly optimized to reconstruct $X$ while maximizing the independence between the rows of $S$ . One way of defining independence is through non-Gaussianity. This comes from the central limit theorem: the aggregation (mixing) of independent signals is more Gaussian than any of its constituents (unless one of them is Gaussian). See this question What is mean by the non-gaussianity in the independent component analysis(ICA)? . Two possible measures of non-Gaussinity are kurtosis and negentropy. Standardized Kurtosis is simply defined as: $$\kappa = \frac{\mathbb E [(s - \bar s)^4]}{\mathbb E^2 [(s - \bar s)^2]} - 3$$ . We subtract $3$ because Gaussian variables have kurtosis equal to three. We can maximize $\kappa$ through gradient descent, for example. Negentropy is justified because, among continuous distributions with the same variance, the Gaussian distribution has maximal entropy. We thus try to maximize negative-entropy, which should ensure non-Gaussianity. Estimating negentropy of a signal is not easy, but approximations were proposed, such as: $$J(x)=\frac{1}{12}(\mathbb E(x^3))^2 - \frac{1}{48}(\kappa(x))^2$$ Which, again, is a differentiable function.
