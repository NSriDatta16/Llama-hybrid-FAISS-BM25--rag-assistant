[site]: crossvalidated
[post_id]: 612334
[parent_id]: 
[tags]: 
How are the embedding split between multihead in transformers

I red the paper about transformers, and fully understood every single piece of that, so now I'm implementing it from scratch in tensroflow, without using any shipped layer from the library. The only missing part is how they intend to take a single tensor (batch size, time-steps, embeddings) and give it to the multihead module. For what I can tell, it seems that they take the embeddings of size $d_{model}$ , then split the into $n$ heads, so each head get a piece of the embedding of size $d_{model}/n$ , however I don't quite see what's the intuition why this should work Am I missing something? are they just duplicating the input for each head instead?
