[site]: crossvalidated
[post_id]: 176407
[parent_id]: 176404
[tags]: 
First it is worth noting that while we often model a certain data type as being normally distributed, this does not necessarily mean that we believe it exactly follows a normal distribution. Take your height example. Many researchers (myself included) believe that modeling height by a normal distribution does a fairly good job of describing the distribution of heights. But clearly it's not exactly normal: you can't have negative height, but all normals with positive variance have a positive probability of being negative. Despite this, the distribution of heights is fairly normal. So why is it that many data types will follow an approximately normal distribution? To me, this can be motivated by central limit theorems (in an undergraduate course, you probably heard of *the* central limit theorem, but there's more than one...). Under a variety of conditions, averages approach a normal distribution. Why does this explain why various phenomena are approximately normal? Well, suppose the outcome you are measuring is actually the combination of a whole lot of different small effects. In the case of height, there are a lot of genes that each contribute to your height, along with your diet as a youth, etc. So you could think of the final measure of height as a combination of a large number of factors. Even as an average, if you will. As long as a small number of these factors do not completely dictate height (or similarly, these factors aren't too correlated), then there's a central limit theorem that will suggest that this measurement should be approximately normal. Keep in mind, this is merely heuristic reasoning and I wouldn't use such logic to decide that a given outcome must be approximately normal. But in my own musings, it's the been my personal explanation why an approximately normal distribution might naturally occur.
