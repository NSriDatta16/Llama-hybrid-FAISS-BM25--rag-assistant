[site]: stackoverflow
[post_id]: 1375505
[parent_id]: 1371822
[tags]: 
Expanding on @Mr Fooz's answer They look to be pretty similar based on the official docs of cvpartition and crossvalind, but crossvalind looks slightly more flexible (it allows for leave M out for arbitrary M, whereas cvpartition only allows for leave 1 out). ... isn't it true that you can always simulate a leave-M-out using kfold cross validation with an appropriate k value (split data into k fold, test on one, train on all others, and do this for all folds and take average) since leave-one-out is a special case of kfold where k=number of observations?
