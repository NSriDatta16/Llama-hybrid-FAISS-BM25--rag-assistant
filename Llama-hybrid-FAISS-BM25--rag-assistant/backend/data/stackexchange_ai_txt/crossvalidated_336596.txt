[site]: crossvalidated
[post_id]: 336596
[parent_id]: 295161
[tags]: 
The problem as you describe it is close to the simplest setup you can get for a RL problem (stationary distributions everywhere, sufficiently small state and action spaces for tabular RL to be feasible, etc. The Q-learning update rule as you described it also looks correct. The only somewhat unusual thing that comes to mind from your description is this bit: Each episode has 9 iterations i.e. the agent can take 9 actions and make 9 transitions before a new episode begins. This means that "time" is a relevant property of your state-space. Suppose that your state-space is a grid, and your agent has 4 actions to move in the 4 Up/Down/Left/Right. I realise this is different from your description, but this is easier to understand / "visualize" in your head. Suppose that there is a small reward 1 step to the left, and a very big reward 2 steps to the right. In exactly this same "state", the optimal action depends on how much time you have left / how many steps you have already taken; if the episode terminates after one more action, your agent won't have time to pick up the big reward 2 steps to the right anymore, so the optimal action will be to move to the left. If there is time left more two or more actions, the optimal action will probably be to move to the right and pick up the big reward instead. So, this implies that "time spent" or "time left" should be included in your state-space representation. The most obvious way to do this is to simply multiply the size of your state space by 9 . For every state you have right now, you'll get: A copy for the case where there are 9 actions remaining to take A copy for the case where there are 8 actions remaining to take ... A copy for the case where there is 1 action remaining to take I don't think you need one for the case with 0 remaining actions, because you don't need Q-values for those states. This will increase the size of your state-action space (and therefore the size of your table of Q-values) from 10 x 11 = 110 to 9 x 10 x 11 = 990 . That's obviously a bit of an increase, but... probably should still be fine on modern hardware? You'll have to try and see how it works for you, it also depends on how much computation time you have obviously. Is there an open AI gym in R? Are there Q-learning packages for problems of this structure? These questions I'm not sure about, I've never done any Reinforcement Learning in R.
