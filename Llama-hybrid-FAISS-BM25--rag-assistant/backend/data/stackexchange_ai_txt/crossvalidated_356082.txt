[site]: crossvalidated
[post_id]: 356082
[parent_id]: 339054
[tags]: 
Initializations are a topic where only two, rather unhelpful, facts are known for certain: Neural networks can be very sensitive to the initialization strategy that you use. There is no universal "best choice" that is consistently good across different problem types, network architectures, activation functions, and data sets. There's a lot of experimentation involved. This is why there are often a large number of different initialization strategies implemented in any modern neural network software. Each of these also has a uniform variant. Glorot Normal (aka Xavier initialization) "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor." - Keras documentation He Normal initialization "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor." - Keras documentation LeCun Normal initializer. "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor." - Keras documentation
