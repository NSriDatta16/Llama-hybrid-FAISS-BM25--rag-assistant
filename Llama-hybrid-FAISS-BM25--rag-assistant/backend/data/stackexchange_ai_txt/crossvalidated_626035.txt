[site]: crossvalidated
[post_id]: 626035
[parent_id]: 
[tags]: 
Why use sliding window input features in sequence modeling?

I was reading through the DNABERT paper and found that their input features were k-mers. This is equivalent to using rolling/sliding window features in the other common family of sequential problem, time series. I have seen this be done, but is there any theory explaining why this can be beneficial? Why would I give a transformer-like model sequences of the form {a_n,..,a_{n+k}} rather than training it on the sequence {a_1,...,a_{m}} with positional encoding?
