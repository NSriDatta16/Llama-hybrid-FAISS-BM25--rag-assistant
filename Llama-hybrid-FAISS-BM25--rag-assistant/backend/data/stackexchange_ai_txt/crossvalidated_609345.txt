[site]: crossvalidated
[post_id]: 609345
[parent_id]: 
[tags]: 
Intuitive difference between NN and attention for text prediction

If your task is to predict $t_{n+1}$ given tokens $(t_1,...,t_n)$ , you could do two things: Straight NN - feed $t=(t_1,...,t_n)$ into a neural network as an n-dimensional input and train it on predicting $t_{n+1}$ (with all the embedding / layernorm / skip connection stuff that transformer models have) Attention - take $t_n$ plus information from $t_1,...,t_{n-1}$ relevant to $t_n$ computed in the attention layer (plus positional encoding) and feed that into a neural network to predict $t_{n+1}$ In one case you're putting everything into the NN, equally weighted. In another, you're taking certain information from each token weighted by relevance to $t_n$ . My question is: why can't the straight NN learn this? What was wrong with the straight NN model so that we needed into introduce attention? If it was helpful to upweight certain things by a relevance metric, why wouldn't the NN learn that? My only thought so far is long-range dependency: that a NN might forget information far back in the sequence (as in general it will be less useful). For example, 'he walked into the kitchen, took the chocolate from the drawer, opened wide and put it into' â€“ to predict the next word you really need to look all the way back to the start of the sentence.
