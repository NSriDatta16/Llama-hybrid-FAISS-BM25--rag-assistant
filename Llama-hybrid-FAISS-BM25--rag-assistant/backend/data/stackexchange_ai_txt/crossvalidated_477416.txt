[site]: crossvalidated
[post_id]: 477416
[parent_id]: 
[tags]: 
Bayes Theorem and Neural networks

For example, I have MNIST dataset and a trained neural network: input image and the output is a probability distribution over 10 classes. I show image and prediction is: 5% for each class and 55% for the number 2. I choose another image from the same class as the first one and get the same prediction: 5% for each class and 55% for number 2. Can you please suggest how to update beliefs about the unknown class due to new information? Should the confidence about the number 2 increase (from 55%) after the second observation? Can we use Bayes Theorem for this problem? Example: Select 2 images of the same class from the dataset. For example 2 images of number 2 NN(image_1) -> probability distribution {5%,55%,5%,5%...,5%} over {1,2,3,...9} NN(image_2) -> probability distribution {5%,55%,5%,5%...,5%} over {1,2,3,...9} What is the final distribution over the 10 classes? In other words: There is a hidden class from a set {1,2...9}. You sequentially get 2 observations (images) of this class and sequentially update your beliefs about what is in a black box.
