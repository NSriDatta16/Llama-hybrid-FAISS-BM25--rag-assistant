[site]: crossvalidated
[post_id]: 86431
[parent_id]: 86429
[tags]: 
The gamma and the lognormal are both right skew, constant-coefficient-of-variation distributions on $(0,\infty)$, and they're often the basis of "competing" models for particular kinds of phenomena. There are various ways to define the heaviness of a tail, but in this case I think all the usual ones show that the lognormal is heavier. (What the first person might have been talking about is what goes on not in the far tail, but a little to the right of the mode (say, around the 75th percentile on the first plot below, which for the lognormal is just below 5 and the gamma just above 5.) However, let's just explore the question in a very simple way to begin. Below are gamma and lognormal densities with mean 4 and variance 4 (top plot - gamma is dark green, lognormal is blue), and then the log of the density (bottom), so you can compare the trends in the tails: It's hard to see much detail in the top plot, because all the action is to the right of 10. But it's quite clear in the second plot, where the gamma is heading down much more rapidly than the lognormal. Another way to explore the relationship is to look at the density of the logs, as in the answer here ; we see that the density of the logs for the lognormal is symmetric (it's normal!), and that for the gamma is left-skew, with a light tail on the right. We can do it algebraically, where we can look at the ratio of densities as $x\rightarrow\infty$ (or the log of the ratio). Let $g$ be a gamma density and $f$ lognormal: $$\log(g(x)/f(x)) = \log(g(x)) - \log(f(x))$$ $$=\log\left(\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta}\right)-\log\left(\frac{1}{\sqrt{2\pi}\sigma x}e^{-\frac{(\log(x)-\mu)^2}{2\sigma^2}}\right)$$ $$=-k_1-(\alpha-1)\log(x)-x/\beta - (-k_2-\log(x)-\frac{(\log(x)-\mu)^2}{2\sigma^2})$$ $$=\left[c-(\alpha-2)\log(x)+\frac{(\log(x)-\mu)^2}{2\sigma^2}\right]-x/\beta $$ The term in the [ ] is a quadratic in $\log(x)$, while the remaining term is decreasing linearly in $x$. No matter what, that $-x/\beta$ will eventually go down faster than the quadratic increases irrespective of what the parameter values are . In the limit as $x\rightarrow\infty$, the log of the ratio of densities is decreasing toward $-\infty$, which means the gamma pdf is eventually much smaller than the lognormal pdf, and it keeps decreasing, relatively. If you take the ratio the other way (with lognormal on top), it eventually must increase beyond any bound. That is, any given lognormal is eventually heavier tailed than any gamma. Other definitions of heaviness: Some people are interested in skewness or kurtosis to measure the heaviness of the right tail. At a given coefficient of variation, the lognormal is both more skew and has higher kurtosis than the gamma .** For example, with skewness , the gamma has a skewness of 2CV while the lognormal is 3CV + CV$^3$. There are some technical definitions of various measures of how heavy the tails are here . You might like to try some of those with these two distributions. The lognormal is an interesting special case in the first definition - all its moments exist, but its MGF doesn't converge above 0, while the MGF for the Gamma does converge in a neighborhood around zero. -- ** As Nick Cox mentions below, the usual transformation to approximate normality for the gamma, the Wilson-Hilferty transformation, is weaker than the log - it's a cube root transformation. At small values of the shape parameter, the fourth root has been mentioned instead see the discussion in this answer , but in either case it's a weaker transformation to achieve near-normality. The comparison of skewness (or kurtosis) doesn't suggest any necessary relationship in the extreme tail - it instead tells us something about average behavior; but it may for that reason work better if the original point was not being made about the extreme tail. Resources : It's easy to use programs like R or Minitab or Matlab or Excel or whatever you like to draw densities and log-densities and logs of ratios of densities ... and so on, to see how things go in particular cases. That's what I'd suggest to start with.
