[site]: datascience
[post_id]: 93655
[parent_id]: 93638
[tags]: 
First, we should clarify that positional encodings are not necessarily applied to words but tokens . Nowadays, using subword tokens is the norm, and infrequent words can be divided into many subword tokens. Regarding the role of positional encoding: according to research, positional encodings have different roles depending on where they are used: Transformer decoders for autoregressive language modeling: positional embeddings learn about absolute positions (see What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding ). Masked language models (i.e. BERT): the positional encodings only help differentiating between tokens, and the order does not actually matter much (see Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little ). So, to answer the question : No, positional encodings do not estimate the relative positions of words in the training corpus texts.
