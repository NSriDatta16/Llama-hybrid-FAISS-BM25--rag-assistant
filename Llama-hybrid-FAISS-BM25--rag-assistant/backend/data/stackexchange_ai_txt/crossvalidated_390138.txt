[site]: crossvalidated
[post_id]: 390138
[parent_id]: 
[tags]: 
Is Cohen Kappa (Weighted or not) appropriate for medical research? | Agreement between measures

I'm working on a experiment when we test the effectiveness of a new diagnostic compared to an existing measure. Our results will be the format |Patient | New test | Old test | |-- -|----------------|----------------| | 1 | Predicted | Not predicted | | 2 | Predicted | Predicted | | 3 | Not predicted | Not predicted | I'm interested in looking deeper then just pure accuracy/f1-score etc. To given a Î± and power level, the statically significant that within this data the new test is close, or as good as the old test. Cohen Kappa seems reasonable, but i'm wondering what other options I would have? Some additional points; For the purpose of the study, we are comparing to the old test (gold standard). I'm aware a better evaluation would be comparing our method to actually diagnosis of the illness. The current literature hasn't looked at a test like this before, hence I can't just improve on the existing work. (This is a compassion of a machine learning prediction verse a current test) I have read research regarding measuring agreement between measurements , but we're looking at categorical data here not continuous. TLDR; Is Cohen Kappa an suitable test for measuring agreement between measurements for categorical data.
