[site]: crossvalidated
[post_id]: 446865
[parent_id]: 
[tags]: 
Why Does My Model Perform Better Without Dimensionality Reduction (features > samples)

I created a classifier (a linear SVM in scikit-learn) to classify tweets about the fat acceptance movement (yeah that's a thing) as supporting the movement, opposing the movement, or having an unclear opinion. After converting the data into a TF-IDF, I found that I had around 30000 features compared to 1000 samples. Intuitively, I would expect a model to perform poorly and be overfit when there are more features than samples, and a quick search kind of confirmed this, so I used a TruncatedSVD to reduce the features to 100. However, doing k-fold cross validation has shown me that not reducing features does substantially better than reducing features (0.72 f1 compared to 0.63 f1). Why am I seeing this behavior?
