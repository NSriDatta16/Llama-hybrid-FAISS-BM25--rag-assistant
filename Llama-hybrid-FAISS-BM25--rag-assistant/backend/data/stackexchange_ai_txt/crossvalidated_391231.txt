[site]: crossvalidated
[post_id]: 391231
[parent_id]: 
[tags]: 
Can you implement Replay Buffers for Reinforcement Learning when most experiences give zero reward?

Specifically, for a deep deterministic policy gradient, DDPG, to expedite the learning speed, it's recommended to use a Replay Buffer What if the reward is only given at a terminal state? Or, most of the experiences rarely give immediate rewards but the reward at the terminal state is sort of "what really matters"? Cause in that case, in that sample of a random minibatch, if it does not include the terminal state, then you would most likely have many 0 immediate rewards. Would DDPG algorithm with Replay Buffer still work for such situation?
