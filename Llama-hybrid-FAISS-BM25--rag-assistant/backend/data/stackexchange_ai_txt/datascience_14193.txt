[site]: datascience
[post_id]: 14193
[parent_id]: 
[tags]: 
Using tensorflow to test a variable amount of correct labels

I'm using a neural network to analyze item choices made by players in a computer game. In the game players can choose between 0 and 7 items. Right now I'm struggling with how I can evaluate my data. Tensorflow provides a nice method for getting the k highest values. tf.nn.top_k(input, k=1, sorted=True, name=None) The input here would be the prediction by my neural network. The output I get by applying top_k to the prediction would then be compared to applying top_k to the correct_output (the one the neural net should have had) and by doing this multiple times and averaging I get the accuracy that I want to have. The problem I'm running into is that k should depend on the amount of 1's in the correct_output. I am lost as to how I can achieve this. edit: correct output (if I understand how this works correctly) should already be a tensor. It is loaded from a .pickle file and at the beginning of the code it is prepared as follows: correct_output = tf.placeholder('float') As to what it looks like: it is simply a list of given length of 1's and 0's
