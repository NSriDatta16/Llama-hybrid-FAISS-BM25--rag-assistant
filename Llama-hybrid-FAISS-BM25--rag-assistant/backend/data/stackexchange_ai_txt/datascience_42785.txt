[site]: datascience
[post_id]: 42785
[parent_id]: 
[tags]: 
Sequential Modelling: Multiple Sequence to One or Sequence to Sequence

Suppose I have a single sequence of $x_1, x_2, ..., x_n$ and corresponding labels $y_1, y_2, ..., y_n$ . An example would be a person makes website visits $x_i$ and the label $y_i$ tells us if there was a purchase in any of the previous visits. Is there a difference between training multiple sequence to one or a single sequence to sequence ? Multiple sequence to one This approach gives us $n$ data points. Additionally it seems that the model will see $x_1$ n times, while $x_n$ will be only fed in once. $[x_1] , [y_1]$ $[x_1, x_2] , [y_2]$ ... $[x_1, x_2, ..., x_n], [y_n]$ Single Sequence to Sequence This approach gives us just 1 data point. $[x_1, x_2, ..., x_n], [y_1, y_2, ..., y_n]$ , with no repetition in data. Do these yield the same outcome in RNN (e.g. LSTM) training? It seems that when your data is imbalanced, you would greatly benefit from the 1st approach, since you get more observations. My understanding is that, since even a single data point is technically a valid data point in my case, I should be using the multiple sequence to one approach. But in cases of NLP (where the first word alone has no context), obviously we need the whole sequence.
