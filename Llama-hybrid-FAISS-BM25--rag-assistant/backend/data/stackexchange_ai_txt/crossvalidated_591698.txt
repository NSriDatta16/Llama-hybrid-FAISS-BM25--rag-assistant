[site]: crossvalidated
[post_id]: 591698
[parent_id]: 590914
[tags]: 
NO (Assume ordinary least squares linear regression, though I expect a similar argument to work for other models.) If the control variables are not multicollinear with our variables of interest, they do not affect the inference on the parameters on those variables of interest. Consider the formula for variance inflation factor . It is this inflation of the variance of the coefficient estimates that we would prefer to avoid. However, if the rest of the variables are not predictive of the variables of interest, there is no variance inflation. (I remember first discovering this when I thought I can wrestle out some additional power by running PCA on my control variables and retaining all of the PC, thinking that this was a trick to remove the covariance but retain all of the information, only to find my tests of the other variable completely unaffected.)
