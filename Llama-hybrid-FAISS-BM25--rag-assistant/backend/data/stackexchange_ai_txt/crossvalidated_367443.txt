[site]: crossvalidated
[post_id]: 367443
[parent_id]: 367315
[tags]: 
In the context of neural networks you could use dropout and regularization (L1 or L2). dropout is a rather simple technique - you simply don't use some the neurons' values in the forward pass (e.g. as you have set their weights to 0). This simulates a sparse network or multiple network architectures at the same time. Having more networks helps with the overfitting because it's like you have more models that know more (different stuff) about your data. L1 and L2 regularisations are kind of harder to explain hopefully you'll find good resources I, personally, found Andrew Ng's Deep learning specialisation very useful in this regard. I think the second course of the specialisation was dealing a lot with similar techniques.
