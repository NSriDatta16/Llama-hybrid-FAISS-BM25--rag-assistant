[site]: datascience
[post_id]: 115538
[parent_id]: 115536
[tags]: 
Yes, it is correct. If you compare Neural Network to the human brain, it would be similar to new learning and prediction adaptability (otherwise it would be regarded as "psychorigid"). No dropout at all is like learning something too well, without being able to learn something else new. Indeed, a NN cannot be able to modify its weights and learn new things, because weights are already fixed and cannot correct themselves. That's why the NN learning process needs a good balance with some error margin, just like the human brain needs some doubts when knowing something. So the dropout is mainly useful to make the NN continue learning through many iterations. Otherwise, it would reach a threshold and new data cannot be learned. Of course, too many dropouts would result in poor learning.
