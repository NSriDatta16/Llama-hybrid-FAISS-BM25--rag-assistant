[site]: crossvalidated
[post_id]: 305979
[parent_id]: 305201
[tags]: 
Note sure if your prof. meant it, but I'd like to mention another alternative to grid search for hyperparameter tuning: Bayesian optimization . The idea is to treat $f: (C, \gamma) \rightarrow Result_{svm}$ as an unknown function, which we can evaluate only in certain points and would like to optimize as fast as possible. Bayesian optimization method builds a model of the function $f$ using a Gaussian Process (GP) and at each step chooses the most "promising" point based on the current GP model. It starts with no information and acts like a random search, but gradually learns that some areas are "better" then others. These methods also try to deal with exploration-exploitation dilemma. In my research, I use Bayesian optimization a lot, especially when I need to optimize not two, but a dozen of hyperparameters. See this question is you need further details.
