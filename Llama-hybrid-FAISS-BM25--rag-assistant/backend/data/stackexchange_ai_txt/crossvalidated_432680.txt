[site]: crossvalidated
[post_id]: 432680
[parent_id]: 402224
[tags]: 
The increased complexity of the model is an example of the Neyman-Scott phenomenon. It justifies the need for conditional likelihood to model effects. There are Bayesian and Frequentist analogues of linear mixed models, but none that I know of are amenable to sequential analysis. That doesn't mean you can't store all the data and "update" models by refitting with all available data. The random forest is not what you need/want for this problem. It is actually quite a bit easier. These issues of imbalanced sampling typically resolve using survey weighting techniques. Specifically, a simple frequency weight accounts should reweight observations and residuals so that random and fixed effects are estimated with precision. If subject A has only 10 cells analyzable per assay, then set the weight to 10. If subject B has 100, set their weight to 100.
