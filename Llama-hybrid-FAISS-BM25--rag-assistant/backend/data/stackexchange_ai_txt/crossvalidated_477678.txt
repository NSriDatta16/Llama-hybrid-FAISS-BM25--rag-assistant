[site]: crossvalidated
[post_id]: 477678
[parent_id]: 
[tags]: 
How does pymc3 posterior simulation work in this simple case without having the full conditional distributions?

I'm trying to estimate the posterior distribution of the gamma parameters alpha and beta given that my data comes from a gamma distribution and the priors I chose come from two uniform distributions. I am using the pymc3 library. with pm.Model() as model: alpha= pm.Uniform('alpha', lower=8, upper=16) beta= pm.Uniform('beta', lower=0.01, upper=0.1) observed_data = pm.Gamma('observed_data', alpha=alpha, beta=beta, observed=df.values) with model: trace = pm.sample(draws=60, chains=50, tune=100, discard_tuned_samples=True) However I am not able to understand the theory behind it. It appears that in MCMC methods we are building long chains in order to take "indipendent" draws. I studied that in Gibbs sampling for example if X and Y have to be simulated, X|Y is simulated and then Y|X, many times. In this case I understand why I need a long chain. However in this case I only know the distribution of Gamma|alpha,beta and I don't have the full conditional distributions. Therefore, why do I need exactly a long chain, if I am just sampling from two prior distributions and then use these values to compute the likelihood? Am I missing something? Thank you very much for your patience!
