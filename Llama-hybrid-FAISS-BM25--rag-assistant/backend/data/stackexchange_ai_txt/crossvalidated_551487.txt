[site]: crossvalidated
[post_id]: 551487
[parent_id]: 
[tags]: 
Demonstration of DOE of mixture design applied to 2-stage gain schedule for machine learner

tl;dr Please provide an example of statistical design of experiment (DOE) of "mixture design" of learning rates to 2-stage gain schedules for machine learners. Background Statistical DOE is a rigorous approach that helps to get the most information from experiments performed. It gives a rigorous structure to the approach that makes it high yield. According to NIST , factors of a "Mixture Design" are described as: In a mixture experiment, the independent factors are proportions of different components of a blend. In machine learning, for many optimizers, there is a "learning rate" value that increases quality either by reducing time to learn or reducing distance from estimated "best" to actual "best" at the end of the process. One popular optimizer is "Adam" , and its variable is called "learning rate", but its adaptation is dynamic. If possible I would like to use piecewise constant learning rates (like this ). Also in machine learning there is a "rule of thumb" that when learning rate is modulated, in general it is better to go from higher values to lower values. (See learning rate adaptation here and here ) If the number of epochs is fixed (100 epochs), then training using a sequence of of gains sorted from largest to smallest can be considered a variation on a "mixture design". If half of all epochs are at learning rate 0.9, and the other at 0.1, then the mixture proportions are 90% and 10%, respectively. Question: Are there any existing examples of an approach like this being used to design rate schedules in machine learners? For demo case: An acceptable example could be using MNIST and a MLP ( like this ) except with gain schedules DOE applied to the learning rates.
