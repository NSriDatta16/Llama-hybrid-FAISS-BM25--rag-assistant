[site]: crossvalidated
[post_id]: 615204
[parent_id]: 
[tags]: 
Regression as classification: advantages?

I have read on many occasions deep learning practitioners recommending to treat regression problems (with continuous variables) as classification problems, by quantizing the output into bins and using cross-entropy loss instead of L2 loss. For example these notes from Stanfords's CS231n class on CNNs: It is important to note that the L2 loss is much harder to optimize than a more stable loss such as Softmax. Intuitively, it requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations). Notice that this is not the case with Softmax, where the precise value of each score is less important: It only matters that their magnitudes are appropriate. Additionally, the L2 loss is less robust because outliers can introduce huge gradients. When faced with a regression problem, first consider if it is absolutely inadequate to quantize the output into bins. [...] Classification has the additional benefit that it can give you a distribution over the regression outputs, not just a single output with no indication of its confidence. If you’re certain that classification is not appropriate, use the L2 but be careful: For example, the L2 is more fragile and applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea. When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible. Or Pixel-RNN which quantizes pixel values into bins: Furthermore, in contrast to previous approaches that model the pixels as continuous values [...], we model the pixels as discrete values using a multinomial distribution implemented with a simple softmax layer. We observe that this approach gives both representational and training advantages for our models. Treating regression as classification to me seems counter-intuitive, since the model cannot differentiate between a point that has been misclassified to a neighboring bin, vs. a distant bin. Apart from the few points cited above (allowing for small errors, and being more robust to outliers), is there any theoretical evidence – or at least some intuition – as to why treating regression as classification performs better? Is it purely an optimization issue, because the optimization landscape in classification is easier to navigate than in regression? Or are there any other reasons? I am mostly asking in the context of deep learning, but am also curious about more general answers.
