[site]: crossvalidated
[post_id]: 416942
[parent_id]: 
[tags]: 
Is experience replay taking individual memory samples out of context?

I understand that in reinforcement learning experience is collected in a memory buffer, which then contains state0, reward, done and state1. When the deep q network is trained, random elements from the memory are being sampled and the network is then trained to predict rewards based on certain states. My question is: What elements does state1 play in training? If individual actions are trained, wouldn't they be taken out of context? I.e. most of them are intermediary steps that may not even have any reward. How would training on them without knowing the end state (end of episode) have any benefits? Are rewards adjusted for each intermediary steps before training happens?
