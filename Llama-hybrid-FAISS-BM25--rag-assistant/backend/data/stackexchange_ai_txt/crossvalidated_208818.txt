[site]: crossvalidated
[post_id]: 208818
[parent_id]: 208797
[tags]: 
This is more of a comment than an answer, but it got too long for the comment box and hopefully it will prompt good discussion. Your approach is basically a repurposed version of propensity score matching . The basic technique is general and sound. It's worth noting that Gary King, one of the gurus of logistic regression, objects to propensity score matching for the purposes of causal inference [1], but his objections are specific to causal inference and not to the more general end of inferring similarity. The Python module Dedupe [2] makes use of similar techniques for matching records. This requires a notion of equality between entities in A and B, which might not be applicable to your problem. But the sources they cite in their bibliography could be interesting. And, as mentioned in the other answer , using a Random Forest instead of a logistic regression allows you to compute a proximity matrix between points as a side effect [3], [4]. Note that this will also return similarities between points in the same data set. This opens up an interesting can of worms: if you compute the similarities among all points in both data sets, you'll naturally have similarities between points in different sets. References King, G. and Nielsen, R. (2016), "Why Propensity Scores Should Not Be Used for Matching." Available at http://j.mp/1FQhySn . Eder, D. and Gregg, F. (2016), Dedupe . Available at https://github.com/datamade/dedupe . Liaw, A. and Wiener, M. (2002), Classification and Regression by randomForest. R News 2(3), pp. 18-22. Available at https://cran.r-project.org/web/packages/randomForest/ . https://stackoverflow.com/a/26611881/2954547
