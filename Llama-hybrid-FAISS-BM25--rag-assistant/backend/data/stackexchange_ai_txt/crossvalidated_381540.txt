[site]: crossvalidated
[post_id]: 381540
[parent_id]: 
[tags]: 
Binomial distribution as likelihood in Bayesian modeling. When (not) to use it?

I am currently trying to figure out some strangeness about using the Binomial distribution in Bayesian modeling to define the likelihood. To make an example assume I have two conditions, and in each condition I do five repeated measurements, each of which can be defined as a single Bernoulli trial. So let's just say I get the results $Y_{1,i}=(1,1,1,0,0)$ for the first condition and $Y_{2,i}=(1,0,0,0,0)$ . I want to compare the hypotheses that both $Y_{1}$ and $Y_{2}$ have the same probability of producing a $1$ (H1) vs. that they have a different probability (H2). For simplicity, I assume equal prior probability of both hypotheses. Also, assume flat prior on all parameters. So the first hypothesis can be parametrized by two probabilities $\theta_1$ and $\theta_2$ . So for $Y_1$ using a Binomial distribution I get three $1$ s out of five and therefore $P(N=3|\theta_1)={5 \choose 3}\theta_1^3(1-\theta_1)^2$ . Similarly, for $Y_2$ I get one $1$ out of five and therefore $P(N=1|\theta_2)={5 \choose 1}\theta_2^1(1-\theta_2)^4$ . Now to get the total probability of H1 independently of $\theta_1$ and $\theta_2$ I need to multiply the two and marginalize out the parameters (i.e. integrate over the prior). Since I can split the multidimensional integral, I can just integrate each probability separately and then integrate: $\int_0^1 {5 \choose 3}\theta_1^3(1-\theta_1)^2\;d\theta_1= {5 \choose 3}B(4,3)={5 \choose 3}\frac{\Gamma(4)\Gamma(3)}{\Gamma(7)}= \frac{5!}{(3!)(2!)}\frac{(3!)(2!)}{6!}=1/6$ for $Y_1$ and $\int_0^1 {5 \choose 1}\theta_2^1(1-\theta_1)^4\;d\theta_2= {5 \choose 1}B(2,5)={5 \choose 1}\frac{\Gamma(2)\Gamma(5)}{\Gamma(7)}= \frac{5!}{(1!)(4!)}\frac{(1!)(4!)}{6!}=1/6$ for $Y_2$ and therefore $P(H1)=1/36$ . For the second hypothesis I only need a single parameter $\theta_1$ , and thus I get four $1$ s out of 10 and therefore $P(N=4|\theta_1)={10\choose 4}\theta_1^4(1-\theta_1)^6$ . Now again I marginalize out $\theta_1$ and thus I get $P(H2)=\int_0^1{10\choose 4}\theta_1^4(1-\theta_1)^6\;d\theta_1= \frac{10!}{(4!)(6!)}\frac{(4!)(6!)}{11!}=1/11$ So hypothesis H2 seems more likely. But looking at the formulas, I find that I will get $P(H1)=1/36$ and $P(H2)=1/11$ independently of the observation, because all values determined by the numbers of $1$ s completely cancel out. If I instead use a Bernoulli likelihood I get (derivation only for H2) $P(H2)=\int_0^1 \theta_1^4(1-\theta_1)^6=\frac{(4!)(6!)}{11!}$ and $P(H1)=\frac{(3!)(2!)(1!)(4!)}{6!}$ Which is actually dependent on the observation and therefore seems more correct. Now I have seen people using Binomial distributions as the final step in the likelihood definition in Bayesian samplers. So the question is, when would this work, and when would it fail? I can see, that this might work (however I am not sure) when one is trying to estimate the parameters of each of the two models for H1 and H2. However, I have also seen this in tutorials about Baysian model selection, where a discrete random variable is used to switch between the two models. As far as I understood this method, the discrete variable just compares the integrals for each of the models (i.e. the probabilities after marginalizing out the parameters). So in that case, I assume that I would just get results independent of the observation? So when is summarizing the data and then using a Binomial distribution safe, and when will it fail?
