[site]: datascience
[post_id]: 94330
[parent_id]: 
[tags]: 
Do Activation Functions map to Higher Dimensions?:

I just started learning tensorflow and I have a question regarding activation functions used in neural networks, I watched a 3b1b video a while ago and it seems it squished the value into an interval like sigmoid does so by squishing it between 0 and 1 so we could make more concrete comparisons however while watching a tutorial today the instructor said that it projects the data points into higher dimensional spaces. I didn't really get how that's the case as it seems it's being converted to a scalar. Is there an interpretation/example for the latter claim? This is the timestamped URL he talks about it during the couple of minutes following this.
