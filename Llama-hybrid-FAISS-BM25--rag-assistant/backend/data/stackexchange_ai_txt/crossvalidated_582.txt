[site]: crossvalidated
[post_id]: 582
[parent_id]: 577
[tags]: 
As you mentioned, AIC and BIC are methods to penalize models for having more regressor variables. A penalty function is used in these methods, which is a function of the number of parameters in the model. When applying AIC, the penalty function is z(p) = 2 p . When applying BIC, the penalty function is z(p) = p ln( n ), which is based on interpreting the penalty as deriving from prior information (hence the name Bayesian Information Criterion). When n is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC. However, as stated in Wikipedia on BIC : it should be noted that in many applications..., BIC simply reduces to maximum likelihood selection because the number of parameters is equal for the models of interest.
