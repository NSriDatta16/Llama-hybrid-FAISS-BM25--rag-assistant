[site]: datascience
[post_id]: 45881
[parent_id]: 
[tags]: 
Target encoding with cross validation

I am trying to understand this way of target (mean/impact/likelihood) encoding using (two-level) cross validation. It's taking mean value of y. But not plain mean, but in cross-validation within cross-validation way; Let's say we have 20-fold cross validation. we need somehow to calculate mean value of the feature for #1 fold using information from #2-#20 folds only. So, you take #2-#20 folds, create another cross validation set within it (i did 10-fold). calculate means for every leave-one-out fold (in the end you get 10 means). You average these 10 means and apply that vector for your primary #1 validation set. Repeat that for remaining 19 folds. It is tough to explain, hard to understand and to master :) But if done correctly it can bring many benefits:) As far as I understand, the motivation of this approach is that: target encoding requires the knowledge of output, which is not available on the test set. So if we use the means obtained from the whole train set and apply on test set, that may cause overfitting. So instead, we will use other values derived from its subset. I found some discussions from this post but have trouble of understanding the following points: 1) It seems to me that the his second-level CV is nothing but taking the average of the whole #2-#20 fold. So basically, this is just one-level cross validation, where instead of using the mean of #1 fold, we use that of #2-#20 fold as the mean value for #1 fold. Am I missing something here? 2) Once we obtain the means of all 20 folds, what will we do next? If we average, this is again nothing but taking average of all train set.
