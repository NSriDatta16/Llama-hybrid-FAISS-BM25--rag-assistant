[site]: crossvalidated
[post_id]: 636343
[parent_id]: 413251
[tags]: 
The central equation of inference defines the integral for computing the expected (average) of a function $F(z)$ with a probability density $\Pi(z)$ as $E_{\: \Pi(z)} \: \big\lbrack F(z) \big\rbrack \: = \: \int F(z) \cdot \Pi(z)$ Variational inference considers the case when the probability density $\Pi(z)$ is intractable, and instead utilizes approximate inference for approximation of the distribution. There are multiple approximate inference algorithms to choose from (Junction tree, MCMC, VI), however, it is optimization-based approaches for estimation of the target density that are commonly used in the machine learning literature. The general framework is the following: Variational inference fits a surrogate and tractable distribution $q_{\phi}(z)$ to the true posterior $\Pi(z)$ distribution. In the foregoing notation, we used $\phi$ to denote a common family of probability distributions from which the surrogate distribution is chosen. The variational idea is detailed as $E_{\Pi(z)} \: \big\lbrack \: F(z) \: \big\rbrack \: \approx \: E_{q_{\phi}(z)} \: \big\lbrack \: F(z) \: \big\rbrack$ We can think of the method as restricting the set of functions over which the optimization is performed. The minimization of the KL-divergence or synonymous, the maximization of the Evidence of Lower Bound (ELBO) is a common objective for performing optimization. In summary, the technique enables Bayesian inference to scale to large datasets and complex models with intricate posterior geometries. While variational inference does not provide guarantees on the bounds, the stochastic optimization and distributed optimization speed up inference and enables the exploration and comparison of many different models. The following excerpts are taken from my book on Variational inference. Learn more by visiting https://www.thevariationalbook.com/
