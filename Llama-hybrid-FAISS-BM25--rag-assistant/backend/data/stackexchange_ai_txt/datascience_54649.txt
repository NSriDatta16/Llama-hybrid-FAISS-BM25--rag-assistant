[site]: datascience
[post_id]: 54649
[parent_id]: 10424
[tags]: 
I've been working on a very similar problem extracting materials details from invoicing information. One approach that has shown promise has been creating a dictionary of lists of dictionaries for each "staple" token - in your case I believe this would be the dictionary of all possible values. Each token is a key with each value being a list of dictionaries, with each of these dictionaries being comprised of all other tokens found in the sentence of the top level key, their frequency seen together, and the (average) distance from the top level key in each sentence. With this, I was able to generate a probability distribution depicting the likelihood of each sub level token's presence and placement in the sentence relative to the top level token/key. This has shown promise with identifying what measure the numeric tokens represent in a sentence when present with a top level token/key. In your example above, it may even result in a low level confidence that the "2" token actually maps to anything, which you can code in logic to flag. For instance, I discovered that if a material could have dimensions length x height x depth, it was most likely that any given numeric measure would be length or height, and that only a very small amount of the time it could be depth. I then expanded that logic to "step forward" and use the sub level's dictionary (ie length) to further extrapolate the likelihood of the next numeric token identified, and so on. This also led to being able to generalize characteristics in sentences based on the source/publisher (in your case this may correlate to the manufacturer's preferred method of naming convention.) If you've got to use reflex, pythex.org is also an extremely useful tool for testing. Just remember that pythex operates under the r'' assumption, so escaping special characters may have different results in your compiler. This approach also assumes you have a large amount of data with which to experiment -if your corpus is too small or the same words are repeated too often, many NLP/ngram/common words approaches aren't going to very well.
