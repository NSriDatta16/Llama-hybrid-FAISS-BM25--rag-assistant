[site]: crossvalidated
[post_id]: 377047
[parent_id]: 377033
[tags]: 
Actually, the blog post does not say that there is an issue with correlated features. It says only that the feature importances that they calculated did not yield correct answer. Now, this does not have to be a problem with random forest itself, but with the feature importance algorithm they used (for example, the default one seems to be biased ). They also noticed that including the correlated feature did not hurt the cross-validation performance on the particular dataset they used. So the question is if you want to make predictions, or use the model to infer something about the data? By design random forest should not be affected by correlated features. First of all, for each tree you usually train on random subset of features, so the correlated features may, or may not be used for a particular tree. Second, consider extreme case where you have some feature duplicated in your dataset (let's call them $A$ and $A'$ ). Imagine that to make decision, a tree needs to make several splits given this particular feature. So the tree makes first split on $A$ , then it may make second split using either $A$ or $A'$ . Since they are the same, it could as well throw a coin to choose between them and get exactly the same result. If they are not perfectly correlated, this is only a question if decision tree can pick between features the one that works better, but that's a question about quality of the algorithm in general, not about correlated features.
