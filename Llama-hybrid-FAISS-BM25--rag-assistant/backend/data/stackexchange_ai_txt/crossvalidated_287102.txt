[site]: crossvalidated
[post_id]: 287102
[parent_id]: 287085
[tags]: 
Call $\theta$ the parameter and $X$ the observed variable. The likelihood is $P(X|\theta)$ : the probability to observe something, depending on (given) the parameter. not the same as the probability that those parameters are the right ones This instead is called the posterior and is formally $P(\theta|X)$ : the probability that the parameter is something, depending on (given) the observation. Bayesian inference says : $$P(\theta|X)\propto P(X|\theta)P(\theta)$$ When the prior $P(\theta)$ is uniform then the likelihood and the posterior are proportional to each other and can be meaningfully identified. Note that the posterior has no meaning in the frequetist framework. Yet, it is a reasoning we sometimes unwillingly do, simply because : we are not aware of reversing the conditioning in our heads there is an intuition this "tends to be rather true" practically This intuition is rather justified in many simple practical situations. You can see my answer for confidence intervals : What, precisely, is a confidence interval? . But it is false generally.
