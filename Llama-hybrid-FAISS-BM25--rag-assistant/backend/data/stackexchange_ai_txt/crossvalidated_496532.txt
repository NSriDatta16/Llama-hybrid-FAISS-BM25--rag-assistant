[site]: crossvalidated
[post_id]: 496532
[parent_id]: 
[tags]: 
Probability distribution fitting with Wasserstein metrics

I'm a physicist. I have a relatively complex physical model for a process occurring, and I can numerically solve the relevant differential equations, evaluate their solutions and produce observables, $x_i$ and $z$ , let's say. The model has some parameters in it that are of interest, and correspond to my idea of physical reality, let's call them $\hat\theta$ . In some sort of pseudo-maths, my physical model $f(\hat\theta)$ specifies two separate functions $g$ and $h$ , $[g, h]=f(\hat\theta)$ and the functions themselves are physically observable: $h$ is a function of time, $z=h(t, \hat\theta)$ , whereas $g$ is a discrete probability distribution function from which I obtain specific values; $g(x_i)$ . Basically, the trouble is that my observables are naturally different. $x_i$ is effectively a probability, so that the "output" is a [naturally discrete] histogram, which I can also experimentally measure; whereas $z$ is the value of something a function of time. I can also measure this data directly as a function of time. I'd like to do a joint fit for $\hat\theta$ using, if it's computationally feasible, MCMC, or ABC if it isn't. If I were to just have the data as a function of time, I'd be quite happy to do this using a maximum likelihood approach based on minimising the $L_2$ metric between my experimental values of $h(t)$ and the model's. When it comes to fitting the histogram , however, I'm far less sure: whilst I could just do a least squares fit between the predicted PDF and actual bin height in a given range, this seems dependent upon, for example, bin width, or some other sort of boring hyperparameter. An alternative approach, and one that may be more robust, would be to use an appropriate probability space metric between the two. Of these, I think that the Wasserstein metric [aka earth movers' distance] would be most appropriate for minimising between the experimental and theoretical PDFs. If we denote this metric by $W_p(\text{data}, \text{theory}(\hat\theta))$ my question is this: Is there any sort of theoretical justification for minimising $$ \underset{\hat{\theta}}{\mathrm{argmin}}\quad \left|\left|h(t,\hat\theta) - z_\text{experiment} \right|\right|_{2}^{2} + W_p(x_i, g(\hat\theta)) $$ Alternatively, can I consider $W_p$ explicitly in some-sort of quasi-log-likelihood function, e.g. just by adding $1-W_p$ to the terms considered? What is a good framework to work in? Finally, what sort of name is given to these types of joint estimation problems?
