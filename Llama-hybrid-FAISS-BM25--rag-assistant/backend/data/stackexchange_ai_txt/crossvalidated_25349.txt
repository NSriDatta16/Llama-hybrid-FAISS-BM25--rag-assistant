[site]: crossvalidated
[post_id]: 25349
[parent_id]: 
[tags]: 
How does pooling and resampling affect variance of sample mean?

Suppose I have $N$ independent random variables $X_n$. I draw a sample of predetermined size $K_n$ from each of them. Denote the average of each sample $\bar{\hat{X}}_n$, and the total number of observations $M = \sum_{n=1}^{N} K_n$. Next, I use all the $M$ sample observations $\hat{X}_{n,k}$, with no regard as to which r.v. they came from, to construct the empirical distribution $\hat{F}$. Next, I create $N$ new i.i.d. random variables $Y_n$ distributed as $\hat{F}$, and draw from the them i.i.d. samples of the same sizes $K_n$. Denote the average of each sample $\bar{\hat{Y}}_n$. Let $T$ be some commonly used sample statistic that represents the dispersion; e.g., variance (or interquartile distance). Finally, I compare $T_X = E[T(\bar{\hat{X}}_1, …, \bar{\hat{X}}_N)]$ and $T_Y = E[T(\bar{\hat{Y}}_1, …, \bar{\hat{Y}}_N)]$. Under what (preferably intuitive) conditions would I find $T_X = T_Y$, $T_X > T_Y$, or $T_X Motivation: I've seen bootstrap used in social sciences to analyze a large group of independent random variables. Specifically, the researcher would examine the variation in the sample statistics across these variables. If it seems that the variation is bigger than "expected", the researcher would draw some conclusions about the original r.v. But I could never figure out precisely what null hypothesis this approach tests, so I'm also unsure about what conclusions would be justified.
