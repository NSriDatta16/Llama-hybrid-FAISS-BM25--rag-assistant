[site]: crossvalidated
[post_id]: 259872
[parent_id]: 
[tags]: 
Why does my Lasso regression show that the lowest error is achieved with 0 variables?

I have a data set with n=199 observations and p=149 variables. I've tried to fit a lasso regression for reducing the number of variables in a logistic regression, but when i used it, I got that the best model have 0 variables. I don't know if my variables need to be standarized, or maybe my data set is too small. My code is: library(glmnet) set.seed(914) ## number for reproducing modelfitted The output is 49 x 1 sparse Matrix of class "dgCMatrix" 1 (Intercept) -0.567521 capturing_times_on_page . times_on_perfil . times_on_home . times_on_faq . times_on_terminos . times_on_privacidad . times_on_acerca . times_on_lugares_de_pago . times_on_blog . times_on_contacto . times_on_libro_de_reclamaciones . capturing_time_on_steps . time_on_step1 . time_on_step2 . time_on_step3 . time_on_step4 . time_on_step5 . time_on_subir_fotos . time_on_confimado . time_on_reintentar_subir_fotos . amount_changes . time_changes . some rows were omitted. The minimum miss classification error is for when the number of variables is zero. I hope that someone can help me. Why is this happening?
