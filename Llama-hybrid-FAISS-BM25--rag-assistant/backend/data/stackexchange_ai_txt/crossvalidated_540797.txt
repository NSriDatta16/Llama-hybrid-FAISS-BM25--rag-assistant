[site]: crossvalidated
[post_id]: 540797
[parent_id]: 
[tags]: 
Empirical Implications of Unbiased Estimators

I am familiar with the layperson explanation of an unbiased estimator as follows: if we repeat an experiment under identical conditions many times, the average value of the estimate will be close to the true value (see the following question and answers ). I find that, when I explain this to people, the common answer I get is something along the lines of: "But I only ever have one sample and I compute the estimator based on this one sample, so what implication does unbiasedness have for my one sample?" Intuitively, my answer would be something like the following: if we have two estimators with equal variance but the first is unbiased and the second is biased, then for a given random sample, the unbiased estimator is more likely to be closer to the true value than the biased estimator. My question is whether (1) this is actually correct and (2) if it is correct, is there an explanation of unbiasedness that does not also involve the variance of the estimator as well?
