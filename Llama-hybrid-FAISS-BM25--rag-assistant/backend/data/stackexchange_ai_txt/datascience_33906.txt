[site]: datascience
[post_id]: 33906
[parent_id]: 
[tags]: 
Neural Network Prediction regression task, output is a multiple factor of input with same peaks

When I missed some details please point this out. I made a simple sequential LSTM model for regression. The model loss is 3.2145e-06. The data is scaled between 0 and 1. I tried different variations of layers etc but the same happend, the simpliest model I used is the following one: from keras.models import Sequential from keras.layers import LSTM model = Sequential() model.add(LSTM(16,return_sequences=True,input_shape=(None,1)) model.add(LSTM(1,activation='sigmoid', return_sequences=True)) model.compile(loss='mae', optimizer='adam', metrics=['mae','mse']) history=model.fit(train,train, epochs=10, batch_size=78, shuffle=False) predicted=model.predict(test) My train data looks like: When I compare the test and predicted, predicted looks likes being many times smaller than the test data and showing the exact peak values. In my case I expect the values to be small because I trained with totally different data (kind of anomaly regression), what is surprising are the peak values being identical. testdata plot above and predicted below. So my question why this with the peaks are happening and how can I make the network not be able to detect the peaks and predict as trained ? Edit: After using MAE as loss function and 500 epochs training I reached 0.0180 loss. The prediction looks like mirrored of the mse prediction (see below). After 2000 epochs I reached a minium of 0.0030 and th After 2000 epochs I reached a minium of 0.0030 and the prediction looks like this: Edit: my data is on supposed to work with LSTM, I needed an other model.
