[site]: crossvalidated
[post_id]: 552469
[parent_id]: 552464
[tags]: 
This is very fun to think about. I will answer in terms of probabilities since that is how you framed your original question. Ben has chosen to provide a non-probabilistic answer by retrospectively looking at the sequence of Babe's observed outcomes. Let's consider a simplified Bernoulli probability model for whether Babe hits a home run. If we let $X$ be the result of an at-bat where $X=1$ "with probability $p$ " for a homerun and $X=0$ otherwise, then $P(X=1|p)=p:=\underset{n\rightarrow \infty}{lim}\frac{1}{n}\sum_{i=1}^n X_i$ is a statement about the proportion of homeruns over many, many (infinite in fact) at-bats. $p$ is an unknown fixed constant. When we use probability we are describing the emergent pattern of events over many, many samples. In order for a pattern to emerge that contains homeruns, an eventual home run is inevitable with enough at-bats. It might be more clear to simply say that each at-bat brings him closer to the next home run. This, then, is a no-brainer, a statement of the obvious âˆ’ Babe cannot strike out indefinitely. Using our simplified Bernoulli model, does this mean that the long-run probability of Babe hitting a home run depends on his past performance? Where we sometimes run into trouble is if we try assigning probability statements to single events. We can be confident in a single event occurring based on our knowledge of the long-run performance of the process, but a single event does not have a probability. Probability is a proportion so always ask yourself, "A proportion of what?" A proportion of many samples. The memoryless property is stating that the emergent pattern over many, many samples is the same no matter how the sequence begins, i.e $P(X=1|p, x_1,...,x_n)=P(X=1|p)$ where we are considering that each at-bat is independent of the others. I don't think these ideas are at odds with Babe's statement. Dave's answer (now since deleted) is correct in terms of expected values, but he is incorrect to apply these probability statements to a single at-bat. In some sense Babe is owed a homerun ( in repeated trials ) because he cannot strike out indefinitely and still have an expected value of $p$ (or $\frac{1}{p}$ for the geometric distribution of the number of at-bats) where $0 . Some discuss this idea using the phrase "regression to the mean." Perhaps Babe's statement and your intuition are leading us to the CDF of the geometric distribution, $P(K\le k)=1-(1-p)^{k}$ where $K$ is the number of at-bats until Babe's next homerun. $P(K\le k)$ is also a limiting proportion of many samples or trials, where each homerun constitutes the conclusion of a single sample or trial. Such statements about repeated trials give us confidence in what we should expect to see in our reality since our reality can be viewed as one such trial. Suppose we had many measurements allowing us to estimate Babe's long-run homerun rate, as well as knowing his observed sequence. We could use this information to calculate a predictive p-value testing a hypothesis about the number of at-bats until Babe's next home run. This predictive p-value is also a long-run probability, but it gives us confidence in our hypothesis regarding the next observed result while incorporating information on Babe's long-run performance AND his most recent performance. This is not a p-value testing a hypothesis about $p$ , it is a p-value testing a hypothesis about the next observed result, $H_0$ : $K\ge k$ or $H_0$ : $K\le k$ , unconditional on the unknown fixed true $p$ . This is the sort of analysis used to construct prediction intervals for, say, a time series or a Poisson process. Here is an answer discussing the prediction of a single coin toss, analogous to predicting the result of the next at-bat without considering the recent sequence leading up to the next at-bat. As a separate but related thought experiment, think of flipping a coin with $p=0.4$ for the probability of heads and getting straight tails in 10 flips, where $X=1$ denotes a flip landing on heads and $K=k$ is the number of flips until a heads appears. If we are confident $p$ is indeed close to $0.4$ from earlier experiments based on 1,000 flips (and nothing about the coin or the flipper has changed) then we must be witnessing a rare event and it would only be natural to bet on $K=11$ since the likelihood of such a streak continuing is exceedingly rare. The predictive p-value testing the hypothesis $H_0$ : $K\ge 12$ $[X=0$ (tails) on the next flip $]$ is the probability of the discrepancy between the observed result and the hypothesized result or something more extreme, $1-\Phi\bigg(\frac{\frac{400}{1000}-\frac{0}{11}}{\sqrt{0.4(1-0.4)/1000 + 0.4(1-0.4)/11}}\bigg)=0.004,$ approximated using a Wald-type test. We are therefore $100(1-0.004)\%=99.6\%$ confident the next flip will result in a heads. Additionally the p-value testing the hypothesis $H_0$ : $K=11$ $[X=1$ (heads) on the next flip $]$ is $\Phi\bigg(\frac{\frac{400}{1000}-\frac{1}{11}}{\sqrt{0.4(1-0.4)/1000 + 0.4(1-0.4)/11}}\bigg)=0.981$ . I am not suggesting the coin becomes more likely to land heads or tails in any given flip (see my discussion above regarding probability). I am suggesting that if we were to repeat this experiment many thousands of times, the proportion of times where the coin produces a $\hat{p}=0.4$ in 1,000 flips and then lands on tails after a string of 10 consecutive tails (or something more extreme) is incredibly small. The key to succeeding in the long run with this predictive p-value strategy is to bet only on the length of a run before any data are observed (Neyman-Pearson's error rate). Otherwise, if you have historical data, are in the middle of a run, and interested not only in the ultimate length of the run but also the result of the next flip, one can view the predictive p-value as a weight of the evidence without error rate guarantees (Fisher's evidential p-value). If we take it as known that $p=0.4$ and reference the CDF of a geometric distribution we see that the probability of the number of flips being at least 12 until a head is observed is $(1-0.4)^{11}=0.004$ . This probability from the geometric CDF could be viewed as a p-value testing the hypothesis $H_0$ : $p=0.4$ . We could instead retain this hypothesis and call into question whether the next flip will land heads. One might also examine the conditional probability $P(K>k|K\ge 11)$ . Knowing the limiting proportion of heads, if we want a safe bet on the outcome of each coin toss that will pay off in the long run then we should just always bet tails. This is like investing in the S&P500. If we want a safe bet on the length of a run then the geometric distribution would indicate we should bet on shorter-length runs. Above is a time series of 50 flips from a coin with $p=0.4$ . Observing runs of a few heads or tails is not uncommon, but if a run were to continue long enough we would naturally anticipate it to end, e.g. consider the string of tails leading up to flip 21. In order for $p$ to remain a constant $0.4$ the coin cannot land on heads or tails indefinitely. This is the probabilistic way of interpreting Babe's statement without invoking the gambler's fallacy, i.e. that the limiting proportion $p$ changes as a result of what we observe. Here is a wikipedia page on prediction intervals and predictive p-values. Here is a paper on the topic.
