[site]: datascience
[post_id]: 45584
[parent_id]: 14007
[tags]: 
Clearing the confusion of gradient descent in XgBoost Gradient descent is NOT used in Xgboost. If you look at the generalized loss function of XgBoost, it has 2 parameters pertaining to the structure of the next best tree (weak learner) that we want to add to the model: leaf scores and number of leaves . Gradient descent cannot be used to learn them. The other variables in the loss function are gradients at the leaves (think residuals). This is why the algorithm is called gradient boosting . It has nothing to do with gradient descent. So how do we learn in XgBoost and when is gradient boosting invoked during the algorithm? Ideally, we need the loss function to find the next best tree (weak learner) that brings the maximum reduction in loss. So if you take the derivative of the loss function to find its minima, you will get the following scoring function to find the best tree- Iterate overall all possible trees and the tree with the lowest score is our next best tree. BUT it is not practical to scan through all the possible trees. So, the loss function is modified further to find the next best split and now you can call it the gain. The equation is as follows- Here, gamma is the cost of adding an additional leaf, lambda is the L2 regularization parameter, G and H are derivate and second-derivative of loss, respectively. Now that we have the equation to calculate gain for each split, find the best split using this equation and ta-daa! That's gradient boosting :) To understand this in details, read this .
