[site]: crossvalidated
[post_id]: 525812
[parent_id]: 
[tags]: 
Is there any common characteristics for embedding?

When I was learning the embedding techniques, I tried PCA of checking the "embedding data", I found that there are no major principle components in there, i.e., variance is more "evenly distributed" across different dimensions. Also, if we look at the distribution for some dimensions, the data seems to be "evenly distributed". Above examples can be verified by building a MNIST CNN classifier and remove the last layer to get the embedding. Why these are happening? Is there any common characteristics for embedding?
