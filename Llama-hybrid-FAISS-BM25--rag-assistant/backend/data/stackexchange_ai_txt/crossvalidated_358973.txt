[site]: crossvalidated
[post_id]: 358973
[parent_id]: 
[tags]: 
looking for examples: inverse reinforcement learning original paper

I am reading the original Inverse reinforcement learning by Ng and Russell. One of the issues they encountered (as said in the case of small state space) is that the recovered reward functions are infinite (many zeros, and non-unique). To solve the problem of many zeros (degeneracy) they proposed some heuristics (in Chapter 3.2), by introducing a condition that will maximise the difference between the Q values of the optimal policy to the Q values of the next best thing (the non-policy). The downside to this is that recovered rewards are extremely different from actual ones. I am trying to teach this concept to first timers of IRL. And I want a simple, yet profound example to illustrate this point (among other concepts). Can you suggest ways on how I can effectively illustrate this point?
