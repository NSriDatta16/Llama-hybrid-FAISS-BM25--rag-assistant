[site]: datascience
[post_id]: 121026
[parent_id]: 
[tags]: 
Feature Engineering and Score Generation for a List of Features

I have a decision tree trained for a binary classification task. It uses a model score plus some other attributes as features. It performs well, majorly because the model score is a strong feature which has ~0.8 importance in the tree. This score was developed by another team and we want to develop something similar, say, our own score modeled from our own attributes (hundreds more available). Ideally, we want our tree model to only use several score-like features, like score1+score2+score3 to make prediction, instead of using socre1 + regular feature ABC like we have right now. Then we are facing two problems. Feature Selection Stage - How to select good features out of hundreds? I know there are several technologies to use. e.g., Filter and Wrapper methods. A. But how many should I select? Top 80, 50, or something else? How do I evaluate the quality of these selected features? B. How about running feature reduction after feature selection to reduce feature space to a lower dimension, which makes it easier to use boosting to produce the score later, in terms of computational resource. Modeling Stage - How to generate a good score out of these features? A. How about aggregating "original value times weight" to output as the final score? B. How about using regression? The label is 0 or 1, can I get scores belonging to [0, 1] if I apply regression? C. How about using boosting algorithms? There are so many, but which one should I select? Thanks and I really appreciate your answers!
