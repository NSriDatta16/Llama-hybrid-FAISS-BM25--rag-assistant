[site]: crossvalidated
[post_id]: 463091
[parent_id]: 463075
[tags]: 
(This was asked earlier [independently] at https://datascience.stackexchange.com/q/42133/55122 . My answer here is an extension of my answer there.) I think the authors make it clearest in the introduction: In this work we restrict ourselves to tree-structured configuration spaces. Configuration spaces are tree-structured in the sense that some leaf variables (e.g. the number of hidden units in the 2nd layer of a DBN) are only well-defined when node variables (e.g. a discrete choice of how many layers to use) take particular values. See for instance this example in HyperOpt : from hyperopt import hp space = hp.choice('classifier_type', [ { 'type': 'naive_bayes', }, { 'type': 'svm', 'C': hp.lognormal('svm_C', 0, 1), 'kernel': hp.choice('svm_kernel', [ {'ktype': 'linear'}, {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)}, ]), }, { 'type': 'dtree', 'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']), 'max_depth': hp.choice('dtree_max_depth', [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]), 'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1), }, ]) So, the answers to your questions: Nodes are (potentially collections of) hyperparameters, and (at least) when a discrete list of values is provided, child nodes can be created for values from that list. (Above, the choice of C only needs to be made when the classifier type is svm .) No, the quantile values are just used to separate "good" from "poor" values of the hyperparameter. (I found this blog post very helpful, although it doesn't really discuss the tree structure.) The densities are approximations of (subsets of) the configuration space. So sampling points according to those densities amounts to tracing the tree structure, with probabilities of each path being determined by the approximation functions $\ell, g$ .
