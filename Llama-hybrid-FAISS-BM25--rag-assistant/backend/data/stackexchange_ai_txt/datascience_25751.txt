[site]: datascience
[post_id]: 25751
[parent_id]: 25740
[tags]: 
If you are using two hidden layers, it may mean that your data is not linearly separable. If you just use one unit in a single hidden layer then you can claim that your data is linearly separable, which in your case you may say that there exist a hyper-plane that separates your data linearly. As the answer to your question, yes. It is possible to reduce the amount of error by adding layers or adding extra units to the current architecture but there are some points that are necessary to be considered. Whenever you add more layers, there will be vanishing and exploding gradients which may cause your network not to learn, or learning may happen so slowly. To avoid, you should use ReLU activation in order to avoid saturation of gradients. Moreover you have to use He or Xavier initialization techniques to avoid having bad random weights. There are other techniques for solving this problem which are called skip connections but at least I've never seen the use of them in MLP s Although they are really helpful for solving the mentioned problem. Covariat shift is the problem of learning for deeper layers. As a solution you have to use Batch Normalization to somehow normalize the activations of the deeper layers. Overfitting is a problem that happens for large architectures that are not fed with enough training data. You should use regularization techniques. There are so many papers about this but in your case which is using MLP I highly recommend you using Dropout technique which is invented by the so called God Father of deep learning.
