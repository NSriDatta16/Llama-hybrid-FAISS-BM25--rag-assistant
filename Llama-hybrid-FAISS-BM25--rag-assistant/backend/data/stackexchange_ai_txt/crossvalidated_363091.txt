[site]: crossvalidated
[post_id]: 363091
[parent_id]: 
[tags]: 
Incorporating knowledge of aggregate outcomes to constrain predictions on finer scales

I've got county-level longitudinal data on the timing of an event between the years 1998 and 2012, and I want to use it to form a predictive model for the time that that event will occur in future years, given features of those years. Now, I've got state-level data on the (median) time of the event between 1979 and the present. I've also got county-level covariates (but not outcomes) between 1979-1997 and 2013-present. If I take county-level predictions for the periods 1979-1997 and 2013-present, aggregate them to the state level and compare them to the known state-level aggregates, I find that that my predictions are biased downwards, both in the early and later periods. In other words, I'm consistently predicting that the event will come before it does. Intuitively, I should be able to use the state-level known outcomes to constrain the model. One way to do this would simply be to impute the missing county-level values for 1979-1997 and 2013-present with their state-level values. But I fear that this will distort the model in ways that are hard to predict. Are there more sophisticated approaches to this problem? Is this a use-case for something explicitly bayesian? I'm also reminded of applications of maximum entropy/information theory that I've seen used in ecology. But I'm not sure how to proceed. For more context, the predictive model is xgboost with fairly shallow trees as the base learner. I've got about 1000 features and about 50K observations.
