[site]: crossvalidated
[post_id]: 401629
[parent_id]: 401627
[tags]: 
I wouldn't see this as a problem for standardization or normalization at all. Watch out: standardization and normalization have a range of meanings across statistics and machine learning. You should not assume that readers know what you mean. But I can't see that anything I have seen under either name would help here, yet there is still a need to be precise. Either term often means some kind of scaling, say (value $-$ mean) /SD to ensure mean 0 and SD 1 or (value $-$ min) / (max $-$ min) to ensure all values lie between 0 and 1. You have a problem in predicting sales from promotion and number of stores sold. Whether you predict the products separately or together is a delicate question depending closely on the products: are they competing or are they quite different? I have heard folklore that big stores put beer and baby stuff side by side, so that fathers buying baby stuff make it clear that they are macho enough by buying beer too. If it's two brands of toothpaste or kinds of ballpoint pen, the story is quite different. Most purchases are going to be one or the other not both. Either way, the problem to me shouts regression. As some stores might sell nothing in a period, and some stores might sell much more, it's not obvious that plain or vanilla regression is quite enough. I would lean towards generalised linear models with logarithmic link. If you can get a predictive relationship, then you can estimate sales with no promotion for some given number of stores. Standardization would just throw away information you really need for a good prediction.
