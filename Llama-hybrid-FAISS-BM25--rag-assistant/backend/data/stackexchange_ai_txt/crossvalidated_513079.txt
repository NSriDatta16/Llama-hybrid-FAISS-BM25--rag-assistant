[site]: crossvalidated
[post_id]: 513079
[parent_id]: 
[tags]: 
A VECM in logs or a ARDL in the first difference of logs?

Suppose I have a number of time series that appear to have exponential growth at similar rates, with errors I believe to be generally proportional to the level of the variable. I believe that one of these variables, Y, is an approximately linear function of lags of itself and some other variables X 1 , X 2 , â€¦, and lags of these. My goal is to forecast the Y variable. On my data, I can not reject a unit root. My current understanding is that I have two ways to proceed. First, I can take the log of the series (to remove heteroskedasticity), check them with one of the cointegration tests, and, if present, estimate them as a VAR in one of the vector error correction forms. Alternatively, I can take the first difference of the logs, test them for (weak) stationarity, and if present, estimate as an autoregressive distributed lag model, where neither VAR methods nor testing for cointegration is needed. (And if I am wrong about any of these things I want to know). In both cases I then need to do back-transformation into levels. My question is, suppose the log series pass the cointegration test and the first difference in logs passes the stationarity test. Is there any theoretical reason to believe one is better than the other? I could hold out some data and do forecasts under each method, and see which produces smaller out-of-sample forecast error, but if the explanatory power is not to far apart, I am not confident that this is a powerful test to discriminate between the models. I would like either a test or tests that tell me that one of these models is right and the other wrong on my data, or a model selection test that is among the best available for ranking models in this setting. In both cases, I am looking for tests based on frequentist, rather than Bayesian or information-theoretic, assumptions.
