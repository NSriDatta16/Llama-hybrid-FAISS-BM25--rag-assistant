[site]: crossvalidated
[post_id]: 484824
[parent_id]: 484627
[tags]: 
Welcome to Cross Validated! Here are some thoughts: Because correlation is not the same as causation, without knowing a priori that there is a single, known causal direction, there is no reason why cannot consider any variable as the response variable. Only further experimentation can tell you if there is a distinct causal direction. p = 0.23 does NOT mean that two variables "do not interact." It only means that you don't have enough evidence to reject the null hypothesis of zero slope in your particular model, with your particular data. Something to drill in your head over and over is that a non-significant p-value does not prove no effect. Your hypothesis test is only constructed to either reject or fail to reject the null hypothesis of no effect. It is perfectly reasonable to run your model in two directions. Such situations occur all of the time. Does the presence of a chemical in the blood cause a condition or result from it? Or, do both result from some other cause? There are, of course, things that cannot be response variables, such as the genotype of a subject. Additional tips: Look for additional variables that you might need to include in your model. Do a lot of exploratory analysis. Make sure you study your data visually. Do all of the requisite regression diagnostics to make sure your data and model conform to the assumptions of the type of regression you are performing. I hope this helps. Continued based on additional information from OP If factor_3Level and factor_2Level are independent, it could still make sense to include one as a predictor and one as an outcome if one acts as an effect modifier of continuous . However... If I understand correctly, you experimentally adjusted factor_2Level and factor_3Level , and then measured continuous . In this case, perhaps I was wrong to say that you could consider any variable as the outcome. I was thinking in terms of observational data. I think what you have is a straightforward two-way (2x3) ANOVA model continuous ~ factor_3Level*factor_2Level . Of course, that is roughly equivalent to linear regression, but you would want to handle multiple comparisons differently (e.g. Tukey, Bonferroni). Regarding the assumptions, after you run lm you need to look at the residuals, qq plot, etc. Regression diagnostics. Is that more helpful than before? More based on information from comments Consider the problem: The gate is behind a screen and you don't know if it is open or not. You observe the animal pacing and want to predict if the gate is open or closed. Yes, you could definitely perform logistic regression of gate (open/closed) on weather, pacing gate ~ pacing + weather . This amounts to asking a specific statistical question: Can I categorize the gate state by observing weather and pacing. That being said, from the little I know of Bayesian statistics, that may be a better framework. But logistic regression would be a good start nevertheless. Alternatively, you could consider other classification methods from the machine learning repertoire such as Bayes nets or discriminant analysis. I can't be much help with that.
