[site]: datascience
[post_id]: 47145
[parent_id]: 47094
[tags]: 
Thank you for your answer. I changed sigmoid for RELU, and the result is the same. Anyway, I will keep RELU now!! Here are pictures of training for both the one who does 79% accuracy (merged two classic neural networks), and the one in which I do 70: (of course I don't talk about the training accuracy but the test accuracy). I see a difference in the loss value, but I don't know how to interpret it? Does this mean that for the less good architecture, I don't reach a minimum? If yes, what can I modify in my NN ? Thank you for the time you took to answer!! Edit: Where can I modify parameters like learning rate in the LSTM part of the Neural network? left_branch = Input(shape=(100,), dtype='int32') # input_dim: Size of maximum integer (7001 here); output dim: Size of embedded vector; # input_length: Size of the array left_branch_embedding = Embedding(7000, 300, input_length=100)(left_branch) lstm_out = LSTM(256)(left_branch_embedding) lstm_out = Dropout(0.7)(lstm_out) lstm_out = Dense(128, activation='sigmoid')(lstm_out) right_branch = Input((7012, )) merged = Concatenate()([lstm_out, right_branch]) output_layer = Dense(3, activation = 'softmax')(merged) model = Model(inputs=[left_branch, right_branch], outputs=output_layer) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
