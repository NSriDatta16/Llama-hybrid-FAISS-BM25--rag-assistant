[site]: stackoverflow
[post_id]: 1289089
[parent_id]: 1288630
[tags]: 
You should consider having a table off of your category/department table which has a unique URL for each category. Then you can use a special routine to generate the URLs. This can be a SQL scalar function, or a CLR function, but one of the things it would do is normalize the URL for the web. You can convert "Beverage & Bar" to "Beverage-And-Bar" and "Pastry / Decorating" to "Pastry-Decorating". Mainly, the routine needs to replace all invalid HTTP URL characters with something else. An example is this: public static class URL { static readonly Regex feet = new Regex(@"([0-9]\s?)'([^'])", RegexOptions.Compiled); static readonly Regex inch1 = new Regex(@"([0-9]\s?)''", RegexOptions.Compiled); static readonly Regex inch2 = new Regex(@"([0-9]\s?)""", RegexOptions.Compiled); static readonly Regex num = new Regex(@"#([0-9]+)", RegexOptions.Compiled); static readonly Regex dollar = new Regex(@"[$]([0-9]+)", RegexOptions.Compiled); static readonly Regex percent = new Regex(@"([0-9]+)%", RegexOptions.Compiled); static readonly Regex sep = new Regex(@"[\s_/\\+:.]", RegexOptions.Compiled); static readonly Regex empty = new Regex(@"[^-A-Za-z0-9]", RegexOptions.Compiled); static readonly Regex extra = new Regex(@"[-]+", RegexOptions.Compiled); public static string PrepareURL(string str) { str = str.Trim().ToLower(); str = str.Replace("&", "and"); str = feet.Replace(str, "$1-ft-"); str = inch1.Replace(str, "$1-in-"); str = inch2.Replace(str, "$1-in-"); str = num.Replace(str, "num-$1"); str = dollar.Replace(str, "$1-dollar-"); str = percent.Replace(str, "$1-percent-"); str = sep.Replace(str, "-"); str = empty.Replace(str, string.Empty); str = extra.Replace(str, "-"); str = str.Trim('-'); return str; } } You could make this a SQL enhance function, or run URL generation as a separate process. Then to implement mapping, you would map the entire URL directly to a category ID. This approach is better in the long run for several reasons. First, you are not always generating URLs, you do this once and they stay static, you don't have to worry about your procedure changing, and then GoogleBot not being able to find old URLs. Also, if you get a collision, you may notice a potential duplicate category name, because a collision would only be different by special characters. Finally, you can always view your URLs from the database, without having to run the mapping function.
