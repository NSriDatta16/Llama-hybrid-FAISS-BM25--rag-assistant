[site]: crossvalidated
[post_id]: 520705
[parent_id]: 520693
[tags]: 
A couple of comments... First, the data are completely separable. There is a hyper plane in $(x_1, x_2)$ space which can completely separate the positive and negative case. This is bad and results in a logistic regression which does not converge model = glm(y~., data = train_set, family = binomial()) Warning messages: 1: glm.fit: algorithm did not converge 2: glm.fit: fitted probabilities numerically 0 or 1 occurred If you're looking for a better way to generate data, might I suggest this thread. None the less, you can use the model and plot the decision boundary. The decision boundary is where $$ 0 = \beta_0 + \beta_1 x_1 + \beta_2 x_2 $$ In $(x_1, x_2)$ space, that would be $$ x_2 = -\dfrac{\beta_0}{\beta_2} - \dfrac{\beta_1}{\beta_2}x_1$$ As you've correctly identified. Let's plot the training data and this line b = coef(model) slope = -b[2]/b[3] int = -b[1]/b[3] train_set %>% ggplot(aes(X1, X2, color = y))+ geom_point()+ geom_abline(aes(slope = slope, intercept=int), color = 'red') The plane seperates the two classes perfectly (which is why glm throws a complaint). Plotting the decision boundary and the test set results in a similar picture; the two classes are perfectly separated. So, aside from generating data which can not be fit by logistic regression, you've done everything perfectly. I'm not sure why you're experiencing a problem. You might have an error in your plotting code (I use ggplot2, not lattice).
