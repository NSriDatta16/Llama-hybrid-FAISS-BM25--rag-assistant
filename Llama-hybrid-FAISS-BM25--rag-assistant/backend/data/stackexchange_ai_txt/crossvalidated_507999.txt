[site]: crossvalidated
[post_id]: 507999
[parent_id]: 
[tags]: 
Why research on time series always use data with the same (instead of lower) level of precision to make forecast?

Suppose there is a dataset with data points for every 1 second: x1, x2, ..., x15 and suppose the following are the average values for every 5 seconds: y1 = (x1 + x2 + ... + x5) / 5 y2 = (x6 + x7 + ... + x10) / 5 y3 = (x11 + x12 + ... + x15) / 5 To forecast the value of y3 , as far as I know it's very common to use y1 and y2 to predict, instead of using x1, x2, ..., x10 . What is the reason for this? Is it because of high correlation (values should be all averages), or difficulty of using raw data (e.g. due to noise), or any other reason? I want to ask this question because I thought that using more information (from raw data before averaging) would provide higher forecasting accuracy.
