[site]: crossvalidated
[post_id]: 350376
[parent_id]: 350364
[tags]: 
Interesting question philosophically. I will give and answer, but I may be corrected mathematically. The idea is that you go into the experiment with a null hypothesis, or as my statistics professor used to say, "the boring and uninteresting result." SO, if you are trying to prove that smoking leads to earlier death, the null hypothesis is "smoking does not lead to earlier death", which means that the average age of a smoker is the same as a non-smoker if you draw them randomly from the real world. The statistics is about proving that the "boring" result does not appear to hold, because the odds of that happening given the data you looked at makes it highly unlikely it was just due to randomness that those averages are not close enough. Think of it this way: if you had exactly one smoker and non-smoker, you really don't have enough data to make such a determination (especially if the non-smoker was killed by a bus). But, if you have 10 million of each, you can have a lot of accuracy (or confidence), statistically speaking, if you don't get averages that are close to each other. I believe the alpha logic comes from coming into it. The question is, a priori, what level of confidence do you expect (or would be acceptable) to make you believe the null hypothesis was wrong? For one thing, that keeps you from designing the experiment in such a way that what you truly believe in your heart (and is consistent with the experiment you set up) is that "95% would really be convincing" doesn't lead to your analyzing the data, getting 92%, and then saying "90% makes it true". The level of significance changes by field. In some areas of particle physics, for example, where they record trillions of 'events' looking for something elusive, the standard is 5- or 6-sigma. Otherwise, you will find false positives all the time given the number of events. That is an example of how the experiment plays into it. On the other hand, when I did business consulting, we often presented regressions with 75% p-values. The logic there is that companies must make a lot of decisions, and a lot of decisions that have a p-value of 75% that get done quickly (and can be reversed if they prove wrong) is a lot more useful than proving a small number of things at 99% confidence. I'm not sure this is entirely helping you or answering what you are asking, but in practice (especially in a field like economics) people sort of do treat it as continuous. Any variables that come out of regression with a p-value of 95% feel really strong; any above 90% seem quite strong; but you wouldn't ignore a variable at 89%. In economics, one issue is that you don't really design experiments, and data can be a bit 'dirty' itself. Maybe another way to see this is lime this: suppose you are testing a diagnostic screening against a disease that is surely fatal, but the treatmen itself kills 10% of the people who get it whether or not they have the disease, but saves 100% of the people who have the disease (imagine Ebola, or AIDS in the early years). But suppose only 0.1% (one in 1,000) of the population is infected. If your test gives 5% false positives, and perfect actual positives, that means out of every 1000 people tested 1 will have the disease but 51 will test positive (the one real and 50 false). That means the drug will kill, out of every 1000 people tested, 5.1 people - only one of whom had the disease. Therefore, it would be better not to use you test because without it there will only be one death. Clearly, in that case, you would come in needing a much 'tighter' alpha a priori to justify proving that using the test is a good idea. That's all a bit sloppy technically, but I think it gets at what you are asking.
