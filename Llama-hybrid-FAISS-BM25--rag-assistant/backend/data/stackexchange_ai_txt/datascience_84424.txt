[site]: datascience
[post_id]: 84424
[parent_id]: 84423
[tags]: 
Yes, the loss of a normal autoencoder is simply the difference between the input image and the decoded image. While encoder and decoder have different names, they are effectively part of the same neural network, so the prediction error is backpropagated through the decoder up to the encoder. In more sophisticated autoencoders, like variational autoencoders, the loss may present extra terms, e.g. to enforce some structure in the output of the encoder.
