[site]: crossvalidated
[post_id]: 338995
[parent_id]: 
[tags]: 
Difference in parameters between grid summary and individual models

I was using h2o for getting a feel of what learning rate does a good job on my data set. I chose to use cross validation for selecting a learning rate. fit.gbm.grid = h2o.gbm(...learn_rate=seq(0.1, 0.5, 0.1), nfold=5, ....) When I looked at the summary table of the grid(ordered by increasing deviance(rmse^2)): fit.grid@summary_table %>% data.frame %>% select(learn_rate, residual_deviance) learn_rate residual_deviance 1 0.02 0.02523190479495407 2 0.01 0.025250571726902316 3 0.02 0.025250821218483113 4 0.03 0.025259080075877502 5 0.01 0.0252612284475692 6 0.01 0.025263035251822462 7 0.01 0.025263035251822462 8 0.04 0.025269296481300323 9 0.03 0.02526962790624404 10 0.02 0.02527191327680234 This suggests that a decent learning rate to start with is 0.01-0.02 I also plotted the individual model's score history, i.e. RMSE vs ntrees grouped by learning rate: getRMSEScoreHistory = function (models) { # Given a list of model ids get a data frame with its RMSE score history and the learning rates for the models that created it. score.history = data.frame() for (i in 1:length(models)) { lr = models[[i]]@parameters$learn_rate v_rmse= h2o.scoreHistory(models[[i]]) %>% data.frame %>% pull(validation_rmse) t_rmse= h2o.scoreHistory(models[[i]]) %>% data.frame %>% pull(training_rmse) score.history = rbind(score.history, data.frame( ntrees = 1:length(v_rmse), validation_rmse = v_rmse, training_rmse = t_rmse, id = i, lr = lr ) ) } score.history } And plotted the validation RMSE vs ntrees for each learning rate using the following code: models = sapply(FUN=function(m) h2o.getModel(m), fit.grid@model_ids) score.history.combined = getRMSEScoreHistory(models) min_y_lim = score.history.combined %>% pull(validation_rmse) %>% min max_y_lim = score.history.combined %>% pull(validation_rmse) %>% max ggplot(score.history.combined) + facet_wrap(~ lr) + geom_line(aes(y=validation_rmse, x=ntrees, color=as.factor(id))) + ylim(c(min_y_lim, max_y_lim)) + theme( legend.position="NULL", strip.background = element_blank() ) I get: The colors represent the different models within each cross validated model. I also plotted the training_rmse and got: Which indicates that the higher learning rate is achieving a lower RMSE, which is at odds with the summary table. So how is the summary table calculated from the individual models? And why is it different from the individual models' scoring history? PS: I had a large number of trees and early stopping enabled at a tree depth of 10. Cluster Info: > h2o.clusterInfo() R is connected to the H2O cluster: H2O cluster uptime: 1 days 50 minutes H2O cluster timezone: America/New_York H2O data parsing timezone: UTC H2O cluster version: 3.18.0.5 H2O cluster version age: 8 days H2O cluster name: H20Demo H2O cluster total nodes: 1 H2O cluster total memory: 2.42 GB H2O cluster total cores: 8 H2O cluster allowed cores: 8 H2O cluster healthy: TRUE H2O Connection ip: localhost H2O Connection port: 54321 H2O Connection proxy: NA H2O Internal Security: FALSE H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 R Version: R version 3.3.3 (2017-03-06)
