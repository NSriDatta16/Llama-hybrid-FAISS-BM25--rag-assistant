[site]: crossvalidated
[post_id]: 381680
[parent_id]: 
[tags]: 
Should I try to estimate logistic regression by F1 maximization, rather than Liklihood?

Assume I have a dataset of covariates $x_i$ and binary outcomes $y_i \in \{0,1\}$ . I want to predict outcome for unknown a unknown $y_k$ given $y$ . Quite common is to do this with logistic regression, so that $P(y_i = 1 | x) = \frac{1}{1+e^{-\beta'x + \epsilon}}$ and $\epsilon ~ N(0,\sigma^2)$ . The $\beta$ vector is usually found via Maximum Likliehood. That is well described all over Internet. You can then predict outcomes by defining some cutoff $c$ , and decide that the predicted outcome $\hat y = 1$ if $P(y = 1|x) > c$ , and $\hat y = 0$ otherwise. It is customary to choose $c = 0.5$ . To evaluate the model, and compare it to other models, one does not use the Likliehood. Instead, one quite often use the F1 score instead. One can adjust the cutoff $c$ to make a tradeoff between precision and recall in the predictor. It seems to me that one should use a Maximum F1-estimator instead, fitting both $\beta$ and $c$ . I have not found that approach anywhere online. Should I try to use that method for fitting a binary classifier?
