[site]: crossvalidated
[post_id]: 543418
[parent_id]: 
[tags]: 
How to handle retraining after model introduces bias

Here is my problem. I am retraining a machine learning model to detect fraudulent purchases. The training data is based on purchases and the target is whether or not they the purchase was considered fraudulent. The original model is logistic regression, performs pretty well with 8 significant features. This model is used in production to deny any purchases that have higher than 10% of being fraudulent. After a year the model needs to be retrained as it is performing worse and worse. When retraining based on newer data some of the original features are no longer significant. So here are my questions: How can you tell if the features losing their significance is due to the fact that the population has changed their behavior or the fact that we no longer see certain types of frauds since we use a filter? (our sample is no longer representative of the general population) How do you go about retraining a model where you know you introduce a bias but have no information on the outcome of the cases you have filtered away. Any help/pointers would be greatly appreciated. Edit: With the help of @seanv507 and @Ben-Reiniger I found good amounts of information. Sample bias, rejection bias are great things to search for. Some papers I found interesting were: "Shallow Self-Learning for Reject Inference in Credit Scoring" and "The Impact of Sample Bias on Consumer Credit Scoring Performance and Profitability".
