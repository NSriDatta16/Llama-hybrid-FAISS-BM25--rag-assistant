[site]: datascience
[post_id]: 88097
[parent_id]: 
[tags]: 
why do transformers mask at every layer instead of just at the input layer?

working thru the annotated transformer , I see that every layer in both the encoder (mask paddings) and decoder (mask padding + future positions) get masked. Why couldn't it be simplified to just one mask at the first layers of encoder and decoder?
