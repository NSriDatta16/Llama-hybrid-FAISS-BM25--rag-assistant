[site]: crossvalidated
[post_id]: 632221
[parent_id]: 630918
[tags]: 
This is not a complete answer, but two possible initial aveneues for an answer. There are two key concepts which I think are worth considering. Each LL value is obtained by re-sampling the original data. So by re-sampling the data we are able to obtain LL values which occur more often, as well as values which occur less often The peak of the distribution gives the most probable value While the LL value itself is arbitrary (we can add any constant to our LL value and obtain the same distribution of results, shifted by a constant) the distribution is meaningful, in particular making a comparison between the LL value obtained from data and the distribution obtained from bootstrap re-sampling is meaningful. If our observed data LL value lives inside a bin with small probability, then almost any possible re-sampling of the data will produce a more frequently occuring LL value. If our observed data LL value lives at the peak of the distribution, then almost no re-sampling produces a more probable value. Since the bootstrap samples themselves are generated from the data I am not sure exactly how significant the above two statements are. One possible consideration might be that if our data contains a small number of outliers which distort the LL value, then we might expect a re-sampling to remove those values, and therefore on average with high probability we will frequently obtain a bootstrap sample with smaller negative LL value. (Better fit to data.) This could be tested with some simple MC, and I will try to test it soon.
