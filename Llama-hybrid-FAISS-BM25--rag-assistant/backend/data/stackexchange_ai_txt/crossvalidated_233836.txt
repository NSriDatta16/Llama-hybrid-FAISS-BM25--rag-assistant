[site]: crossvalidated
[post_id]: 233836
[parent_id]: 
[tags]: 
In Bayesian linear regression, why do we assume the prior parameter w has zero mean?

Likely question: In Bayesian linear regression, why do we assume parameter prior has zero mean? . But, I just focus on can we assume $p(w \mid \alpha) = \mathcal{N}(w \mid \mu, \alpha^{-1})$, and the reason of zero mean. If mean of $w$ is $\mu$, we should minimum of $$\frac{\beta}{2} \sum_{n=1}^N\{y(x_n,w)-t_n\}^2+\frac{\alpha}{2}(w-\mu)^T(w-\mu)$$ so the result is not Ridge Regression (just my opinion). My Question : can we assume $w$ is not zero mean? Does the result of whether the mean of $w$ is zero lead to different result? Reason of ask the question: I didn't find the answer in the question above. I also want to know the reason \ undermeaning of assume zero mean. This problem is from book PRML 1.2.5 Curve fitting re-visited Page28. Apologize for not being word-perfect in English.
