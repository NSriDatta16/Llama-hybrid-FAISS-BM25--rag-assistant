[site]: crossvalidated
[post_id]: 254039
[parent_id]: 
[tags]: 
Model developed by optimizing one measure of performance, but assessed by another

I'm trying to predict a binary outcome - let's call it "Cancer/no cancer" after this example here . So, $a$ refers to cancer correctly diagnosed as such, and so on. Using logistic regression I can find the parameters $\theta$ that will minimize the cost function: $J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$, where $h_\theta (x^{(i)})$ is the model prediction, $y^{(i)})$ is the outcome, and $m$ is the number of training examples. Following Andrew Ng's Coursera subject on machine learning I have no problem writing a MATLAB function that finds appropriate parameters: function [J, grad] = costFunction(theta, X, y) %COSTFUNCTION Compute cost and gradient for logistic regression % J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the % parameter for logistic regression and the gradient of the cost % w.r.t. to the parameters. m = length(y); % number of training examples J = 0; grad = zeros(size(theta)); the_hypothesis = sigmoid(X*theta); first_part = y'*log(the_hypothesis); second_part = (1-y)'*log(1-the_hypothesis); J = -(1 / m) * sum(first_part + second_part); grad = (1/m) * ((the_hypothesis-y)'*X); end I'm now trying to apply this stuff to my own research area. However, ultimately my models won't be judged in relation to this cost function. Instead they'll be judged in relation to a "skill score" of a type traditionally applied in my research area. Here are some examples of skill scores: What I really want is to find parameters that will maximize whatever skill score I need to deal with. So let's say I get told "you're going to be judged by the Peirce Skill Score", then I'd just edit my cost function and run my model, and it'd produce parameters that would maximze that particular skill score. However, it wasn't apparent to me how I could do that. Daydreaming about this problem I came up with a lame solution. I was imagining some set-up where I split up the data into training/cross-validation/test sets, and then do the following: 1. Training phase. We find the best $\theta$ values according to the logistic regression cost function. 2. Cross-validation phase. We apply various threshold values to probabilities predicted by those $\theta$s and see how this affects performance on the relevant skill score. We might, for example, find that skill score performance is best when model-forecasted probabilities $\geq 0.45 $ are converted to $1$, while those $ 3. Test phase. We apply the $\theta$ and threshold values found in the previous two phases, and apply them to the test data, hopefully achieving good skill score performance. However, what would be really cool if there was some way to just edit my cost function, and make it directly maximise the skill score I am assigned. Ideally I'd just edit my logistic regression cost function with a new cost function that matches the skill score I'm using, and which doesn't run into any computation problems (e.g. local optima). EDIT: Thanks to @deltaiv's answer, I realise I misspoke here - I understand that I'd need to change the model as well as the cost function. Is there some model and cost function that 'matches' a skill score I might be interested in? Is it realistic to think there's a model (and associated cost function) that would come up with parameters that directly maximize one of those skill scores I'm interested in? So maybe this hypothetical model would produce probabilities and a threshold, or maybe it would produce probabilities that optimize the skill score when some standard threshold like 0.5 is applied to it, or maybe it would just produce binary forecasts of 0 and 1 . I'm completely open to the possibility that such a model is not feasible - but why is it not feasible? If such a model is unfeasible (or wouldn't work well), what should I do?
