[site]: datascience
[post_id]: 31257
[parent_id]: 
[tags]: 
Keras custom loss using multiple input

I have a model in keras with a custom loss. this loss is calculated using actual and predicted labels(or values) and is also based on some input value . (an example would be to define loss based on reward or advantage as in a policy gradient method in reinforcement learning context ) example code: note: the input value used in loss calculation is named 'in2' and variable is input_2 input_1=keras.layers.Input(...,name="in1") output= keras.layers.Dense(...)(input_1) input_2=keras.layers.Input(...,name="in2") def loss_func(y_true,y_pred): # some custom loss i define based on input_2 loss = keras.layers.Dense(..)(input_2) return loss my_model=keras.Model(inputs=[input_1,input_2],outputs=output) my_model.compile(...,loss=loss_func,) Now the problem is that when I want to just predict an output, I have to give a dummy input for the unused input_2 . The input_2 is only used for computing the loss and it is not used for computing the model output. but when i use this code, an error ocures for not providing 'in2': # this throws error for not providing input value for 'in2' my_model.predict({'in1':some_value}) so I have to provide some dummy value for in2 so the model outputs its value, although it does not use the in2 value provided. note that providing None value does not work either. I thought this functionality is implemented inside keras. or maybe I am doing it the wrong way? if yes, what is the appropriate way? idea: An idea came to my mind was that maybe we can define two models. one model for training and another for predicting: my_model_for_training = keras.Model(inputs=[input_1,input_2],outputs=output) my_model_for_predicting = keras.Model(inputs=[input_1],outputs=output) and when I want to predict, I just use the second model which does not require the input_2 (or in2 ) as in example. both models share the same layers so when train the first model, the weights are updated for both models as explained in docs (section: Shared layers) . is there a better solution?
