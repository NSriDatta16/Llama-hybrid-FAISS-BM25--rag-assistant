[site]: crossvalidated
[post_id]: 368601
[parent_id]: 368566
[tags]: 
Like @amoeba, I don't understand your difference between Q1 and Q2: in any case, LDA obtains at most $k-1$ dimensions. For $k=2$ that's one dimension, for $k=8$ it would be 7. And of course, if the input space has lower dimensionality that that's the limiting factor. One idea/heuristic behind LDA is that separating classes is easy if they are all spheres of the same size: all you then need is distance to the class means. Alternatively, you can say that the connection between class means are normal vectors for suitable separation planes. In that narrative, you can look at LDA as a projection that in the first place does singular value decomposition (i.e. something very similar to PCA) of the pooled covariance matrix in order to get a space with such spherical classes. Let's call this our primary score space. Depending on the customs of your field, this is either the PCA score space of the pooled covariance matrix = the PCA score space of the data matrix centered to the respective class means, or the PCA score space squeezed according to the eigenvalues/SVD diagonal matrix (some fields by default put this scaling for PCA into the loadings, others [mine] into the scores). The primary score space still has the dimensionality corresponding to the rank of the pooled covariance matrix/data matrix centered to the respective class means. Pooled covariance matrix in this space is a unit sphere, i.e. each class is considered as a unit-sphere centered at the respective class mean, the means also projected into score space. (At this point, we could derive other classification algorithms that use the SVD projection as heuristic but don't exploit/rely on the assumed unit-sized spherical shape of the classes.) Now, as we have unit-spherical class shapes, the only thing remaining to care about are the class means. I.e., $k$ points. Even if our primary score space has higher dimensionality, the $k$ points will form a $k-1$ dimensional shape (simplex). Thus without loss of anything, we can further rotate our primary score space so that our $k-1$ -simplex of class means lies in the first $k-1$ dimensions. For the postulted spherical classes, all further dimensions cannot not help with the distinction: the classes have exactly the same size and position in these further dimensions. Thus, we throw them away. (Again, you may derive another classifier that uses the first two projection steps but then keeps [some of] those further dimensions as the classes in practice may not be spherical.) One classifer to look into when comparing LDA and PCA is SIMCA which can be seen as a one-class classifier analogous to LDA. In SIMCA, you'll find the notion of in-model space and out-of-model space: in in-model space you detect changes of the same type of the usual variation within your class (but possibly of unusual magnitude). In out-of-model space you detect variation of a type that does not usually occur within any of the classes. For our description of LDA, the $k-1$ dimensions we keep would be the in-model space, whereas the remaining dimensions are the out-of-model space. Interpretation will be slightly different compared to SIMCA, but you should have a start for your thoughts with this.
