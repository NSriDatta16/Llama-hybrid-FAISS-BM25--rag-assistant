[site]: datascience
[post_id]: 93414
[parent_id]: 
[tags]: 
BERT MLM overfitting

We are training the BERT model on masked language modeling task for the Russian Language. Our dataset consists of 60 mln texts with (128 tokens for each text) from online social networks, predominantly in the Russian language. We have not performed any text preprocessing. We use AdamW optimizer and RuBERT as a pre-trained model. The training parameters are the following: Learning rate: 1**e-5 Warmup: 30 000 Seq. len: 128 The charts below show the cross-entropy(CE) loss function for test evaluation as iterations go on. One iteration contains 100 batches, One epoch contains ~ 3500 iterations. The blue chart shows loss function when we mask only Cyrillic tokens (Cyrillic BERT). The orange chart shows loss function when any token (including punctuation, URLS, numerals, English letters) can be masked and it is the only difference among the two charts (Default BERT). My questions are the following: Why "Cyrillic BERT" overfits dramatically after the ~2200 iteration while the "Default BERT" oscillates near 1.67 at the same time? What Bert config parameters may change it? Why The "Default BERT" has no improvements does not improve after the ~3000 iterations? How can I improve it? (already trying to change hidden_dropout_prob to 0.2 from 0.1) Does "Cyrillic BERT" shows worse loss function because it is easier to predict punctuation and several English names?
