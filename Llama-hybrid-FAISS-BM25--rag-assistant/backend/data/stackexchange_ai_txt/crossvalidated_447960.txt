[site]: crossvalidated
[post_id]: 447960
[parent_id]: 447775
[tags]: 
"denote this information X, and trivially assign probability 1.0 to Heads and 0.0 to Tails." You assign a probability to the frequency, the property how often the coin will flip heads/tails, and not to individual events that have occurred with the coin. Even when an event has already occurred we may assign a probability to it (the coin) that differs from that single event. Yes, you could say, when I have won a lottery then in hindsight my chances to win that lottery were 1.0 (it was written in the stars), and you might argue that in a deterministic world every single coin flip is fixed to 0 or 1. But, that's not the way to think about it. In a deterministic world nothing may be truly random in principle . But while God knows exactly how the dice roll (although that is debated), we do not know all information, and in practice we need to work with probability. (you could say that probability is an expression of lack of information) We have no way to compute all the factors, influences of wind, variable rotation speed each flip, spinning on the floor, different starting positions, etc. in order to know whether the coin flips head or tails. Therefore we model the coin as a random variable. Even afterwards, when the coin has already landed and we know how the (in principal) deterministic event unfolded, we can still treat it as a result that was from the practical point of view a random event. "When I see a head landed first, I would naively guess that the next one to be Tail." This is the Gambler's fallacy . If the coin is fair (50/50) then it is just as probably to flip heads-heads as heads-tails, so after you flipped heads you should you expect with equal probability tails as heads ( if the coin is fair). However, in relation to Bayesian statistics, you may not be certain whether the coin is fair and have some knowledge about distributions of fair/unfair coins. In that case you have heads-heads occurring slightly more often than heads-tails (because not all coins are fair and there is a slight chance to encounter coins that fall relatively more often on the same side than mixed). Thus, in fact, if a coin lands heads then we should actually expect the next flip to be slightly more probable to land heads as well (although in most practical settings this is only slight). Thus it is the opposite from "I would naively guess that the next one to be Tail". In most practical settings (with coins) this is only slight, but you could create examples where it is more obvious. For instance , imagine the case where you have a mixture of fair and unfair coins for which the probability of an unfair coin is reasonable high. This makes me think of an interresting didactical method to teach students learn Bayesian statistics. As a sort of practical course, let them play gambling games but with dice among which some predetermined fraction of the dice are slightly unfair but the students do not know which dice are unfair. To spice it up the outcome will determine the mark of the student. For a practical example that illustrates the updating of probabilities see: Probability of a two-headed coin given a few sample flips? (and there are many others of this type: https://www.google.com/search?q=urn+fair+unfair+coins+site:math.stackexchange.com )
