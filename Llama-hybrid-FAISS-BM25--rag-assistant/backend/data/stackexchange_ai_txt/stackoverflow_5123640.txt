[site]: stackoverflow
[post_id]: 5123640
[parent_id]: 5118595
[tags]: 
2 million rows in 6,5 hours? How large is the data set you are storing? I use the following back-of-the-envelope calculation to arrive at a somewhat useful figure: Assuming 1 single crappy disk that swallows 35 mb per sec, you should be able to write (35 * 6,5 * 3600) = ~ 800 gb in that time frame. Calculating backwards (800 gb / 2 mrows), gives an average row size of 400 kb. If those numbers seem about right, you need to beef up your hardware to increase speed. If they are completely off, there is likely some other problem. Also, have a look at comparisons of disk i/o for a dedicated MySQL server on ServerFault, for a way of measuring I/O. Here are some random suggestions (in case you suspect some other problem) Make sure you eliminate all row-by-row operations in your loading process If most of the csv data end up being stored, consider bulk loading into intermediate tables and process the data inside the database using set-based processing. If most of the data is discarded, consider moving/caching your reference tables outside of the database to be able to filter the csv data in your C-code MySQL don't have hash joins, but rely on indexed loops. Make sure those other tables have approproate indexes Try pre-sorting the data outside of the database to match the index of some other table used in the process (to increase the likelyhood the relevant data doesn't get flushed out of cache) Read up on partitioning , and see if you can replace some of your indexes with a smart partitioning scheme instead of maintaining all those indexes. Edited Corrected calculation (400kb)
