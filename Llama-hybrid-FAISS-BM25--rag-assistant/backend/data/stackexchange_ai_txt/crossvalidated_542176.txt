[site]: crossvalidated
[post_id]: 542176
[parent_id]: 542172
[tags]: 
I think your intuition is right; moving from $\mathbb{R}^n$ to an affine parameter along a space-filling curve will discard information about what points are close to one another in the high-dimensional space. Points in the same neighborhood can be separated by arbitrarily large distances along the curve. Consider, as an example, a problem where your prediction targets lie in a compact region in $\mathbb{R}^n$ . Your machine learning task is to find a way to characterize that region. In the space-filling curve representation, the curve likely dips in and out of that region for an infinite number of ranges of the affine parameter $\lambda$ . Finding these segments of the curve is not only much harder than finding the boundaries of the region in $\mathbb{R}^n$ , it is likely impossible because you probably have arbitrary large $\lambda$ that lie in the region. Your generalization error will be terrible, since any new case that lies along a segment of the curve you haven't explored yet will generate a missed prediction, even if it differs imperceptibly from a training point in the $\mathbb{R}^n$ representation. Dimension reduction does have a place in machine learning, but the trick is to discard dimensions that are not providing useful information for your prediction problem. Just forcing everything into one dimension using a construct like a space-filling curve doesn't accomplish that.
