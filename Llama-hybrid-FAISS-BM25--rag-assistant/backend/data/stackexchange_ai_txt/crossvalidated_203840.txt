[site]: crossvalidated
[post_id]: 203840
[parent_id]: 
[tags]: 
Measure quality of out-of-sample extension for spectral embedding methods (Laplacian eigenmaps)

For my work, I analyse a rather new method for out-of-sample extension of spectral embedding methods (mostly Laplacian Eigenmaps). Instead of just providing a discrete embedding for every data point, based on a training set a function is generated, which only needs to be evaluated for out-of-sample points. In the papers I considered (e.g. the one by Bengio et al proposing a Nystr√∂m-based out-of-sample extension) very different methods are used to prove quality and performance of such out-of-sample-extension. Bengio measures the difference between some sort of "training set variability" and the out-of-sample-error obtained via leave-one-out-cross-validation. Gisbrecht et al simply apply ten-fold cross-validation, while another paper uses a mean adjusted rand index of several test sets out-of-sample data after performing some clustering on the data. Question: Is there any up-to-date concept, which method is the most useful for such a quality/performance measurement, or is it left to the researcher? (I am considering embedding specificially; results for clustering would be interesting, too, as mostly my embedding is a preprocessing step for clustering, but would not solve the problem.)
