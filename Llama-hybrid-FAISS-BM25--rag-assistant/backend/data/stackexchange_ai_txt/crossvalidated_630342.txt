[site]: crossvalidated
[post_id]: 630342
[parent_id]: 
[tags]: 
Proof of estimation error increases (logarithmically) with hypothesis class size for a finite hypothesis class

In section 5.2 error decomposition (p.404) from the book "Shai et al., Understanding Machine Learning: From Theory to Applications ", the authors wrote: As we have shown, for a finite hypothesis class, $\epsilon_{est}$ increases (logarithmically) with $|\mathcal{H}|$ and decreases with $m$ . Where $\epsilon_{est}$ denotes the estimation error, $\mathcal{H}$ is the hypothesis class and $m$ is the size of the training set. But I couldn't find any proof of this statement anywhere in the book, so I've tried to construct one. I can deduce the second part of the statement, i.e the estimation error decreases with $m$ but I'm still struggling with the first part to show that the error increases logarithmically with $|\mathcal{H}|$ .
