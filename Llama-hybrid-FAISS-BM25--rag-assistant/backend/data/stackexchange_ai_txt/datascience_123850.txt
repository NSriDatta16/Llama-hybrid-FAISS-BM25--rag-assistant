[site]: datascience
[post_id]: 123850
[parent_id]: 
[tags]: 
Expanding torch dataLoader by repeating elements (or something equivlent)

I have a pytorch classifier with stochastic elements. For each MNIST minibatch, I would like to feed it in K times (from the same starting state), and then average the resulting weight updates in order to average out some of the noise due to randomness. What is the correct way to do this? My first thought was to expand the dataLoader by repeating every element K times (which I don't know how to do), and then also increase the batch size by K. But maybe there is a better method. I'm somewhat new to pytorch.
