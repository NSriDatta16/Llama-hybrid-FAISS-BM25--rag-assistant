[site]: crossvalidated
[post_id]: 305434
[parent_id]: 305430
[tags]: 
One answer is that maximizing variance minimizes squared error â€“ a perhaps more immediately plausible goal. Assume we want to reduce the dimensionality of a number of data points $\mathbf{x}_1,\cdots, \mathbf{x}_N$ to 1 by projecting onto a unit vector $\mathbf{v}$ , and we want to keep the squared error small: $$\underset{\mathbf{v}}{\text{minimize}} \, \sum_{n = 1}^N \left\|\mathbf{x}_n - (\mathbf{v}^\top \mathbf{x}_n)\mathbf{v}\right\|^2 \text{ subject to } \|\mathbf{v}\| = 1$$ This optimization problem can be turned into the equivalent problem $$\underset{\mathbf{v}}{\text{maximize}} \,\, \mathbf{v}^\top \mathbf{C}\mathbf{v} \text{ subject to } \|\mathbf{v}\| = 1,$$ where $\mathbf{C} = 1/N\sum_{n = 1}^N \mathbf{x}_n\mathbf{x}_n^\top$ . I.e., minimizing squared error is equivalent to maximizing variance along the direction of $\mathbf{v}$ (for centered data). Another answer is that PCA is trying to fit a Gaussian model to the data ( squared error and the Gaussian model are closely related ). If you tried to fit another model to your data, you'd observe other moments as well (e.g., kurtosis becomes important when fitting a model via independent component analysis ).
