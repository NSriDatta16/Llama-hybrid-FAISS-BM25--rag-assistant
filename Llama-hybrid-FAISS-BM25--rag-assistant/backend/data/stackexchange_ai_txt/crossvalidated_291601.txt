[site]: crossvalidated
[post_id]: 291601
[parent_id]: 246829
[tags]: 
$Eout=P((\hat f(x)≥0.5)≠y) $ is rarely used. You're assuming that 0.5 is a useful decision threshold. What if all of $\hat f(x)$ is above 0.5, or all below? Because $\hat f(x)$ is continuous, you can set this threshold in many different ways. At one, true positive rate is maximized. At the other, false positive rate is maximized. Somewhere in between F1 is maximized. Which is best, really depends on your application. For classification problems, AUC and Average Precision scores ingrate over all decision thresholds to give you a single number. However these have their own problems (as do TPR and FPR). AUC is a terrible metric when class population size are imbalanced. But to answer your specific question, which I understood as, how can I evaluate my algorithm without being 'polluted' by train data. Cross validation is the gold standard here. You separate the train/test into N non-overlapping pairs of train and test. The trainer never see the test data. You end up with k models and k independent tests. In the extreme case, you can set the test set size=1, and train on the rest. If you assume this tiny difference in training data has little effect on the model, your test set size can be as large as your training set size (although small test sets aren't suitable for calculating AUC). Confidence intervals on TPR, FPR, AUC can also be calculated and are useful. See: binomial CI and AUC CI
