[site]: crossvalidated
[post_id]: 538468
[parent_id]: 536317
[tags]: 
We have established in the comments that you are not really looking for scoring-rules to assess predictive densities , but for error measures to assess point predictions. One tool that forecasters very often use and that would probably be helpful to you would be to use the Mean Squared Error (MSE), as you are doing - but using it in a relative way against a benchmark. So you really want to check whether your prediction improves on this benchmark. For instance, if your outcomes are always constrained to be between 0 and 1, then one very simple benchmark prediction would be a flat 0.5 across all pixels. So you would calculate the MSE of your predictions, as you have been doing (calculating squared errors per pixel, averaging across pixels). In the second step, you would calculate the MSE of this very simple benchmark prediction. In the third step, you would divide your prediction MSE by the benchmark MSE. If this ratio comes out greater than one, then you did worse than the very simple benchmark. Note how this approach indeed penalizes cases where your prediction is on the wrong side of 0.5, compared to the actual: if the actual is 0.8 and you predicted 0.3, then the flat prediction of 0.5 across all pixels would indeed have been better. Another, possibly even better, benchmark would be to take the average across all pixels in your training data. Maybe your pixels are all between 0 and 1 - but the average is 0.4, not 0.5. In this case, a "naive" prediction would also rather be 0.4. (Forecasters like to use exactly this kind of "naive" forecast, the historical mean, as a sanity check. If your extremely sophisticated method cannot even beat this simple benchmark, you really don't have anything to be proud of. You would be surprised to see how often this happens.)
