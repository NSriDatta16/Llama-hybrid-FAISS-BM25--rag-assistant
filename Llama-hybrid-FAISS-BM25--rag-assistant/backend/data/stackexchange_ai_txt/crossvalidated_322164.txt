[site]: crossvalidated
[post_id]: 322164
[parent_id]: 
[tags]: 
What are the differences between contextual bandits, actor-citric methods, and continuous reinforcement learning?

Let's imagine we have a blackbox function f(X) -> y which we don't know. X is a vector of 10 continuous variables, which we want to optimize to reduce y output (a continuous reward with unknown min and max). Also, f(X) maps a different optimum y depending on some other continuous parameters Z , which are observable. Each day we can test a new X combination, and at the end of it we know the result. I Could find out three posible solutions to this problem: The contextual multi-armed bandit problem solvable with Bayesian Optimization Variants of continuous reinforcement-learning/Q-learning Actor-critic methods In the case when some of Z values are influenced by X (let's say only 2 non-related binary values), and other are external (3 real valued), which RL algorithm would fit better this problem, and how would I add these independent Z values to take them into account? If all Z values are extenral, is multiarmed bandits the best approach?
