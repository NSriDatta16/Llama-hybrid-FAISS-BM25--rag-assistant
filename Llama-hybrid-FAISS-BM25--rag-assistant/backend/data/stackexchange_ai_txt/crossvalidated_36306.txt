[site]: crossvalidated
[post_id]: 36306
[parent_id]: 36298
[tags]: 
Roughly speaking, some of the potential over-fitting that might happen in a single tree (which is a reason you do pruning generally) is mitigated by two things in a Random Forest: The fact that the samples used to train the individual trees are "bootstrapped". The fact that you have a multitude of random trees using random features and thus the individual trees are strong but not so correlated with each other. Edit: based on OP's comment below: There's definitely still potential for over-fitting. As far as articles, you can read about the motivation for "bagging" by Breiman and "bootstrapping" in general by Efron and Tibshirani. As far as 2., Breiman derived a loose bound on generalization error that is related to tree strength and anti-correlation of the individual classifiers. Nobody uses the bound (most likely) but it's meant to give intuition about what helps low generalization error in ensemble methods. This is in the Random Forests paper itself. My post was to push you in the right direction based on these readings and my experience/deductions. Breiman, L., Bagging Predictors, Machine Learning, 24(2), pp.123-140, 1996. Efron, B.; Tibshirani, R. (1993). An Introduction to the Bootstrap. Boca Raton, FL Breiman, Leo (2001). "Random Forests". Machine Learning 45 (1): 5â€“32.
