[site]: crossvalidated
[post_id]: 203759
[parent_id]: 120180
[tags]: 
I agree with whuber. You should have some methodology, for example bootstrapping and stability criterion (that is perturb your data a little bit and check that your estimates do not change too much), or better sharp theoretical results, to help you decide if you have enough samples to compute your estimates correctly. Then, you should use the minimum amount of data that provides you strong guarantee (theoretical or empirical) on the relevance of your results, but no more as you will smooth the signal (or violate even more the stationary hypothesis). As long as you stay with estimating mean, variance, and so on, you should be able to find theoretical results and guidelines. If you want to determine the minimum length of the window for a complex processing (say machine learning algorithms), you should go for an empirical study, cf. this study for an example on the clustering of correlated random variables.
