[site]: datascience
[post_id]: 8866
[parent_id]: 8860
[tags]: 
Bagging and dropout do not achieve quite the same thing, though both are types of model averaging. Bagging is an operation across your entire dataset which trains models on a subset of the training data. Thus some training examples are not shown to a given model. Dropout , by contrast, is applied to features within each training example. It is true that the result is functionally equivalent to training exponentially many networks (with shared weights!) and then equally weighting their outputs. But dropout works on the feature space, causing certain features to be unavailable to the network, not full examples. Because each neuron cannot completely rely on one input, representations in these networks tend to be more distributed and the network is less likely to overfit.
