[site]: crossvalidated
[post_id]: 582459
[parent_id]: 582452
[tags]: 
The problem is that in your training data, the "positive" class (since recall is the problem, I guess "fraud" is the "positive") is underrepresented and the algorithm has not enough data to pick up on this signal. There is even a valid easy strategy for it: It can get very high accuracy by only predicting the abundant class per default, since it will be wrong only very seldom. What you can do now is to over- or undersample your data to make the classes artificially more equal. There are numerous techniques for that (see here ), plus you can also augment the positive samples that you use. There is also the option to weight your samples, meaning that positive samples get a very stronger influence on the error metric (scaled up relative to its reciprocal proportion in the data) assigned for backpropagation compared to the abundant class. For Keras this looks loke this . The alternative is to use another model (for outlier detection, sometimes autoencoders are employed ( here and here )).
