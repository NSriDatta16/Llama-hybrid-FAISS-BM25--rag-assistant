[site]: crossvalidated
[post_id]: 499978
[parent_id]: 499976
[tags]: 
Yes I'm not sure how this assesses the assumption. One way to assess the linearity assumption is to check the deviance residuals. If the outcome is 0/1 you will have to group the variables in an intelligent way so that the outcome is binomial rather than bernoulli. Here is an example. In the following code, I purposefully create a non-linear logistic regression. I fit a model with only a linear term and evaluate the deviance residuals. The residuals display a non-linear pattern where they should look like a cloud around 0. This is evidence the fit is poor. library(tidyverse) library(broom) set.seed(0) N = 10000 x = rnorm(N, 0, 2) eta = -0.4 + 0.2*x - 0.1*x^2 p = plogis(eta) y = rbinom(N,1,p) d = tibble(x, y) %>% # bucket the continuous predictor into deciles mutate(q = ntile(x, 10)) %>% group_by(q) %>% # n- how many onbservations in the bucket, y - how many successes summarise(n = n(), u = sum(y)) model = glm(cbind(u, n-u)~q, data = d, family=binomial()) d $fitted = predict(model) d$ resid = resid(model, type = 'deviance') d %>% ggplot(aes(fitted, resid))+ geom_point() See how the residuals look U shaped? That means the linearity assumption is likely incorrect. If you have good reasons to include an interaction or higher order terms, you may do so. Else, you could use splines. Here is an example library(splines) model = glm(cbind(u, n-u)~bs(q), data = d, family=binomial()) d $fitted = predict(model) d$ resid = resid(model, type = 'deviance') d %>% ggplot(aes(fitted, resid))+ geom_point() ] Note how the deviance residuals are clustered around 0 now, with no discernible pattern.
