[site]: crossvalidated
[post_id]: 28950
[parent_id]: 28944
[tags]: 
Outlier detection in time series encompasses a large body of literature. First you would want to have a time series model that fit well to the data when there were no suspect observations. If for example an ARMA model works you might assume that the noise distribution is Gaussian. There are at least two types of outliers. Fox defined them in a 1972 paper. The best source to start with on this subject is the latest edition of Barnett and Lewis' "Outliers in Statistical Data" published by Wiley. They have a chapter on time series. My 1982 paper with Downing took the approach of looking at influence functions for autocorrwlation. Our idea is that if an observation had a big effect on one of more of the lagged correlations it would also affect the model parameters adversely. Martin, Yohai and others defined influence functionals for time series in a different way that seems to have better theoretical justification but addresses the same issue . Ruel Tsay, George Tiao and others have also published work on outliers in time series. I am less familiar with that. But our colleague IrishStat can probably comment on that and more. In the process of improving his autobox software over the years IrishStat and his son Tom have invested time into keeping up on the literature about outliers and level shifts (sometimes called interventions) in order to make their product state-of-the-art. Just like with outliers in data that are not time dependent any outliers that are detected using time series methods should be studied to see why they occurred. Were they measurement errors? Maybe a change in the behavior of the process? Maybe a temporary intervention (like the Federal Reserve changing interest rates as an example)? The reason if it can be found will dictate how the outlier should be treated.
