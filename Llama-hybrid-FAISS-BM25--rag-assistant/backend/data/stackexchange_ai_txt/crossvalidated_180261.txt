[site]: crossvalidated
[post_id]: 180261
[parent_id]: 180019
[tags]: 
Weight decay is an alteration to backpropagation, seeking to avoid overfitting, in which weights are decreased by a small factor during each iteration. From Mitchell Machine Learning , p. 111: This is equivalent to modifying the definition of $E$ [error function] to include a penalty term corresponding to the total magnitude of the network weights. The motivation for this approach is to keep weight values small, to bias learning against complex decision surfaces. Weight sharing is when different units in the network literally share the same weights: They use identical values, "usually to enforce some constraint known in advance to the human designer." (Ibid. p. 118)
