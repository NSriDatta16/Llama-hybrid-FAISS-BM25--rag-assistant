[site]: stackoverflow
[post_id]: 2459571
[parent_id]: 2458296
[tags]: 
350,000 tasks * 48 times per day is 16,800,000 tasks executed per day. To schedule the jobs, you don't need a database. Databases are for things that are updated . The only update visible here is a change to the schedule to add, remove or reschedule a job. Cron does this in a totally scalable fashion with a single flat file. Read the entire flat file into memory, start spawning jobs. Periodically, check the fstat to see if the file changed. Or, even better, wait for a HUP signal and use that to reread the file. Use kill -HUP to signal the scheduler to reread the file. It's unclear what you're updating the database for. If the database is used to determine future schedule based on job completion, then a single database is a Very Dad Idea. If you're using the database to do some analysis of job history, then you have a simple data warehouse. Record completion information (start time, end time, exit status, all that stuff) in a simple flat log file. Process the flat log files to create a fact table and dimension updates. When someone has the urge to do some analysis, load relevant portions of the flat log files into a datamart so they can do queries and counts and averages and the like. Do not directly record 17,000,000 rows per day into a relational database. No one wants all that data. They want summaries: counts and averages.
