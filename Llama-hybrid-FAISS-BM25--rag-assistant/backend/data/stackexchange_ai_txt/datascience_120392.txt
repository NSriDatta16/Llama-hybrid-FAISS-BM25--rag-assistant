[site]: datascience
[post_id]: 120392
[parent_id]: 
[tags]: 
Training a network on time-series data with very long data

This question has come up a few times, but I haven't seen a lot of good answers. I have data where I have about 1000 samples and 3 time-series data for each sample. The time-series data is extremely long though (about 300000 time points per series). There is a lot of information per individual sample and it is highly over-sampled. I want to classify each sample into a few discrete categories. I could subsample the series and use tsfresh or something, but I was hoping to train a RNN, CNN, or dilated-CNN to get better results and learn some interesting patterns in the data relating to each category. Because of the oversampling, I had the idea to break every individual into, for example, 100 subsamples and use them as the new samples, giving me 100000 samples as input. The issues is that any network is likely to just learn the idiosyncrasies of each individual and just tease apart the original 1000. That is, it just learns the individual patterns and not something particular to the category. I have verified this with some basic CNNs, where I can get super high accuracy on the training set, but the validation set has better than chance, but does not converge to any significant accuracy. Any ideas on this? Perhaps a loss function that penalizes learning that the data comes from the same person? Classic regularization techniques don't do much because it ultimately doesn't focus on the correct problem.
