[site]: crossvalidated
[post_id]: 434135
[parent_id]: 434125
[tags]: 
Why can't algorithms avoid overfitting themselves? They can. If you design an algorithm that implements model selection based on cross-validation or information criteria, you should achieve a good balance between overfitting and underfitting. What I don't understand is why this requires a human to hide data from the algorithm. Doesn't this imply that you don't have the best algorithm to begin with if extra information will make your model worse? In the above setup, all information would be used for selecting the model and all or some information would be used for estimating it. (All - if the selected model contains all variables, some - if the selected model excludes some variables. See also the note on ridge regression and $L_2$ regularization below.) I know that you could program the computer to compute the AIC/BIC/etc and hide info from itself, but it still seems like more information shouldn't make models worse. Consider AIC- or BIC-based model selection, e.g. in the context of multiple linear regression. All of the data is used for training, yet the selected model (under some assumptions) offers an optimal (in a well-defined sense which differs for AIC vs. BIC vs. ...) balance between over- and underfitting. Why don't we have algorithms which can produce models involving all the variables, even those whose effect size is too small to be reliably detected from the sample, and estimate a coefficient for them without overfitting and making the fit on the holdout set worse? Variable selection is just one way of fighting overfitting; regularization is another, e.g. ridge regression or a neural network trained using $L^2$ regularization. When using regularization, the intensity can be determined via cross validation or via information criteria. There again, all data is used for training.
