[site]: crossvalidated
[post_id]: 383129
[parent_id]: 
[tags]: 
Using word embeddings in text classifier

I have a bunch of sentences that I want to do binary classification with SVM. My sentences have varying lengths form 4 to 34. If I use word embeddings such as word2vec or skip gram to convert my words into word vectors, I would end up with matrices of very different sizes due to differences in sentence length. What is the best way to get around that? I know that if I were to use a neural network classifier, I would just pad with zeroes and let the neural network figure out the features. But If I were to use a classical machine learning classification method, what is the best way to deal with sentences of varying lengths?
