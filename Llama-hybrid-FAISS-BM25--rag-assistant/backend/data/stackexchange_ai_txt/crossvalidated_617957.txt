[site]: crossvalidated
[post_id]: 617957
[parent_id]: 
[tags]: 
Bayesian Gaussian mixture - is my prior correct?

I'd like to sample from the Bayesian Posterior of a Gaussian mixture model, but I am not sure about the correct Bayesian formulation of the latter. Is the following correct? I consider the 1-dimensional setting and want to work with a $K$ -component mixture. I start with artificially generated data and want to learn the known parameters of the GM from the data. For chosen $\boldsymbol{\mu}:=(\mu_1,...\mu_K)$ , $\boldsymbol{\sigma^2}:=(\sigma_1^2,...,\sigma_K^2)$ , and $\boldsymbol{\phi}:=(\phi_1,...,\phi_K)$ , I generate the data from the likelihood $$ p(x|\boldsymbol{\mu},\boldsymbol{\sigma},\boldsymbol{\phi})=\sum_{i=1}^{K}\frac{\phi_i}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(x-\mu_i)^2}{2\sigma_i^2}}$$ with $\sum_i \phi_i=1$ . $x\in \mathbb{R}$ denotes a single data point here. I choose the prior distributions as \begin{eqnarray} \boldsymbol{\phi}|\boldsymbol{\delta} & \sim & {\rm Dir}(\boldsymbol{\delta}),\label{eq:1D_gaussmix_prior1}\\ \mu_{i}|\sigma_{i}^{2},\mu_{0},\lambda & \sim & \mathcal{N}\big(\mu_{0},\lambda\sigma_{i}^{2}\big),\\ \sigma_{i}^{2}|\alpha,\beta & \sim & \Gamma^{-1}(\alpha,\beta),\label{eq:1D_gaussmix_prior2} \end{eqnarray} where $\Gamma^{-1}$ denotes the inverse Gamma distribution. The prior hyperparameters are $\mu_0\ge0$ , $\alpha>0$ , $\beta>0$ , $\lambda>0$ , and $\boldsymbol{\delta}:=(\delta_1,...,\delta_K)$ with $\delta_i>0$ . The total prior is then given by $$ p(\boldsymbol{\mu},\boldsymbol{\sigma},\boldsymbol{\phi}|\mu_0,\alpha, \beta, \lambda, \boldsymbol{\delta})=p(\boldsymbol{\phi}|\boldsymbol{\delta})\prod_{i=1}^{K}p(\mu_{i}|\sigma_{i}^{2},\mu_{0},\lambda)p(\sigma_{i}^{2}|\alpha,\beta), $$ where I pick $\alpha=\beta=\lambda=1$ , $\mu_0=0$ , and $\boldsymbol{\delta}=(1,...,1)$ . Is this setup correct thus far?
