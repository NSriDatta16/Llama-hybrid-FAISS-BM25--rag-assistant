[site]: stackoverflow
[post_id]: 3263503
[parent_id]: 3260653
[tags]: 
Top 10 search terms for the past month Using memory efficient indexing/data structure, such as tightly packed tries (from wikipedia entries on tries ) approximately defines some relation between memory requirements and n - number of terms. In case that required memory is available ( assumption 1 ), you can keep exact monthly statistic and aggregate it every month into all time statistic. There is, also, an assumption here that interprets the 'last month' as fixed window. But even if the monthly window is sliding the above procedure shows the principle (sliding can be approximated with fixed windows of given size). This reminds me of round-robin database with the exception that some stats are calculated on 'all time' (in a sense that not all data is retained; rrd consolidates time periods disregarding details by averaging, summing up or choosing max/min values, in given task the detail that is lost is information on low frequency items, which can introduce errors). Assumption 1 If we can not hold perfect stats for the whole month, then we should be able to find a certain period P for which we should be able to hold perfect stats. For example, assuming we have perfect statistics on some time period P, which goes into month n times. Perfect stats define function f(search_term) -> search_term_occurance . If we can keep all n perfect stat tables in memory then sliding monthly stats can be calculated like this: add stats for the newest period remove stats for the oldest period (so we have to keep n perfect stat tables) However, if we keep only top 10 on the aggregated level (monthly) then we will be able to discard a lot of data from the full stats of the fixed period. This gives already a working procedure which has fixed (assuming upper bound on perfect stat table for period P) memory requirements. The problem with the above procedure is that if we keep info on only top 10 terms for a sliding window (similarly for all time), then the stats are going to be correct for search terms that peak in a period, but might not see the stats for search terms that trickle in constantly over time. This can be offset by keeping info on more than top 10 terms, for example top 100 terms, hoping that top 10 will be correct. I think that further analysis could relate the minimum number of occurrences required for an entry to become a part of the stats (which is related to maximum error). (In deciding which entries should become part of the stats one could also monitor and track the trends; for example if a linear extrapolation of the occurrences in each period P for each term tells you that the term will become significant in a month or two you might already start tracking it. Similar principle applies for removing the search term from the tracked pool.) Worst case for the above is when you have a lot of almost equally frequent terms and they change all the time (for example if tracking only 100 terms, then if top 150 terms occur equally frequently, but top 50 are more often in first month and lest often some time later then the statistics would not be kept correctly). Also there could be another approach which is not fixed in memory size (well strictly speaking neither is the above), which would define minimum significance in terms of occurrences/period (day, month, year, all-time) for which to keep the stats. This could guarantee max error in each of the stats during aggregation (see round robin again).
