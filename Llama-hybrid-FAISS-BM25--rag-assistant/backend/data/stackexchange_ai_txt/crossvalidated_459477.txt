[site]: crossvalidated
[post_id]: 459477
[parent_id]: 
[tags]: 
Downweight or partially mask certain inputs to Neural Network

I have an NLP classification task for sentences set up, in which the goal is to predict a sentence label that depends on the primary verb used in the sentence. This task can be solved by just memorizing the verb - label association, but I would like to regularize or "encourage" the model to use information from the surrounding context as well, so that the model can generalize well to unseen verbs. Fully masking the embedding corresponding to the verb results in an underspecified task, since information from the verb is needed to fully determine the label. In short, I'd like a way to partially mask or downweight a specific input embedding to a neural network classifier, to encourage the network to use information from the surrounding context in addition to that input. I've thought about rescaling the verb embedding by a constant $c , but then $c$ becomes a hyperparameter that I would have to set somewhat arbitrarily. Any suggestions or pointers to references would be greatly appreciated. Thanks!
