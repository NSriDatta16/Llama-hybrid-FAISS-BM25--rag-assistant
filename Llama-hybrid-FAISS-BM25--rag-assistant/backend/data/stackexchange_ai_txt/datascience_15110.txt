[site]: datascience
[post_id]: 15110
[parent_id]: 
[tags]: 
How to implement global contrast normalization in python?

I am trying to implement global contrast normalization in python from Yoshua Bengio's Deep Learning book (section 12.2.1.1 pg. 442). From the book, to get a normalized image using global contrast normalization we use this equation: $$\mathsf{X}^{\prime}_{i,j,k}=s\frac{\mathsf{X}_{i,j,k}-\overline{\mathsf{X}}}{max\left\lbrace \epsilon, \sqrt{\lambda+\frac{1}{3rc}\sum_{i=1}^{r}\sum_{j=1}^{c}\sum_{k=1}^{3}(\mathsf{X}_{i,j,k}-\overline{\mathsf{X}})^2}\right\rbrace }$$ where $\mathsf{X}_{i,j,k}$ is a tensor of the image and $\mathsf{X}^{\prime}_{i,j,k}$ is a tensor of the normalized image, and $\overline{\mathsf{X}} = \frac{1}{3rc}\sum_{i=1}^{r}\sum_{j=1}^{c}\sum_{k=1}^{3} \mathsf{X}_{i,j,k}$ is the average value of the pixels of the original image. $\epsilon$ and $\lambda$ are some constants, usually with $\lambda=10$ and $\epsilon$ set to be a very small number, and here is my implementation: import Image import numpy as np import math def global_contrast_normalization(filename, s, lmda, epsilon): X = np.array(Image.open(filename)) X_prime=X r,c,u=X.shape contrast =0 su=0 sum_x=0 for i in range(r): for j in range(c): for k in range(u): sum_x=sum_x+X[i][j][k] X_average=float(sum_x)/(r*c*u) for i in range(r): for j in range(c): for k in range(u): su=su+((X[i][j][k])-X_average)**2 contrast=np.sqrt(lmda+(float(su)/(r*c*u))) for i in range(r): for j in range(c): for k in range(u): X_prime[i][j][k] = s * (X[i][j][k] - X_average) / max(epsilon, contrast) Image.fromarray(X_prime).save("result.jpg") global_contrast_normalization("cat.jpg", 1, 10, 0.000000001) original image: result image: I got an unexpected result. What is wrong with my implementation?
