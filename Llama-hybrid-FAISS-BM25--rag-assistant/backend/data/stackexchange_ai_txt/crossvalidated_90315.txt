[site]: crossvalidated
[post_id]: 90315
[parent_id]: 90298
[tags]: 
I would read through some of Steyerberg's articles on the subject. Steyerberg: "Validation and updating of predictive logistic regression models: a study on sample size and shrinkage" may be of interest. External validation is important. But you can't fairly compare the US model to your model build with your data. That is an almost meaningless comparison since your model should outperform. (Though if it doesn't I guess that is meaningful) some level of recalibration (as discussed in Steyerberg) article should be considered. your model is overfit. some thought as how to address this should be considered. If you want to update/re-calibrate the US model be aware that you need a fairly large sample size to do this. This is why most just recalibrate the intercept+/- slope. Again this is addressed in above reference. To compare models usually you use some combination of the measures below. At a minimum calibration and discrimination should be evaluated. Utility is, unfortunately, rarely assessed. global measure (scaled Brier) discrimination (c-statistic) calibration (often plot) measure of utility based on domain knowledge or could consider Decision Curve (Vickers et al.) I don't think AIC or deviance are reasonable suggestions for model comparison. Regarding NRI, you should read Leening et al in Annals of Internal Medicine Jan 21, 2014 (+/- Vickers Comment on article in same) before considering using. Sure, other institutions can use your model. In much the same way, and with the same limitations and caveats, as they can use the US model.
