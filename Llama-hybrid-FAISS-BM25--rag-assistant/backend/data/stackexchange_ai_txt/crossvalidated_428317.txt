[site]: crossvalidated
[post_id]: 428317
[parent_id]: 
[tags]: 
Derivation of Logistic regression

I am trying to understand the derivation of logistic regression. Some books like the famous Elements of Statistical Learning just give you the formula without too much effort. Others try to explains it a little further. The book A first course in Machine Learning for example does the following steps. First of all it starts from applying Bayes to $$P(\mathbf{w|t,X)}=\frac{P(\mathbf{t|w,X)}P(\mathbf{w)}}{P(\mathbf{t|X})}$$ where $\mathbf{w}$ is the vector of the model parameters, $\mathbf{X}$ is the input matrix and $\mathbf{t}$ the output vector. My first question is: Why is it starting from this? Should not it start from $P(\mathbf{t|w,X)}?$ Then it explains each term in the Bayes expression. Basically it says the numerator can be calculated while the denominator can not. The denominator is re-expressed as $$P(\mathbf{t|X}) = \int P(\mathbf{t|w,X}) P(\mathbf{w}) d\mathbf{w}$$ and it says [...] we cannot analytically perform the integration required to compute the marginal likelihood. In other words we have a function $g(\mathbf{w,X,t}) = P(\mathbf{t|w,X}) P(\mathbf{w})$ which we know is proportional to the posterior, $P(\mathbf{w|t,X)}=Zg(\mathbf{w,X,t})$ , but we do not know the constant of proportionality $Z^{-1}$ . Why it is not possible to perform the integration? Also assuming that when talking about proportionality it is rexpressing the posterior as: $P(\mathbf{w|t,X)}= \frac{P(\mathbf{t|w,X)}P(\mathbf{w)}}{\int P(\mathbf{t|w,X}) P(\mathbf{w}) d\mathbf{w}}$ , which is not explained, it is not saying which type of proportionality is that (inversely/directly) is it? if so how? Since we cannot compute these quantities it says we are left with 3 options (as I have understood to find $\mathbf{w}$ ): 1)Find the single value of $\mathbf{w}$ that corresponds to the highest value of posterior. As $g(\mathbf{w,X,t})$ is proportional to the posterior, a maximum of $g$ will correspond to a maximum of the posterior. $Z^{-1}$ is not a function of $w$ . 2)Approximate $P(\mathbf{w|t,X})$ with some other density that we can compute analytically. 3) Sample directly from the posterior $P(\mathbf{w|t,X)}$ knowing only $g$ It says the first is not very Bayesian: Why? It gives the example Newton-Raphson method but other books use this method to fit the regression. Also if $$P(\mathbf{w|t,X)}= \frac{P(\mathbf{t|w,X)}P(\mathbf{w)}}{\int P(\mathbf{t|w,X}) P(\mathbf{w}) d\mathbf{w}}$$ is valid, why a maximum in $g$ should correspond to a maximum in the posterior ? And does logistic regression use a particular method or is it independent from that? Then it says Once we have the posterior density, we can predict the response class of new objects by taking an expectation with respect to this density: $P(t_{new}=1|x_{new}|\mathbf{X,t}) = \mathbf{E}_{P(\mathbf{w|t,X)}}{\frac{1}{1+e^{-\mathbf{w^Tx_{new}}}}}$ Why is the expectation taken over $P(\mathbf{w|t,X)}$ ? What is the first motivation of logistic regression? Does it start from the desire to keep the linearity hence using $w^Tx$ but forcing the model to output probabilities so passing the linear function to a sigmoid? Or is this just a consequence of the desire to use log-odds?
