[site]: crossvalidated
[post_id]: 436927
[parent_id]: 436766
[tags]: 
Mathematically, softmax with finite inputs produces results $o_i \in (0,1) \forall i$ such that $\sum_i o_i =1$ . This implies that softmax is never 0, so $\log(o_i)$ is always a real number. Numerically, overflow or underflow could cause softmax to output a zero. This is common enough when training neural networks using floating point numbers. A common work-around to avoid numerical underflow (or overflow) is to work on the log scale via log_softmax , or else work on the logit scale and do not transform your outputs, but instead have a loss function defined on the logit scale. These methods avoid round-tripping (which causes a loss of precision) and use numerical tricks to keep values in nice floating point ranges. Obviously, working on the log scale, or the logit scale, requires making algebraic adjustments so that the loss is also on the appropriate scale. So if you use identity activations in the final layer, you use CrossEntropyLoss . If you use log_softmax in the final layer, you use NLLLoss . Consider $0 the probability output from the network, produced by softmax with finite input. We wish to compute the cross-entropy loss. One option is to do things the na√Øve way, using $o_i$ and $t_i$ directly, and computing $-\sum_i t_i \log(o_i)$ . A second option is to use log-probabilities instead. This means you have $z_i = \log(o_i)$ in hand, so you compute $-\sum_i t_i \log(o_i) = -\sum t_i z_i$ . I can't answer the part of your question about re-labeling because it doesn't make sense. When you're using a numerically stable procedure, $\log(o_i)$ is always a finite number, so $t_i \log(o_i)$ for $y \in \{0,1\}$ is also finite. In fact, in the case of 1-hot labels, only one index $i$ has a non-zero value of $t_i \log (o_i)$ . See also: Infinities with cross entropy in practice
