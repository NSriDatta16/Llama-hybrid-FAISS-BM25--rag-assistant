[site]: crossvalidated
[post_id]: 389412
[parent_id]: 
[tags]: 
In linear regression, is finding the minimum of the parameters the same as gradient descent?

I'm taking a course on ML and have just began. Given a loss function, $$L = \frac{1}{N}\sum^N_{n=1}(t_n - w_0 + w_1x_n)^2$$ I am confused between the difference of using gradient descent (and maybe even its purpose) as opposed to minimizing the error by finding the best values of our parameters $w_0, w_1$ by differentiating them to find a minima for each. $$w_0 = \bar{t} - w_1\bar{x}, \;\;\;\;\;\;w_1 = \frac{\bar{x}\bar{t} - \bar{x}t}{\bar{x}\bar{x} - \bar{x}^2}$$ Or, am I greatly confused, and this is gradient descent?
