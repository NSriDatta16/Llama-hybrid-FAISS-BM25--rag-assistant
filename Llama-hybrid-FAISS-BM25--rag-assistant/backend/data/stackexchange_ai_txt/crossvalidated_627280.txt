[site]: crossvalidated
[post_id]: 627280
[parent_id]: 627279
[tags]: 
Just an extended comment, and probably not an answer: The "squared error" is a typical loss function for regression. The mean squared error (MSE) is the average of a couple of squared errors. It is frequently used as objective criterion in regression situations. The sum of squared errors is equivalent to the MSE in terms of optimization. Multiplying with a constant has no impact on the solution. I have never heard of LSE. There is an optimization technique called "ordinary least-squares" that minimizes the MSE, but it is fully unrelated to neural nets.
