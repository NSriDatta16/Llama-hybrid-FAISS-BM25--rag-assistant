[site]: datascience
[post_id]: 68720
[parent_id]: 68708
[tags]: 
A neural network is a succession of layers (dense/linear, convolutional). After each layer, you need a non-linear activation function. Why do you need a non-linear activation function? Because composing two linear transformations is equivalent to having a single linear transformation: $ (x \cdot W_1 + b_1) \cdot W_2 + b_2 = x (W_1 \cdot W_2) + (b_1 \cdot W_2 + b_2)$ Therefore, stacking linear transformations would be pointless, because the whole network would be equivalent to a linear transformation. A ReLU may seem similar to a linear function, but its non-linear nature gives the network the ability to model non-linear functions. No, there is nothing unique about $0$ being the point of change. Most neural network layers have a bias term that makes the point of change irrelevant, as during the training the bias will be adjusted to the proper value. While in deep learning everything is data-dependent, the fact that the ReLU's changing point is irrelevant is independent from the data (see previous answer). The advantage of ReLU is against other non-linearities, like sigmoid and tanh, which suffer the vanishing gradient problem . ReLU itself has its own problem, namely the "dying ReLU problem" .
