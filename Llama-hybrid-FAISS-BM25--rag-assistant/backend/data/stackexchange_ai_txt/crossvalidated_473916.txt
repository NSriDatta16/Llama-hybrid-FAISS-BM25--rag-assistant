[site]: crossvalidated
[post_id]: 473916
[parent_id]: 
[tags]: 
Understanding the weak learners in boosting

My understaning is that boosting is a method by which you have several weak models trained in sequence. Each is trained on the full trainig data, but with greater emphasis placed on the weaknesses of the previously trained model. I have several questions about the weak models that go into a boosted model. Please tell me if they are different enough that they should be separated into new questions. Do the weak models all need to be of the same kind? Could I follow a decision stump with a small neural network, for instance? Why not train a more complex model sequentially the same way - find where it was wrong, and put greater emphasis on what it got wrong? If it's speed, why is training lots of small models so much faster than one larger model? Related How are the features and architecture for a weak model chosen? I'd guess that they'd be random and identical, respectively, but am unsure how that works with their ability to work as feature extraction algorithms. I ask about that further here . Thanks in advance!
