[site]: crossvalidated
[post_id]: 406796
[parent_id]: 
[tags]: 
ACER: optimization using the KKT conditions

In Page 5 Sample Efficient Actor-Critic with Experience Replay , the authors define an optimization problem with a linearized KL divergence constraint (Eq.11)as follow $$ \min_z{1\over 2}\Vert \hat g_t^{acer}-z\Vert_2^2\\ s.t.k^Tz\le\delta\\ where\quad k=\nabla_{\phi_\theta}D_{KL}[f(\cdot|\phi_{\theta_a}(x_t))\Vert f(\cdot|\phi_{\theta}(x_t))] $$ where $f(\cdot|\phi_{\theta_a}(x_t))$ denotes the policy network, $f$ alone is the categorical distribution. $\phi_{\theta_a}$ is the average policy network, whose parameters are updated according to: $\theta_a\leftarrow\alpha\theta_a+(1-\alpha)\theta$ . They solve this quadratic programming problem, getting the solution: $$ z^*=\hat g_t^{acer}-\max\left\{0, {k^T\hat g_t^{acer}-\delta\over \Vert k\Vert_2^2}\right\}k\tag 1 $$ I cannot see how they get this. where does the maximum term come from? To my best knowledge, the KKT conditions can be written as $$ \begin{align} z-\hat g_t^{acer}+\lambda k&=0\\ \lambda(k^Tz-\delta)&=0\\ \lambda&\ge0 \end{align} $$ from which I get $$ z^*=\begin{cases} k^{-1}\delta&\mathrm{if}\ \lambda>0\\ \hat g_t^{acer}&\mathrm{if}\ \lambda=0 \end{cases} $$ This seems not consistent with Equation1. Where do I make mistakes?
