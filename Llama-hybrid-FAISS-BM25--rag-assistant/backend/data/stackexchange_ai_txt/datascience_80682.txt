[site]: datascience
[post_id]: 80682
[parent_id]: 
[tags]: 
Why does the non autoregresive transfomer model in fairseq require the prev_output_tokens input?

fairseq includes an implementation of a non autoregressive transformer - which (as much as I understand) means that the whole output sequence is generated in a single forward run (in contrast to autoregresive models where each forward run predicts the next token from the input and the previous predicted tokens) However, from the code it appears that the models still expects the previous tokens as input: def forward(self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs) https://github.com/pytorch/fairseq/blob/master/fairseq/models/nat/nonautoregressive_transformer.py#L78
