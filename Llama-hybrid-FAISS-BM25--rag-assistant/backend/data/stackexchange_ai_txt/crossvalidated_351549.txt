[site]: crossvalidated
[post_id]: 351549
[parent_id]: 
[tags]: 
Maximum Likelihood Estimators - Multivariate Gaussian

Context The Multivariate Gaussian appears frequently in Machine Learning and the following results are used in many ML books and courses without the derivations. Given data in form of a matrix $\mathbf{X} $ of dimensions $ m \times p$, if we assume that the data follows a $p$-variate Gaussian distribution with parameters mean $\mu$ ( $p \times 1 $) and covariance matrix $\Sigma$ ($p \times p$) the Maximum Likelihood Estimators are given by: $\hat \mu = \frac{1}{m} \sum_{i=1}^m \mathbf{ x^{(i)} } = \mathbf{\bar{x}}$ $\hat \Sigma = \frac{1}{m} \sum_{i=1}^m \mathbf{(x^{(i)} - \hat \mu) (x^{(i)} -\hat \mu)}^T $ I understand that knowledge of the multivariate Gaussian is a pre-requisite for many ML courses, but it would be helpful to have the full derivation in a self contained answer once and for all as I feel many self-learners are bouncing around the stats.stackexchange and math.stackexchange websites looking for answers. Question What is the full derivation of the Maximum Likelihood Estimators for the multivariate Gaussian Examples: These lecture notes (page 11) on Linear Discriminant Analysis, or these ones make use of the results and assume previous knowledge. There are also a few posts which are partly answered or closed: Maximum likelihood estimator for multivariate normal distribution Need help to understand Maximum Likelihood Estimation for multivariate normal distribution?
