[site]: crossvalidated
[post_id]: 189463
[parent_id]: 189331
[tags]: 
Softmax is also a generalization of the logistic sigmoid function and therefore it carries the properties of the sigmoid such as ease of differentiation and being in the range 0-1. The output of a logistic sigmoid function is also between 0 and 1 and therefore naturally a suitable choice for representing probability. Its derivative is also exoressed in terms of its own output. However, if your function has a vector output you need to use the Softmax function to get the probability distribution over the output vector. There are some other advantages of using Softmax which Indie AI has mentioned, although it does not necessarily has anything to do with the Universal Approximation theory since Softmax is not a function only used for Neural Networks. References Logistic function Softmax function Ease of Differentiation on Softmax Ease of Differentiation of Sigmoid
