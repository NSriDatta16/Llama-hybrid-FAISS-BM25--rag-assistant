[site]: crossvalidated
[post_id]: 253273
[parent_id]: 
[tags]: 
Inverse logit/sigmoid algebraic manipulations in Ian Goodfellow's Deep Learning Book derivation

In Ian Goodfellow's deep learning book page 177, the authors motivate the functional form of the sigmoid by normalizing $z$, a linear function of hidden units $h$, $z = w^T h + b$. Unnormalized, $log(P(y)) = yz$ Exponentiating both sides and normalizing: $P(y) = exp(yz) / \sum_{y'=0}^1 exp(y' z) = \sigma((2 y - 1) z)$ Where $\sigma$ is defined as $\sigma(x) = exp(x)/(exp(x) + 1)$ Can someone explain where the $(2 y - 1) z$ comes from? Probably missing some simple algebra but I'm obtuse.
