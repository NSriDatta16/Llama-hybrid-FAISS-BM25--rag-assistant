[site]: datascience
[post_id]: 9991
[parent_id]: 
[tags]: 
Is information lost when converting an image to a single vector for analysis?

On several tutorials on object and pattern recognition in images (such as the typical mnist digit recognition problem), when working with a sample of $m$ images of $n \times n$ pixels each, the first step is to build a matrix of $m \times (n * n)$, where each image is converted into a single row-vector with all of its pixels, one next to the other. Different techniques are then applied to this matrix, such as PCA. My question is: when converting a two-dimensional image to a single-dimensional vector by placing each image row next to another, the fact of which pixels are neighbours (in the same column, different row) seems to be lost. For instance, in image: a b c d e f g h i Which is converted to: a b c d e f g h i The fact that b and e were next to each other seems to be lost. Is that conclusion correct? Does that fact even matter for analysis? Are there other ways to represent the image that do preserve that piece of information? Apologies in advance if the question is too vague or basic, I'll be glad to reformulate if needed. Thank you.
