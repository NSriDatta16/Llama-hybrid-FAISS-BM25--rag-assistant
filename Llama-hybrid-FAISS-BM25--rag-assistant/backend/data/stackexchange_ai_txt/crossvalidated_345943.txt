[site]: crossvalidated
[post_id]: 345943
[parent_id]: 345682
[tags]: 
Cross validation or more precisely: resampling validation relies on some assumptions. Relevant for your question are: A (surrogate) model trained on the whole data set minus a few cases (the left-out fold) is approximately equivalent to the model trained on the whole data set (the one for which you want to measure generalization error). This assumption allows using the observed generalization performance of the surrogate models as approximation for the generalization performance of the model trained on the whole data set. Due to the lower number of training cases, you may have on average somewhat lower performance of the surrogate models: this is the well-known slightly pessimistic bias of resampling validation. A second, weaker assumption is that the surrogate models are equivalent to each other. This allows sensible pooling of the observed results for all surrogate models and this assumption is violated if your models are not stable. So your point about "each iteration [fold] generates a different model" is rather seen the other way round: if the generated surrogate models are actually different, you do have a problem with model instability and anyways should go back and adapt your modeling strategy - and checking this should be part of your cross validation. So once you checked that the surrogate models are reasonably similar, you are fine using their performance as approximation for the whole-data model's performance.
