[site]: crossvalidated
[post_id]: 631727
[parent_id]: 620053
[tags]: 
Some notes on the mater, coming from econometrics: After the advances in computing power made Monte Carlo simulations easy, it has become standard to assess finite-sample properties of estimators through such simulations (since their finite-sample distribution is almost always unknown). And in them, casual observation indicates that almost always the Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) criterion is used to assess performance, stand-alone or in comparison... so it would appear that academic econometrics at least have embraced the ``bias-variance" trade-off, since we can get better MSE, with a succesful such trade-off... Is that ok? We should ask ourselves Q: When MSE is a reasonable criterion to apply in order to assess the performance of an estimator? A: when our goal is to construct an estimator to be used repeatedly as a prediction/forecasting tool. Think macroeconomic models for government policy or big-business planning, think large-scale factory production. In such situations, what is our prediction error ``on average" does matter as regards the overall costs of inaccuracy. But in very many cases of applied econometric research, the goal is not to predict the dependent variable, but to measure regressor effects on the dependent variable after the fact , to uncover what has happened. This will certainly inform future decisions in comparable situations, but not in the same way as mentioned just above. Here, I argue that it is more pertinent and more useful to be able to determine the range of values that the unknown effect is likely to have taken, given your estimate . This points towards the construction of confidence intervals ( CI ) around the estimate, and the measurement of the``coverage probability": what is the percentage of times the true value of the unknown effect falls inside the constructed confidence interval. Obviously, we would want to have a CI as short as possible with a coverage probability as high as possible... (and I always talk about the empirical properties of the CI, not the theoretical ones if the distribution used to construct it happens to be exactly what holds in the data). Focusing on the case where we treat our estimator as having a Normal distribution (which is what we do most of the times relying on asymptotics), I will show next that Claim 1. The introduction of any bias keeping the variance unchanged reduces the coverage probability, without shortening the CI . Claim 2. A successful "bias-variance trade-off" reduces the coverage probability even more - but at least here, we get a shorter CI . So the bias-variance trade-off creates another trade-off: the "length-coverage" one. Claim 3. For small relative bias, the reduction in coverage probability appears tolerable, against the magnitude of the benefit of a shorter CI . Let $\beta$ be the unknown effect (say, a coefficient), and $b$ its estimator with standard deviation $\sigma$ . We treat $b$ as Normal, and we allow for the possibility of (positive or negative) bias $B$ (which we do not know if it exists). Then $$\frac{b-\beta}{\sigma} \sim {\rm N}\left(\frac {B}{\sigma}, 1\right).$$ In fact we will define $\xi \equiv |B/\sigma|$ and $s\equiv {\rm sgn}(\xi)\in \{-1,0,1\}$ , and so $$\frac{b-\beta}{\sigma} \sim {\rm N}\left(s\xi, 1\right).$$ A typical CI is constructed as $[b \pm\sigma z_{\alpha/2}]$ , where $z_{\alpha/2}$ is the $\alpha/2$ -quantile of the Normal distribution. We want the coverage probability ${\rm CP}$ , which is the probability of the event $$\Big\{b -\sigma z_{\alpha/2} \leq \beta \leq b +\sigma z_{\alpha/2}\Big\}$$ which after standard manipualtions gives $${\rm CP} = \Pr\left(-z_{\alpha/2} \leq \frac{b-\beta}{\sigma} \leq z_{\alpha/2}\right).$$ Taking into account the possibility of bias, and the Normality of our estimator, this is, for $\Phi$ the standard Normal CDF, $${\rm CP} = \Phi\left(z_{\alpha/2}- s\xi\right) - \Phi\left(-z_{\alpha/2}- s\xi\right).$$ Consider now how the coverage probability changes with $\xi$ , $\phi$ being the standard Normal density (an even function): \begin{align} \frac{\partial {\rm CP}}{\partial \xi} &= -s\phi\left(z_{\alpha/2}- s\xi\right) + s\phi\left(z_{\alpha/2}+ s\xi\right)\\ &= s\cdot \left[\phi\left(z_{\alpha/2}+ s\xi\right) - \phi\left(z_{\alpha/2}- s\xi\right)\right]\\ &=s\cdot \phi\left(z_{\alpha/2}+ s\xi\right)\left[1 - \frac{\phi\left(z_{\alpha/2}- s\xi\right)}{\phi\left(z_{\alpha/2}+ s\xi\right)}\right]\\ &=s\cdot \phi\left(z_{\alpha/2}+ s\xi\right)\left[1 - \exp\big\{2z_{\alpha/2}s\xi \big\}\right]. \end{align} One can verify that $$\begin{cases} \xi = 0 \qquad \frac{\partial {\rm CP}}{\partial \xi} = 0\\ \\ \xi \neq 0 \qquad \frac{\partial {\rm CP}}{\partial \xi} So the existence of bias always reduces the coverage probability, and if we managed that without reducing the variance, we are unambiguously worse-off, whether we look at MSE or the $CI$ . ( Claim 1. ) Consider now a successful "bias-variance trade off". This means that we have $|B|\neq 0$ and enough $\sigma \downarrow$ to reduce MSE ... but this will increase $\xi$ on two counts -its numerator will increase and its denominator will decrease. So we will have larger decrease in the coverage probability, but a shorter CI ( Claim 2 ). As for Claim 3. , what kind of bias magnitudes are we talking about here? I would say, some small percentage of the variance, otherwise I doubt anyone would accept it. So suppose we have $B^2 = 0.1\sigma^2$ and let's say we achieve a $20\%$ reduction in the variance. Our MSE was without bias $\sigma^2$ and now it becomes $${\rm MSE}|_{B} = (1-0.2)\sigma^2 + 0.1\sigma^2 = 0.9 \sigma^2 That's a successful bias-variance trade-off. In this numerical example, we have $\xi = \sqrt{0.1/0.8}= 0.3535$ . Let's turn now to the Coverage Probability. Suppose we want a $0.9-{\rm CI}$ so $z_{\alpha/2} = z_{0.95} = 1.645$ . Suppose the bias is positive (overestimation of $\beta$ on average, sign included). Then $${\rm CP}|_{B=0} = \Phi\left(1.645\right) - \Phi\left(-1.645\right) = 0.90.$$ $${\rm CP}(B=\sigma\sqrt{0.1},\, \sigma(B) = \sqrt{0.8}\sigma) = \Phi\left(1.645- \frac{\sqrt{0.1}}{\sqrt{0.8}}\right) - \Phi\left(-1.645- \frac{\sqrt{0.1}}{\sqrt{0.8}}\right) = 0.879.$$ This bias-variance trade-off resulted in our ${\rm CI}$ being now $1-\sqrt{0.8}=11.5\%$ shorter, while the coverage probability reduced by $2$ probability points... many people would be willing to live with this trade-off. It is an interesting exercise to compute how the coverage probability reduces as $\xi$ increases and, for the different variance reductions that can be associated with the same $\xi$ , how much the length of the CI shortens, to get a fuller quantitative picture of this ``length-coverage" trade-off.
