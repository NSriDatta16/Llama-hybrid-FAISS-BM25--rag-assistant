[site]: crossvalidated
[post_id]: 622553
[parent_id]: 
[tags]: 
Rolling window validation for time series classification: good idea?

I have a time series dataset (interval = 10 minutes) that contains a user's visited locations. I derive several features from the timestamps to capture the user's trend: hour of the day, day of the week, and day of the month. A sample of the data: time location weekday hour day 2023-01-01 03:10:00 11 6 3 1 2023-01-01 03:20:00 11 6 3 1 2023-01-01 03:30:00 11 6 3 1 2023-01-01 03:40:00 7 6 3 1 2023-01-01 03:50:00 7 6 3 1 The features are chosen based on some periodicity assumptions: most users will have a daily, weekly, and monthly pattern in their visited locations. The dataset contains data from one year (so 365*24*6 = 52.560 records). I am looking for a way to validate how well my Machine Learning model (currently RandomForestClassifier) can predict the user's location between one day and seven days into the future. Currently, I take the first 30 days as training set and predict the next seven days. This gives me performance scores for each of the seven days. Then I use a rolling window and shift the training/test sets one day into the future. This method allows me to validate 365-30-7 = 328 times. Obviously, successive models have a large overlap, therefore the performance will only slightly change. Alternatively, I could use some kind of blocked cross validation , but that will only allow me to validate my model +- 10 times with this dataset since each model then needs 37 unique days. My question: how do I validate this model's performance? Is my current method sufficient or are there better methods?
