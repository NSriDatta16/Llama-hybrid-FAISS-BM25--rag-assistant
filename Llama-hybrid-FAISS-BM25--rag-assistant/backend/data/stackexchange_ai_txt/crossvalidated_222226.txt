[site]: crossvalidated
[post_id]: 222226
[parent_id]: 222214
[tags]: 
Cross validation has two uses: assessing a model's likely performance on new data comparing multiple models' performance In use #2, you apply CV to different models, assess each one's likely out-of-sample performance per #1, and then choose the better one. So, assuming we already have chosen a model, what's the remaining point in #1? After all, CV won't improve the performance of the model chosen. The point now is that a prediction is good... but what is almost as important is a measure of how good that prediction is likely to be. Having an idea of the reliability of your prediction helps your subsequent processes. If you have a very reliable prediction, you need to invest less resources in mitigation efforts for mispredictions than if you had a less reliable prediction. For instance, my own field is forecasting daily sales in supermarkets per stock-keeping unit (SKU), for replenishment. The point forecast is pretty much useless, because we want to achieve a certain service level. To stock enough product, we need a quantile forecast, which (given a distributional assumption) is conceptually similar to a point forecast plus a notion of the variability of the forecast error. We don't use time series cross validation for this, but it would certainly be a valid approach.
