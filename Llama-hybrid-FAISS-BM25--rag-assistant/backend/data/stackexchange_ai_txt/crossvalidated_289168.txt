[site]: crossvalidated
[post_id]: 289168
[parent_id]: 
[tags]: 
Using the central limit theorem in PCA on non-multivariate normal data

In ' Principal Component Analysis (Second Edition) by I.T. Jolliffe ', section 10.1, page 236, the following is written: It is possible to set up more formal tests for outliers based on PCs, assuming that the PCs are normally distributed. Strictly, this assumes that $x$ has a multivariate normal distribution but, because the PCs are linear functions of $p$ random variables, an appeal to the Central Limit Theorem may justify approximate normality for the PCs even when the original variables are not normal. ($x$ here is a vector of $p$ random variables. PCA is performed on a $n\times p$ matrix $X$, which contains $n$ observations of $x$.) I do not understand this statement. As far as I know the classical version of the central limit theorem refers to a sequence of independent and identically distributed variables. But the $p$ random variables are mostly not independent plus not necessarily identically distributed. One example can be given by computing the PCA for a set of data containing height and weight of pupils. Both variables are highly correlated. (This is the example that the book uses in section 10.1) What did the author mean by that? (That is, how can CLT be used to show that a principal component is approximately normally distributed?)
