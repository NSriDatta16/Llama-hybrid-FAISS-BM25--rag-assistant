[site]: crossvalidated
[post_id]: 614623
[parent_id]: 
[tags]: 
Derivation of the target upper-bound with Jensen's inequality in 'Divide-and-Conquer RL'

I am currently stuck in one line in the paper named "Divide-and-Conquer Reinforcement Learning"(Ghosh et. al., 2018 ICLR) . It is the equation (1) in the page 4 which is like below. $$E_\pi[D_{KL}(\pi \Vert \pi_c)] \leq \sum_{i,j}\rho(\omega_i)œÅ(\omega_j )E_{\pi_i}[D_{KL}(\pi_i\Vert \pi_j )]$$ Here, the policy in the Augmented MDP (with context information added to the state) $\pi$ is the family of context-wise policies (which I assume it means it will just use the context specific policy when given the context info), $\pi_c$ is the policy in the original MDP without the context info which is represented as the weighted (weight is the context belief distribution given state) sum of the context-wise policies that are in the family $\pi$ . Although I understand the basic concept, I still cannot understand how this inequality is driven. Especially when defining the $\pi_c$ , there were no comments on $\rho(w)$ and it should not since it is for the original MDP without any contextual information... More info: w : context where it determines the initial state distribution $\rho$ : initial state distribution $\pi$ : $(\pi_i)^n_{i=1}, \pi_i$ =context-wise policy $\pi_c$ : $\sum_{\omega \in \Omega} p(\omega |s)\pi_\omega,p(\omega|s)$ = belief distribution of what context the state is in.
