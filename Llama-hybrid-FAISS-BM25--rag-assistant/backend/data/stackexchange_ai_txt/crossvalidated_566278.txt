[site]: crossvalidated
[post_id]: 566278
[parent_id]: 
[tags]: 
Feature selection - Before vs After oversampling

I understand that class imbalance is not considered as a problem by some of experts here. But for the sake of experimentation, I would like to try out the oversampling for my data (which is of 77:23 proportion). I am working on a binary classification problem using traditional ML algos (e.g., random forest) and neural networks. As most of my variables are categorical and highly cardinal in nature, I use encoding methods like ordinal_encoder , rare_encoder etc. So, my question is a) Should I select the best features first and then oversample my data using SMOTE? or I have to upsample first and then do the feature selection? can you also let me know the reason as to why it should be done the way you suggest (and not the other way round)?
