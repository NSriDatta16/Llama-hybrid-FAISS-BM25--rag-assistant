[site]: datascience
[post_id]: 53966
[parent_id]: 
[tags]: 
Classification report returns same accuracy precision recall averages at binary classification problem

My results are: This is the Code: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report df = pd.read_csv(dataset) X = df.drop(columns=['backers_count','converted_pledged_amount','pledged' ,'id','usd_pledged','state','static_usd_rate','funded_percentage']) y = df['state'].values # Split dataset into train and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42,stratify=y) # Create Logistic Regression Classifier logreg = LogisticRegression(solver='lbfgs',max_iter=400) logreg.fit(X_train, y_train) log_predict = logreg.predict(X_test) #train model with cv of 10 cv_scores = cross_val_score(logreg, X, y, cv=10) # Print the results print("=== Confusion Matrix ===") print(confusion_matrix(y_test, log_predict)) print('\n') print("=== Classification Report ===") print(classification_report(y_test, log_predict)) print('\n') print("=== All Accuracy Scores ===") print(cv_scores) print('\n') print("=== Mean Accuracy Score ===") print("Mean Accuracy Score - Logistic Regression: ", cv_scores.mean()) Is it normal to have identical values to micro avgs macro avgs and weighted for precision recall f1-score and to accuracy as well or there's something wrong with my model? This also happens(getting identical results to precision recall f1-score accuracy) at other classification models with the same dataset that I've tried like random forest and SVC. If its normal how do I interpret this, or is it just wrong code and try to fix this? Which avg matters and should record for binary classification(micro macro or weighted) or all?
