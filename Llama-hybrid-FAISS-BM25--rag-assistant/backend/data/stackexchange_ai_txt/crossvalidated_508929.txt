[site]: crossvalidated
[post_id]: 508929
[parent_id]: 508863
[tags]: 
Consider the following GAN, where $\mathcal D$ is a discriminator and $\mathcal G$ is a generator. $$\min_{\theta_\mathcal D, \theta_\mathcal G} \mathcal {L}_{\mathcal D}(\mathcal D(X),\mathcal D(\mathcal G(Z)))$$ $X$ represents batches of real data and $Z$ represents batches of random generated numbers. If $X$ are "image features" as you defined them, i.e. description of lines and other geometric features, then yes, $\mathcal G$ can theoretically learn how to generate them. This represents two issues, however: (1) you need to represent these features at once for the real images, with tensor notation, which invariably leads to loss of information, and (2) you need to reverse the process to check the quality of generated images. Non purely convolutional architectures, such as Graph Neural Networks, may ameliorate the issues however (though I'm not sure how to represent a curved contour in a graph embedding).
