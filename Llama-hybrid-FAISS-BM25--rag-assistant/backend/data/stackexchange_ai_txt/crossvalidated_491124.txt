[site]: crossvalidated
[post_id]: 491124
[parent_id]: 
[tags]: 
Misspecification in the outcome model with regression adjustment for propensity score?

The propensity score is a popular tool used to control for confounding by covariates $C$ on the effect of an exposure $A$ on an outcome $Y$ . There are several ways to incorporate the propensity score into analyses; one popular method is to adjust for the propensity score alongside the exposure variable in a regression model for the outcome. Vansteelandt and Daniel (2014) develop insights into this method, in particular when the regression model is nonlinear. The method enjoys certain robustness properties. For example, consider the logistic regression model for a binary outcome: \begin{align} P(Y = 1 | A = a, p(C) = p(c)) = \operatorname{expit}(\theta_0 + \theta_1 a + \theta_2 p(c)) \end{align} where $p(c) = P(A = 1| C = c)$ is the propensity score and $\operatorname{expit}(x) = 1/(1 + \operatorname{exp}(-x))$ . We can test the null hypothesis of no exposure effect using the standard test of $\theta_1 = 0$ for logistic regression. Vansteelandt and Daniel say the following: ...we show that tests of the null hypothesis of no exposure effect in such generalised linear models that adjust for the propensity score and are based on a robust variance estimator do not falsely reject the null hypothesis more than 100% of the time in large samples (with the nominal significance level) when the propensity score model is correctly specified, even when the outcome regression model is misspecified." My questions are: What does "misspecified" mean in this context? Does the result mean that the standard test of $\theta_1 = 0$ has level $\alpha$ regardless of the true conditional distribution $P(Y = 1 | A = a, p(C) = p(c))$ ? Suppose we adjusted for $\operatorname{logit}(p(c)) = \operatorname{log}(p(c)/(1 - p(c)))$ instead of $p(c)$ ; i.e. suppose we fit the outcome model $P(Y = 1 | A = a, p(C) = p(c)) = \operatorname{expit}(\theta_0 + \theta_1 a + \theta_2 \operatorname{logit}(p(c)))$ . Does the result hold in this case? Can we still expect Type 1 error to be bounded by $\alpha$ ?
