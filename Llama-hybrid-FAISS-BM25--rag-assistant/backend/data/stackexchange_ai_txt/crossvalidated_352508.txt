[site]: crossvalidated
[post_id]: 352508
[parent_id]: 
[tags]: 
Why is the prior omitted from this Bayes rule?

I'm trying to understand the EM algorithm. I've found a tutorial on it. It goes like this: Two coins (A & B). 5 rounds of flipping 10 times. We forgot, however, which coin was flipped each round. What are the probabilities of heads $\theta_A$, $\theta_B$ of the coins? We map the latent variable (coin used) in variable $Z$ with $p(Z_r=A)=p(Z_r=B)=0.5$. In each round $r$ the number of heads is $x_r$. $p(X_r = x_r | Z_r=A;\theta)=\theta_A^{x_r}(1-\theta_A)^{10-x_r}$ Now the tutorial says that the posterior using Bayes' rule is: $p(Z_r = A|x_r;\theta)=\frac{\theta_A^{x_r}(1-\theta_A)^{10-x_r}}{\theta_A^{x_r}(1-\theta_A)^{10-x_r}+\theta_B^{x_r}(1-\theta_B)^{10-x_r}}$ I don't understand why the prior is omitted from the rule here? Shouldn't it be: $p(Z_r = A|x_r;\theta)=\frac{p(X_r=x_r|Z_r=A;\theta)p(Z_r=A)}{\sum_{C=A,B}\theta_C^{x_r}(1-\theta_C)^{10-x_r}}=\frac{\theta_A^{x_r}(1-\theta_A)^{10-x_r} * 0.5}{\theta_A^{x_r}(1-\theta_A)^{10-x_r}+\theta_B^{x_r}(1-\theta_B)^{10-x_r}}$ Source: http://people.inf.ethz.ch/ganeao/em_tutorial.pdf Edit: Martijn made things clear for me. For any future readers: $p(Z_r = A|x_r;\theta)=\frac{p(X_r=x_r|Z_r=A;\theta)p(Z_r=A)}{p(x_r;\theta)}$ \begin{align} p(x_r;\theta) &= {\sum_{C=A,B}p(x_r|Z_r=C;\theta)p(Z_r=C)} \\ &= p(x_r|Z=A;\theta)p(Z_r=A)+p(x_r|Z_r=B;\theta)p(Z_r=B) \\ &= \theta_A^{x_r}(1-\theta_A)^{10-x_r}*0.5+\theta_B^{x_r}(1-\theta_B)^{10-x_r}*0.5 \\ &= 0.5(\theta_A^{x_r}(1-\theta_A)^{10-x_r}+\theta_B^{x_r}(1-\theta_B)^{10-x_r}) \end{align} So $p(Z_r = A|x_r;\theta)=\frac{0.5(\theta_A^{x_r}(1-\theta_A)^{10-x_r})}{0.5(\theta_A^{x_r}(1-\theta_A)^{10-x_r}+\theta_B^{x_r}(1-\theta_B)^{10-x_r})}=\frac{\theta_A^{x_r}(1-\theta_A)^{10-x_r}}{\theta_A^{x_r}(1-\theta_A)^{10-x_r}+\theta_B^{x_r}(1-\theta_B)^{10-x_r}}$
