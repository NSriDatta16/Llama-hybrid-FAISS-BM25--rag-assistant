[site]: crossvalidated
[post_id]: 205479
[parent_id]: 110757
[tags]: 
This boils down to maximum likelihood vs. methods of moments, and finite sample efficiency vs. computational expediency. Using a 'proper' AR(1) process and estimating the parameter $\rho$ (and unknown variance $\sigma^2$) via maximum likelihood (ML) gives the most efficient (lowest variance) estimates for a given amount of data. The regression approach amounts to the Yule-Walker estimation method, which is the method of moments. For a finite sample it isn't as efficient as ML, but for this case (i.e. an AR model) it has an asymptotic relative efficiency of 1.0 (i.e. with enough data it should give answers nearly as good as ML). Plus, as a linear method it is computationally efficient and avoids any convergence issues of ML. I gleaned most of this from dim memories of a time series class and Peter Bartlett's lecture notes for Introduction to Time Series , lecture 12 in particular. Note that the above wisdom relates to traditional time series models, i.e. where there are no other variables under consideration. For time series regression models, where there are various independent (i.e. explanatory) variables, see these other references: Achen, C. H. (2001). Why lagged dependent variables can supress the explanatory power of other independent variables. Annual Meeting of the Polictical Methodology Section of the American Politcal Science Association, 1–42. PDF Nelson, C. R., & Kang, H. (1984). Pitfalls in the Use of Time as an Explanatory Variable in Regression. Journal of Business & Economic Statistics, 2(1), 73–82. doi:10.2307/1391356 Keele, L., & Kelly, N. J. (2006). Dynamic models for dynamic theories: The ins and outs of lagged dependent variables. Political analysis, 14(2), 186-205. PDF (Thanks to Jake Westfall for the last one). The general take away seems to be "it depends".
