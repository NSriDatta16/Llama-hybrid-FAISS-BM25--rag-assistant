[site]: datascience
[post_id]: 69029
[parent_id]: 37281
[tags]: 
I've had a very similar situation. When I displayed images with plt.imshow() (matplotlib.pyplot.imshow() for the uninitiated ) They all looked perfectly fine, and I knew I wasn't doing any fancy pre-processing, so I figured my I was loading the data and "pre-processing" it just fine and identically for the train and test. Long story short, I re-scaled my training data to [0,1] but I forgot to do the same for my test data (I was loading it with a slightly different method than I did my training data), so my test data remained in the range [1,255]. That resulted in very bad metrics on the test set. plt.imshow() rescaled my data back so that I couldn't see the difference in scale between the images. Moral of the story, if you exhausted all possible causes for meaningful differences between your validation set and test set (assuming CNN, and image classification, and no likely overfit on hyperparameters leading to data leakage from the validation set), check again, because you clearly didn't. There is still some cause for difference in the distribution of your train, validation , and test sets.
