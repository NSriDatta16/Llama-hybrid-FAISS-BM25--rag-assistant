[site]: datascience
[post_id]: 67247
[parent_id]: 67236
[tags]: 
For predictive power, in general, including both shouldn't be a problem. But there is a lot of nuance here. Foremost, if predictive power isn't all you care about: if you're making statistical inferences, or care about explainability and feature importances, then including both can cause issues. Briefly, your model may split the importance of the underlying variable across all the derived ones. In some cases, it might not help at all: a tree model on your second example can already easily discover the derived variable given only the original. It may be actively harmful: adding too many of these derived variables might provide noise for your model to overfit to, rather than useful signal. In some cases, it might help a lot: in your first example, a linear classifier won't see the derived feature at all from the original, and a tree model would require several consecutive splits to see it. A neural network could build it, but it's not clear whether the training process would find it.
