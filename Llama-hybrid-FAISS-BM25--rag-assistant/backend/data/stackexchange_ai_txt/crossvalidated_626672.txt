[site]: crossvalidated
[post_id]: 626672
[parent_id]: 
[tags]: 
Statistical procedure to remove a time series which is an "outlier" in a set of replicated time series

This data are from measuring optical density in a bacterial growth experiment. These correspond to 4 time series which are biological replicates of exactly the same treatment (with the label 0_4) The series F6 looks like an "outlier", the trend is similar but the magnitude is totally different. This is evidenced further when we aggregate the data to find the mean and confidence band. Is there an statistical procedure/test to remove this (whole) time series which as "outlier" in the set of replicates, that takes into account every time? I first thought of calculate the difference between the "outlier" series and the aggregated mean and out everything into one score, but doesn't seem to be the right approach?.
