[site]: datascience
[post_id]: 68923
[parent_id]: 68637
[tags]: 
Bag of words representation Tackling your questions one-by-one, a classic way to represent bag of words in DNN solutions is not as a key-value representation, but as a "n-hot" vector. So if the entity consists of tokens #123, #456 and #567 (from a pre-computed dictionary), then you can have a vector with the length of the dictionary where all values are 0 but the bits #123, #456 and #567 are set to one. If precomputed embeddings are used, another option that I have seen is to add or average the embeddings for these multiple tokens - that way you have a fixed length representation of the entity, no matter if it has 1 token or 5 tokens.
