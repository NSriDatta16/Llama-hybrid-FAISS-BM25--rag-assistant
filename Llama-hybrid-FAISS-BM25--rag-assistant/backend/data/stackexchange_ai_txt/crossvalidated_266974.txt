[site]: crossvalidated
[post_id]: 266974
[parent_id]: 266968
[tags]: 
The reason to use mini batches is to have a good amount of training example such that the possible noise of it is reduced by averaging their effects, but also it's not a full batch that for many datasets could require a huge quantity of memory. One important fact is that the error that you evaluate is always a distance between your predicted output and the real output: that means that it can't be negative, so you can't have, as you said, an error of 2 and -2 that cancel out, but it would instead become an error of 4. You then evaluate the gradient of the error with respect to all the weights, so you can compute which change in the weights would reduce it the most. Once you do so, you take a "step" in that direction, based on the magnitude of your learning rate alpha. (This is the basic concepts, I'm not going into detail about backpropagation for deep NN) After running this training on your dataset for a certain number of epochs, you can expect your network to converge if your learning step is not too big to make it diverge. You could still end up in a local minimum , this can be avoided by initializing differently your weights, using differenr optimizers, and trying to regularize.
