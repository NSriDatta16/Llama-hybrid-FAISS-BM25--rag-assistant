[site]: crossvalidated
[post_id]: 609848
[parent_id]: 
[tags]: 
LSTM validation accuracy fluctuating

I am using LSTM to model time series data. My target variable is categorical so I am using one-hot encoding. The goal is to predict the target class based on the given time. My dataset spans over eight days. input_nodes = look_back = 10 batch_size = 128 train_generator = create_data_generator(train, look_back, outputs, batch_size, class_weights_dict) validation_generator = create_test_generator(test, look_back, outputs, batch_size) model = Sequential() model.add(LSTM(50, input_shape=(input_nodes, outputs))) #model.add(Dense(50, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(outputs, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network history = model.fit_generator(train_generator, steps_per_epoch=math.ceil(len(train)/batch_size), epochs=25, validation_data=validation_generator, validation_steps=math.ceil(len(test)/batch_size)) Here are my results. Is there anything that I can try to improve my validation accuracy and loss?
