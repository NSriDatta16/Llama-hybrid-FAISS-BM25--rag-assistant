[site]: datascience
[post_id]: 37109
[parent_id]: 
[tags]: 
Importance Sampling in Off-policy n-step Sarsa

In Chapter 7.3 of Reinforcement Learning: An Introduction by Sutton and Barto, the off-policy pseudocode has the following update equation for $Q$: Compute importance sampling ratio: $$ \rho \leftarrow \prod^{\min(\tau+n-1, T-1)}_{i=\tau+1} \frac{\pi(A_i \mid S_i)}{b(A_i \mid S_i)} $$ Compute truncated return $$ G \leftarrow \sum^{\min(\tau+n, T)}_{i = \tau+1} \gamma^{i-\tau-1} R_i $$ Compute discounted estimate: $$ \text{If } \tau+n Update $Q$ $$ Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha \rho [G - Q(S_\tau, A_\tau)] $$ ($\tau$ is the time whose estimate is being updated, $t$ is current timestep, $n$ is $n$-step return, and $T$ is termination timestep) I believe that the importance sampling ratio $\rho$ should only be multiplied to $G$ and not to $Q$, since $Q$ is for target policy $\pi$ and $G$ is from behavior policy $b$. In other words, this is how I see this equation: $$ Q_\pi \leftarrow Q_\pi + \alpha \rho_{b \to \pi} [G_b - Q_\pi]$$ Am I correct in thinking that this is a typo, or did I miss something? Thank you for your help!
