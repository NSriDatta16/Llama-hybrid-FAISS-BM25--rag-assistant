[site]: datascience
[post_id]: 77580
[parent_id]: 77298
[tags]: 
- Use RandomForest as XGBoost is more prone to overfitting and comparatively difficult to tune hyperparameters Tune at least these parm - param_grid = { 'n_estimators': [ ], 'max_features': [ ], 'max_depth' : [ ], 'criterion' :['gini', 'entropy']} - Try imputation based on your domain knowledge and using other Features e.g. Correleation - Scaling is not very much needed with Tree models - Monitor another metrics along with $R^2$ score. I mean being in the domain you must know how much error is "too much" . $R^2$ rewards useless Features, so be mindful of that and may use adjusted $R^2$ . - Have K=10 only when you have sufficient samples. Otherwise, try K=5,3. If we use K=10 on a small dataset, then the cross-val test-set will be very small and we may see a very high variance in the 10 different predictions. I suspect the same in your result. We have an output between 0.82 to 0.94 array([0.8484691 , 0.86808136, 0.91821645, 0.93616375, 0.94435934, 0.82065733, 0.84856025, 0.8267642 , 0.84561417, 0.89567455] - Feature selection/engineering - A very separate and broad topic in itself. Would only suggest trying multiple things and trying one thing at a time and maintaining a proper track of which activities resulted in what . It seems from the question that you are trying to do many things randomly.
