[site]: crossvalidated
[post_id]: 200390
[parent_id]: 200372
[tags]: 
Most successful/popular deep network architectures perform end-to-end training: we simultaneously learn a feature representation and a classifier. Typically the classifier is a linear classifier, like a softmax classifier, or an SVM classifier. So in order to these sorts of architectures to perform well, the corresponding feature representation portion of the network, which is essentially everything but the last classification layer, must be able to linearly separate the raw input data. Or try to separate the raw input data as best as it can. In general, it need not be this way: you can have a nonlinear classifier with linear features, nonlinear feature representation with nonlinear classifiers, etc. However there seems to be a very large emphasis on learning feature representations and empirically we have seen lots of breakthroughs on various benchmark tasks (ImageNet, Microsoft COCO, Pascal). It appears that investing most of the computational budget to learning good feature representations has been very effective and seems to be a standard practice (as of now). For many learning tasks, particularly those in the "AI realm" (natural language processing, computer vision tasks, speech recognition, etc), the raw features are "highly entangled" and in particular are not able to be separated by a linear transformation. Just like @Net_Raider mentioned in their answer: a composition of linear maps is a linear map, so there is no real notion of "depth" with linear networks. In the above context: by only using linear layers, we are only able to learn linear transformations. For complex data, this is simply not sufficient to linearly separate the data and learn good features. In the current "standard" supervised deep net architecture, this is bad since we ultimately are learning a linear classifier on top of our learned features. Using non-linearity now allows us to have a real notion of depth and also allows us to learn non-linear transformations. Now we may learn a non-linear transformation which maps our raw input features into a space where they are linearly separable.
