[site]: crossvalidated
[post_id]: 412811
[parent_id]: 412479
[tags]: 
The standard (in-bag?) predictions are generated using all the trees in the random forest. Because the data points being predicted are part of the fitting procedure, this is susceptible to overfitting. These predictions will therefore tend to understate the error that you would encounter when making predictions on a new dataset. Out-of-bag (OOB) predictions take advantage of bagging , a feature of random forests (but not exclusive to them). Every tree in the random forest is fitted to only 2/3 of the data points in the dataset, and so every data point is used for fitting by only 2/3 of the trees in the forest. OOB predictions for a specific data point are generated by using only the 1/3 of the trees that do not have that data point in them to generate the predictions. This is repeated for all data points. This provides a more unbiased estimate of the prediction error of the random forest. In principle, it should give you a similar error rate to that when making predictions on a new dataset. In practice, I have not really encountered a rigorous test of this - I would be very interested to know if this actually holds.
