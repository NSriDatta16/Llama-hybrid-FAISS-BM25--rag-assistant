[site]: crossvalidated
[post_id]: 554815
[parent_id]: 
[tags]: 
Approximating a countable-state (infinite) Markov model with a finite-state one

TL;DR—in a nutshell I have a countable-state Markov model (with a countably infinite number of states) in which the probability of transitioning to states $S_{i>k}$ for large $k$ s are practically zero, thus negligible. Given this, I want to approximate the infinite-dimensional transition matrix with a finite transition matrix of size $k$ , retaining only the first $k$ rows and columns. However, although the off-diagonal elements are close to zero for the states beyond $k$ , the diagonal elements have non-zero values. The question is: How can I make the smaller, approximate, transition matrix without messing with row-wise/column-wise properties of the stochastic transition matrix? Background and context (You may skip) I am working with Jacob and Lewis's (1978) discrete autoregressive model (DAR), which can accommodate an AR(1)-like conditional expectation for any arbitrary marginal distribution as long as it is discrete. In short, if you have an iid sequence $Z_t \sim \Pi$ (where $\Pi$ is a discrete distribution such that $P(z=i)=\pi_i$ ), and an independent binary iid sequence $V_t \sim binom(1, \tau)$ (wherein $0 \leq \tau \leq 1$ ), the sequence $$ X_t = V_t X_{t-1} + (1-V_t)Z_t = \begin{cases} Z_t, & \mbox{if } V_t = 0 \\ X_{t-1}, & \mbox{if } V_t = 1 \end{cases} $$ is a $DAR(1)$ process with the same marginal distribution as $Z_t$ (i.e., $X_t \sim \Pi$ ), with the conditional expectation of $$ E[X_t|X_{t-1}] = \tau X_{t-1}, $$ and an AR(1)-type given by $\rho(l) = \tau^l$ for nonnegative lags $l$ . The $DAR(1)$ model is a Markov process with transition probabilities given by $$ p_{i|j} = P(X_t=i | X_{t-1} = j)= \begin{cases} (1-\tau)\pi_i, & \mbox{for } j \neq i \\ \tau + (1-\tau)\pi_i, & \mbox{for } j = i \end{cases}. $$ Note that I write transition matrices as left-stochastic matrices, such that columns add to one. Situation at hand Since $DAR(1)$ can take any parametric or nonparametric discrete marginal distribution, I am trying to use a Poisson distribution for $\Pi$ such that $\pi_i = \frac{\lambda^i e^{-\lambda}}{i!}$ . In this case, by setting $\upsilon = 1 - \tau$ for readability, the elements of the (left-)stochastic transition matrix (for states $S_i \in {0, 1, 2, \ldots}$ ) are $$ t_{ij} = p_{i|j} = \begin{cases} \upsilon\frac{\lambda^i e^{-\lambda}}{i!}, & \mbox{for } j \neq i \\ \tau + \upsilon\frac{\lambda^i e^{-\lambda}}{i!}, & \mbox{for } j = i \end{cases}; \ \ \ i,j = 0, 1, 2, \ldots $$ Thus the transition matrix (with elements starting from 0) would look like $$ T_1 = \begin{bmatrix} \tau + \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \ldots & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \ldots \\ \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \tau + \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \ldots & \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \ldots \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \ldots & \tau + \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \ldots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{bmatrix} $$ Since $\frac{\lambda^i e^{-\lambda}}{i!} \xrightarrow{i>k} 0$ quite quickly for large-enough $k$ , the off-diagonal elements of $T_1$ become practically zero (for my application, $0 and $k \approx 20$ ). Thus, I want to reduce $T_1$ to a $k \times k$ matrix $T_2$ Problem and questions, specifically: From what I have said, it seems reasonable to make the following approximation: $$ T_2 = \begin{bmatrix} \tau + \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \ldots & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & 0 & 0 & \ldots \\ \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \tau + \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \ldots & \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & 0 & 0 & \ldots \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\ \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \ldots & \tau + \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & 0 & 0 & \ldots \\ 0 & 0 & \ldots & 0 & 0 & 0 & \ldots \\ 0 & 0 & \ldots & 0 & 0 & 0 & \ldots \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \end{bmatrix} \approx T_1 $$ Then, to derive a finite-state Markov model, I want to formulate $T_3 for large enough $ k$$ such that for a large enough $k$ I can "show"/"argue" that $$ T_3 = \begin{bmatrix} \tau + \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} & \ldots & \upsilon\frac{\lambda^0 e^{-\lambda}}{0!} \\ \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \tau + \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} & \ldots & \upsilon\frac{\lambda^1 e^{-\lambda}}{1!} \\ \vdots & \vdots & \ddots & \vdots \\ \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \upsilon\frac{\lambda^k e^{-\lambda}}{k!} & \ldots & \tau + \upsilon\frac{\lambda^k e^{-\lambda}}{k!} \end{bmatrix} \equiv T_2 $$ I have difficulty regarding the first approximation $T_2 \approx T_1$ it for a couple of reasons: Although the off-diagonal elements of $T_1$ are almost zero for improbable states ( $i,j>k$ ), the diagonal elements are non-zero (they have an additional $\tau$ in them). So, the approximation isn't simply replacing all improbable elements with zeros. By replacing the improbable elements with zero, $T_2$ is no longer a (left-)stochastic matrix: The columns would not add to one. There would be a similar problem for the marginal distribution of $T_2$ is messed up and cannot be derived from the Bayes rule applied to rows of $T_2$ . More specifically, $\hat \pi_i = \sum_j (p_{i|j}\cdot \pi_i) \neq \pi_i$ . Following 3, the "marginal probability" (estimated using the Bayes rule in 3) doesn't add to one: $\sum_i \hat\pi_i \neq 1$ . And there is another problem in deriving the final conclusion that the finite-state Markov model described by $T_3$ being an approximation of the countable-state Markov model described by $T_1$ : How can one, algebraically, think of "equivalence" $T_3 \equiv T_2$ between matrices of different sizes? One solution to prevent the whole mess would have been using another marginal distribution with (upper-)bounded support. (Maybe an upper-truncated "Poisson" distribution, if exists?) However, I cannot do that because it has implications that I must avoid for substantive/practical reasons. I really appreciate your thoughts and help. Thanks in advance! Reference I cited Jacobs, P. A., & Lewis, P. A. W. (1978). Discrete Time Series Generated by Mixtures. I: Correlational and Runs Properties . Journal of the Royal Statistical Society . Series B (Methodological), 40(1), 94–105. https://sci-hub.se/https://www.jstor.org/stable/2984870
