[site]: stackoverflow
[post_id]: 5759258
[parent_id]: 
[tags]: 
Chunk Size Issues in Google Filesystem

Google File System Paper - Chunk size is one of the key design parameters. We have chosen 64 MB, which is much larger than typical file sys- tem block sizes. Each chunk replica is stored as a plain Linux file on a chunkserver and is extended only as needed. Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size. What is lazy space allocation and how is it going to solve the internal fragmentation problem? A small file consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file ... We fixed this problem by storing such executables with a higher replication factor and by making the batch- queue system stagger application start times. What is staggering application start times and how does it avoid chunk-servers from becoming hot-spots?
