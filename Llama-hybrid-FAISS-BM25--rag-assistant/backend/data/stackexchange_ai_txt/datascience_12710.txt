[site]: datascience
[post_id]: 12710
[parent_id]: 
[tags]: 
Why does xgboost give this unexpected result?

This is a really simple example where my training data has a single feature vector (1,2,3) and an equivalent target vector (1,2,3). I can get xgboost to build a regression tree that perfectly fits the data, but when I use it to make predictions, the predictions against the training data, the predictions don't match the target. What gives? train xgb.plot.tree("f1", model=bst) xgb.model.dt.tree("f1", model=bst) ID Feature Split Yes No Missing Quality Cover Tree Yes.Feature Yes.Cover Yes.Quality No.Feature No.Cover No.Quality 1: 0-0 f1 2.5 0-1 0-2 0-1 1.50 3 0 f1 2 0.50 Leaf 1 0.75 2: 0-1 f1 1.5 0-3 0-4 0-3 0.50 2 0 Leaf 1 0.15 Leaf 1 0.45 3: 0-2 Leaf NA NA NA NA 0.75 1 0 NA NA NA NA NA NA 4: 0-3 Leaf NA NA NA NA 0.15 1 0 NA NA NA NA NA NA 5: 0-4 Leaf NA NA NA NA 0.45 1 0 NA NA NA NA NA NA
