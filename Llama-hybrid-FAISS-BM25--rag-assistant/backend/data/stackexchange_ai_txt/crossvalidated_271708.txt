[site]: crossvalidated
[post_id]: 271708
[parent_id]: 158404
[tags]: 
If you already have a prior $p(\theta)$ and a likelihood $p(x|\theta)$, then you can easily find the posterior $p(\theta|x)$ by multiplying these and normalizing: $$p(\theta|x)=\frac{p(\theta)p(x|\theta)}{p(x)}\propto p(\theta)p(x|\theta)$$ https://en.wikipedia.org/wiki/Posterior_probability The following code demonstrates estimating a posterior represented as a histogram, so it can be used as the next prior: import numpy as np import matplotlib.pyplot as plt from scipy.stats import gaussian_kde # using Beta distribution instead on Normal to get finite support support_size=30 old_data=np.concatenate([np.random.beta(70,20,1000), np.random.beta(10,40,1000), np.random.beta(80,80,1000)])*support_size new_data=np.concatenate([np.random.beta(20,10,1000), np.random.beta(10,20,1000)])*support_size # convert samples to histograms support=np.arange(support_size) old_hist=np.histogram(old_data,bins=support,normed=True)[0] new_hist=np.histogram(new_data,bins=support,normed=True)[0] # obtain smooth estimators from samples soft_old=gaussian_kde(old_data,bw_method=0.1) soft_new=gaussian_kde(new_data,bw_method=0.1) # posterior histogram (to be used as a prior for the next batch) post_hist=old_hist*new_hist post_hist/=post_hist.sum() # smooth posterior def posterior(x): return soft_old(x)*soft_new(x)/np.sum(soft_old(x)*soft_new(x))*x.size/support_size x=np.linspace(0,support_size,100) plt.bar(support[:-1],old_hist,alpha=0.5,label='p(z)',color='b') plt.bar(support[:-1],new_hist,alpha=0.5,label='p(x|z)',color='g') plt.plot(x,soft_old(x),label='p(z) smoothed',lw=2) plt.plot(x,soft_new(x),label='p(x|z) smoothed',lw=2) plt.legend(loc='best',fontsize='small') plt.show() plt.bar(support[:-1],post_hist,alpha=0.5,label='p(z|x)',color='r') plt.plot(x,soft_old(x),label='p(z) smoothed',lw=2) plt.plot(x,soft_new(x),label='p(x|z) smoothed',lw=2) plt.plot(x,posterior(x),label='p(z|x) smoothed',lw=2) plt.legend(loc='best',fontsize='small') plt.show() If, however, you want to combine your empirical prior with some MCMC models, I suggest you take a look at PyMC's Potential , one of its main applications is "soft data". Please update your question if you need an answer targeted towards that.
