[site]: crossvalidated
[post_id]: 253088
[parent_id]: 
[tags]: 
Why does cost function increases over time? (OpenAI cartpole)

I was trying the cartpole environment from OpenAI gym. The cost function goes up with time and the reward function goes down. I have no clue why it happens and how to solve it. Top: Cost, bottom: Reward import numpy as np import tensorflow as tf import gym import matplotlib.pyplot as plt env = gym.make('CartPole-v0') episodes = 1000 batchSize = 50 simulationSteps = 200 maxExperienceSize = 400 trainingSessions = 10 skip = 10 decimals = 2 # Hyper parameters epsilon = 0.0 learningRate = 1e-4 gamma = .9 # Handles state = tf.placeholder(tf.float32,[None,4],name='state') action = tf.placeholder(tf.float32,[None,1],name='action') nextQ = tf.placeholder(tf.float32,[None,1],name='nextQ') # Model def model(state,action): stateAction = tf.concat(1,[state,action],name='stateAction') weights1 = tf.Variable(tf.random_normal([5,3]),name='weights1') weights2 = tf.Variable(tf.random_normal([3,1]),name='weights2') activations1 = tf.nn.relu(tf.matmul(stateAction,weights1),name='activations1') activations2 = tf.nn.relu(tf.matmul(activations1,weights2),name='activations2') return activations2 prediction = model(state,action) cost = tf.reduce_sum(tf.squared_difference(prediction,nextQ)) optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost) experience = [] avgRewards = [] costs = [] def getQValues(sess,s): fd = {state:[s],action:[[0.0]]} q0 = sess.run([prediction],fd) fd = {state:[s],action:[[1.0]]} q1 = sess.run([prediction],fd) return [q0[0],q1[0]] with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for episode in range(episodes): env.reset() totalReward = 0 resets = 1 for t in range(simulationSteps): if len(experience) > maxExperienceSize: break lastState = tuple(np.around(env.state,decimals)) # Take a random action a = env.action_space.sample() # Otherwise query the neural network if np.random.uniform() > np.exp(-epsilon): q = getQValues(sess,env.state) a = np.argmax(q) observation, reward, done, info = env.step(a) totalReward += reward qPrime = getQValues(sess,env.state) newQ = reward + gamma * max(qPrime) # Experience table row = (lastState,(a,),tuple(np.around(newQ[0],decimals))) experience.append(row) if done: env.reset() resets += 1 # Remove duplicates experience = np.vstack({tuple(row) for row in experience}) totalCost = 0 for _ in range(trainingSessions): # Shuffle the experience np.random.shuffle(experience) # Get a batch batch = np.array(experience[:batchSize]) states = np.array(list(batch[:,0])) actions = np.array(list(batch[:,1])) nextQs = np.array(list(batch[:,2])) fd = {state:states,action:actions,nextQ:nextQs} c,_ = sess.run([cost,optimizer],fd) totalCost += c experience = list(experience) # Forget half the experience if len(experience) >= maxExperienceSize: experience = experience[:int(maxExperienceSize/2)] epsilon += 2/(episodes) if episode % skip == 0: costs.append(totalCost/trainingSessions/batchSize) avgRewards.append(totalReward/resets) print(str(episode)+'\t'+str(costs[-1])+'\t'+str(avgRewards[-1])+'\t' +str(len(experience))) plt.figure(1) plt.subplot(211) plt.plot(costs) plt.subplot(212) plt.plot(avgRewards) plt.show() I decreased the learning rate from 1e-3 to 1e-4 but no luck. EDIT 1: Adding a bias term solved the cost issue but the reward just wont go up.
