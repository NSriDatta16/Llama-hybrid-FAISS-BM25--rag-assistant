[site]: crossvalidated
[post_id]: 459537
[parent_id]: 459512
[tags]: 
It depends on what you mean by "proves to be a valid fact." If, by "proves to be a valid fact," you mean that you should reject the null if the value is below a cutoff that you set, then the answer is "yes," but with caveats. So, you should reject the null but you must be aware that does not mean that the null is false. It means that you should behave as if the null is false because you have no other information to work with. If, by "proves to be a valid fact," you mean that a low p-value indicates that the null is not true, then that is not a valid statement. Rejecting the null does not imply that the null is false. That is because the null hypothesis method in statistics cannot distinguish between the case where a null hypothesis is false and the case where an extreme set of observations happened by chance. Of course, if there is a bad methodology, then a p-value means nothing at all. Consider the following three cases for the z-test where $\mu\in\{0,0.0001,1\}$ and $\sigma^2=1$ and the null hypothesis is $\mu=0$ in all three cases. You set a cutoff of $p . Now, p-values are not real probabilities unless you repeat the experiment an infinite number of times and the null is true. I ran a simulation with 100,000 samples, each with a sample size of 30. For the first experiment, the null was incorrectly rejected 10.009% of the time when $\mu=0$ . In the second experiment, the null was incorrectly accepted 89.998% of the time when $\mu=.0001$ , and in the third experiment, the null was incorrectly accepted .004% of the time. So let us think about what this would mean. First, p-values assume that you repeat the experiment an infinite number of times. I only did it 100,000 times and so the rejection is slightly higher than the set cutoff would allow on infinite repetition. It was close. Another way to think about it is to imagine I did one repetition. If there were a false rejection, then the rejection rate would be 100% of the time. In the second and third experiments, the null was false. The problem in the second experiment, however, is related to the concept of power . Power asks the question "is my sample size large enough to detect a small difference between a null hypothesis and the real value?" A sample size of 30 is way too small to be able to detect a true value that is only .0001 from the null. As a result, the answer was wrong close to 90% of the time. In the third example, there is a ton of power available. It only stated that the null was true 4 times in 100,000 experiments. It rejected the null every other time. The code for the R simulation is: library(ggplot2) library(export) library(quantreg) library(LaplacesDemon) library(parallel) library(cowplot) rm(list = ls()) set.seed(500) n_samples $p_values[output$ p_values Sorry, the code is a little sloppy. It isn't commented and it has unnecessary library functions in it. EDIT Okay, so let us discuss what a p-value is and what a p-value is not. Unfortunately, this cannot be a straightforward discussion because it turns out that a lot is going on inside these concepts. There are three main branches of probability and statistics, and several minor ones as well. Two of those branches use p-values as a concept. They interpret them differently. Let us stick with the simple z-test above because they would, numerically, end up with the same result. That is not always true. The older of the two methods originated in another branch of probability, Laplace’s method of inverse probability. That method is now called Bayesian statistics. Its child is called the method of maximum likelihood. Ysidro Edgeworth and William Gosset (who went by Student to hide his identity) did the primary work on this. That work would, brilliantly, be picked up by Ronald Fisher. Fisher’s involvement began innocently enough when he was a young post-doc. He was working under the supervision of Muriel Bristol, and it was tea time in the United Kingdom. Fisher was a prickly sort of person, and Dr. Bristol sent him to make her some tea. He brought the tea back, and she tasted and rejected it because he made it incorrectly. He poured the milk into the tea instead of the tea into the milk. He insisted the order did not matter; she insisted that it did. If you have ever watched “The Big Bang Theory,” imagine telling Sheldon Cooper that he is wrong. As a technical aside, it does matter. If you pour milk into the tea, it burns the proteins in the milk, and it creates a very different flavor. Because I prefer bitter foods, I prefer it poured incorrectly. The room, being filled with British PhDs debating the proper pouring of tea, turned into a gambling den. Fisher proposed that any differences were purely due to chance and that Dr. Bristol could not actually tell the difference. Dr. Bristol argued it was not due to chance. Wagers ensued, and a blind taste test experiment was performed. Fisher realized that if it really was due to chance, then there was a 50% chance on each cup of Dr. Bristol correctly identifying how each cup was poured. If she got every cup correct, then the probability of that happening if his model was right was $\left(\frac{1}{2}\right)^8$ . Note the dependency on the null. If his null had been something else, such as there being a 40% chance of her choosing any one correctly, then the probabilities would be different. That is the very first null hypothesis ever created, at least as far as we know of. The methods by Laplace that precede this do not have any concept like a null hypothesis. The null determines what the probabilities would be. Note, however, there is an implicit implication of infinite repetition for these probabilities of holding. If Fisher’s model of chance-based selection were valid, then there would be nine possibilities. Minimally, she could get every cup wrong. Maximally, she could be correct every time. All options in between could also happen in any one setting. All possibilities can arise from chance alone. If Fisher is right, then they all must be observed eventually. The graph of those long-run mass functions is below. However, because getting eight cups correct is improbable under the null, Fisher felt it was reasonably safe to reject the null if that were observed. Note, however, there is no alternative hypothesis. All that exists is the null. In Fisher’s understanding, if you reject the null, then you possess new knowledge. Subject to the p-value, one can tentatively reject the null as valid. That is knowledge. Because the extreme events are supposed to happen by chance from time to time, you may reject the null in error. Because of this, rejecting the null does not mean it is true. It implies that you must continue your investigations along these lines. If you did not reject the null, you do not accept the null as true in Fisher’s thinking. If the null is not rejected, then you go on with life and investigate new things. Fisher did not use a cutoff value, though many pestered him for it, and he told them 5%. A p-value, to Fisher, was the weight of the evidence against the null hypothesis and nothing more. From Fisher’s perspective, you rejected the null hypothesis if you felt the p-value was extreme enough for you to do so. Such a perspective implies that I can reject the null, and you cannot reject the null upon seeing the same data. It depends on how important the result is to us, our skepticism, and the utility of the result. You could imagine that you want a different p-value to choose a toothpaste at the store than to select a wife. You should probably also have a different p-value if your concern is whether to reject items as defective in inventory or to trust a signal implying nuclear weapons launch against your country by another country. Let us assume that Muriel Bristol believed that her skill was such that she would correctly identify 19 in 20 cups of tea. She conceded she could make the rare error. Under her null, the mass function is graphed below. As the experiment turned out, Dr. Bristol correctly identified eight out of eight cups of tea. That would reject Fisher’s null. The mythical null that we created for Dr. Bristol would not be rejected. In Fisher’s world, that would imply that he should investigate the chemical causes of the differences. In Dr. Bristol’s world, there is nothing to do. The world is as she believed it to be. Fisher rejected his null, but he might have dismissed it in error. We now have a chemical explanation for the effect, but it did not exist at the time. Some rejections of the null are false discoveries. The first use of the null hypothesis in the literature was by Fisher to support Mendel’s laws. His null hypothesis was that Mendel’s laws do not affect the descendants of a mating pair. He was invoking a form of modus tollens. By rejecting the null of “no effect,” all explanations except Mendel’s laws were rejected with the test. “Anything other than Mendel,” is the same thing as “Mendel’s laws have no effect.” That is how we know evolution is valid. In the hundreds of thousands of tests of evolution, the null hypothesis has always been that evolution is false. Nature itself rejects that null. That brings us to the second understanding of a p-value, that of Pearson and Neyman. Pearson and Neyman took Fisher’s method and changed it for a variety of reasons. However, going back to our z-test above, they were as concerned with false rejections as with false acceptances. It can be as important to tell someone they have cancer when they do not as to tell someone they do not have cancer when they really do. All that Fisher’s method really does is tell you that you may have new knowledge. Of course, that is all he wanted it to do. Pearson and Neyman built a system of inference that they constructed around the concept of a minimum variance unbiased estimator instead of the maximum likelihood estimator. For the z-test, the numerical results are identical, but not the interpretation. They discovered that they could control false positives and negatives if they fixed a cutoff p-value. The calculated p-value no longer matters. What was required was a cutoff for the p-value and a sample size large enough to assure that the desired level of statistical power existed to measure the size of effect reliably. If you set your $\alpha , then it does not matter if $p=.04$ or $p=.06$ because either is less than 10%. The measured p-value does not matter and contains no informational content. They created two regions. The first region is the acceptance region; the second is the rejection region. That linked, or at least potentially linked, inference and decisions about that inference. If you needed to use the inference in the real world, then that would dictate a decision. Fisher’s method just provided inference. It did not provide direction. If the measured result is in the rejection region and adequate power existed, then you should behave as if it is false. If it is in the acceptance region, then you should behave as if the null were true. As with Fisher’s method, this method cannot distinguish between extreme random events and true false nulls. However, by setting a cutoff, $\alpha$ , and controlling for power, what you have done is that you have controlled the frequency that the methodology will make a fool of you. A final note, of the three main interpretations of probability, the Bayesian, the Fisherian, and the Frequentist (Pearson and Neyman), none of them are superior to the others. Each one is better at some types of problems and worse at others. They often generate different numerical results. The two methods that use p-values interpret them differently. Both depend on the null being treated as how nature really works. Both depend on repeating experiments because one result cannot distinguish chance effects where the null is true from a false null. That makes a p-value a tentative conclusion.
