[site]: crossvalidated
[post_id]: 338167
[parent_id]: 
[tags]: 
If LDA (Linear Discriminant Analysis)'s bases are not orthogonal, does it make sense to re-construct the input dimension?

PCA and LDA are two of the most widely used factor analysis tools. PCA construct the orthogonal basis that capture the maximum variance of the input space, $AP=PD$ , where each column of $P$ is an eigenvector. Since each column is orthogonal by definition, we can project the low-dimension data back to the original dimension. This nice property can serve as a "anomaly detection" tool. However, LDA does not have this property. The basis we build are not orthogonal anymore. Therefore, I think it does not make sense to project the data back from the low dimension manifold. I did some experiments: if I back-project the low-dimension 'features' back to the original dimension, the reconstructed data will be hugely different from raw data. So, can I say that LDA is more a dimension-reduction tool? Surely we can do classification in the low-dimension feature space, but that's not what I want to discuss in this post.
