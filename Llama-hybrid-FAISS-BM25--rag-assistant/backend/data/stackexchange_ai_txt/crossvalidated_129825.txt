[site]: crossvalidated
[post_id]: 129825
[parent_id]: 
[tags]: 
How to estimate the percent of the variation of a time series explained by another time series (non-stationary)?

I've been learning about time series analysis because I want to understand how much groundwater level changes in an aquifer affect land subsidence (land sinking). I have two time series: (1) measurements of aquifer water levels and (2) measurements of relative land surface movement. Both series are regularly sampled; monthly measurements for 40 years (480 observations). First, I decomposed each of the time series for exploratory purposes (the series seemed like "trend stationary"). Then conducted ADF tests to check for stationarity, for which I found they're not. Proceeded to run ADF tests on the 1st differences for each time series and found that the differences are stationary. Finally, I ran a cross-correlation on the differences and got that the series are correlated at different lags. From the literature, I know that groundwater levels influence land surface elevation. Just by looking at the plots, I can see that the rate at which the land was sinking has slowed down and reversed as the aquifer water levels recovered over time. After all the time series analysis research I've done, I still cannot grasp how to get at an estimate of the percent of the variation in land surface movement explained by groundwater level.
