[site]: crossvalidated
[post_id]: 330907
[parent_id]: 330897
[tags]: 
where D is a discrepancy statistic, d is the observed discrepancy. As I understand it, D gives a measure of how "inconsistent" the observed data is with the null hypothesis. No, d is the discrepancy statistic and a measure of inconsistency. D is just a dummy variable used for calculating p. D=0 corresponds for the "best evidence" to support the null hypothesis, whereas larger and values of D indicate that the data is less consistent with H0. d=0 is best evidence for the null in a two-tailed test, but for a one-tailed test, the best evidence for the null is as far away from the tail as possible. In this case, since you're testing the right tail, the best evidence for the null would be $-\infty$. So, in my textbook, we have the following table: That table's not quite right. I can understand how, if p is small and d is small, that's evidence supporting H0, since the probability of even a moderately high discrepancy is very low, meaning discrepancies will generally be near 0. This sort of hypothesis testing either rejects or fails to reject the null hypothesis. One is on shaky grounds claiming that low p provides "evidence" against the null, and even shakier ground claiming that a high p provides evidence for the null. I can understand how, if p is large and d is large, that's evidence against H0, since if p is large and d is large then we still have a very high probability of discrepancies that are very far from 0, which is very inconsistent with H0. As @Nuclear Wang said, you have it backwards. [taken from comments] what if we were to run the experiment and we "accidentally" got a very large value of d even when thats statistically very unlikely, and then we got a very small p value. We would conclude that it's "very strong evidence against H0" when it's not really, right? The p value is a measure of how likely "accidentally" getting a large d is. You can get a large d either from the null hypothesis being false, or from being "unlucky". The smaller the p is, the less chance you have of being "unlucky", and therefore the more confident you can be that you got it from the hypothesis being false. In Bayesian terms, E is evidence for H if P(E|H)>P(E|~H). So to say whether E is evidence for H, one has to have both the probability if the null hypothesis is true, and the probability if the null hypothesis is false, and with this sort of hypothesis testing the latter is not as well defined. Note, however, that whether something is "evidence" depends on its conditional probabilities. It does not depend on whether the hypothesis is actually true. A high d would be evidence against H0. If you later found out that H0 was true all along, that doesn't mean that the d wasn't evidence against H0. Evidence is about what probabilistic conclusions you can derive from the evidence at hand , not what's true from an omniscient perspective. If we had an omniscient perspective, we wouldn't need to be doing any of this in the first place. For example, we could take p(Y)=P(D>D(Y) | H0) What does that mean? What's D? What's D(Y)? For example, p would look linear if the probability of getting any discrepancy was equal (that is, D is uniformly distributed), which is strong evidence against H0, right If p is a function, then it does not depend on the observed data (and here I am making a distinction between a function varying, and the value of a function varying), so it's can't possibly be evidence for or against H0.
