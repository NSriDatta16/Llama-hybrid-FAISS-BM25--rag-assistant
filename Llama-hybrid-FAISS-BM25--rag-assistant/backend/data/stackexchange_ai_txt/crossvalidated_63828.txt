[site]: crossvalidated
[post_id]: 63828
[parent_id]: 
[tags]: 
Beyond observable and unobservable data, is there any term "semi-observable" defined?

When dealing with data in fields such as Natural Language Processing(NLP) or Speech Recognition(ASR) and trying to model the data using Hidden Markov Model(HMM) one should first make it clear that if the available data is observable or unobservable. When applying Markov property to the data, that is the current data probability at time T is only dependent on the data at time T-1 (or more). If all the variables in probability function are observed in the training data, it is called observed data, thus could be trained simply using Maximum Likelihood criterion. In cases, that there are variables in the probability function, which are not observed in the training data it is called a "hidden variable" and so using HMM and Expectaion-Maximization(EM) training criterion, by just defining the number of hidden states, it can be trained. I'm trying to address a special case, in which there are many hidden states (e.g. 100 000 hidden states) which makes it impossible for EM to train the model. By some assumption on the model, by observing each training instance and using the assumptions, the model can restrict the model to a limited number of states among all those hidden states, e.g. every instance is limited to 10-20 states among 100 000 hidden states. This special case has been worked in some problems such as Hidden Vector States proposed by Yulan He. Is this called "semi-observable" case or it's just a hidden variable with some simple assumptions made it possible to be trained? How is it converted to a standard HMM problem? EDIT: In case there is some ambiguities about the terms 'hidden' or 'observable', please read this tutorial on Hidden Markov Model by Rabiner describing all these terms a great manner. EDIT2: This cool tutorial on Expectation-Maximization has a good and brief explanation on 'hidden variables' too.
