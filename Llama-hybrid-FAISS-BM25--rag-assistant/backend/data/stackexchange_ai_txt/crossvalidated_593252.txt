[site]: crossvalidated
[post_id]: 593252
[parent_id]: 
[tags]: 
How to apply Bayes' rule when new information makes a parameter known?

Suppose 100 balls where it is known that 1 is red and 99 are blue. Let y=1 if I select the red ball and 0 otherwise. All of this implies the marginal probability Pr(y=1) = 1/100. Now I notice some balls are shaped differently. With certainty I can then identify 20 balls that are not red. Given this info (call it x), the conditional probability that I choose the red ball would be Pr(y=1|x) = 1/80. What is confusing to me is how a Bayesian would use this information to obtain a posterior. In this case, x is presumably a likelihood (or at least info for updating my beliefs), 1/100 is the marginal probability...but what is then the prior? Is the prior simply the marginal? How would I represent x as a likelihood? More generally: I see how we can use Bayes' rule to update our beliefs if the evidence we observe is treated as a draw from the unknown posterior distribution. However,if the information we receive is a KNOWN quantity, how do we use this in Bayes' rule?
