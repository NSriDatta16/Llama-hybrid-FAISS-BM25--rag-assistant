[site]: datascience
[post_id]: 14680
[parent_id]: 
[tags]: 
Does the Bishop book imply that a neuron feeds to itself in chapter 5.3?

I just read Bishop's book Pattern Recognition and Machine Learning . I read the chapter 5.3 about backpropagation, and it said that, in a general feed-forward network, each unit computes a weighted sum of its inputs of the form $$a_j=\sum\limits_{i}w_{ji}z_i$$ Then the book says that the sum in the above equation transformed by the non-linear activation function $h(.)$ to give the activation $z_j$ of unit $j$ in the form $$z_j=h(a_j)$$ I think the notation is somehow akward: suppose I want to compute $a_2$, then $$a_2=w_{21}z_1+w_{22}z_2+\dots$$ Then does $$a_2=w_{21}z_1+w_{22}h(a_2)+\dots$$ mean that the neuron $a_2$ is connected to itself?
