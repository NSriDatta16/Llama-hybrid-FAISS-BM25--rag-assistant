[site]: crossvalidated
[post_id]: 63199
[parent_id]: 63152
[tags]: 
Let us take the case of classification. What the output layer is trying to do is estimate the conditional probability that your sample belongs to a given class, i.e. how likely is for that sample to belong to a given class. In geometrical terms, combining layers in a non-linear fashion via the threshold functions allows the neural networks to solve non-convex problems (speech recognition, object recognition, and so on), which are the most interesting ones. In other words, the output units are able to generate non-convex decision functions like those depicted here . One can view the units in hidden layers as learning complex features from data that allow the output layer to be able to better discern one class from another, to generate more acurate decision boundaries. For example, in the case of face recognition, units in the first layers learn edge like features (detect edges at given orientations and positions) and higher layer learn to combine those to become detectors for facial features like the nose, mouth or eyes. The weights of each hidden unit represent those features, and its output (assuming it is a sigmoid) represents the probability that that feature is present in your sample. In general, the meaning of the outputs of output and hidden layers depend on the problem you are trying to solve (regression, classification) and the loss function you employ (cross entropy, least squared errors, ...)
