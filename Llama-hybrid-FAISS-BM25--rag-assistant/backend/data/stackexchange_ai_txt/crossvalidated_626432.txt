[site]: crossvalidated
[post_id]: 626432
[parent_id]: 272791
[tags]: 
A more obvious thing that is done frequently (e.g. in Kaggle competitions that have featured prediction interval coverage in their scoring metric) is to have more than one layer with non-linear activation functions (e.g. ReLU), which leads to richer representations/more complex functional space (e.g. the proposal above cannot capture interactions between inputs), output both the point estimate and the prediction interval limits from the same neural network, you can achieve that by firstly outputting the estimate as usual and two quantities (or one, if you want to impose equal-length of the intervals in both directions) that enter an activation that goes from $(-\infty, \infty)$ to $(0, \infty)$ (e.g. exponential) and give how far the interval limits are below and above the estimate (quantile regression techniques, e.g. pinball loss are options for the loss function here). You just need to balance the trade-off between interval coverage and estimation loss. Doing this through a single network has the potential to gain on each task through multi-task learning (which may lead to better internal representations of the data) and seems to usually do better than a multi-step process.
