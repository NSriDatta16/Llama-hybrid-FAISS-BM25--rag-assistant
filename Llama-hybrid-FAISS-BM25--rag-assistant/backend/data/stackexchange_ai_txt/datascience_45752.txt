[site]: datascience
[post_id]: 45752
[parent_id]: 45750
[tags]: 
In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. Source :: https://en.wikipedia.org/wiki/Vanishing_gradient_problem Thus, gradient is vanishing till it reaches initial layer of neural network and in turn very little change in weights.
