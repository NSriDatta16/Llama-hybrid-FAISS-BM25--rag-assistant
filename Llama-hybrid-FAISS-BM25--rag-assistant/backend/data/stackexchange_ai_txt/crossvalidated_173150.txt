[site]: crossvalidated
[post_id]: 173150
[parent_id]: 173060
[tags]: 
The tricky thing about Big Data vs. its antonym (presumably Small Data?) is that it is a continuum. The big data people have gone to one side of the spectrum, the small data people have gone to the other, but there's no clear line in the sand that everyone can agree upon. I would look at behavioral differences between the two. In small data situations, you have a "small" dataset, and you seek you squeeze as much information as possible our of every data-point you can. Get more data, you can get more results. However, getting more data can be expensive. The data one collects is often constrained to fit mathematical models, such as doing a partial factorial of tests to screen for interesting behaviors. In big data situations, you have a "big" dataset, but your dataset tends not to be as constrained. You usually don't get to convince your customers to buy a latin-square of furniture, just to make the analysis easier. Instead you tend to have gobs and gobs of poorly structured data. To solve these problems, the goal tends not to be "select the best data, and squeeze everything you can out of it," like one might naively attempt if one is used to small data. The goal tends to be more along the lines of "if you can just get a tiny smidgen out of every single datapoint, the sum will be huge and profound." Between them lies the medium sized data sets, with okay structure. These are the "really hard problems," so right now we tend to organize into two camps: one with small data squeezing every last bit out of it, and the other with big data trying to manage to let each data point shine in its own right. As we move forward, I expect to see more small-data processes trying to adapt to larger data-sets, and more big-data processes trying to adapt to leverage more structured data.
