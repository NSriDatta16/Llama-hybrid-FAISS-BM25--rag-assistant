[site]: crossvalidated
[post_id]: 32974
[parent_id]: 6167
[tags]: 
The difficulty with specifying a model to resolve this problem is one of how to interpret the strength of preference information. Does A vs B 999:1 mean that 999 times out of 1000 people will prefer A, or, does it mean that a person prefers A by a large amount relative to B? If we take the interpretation that the data means that A is preferred to B 999 out of 1000, then you can fit a Bradley-Terry(-Luce) model, but most people these days would instead estimate a logit model, or a generalization thereof, as their "choice model": $ P(A > B\; |\; \vec{w} ) = \frac{e^w_A}{e^w_A + e^w_B} $ Maximum likelihood estimation with large data sets and aggregate data is straightforward as the sample size enters the log-likelihood as a weight for each pair. Complication arises if one wants to take into account how people differ in their preferences, in which case some type of mixture is required (see Train, Kenneth E. (2009), Discrete Choice Methods with Simulation (Second ed.). Cambridge: Cambridge University Press.). It is not unknown for researchers to take this frequency interpretation when modeling even if it is believed that it is not an accurate characterization of the problem. This is because it is not a straightforward exercise to specify a good model which deals with degree, as you then have to find some way of working out what, precisely, 999:1 means and how it relates to 998:2 and so on. There are lots of different models that have been developed for this problem (e.g., models designed for constant sum dependent variables, models designed to predict probabilities, diffusion models). It is impossible to say with any exactitude which model is most appropriate as it really depends upon the appropriateness of the inherent assumptions to your data and how well it fits your data.
