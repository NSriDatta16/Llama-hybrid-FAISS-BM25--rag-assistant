[site]: crossvalidated
[post_id]: 451795
[parent_id]: 
[tags]: 
Low OOB error but high CV error with MABoost

I am using Mirror Ascent Boosting (R package maboost) to learn a 3-class predictor over a set of 123 patients (very small , I know). Classes are almost balanced. I am getting excellent OOB errors (sorry, the x-axis should be logarithmic and labeled "model size" for consistency with the other plots): However, if I do a 10-fold cross-validation, using caret's createFolds() functionality to stratify by the outcome variable, the average accuracy across folds is not great, and does not seem to change with model size: I first suspected that using only 90% of 123 cases might make the data small enough to overfit, however, the average OOB curve across CV instances converges to zero as well: What could be the cause for good OOB errors but lousy CV?
