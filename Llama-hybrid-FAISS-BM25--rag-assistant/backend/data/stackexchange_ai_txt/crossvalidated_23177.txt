[site]: crossvalidated
[post_id]: 23177
[parent_id]: 
[tags]: 
Maximum entropy classifier and sentiment analysis

I am doing a project work in sentiment analysis (on Twitter data) using machine learning approach. In order to find the 'best' way to this I have experimented with naive Bayesian and maximum entropy classifier by using unigrams, bigrams and unigram and bigrams together. I'm using the SharpEntropy library for ME, and an own implementation for the NB. My question is related to the ME classifier. In one of my tests I've trained it using both unigrams and bigrams because, these outperformed the unigram only and bigram only solutions. I used 100 iterations and no cutoff (= minimum appearance for using it in the training). After the initial training I've started the validation and the strange thing I encountered is the following: if the test data is not converted to the unigram-bigram representation my classifier was 4% more accurate (73%->77%) than the case when I did the transformation. So basically my classifier is trained using uni- and bigrams, but the test data remained in the unigram representation and I got better results. (unigram training - unigram test data resulted in 58% accuracy, bigram only training - bigram only test resulted in 68% accuray) What could be the reason for this behavior? I am looking for some help to get a deeper understanding for this.
