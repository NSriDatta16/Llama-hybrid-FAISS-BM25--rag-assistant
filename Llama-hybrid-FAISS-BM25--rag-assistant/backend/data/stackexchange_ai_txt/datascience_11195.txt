[site]: datascience
[post_id]: 11195
[parent_id]: 11060
[tags]: 
This seems like a job for Principal Component Analysis. In Scikit is PCA implemented well and it helped me many times. PCA, in a certain way, combines your features. By limiting the number of components, you fetch your model with noise-less data (in the best case). Because your model is as good as your data are. Consider below a simple example. from sklearn.pipeline import Pipeline pipe_rf = Pipeline([('pca', PCA(n_components=80)), ('clf',RandomForestClassifier(n_estimators=100))]) pipe_rf.fit(X_train_s,y_train_s) pred = pipe_rf.predict(X_test) Why I picked 80? When I plot cumulative variance, I got this below, which tells me that with ~80 components, I reach almost all the variance. So I would say give it a try, use it in your models. It should help.
