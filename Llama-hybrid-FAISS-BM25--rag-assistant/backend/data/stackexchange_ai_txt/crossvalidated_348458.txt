[site]: crossvalidated
[post_id]: 348458
[parent_id]: 
[tags]: 
Conditional distribution of the observed variable given the latent variable in PPCA

In the Probabilistic Principal Component Analysis (PPCA) paper ( Tipping and Bishop, 1999 ), the model is given as a linear relationship as follows: $\mathbf{t}=\mathbf{W}\mathbf{x}+\mu+\epsilon$ (1) where $\mathbf{t}$ is the observed variable, $\mathbf{x}$ is the latent variable, $\epsilon$ is the Gaussian error, $\mu$ is bias term, and $\mathbf{W}$ is the parameter matrix. It is assumed that $\epsilon \sim \mathcal{N}(0,\sigma^2\mathbf{I})$, and $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I})$. I didn't understand how we ended up the following conditional distribution: $\mathbf{t}|\mathbf{x}\sim\mathcal{N}(\mathbf{W}\mathbf{x}+\mu, \sigma^2\mathbf{I})$ (2) Since the sum of two Gaussians has a mean $\mu_1 + \mu_2$ and variance $\sigma^2_1 + \sigma^2_2$, and $E[\mathbf{C}\mathbf{x}]=\mathbf{C}E[\mathbf{x}]$, $Var[\mathbf{C}\mathbf{x}]=\mathbf{C}Var[\mathbf{x}]\mathbf{C}^T$ where $\mathbf{C}$ is a constant matrix, I can see that $\mathbf{t}$ has a marginal distribution $\mathcal{N}(\mu, \mathbf{W}\mathbf{I}\mathbf{W}^T+\sigma^2\mathbf{I})$. However, I didn't understand why the conditional is written in the first place and $p(\mathbf{t})$ is obtained by marginalization in the paper. Another question is that, in a latent variable model like this, how to obtain the marginal distribution $p(\mathbf{t})$ if the prior on $\mathbf{x}$ is not Gaussian. For instance, if we want to do sparse modeling, in which $\mathbf{x}$ is the sparse code, we place a Laplace prior on $\mathbf{x}$. Though I am not sure, I think $\mathbf{t}|\mathbf{x}$ would still be Gaussian since the error term is Gaussian, and $\mathbf{t}$ would have another unknown distribution without an analytical solution, therefore we would need to use approximation methods like Laplace's method, variational Bayes, or MCMC. Is this correct?
