[site]: crossvalidated
[post_id]: 311831
[parent_id]: 
[tags]: 
Why does VGG16 double number of features after each maxpooling layer?

VGG16 is a Convolutional Neural Network architecture, presented in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition (ILSVRC-2014 conference presentation here ). As explained in the paper, The width of convolutional layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. Why is the number of channels doubled after each convolutional layer? Jeremy Howard in the fast.ai course says it is not to lose information. Is it important to maintain the same number of weights on every layer for a convolutional neural network? Can it be beneficial to lose information?
