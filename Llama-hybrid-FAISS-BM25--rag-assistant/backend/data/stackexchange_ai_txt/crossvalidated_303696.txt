[site]: crossvalidated
[post_id]: 303696
[parent_id]: 303628
[tags]: 
Yes, there are some. They are used for unsupervised learning, e.g. to learn a distribution of the data. For example the Restricted Boltzmann Machine and Variational Auto Encoders. I've written a short piece on both of them, which i'm posting here. Restricted Boltzman Machines Boltzmann Machines are a type of Markov Random Fields (MRF), specifially log-linear, i.e. their energy function is a linear function of its free paramters. By adding hidden variables the models capacity can be increased. The energy function is defined as $$E(v,h) = - -b'v - c'h - h'Wv$$ where $v$ are the visible units, $h$ the hidden units, $b,c$ their bias and $W$ the weight matrix. The free energy is then given by $$F(v) = -b'v - \sum_i log \sum_{h_i} e^{h_i(c_i+W_iv})$$ Becaus of it's structure the visible and hidden units are independent of each other. Using this property we can derive $$p(h|v) = \prod_i p(h_i|v)$$ $$p(v|h) = \prod_j p(v_i|h)$$ Sampling in an RBM Samples of $x \sim p(x)$ can be drawn by running a Markov Chain to convergence by using Gibbs sampling as the transition operator. Gibbs sampling works by computing the joint probability of $N$ rvs by $S_i ~ p(S_i|S_{-i})$ where $S_{-i}$ contains the remaining $N-1$. In an RBM $S$ is the set of hidden and visible units. A Gibbs sampling step can then be performed over one layer while keeping the other one fixed. A step in the Markov chain is then taken as follows: $$h^{n+1} \sim sigm(W'v^n + c)$$ $$v^{n+1} \sim sigm(W'h^{n+1} + b)$$ So for example $h_i$ is set to be 1 with probabilty $sigm(W_i'v_i^n + c_i)$ I theory as $t \rightarrow \infty$ one obtains samples from $p(v,h)$. As this is highly expensive there are some algortihms to sample effiecently. Contrastive Divergence (CD-k) It uses two tricks: Don't intialize the sampling process randomly but rather with a sample from the training data. Stop sampling after a given amount of steps ($k$) Persistent CD Uses a single chain and takes $k$ steps after each sample instead of resetting after a sample has been generated Variational Auto Enoders In a classical variational approach the goal is to model the data $X$, i.e. to find $P(X)$. We assume there is a latent variable $z$ that explains the data but can't be observed directly. The latent variable can be marginalized out from the joint distribution $P(X, z)$: $$P(X) = \int P(X|z)P(z)dz$$ The idea of VAE is to model $P(z)$ using $P(z|X)$, i.e. we optimize of the latent varialbe $z$ that explains the data best. As we don't know $P(z|X)$ we try to approximate it modeling it with known and simpler distribution, e.g. a gaussian and minimize the difference between those to usin KL divergence: $$D_{KL}[Q(z \vert X) \Vert P(z \vert X)] = \sum_z Q(z \vert X) \, \log \frac{Q(z \vert X)}{P(z \vert X)} $$ $$D_{KL}[Q(z \vert X) \Vert P(z \vert X)] = E \left[ \log \frac{Q(z \vert X)}{P(z \vert X)} \right] $$ $$D_{KL}[Q(z \vert X) \Vert P(z \vert X)] = E[\log Q(z \vert X) - \log P(z \vert X)]$$ The VAE objective is as follows: $$\log P(X) - D_{KL}[Q(z \vert X) \Vert P(z \vert X)] = E[\log P(X \vert z)] - D_{KL}[Q(z \vert X) \Vert P(z)]$$ Here, $P(z)$ is the latent variable distribution. We might want to sample $P(z)$ later, so the easiest choice is $\mathcal{N}(0,1)$. Hence, we want to make $Q(z|X)$ to be as close as possible to $\mathcal{N}(0,1)$ so that we could sample it easily. Having $P(z)=\mathcal{N}(0,1)$ also add another benefit. Let’s say we also want $Q(z|X)$ to be Gaussian with parameters $\mu(X)$ and $\Sigma(X)$, i.e. the mean and variance given $X$. Then, the KL divergence between those two distribution could be computed in closed form: $$D_{KL}[\mathcal{N}(\mu(X), \Sigma(X)) \Vert \mathcal{N}(0, 1)] = \frac{1}{2} \, \left( \textrm{tr}(\Sigma(X)) + \mu(X)^T\mu(X) - k - \log \, \det(\Sigma(X)) \right)$$ Above, $k$ is the dimension of our Gaussian. tr(X) is trace function, i.e. sum of the diagonal of matrix $X$. The determinant of a diagonal matrix could be computed as product of its diagonal. In practice, however, it’s better to model Σ(X) as logΣ(X), as it is more numerically stable to take exponent compared to computing log. Hence, our final KL divergence term is: $$D_{KL}[ \mathcal{N}(\mu(X), \Sigma(X)) \Vert \mathcal{N}(0, 1)] = \frac{1}{2} \sum_k \left( \exp(\Sigma(X)) + \mu^2(X) - 1 - \Sigma(X) \right)$$ The following figure shows the full model (see https://arxiv.org/abs/1606.05908 ): So suppose we want to learn the distribution given data $X$. First we run it through a feed forward neural net we'll call the Encoder $Q$. The encoder produces two paramerters $\mu(X)$ and $\Sigma(X)$. These two provide the first term of the error function, i.e. $D_{KL}[\mathcal{N}(\mu(X), \Sigma(X)) \Vert \mathcal{N}(0, 1)]$. We can now also sample the latent variable $z \sim \mathcal{N}(\mu(X), \Sigma(X))$ and feed that through a second neural net we'll call the decoder $P$. The decoder produces an approximation of $X$ $f(z)$ and provides the second term of the error, i.e. $\Vert (X - f(z))\Vert^2$. Generating Data After training is complete we can generate new data points according to $P(X)$ by sampling $z \sim \mathcal{N}(\mu(X), \Sigma(X))$ and transform it using the decoder net. Reparametrization Trick In order to make VAEs work, it’s essential to drive $Q$ to produce codes for $X$ that $P$ can reliably decode. The forward pass of this network works fine and, if the output is averaged over many samples of $X$ and $z$, produces the correct expected value. However, we need to back-propagate the error through a layer that samples $z$ from $Q(z|X)$, which is a non-continuous operation and has no gradient. Stochastic gradient descent via backpropagation can handle stochastic inputs, but not stochastic units within the network! The solution, called the “reparameterization trick” is to move the sampling to an input layer. Given $\mu(X)$ and $\Sigma(X)$, we can sample from $\mathcal(N)$
