[site]: crossvalidated
[post_id]: 468769
[parent_id]: 468736
[tags]: 
Backpropagation Backpropagation is the heart of every neural network. Firstly, we need to make a distinction between backpropagation and optimizers. Backpropagation is for calculating the gradients efficiently, while optimizers is for training the neural network, using the gradients computed with backpropagation. In short, all backpropagation does for us is compute the gradients. Nothing more. Pic Credits We always start from the output layer and propagate backwards, updating weights and biases for each layer. The idea is simple: adjust the weights and biases throughout the network, so that we get the desired output in the output layer. Say we wanted the output neuron to be 1.0, then we would need to nudge the weights and biases so that we get an output closer to 1.0. We can only change the weights and biases, but activations are direct calculations of those weights and biases, which means we indirectly can adjust every part of the neural network, to get the desired output â€” except for the input layer, since that is the dataset that you input. The backpropagation algorithm The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm: 1. Input $x$ : Set the corresponding activation $a^{1}$ for the input layer. 2. Feedforward: For each $l = 2, 3, \ldots, L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$ 3. Output error $\delta^L$ : Compute the vector $\delta^{L} = \nabla_a C \odot \sigma'(z^L)$ . 4. Backpropagate the error: For each $l = L-1, L-2, \ldots, 2$ compute $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$ . 5. Output: The gradient of the cost function is given by $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ and $\frac{\partial C}{\partial b^l_j} = \delta^l_j$ . Examining the algorithm you can see why it's called backpropagation. We compute the error vectors $\delta^l$ backward, starting from the final layer. It may seem peculiar that we're going through the network backward. But if you think about the proof of backpropagation, the backward movement is a consequence of the fact that the cost is a function of outputs from the network. To understand how the cost varies with earlier weights and biases we need to repeatedly apply the chain rule, working backward through the layers to obtain usable expressions. Below are the resources which would help you to Understand Backpropagation How might we measure the change in the cost function in relation to a specific weight, bias or activation? Understanding Backpropagation Algorithm Backpropagation Wiki
