[site]: crossvalidated
[post_id]: 601092
[parent_id]: 600913
[tags]: 
Concerning your first question: p-values (which are decreasing monotonously transformed values of the t statistic) can be used as a measure for variable importance, provided there is no strong multicollinearity. Actually, the function varImp of the R package caret returns simply the absolute value of the t statistic. According to this measure, variables with low t (or high p-value) are "unimportant". It might be, however, that including such a variable still reduces the cross-validated mean squared error (MSE), so I would check this, too. A proxy for the leave-one-out MSE is the Akike Information Criterion (AIC) (both are asymptotically equivalent), so you can test this instead. This, however, only measures the effect of dropping each variable alone while keeping all other variables. Moreover it does not describe the contribution of that variable on $R^2$ . Other methods are based on the decomposition of $R^2$ and measuring the contribution of each each variable. There are different ways to do this, and each gives different results. For a comprehensible overview and an R package, see U. Gr√∂mping: "Relative Importance for Linear Regression in R: The Package relaimpo." Journal of Statistical Software 17(1), pp. 1-27 (2006) The problem of the decomposition by sequential addition of variables in a type I ANOVA is, that the results depend on the order in which the variables are added. A workaround is to average over all variable permutations, which has runtime of order $O(p!)$ for $p$ parameters, and is thus not feasible for models with many parameters. With only a few parameters as in your case, this is no problem and might be a method to try out for measuring variable importance.
