[site]: crossvalidated
[post_id]: 512129
[parent_id]: 512077
[tags]: 
No, I don't think this is possible in a vanilla (no feature engineering) approach. Any neural network with continuous activations will produce a continuous function at the end. Given a training set as you describe, it is reasonably likely that a network with enough neurons and, say, two layers will perfectly classify the training set: learn triples of first-layer neurons like $\operatorname{relu}(x-1.99)$ , $\operatorname{relu}(x-2)$ , and $\operatorname{relu}(x-2.01)$ , then combine them (the first minus twice the second plus the third?) in a second-layer neuron to get a single spike around $x=2$ . This can happen around every integer in your training set, and nearby non-integers will regulate the width of the spikes. (Although, as usual, in practice I can't currently get the neural net to learn its way into this structure. I'll keep playing a little bit and update with a notebook if I'm successful.) But , for any integer not in your training set, the network has no reason to deviate from this structure and give (significantly) positive values anywhere else. And outside the training range, all bets are off for even the non-integers. You would need some mechanism to encourage periodicity, but if you do that then the problem becomes (almost) trivial. I got a bit of improvement in my attempts to train into the explicit network described above. It's still rather dependent on the initial weights, and there are a couple of points it doesn't quite learn, but I lost interest in cleaning it up, so here it is: notebook link The training data has each digit except 6 and then random non-integers in $(0, 10)$ . So there's nothing to enforce anything about negative numbers, and the integer 6 is unnoticed by the network. (But the model has failed to understand 1, 2, 3, and seems to have superfluous kinks in the negative range.)
