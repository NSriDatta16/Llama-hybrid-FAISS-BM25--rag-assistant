[site]: crossvalidated
[post_id]: 18852
[parent_id]: 18844
[tags]: 
If you assume a model form that is non-linear but can be transformed to a linear model such as $\log Y = \beta_0 + \beta_1t$ then one would be justified in taking logarithms of $Y$ to meet the specified model form. In general whether or not you have causal series , the only time you would be justified or correct in taking the Log of $Y$ is when it can be proven that the Variance of $Y$ is proportional to the Expected Value of $Y^2$ . I don't remember the original source for the following but it nicely summarizes the role of power transformations. It is important to note that the distributional assumptions are always about the error process not the observed Y, thus it is a definite "no-no" to analyze the original series for an appropriate transformation unless the series is defined by a simple constant. Unwarranted or incorrect transformations including differences should be studiously avoided as they are often an ill-fashioned /ill-conceived attempt to deal with unidentified anomalies/level shifts/time trends or changes in parameters or changes in error variance. A classic example of this is discussed starting at slide 60 here http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation where three pulse anomalies (untreated) led to an unwarranted log transformation by early researchers. Unfortunately some of our current researchers are still making the same mistake. Several common used variance-stabilizing transformations Relationship of $\sigma^2$ to $E(y)$ Transformation $\sigma^2 \propto$ constant $y'=y$ (no transformation) $\sigma^2 \propto E(y)$ $y' = \sqrt y$ (square root: Poisson data) $\sigma^2 \propto E(y)(1-E(y))$ $y' = sin^{-1}(\sqrt y)$ (arcsin; binomial proportions $0\le y_i \le 1$ ) $\sigma^2 \propto (E(y))^2$ $y'=log(y)$ $\sigma^2 \propto (E(y))^3$ $y' = y^{-1/2}$ (reciprocal square root) $\sigma^2 \propto (E(y))^4$ $y' = y^{-1}$ (reciprocal) The optimal power transformation is found via the Box-Cox Test where -1. is a reciprocal -.5 is a recriprocal square root 0.0 is a log transformation .5 is a square toot transform and 1.0 is no transform. Note that when you have no predictor/causal/supporting input series, the model is $Y_t=u +a_t$ and that there are no requirements made about the distribution of $Y$ BUT are made about $a_t$ , the error process. In this case the distributional requirements about $a_t$ pass directly on to $Y_t$ . When you have supporting series such as in a regression or in a Autoregressiveâ€“moving-average model with exogenous inputs model ( ARMAX model ) the distributional assumptions are all about $a_t$ and have nothing whatsoever to do with the distribution of $Y_t$ . Thus in the case of ARIMA model or an ARMAX Model one would never assume any transformation on $Y$ before finding the optimal Box-Cox transformation which would then suggest the remedy (transformation) for $Y$ . In earlier times some analysts would transform both $Y$ and $X$ in a presumptive way just to be able to reflect upon the percent change in $Y$ as a result in the percent change in $X$ by examining the regression coefficient between $\log Y$ and $\log X$ . In summary, transformations are like drugs some are good and some are bad for you! They should only be used when necessary and then with caution.
