[site]: crossvalidated
[post_id]: 602160
[parent_id]: 601438
[tags]: 
The marginal log likelihood $\log p_\theta(x)$ doesn't depend on $q_\phi$ : the variational distribution $q_\phi$ is introduced only to allow a numerically tractable decomposition of $\log p_\theta(x)$ (put it differently, $q_\phi$ cancels out from the right hand side of your first equation). So any change to the variational parameters $\phi$ will not modify the $\log p_\theta(x)$ . However the evidence lower bound (ELBO) does depend on $q_\phi$ â€”changes in $\phi$ will affect the ELBO. Maybe it's easier to understand what happens if we consider the alternating optimization procedure (as it is the case with expectation maximization). If we keep the parameters $\theta$ fixed and maximize the ELBO with respect to $\phi$ that reduces the gap to the true marginal log likelihood (ideally, reaching $\log p_\theta(x)$ ). Note that this step doesn't change the log likelihood. If we keep the variational parameters $\phi$ fixed and maxize the ELBO with respect to $\theta$ that increases the lower bound and as a consequence also the marginal log likelihood (provided that the ELBO was tight already). For completeness, let me include Figures 9.12 and 9.13 from Bishop's Pattern Recognition and Machine Learning ; they might offer a helpful visual depiction of the underlying process:
