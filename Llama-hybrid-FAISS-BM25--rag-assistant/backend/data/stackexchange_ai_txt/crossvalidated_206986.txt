[site]: crossvalidated
[post_id]: 206986
[parent_id]: 206985
[tags]: 
It comes from the following: $$p(D \vert \xi) = \int_\Theta p(D, \theta | \xi)d\theta = \int_\Theta p(D \vert \theta, \xi) p(\theta \vert \xi) d\theta.$$ This isn't actually coming from Eqn. 1 (which is just Bayes rule). It is just following the basic rules of probability. To be more explicit: there are two steps happening here. The first is that we can integrate $p(D, \theta \vert \xi)$ over $\theta$ to get $p(D \vert \xi)$. We are going from a joint conditional distribution to a marginal conditional distribution. The second step is using the laws of conditional probability to write $p(D, \theta \vert \xi) = p(D \vert \theta, \xi) p(\theta \vert \xi)$. We then integrate this. $$ \ $$ Update: I just actually looked at the paper that you linked to, so here's an expanded explanation. Equation 1 is (conditional) Bayes rule, which relates the posterior $p(\theta \vert D, \xi)$ to the prior $p(\theta \vert \xi)$, the likelihood $p(D \vert \theta, \xi)$, and the evidence $p(D \vert \xi)$. Bayesian inference flows from this. $p(D \vert \xi)$ is a normalizing constant (with respect to $\theta$, the thing we care about), and generally is not of interest in and of itself, although we need it if we want to evaluate our posterior at certain points (note that one of the appealing things about the Metropolis-Hastings algorithm for MCMC is that you don't need to compute the evidence). The reason it's nice to write $p(D \vert \xi)$ the way that he did is because then we have expressed the posterior entirely in terms of the likelihood and the prior (which are things we very much do care about). So once we have a prior and a likelihood we can (in principle) compute the posterior.
