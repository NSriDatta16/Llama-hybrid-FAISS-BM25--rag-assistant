[site]: crossvalidated
[post_id]: 125914
[parent_id]: 
[tags]: 
Gaussian Process Regression with positive prediction weights

I want to do Gaussian Process Regression for a density function $f(x)$ with a gaussian kernel function $k(x, x')$. Given the training data $\mathbf{x} = (x_1, x_2, \dots, x_N)$ and $\mathbf{f} = (f(x_1), f(x_2), \dots, f(x_N))$, the prediction at a new point $x^*$ is: $$ f(x^*) = \mathbf{k}^T \mathbf{C}_N^{-1}\mathbf{f} $$ where $$ \mathbf{k} = \begin{bmatrix} k(x_1, x^*)\\ \vdots\\ k(x_N, x^*) \end{bmatrix} \mathbf{C}_N = \begin{bmatrix} k(x_1, x_1) &\dots &k(x_1, x_N)\\ &\vdots\\ k(x_N, x_1) &\dots &k(x_N, x_N) \end{bmatrix} $$ As mentioned in Bishop's machine learning book, I can rewrite $$ f(x^*) = \sum_{i = 1}^N a_i k(x_i, x^*) $$ with $a_i$ is the $i^{th}$ component of $\mathbf{C}_N^{-1}\mathbf{f}$. As I use an gaussian kernel $k$, I can directly sample my density function, if all $a_i$ are positive. My questions is if there exists a way to constraint positive $a_i$ in the GP regression?
