[site]: crossvalidated
[post_id]: 325314
[parent_id]: 325305
[tags]: 
You're right, and not just because log zero is not defined. Any one-to-one transformation of $0$ and $1$ to $a$ and $b$ would just be a linear rescaling. This is true with any rule or function: even if it is a nonlinear function (say $\log(x + c)$) all that matters are the results of transforming $0$ and $1$. Think of this geometrically: for the data any transformation that preserves a difference between the two values defines two points in the plane differing on both coordinates, and so a linear transformation. So it could not possibly do anything to improve that was even thought to be a problem. For example, contrary to a surprisingly common myth, there aren't strict assumptions about marginal distributions of predictors (which is not to say that 665 zeros and 1 one for a predictor (say) is not a situation that needs care and attention). (0, 1) predictors are fine and convenient because they lead to clean parameterisations and explanations of changes in level and slope.
