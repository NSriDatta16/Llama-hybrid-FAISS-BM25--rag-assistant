[site]: crossvalidated
[post_id]: 205085
[parent_id]: 204670
[tags]: 
If your data are simple, and you are just toying around, the decomposition into seasonal and overall trend should already be pretty good. But if you want to dig deeper, there is a more formal approach: Using Kernels to define the relations between your points. The best exemple I have seen of this kind of task is on the Mauna Loa CO2 concentration dataset ( Wikimedia Commons illustration ) A possible model for this dataset is shown in Gaussian Processes for Machine Learning , by Rasmussen & Williams (Second edition available online) , in section 5.4.3. Warning: Gaussian Processes (Wikipedia) are not easy to understand, especially if you do not have a background in Kernels. In a very simplified way, it is like doing Nearest Neighbour regression (a point has the value of close points), but using Kernels (Wikipedia) to encode prior knowledge about the data and the form of the function. What kernels allow you to do is specify how "related" two points are. In this case, you can build a kernel that defines "relatedness" by how close in month we are, and how close in years, such that 12/2015 is related to 01/2016 and 12/2014 , but not so much related to 06/2015 and not to 06/2004 . Since you mention Support Vector Regression, you could probably do something similar using such a Kernel there. Considering your second question, it would be best to leave predicted data outside of your training data. Doing this does not add information, and if your model is biased, the predicted data will only reinforce this bias and lead to poorer prediction. This is true for all methods.
