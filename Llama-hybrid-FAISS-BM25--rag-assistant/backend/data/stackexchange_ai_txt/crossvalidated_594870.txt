[site]: crossvalidated
[post_id]: 594870
[parent_id]: 594774
[tags]: 
You can regard this as an example of 'All models are wrong but some are useful' . Null hypothesis testing is a simplification. Null hypothesis testing is often not the primary goal and instead it is more like a tool for some other goal, and it is used as an indicator of the quality of a certain result/measurement. An experimenter wants to know the effect size and know whether the result is statistically significant. For the latter, statistical significance, one can use a null hypothesis test (which answers the question whether the observation has a statistically significant deviation from zero). The null hypothesis test and p-values are now considered as a bit of an old fashioned tool. Better expressions of experimental results are confidence intervals or intervals from Bayesian approaches. The equality hypothesis is a priori known to be wrong Yes, if you consider coins. But an exception might be hardcore science like physics or chemistry where certain theories are tested. For instance the equivalence principle . In addition if the equality hypothesis is a priori wrong, then why perform an experiment? If something is a priori wrong then the point is not to show that this something is wrong, instead the point is to show that there is an effect that can be easily measured. A casino that wants to test coins may not care about the theoretical probability that coins are not exactly p=0.5 fair and might differ by some theoretically small value, they care about finding out coins with a larger difference. And the point of the null hypothesis test is to prevent false positives. Also note the two approaches/philosophies behind null hypothesis testing Fisher: You may have observed some effect, and as expected it is not zero, but if your p-value is high then it means that your test has little precision and little strength in differentiating between different effect sizes (even down to a true effect size of zero, the observed effect may have likely occured and thus statistical fluctuations govern your observation). So you better gather some more data. The p-value and null hypothesis is a rule of thumb for indicating precision of an experiment. Neyman and Pearson: (from 'On the Problem of the Most Efficient Tests of Statistical Hypotheses') Indeed, if $x$ is a continuous variable – as for example is the angular distance between two stars – then any value of $x$ is a singularity of relative probability equal to zero. We are inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis. But we may look at the purpose of tests from another view-point. Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong. The hypothesis test is a practical device to create a decision rule . One goal is to make this rule efficient and using a likelihood ratio with a null hypothesis is one method of achieving that. In "real life" one is always only interested in some finite accuracy $\epsilon$ , meaning the hypothesis of interest is actually of the form $H_0: |\mu-\mu_0| . This is captured by hypothesis testing. An example is two one-sided t-tests for equivalence testing and can be explained with the following image and can be considered as testing three hypotheses instead of two for the absolute difference $$\begin{array}{}H_0&:& \text{|difference|} = 0\\ H_\epsilon&:& 0 Below is a sketch of the position of the confidence interval within these 3 regions (unlike the typical sketch of TOST, there are actually 5 situations instead of 4). The point of observations and experiments is to find a data driven answer to questions by excluding/eliminating what is (probably) not the answer (Popper's falsification ). Null hypothesis testing does this in a somewhat crude manner and does not differentiate between the situations B, C, E. However, in many situations this is not all too much of a problem. In a lot of situations the problem is not to test tiny effects with $H_0: |\mu-\mu_0| . The effect size is expected to be sufficiently large and above some $\epsilon$ . In many practical cases testing $|\text{difference}| > \epsilon$ is nearly the same as $|\text{difference}| > 0$ and the null hypothesis test is a simplification. It is in the modern days of large amounts of data that effect sizes of $\epsilon$ play a role in results. Before this issue was dealt with by having arbitrary cut-off values for p-values and by power analysis. If a test had a p-values below some significance level, then the conclusion is that the effect must be some effect beyond some size. These p-values are still arbitrary, also with TOST equivalence testing. A researcher has some given significance level and computes a required sample size to obtain a given power for a given effect that the researcher wants to be able to measure. The effect of replacing $H_0$ by some range within $\epsilon$ is effectively changing the power curve . For a given effect size close to $\epsilon$ the power is reduced and it becomes less likely to reject the null hypothesis. It is effectively just a shift in the power. Why are standard frequentist hypotheses so uninteresting? They are simple basic examples that allow for easy computations. It is easier to work with them. But indeed, it is more difficult to imagine the practical relevance.
