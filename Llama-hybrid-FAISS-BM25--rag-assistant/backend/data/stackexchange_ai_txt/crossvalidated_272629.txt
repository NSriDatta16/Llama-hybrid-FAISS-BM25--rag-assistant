[site]: crossvalidated
[post_id]: 272629
[parent_id]: 
[tags]: 
Can autoencoders learn sound transformations?

I've been working on the generation of music. Particularly, given a target sound I would like this sound to be modified depending on an input signal. Say, I play the piano and I want to generate a drum sequence based on the targeted one. Both sounds are in a time-frequency representation (STFT). I'm reading here that during the training the target is set equal to the input. What if I set the target equal to something else, i.e. a sound other than the input? A second hypothesis would be to train an autoencoder only on a family of targets, e.g. drum sounds, and to encode the input sound in a way such that it can be used for conditioning in a generative model - this paper should do something similar. But I see this as an expensive choice, especially in a real-time context.
