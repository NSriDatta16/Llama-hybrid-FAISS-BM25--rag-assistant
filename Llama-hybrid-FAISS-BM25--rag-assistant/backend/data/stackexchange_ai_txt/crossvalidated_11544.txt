[site]: crossvalidated
[post_id]: 11544
[parent_id]: 
[tags]: 
Testing for stability in a time-series

Is there a standard (or best) method for testing when a given time-series has stabilized? Some motivation I have a stochastic dynamic system that outputs a value $x_t$ at each time step $t \in \mathbb{N}$ . This system has some transient behavior until time step $t^*$ and then stabilizes around some mean value $x^*$ with some error. None of $t^*$ , $x^*$ , or the error are known to me. I am willing to make some assumptions (like Gaussian error around $x^*$ for instance) but the less a priori assumptions I need, the better. The only thing I know for sure, is that there is only one stable point that the system converges towards, and the fluctuations around the stable point are much smaller that the fluctuations during the transient period. The process is also monotonic-ish, I can assume that $x_0$ starts near $0$ and climbs towards $x^*$ (maybe overshooting by a bit before stabilizing around $x^*$ ). The $x_t$ data will be coming from a simulation, and I need the stability test as a stopping condition for my simulation (since I am only interested in the transient period). Precise question Given only access to the time value $x_0 ... x_T$ for some finite $T$ , is there a method to say with reasonable accuracy that the stochastic dynamic system has stabilized around some point $x^*$ ? Bonus points if the test also returns $x^*$ , $t^*$ , and the error around $x^*$ . However, this is not essential since there are simple ways to figure this out after the simulation has finished. Naive approach The naive approach that first pops into my mind (which I have seen used as win conditions for some neural networks, for instance) is to pick to parameters $T$ and $E$ , then if for the last $T$ timesteps there are not two points $x$ and $x'$ such that $x' - x > E$ then we conclude we have stabilized. This approach is easy, but not very rigorous. It also forces me to guess at what good values of $T$ and $E$ should be. It seems like there should be a better approach that looks back at some number of steps in the past (or maybe somehow discounts old data), calculates the standard error from this data, and then tests if for some other numbers of steps (or another discounting scheme) the time-series has not been outside this error range. I included such a slightly less naive but still simple strategy as an answer . Any help, or references to standard techniques are appreciated. Notes I also cross posted this question as-is to MetaOptimize and in a more simulation-flavored description to Computational Science .
