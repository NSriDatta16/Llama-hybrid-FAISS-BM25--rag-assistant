[site]: crossvalidated
[post_id]: 255194
[parent_id]: 35105
[tags]: 
While I did not test it , reading the article, the optimization problem, both for SVM and LapSVM , is given as: $$\beta^*=\max_{\beta\in\mathbb R^l} \sum_{i = 1}^{l}\beta_i - {1\over 2}\beta^TQ\beta$$ subject to: $$\sum_{i = 1}^{l}\beta_iy_i = 0\\ 0 \le \beta_i \le {1\over l}\text{, with }i=1,\dots,l$$ For SVM : $$Q_{\text{SVM}} = Y\left(K \over 2\gamma\right)Y\\ \alpha^*_{\text{SVM}}={Y\beta^* \over 2\gamma}$$ While for LapSVM we have the following (added parentheses to make the relationship clearer): $$Q_{\text{LapSVM}} = Y\left( JK \left(2\gamma_AI+2\frac{\gamma_I}{(l+u)^2}LK\right)^{-1} J^T\right)Y\\ \alpha^*_{\text{LapSVM}}= \left(2\gamma_AI+2\frac{\gamma_I}{(l+u)^2}LK\right)^{-1}J^TY\beta^*$$ We can define $$Q_{\text{SVM*}} \equiv Q_{\text{LapSVM}}$$ if: $$ \left\{\begin{matrix} \gamma_{\text{SVM*}} = 1/2 \\ K_{\text{SVM*}}=JK_{\text{LapSVM}}\left(2\gamma_AI+2\frac{\gamma_I}{(l+u)^2}LK_{\text{LapSVM}}\right)^{-1}J^T \end{matrix}\right. $$ Last: $$\alpha^*_{\text{LapSVM}}= K_{\text{LapSVM}}\left(2\gamma_AI+2\frac{\gamma_I}{(l+u)^2}LK_{\text{LapSVM}}\right)^{-1}J^T \alpha^*_{\text{SVM*}}$$ I can confirm that it works. See this example with a Gaussian kernel, and how the class virginica starts creeping into the unlabelled data when $\gamma_I = 2500$ compared to $\gamma_I = 0$, which is the standard SVM.
