[site]: crossvalidated
[post_id]: 195218
[parent_id]: 
[tags]: 
Why is the MEAN taken in Simple Linear Regression?

Question Why take the mean of the squared residuals? wouldn't it be simpler and produce the same result ( parameters $\theta_0$ and $\theta_1$ ) if you just minimised the sum of the square residuals? How will it affect the fit? Why $$ \frac{1}{m} \sum _{i=1}^m \left(h_\theta(X^{(i)})-Y^{(i)}\right)^2 $$ and not simply: $$ \sum _{i=1}^m \left(h_\theta(X^{(i)})-Y^{(i)}\right)^2 $$ Context I am just beginning the Machine Learning course via coursera: Andrew Ng minimizes this cost function to find a line of best fit: His cost function consists of: finding the residuals squaring them summing the squares taking the mean of the squares minimising the algebraic result to produce minimising parameters $\theta_0$ and $\theta_1$ $$J(\theta) = \frac{1}{m}[\sum_{i=1}^m(h_\theta (x^{(i)}) - y^{(i)})^2]$$ to find a line of best fit constructed out of the thetas (the slope and the y intercept) $$h_\theta = \theta_0 + \theta_1 x$$ the vertical black line segments are some residuals
