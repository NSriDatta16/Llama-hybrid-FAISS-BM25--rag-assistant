[site]: crossvalidated
[post_id]: 311761
[parent_id]: 
[tags]: 
What is D in Optimal Bayes Classification (Machine Learning by Tom Mitchell Ed2)?

I'm reading chapter 6 from "Machine Learning" by Tom Mitchell, 2nd edition. It seems like the author changes in each paragraph what "D" is without saying anything, but it becomes really confussing at the section 6.7 -> Bayes Optimal Classifier (page 175). It says that the probability of a new instance x to be classified as a value vj (from a set of values V) is: P(vj|D) = sumatory from all hi of H( P(vj|hi)P(hi|D) ) However, it was all the time understood that D is the whole training set we use in the classifier. If we call the new instance x, that x doesn't appear anywhere in the formula so that formula, if we are rigorous, means that we get the probability that, given a training set D (and not any new instance x), we find that our classifiers say "it's vj".
