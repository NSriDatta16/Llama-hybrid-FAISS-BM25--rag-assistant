[site]: crossvalidated
[post_id]: 619809
[parent_id]: 411767
[tags]: 
This Medium article I wrote might help as well: 4 ways to encode categorical features with high cardinality . It explores four encoding methods applied to a dataset with 26 categorical features with cardinalities up to 40k (includes code): Target encoding PROS: parameter free; no increase in feature space CONS: risk of target leakage (target leakage means using some information from target to predict the target itself); when categories have few samples, the target encoder would replace them by values very close to the target which makes the model prone to overfitting the training set; does not accept new values in testing set Count encoding PROS: easy to understand and implement; parameter free; no increase in feature space CONS: risk of information loss when collision happens; can be too simplistic (the only information we keep from the categorical features is their frequency); does not accept new values in testing set Feature hashing PROS: limited increase of feature space (as compared to one hot encoding); does not grow in size and accepts new values during inference as it does not maintain a dictionary of observed categories; captures interactions between features when feature hashing is applied on all categorical features combined to create a single hash CONS: need to tune the parameter of hashing space dimension; risk of collision when the dimension of hashing space is not big enough Embedding PROS: limited increase of feature space (as compared to one hot encoding); accepts new values during inference; captures interactions between features and learns the similarities between categories CONS: need to tune the parameter of embedding size; the embeddings and a logistic regression model cannot be trained synergically in one phase, since logistic regression do not train with backpropagation. Rather, embeddings has to be trained in an initial phase, and then used as static inputs to the decision forest model.
