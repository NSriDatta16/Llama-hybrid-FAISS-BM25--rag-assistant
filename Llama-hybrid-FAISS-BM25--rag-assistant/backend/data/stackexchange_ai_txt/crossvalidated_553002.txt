[site]: crossvalidated
[post_id]: 553002
[parent_id]: 
[tags]: 
Discrete Bayes Net learning under parameter constraints

What is some relevant research available on estimating the parameters of a Bayes Net ( with known structure ) when there are known constraints on conditional and marginal probabilities? For example, consider a simple example of the Bayes Net over 4 boolean variables {A, B, C, D} with the DAG structure A->C, B->C, C->D, where it is known that $P(D=1|A=0) = 0.2$ , $P(C=1|B=1) = 0.8$ . We would like to estimate the full joint distribution -- either using maximum entropy under these constraints , or maximum likelihood from data (with possible missing values) with the resulting joint respecting the constraints . I would also like the learning algorithm to produce a certificate of infeasibility if given constraints are mutually inconsistent. What is a good way to approach this general problem? I am trying to determine the most general case where fast algorithms are known, and any negative results in this area. I have already encountered: Bayesian Network Learning with Parameter Constraints, Niculescu et. al. Maximum Entropy Distributions between Upper and Lower Bounds, Abbas
