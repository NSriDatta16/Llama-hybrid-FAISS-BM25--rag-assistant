[site]: datascience
[post_id]: 56255
[parent_id]: 56178
[tags]: 
You may want to consider a stratified cross-validation approach where you specifically undersample the data from users that have lots of observations or oversample from uses with fewer observations. Another approach which may also work is to duplicate the observations of some of the users with less observations, this would even out the weighting of the different kinds of observations. -- Adding on to answer the additional questions in the comments There are a number of ways to undersample the majority or oversample the minority. Here's an overview of how to handle the situation (in general). To undersample the majority, simply remove some % of the training data that is in the majority for each run. To oversample the minority, duplicate the values that you want to have a higher weight. Additionally, there is a parameter for XGBoost (scale_pos_weight) which allows you to set the weight of the different samples, see here . Additionally you could leverage SMOTE from the imblearn library.
