[site]: crossvalidated
[post_id]: 440126
[parent_id]: 
[tags]: 
Do we use different number of dummy variables for Classical and Bayesian stats?

Let's say we are building a regression model with one nominal predictor, which has three levels, let's say red, blue, and yellow. I remember being taught that when we build the model, we use j - 1 predictors in our model, where j is the total levels in your nominal predictor. In the above example where j = 3, we will have: Y = B0 + B1X1 + B2X2 Where X1 and X2 are coded [0, 1] for red and blue respectively. If we want to see the effect of yellow on Y, it would just be B0. I am reading a text on Bayesian stats where the author is building a model with a metric response and nominal predictor. It seems like in Bayesian stats, we end up having j predictors instead of j-1. Is my understanding correct? Depending on your approach, you may have a different number of predictors. Why can we do this in Bayesian and not have the issue of multicollinearity? What am I missing here?
