[site]: crossvalidated
[post_id]: 557378
[parent_id]: 
[tags]: 
Which part of an encoder/decoder generative network is improved by adding a discriminator loss term?

Lets say you're doing a superresolution image task with "deep learning" constructs. You encode to a latent representation using some parameterized model (like a neural network), then decode to a higher resolution than the input image (also using a neural network). You minimize the MSE loss in minibatches using the usual optimizers, with downsampled images into the network and computing the loss against the original full-resolution images. Lets say the serial combination of the encoder and decoder is the generator. It is well known that you can broadly get the colors close in large swaths of the image without high frequency details and still greatly minimize the MSE loss. So one trick is to use a discriminator trained to classify images created by the superresolution network vs ones that are from the original dataset. Then you can incorporate the discriminator's output into the generator's loss, and alternately train the networks using the usual GAN process. This process makes for "better" images, i.e., ones with lots of detail that could fool a human. My question is this: does adding the discriminator loss help 1) make for a better encoder that makes better latent representations that capture more detail, 2) make for a better decoder that makes more realistic details out of the latent representation, or 3) a little of both? Can you point to studies showing exactly where the changes/improvements come from with an MSE+discriminator loss vs. just MSE loss? In the GAN setting, as training progresses and the discriminator gets better and the encoder/decoder get more convincing, are there studies of the shifts in the encoder and decoder weights that could justify just where the improvements are?
