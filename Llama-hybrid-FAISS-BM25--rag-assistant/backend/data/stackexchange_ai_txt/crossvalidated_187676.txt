[site]: crossvalidated
[post_id]: 187676
[parent_id]: 25949
[tags]: 
The reason online gradient is useful is for large scales applications. In any case, now there are libraries that implement it so you don't need to program it. It is a good way to learn how things work. In Leon Bottou words: Large-scale machine learning was first approached as an engineering problem. For instance, to leverage a larger training set, we can use a parallel computer to run a known machine learning algorithm or adapt more advanced numerical methods to optimize a known machine learning objective function. Such approaches rely on the appealing assumption that one can decouple the statistical aspects from the computational aspects of the machine learning problem. This work shows that this assumption is incorrect, and that giving it up leads to considerably more effective learning algorithms. A new theoretical framework takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximationâ€“estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways. For instance, Stochastic Gradient Descent (SGD) algorithms appear to be mediocre optimization algorithms and yet are shown to perform extremely well on large-scale learning problems. Large scale learning sgd project
