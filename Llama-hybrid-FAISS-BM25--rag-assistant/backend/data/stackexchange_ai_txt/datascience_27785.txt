[site]: datascience
[post_id]: 27785
[parent_id]: 27366
[tags]: 
Consider a data set of 1000 patterns, each with 100 features. It is impossible to plot this data. When this data is run through SOM, the network learns the weights such that each of the neurons summarizes a subset of the vectors. After the training is complete, similar patterns get mapped to neighborhood neurons and farther apart neurons represent dissimilar patterns. Visualizing this mapping tells which patterns lie where, in a 2D space. For example, in the world poverty map [1] [2] , each pattern is a vector of length 39 and represents one country. When this data set is run through SOM, the countries(patterns) are mapped to nearest neurons and it can be seen that the countries are arranged together based on their economic wellness. After SOM is run until the stopping criterion is met, each pattern/input vector is mapped to the best matching unit. Hence, for example, Belgium (BEL) is closest to the weight vector of the first node in the map (top-right corner) and hence, mapped to it. Each node is colored based on the average distance of its weight vector with the weight vectors of its neighbors (U-matrix). However, it is still not clear how dimensionality reduction is achieved.
