[site]: datascience
[post_id]: 122313
[parent_id]: 
[tags]: 
Bayesian Neural Network Inference

In a paper about Bayesian Neural Network, I saw the following algorithm: Algorithm 1 Inference procedure for a BNN. Define $p(\theta\mid D)$ ; for $i = 0$ to $N$ do Draw $Î¸_i \sim p(\theta\mid D)$ ; $y_i = \Phi\theta_i(x)$ ; end for return $Y = \{y_i\mid i \in [0, N)\}, \Theta = \{\theta_i\mid i \in [0, N)\}$ ; The paper then states : In Algorithm 1, $Y$ is a set of samples from $p(y\mid x, D)$ and $\Theta$ a collection of samples from $p(\theta\mid D)$ . Usually, aggregates are computed on those samples to summarize the uncertainty of the BNN and obtain an estimator for the output $y$ . This estimator is denoted by $\hat y$ . But I dont understand why it states that $Y$ is a set of samples from $p(y\mid x, D)$ . Shouldn't it be a sample from $p(y/x,0)$ ? If someone could give me a math proof or the intuition.
