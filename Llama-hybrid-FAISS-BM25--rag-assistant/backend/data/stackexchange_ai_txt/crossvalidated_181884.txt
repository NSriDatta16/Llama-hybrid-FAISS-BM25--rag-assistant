[site]: crossvalidated
[post_id]: 181884
[parent_id]: 
[tags]: 
Reducing variance of Monte Carlo estimates via a known relationship

Here's a simplification of the actual problem, but I believe it captures the essence of it. Say we are interested in estimating the expectation of a stochastic time series. We can simulate the time series, so we can come up with the estimate, $\bar{x}(t)$, by just averaging many time series. $x$ could be a vector. We also estimate the covariance matrix, $\Sigma(t)$, in the usual way. Say I have some equation that relates the two estimated quantities. $$f(\Sigma(t)) = g(\mathbb{E}x(t))$$ $f$ and $g$ are known and are at least continuous. The goal is to reduce the variance of the estimates. Can I somehow "correct" the estimates, $\bar{x}$ and $\bar{\Sigma}$? It seems like nonlinear least squares might help, but I'd rather not cook up my own method if there's already theory out there. Are there are already methods and theory out there for this sort of problem? Thanks. Edit: Changed the equation such that $g$ is a function of the expectation of $x$.
