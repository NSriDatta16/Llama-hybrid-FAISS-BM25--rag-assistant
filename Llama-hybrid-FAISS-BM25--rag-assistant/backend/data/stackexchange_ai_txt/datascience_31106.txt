[site]: datascience
[post_id]: 31106
[parent_id]: 31100
[tags]: 
[NOTE: I have not myself worked through Aurélien Geron's tutorials, but I have read the book] On an intuitive level, I can persuade myself that the training would actually be slower for a pretrained model. In other words, it could make sense that the rate at which a error decreases (or accuracy increases) might be lower. The fact that training accuracy is lower is (for me at least) a little more complex and, perhaps, case specific. Rate of learning However the pretraining seems to make training slower. Using a pretrained model, we have essentially taken a set of weights, which are already (at least partially) optimised for one problem. They are geared towards solving that problem based on the dataset they received, which means they expect the input to correspond to a certain distribution. You have frozen the first two layers with this line: if pretraining: training_op = optimizer.minimize(loss, var_list=[weights3, biases3]) Freezing two layers (in your case, out of three), intuitively kind of restricts the model. Here is a somewhat contrived analogy that I might use to explain such cases to myself. Imagine we had a clown who could juggle with three balls, but now we want them to learn to use a fourth ball. At the same time, we ask an amateur to learn how to juggle, also with four balls. Before measuring their rate of learning, we decide to tie one of the clown's hands behind their back. So the clown already knows some tricks, but is also constrained in some way during the learning process. In my mind, the amateur would most likely learn a lot faster (relatively), as there is more to learn - but also because they have more freedom to explore the parameter space i.e. they can move more freely using both arms. In the setting of optimisation, one might imagine that position of the pretrained model on a loss curve is already in a place where gradients are very small in certain dimensions (don't forget, we have a high-dimensional search space). This ends up meaning that it cannot as quickly make changes to the output of the weights whilst backpropagating errors, as the weight updates are multiples of these potentially small optimised weights. ... Ok - might sounds plausible, but this only addresses the problem of slow learning - what about the fact that the actual training accuracy is lower that that of the model with random initialisation?? Intial training accuracy I expected the pretrained network would start with a lower error (compared to the network not using pretraining)... Here I tend to agree with you. In the optimal case, we could take a pretrained model, use the initial layers as they are and just fine-tune the final layers. There are, however, some cases in which this might not work. Looking into related literature, there is a possible explanation from the abstract of the paper: How transferable are features in deep neural networks? (Yosinski et al.) : Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. I find the second reason to be particularly interesting and relevant to your setup. This is because you actually only have three layers. You are therefore not allowing must freedom to fine-tune, and the final layer was likely very dependent on its relationship to the preceding layer. What you might expect to see as a result of using a pretrained model, is rather that the final model exhibits better generalisation. This may indeed come at the cost of a lower test accuracy on the hold-out set of the specific dataset you train on. Here are another thoughts , summarised well by the amazing (and free) Stanford CS231n course: Learning rates . It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. In your code, the learning rate seems to be fixed for all learning phases at 0.01 . This is something you could experiment with; making it smaller for the pretrained layers, or just starting with a lower learning rate globally. Here is a comprehensive introduction to tranfer learning that might give you some more ideas about why/where you might make some different modelling decisions.
