[site]: crossvalidated
[post_id]: 605996
[parent_id]: 
[tags]: 
Compare Test Set Performance of Different Classifier Models

I have a machine learning classifier model and I want to evaluate how its performance on the test set depends on stochastic effects during training. Such effects happen, for example, because the weights for the model get initialized randomly, or the GPU's architecture is different, which can have an effect as well. Ideally, the training routine is robust against such effects and produces a more or less "similar" model with different seeds, gpus etc. In my understanding, "similar" means, that the performance on the test data set of each model variant is "similar" – which brings me to my question, what statistic can I provide to show how similar different variants are? What I have done so far is to train my model 10 times and use a different seed each time (everything else stays the same). As the task is multi-class classification, I am using the macro averaged accuracy as key metric. You can see the results here: I've also added 95% (bootstrapped) confidence intervals. All in all the result suggests, that the variants are pretty similar, as the CIs overlap – which in my limited statistical understanding is a good indicator. However, there has to be a better way to express this. I have found the Friedman test to compare the prediction of each variant for each test input sample, but here I get very low p values like "1.7e-11" (Q=70), which should be due to the 1 Million test samples that make it 'easy' to get low p-values. So I think this test is not appropriate for my case, but what is?
