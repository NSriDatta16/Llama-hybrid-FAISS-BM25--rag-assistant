[site]: crossvalidated
[post_id]: 599894
[parent_id]: 27300
[tags]: 
Some really great thoughts in the answers so far, and do a good job of explaining that the main job of PCA is to provide a few variables which are linear combinations of our original ones, not to select individual features of our original space. But it is actually possible to do something like that with the CUR decomposition. See CUR matrix decompositions for improved data analysis . This has as hyperparameter an integer $K$ which is the number of PCA components to retain per usual, and then you select features to retain randomly based on their total squared contributions to the first $K$ principle components. In particular, define the normalized statistical leverages to be $\pi_j=\sum_{k=1}^K\frac{v_{j,k}^2}{K}$ for each variable $j$ (here $v_{j,k}$ gives the $j$ element of singular vector $k$ ). The theory presented in the referenced article relies on keeping each variable with probability proportional to $\pi_j$ . Keep this between you and me, but I usually like to just look at the variables with top scores instead of choosing randomly.
