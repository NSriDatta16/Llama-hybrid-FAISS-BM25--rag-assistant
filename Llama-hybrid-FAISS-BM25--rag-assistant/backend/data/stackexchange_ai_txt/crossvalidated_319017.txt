[site]: crossvalidated
[post_id]: 319017
[parent_id]: 181
[tags]: 
I'd like to suggest a less common but super effective method . Basically, you can leverage a set of algorithms called "genetic algorithms" that try a small subset of the potential options (random number of layers and nodes per layer). It then treats this population of options as "parents" that create children by combining/ mutating one or more of the parents much like organisms evolve. The best children and some random ok children are kept in each generation and over generations, the fittest survive. For ~100 or fewer parameters (such as the choice of the number of layers, types of layers, and the number of neurons per layer), this method is super effective. Use it by creating a number of potential network architectures for each generation and training them partially till the learning curve can be estimated (100-10k mini-batches typically depending on many parameters). After a few generations, you may want to consider the point in which the train and validation start to have significantly different error rate (overfitting) as your objective function for choosing children. It may be a good idea to use a very small subset of your data (10-20%) until you choose a final model to reach a conclusion faster. Also, use a single seed for your network initialization to properly compare the results. 10-50 generations should yield great results for a decent sized network.
