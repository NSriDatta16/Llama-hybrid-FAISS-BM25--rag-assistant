[site]: crossvalidated
[post_id]: 529599
[parent_id]: 526864
[tags]: 
The class of methods devised to solve this generic problem are known in the optimisation literature as projected Newton methods. The following is extracted from Bertsekas and Gafni (1983) , with some minor additions to the notation. In their simplest formulation, these are methods which solve simply constrained problems , which take the form $$\min{J(x)} \quad \text{subject to} \quad x \geq 0, \tag{4}$$ where $x = [x^1, \dots, x^n] \in \mathbb{R}^n$ , $J: \mathbb{R}^n \rightarrow \mathbb{R}$ is a continuously differentiable function, and the vector inequality $x \geq 0$ is component-wise, i.e. $x^i \geq 0$ for all $i = 1, \dots, n$ . The above problem admits an iterative solution of the form $$x_{k+1} = [x_k - \alpha_k D_k \nabla J(x_k)]^+, \tag{5}$$ where $a_k$ is a positive scalar step size, $D_k$ is a positive definite symmetric matrix which is diagonal with respect to some of the co-ordinates of $x$ , and $[\cdot]^+$ denotes projection (with respect to the standard norm) on the positive orthant. For avoidance of doubt on this latter operation, Bertsekas (1982) defines $[\cdot]^+$ as $$[z]^+ = \begin{bmatrix} \max \{0, z^1 \} \\ \vdots \\ \max \{0, z^n \} \\ \end{bmatrix},$$ where $z \in \mathbb{R}^n$ . The precise details of how $\alpha_k$ and $D_k$ are chosen; as well as convergence properties (in particular superlinear convergence in the case where $D_k$ is chosen on the basis of the second derivatives of $J$ ) can be found in the aforementioned references. These are: Bertsekas, D. (1982). Projected Newton methods for optimization problems with simple constraints. SIAM J. Control and Optimization, 20(2), 221-246. https://doi.org/10.1137/0320018 Bertsekas, D., Gafni, E. (1983). Projected Newton methods and optimization of multicommodity flows. IEEE Transactions on Automatic Control, 28(12), 1090-1096. https://doi.org/10.1109/TAC.1983.1103183
