[site]: datascience
[post_id]: 88107
[parent_id]: 80660
[tags]: 
@Jind≈ôich is exactly right. this is not an answer by itself, but rather some pointers to the implementation in the annotated transformer of every item he mentioned: The input of the dense layers is of shape $b \times l \times d_m$ : torch.from_numpy(np.random.randint(1, V, size=(batch, l))) : $b \times l$ self.lut = nn.Embedding(vocab, d_model); self.lut(x) : $b \times l \times d_m$ The output of the dense layer has the same shape $b \times l \times d_m = b \times l \times hd_k$ : self.linears = clones(nn.Linear(d_model, d_model), 4) : these are $W^Q,W^K,W^V,W^O$ respectively, and their output is $b \times l \times d_m$ l(x).view(nbatches, -1, self.h, self.d_k) : converts the output to $b \times l \times h \times d_k$ Then you can reshape the queries and keys to have shape $b \times h \times l \times d_k$ l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) : converts the output to $b \times h \times l \times d_k$ , done for K, Q and V. Now, if you permute the dimensions... scores = torch.matmul(query, key.transpose(-2, -1)) : $[b \times h \times l \times d_k]\times[b \times h \times d_k \times l] = [b \times h \times l \times l]$ scores = scores.masked_fill(mask == 0, -1e9) : mask is $b \times 1 \times 1 \times l$ for encoder layers, and $b \times 1 \times l \times l$ for decoder layers (actually $l-1$ but lets ignore that). Both are broadcastable to the scores tensor, the encoder mask is the same for all heads and for all positions, while the decoder mask is different for every position, as it hides the subsequent positions. Then, if you do softmax in the last dimension... p_attn = F.softmax(scores, dim = -1) : $b \times h \times l \times l$ , but now each vector sums to 1 By doing the batch matrix multiplication ... you get the weighted average torch.matmul(p_attn, value) : $[b \times h \times l \times l] \times [b \times h \times l \times d_k] = [b\times h\times l \times d_k]$ , which are the weighted averages we wanted So finally, the "concatenation" of all heads is in fact just another tensor reshaping x.transpose(1, 2).contiguous() : $[b\times l\times h \times d_k]$ x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) : $[b\times l\times d_m]$
