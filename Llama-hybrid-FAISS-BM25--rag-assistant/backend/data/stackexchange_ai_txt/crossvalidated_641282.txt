[site]: crossvalidated
[post_id]: 641282
[parent_id]: 641277
[tags]: 
If we are talking about the usual fully connected / convolution layers combined with activations (i.e. not a complicated deep learning model with all sorts of layers), yes, they are second-order differentiable given that the activations are second-order differentiable. Or, even, we can say that, the overall function is n-th order differentiable if activations are; because the rest of the operations are just matrix multiplication. If you think about the partial derivative wrt some variable in the network, it's just scalar multiplication, addition, activation; then the same thing again. For a two layer, it looks like $$g(x)=f(a_2f(a_1x+b_1)+b_2)$$ which can be easily differentiated as long as $f$ is differentiable.
