[site]: crossvalidated
[post_id]: 575865
[parent_id]: 575757
[tags]: 
I'm answering this purely from a deep learning architecture persepective. There may well be domain-specific reasons why the authors have chosen the architecture described. This review paper: Stahlschmidt et al.'s Multimodal deep learning for biomedical data fusion: a review may provide some domain-specific insights. Data fusion (the term for what you are describing) can be done at the input layer (early fusion), immediately before the output layer (late fusion) or somewhere in between (middle or intermediate fusion). If the fusion is followed by CNN layers, the two inputs are usually stacked channel-wise. This means that they have to be compatible - so the same sizes across all dimensions except the channels. You probably also would want the time steps to be synchronised. When using early fusion, the model will learn joint features from the inputs; while with late fusion it learns separate features from each input. Middle fusion does a bit of both. Which one is best depends on the data. If you keep the same convolutional parameters, and have the same total number of filters for each layer, then the number of parameters in your model will be about the same whichever strategy you use.
