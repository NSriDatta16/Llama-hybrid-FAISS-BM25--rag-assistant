[site]: crossvalidated
[post_id]: 386507
[parent_id]: 
[tags]: 
Strange batch loss in keras

Im training a Bidirectional RNN with keras.losses.MSE and have my dataset shuffled before training. I manually split it into validation and train data. However when i start training loss follows some strange pattern like the one on the picture with batch loss having drops at the epoch end. Secondly, sometimes loss has this pattern: gradually decreasing batch loss, then huge drop at validation loss at the epoch end, and after this new epoch batch losses start with the last validation loss, but not the last batch loss, just like my NN learned something during validation data testing. Also, i have some concerns about MSE loss, which increases proportionally to the batch size. It is MEAN square error, so should not it be independent from the batch size?
