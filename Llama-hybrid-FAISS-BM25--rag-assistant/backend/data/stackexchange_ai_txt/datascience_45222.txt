[site]: datascience
[post_id]: 45222
[parent_id]: 45149
[tags]: 
I have done something similar in the past. I'll sketch an outline for you. First you break the text into paragraphs and tokenize. Then write some regexp rules to capture the data you want to remove. For instance, if an email signature commonly contains a phone number, paragraph, and a website, you can count those features and flag it based on some threshold you decide. Next, do likewise with the other features you mentioned. My experience is it's highly domain dependent so you really need to look at the data and use your best judgement. The result of this process should be a data consisting of tokenized paragraphs where the paragraph has been labeled 'noise' or 'clean' based on the feature count. From there, convert your token representation using tf-idf or another type of embedding. You should be able to use this as input to your favorite classifier, and I have had success using SVMs to that end. The result is going to be biased towards your rules but you are also leveraging features that are in the labeled examples that are not explicitly in the rules. Particularly so for longer paragraphs. It might seem a bit a bit janky but believe it or not it works.
