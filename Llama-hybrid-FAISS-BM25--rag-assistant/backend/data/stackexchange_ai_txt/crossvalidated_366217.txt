[site]: crossvalidated
[post_id]: 366217
[parent_id]: 362987
[tags]: 
As Frans Rodenburg already correctly stated in his comment, in most cases instance or sample weights factor into the loss function that is being optimized by the method in question. Consider the equation the documentation provides for the primal problem of the C-SVM $$\min_{w,b,\zeta} \frac{1}{2}w^Tw + C\sum_{i=1}^{n} \zeta_i. $$ Here $C$ is the same for each training sample, assigning equal 'cost' to each instance. In the case that there are sample weights passed to the fitting function "The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right." As this example puts it , which also provides a nice visualization, showing how instances represented by bigger circles (those with larger weight) influence the decision boundary.
