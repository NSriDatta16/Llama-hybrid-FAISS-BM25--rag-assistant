[site]: crossvalidated
[post_id]: 610095
[parent_id]: 
[tags]: 
How do we train a neural network?

Yes, it's a very basic question, but I'm not sure if I'm undertanding the process. I feed my network with the first observation, with random W for each neuron. I get a predicition, with an error. Then comes all the iterative process of adjusting the W values (backpropagation, gradient descending, learning rate..), until I have an aceptable error for this single observation (or I have performed a max numbers of iterations). Is that correct? Then...? I feed the network with the next observation, starting with the W values calculated in previous one observation, and repeat the adjusting process, until I get new W's that minimize the error for this new observation? This changes the W values for many (or almost all) neurons... So the idea of a al the process is that, after feeding my network with all observations (training data) the W's of all neurons CONVERGE to a set of values that gives me a valid prediction to all (or a acceptable percentage of) future observation? Thanks
