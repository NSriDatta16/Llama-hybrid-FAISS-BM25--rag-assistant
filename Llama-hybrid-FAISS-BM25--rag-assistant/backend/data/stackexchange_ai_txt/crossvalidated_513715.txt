[site]: crossvalidated
[post_id]: 513715
[parent_id]: 513710
[tags]: 
Considering a statistical model represented by a family of probability densities (wrt the same measure $\text dx$ ) $$\mathfrak F=\{f(\cdot;\theta)\,;\ \theta\in\Theta\}$$ each density within that family is associated with a parameter $\theta$ . Given in addition a probability density $\pi(\cdot)$ over $\Theta$ (endowed with a measure $\text d\theta$ ) the marginal likelihood is defined as $$m(x) = \int_{\Theta} f(x;\theta)\,\pi(\theta)\text d\theta\tag{1}$$ and is therefore the marginal density of $X$ associated with the joint density $f(x;\theta)\,\pi(\theta)$ of the pair $(X,\theta)$ free of the parameter $\theta$ , which is effectively integrated out in (1) a misnomer in that it is a function of $x$ and only of $x$ , while the likelihood is primarily a function of $\theta$ , which is why the term evidence or integrated likelihood is often preferred an average of all possible density values at $x$ , when weighting each value of $\theta$ with $\pi(\theta)$ the prior predictive density resulting from (i) generating $\theta$ from the prior and (ii) $X$ from the model associated with the generated value of $\theta$ the normalising constant in the posterior distribution, so that it integrates to one in $\theta$ a measure of how the model $\mathfrak F$ fits the realisation $x$ of $X$ and hence a tool for model comparison (via Bayes factors) not necessarily needed when approximating Bayesian procedures by simulation (although this approach remains "fully Bayesian") since simulation techniques often bypass the normalising constant.
