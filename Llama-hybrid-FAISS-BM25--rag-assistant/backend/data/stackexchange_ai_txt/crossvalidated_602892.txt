[site]: crossvalidated
[post_id]: 602892
[parent_id]: 602840
[tags]: 
Fermat's Last Theorem is a proposition of Number Theory, so you'd want to prove it from Peano's axioms; the parallel postulate, of Euclidean geometry, so from Euclid's other four postulates: but the Weak Likelihood Principle (a.k.a. the Sufficiency Principle) isn't a proposition of Probability Theory, so it's not obvious what you'd want to prove it from. Birnbaum (1962) kicked off the approach of giving a formal account of the relationships between various "principles". He took the concept of evidential meaning as basic & the W.L.P. as axiomatic, & went on to derive the Strong Likelihood Principle from this & another axiom, the Conditionality Principle. His formal statement of the W.L.P. is that for inference about a parameter $\theta$ in an experiment $E$ , where $T$ is a sufficient statistic for $\theta$ , if $T(x) = T(y)$ for samples $x$ & $y$ , then $\operatorname{Ev(E,x)} = \operatorname{Ev}(E, y)$ ; in which $\operatorname{Ev(E,x)} = \operatorname{Ev}(E, y)$ denotes "evidential equivalence" or your "containing the same inferentially useful information". This is not an empirical claim, or even a mathematical one, but purports to constrain (sensible) inferential procedures: if it's entailed by other foundational principles you hold dear, then all well & good; if not then you may try & balance it against those or to eschew it altogether. The W.L.P. is part & parcel of Bayesian frameworks (e.g. Savage, 1954): the likelihood's all from the data that goes into the calculation of posterior probabilities. (Not necessarily so for the S.L.P.—see Do you have to adhere to the likelihood principle to be a Bayesian? .) Perhaps more interestingly, purely frequentist desiderata tend to mandate the use of sufficient statistics in estimation & testing—consider the Rao–Blackwell Theorem & the Neyman–Pearson Lemma & their ramifications. On those occasions when a randomized estimator or test does enjoy some kind of optimality, that's more prone to be taken as evincing the need for the W.L.P. than as a counter-example. In complex situations different criteria often clash. For example, a solution to the Behrens–Fisher problem was posted here last year: an exact test with better power properties than several alternatives: the only thing wrong with it is that it violates the W.L.P. (But note that in all cases, it's a matter of 'padding out' the sufficient statistic with random noise, & it makes no odds whether the noise is real—from the ancillary part of the data—or synthetic—introduced by the statistician—see the first bullet point of @Sextus Empiricus' answer . There's no "non-likelihood information" being exploited.) Fiducial inferences may violate the W.L.P. in a different way—in cases where reduction of the data to a sufficient statistic positively discards information held to be pertinent. See Fraser (1963) for discussion & an example. In fact the difficulty isn't unique to fiducial approaches: the nub of the matter is that a premature reduction may conflate events you'd prefer to separate through conditioning (Kalbfleisch, 1975). (This is generally seen as calling for strictures on when to invoke the W.L.P. rather than for its abandonment.) Birnbaum (1962), "On the Foundations of Statistical Inference", J. Am. Stat. Assoc. , 57 , 298 Fraser (1963), "On the Sufficiency & Likelihood Principles", J. Am. Stat. Assoc. , 58 , 303 Kalbfleisch (1975), "Sufficiency & Conditionality", Biometrika , 62 , 2 Savage (1954), The Foundations of Statistics
