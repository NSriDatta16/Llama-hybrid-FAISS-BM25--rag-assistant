[site]: datascience
[post_id]: 41649
[parent_id]: 
[tags]: 
What is the difference between SVM and logistic regression?

While reading the book by Aurelien Geron, I noticed that both logistic regression and SVM predict classes in exactly the same way, so I suspect there must be something that I am missing. In the Logistic regression chapter we can read: $σ(t) when $t , and $σ(t) ≥ 0.5$ when $t ≥ 0$ , so a Logistic Regression model predicts $1$ if $θ^T · x$ is positive, and $0$ if it is negative. Similarly, in the SVM chapter: The linear SVM classifier model predicts the class of a new instance x by simply computing the decision function $w^T · x + b = w_1 x_1 + ⋯ + w_n x_n + b$ : if the result is positive, the predicted class $ŷ$ is the positive class ( $1$ ), or else it is the negative class ( $0$ ). I know that one way they could be different is because of the loss function they use: while log loss is used in logistic regression, SVM uses hinge loss to optimize the cost function. However, I would like to get this thing completely clear. How are the two models actually different?
