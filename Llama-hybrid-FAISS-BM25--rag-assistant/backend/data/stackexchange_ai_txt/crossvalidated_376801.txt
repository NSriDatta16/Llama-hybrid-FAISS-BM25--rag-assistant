[site]: crossvalidated
[post_id]: 376801
[parent_id]: 376772
[tags]: 
Your series of experiments can be viewed as a single experiment with far more data, and as we know, more data is advantageous (eg. typically standard errors decrease as $\sqrt{n}$ increases for independent data). But you ask, "Is this ... enough evidence to conclude that H0 is true?" No. A basic problem is that another theory may produce similar patterns in data! As @Bj√∂rn discusses in his answer, you will fail to reject a false $H_0$ if your experiment lacks power to distinguish $H_0$ from other possibilities. For centuries, we failed to reject Newton's theory of gravity because no one had conducted the types of tests where Newton's theory gives sufficiently different predictions than Einstein's theory of general relativity. Less extreme examples are commonplace. David Hume and the problem of induction Perhaps a rephrasing is, "If I obtain more and more data consistent with $H_0$ being true, can I ever conclude that $H_0$ is true?" That question is deeply related to 18th century philosopher David Hume's problem of induction . If all observed instances of A have been B , can we say that the next instance of A will be B? Hume famously said no, that we cannot logically deduce that "all A are B" even from voluminous data. In more modern math, a finite set of observations cannot logically entail $\forall_{a \in A} \left[ a \in B \right]$ if A is not a finite set. Two notable examples as discussed by Magee and Passermore : For centuries, every swan observed by Europeans was white. Then Europeans discovered Australia and saw black swans. For centuries, Newton's law of gravity agreed with observation and was thought correct. It was overturned though by Einstein's theory of general relativity. If Hume's conclusion is correct, proving $H_0$ true is unachievable. That we cannot make statements with certitude though is not equivalent to saying we know nothing at all. Experimental science and statistics have been successful in helping us understand and navigate the world. An (incomplete) listing of ways forward: Karl Popper and falsificationism In Karl Popper's view, no scientific law is ever proven true. We only have scientific laws not yet proven false. Popper argued that science proceeds forward by guessing hypotheses and subjecting them to rigorous scrutiny. It proceeds forward through deduction (observation proving theories false), not induction (repeated observation proving theories true). Much of frequentist statistics was constructed consistent with this philosophy. Popper's view has been immensely influential, but as Kuhn and others have argued, it does not quite conform to the empirically observed practice of successful science. Bayesian, subjective probability Let's assume we're interested in a parameter $\theta$ . To the frequentist statistician, parameter $\theta$ is a scalar value, a number. If you instead take a subjective Bayesian viewpoint (such as in Leonard Jimmie Savage's Foundation of Statistics ), you can model your own uncertainty over $\theta$ using the tools of probability. To the subjective Bayesian, $\theta$ is a random variable and you have some prior $P(\theta)$ . You can then talk about the subjective probability $P(\theta \mid X)$ of different values of $\theta$ given the data $X$ . How you behave in various situations has some correspondence to these subjective probabilities. This is a logical way to model your own subjective beliefs, but it's not a magic way to produce probabilities that are true in terms of correspondence to reality. A tricky question for any Bayesian interpretation is where do priors come from? Also, what if the model is misspecified? George P. Box A famous aphorism of George E.P. Box is that "all models are false, but some are useful." Newton's law may not be true, but it's still useful for many problems. Box's view is quite important in the modern big data context where studies are so overpowered that you can reject basically any meaningful proposition. Strictly true versus false is a bad question: what matters is whether a model helps you understand the data. Additional comments There's a world of difference in statistics between estimating a parameter $\theta \approx 0$ with a small standard error versus with a large standard error! Don't walk away thinking that because certitude is impossible, passing rigorous scrutiny is irrelevant. Perhaps also of interest, statistically analyzing the results of multiple studies is called meta-analysis . How far you can go beyond narrow statistical interpretations is a difficult question.
