[site]: crossvalidated
[post_id]: 398520
[parent_id]: 343481
[tags]: 
I wasn't totally sure about the pasting method. From Hands-On Machine Learning with Scikit-Learn it's written : Another Approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is perform with replacement, this method is called bagging , when sampling is performed without replacement it is called pasting Because the bagging method use replacement, you can see data that wouldn't appear IRL ( let say two occurences of the same unique data), so the bias can be higher. But, you are not limited by the number of classifier you train, because you can take the same data other times. The more classifier you train, the less variance you have
