[site]: crossvalidated
[post_id]: 205017
[parent_id]: 86472
[tags]: 
I feel like the answer that I was looking for when it came to this question is best summarized by Lesaffre and Lawson in Bayesian Biostatistics The posterior precision is the sum of the prior and the sample precision, i.e.: $$ \frac{1}{\sigma^2} = w_{0} + w_{1} $$ This shows that the posterior is more peaked than the prior and the likelihood function which means that the posterior contains more information about $\mu$ than the prior and the likelihood function. This property holds even when the likelihood and prior are in conflict (in contrast to the binomial-beta case). This may look counterintuitive since, in the presence of conflicting information, there is more uncertainty a posteriori rather than less uncertainty. Note that this result only holds for the special, and unrealistic, case of a known $\sigma$. What this summarizes for me, and is roughly outlined in the other answers, is that the case of modeling normal priors with a normal likelihood can result in a situation where the posterior is more precise than either. This is counterintuitive, but is a special consequence of modeling these elements this way.
