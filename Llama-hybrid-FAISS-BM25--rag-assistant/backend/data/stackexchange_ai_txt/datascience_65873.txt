[site]: datascience
[post_id]: 65873
[parent_id]: 65815
[tags]: 
I agree with @peter's detailed answer on points #1 through #5 and would like to supplement that with some more details: Perform a PCA or MFA of the correlated variables and check how many predictors from this step explain all the correlation. For example, highly correlated variables might cause the first component of PCA to explain 95% of the variances in the data. Then, you can simply use this first component in the model. Random forests can also be used for feature selection by looking at the feature importances of the variable. However, correlated variables can cause misleading feature importances. You can use permutation feature importance ( https://scikit-learn.org/stable/modules/permutation_importance.html ) You might be overfitting when using all the features . Therefore, it is essential to use a validation set to check for overfitting. You can also drop useless features by adding a feature with random values and dropping any feature that has lower feature importance than that feature. For this topic, I highly recommend the book: Feature Engineering and Selection: A Practical Approach for Predictive Models ; Open-sourced here: http://www.feat.engineering/ And also available on Amazon.
