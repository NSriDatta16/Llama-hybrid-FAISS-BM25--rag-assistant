[site]: datascience
[post_id]: 76576
[parent_id]: 76328
[tags]: 
ELMo does not lookup the embeddings from a pre-precomputed table as Word2Vec and GloVe. Embeddings from ELMo are hidden states of an LSTM-based language model, i.e., they are computed on the fly when you give a sentence to the network. ELMo even does use standard word embeddings as the LSTM input. Words are treated as character sequences and those are processed with a 1-dimensional CNN that provides vector representations of the words. The word representations are then passed into two two-layer LSTMs which are trained as a forward and a backward language model repsectively. What you get as a contextual word embedding is a an average of the LSTM states.
