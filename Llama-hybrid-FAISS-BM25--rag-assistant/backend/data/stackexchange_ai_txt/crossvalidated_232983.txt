[site]: crossvalidated
[post_id]: 232983
[parent_id]: 
[tags]: 
why slow learning rate, more iterations would be better (comparing to large learning rate less iterations)?

why slow learning rate, more iterations would be better (comparing to large learning rate less iterations)? I think this statement is generally true for neural network or gradient boosting models. Here is one example to demonstrate the problem. In the demo, we are trying to approximate a 2D quadratic function. Ground truth is shown in left figure, and the approximation is shown in right figure. Gradient boosting model is used, where $f(x)=\sum_{i=1}^n \alpha\cdot b_n(x)$. $n$ is the iterations, and $\alpha$ is the learning rate. learning rate $0.2$, iteration $50$. learning rate $0.02$, iteration $500$. Intuitively I can understand small learning rate is "fine tuning" instead of "rough tuning", but is there any formal explanations? a related post Boosting: why is the learning rate called a regularization parameter?
