[site]: crossvalidated
[post_id]: 580572
[parent_id]: 
[tags]: 
Understanding error in bayesian inference

Let us say we have: Data $X$ Parameter that we are trying to estimate is $\Theta$ The Bayesian estimation method is to Assume a prior on $\Theta$ Sample $x$ from $X$ Use Bayes theorem. Compute the posterior $\Theta\mid(X=x)$ Now, in case a point estimate is to be reported then one way is to report lowest mean square estimate $\hat{\Theta} = E[\Theta \mid X]$ . This is supposed to be the parameter that has lowest mean squared error. The error being defined as $\bar{\Theta}=\hat{\Theta} - \Theta$ . Now, two supposedly intuitive properties of $\bar{\Theta}$ are $E[\bar{\Theta}] = 0$ $E[\bar{\Theta} \mid X] = 0$ Whenever I think of expected values I think there is an inherent distribution that the expectation is computed using. What is the distribution over which $E[\bar{\Theta}] = 0$ is computed? There are two sources of randomness namely $X$ and $\Theta$ . Moreover, Expectations should be able to be estimated by iterated sampling. That procedure, I think, would like the following in the case of $E[\bar{\Theta} \mid X]$ Sample $x$ from $X$ . Compute posterior $\Theta \mid (X = x)$ Compute $\hat{\theta} = E[\Theta \mid X = x]$ . The expectation of the posterior computed above Sample $\theta$ from posterior $\Theta \mid (X = x)$ Compute $\bar{\theta}=\hat{\theta} - \theta$ If you iterate over these 5 steps large number of times then the average of $\bar{\theta}$ would converge to 0 What would that procedure look like for $E[\bar{\Theta}]$ ? My attempt: Sample $x$ from $X$ Sample $\theta$ from Prior $\Theta$ Compute posterior $\Theta \mid (X = x)$ Compute $\hat{\theta} = E[\Theta \mid X = x]$ . The expectation of the posterior computed above Compute $\bar{\theta}=\hat{\theta} - \theta$ If you iterate over these 5 steps large number of times then the average of $\bar{\theta}$ would converge to 0 Am I right?
