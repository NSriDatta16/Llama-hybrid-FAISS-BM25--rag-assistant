[site]: datascience
[post_id]: 122936
[parent_id]: 122865
[tags]: 
+1 for a great reproducible code sample. The answer to your main question can be seen by looking at the linear algebra, of what self-attention does. Go back to what matrix multiplication is doing, and you can see it is just summing a bunch of multiplies. The order of the tokens has to be lost. So, safe in that knowledge, I then spent some time trying to work out why your script still works. I noticed the following. With torch.manual_seed(42) I get as you reported: A hit B predicting ! B hit A predicting ? But with torch.manual_seed(77) I get: A hit B predicting B B hit A predicting hit And with torch.manual_seed(88) I get: A hit B predicting hit B hit A predicting ? To be honest I've not convinced myself, as quite a few other random seeds give !/? still. I also seem to get ? for the second one more often than ! for the first. I guess the proper thing to do would be to run the experiment for a large number of different seeds and report how random the results are. If I reduce head_size to 2, so it is smaller than T , rather than larger, I get the desired result for a seed of 88, but every other seed I've tried fails. I find that curious, rather than an explanation. If we believe that positional embeddings are not needed, a dim-2 transformer still ought to have enough capacity to memorize ("over-fit") two sentences of length 3 with a vocab size of 5? And if we believe positional embeddings are needed, the head size shouldn't matter?
