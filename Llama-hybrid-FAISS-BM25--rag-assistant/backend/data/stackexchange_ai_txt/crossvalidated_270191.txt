[site]: crossvalidated
[post_id]: 270191
[parent_id]: 270161
[tags]: 
Hopefully this answer will help you clarify some things. I don't say anything about Bayesian hypothesis testing, however. Your model is the probability distribution that generates your data. I can guess that in this case, your model is a normal distribution with mean parameter $\mu$ and variance parameter $\sigma^2$. Your data is assumed to be a bunch of realized random variables drawn from this model. I am guessing that you assume you have a random sample, so $\xi_1,\ldots,\xi_n$ are assumed to be independently and identically drawn from the normal distribution I just mentioned. Let's write this as $$ \xi_1,\ldots,\xi_n \overset{iid}{\sim} \text{Normal}(\mu,\sigma^2). $$ Another way to write this is that the joint density or likelihood of all your data is $$ f(\xi_1, \ldots, \xi_n ; \mu, \sigma^2) = (2\pi\sigma^2)^{-n/2}\exp\left[-\frac{\sum_i (\xi_i-\mu)^2}{2\sigma^2} \right]. $$ If you think of this as a function of your data, and assume some parameters are known, you can do things with this density, like calculate means, variances, find probabilities, etc. If you have some known fixed data, you can think of it as a function in the parameters, and do things like find the most likely parameters via maximizing the likelihood. In real life, you don't know all of the parameters of your distribution usually, though. In hypothesis testing , you make up some claim or hypothesis about your model or data, and then test the claim. You always end up either rejecting the claim or failing to reject it. The claim is the null hypothesis , and it is usually denoted by $H_0$. You assume it's true, calculate some test statistic, and then calculate how "rare" your observed data was. If it's too rare, then you reject this null hypothesis in favor of an alternative hypothesis because your assumption about $H_0$ is probably wrong. In your case I would write these two as $$ H_0: \mu = \mu_0 $$ and $$ H_a: \mu > \mu_0. $$ Here $\mu$ is the true unknown mean, and $\mu_0$ is the specific value you think your mean might be equal to. One-sided tests: Let $\mu_1$ be an alternative parameter that's greater than $\mu_0$. The Neyman-Pearson Lemma suggests you reject in favor of $H_a$ when $$ \frac{f(\xi_1, \ldots, \xi_n ; \mu_1, \sigma^2) }{f(\xi_1, \ldots, \xi_n ; \mu_0, \sigma^2) } $$ is greater than some threshold. This is equivalent to rejecting if $$ \frac{\sqrt{n}(\bar{X} - \mu_0) }{\sigma} > z_{\alpha} $$ for an $\alpha$ level test if $\sigma$ is known, or rejecting when $$ \frac{\sqrt{n}(\bar{X} - \mu_0) }{s} > t_{\alpha,n-1} $$ when $\sigma$ is unknown. This test is guaranteed to be uniformly most powerful. Two-sided tests You mention you want to calculate the likelihood ratio test. This is more suited to two-sided tests. You would calculate $$ \frac{\sup_{\sigma^2 \in \mathbb{R}^+} f(\xi_1, \ldots, \xi_n ; \mu_0, \sigma^2) }{\sup_{\mu,\sigma^2 \in \mathbb{R} \times \mathbb{R}^+} f(\xi_1, \ldots, \xi_n ; \mu, \sigma^2) } = \frac{\sum_i(x_i-\bar{x})^2 }{\sum_i(x_i - \mu_0^2) }, $$ and then calculate it's p-value, rejecting when it's small. You can do this a few different ways, but i would recommend just googling "two sided two-sample t-test for normal data" and then just use an implementation in some software package.
