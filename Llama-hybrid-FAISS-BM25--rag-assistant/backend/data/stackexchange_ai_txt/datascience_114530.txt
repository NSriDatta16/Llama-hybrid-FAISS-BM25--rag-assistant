[site]: datascience
[post_id]: 114530
[parent_id]: 114462
[tags]: 
After looking over the model, I discovered that the problem was insufficient convergence on the test set due to overfitting, which was resolved through increasing the strength of the L2 regularization. The deviance of the fitted model is guaranteed to be lower than that of the null model, but only if it has properly converged on the universe of data and is evaluated on the optimal $y$ . Otherwise, it is possible for an overfit model to produce a deviance (evaluated on the test set) that is lower than that of the null model (evaluated using the average value from the test set for the probability, and thus converged on the test set), even if it shows better-than-chance performance . In particular, since the deviance for a binary logistic model consists of the sum of terms of the form $y_k\ln(p(1,k))+(1-y_k)\ln(1-p(1,k))$ , which incorporates the magnitude of the probabilities, it is possible for most of the terms in this sum to be lower in absolute than the terms in the sum for the null model, with the overall sum still being more negative due to outliers, as was the case here. In other words, if when the model is good, it is moderately good, but when it is bad, it is quite bad, this behavior can occur. These outliers seemed to be a symptom of overfitting, unsurprisingly given the large number of variables relative to the samples (perhaps 75% as many variables as samples), as is common in textual analysis. To solve this overfitting, I substantially increased the regularization strength, which resolved the problem and made the deviance of the fitted model lower in absolute value.
