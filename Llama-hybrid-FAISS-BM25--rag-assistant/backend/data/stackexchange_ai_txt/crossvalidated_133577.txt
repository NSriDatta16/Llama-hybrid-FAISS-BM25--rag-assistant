[site]: crossvalidated
[post_id]: 133577
[parent_id]: 133571
[tags]: 
I can't answer all of your questions, but this is what I know for sure. Adding a random effect to clmm has much the same impact as adding a random term to a logistic regression model. Take a simple case with ordinal response $Y$ , continuous covariate $X$ and grouping variable $G$ . Write $p_k=Prob(Y \leq k)$ , for some level of the ordinal response. The model posits that for subject $i$ in group $j$ , $$\log \frac{p_k}{1-p_k}=\theta_k - \beta x_{ij} - g_j$$ Takeaways: We are modeling a chain of logistic regressions where the binary response is: "less than or equal to a certain level" vs "greater than that level". the $\theta$ parameter depends only on the ordinal response. It's like an intercept term, although there is one of these for each level. $\beta$ is the slope parameter for the fixed effect $x$ , and it does not depend on $k$ . The impact of $x$ applies proportionally to all levels of the response. $g_j$ is an unseen, grouping effect. clmm will estimate the value of $g_j$ and the variance parameter of its (assumed) normal distribution. The minus signs are a convention of clm and clmm. In interpreting the parameters, negative values imply a greater probability for low ranking responses and positive values imply that the odds favour higher levels of the response. I do not believe that this convention is universal. SPSS, I'm looking at you. In this simple model, the random effect increases or decreases the odds ratio by a factor of $e^{-g_j}$ . This factor is independent of the level itself. Say, if $e^{-g_j}=0.5$ in some group, then all odds ratios are cut by a half for members of that group. This assumption is the weak link of the proportional odds model. You could include random slopes as well, if you believe there is an interaction between the group and covariate $x$ . You can extract the random effects that the model estimates using method ranef , as in ranef(clmm.complete) . Laplace approximation is a method of numerical integration. Random effects models involve an iterative procedure to estimate the random effects and the process parameters, the EM algorithm. I believe that's where the Laplace approximation comes in, since during the estimation phase, you have to integrate over the unknown parameters. However, I don't know the details of the algorithm employed by clmm. Perhaps someone else can give a better answer. Be aware that clmm requires an optimization to produce estimates. As part of the summary function, you get some information about whether or not the optimization succeeded. The gradient should be near zero and the Hessian index should be less than 10000. Regarding the specifics of your model, I don't quite understand the difference between variable SV and variable SVid. In any case, adding (1|SVid) basically changes the odds ratio for each group indexed by SVid. The model has to estimate one parameter for each SV group, a variance and possibly some covariance terms with other parameter estimates. Your description of the study suggests a lot of groups and not much data in each, so that could lead to problems with instability and convergence of the model. I'm not sure what you mean by "grouping errors around individuals".
