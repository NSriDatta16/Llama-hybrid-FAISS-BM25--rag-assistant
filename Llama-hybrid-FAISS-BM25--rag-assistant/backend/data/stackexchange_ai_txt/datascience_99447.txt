[site]: datascience
[post_id]: 99447
[parent_id]: 
[tags]: 
Why do we fine-tune language models and not just include the data in the pre-training datasets?

One question about the pre-training & fine-tuning process for language models: why is it better to fine-tune using a small dataset rather than including the fine-tuning dataset into the pre-training dataset? Or am I misunderstanding and normally the fine-tuning dataset is already included in the pre-training one, and we only change the learning parameters to better fit the data properties? Any paper reference is very welcome! Thank you.
