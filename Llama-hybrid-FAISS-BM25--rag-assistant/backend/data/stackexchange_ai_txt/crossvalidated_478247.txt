[site]: crossvalidated
[post_id]: 478247
[parent_id]: 478100
[tags]: 
Whether 1. or 2. happen depends on the specifics of the priors $p(v_j)$ and $p(\sigma)$ . In general, the Bayesian approach "shrinks" the specified likelihoods towards the priors. If the priors are "loose" (i.e. they assign non-negligible probability to a wide range of values), then the shrinkage is low and the likelihood dominates the posterior. If your likelihoods are also "loose", then the algorithms used to obtain the posterior through simulation (such as MCMC, HMC, etc) have a hard time. In your case, 2. happens. If on the other hand the priors are more specific, then the posterior is a mix between the likelihood and the prior. In that case, 1. happens. To be precise, both 1. and 2. will happen, to the extent that the priors are more or less specific. The tighter they are, the more pronounced the effect of 1. on the posterior. Otherwise, the effect of 2. will dominate the posterior. Response to comments It seems as though you have little in the way of prior information about the free variables in your model (i.e. $\sigma _v$ and $\sigma$ ). Possible solutions are: Refactor your model so it depends on free variables about which you have a better intuition or more consistent prior information. For instance, refactor your model so the free variable is a general mean response time; in that case, you can set your priors based on a meta analysis, for instance; or Run a sensibility analysis to understand how much the posterior is influenced by the center of the probability mass on your priors. For instance, you can set up three cenarios: $\sigma _v$ centered around $10 ^{-3}$ $\sigma _v$ centered around $10 ^{-2}$ $\sigma _v$ centered aroung $1$ and see if they affect the posteriors too much.
