[site]: crossvalidated
[post_id]: 531656
[parent_id]: 531652
[tags]: 
This is a topic that comes up a lot in randomized clinical trials. As a result, there's an extensive literature on covariate adaptive randomization in that area (see e.g. this review ). However, how you randomize affects how you should analyze ("Analyze as you randomize."), see e.g. this paper and this one , which especially matters if you do anything particularly complicated. E.g. I'm not, at all, sure how to handle your randomization scheme in the analysis. Some simple schemes that make the analysis straightforward: Randomize by strata with separate strata for those without a previous event and those with a previous event. This ensure those two groups will be exactly balanced within the base and target arms, the main question is whether you'd still be concerned about possible imbalances amongst those with previous events (for some possible strategies to address see further down). Randomize each record sequentially in such a way that it is more likely that balance will be improved. Example: There's too many zeros in the base arm and too few in the target arm, another zero record comes up, we assign it to the target arm with probability 0.75 and to the base arm with probability 0.25 (and the other way around if the imbalance is the other way, 1:1 if there's no imbalance). Or any other scheme that's similar and you just go through the records one after another trying to balance each time. Under that scheme, you record the probability each item had to be assigned to each arm and then adjust for that in the analysis. If you just have a few possible assignment options (e.g. 0.25:0.75, 0.5:0.5, 0.75:0.25), then we can just do the analysis stratified by assignment stratum. Note that this approach assumes that your observations are independent, if that's not the case (e.g. with infectious diseases intervening on one household member, for example with a vaccine, might substantially affect the infection probability of other household members), things become a lot more complicated. One easy way to avoid complications could be to randomize at a level where indepence exists again (e.g. in the infectious disease example at the household or community level). Importantly, you never want there to be too high a probability to be assigned to one arm or the other (otherwise as the probability for one arm goes towards 1.0, the record no longer contributes information for the comparison of interest). The other question is whether you can cope with a certain level of imbalance. How much does this really affect your analysis, especially if you can use the pre-intervention levels as a covariate in your analysis model (usually an obvious thing to do)? Are there transformations (e.g. with some variables like the albumin-creatinine ratio in patients, because regression residuals are really not normal so that you'd probably want to log-transform to not have one or two really extreme values massively drag the arithemtic mean upwards by chance) or models (e.g. for count data a negative binomial or zero-inflated negative binomial model with adjustment for the logarithm of 0.5 + the number of previous events) that are less affected by extreme values? You might want to do these things, even if you balance covariates via your randomization (but check the literature whether this impacts how you do it, e.g. I'm not 100% sure whether you should have different covariate coefficients per stratum). Also note, that in a randomized experiment the goal of the randomization is not per-se to balance the groups at baseline (see e.g. the extensive discussion here with links), but to ensure you understand the randomness process. That does not mean you'd not want to increase the efficiency of your analysis by e.g. adjusting for known prognostic variables in your analysis (or perhaps balancing covariates also helps).
