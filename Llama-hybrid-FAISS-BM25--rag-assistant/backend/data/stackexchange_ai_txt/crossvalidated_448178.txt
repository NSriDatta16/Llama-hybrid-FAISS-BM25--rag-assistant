[site]: crossvalidated
[post_id]: 448178
[parent_id]: 
[tags]: 
Finding the relative 'importance' of different vector components when defining distance of two vectors in a space

I have a multi-dimensional space with $d$ dimensions wherein $i$ vectors ( $v_1 ... v_i$ ) with $d$ components live. I want to find a function $s(v_a, v_b)$ which takes in two of these vectors and outputs a value representing the similarity of two vectors. Since the vectors live in a space, this function should be a function measuring the distance between these vectors. A straightforward answer would be to calculate the Euclidean distance between the two vectors: $s(v_a, v_b) = || v_a - v_b||$ . This is perfectly fine for my problem, however, every component (or dimension in the space) should be weighted differently when calculating the distance between the two vectors) . So, in other words: since every component of the vector conveys a real-life characteristic (for example: temperature in degrees centigrade, distance in meters, whatever), each of these components (or characteristics of the event described by the vector) has a different degree of importance or salience when compared with another situation. So, the final formula could become a dot product: $s(v_a, v_b) = w \cdot | v_a - v_b|$ ; where $w$ is a vector of $d$ components containing the weights for every dimension/component/feature/characteristic. I guess this will become something like a Mahalanobis distance (?) My big question is: how to can I learn these weights? Now, I came up with an answer myself, but I am not quite sure if I am thinking in the right direction. My approach would be as follows: I can create some kind of outcome value for every of the millions of vectors I have - there will be a couple hundred different 'outcome values'. These 'outcome values' can be defined as just being a class I assign groups of vectors to. Let's say I have $m$ different outcomes. For every group of vectors with the same outcome I know that all vectors in that group, when compared with each other (so, with members of the same group of vectors), the similarity function $s(v_a, v_b)$ should return the maximum value , since I assume them (for now) to be situations with identical outcomes. Now, for every dimension $d$ I will calculate the mean (just the simple average) of all values for $d$ in the vectors in that group. I will then end up with $m$ mean values for every $d$ . For every $d$ , I can perform MLE to fit a Gaussian (or another suitable distribution) to the means, and find a value for the variance. These variance values could be the components of my $w$ , since: the more the means among different groups of different situations differ from each other, the more useful this dimension/component is to distinct vectors from each other (or in other words: the more this dimension matters when comparing two vectors with each other). To make things even more complicated (please, if someone has anything useful to teach me on my main question, don't hesitate...): it could be that the $w_d$ -weight values for two certain dimensions are very low, or in other words: these features/characteristics do not really matter , but, if these two dimensions have certain types of values (I mean values for the actual components of the vectors itself, not the weights, we in this case already know the weight values are low) (for example: a low value and a high value, two low values, or two high values - this depends on the pair), their combined weight should become super large , so, certain values appearing for two dimensions with low weights simultaneously means that these dimensions are of very high importance when comparing two vectors. My thoughts on this: I could define a matrix, and for every pair of dimensions $d$ I can define a correlation, and these correlations should again be weighted, similar like I mentioned earlier.
