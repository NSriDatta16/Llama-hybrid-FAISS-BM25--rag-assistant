[site]: crossvalidated
[post_id]: 580388
[parent_id]: 
[tags]: 
What is the derivative of a matrix with regard to a vector defined?

I had this question when I read equation (C.20) in Appendix C of "Pattern Recognition and Machine Learning" written by Christopher M. Bishop. Here I copy the equation below for reference: Capital bold $\mathbf A$ and $\mathbf B$ are matrices; so is their product. Lower case bold $\mathbf x$ is a column vector, according to the convention in this book. But, what is the derivative of a matrix with regard to a vector defined, like $\frac{\partial\mathbf A}{\partial\mathbf x}$ ? It seems that $\frac{\partial\mathbf A}{\partial\mathbf x}$ is still a matrix of the same size as $\mathbf A$ . Otherwise, the subsequent matrix multiplication with $\mathbf B$ can't be conducted. But what are the elements of $\frac{\partial\mathbf A}{\partial\mathbf x}$ ? I have googled and read wiki page like this , but that page does not cover such a derivative type. If you happened to read this book before, can you please let me know the definition of the derivative of a matrix with regard to a vector in this equation? Thanks a lot.
