[site]: crossvalidated
[post_id]: 525126
[parent_id]: 514013
[tags]: 
As far as I can understand, you are saying final Fully Connected Layer will consider the tokens of future words as well. Please see the image below: As we can see in the right image, scaled dot product attention happens before the multiplication by W^o. Now see the left image. Inside the scaled dot product attention, masking is used to nullify the effect of the future positions. Thus, the future tokens are already nullified before final output fully connected layer.
