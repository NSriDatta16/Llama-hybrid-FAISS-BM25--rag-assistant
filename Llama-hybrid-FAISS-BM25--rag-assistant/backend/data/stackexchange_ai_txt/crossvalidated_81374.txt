[site]: crossvalidated
[post_id]: 81374
[parent_id]: 81373
[tags]: 
It is unclear what you are referring to by dimensions, do you mean number of input dimensions or number of dimensions in feature space? Either way, it does not matter as this is all tackled by using kernels. The complexity of training a nonlinear SVM is a function of the number of instances $n$, ranging from $O(n^2)$ to $O(n^3)$ for most current solvers. The impact of the number of input dimensions depends on the kernel function that is being used but is generally small, because basically the kernel value needs to be computed once for all pairs of instances to construct the full kernel matrix. In terms of overall training time, this is almost always negligible. The impact of the number of dimensions in feature space on complexity is arguably none, because we never transform to feature space explicitly. For example, the RBF kernel maps to an infinite dimensional space at hardly any computational cost. That said, some kernel functions can be more involved to evaluate because of the feature space they induce (graph kernels come to mind).
