[site]: crossvalidated
[post_id]: 137697
[parent_id]: 136564
[tags]: 
As you state, linear models are typically simpler than non-linear models, meaning they run faster (building and predicting), are easier to interpret and explain, and usually straight-forward in error measurements. So the goal is to find out if the assumptions of a linear regression hold with your data (if you fail to support linear, then just go with non-linear). Usually you would repeat your single-variable plot with all variables individually, holding all other variables constant. Perhaps more importantly, though, you want to know if you can apply some sort of transformation, variable interaction, or dummy variable to move your data to linear space. If you are able to validate the assumptions, or if you know your data well enough to apply well-motivated or otherwise intelligently informed transformations or modifications, then you want to proceed with that transform and use linear regression. Once you have the residuals, you can plot them versus predicted values or independent variables to further decide if you need to move on to non-linear methods. There is an excellent breakdown of the assumptions of linear regression here at Duke . The four main assumptions are listed, and each one is broken down into the effects on the model, how to diagnose it in the data, and potential ways to "fix" (i.e. transform or add to) the data to make the assumption hold. Here is a small excerpt from the top summarizing the four assumptions addressed, but you should go there and read the breakdowns. There are four principal assumptions which justify the use of linear regression models for purposes of inference or prediction: (i) linearity and additivity of the relationship between dependent and independent variables: (a) The expected value of dependent variable is a straight-line function of each independent variable, holding the others fixed. (b) The slope of that line does not depend on the values of the other variables. (c) The effects of different independent variables on the expected value of the dependent variable are additive. (ii) statistical independence of the errors (in particular, no correlation between >consecutive errors in the case of time series data) (iii) homoscedasticity (constant variance) of the errors (a) versus time (in the case of time series data) (b) versus the predictions (c) versus any independent variable (iv) normality of the error distribution.
