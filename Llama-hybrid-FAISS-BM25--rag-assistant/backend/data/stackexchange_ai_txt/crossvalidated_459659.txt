[site]: crossvalidated
[post_id]: 459659
[parent_id]: 459607
[tags]: 
The raw moments $\mathbb E[X^k]$ all admit unbiased estimators based on an iid sample of size $n$ from the distribution behind the expectation, as for instance $$\frac{1}{n} \sum_{i=1}^n X_i^k$$ As demonstrated in the final page reproduced below Paul Halmos wrote a now famous paper in 1946, called the theory of unbiased estimation , where he gives necessary and sufficient conditions for the existence of unbiased estimators of some expectation based on an iid sample of size $n$ from the distribution behind the expectation. In particular, he studies the existence of unbiased estimators of the $k$ -th centred or central moments $$\mu_k=\mathbb E[(X-\mathbb E\{X\})^k]$$ for which he shows that they only exist when $k\le n$ that they can only be expressed as a rescaling of the empirical moment $$\hat\mu_k^n = \frac{1}{n}\sum_{i=1}^n (X_i-\bar{X}_n)^k= \frac{1}{n}\sum_{i=1}^n (X_i-\hat\mu_1^n)^k$$ when $k\le 3$ . For larger values of $k\le n$ , the unbiased estimator of $\mu_k$ also depends on $\mu_\ell^n$ for $1\le\ell\le k-1$ . Note however that $\hat\mu_k^n$ is a converging estimator of $\mu_k$ (as $n$ grows to infinity). Let me also point out that it is always possible to turn biased estimators into unbiased estimators if sequential sampling is available. Using coupling and stopping time and a telescoping sum argument, as demonstrated by Glynn and Rhee ( Exact estimation for Markov chain equilibrium expectations . Journal of Applied Probability , 51(A):377â€“389, 2014.)
