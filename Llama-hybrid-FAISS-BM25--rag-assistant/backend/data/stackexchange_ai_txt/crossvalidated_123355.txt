[site]: crossvalidated
[post_id]: 123355
[parent_id]: 123080
[tags]: 
What's the distinction between parallelizing the feature vector and parallelizing the neurons? I assume that the neurons of the input layer are features of a training instance. Suppose the feature vector of an instance is a 1 by n1 matrix, the weights between the input layer and the first hidden layer is a n1 by n2 matrix, and there are m training instances. Then a feed-forward pass from the input to the hidden layer is just a matrix multiplication, the multiplication of the m by n1 matrix and the n1 by n2 matrix. The training process is made up of many matrix multiplications, so the problem is essentially about parallelizing matrix multiplication. In machine learning, researchers more often parallelize training examples instead of parallelize features. There are three types of learning (1) batch learning (2) mini-batch learning (3) stochastic learning, and mini-batch algorithms are often considered for parallelization speedup. The parallelization can be seen as decomposing (m by n1) * (n1 by n2) to many (b by n1) * (n1 by n2) matrix multiplications where b is the batch size. In comparison, parallelizing features is less conceptually sensible. Particularly, for deep neural network learning, the features are considered interconnected with each other, and the connections are what the algorithms try to learn. When performing unsupervised pre-training, if features are paralleled, then different parts of features are separated and their connections cannot be learned.
