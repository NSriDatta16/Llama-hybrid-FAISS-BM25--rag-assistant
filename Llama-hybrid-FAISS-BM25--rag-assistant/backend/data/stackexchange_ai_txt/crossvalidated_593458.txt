[site]: crossvalidated
[post_id]: 593458
[parent_id]: 
[tags]: 
Techniques for strategically crafting a ML dataset

For a supervised machine learning application where the input features can be readily calculated and the corresponding labels are the result of a somewhat time-consuming simulation using the inputs, it is advantageous to "craft" the best dataset with the fewest data. We have several ideas to this end, a few are: Use unsupervised learning techniques to build an initial set of inputs that maximally spans the feature space. After an iteration of training, generate new inputs and add to the training set if the trained model is uncertain of the input class (predicted probability near 0.5 for a binary system). Add new inputs to the training set based on whether they are similar to previous inputs that were misclassified (using unsupervised learning) I am wondering if any of these are valid approaches and more broadly if there is a name for techniques likes this where a ML dataset is strategically crafted and improved in an iterative manner? This is different than most data augmentation techniques I'm familiar with because rather than processing existing inputs (e.g. rotating, translating images) in this application it's trivial to generate entirely new inputs.
