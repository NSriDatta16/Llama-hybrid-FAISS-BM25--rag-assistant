[site]: crossvalidated
[post_id]: 8660
[parent_id]: 8502
[tags]: 
This is "taylor made" almost for a Bayesian regression. First of all, there is nothing "fundamentally wrong" with what you suggest. You result may not be optimal by some mathematical standard, but it will almost certainly be optimal time wise. Most other methods will involve much more time than a straight multiplication and division. I would use a normal likelihood $(y_i|\beta,\sigma,x_i,I)\sim N(x_i^T\beta,\sigma^2)$ and the "jeffreys prior" $p(\beta,\sigma|x_i,I) \propto \frac{1}{\sigma}$. This gives a posterior for $\beta$ as a multivariate t-distribution, with scale matrix $s^2(X^TX)^{-1}$ and mean vector $\beta_{ols}$ with the standard $n-p$ degrees of freedom. Now you simply use this posterior based on the "A" data set as the prior for the "B" data set. Now because you have a "t" prior and a "normal" likelihood, the posterior for beta will favour the normal likelihood, because the t has fatter tails - hence less "pulling power". This regression will balance the A and B regression between how accurately A was estimated, and how well the B estimate fits the data. An "add-hoc" way that you could add more weight to "B" is by setting the degrees of freedom to 1 in the "A" posterior. But then you may as well save some time and do the multiply the B estimate by two. I don't think there is a simple analytic expression for this posterior, so will likely need to simulate. But you only require the estimate from the "A" data set, and the covariance matrix from the "A" data set, and the number of observations in the "A" data set. Once you have these quantities, you don't require the original data set.
