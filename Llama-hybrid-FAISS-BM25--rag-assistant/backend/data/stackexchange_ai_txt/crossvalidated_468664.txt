[site]: crossvalidated
[post_id]: 468664
[parent_id]: 1432
[tags]: 
Pearson Residuals As tosonb1 points out, "The Pearson residual is the difference between the observed and estimated probabilities divided by the binomial standard deviation of the estimated probability". I just wanted to mention that Pearson residual is mostly useful with grouped data i.e, say, there are $n_i$ trials at setting i of the explanatory variables (many observations for the same value of predictors) and let $y_i$ denote the number of “successes” for $n_i$ trials. Also, let $\hat{π_i} $ denote the estimated probability of success for the logistic regression model we have fit. Pearson residual = $e_i = \frac{y_i - \hat{π_i}}{\sqrt {n_i \hat{π_i}(1 - \hat{π_i})}}$ For ungrouped binary data and often when explanatory variables are continuous, each $n_i$ = 1. Then, $yi$ can equal only 0 or 1, and a residual can assume only two values and is usually uninformative. Plots of residuals also then have limited use, consisting merely of two parallel lines of dots. From, An Introduction to Categorical Data Analysis, 2nd Edition by Alan Agresti - vide chapter 5, section 5.2.4 Deviance Residuals (I am not entirely sure about this one, please point out errors, if any) The i-th deviance residual can be computed as square root of twice the difference between loglikelihood of the ith observation in the saturated model and loglikelihood of the ith observation in the fitted model . Saturated Model is the model that predicts each observation perfectly and for all purposes in logistic regression, loglikelihood of saturated model $= ln(1) = 0$ . Finally, we add a sign '+' in front of the residual if the observed response is 1 and put '-' if the observed response is 0. Hence, deviance residual for the ith observation, $$d_i = (-1)^{y_i + 1}\sqrt{-2 (y_i ln(\hat{π_i}) + (1-y_i) ln(1 - \hat{π_i}))}$$ $y_i \in $ { 0,1 } The sum of squares of deviance residuals add up to the residual deviance which is an indicator of model fit. If a deviance residual is unusually large (which can be identified after plotting them) you might want to check if there was a mistake in labelling that data point.
