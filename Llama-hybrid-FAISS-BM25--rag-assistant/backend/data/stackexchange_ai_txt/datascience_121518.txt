[site]: datascience
[post_id]: 121518
[parent_id]: 121369
[tags]: 
There are a few reasons why training a machine learning model on aggregated data might be different or harder than training on raw event data: Loss of Information: Aggregated data loses some of the information that is present in the raw event data. For example, aggregating clicks and displays by category will give you the click-through rate for each category, but you lose information about the individual clicks and displays that went into that calculation. This loss of information can make it harder for the model to learn patterns and correlations in the data. Bias: Aggregating data can introduce bias into the dataset. For example, if you aggregate clicks and displays by category, you may have categories that are overrepresented or underrepresented in the aggregated data. This can lead to bias in the model's predictions. Non-linear Relationships: In some cases, the relationship between the features and the target variable may be non-linear. Aggregating the data can make it harder for the model to learn these non-linear relationships. Regarding your second question, a machine learning model is able to learn more than just the historical probabilities of a click. Even if you have all possible feature crosses in your dataset, a complex DL model is able to learn more sophisticated relationships between the features and the target variable. For example, a deep neural network can learn hierarchical representations of the data that capture increasingly abstract features. This can lead to better performance on the task at hand. Additionally, a model can learn to generalize to new data that it has not seen before, whereas simply aggregating historical data would not allow for this kind of generalization.
