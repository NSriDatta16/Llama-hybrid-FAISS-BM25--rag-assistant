[site]: crossvalidated
[post_id]: 488606
[parent_id]: 487593
[tags]: 
Ordinary least squares (OLS) minimizes the residual sum of squares (RSS) $$ RSS=\sum_{i}\left( \varepsilon _{i}\right) ^{2}=\varepsilon ^{\prime }\varepsilon =\sum_{i}\left( y_{i}-\hat{y}_{i}\right) ^{2} $$ The mean squared deviation (in the version you are using it) equals $$ MSE=\frac{RSS}{n} $$ where $n$ is the number of observations. Since $n$ is a constant, minimizing the RSS is equivalent to minimizing the MSE. It is for this reason, that the Ridge-MSE cannot be smaller than the OLS-MSE. Ridge minimizes the RSS as well but under a constraint and as long $\lambda >0$ , this constraint is binding. The answers of gunes and develarist already point in this direction. As gunes said, your version of the MSE is the in-sample MSE. When we calculate the mean squared error of a Ridge regression, we usually mean a different MSE. We are typically interested in how well the Ridge estimator allows us to predict out-of-sample . It is here, where Ridge may for certain values of $\lambda $ outperform OLS. We usually do not have out-of-sample observations so we split our sample into two parts. Training sample, which we use to estimate the coefficients, say $\hat{\beta}^{Training}$ Test sample, which we use to assess our prediction $\hat{y}% _{i}^{Test}=X_{i}^{Test}\hat{\beta}^{Training}$ The test sample plays the role of the out-of-sample observations. The test-MSE is then given by $$ MSE_{Test}=\sum_{i}\left( y_{i}^{Test}-\hat{y}_{i}^{Test}\right) ^{2} $$ Your example is rather small, but it is still possible to illustrate the procedure. % Generate Data. X = [3, 3 1.1 1 -2.1 -2 -2 -2]; y = [1 1 -1 -1]'; % Specify the size of the penalty factor lambda = 4; % Initialize MSE_Test_OLS_vector = zeros(1,m); MSE_Test_Ridge_vector = zeros(1,m); % Looping over the m obserations for i = 1:m % Generate the training sample X1 = X; X1(i,:) = []; y1 = y; y1(i,:) = []; % Generate the test sample x0 = X(i,:); y0 = y(i); % The OLS and the Ridge estimators b_OLS = ((X1')*X1)^(-1)*((X1')*y1); b_Ridge = ((X1')*X1+lambda*eye(n))^(-1)*((X1')*y1); % Prediction and MSEs yhat0_OLS = x0*b_OLS; yhat0_Ridge = x0*b_Ridge; mse_ols = sum((y0-yhat0_OLS).^2); mse_ridge = sum((y0-yhat0_Ridge).^2); % Collect Results MSE_Test_OLS_vector(i) = mse_ols; MSE_Test_Ridge_vector(i) = mse_ridge; end % Mean MSEs MMSE_Test_OLS = mean(MSE_Test_OLS_vector) MMSE_Test_Ridge = mean(MSE_Test_Ridge_vector) % Median MSEs MedMSE_Test_OLS = median(MSE_Test_OLS_vector) MedMSE_Test_Ridge = median(MSE_Test_Ridge_vector) With $\lambda =4$ , for example, Ridge outperforms OLS. We find the following median MSEs: MedMSE_Test_OLS = 0.1418 MedMSE_Test_Ridge = 0.1123 . Interestingly, I could not find any value of $\lambda $ for which Ridge performs better when we use the average MSE rather than the median. This may be because the data set is rather small and single observations (outliers) may have a large bearing on the average. Maybe some others want to comment on this. The first two columns of the table above show the results of a regression of $x_{1}$ and $x_{2}$ on $y$ separately. Both coefficients positively correlate with $y$ . The large and apparently erratic sign change in column 3 is a result of the high correlation of your regressors. It is probably quite intuitive that any prediction based on the erratic OLS estimates in column 3 will not be very reliable. Column 4 shows the result of a Ridge regression with $\lambda=4$ . Important note: Your data are already centered (have a mean of zero), which allowed us to ignore the constant term. Centering is crucial here if the data do not have a mean of zero, as you do not want the shrinkage to be applied to the constant term. In addition to centering, we usually normalize the data so that they have a standard deviation of one. Normalizing the data assures that your results do not depend on the units in which your data are measured. Only if your data are in the same units, as you may assume here to keep things simple, you may ignore the normalization.
