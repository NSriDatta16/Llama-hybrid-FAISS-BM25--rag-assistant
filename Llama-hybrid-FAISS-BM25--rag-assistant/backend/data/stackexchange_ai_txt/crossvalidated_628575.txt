[site]: crossvalidated
[post_id]: 628575
[parent_id]: 
[tags]: 
Opposite results from different modelling decisions in logistic regression

I'm trying to test the significance of 2 independent variables ( b & c ) in a regression model, whose values depend on another independent variable a . To make this easier to understand, let's say we're studying 5 different frogs which may or may not live in the area, and there are 100 areas under study. a is the number of types of frogs found in a particular area. The known number of natural enemies for all of them is b , and the number of available sources of food is c . For example, if in an area there are 2 types of frogs ( a = 2), type1 has 2 natural enemies and type2 has 1, then b = 3; type1 has 5 food sources available and type2 has 3, then c = 8 (row 4 in the dataset below) The dataset looks like something like this: a b c 0 0 0 1 2 5 2 3 8 0 0 0 1 5 3 0 0 0 0 0 0 4 17 9 0 0 0 It is worth mentioning that a = 0 in almost half of the data collected and is heavily right skewed. I've fitted three regression models to study the correlation between a , b , c and the presence versus absence of mosquitoes (y) in an area, which is a binary dependent variable. The parameters and results of each logistic regression model are listed below. regression model #1: because a is extremely right skewed, it's binarised as presence vs absence of frogs; b and c are averaged (when a,b&c =/= 0) because the information about number of frogs is (more or less) given in a . The regression model is something like this: y = β1 a (binarised) + β2 b / a + β3 c / a . Results: binarised a as well as b are highly significant ( p -value c is borderline significant (p-value = 0.059). regression model #2: b and c are put into the regression model as they are, i.e., sums instead of averages, while a is omitted because the number information is present in b and c . The regression model is something like this: y = γ1 b + γ2 c . Results: b is not significant at all ( p -value = 0.9) and c is highly significant, which is opposite to the results from model #1. regression model #3: in order to study the interactions between a and averaged b and between a and averaged c , the model is fitted like this: y = δ1 a + δ2 b / a + δ3 c / a + δ4 a : b / a + δ5 a : c / a . Results: a is highly significant, neither averaged b or c is significant ( p -value >0.1); the interaction between a and averaged b is significant ( p -value = 0.03) and the interaction between a and averaged c is borderline significant ( p -value = 0.068). I'm extremely confused by the results, especially the difference between model #1 and model #2, because I thought they were just different modelling specifications that mean the same thing, but it seems they are radically different from each other and I don't know which one to trust.
