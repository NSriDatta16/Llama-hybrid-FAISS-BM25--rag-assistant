[site]: crossvalidated
[post_id]: 71455
[parent_id]: 
[tags]: 
Measuring the smoothness of time series

I work on a method that gives a (noisy) estimation of brain volume over time in Alzheimer's patients. As we know that the evolution is smooth and even mostly linear if looked at over a time frame of a few years, one way of evaluating the algorithm is to look at the smoothness or linearity of the estimated time series (brain volume over time). This is why I'm looking for a good metric of smoothness or linearity of time series. What I've been using for now is the average RÂ² from patient-wise linear regressions. This does reflect linearity, but it depends heavily on the sampling (it will give better results with fewer data points). Could anyone suggest a better metric? Some details since apparently my question wasn't clear . I have an algorithm that gives a few (from one to five or six) discrete $V(t_n)$ brain volume measurements for many patients. I want to measure to which degree $V$ is close to something linear, eg. $V(t_n) = a t_n + b$ accross all patients (all patients will have different coefficients, but I want an overall measure of linearity). I should also mention that the $t_n$ values are not regularly spaced. One more comment Obviously patients with one or two data points are not informative. However I want to take advantage of all the other patients. Even a patient with three data points is informative: three aligned points are much better than three completely jagged points.
