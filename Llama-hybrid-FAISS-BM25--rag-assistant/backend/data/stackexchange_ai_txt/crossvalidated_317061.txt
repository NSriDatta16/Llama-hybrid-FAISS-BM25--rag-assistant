[site]: crossvalidated
[post_id]: 317061
[parent_id]: 317056
[tags]: 
That quote is basically a tautological statement if you assume that the network converges to its true parameter values. A neural network is defined as $$ y = a(a(a(...a(a(X\Gamma_L)\Gamma_{L-1}...)\Gamma_3)\Gamma_2)\Gamma_1 + \epsilon $$ Define $$ V \equiv a(f(\Gamma_{2:L}, X)) $$ and $$ \hat{y} = V\Gamma_1 $$ Those $V$ are derived features, related linearly to the outcome by weights $\Gamma$. They are "good" features if $E[(y-\hat{y})^2]$ is small. And this tends to be the case, because neural nets typically perform quite well once tuned adequately. The statement is tautological because deriving features is precisely what a neural network does. A neural net is basically a regressor-picker for OLS (or for penalized regression, or a penalized GLM). (Obviously this is for the squared error loss, but the generalization to classification is simple).
