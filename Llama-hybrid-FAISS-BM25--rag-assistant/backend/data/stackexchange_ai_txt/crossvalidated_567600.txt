[site]: crossvalidated
[post_id]: 567600
[parent_id]: 
[tags]: 
How do CNN filters learn from back-propagation?

I have some intermediate knowledge of Image-Classification using convolutional neural networks. I'm pretty aware to concepts like 'gradient descent, 'derivatives', 'back-propagation & 'weight update process'. I know that filters are randomly initialized and during training they are learned. But I really can't digest the concept of derivation for filter For example, there is a simple dense layer, then increasing or decreasing single tensor's weight results in change to cost function. But for filters, how can you decide that increasing or decreasing the value of filter would give better or worst result? Because there is no mathematical relationship like in weight and biases. Filters are just transformers of image. One way to solve this, we have to change a small amount in single filter and run the whole image dataset to check the cost function. But it gives just a single filter's single value gradient. I'm pretty sure this is not a way to do it. I researched this question for more than a month and wherever I go, it states that 'Filters are learned from back propagation' but no one explains the maths behind it. There are plenty of videos that explain backpropagation for dense layer but "no video I found which state filter learning"
