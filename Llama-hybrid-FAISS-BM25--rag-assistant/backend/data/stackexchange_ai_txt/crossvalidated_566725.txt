[site]: crossvalidated
[post_id]: 566725
[parent_id]: 566723
[tags]: 
It seems you're averaging the target values for each repeat to get a single one. It's not wrong, and I'm not sure what caret does if it does anything at all, but calculating the ROC values for each repeat and averaging the AUCs would make more sense. Especially, in multi-class scenarios. You may need to select repeatedcv since you have repeats != 1 . I'm not what the folllowing line does as my R knowledge is limited, but it seems you're again creating a single prediction out of repeated ones. I'm not sure if it's the average of repeats, but this aligns with your first implementation. caret_predicted_prob % arrange(rowIndex) %>% pull(pos) The final test is used for final evaluation. A separate validation set, or cross-validation is used to tune the hyperparameters (HP). That's what the authors are doing in section 5.5.2. You don't search for best set of HPs above, so there is no need to separate into train and test. You're already estimating the generalization performance using repeated cross validation.
