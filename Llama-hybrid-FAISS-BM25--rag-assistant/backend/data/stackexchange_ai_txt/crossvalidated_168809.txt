[site]: crossvalidated
[post_id]: 168809
[parent_id]: 
[tags]: 
Error metric for cross-validation on interval-censored data?

I want to compare crossvalidated model fit (of two Bayesian models, one using a normal distribution and the other a t -distribution) on interval-censored data - data where the exact point is not known, just a wide interval it must fall into. In this case, it's temporal data, so the intervals are 'before' or 'after' a check-time. Since I don't know the exact datapoint, I can't compute the usual mean-squared error, and it's not too obvious to me what kind of error I can compute. Thinking about it, I think one can define a zero-one loss based on whether the model's estimate of before/after matches the heldout datapoint's before/after intervals. This seems to make sense but I'm not sure it's right or that I coded up an implementation correctly. Thoughts? In this case, the interval-censored data is collected as a specific check-time and then a boolean. The data is whether the mail was delivered when I checked each morning over the past two months, and so we're modeling my local postal service. The raw data looks like this: Date Delivered Time 1 2015-06-20 11:00:00 FALSE 660 2 2015-06-21 11:06:00 FALSE 666 3 2015-06-23 11:03:00 TRUE 663 4 2015-06-24 11:05:00 TRUE 665 ... Most survival libraries or other statistical libraries require interval-censored to be in interval format. In this case, we define the low/high endpoints as 0 or 1440 (since the day begins/ends at midnight) and work in minutes-since-midnight, yielding data that looks like this: Time1 Time2 1 660 1440 2 666 1440 3 0 663 4 0 665 ... (If the mail was not delivered at 11AM/660-minutes-since-midnight, then it must have been delivered sometime in the 660-1440 interval; while if it was delivered at 11:03AM/663, then it must have been delivered sometime in the 0-663 interval.) Full data & formatting code: set.seed(2015-06-24) library(lubridate) fromClock To model the distribution of delivery times, I can fit two models like the following in BUGS/JAGS, to the normal and t -distributions (with informative priors, where they come from is not relevant): library(R2jags) modelN I want to compare them. (You might say use DIC, but R2jags doesn't provide that for interval-censored data). Since this is small data, leave-one-out crossvalidation seems like a good idea. But what is the rror? Mean-squared-error is out due to the intervaling, as are variants like absolute error. Thinking about it some, it seems to me that each datapoint is really made of two things: the check-time, and delivery status. The check-time has nothing to do with the model quality and the model isn't trying to predict it; what I want the model to predict is whether the mail is delivered or not if I were to check at particular times. The check-time is the predictor variable and the delivery-status is the response variable. So I could ask the model's posterior predictive distribution of delivery-times ( y.new ) whether or not the mail would or would not be delivered at a particular time, and compare it against the held-out datapoint's actual delivery status, 1 if the model correctly predicts delivered/not-delivered and 0 if it predicts the opposite of what the data said. (So a zero-one loss.) Here's a try at implementing leave-one-out crossvalidation for those two JAGS models on the interval-censored mail data with a zero-one loss defined that way based on delivery status: loocvs checkTime && !there) { 1 } else { 0 } }})) } return(results) } loocvsN As expected they both seem to fit the data adequately and there is no particular reason to not use the simpler normal distribution here.
