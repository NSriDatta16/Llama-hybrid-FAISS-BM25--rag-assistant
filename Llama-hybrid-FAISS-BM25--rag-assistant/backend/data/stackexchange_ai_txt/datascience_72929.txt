[site]: datascience
[post_id]: 72929
[parent_id]: 
[tags]: 
Displaying network error as a single value

I've been writing a neural network from scratch. I've completed the feedforward, backpropagation, and mini-batch gradient descent methods, so I can train the network. Other neural networks I've worked with usually display the error/loss after each batch as a single decimal value, and I'd like to implement this functionality but I'm not sure how. I understand squared error is given by $(y - \hat{y})^2$ , and that for an output layer with $m$ neurons, you should have an error vector of size $m$ . However, how is the error vector displayed as one value?
