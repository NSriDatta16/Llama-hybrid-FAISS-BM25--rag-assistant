[site]: datascience
[post_id]: 106410
[parent_id]: 106388
[tags]: 
In general, yes: decreasing the number of classes mechanically increases the probability that the classifier finds the right one. Even in the worst case scenario where the class is picked randomly, the probability of every remaining class increases when there's one less class. Another way to look at it: all other things equal, the number of errors can only decrease when two classes are merged. Whether it's advisable is a different matter. It's completely artificial: the performance may be higher but only because the problem is made easier. So it might look better on paper, but it's not in any way a real improvement. Additionally it would rarely cause a large performance increase unless a large number of classes are removed. Normally the decision should be made only because it makes sense for the task. Finally I think that in this case this is not the right question: this task should probably be designed as a regression problem, not a classification one. The target ranges are ordinal, so a regression model should be able to make a better use of the information with a continuous target variable. For example each range could be represented as the mean: 5 for 1-10, 15 for 10-20, etc. If needed the predicted value can be mapped back to a range at the end of the process. Also my usual advice is to start with a simple model first, for instance decision tree or SVM regression (SVR).
