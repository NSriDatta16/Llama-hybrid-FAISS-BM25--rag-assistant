[site]: crossvalidated
[post_id]: 126970
[parent_id]: 126966
[tags]: 
The dependency structure of a HMM is described by this graph . Absence of an arrow connecting two random variables means that they are conditionally independent given the values of their parents. Using the law of total probability and the product rule you have $$ p(x_{n+1}\mid y_0,\dots,y_n) = \int p(x_{n+1},x_n\mid y_0,\dots,y_n)\,dx_n $$ $$ = \int p(x_{n+1}\mid x_n,y_0,\dots,y_n)\,p(x_n\mid y_0,\dots,y_n)\,dx_n \, . $$ Inspecting the graph you will find that if you are given the value of $x_n$, then $x_{n+1}$ and $y_0,\dots,y_n$ are conditionally independent. Hence, $p(x_{n+1}\mid x_n,y_0,\dots,y_n)=p(x_{n+1}\mid x_n)$ and you have the first desired equation. From the definition of conditional probability / density and the product rule, you have $$ p(x_{n+1}\mid y_0,\dots,y_{n+1}) \propto p(x_{n+1},y_0,\dots,y_{n+1}) $$ $$ \propto p(y_{n+1}\mid x_{n+1},y_0,\dots,y_n)\,p(x_{n+1}\mid y_0,\dots,y_n) \, , $$ in which $\propto$ means proportionality up to terms that do not depend on $x_{n+1}$. Inspecting the graph you will find that if you are given the value of $x_{n+1}$, then $y_{n+1}$ and $y_0,\dots,y_n$ are conditionally independent. Hence, $p(y_{n+1}\mid x_{n+1},y_0,\dots,y_n)=p(y_{n+1}\mid x_{n+1})$ and you get the desired result. The normalization constant is $$ Z=\int p(y_{n+1}\mid x_{n+1}) \, p(x_{n+1}\mid y_0,\dots,y_n)\,dx_{n+1} \, . $$ If you find the $\propto$ reasoning above confusing, write things out explicitly (and tediously). $$ p(x_{n+1}\mid y_0,\dots,y_{n+1}) = \frac{p(x_{n+1},y_0,\dots,y_{n+1})}{p(y_0,\dots,y_{n+1})} $$ $$ = \frac{p(y_{n+1}\mid x_{n+1},y_0,\dots,y_n)\,p(x_{n+1}\mid y_0,\dots,y_n)\,p(y_0,\dots,y_n)}{p(y_0,\dots,y_{n+1})} $$ Therefore, $$ Z = \frac{p(y_0,\dots,y_{n+1})}{p(y_0,\dots,y_n)} \, . $$ But, especially if you are planning to do some Bayesian inference, get confortable with $\propto$ computations. For instance, check this one.
