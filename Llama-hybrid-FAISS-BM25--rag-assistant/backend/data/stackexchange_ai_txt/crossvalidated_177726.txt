[site]: crossvalidated
[post_id]: 177726
[parent_id]: 
[tags]: 
Do the number of contestants in a machine learning competition relate to a lack of confidence in declaring a winner due to multiple comparison errors?

Machine learning competitions should account for multiple comparison penalties on the hold out validation data used to declare the winner. As an example, if contestants used the exact same stochastic algorithm like a random forest, their would be a distribution of performance that would depend on the number of contestants. But the winner would not be truly better than the worst. Is there a way to account for this problem?
