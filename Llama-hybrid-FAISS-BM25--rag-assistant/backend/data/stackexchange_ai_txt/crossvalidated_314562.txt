[site]: crossvalidated
[post_id]: 314562
[parent_id]: 
[tags]: 
PCA: how to select eigen vectors corresponding to small eigenvalues for regression

Problem: Principal component regression for prediction. the input data has dimensionality of 500. I want to select a fixed number of components (around 10) to be used in a linear regression model. The common practice of using PCA is to select the 'major basis' that capture most of the variance. This is done by sorting the eigenvalues and select those biggest. Recently I read a few articles talking about those smaller eigenvalues. As opposite to the common belief that these smaller eigenvalues correspond to 'noise', they actually could play an important role in regression. My question is : is there any established procedure to select the best eigenvectors for regression? I mean, "best" as in out-of-sample R^2 sense. For example, my input data have 250 dimensions, I perform PCA and sort the 250 {eigenvalue eigenvector} pairs by eigenvalue. I would select the first 5 eigenvectors to capture the major variance. However, I also want to select a certain number (maybe another 5) from the rest 245 eigenvalues. This becomes tricky because there are $c_{245}^{5}$ combinations. Some people suggested selecting the smallest 5. I am not sure if this makes sense. Intuitively the 5 new eigenvalues should be some where in the middle of the list of 245 Does anybody have any insights here?
