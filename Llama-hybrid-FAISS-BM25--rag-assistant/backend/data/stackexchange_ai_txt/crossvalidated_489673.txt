[site]: crossvalidated
[post_id]: 489673
[parent_id]: 
[tags]: 
Prediction interval for regression with time series data

Suppose that ${y_{t}}$ and ${x_{t}}$ are time series variables. A simple linear regression model with autoregressive errors of order ${p}$ , say AR(P), can be written as \begin{equation} y_{t}= \beta_{0}+\beta_{1} x_{t} +w_{t}, \hspace{0.5 cm} t=1, 2, ..., n (1) \end{equation} where ${y_{t}}$ is the value of the response variable at period ${t}$ . ${\beta_{0}}$ and ${\beta_{1}}$ are parameters. Time, ${t}$ , is a known constant which is an explanatory variable. let's suppose ${w_{t}}$ to be an auto-regressive process.The process is giving as follows: \begin{equation} w_{t}=\sum _{i=1}^{p}\phi_{t}w_{t-i} + z_{t} (2) \end{equation} where ${ z_{t}}$ is called disturbance or error terms. By applying the back-shift operator to (2), the AR( $p$ ) process is given by the equation \begin{equation*} \varphi(B) w_{t}= z_{t}, \hspace{ 0.5 cm } t= 1, 2, ..., n. \end{equation*} $\varphi(B)$ is known as the characteristic polynomial of the process and its roots determine when the process is stationary or not. If we assume that an inverse operator, $\varphi^{-1}(B)$ , exists, then ${w_{t}=\varphi^{-1}(B)z_{t}}$ . So the model (1) can be written as \begin{equation} y_{t}= \beta_{0}+\beta_{1} x_{t} +\varphi^{-1}(B)z_{t}, \hspace{0.5 cm} t=1, 2, ..., n. (3) \end{equation} If we multiply all elements of the equation (3) by $\varphi(B)$ , we get \begin{equation} y^{*}_{t}= \beta^{*}_{0}+\beta_{1} t^{*} +z_{t}, \hspace{0.5 cm} t=1, 2, ..., n. (4) \end{equation} where $ y^{*}_{t} = \varphi(B)y_{t} = y_{t}-\phi_{1}y_{t-1} - ...- \phi_{p}y_{t-p}$ $ x^{*}_{t} = \varphi(B)x_{t} = x_{t}-\phi_{1}x_{t-1} - ...- \phi_{p}x_{t-p}$ $ \beta^{*}_{0} = \varphi(B)\beta_{0} = \left( 1-\phi_{1}- \phi_{2}-...-\phi_{p}\right)\beta_{0} $ Thus, model (4) is just the usual linear regression with the unobserved $\{z_{t}\}$ . We assume that the unobserved $\{z_{t}\}$ is a sequence of i.i.d random variables ${E(z_{t})=0}$ , ${E(z^{2}_{t})=\sigma^{2}}$ , with probability density function (p.d.f) $f(z)$ and cumulative distribution function (c.d.f) ${F(z)= \int_{-\infty}^{z}f(u)du}$ . The ${h}$ - step- ahead linear predictor of ${\hat{y}_{n+h|1, 2, ..., n}}$ , for ${y_{n+h}}$ , ${h \geq 1}$ based on $\{y_{t}\}_{t=1}^{n}$ is defined recursively by \begin{equation} \hat{y}^{*}_{n+h}= \hat{\beta}^{*}_{0}+\hat{\beta}_{1}x^{*}_{ n+h} \end{equation} and $\hat{z}_{n+h}=y^{*}_{n+h}-\hat{y}^{*}_{n+h}$ as the $h$ - step-ahead prediction residuals. Given ${x^{*}_{n+h}}$ , an approximate ${1-\alpha}$ prediction interval for $\hat{y}^{*}_{n+h}$ is given by \begin{equation} \hat{y}^{*}_{n+h} \pm q_{Empirical} \end{equation} where $q_{Empirical}$ is quantile function of the empirical of $(\hat{z}_{n+h})$ . It is clearly the interpretation of the prediction interval for $\hat{y}^{*}_{n+h}$ is not easy. My question: How to get a prediction interval for original variable $\hat{y}_{n+h}$ ? Note: $(\hat{z}_{n+h})$ is not normal.
