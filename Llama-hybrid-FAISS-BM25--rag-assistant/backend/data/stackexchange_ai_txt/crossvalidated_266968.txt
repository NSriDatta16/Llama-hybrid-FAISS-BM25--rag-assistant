[site]: crossvalidated
[post_id]: 266968
[parent_id]: 
[tags]: 
How does minibatch gradient descent update the weights for each example in a batch?

If we process say 10 examples in a batch, I understand we can sum the loss for each example, but how does backpropagation work in regard to updating the weights for each example? For example: Example 1 --> loss = 2 Example 2 --> loss = -2 This results in an average loss of 0 (E = 0), so how would this update each weight and converge? Is it simply by the randomization of the batches that we "hopefully" converge sooner or later? Also doesn't this only compute the gradient for the first set of weights for the last example processed?
