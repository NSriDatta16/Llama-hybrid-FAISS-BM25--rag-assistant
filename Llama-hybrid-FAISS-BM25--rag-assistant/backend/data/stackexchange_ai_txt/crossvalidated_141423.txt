[site]: crossvalidated
[post_id]: 141423
[parent_id]: 
[tags]: 
R seasonal time series

I use the decompose function in R and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality. However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality. I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong? I add here some useful details. This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year). Below is the output of the decompose function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think. However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following: Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 5144454056 372840549 13.798 Basically, all the seasonality coefficients are not statistically significant. To run linear regression I use the following function: lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov) where I set up Yvar as a time series variable with monthly frequency (frequency = 12). I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change. Estimate Std. Error t value Pr(>|t|) (Intercept) 3600646404 96286811 37.395 Hence my question is: am I doing something wrong in the regression analysis?
