[site]: crossvalidated
[post_id]: 620349
[parent_id]: 
[tags]: 
Question about Platt Scaling in sklearn's implementation of SVM

From sklearn's documentation here : The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities are calibrated using Platt scaling [9]: logistic regression on the SVMâ€™s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per [10]. I'm confused on the procedure sklearn uses. From the reference on Platt Scaling , my understanding of the procedure is: Run SVM on the entire training set as normal Run SVM with k-fold cross validation on the training set Train one logistic regression model using SVM scores for the entire training set (by combining them from each hold out set) At inference, data is first ran through the SVM model from 1 then through the logistic regression model Is the above correct? In the sklearn docs, it seems to read that cross validation is applied to the logistic regression portion?
