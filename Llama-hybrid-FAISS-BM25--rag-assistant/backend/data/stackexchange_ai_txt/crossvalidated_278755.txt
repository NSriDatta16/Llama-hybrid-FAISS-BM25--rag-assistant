[site]: crossvalidated
[post_id]: 278755
[parent_id]: 
[tags]: 
Why use gradient descent for linear regression, when a closed-form math solution is available?

I am taking the Machine Learning courses online and learnt about Gradient Descent for calculating the optimal values in the hypothesis. h(x) = B0 + B1X why we need to use Gradient Descent if we can easily find the values with the below formula? This looks straight forward and easy too. but GD needs multiple iterations to get the value. B1 = Correlation * (Std. Dev. of y/ Std. Dev. of x) B0 = Mean(Y) â€“ B1 * Mean(X) NOTE: Taken as in https://www.dezyre.com/data-science-in-r-programming-tutorial/linear-regression-tutorial I did checked on the below questions and for me it was not clear to understand. Why is gradient descent required? Why is optimisation solved with gradient descent rather than with an analytical solution? The above answers compares GD vs. using derivatives.
