[site]: crossvalidated
[post_id]: 384317
[parent_id]: 384186
[tags]: 
In my opinion, a potentially more important reason as to why random forests are doing quite poorly here is mostly due to the size of your dataset. Your dataset is comprised of a grand total of 80 observations with five features which is incredibly small for fitting a random forest. Consider how a classification tree generates its predictions, the base learner for a random forest ensemble. The final predicted probability of being in a specific class will just be the proportion of observations (in a single terminal node of the tree) that belong to this class to the total number of observations in that same terminal node. A random forest fits many of these decision trees on bootstrap samples of your data and then takes an average of all of the trees to get the final prediction. Thus, with only 80 observations your fitted probabilities of belonging to one class or the other will be based off very few observations which will more than likely lead to large variances in your predictions (i.e. severe overfitting). Thus, it is really not surprising to see that simpler models work better here because a random forest in general requires a large amount of observations to work well. This also holds true for models such as neural networks, gradient boosting, regularized greedy forests, and other very (popular) methods that many regard as "high performing" algorithms (as of 2018, anyway). In general, I have found that the less data you have, the less complex your models can be. Ensemble based methods and models that fit a large amount of parameters do not fare well on small amounts of data because there simply isn't enough information to estimate very complicated relationships within the data; which is in general why most people use these more complex algorithms in the first place.
