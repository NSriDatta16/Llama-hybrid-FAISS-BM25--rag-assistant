[site]: crossvalidated
[post_id]: 636319
[parent_id]: 636288
[tags]: 
When there is a large imbalance between cases & controls in a retrospective dataset, there will likely be a large difference in confounders, or nuisance factors - things that are not being studied. Randomized clinical trials, however, employ inclusion and exclusion criteria to ensure that similar patients are enrolled in the study. In addition, randomization of subjects to the various arms attempts to reduce confounder differences between arms, but it has nevertheless still been observed in sample sizes $n . Since propensity matching attempts to extract data from a retrospectively collected dataset for which there was no inclusion/exclusion criteria applied, and no randomization, certain subjects in the retrospective dataset will need to be dropped. Which subjects? Subjects whose confounder values (medical histories, severity of disease, disease duration, repeated recurrences, etc.) are significantly different from cases (diseased). Why? Because the goal is to reduce heterogeneity across nuisance factors among subjects with which treatment efficacy and adverse events (safety) are being studied. You cannot "be all over the map" with variation in nuisance factors across subjects/arms -- since you want a highly controlled experiment. Strong heterogeneity in confounder influences across treatment (dose) arms diminishes the chance of observing efficacy. One of the worst scenarios for confounder heterogeneity results from studying subjects that are not de novo cases, i.e., are not newly diagnosed, but rather have been diseased for several years with extensive therapeutic histories. Subjects with long duration of disease and high number of repeat occurrences will have huge amounts of "excess baggage" which can render a trial to be futile (useless). For propensity matching, I often first run a single logistic regression model with dependent variable values: y=1 if case, y=0 if control and independent (predictor) variables based on confounder or nuisance factors -- not being studied -- whose values are significantly different between cases & controls. Next, sort/get the predicted "logits" and their range for the cases, and then find an equal number of controls whose (sorted) logits are in the same range. Then run the analysis of choice, such as the logrank test or Cox PH regression on survival for selected subjects with the new binary (0,1) treatment (exposure) variable. If there's a lot of controls whose predicted logit values are in the same range of predicted logit values of cases, then I just use resampling for the e.g. logrank test or Cox PH regression, randomly fetching controls with logits in the same range as cases and performing logrank test or Cox PH ~500 times. This is a "bootstrapped" analysis based on randomly selecting controls via if statements like Note: cases will have the greatest predicted logit values since they are cases (y=1). Logits are used because logits are more normally-distributed when compared with y={0,1}. There are limitations of propensity matching, especially when logits of most of the controls are nowhere near those of the cases, indicating large differences in confounder influences. Large differences in severity of disease will also cause this, as prior meds/therapies, histories, and comorbidities (all nuisance factors, confounders) will be very different between cases & controls. The logistic run mentioned uses confounders which are significantly different between cases & controls as input predictor. The logits are then predictors of "caseness." There's no rationale for using propensity matching when logits for confounders are extremely different between cases and controls. As far as dropping subjects from an analysis, if age was not being studied in a clinical trial (nuisance factor, confounder), would you e.g. want to include controls of age 18-40 in a propensity-matched study of Alzheimer's disease? Answer: No. 18-40 year-old controls likely won't have Alzheimer's, will have fewer prior meds, negligible histories, fewer comorbidities, etc. Young, low-risk subjects enrolled in a drug trial for a novel molecule for Alzheimer's disease would not respond to treatment and would contribute nothing to efficacy. Propensity matching would be able to identify and drop such patients. Most academic clinicians who perform randomized clinical trials know the rule-of-thumb: "A clinical trial for a high-risk treatment will fail if low-risk patients are enrolled." Low-risk patients will have confounders or nuisance factors not being studied (e.g. histories, prior meds, comorbities) that are significantly different from high-risk patients. Propensity matching attempts to find and eliminate (drop) patients whose confounder values are significantly different from those of the cases.
