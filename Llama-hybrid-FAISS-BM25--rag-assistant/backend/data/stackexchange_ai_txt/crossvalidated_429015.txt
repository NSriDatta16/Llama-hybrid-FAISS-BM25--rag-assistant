[site]: crossvalidated
[post_id]: 429015
[parent_id]: 
[tags]: 
OOB vs CV for Random Forest

I know this question has been asked dozens of times, but I want to really clarify what is going on when finding the best forest using OOB Error versus CV with Accuracy. From my understanding, a Random Forest with $n$ observations, $p$ variables, and $t$ trees, the following procedures occur: Compute RF with OOB Error: For the first tree, $t_1$ , and utilizing $\sqrt(p)$ variables, the RF will bootstrap $q$ samples from $n$ , the In-Bag Data Set , and the rest will be saved as the Out-of-Bag Data Set . Then the RF will train $t_1$ on $q$ samples and test with the Out-of-Bag Data Set . This test produces an OOB Error for $t_1$ , $OOB_{t_1}$ . Next tree, $t_2$ , the RF bootstraps samples again, tests again to get $OOB_{t_2}$ . Rinse and Repeat 1-4 until have all $OOB_{t_i}$ . If required to tune the number of variables, utilize a different number and repeat steps 1-5. Then compare $OOB_{t_i,p}$ between trees and variables. Compute RF with CV and Accuracy: Split data into $k$ folds. For the first tree, $t_1$ , and utilizing $\sqrt(p)$ variables, the RF will bootstrap $q$ samples from $k-1$ folds. Then the RF will train $t_1$ on $q$ samples and test, validating the Accuracy, on the $k^{th}$ fold. Rinse and Repeat 1-3, until have all $Accuracy_{t_i}$ for that set of folds. Repeat steps 1-4, $k$ times with a different combination of folds. Then return the Accuracy for each tree as the Average Accuracy across all the folds. If required to tune the number of variables, Repeat steps 1-4 $k$ times varying $p$ variables. Then repeat step 5 until all combinations of variables and trees as required. My understanding is that they are very similar, here . But the OOB method utilizes a smaller training/learning set. It seems like best practice is to use OOB, but I feel more comfortable with CV and Accuracy. This is because by leaving a set of data out for each tree or forest of trees instead of bootstrapping for each tree with the same original $n$ samples, we validate each tree/forest based on some data never seen versus data that may have been seen by other trees due to bootstrapping. Even though with CV, for a tree, not all the data would be used from bootstrapping, but as the number of trees increase, this, in theory , decreases. While, for OOB, we always utilize every observation. Would this be the proper way to think about this problem?
