[site]: crossvalidated
[post_id]: 595854
[parent_id]: 595842
[tags]: 
Your best bet is likely a very simple double loop for an exhaustive search. Loop over the possible dates in a year (or week starts, or month starts for weekly or monthly data). Call this $t_1$ . Loop over the possible later dates (week starts, month starts) in a year. Call this $t_2$ . You may want to start your loop at $t_1+\Delta$ for some minimum season length $\Delta$ . For each pair $(t_1, t_2)$ , calculate average observations $\overline{y}_i$ ("in-season") between $t_1$ and $t_2$ , and average observations $\overline{y}_o$ ("out of season") between $t_2$ and $t_1$ (of the next year). Note the boundaries of your time series (which is why I would work with averages in seasons, not totals). Calculate the absolute difference $|\overline{y}_i-\overline{y}_o|$ . Finally, pick the pair $(t_1, t_2)$ with the largest difference $|\overline{y}_i-\overline{y}_o|$ . This should be easy to implement in any type of software, far easier than teaching some ML algorithm to output time intervals. Unless you need this in real time for massive numbers of time series (in which case I hope you would not be asking here), it should be also absolutely competitive in terms of runtime.
