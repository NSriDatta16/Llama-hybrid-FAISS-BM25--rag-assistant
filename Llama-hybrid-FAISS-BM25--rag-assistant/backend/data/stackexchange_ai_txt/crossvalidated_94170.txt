[site]: crossvalidated
[post_id]: 94170
[parent_id]: 
[tags]: 
Evaluating productivity metrics in an A/B test system

I'm beginning work on an A/B testing system for UX changes to a tool my team builds. Our users are doing simple, repeatable tasks assigned by another team's system. My team handles the UI/UX part. Users typically have a steady stream of work assigned to them, and they can see their current task, and complete it. Tasks typically take under a minute. Each task has an expected time to complete it and the users are rated based on their overall "actual / expected" for a day's work. My question is this: Given that a random subset of users are consistently seeing a slightly different treatment (the deterministic 'random' selection is a solve problem), and I can get a steady stream of [User, expected time, actual time, treatment] for each complete task, how can I evaluate the overall productivity changes for users in a statistically valid way? To give some insight into why this is tricky, here are some things we've been worried about: Let's say users under Treatment A are 10% more efficient. As a result, they will produce more data points . With the wrong analysis, more data points might skew the results, and we want exact figures, with high confidence. We want to know how much the change helps the average user. The data is very noisy, but seems to average out over an hour/day/shift for each user. One task may take a fraction of the expected time, or a magnitude more than expected. This is in part because our ability to estimate completion time is in need of improvement (but on average, it's pretty accurate). We worry that our high variability in estimated completion time will increase our uncertainty in results analysis. Thanks very much.
