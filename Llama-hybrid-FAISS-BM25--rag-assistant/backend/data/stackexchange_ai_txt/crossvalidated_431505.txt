[site]: crossvalidated
[post_id]: 431505
[parent_id]: 431367
[tags]: 
$k$ -NN just measures the distances between observations and may suffer the curse of dimensionality as well as other algorithms. It also does not try finding the distribution of the variables, just makes local approximations. So it is hard to compare to the two other methods you mention. Logistic regression (same applies to linear regression) makes the assumption that the model is linear $$ p(y|x) = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k) $$ Naive Bayes algorithm makes the assumption that the features are independent $$ p(x, y) = p(x_1 | y) \, p(x_2 | y) \dots p(x_k|y) \, p(y) $$ In both cases we assume a model that simplifies the conditional distribution to something computationally manageable. You seem to be asking why can't we use the "full Bayes" algorithm, i.e. calculate $p(x_1, x_2, \dots, x_k | y)$ directly from the data. The problem is that the dimensionality of such distribution is so large , that you would need huge amount of data and tremendous computational resources. Moreover, it might simply not be possible to find the full distribution. Imagine, for example, that you are building a spam detection algorithm. To calculate the full distribution of the data, you would need to observe $n$ samples per each of the possible combination of all the possible words . Even if you limit yourself to limited grammar of, say, 100 000 most common words, the number of possible combinations of those words is literally infinite .
