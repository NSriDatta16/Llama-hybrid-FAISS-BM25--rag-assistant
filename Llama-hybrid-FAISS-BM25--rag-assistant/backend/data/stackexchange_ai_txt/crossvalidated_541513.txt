[site]: crossvalidated
[post_id]: 541513
[parent_id]: 541487
[tags]: 
Yes, these two approaches can be equivalent. The other source of differences besides the neural network weights is the optimizer state, learning-rate/momentum-schedule, extent of augmentation, progressive resizing and anything else that gets adapted during training in some manner (either adaptively or following a fixed schedule). For the 2 times 100 epoch training to be equivalent to the 200 epoch training, the same schedule of all these things to be used for all the 200 epochs - which means restoring the state of these when restarting training after an interruption. This would be an issue with many commonly used approaches (e.g. Adam, RMSProp, one-cycle policy, reducing learning rate on plateaus, progressive re-sizing etc.).
