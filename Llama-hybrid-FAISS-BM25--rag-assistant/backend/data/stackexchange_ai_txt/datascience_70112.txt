[site]: datascience
[post_id]: 70112
[parent_id]: 49661
[tags]: 
I think there is an easier way to compare distance preservations: 1) In case your original dataframe is big, sample let's say random N=10,000 points from there. 2) Fit all your dimensionality reduction algorithms and transform this newly sampled dataframe. 3) Obtain euclidean distances between points in originally sampled dataframe, then other newly produced transformed dataframes in step 2. For example: dist_orig = np.square(euclidean_distances(X_sampled, X_sampled)).flatten() dist_pca = np.square(euclidean_distances(pca, pca)).flatten() dist_tsne = np.square(euclidean_distances(tsne, tsne)).flatten() dist_umap = np.square(euclidean_distances(umap, umap)).flatten() 4) Run spearman or other correlation metric by comparing original sampled euclidean space vs each of the reduced spaces. For instance: coef_pca, p_pca = spearmanr(dist_orig, dist_pca) coef_tsne, p_tsne = spearmanr(dist_orig, dist_tsne) coef_umap, p_umap = spearmanr(dist_orig, dist_umap) 5) See which method has the highest correlation coefficient and, hence, preserves the best distances between points in original space.
