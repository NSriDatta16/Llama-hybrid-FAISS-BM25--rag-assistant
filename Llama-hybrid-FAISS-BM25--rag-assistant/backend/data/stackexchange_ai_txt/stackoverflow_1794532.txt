[site]: stackoverflow
[post_id]: 1794532
[parent_id]: 
[tags]: 
Algorithmically suggest best node to perform demanding computation

At work we perform demanding numerical computations. We have a network of several Linux boxes with different processing capabilities. At any given time, there can be anywhere from zero to dozens of people connected to a given box. I created a script to measure the MFLOPS (Million of Floating Point Operations per Second) using the Linpack Benchmark; it also provides number of cores and memory. I would like to use this information together with the load average (obtained using the uptime command) to suggest the best computer for performing a demanding computation. In other words, its 3:00pm; I have a meeting in two hours; I need to run a demanding process: what node will get me the answer fastest? I envision a script which will output a suggestion along the lines of: SUGGESTED HOSTS (IN ORDER OF PREFERENCE) HOST1.MYNETWORK HOST2.MYNETWORK HOST3.MYNETWORK Such suggestion should favor fast computers (high MFLOPS) if the load average is low and, as load average increases for a given node, it should favor available nodes instead (i.e., I'd rather run in a slower computer with no users than in an eight-core with forty dudes logged in). How should I prioritize? What algorithm (rationale) would you use? Again, what I have is: Load Average (1min, 5min, 15min) MFLOPS measure Number of users logged in RAM (installed and available) Number of cores (important to normalize the load average) Any thoughts? Thanks!
