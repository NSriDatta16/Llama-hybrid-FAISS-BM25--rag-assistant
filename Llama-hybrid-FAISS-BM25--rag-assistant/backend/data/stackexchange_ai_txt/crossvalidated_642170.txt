[site]: crossvalidated
[post_id]: 642170
[parent_id]: 
[tags]: 
Fluctuating Validation Loss & Accuracy during Transfer Learning (ResNet50) - FER+ Dataset

I'm trying to build a CNN model for image classification, more specifically emotion classification using the FER+ dataset which is proving difficult to work with. I've tried several variations of models to work with ranging from 'from-scratch' to my current best solution - transfer learning. My best results so far have come from using either ResNet50 or MobileNetV2 as my base model as well as having my own custom trainable head. More specifically, I truncate the base models and only freeze the first few blocks while leaving a block closest to the head, trainable. So far, this mix of feature extraction and fine tuning has culminated to a 93-95% validation accuracy (depending on whether its ResNet50 or MobileNetV2). While this sounds good, the issue arises while training, where validation loss & accuracy fluctuate quite heavily despite attempts to regularize, add more augmentations to the data generator and decrease the learning rate while fine-tuning. Below is an image of my training results when using ResNet50 as my base model. In addition, in the confusion matrix you can also see how it never predicts any of the 'disgust' images correctly from the test set, which contains 20 images in the 'disgust' class. In addition, regardless of whether I use ResNet or MobileNet as my base model, I still seem to get the same fluctuation in loss and accuracy. Some details about my model and training: Base model: ResNet50 or MobileNetV2 using weights from 'imagenet' Loss function: categorical crossentropy Optimizer: Adam Epochs: 100 Learning rate: 0.001 As far as preprocessing goes, I re-scale my images and my best results have come from only applying a random horizontal flip to my training data. FER+ images are 48x48 grayscale as standard and so I upsample to 224x224 and convert to RGB. My network is quite deep so I want to mitigate any instances of convolutions being operated on small dimensions (e.g. 1x1, 2x2...). I should also note that the dataset is inherently quite imbalanced, which would normally warrant some implementation of class weights however, I find that when using class weights my results become significantly worse, as in a 15%-20% reduction in validation accuracy as well as poor results from the confusion matrix. Below is an image of the class distribution for the training set. How can I stabilize the validation loss and accuracy during training?
