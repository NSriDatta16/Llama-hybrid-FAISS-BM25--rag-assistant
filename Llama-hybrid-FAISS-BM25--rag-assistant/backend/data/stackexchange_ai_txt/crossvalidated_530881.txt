[site]: crossvalidated
[post_id]: 530881
[parent_id]: 530513
[tags]: 
The pretty much standard suggestions are: Remove stop words (either use a stop-word list) or do POS tagging and only keep nouns, adjectives, and verbs. If speed is not your primary concern, you can use something more complex than stemmer and lemmatize the text. Using Spacy , it is almost one-liner including removing stop-words: [token.lemma_ for token in nlp(your_text) if not token.is_stop and not token.is_punct] Classical, old-fashioned ML still works quite well. Random forest on 10k features might work well too. You can use pre-trained word embeddings and mean-pool or max-pool them before providing them to the classifier. There is also an interesting work on getting sentence/text embeddings from word embeddings by discrete cosine transform . You can try Doc2Vec for representing the documents. Your idea about finetuning. BERT would certainly work well. You say your texts are 500-600 words long. If you think the relevant information is more towards the beginning of the text, you can cut off the first 512 tokens and to the classification on that.
