[site]: crossvalidated
[post_id]: 500149
[parent_id]: 
[tags]: 
Confusion about Understanding Supervised Learning as Bayesian Inference

I am going through a lecture that is explaining how supervised learning can be thought of from a Bayesian perspective, where we are trying to maximize log p(theta | data). I am confused as to what the statement p(theta | data) means, where theta is the model parameters. I do understand what p(theta), the prior, and p(data|theta) represent. p(data|theta) represents the probability that we encounter the given data points given the model parameters theta. But what exactly does "p(theta|data)" mean in words? In other words, I'm confused about why we are trying to maximize "p(theta|data)" as opposed to trying to maximize "p(data|theta)" since that's the actual measure of model performance.
