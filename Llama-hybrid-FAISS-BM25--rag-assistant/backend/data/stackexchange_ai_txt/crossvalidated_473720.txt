[site]: crossvalidated
[post_id]: 473720
[parent_id]: 473714
[tags]: 
x is usually a vector of uniformly distributed random variables. G(x) is a function which maps a vector of uniform random variable into something else (usually chosen to be a neural network). D(z) can be any classifier you like, but is also usually a neural network. The basic idea comes from inverse transform sampling: https://en.wikipedia.org/wiki/Inverse_transform_sampling Suppose we want to draw a random variable from an arbitrary distribution F. We can do this by sampling a uniform U[0,1] random variable X and then passing it through the inverse CDF of F, to give Y=F^{-1}(X). Then, it is easy to show that Y is distributed as F. In a GAN, we are trying to learn the function G = F^{-1} since it is not known beforehand. This is a 'standard' function estimation problem, so we could do this nonparametrically, or specify a parameteric form for this function. Most typically, a neural network is used to approximate it, since neural nets are typically good function approximators. In other words, given a set of data (eg images) we are trying to learn the unknown distribution F which generated these images, so that we can generate new images from the same distribution. To do this, we define G to be the inverse CDF of F, and approximate it using a neural network. Once we have learned the inverse CDF, we can create new images (i.e. draw samples from F) by sampling uniform random variables and passing them through the inverse CDF G (i.e. passing them through the neural network which is approximating G). D(z) is just a classifier, it can be anything you like; a SVM, decision tree, logistic regression, whatever. In practice neural networks are usually used, because they tend to be good on image/vision problems, which is where GANs are typically applied. In a typical image problem its probably going to be a convolution net.
