[site]: crossvalidated
[post_id]: 422406
[parent_id]: 
[tags]: 
Why SVM performs better than logistic regression for well separated case?

In ISLR page 357, the author mentioned that " When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred." But he didn't mention why. Here is my thought: Since the loss function for logistic is zero only when the value reaches infinity, So when we train the loss function for logistic regression, this method is greedy, it will try its best to separate the data points, unluckily, the well separable case will lead logistic loss function to try its best to achieve zero. Thus the sigmoid function will look similar to I[x>0]. Thus the result has high variance. While for SVM, since all value larger than 1 will be 0, so the training won't be too much greedy. Am I right? Is it also the reason such that logistic regression is more suitable for overlapping case? Thanks.
