[site]: crossvalidated
[post_id]: 521718
[parent_id]: 521566
[tags]: 
You can often solve this problem with a linear search. This post provides some theory and code. Theory Let the data be, as in the question, $x_1,\ldots, x_n.$ These began as non-negative integers $a_1,\ldots,a_n$ which, when expressed as proportions of the whole (the sum $N=a_1+\cdots+a_n$ ), are rounded versions of $z_i = a_i/N.$ Rounding is straightforward but a little complicated to describe. When we round $z$ to $d\ge 0$ digits we are rounding the value $10^d z$ to the nearest integer and then multiplying the result by $10^{-d}.$ Thus we may focus on the process of rounding numbers $z$ to the nearest integer. For this post alone, let $[z]$ designate the result of rounding to an integer. (This is a conventional notation for discussions of rounding.) The nearest integers to $z$ are its floor $\lfloor z \rfloor$ and its ceiling $\lceil z \rceil,$ for which $\lfloor z \rfloor \le z \le \lceil z \rceil.$ When one of these integers is closest to $z,$ it is the rounded version of $z.$ In particular, the distance between $z$ and its rounded version cannot exceed $1/2:$ $|\,z - [z]\,| \le 1/2.$ When $z$ lies equidistant from its floor and ceiling, it is an odd multiple of $1/2.$ Many systems -- R is one of them -- round to an even value (multiple of $2$ ) in those circumstances. Thus, $[1/2] = 0,$ $[3/2] = 2,$ $[5/2] = 2,$ $[7/2]=4,$ and so on. See, for instance, this R command and its output: > round(seq(1/2, 7/2, by=1)) [1] 0 2 2 4 Our objective is to infer plausible values of the $a_i$ and $N$ from the data. This requires us (somehow) to invert the rounding process. To that end, it is helpful to note (from the foregoing description of rounding) that $|a_i/N - x_i|\,10^d = |10^d (z_i - x_i)| \le 1/2,$ whence $$x_iN - \left(\frac{10^{-d}}{2}\right)N \le a_i \le x_iN + \left(\frac{10^{-d}}{2}\right)N.\tag{*}$$ When $|10^d a_i/N - 10^d x_i| = 1/2,$ then $10^dx_i$ is a multiple of $2.$ This refines $(*)$ by indicating which of those inequalities are strict ones. One way to rephrase this is to consider whether $10^d x_i$ (which must be integral) is odd or even. When it is even, it is possible for the first inequality to be equality; when it is odd, it is possible for the second inequality to be equality. The original question arises in circumstances where the $x_i$ do not sum to unity. As an example, consider $(a_1,a_2,a_3,a_4)=(1,1,5,9).$ Here $N=1+1+5+9=16$ and, using $d=3$ digits, the fractions round to $[1/16]=0.062,$ $[5/16]=0.312,$ and $[9/16]=0.562,$ which sum to $0.998 = 1 - 2\times 10^{-d}.$ In particular, this example shows that the sum of the rounded values may differ from $1$ by as much as $n$ times the maximum rounding error of $10^{-d}/2.$ In such cases, we would like to infer the integers $a_i$ so that we may recover more accurate versions of the $z_i$ from the data. Note, though, that any solution $(a_i)$ automatically gives rise to a host of solutions of the form $(ma_i)$ for any multiple $m=1,2,3,\ldots.$ There may be other solutions, too, which are not multiples of this one. How to choose among them? I propose the following, which arises from an application of Occam's Razor (viewing smaller denominators as "simpler"): To solve this problem, first specify an allowable range of values for $N,$ say from $N_0$ to $N_1,$ inclusive. Among all possible solutions $(a_i),$ write $N=\sum a_i$ and select the solution for which $N_0 \le N \le N_1$ and $N$ is as small as possible. An Algorithm Let $N \ge 1$ be any candidate for the sum of the $a_i.$ When it is the correct (original) value, $(*)$ must hold for every $x_i.$ This pair of inequalities defines a (possibly empty) set $\mathcal{A}(x_i;N)$ of integers $a_i$ that satisfy them. Ordinarily (for $N$ sufficiently small) these sets are empty or contain just one element. It therefore is feasible and efficient to consider all tuples $(a_1,\ldots,a_n)$ for which both $a_i\in\mathcal{A}(x_i;N)$ for all $i$ and $a_1+\cdots+a_n=N.$ If rounding these $a_i/N$ to $d$ digits exactly reproduces the data $(x_i),$ we have a solution. Let the set of all such solutions (usually empty) be $\mathcal{S}(N).$ The algorithm is now simple to describe. I will use pseudocode: Input: Array x, integers N0, N1, and digit count d n = length(x) For N from N0 to N1: For each i from 1 to n: Let A[i] = A(x[i],N) If U (in the next step) will be too large then stop Let U = A[1] X A[2] X ... X A[n] {the Cartesian product} Let S = Empty collection For each array a in U: If sum(a) == N and round(a/N, d) == x then adjoin a to S Return S. The output is a set (usually empty or a singleton) of plausible solutions. Comments Notice there is no test related to rounding to even values. Omitting this test will occasionally cause the search set U to be too large, at some cost in computation time. But since a solution is returned only when it checks out -- it must round exactly to the given data x -- the algorithm remains correct. This implies this algorithm will work regardless of what form of rounding the software platform supports, provided it is the same form used to create the data in the first place. When $N_0$ is not too large, most of the time the search set U will be very small, making the algorithm efficient. But note the risk: when many of the $x_i$ have more than one candidate for $a_i,$ the size of $U$ explodes. With $n=60$ data values and just two candidates per value, for instance, $U$ has $2^{60}$ elements. That's why we need to check the size of $U$ before going on. Limiting the upper search limit $N_1$ to a sufficiently small value avoids this problem. If you cannot find any solutions, then increase $N_1$ and try again. If the number of digits used for the original rounding was crude -- that is, it loses a lot of precision -- then this method will usually return a solution with a smaller value of $N$ than the original. But when $d$ is reasonably large; say, when $10^{-d}N \approx 1$ (or less), then it will almost always return the original value of $N.$ The notable exceptions are when the greatest common divisor of the $a_i$ is greater than $1,$ for then there is no way to distinguish the original $a_i$ from $a_i/\operatorname{gcd}(a),$ no matter how precise the rounding might be. Examples and Code The question asks about datasets of $60$ or fewer numbers and intimates they are rounded to $\pm0.05\%,$ which is $d=3$ decimal digits. That rounding precision would be unsuitable for numbers of three or more digits. Let us, then, create a dataset of integers $z_i$ between $0$ and $999$ (perhaps favoring the smaller ones, since rounding them can create more relative imprecision), round them to form the $x_i,$ and see what solution(s) this algorithm produces. The dataset processed in this example is $(a_i) = (1,2,4,5,6,\ldots, 206,238),$ whose sum is $N=4490.$ I conducted the search starting at $N_0=1$ (and limiting it, if needed, to $2\times 10^d,$ which should be more than enough). The solution found is $N^{*}=1004,$ thereby estimating the original data as $a_i^{*} = (0,0,1,1,1,\ldots, 46,54).$ However, as promised, this solution exactly reproduces the rounded values of $a_i/N.$ One way to compare the original with the solution is to plot the relative errors between the true ratios $a_i/N$ and the estimated ratios $a_i^{*}/N^{*}.$ As one would expect, rounding afflicts the relative precision of the smallest values the most. This is one reason we might want to work a little bit to recover the original ratios as accurately as we reasonably can. Here's the R code implementing the algorithm and the data generation process to test it. # # Determine whether a denominator `N` will yield a solution when `x` has been # rounded to `digits` digits (base 10). # test 0) break # Stop at smallest `N` with a solution } }) if (nrow(solution) > 0) rownames(solution) 0) { sapply(rownames(solution), function(sname) { s
