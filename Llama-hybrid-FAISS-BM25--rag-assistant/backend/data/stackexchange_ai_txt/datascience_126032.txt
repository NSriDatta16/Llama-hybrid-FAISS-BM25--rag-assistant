[site]: datascience
[post_id]: 126032
[parent_id]: 
[tags]: 
Attention mechanisms without a linear layer

I am currently looking into attention mechanism as they are used in (non-Transformer) encoder-decoder architectures, meaning an architecture where some RNN (usually LSTM or GRU) is used in both the encoder and decoder and is unrolled as many times as the input sequences are long. In particular, I am working on a project that is based on an older work on machine translation that uses such a model in combination with an attention-mechanism in AllenNLP 0.9. Looking at the AllenNLP code, I found that they call the attention-mechanism that is being used "dot_product" (please note that this is definitely not referring to the scaled dot product attention that is used in transformers), and apparently all it does is, for the hidden state ht of the decoder at timestep t, calculate the dot-product to the hidden-states of each encoder step: def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor: return matrix.bmm(vector.unsqueeze(-1)).squeeze(-1) The result is then ran through a softmax-function and the resulting weights are used to calculated a weighted sum of the encoder hidden-states. Lastly, this vector is concatenated to ht and ran through a final linear layer that has the same output dimension as the vocabulary. This matches the attention-descriptions in Josh Starmer's StatQuest on the topic and in Section 3.1 of a very prominent paper . Here is an illustration from the paper: What confuses me about this is how this improves model performance. What is described above is simply a static calculation based on the dot-product, there are no learned weights since there is no linear layer in the attention-mechanism. What further confuses me is that there are many implementations of attention that DO use a linear layer for learning weights, e.g Bahdanau Attention or scaled dot product attention, and sources that do not even mention that there is a linear-layer-less attention 1 2 . So I guess to sum up, my question is : How does attention without a linear layer help the model and how does it compare to an approach that does use a linear layer?
