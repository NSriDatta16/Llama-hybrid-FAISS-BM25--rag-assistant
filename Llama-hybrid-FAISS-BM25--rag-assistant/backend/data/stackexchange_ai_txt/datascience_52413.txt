[site]: datascience
[post_id]: 52413
[parent_id]: 
[tags]: 
Am I using optim.SGD incorrectly in pytorch?

I am doing reinforcement learning in checkers. After each game the network beats itself, I calculate the loss of every individual position in the game, call backward(), and step(). I am beginning to believe that this is not how SGD is supposed to be used, and that I should be feeding it batches, say a whole game at a time. Strictly in terms of code: Do I do this by just wrapping the collective inputs in a one dimensional tensor? If I give SGD a collection of inputs does it sample from the collection, calculate the average loss, and use that gradient? Is SGD actually intended to be used in this way with large 'collections'/batches?
