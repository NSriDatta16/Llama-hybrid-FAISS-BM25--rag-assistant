[site]: crossvalidated
[post_id]: 94610
[parent_id]: 90340
[tags]: 
Something very similar is Logistic regression with a Radial Basis Function (sometimes called Gaussian) kernel ( http://en.wikipedia.org/wiki/Radial_basis_function_kernel ), with all weights constrained to be $1$. A fitted RBF kernel regression will compute weighted distances to various points, compute a Gaussian pseudo-probability of being a neighbor based on the distance, and average the values at the points in the training set weighted by that Gaussian pseudo-probability. This is very similar to kNN, where $k$=size of the training set. You can reduce the number of points under consideration via a Laplacian prior ($L_1$ regularization), but this won't have the same effect as varying $k$ in kNN, as the $k$ points chosen will be the only ones considered. If weights are fixed at $1$, only the distance matters. When weights are allowed to vary, it affects how much each point is allowed to vary.
