[site]: crossvalidated
[post_id]: 343349
[parent_id]: 343323
[tags]: 
This is probably coming from perfect or near-perfect separation in your logistic regression analysis of the full data set. With a large number of predictors it is common to find that some combination of them happens perfectly to distinguish the 2 classes in your particular data set . That can lead to difficulties in performing the analysis at all, very large apparent regression coefficients, or instability of results from subsample to subsample. So in your example I would suspect separation in your full data set, leading to the extremely large coefficients. As MichaelM points out in a comment, you really do have to pay attention to the magnitudes of coefficients; do you really think that it's reasonable to expect a single predictor to have a million-fold influence on the log-odds of class membership? That's what a coefficient of 1e+06 means. It's not good practice just to focus on p -values. Then when you look at a subset the (highly data-dependent) separation disappears, as might be expected. Penalized regression approaches like ridge or LASSO can help work around this type of problem.
