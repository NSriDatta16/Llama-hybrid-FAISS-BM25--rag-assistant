[site]: crossvalidated
[post_id]: 520534
[parent_id]: 
[tags]: 
What is the difference between a covariance matrix created by an RBF kernel and a covariance matrix created by

I can't explain something simple to myself and it is probably a matter of vocabulary, I am not sure... If I create and random normal $Z \in \mathbb{R}^{3\times5}$ , each row and column has a mean of 0. If I then wanted to created a covariance matrix of the first dimension, I could either calculate $$ \frac{XX^\top}{n-1} $$ which would give the average squared distance from the mean of 0 between each of the rows in $X$ , which is the covariance. Alternatively, I could use the RBF kernel and calculate $RBF(x)$ which would also be called the covariance matrix between the rows of $X$ . Unless I have gotten the approach wrong, these two things are not equal as the code demonstrates below. I would like to gain a deeper understanding of the difference between these two things and why they are both called covariance matrices even though they are different. For example, could I use the $XX^\top$ covariance matrix for Gaussian process regression if it is a valid covariance matrix, or is there something that prevents it from working? import gpytorch # type: ignore import torch # type: ignore RBF_KERN_FUNC = gpytorch.kernels.RBFKernel() def run(): x_tr = torch.randn(3, 5) cov_rbf = RBF_KERN_FUNC(x_tr).evaluate() print(f"rbf size: {cov_rbf.size()}") cov = x_tr @ x_tr.t() print(f"cov size: {cov.size()}") print(f"rbf: {cov_rbf}") print(f"cov: {torch.exp(-cov / 4)}") if __name__ == '__main__': run() Output: rbf size: torch.Size([3, 3]) cov size: torch.Size([3, 3]) rbf: tensor([[1.0000, 0.0970, 0.0107], [0.0970, 1.0000, 0.0320], [0.0107, 0.0320, 1.0000]], grad_fn= ) cov: tensor([[0.1803, 0.4715, 0.8288], [0.4715, 0.3840, 0.9206], [0.8288, 0.9206, 0.3947]])
