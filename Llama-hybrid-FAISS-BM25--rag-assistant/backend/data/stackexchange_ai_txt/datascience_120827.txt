[site]: datascience
[post_id]: 120827
[parent_id]: 120816
[tags]: 
For context, we are talking here about a language modeling task, that is, predicting the next word or token. In normal transformers, there is no “state transfer” between different segments (either within the same batch or across batches) at all. Other Transformer variants are specifically meant to model long sequences and have a memory to transfer the past hidden states to the following batches. An example is TransformerXL .
