[site]: datascience
[post_id]: 16763
[parent_id]: 16757
[tags]: 
Your padded vector should be OK as a starting model. You could take it a step further and have a specific "no character" encoding, as that will be a clearer learning signal. You say that you are using a feed-forward neural network. In that case, there is a possible change and improvement to your model, which is to use Recurrent Neural Network (RNN) , probably a long short-term memory (LSTM) variation, because that has had some success in language models. In a RNN, instead of having the character encoding repeated N times, you have it represented just once, and run the network N times for each sample, feeding in each character in turn. Instead of a "no character" symbol, you would have an "end of sequence" symbol. For an open-ended model with no specific target, it is common to have the output of the network be a prediction of the next character. This allows you to explore what the model has learnt by sampling from its predictions - a lot like using n-grams to generate realistic-looking words. The advantage of a RNN for character-level language modelling is that the model is built around the assumption that each input is somehow equivalent - the same encoding is used at each time step. This is clearly true for character strings, so it can lead to better models from less training data. The main disadvantage of using a RNN is that it is more complex to understand, build and train such a model.
