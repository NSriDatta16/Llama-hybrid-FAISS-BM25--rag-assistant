[site]: crossvalidated
[post_id]: 175166
[parent_id]: 175127
[tags]: 
It would depend on the nature of the heteroskedasticity. If you wanted a prediction interval, you usually need a parametric specification like: $$ y_i \sim N(\mathbf{x}_i'\beta,\sigma_i(\mathbf{x}_i,\mathbf{z}_i )) $$ i.e. $y_i$ is normally distributed with mean $\mathbf{x}_i'\beta$, and standard deviation $\sigma_i(\mathbf{x}_i,\mathbf{z}_i )$, where the standard deviation is some known function of the $\mathbf{x}_i$ or perhaps some other set of variables $\mathbf{z}_i $ , that way you can estimate the standard deviation for each $i^{th}$ observation. Examples of possible functions include; $\sigma^2_i(\mathbf{x}_i)=\sigma^2x_{i,k}$ (Studies of firm profits, an example from Greene's "Econometric Analysis" 7th edition CH 9), where $x_{i,k}$ is the $i^{th}$ observation of the $k^{th}$ dependent variable, or, if working with time series data, GARCH and/or stochastic volatility specifications. You can use the estimates $\hat \sigma_i(\mathbf{x}_i,\mathbf{z}_i )$ as the standard errors for your prediction intervals if you like. I will forgo a formal treatment here because accounting for estimation errors in $\hat \sigma_i(\mathbf{x}_i,\mathbf{z}_i )$ can be complicated but, with a sufficiently large sample, ignoring the estimation error does not effect the prediction interval that much. In short, it is not necessary to open that can of worms here. For a more detailed explanation of all this and more examples, see Wooldridge's book "Introductory Econometrics: A Modern Approach" , Ch 8. The problem is that when people refer to heteroskedastic or "robust" regression, they are usually referring to the situation in which the precise nature of the heteroskedasticity (the function $\sigma_i(\mathbf{x}_i,\mathbf{z}_i )$) is not known, in which case a White or two-step estimator is used. These offer consistent estimates for $var(\hat \beta)$ but not for the $\sigma_i$, and so you have no naturally way to estimate prediction intervals. I would argue that prediction intervals are not meaningful in this context anyway. The idea behind these sandwich type estimators is to consistently estimate the standard error of the coefficients, $\hat \beta$, without the burden of offering accurate prediction intervals for each individual observation, thus making the estimates more "robust". Edit: Just to be clear, the above only considers least squares regression. Other forms of non-parametric regression, such as quantile regression, may offer means of obtaining a prediction interval without parametric specification of residual standard error.
