[site]: datascience
[post_id]: 64976
[parent_id]: 64827
[tags]: 
Are you reffering to biological "cognitive function attention"? If yes than it could be argued its different. TL;DR Fixed-length Context vector in the encoder decoder architecture could not remember long term dependencies. Attention could learn these long term dependencies, hence have a long-term memory. If we speak about human attention it is usually short spaned (relative). On the other hand I believe it was Yoshua who first talked in the terms of attention and one could argue that we could implicitly focus (our attention) on some older parts of text and hence extract deeper meaning. TL;DR TL;DR One can make a case its actually more natural to talk about long-term memory when discussing attention-ML-mechanism
