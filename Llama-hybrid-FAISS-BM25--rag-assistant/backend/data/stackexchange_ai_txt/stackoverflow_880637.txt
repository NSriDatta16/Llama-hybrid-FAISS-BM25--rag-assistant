[site]: stackoverflow
[post_id]: 880637
[parent_id]: 880559
[tags]: 
A simple-minded approach is to generate all the "substrings" and, for each of them, check whether it's an element of the set of acceptable words. E.g., in Python 2.6: import itertools import urllib def words(): f = urllib.urlopen( 'http://www.cs.umd.edu/class/fall2008/cmsc433/p5/Usr.Dict.Words.txt') allwords = set(w[:-1] for w in f) f.close() return allwords def substrings(s): for i in range(2, len(s)+1): for p in itertools.permutations(s, i): yield ''.join(p) def main(): w = words() print '%d words' % len(w) ss = set(substrings('weep')) print '%d substrings' % len(ss) good = ss & w print '%d good ones' % len(good) sgood = sorted(good, key=lambda w:(len(w), w)) for aword in sgood: print aword main() will emit: 38617 words 31 substrings 5 good ones we ewe pew wee weep Of course, as other responses pointed out, organizing your data purposefully can greatly speed-up your runtime -- although the best data organization for a fast anagram finder could well be different... but that will largely depend on the nature of your dictionary of allowed words (a few tens of thousands, like here -- or millions?). Hash-maps and "signatures" (based on sorting the letters in each word) should be considered, as well as tries &c.
