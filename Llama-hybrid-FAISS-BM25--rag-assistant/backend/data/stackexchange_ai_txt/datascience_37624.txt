[site]: datascience
[post_id]: 37624
[parent_id]: 37621
[tags]: 
Out of the two pipelines you mentioned, I'd recommend the second (i.e. real-time augmentation). This is better than the first, because by performing random augmentations the network sees different images at each epoch. I'd recommend imgaug , which is a python library for performing data augmentation. I've found it very helpful as it can work with keras' ImageDataGenerator very well. The way can do this is: from imgaug import augmenters as iaa seq = iaa.Sequential([...]) # list of desired augmentors ig = ImageDataGenerator(preprocessing_function=seq.augment_image) # pass this as the preprocessing function gen = ig.flow_from_directory(data_dir) # nothing else changes with the generator One final note I'd like to make is that $7 \cdot 50 = 350$ images are very few for deep learning, even with augmentation. Make sure you use a pre-trained network or else you will have a serious overfitting problem.
