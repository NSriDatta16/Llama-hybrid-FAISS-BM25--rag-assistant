[site]: crossvalidated
[post_id]: 521470
[parent_id]: 521453
[tags]: 
Many types of model have equations that do not necessarily use all the predictors - i.e. they have built-in feature selection. These could be useful for this problem e.g. you could build a random forest model, specifying all 30 predictors using the ranger package in R. This works by training a number of decision trees and then these trees are averaged - the final model will only include the best predictors. Furthermore, once the model is fitted you can randomly shuffle each predictorâ€™s data one at a time - the difference between the MSE (or other evaluation criterion) before and after the shuffling gives a measure of each predictor's importance. This is because important variables will be affected by this random sampling, whereas unimportant predictors will show minor differences. So you could use these importance scores to find the k best predictors. You could also use a function like dredge from the MuMIn package on a regression model - this automates the process of building all combinations of your predictors. You can then produce a weighted average of the regression coefficients, where the weights are based on the AICc score of each model. This method could be more useful if you're only interested in using linear regression, but I imagine it might take a while for a model with 30 predictors... in my experience the random forest method mentioned above will be much quicker.
