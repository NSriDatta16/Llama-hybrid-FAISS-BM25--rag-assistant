[site]: crossvalidated
[post_id]: 49590
[parent_id]: 49557
[tags]: 
(a) The purpose of using a kernel is to solve a nonlinear regression problem in this case. A good kernel will allow you to solve problems in a possibly infinite-dimensional feature space. But, using a linear kernel $K(\mathbf{x,y}) = \mathbf{x}^\top \mathbf{y}$ and doing the kernel ridge regression in the dual space is same as solving the problem in the primal space, i.e., it doesn't bring any advantage (it's just much slower as the number of sample grows as you observed). (b) One of the most popular choices is the squared exponential kernel $K(x,y) = \exp(-\frac{\tau}{2} ||\mathbf{x}-\mathbf{y}||^2)$ which is universal (see ref below). There are many many kernels, and each of them will induce different inner product (and hence metric) to your feature space. (c) Straightforward implementation requires solving a linear equation of size $n$, so it's $O(n^3)$. There are many faster approximation methods such as Nyström approximation. This is an area of active research. References: Bharath Sriperumbudur, Kenji Fukumizu, and Gert Lanckriet. On the relation between universality, characteristic kernels and RKHS embedding of measures. Journal of Machine Learning Research, 9:773–780, 2010. Bernhard Schlkopf, Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond 2002
