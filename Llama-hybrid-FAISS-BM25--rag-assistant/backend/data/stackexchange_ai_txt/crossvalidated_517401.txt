[site]: crossvalidated
[post_id]: 517401
[parent_id]: 517399
[tags]: 
Decision tree are extremely efficient, very easy to tune and can handle non-linearity extremely well. Adaboost and RandomForest should be better than Decision Tree but they might be overfitting a bit. If the exemple of feature is correct and you have that many precision digits, then all the more for Trees because they can split on specific values that would be characteristic of your binary outcome. For instance they could split 2.50001 from 2.50000. Now Neural Network can be as good as trees but they are a lot harder to fine tune and that could explain your result. Usually on small to medium tabular data, Decision Tree (Xgboost for intance) will outperform Neural Net. Or put it differently it would take a very large amount of time to fine tune NN to match Gradient Boosted trees. Finally, SVM is very inefficient and linear by default. So except if you set the kernel to a non-linear one like RBF, this result is completely expected.
