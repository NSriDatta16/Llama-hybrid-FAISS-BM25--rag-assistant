[site]: datascience
[post_id]: 16265
[parent_id]: 
[tags]: 
Is stratified sampling necessary (random forest, Python)?

I use Python to run a random forest model on my imbalanced dataset (the target variable was a binary class). When splitting the training and testing dataset, I struggled whether to used stratified sampling (like the code shown) or not. So far, I observed in my project that the stratified case would lead to a higher model performance. But I think if I will use my model to predict the new cases which would highly probably differ in the distribution of target class with my current dataset. So I inclined to loosen this constrain and use the unstratified split. Could anyone can advice to clarify this point? train,test=train_test_split(myDataset, test_size=0.25, stratify=y)
