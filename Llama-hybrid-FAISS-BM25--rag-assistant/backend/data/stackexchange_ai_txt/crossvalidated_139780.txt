[site]: crossvalidated
[post_id]: 139780
[parent_id]: 
[tags]: 
Akaike information criterion for categorical and numerical data

How should I compute AIC for categorical and for numeric variables in classification problems? I see in Chapter 6 of Zumel and Mount that they use AIC before they train classification algorithms (trees, logistic regression, kNN) on a classification problem with both categorical and numerical features. They compute AIC as $2\,\left(\log L- \log L_{base}\right)-2^S$ for categorical variables, and $2\,\left(\log L- \log L_{base}\right)-1$ for numeric variables ($L$ is likelihood, $L_{base}$ is likelihood of the saturated model, $S$ is entropy). They keep features with AIC above a certain threshold, which presumably can be modified to improve algorithm performance. The choice of $2^S$ seems odd: my intuition tells me that $2^S$ should scale as $L$, not $\log L$. Also, why does entropy enter one and not the other? Could someone provide an explanation, or references justifying this choice? Intuition vs equation is highly appreciated!
