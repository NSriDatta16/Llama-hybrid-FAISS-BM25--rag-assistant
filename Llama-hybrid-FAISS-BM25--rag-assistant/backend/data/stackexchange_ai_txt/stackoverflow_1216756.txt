[site]: stackoverflow
[post_id]: 1216756
[parent_id]: 1216741
[tags]: 
Definitely B. The advantage of hash tables is that the average number of comparisons per lookup is independent of the size. If you split your map into N smaller hashmaps, you will have to search half of them on average for each lookup. If the smaller hashmaps have the same load factor that the larger map would have had, you will increase the total number of comparisons by a factor of approximately N/2. And if the smaller hashmaps have a smaller load factor, you are wasting memory. All that is assuming you distribute the keys randomly between the smaller hashmaps. If you distribute them according to some function of the key (e.g. a string prefix) then what you have created is a trie , which is efficient for some applications (e.g. auto-complete in web forms.)
