[site]: crossvalidated
[post_id]: 443732
[parent_id]: 443724
[tags]: 
If both of these models are perfect, your predictions will indeed become the true conditional variance. When they're sometimes wrong (as they always will be), you may end up with e.g. negative variance estimates. This might or might not matter to your setting. There are some alternative approaches: Fan and Yao (1998) do a similar thing, modeling the squared residuals $(y_i - \hat y_i)^2$ , and show that asymptotically you get results as good as if you knew the true $\mathbb E[Y \mid X]$ function. You can use a variant of e.g. Bayesian linear regression to predict a normal distribution $\mathcal N(\hat y_i, \hat \sigma_i^2)$ , jointly learning predictors $\hat y_i$ and $\hat \sigma_i^2$ as e.g. a neural network, and maximize the likelihood of the data under this model. (Or you could of course use a non-normal likelihood here.) There are many general approaches for conditional density estimation; you can use one of these and then pull the variance out from the estimated conditional density. Probably many more possibilities.
