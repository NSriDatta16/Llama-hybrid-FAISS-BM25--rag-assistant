[site]: crossvalidated
[post_id]: 491333
[parent_id]: 
[tags]: 
Why the simulation of a FARIMA process using the autocovariance function should be better?

Let us consider a Fractional Autoregressive Moving Average process: $ (1 - L)^d y_t = \epsilon_t$ where $d \in (-0.5,0.5)$ and $\epsilon_t$ is a white noise sequence. Let $\gamma(k)$ be the autocovariance function of the above process at lag $k$ . It is possible to simulate this process by truncating its MA( $\infty$ ) representation. However, we know from Hosking (1984) that we can simulate this process via the Durbin-Levinson method, by using $\gamma(k)$ as an input in the Durbin-Levinson recursion (see Brockwell and Davids (1986)). Why this last method should be better with respect to the first one? This is true also for a general stationary and invertible ARMA(p,q) process with autocovariance $\gamma(k)$ ? Reference: Brockwell, P. J., and Davis, R. A. (1986). Time series: Theory and methods. Berlin, Heidelberg: Springer-Verlag. Hosking, J. R. M. (1984). Modeling persistence in hydrological time series using fractional differencing. Water Resources Research, 20 (12), 1898-1908. doi: 10.1029/WR020i012p01898
