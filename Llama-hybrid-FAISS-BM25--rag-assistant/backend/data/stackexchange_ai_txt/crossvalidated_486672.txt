[site]: crossvalidated
[post_id]: 486672
[parent_id]: 
[tags]: 
Why don't linear regression assumptions matter in machine learning?

When I learned linear regression in my statistics class, we are asked to check for a few assumptions which need to be true for linear regression to make sense. I won't delve deep into those assumptions, however, these assumptions don't appear when learning linear regression from machine learning perspective. Is it because the data is so large that those assumptions are automatically taken care of? Or is it because of the loss function (i.e. gradient descent)?
