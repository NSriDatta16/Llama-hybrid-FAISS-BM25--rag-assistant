[site]: crossvalidated
[post_id]: 13678
[parent_id]: 13650
[tags]: 
This may be overkill, but Geoffrey Hinton has been doing something like this using artificial neural networks and backpropagation in time . He trains his model using blocks of text from wikipedia or NYTimes, the network learns the chained statistical relationships you mention in your question. Then the network can iteratively generate characters from a short prompt. The sample below was obtained by running the MRNN less than 10 times and selecting the most intriguing sample. The beginning of the paragraph and the parentheses near the end are par- ticularly interesting. The MRNN was initialized with the phrase “ The meaning of life is ”: The meaning of life is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger. In the show’s agreement unanimously resurfaced. The wild pasteured with consistent street forests were incorporated by the 15th century BE. In 1996 the primary rapford undergoes an effort that the reserve conditioning, written into Jewish cities, sleepers to incorporate the .St Eurasia that activates the popula- tion. Mar??a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu’s thought is. To adapt in most parts of North America, the dynamic fairy Dan please believes, the free speech are much related to the The take-home from this paper is that the model was able to learn the fact that characters are organized into words, to learn many of those words, and to learn some of the rules of how words are strung together grammatically. That may be much more structure than the ABC character sequence you had in mind, so I think user4581 's answer is more appropriate. But this one is a lot of fun.
