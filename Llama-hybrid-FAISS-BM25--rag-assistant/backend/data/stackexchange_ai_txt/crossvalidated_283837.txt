[site]: crossvalidated
[post_id]: 283837
[parent_id]: 169943
[tags]: 
There are two sides to this coin. When you want to use the logistic regression for explaining the goal is to interpret the betas. When you have perfect multicolinearity in your data (say for $x_1$ and $x_2$), $b_1$ and $b_2$ (belonging to $x_1$ and $x_2$) are not reliable for interpretation. $b_1$ and $b_2$ could be 10 and 0, or 5 and 5, or 7 and 3, since that would yield the same result p (which is $\frac{1}{1+e^{logit}}$, and therefore the same error. That is why you generally do not want to use data that is multicollinear. Therefore you could call a model that does not have collinearity a 'better' model. When you are focussed on the 'p', as is the case for e.g. predictive modeling, it does not matter that the betas are not interpretable, as long as the value of 'p' is as close to the 'true' p as possible. In that case it might be preferable to use multicollinear data. See also "Shmueli, G., 2010. To Explain or to Predict. Statistical Science, 25(3), pp. 289-310"
