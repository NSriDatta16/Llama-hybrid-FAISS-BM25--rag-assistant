[site]: crossvalidated
[post_id]: 440941
[parent_id]: 440940
[tags]: 
You have too many variables for the amount of data you have. A rough rule of thumb is that, in logistic regression, you can have $1$ predictor variable for every $15$ observations in the less commonly occurring category. With $193$ data, you can have at most $97$ instances of yeses or noes. That implies you should use no more than $6$ predictors. That reasoning relates to all variables. Regarding that specific variable as distinct from the others, it is presumably either collinear or completely separates the yeses or noes. To diagnose which might lie behind this, see how many fitting iterations were used, $>10$ is some evidence of separation; and fit an ordinary least squares multiple regression of var 8 against the all rest (it doesn't matter if the assumptions are met) and check the multiple $R^2$ , a value $>.9$ considered problematic. To understand separation, it may help to read my answer here: Why does logistic regression become unstable when classes are well-separated? To understand multicollinearity, it may help to read my answers here: What is the effect of having correlated predictors in a multiple regression model? , and here: How seriously should I consider the effects of multicollinearity in my regression model? Regarding your last question of whether you can conclude that the null holds for the non-significant variables, it might help you to read my answer here: Why do statisticians say a non-significant result means “you can't reject the null” as opposed to accepting the null hypothesis?
