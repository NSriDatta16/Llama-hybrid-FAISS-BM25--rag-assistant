[site]: crossvalidated
[post_id]: 223411
[parent_id]: 163178
[tags]: 
It might depend on the scaling of your features: remember $w$ is, indeed, a geometric hyperplane which normal vector is given by weights on features, and these weights give a sense of importance to features (weights $w_i \approx 0$ mean that feature doesn't contribute much to the hyperplane, the plane being almost constant in it). About other kernels, you have to understand the separating hyperplane is given not in the feature space, but on the kernelized space, whose dimension might even be infinite (in the case of the RBF kernel, for example). It's really hard (or even impossible) to transpose the weights on the kernelized space to the feature space due to the non-parametric nature of the solution of SVMs for most kernels (the downside of the dual formulation).
