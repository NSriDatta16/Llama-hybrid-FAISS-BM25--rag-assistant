[site]: crossvalidated
[post_id]: 483899
[parent_id]: 222584
[tags]: 
I think the difference between regular RNNs and the so-called "gated RNNs" is well explained in the existing answers to this question. However, I would like to add my two cents by pointing out the exact differences and similarities between LSTM and GRU. The original definitions mostly come in the following form (I omitted bias terms, as well as time indices where not needed, in order to reduce some noise in the formulas): $$\begin{align} & \text{GRU} & & \text{LSTM} \\ \\ \boldsymbol{z} &= \mathrm{gate}(\boldsymbol{x}, \boldsymbol{h}) & \boldsymbol{i} &= \mathrm{gate}(\boldsymbol{x}, \boldsymbol{h}) \\ \boldsymbol{r} &= \mathrm{gate}(\boldsymbol{x}, \boldsymbol{h}) & \boldsymbol{o} &= \mathrm{gate}(\boldsymbol{x}, \boldsymbol{h}) \\ && \boldsymbol{f} &= \mathrm{gate}(\boldsymbol{x}, \boldsymbol{h}) \\ \hat{\boldsymbol{h}}[t] &= \phi(\boldsymbol{W} \boldsymbol{x} + \boldsymbol{U} (\boldsymbol{r} \odot \boldsymbol{h}[t-1])) & \boldsymbol{c}[t] &= \boldsymbol{f} \odot \boldsymbol{c}[t-1] + \boldsymbol{i} \odot \phi_1(\boldsymbol{W} \boldsymbol{x} + \boldsymbol{U} \boldsymbol{h}[t-1]) \\ \boldsymbol{h}[t] &= (\boldsymbol{1} - \boldsymbol{z}) \odot \boldsymbol{h}[t-1] + \boldsymbol{z} \odot \hat{\boldsymbol{h}}[t] & \boldsymbol{h}[t] &= \boldsymbol{o} \odot \phi_2(\boldsymbol{c}[t]). \end{align}$$ Note that the GRU has only 2 gates, whereas the LSTM has 3. Also, the LSTM has two activation functions, $\phi_1$ and $\phi_2$ , whereas the GRU has only 1, $\phi$ . This immediately gives the idea that GRU is slightly less complex than the LSTM. Now, if we rewrite the recurrences of both models so that they fit on a single line (instead of spreading over two lines), we would end up with the following equations: \begin{align*} \boldsymbol{h}[t] &= (\boldsymbol{1} - \boldsymbol{z}) \odot \boldsymbol{h}[t-1] + \boldsymbol{z} \odot \phi(\boldsymbol{W} \boldsymbol{x} + \boldsymbol{U} (\boldsymbol{r} \odot \boldsymbol{h}[t-1])) \tag{GRU} \\ \boldsymbol{c}[t] &= \boldsymbol{f} \odot \boldsymbol{c}[t-1] + \boldsymbol{i} \odot \phi_1(\boldsymbol{W} \boldsymbol{x} + \boldsymbol{U} (\boldsymbol{o}\odot \phi_2(\boldsymbol{c}[t-1]))) \tag{LSTM} \end{align*} By putting the recurrences next to each other like this, it should become clear that if all of the follwing equalities hold $$\begin{align} \boldsymbol{f} & = \boldsymbol{1} - \boldsymbol{z} & \boldsymbol{i} & = \boldsymbol{z} & \boldsymbol{o} & = \boldsymbol{r} & \phi_2(\boldsymbol{x}) & = \boldsymbol{x}, \end{align}$$ the GRU and LSTM models implement the same recurrence. The only difference that remains, is that, in the LSTM, the gates are computed from $\boldsymbol{c}[t-1]$ , whereas the GRU directly uses the result of the recurrence, $\boldsymbol{h}[t-1]$ . In this sense, the GRU is a strictly simplified version of the LSTM.
