[site]: crossvalidated
[post_id]: 418779
[parent_id]: 
[tags]: 
What difference it makes to normalize the features

I came across this problem today, I was training a basic logistic regression on the Iris dataset using one vs all approach, It's my own implementation of logistic regression which uses gradient descent and I found out my model accuracy was 66% as a class Iris-versicolor is not linearly separable. Here is the screenshot (one vs all, one class is positive and all others are negative) (notice these features are not normalized) log-loss was some like -: 3.39, 8.42, 0.86 ,1.07, 8.52 on some iterations. After applying mean normalization to feature like : (notice these features are mean normalized now) log loss went down to 0.63 0.47, 0.46, 0.45, 0.45. And I got an accuracy of 96% - 100% (on hyperparameter optimization) Please if someone could shed a light on me so I could understand what's going on in here, what affect normalized features is having on my classifier than unnormalized features. These two classes are still not linearly separable, right? I want to understand it as bad as I want to see aliens in area 51. Thanks Edit: Here is the link to google colab notebook to make things more clearer https://colab.research.google.com/drive/16AaPWCPeV-9i4aZV3XddDaBh7VlUCQmO
