[site]: crossvalidated
[post_id]: 602350
[parent_id]: 402474
[tags]: 
Dividing by (n+1) minimizes the MSE only for normally distributed data. In general, a variance estimator of the form $$s_k^2 = \frac{1}{k}\sum_{i=1}^n (x_i-\overline{x})^2$$ has minimal MSE for $$k=(n-1)\left[ \frac{1}{n}\left(\frac{\mu_4}{\mu_2^2} - \frac{n-3}{n-1}\right) + 1\right]$$ where $\mu_2$ and $\mu_4$ are the second and fourth central moment of the distribution (see here for a proof ), which means that $\mu_4/\mu_2^2$ is the kurtosis . For the normal distribution, it is $\mu_4 = 3\mu_2^2$ and the above formula reduces to $k=n+1$ . As you cannot know whether your data is normally distributed, it is thus not clear whether n+1 actually is the best choice. The bias correction by dividing through n-1, however, is universal and does not require normality. This might be a reason why the bias corrected version is preferred. From Jensen's inequality, it follows that $E(X^4)\geq \big(E(X^2)\big)^2$ , and therefore $\mu_4/\mu_2^2\geq 1$ . The optimal choice for k is thus always greater than n-1: $$k \geq \frac{n-1}{n} \left(1 - \frac{n-3}{n-1}\right) + (n-1) = \frac{2}{n} + (n-1) > n-1$$ There might even be distributions (those restricted to a small range without outliers) for which k=n-1 actually is close to the optimal choice with respect to the MSE. The example of the normal distribution shows, however, that in most practical situations the optimal k is greater than this value and choosing the empirical variance (dividing by n) typically yields an estimate that is on average closer to the true value than the bias corrected empirical variance (dividing by n-1). Unlike the bias correction, this is not guaranteed in all cases, though.
