[site]: crossvalidated
[post_id]: 459661
[parent_id]: 
[tags]: 
Equivalency of regularization among LME, Bayesian and penalized least squares

Suppose that we have a linear mixed-effects (LME) model $\boldsymbol{y}=X\boldsymbol{\beta} + Z\boldsymbol{u} + \epsilon$ where $y$ is an $n \times 1$ vector of responses and $X$ and $Z$ are design matrices for the fixed and random effects of dimensions $n × m$ and $n × q$ , respectively. With the assumptions of $u \sim N(0, R)$ and $\epsilon \sim N(0, \sigma^2I)$ , $u$ can be predicted through BLUP. I believe that $u$ can be equivalently predicted through two other approaches. One is the hierarchical Bayesian framework, $\boldsymbol{y}|Z,\beta,Z,u \sim N(X\boldsymbol{\beta}+ Z\boldsymbol{u}, \sigma^2I)\\ u\sim N(0, R)$ And the other approach is penalized least squares (PLS), $\underset{\beta,u}{argmin} (y-X\beta-Zu)^T(y-X\beta-Zu) + \lambda u^TR^{-1}u$ , where $R^{-1}$ and $\lambda$ are the penalty matrix and parameter, respectively. My question is, does the equivalency among the three approaches still hold when the assumption of $\epsilon ~ N(0, \sigma^2I)$ is relaxed? For example, what happens when there is a serial correlation structure (e.g., AR(2) or ARMA(1,1)) in residuals $\epsilon$ ? I guess that the estimation for the serial structure can be incorporated as part of the iterative process for LME and the Bayesian framework. How about PLS?
