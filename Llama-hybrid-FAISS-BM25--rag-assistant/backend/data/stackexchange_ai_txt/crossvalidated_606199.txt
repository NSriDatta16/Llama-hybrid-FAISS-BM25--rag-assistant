[site]: crossvalidated
[post_id]: 606199
[parent_id]: 558777
[tags]: 
I have looked into ROUGE-N implementations and found that there are three ways to compute ROUGE: (a) compute a ROUGE for each reference summary and then take their maximum value [1,2,3,4] (b) compute the micro average of ROUGEs, which is the first definition in the question [4] (c) compute the macro average of ROUGEs [2,4] I think the possible interpretation is (a) or (b). Although (b) seems to be the original definition, (a) and (c) are prevalent in the implementations. references [1] https://github.com/google-research/google-research/tree/master/rouge implementation by google research [2] https://torchmetrics.readthedocs.io/en/stable/text/rouge_score.html pytorch wrapper of [1] [3] https://huggingface.co/spaces/evaluate-metric/rouge huggingface wrapper of [1] [4] https://github.com/li-plus/rouge-metric/blob/master/rouge_metric/RELEASE-1.5.5/ROUGE-1.5.5.pl
