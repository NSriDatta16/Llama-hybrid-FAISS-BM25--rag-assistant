[site]: crossvalidated
[post_id]: 378474
[parent_id]: 316522
[tags]: 
You may not need to "normalize" your rankings like you are doing it. Just avoid the average and division steps, and compare the ranks directly: Let's assume you have 2 models, from which n ranks are generated by each model. Then, each rank can be evaluated, say using Precision@K, NDCG@K etc, and only then you can measure the quality of each model by averaging its ranks. The compare the averages from each model, which gives you a first good analysis. You can also perform statistical analysis to check about how confident you think a model is better than the other, but for now, you are fine in comparing their means. The term K, for those measurements mentioned, means the cut-off of interest, up to which position you are interested in evaluating. Other measures, such as MAP, evaluate the whole rank regardless of the cut-off.
