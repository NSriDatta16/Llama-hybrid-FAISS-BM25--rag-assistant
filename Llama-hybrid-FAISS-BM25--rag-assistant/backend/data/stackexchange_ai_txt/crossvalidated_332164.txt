[site]: crossvalidated
[post_id]: 332164
[parent_id]: 330179
[tags]: 
You can actually dodge the question with a fully Bayesian approach. If you go full Bayes, the question is not "can I estimate X?" (which is basically always true) but "how precisely can I estimate X?". And a reliable estimate of the uncertainty in X is a part of the result of fitting the model. In this sense, fitting an overly complex Bayesian model is safe: you fit the model and if the posterior uncertainty in some of the parameters is too large you know you need more data or a simpler model. This is in contrast to lme4 where AFAIK estimates can be unrealiable and a too complex model may overfit. Also note that the posterior uncertainty is not simply a function of the size of the dataset, but also of its content - if the groups (years) are very similar, the uncertainty will be smaller than if they differe a lot. Further, how much uncertainty is acceptable depends on your intended use of the model's results, so I don't think you can make a good general rule of how much data you need. If you want to use full Bayes rstanarm provides methods that are almost drop-in replacements for lme4 (see the vignette ). It is however possible that rstanarm will be too slow for your dataset (hard to guess without actually running it). If this is so, INLA will give you almost the same results with much less computing power required (and should be able to handle your model with little or no modification). The classical reference is Gelman et al.: Bayesian Data Analysis, 3rd Edition
