[site]: crossvalidated
[post_id]: 63080
[parent_id]: 63074
[tags]: 
Modeling to me involves specifying a probabilistic framework for observed data with estimable parameters that can be used to discern valuable differences in observable data when they exist. This is called power. Probabilistic models can be used for either prediction or inference. They can be used to calibrate machinery, to demonstrate deficiency in return on investment, to forecast weather or stocks, or simplify medical decision making. A model does not necessarily need to be built. In an isolated experiment, one can use a non-parametric modeling approach, such as the t-test to determine whether there is a significant difference in means between two groups. However, for many forecasting purposes, models can be built so as to detect changes in time. For instance, transition based Markov models can be used to predict up and down swings in market value for investments, but to what extent can a "dip" be considered worse than expected? Using historical evidence and observed predictors, one can build a sophisticated model to calibrate whether observed dips are significantly different from those which have historically been sustained. Using tools like control charts, cumulative incidence charts, survival curves, and other "time based" charts, it's possible to examine the difference between observed and expected events according to a model based simulation and call in judgement when necessary. Alternately, some models are "built" by having the flexibility to adapt as data grow. Twitter's detection of trending and Netflix's recommendation system are prime examples of such models. They have a general specification (Bayesian Model Averaging, for the latter) that allows a flexible model to accommodate historical shifts and trends and recalibrate to maintain best prediction, such as the introduction of high impact films, a large uptake of new users, or a dramatic shift in film preference due to seasonality. Some of the data mining approaches are introduced because they are highly adept at achieving certain types of prediction approaches (again, the issue of obtaining "expected" trends or values in data). K-NN is a way of incorporating high dimensional data and inferring whether subjects can receive reliable predictions simply due to proximity (whether from age, musical taste, sexual history, or some other measurable trait). Logistic regression on the other hand can obtain a binary classifier, but is much more commonly used to infer about the association between a binary outcome and one or more exposures and conditions through a parameter called the odds ratio. Because of limit theorems and its relationship to the generalized linear models, odds ratios are highly regular parameters that have a "highly conserved" type I error (i.e. the p-value means what you think it means).
