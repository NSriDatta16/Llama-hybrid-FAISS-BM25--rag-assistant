[site]: crossvalidated
[post_id]: 233693
[parent_id]: 233688
[tags]: 
If the underlying process is a random walk with or without a drift, then no, you can't get rid of this term. For instance, if the underlying process (data generating process, or DGP) is as follows: $$y_t=a+ y_{t-1}+\varepsilon_t,$$ where $\varepsilon_t$ is not correlated with its past and its variance is stable at $\sigma^2$, then you have the following: $$y_t=a+(a+ y_{t-2}+\varepsilon_{t-1}) +\varepsilon_t= 2a+ y_{t-2}+\varepsilon_{t-1}+\varepsilon_t$$ Let's look at the variance of the error term: $$Var(\varepsilon_{t-1}+\varepsilon_t)=Var(\varepsilon_{t-1})+Var(\varepsilon_t)=2\sigma^2$$ Basically, your variance will increase linearly with your forecast horizon increasing. You can't get rid of this. However, do you really know your underlying process (DGP)? Do you know for sure that it's a random walk? That's where the trick lies in. If for some reason your true DGP is not the one stated above but some variation of it such as ARIMA(0,1,1), you have a chance to slightly decrease the error by estimating the moving average (MA) coefficients of error terms. The error variance is additive only when the errors are uncorrelated. If they're negatively correlated the variance could be slightly smaller (or bigger) Another "trick" is to misspecify the model as autoregressive (AR). Often it's very hard to differentiate between AR and random walk. So, you could estimate AR process instead. Its error variance is not increasing as in a random walk (differenced) process. Obviously, the reality will show up one day in the form of large forecast (out of sample) errors, but you can keep re-estimating your model, so that it will not be so obvious.
