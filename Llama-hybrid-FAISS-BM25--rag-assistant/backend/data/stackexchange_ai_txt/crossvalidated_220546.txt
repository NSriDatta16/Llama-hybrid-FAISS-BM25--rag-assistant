[site]: crossvalidated
[post_id]: 220546
[parent_id]: 
[tags]: 
Understanding the weights of a neural network

I've been working with an artificial neural network in Torch, to solve a binary classification problem. I've had several troubles by using the mini-batch gradient update. Each element of my dataset can be -1 (false) or +1 (true). I cannot explain why, but my mini-batch execution tests produce sufficient results (Matthews correlation coefficient in the test set around +0.30) if the percentage of negative elements in the training set is 49% . On the contrary, if it is 50% or greater, the neural network will produce only ones as output, and if it's lower than 49%, the neural network will produce only zeros as output. In all the cases I'm using a mini-batch size of 20. I decided to take a look to the neural network weights, after training. This is the heatmap of the weights of my input layer neurons (82x100) for my 49% case: And this is the heatmap of the weights of my input layer neurons (82x100) for my 50% case: As you can see, the first one looks more stable: almost all the values are around 0, and the extremes are very few. On the contrary, the second one looks quite "crazy", with apparently random values always extremes around -1 and +1. What are these images telling me? Is there some addition information I can get from these images? Why does the second image look so polarized towards the extreme values?
