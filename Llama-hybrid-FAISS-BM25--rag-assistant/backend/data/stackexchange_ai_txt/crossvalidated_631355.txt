[site]: crossvalidated
[post_id]: 631355
[parent_id]: 631352
[tags]: 
Multicollinearity might mess things up and might not. It really depends on what you're trying to do and what do you consider as risks... If you want to use Logistic Regression for prediction (classification): Multicollinearity won't distort the model's output. You'll have the same numbers with features' removal or without it. One thing that's many times overlooked, is that multicollinearity might lead to numerical errors, as there's no limit to how big your coefficients may be. If one of your coefficients is at 1,000,000 the tiniest gap could distort your model's performance. If you're using Logistic Regression as a descriptive or inference model: Multicollinearity is a serious issue. Your coefficients would probably not make any sense. Overall, many scholars advice to remove features having perfect multicollinearity, as it may only harm your model. Reduction of features with a high VIF is one common way, there are plenty others.
