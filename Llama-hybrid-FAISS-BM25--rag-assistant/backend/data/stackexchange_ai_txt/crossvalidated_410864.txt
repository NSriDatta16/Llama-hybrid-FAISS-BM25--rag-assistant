[site]: crossvalidated
[post_id]: 410864
[parent_id]: 
[tags]: 
Is a Bayesian posterior kind of like the marginal distribution of a frequentist estimator?

I've been thinking a lot about the relationships between various concepts like hypothesis testing, posterior distributions, and estimators. If I understand correctly, a frequentist estimator $\hat\theta$ aims to approximate an unknown but constant $\theta$ based on some observed data $X$ , or in other words it attempts to optimize some pointwise metric by marginalizing out $X$ (for example unbiasedness, $E_X[\hat\theta(X)-\theta|\theta]=0$ for all $\theta$ ). Confidence intervals are also implicitly conditioned on the unobserved $\theta$ (i.e. $P(\theta\in[a(X),b(X)]|\theta)=0.95$ ). On the other hand, Bayesian posteriors attempt to marginalize out the unknown $\theta$ (as opposed to $X$ ) according to a prior distribution $P(\theta)$ , replacing the "pointwise" estimate $\hat\theta$ with a single posterior distribution $P(\theta|X)$ . The answer is simpler because we are assuming more than in the frequentist approach. Both methods assume the same known data generating distribution $X\sim D(\theta)$ . Am I correct in my interpretation that frequentist methods try to marginalize out $X$ while Bayesian methods try to marginalize out $\theta$ ? I found another (the same?) example of this "symmetry": https://en.wikipedia.org/wiki/Admissible_decision_rule#Generalized_Bayes_rules
