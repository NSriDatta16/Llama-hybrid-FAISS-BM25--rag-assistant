[site]: crossvalidated
[post_id]: 309305
[parent_id]: 309302
[tags]: 
genetic approaches are generally fairly junky. At least, since we have the gradient available, typically, using the gradient will learn faster. It's true that Karpathy/OpenAI have proposed to use ES ("evolutionary strategies"), and showed that it learns faster, in terms of elapsed time, but they are parallelizing onto zillions of machines at a time. On a single machine, ES runs something like ten times slower than eg PG ("Policy gradients", a gradient approach), or perhaps TRPO ("trust region policy optimization"). Just with eg 100 computers in parallel, the ability to parallelize onto 100 machines, means that overall, ES will train 10 times faster than TRPO, albeit at huge expense, in terms of hardware cost. NEAT only works for simple-tastic tasks. Look at the paper. They detect a big circle, in a picture with a big circle and a small circle. If you must use genetic methods, ES is probably as good approach as any, since you have the resources/documentation of OpenAI behind it, eg https://blog.openai.com/evolution-strategies/ Edit: in response to questions: 1. why does neat only work for simple tasks? Not using a gradient makes it very hard to know in which direction to move, in parameter hyperspace. NEAT doesnt handle functions that are not very smooth, eg "Critical Factors in the Performance of HyperNEAT", van der Berg and Whiteson, 2013, does some analysis on this point, using the term 'fracture' to describe this concept. (Admittedly this is for HyperNEAT, rather than basic NEAT). http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/vandenberggecco13.pdf 2. How to backprop in RL? You run an episode, and get a reward. The reward is then the supervised training signal, that you backprop through the network, given the sequence of actions and states that you walked through during the episode as the inputs. Highly recommend David Silver's lectures on the topic https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT Alternatively, Karpathy's blog post on Policy Gradients to play Pong is pretty short and easy to read/understand, http://karpathy.github.io/2016/05/31/rl/
