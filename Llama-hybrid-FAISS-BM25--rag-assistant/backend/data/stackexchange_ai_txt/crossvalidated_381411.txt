[site]: crossvalidated
[post_id]: 381411
[parent_id]: 
[tags]: 
Different result while training in CPU and GPU for Google BERT

I was running few examples exploring the pytorch version of Google's new pre-trained model called the Google BERT . I ran the example in both CPU as well as GPU machines. I run the following code for sentence pair classification using the MRPC data as given in the readme python run_classifier.py \ --task_name MRPC \ --do_train \ --do_eval \ --do_lower_case \ --data_dir $GLUE_DIR/MRPC/ \ --bert_model bert-base-uncased \ --max_seq_length 128 \ --train_batch_size 32 \ --learning_rate 2e-5 \ --num_train_epochs 3.0 \ --output_dir /tmp/mrpc_output/ I get the following results. In the CPU machine: 12/06/2018 02:01:20 - INFO - __main__ - label: 0 (id = 0) 12/06/2018 02:01:21 - INFO - __main__ - ***** Running evaluation ***** 12/06/2018 02:01:21 - INFO - __main__ - Num examples = 408 12/06/2018 02:01:21 - INFO - __main__ - Batch size = 8 12/06/2018 02:04:24 - INFO - __main__ - ***** Eval results ***** 12/06/2018 02:04:24 - INFO - __main__ - eval_accuracy = 0.8259803921568627 12/06/2018 02:04:24 - INFO - __main__ - eval_loss = 0.40949390638692706 12/06/2018 02:04:24 - INFO - __main__ - global_step = 345 12/06/2018 02:04:24 - INFO - __main__ - loss = 0.22680548837651376 In the GPU machine: 12/06/2018 04:43:55 - INFO - __main__ - label: 0 (id = 0) 12/06/2018 04:43:55 - INFO - __main__ - ***** Running evaluation ***** 12/06/2018 04:43:55 - INFO - __main__ - Num examples = 408 12/06/2018 04:43:55 - INFO - __main__ - Batch size = 8 12/06/2018 04:44:49 - INFO - __main__ - ***** Eval results ***** 12/06/2018 04:44:49 - INFO - __main__ - eval_accuracy = 0.678921568627451 12/06/2018 04:44:49 - INFO - __main__ - eval_loss = 0.5874785494570639 12/06/2018 04:44:49 - INFO - __main__ - global_step = 345 12/06/2018 04:44:49 - INFO - __main__ - loss = 0.5948529090570367 Why are the results coming different? And also there is a very significant difference.
