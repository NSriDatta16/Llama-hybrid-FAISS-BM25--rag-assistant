[site]: crossvalidated
[post_id]: 181155
[parent_id]: 
[tags]: 
Geometric understanding of PCA in the subject (dual) space

I am trying to get an intuitive understanding of how principal component analysis (PCA) works in subject (dual) space . Consider 2D dataset with two variables, $x_1$ and $x_2$, and $n$ data points (data matrix $\mathbf X$ is $n\times 2$ and is assumed to be centered). The usual presentation of PCA is that we consider $n$ points in $\mathbb R^2$, write down the $2\times 2$ covariance matrix, and find its eigenvectors & eigenvalues; first PC corresponds to the direction of maximum variance, etc. Here is an example with covariance matrix $\mathbf C = \left(\begin{array}{cc}4&2\\2&2\end{array}\right)$. Red lines show eigenvectors scaled by the square roots of the respective eigenvalues. $\hskip 1in$ Now consider what happens in the subject space (I learned this term from @ttnphns), also known as dual space (the term used in machine learning). This is a $n$-dimensional space where the samples of our two variables (two columns of $\mathbf X$) form two vectors $\mathbf x_1$ and $\mathbf x_2$. The squared length of each variable vector is equal to its variance, the cosine of the angle between the two vectors is equal to the correlation between them. This representation, by the way, is very standard in treatments of multiple regression. In my example, the subject space looks like that (I only show the 2D plane spanned by the two variable vectors): $\hskip 1in$ Principal components, being linear combinations of the two variables, will form two vectors $\mathbf p_1$ and $\mathbf p_2$ in the same plane. My question is: what is the geometrical understanding/intuition of how to form principal component variable vectors using the original variable vectors on such a plot? Given $\mathbf x_1$ and $\mathbf x_2$, what geometric procedure would yield $\mathbf p_1$? Below is my current partial understanding of it. First of all, I can compute principal components/axes through the standard method and plot them on the same figure: $\hskip 1in$ Moreover, we can note that the $\mathbf p_1$ is chosen such that the sum of squared distances between $\mathbf x_i$ (blue vectors) and their projections on $\mathbf p_1$ is minimal; those distances are reconstruction errors and they are shown with black dashed lines. Equivalently, $\mathbf p_1$ maximizes the sum of the squared lengths of both projections. This fully specifies $\mathbf p_1$ and of course is completely analogous to the similar description in the primary space (see the animation in my answer to Making sense of principal component analysis, eigenvectors & eigenvalues ). See also the first part of @ttnphns'es answer here . However, this is not geometric enough! It doesn't tell me how to find such $\mathbf p_1$ and does not specify its length. My guess is that $\mathbf x_1$, $\mathbf x_2$, $\mathbf p_1$, and $\mathbf p_2$ all lie on one ellipse centered at $0$ with $\mathbf p_1$ and $\mathbf p_2$ being its main axes. Here is how it looks like in my example: $\hskip 1in$ Q1: How to prove that? Direct algebraic demonstration seems to be very tedious; how to see that this must be the case? But there are many different ellipses centered at $0$ and passing through $\mathbf x_1$ and $\mathbf x_2$: $\hskip 1in$ Q2: What specifies the "correct" ellipse? My first guess was that it's the ellipse with the longest possible main axis; but it seems to be wrong (there are ellipses with main axis of any length). If there are answers to Q1 and Q2, then I would also like to know if they generalize to the case of more than two variables.
