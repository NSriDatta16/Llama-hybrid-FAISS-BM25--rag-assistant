[site]: crossvalidated
[post_id]: 340215
[parent_id]: 212961
[tags]: 
This is relatively old question, but I'll add my 5 cents for the people who (like myself) came across it searching for something related. An alternative approach for dealing with zero emission probabilities is to "close the vocabulary". An idea is to define "rare" words in training set - those that appear less than predefined number of times and substitute them with "word classes" before the model is trained. When applying a model to a new sequence of words, all words that were not seen in a training set are converted to "word classes" as well (effectively considering them as "rare"). It guarantees that for a model there will be no unseen words. The rules for producing "word classes" from words have to be selected manually (which is a downside). For instance, in a (probably) first article when this approach was utilized (Bikel, D.M., Schwartz, R. & Weischedel, R.M. Machine Learning (1999) 34: 211.; https://link.springer.com/article/10.1023/A:1007558221122 ; http://curtis.ml.cmu.edu/w/courses/index.php/Bikel_et_al_MLJ_1999 ) an examples of classes are: Word Feature | Example Text | Intuition -----------------------|------------------------|----------------------------------------- twoDigitNum | 90 | Two-digit year fourDigitNum | 1990 | Four digit year containsDigitAndAlpha | A8956-67 | Product code containsDigitAndDash | 09-96 | Date containsDigitAndSlash | 11/9/89 | Date containsDigitAndComma | 23,000.00 | Monetary amount containsDigitAndPeriod | 1.00 Monetary | amount, percentage otherNum | 456789 | Other number allCaps | BBN | Organization capPeriod | M. | Person name initial firstWord | first word of sentence | No useful capitalization information initCap | Sally | Capitalized word lowerCase | can | Uncapitalized word other | , | Punctuation marks, all other words An example of pre-processed tagged sentence from a training set (from lectures of Michael Collins): "Profits/NA soared/NA at/NA Boeing/SC Co./CC ,/NA easily/NA topping/NA forecasts/NA on/NA Wall/SL Street/CL ,/NA as/NA their/NA CEO/NA Alan/SP Mulally/CP announced/NA first/NA quarter/NA results/NA ./NA" is transformed (with some hypothetical set of tags and "rare words") into (substituted words as shown in bold ) " firstword /NA soared/NA at/NA initCap /SC Co./CC ,/NA easily/NA lowercase /NA forecasts/NA on/NA initCap /SL Street/CL ,/NA as/NA their/NA CEO/NA Alan/SP initCap /CP announced/NA first/NA quarter/NA results/NA ./NA" It is still possible that in training set not all pairs of "tag -> word/word class" are seen, which makes it impossible for a certain word or word class being tagged with those tags. But that doesn't prevent of those words to be tagged with other tags - unlike when there is a word that was not seen in a training set.
