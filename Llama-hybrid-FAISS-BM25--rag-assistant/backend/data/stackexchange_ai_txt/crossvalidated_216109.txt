[site]: crossvalidated
[post_id]: 216109
[parent_id]: 214908
[tags]: 
It shouldn't be too complicated to do. Haven't read the article mentioned, here's my recipe: Variational Auto Encoders Online demo with morphing faces: http://vdumoulin.github.io/morphing_faces/online_demo.html and https://jmetzen.github.io/2015-11-27/vae.html for teh codez. Basically, this gives you a way to parametrize the 'style' in your case, for example let's say how wide or fuzzy should the brush stroke be. Stuff that depends on the particular style you are trying to emulate. In the example above different 'morphed' or 'imagined' faces are a function of the parameters in the latent space. In the image below that would be what you get by changing stuff at the 'code' level. Here's the basic idea: original image left, stylised version of the same image on the right: Now, in theory, if you would train such a model on a normal image and a stylised image as a target and add convolutions, you should be able to learn the kernel filters that correspond to the type of "brush strokes" that the artist uses. Of course, that means that you need to have a few examples of images in both original and stylized versions. Such a dataset would be nice to donate to the community - if you end up doing this I'd be very keen to see this sort of work. Good luck! The wiki article on auto encoders would be a good starting point: https://en.wikipedia.org/wiki/Autoencoder
