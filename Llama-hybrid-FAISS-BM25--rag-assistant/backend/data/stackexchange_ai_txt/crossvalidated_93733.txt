[site]: crossvalidated
[post_id]: 93733
[parent_id]: 93661
[tags]: 
Start with some simple pairwise measure/test and see how that does. Random forests are commonly used for genomic data and can be used for feature selection (importance scores). Variations on this methods life artificial contrasts (ACE), using out of bag cases to valuate splitter importance and roughly balanced bagging or weighted rf help deal with some of the oddities of data sets you describe. It is also common to calculate gene or pathway burden scores/summaries which can be as simple as summing the number of non reference SNPs in each gene or calculating a distance from the mean. Summarizing by pathway is a really interesting and promising field and there are a number of papers out there on "subnetwork markers" or pathway burden. Finally methods like Eigenstrat may be interesting because they correct for confounding population stratification by doing eigen decomposition dimensionality reduction and identifying dimensions that have to do with admixture etc. I'm not sure if you can just take the top n non confounding dimensions after correction for further analysis but it would be worth a try.
