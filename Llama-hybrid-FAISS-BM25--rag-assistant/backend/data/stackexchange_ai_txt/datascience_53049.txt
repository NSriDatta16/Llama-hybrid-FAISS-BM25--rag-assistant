[site]: datascience
[post_id]: 53049
[parent_id]: 53045
[tags]: 
I suggest watching 3blue1brown's YouTube series on neural networks to get a better conceptual understanding of what's going on. It's not that the sigmoid function itself transforms the dimension of a vector - the sigmoid function is just used as an activation function that affects the output of each individual node - it's the number of units/nodes you have in a layer that does what you're describing. If you only have one unit in your output layer it will only output one number between 0 and 1. If you have 100 units in a dense layer it will output 100 numbers, each between 0 and 1.
