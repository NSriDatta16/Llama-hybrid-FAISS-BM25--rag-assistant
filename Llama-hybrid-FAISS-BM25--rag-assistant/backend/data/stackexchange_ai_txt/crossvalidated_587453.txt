[site]: crossvalidated
[post_id]: 587453
[parent_id]: 
[tags]: 
Is there a standard name for this variant of KL divergence?

Since KL divergence can be decomposed as \begin{equation*} D_{\mathrm{KL}}(q \| p) = H(p \| q) - H(p), \end{equation*} I wonder if there exists a weighted version of KL divergence, i.e., \begin{equation*} D^{\gamma}_{\mathrm{KL}}(q \| p) = H(p \| q) - \gamma H(p), \end{equation*} BTW, the motivation of this weighted version is that I conjuncture that the weighted KL divergence is highly related to the temperature scaling technique applied in confidence calibration and knowledge distillation tasks. Some references to temperature scaling: Distilling the Knowledge in a Neural Network On Calibration of Modern Neural Networks Temperature Check: Theory and Practice for Training Models with Softmax-Cross-Entropy Losses
