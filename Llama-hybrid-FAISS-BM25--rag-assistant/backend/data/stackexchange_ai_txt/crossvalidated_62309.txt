[site]: crossvalidated
[post_id]: 62309
[parent_id]: 62253
[tags]: 
On a toy example in 2d or 3d, it shouldn't make much of a difference, it just adds some redundancy to your data: all your points are on an odd, (d-1) dimensional hyperplane. So are the cluster means. And distance in this (d-1) dimensional hyperplane is a linear multiple of the same distance, so it doesn't change anything. If you artificially construct such data, e.g. by doing $(x,y)\mapsto(x,y,x+y)$ then you do distort space, and emphasize the influence of $x$ and $y$. If you do this to all variables it does not matter; but you can easily change weights this way. This empasizes the known fact that normalizing and weighting variables is essential . If you have correlations in your data, this is more important than ever. Let's look at the simplest example: duplicate variables. If you run PCA on your data set, and duplicate a variable, this effectively means putting duplicate weight on this variable. PCA is based on the assumption that variance in every direction is equally important - so you should, indeed, carefully weight variables (taking correlations into account, also do any other preprocessing necessary) before doing PCA.
