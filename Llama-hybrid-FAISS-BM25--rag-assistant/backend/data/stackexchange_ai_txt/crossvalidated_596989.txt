[site]: crossvalidated
[post_id]: 596989
[parent_id]: 102979
[tags]: 
Do you calculate the likelihood for all the possible values the random effect can take? Does it mean that you let the random effect take all possible values and just pick the most probably It’s kind of both. This is a common technique in statistics that’s often not explained explicitly: you “integrate out” a random variable that you don’t know or don’t observe, so you can get the distribution of the random variable you’re interested in. I’ll take a step back to basic probability theory to draw a parallel to the expected value. Remember the definition of expected value for discrete random variables is $$\mathbb{E}(X) = \sum_{x \in \mathcal{X}} x P(X = x).$$ To get the average value ( expectation ) of a random variable, you take a weighted average of every possible value of X. The weight given to each $x$ is how likely each event is to occur, e.g. $P(X=x)$ . Remembering that integrals act like sums over infinite sets, the “weighted average” of a continuous variable is $$\mathbb{E}(X) = \int_{-\infty}^{\infty} x f(x)dx.$$ Like $P(X=x)$ did in the discrete case, the $f(x)$ term tells us how much weight to assign to each value $x$ when we “add” everything together with the integral. It’s the same idea in your example. We want to know the distribution $f(y | x; \beta, \sigma_c)$ , but we don’t observe $c$ . So, we “integrate out” $y$ ’s dependence on $c$ : $$ \int_{-\infty}^{+\infty}f(y|x;c,\beta) (1/\sigma_c)\phi(c/\sigma_c)~\mathrm dc. $$ Notice that only $c$ varies in the integral. $y, x$ , and $\beta$ are fixed. We’re plugging in every possible value of $c$ (using an integral over the real numbers), weighing each value by its likelihood ( $\phi(c/\sigma_c))$ , and averaging over everything. That gives us the most plausible distribution of $y | x; \beta$ after we’ve removed (“integrated out”) everything we know about $c$ .
