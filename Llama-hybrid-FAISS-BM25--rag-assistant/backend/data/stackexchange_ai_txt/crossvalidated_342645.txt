[site]: crossvalidated
[post_id]: 342645
[parent_id]: 342512
[tags]: 
Thanks for the question as it leads to a teaching moment .... An often overlooked caveat when dealing with data is the assumption that the parameters to be optimized are invariant . In practice with time series data , the question should be asked "have the parameters changed over time" or equivalently do we have too much data ?. Tong in 1980 considered this issue with the following idea in mind . Following is a data set from his work suggesting how to deal with this issue. with acf here If one identified a model based upon the 40 values, we get and a residual plot of The residual plot suggests model deficiency which might be treated by incorporating variance change detection ala Tsay: Outliers, Level Shifts, and Variance Changes in Time Series or more simply parameter change detection via a sequential use of the Chow test for constant parameters where the grouping is based upon a search. The acf of the residuals does not warn us about the violation. If we estimate the AR(1) model separately for different possible grouping, we get suggesting the optimal partitioning is 1-22 vs 23-40. The two separate estimations are here and here As to whether or not the segmentation (discarding the first 22 observations) yields a more accurate prediction of some future values is unknown as it all depends on the origin of the forecast, the length of the forecast and the dictatorial impact of these future values. As they say, “Results may vary!“ as only your data knows whether or not your analysis has been correctly/sufficiently accomplished. All I know is that one needs to know and test the assumptions of any model imposed on the data. If one simulates an ar(1) process with a value of .9 and then follows with realizations from a process using -.9 ... we globally conclude using the ensemble that the series is free of auto-correlation which is quite flawed.
