[site]: crossvalidated
[post_id]: 152916
[parent_id]: 
[tags]: 
hypothesis testing with uncertainty in variables

This is one of those questions that are easier to be explained with an example. Suppose we have the following data (made in R) #Generate artificial data# DateMeans Let’s say we are measuring the size of two objects labelled “a” and “b” (the column “type”), and we are interested whether one is bigger than the other. They are normally distributed so a standard t-test will suffice. However, suppose also that these object have a time-stamp (column dateAvg), each with some degree of uncertainty (column dateSd) described as a Gaussian error. If I want to now the average size of type “a” for a given interval (let’s say between 1300 and 1400), I could simply simulate the dates as follow: nsim =interestBlock[1]&simdates This will give me a distribution of means that will include the uncertainty of our summary statistics. I think there is nothing wrong with this. But suppose I want to test whether objects of type “a” are different in size in comparison with objects of type “b”. I don’t think comparing the means obtained from the simulation is fair, as the significance will change as a function of the number of simulations. All I can think of is to compute a t-test for each iteration, something like the following: nsim=1000 interestBlock=c(1400,1500) statistic =interestBlock[1]&simdates And then get a distribution of p-values (that I frankly find useless). Is there a way to get a single p-value? The only alternative I can think of is to get a 95% confidence interval of the difference in mean, and see if this includes no difference in mean or not… Many thanks in advance, Enrico
