[site]: datascience
[post_id]: 116334
[parent_id]: 
[tags]: 
Why backpropagation is done in every epoch when loss is always scalar?

I understand the backpropagation algorithm that it calculates the derivate of loss with respect to all the parameters in the neural network. My question is this derivate is constant right because the loss function is not changing. The weights are changing in every epoch but they are scalar values. The output of the loss function is also a scalar value. So why do we need to calculate the derivate(or backpropagate) in every epoch? I think if it is done once, it is enough and in the subsequent iteration or epoch we can just calculate its scalar value because calculating the derivate with respect to a scalar value does not make any sense.
