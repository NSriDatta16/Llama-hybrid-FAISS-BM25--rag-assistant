[site]: datascience
[post_id]: 126973
[parent_id]: 126969
[tags]: 
if you carefully calibrated your model and it is not overfitting, e.g. you used k-folds cross validation or at least a training, validation, and test set, then you can find cases where the residuals are high for positive classes and begin a search to find "wrong labels". domain expertise is essential here or at least devising a basic theory of potential reasons for "wrong labels". you already started to examine this when you mention that some people may have forgotten (e.g. has mental illness, dementia, larger bulk of spending in that time period so it was forgotten etc) or some people may be trying to get a quick buck (e.g. lower income, higher fraud reporting rate, lower credit score etc.) by first developing theories, then identifying samples when a model fails (large residuals), you can see if your theories align with features that are correlated with those failures. another consideration would be to develop a random forest model. Because each tree is composed of different samples/features you can infer which samples are associated with lowering the predictive power of that individual tree and ultimately calculate that on average for each sample. This assumes that you have many more correct labels than wrong labels, and that relationships used to identify correct labels are partially unique compared to the wrong labels. for a simple decision tree you could examine the leaf nodes and see if any leaf node has a clustering of samples with similar feature importance that correlate with your theories on fraud (lower credit score, mental illness etc). for your boosted model you can do something similar with SHAP package. basically do some unsupervised clustering of each sample using the feature importance. maybe you will get some clustering of samples where for "wrong labels" you see a clustering of feature importance for certain features like mentioned above. review is essential so you don't simply identify outliers that are actual fraud regardless of how you approach it. nothing is perfect and like any prediction if you employ methods to identify "wrong labels" you will likely classify some authentic fraud cases as "wrong" hope this helps
