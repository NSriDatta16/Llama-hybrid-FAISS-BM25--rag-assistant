[site]: crossvalidated
[post_id]: 535956
[parent_id]: 532813
[tags]: 
"If neural networks are optimizing a proper scoring rule like cross-entropy loss, how can this be?" This is likely to be traditional over-fitting of the training data. A deep neural network can implement any mapping that a radial basis function neural network can implement (they are both universal aproximators). Consider a problem with a small data set and a narrow width for the Gaussian radial basis functions. It is possible that you might be able to place a basis function directly over each positive pattern, such that the value has decreased to nearly zero by the time you get to the nearest negative pattern. This model will give a probability of class membership of essentially zero or one for every training pattern (probably way over-confident) and a training set cross-entropy of zero. This means there will also be a zero cross-entropy solution for a suitably large deep neural network as well (the good thing is that solution is a lot harder to find for a DNN - sometimes local minima are a good thing). Making architecture or hyper-parameter choices gives more ways in which to over-fit the data, but I suspect the largest part of the problem is traditional over-fitting of the training set, unless steps are taken to avoid it. BTW using cross-entropy as the model selection criterion for tuning the model is not without it's own problems, for instance if you have one very confident miss-classification, then the entire cross-entropy is dominated by the contribution of that one test example. Something a little less sensitive, like the Brier score might be better (if less satisfying).
