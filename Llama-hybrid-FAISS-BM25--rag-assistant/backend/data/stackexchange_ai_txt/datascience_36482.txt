[site]: datascience
[post_id]: 36482
[parent_id]: 
[tags]: 
LSTM training/prediction with no starting sequence

ML newbie here. As an exercise, I'm trying to build a character based language model based on a simple 1 layer LSTM. Based on what I've learned about LSTMs, a common usage is to take in a sequence of characters and then predict the next character. What I don't fully understand is how I would go about predicting the very first character when there's no preceding sequence yet (or, by extension, predicting a character when there's not a long enough preceding sequence as input to the LSTM units). The best solution I can thing of is to reserve a special character in the vocabulary to represent the abscence of any characters. A toy example: Full training corpus: "foo" LSTM unit count: 2 "Absent character" symbol: ABSENT "End of file" symbol: EOF Training inputs: sample: [ABSENT, ABSENT] label: 'f' sample: [ABSENT, 'f'] label: 'o' sample: ['f', 'o'] label: 'o' sample: ['o', 'o'] label: EOF My question is: what's the best practice for doing this type of thing? Am I on the right track?
