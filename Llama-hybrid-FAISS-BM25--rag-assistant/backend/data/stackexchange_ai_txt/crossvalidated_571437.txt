[site]: crossvalidated
[post_id]: 571437
[parent_id]: 
[tags]: 
Expand a Gaussian numerically with a basis of functions

I am asking a question that is math, but I am not sure if machine learning can help too, so posting here! I have a univariate Gaussian function $\phi_{\mu, \sigma}(x)$ , with mean $\mu$ and variance $\sigma^2$ . I need to do some integral of the type: $\int dx \phi_{\mu, \sigma}(x) F(x, ...)\ (1)$ where $F$ is some known function. Doing integral (1) can be expensive, and I need to know it for a variety of $(\mu, \sigma)$ values. So, what I want to do is the following: Expand in some basis my Gaussians (or some weakly non-Gaussian function) $\phi_{\mu, \sigma}(x) = \sum_i A_i(\mu, \sigma) \psi_i(x)\ (2)$ where $A_i(\mu, \sigma)$ are some coefficients independent of $x$ , and $\psi_i(x)$ represents a basis of functions. Then, I save $\int dx \psi_i(x) F(x, ...) \forall\ i$ , and have just to calculate the coefficients $A_i(\mu, \sigma)$ for each case. For example, I can do a Taylor expansion: the problem is that I need lots of terms!! (e.g. for some peaked Gaussian more than 50, and it is not even enough! And this should be expected as Gaussians have some 'hard' boundary outside which they are 'zero'). Is there another basis of functions? Ideally I would want just an order $\mathcal{O}(1)$ of terms. And is there a machine learning method that allows for general learning of functions?
