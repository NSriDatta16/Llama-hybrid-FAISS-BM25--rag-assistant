[site]: datascience
[post_id]: 109233
[parent_id]: 
[tags]: 
What is the meaning of two embedding layers in a row?

I've noticed in one deep pre-trained textual neural network that there are two embedding layers in the beginning and I don't quite understand why there are two of them. As far as I understand (correct me if I'm wrong, I am a newcomer in NLP) in embedding layer there is a vector of trainable weights that forms set of parameters for every unique word. So, what is the meaning of two such layers in a row? Does the second layer create sub-parameters for every parameter in the original embedding layer?
