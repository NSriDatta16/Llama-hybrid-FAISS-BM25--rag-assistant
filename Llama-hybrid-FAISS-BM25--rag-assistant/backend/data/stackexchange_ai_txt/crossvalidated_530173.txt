[site]: crossvalidated
[post_id]: 530173
[parent_id]: 
[tags]: 
Rate of convergence of Machine learning models

I am currently doing some work on the double debiased machine learning algorithm by Chernozhukov et al. 2016 . They achieve $ \sqrt{N} $ rate of convergence for estimation of a treatment parameter. Take as an example the following data generating process. $$ Y_i=\theta D_i + g(X_i) + U_i $$ where $ \theta $ is the parameter of interest and $ D $ and $ X $ are the treatment and the covariates respectively. $ D $ is generated by $$ D_i = m(X_i) + V_i $$ $ U_i $ and $ V_i $ are random disturbances. We have to estimate $ g(X) $ and $ m(X) $ . The $ \sqrt{N} $ rate only holds if our ML algorithm converges with a rate of $ n ^ {\frac{1}{4}} $ . So this was a bit of a stretch but now my question. Are there any reliable results for common ML techniques about their rate of convergence to the true function?
