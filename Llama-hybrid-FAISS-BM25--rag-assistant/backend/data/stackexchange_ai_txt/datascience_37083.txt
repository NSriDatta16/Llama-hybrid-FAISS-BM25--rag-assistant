[site]: datascience
[post_id]: 37083
[parent_id]: 
[tags]: 
Learning word embeddings using RNN

The common way of learning word embeddings is based on BOW, and Skip-gram models. Is it possible to train a RNN-based architecture like GRU or LSTM with random sentences from a large corpus to learn word embeddings? Basically, we train a network with positive and negative samples, and back-propagate into word vectors. What are the drawbacks of this technique? Any reference to similar works is highly appreciated.
