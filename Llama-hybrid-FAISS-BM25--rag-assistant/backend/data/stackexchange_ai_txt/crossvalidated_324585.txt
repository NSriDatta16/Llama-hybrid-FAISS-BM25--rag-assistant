[site]: crossvalidated
[post_id]: 324585
[parent_id]: 324516
[tags]: 
You want citations for the claims, but I think this advice, while useful, stems from this context specifically. The fact that these methods work so well on Kaggle and other competitions has to do with the type of datasets in those competitions. Often, especially in elementary competitions, data consists of many examples, with heterogeneous data types (categorical, continuous, missing), that are all mildly predictive, and there are interaction effects to leverage. Housing pricing competitions are a typical example of that. In such cases tree based ensembles like XGBoost and Random Forests are extremely effective and practical. They are flexible enough to learn interactions and nonlinearities, don't overfit too much if done correctly, and can deal naturally with all kinds of data. Both in more advanced competitions and in practice, with other types of data (images, text, or just not a lot of data), and other problem settings (forecasting, recommendation systems), XGBoost will typically not be the best solution, or at most a part of the approach. Not to say that there are also many problems in practice where XGBoost is just the best approach in terms of classification or regression performance. On textual data, or on images, tree-based methods are known to not work so well. Timeseries data is also a really different beast. An interesting example on Kaggle I think is for example the chess rating competition, you can find the winning methods here: https://www.kaggle.com/c/ChessRatings2/discussion/568 .
