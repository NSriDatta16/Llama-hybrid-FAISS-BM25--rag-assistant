[site]: crossvalidated
[post_id]: 267679
[parent_id]: 267657
[tags]: 
Assuming you're doing OLS ($Y=X\beta_{ols}+\epsilon$), here's one approach using principal component analysis (PCA) that may provide some insight: 1) Demean and divide each of your independent variables by their standard deviation - this will allow easier interpretation later on. 2) Perform PCA on your $k$ number of independent variables, with $n$ observations. This will decompose them into scores, an $n$ by $k$ matrix $S$, and a $k$ by $k$ square transformation matrix $T$: $$ X=ST $$ The scores have the property that each column is completely uncorrelated with every other column, so that $cor(s_j,s_i)=0$ for all values of $j$ and $i$, and the transformation matrix $T$ will have the property of being invertable. Furthermore, the first column of $S$, lets say $s_1$, will be the most important in terms of explaining the commonalities between columns of $X$ (this is because the first column is associated with the largest eigenvalue). 3) Estimate your model using the scores: $$ Y=S\beta_{pca}+\epsilon $$ The coefficients are not immediately interpretable, so next is where the trick comes in. 4) Look at the $j^{th}$ column of $T^{-1}$ corresponding to the $\beta_{pca,j}$ with largest t-statistic - this will tell you what relationships within $X$ are most important. The reason is made clear after re-writing identities above: $$ \hat{Y}=X\beta_{ols}=(S)\beta_{pca}=(XT^{-1})\beta_{pca}$$so$$ \beta_{ols}=T^{-1}\beta_{pca} $$ Because of the first step, the magnitudes of the elements in $T^{-1}$ are directly comparable, and each column denotes a linear combination of your $X$s. You're interested in what linear combinations are most relevant, so that, for example, if one variable is the most important, the $j^{th}$ column of $T^{-1}$ will have a an element in it much larger than the rest of the elements, and will correspond to the "most important" variable in $X$.
