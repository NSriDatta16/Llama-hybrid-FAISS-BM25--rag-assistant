[site]: crossvalidated
[post_id]: 460354
[parent_id]: 
[tags]: 
Why BERT keep some masked tokens unchanged?

As I understand, out of all masked tokens in BERT Replace some with [mask], this is because of MLM Replace some with other token, this will force model to generate proper contextual embedding for all tokens in the sequence, not only the [mask] ones. This is consistent with the goal of finetuning. But I don't understand why BERT keep some masked tokens unchanged, could anyone please help me to understand it?
