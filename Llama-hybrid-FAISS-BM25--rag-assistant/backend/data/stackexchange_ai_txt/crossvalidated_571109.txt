[site]: crossvalidated
[post_id]: 571109
[parent_id]: 571064
[tags]: 
The sklearn implementation offers different options for multi_class and average which explains the main difference: the Hand-Till paper and the implementations you linked use a one-vs-one approach as you do in your sklearn call, but also uses a macro-average compared to your weighted average approach. There's another issue that prevents the scores from agreeing completely, though it will be more minor than the averaging issue. The gist and github implementations both sort the samples by probability, but ties are left up to the numpy sorting. In sklearn however, a tie of probabilities is handled by having a sloped line in the ROC curve, which also affects the area computation. Tweaking the toy example in [5] (since gunes pointed out the difference there, even though the classes are balanced and so the averaging plays no role) to have no ties in probability scores yields equal scores.
