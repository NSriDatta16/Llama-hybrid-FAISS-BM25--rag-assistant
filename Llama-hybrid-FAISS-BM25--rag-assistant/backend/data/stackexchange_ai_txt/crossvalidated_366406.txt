[site]: crossvalidated
[post_id]: 366406
[parent_id]: 364110
[tags]: 
(This answer assumes you want to use neural networks) A single perceptron actually learns a linear weight of your input: $output = w*input + b$ Keeping this as a separate neural network allows you to reset the weights training at any point of whichever network you want. You can also define different optimizers with different learning rates and so on. To assimilate this with your other neural network outcomes simply integrate wherever you want, for example: feed the outcome of this neural network as one of the inputs of another neural network (for example by concatenating the result to any other inputs you want to use) incorporate it with the output of another neural network (by summing, multiplying, concatenating,...) By backpropagating the loss, you should be able to improve different parts of your setup as desired.
