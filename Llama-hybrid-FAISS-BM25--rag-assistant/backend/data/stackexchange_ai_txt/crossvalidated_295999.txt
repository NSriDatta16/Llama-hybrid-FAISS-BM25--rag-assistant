[site]: crossvalidated
[post_id]: 295999
[parent_id]: 
[tags]: 
Should I use the cross validation score or the test score to evaluate a machine learning model?

Let's say I want to compare two machine learning models (A and B) on a classification problem. I split my data into train (80%) and test set (20%). Then I perform 4-fold cross-validation on the training set (so every time my validation set has 20% of the data). The average over the folds cross validation accuracy I get is: model A - 80% model B - 90% Finally, I test the models on the test set and get the accuracies: model A - 90% model B - 80% Which model would you choose? The test result is more representative of the generalization ability of the model because it has never been used during the training process. However the cross-validation result is more representative because it represents the performance of the system on the 80% of the data instead of just the 20% of the training set. Moreover, if I change the split of my sets, the different test accuracies I get have a high variance but the average cross validation accuracy is more stable.
