[site]: crossvalidated
[post_id]: 467553
[parent_id]: 467497
[tags]: 
PCA gives you the answer and the reason is because when it is able to find what you call intrinsic dimension $d it also means that the manifold is linear hyperplane. In fact all that PCA does is find that hyperplane or does not find it. So, your problem reduces to looking for a rotation of your D-dimensional data set X such that $Z=XA$ , where A is D-dimensional rotation matrix and X is the variables, which produces Z with a rank $d . Now, you should see that we got to an eigen value problem. Whether you do SVD or PCA, these would be the methods that are the answer to your question. In case of PCA you look at explained variance, and if the first d PCs explain enough of the variance in the data, then you got your linear transformation. Now, if you were interested in nonlinear transformations then things would get more interesting. What if there was a transformation $Z=f(X)$ such that matrix $Z$ has a lower rank that $X$ ? In this case PCA can't help you. You could run an autoencoder or something along those lines, but then your question would be valid: is there some quick diagnostic I could check before running computationally intensive techniques? Example Let's pick points from a 3-D sphere, i.e. $x^2+y^2+z^2=1$ . Adding (padding) with zeros like in the question doesn't make any difference for eigen analysis, but I added two columns of zeros just for the sake of it. Here, columns B-H are simulated data set from sphere using inefficient but very simple method : We have a data matrix 100x5, where two last columns are zeros. Now, look at the covariance matrix in cells M2:Q6 - you can see how zero columns drop off of it immediately, you can see visually that the rank of the matrix is 3 or less. Next, we apply eigen analysis, and in cells L8:L12 you get the eigen values. There are 5 of them with last two zeros. Again you see that the rank or three or less. In column S I'm showing the ratio of the eigen value to the sum of eigenvalues, which shows how much each adds to the total variance. You see that all three variables add approximately 1/3 to the variance. Hence, we can conclude that we can't drop any one the remaining three degrees of freedom. In other words, NO, your dataset does not come from a linear hyperplane. There's no hidden linear structure beyond trivial linear (constant) columns. However, the zeros are coming from a hyperplane, namely, a trivial one - a point. So, if you were to add all 47 zeros, then eigen analysis would have shown that 47 variables are coming from the trivial hyperplane, a point; and that the first three do not. Now, instead of using x,y,x, let's use the squares of them. Here's what you get in eigen analysis: only two explained variances are large, the third one is basically a rounding error. So, PCA picks up immediately that $x^2,y^2,z^2$ are coming from two dimensional hyperplane.
