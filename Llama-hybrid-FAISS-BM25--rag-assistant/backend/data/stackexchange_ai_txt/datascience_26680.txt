[site]: datascience
[post_id]: 26680
[parent_id]: 26675
[tags]: 
Instead of using embeddings and doc2vec, maybe you should start by an easier and more straightforward information retrieval approach. Start by building a bag-of-words representation of your long document. Given its vocabulary $V$ of size $|V|$, you represent it as well as the short texts for the second document in a space $\mathbb{R}^{|V|}$. This will give a vector for the first document and several vectors, one for each of the lines, for the second document. Proceed by calculating the similarity (euclidean distance) of the vectors of the lines of the second document with the vector of the first document. There are many ways in this line of though that would enable you to use word embeddings if you want, but I would suggest starting with the simplest approach first.
