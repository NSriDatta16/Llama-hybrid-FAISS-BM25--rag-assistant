[site]: crossvalidated
[post_id]: 291674
[parent_id]: 
[tags]: 
Calculating Confidence Intervals for Cross Validated Binary Classifiers

I'm experimenting with a number of models for a binary classification problem. To evaluate the performance of each model, I've used 10x repeated 10-fold cross validation to calculate the PR AUC (Area Under the Precision-Recall Curve), and a number of other metrics. My professor would like me to report the confidence intervals for these metrics and the p-value of the conclusion that model X is better than model Y. This is standard practice in Statistical Machine Translation (SMT) experiments (his background), where they use bootstrapping to calculate these (my experiment is NLP-related, but not SMT). Since I have used repeated cross-validation to estimate each model's performance, I'm not sure how to calculate the confidence intervals or p-values. This question is similar, where they suggest bootstrapping the resampled mean, but I'm not sure what this means. Is this the correct approach, and, if so, can anyone explain how to do that in more detail?
