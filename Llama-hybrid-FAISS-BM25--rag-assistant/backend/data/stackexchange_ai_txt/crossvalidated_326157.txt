[site]: crossvalidated
[post_id]: 326157
[parent_id]: 325881
[tags]: 
If you expand $Q_{m \times k}$ to $Q_{m \times k + 1}$ with a randomly initialized vector for the new movie it's equivalent to having had the new episode of Star Wars as a known movie all along, but with no observations in the rating matrix. All movies start with randomly initialized vectors in $Q$, and those vectors are updated based on the entries in the rating matrix. Since this new movie has no entries, it never gets updated, and remains at it's initial random initialization. This allows the system to "work" in the sense that you can now make predictions for the new episode, but clearly the predictions will be meaningless. In order to actually get meaningful recommendations you can follow the the last few paragraphs in the ADDITIONAL INPUT SOURCES section of the paper. For example, before the new episode of Star Wars is released you will have other non-rating information about the movie: genre, director, MPAA rating, actors, and so forth. Each of these attributes can be factorized, and the factorized representation added to the base movie vector. You can adapt equation (6) in the paper like so: $\hat{r}_{ui} = \mu + b_{i} + b_{u} + [q_{i} + \sum_{a \in A(i)} y_a ]^{T} p_{u} $ where movie $i$ corresponds to the set of attributes $A(i)$. Then you learn the factor representations for each attribute during the training process. As an example, the attribute "has JJ Abrams as director" will have a learned factor representation that is added in to all of item vectors for movies that JJ Abrams directed.
