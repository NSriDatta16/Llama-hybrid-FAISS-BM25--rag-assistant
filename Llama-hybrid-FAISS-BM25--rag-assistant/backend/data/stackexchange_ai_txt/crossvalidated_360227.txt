[site]: crossvalidated
[post_id]: 360227
[parent_id]: 
[tags]: 
How are the internal representations in ELMo averaged?

I have been reading the paper " Deep contextualized word representations " (by Peters et al, 2018) to learn about the new embedding method called ELMo. In this paper, the authors train a charCNN + bi-LSTM language model then use the internal representations from this model to produce "ELMo" representations. These representations are given by the formula: $$\text{ELMo}_k^{task} = \gamma^{task}~\sum_{j=0}^L s_j^{task}\textbf{h}_{k,j}^{LM}$$ where: $\quad \gamma^{task}$ and $s_j^{task}$ are task specific parameters $\quad \textbf{h}_{k,0}^{LM}$ is the token representation of token $k$ $\quad \textbf{h}_{k,j>0}^{LM}$ is the biLSTM representation of token $k$ In the implementation available on Tensorflow Hub we can see that the token representation has a size of 512 while the biLSTM representations have a size 1024. I don't understand how they are making the sum over vectors that have different sizes. Am I missing something here?
