[site]: crossvalidated
[post_id]: 387185
[parent_id]: 387175
[tags]: 
As you probably already know LDA assumes equal covariance matrices for both classes. So the first counter example might be a scenario where the two classes have different covariance matrices. LDA would assume them to be equal and compute the wrong distances. But assuming the variance-covariance matrices are equal in both classes: Why should we then solve a generalized eigenvalue problem if instead we could just compute all distances and take the smallest one? The solution returns a hyperplane separating the classes. A boundary. Once you know this boundary it is a lot easier to check if the observation is above it (belong to 1st class) or below it (belong to the 2nd class) compared to computing the Mahalanobis distance to the averages of both classes and choosing the smaller one.
