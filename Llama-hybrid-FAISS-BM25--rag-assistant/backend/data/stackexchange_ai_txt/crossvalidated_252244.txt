[site]: crossvalidated
[post_id]: 252244
[parent_id]: 
[tags]: 
Deep neural network: categorical cross entropy with l1-norm (sparsity)

I am using a deep neural network in order which consists of: 1i nput layer, 2 hidden (dense) layers, 2 dropout layers (right after each dense layer), 1 softmax classifier (output). The cost function is the categorical cross-entropy with a regularization term over the weights W, which helps in reducing the number of connections W. I would like to obtain a sparser output layer by adding an l1-norm over the output, in other words I would like to reduce the number of output neurons with values close to 1 and, at the same time, to increase the number of 0s. In order to do this, I have just simply added this l1-norm term to the cost function, also trying different weighting constants. The problem is: the result is exactly the same of the case without l1-norm, it doesn't change anything. Do you think that I am making any kind of mistake, at least theoretically? Any other idea to get to the same point? Ideally my method has to work, but in practice doesn't. For the same purpose, I have been also trying the lp norm (p However, I am working with theano and keras, so if you have any more specific suggestion, please let me know. Thanks.
