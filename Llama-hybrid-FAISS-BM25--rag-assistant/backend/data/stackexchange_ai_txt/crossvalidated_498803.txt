[site]: crossvalidated
[post_id]: 498803
[parent_id]: 
[tags]: 
Is number of features greater than minority class size a problem for tree based models (boosting, random forest)?

In Logistic Regression there is the "One in Ten Rule" ( https://en.wikipedia.org/wiki/One_in_ten_rule ). For example, there is a sample of 2000 customers, and 50 of them belong to the positive class (1950 of them are negative). Then the maximum number of features we can have in the Logistic Regression model is 50 / 10 = 5, although there may be 150 features available. My questions is: For tree based models (e.g. XGBoost, random forest), is there any constraint / rule of thumb on number of features allowed in the model (at the same time), given the minority class sample size? Can you please also explain the reason? Thank you.
