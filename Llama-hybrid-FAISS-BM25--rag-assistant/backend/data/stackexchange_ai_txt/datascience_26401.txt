[site]: datascience
[post_id]: 26401
[parent_id]: 
[tags]: 
How to implement "one-to-many" and "many-to-many" sequence prediction in Keras?

I struggle to interpret the Keras coding difference for one-to-many (e. g. classification of single images) and many-to-many (e. g. classification of image sequences) sequence labeling. I frequently see two different kind of codes: Type 1 is where no TimeDistributed applied like this: model=Sequential() model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode="valid", input_shape=[1, 56,14])) model.add(Activation("relu")) model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1])) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=pool_size)) model.add(Reshape((56*14,))) model.add(Dropout(0.25)) model.add(LSTM(5)) model.add(Dense(50)) model.add(Dense(nb_classes)) model.add(Activation("softmax")) Type 2 is where TimeDistributed is applied like this: model = Sequential() model.add(InputLayer(input_shape=(5, 224, 224, 3))) model.add(TimeDistributed(Convolution2D(64, (3, 3)))) model.add(TimeDistributed(MaxPooling2D((2,2), strides=(2,2)))) model.add(LSTM(10)) model.add(Dense(3)) My questions are: Is my assumption correct that Type 1 is the one-to-many kind and Type 2 is the many-to-many kind? Or TimeDistributed has no relevance in this aspect? In either case of one-to-many or many-to-many is the last dense layer supposed to be 1 node "long" (emitting only one value in turn) and the previous recurrent layer is responsible to determine how many 1-long value to emit? Or the last dense layer is supposed to consist of N nodes where N=max sequence length ? If so, what is the point of using RNN here when we could produce a similar input with multiple outputs with N parallel "vanilla" estimators? How to define the number of timesteps in RNNs? Is it somehow correlated with the output sequence length or is it just a hyperparameter to tune? Inn case of my Type 1 example above what is the point of applying LSTM when the model emits only one class prediction (of possible nb_classes )? What if one omits the LSTM layer?
