[site]: crossvalidated
[post_id]: 273654
[parent_id]: 273484
[tags]: 
Neural network language models usually take their input in the form of one-hot encoded vectors. This means that the dimensionality of the vector is equal to the size of the vocabulary and that the vector is all zeros except for a single one at the position that corresponds to the id number for that word. The one-hot input vectors are multiplied by an matrix that converts the inputs to low-dimensional embeddings. The first generation of neural network language models took as input the previous n-words and used them to predict the next word. The current generation of neural network language models uses something called recurrent neural networks. You can read more about those elsewhere. For the output you need to produce a vector the size of the vocabulary. Then you use a softmax activation function to turn that vector into a probability distribution over the vocabulary.
