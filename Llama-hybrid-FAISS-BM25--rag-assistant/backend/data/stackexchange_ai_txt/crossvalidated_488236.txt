[site]: crossvalidated
[post_id]: 488236
[parent_id]: 
[tags]: 
Confused with the fundamental assumptions of Frequentist and Bayesian Linear Regression

In Frequentist Linear Regression, I have seen 2 approaches which lead to basically similar models. We have $W,y,X,\epsilon$ related as $y=W^TX+\epsilon$ , where $y$ is the dependent random variable, and $X$ is assumed to be a constant ( first approach ), or random ( second ) independent variable. $\epsilon$ is assumed to be the Gaussian error. Now let us say we assume $X$ as a random variable, of which, we don't know the probability distribution. (At least the sources I've read don't talk about its distribution) We also write the the data as $\{(x_i,y_i)_n\}$ , and this notation is widely used: $$p(D)=p(y|X)\tag{i}$$ where " $D$ " is often called the Data. ( (1)Is it a random variable? ) Well then, we get (assuming parameters of $\epsilon$ to be constant), $$p(y|W,X)=N(W^TX,\sigma_\epsilon^2)$$ and compute the MLE. Now coming to the semi Bayesian, we know a prior distribution of $W$ . Now we wish to know the posterior, given the data D. That is, $$p(W|D)=_{\text{Def of Conditional Probability}} \frac{p(W,D)}{p(D)}=\frac{p(D|W)p(W)}{p(y|X)}$$ (2)Now, how do we compute $p(D|W)$ ? (All we are given is $(i)$ , about $p(D)$ ) Now coming to the pure Bayesian, we basically want $$p(y|X,D) =\frac{p(y,X,D)}{p(x,D)}$$ I was told at school that this equals $$\int_Wp(y|W,X)p(W|D)dW$$ (3)How to arrive at this? Please use only basic stuff like definitions to derive this. I have found so many interpretations and ways of these, that I literally have no idea what is the correct way to look at it. So please provide answers to the questions in bold. Also, if there is any mistake in the above reasoning, please point out.
