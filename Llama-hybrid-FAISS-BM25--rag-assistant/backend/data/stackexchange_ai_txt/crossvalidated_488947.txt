[site]: crossvalidated
[post_id]: 488947
[parent_id]: 
[tags]: 
Understanding Training Word Embeddings?

I am new to Natural Language Processing. I am trying to understand how word embeddings are created. When we are training Neural Networks, it is usuallly the weights of the neural network that are changed. So for when we are training in a mechanism like auto-encoders (which according to me is: given an input sequence "stackoverflow community is great" will try to predict "stackoverflow community is great". To do this the weights of the neural network is trained. Now if only the weights of the neural network is trained, how exactly do we obtain the word embeddings (that is how do we get vectors for words, so that we can do things like King-Queen+Woman=Man or something similar. How exactly are we getting the word vectors if we are only changing neural network weights and not the input vector components). Some resources I have looked at: Generating Code Embeddings Part 1 Generating Code Embeddings Part 2 Generating Code Embeddings Part 3 Reddit Anything2vec Word2vec Generate your own embeddings
