[site]: crossvalidated
[post_id]: 560425
[parent_id]: 
[tags]: 
Variational Bayes for a univariate Gaussian

I'm following an example from Murphy's book (Sec 21.5.1) on how to apply Variational Bayes to infer the posterior over the parameters for a 1D Gaussian $p(\mu,\lambda|\mathcal{D})$ . The example uses a prior of the form $$ p(\mu, \lambda)=p(\mu|\lambda)p(\lambda)=\mathcal{N}(\mu|\mu_0,(\kappa_0\lambda)^{-1})\mathcal{G}a(\lambda|a_0, b_0) $$ and an approximate factored posterior of the form $$ q(\mu, \lambda)=q_{\mu}(\mu)q_{\lambda}(\lambda) $$ The unnormalized log posterior has the form $$ \log[p(\mu,\lambda,\mathcal{D})]=\log[p(\mathcal{D}|\mu,\lambda)p(\mu|\lambda)p(\lambda)] $$ Now what I don't understand is that in the following paragraph, Updating $q_{\mu}(\mu)$ : The optimal form for $q_{\mu}(\mu)$ is obtained by averaging over $\lambda$ : $$\begin{aligned} \log q_{\mu}(\mu) &=\mathbb{E}_{q_{\lambda}}[\log p(\mathcal{D} \mid \mu, \lambda)+\log p(\mu \mid \lambda)]+\text { const } \\ &=-\frac{\mathbb{E}_{q_{\lambda}}[\lambda]}{2}\left\{\kappa_{0}\left(\mu-\mu_{0}\right)^{2}+\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}\right\}+\text { const } \end{aligned}$$ why we don't have $p(\lambda)$ inside the expectation? Does that mean $\mathbb{E}_{q_{\lambda}} [\log p(\lambda)]$ is a constant? why? How $\mathbb{E}_{q_{\lambda}} [\dots]$ is different from $\mathbb{E}_{\lambda} [\dots]$ in general? i.e. How does expectation with respect to a variable ( $\lambda$ ) differ from expectation w.r.t. a distribution ( $q_{\lambda}$ )?
