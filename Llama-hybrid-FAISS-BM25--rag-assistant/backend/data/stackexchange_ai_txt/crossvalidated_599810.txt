[site]: crossvalidated
[post_id]: 599810
[parent_id]: 
[tags]: 
Equivalence between one-dimensional embedding and one-hot encoding?

This may not sound like a practical question, but I am trying to create essentially a toy problem that can help me understand and apply some explainability algorithms (eg Integrated Gradients). To do this I am starting with a fairly simple regression model that only has categorical features, and I want to use an embedding layer with just one dimension as the input layer. It seems to me that ultimately this should give equivalent results to one-hot encoding the input features. And what I mean by "equivalent" is that the weights inside the embedding for each input feature would be roughly equal to the coefficients for the OHE features. Is that right?
