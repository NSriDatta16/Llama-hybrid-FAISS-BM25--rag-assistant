[site]: datascience
[post_id]: 117615
[parent_id]: 117612
[tags]: 
"Good" or "bad" is always relative in data science. You need to establish a benchmark for comparison. First of all, you need to know that accuracy is not a very good performance metric for classification. Ironically, even if it is very popular (because it is easy to understand), it is just about the worst classification metric that exists (because every other classification metric explicitly tries to do better than accuracy). If you are relatively new to data science, then I recommend that you focus on AUC ROC (area under the curve of the ROC curve), whose interpretation is quite easy to understand: 0.5 is a random guess, anything better than 0.5 is better than random, and 1.0 is perfect. (By contrast, the question of what is a "good" accuracy score is surprisingly difficult to answer--AUC is much easier to interpret and has much better statistical properties. There are some other measures like log loss that are even statistically better than AUC, but they are hard to interpret, so I recommend that you stick with AUC for now.) So, the base benchmark is that you need an AUC score better than 0.5. But is that "good"? You need a benchmark model for comparison. That is, if you do not use LSTM, what would be something simpler to use (e.g., maybe logistic regression)? You should see what the AUC of a simple and decent model gives. Then that becomes your benchmark. For example, you could compare the LSTM results with sentiment analysis using na√Øve Bayes or VADER and then compare the AUC of all of these . If your LSTM results have superior AUC, then the results are "good". But if they are not as good as other techniques, then it would be hard to call them "good". But in that case, you would have other models that are then "good". That is the standard approach to take to determine if any data science result is "good" or not.
