[site]: crossvalidated
[post_id]: 609178
[parent_id]: 609169
[tags]: 
If your additional features are simply not highly predictive, then this can certainly happen. Not every additional predictor or ore complex model necessarily improves accuracy. You may find this helpful: How to know that your machine learning problem is hopeless? Also, take a look at the bias-variance tradeoff . In addition, it may of course be that the precise hyperparameter you need for your focal model to outperform the baseline varies, too. A learning rate of 0.01 may mean that the focal model is better than the baseline on your particular test set . On another test set, the optimal learning rate may well be different. I would suggest you do some cross-validation to get an idea of how variable the optimal hyperparameter is - and to see how confident you can be that a hyperparameter you set to its optimal value in training and testing continues to perform well in production on yet newer data.
