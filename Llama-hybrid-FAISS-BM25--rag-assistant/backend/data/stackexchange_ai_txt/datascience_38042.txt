[site]: datascience
[post_id]: 38042
[parent_id]: 
[tags]: 
Classifying Car Data By Year

I have huge car photos. I want to predict car's "brand-model-body type and production year" First, I splitted data into train and validation, and I categorized them like this. Every category has about 1000 train and 900 validation images. My plan was: I train my keras model with these categories after training, model can predict labels like below: audi a3 sedan 2008 => %25 audi a3 sedan 2009 => %25 audi a3 sedan 2010 => %25 audi a3 sedan 2011 => %25 And I can tell user that: "This car is Audi A3 Sedan 2008-2011" My problem is, some of these categories have very similar photos. For example: audi a3 2009 and audi a3 2010 have same body type and there is not much difference between photos (No difference in reality). Because of that, train accuracy has improved to about 0.9 but validation accuracy hasn't improved above 0.55 When I try some predictions, it usually gives same label, "Ford Focus sedan 2009" :) Here is my output: epoch, acc, loss, val_acc, val_loss 27, 0.7965514530544776, 0.56618134500483, 0.5192149643316993, 1.729015349846447 28, 0.8058803490480816, 0.5408204138258657, 0.5176764522193236, 1.778763979018732 29, 0.8167710489770164, 0.5116128672937693, 0.523258489762041, 1.7806432932022545 30, 0.8256544639818643, 0.4872381848016096, 0.5207534764479939, 1.8059904007678271 31, 0.8355546238309248, 0.4629556378035959, 0.5237253032663666, 1.8191414148756815 32, 0.8424464767701014, 0.4444190686917562, 0.5242512903147193, 1.8496954914466912 33, 0.8508739288802705, 0.422022156655134, 0.5303593149032422, 1.8565427863780883 34, 0.8576819265745635, 0.40545297008116027, 0.5262894901236571, 1.909881308499735 My train code is here: Image_width, Image_height = 224, 224 num_epoch = 5000 batch_size = 16 learning_rate = 0.0001 model = ResNet50(weights='imagenet', include_top=False, input_shape=(Image_width, Image_height, 3)) fc_neuron_count = 1024 output = model.output output = GlobalAveragePooling2D()(output) output = Dense(fc_neuron_count, activation='relu')(output) predictions = Dense(num_classes, activation='softmax')(output) model = Model(inputs=model.input, outputs=predictions) model.compile(optimizer=opt.Adam(lr=learning_rate), loss=losses.categorical_crossentropy, metrics=['accuracy']) history_transfer_learning = model.fit_generator( train_generator, epochs=num_epoch, steps_per_epoch=num_train_samples // batch_size, validation_data=validation_generator, validation_steps=num_validate_samples // batch_size, class_weight='auto', callbacks=callbacks_list) Am I doing something wrong? How can I achieve this result? Should I change validation accuracy calculation, or should I give more photos per category?
