[site]: crossvalidated
[post_id]: 532927
[parent_id]: 532907
[tags]: 
The answer depends on how exactly does your training data looks. If you are predicting the next word and you have only a single word in your training data, then the data doesn't have explicit ambiguities. If you have standard NLP data, just train it as usual. From the ambiguous examples in your data (e.g. you have both "I have an animal, the animal is a dog", and "I have an animal, the animal is a cat" sentences in your data), it will learn by itself ambiguities that exist in the natural language. In prediction time, don't take the word with the highest probability as your prediction, but pick $k$ words with the highest probabilities. Alternatively, you could come up with some better rules, e.g. that the probabilities need to be higher than some $\varepsilon$ , or not drastically smaller than the highest probability, etc. If your data is tagged in a way that already allows for the alternatives, e.g. "I have an animal, the animal is a (dog|cat)", treat it as a multi-label problem. In multi-class classification, your target words would be a one-hot vector, with only a single "correct" word. In the multi-label scenario, you have a vector of zeroes and ones. What follows, you don't use softmax as an activation but need to use sigmoids and a binary loss function. This StackOverflow.com thread describes how it is done in Keras. In such a case, for each word, it would independently make a classification.
