[site]: crossvalidated
[post_id]: 161093
[parent_id]: 
[tags]: 
How exactly do convolutional neural networks use convolution in place of matrix multiplication?

I was reading Yoshua Bengio's Book on deep learning and it says on page 224: Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. however, I was not 100% sure of how to "replace matrix multiplication by convolution" in a mathematically precise sense. What really interest me is defining this for input vectors in 1D (as in $x \in \mathbb{R}^d$), so I won't have input as images and try to avoid the convolution in 2D. So for example, in "normal" neural networks, the operations and the feed ward pattern can be concisely expressed as in Andrew Ng's notes: $$ W^{(l)} a^{(l)} = z^{(l+1)}$$ $$ f(z^{(l+1)}) = a^{(l+1)}$$ where $z^{(l)}$ is the vector computed before passing it through the non-linearity $f$. The non-linearity acts pero entry on the vector $z^{(l)}$ and $a^{(l+1)}$ is the output/activation of hidden units for the layer in question. This computation is clear to me because matrix multiplication is clearly defined for me, however, just replacing the matrix multiplication by convolution seems unclear to me. i.e. $$ W^{(l)} * a^{(l)} = z^{(l+1)}$$ $$ f(z^{(l+1)}) = a^{(l+1)}$$ I want to make sure I understand the above equation mathematically precisely. The first issue I have with just replacing matrix multiplication with convolution is that usually, one identifies one row of $W^{(l)}$ with a dot product. So one clearly knows how the whole $a^{(l)}$ relates to the weights and that maps to a vector $z^{(l+1)}$ of the dimension as indicated by $W^{(l)}$. However, when one replaces it by convolutions, its not clear to me which row or weights corresponds to which entries in $a^{(l)}$. Its not even clear to me that it makes sense to represent the weights as a matrix anymore in fact (I will provide an example to explain that point later) In the case where the input and outputs are all in 1D, does one just compute the convolution according to its definition and then pass it through a singularity? For example if we had the following vector as input: $$x = [1,2,3,4]$$ and we had the following weights (maybe we learned it with backprop): $$W = [5,6,7] $$ then the convolution is: $$ x * W = [5, 16, 34, 52, 45, 28]$$ would it be correct to just pass the non-linearity through that and treat the result as the hidden layer/representation (assume no pooling for the moment)? i.e. as follows: $$ f(x * W) = f([5, 16, 34, 52, 45, 28]) = [f(5), f(16), f(34), f(52), f(45), f(28)])$$ (the stanford UDLF tutorial I think trims the edges where the convolution convovles with 0's for some reason, do we need to trim that?) Is this how it should work? At least for an input vector in 1D? Is the $W$ not a vector anymore? I even drew a neural network of how this is suppose to look like I think:
