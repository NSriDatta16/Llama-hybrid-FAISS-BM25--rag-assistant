[site]: crossvalidated
[post_id]: 548443
[parent_id]: 
[tags]: 
Average of standard deviations

I have two sets of data Set 1: 0.051, 0.047, 0.044, 0.049, 0.043, 0.048, and 0.042. Using Excel, the average is 0.046286 and the Std Dev is 0.003352 Set 2: 0.051, 0.046, 0.053, 0.047, 0.047, 0.048, and 0.049. Using Excel, the average is 0.048714 and the Std Dev is 0.002498 If I average all of the data in Set 1 and Set 2 together, I get an average of 0.0475. If I average the average from set 1 (0.046286) and the average from set 2 (0.048714), I get the same thing of 0.0475. Okay, that seems reasonable to me. If I want the SD of all of the data in Set 1 and Set 2 together, I get a Std Dev of 0.003107. If I average the Std Dev from set 1 (0.003352) and the Std Dev from set 2 (0.002498) I get 0.0029250, which is not the same thing. Ugh, that does NOT seem reasonable to me. Why does my average std dev from the two sets not equal the total std dev of the two sets together? Why does it work for average, but not std dev?
