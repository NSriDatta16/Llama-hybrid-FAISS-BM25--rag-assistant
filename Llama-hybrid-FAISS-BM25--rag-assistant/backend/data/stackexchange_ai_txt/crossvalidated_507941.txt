[site]: crossvalidated
[post_id]: 507941
[parent_id]: 465483
[tags]: 
Yes, your observation is right. Even though it is quite slow in training (exponentially slow), it is still an offpolicy algorithm. There is at least 1 update step at the tail of the episode that happens even if the sampled trajectory does not match the deterministic target policy's trajectory. This ensures that all the last Q(s,a) that are possible in our model are sampled and averaged quite often. It inevitably leads to update of our deterministic policy such that the last step taken by its (deterministic) trajectory is optimal. Similarly the second last step can be optimised provided the last step sampled using our behaviour policy is optimal and so on and so forth... So it gets exponentially more difficult to get the optimal action as we move up the trajectory. However, we do find a deterministic optimal policy in the limit.
