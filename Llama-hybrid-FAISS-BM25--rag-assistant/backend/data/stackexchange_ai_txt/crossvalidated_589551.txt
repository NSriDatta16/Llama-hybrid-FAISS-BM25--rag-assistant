[site]: crossvalidated
[post_id]: 589551
[parent_id]: 577846
[tags]: 
The existence of a bias-variance tradeoff has been assumed as inevitable (i.e., an axiom) in any model using data, including neural networks. However, it has been observed since about 2018 that surprisingly, some cases of very large deep neural networks, trained with a correspondingly sufficiently large dataset, do not exhibit the classical bias-variance tradeoff. This means that these networks also generalize better. This phenomenon, termed " double descent ", has been duplicated by other researchers. See for example: "Reconciling modern machine-learning practice and the classical biasâ€“variance trade-off", 2019, by Belkin, Hsu, Ma, Mandal, https://www.pnas.org/doi/10.1073/pnas.1903070116 . As of 2022, conclusively explaining this phenomenon is still an open research question, but there have recently been interesting inroads to answering it. For example, the following paper is a theoretical explanation to justify this mysterious phenomenon: "A Universal Law of Robustness via Isoperimetry", 2021, by Bubeck and Sellke, https://arxiv.org/abs/2105.12806 (Outstanding Paper award at NeurIPS 2021) The analysis/explanation is based on a network having a small Lipschitz constant (maximum value of the gradients), meaning the function represented by the network is smooth. The paper also claims that in addition to good generalization, such a phenomenon also implies better robustness to adversarial attacks. The analysis is not limited to neural networks, but is general enough for many other function approximations (including Reproducing Kernel Hilbert Space). The paper gives specific guidance on the number of parameters vs. the amount of data for this phenomenon to occur. Emphasis: This "double descent" phenomenon does not occur in all deep neural networks trained with a correspondingly sufficiently large dataset. Rather, according to Bubeck and Sellke, it depends on the number of input data points, the effective dimension of the classification, the depth of the network (number of layers), and the overall number of parameters in the network. Therefore, in other cases, neural networks, even deep ones, will still exhibit the bias-variance tradeoff. In a sense, this guidance on parameter values in the Bubeck and Sellke paper can be regarded as a falsifiable prediction as to whether their analysis/explanation is (in)correct.
