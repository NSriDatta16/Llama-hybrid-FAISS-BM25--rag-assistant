[site]: datascience
[post_id]: 69834
[parent_id]: 
[tags]: 
Since is not possible test all the possible combination, what is the correct procedure to follow on building Machine Learning?

Sorry, I'm a little confused and this is a general question. How can I be sure that the procedure that I am following is the correct one? Following the steps for building a machine learning model, we start by analyzing the data and then by prepare that data (among other steps). In the data preparation, we have various options. For example, if the data have missing values, I can choose to remove those values or replace them (among other techniques). How can I be sure what is the “correct” technique to follow? I mean I can always use an algorithm for getting an initial accuracy and then I can compare the changes that I will do to that accuracy. However, can I rely on that accuracy? The same happens with feature selection. I cannot be sure among all the techniques witch one is the best. The question is the same if we talk about choosing a model and then make the hyper parametrization of that model. How can I be sure that the model that I choose is the correct one? Because without the hyperparameters tunning can be better for one specific model but after the hyperparameters tunning(in all of them) I can have better accuracy in another one. For example, between SVM and KNN without tunning the accuracy is better in SVM, so I will choose that one and then make the tunning. However, if I try tunning on both models maybe KNN will be better than SVM. In short, my question is, they’re a lot of possible combinations between all the steps to build a machine learning model. I know it's practically impossible to test all these combinations (computation power, and so on), however, it is correct to rely on an “ephemeral” accuracy? Or there is a “correct” procedure to follow? I also read that there is a dummy classifier for getting a baseline accuracy. Should I use that method to evaluate, for example, different data preparing methods? I mean, I can obtain a baseline accuracy. Then use a random ML algorithm and compare it with that baseline accuracy, and make changes step by step. But I have the same problem because some types of algorithms may be good with specific types of data preprocessing, and I am not testing all the hypotheses. Thanks in advance, and sorry for my English because it’s not my maternal language.
