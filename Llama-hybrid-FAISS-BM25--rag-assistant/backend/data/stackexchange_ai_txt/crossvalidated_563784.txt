[site]: crossvalidated
[post_id]: 563784
[parent_id]: 563757
[tags]: 
I think that one advantage of $KL$ distance is that is has a simple interpretation, according to wiki $D_{KL}(P, Q)$ is "the expected excess surprise from using $Q$ as a model when the actual distribution is $P$ ". Also it is easy to compute. On the contrast, the Wasserstein metric is defined as the solution to some linear optimization problem. One can solve it effectively but it definitely longer compared to using a simple formula. The symmetrized $KL$ divergence is super similar but its interpretation is just a bit less straightforward: "The Jensenâ€“Shannon divergence is the mutual information between a random variable $X$ associated to a mixture distribution between $P$ and $Q$ and the binary indicator variable $Z$ that is used to switch between $P$ and $Q$ to produce the mixture." I didn't look up all the items on the list of statistical distances but some of them are definitely harder to compute compared to $KL$ distance. Also $KL$ distance is easy to differentiate, and that is great for any applications in machine learning (using as a loss function).
