[site]: datascience
[post_id]: 77395
[parent_id]: 77391
[tags]: 
Artificial Neural Networks are like a black box learning system. There is no known, or generally agreed upon, method that dictates what each weight represents or means for a given learning problem. Its internal representation of the problem is opaque to the architect. In fact, the final trained weights are very closely tied to the neural network architecture and is very logical to assume that they cannot be transfered to another arbitrary architecture of another neural network. That being said, there is research related to re-purposing an already trained neural network to another similar task. This is called Transfer Learning in machine learning literature. Some resources to get you started: A Gentle Introduction to Transfer Learning for Deep Learning Transfer learning only works in deep learning if the model features learned from the first task are general. How transferable are features in deep neural networks? Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset. Deep Learning using Transfer Learning What to transfer â€” We need to understand what knowledge is common between the source and target task. What knowledge can be transferred from source task to target task that will help improve the performance of the target task When to transfer or when not to Transfer - When the source and target domains are not related at all we should not try to apply transfer learning. In such a scenario the performance will suffer. This type of transfer is called Negative Transfer. We should apply Transfer learning only when source and target domains/tasks are related How to transfer: Identifying different techniques to apply transfer learning when the source and target domain/task are related. We can use Inductive transfer learning, Transductive transfer learning or unsupervised transfer learning. An overview of attempts to interpret deep-learning models and a new suggestion in Causality Learning: A New Perspective for Interpretable Machine Learning Recent years have witnessed the rapid growth of machine learning in a wide range of fields such as image recognition, text classification, credit scoring prediction, recommendation system, etc. In spite of their great performance in different sectors, researchers still concern about the mechanism under any machine learning (ML) techniques that are inherently black-box and becoming more complex to achieve higher accuracy. Therefore, interpreting machine learning model is currently a mainstream topic in the research community. However, the traditional interpretable machine learning focuses on the association instead of the causality. This paper provides an overview of causal analysis with the fundamental background and key concepts, and then summarizes most recent causal approaches for interpretable machine learning. The evaluation techniques for assessing method quality, and open problems in causal interpretability are also discussed in this paper. Now, to answer your main question given the already mentioned points, is to try heuristics in a trial and error manner, there is no standard procedure. For example you can set superfluous output weights to zero, or missing output weights to zero. One can try other linear (or non-linear) combinations to change the amount of output weights to match the original neural network to the new dataset. One can even train a neural net whose sole purpose is to adapt the output classes of the original network to the output classes of the new problem, and concatenate it with the orignal neural net. However if one takes this approach, why not train a new convolutional network from scratch that directly classifies the new problem.
