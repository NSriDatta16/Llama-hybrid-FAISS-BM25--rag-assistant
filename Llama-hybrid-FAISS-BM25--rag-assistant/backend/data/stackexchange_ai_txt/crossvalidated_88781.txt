[site]: crossvalidated
[post_id]: 88781
[parent_id]: 
[tags]: 
Comparable training and test cross-entropies result in very different accuracies

Premises I'm training a convolutional neural network (ConvNet) on 51 subclasses in the ImageNet dataset . In order to keep an eye on overfitting , I have been suggested to plot training and testing loss function values (using a negative log-likelihood criterion) and accuracy (correct guesses over total number of sample). So far, I have obtained consistent results, similar to the example below. Then, I killed the overfitting by using a dropout technique, producing the following chart. Question Now I'm puzzled. I have comparable cross-entropy errors, but quite different accuracies. Shouldn't a similar loss function value, for train and test, give me similar precisions? Rewording the question: why $e_{\mathrm{test}}\simeq e_{\mathrm{train}}\not\Rightarrow a_{\mathrm{test}}\simeq a_{\mathrm{train}}$ where $e$ stays for cross-entropy error (defined below) and $a$ stays for accuracy ? Definitions I've been using the terms cross-entropy errors and loss function values interchangeably. What I am referring at is the average prediction error which is given by $$e = - \frac{1}{m}\sum_{i=1}^{m}{\ln[p(y_i|x_i)]}$$ where $m$ is the number of images in the training or testing sets, $y_i$ is the true label and $p(y_i|x_i)$ is the model's output probability associated to the input $x_i$ to be classified as $y_i$.
