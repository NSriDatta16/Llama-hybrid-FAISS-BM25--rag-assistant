[site]: crossvalidated
[post_id]: 559664
[parent_id]: 559649
[tags]: 
The probabilistic model for linear regression is $$\begin{align} \mu &= \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k \\ y &\sim \mathcal{N}(\mu, \sigma) \end{align}$$ which is the same as saying $$ y - \mu = \varepsilon \sim \mathcal{N}(0, \sigma) $$ This follows from the properties of a normal distribution if $Y \sim \mathcal{N}(\mu, \sigma)$ , then $Y + c$ would follow a normal distribution with mean $\mu + c$ . Notice that this does not hold for all distributions. For example, the difference between $y$ and the mean predicted by Poisson regression could result in negative values of the residuals, so they cannot follow a Poisson distribution that has non-negative support. The same is tue with logistic regression : if you subtract the predicted mean (probabilities) from zeros and ones, the residuals could not be zeros or ones, so won't follow the Bernoulli distribution assumed by logistic regression. As you can see, there is no one-to-one correspondence between the distribution of residuals and the likelihood function. Here you can read about the distribution of residuals for logistic regression. For models other than linear regression, the distribution of residuals is not equally elegant. But yes, if your data is inconsistent with the distribution you assumed by the model, then your results will be flawed. The estimates of the parameters may be invalid; the intervals for the parameters and predictions would be incorrect as well; if you conducted hypothesis testing, tests would not work properly. This would be true for both Bayesian and frequentist models.
