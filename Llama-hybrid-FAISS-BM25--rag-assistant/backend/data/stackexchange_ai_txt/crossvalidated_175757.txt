[site]: crossvalidated
[post_id]: 175757
[parent_id]: 
[tags]: 
What is an appropriate machine learning model for a dice game?

I'm having trouble thinking of the correct way to pose the following problem: Say a dice game (like Yahtzee) involves throwing up to 5 6-sided die in three rounds. After three rounds, a score is awarded based on which pattern the die conform to. In the first round, all five dice are thrown. In subsequent rounds, some dice may be held back and the remainder thrown in an effort to confirm to a higher scoring pattern. (Eg - You roll 3 of a kind and now you want to roll the remaining two to get 4 or 5 of a kind.) I'd like to create a machine learning model (for fun) to train and determine which dice to hold back and which dice to roll in a given round to improve the pattern. As input variables, the state S (1-6) of each of five dice is given, a decision D to roll or not constructed of (0 or 1) for each of the five dice (encoded binary from 0-31), and I can generate a training set comprising the above information plus simulated results R. So in summary, each observation in the training set will contain a vector of initial dice values S, a bitmask of which dice to roll D, a simulated vector of final dice values R, and +1 if this improved the pattern and -1 if it did not improve. What is the best way to pose this problem? If I'm predicting D, it seems like the model should be D~S, but this ignores R. If I include R, and train on D~S+R, how to I predict results knowing only D?
