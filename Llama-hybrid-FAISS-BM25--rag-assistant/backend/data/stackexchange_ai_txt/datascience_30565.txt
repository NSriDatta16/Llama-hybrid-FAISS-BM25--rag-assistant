[site]: datascience
[post_id]: 30565
[parent_id]: 30555
[tags]: 
It does make sense, they are just two different things. Dropout only makes your model learning harder, and by this it helps the parameters of the model act in different ways and detect different features, but even with dropout you can potentially overfit your traning set. On the other hand, early stopping prevents your model from overfitting by taking the best model on your validation data so far. However , for the sake of simplicity, I think it is easier to just use dropout (training a neural network is not easy and the training may not be successful due to many different reasons, it is a good practice to reduce the possible reasons why the training is failing as much as possible). Unless you have short time to train your network, with a sufficiently high amount of dropout you will ensure that your model is not overfitting. My final recommendation is: just use dropout. If using a 0.5 dropout rate still overfits, set a higher dropout rate.
