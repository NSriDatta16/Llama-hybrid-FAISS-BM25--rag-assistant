[site]: crossvalidated
[post_id]: 283268
[parent_id]: 
[tags]: 
Almost all predictions of a SVM are positives(or are all negatives)

I'm facing a binary classification problem using svm light. However using 5-fold-validation I noticed that later I train SVM with training set (Half positive and half negative samples about) the predictions on the test set(Half positive and half negative samples about) are almost all positives or all negatives causing an accuracy of 50% about. What could be the reason for this problem? Thanks in advance! Update: Added the code of k-fold-validation double gi::approximateOracle::k_fold_validation(const vector , labelType> > &mapped_samples ,const int * indices ,const int k,const string &C ,const double &gamma) { double error; double accuracy=0.0; double mean_accuracy; int num_samples = mapped_samples.size(); //num of samples int remainder = num_samples % k; //reminder of division num_samples/k int num_samples_less_remainder = num_samples - remainder; int base_dimension_one_fold = num_samples_less_remainder / k; //It's sure that rest of this division is zero. This is the base dimension of test set too int base_dimension_train_set = base_dimension_one_fold * (k-1); //dimension of one fold multiplied for k-1 (because one fold go in test set) /*In the k-fold-validation in turn a fold have to be inserted in test set and the others in testing set. Starting from * the last (those with index of the fold higher) in the test set, the first k-remainder times the training set dimesion * have to be base_dimension_train_set+remainder and the others remainder times the dimension of training set have to be * base_dimension_train_set+remainder-1 . The dimension of trainig set, in anlogous way, is the first k-remainder times * is base_dimension_one_fold and the other remainder times is base_dimension_one_fold+1 */ vector , labelType> > train_samples(base_dimension_train_set+remainder); vector , labelType> > test_samples; test_samples.reserve(base_dimension_one_fold+1); test_samples.resize(base_dimension_one_fold);//This operation and that above for avoid reallocation. for(int num_round = 1 ; num_round =0 && mean_accuracy and here the functions shuffle and kfold used for create the indices /*Implemented following Knuth shuffle. More details at https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle */ void shuffleArray(int* array,int size) { int k; int temp; int n = size; while (n > 1) { // 0 The indices are created in this way: srand(time(0)); indices = kfold(mapped_samples.size() , 5 ); //this method return dynamic memory The function build_classifier() do the training (there is there a call to a function of svm_light). testing() use the test set (writed in the file) and the model (writed in another file) for do predictions and return the accuracy and in another file return for each sample of the test set a number that point the prediction is positive or negative (>0 or not)
