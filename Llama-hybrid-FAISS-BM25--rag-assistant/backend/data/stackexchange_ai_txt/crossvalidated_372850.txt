[site]: crossvalidated
[post_id]: 372850
[parent_id]: 372848
[tags]: 
You didn't do anything wrong. The relevant comparison is test rmse (2.6) vs. the one obtained from cross-validation (3.8). So your model does even better on the hold-out test data than found by cross-validation. Possible reasons are the small sample size (i.e. luck) and spatial correlation across data lines. Especially for random forests, it does not make much sense to compare insample performance (rmse 1.5) with validation/test performance as a random forest is very greedily overfitting the training data. Instead of looking at insample performance, for random forests you could consider the out-of-bag performance (an implicit approximation of the true performance). Since you are working with the meta-package caret , this information might not be directly available if optimizing with usual cross-validation though.
