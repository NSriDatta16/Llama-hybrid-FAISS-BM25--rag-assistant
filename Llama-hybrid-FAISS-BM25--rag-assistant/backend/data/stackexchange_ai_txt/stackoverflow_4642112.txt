[site]: stackoverflow
[post_id]: 4642112
[parent_id]: 4624222
[tags]: 
You ask in a comment about possible alternative storage mechanisms. If the data you're storing is a fifth of the available RAM, I'd say that you need to manage memory very carefully, since you can't have the data copied much before you run out of RAM; and in any case you're going to be killing performance. You may be able to get sqlite to do the right thing for you with BLOBs since they are expected to potentially be large. It looks like you're using sqlite as a simple key/value store. Have you considered using flat files? If you need ATOMicity, you could still use an sqlite database to define which files in your flat file store are valid. You can safely do this by committing to the DB only after writing all flat files cleanly, and removing flat files only after committing the corresponding deletion in the DB. To make this work, you'll need some mechanism to serialise your dataObject to a file-type object in Python. Pickle can do this for you if you pass it a file-type object, but I suspect that it'll still be quite inefficient. You say you're doing numerical simulations; are you aware of numpy? numpy arrays support a tofile function that will be more efficient than pickling, and you might get a significant performance boost in your simulations if you're not using it already.
