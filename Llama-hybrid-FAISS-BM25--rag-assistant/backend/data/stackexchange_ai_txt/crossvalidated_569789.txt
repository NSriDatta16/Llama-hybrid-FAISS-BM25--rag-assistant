[site]: crossvalidated
[post_id]: 569789
[parent_id]: 569708
[tags]: 
My question is if it is possible to estimated the level of over-fit (or under-fit) just by looking (in a broad sense) at this single function and the given data set? Consider: you can design an experiment that collects data suitable for evaluation of a particular model performance aspect, which you then measure by comparing predictions with reference for the gathered samples/cases/specimen and summarizing these differences in a suitable fashion. E.g., as RMSE. This needs only predicted value and reference, your model can be entirely a black box. Some kinds of evaluation of prediction quality are possible, others not. Which, will depend on how the data set was put together. If you can only use the data that was already used for training (in addition to the restriction on the one given (even black box) model), this will severely hamper your ability to spot overfitting. How useful any figure of merit is if calculated on training data as opposed to an independent set of cases (and there may be various "levels" of independence) is a matter that you need to carefully judge. In my field, they can serve only as a stepping stone on the way to figures of merit based on independent cases. In addition, an experiment (and in consequence, the data set) may be designed so that various influencing factors can be measured, or cannot be measured. Regularization will often shrink the predictions towards the mean target during training. You can visualize this (in analytical chemistry, the so-called calibration plot plots prediction over reference and will clearly show this) or you could write down e.g. the regression function target = f (reference) and thus check for additive or multiplicative bias. Also, this allows you to measure certain aspects of bias . The "diagnosis" of over- or underfitting in addition requires you to weigh this against variance-type error. I.e., whether the model found an acceptable compromise or not, and acceptable will depend on application as well as e.g. what training data was available (the best possible compromise on a small data set may have considerably more systematic and more random error than the best possible compromise on a larger data set). Or, you may measure a certain aspect of overfitting by looking into repeated measurements of the same case which differ by some measurement noise. You may say the model is overfit if this measurement noise is amplified by the model and leads to unacceptably large differences in the predicted values. Whether you do this graphically or write down e.g. the variance is again your choice. This is even possible for trainig data if the training set was designed accordingly. If it wasn't: no chance. If you can get new data and get it predicted by the one model that is available, more is possible. I.e., you could design validation experiments that allow you to measure various aspects of performance/the influence of various factors on the prediction quality. This would be very hard to automate, since it requires substantial knowledge on potential influencing factors. You could also do what I as a chemist would call perturbation experiments with your black box regression model to measure sensitivity to particular influencing factors, while machine learning folks may call the same approach augmentation. Or dummy samples (@Mayeulsgc). Yet another data set may be representative for the expected application distribution of cases, and thus allow you to measure expected total error. If you want to estimate performance for future samples, you need a data set with future samples. So for example, this would not be possible for the data set you describe. And so on. A given data set may be perfectly suitable to check one or more such aspects of predictive performance. If you have the model as function and coefficients, and just lack the training algorithm that produced this model, and you have additional knowledge about the data at hand, you may be able to go a bit further. E.g. for certain types of data and models I frequently work with, I can spot certain aspects of overfitting (visually) by looking at the coefficient pattern. (For these data, there should be a certain smoothness for physical and mathematical reasons, if the coefficients look noisy, they are :-). The converse is not true: they may look smooth but still be overfit)
