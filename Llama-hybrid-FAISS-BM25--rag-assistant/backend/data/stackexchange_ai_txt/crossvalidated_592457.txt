[site]: crossvalidated
[post_id]: 592457
[parent_id]: 
[tags]: 
Should outliers be removed for goodness-of-fit tests?

If you allow a bit digression about the context: I am on a journey to better understand the power and usefulness of parametric distributions; I am a bit scared of them. Maybe due to the fact that I've entered the world of data analysis more from the side of "data science" & ML rather than from the side of pure statistics, I believed that all the answers lay in the dataset we have and that nonparametric statistics was THE best, safer answer. Reading around I also understood its limitations, so I'm willing to understand for once how parametric statistics fit into the picture and how can I take advantage of it. On thing I never fully understood is how to match those neat parametric distributions I see in stats courses with the ugly dirty distributions of real-world data . Actually, this would be the real question for me. The way I see it, there are so many distributions to chose from that I feel I should know of all them to pick the right one, and there is high chance of being wrong. However, I learned that there are tests like the Kolmogorov-Smirnov Test for goodness-of-fit which tests whether your data matches a given parametric distribution. Is running a KS test against all main distributions the ultimate solution? And if it is, what's still not clear to me is if I should remove outliers first.
