[site]: crossvalidated
[post_id]: 508369
[parent_id]: 399791
[tags]: 
You got the intuition right about F distribution being the ratio of one $\chi^2$ number over the other. The puzzle is solved easily if you start from the more general case of $q$ linear restrictions of the form $R\beta$ imposed on the coefficients $\beta$ where $R$ is the $q\times(p+1)$ matrix for $p$ bona fide coefficients (excluding the intercept). If you assume that the errors are normal and satisfy other Gauss-Markov conditions then when you look at the restrictions $R\hat\beta$ the follwing quadratic form ends up being a variable from F-distrobution: $$\frac{(R\hat\beta)'[Rs^2(X'X)^{-1}R']^{-1}(R\hat\beta)}{q}\sim F_{q,n-(p+1)}$$ It's easy to see why it's F-distribution because the numerator ( $\sim\hat\beta'\hat\beta$ ) and denominator ( $\sim s^2(X'X)^{-1}$ ) are $\chi^2$ . Next, you can show that the nested model testing will lead to the simple expression that you brought up with residual sums of squares. The restrictions for relevant variable test such as your look as follows: $R=[0,\dots,1,\dots,0]$ where non-zero elements are for those variables that are in unrestricted variable but not in restricted. You have $q$ rows in $R$ matrix, corresponding to $q$ variables that you are testing. You plug this $R$ into the expression and work out the math, which is simple but long and tedious. In machine learning context almost certainly you have a lot of data, so you can go for even simpler version of the test like $nR^2\sim\chi^2_q$
