[site]: crossvalidated
[post_id]: 350721
[parent_id]: 350716
[tags]: 
If Y3 is always exactly equal to one minus the sum of Y1 and Y2, as I think you are saying, then provide it no further consideration. You can treat this as a multivariate regression problem (i.e., 2 or more possibly dependent outputs per "input" data point), for which there are many links on this forum. If the errors are uncorrelated between Y1 and Y2 for any input data point, then you can solve this as a nonlinear least squares problem, just lumping together all the Y1 and Y2, i.e. $2n$ residuals if you have $n$ sets of Y1, Y2 (and Y3) observations. However, if the errors (distribution) are bigger for one of Y1 and Y2 than the other, then you can weight the Y1 vs. the Y2 residuals accordingly (which is a special case of what id described in the following paragraph), i.e., weighted least squares. More generally, if the errors in Y1 and Y2 are not uncorrelated for any given data point, you can incorporate this correlation into a weighting matrix, $W$, equal (or proportional) to the inverse of the error covariance matrix $C$ and use generalized least squares.. If you stack all $2n$ residuals into a vector $r$, you would minimize $\frac{1}{2}r^TWr$. If you let $R$ be the upper triangular Cholesky factor of $C$, i.e., such that $R'R = C$, then you could form the adjusted residuals $r_{adj} = R^{-1}r$, for which the residuals would have covariance matrix equal to the identity, resulting in the problem, minimize $\frac{1}{2}r_{adj}^Tr_{adj}$. By taking this latter approach, you can apply standard "off the shelf" nonlinear least squares software to the quantity $2n$ of adjusted residuals. So the key is paying attention to the relation of the errors across Y1 and Y2, and more generally across all the data points, and proceeding accordingly.
