[site]: datascience
[post_id]: 122211
[parent_id]: 
[tags]: 
Should a Learning Rate Scheduler adjust the learning rate by optimization step (batch) or by epoch?

In PyTorch doc , it suggests torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs . However, from other sources it looks like the learning rate should be adjusted in every optimization step (batch) : https://kikaben.com/transformers-training-details/ Warmup steps in deep learning https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd So, Should the learning rate in a Learning Rate Scheduler be adjusted at each optimization step (batch) or at each epoch? Is there a definitive answer to this, or it depends on the model? For transformer models, it looks like the learning rate is adjusted at every step (training batch). In the following example, there are a few thousands of steps so I think it cannot be epochs, is that right? https://nn.labml.ai/optimizers/noam.html
