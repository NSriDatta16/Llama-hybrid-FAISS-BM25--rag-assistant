[site]: crossvalidated
[post_id]: 271119
[parent_id]: 271113
[tags]: 
This isn't a question specific to GBMs, but a general machine learning question. In a classification task, most algorithms take in feature values and output what's called a "posterior probability", a number from 0 to 1 indicating the probability of being in the positive class based on your training data . (The bolded part is actually important, because your training data might not always accurately reflect the distribution of the real problem space, e.g., in unbalanced datasets). The choice of cutoff for this posterior probability is entirely up to you. In the construction of the ROC curve, you essentially sample cutoffs to obtain different points trading off true positive rate and false positive rate. Finally, also note that GBMs are a specific algorithm that uses gradient boosting and are not equivalent terms. Gradient boosting is a much more general idea implemented in many different algorithms.
