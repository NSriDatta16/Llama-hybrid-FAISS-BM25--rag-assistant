[site]: datascience
[post_id]: 107538
[parent_id]: 107208
[tags]: 
I've never had the pleasure of working on AI in a space like this. And boy would I love to. you have so many good things working for you you don't need a dataset, as playing the game and winning effectively validates itself Judging model performance is fairly easy, as playing the game and winning means your better than the thing you played against (generally) If I were you, I would use this to your advantage. Adversarial training strategy In terms of how to train and test your model, this seems like the best approach to me. Have your model play the game against other models, and be positively or negatively reinforced based on the outcome of that game. Exactly how you choose to do this is up to you, but some things to consider: you might want to start with small games so your models can "learn the basics" you may want to employ regularization strategies, like batching games, dropout layers, etc. with a one player game, whoever gets the better score wins. Evolutionary hyperparameter selection Some people mentioned evolutionary strategies in the comments. Something that particularly resonates with me is NEAT , and how it handles "breeding" models in "species". You may be interested in this for your problem. Not only could you use an adversarial strategy to adjust model weights, you could also have the win/loss ratio for a particular model function as it's fitness, allowing you to breed hyperparameter settings for a particular "species" of model. That would allow you to explore multiple modeling paradigms and hyperparameter settings simultaneously. In essence, this would be a smarter version of random search hyperparameter tuning. Model choice In terms of the actual model chosen, I must admit this isn't my field of expertise. Some people mentioned canned models designed for the use case, which are probably fine choices. Some modeling strategies that seem like they would work in my opinion: (keep in mind, you can have these compete against each other by encapsulating in an evolutionary strategy) RNN iteratively picking up cards and deciding on tasks based on that card, and the previous cards, seems like a task RNN's are designed to handle. Particularly an LSTM seems like a popular choice. You could mount an LSTM as your input layer, and pass the state between successive predictions to get an efficient way to encapsulate a long state space A really big multi dimensional NN you could also have a dimension for each card, and a big long line of them. You could then compress that into a much smaller series of dense layers. That seems like it may be an ok way of distilling your large space into a more abstractaction, smaller dimension space quickly. ensemble decision tree It seems like the decisions are pretty simple, the issue is you have a lot of them. You could use an ensemble of decision trees looking at different aspects of the historical record of past cards, and attempt to use them as a "funnel" to spit out a sensible solution.
