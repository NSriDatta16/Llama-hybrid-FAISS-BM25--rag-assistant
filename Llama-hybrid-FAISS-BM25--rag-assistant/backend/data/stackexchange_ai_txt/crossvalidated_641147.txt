[site]: crossvalidated
[post_id]: 641147
[parent_id]: 
[tags]: 
My WGAN-GP isn't capturing bi-modal distributed underlying data

I am training a WGAN to generate a vector of 6 data points (6 dimensional output). I plotted a correlation matrix of my dataset to see the underlying distribution of the 6 data points, and the correlations between each of the data points. This way I can see if my WGAN has captured correlations between the data points and if it captures the underlying distribution of each data point. The following is the correlation matrix of the real dataset. We can observe that the top 2 data points have bimodal underlying distributions. The bottom 4 data points have unimodal underlying distributions. After training for around 10,000 epochs, my generator has learnt the correlations between the 6 points very well. In addition, it has learnt the underlying distributions to near perfection for the bottom 4 data points (which are unimodally distributed in the underlying). However a big issue is it never seems to learn the underlying bimodally distributed data for the top 2 data points. The issue is I'm not sure why my generator would assume a unimodal underlying for all the data points. Isn't the whole point of a GAN is that it is meant to learn the underlying distribution without assuming anything? What could be the cause of this? The following is the architecture of both my generator and discriminator (They have the same architecture, only difference is discriminator output layer is only 1 instead of 12. As it is outputting a vector of dimension 12, I simply take the middle 6 values and plotted the correlation matrix of the middle 6 data values (as they're the only important values). The random noise is sampled from a gaussian distribution. Does my architecture need to change? Is there any underlying distribution a WGAN assumes that I have missed? I have read many papers and none of them seem to encounter this issue. Also another point to note is this is a conditional GAN hence there are 2 inputs to the generator. class Generator(nn.Module): def __init__(self, latent_dim=6, n=6): super(Generator, self).__init__() self.h_11 = nn.Linear(latent_dim,64).to(device) self.h_12 = nn.Linear(n,64).to(device) self.h_21 = nn.Linear(64,64).to(device) self.h_22 = nn.Linear(64,64).to(device) self.h_32 = nn.Linear(128, 128).to(device) #output layer self.output = nn.Linear(128, 12).to(device) def forward(self, Z_t, S_t): Z_t = Z_t.to(device) # Move input tensor to the device S_t = S_t.to(device) # Move input tensor to the device h_11_output = torch.relu(self.h_11(Z_t)) h_21_output = torch.relu(self.h_21(h_11_output)) h_12_output = torch.relu(self.h_12(S_t)) h_22_output = torch.relu(self.h_22(h_12_output)) # Concatenation h_31_output = torch.cat((h_21_output, h_22_output), dim=1) h_32_output = torch.relu(self.h_32(h_31_output)) output = self.output(h_32_output) return output
