[site]: datascience
[post_id]: 24336
[parent_id]: 24303
[tags]: 
Perfect training AUC is the hallmark of overfitting. When searching parameters, I often use a combination of test set AUC and the difference between training AUC and test AUC. When just using test set AUC in the search loss function, I found the model to still be pretty sensitive to which random set of test rows it sampled. I also agree with the commenters that you should get better results with much smaller max_depth, probably on the order of 1-10, and ensemble models. Random forests are an excellent place to start.
