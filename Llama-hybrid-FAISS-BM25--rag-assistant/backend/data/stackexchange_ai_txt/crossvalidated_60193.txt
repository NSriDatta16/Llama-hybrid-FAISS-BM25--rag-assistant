[site]: crossvalidated
[post_id]: 60193
[parent_id]: 
[tags]: 
F-test in Regression and Crossvalidation

I am wondering what is the better measure of predictive power in a multivariate regression setting. So an F-test for regression tests "the utility of the model", or exactly, if any of the $\beta$ coefficients are non-zero. When the null is rejected, the model is considered "useful"; so, useful for prediction? Crossvalidation also can help tell you if your model is useful for prediction. Can you talk about them using the same phrasing? What about the $R^{2}$ statistic? Say $n$ is large, and the degrees of freedom in your model is small. It is pretty unlikely overfitting has occurred. What can you say about the predictive power of your model in this case when $R^{2}$ is high? medium? zero? Examining the predictive power of some model is a huge subject, but do F-tests play into that? Has crossvalidation become so popular because of machine learning methods where statistical tests don't exist? Was it popularized to fill that void in algorithmic models, or do they really address different issues?
