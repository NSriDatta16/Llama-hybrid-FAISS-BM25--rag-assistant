[site]: crossvalidated
[post_id]: 128712
[parent_id]: 128692
[tags]: 
The problem isn't as profound as it may appear. Because $$\frac{1}{n} \sum_{i=1}^{n} e^{\phi(X_i)} = e^Y\frac{1}{n} \sum_{i=1}^{n} e^{\phi(X_i)-Y} $$ for $$Y = \max_i\{\phi(X_i)\},$$ an algebraically equivalent expression is $$\mu = Y -\log n + \log \sum_{i=1}^{n} e^{\phi(X_i)-Y}. $$ In this one there will be no difficulties computing the log of the sum, which necessarily lies between $1$ and $n$ (since all the exponents are non-positive). In particular, there is no chance of overflow and any underflow will be absorbed (to high precision) in the summation, where at most $\log_2 n$ bits will be lost (and almost certainly the loss in precision will be less than around $ \frac{1}{2}\log_2 n$ bits). If you have any concern about precision losses, sum the terms in ascending order of $\phi(X_i)$. The same approach applies to computing the moments needed to obtain an estimated standard deviation. Using a normal approximation may be unwise unless you are sure that all the $\exp(\phi(X_i))$ will be well within an order of magnitude of each other (which means the $\phi(X_i)$ should all lie within an interval of around $2$ or less). Even then you might need a fairly large value of $n$. If just a few of those values dominate the others, then the averaging-out that justifies this approximation will not occur, regardless of the size of $n$ .
