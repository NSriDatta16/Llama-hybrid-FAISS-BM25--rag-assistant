[site]: crossvalidated
[post_id]: 642017
[parent_id]: 
[tags]: 
Ensuring numerical stability of probabilities in identifiable multinomial regression

I'm looking to understand the differences between the functions for probabilities induced in an identified multinomial GLM, and the softmax function commonly used in classification for neural networks. Consider two "versions" of the softmax function. First, the softmax used in a neural network. Suppose we have $K$ classes, and $\eta_{ij}$ are the nodes in the final layer of a network, or in a multiclass regression we may view them as linear predictors, where $j$ represents the class, and $i$ the unit of data. We have $$\mathbb{P}(Y_i = k) = \frac{e^{\eta_{ik}}}{\sum_{j = 1}^K e^{\eta_{ij}}}.$$ Note that $\sum_{k = 1}^K \mathbb{P}(Y_i = k) = 1$ , so the formula yields valid probabilities. In the machine learning literature, I frequently see the following modification used to avoid numerical overflow. Let $c = \max \{\eta_{ij}\}_{j=1}^K$ , and write $$ \mathbb{P}(Y_i = k) = \frac{e^{\eta_{ik}-c}}{\sum_{j = 1}^K e^{\eta_{ij}-c}}. $$ If we let $k^* = \arg \max \{\eta_{ij}\}_{j=1}^K$ , and $\eta_{ij} - c = \eta_{ij}^*$ , we have $$ \mathbb{P}(Y_i = k) = \frac{e^{\eta_{ik}^*}}{1 + \sum_{j \neq k^*} e^{\eta_{ij}^*}}, \,\,\,\,\,\,\, k \neq k^*, $$ and $$ \mathbb{P}(Y_i = k^*) = \frac{1}{1 + \sum_{j \neq k^*} e^{\eta_{ij}^*}}, \,\,\,\,\,\,\, k = k^*. $$ Since each $\eta_{ij}^* \leq 0$ , each $e^{\eta_{ij}^*}$ is bounded between $0$ and $1$ , so the probabilities are stably computed, and still sum to $1$ . The other version of the softmax function, used as the link in a multinomial GLM, is motivated by observing that the original formula above is overparametrized. We avoid this by fixing one of the linear predictors to $0$ e.g. $\eta_{iK} = 0$ , writing $$ \mathbb{P}(Y_i = k) = \frac{e^{\eta_{ik}}}{1+\sum_{j = 1}^{K-1} e^{\eta_{ij}}}, \,\,\,\,\,\,\, k = 1, \ldots, K-1 $$ and let $$ \begin{split} \mathbb{P}(Y_i = K) &= 1-\sum_{k = 1}^{K-1}\mathbb{P}(Y_i = k) \\ &= \frac{1}{1+\sum_{j = 1}^{K-1} e^{\eta_{ij}}} \end{split} $$ While this is identified, the function could still potentially be unstable. There are no constraints on $\eta_{ij}$ , so if any are sufficiently large, some probabilities could overflow to infinity/nan values. My question: in the identified (second) version of the softmax commonly used in multinomial regression, how would one go about avoiding numerical overflow? It seems natural to me to do something similar as with the first, subtracting the maximums, but care must be taken to ensure the probabilities sum to $1$ , by rewriting $1 = e^{0}$ and subtracting the maximums $c$ from the exponents of these terms as well.
