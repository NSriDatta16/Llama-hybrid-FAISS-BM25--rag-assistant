[site]: crossvalidated
[post_id]: 418335
[parent_id]: 
[tags]: 
How to compare two bayesian linear models?

Let say, we have two bayesian linear models learned from some data (not necessarily same but from the same data distribution), how can we compare or what is the notion of similarity between the two models? For example, consider two bayesian linear models (second order) as given below: $$f(x) = \sum_{i=1}^n \alpha_i x_i + \sum_{i=1}^n\sum_{j=1}^n \alpha_{ij}x_i x_j \quad\quad (1)$$ $$f(x) = \sum_{i=1}^n \beta_i x_i + \sum_{i=1}^n\sum_{j=1}^n \beta_{ij}x_i x_j\quad \quad (2)$$ where each $\alpha_i$ and $\beta_i$ is distributed according to a posterior learned over data. My initial thought was to compare the KL-divergence of each $\alpha_i$ with corresponding $\beta_i$ but I am not sure whether it is a good idea. To describe more context, once the models are learned, I would optimize the model over the input space $x$ to find the best point with the trained model as the objective function. This can be thought of as some form of thompson sampling. My overall goal is to compare how similar the two optimization problems will be over these two models. Could you please explain some mechanism or insight to find how similar they are? I am also looking to extend the same idea to two gaussian processes trained on different sampled data sets but from the same data distribution.
