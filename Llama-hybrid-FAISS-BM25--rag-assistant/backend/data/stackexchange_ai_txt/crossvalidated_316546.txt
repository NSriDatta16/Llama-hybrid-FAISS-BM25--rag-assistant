[site]: crossvalidated
[post_id]: 316546
[parent_id]: 315279
[tags]: 
I believe this is a pure exploration bandit problem. Bubeck has a nice paper on it on arXiv. From the introduction: Our setting is as follows. The forecaster may sample the arms a given number of times n (not necessarily known in advance) and is then asked to output a recommended arm. He is evaluated by his simple regret, that is, the difference between the average payoff of the best arm and the average payoff obtained by his recommendation. They provide upper and lower bounds for a number of strategies. Which strategy is best depends on how many rounds are allowed, as they explain in their conclusions: To make short the longer story described in this paper, one can distinguish three regimes, according to the value of the number of rounds n. The statements of these regimes (the ranges of their corresponding n ) involve distribution- dependent quantifications, to determine which n are considered small, moderate, or large. For large values of n, uniform exploration is better (as shown by a combination of the lower bound of Corollary 2 and of the upper bound of Proposition 1). For moderate values of n, sampling with UCB($\alpha$) is preferable, as discussed just above (and in Section Appendix A.2). For small values of n, little can be said and the best bounds to consider are perhaps the distribution-free bounds, which are of the same order of magnitude for the two pairs of strategies
