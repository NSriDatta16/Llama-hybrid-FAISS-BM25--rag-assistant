[site]: stackoverflow
[post_id]: 2059501
[parent_id]: 2059251
[tags]: 
(You said that your variables are ints, a and b is what I've used below, and I'm using this assumption: int a, b; .) The problem with taking the average of two numbers by adding and then dividing is you might overflow (as Laurence Gonsalves alluded to in his comment). If you know that the sum won't exceed INT_MAX , then using int is fine, and nothing more needs to be done: int avg = (a + b) / 2; If the sum might overflow, then moving up to a type which won't overflow, such as long , is fine; however, remember that long and int might be the same size ( INT_MAX might equal LONG_MAX ), in which case this doesn't help, except that INT_MAX would be much largerâ€”at least 2,147,483,674. int avg = ((long)a + b) / 2; Note that when b is added to a , b is automatically converted to a long . You might need to cast the final result to int to avoid warnings (including from a lint program): int avg = (int)(((long)a + b) / 2); However, remember that integer limits are defined by the implementation; the real concern is whether the addition could ever be more than INT_MAX and casts are just a way to explicitly specify type and thus avoid that unintentional overflow. When you want to divide by two, use / 2 , don't try to get clever with bitshifts. Compilers have been smart enough to optimize this kind of thing for years, especially when you use constants, and code clarity is more important.
