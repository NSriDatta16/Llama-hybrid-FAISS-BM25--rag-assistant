[site]: crossvalidated
[post_id]: 498958
[parent_id]: 
[tags]: 
Loss function for regression

I am studying Christopher's Bishop book on "pattern recognition and machine learning". I have come across the regression loss function before, usually it is expressed as $$\sum_{i=1}^N \{t_i - y(x_i)\}^2$$ where $t_i$ represents the true value, $y(x_i)$ represents the function to approximate $t_i$ . In the book however, the regression loss is written in the form $$E[L] = \int \int L(t,y(x))p(x,t)dx \,dt$$ The expectation is taken with respect to samples from the joint distribution of $p(x,t)$ . How do we go about thinking about the joint distribution of $p(x,t)$ ? How do we actually compute the joint distribution for $p(x,t)$ in the regression sense. For classification, the Naive Bayes algorithm can be used to compute the distribution for $p(x,C)$ where $C \in \{C_1, C_2, ..., C_k\}$ classes from the data itself by combining the likelihood and the prior. Hence, $p(x,C_i)$ for classification is just a scalar value.
