[site]: datascience
[post_id]: 55513
[parent_id]: 55508
[tags]: 
You mentioned that you're dealing with time-dependent observations, and I'm guessing that's why you chose a train-test split for evaluation. If you continue along this path, then I think you just need a larger sample of class2 in your test set. How did you come up with 25 examples of class2 in your testing set? You should be using stratified sampling to produce your train-test sets. It looks like you started with a 60-40 train-test split with class1. If so, then your datasets should have the following distributions: Train: 900,000 of class 1, 60,000 of class2 Test: 600,000 of class1, 40,000 of class2 Since you have a decent amount of data, you might want to train with more than 60%. An 80-20 split would probably provide a better estimate of your model's final accuracy. In any case, I think you can do better than train-test splitting! There's a modified version of cross-validation designed for time-dependent data . And scikit-learn has an implementation: TimeSeriesSplit If you go this route, then you will probably want to use different weights for your classes. Since class1 outnumbers class2 15:1, then you could assign a weight of 1 to class1 and 15 to class2 (assuming you care equally about the two classes). To answer a few more of your questions: Should I undersample class1 from my test data? I don't think this will help since you're already adjusting for the class imbalance with sample weighting. Are there more parameters besides class weights that I should try to tune? You should always tune model hyperparameters :) For Random Forests, the most salient parameters are probably the number of trees, the max depth, and minimum impurity decrease.
