[site]: datascience
[post_id]: 93768
[parent_id]: 
[tags]: 
Dimensions of Transformer - dmodel and depth

Trying to understand the dimensions of the Multihead Attention component in Transformer referring the following tutorial https://www.tensorflow.org/tutorials/text/transformer#setup There are 2 unknown dimensions - depth and d_model which I dont understand. For example, if I fix the dimensions of the Q,K,V as 64 and the number_of_attention_heads as 8 , and input_embedding as 512 , can anyone please explain what is depth and d_model ?
