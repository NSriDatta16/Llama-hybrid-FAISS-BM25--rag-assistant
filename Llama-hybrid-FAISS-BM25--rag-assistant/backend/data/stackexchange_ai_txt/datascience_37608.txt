[site]: datascience
[post_id]: 37608
[parent_id]: 37598
[tags]: 
It depends. If your data is small and can be computed via online methods or within memory of a decent server, you'll probably be ok just using normal machine learning libraries like sk-learn or shogun. However, in the real world, companies collect millions of data points with thousands of features. Holding this in memory becomes impossible or computationally impossible to run on a single machine. Prior to Spark, and after mid 2000's, we handled this issue by writing MapReduce jobs and implementing distributive algorithms ie methods that could be ran across hundreds of machines and then combined again to get the same result. The problem was that MapReduce had a big issue with iterative algorithms, streaming data or being interactive. Spark was introduced to solve this problem by the use of RDD and lineage. So essentially, you should use spark if you have a lot of data and you want to apply some iterative algorithm, stream data or have an interactive feel with the application. Otherwise, you'll probably be ok with traditional MR or if your data is small using python and sklearn. If you're new to ML, I would recommend sticking with python and sklearn for the time being until you become comfortable with the algorithms there and framework.
