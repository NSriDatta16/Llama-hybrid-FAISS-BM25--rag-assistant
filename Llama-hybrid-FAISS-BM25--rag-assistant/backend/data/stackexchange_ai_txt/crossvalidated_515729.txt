[site]: crossvalidated
[post_id]: 515729
[parent_id]: 237975
[tags]: 
You ask in a comment: Let's say we backprop the matrix. If we do that we will get 32 gradient updates for each parameter in the neural net. For each parameter, can't we just take the average of the 32 gradients and use that in gradient descent? Well, suppose you have 1,000,000 parameters. You're suggesting that we calculate 32,000,000 partial derivatives and then average them, 32 at a time, in order to get 1,000,000 partial derivatives that we can then apply to the parameters. If the cost is a scalar, on the other hand, then we only need to calculate 1,000,000 partial derivatives in the first place, and then we can apply them to the parameters immediately, without needing to do any averaging first. So, you're asking if we can't "just" do 32 times as much work. And the answer is yes, we can... but it's 32 times as much work. Isn't doing that different than just taking a mean of the output activations and backpropping the scalar? No, it's the same. Suppose you have several loss functions $L_1$ , $L_2$ , $L_3$ , $L_4$ that you want to optimize by changing some parameter $p$ . Then the average of the derivatives, $$\frac14 \left (\frac{\partial L_1}{\partial p} + \frac{\partial L_2}{\partial p} + \frac{\partial L_3}{\partial p} + \frac{\partial L_4}{\partial p} \right),$$ is simply the derivative of the average, $$\frac14 \cdot \frac{\partial}{\partial p} (L_1 + L_2 + L_3 + L_4).$$ So, taking the average first and then doing backpropagation is equivalent to first doing backpropagation and then taking the average. It's also much faster.
