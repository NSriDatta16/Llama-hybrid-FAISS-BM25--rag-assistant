[site]: crossvalidated
[post_id]: 467586
[parent_id]: 467583
[tags]: 
Yes, but the strength of my Yes varies by the assumption. For instance, one of the assumptions of OLS is that the model is linear: $y=X\beta+\varepsilon$ , You should agree with me that if this assumption is broken, then predicting becomes problematic. Also think about autocorrelation: if $E[X\varepsilon]>0$ then larger $X$ would tend to underestimate and predict with bias. etc. Machine learning is almost entirely an interpolation exercise. You make the network weights remember a lot of combinations of inputs then interpolate between them when you get a new input. Although the formulae can be exactly the same, there is a difference in application. In a typical OLS setup you get some data X, then try to come up with a few coefficients $\beta$ , typically much smaller set of them compared to the umber of observations. In ML you often devise much larger set of weights, even larger than the number of observations often. In this case the weights effectively "memorize" the mapping of $y$ to combinations of $X$ . The OLS machinery is simple used as a way to memorize this mapping. The trouble comes when new $X'$ comes that is completely out of the field of previous $X$ that the algorithm has seen so far. In this case it's easy to get a nonsensical result.
