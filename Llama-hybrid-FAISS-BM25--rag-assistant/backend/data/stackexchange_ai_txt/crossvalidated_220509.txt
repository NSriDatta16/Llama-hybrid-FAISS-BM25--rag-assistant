[site]: crossvalidated
[post_id]: 220509
[parent_id]: 220507
[tags]: 
In the probability model underlying linear regression, X and Y are random variables. if so, as an example, if Y = obesity and X = age, if we take the conditional expectation E(Y|X=35) meaning, whats the expected value of being obese if the individual is 35 across the sample, would we just take the average(arithmetic mean) of y for those observations where X=35? That's right. In general, you cannot expect that you will have enough data at each specific value of X, or it may be impossible to do so if X can take a continuous range of values. But conceptually, this is correct. yet doesn't the expected value entail that we must multiply this by the probability of occurring ? This is the difference between the unconditional expectation $E[Y]$ and the conditional expectation $E[Y \mid X = x]$. The relationship between them is $$ E[Y] = \sum_x E[Y \mid X = x] Pr[X = x] $$ which is the law of total expectation. but how in that sense to we find the probability of the X-value variable occurring if it represent something like age? Generally you don't in linear regression. Since we are attempting to determine $E[Y \mid X]$, we don't need to know $Pr[X = x]$. If we don't assume the independent variables are themselves random variables, since we don't obverse the probability, what do we assume they are? just fixed values or something? We do assume that Y is a random variable. One way to think about linear regression is as a probability model for $Y$ $$ Y \sim X \beta + N(0, \sigma) $$ Which says that, once you know the value of X, the random variation in Y is confined to the summand $N(0, \sigma)$.
