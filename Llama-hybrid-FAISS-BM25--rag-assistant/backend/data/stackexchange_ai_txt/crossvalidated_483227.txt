[site]: crossvalidated
[post_id]: 483227
[parent_id]: 483185
[tags]: 
This is a complementary to Andrey's answer (+1). When looking for a generally accepted reference on AUC-ROC values, I came across Hosmer's " Applied Logistic Regression ". In Chapt. 5 " Assessing the Fit of the Model ", it emphasised that " there is no “magic” number, only general guidelines ". Therein, the following values are given: ROC = 0.5 This suggests no discrimination, (...). 0.5 0.7 $\leq$ ROC 0.8 $\leq$ ROC ROC $\geq$ 0.9 We consider this outstanding discrimination. These values are by no means set-to-stone and they are given without any context. As Star Trek teaches us: " Universal law is for lackeys, context is for kings " , i.e. (and more seriously) we need to understand what we are making a particular decision and what our metrics reflect. My guidelines would be: For any new task we should actively look at existing literature to see what is considered competitive performance. (e.g. detection of lung cancer from X-ray images) This is practically a literature review. If our tasks is not present in literature, we should aim to provide an improvement over a reasonable baseline model. That baseline model might be some simple rules of thumb, other existing solutions and/or predictions provided by human rater(s). If we have a task with no existing literature and no simple baseline model available, we should stop trying to make a "better/worse" model performance comparison. At this point, saying " AUC-R0C 0.75 is bad " or " AUC-ROC 0.75 is good " is a matter of opinion.
