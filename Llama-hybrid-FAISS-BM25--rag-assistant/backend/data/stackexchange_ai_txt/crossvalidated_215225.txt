[site]: crossvalidated
[post_id]: 215225
[parent_id]: 
[tags]: 
n/m asymptotics and logistic regression goodness of fit tests

in the case of logistic regression, say our model has $p$ covariates and we have $J$ distinct covariate patterns (distinct combinations of the values of the p covariates) and we have a total of $n$ observations. The number of covariate patterns with pattern $i$ is denoted by $m_i$. Then, let $y_j$ denote the number of success responses out of the $m_j$. We have the statement: The distribution of goodness of fit tests are obtained by letting $n \to \infty$. They are said to be $n$ asymptotic. If we fix $J I am trying to understand why the deviance and pearson chi squared statistics produce incorrect $p$-values in the case that $J \approx n$. The text by Hosmer, Lemeshow states that when $J \approx n$, the distribution of both the pearson residual and the deviance residual is achieved under n asymptotics, and hence, the number of 'parameters' is increasing at the same rate as the sample size. I don't quite understand why this is problematic, I'm looking for an intuitive explanation of this
