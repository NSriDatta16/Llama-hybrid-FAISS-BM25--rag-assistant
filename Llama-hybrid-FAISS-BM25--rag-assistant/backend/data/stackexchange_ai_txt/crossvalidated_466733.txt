[site]: crossvalidated
[post_id]: 466733
[parent_id]: 
[tags]: 
Interpretation of pooling in Graph Neural Networks

The paper Hierarchical Graph Pooling with Structure Learning (2019) introduces a distance measure between: a graph's node-representation matrix $\text{H}$ , and an approximation of this constructed from each node's neighbours' information $\text{D}^{-1}\text{A}\text{H}$ : Here, we formally define the node information score as the Manhattan distance between the node representation itself and the one constructed from its neighbors: $$\mathbb{p} = \gamma(\mathcal{G}_i) = ||(\text{I}^{k}_{i} - (\text{D}^{k}_{i})^{-1}\text{A}^{k}_{i})\text{H}^{k}_{i}|| $$ (where $\text{A}$ and $\text{D}$ are the Adjacency and Diagonal matrices of the graph, respectively) Expanding the product on the RHS we get (ignoring index notation for simplicity): $$||\text{H} - (\text{D}^{-1}\text{A}\text{H})||$$ Problem: I don't see how $\text{D}^{-1}\text{A}\text{H}$ is a "node representation... constructed from its neighbors". $\text{I} - \text{D}^{-1}\text{A}$ is clearly equivalent to the Random Walk Laplacian , but it's not immediately obvious to me how multiplying this by $\text{H}$ provides per-node information on how well one can reconstruct a node from its neighbours.
