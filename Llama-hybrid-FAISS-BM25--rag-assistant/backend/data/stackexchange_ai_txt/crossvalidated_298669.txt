[site]: crossvalidated
[post_id]: 298669
[parent_id]: 
[tags]: 
Inference on joint Poisson and Binomial observations

Pick a positive integer $N$. I want to know $N$, but you won't tell me. Instead, you tell me four things: Draw a random number $n_{p}$ from a Poisson distribution defined by its expected value $\lambda = hN$; tell me $n_{p}$. Draw a random number $n_{b}$ from a Binomial distribution defined by $N$ trials and probability $p$ of success in each trial; tell me $n_{b}$. Tell me $h$ (which is a positive real number) Tell me $p$ (which is a positive real number less than $1$) I want to infer $N$. How should I do so? In my ignorance, I tried picking an $N$ which maximizes the product: $\left(\frac{(hN)^{n_{p}}\exp^{-hN}}{n_{p}!}\right)\left(\frac{N!}{n_{b}!(N-n_{b})!}p^{n_{b}}(1-p)^{N-n_{b}}\right)$ Unfortunately, this seems to systematically underestimate $N$. I assume there's a 'right' way to do this. Please teach me! Here's simple code that maximizes that likelihood. Note that it systematically underestimates $N$. I'd love it if there were a "standard" way to estimate $N$ that was unbiased. #!/usr/bin/python3 import numpy as np from numpy import log as ln from scipy.special import gammaln N_true = 50 # To be inferred p = 0.90 # Binomial probability of success per trial h = 3.9 # Scale factor for Poisson distribution N_inferred = [] for trial in range(10000): n_p = np.random.poisson(h*N_true) n_b = np.random.binomial(N_true, p) def poisson_neg_log_likelihood(N): return -(n_p*ln(h*N) + -h*N - gammaln(1 + n_p)) def binomial_neg_log_likelihood(N): return -(gammaln(1+N) - gammaln(1+n_b) - gammaln(1+N-n_b) + n_b*ln(p) + (N-n_b)*ln(1-p)) N_candidates = np.arange(max(n_b, 1), 2*(1+n_b)/(1-p)) nlls = (poisson_neg_log_likelihood(N_candidates) + binomial_neg_log_likelihood(N_candidates)) N_inferred.append(N_candidates[nlls.argmin()]) average_inference = np.mean(N_inferred) print("My average inference for N is:", average_inference) if average_inference
