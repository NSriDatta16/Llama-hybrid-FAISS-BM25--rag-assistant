[site]: crossvalidated
[post_id]: 442955
[parent_id]: 
[tags]: 
Randomness in parameters per se captures the incomplete knowledge on the phenomenon: analysis in Bayesian models

I have been studying some books on uncertainty quantification for stochastic systems: Numerical Methods for Stochastic Computations: A Spectral Method Approach and Spectral Methods for Uncertainty Quantification: With Applications to Computational Fluid Dynamics . Given a response $Y$ with a model of the form $Y=f(\theta)$ , where $f$ is a deterministic map and $\theta$ is a random variable/vector, the goal is to obtain statistical information of $Y$ (statistics such as the mean, the variance, etc., the probability density function...) given the probability law of $\theta$ . In general, the model is expressed via ordinary differential equations, partial differential equations, etc. For example, consider the exponential growth model $X'(t)=\alpha X(t)$ , $X(0)=\beta$ , where $\alpha$ and $\beta$ are random variables. Let $\theta=(\alpha,\beta)$ . Given a time $t_1$ of interest, let $Y=X(t_1)$ . Let $f(x)$ be the solution at time $t_1$ ( $f(x)=\beta e^{\alpha t_1}$ ). The differential equation model becomes $Y=f(\theta)$ . If two times are of interest, say $t_1$ and $t_2$ , let $Y=(X(t_1),X(t_2))$ and $f(x)=(f_1(x),f_2(x))$ be the solution at times $t_1,t_2$ ( $f(x)=(\beta e^{\alpha t_1},\beta e^{\alpha t_2})$ ); then $Y=f(\theta)$ . Several methods exist for uncertainty quantification: Monte Carlo simulation, perturbation methods, generalized polynomial chaos expansions, etc. The study of this type of models with uncertainties is usually justified by accounting for model errors and data errors. Model errors exist due to the incomplete knowledge of the true physics. Data errors arise due to measurement errors. I have doubts on this interpretation. Let us consider a Bayesian model: given $t_1,\ldots,t_m$ times of interest and $Y=(Y_1,\ldots,Y_m)$ the vector of responses at those instants, the likelihood is expressed as $Y|\theta\sim\prod_{i=1}^m\text{Normal}(f_i(\theta),\sigma^2)$ , where $\sigma^2$ is the error variance. The random parameters of the model have prior distributions. This likelihood constrasts with the model of the first paragraph, $Y|\theta=f(\theta)$ , $f=(f_1,\ldots,f_m)$ , which does not have error variance (i.e. $\sigma^2=0$ ). I wonder why the randomness of the parameters $\theta$ is not sufficient to account for modeling errors. Why is $\sigma^2>0$ needed? I would understand the addition of variance errors in a frequentist nonlinear regression, where the parameters are treated as constants. But not in this case.
