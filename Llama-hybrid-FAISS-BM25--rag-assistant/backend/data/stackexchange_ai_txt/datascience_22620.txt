[site]: datascience
[post_id]: 22620
[parent_id]: 19124
[tags]: 
Something that I have learned the hard way is to plot the learning curves, I know, it is not as fun as writing the machine learning code per-se, but it is fundamental to visually understand what is happening. A rule of thumb definition is that over fitting occurs when your train accuracy keeps improving while your validation accuracy stops improving (or even starts getting worse). Simplest solution to avoid over fitting is early stopping (stop training as soon as things look bad), of course being the simplest solution comes at a cost: it is not the best solution. Regularisation and dropout are good tools to fight over fitting, but that is a different matter :) Hope it helps
