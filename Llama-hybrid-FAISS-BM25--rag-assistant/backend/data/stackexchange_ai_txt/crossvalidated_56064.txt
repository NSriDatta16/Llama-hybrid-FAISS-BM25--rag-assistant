[site]: crossvalidated
[post_id]: 56064
[parent_id]: 56062
[tags]: 
Averaging the weights of different networks won't work. It is not an implementation error. One possibility is to train several networks using bootstrap on the training data, and then average the prediction (or using some voting mechanism if you do classification) of the whole ensemble of networks. This is called bagging . Bagging makes most sense when your classifier has a strong tendency to overfit. A very recent approach which is reported to work very well in terms of performance and computation is dropout .
