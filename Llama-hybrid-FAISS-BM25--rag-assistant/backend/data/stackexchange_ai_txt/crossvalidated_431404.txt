[site]: crossvalidated
[post_id]: 431404
[parent_id]: 
[tags]: 
One-Class SVM - Decision function

The following is based on the paper: Sch√∂lkopf et.al - SVM for Novelty Detection First let us consider the (classical) Soft Margin SVM optimization problem: ${\displaystyle {\text{minimize }}{\frac {1}{n}}\sum _{i=1}^{n}\zeta _{i}+\lambda \|w\|^{2}}\\ {\displaystyle {\text{subject to }}y_{i}(w\cdot x_{i}-b)\geq 1-\zeta _{i}\,{\text{ and }}\,\zeta _{i}\geq 0,\,{\text{for all }}i.}$ I assume that the intuitive interpretation of this optimization problem is understood (small weights correspond to large margin which is in tradeoff to small $\zeta_i$ which corresponds to how much the margin is violated. Now we consider the algorithm described in the paper I linked above: $\min_{w, \zeta, \rho\in\mathbb{R}}\ \frac{1}{2}\|w\|^2 + \frac{1}{\nu n}\sum_i \zeta_i -\rho\\ \text{subject to }(w\cdot x_i)\ge \rho - \zeta_i,\text{ and } \zeta_i\ge0$ My intuition is that the $\rho$ just pushes the boundary in a direction so that most data points will have a positive decision function. In general $\rho$ will be positive and therefore directly lead to smaller values of the objective function, which is in tradeoff to small values of to the margin violations. Now what I don't understand is why the decision function is given by $\mathrm{sgn}(w\cdot x - \rho)$ in contrast to $\mathrm{sgn}(w\cdot x)$ for the orignal Soft Margin SVM. (Note that in the decision function I absorbed the bias $b$ ). Why is it necessary to include $\rho$ in the decision function?
