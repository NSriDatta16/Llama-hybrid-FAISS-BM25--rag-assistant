[site]: crossvalidated
[post_id]: 446737
[parent_id]: 446423
[tags]: 
I find Thomas's answer very detailed, but I will nevertheless add a couple things to it: Since random forest models already select features, is it possible to gain much by such a method? You could have some gains from feature selection in cases of highly correlated features and when having many unimportant features. Many high correlated features might degrade the performance of your trees in the sense that, since they carry the same information, every split to one of them will affect the "remaining" information in the other ones. If you therefore split early in the tree on one of these variables, if features that are correlated to it will be selected for the subsequent splits they might generate useless or "bad" splits. In the same way, when you have too many unimportant features, since you are subsampling the variables for the splits you might find yourself making multiple splits on useless variables (to my knowledge, this is particularly a problem in application in biology). Therefore, you might want to remove/work on highly correlated features, and at the same time always re-tune your mtry parameter, as this will need to change based on the importance and number of your variables. What would be a suitable information criterion for a random forest model? Here I also agree with Thomas. RF will give you variable importances and these can be used to do some selection. However: you should only use variable importances if your model is strong enough. If your model is overfitting (can happen when having simple/few data) or too weak, your variable importances might fluctuate a lot from run to run. Also, be aware that feature importances are ALSO strongly affected by correlated features, so if you plan on using this, take it into account. Finally, default Variable Importances based on Gini/Entropy (or splits in general for the matter) are done on the train set and are strongly biased towards numerical features or categorical features that have many levels, since they allow for splitting more times within the same tree. For this reason, you might also want to check Permutation Importances, which are instead model agnostic and can be use on a performance metric of your choice. (More here )
