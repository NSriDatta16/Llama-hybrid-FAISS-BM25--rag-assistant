[site]: datascience
[post_id]: 68438
[parent_id]: 66963
[tags]: 
There are a few different perspectives from which to determine the amount of data you need. Those include: Project complexity: Each parameter that your model has to consider in order to perform its task increases the amount of data that it will need for training. Training method: As your models is forced to understand a greater number of interlinking parameters, the resulting complexity forces a change in the way that it is trained. Labeling needs: Depending on the task you’re doing, data points can be annotated in different ways. Diversity of input: In cases where your model’s input won’t be highly controlled, more data will be necessary to help your model function in that unpredictable environment. You could begin working on your model with the data you have, then add more data when you feel it becomes necessary. Ideally, this will give you a very clear picture of your data needs, and they'll likely fit into one of the above categories. This article covers training data needs in-depth, so it might be helpful. It also includes examples of dataset sizes for a variety of language-based and NLP projects. Unfortunately, there's not a lot of consistency among projects; some of the projects ( text language identification ) used 3000 training and 1000 test samples, while others ( sentiment analysis ) used datasets with over 10,000 samples. The differences here can likely be attributed to availability of data (tweet data is much easier to collect than South African language texts, for example).
