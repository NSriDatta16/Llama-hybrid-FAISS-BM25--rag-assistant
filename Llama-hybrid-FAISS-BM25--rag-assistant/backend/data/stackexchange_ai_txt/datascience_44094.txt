[site]: datascience
[post_id]: 44094
[parent_id]: 
[tags]: 
Pretrained word embeddings vs model weights

I'm trying to understand the relationship between pretrained word vectors and pretrained weights when using pretrained word vectors in another neural network. Are the vectors themselves the weights or do the weights need to be transferred in addition to selecting embedded representations? What's the high level conceptual process in using pretrained vectors? Thanks!
