[site]: crossvalidated
[post_id]: 502080
[parent_id]: 
[tags]: 
Having trouble with overfitting XGBoost on a large dataset with many features and samples

I have tried tuning every hyperparameter to avoid overfitting but I cannot get XGBoost to generalize well. While this dataset is difficult to get excellent results, I have seen a genetic linear regression algorithm do well. So I figure that XGBoost should be able to do at least as well since it is much more complex than a linear regression, and this dataset should have interaction between features. The genetic linear regression was more successful in the sense that train and test rmse both went down, although the rmse scores wouldn't blow your socks off. Are there any other tricks I can do to prevent this overfitting?
