[site]: crossvalidated
[post_id]: 346674
[parent_id]: 
[tags]: 
Is it statistically correct if my normalized data is highly concentrated inside a range?

I'm building a simple neural network that has worked pretty well when passing variables that are measured in time. Now I want to add other quantitative variables to the model that aren't time measures, and I was told that in order to do that I needed to normalize my data. I have normalized my time variables as suggest in this post and it was brought to my attention how small the ranges of my data have become: Variable 1 goes from 0.0 to 9.727535e-01 Variable 2 goes from 0.092662 to 0.165131 EDIT: Everything just seemed to be fine, the max and min values for each column were getting retrieved correctly, the arithmetic was fine. So I decided to print the max and min values of the transformed dataset, and they were 0.0 and 1.0 for all rows respectively. Everything was fine all the time. I thought I had a mistake because I was printing my dataframe in the console and it first glance I said "woah, this doesn't look like it has 0.0 and 1.0 as min and max values", but the truth is that the console doesn't print every row of the dataframe if the number of rows surpasses certain number.
