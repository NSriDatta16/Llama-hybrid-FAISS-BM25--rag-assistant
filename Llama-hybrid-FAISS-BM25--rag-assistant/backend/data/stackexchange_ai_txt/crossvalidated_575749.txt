[site]: crossvalidated
[post_id]: 575749
[parent_id]: 575740
[tags]: 
This is one of those ideas that starts out sounding great but winds up being less helpful than one might hope. For instance, just because KS (or a similar test) says that the feature has a different distribution for each outcome category doesn’t mean that every model will care. If the distributions only differ in variance, for instance, a linear predictor in a logistic regression will not catch that. Consequently, your feature will wind up being a dud that lacks predictive ability, despite a difference in variance between your outcome. Consequently, distribution differences need not reveal important predictors. On the other hand, even if univariate distributions are identical, multivariate distributions might not be. For instance, imagine a scatterplot of your points that form an X shape, where the “slash” portion is one group and the “backslash” is another. Those univariate marginal distributions are the same for each group, yet those predictors have considerable discriminative ability. Not every model will be able to use the correlation, but some will. Consequently, just because you get a result that the distributions are similar or even the same does not mean that a feature is useless. Overall, we cannot conclude much from this kind of screening. EDIT Let's look at some simulations. 1) set.seed(2022) N In this simulation, $Y=0$ and $Y=1$ have different distributions on the $x$ feature. However, since the difference is in the variance, a logistic regression that only uses a linear predictor does not care. We have to know to analyze the quadratic term to get a significant predictor. This means that we either have to know what kind of difference there is between the distributions (which the KS test does not provide) or allow for some kind of flexible model that will figure it out, as Frank Harrell mentions in the comments. Consequently, a significant result from the KS test does not help us determine the features to include in the model. 2) library(MASS) set.seed(2022) N Here, the KS test says that both features have the same distribution for both levels of the response. However, the joint distribution is dramatically different, due to the difference in correlation (one strongly positive, one strongly negative). Consequently, a model that has an interaction term (such as L1 ) or that discovers such interactions (e.g., neural networks, SVMs with certain kernels) winds up screaming that the features have discriminative ability. Consequently, an insignificant result from the KS test does not help us determine which features should be excluded from the model.
