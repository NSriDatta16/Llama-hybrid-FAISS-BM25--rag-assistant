[site]: crossvalidated
[post_id]: 411591
[parent_id]: 
[tags]: 
Balanced datasets are almost all predicted negative

Problem I am trying to do sentiment analysis using pretrained word vectors GloVe, which is essentially a look-up table that maps word to a fix-dimension vector. Since GloVe is initially designed to provide word embedding for each individual word, for the entire document, I take the average of the all words and use the resulting averaged vector to represent the document. Then the vectors are treated as features and sent to SVM for classification. I know this approach might not provide satisfying result, surprisingly, it performs very poor (see below). In terms of accuracy, it is almost like random guess. The F1 score is extremely low . More surprisingly, true negative rate (TNR) is almost 1 but true positive rate (TPR) is almost 0 (three confusion matrices are similar and only one is included here). I kind of understand why the accuracy is low after I did some visualizations of my document vectors using TSNE (see below). But I do not understand why the predictor will be biased to negative samples. Updates : I did make the dataset balanced before any processing. There are 10000 positive samples and 10000 negative samples.
