[site]: datascience
[post_id]: 96544
[parent_id]: 
[tags]: 
Accuracy goes low with attention layer

Below is code 1 which is not using Attention layer : model = Sequential() model.add(Input(shape=(maxlen,))) model.add(Embedding(max_features, embed_size)) model.add(Bidirectional(LSTM(10))) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print(model.summary()) model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64) The output is (Accuracy=98.22%): Epoch 1/3 164/164 [==============================] - 49s 282ms/step - loss: 0.3407 - accuracy: 0.9317 - val_loss: 0.1843 - val_accuracy: 0.9392 Epoch 2/3 164/164 [==============================] - 46s 278ms/step - loss: 0.1579 - accuracy: 0.9329 - val_loss: 0.1719 - val_accuracy: 0.9403 Epoch 3/3 164/164 [==============================] - 46s 282ms/step - loss: 0.0727 - accuracy: 0.9822 - val_loss: 0.2001 - val_accuracy: 0.9227 Below is code with Attention layer: from keras_self_attention import SeqSelfAttention modelAtt = Sequential() modelAtt.add(Input(shape=(maxlen,))) modelAtt.add(Embedding(max_features, embed_size, input_length=maxlen)) modelAtt.add(Bidirectional(LSTM(10, return_sequences=True))) modelAtt.add(SeqSelfAttention()) modelAtt.add(Dense(1, activation='sigmoid')) modelAtt.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print(modelAtt.summary()) modelAtt.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64) The output is (Accuracy=97.45%): Epoch 1/3 164/164 [==============================] - 68s 395ms/step - loss: 0.3464 - accuracy: 0.8997 - val_loss: 0.1741 - val_accuracy: 0.9418 Epoch 2/3 164/164 [==============================] - 64s 391ms/step - loss: 0.1451 - accuracy: 0.9424 - val_loss: 0.1647 - val_accuracy: 0.9430 Epoch 3/3 164/164 [==============================] - 64s 391ms/step - loss: 0.0773 - accuracy: 0.9745 - val_loss: 0.1898 - val_accuracy: 0.9337 The data is quora questions - Quora Dataset on Kaggle I have tried reducing the number of LSTM layers to avoid overfitting. What is causing the accuracy to go low when using attention?
