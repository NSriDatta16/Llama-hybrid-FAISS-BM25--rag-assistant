[site]: datascience
[post_id]: 28640
[parent_id]: 28626
[tags]: 
Very interesting question! First Approach: PCA + K-means Your data will be explained very well on the second principle component . If you apply PCA on your data the first PC captures the data along the lines in which you completely lose the differentiation but the second PC is prependicular to the first one so your data will be projected in a way that points correspondig to each line are placed closer to each other. As you know the number of lines (number of clusters) a priori then you simply apply k-means and that's it! See image in the link to have an idea how the second pc vector would be. Second approach: GMMs Gaussian Mixture Models are fitted to clusters in a data using maximum likelihood estimation (you use Expectation Maximization algorithm for that). Your clusters along second PC are pretty gaussian so you will get a good soft-clustering if you fit a mixture of $n$ (number of lines again) Gaussian kernels to them. Variant: $a$s Are Not Equal In this case your lines cross each other as slopes are different. Your image does not show that but I include it here anyways. In this case you fit a linear regression to each line and keep the coefficients of the line. The you have a $2D$ data in which each line is described by just a slope and an intercept. Then the prependicular distance between each point and all lines tells you which line is closer so that is the cluster. (The distance can also simply be the residual of that point from regression line . You just need a distance metric to determine the closest line) If you need implementation as well, please drop a comment here so I can update answer with Python code. Good luck :)
