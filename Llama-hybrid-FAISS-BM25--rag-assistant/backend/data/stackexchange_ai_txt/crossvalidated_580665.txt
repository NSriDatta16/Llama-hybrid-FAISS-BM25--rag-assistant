[site]: crossvalidated
[post_id]: 580665
[parent_id]: 580661
[tags]: 
GLMs don’t usually care about square loss, so $R^2$ is of limited use. You’re always allowed to calculate a sum of squares, but it might not be meaningful. UCLA has a nice page on $R^2$ -style metrics for logistic regression. The McFadden variant is how I would think about it: as a ratio of loss functions (or likelihoods). $R^2$ can be very low without violating linearity, and $R^2$ can be very high without having linearity. For the former, imagine $\hat y_i =x_i+\epsilon_i$ with a very high variance for each $\epsilon_i$ . For the latter, imagine slight curvature, such as a $y_i=x_i^3+\epsilon_i$ With a low variance for each $\epsilon_i$ . Add in some lower-order polynomial terms to make that non-monotonic, if you want.
