[site]: crossvalidated
[post_id]: 229376
[parent_id]: 
[tags]: 
how does a neural network with stochastic backpropagation make sure it doesn't "undo" previous learning?

Assume we have a neural network with stochastic gradient descent used for backpropagation, and therefore each element in the training set is used once to calculate the error, and then to adjust the weights (assume each element in the training set is used only once). Assume we're doing a simple regression without regularization, and with 1 or 2 hidden layers. I can't quite understand why such an algorithm doesn't, through learning at training point N, "undo" learning that it did with training point N-1, N-2, etc. Is such an algorithm even supposed to converge to the optimal parameters? My intuition would say that it would "hops back and forth", because it doesn't take into account all the data all at once.
