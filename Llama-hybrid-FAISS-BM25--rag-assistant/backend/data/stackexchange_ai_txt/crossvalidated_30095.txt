[site]: crossvalidated
[post_id]: 30095
[parent_id]: 30042
[tags]: 
The answer to your question is in my experience "no", SVMs are not definitely superior, and which works best depends on the nature of the dataset at hand and on the relative skill of the operator with each set of tools. In general SVMs are good because the training algorithm is efficient, and it has a regularisation parameter, which forces you to think about regularisation and over-fitting. However, there are datasets where MLPs give much better performance than SVMs (as they are allowed to decide their own internal representation, rather than having it pre-specified by the kernel function). A good implementation of MLPs (e.g. NETLAB) and regularisation or early stopping or architecture selection (or better still all three) can often give very good results and be reproducible (at least in terms of performance). Model selection is the major problem with SVMs, choosing the kernel and optimising the kernel and regularisation parameters can often lead to severe over-fitting if you over-optimise the model selection criterion. While the theory under-pinning the SVM is a comfort, most of it only applies for a fixed kernel, so as soon as you try to optimise the kernel parameters it no longer applies (for instance the optimisation problem to be solved in tuning the kernel is generally non-convex and may well have local minima).
