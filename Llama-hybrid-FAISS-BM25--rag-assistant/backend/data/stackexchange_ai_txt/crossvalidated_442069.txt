[site]: crossvalidated
[post_id]: 442069
[parent_id]: 
[tags]: 
Why do regression estimates provide lower relative error than averaged values?

I am trying to estimate the per-cell protein concentration for some samples. I have performed a series of protein extractions for each of my samples, with each extraction using an increased (and known) number of cells (for example, 300000 cells for the first sample, 500000 cells in the second, 700000 cells in the third). I have tried two different methods to estimate the per-cell protein concentration from the extraction results. The first approach is to perform a regression of the number of cells against the protein concentration, interpreting the regression slope as the per-cell protein concentration (see image below, which shows regressions for three different sets of extractions). The second approach is to normalize/divide the protein concentration in each extraction by its corresponding number of cells, then interpreting the average of these normalized values as the per-cell protein concentration To compare the two approaches, I have calculated the relative error for both sets of results. In the case of regression estimates, I calculated the relative error as the ratio of the regression slope standard error to the regression slope itself. In the case of the normalized estimates, I calculated the relative error as the ratio of the standard error of the normalized values to the mean of the normalized values for each set of extractions. Looking at my results, I have found that the relative error for the regression estimates is considerably lower than the relative error for the normalized estimates. Moreover, in the case of the normalized estimates, the relative error is significantly negatively correlated with the mean protein concentration. There is no trend in the relative error of the regression estimates. This can be seen in the following figure, where each point corresponds to the per-cell protein estimate for one set of extractions. The relative error is plotted on the y-axis, the mean protein concentration is plotted on the x-axis. (The colors correspond to different classes of proteins). So, my question is: why is the relative error reduced in the regression estimates compared to the normalized estimates? My guess is that, by being based on the regression slope alone, the regression estimates omit any error associated with the regression intercept and are lower as a result. Does that seem correct, or is there something about the regression operation itself that would cause the error to be lower in the regression estimates? Sort of a broad question, but would love to hear whatever thoughts anyone might have. Thanks!!
