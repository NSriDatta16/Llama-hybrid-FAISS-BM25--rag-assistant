[site]: crossvalidated
[post_id]: 232741
[parent_id]: 
[tags]: 
Why is it important to include a bias correction term for the Adam optimizer for Deep Learning?

I was reading about the Adam optimizer for Deep Learning and came across the following sentence in the new book Deep Learning by Begnio, Goodfellow and Courtville: Adam includes bias corrections to the estimates of both the first-order moments (the momentum term) and the (uncentered) second-order moments to account for their initialization at the origin. it seems that the main reason to include these bias correction terms is that somehow it removes the bias of the initialization of $m_t = 0$ and $v_t = 0$ . I am not 100% sure what that means but it seems to me that it probably means that the 1st and 2nd moment start at zero and somehow starting it off at zero slants the values closer to zero in an unfair (or useful) way for training? Though I would love to know what that means a bit more precisely and how that damages the learning. In particular, what advantages does un-biasing the optimizer have in terms of optimization? How does this help training deep learning models? Also, what does it mean when it's unbiased? I am familiar what unbiased standard deviation means but it's not clear to me what it means in this context. Is bias correction really a big deal or is that something overhyped in the Adam optimizer paper? Just so people know I've tried really hard to understand the original paper but I've gotten very little out of reading and re-reading the original paper. I assume some of these question might be answered there but I can't seem to parse the answers.
