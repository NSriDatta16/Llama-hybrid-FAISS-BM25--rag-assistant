[site]: crossvalidated
[post_id]: 261307
[parent_id]: 
[tags]: 
Density Estimators: Is average test set log-likelihood adequate to assess performance? Why isn't noise data used for low probability testing?

reading NADE papers , I noticed that the average (negative) log-likelihood is often used to assess the accuracy of the density estimation model. At the beginning this made sense to me, but then I thought at an overfitting simple "model" in which binary input units are directly connected to output units. Each of these output units would represent a probability specific for its input unit. For example, NADE models the outputs as representing conditional probabilities of a specific input variable given all the previous in a pre-determined ordering of input variables. In such a simple toy model, the binary inputs are connected with constant weights set at one to the outputs, and there is no learning, also because the gradients would be always be 0 as there is no "error" in estimating the probability. This "model" would always estimate a probability 1 for all the dataset samples that are given to him. It would also return a probability of 1 for unseen samples from a test set, and it would always return 1 from noise. If the NADE can be interpreted as an autoencoder, then I think that it learns to generalize to the correct probability distribution via a sufficiently narrow latent-code/hidden layer. If the hidden layer is not sufficiently narrow, then an autoencoder might learn the identity function, and the same might happen with the simple case of bernoulli probabilities on binary-valued input vectors. Measurement of this phenomenon is not taken into account in the average log-likelihood measure. As noise or negative samples are not used in order to make sure that their probabilities returned would be around 0, how can the training dataset (or test set) log-likelihood be considered as a serious quality measure?
