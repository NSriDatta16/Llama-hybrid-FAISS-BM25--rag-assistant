[site]: datascience
[post_id]: 31305
[parent_id]: 25722
[tags]: 
So far this part hasn't been answered: "should it be used after pooling or before pooling and after applying activation?" One team did some interesting experiments and found that, at least for residual networks, batch norm and activation should come before the weights layer (So it goes BN --> ReLU --> Weights). They found pooling placement could vary without detriment. If you try to replicate that paper note very carefully the paragraph on the exceptions in placement for the beginning and end of the residual stacks. Publication: He et al. - 2016 - Identity mappings in deep residual networks
