[site]: crossvalidated
[post_id]: 329902
[parent_id]: 
[tags]: 
making n-grams, best practices

if I encode my data in 2-grams, thats (26+26+10)^2~3800 possible pairs if I use 3-grams that's ~200,000 possible triplets. We can reduce this number by using only lower case, but basic combinatorics shows the number of combinations still blows up as N increases. BUT of those 200,000 triplets, I might use only 50000. So, what I'm doing is sifting through all of my data, and making only the N-grams I find. I start by assigning the 2-gram " " (double space) the number zero. Then, I just go through all my data and if I find a new n-gram, I give it the next number. so "cat dog " would result in the n-gram dictionary " ":0 "ca":1 "t ":2 "do":3 "g ":4 BUT, this gives a numerical value, an "order" in a sense to the n-grams. This order is based on first appearance in my corpus, which isn't necessarily something that maps to a "high" or "low" value in my data. So I'm wondering if any best practice exists to shuffle this order up? For example, instead of incrementing my index, should I instead select randomly from a list of numbers between 0 and 62^2? Or does this not usually matter? EDIT: more info: i'm going to take the padded-encoded data and treat it like a 1-pixel-tall (or is it wide?) image for a convolution neural network. the assumption and long term plan is that there is some kind of "grammar" and I'm trying to capture the rules of it in order to match entries in a local database and a global database
