[site]: crossvalidated
[post_id]: 169279
[parent_id]: 161366
[tags]: 
The second approach is reasonable. If the 10 measurements of $M$ were independent, then this is just the standard error of the mean . Since you are cross-validating, the datasets used in each round and therefore the estimates of $M$ are not independent. So you are probably going to underestimate the standard error a bit. The paper by Bengio & Grandvalet (2003) suggests that there is not much you can do about this bias. I ran a toy example where the bias seemed small. I fitted a distribution over 10 states with a histogram. The fit was evaluated using average log-likelihood, $L$. Using 100 data points and leave-one-out cross-validation, I got $$\hat L = 3.5641, \quad \hat{SEM} = 0.7133.$$ This is an average over 10,000 cross-validations. Since I know the true distribution, I can repeat the cross-validation experiment many times with different datasets. I can also generate independent training sets (of 99 data points, as in leave-one-out cross-validation). Using independent datasets, I got $$\hat L = 3.5636, \quad \hat{SEM} = 0.7185.$$ This is of course only one simple example ( code here ), and in general the bias might be larger.
