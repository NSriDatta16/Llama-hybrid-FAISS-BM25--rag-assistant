[site]: crossvalidated
[post_id]: 314106
[parent_id]: 314046
[tags]: 
@amoeba had excellent answers to PCA questions, including this one on relation of SVD to PCA. Answering to your exact question I'll make three points: mathematically there is no difference whether you calculate PCA on the data matrix directly or on its covariance matrix the difference is purely due to numerical precision and complexity. Applying SVD directly to the data matrix is numerically more stable than to the covariance matrix SVD can be applied to the covariance matrix to perform PCA or obtain eigen values, in fact, it's my favorite method of solving eigen problems It turns out that SVD is more stable than typical eigenvalue decomoposition procedures, especially, for machine learning. In machine learning it is easy to end up with highly collinear regressors. SVD works better in these cases. Here's Python code to demo the point. I created a highly collinear data matrix, got its covariance matrix and tried to obtain the eigenvalues of the latter. SVD is still working, while ordinary eigen decomposition fails in this case. import numpy as np import math from numpy import linalg as LA np.random.seed(1) # create the highly collinear series T = 1000 X = np.random.rand(T,2) eps = 1e-11 X[:,1] = X[:,0] + eps*X[:,1] C = np.cov(np.transpose(X)) print('Cov: ',C) U, s, V = LA.svd(C) print('SVDs: ',s) w, v = LA.eig(C) print('eigen vals: ',w) Output: Cov: [[ 0.08311516 0.08311516] [ 0.08311516 0.08311516]] SVDs: [ 1.66230312e-01 5.66687522e-18] eigen vals: [ 0. 0.16623031] Update Answering to Federico Poloni's comment, here's the code with stability testing of SVD vs Eig on 1000 random samples of the same matrix above. In many cases Eig shows 0 small eigen value, which would lead to singularity of the matrix, and SVD doesn't do it here. SVD is about twice more precise on a small eigen value determination, which may or may not be important depending on your problem. import numpy as np import math from scipy.linalg import toeplitz from numpy import linalg as LA np.random.seed(1) # create the highly collinear series T = 100 p = 2 eps = 1e-8 m = 1000 # simulations err = np.ones((m,2)) # accuracy of small eig value for j in range(m): u = np.random.rand(T,p) X = np.ones(u.shape) X[:,0] = u[:,0] for i in range(1,p): X[:,i] = eps*u[:,i]+u[:,0] C = np.cov(np.transpose(X)) U, s, V = LA.svd(C) w, v = LA.eig(C) # true eigen values te = eps**2/2 * np.var(u[:,1])*(1-np.corrcoef(u,rowvar=False)[0,1]**2) err[j,0] = s[p-1] - te err[j,1] = np.amin(w) - te print('Cov: ',C) print('SVDs: ',s) print('eigen vals: ',w) print('true small eigenvals: ',te) acc = np.mean(np.abs(err),axis=0) print("small eigenval, accuracy SVD, Eig: ",acc[0]/te,acc[1]/te) Output: Cov: [[ 0.09189421 0.09189421] [ 0.09189421 0.09189421]] SVDs: [ 0.18378843 0. ] eigen vals: [ 1.38777878e-17 1.83788428e-01] true small eigenvals: 4.02633695086e-18 small eigenval, accuracy SVD, Eig: 2.43114702041 3.31970128319 Here code the code works. Instead of generating the random covariance matrix to test the routines, I'm generating the random data matrix with two variables: $$x_1=u\\ x_2=u+\varepsilon v$$ where $u,v$ - independent uniform random variables. So, the covariance matrix is $$\begin{pmatrix} \sigma_1^2 & \sigma_1^2 + \varepsilon \rho \sigma_1 \sigma_2\\ \sigma_1^2 + \varepsilon \rho \sigma_1 \sigma_2 & \sigma_1^2 + 2 \varepsilon \rho \sigma_1 \sigma_2 + \varepsilon^2 \sigma_2^2\sigma^2\end{pmatrix}$$ where $\sigma_1^2,\sigma_2^2,\rho$ - variances of the uniforms and the correlation coeffient between them. Its smallest eigenvalue: $$\lambda= \frac 1 2 \left(\sigma_2^2 \varepsilon^2 - \sqrt{\sigma_2^4 \varepsilon^4 + 4 \sigma_2^3 \rho \sigma_1 \varepsilon^3 + 8 \sigma_2^2 \rho^2 \sigma_1^2 \varepsilon^2 + 8 \sigma_2 \rho \sigma_1^3 \varepsilon + 4 \sigma_1^4} + 2 \sigma_2 \rho \sigma_1 \varepsilon + 2 \sigma_1^2\right)$$ The small eigenvalue can't be calculated by simply plugging the $\varepsilon$ into formula due to limited precision, so you need to Taylor expand it: $$\lambda\approx \sigma_2^2 \varepsilon^2 (1-\rho^2)/2$$ I run $j=1,\dots,m$ simulations of the realizations of the data matrix, calculate the eigenvalues of the simulated covariance matrix $\hat\lambda_j$ , and obtain the errors $e_j=\lambda-\hat\lambda_j$ .
