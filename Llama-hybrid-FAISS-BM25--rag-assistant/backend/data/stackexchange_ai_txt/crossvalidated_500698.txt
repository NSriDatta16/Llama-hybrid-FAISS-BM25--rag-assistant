[site]: crossvalidated
[post_id]: 500698
[parent_id]: 
[tags]: 
use of SAFE rule for initial feature selection

I am building a binary classification model with a feature matrix of size 100k x 100k where the rows/columns represent documents/ngrams (1,2,..., 5). ngram must be in at least .5% of the documents to be a column. Re-writing the SAFE rule $$ |\boldsymbol{x}_j^\intercal \boldsymbol{y}| I constructed a table (f | #features) which shows the the number of features which would survive for 1000 $f$ values between 0 and 1 (I realize lower bound for f will be $\lambda_{\text min}/\lambda_{\text max}$ which may be different from 0). Suppose I want to select 5000 features, I look at the row for which #features is closest to 5000 and get the corresponding f* value. The SAFE formula is then used to identify the features for that f* so that the matrix is reduced to 1000k x ~5000 and the LASSO algorithm is run on that matrix. For the experiments below I used a sparse binary feature matrix indicating the presence of an ngram in a document. Exp1: selected 5k features as described above, ran LASSO with xval=5, used 1se to select final feature set. Features make sense, model looks good. Exp 2: selected 500 features using SAFE. The top 5 features (highest abs coef) from Exp 1 are not even in this list! Exp 3: selected 20 features using SAFE. None of the features selected make any sense. Is my use of SAFE for dimensionality reduction as described above not correct? I realize that it is a univariate selection method which is suggesting which coef should surely be zero, but I am a bit surprised about 2 & 3. I am wondering if using 5k features in Exp1 is sufficient.
