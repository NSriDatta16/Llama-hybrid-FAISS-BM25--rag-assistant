[site]: datascience
[post_id]: 17047
[parent_id]: 
[tags]: 
Question about the simple example for batch normalization given in "deep learning" book

In the section about batch normalization of Deep Learning book by Ian Goodfellow ( chapter link ) there is the follwing text: As example, suppose we have a deep neural network that has only one unit per layerand does not use an activation function at each hidden layer: $y=x w_1 w_2 w_3 \ldots w_l$. Here, $w_i$ provides the weight used by layer $i$. The output of layer $i$ is $h_i=h_{iâˆ’1} wi$. The output $y$ is a linear function of the input $x$, but a nonlinear function of the weights $w_i$. Why y is nonlinear with respect to w_i?
