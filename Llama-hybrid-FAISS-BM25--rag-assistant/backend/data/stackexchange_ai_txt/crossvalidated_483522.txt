[site]: crossvalidated
[post_id]: 483522
[parent_id]: 483517
[tags]: 
Cross validation can be used for many tasks: hyperparameter tunning, how stable your out of sample error is, but I would say that it is most useful for comparing different models. For example, if you have two models, and you run cross validation on both of them, then you can compare the performance of different folds and see if one model outperforms the other. By doing this, say, 10-fold, you get a more robust estimate of the out of sample performance compared to only using one test set (i.e. 1-fold validation). You might find that a more complex model is able to get an average AUC of 0.97, or maybe if overfits and give you a worse AUC of 0.9. You are only able to say if a model overfits if you actually compare it out of sample with a simpler model. For your last question: After you have found the best model doing cross-validation, and you have decided that this model is going to be used in production, you should train the model on all data available, so that you get the most accurate estimates possible.
