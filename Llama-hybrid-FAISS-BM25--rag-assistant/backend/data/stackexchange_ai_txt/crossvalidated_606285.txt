[site]: crossvalidated
[post_id]: 606285
[parent_id]: 606262
[tags]: 
$\theta$ may be random to you (or me) as we do not know it and hence we put a prior on it. That does not preclude that there is some true value $\theta_0$ out there. (To use the perennially poor but well-known statistician's example: consider a coin that is or is not fair due to its physical properties, but for which the owner of the coin does not know if it is or is not.) If, then, some estimator is used over repeated samples, there is nothing wrong with computing its expected value (and similar comments apply to its variance) over repeated samples and see if that expectation coincides with the true value or not. Spoiler: mostly, Bayesian estimators are not unbiased. For instance, the expected value of the posterior mean for binomial data with true success probability $\theta_0$ and beta priors with hyperparameters $(\alpha_0,\beta_0)$ can be written as $$ E_{Y|\theta_0}\left[E(\theta|y)\right]=w\frac{\alpha _{0}}{\alpha_{0}+\beta_0}+E_{Y|\theta_0}\left[(1-w)\frac{k}{n}\right]=w\frac{\alpha _{0}}{\alpha_{0}+\beta_0}+(1-w)\theta_0\neq\theta_0 $$ with $k$ the number of successes and $$ w=\frac{\alpha _{0}+\beta_0}{\alpha_{0}+\beta_0+n} $$ It is argued by Bayesians that we may not care since we want a good rule for the sample at hand (say, the above posterior mean).
