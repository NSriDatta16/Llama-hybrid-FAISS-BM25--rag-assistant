[site]: crossvalidated
[post_id]: 229786
[parent_id]: 
[tags]: 
GARCH vs GJR-GARCH

Both the GJR and the GARCH-specifications are used quite often in the finance literature. The GARCH is defined as: $$ \sigma^2_t = \omega + \alpha\varepsilon^2_{t-1} + \beta\sigma^2_{t-1}$$ and the GJR-GARCH reads as follows $$ \sigma^2_t = \omega + (\alpha+\gamma \mathbb{I}_{t-1})\varepsilon^2_{t-1} + \beta\sigma^2_{t-1}$$ where $\mathbb{I}_{t-1}$ is the indicator function: $\mathbb{I}_{t-1}(\varepsilon_{t-1})=\varepsilon_{t-1}$ for $\varepsilon_{t-1}>0$ and $\mathbb{I}_{t-1}(\varepsilon_{t-1})=0$ otherwise. According to research (Laurent et al. and Brownlees et al.) the GJR models generally perform better than the GARCH specification. Thus, including a leverage effect leads to enhanced forecasting performance. My question is: what are reasons for the GJR specification to perform better than the GARCH model. Also, it can occur that both models perform on par. What could be reasons for this observation of equal forecasting performance? Thank you very much. References: Brownlees, C. T., Engle, R. F., & Kelly, B. T. (2011). A practical guide to volatility forecasting through calm and storm. Available at SSRN 1502915. Laurent, S., Rombouts, J. V., & Violante, F. (2012). On the forecasting accuracy of multivariate GARCH models. Journal of Applied Econometrics, 27(6), 934-955.
