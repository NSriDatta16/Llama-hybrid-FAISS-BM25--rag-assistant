[site]: crossvalidated
[post_id]: 588945
[parent_id]: 588863
[tags]: 
The nature of your question itself suggests a conceptual misunderstanding. When we consider a binomial PMF, e.g. $$X \sim \operatorname{Binomial}(n, p)$$ with $$\Pr[X = x] = \binom{n}{x} p^x (1-p)^{n-x}, \quad x \in \{0, 1, \ldots, n\},$$ the support of this random variable is $X \in \{0, 1, 2, \ldots, n\}$ . This represents the set of possible elementary outcomes of $X$ , and it is in this regard that the sum of the probabilities of all such outcomes equals unity: $$\sum_{x=0}^n \Pr[X = x] = \sum_{x=0}^n \binom{n}{x} p^x (1-p)^{n-x} = (p + (1-p))^n = 1^n = 1$$ by the binomial theorem. However, for a beta distributed random variable, say $P \sim \operatorname{Beta}(a,b)$ , the probability density is $$f_P(p) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} p^{a-1} (1-p)^{b-1}, \quad 0 where $a$ and $b$ are shape parameters. Here, $0 is the support representing the set of elementary outcomes of $P$ , and the "sum" of the probabilities of all such outcomes is $$\int_{p=0}^1 f_P(p) \, dp = 1.$$ These distributions do not have the same behavior at all. As other answers have pointed out, it makes no sense to model the (Bayesian) prior $p$ in a binomial likelihood with another binomial distribution because $p$ is a probability, not a count. The notion that $\binom{n}{x}$ and $\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}$ are just "normalization constants" reflects a fundamental misunderstanding that occasionally occurs among students of Bayesian statistics. The idea behind conjugate distributions really has to do with the concept of the kernel of a probability function. The kernel is the part that depends on the parameter(s), and excludes any multiplicative factors that are constant with respect to those parameter(s). For instance, with respect to the parameter $p$ , the kernel of the binomial PMF is $$\ker(\Pr[X=x]) = p^x (1-p)^{n-x}.$$ The factor $\binom{n}{x}$ is excluded because it does not depend on $p$ . The kernel is the basis for the likelihood function with respect to those same parameter(s); e.g., $$\mathcal L(p \mid x) \propto \ker(\Pr[X = x]).$$ The essence of this idea is that a likelihood is a function of the parameter(s) for some observed data $x$ ; as such, it is only uniquely determined up to a constant of proportionality. For instance, if $X \sim \operatorname{Binomial}(n = 5, p)$ and we observed $X = 3$ , we could write the likelihood function of $p$ as $$\mathcal L(p \mid X = 3) = p^3 (1-p)^2, \quad 0 or we could write it as $$\mathcal L(p \mid X = 3) = 157839027384 p^3 (1-p)^2, \quad 0 It doesn't matter because $\mathcal L$ need not satisfy $\int_{p=0}^1 \mathcal L(p) \, dp = 1$ . There is a choice that makes this true for general $n, x$ , and when we do choose this, we get a beta distribution over $p$ . This is why the beta distribution is the conjugate prior for a binomial likelihood. As another example, if we have a Poisson distributed random variable $\Pr[Y = y] = e^{-\lambda} \lambda^y/y!$ with unknown rate $\lambda$ , its kernel with respect to $\lambda$ is $$\ker(\Pr[Y = y]) = e^{-\lambda} \lambda^y, \quad y > 0.$$ So its likelihood is $$\mathcal L(\lambda \mid y) \propto e^{-\lambda} \lambda^y$$ which is proportional to a gamma density with shape $y+1$ and rate $1$ ; hence the gamma distribution is the conjugate prior for a Poisson likelihood.
