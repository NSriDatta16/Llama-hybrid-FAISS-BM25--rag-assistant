[site]: crossvalidated
[post_id]: 581744
[parent_id]: 
[tags]: 
Parametric vs non-parametric generative models

I have a little perplexity trying to distinguish parametric vs non-parametric generative model. In my understanding, a parametric generative model would try to learn the probability density function by estimating the parameters of an underlying distribution we are assuming. So just doing for example, $$\theta^* = arg\max_\theta \,\prod_{i=1}^N p_\theta(\textbf{x}_i)$$ I realize that in practice, we need to figure out what is the basic distribution that we are going to modify by adjusting the parameters $\theta$ . So in the case of VAEs we use latent variables assumption to make training feasible, we jointly train $q_\phi(\textbf{z}|\textbf{x})$ and $p_\theta(\textbf{x}|\textbf{z})$ prametrizing both distributions with neural networks (i.e. encoder & decoder). In such case, we end up with the situation that all our distributions are Gaussians (assuming that prior and conditional are gaussians). So, having said that, can we conclude that VAEs are parametric ? Also, what could be an example of non-parametric generative model? I would say that, for example, GANs maybe an example of non-parametric model, as we start with a latent normal distribution but then applying a stack of non-linear transformations ending up with something potentially very complicated.
