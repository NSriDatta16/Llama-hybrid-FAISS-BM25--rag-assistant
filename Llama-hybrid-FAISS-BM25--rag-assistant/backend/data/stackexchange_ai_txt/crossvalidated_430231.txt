[site]: crossvalidated
[post_id]: 430231
[parent_id]: 430184
[tags]: 
I can think of two benefits. The first one is that it simplifies interpretation considerably which makes understanding our models easier (probably not so relevant in the case of neural networks) Another benefit of having sparse representations, that arise e.g. in Lasso-Regression is that the penalization that induces the sparsity reduces the estimation variance (at the cost of larger bias) Consider e.g. a really noisy linear regression where there are only 2 covariates that have a real influence on $y$ and 100 noisy ones (that have no real influence). If we fit a linear model (including the noisy covariates) there will be a lot of estimation variance which will likely result in a bad fit. L1-regularization however can handle this pretty well by inducing sparsity in the parameters and thereby reducing the estimation variance. (see here )
