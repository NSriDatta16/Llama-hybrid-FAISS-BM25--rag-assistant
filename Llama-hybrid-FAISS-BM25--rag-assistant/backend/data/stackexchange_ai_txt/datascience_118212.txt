[site]: datascience
[post_id]: 118212
[parent_id]: 
[tags]: 
How to calculate accuracy of a logistic regression?

A logistic regression involves a linear combination of features to predict the log-odds of a binary, yes/no-style event. That log-odds can then be transformed to a probability. If $\hat L_i$ is the predicted log-odds of observation $i$ , then define the predicted probability of observation $i$ , $\hat p_i$ , by $\hat p_i=\frac{1}{1+e^{-\hat L_i}}$ . How, then, do data scientists obtain accuracy values for such models? In particular, every predicted probability is going to be greater than zero and less than one, so every prediction is going to be a little bit wrong, since binary values in this framework tend to be coded as $0$ and $1$ , yet I routinely see data scientists claiming nonzero accuracy of such models.
