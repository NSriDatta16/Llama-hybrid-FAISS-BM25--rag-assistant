[site]: crossvalidated
[post_id]: 616925
[parent_id]: 
[tags]: 
What does extrapolation mean in the context of regression weights and what are its downsides?

In numerous articles I have read ( Abadie 2021 , Samii 2016 , basically any Abadie piece that talks about the synthetic control method), the authors cite regression's reliance on extrapolation for weights as a negative feature for estimating causal effects. For example, Abadie 2021, in the section discussing the benefits of synthetic controls compared to regression cites: Synthetic control estimators preclude extrapolation, because synthetic control weights are nonnegative and sum to one. It is easy to check that, like their synthetic control counterparts, the regression weights in $W^{reg}$ sum to one. Unlike the synthetic control weights, however, regression weights may be outside the [0, 1] interval, allowing extrapolation outside of the support of the data In Samii 2016: Where there is no overlap, one can only make comparisons with interpolated or extrapolated counterfactual potential outcomes values. King and Zeng (2006) brought the issue to political scientists’ attention, characterizing interpolations and, especially, extrapolations as “model dependent,” by which they meant that they were nonrobust to modeling choices that are often indefensible. By pointing out how common such model dependent estimates are in political science research, King and Zeng raised troubling questions about the validity of many generality claims in quantitative causal research in political science. In sum, I think the critique is less with the method and more with the lack of transparency for how regression weights are generated (relying on extrapolation and not equally weighting observations in the data set such that a small subset of weights can dominate the weighting of an entire sample. However, in these pieces, what this entails is somewhat vague. I can see that regression can produce negative weights (as opposed to the synthetic control method where the lower bound of weights is 0), but I do not understand how this is a negative feature. Abadie (2021) notes that weights beyond [0,1] leads to extrapolation outside the support of the data. I am confused on what this means. How do negative weights or weights > 1 extrapolate beyond the support of the data? I am sure that the answer is fairly simple, I am just having a hard time explaining this back to myself in a way that I can understand.
