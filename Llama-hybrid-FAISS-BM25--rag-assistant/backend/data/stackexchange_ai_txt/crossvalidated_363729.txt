[site]: crossvalidated
[post_id]: 363729
[parent_id]: 363701
[tags]: 
The documentation describes non-linear SVC using Kernels. The $\ell_1$ penalty is only available for linear SVC (does not use Kernels). Linear SVC is similar to penalised logistic regression, but uses the hinge loss: $$ \arg\min_{\mathbf{w}} \frac{1}{2} \lVert \mathbf{w} \rVert_2^2 + C \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^\top \mathbf{x}_i) , $$ where the first term is an $\ell_2$ penalty and the max part is the hinge loss. Sometimes you also see the squared hinge loss, which has the advantage of being differentiable. As in logistic regression, you can use an $\ell_1$ penalty to obtain sparse coefficients. In either case, the regularisation and the loss function are convex and standard solvers for convex problems can be employed, albeit more advanced and efficient solvers are available.
