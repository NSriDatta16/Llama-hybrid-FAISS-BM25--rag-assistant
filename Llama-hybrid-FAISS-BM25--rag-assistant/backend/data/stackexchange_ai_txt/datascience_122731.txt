[site]: datascience
[post_id]: 122731
[parent_id]: 
[tags]: 
Weight decay used by Adam optimizer for neural network caused NaN validation loss

I've built a model with BCE loss for CTR prediction in which the major part is a transformer encoder. I've used 0.1 for dropout probability. When using 0 weight decay for Adam the training and validation losses both decreased normally. However when using 1e-4 or 1e-6 as the weight decay, after some steps, the validation loss became NaN . What is the possible reason for it? The training loss never became NaN though. What does NaN validation loss indicate? Does it mean -log(0)?
