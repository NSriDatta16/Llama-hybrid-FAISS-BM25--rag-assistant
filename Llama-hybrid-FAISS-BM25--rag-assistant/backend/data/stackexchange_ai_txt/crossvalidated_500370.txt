[site]: crossvalidated
[post_id]: 500370
[parent_id]: 
[tags]: 
What is the point of redundant samples in bootstrapping?

While studying machine learning I read about different sampling methods. Simple holdout, N-fold cross validation are straightforward. However, I somehow miss the point of bootstrapping. Its definition says that it's just a way to inflate the sample set simply by duplicating some random samples and I cannot figure what is the point in this -- seemingly no additional information in a learning process just by seeing the same instances again and again (on the contrary: others say that omitting redundant points from the training set is recommended for computational efficiency and for some other statistical reason as well). So what is the explanation here?
