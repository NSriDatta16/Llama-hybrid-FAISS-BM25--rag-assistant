[site]: crossvalidated
[post_id]: 573078
[parent_id]: 572889
[tags]: 
Background As stated in this as well in his prior question the OP wants to perform Bayes quadrature of an expensive function against a density, which is a Gaussian mixture as the result of applying a kernel density smoother. The library he wants to use for Bayes quadrature allows only uniform distributions to integrate against. This is why he wants to transform the integrand. In the following I construct a function $\Psi_0:]0;1[^{d+1} \rightarrow \mathbb{R}^d$ such that $$ \int_{\mathbb{R}^d}f(x)P(x)dx=\int_{]0;1[^{d+1}}f(\Psi_0(u))du$$ where the density $P(x)$ is a Gaussian mixture. I will explain this in steps, starting with very simple densities $P$ and transformations $\Psi$ . I also provide some code for testing and further documentation at the end. A: Multivariate, independent standard normals Let $F:\mathbb{R}\rightarrow ]0;1[$ denote the standard normal univariate CDF. For univariate standard normal density $P$ the transformation is $ \Psi(u) = F^{-1}(u).$ In the multivariate case apply this transformation to each margin. I.e. for $x=(x_1,\ldots,x_d)$ and $u=(u_1,\ldots,u_d)$ define the function as $$ x = \Psi(u)= (F^{-1}(u_1), \ldots, F^{-1}(u_d)).$$ B: General multivariate normal distribution The density $P$ is now defined by a d-dimensional mean vector $\mu$ and a d-by-d correlation matrix $\Sigma.$ First you need to find a square root of $\Sigma.$ This is a d-by-d matrix $C$ such that $\Sigma=C C^T$ . The Cholesky factorisation as provided by numpy.linalg.cholesky is a good choice for this. The matrix $C$ transforms an uncorrelated zero-mean d-dimensional multivariate normal vector $Z$ to a properly correlated vector $X$ as follows: $$ X = C Z + \mu.$$ Apply step A. above to $Z$ and you have your transformation: $$ x = \Psi(u)=\mu + C (F^{-1}(u_1), \ldots, F^{-1}(u_d)).$$ C: Gaussian mixtures Now the density $P$ is a sum of $N$ components which are Gaussian densities $G_j$ , each with its own parameters $\mu_j$ and $\Sigma_j:$ $$ P(x) = \frac 1 N \sum_{j=1}^N G_j(x).$$ Note that an integral against such a density is a sum of $N$ integrals against the components and can accordingly be reduced to step B. But calculating $N$ integrals would defeat the purpose of doing a Bayesian quadrature with few function evaluations. So one should transform this case as well. To take care of the mixture you need to introduce an additional dimension, i.e. you need to use a $d+1$ dimensional uniform density. The idea is to use this additional variable to select the component and then apply a properly parameterized function $\Psi$ from step B to the remaining $d$ coordinates: $$ x = \Psi_0(u_0, u_1, \ldots,u_d) = \sum_{j=1}^N \mathbf 1_{[\frac{j-1}{N};\frac{j}{N}]}(u_0) \Psi_j(u_1,\ldots,u_d).$$ Here $\mathbf 1_{]\frac{j-1}{N};\frac{j}{N}]}$ is the 0-1 indicator function of the interval $]\frac{j-1}{N};\frac{j}{N}]$ and $\Psi_j$ a function according to step B. with mean $\mu_j$ and covariance $\Sigma_j$ according to the mixture parameters. Code example Note that the code is not written for speed and you need quite a sample size to see comparable means/integrals: import numpy as np from scipy.stats import norm # convention for axis 0: features 1: samples # Parameters: # number of features/dimension d = 2 # size of sample nsample = 100000 # number of components/centres in the mixture ncomp = 10 # target function for integration def f(x): return(np.sqrt(np.sum(x ** 2, 0))) # define data (locations, covariances) for the components, i.e. Gaussian distributions # locations/mean vectors (d-dimensional) mu = np.random.normal(loc=0, scale=1, size=(d, ncomp)) # Create for each component a Covariance matrix (d-by-d symmetric, positive definite matrix) covmat = np.zeros((ncomp,d,d)) L = np.zeros((d,d)) # Factor is upper triangular with d*(d + 1)/2 entries for imat in range(ncomp): # the entries just generated from random uniform L[np.tril_indices(d)] = np.random.uniform(size=int(d*(d + 1)/2)) covmat[imat,:,:] = np.matmul(L, L.T) # Generate sample from mixture distribution smp_mix = np.zeros((d, nsample)) # draw the components smpcomp = np.random.randint(0,ncomp,nsample) # draw appropriate normal from each component for isample in range(nsample): comp = smpcomp[isample] smp_mix[:,isample] = np.random.multivariate_normal(mu[:,comp], covmat[comp, :,:], size=1) # preparation for the uniform sample # calculate cholesky factors (pretend we did not know them already) C = np.zeros((ncomp,d,d)) for imat in range(ncomp): C[imat,:,:] = np.linalg.cholesky(covmat[comp, :,:]) # cumulative probabilities, here all equal 1/nsample for simplicity qprob = np.arange(ncomp)/ncomp # Function Psi doing the transformation def psi(u): # determine component according to first coordinate comp = sum(qprob
