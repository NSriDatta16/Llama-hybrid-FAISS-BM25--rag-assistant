[site]: crossvalidated
[post_id]: 494905
[parent_id]: 493254
[tags]: 
I'd argue that there are two potential complications with discarding the ordering information and just running multiclass regression: Model complexity: the parameters of the predictions for each category won't be tied together in any way. So the model is trying to learn how to predict category a, b, c, and d as four separate problems, without realizing that there is some structure (e.g. examples of class a will look more similar to those of class b than those of class c or d). This could lead to poor performance on relatively small datasets. As mentioned in the comments above, the multiclass loss function doesn't take the ordering into account. Presumably in your application mispredicting class a as class b is less bad than mispredicting it as class c. I can think of a few possible solutions to these issues. One option is to define a similarity matrix across conditions in some way. This could come from some measure of uncertainty between the categories (e.g. a confusion matrix from human labels), or be related to the consequences for misprediction (e.g. if class d is a very elite class that gets a special credit score, that could be designated as less similar to the other classes). This kind of similarity matrix could be used to solve both problems listed above, if it is used for regularization (encouraging similar classes to have similar parameters) and for loss (penalizing mispredictions less harshly for similar classes). Another possible answer is to just abandon ordinal prediction entirely, and try to predict the amount of money as a continuous value (which you could then discretize into bins if you wanted to). You still might need to think carefully about the loss function (e.g. if these values span multiple orders of magnitude you may want to penalize squared loss of the log of the values, rather than the values themselves).
