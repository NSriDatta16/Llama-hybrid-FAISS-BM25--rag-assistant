[site]: crossvalidated
[post_id]: 541524
[parent_id]: 335690
[tags]: 
For each individual datum in the minibatch, we'd be backpropagating through a different network Let's illustrate the dropout using three different scenarios(can be just treated as a simple logistic regression) as follows where the blue points are inputs $X$ (features, or the output of a layer) and red points are weights/parameters $W$ . This is an illustration of an input with batch size one: And here is that of batch size 3: In the mini-batch situation, the losses of all cases in a mini-batch sum to one loss, and then the gradient of each case is just added to update the weights in one go. The process of mini-batch is often accelerated by vectorization . Now let's add the dropout to the weights: You see that the dropout is applied to inputs rather than parameters. Or the dropout mechanism applied on $W$ works on the $X$ / preceding layer . So, it is just like zero out(multiply a mask with 1's and 0's with the same shape of the input/blue point matrix) some of the input features and the back propagation just works as that for the middle illustration here. For details of the mini-batch gradient descent please refer to this answer . An application of this trick is SimCSE .
