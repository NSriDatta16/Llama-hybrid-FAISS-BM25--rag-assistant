[site]: crossvalidated
[post_id]: 534556
[parent_id]: 346582
[tags]: 
In my experience feature selection tends to make performance worse rather than better if you are using a modern machine learning method that has some feature, such as regularisation, to avoid over-fitting. Miller's monograph on feature selection has similar advice hidden away in the appendices (sadly someone has borrowed my copy, so I can't find it). Basically feature selection is adding one binary degree of freedom to the learning problem for each input feature. This means the feature selection criteria can be reduced in two ways (i) getting rid of genuinely uninformative features (ii) selecting a set of features that happens to exploit some random sampling peculiarity of the data (i.e. overfitting). For an example of over-fitting in feature selection, see my answer to a related question about cross-validation and feature selection. The paper by Ambroise and MacLachlan is also well worth reading by anybody thinking of using feature selection with modern machine learning methods. Secondly, the class imbalance problem is not really due to the imbalance itself, but because there are too few patterns belonging to the minority class to adequately describe it's distribution. Most classifiers work fine with imbalanced data provided you have a lot of data. Attempts to balance the dataset can make things worse rather than better by over-correcting for the bias due to class imbalance. So if you have a performance problem due to class imbalance, it means you don't have enough data to adequately estimate the model parameters, in which case the last thing you should do is to perform feature selection as the added degrees of freedom this adds to the problem will only make the estimation problem worse. Regularisation is likely to be a much better solution as it adds essentially one continuous degree of freedom, and will be much less susceptible to over-fitting.
