[site]: crossvalidated
[post_id]: 66040
[parent_id]: 66018
[tags]: 
At the highest level, we can think of all manner of priors as specifying some amount of information that the researcher brings to bear on the analysis outside of the data itself: before looking at the data, which values of parameters are more likely? In the dark ages of Bayesian analysis, when the Bayesians were fighting it out with frequentists, there was a belief that the researcher would want to introduce as little information to the analysis via the prior as possible. So there was a lot of research and argument devoted to understanding how, precisely, a prior could be "non-informative" in this way. Today, Gelman argues against the automatic choice of non-informative priors, saying in Bayesian Data Analysis that the description "non-informative" reflects his attitude towards the prior, rather than any "special" mathematical features of the prior. (Moreover, there was a question in the early literature of at what scale a prior is noninformative. I don't think that this is especially important to your question, but for a good example of this argument from a frequentist perspective, see the beginning of Gary King, Unifying Political Methodology. ) A "flat" prior indicates a uniform prior where all values in the range are equally likely. Again, there are arguments to be had about whether these are truly non-informative, since specifying that all values are equally likely is, in some way, information, and may be sensitive to how the model is parameterized. Flat priors have a long history in Bayesian analysis, stretching back to Bayes and Laplace. A "vague" prior is highly diffuse though not necessarily flat, and it expresses that a large range of values are plausible, rather than concentrating the probability mass around specific range. Essentially, it is a prior with high variance (whatever "high" variance means in your context). Conjugate priors have the convenient feature that, when multiplied by the appropriate likelihood, they produce a posterior distribution with a “nice” expression. (There is some nuance here; see Do conjugate priors just lead to a posterior that is a modification of the parameters of the prior? ) One example of this is the beta prior with the binomial likelihood, or the gamma prior with the poisson likelihood. There are helpful tables of these all over the Internet and Wikipedia. The exponential family is extremely convenient in this regard. Conjugate priors are often the "default" choice for some problems because of their convenient properties, but this does not necessarily mean that they are the "best" unless one's prior knowledge can be expressed via the conjugate prior. Advances in computation mean that conjugacy is not as prized as it once was (cf Gibbs sampling vs NUTS), so we can more easily perform inference with non-conjugate priors without much trouble. Hyper-priors are priors on the prior. This means that rather than specifying, say, a $N(\mu,\sigma^2)$ prior on a parameter with fixed $\mu$ and $\sigma^2$ , you might express a prior on the parameter $\mu$ and a prior on the parameter $\sigma^2$ . Most often, this is used in hierarchical modeling, when you believe that there is a common feature to all of the data points in question (say, because you are performing a statistical analysis on replications of the same experiment), and that variation in the data is explained as being caused by random assignment of parameters from this common distribution to the data points.
