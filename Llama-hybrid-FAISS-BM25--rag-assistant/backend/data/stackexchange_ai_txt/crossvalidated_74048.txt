[site]: crossvalidated
[post_id]: 74048
[parent_id]: 74000
[tags]: 
I think I may have found the problem with my argument, i.e. how what i called "the frequentist interpretation of the bayesian approach" and viceversa don't really make sense: The "frequentist interpretation of the bayesian approach", as described in my quesion, doesn't make much sense because it says that it assumes the likelihood function (and the prior), but then says "no matter which data we get [in the hypothetical large set of experiments]" which is incompatible with the frequentist interpretation of the likelihood function. The "bayesian interpretation of the frequentist approach", is also wrong because it doesn't ensure what I say below. For example, in the frequentist approach, I may well make a measurement and have an emtpy confidence interval for it, which clearly means that it doesn't ensure an a% probability of being right. So I think I understand it better now. And so it seems that if you want to be as cautious as possible, frequentist is the way to go. But if you don't have much statistical significance, or have good reasons for your prior, bayesian is the way to go.
