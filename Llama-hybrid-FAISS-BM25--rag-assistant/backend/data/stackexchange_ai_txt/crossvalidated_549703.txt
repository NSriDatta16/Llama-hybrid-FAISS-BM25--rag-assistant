[site]: crossvalidated
[post_id]: 549703
[parent_id]: 457361
[tags]: 
I don't think it is misleading, it is just a bit short. As regularization is (often) on some norm of the parameter (in your terminology weights) vector, like $\| \beta \|^2$ (ridge), it keeps the overall size of the parameters (weights) down. But it does not do so in a blind way, it does so while minimally destroying the other part (negative log likelihood, sum of squares, ...) of the criterion (loss) function. That is consistent with the explanation in your linked post, as restricting the model space. Since the penalization term can be interpreted as a Bayesian prior in the model space, we are reducing the model space, not by cutting some parts of it off (as we would do by omitting some predictor, say), but by introducing a measure on the space, thereby down-weighting parts of it. It turns out that we are downweighting those parts corresponding to large parameter vectors.
