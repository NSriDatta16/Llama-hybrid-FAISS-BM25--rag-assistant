[site]: crossvalidated
[post_id]: 571258
[parent_id]: 355774
[tags]: 
A sign of Underfitting could be when the accuracy of the test and training are very similar. The way to "fix" it and through "tuning". If you are trainning a NN you could increase the number of training cycles, on the other hand, if you are working with traditionalML, for example a Logistic Regression, you coudl try to tune certain parameters(that come by default) of the algorithm that leas to a better perfomance of this, in the case of Logistic Regression it would be changing the differente values of C i.e signal of underfitting, then I tune my model # default logreg = LogisticRegression(solver='liblinear', random_state=0) worst performance, discard it # instantiate the model logreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=0) it improves performance so I keep it. # instantiate the model logreg100 = LogisticRegression(C=100, solver='liblinear', random_state=0)
