[site]: crossvalidated
[post_id]: 299293
[parent_id]: 299286
[tags]: 
On first thought, I think this dilemma is beside the point and that the distinction is an artificial one. Firstly, the initial "raw" set of numbers could be defined as a feature of the emitting source, so I am not sure that the definition of what is a feature and what is not is too explicit. For example, wikipedia uses the generic definition of a machine learning feature as: an individual measurable property or characteristic of a phenomenon being observed Moreover, classification algorithms - and in general, learning algorithms - work on numbers, manipulating them to build a model that classifies the input. If these numbers are good at describing and delineating the structure of the problem (e.g. good at separating the classes), then all is well. If not, we combine and manipulate these numbers (i.e. computing features out of them) in order to arrive at a different set of numbers that is better at explaining the structure and thus, does better as the input for the classification algorithm. The latter, though, is always doing the same thing - working on a set numbers to perform classification. I do not know if some algorithms could work better on a class of features (eg always do better using low rather than using high level features), though - would be nice to know if some do.
