[site]: crossvalidated
[post_id]: 281183
[parent_id]: 
[tags]: 
Alternative ways of coding the same output in neural networks?

When using a neural network for supervised learning, say recognition of hand written digits, there are several ways to use the output layer to code for the expected output. I was wondering if there are any crucial differences in performance or what the overall tendencies are. Basically, as a specific example with number recognition, say we have the output set of integrals {0,1,2,3}. I could code this using 4 output units, for which each single activation corresponds to one of the numbers (for example: 1000 = 0, 0100 = 1, 0010 = 2, 0001 = 3). I could also code this in binary using just 2 output layers, and train the network for giving the following output : 00 = 0, 01 = 1 , 10 = 2, 11 = 3. Are there any drawbacks of using this kind of architecture, where different outputs activate more than one output unit simultaneously?
