[site]: crossvalidated
[post_id]: 35301
[parent_id]: 35299
[tags]: 
Doubly robust estimation is not actually particularly hard to implement in the language of your choice. All you are actually doing is controlling for variables in two ways, rather than one- the idea being that as long as one of the two models used for control is correct, you've successfully controlled for confounding. The easiest way to do it, in my mind, is to use Inverse-Probability-of-Treatment (IPTW) weights to weight the data set, then also include variables in a normal regression model. This is how the authors approach the problem in the paper linked above. There are other options as well, usually built off propensity scores used for either matching or as a covariate in the model. There are lots of introductions to IPTW in whatever statistical language you prefer. I'd provide code snippets, but all of mine are in SAS, and would likely read very much like the authors. Briefly, what you do is model the probability of exposure based on your covariates using something like logistic regression and estimate the predicted probability of exposure based on that model. This gives you a propensity score. The Inverse Probability of Treatment Weight is, as the name suggests, 1/Propensity Score. This sometimes produces extreme values, so some people stabilize the weight by substituting the marginal probability of exposure (obtained by a logistic regression model of the outcome and no covariates) for 1 in the equation above. Instead of treating each subject in your analysis as 1 subject, you now treat them as n copies of a subject, where n is their weight. If you run your regression model using those weights and including covariates, out comes a doubly robust estimate. A word of caution however: While doubly (or triply, etc.) robust estimation gives you more chances to specify the correct covariate model, it does not guarantee you will do so. And more importantly, cannot save you from unmeasured confounding.
