[site]: crossvalidated
[post_id]: 28646
[parent_id]: 28609
[tags]: 
Since L1 regularization is equivalent to a Laplace (double exponential) prior on the relevant coefficients, you can do it as follows. Here I have three independent variables x1, x2, and x3, and y is the binary target variable. Selection of the regularization parameter $\lambda$ is done here by putting a hyperprior on it, in this case just uniform over a good-sized range. model { # Likelihood for (i in 1:N) { y[i] ~ dbern(p[i]) logit(p[i]) Let's try it out using the dclone package in R! library(dclone) x1 And here are the results, compared to an unregularized logistic regression: > summary(temp) > 1. Empirical mean and standard deviation for each variable, plus standard error of the mean: Mean SD Naive SE Time-series SE b[1] 1.21064 0.3279 0.005987 0.005641 b[2] 0.64730 0.3192 0.005827 0.006014 b[3] 1.25340 0.3217 0.005873 0.006357 b0 0.03313 0.2497 0.004558 0.005580 lambda 1.34334 0.7851 0.014333 0.014999 2. Quantiles for each variable: > > summary(glm(y~x1+x2+x3, family="binomial")) > Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 0.02784 0.25832 0.108 0.9142 x1 1.34955 0.32845 4.109 3.98e-05 *** x2 0.78031 0.32191 2.424 0.0154 * x3 1.39065 0.32863 4.232 2.32e-05 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 > And we can see that the three b parameters have indeed been shrunk towards zero. I don't know much about priors for the hyperparameter of the Laplace distribution / the regularization parameter, I'm sorry to say. I tend to use uniform distributions and look at the posterior to see if it looks reasonably well-behaved, e.g., not piled up near an endpoint and pretty much peaked in the middle w/o horrible skewness problems. So far, that's typically been the case. Treating it as a variance parameter and using the recommendation(s) by Gelman Prior distributions for variance parameters in hierarchical models works for me, too.
