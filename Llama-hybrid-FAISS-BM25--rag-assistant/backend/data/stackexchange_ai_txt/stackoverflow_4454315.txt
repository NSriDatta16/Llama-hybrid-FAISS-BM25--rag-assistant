[site]: stackoverflow
[post_id]: 4454315
[parent_id]: 
[tags]: 
Process Many Files Concurrently â€” Copy Files Over or Read Through NFS?

I need to concurrently process a large amount of files (thousands of different files, with avg. size of 2MB per file). All the information is stored on one (1.5TB) network hard drive, and will be processed by about 30 different machines. For efficiency, each machine will be reading (and processing) different files (there are thousands of files that need to be processed). Every machine -- following its reading of a file from the 'incoming' folder on the 1.5TB hard drive -- will be processing the information and be ready to output the processed information back to the 'processed' folder on the 1.5TB drive. the processed information for every file is of roughly the same average size as the input files (about ~2MB per file). What is the better thing to do: (1) For every processing machine M , Copy all files that will be processed by M into its local hard drive, and then read & process the files locally on machine M . (2) Instead of copying the files to every machine, every machine will access the 'incoming' folder directly (using NFS), and will read the files from there, and then process them locally. Which idea is better? Are there any 'do' and 'donts' when one is doing such a thing? I am mostly curious if it is a problem to have 30 machines or so read (or write) information to the same network drive, at the same time? (note: existing files will only be read, not appended/written; new files will be created from scratch, so there are no issues of multiple access to the same file...). Are there any bottlenecks that I should expect? (I am use Linux, Ubuntu 10.04 LTS on all machines if it all matters)
