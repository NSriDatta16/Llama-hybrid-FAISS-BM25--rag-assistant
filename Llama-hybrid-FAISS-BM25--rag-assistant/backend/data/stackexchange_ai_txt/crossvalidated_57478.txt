[site]: crossvalidated
[post_id]: 57478
[parent_id]: 57467
[tags]: 
I believe what you are getting at in your question concerns data truncation using a smaller number of principal components (PC). For such operations, I think the function prcomp is more illustrative in that it is easier to visualize the matrix multiplication used in reconstruction. First, give a synthetic dataset, Xt , you perform the PCA (typically you would center samples in order to describe PC's relating to a covariance matrix: #Generate data m=50 n=100 frac.gaps In the results or prcomp , you can see the PC's ( res$x ), the eigenvalues ( res$sdev ) giving information on the magnitude of each PC, and the loadings ( res$rotation ). res $sdev length(res$ sdev) res $rotation dim(res$ rotation) res $x dim(res$ x) By squaring the eigenvalues, you get the variance explained by each PC: plot(cumsum(res $sdev^2/sum(res$ sdev^2))) #cumulative explained variance Finally, you can create a truncated version of your data by using only the leading (important) PCs: pc.use $x[,1:pc.use] %*% t(res$ rotation[,1:pc.use]) #and add the center (and re-scale) back to data if(all(res $scale != FALSE)){ trunc center != FALSE)){ trunc You can see that the result is a slightly smoother data matrix, with small scale features filtered out: RAN And here is a very basic approach that you can do outside of the prcomp function: #alternate approach Xt.cen $sdev^2, E$ d) #PCA via a covariance matrix - the eigenvalues now hold variance, not stdev abline(0,1) # same results Now, deciding which PCs to retain is a separate question - one that I was interested in a while back . Hope that helps.
