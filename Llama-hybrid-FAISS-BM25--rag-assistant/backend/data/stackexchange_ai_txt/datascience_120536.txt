[site]: datascience
[post_id]: 120536
[parent_id]: 118260
[tags]: 
The T5 paper (section 3.2.1) has a very readable discussion of various Transformer model structures. It says As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction (Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). where Radford et al., 2018 is the GPT paper. As the T5 paper explains, "decoder" refers to a stack of Transformer layers with masked attention that cannot look ahead to future tokens, like the decoder in the original Transformer paper . It does not imply the presence of a side input from an encoder, because there is no encoder. According to noe's answer, this answer for GPT should carry over to the original question on ChatGPT.
