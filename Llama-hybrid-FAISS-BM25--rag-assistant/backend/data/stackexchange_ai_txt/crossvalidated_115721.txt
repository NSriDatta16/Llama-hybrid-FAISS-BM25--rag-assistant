[site]: crossvalidated
[post_id]: 115721
[parent_id]: 115494
[tags]: 
I'm guessing you have the following: Experiment 1: $n_1$ Bernoulli trials, probability of success $\pi_1$. Experiment 2: $n_2$ Bernoulli trials, probability of success $\pi_2=k\pi_1$ for some known $k$. The Bernoulli trials are independent both within and between experiments. Let $X_1$ be the observed number of successes in Experiment 1, and $X_2$ be the observed number of successes in Experiment 2. $X_1\sim \text{bin}(n_1,\pi_1)$ $X_2\sim \text{bin}(n_2,\pi_2)\,$, where $\pi_2=k\pi_1$ The usual estimates based on each experiment are $\hat{\pi}_1^{(1)}=p_1=\frac{X_1}{n_1}$ and $\hat{\pi}_1^{(2)}=p_2/k=\frac{X_2}{k\,n_2}$. The naive estimator would be $$\hat{\pi}_1^\text{naive}=\frac{X_1+X_2}{n_1+kn_2}$$ which is usually fairly reasonable, especially if the true proportion is small, but this overweights the estimate from the first sample and can produce "impossible" answers (e.g. consider $k=2$ and $n_1=n_2=2$ where $X_1=2$ and $X_2=2$; the estimate is $2/3$ but it cannot exceed $1/2$). The estimates are independent, with variances $\pi_1(1-\pi_1)/n_1$ and $\frac{1}{k^2}\,\pi_2(1-\pi_2)/n_2=\frac{1}{k^2}\,k\pi_1(1-k\pi_1)/n_2=\pi_1(\frac{1}{k}-\pi_1)/n_2$ respectively. Note that because $\pi_2\leq 1\,$, $\pi_1\leq \frac{1}{k}$. One reasonable approach would therefore be to take a weighted average, $\hat{\pi}^{w}=w_1\hat{\pi}_1^{(1)}+w_2\hat{\pi}_1^{(2)}$ where the $w_i$ are inversely proportional to the respective variances and sum to 1: so $$\hat{\pi}^{w}=\frac{\frac{X_1}{n_1}\cdot \frac{n_1}{\pi_1(1-\pi_1)}+\frac{X_2}{k\,n_2}\cdot \frac{n_2}{\pi_1(\frac{1}{k}-\pi_1)}}{\frac{n_1}{\pi_1(1-\pi_1)}+\frac{n_2}{\pi_1(\frac{1}{k}-\pi_1)}}$$ $$\quad=\frac{\frac{X_1}{\pi_1(1-\pi_1)}+\frac{X_2}{\pi_1(1-k\pi_1)}}{\frac{n_1}{\pi_1(1-\pi_1)}+\frac{n_2}{\pi_1(\frac{1}{k}-\pi_1)}}$$ $$\quad=\frac{X_1+X_2\frac{1-\pi_1}{1-k\pi_1}}{n_1+kn_2\frac{1-\pi_1}{1-k\pi_1}}$$ Of course, we don't know $\pi_1$, which is the difficulty here. For small $\pi_1$, the ratio $\frac{1-\pi_1}{1-k\pi_1}$ is very close to 1, while for $\pi_1$ near $\frac{1}{k}$ that ratio becomes large. We could compute an MLE. We have the log-likelihood, $\hspace{1cm}\cal{l}=$ $ \log{n_1\choose X_1}+X_1\log(\pi_1)+(n_1-X_1)\log(1-\pi_1) \\ + \log{n_2\choose X_2}+{X_2}\log(k\pi_1)+(n_2-X_2)\log(1-k\pi_1)$ $\hspace{1cm}\frac{\partial\cal{l}}{\partial \pi_1}=$ $ X_1/\pi_1-(n_1-X_1)/(1-\pi_1) \\ + X_2/(k\pi_1)\cdot k-(n_2-X_2)/(1-k\pi_1)\cdot k$ If we set that equal to 0 and attempt to solve for $\hat{\pi}_1$ we get: $ X_1/\hat{\pi}_1+X_2/\hat{\pi}_1=(n_1-X_1)/(1-\hat{\pi}_1) +k(n_2-X_2)/(1-k\hat{\pi}_1)$ $(X_1+X_2)/\hat{\pi}_1+X_1/(1-\hat{\pi}_1)+kX_2/(1-k\hat{\pi}_1)=n_1/(1-\hat{\pi}_1)+kn_2/(1-k\hat{\pi}_1)$ Multiplying through by the denominators we get a quadratic to solve; there's nothing actually difficult about it, but it's not nice to type up, but anyway I shouldn't give the detailed working if that's what you're supposed to be doing, but that gives the gist of it. (To do this properly you also need to confirm the turning point is actually a maximum; I've also left this to you but in practice this is simple enough.)
