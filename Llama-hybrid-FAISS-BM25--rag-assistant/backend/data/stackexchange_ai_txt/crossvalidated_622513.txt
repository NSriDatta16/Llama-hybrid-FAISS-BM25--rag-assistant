[site]: crossvalidated
[post_id]: 622513
[parent_id]: 
[tags]: 
Using Multiple Imputation Techniques in Data Analysis

I recently worked with two different statisticians who both suggested different strategies for dealing with imputation of missing data. For the sake of this discussion, I'll call them Statistician A and Statistician B. Let's say that I have missing values across some cells in many columns in a dataset (all numeric and I've been told by by Statistician A that a multiple imputation technique (like PMM or CART (mice package in R)) is the most robust method to use. After completing the imputation, I completely understand the need to pool the results in regression analyses or other statistical tests to understand the contribution of each imputed dataset. And I understand that you cannot "pool" the data outside of a statistical test by averaging or stacking the data to combine all imputed datasets into a single dataset. However, if one wants to use a multiple imputation to generate a complete (single) dataset for data visualization purposes (ex. heatmaps) or more complicated techniques like PCA it seems as though the technique is no longer appropriate. Statistician A stated that there is no perfect imputation technique and suggested using one of the imputed datasets (chosen at random) for all analyses/visualizations. I understand that this is in direct opposition to the primary benefits of multiple imputation techniques. However, I am wondering if this option is flat out incorrect and should never be used, or if this can still be utilized as a slightly better option than direct mean imputation (or other similar single imputation techniques) even though it is not technically correct? An exploration of when/if ever this might be a plausible strategy would be most helpful. Statistician B always goes by the book and stated that data imputed via multiple imputation can only be used in analyses where the results can be pooled (linear regression). The suggestion was to drop all rows with missing data in any column. This stance is, I think, correct but somewhat limiting (and results in massive data loss with a large dataset where there are missing values across many columns). This stance is also confusing given the large number of published papers that mention the use of multiple imputation and do not ever describe pooling results or changing the method when producing visualizations, etc. And in a case where I do need to use a regression technique and I employ multiple imputation, but then would like to visualize the results, do I create a visualization with each imputed dataset or choose the values from just one? Would someone be able to comment on how multiple imputation techniques are currently being used in scientific papers (or mis-used) and whether we should be using single imputation techniques instead? Or is this a complete misunderstanding and there is indeed a way to use multiple imputation for the tasks I've described? I am not a statistician myself so a simplified response would be much appreciated!
