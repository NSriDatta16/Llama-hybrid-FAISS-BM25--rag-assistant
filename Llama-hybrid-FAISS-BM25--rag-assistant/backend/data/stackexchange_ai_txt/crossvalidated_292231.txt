[site]: crossvalidated
[post_id]: 292231
[parent_id]: 
[tags]: 
Will this interpretation of my error function lead to the desired behavior?

As a bit of background, I'm trying to design a convolutional neural network that takes in a spectrogram input image where each input's label is a binary number being either 0 or 1. Then at the end of my network I have an output that also ranges from 0 to 1, but is continuous rather than discrete. As far as I know and have read in the past, error functions online is that the error function selection should be solely dependent on the output layer, which makes sense, so I went with mean-squared-error for my error function for determining a continuous output. The catch seems to be, from where I stand, that the labels applied to my data don't mirror the actual output value of what that image evaluates to, but is rather a ceiling function of the data's true labels (the true continuous labels are not possible to determine when the data is gathered). Normally this wouldn't seem like a problem to me, because a continuous network output would naturally then be interpreted as the confidence level of the signal being a 0 or 1 once the output is passed through the network. But what I really want to do is not to have the network's output be a confidence level, but rather I am attempting to recover the true evaluation of the signal using only these binary labels. I.e. if an input's true value is 0.4, it's training label would be a 1, but when passed through the network it should output a 0.4, ideally, that represents the true 0.4 value of the input. Is it possible to interpret that confidence level as a prediction of the true value of the signal? Somehow that doesn't sit right with me, it seems like I'm comparing apples to oranges, in a sense. If my error function is (1/N)*(out - in)^2 for a single input batch, by the mean squared error, I feel like the "out" part is the network's guess at the true function of the input, whereas the "in" part is the ceiling function of the signal's output. It seems as if this would cause my network to simply learn the ceiling function of the function of the signal rather than being able to recover the true function of the signal. If that is the case, what options should I look into to get the behavior that I'm looking for? Or am I completely off base with this approach? Thanks in advance for any help you can provide.
