[site]: crossvalidated
[post_id]: 595009
[parent_id]: 
[tags]: 
Standard error for coefficient on dummy with no variation

Say I have a dummy variable $X$ (e.g., gender) and that units are divided into groups. I want to compute the average value of $X$ in each group. To achieve this, we can estimate the following linear model via OLS: $X_i = \sum_{g = 1}^G Group_{i, g} \, \beta_g + \epsilon $ with $Group_{i, g}$ a dummy variable equal to one if the $i$ -th unit belongs to the $g$ -th group, and $G$ the number of groups. One can show that $\beta_g$ corresponds to the mean of $X$ in the $g$ -th group. So, the advantage of this approach consists of getting standard errors for the average values in each group. Suppose that my $X$ has no variation in one of the groups, and assume that this happens for the first group without loss of generality. Is this a problem for my linear model? The point estimate should be fine, but what about the standard error? Can I interpret it conventionally, i.e., measuring the sampling uncertainty of my estimated coefficient? I am getting a hard time thinking about this, though I may be simply overthinking. How should I think about this sampling uncertainty, if there is no variation of $X$ in the first group? Follows a toy example to make the point, where I code $X$ as x2 and $Groups$ as groups : ## Generate data. set.seed(1986) n [1] 0 ## Regressing x2 on groups to get group means and standard errors. model #> Call: #> lm(formula = x2 ~ 0 + groups, data = dta) #> #> Residuals: #> Min 1Q Median 3Q Max #> -0.5000 -0.4414 0.0000 0.5000 0.5586 #> #> Coefficients: #> Estimate Std. Error t value Pr(>|t|) #> groups1 0.00000 0.02174 0.00 1 #> groups2 0.50000 0.02251 22.22 groups3 0.44144 0.02213 19.95 --- #> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> #> Residual standard error: 0.4039 on 997 degrees of freedom #> Multiple R-squared: 0.4721, Adjusted R-squared: 0.4705 #> F-statistic: 297.1 on 3 and 997 DF, p-value:
