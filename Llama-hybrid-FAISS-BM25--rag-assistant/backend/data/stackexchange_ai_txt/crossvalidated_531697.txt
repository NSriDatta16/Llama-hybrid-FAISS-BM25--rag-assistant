[site]: crossvalidated
[post_id]: 531697
[parent_id]: 
[tags]: 
What does it mean if the distribuctions of TP and FP are really similar?

I was training a Neural Network these days, and I plotted the values of TP, FP, TN, FN on the validation set as the training progresses. I found that on the validation set, the distribuctions of TP and FP are really similar, and the distribuctions of TN and FN are really similar. Namely, it seems that $$ |TP-FP| = a \quad \text{and} \quad |TN-FN| = b $$ for some constants $a,b$ . Also, the total accuracy is almost a straight line. Does this phenomenon mean anything or is it just a common scene? Does it imply that the Neural Network actually does not learn anything at all?
