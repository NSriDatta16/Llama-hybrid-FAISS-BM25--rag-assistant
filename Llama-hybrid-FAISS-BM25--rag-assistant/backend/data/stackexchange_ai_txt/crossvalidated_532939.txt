[site]: crossvalidated
[post_id]: 532939
[parent_id]: 532934
[tags]: 
You don't need a neural network to summarize the data! Why not just summarize it? If you need to summarize the "current state" of a sequence that depends on recent items more than on the older ones, you can just use something like exponential smoothing on the one-hot encoded data: import numpy as np arr = ["a", "a", "a", "b", "b", "a", "a", "c", "a", "b", "b", "c"] enc = np.zeros((len(arr), len(set(arr)))) for i, x in enumerate(arr): for j, ch in enumerate(set(arr)): if x == ch: enc[i, j] = 1 def exponential_smoothing(arr, alpha): out = np.zeros(arr.shape[1]) for i in range(arr.shape[0]): out = (1-alpha)*out + alpha*arr[i,:] return out exponential_smoothing(enc, 0.5) ## array([0.53125 , 0.08764648, 0.38085938]) If you prefer to smooth the embeddings rather than one-hot codes, you can apply the above procedure to the embeddings. Just apply the embeddings algorithm element-wise to the sequence and then calculate the exponential smoothing on the embeddings. Implementing it took few minutes, to code, train, and validate a neural network you would need at best several hours. The result of using exponential smoothing is easily interpretable vs using a "black-box" neural network. I bet that even if you used a naive, pure-python implementation it will run faster than the neural network.
