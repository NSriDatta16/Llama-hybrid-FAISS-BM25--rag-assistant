[site]: crossvalidated
[post_id]: 251293
[parent_id]: 
[tags]: 
Deep Learning: Loss not going down (almost constant) while training GoogLeNet

I am trying to fine-tune GoogLeNet in caffe. DataSet: Images from 8 localities in New York (approx 2500 images in Training data from each of these 8 classes) So, in this fine-grained classification, i wanted the analyze AlexNet and GoogLenet. I retrained only last 2 FC layers of AlexNet and got around 70% Test accuracy. However, on GoogLeNet i am consistently getting around 20-30% train accuracy while finetuning. The loss stays almost constant around 2. Its barely going down. I have played around all the hyperparameters? I have just changed the last FC layers, in both these networks. Is this expected behavior? Shouldn't GoogLeNet give better results? Do i need to tweak something else in GoogLeNet?
