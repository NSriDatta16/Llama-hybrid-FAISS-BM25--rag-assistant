[site]: crossvalidated
[post_id]: 345651
[parent_id]: 
[tags]: 
How do you explain many optimal models in penalized Logistic regression?

I am building penalized logistic regression using Lasso and Ridge methods. I know that the best model chosen by the program is which has alpha = 1 and lambda = 0.06 . But how do you explain the existence of many couples of parameters alpha and lambda giving optimal models (where Accuracy =1)? For example, for these couples of parameters (0.8,0.01),(1.0,0.01),(1.0,0.02), we have optimal models... The code: set.seed(seed) cs_data_train Then the output is: glmnet 4620 samples 29 predictor 2 classes: '0', '1' Pre-processing: centered (149), scaled (149) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4158, 4158, 4158, 4158, 4158, 4158, ... Resampling results across tuning parameters: alpha lambda Accuracy Kappa 0.0 0.01 0.8893939 0.7774836 0.0 0.02 0.8893939 0.7774836 0.0 0.03 0.8885281 0.7757412 0.0 0.04 0.8779221 0.7543317 0.0 0.05 0.8675325 0.7333642 0.0 0.06 0.8612554 0.7206986 0.0 0.07 0.8543290 0.7066855 0.0 0.08 0.8504329 0.6988230 0.0 0.09 0.8443723 0.6865703 0.0 0.10 0.8400433 0.6778066 0.0 0.11 0.8361472 0.6699275 0.0 0.12 0.8331169 0.6637698 0.0 0.13 0.8309524 0.6594043 0.0 0.14 0.8296537 0.6567584 0.0 0.15 0.8270563 0.6515089 0.0 0.16 0.8246753 0.6466761 0.0 0.17 0.8222944 0.6418430 0.0 0.18 0.8199134 0.6369969 0.0 0.19 0.8168831 0.6308568 0.0 0.20 0.8151515 0.6273344 0.1 0.01 0.9411255 0.8817949 0.1 0.02 0.9086580 0.8163165 0.1 0.03 0.8904762 0.7796019 0.1 0.04 0.8803030 0.7590427 0.1 0.05 0.8714286 0.7411121 0.1 0.06 0.8658009 0.7297661 0.1 0.07 0.8584416 0.7149017 0.1 0.08 0.8543290 0.7065741 0.1 0.09 0.8504329 0.6986993 0.1 0.10 0.8478355 0.6934132 0.1 0.11 0.8454545 0.6885879 0.1 0.12 0.8430736 0.6837597 0.1 0.13 0.8400433 0.6776377 0.1 0.14 0.8378788 0.6732583 0.1 0.15 0.8359307 0.6693177 0.1 0.16 0.8344156 0.6662373 0.1 0.17 0.8337662 0.6649162 0.1 0.18 0.8329004 0.6631445 0.1 0.19 0.8316017 0.6604880 0.1 0.20 0.8305195 0.6582842 0.2 0.01 0.9532468 0.9061740 0.2 0.02 0.9112554 0.8215160 0.2 0.03 0.8945887 0.7878405 0.2 0.04 0.8846320 0.7677195 0.2 0.05 0.8781385 0.7546356 0.2 0.06 0.8746753 0.7476490 0.2 0.07 0.8688312 0.7358789 0.2 0.08 0.8619048 0.7218689 0.2 0.09 0.8556277 0.7091517 0.2 0.10 0.8523810 0.7025681 0.2 0.11 0.8484848 0.6946797 0.2 0.12 0.8458874 0.6894193 0.2 0.13 0.8437229 0.6850310 0.2 0.14 0.8415584 0.6806422 0.2 0.15 0.8363636 0.6700747 0.2 0.16 0.8344156 0.6661226 0.2 0.17 0.8335498 0.6643568 0.2 0.18 0.8318182 0.6608323 0.2 0.19 0.8313853 0.6599502 0.2 0.20 0.8300866 0.6573041 0.4 0.01 0.9872294 0.9744139 0.4 0.02 0.9183983 0.8358886 0.4 0.03 0.9047619 0.8083648 0.4 0.04 0.8991342 0.7969970 0.4 0.05 0.8878788 0.7742399 0.4 0.06 0.8822511 0.7628754 0.4 0.07 0.8781385 0.7545657 0.4 0.08 0.8720779 0.7422980 0.4 0.09 0.8655844 0.7291111 0.4 0.10 0.8588745 0.7154915 0.4 0.11 0.8538961 0.7053790 0.4 0.12 0.8512987 0.7000941 0.4 0.13 0.8497835 0.6970093 0.4 0.14 0.8454545 0.6881825 0.4 0.15 0.8437229 0.6846173 0.4 0.16 0.8419913 0.6810437 0.4 0.17 0.8398268 0.6765972 0.4 0.18 0.8374459 0.6717235 0.4 0.19 0.8337662 0.6641936 0.4 0.20 0.8329004 0.6623775 0.6 0.01 0.9995671 0.9991335 0.6 0.02 0.9458874 0.8913305 0.6 0.03 0.9164502 0.8319373 0.6 0.04 0.9073593 0.8135667 0.6 0.05 0.9015152 0.8017627 0.6 0.06 0.8974026 0.7934540 0.6 0.07 0.8859307 0.7702013 0.6 0.08 0.8748918 0.7477933 0.6 0.09 0.8679654 0.7337078 0.6 0.10 0.8640693 0.7257815 0.6 0.11 0.8601732 0.7178373 0.6 0.12 0.8538961 0.7050521 0.6 0.13 0.8480519 0.6931004 0.6 0.14 0.8430736 0.6829322 0.6 0.15 0.8385281 0.6735822 0.6 0.16 0.8324675 0.6611737 0.6 0.17 0.8238095 0.6434110 0.6 0.18 0.8179654 0.6314211 0.6 0.19 0.8114719 0.6180772 0.6 0.20 0.8015152 0.5976493 0.8 0.01 1.0000000 1.0000000 0.8 0.02 0.9995671 0.9991335 0.8 0.03 0.9989177 0.9978338 0.8 0.04 0.9430736 0.8856538 0.8 0.05 0.9099567 0.8187660 0.8 0.06 0.8926407 0.7836762 0.8 0.07 0.8794372 0.7568573 0.8 0.08 0.8714286 0.7405558 0.8 0.09 0.8621212 0.7216059 0.8 0.10 0.8534632 0.7039345 0.8 0.11 0.8452381 0.6871445 0.8 0.12 0.8424242 0.6813920 0.8 0.13 0.8417749 0.6800643 0.8 0.14 0.8417749 0.6800643 0.8 0.15 0.8417749 0.6800643 0.8 0.16 0.8268398 0.6494935 0.8 0.17 0.7930736 0.5803180 0.8 0.18 0.7930736 0.5803180 0.8 0.19 0.7930736 0.5803180 0.8 0.20 0.7930736 0.5803180 1.0 0.01 1.0000000 1.0000000 1.0 0.02 1.0000000 1.0000000 1.0 0.03 1.0000000 1.0000000 1.0 0.04 1.0000000 1.0000000 1.0 0.05 1.0000000 1.0000000 1.0 0.06 1.0000000 1.0000000 1.0 0.07 0.9523810 0.9037432 1.0 0.08 0.8417749 0.6800643 1.0 0.09 0.8417749 0.6800643 1.0 0.10 0.8417749 0.6800643 1.0 0.11 0.8417749 0.6800643 1.0 0.12 0.8417749 0.6800643 1.0 0.13 0.8417749 0.6800643 1.0 0.14 0.8417749 0.6800643 1.0 0.15 0.8417749 0.6800643 1.0 0.16 0.8019481 0.5985388 1.0 0.17 0.7930736 0.5803180 1.0 0.18 0.7930736 0.5803180 1.0 0.19 0.7930736 0.5803180 1.0 0.20 0.7930736 0.5803180 Accuracy was used to select the optimal model using the largest value. The final values used for the model were alpha = 1 and lambda = 0.06. How do you explain this please? Thank you in advance
