[site]: crossvalidated
[post_id]: 264643
[parent_id]: 
[tags]: 
Early Stopping criteria not using loss function

I'm reading this paper , which trains an autoencoder to perform background subtraction on an image. The use SGD batch training with binary cross entropy loss function to train the autoencoder on D training examples ${x_1,...,x_D}$. The trained networks response to the training examples are ${y_1,...y_D}$. However there stopping criteria seems to using the mean squared error loss, not the binary cross entropy loss. They stop training the network when $\sum_{i=1}^{D} |x_i -y_i|^2 This is kind of peculiar to me. I've never seen this kind of stopping criteria. Usually the stopping criterion is when the validation loss stops improving, but they don't even have a validation set. Is there any reason they chose this criteria?
