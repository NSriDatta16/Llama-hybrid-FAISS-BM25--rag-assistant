[site]: datascience
[post_id]: 14095
[parent_id]: 14094
[tags]: 
No, area under receiver operating characteristic (AUROC) is just one metric amongst very many possibilities, even assuming you just want to pick a standard approach. There are too many to list in a simple Stack Exchange answer. You can take a look at this list extracted from scikit learn documentantion on metrics for example, which is not by any means exhaustive: Accuracy classification score. Area Under the Curve (AUC) using the trapezoidal rule Average precision (AP) from prediction scores The Brier score. The F1 score, also known as balanced F-score or F-measure The F-beta score The average Hamming loss. Average hinge loss (non-regularized) Jaccard similarity coefficient score Log loss, aka logistic loss or cross-entropy loss. The Matthews correlation coefficient (MCC) for binary classes Area Under the Curve (AUC) from prediction scores Zero-one classification loss. What you should choose depends entirely on your model class and goals for the learning work. AUROC is a good choice for a binary classifier when you have different business costs associated with false positives and false negatives. That is because it gives you a sense of how well the classifier can be tuned to be more or less sensitive, and get the best outcomes by changing the class threshold. There is no minimum AUC or other metric required to select a model in practice. It depends on performance of existing solutions (including non-ML ones) and what the costs vs benefits would be of using the model. Clearly a poorly-performing model (e.g. with AUROC 0.5, no better than random guessing on a balanced data set) is unlikely to gain much from being implemented in a production environment, and you would want to take a new look at the original problem and see if was reasonable to expect anything from ML at all. To decide which metric to use, you need to define some goal for the eventual model. An ideal metric is one that can be simply converted to terms that the end users will care about. Failing that, one that can be compared to known results from other ML work in the subject area (domain) of the problem.
