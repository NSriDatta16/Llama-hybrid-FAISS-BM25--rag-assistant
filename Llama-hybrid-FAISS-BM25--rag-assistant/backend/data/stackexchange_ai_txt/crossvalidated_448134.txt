[site]: crossvalidated
[post_id]: 448134
[parent_id]: 
[tags]: 
Tensorflow neural network - combined sigmoid prediction probabilities consistently exceed 100%

I have a very basic Tensorflow model: self.model = models.Sequential() self.model.add(layers.Dense(32, input_shape=x_train.shape[1:])) self.model.add(layers.Dense(32, activation="relu")) self.model.add(layers.Dense(3, activation="sigmoid")) self.model.compile(optimizer="adam", loss= "sparse_categorical_crossentropy", metrics=["accuracy"]) self.model.fit(x_train, y_train, batch_size=32, epochs=5) Where y_train contains classes numbered 0, 1, or 2 based on whether certain values exceed a value to the positive or negative side (i.e., 2 > value , 0 -value , and 1 is in between -value and -value ). However, whenever I run this, it consistently predicts values that, in combination, sum to greater than 100%. Here's an example of the output of predictions made by this model. [[0.6157769 0.6730976 0.55768687] [0.5735974 0.68965054 0.5702088 ] [0.5662837 0.6933662 0.5536359 ] ... [0.47471496 0.71833104 0.43336612] [0.49893475 0.70974916 0.45622772] [0.53450066 0.7057426 0.46121603]] If I try to determine the likelihood of a 0 or 2 from the prediction output, the model actually performs pretty well, particularly if I use a margin of safety (e.g., require the 2 class probability to exceed the 0 class probability by some buffer). However, when I switch the activation of the last layer from sigmoid to softmax , the model basically doesn't work in terms of correlating classes with actual values (i.e., high probabilities of class 2 should relate to higher actual values). I'm just trying to figure out why this might be happening, why the model predicts > 100% across all classes, and why softmax does not work nearly as well.
