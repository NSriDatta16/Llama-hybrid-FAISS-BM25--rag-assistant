[site]: crossvalidated
[post_id]: 156281
[parent_id]: 
[tags]: 
Is there a standard way for training neural networks with negative-labeled data?

I have a project ( http://write-math.com ) where I want to classify handwritten recordings into symbols. I get my data from a crowd-sourcing approach (with lots of filtering by hand, because people give obviously wrong labels). For some of my data, I cannot say what the correct label would be. For example, a round shape could be \circ , O , o , 0 , ... But I can say that a round shape is NOT \sigma , \int , \infty , ... Is there a standard way to learn from such negative labels with a multilayer Perceptron? I guess I would have to implement a custom error function which gives no error for any output of the net of the possible values, but an error for anything different from 0 in the negative classes. Do you know any paper where somebody did this kind of learning?
