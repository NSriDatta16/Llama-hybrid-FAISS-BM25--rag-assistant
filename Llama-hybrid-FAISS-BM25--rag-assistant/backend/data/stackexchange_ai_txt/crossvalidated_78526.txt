[site]: crossvalidated
[post_id]: 78526
[parent_id]: 
[tags]: 
Bayesian Linear Regression is so hard to understand?

I'm learning Bayesian Linear Regression from a book, the linear model is $$p(w|x,\phi,\sigma^2)=Norm_w[\phi^Tx,\sigma^2]$$, as put in the book, we use Bayes approach to do the parameters estimation. Here comes the problem: I thought it is pretty clear that we should introduce a conjugate prior for the parameters $\phi$ and $\sigma^2$, which should have a normal-scaled inverse gamma distribution , right? But the book first assume that $\sigma^2$ is known, and introduce a prior distribution for $\phi$ alone, which is a 0-mean Gaussian, and does the estimation for $\phi$. After all that, it assumes $\sigma^2$ is not known, and estimates it. Why should we separate them? UPDATE The book is Machine Learning: A Probabilistic Perspective , p. 232, Section "Baysian Linear Regression." I just found this article which also assumes one is known and later assumes it is unknown. Bayesian Linear Regression
