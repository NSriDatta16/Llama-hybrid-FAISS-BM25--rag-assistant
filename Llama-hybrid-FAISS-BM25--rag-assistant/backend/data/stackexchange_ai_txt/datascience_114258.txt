[site]: datascience
[post_id]: 114258
[parent_id]: 114244
[tags]: 
will the multi-head self attention layer's size have to be tailored to the longest possible sequence even when input batches don't contain any of the long sequences? No. The attention is automatically tailored to the length of the batch, not the maximum possible length. Furthermore, with a technique called “bucketing” you create batches with similar lengths to avoid wasting space of the batch with padding tokens. Deep learning frameworks like Tensorflow and Pytorch make it easy to add bucketing to your data loading logic. Original answer will the multi-head self attention layer's size have to be tailored to the longest possible sequence even when input batches don't contain any of the long sequences? Yes. However, you normally use "bucketing". This technique consists of creating batches with similar lengths, to avoid wasting space of the batch with padding tokens. Deep learning frameworks like Tensorflow and Pytorch make it easy to add bucketing to your data loading logic.
