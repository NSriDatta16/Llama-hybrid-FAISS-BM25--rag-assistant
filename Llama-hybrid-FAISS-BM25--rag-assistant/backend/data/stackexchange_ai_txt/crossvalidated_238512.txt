[site]: crossvalidated
[post_id]: 238512
[parent_id]: 
[tags]: 
Bayesian word2vec

I'm trying to implement word2vec in pymc3 as shown here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb . Now I can implement everything with regards to the weights and biases. However, I don't know how to implement a sampled_softmax_loss . This function does negative sampling from a large set of labels. Is there a Bayesian equivalent (before you say this is Bayesian by default read on)? The number of labels are ~15k. So doing a full softmax might be slow/ infeasible. I'm also thinking of using ADVI so straight out sampling methods are out . Why not use normal sampling? Well the hidden representations of those words/ labels are a 15000 x 100 matrix. The closest Bayesian equivalent that I can think of is recommendation engines/ matrix factorisation. I'd like to use stan but the batch processing capability is too good in pymc3 that I can't go past it. But I welcome any solutions from the stan community as well.
