[site]: crossvalidated
[post_id]: 620066
[parent_id]: 
[tags]: 
Splitting strategy for performing hyperparameter tuning, algorithm comparison and model validation in one experiment

Let's say that for a supervised machine learning experiment I am using a fixed learning algorithm (e.g. Random Forest), and I want to achieve the following: Choose optimal hyper-parameters for the learning algorithm Estimate the performance of the algorithm on unseen data and / or compare it fairly to a benchmark algorithm In this case, a suitable data splitting strategy might be 3-way hold out , splitting my data into train , validation , and test set . Then the setup for the experiment could be: Train models with various hyper-parameter configurations on the train set. Evaluate these models on the validation set to choose optimal hyperparameters. Train a model on train + validation set and evaluate performance / compare to benchmark algorithm on the test set. Now what if, in addition, I want to compare various learning algorithms with each other and select the best performing one for my final model ? I think that in this case a 3-way hold out would not suffice, because the validation set has already been used for hyperparameter optimization. If I would also use the validation set for algorithm selection, algorithms with a larger search space for hyperparameter selection might have an unfair advantage, because there are simply more models trained for this learning algorithm. Also, I shouldn't use the test set for selecting the best learning algorithm, because I want an unbiased estimate of the performance of the final algorithm on unseen data. Doing both algorithm selection and final performance evaluation on the test set would lead to an overly optimistic estimation of algorithm performance. In my opinion, this problem setup should be quite common and one obvious solution would be to use a 4-way hold out , say splitting the data into train , validation , comparison and test set, and the following setup for the experiment: For each algorithm, train models with various hyper-parameter configurations on the train set. Evaluate these models on the validation set to choose, for each algorithm, the best hyperparameters. For each algorithm, train a model on train + validation set, and compare these models on the comparison set . Select the best algorithm. Train a model on train + validation + comparison set using the chosen algorithm, and evaluate performance / compare to benchmark algorithm on the test set. I feel like this should be quite a common issue, yet I haven't heard yet of a 4-way hold out or similar data splitting / experiment setup that achieves all of the following objectives: For various learning algorithms, choose optimal hyper-parameters for each algorithms. Among the tuned learning algorithms, choose the one that performs best. Fairly compare the chosen algorithm to a benchmark algorithm and / or fairly estimate the performance on unseen data. Am I somehow overcomplicating things and you don't really need a 4-way split? Could you provide some references for experiment setups that achieve all of these objectives?
