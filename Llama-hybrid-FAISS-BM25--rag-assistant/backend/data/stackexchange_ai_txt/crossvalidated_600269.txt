[site]: crossvalidated
[post_id]: 600269
[parent_id]: 260885
[tags]: 
Another method to deal with complete separation in logistic regression is penalization. To demonstrate, I'll use Firth's logistic regression implemented in the logistf package. library("logistf") model logistf(formula = response ~ treatment, firth = TRUE) #> #> Model fitted by Penalized ML #> Coefficients: #> coef se(coef) lower 0.95 upper 0.95 Chisq p #> (Intercept) 3.713572 1.431356 1.733332 8.559073 24.38651 7.881859e-07 #> treatmentC -4.651842 1.535565 -9.575829 -2.342167 22.88854 1.716719e-06 #> treatmentD -6.278521 2.050031 -12.052663 -3.138511 23.16264 1.488616e-06 #> method #> (Intercept) 2 #> treatmentC 2 #> treatmentD 2 #> #> Method: 1-Wald, 2-Profile penalized log-likelihood, 3-None #> #> Likelihood ratio test=32.53828 on 2 df, p=8.598101e-08, n=41 #> Wald test = 12.63304 on 2 df, p = 0.001806222 The estimates and confidence intervals agree reasonably well with the Bayesian analysis described in kjetilbhalvorsen's answer. See also How to deal with perfect separation in logistic regression? .
