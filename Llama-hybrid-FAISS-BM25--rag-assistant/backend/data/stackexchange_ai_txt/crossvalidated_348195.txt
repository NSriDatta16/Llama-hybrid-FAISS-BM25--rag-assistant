[site]: crossvalidated
[post_id]: 348195
[parent_id]: 342910
[tags]: 
In the pre-trained word embeddings that have been loaded, each word will have a vector of the same length, $n$. doc_vectors = [] for doc in documents: doc = tokenize(doc) feature_vector = zeros_array(n) for word in document: vec = vector (embedding) for the word feature_vector += vec feature_vec = feature_vec / len(doc) doc_vectors.append(feature_vector) So for the example in the question: vec("banana") = array([[1.56, -2.46, 6.13, ... , -2.81]]) vec("split") = array([[3.56, 9.45, -2.43, ... , 5.32]]) feature_vec("banana split") = vec(banana) + (split) = array([[5.12, 6.99, ..., 2.51]]) feature_vec("banana split") = feature_vec("banana split") / length And that is how feature vectors are achieved for documents embeddings, which was one of the suggestions in the question.
