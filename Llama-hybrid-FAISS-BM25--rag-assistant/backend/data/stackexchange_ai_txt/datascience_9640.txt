[site]: datascience
[post_id]: 9640
[parent_id]: 9632
[tags]: 
Here is a trivial example , which captures the essence of genetic algorithms more meaningfully than the polynomial you provided. The polynomial you provided is solvable via stochastic gradient descent , which is a simpler minimimization technique. For this reason, I am instead suggesting this excellent article and example by Will Larson. Quoted from the original article : Defining a Problem to Optimize Now we're going to put together a simple example of using a genetic algorithm in Python. We're going to optimize a very simple problem: trying to create a list of N numbers that equal X when summed together. If we set N = 5 and X = 200, then these would all be appropriate solutions. lst = [40,40,40,40,40] lst = [50,50,50,25,25] lst = [200,0,0,0,0] Take a look at the entire article , but here is the complete code : # Example usage from genetic import * target = 371 p_count = 100 i_length = 6 i_min = 0 i_max = 100 p = population(p_count, i_length, i_min, i_max) fitness_history = [grade(p, target),] for i in xrange(100): p = evolve(p, target) fitness_history.append(grade(p, target)) for datum in fitness_history: print datum """ from random import randint, random from operator import add def individual(length, min, max): 'Create a member of the population.' return [ randint(min,max) for x in xrange(length) ] def population(count, length, min, max): """ Create a number of individuals (i.e. a population). count: the number of individuals in the population length: the number of values per individual min: the minimum possible value in an individual's list of values max: the maximum possible value in an individual's list of values """ return [ individual(length, min, max) for x in xrange(count) ] def fitness(individual, target): """ Determine the fitness of an individual. Higher is better. individual: the individual to evaluate target: the target number individuals are aiming for """ sum = reduce(add, individual, 0) return abs(target-sum) def grade(pop, target): 'Find average fitness for a population.' summed = reduce(add, (fitness(x, target) for x in pop)) return summed / (len(pop) * 1.0) def evolve(pop, target, retain=0.2, random_select=0.05, mutate=0.01): graded = [ (fitness(x, target), x) for x in pop] graded = [ x[1] for x in sorted(graded)] retain_length = int(len(graded)*retain) parents = graded[:retain_length] # randomly add other individuals to # promote genetic diversity for individual in graded[retain_length:]: if random_select > random(): parents.append(individual) # mutate some individuals for individual in parents: if mutate > random(): pos_to_mutate = randint(0, len(individual)-1) # this mutation is not ideal, because it # restricts the range of possible values, # but the function is unaware of the min/max # values used to create the individuals, individual[pos_to_mutate] = randint( min(individual), max(individual)) # crossover parents to create children parents_length = len(parents) desired_length = len(pop) - parents_length children = [] while len(children) I think it could be quite pedagogically useful to also solve your original problem using this algorithm and then also construct a solution using stochastic grid search or stochastic gradient descent and you will gain a deep understanding of the juxtaposition of those three algorithms. Hope this helps!
