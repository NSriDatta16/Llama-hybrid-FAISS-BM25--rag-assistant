[site]: crossvalidated
[post_id]: 139751
[parent_id]: 139042
[tags]: 
I accomplish a type of weighting by doing the following, once all your models are fully trained up and performing well: Run all your models on a large set of unseen testing data Store the f1 scores on the test set for each class, for each model When you predict with the ensemble, each model will give you the most likely class, so weight the confidence or probability by the f1 score for that model on that class. If you're dealing with distance (as in SVM, for example), just normalize the distances to get a general confidence, and then proceed with the per class f1 weighting. You can further tune your ensemble by taking measure of percent correct over some time. Once you have a significantly large, new data set scored, you can plot threshold in steps of 0.1, for instance, against percent correct if using that threshold to score, to get an idea of what threshold will give you, say, 95% correct for class 1, and so on. You can keep updating the test set and f1 scores as new data come in and keep track of drift, rebuilding the models when thresholds or accuracy fall.
