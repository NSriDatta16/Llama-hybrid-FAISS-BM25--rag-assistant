[site]: crossvalidated
[post_id]: 161669
[parent_id]: 161640
[tags]: 
The aspect emphasised is that the relevance of a term or a document does not increase proportionally with term (or document) frequency. Using a sub-linear function therefore helps dampen down this effect. To that extent, the influence of very large or very small values (e.g. very rare words) is also amortised. Finally, as most people intuitively perceive scoring functions to be somewhat additive, using logarithms will make probability of different independent terms from $P(A, B) = P(A) \, P(B)$ to look more like $\log(P(A,B)) = \log(P(A)) + \log(P(B))$ . As the Wikipedia article you link notes the justification of TF-IDF is still not well-established; it is/was a heuristic that we want to make rigorous, not a rigorous concept we want to transfer to the real world. As mentioned by @Anony-Mousse as a very good read on the matter is Robertson's Understanding Inverse Document Frequency: On theoretical arguments for IDF . It gives a broad overview of the whole framework and attempts to ground TF-IDF methodology to the relevance weighting of search terms.
