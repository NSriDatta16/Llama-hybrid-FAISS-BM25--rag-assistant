[site]: datascience
[post_id]: 112117
[parent_id]: 
[tags]: 
Why input normalization leads to worse performance?

I'm building quite simple NN with Dense layers followed by ReLU activations and I noticed something unexpected. Generally, I've been confident that normalizing the input to have mean of 0 and standard deviation of 1 is always a good thing for linear models, including Neural Networks, as it usually stabilise the training. With my example though, it doesn't work at all - training with Keras Normalization layer included makes the validation error worse and network is training longer (it needs far more epochs to arrive remotely close to the best observed performance). I'm quite puzzled and haven't managed to explain the reason for this phenomenon. My network is defined as follows (simplified version): class MyModel(tf.keras.Model): def __init__(self): super().__init__(dynamic=True) self.norm_layer = tf.keras.layers.Normalization(axis=-1) self.layer1 = tf.keras.layers.Dense(16, activation='relu', use_bias=True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)) self.layer2 = tf.keras.layers.Dense(8, activation='relu', use_bias=True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)) self.r1 = tf.keras.layers.Dense(1, activation=tf.exp, use_bias=True, kernel_initializer=tf.keras.initializers.RandomNormal(mean=-0.1, stddev=0.05, seed=None)) def call(self, inputs, training=False): if training: self.norm_layer.adapt(inputs) inputs = self.norm_layer(inputs) x = self.layer1(inputs) x = self.layer2(x) x = self.layer3(x) return x While the distribution of input features looks as follows: What is the possible explanation behind the failure of normalization layer? isn't standardising inputs always recommended preprocessing step?
