[site]: crossvalidated
[post_id]: 515678
[parent_id]: 515659
[tags]: 
TL;DR : what you found, the marginal likelihood, is useful for model comparison. It's the total evidence for this class of model, over all its possible parameter settings, given the data. Just proved this is upper probability equation is correct. It differs a bit from the ordinary Bayesian rule: In fact, your first equation is a bit better! Compared to the commonplace one, it makes assumptions about the model explicit. it introduces model likelihood: (∣) Let's call this the marginal likelihood today. ('Evidence' is also a fine name for it.) How you would interpret model likelihood in comparison to (∣,) which should be data likelihood, but also parametrized by model and parameters. How you would interpret model likelihood in comparison to (∣,) which should be data likelihood, but also parametrized by model and parameters. Well, $p(X \mid \theta, M)$ is (terminologically) the likelihood of the parameters , not of the data. (If you want to mention the associated data, you may say "the likelihood of the parameters given the data".) In other words, when we talk about likelihood, $X$ is fixed, and we're varying $\theta$ . By contrast, $p(X \mid M)$ which we call the marginal likelihood , is the total evidence for this model , across all possible settings of its parameters $\theta$ . I chose to call it the marginal likelihood to remind you that it is $\int_\theta p(X \mid \theta, M) p(\theta \mid M)~ \mathrm{d}\theta$ . The big point is: how well can this class of model explain the data? The marginal likelihood is necessary to normalize your equation, but when we optimize it's common to ignore it. (After all, it's a constant. So we can just optimize $\mathrm{likelihood} \times \mathrm{prior}$ .) It starts to matter when we do model comparison , where the parameter space may be different. Model comparison: an example. The marginal likelihood is useful for model comparison. Imagine a simple coin-flipping problem, where model $M_0$ is that it's biased with parameter $p_0=0.3$ and model $M_1$ is that it's biased with an unknown parameter $p_1$ . For $M_0$ , we only integrate over the single possible value. For $M_1$ , we integrate over all possible values from $0$ to $1$ . $$ \begin{align} p(M_1 \mid X) &= \frac{P(X \mid M_1) P(M_1)}{P(X)} \\ P(X) &= P(X \mid M_1) P(M_1) + P(X \mid M_0) P(M_0) \\ \end{align} $$ With these equations, we can see how much the data favor $M_1$ , which is the evidence for model $M_1$ .
