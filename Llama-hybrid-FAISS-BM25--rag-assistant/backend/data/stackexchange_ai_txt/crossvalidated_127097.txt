[site]: crossvalidated
[post_id]: 127097
[parent_id]: 126258
[tags]: 
I've had success using Latent Dirichlet Allocation (LDA) to find the latent themes or "topics" in textual data. LDA will create $k$ topics out of terms (words) from your corpus of job descriptions. Each job description is given a probability of containing each of the $k$ topics. For example if you asked LDA to classify a corpus into 3 topics, a job description for a graphic designer might have 80% "photoshop graphic illustrator...", 18% "HTML CSS JS...", and 2% "Java Spring object-oriented...". There's plenty to read about the LDA, just search or start with the Quora question . My analysis with LDA was in R but there is of course a Python package although I have never employed it in my own work. You might consider selecting a topic number that corresponds with the number occupations in the SOC. Once you have generated the topics inspect them and see if you can find meaningful links to the SOC and adjust the topic number accordingly until you are satisfied. To make salary estimates for each job description consider weighting each salary using the topic probabilities. For example if a job description had an 80% probability of being a software developer SOC weight the salary by .80 and the remaining topics likewise. If that creates too much noise just set a cutoff (maybe 20%) and remove the remaining topic weights from the salary estimate. For misspellings you can always attack it with a spell checker and see how it compares to the results without the tool. Also make sure to employ standard NLP techniques such as punctuation removal and word stemming prior to running LDA.
