[site]: crossvalidated
[post_id]: 313962
[parent_id]: 46523
[tags]: 
LogisticRegression is suitable for fitting if probabilities or proportions are provided as the targets, not only 0/1 outcomes. import numpy as np import pandas as pd def logistic(x, b, noise=None): L = x.T.dot(b) if noise is not None: L = L+noise return 1/(1+np.exp(-L)) x = np.arange(-10., 10, 0.05) bias = np.ones(len(x)) X = np.vstack([x,bias]) # Add intercept B = [1., 1.] # Sigmoid params for X # True mean p = logistic(X, B) # Noisy mean pnoisy = logistic(X, B, noise=np.random.normal(loc=0., scale=1., size=len(x))) # dichotomize pnoisy -- sample 0/1 with probability pnoisy dichot = np.random.binomial(1., pnoisy) pd.Series(p, index=x).plot(style='-') pd.Series(pnoisy, index=x).plot(style='.') pd.Series(dichot, index=x).plot(style='.') Here we have three potential targets for logistic regression. p which is the true/target proportion/probability, pnoisy which is p with normal noise added in the log odds scale, and dichot , which is pnoisy treated as a parameter to the binomial PDF, and sampled from that. You should test all 3 -- I found some open source LR implementations can't fit p . Depending on your application, you may prefer pnoisy. In practice, you should also consider how the noise is likely to be shaped in you target application and try to emulate that.
