[site]: crossvalidated
[post_id]: 312458
[parent_id]: 312418
[tags]: 
PCA is a common tactic of feature extraction for further analyses. The scores of the first few principal components are then considered as derived variables, just as if you would take logs, ratios or other transformations of the original set of variables. Here are a few typical situations where principal components are used for subsequent analyses: A group of highly correlated covariables in a modelling setting (e.g. number of rooms in a house, living surface, volume in a house price model): These might be troublesome for interpretation of the regression coefficients and their associated inference. Here you could try to compress the three "size" variables (after taking natural logs each) through a PCA and use just the first PC to represent size. Often, a better tactic is to "decorrelate" the covariables by hand, i.e. by using log(room number), log(surface living / room number), log(volume / surface living). Transformations like this are almost always possible and provide much easier interpretation (which is actually the aim of all the fuzz here). Overfitting in a modelling setting, i.e. too many covariables compared to the number of independent observations. E.g. 1000 observations and 100 covariables. One idea is to "compress" the 100 covariables to the first few relevant PCAs and hope that they retain most statistical association with the response variable. This is basically the tactic you have proposed. It is done quite frequently and is called principal component regression . Because everything is linear, you can even map the coefficients of the regression to the original variables (if required). Alternatives are e.g. penalized modelling like ridge- or Lasso-regression. Any other sort of "higher" dimensional analysis. Imagine e.g. you have a large questionnaire with 200 questions along with demographic information like sex, age etc. Instead of running 200 Wilcoxon- or t-tests to test for association between the answers to the questionnaire and sex, quite commonly, only the first few PCs are chosen instead of the 200 questions. Ideally, this increases the power and does not suffer from extreme multiple testing issues. These settings all share the same problems: There is a danger of not looking enough into the raw data, so the chance of not detecting wrong values could be larger than with explicit transformations. PCA is prone to outliers, so log-transforms are often your friend. Unclear how categorical variables should enter PCA. As dummy variables? This is sometimes being considered as bad habit, but what are the alternatives...? More difficult to interpret the results. What does it finally mean if the third PC is significantly associated with sex? Of course you can say the relevant PC is just a weighted sum of the standardized original variables, but it will stay clumsy. So the answer to the first question is clearly "yes". And the second one, maybe not unexpected, depends on the situation.
