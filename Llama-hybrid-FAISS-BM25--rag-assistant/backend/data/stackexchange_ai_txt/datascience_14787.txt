[site]: datascience
[post_id]: 14787
[parent_id]: 14780
[tags]: 
Let me insert 2Â¢... Generally speaking "autoencoding" is a lossy compression technique, although not very useful due to being data-specific (autoencoder trained on cats not very useful for cars ). In practice it is used for: data denoising dimensionality reduction unsupervised pre-training of feature extracting parts of more complex networks and... generating new unseen samples from seen ones (!) The latter is possible with a variety called Variational Autoencoders (VAE), where you impose some constraints on the compressed representation being learned that force it to represent a set of variables that model the probability distribution that represents input data. In other words in VAE each "bottleneck" variable represents something meaningful about the input (think "color of a cat"). Thus changing this representation produces meaningful output on decoder part of AE.
