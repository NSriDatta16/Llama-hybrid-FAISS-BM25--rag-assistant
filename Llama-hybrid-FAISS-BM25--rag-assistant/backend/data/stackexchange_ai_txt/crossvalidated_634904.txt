[site]: crossvalidated
[post_id]: 634904
[parent_id]: 
[tags]: 
Should the loss terms in diffusion model training be weighted by the diffusion schedule?

I'm following Understanding Diffusion Models to write my own diffusion model training code. I'm working based off the loss showed in equation 58 In equation 99, a useful form of the individual terms in the denoising matching summation is shown: When I saw this my knee jerk reaction was to write a function like: def loss(x0, x0hat): return torch.mean((x0 - x0hat) ** 2) and this worked, in that I was able to generate MNIST digits after training my model. But then I realised two things: The denoising matching terms need the coefficient: . I plotted this out over a typical alpha schedule and it looks like a meaningful change (coefficients for lower t are much larger) For t=1 we are actually supposed to look at the reconstruction term, for which the coefficient is 1. So I made the relevant changes. def training_loss(self, x0hat: FloatTensor, x0: FloatTensor, t: LongTensor): """Compute loss following eqns 58 and 99.""" t_view = t.view(-1, *([1] * (x0.ndim - 1))) # This is the loss for each of the elements of the batch. For t=1 the coefficient is 1 as we are considering the # reconstruction term. For t>=2 we use the coefficient in eqn 99. total_loss = torch.sum( torch.where(t_view >= 2, self.kld_coeff[t_view - 1], 1) * (x0hat - x0) ** 2, dim=tuple(range(1, x0.dim())) ) # Now considering eqn 58, we see that each of the loss terms in the batch have equal weighting, so we can safely # just take a mean now. return total_loss.mean() But training the diffusion model is not so nice now, as in the generated MNIST digits don't looks so realistic. I noticed that at least one open source implementation of diffusion models uses the first, simpler variant of the loss that I wrote above. But wouldn't the second variant be more correct?
