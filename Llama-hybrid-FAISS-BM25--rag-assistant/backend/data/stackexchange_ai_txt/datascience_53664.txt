[site]: datascience
[post_id]: 53664
[parent_id]: 
[tags]: 
TensorFlow: how to restore pre-trained meta model and pass it's weights and biases to the optimizer?

I trained a model on a specific dataset and saved it as a meta, I want to restore the model and use its weights and biases on another dataset the code isn't mine but I'm trying to restore the architecture after it was saved to metafile, the TGCN was designed by the lehaifeng to predict speeds on edges of a graph. You can find the source code belongs to lehaifeng and his teammates through the following link: https://github.com/lehaifeng/T-GCN I tried many workarounds and ran into numerous errors I can't post all of them here, is it possible to do transfer learning using the model I post above because I can't use this architecture on a new dataset I have tried using to tf.get_collection(tf.GraphKeys.GLOBAL/TRAINABLE_VARIABLES) to set the global variable to the restored model global variables and then pass it to the optimizer it's telling me there are no variables in the restored graph. one other problem is the model can't iterate over a tensor unless I'm using eager execution and that I have to use tf.map_fn, you can't use eager execution when calling placeholders from the trained model as tensor using graph.get_tensor_by_name() import pickle as pkl import tensorflow as tf import pandas as pd import numpy as np import math import os import numpy.linalg as la from input_data import preprocess_data,load_doha_data,load_los_data from tgcn import tgcnCell ###from gru import GRUCell from visualization import plot_result,plot_error from sklearn.metrics import mean_squared_error,mean_absolute_error import matplotlib.pyplot as plt import time time_start = time.time() ####Settings#### flags = tf.app.flags FLAGS = flags.FLAGS flags.DEFINE_float('learning_rate', 0.003, 'Initial learning rate.') flags.DEFINE_integer('training_epoch', 10, 'Number of epochs to train.') flags.DEFINE_integer('gru_units', 100, 'hidden units of gru.') flags.DEFINE_integer('seq_len',4 , ' time length of inputs.') flags.DEFINE_integer('pre_len', 1, 'time length of prediction.') flags.DEFINE_float('train_rate', 0.8, 'rate of training set.') flags.DEFINE_integer('batch_size', 64, 'batch size.') flags.DEFINE_string('dataset', 'los', 'doha or los.') flags.DEFINE_string('model_name', 'tgcn', 'tgcn') model_name = FLAGS.model_name data_name = FLAGS.dataset train_rate = FLAGS.train_rate seq_len = FLAGS.seq_len output_dim = pre_len = FLAGS.pre_len batch_size = FLAGS.batch_size lr = FLAGS.learning_rate training_epoch = FLAGS.training_epoch gru_units = FLAGS.gru_units ###### load data ###### if data_name == 'los': data, adj = load_los_data('los') time_len = data.shape[0] print(time_len) num_nodes = data.shape[1] print(num_nodes) data1 =np.mat(data,dtype=np.float32) #### normalization max_value = np.max(data1) data1 = data1/max_value trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len) totalbatch = int(trainX.shape[0]/batch_size) training_data_count = len(trainX) def TGCN(_X, _weights, _biases): ### cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes) cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True) _X = tf.unstack(_X, axis=1) outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32) m = [] for i in outputs: o = tf.reshape(i,shape=[-1,num_nodes,gru_units]) o = tf.reshape(o,shape=[-1,gru_units]) m.append(o) last_output = m[-1] output = tf.matmul(last_output, _weights['out2']) + _biases['out2'] output = tf.reshape(output,shape=[-1,num_nodes,pre_len]) output = tf.transpose(output, perm=[0,2,1]) output = tf.reshape(output, shape=[-1,num_nodes]) return output, m, states ###### placeholders ###### inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes],name="inputs1") labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes],name="lables1") # Graph weights weights = { 'out2': tf.Variable(tf.random_normal([gru_units, pre_len], mean=1.0), name='weight_o')} biases = { 'out2': tf.Variable(tf.random_normal([pre_len]),name='bias_o')} if model_name == 'tgcn': pred,ttts,ttto = TGCN(inputs, weights, biases) y_pred = pred ###### optimizer ###### lambda_loss = 0.0010 Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) label = tf.reshape(labels, [-1,num_nodes]) ##loss loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg) ##rmse error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label))) optimizer = tf.train.AdamOptimizer(lr).minimize(loss) ###### Initialize session ###### variables = tf.global_variables() #Create a saver object which will save all the variables saver = tf.train.Saver(tf.global_variables()) init = tf.global_variables_initializer() sess = tf.Session() gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) sess.run(init) out = 'out2/%s'%(model_name) #out = 'out/%s_%s'%(model_name,'perturbation') path1 = '%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r'%(model_name,data_name,lr,batch_size,gru_units,seq_len,pre_len,training_epoch) path = os.path.join(out,path1) if not os.path.exists(path): os.makedirs(path) ###### evaluation ###### def evaluation(a,b): rmse = math.sqrt(mean_squared_error(a,b)) mae = mean_absolute_error(a, b) F_norm = la.norm(a-b,'fro')/la.norm(a,'fro') r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum() var = 1-(np.var(a-b))/np.var(a) return rmse, mae, 1-F_norm, r2, var x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], [] test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[] for epoch in range(training_epoch): for m in range(totalbatch): mini_batch = trainX[m * batch_size : (m+1) * batch_size] mini_label = trainY[m * batch_size : (m+1) * batch_size] _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred], feed_dict = {inputs:mini_batch, labels:mini_label}) batch_loss.append(loss1) batch_rmse.append(rmse1 * max_value) # Test completely at every epoch loss2, rmse2, test_output = sess.run([loss, error, y_pred], feed_dict = {inputs:testX, labels:testY}) test_label = np.reshape(testY,[-1,num_nodes]) rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output) test_label1 = test_label * max_value test_output1 = test_output * max_value test_loss.append(loss2) test_rmse.append(rmse * max_value) test_mae.append(mae * max_value) test_acc.append(acc) test_r2.append(r2_score) test_var.append(var_score) test_pred.append(test_output1) print('Iter:{}'.format(epoch), 'train_rmse:{:.4}'.format(batch_rmse[-1]), 'test_loss:{:.4}'.format(loss2), 'test_rmse:{:.4}'.format(rmse), 'test_acc:{:.4}'.format(acc)) saver.save(sess, './TGCNpretrain.model') print (tf.contrib.framework.list_variables('./TGCNpretrain.model')) time_end = time.time() print(time_end-time_start,'s') So to sum my problem how do I restore and load this model on another data set, you can download all other code callback functions and data sets from the source code link make sure you change paths so that the main.py can import its input matrices. thanks in advance p.s I'm an extreme beginner to Tesnorflow in particular and a beginner in Python in general. so pardon my inexperience in expressing the problem.
