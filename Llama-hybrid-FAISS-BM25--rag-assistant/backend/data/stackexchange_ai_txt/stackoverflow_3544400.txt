[site]: stackoverflow
[post_id]: 3544400
[parent_id]: 3544331
[tags]: 
With 60,000 or 80,000 unique lines you could just create a dictionary for each unique line, mapping it to a number. mydict["hello world"] => 1 , etc. If your average line is around 40-80 characters this will be in the neighborhood of 10 MB of memory. Then read each file, converting it to an array of numbers via the dictionary. Those will fit easily in memory (2 files of 8 bytes * 3GB / 60k lines is less than 1 MB of memory). Then diff the lists. You could invert the dictionary and use it to print out the text of the lines that differ. EDIT: In response to your comment, here's a sample script that assigns numbers to unique lines as it reads from a file. #!/usr/bin/python class Reader: def __init__(self, file): self.count = 0 self.dict = {} self.file = file def readline(self): line = self.file.readline() if not line: return None if self.dict.has_key(line): return self.dict[line] else: self.count = self.count + 1 self.dict[line] = self.count return self.count if __name__ == '__main__': print "Type Ctrl-D to quit." import sys r = Reader(sys.stdin) result = 'ignore' while result: result = r.readline() print result
