[site]: crossvalidated
[post_id]: 473079
[parent_id]: 472974
[tags]: 
I use the following binary variables: Poop is green: G Am sick: D Ate leafy greens: L First, let's see how you can reach $P(D=1|G=1) = 0.8$ . While you "knew" that you had eaten leafy greens and that it could cause green poop, when you thought about it first, you only considered a disease as a potential cause. That is, you only had in mind the probabilistic graph D -> G in mind, meaning $P(D,G) = P(D)P(G|D)$ . For example, $P(D=1) = 0.1$ (you felt fine other than the poop), and $P(G=1|D=1)$ is also low (you know very little diseases that cause green poop), therefore $P(D=1,G=1)$ is pretty low. So how come you have $P(D=1|G=1)=0.8$ ? The alternative $P(D=0|G=1)$ is even lower: yes, $P(D=0)=0.9$ is high, yet having green poop while not being sick is extremely extremely unlikely (because most days, I am fine, yet my poop is not green)! You can check that by fixing actual probabilities. Now when you learn or are reminded about leafy greens on the internet, you update your graph and add a potential cause "leafy greens". Formally, $P(D,G,L) = P(L) P(D) P(G|D,L)$ . Now, because $P(L)=1$ (I know for sure I ate greens yesterday) and $P(G=1|D=d,L=1)$ for any $d$ is high: that's what I was "reminded" about on the internet: sick or not, leafy greens cause green poop. By Bayes rules, $P(D|G,L) \propto P(D) P(L) P(G|D,L)$ and by fixing concrete probabilities you will find a low probability of disease thanks to the high $P(G=1|D=d,L=1)$ . That's an instance of explaining away: in the V-shaped graph, when you fix the value of the effect (G), the two causes are now dependent (D and L are dependent given G). The observation that one of the cause is present will decrease the probability of the other (in our case, drastically) and vice versa: if one cause is not present, the probability of the other cause will go up (in our case, you didn't eat leafy greens so you'd still think you are sick with high probability). I tried to find a good reference for explaining away but did not. Pearl's automobile example seems to be frequently given, for example here . Relating this to Ben's answer Yes, I did change the model by adding an edge in the graph, and it is not a fully "Bayesian" formalisation of the problem. I am reasoning like a scientist who incrementally builds a Bayesian model. Your want to model your own thought process: you know that leafy greens are a relevant cause that you used to ignore, and therefore you want to put the variable I in the graph. Thanks to Ben's answer, you realize that the probabilistic graph of causes can be encoded in a very flexible way, where every possible cause can have no to a huge influence on the inference you are trying to draw, via these "gating" variables like I. I think that you were looking for Ben's answer, actually. However, I want to point out that even though Ben's fully Bayesian model might (might only, see next paragraph) be a good (although HUGE) model for "thought processes", it does not reflect scientific elaboration of models. Imagine that I is binary, 1 if L causes G and 0 otherwise. A Bayesian scientist needs to put a prior over I, and in doing so, should think about whether L causes G. But as you said, you did not learn that $I=1$ on the internet; you were merely reminded about it. So if you had thought about it, you would have put a very probable I as a prior. In that case, you see that there is no updating going on and you just recover the analysis I provided with the second model. On the contrary, if you did not think about the cause, you would have built the first model I presented. In other words, if the Bayesian scientist is not fully satisfied with his model, he needs to build another one and his approach is not "fully Bayesian" (in the extreme, formal and dogmatic sense of the term). Most importantly, I am still puzzled by Ben's answer, though, because he did not specify the prior over I. If we are modelling thought processes, we could see beliefs of an individual as continually updated throughout his life. For Ben's answer to be fully complete and convincing, we need the "prior" probability (before seeing the information on the internet) $P(I=1)$ to be low. Why would it be the case? I don't think the individual has been exposed to evidence for that in his life. There is something wrong. Therefore, I am more inclined to imagine that we do approximate Bayesian inference in our heads with very partial graphs that are "instantiated" by extracting pieces of a "full knowledge graph" in an imperfect way. I am very curious to hear Ben's opinion on that. There are probably tons of resources discussing the problem (maybe in the "objective vs subjective" or "Bayesian vs frequentist" debates?), but I'm not an expert.
