[site]: datascience
[post_id]: 45224
[parent_id]: 
[tags]: 
Keras importing two images of same object for ConvLSTM

I'm trying to use Keras's recurrent models (ConvLSTM2D here) for my neural network. My objective is to takes two images of the same object at different angles, and based on the features from the two images, try to determine what sort of object it is. The images are produced and stored in separate folders, but the paths are similar such that: -dir1 -> class1 -> image001.jpg -> class2 -> image101.jpg -dir2 -> class1 -> image001.jpg -> class2 -> image101.jpg image001.jpg from dir1 and dir2 are of the same object and but are various angles. My Problem : ConvLSTM2D takes images with an extra dimension for time. So my images used to be (128,128,1), now I will use np.stack to convert that to (2,128,128,1). However, I do not know how to: Match the images while generating Stack only the images and not the labels My attempts I've attempted to resolve the problem using the solution described here . To my understanding, keeping the seed the same will make sure the two images are of the same object (?). However, the problem arises when the model attempts to use the images. Keras has this output message: ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s) , but instead got the following list of 2 arrays: [array([[[[0.07843138], [0.02745098], [0.07450981], ..., [0.02745098], [0.03137255], [0.0509804 ]], [[0.05490196], [0.10980393],... This made me think that I needed to stack the two images using numpy.stack, which is what I attempted next. However, this resulted in the following error: ValueError: Error when checking input: expected conv_lst_m2d_1_input to have shape (2, 128, 128, 1) but got array with shape (100, 128, 128, 1) I can understand this error, as I have stated my input to have the dimensions of (None, no_of_images,width,height,channels). Since it's a bimodal image, 2 is the number of images. The actual input the model got is (100,128,128,1), where the first element is the batch size. So I'm thinking I've not stacked the two images correctly into a time axis . This leaves me confused and lost as to how to solve this. I've defined my model well such that the input shape is (samples=None,2,128,128,1) but I do not know how to get my two images into the same format. My code The model model = Sequential() no_of_img = 2 #Adding additional convolution + maxpool layers 15/1/19 model.add(ConvLSTM2D(32, (5,5), batch_input_shape=(batch_size,no_of_img,img_width,img_height,1),return_sequences=True)) model.add(Activation('relu')) model.add(Dropout(0.4)) model.add(TimeDistributed(MaxPooling2D((2,2)))) model.add(ConvLSTM2D(64, (3,3),return_sequences=True)) model.add(Activation('relu')) model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2)))) model.add(Dropout(0.2)) model.add(ConvLSTM2D(128, (3,3),return_sequences=True)) model.add(Activation('relu')) model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2)))) model.add(Dropout(0.2)) model.add(ConvLSTM2D(256, (3,3),return_sequences=True)) model.add(Activation('relu')) model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2)))) model.add(Dropout(0.2)) model.add(Flatten()) #Possible dense layer with our 128x128 number of pixels is too much, too high. We should add a few convolutional and maxpool layers beforehand. model.add(Dense(128, #dimensionality of output space #input_shape=(128,128,1), #Commented out as only the first layer needs input shape. )) model.add(Activation('relu')) #model.add(Dropout(0.2)) #model.add(Dense(num_classes, activation='softmax')) model.add(Dense(num_classes,activation='softmax')) model.compile(loss='categorical_crossentropy',optimizer='RMSProp',metrics=['accuracy']) Current attempt at data prep train_datagen = ImageDataGenerator( rescale=1./255, #Normalized inputs from 0-255 to 0-1 horizontal_flip=True, vertical_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) def generate_generator_multiple(generator,dir1, dir2, batch_size, img_width,img_height,subset): genX1 = generator.flow_from_directory(dir1, color_mode='grayscale', target_size= (img_width,img_height), batch_size=batch_size, class_mode='categorical', shuffle=False, subset=subset, seed=1) #Same seed for consistency. genX2 = generator.flow_from_directory(dir2, color_mode='grayscale', target_size= (img_width,img_height), batch_size=batch_size, class_mode='categorical', shuffle=False, subset=subset, seed=1) while True: X1i = genX1.next() X2i = genX2.next() yield numpy.stack((X1i[0],X2i[0])),X1i[1] #Yields both images and their mutual label train_generator = generate_generator_multiple(generator=train_datagen, dir1=train_data_dirA, dir2=train_data_dirB, batch_size=batch_size, img_width=img_width, img_height=img_height, subset='training') validation_generator =generate_generator_multiple(generator=test_datagen, dir1=train_data_dirA, dir2=train_data_dirB, batch_size=batch_size, img_width=img_width, img_height=img_height, subset='validation')
