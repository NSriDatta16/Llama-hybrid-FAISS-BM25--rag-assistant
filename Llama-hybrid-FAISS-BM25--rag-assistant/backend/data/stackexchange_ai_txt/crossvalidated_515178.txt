[site]: crossvalidated
[post_id]: 515178
[parent_id]: 514003
[tags]: 
"My interpretation was that $q_{\phi}()$ and $q_{\phi}\left(z | x \right)$ are a prior and posterior on $\mathcal{Q}$ " is not correct at all . $\mathcal{Q}$ is the variational family defining the distribution space. $q_{\phi}()$ and $q_{\phi}\left(z | x \right)$ live in $\mathcal{Q}$ and used to approximate the prior/posterior defined for a data distribution. Both notations are used actually. $q_{\phi}()$ actually stems from the ELBO derivation(You can check these lecture notes from CMU), i.e. the KL divergence definition. For VAE's, we specifically denote $q_{\phi}\left(z | x \right)$ because the encoder tries to learn an amortized posterior distribution , which instead of optimizing a set of free parameters unlike in the lecture notes that I've shared sbove, we can propose parameterized function(like a neural network) that maps from observation space to the parameters of the approximate posterior distribution. In fact, there are advanced VI techniques that allow implicit priors for VAE's like Semi-Implicit Variational Inference (SIVI) . TL;DR, try to stick with $q_{\phi}\left(z | x \right)$ for VAE posteriors and always make sure to understand which distribution you are trying to approximate. Hope this helps.
