[site]: crossvalidated
[post_id]: 589595
[parent_id]: 589593
[tags]: 
I adopt the authors notation and use $\tilde A$ for the normalized adjacency matrix. The largest eigenvalue $\lambda_1$ of the normalized adjacency matrix $\tilde A$ is $\lambda_1 \le 1$ . This normalization is done so to prevent exploding values from repeated multiplication over multiple layers. This is what the authors mean when discussing equation 7: Note that $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ now has eigenvalues in the range $[0, 2]$ . Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following \textit{renormalization trick}: $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\rightarrow \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ , with $\tilde{A} = A + I_N$ and $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$ . The normalized Laplacian is formed from the normalized adjacency matrix: $\hat L = I - \hat A$ . $\hat L$ is positive semidefinite. We can show that the largest eigenvalue is bounded by 1 by using the definition of the Laplacian and the Rayleigh quotient. $$ x^T (I - \tilde A)x \ge 0 \implies 1 \ge \frac{x^T \tilde A x}{x^T x} $$ This works because $A$ (and therefore $\tilde A$ ) is symmetric, which is one of the assumptions stated in the paper.
