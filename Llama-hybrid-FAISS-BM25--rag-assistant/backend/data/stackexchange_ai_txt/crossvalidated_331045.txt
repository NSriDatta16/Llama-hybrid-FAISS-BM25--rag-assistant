[site]: crossvalidated
[post_id]: 331045
[parent_id]: 
[tags]: 
Why is L-BFGS optimization faster when binary features have been standardized?

I have a question related to standardization of binary features within a regularized logistic regression. Suppose that you have a model where all the features are binary (the result of applying the hashing trick ) so, it's not necessary to apply it. The problem comes when I try to train the model using LBFGS, why does the standardized system converge faster? (For example, the standardized version takes 9 minutes and 37 iterations while non-standardized takes 7 hours and 4992 iterations.) What is the intuition here? The optimization function is similar in both cases, isn't it? I don't know if it's related to LBFGS or it occurs with all the optimizers, but I don't get why standardization makes it faster. In addition, if I apply standardization to my training database, should I apply it in prediction time? What if I can't apply this standardization in prediction time for technical reasons (or design restrictions)?
