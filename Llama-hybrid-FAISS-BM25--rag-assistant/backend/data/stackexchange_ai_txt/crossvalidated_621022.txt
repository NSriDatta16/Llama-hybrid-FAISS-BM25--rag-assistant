[site]: crossvalidated
[post_id]: 621022
[parent_id]: 
[tags]: 
Quirks of my first Logistic Regression model; Boolean warnings, all-or-nothing predictions, ill-defined F Statistic

The Setup I have a dataframe df3 which contains film information ( Budget, Year, Runtime, IMDB (score) ) and a personal rating (integer between 0 and 10 inclusive). I'm trying to teach myself data science and use it for various models. This is my attempt at using it for a simple logisitic regression model. For my first step I create a new column called Cinema which assigns 0s to every entry with a rating below a 6 and 1 to those above it. df3["Cinema"] = np.where(df3["Rating"] > 6, 1, 0) Next I split my data into test/train sets. X_train, X_test, y_train, y_test = train_test_split(dtX, df3["Cinema"], test_size=0.25, random_state=16) dtX is a list of column headers I borrowed from the attempt at making a decision tree. features = ['Budget', 'Year', 'Runtime', 'IMDB'] dtX = df3[features] Then I run my model on the training set and predict Cinema values. logr = linear_model.LogisticRegression(random_state=16) logr.fit(X_train, y_train) y_pred = logr.predict(X_test) Finally I create a confusion matrix and the accuracy report. confusion_matrix = metrics.confusion_matrix(y_test, y_pred) cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True]) cm_display.plot() plt.show() print(classification_report(y_test, y_pred)) The problems During my first few runs, I would sometimes get the warning: Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all() I ran the line bool(y_train) to see what happened and got the warning, but after that it never reappeared. I suspected that how I created the Cinema column was throwing it, but why would it stop being a problem? The next, and bigger, problem is that my model only predicts False values when the threshold value for Cinema is 6, and only predicts positive values when it's 5. With 6 precision recall f1-score support 0 0.58 1.00 0.74 120 1 0.00 0.00 0.00 86 accuracy 0.58 206 vs with 5 precision recall f1-score support 0 0.00 0.00 0.00 63 1 0.69 1.00 0.82 143 accuracy 0.69 206 The distribution of 0s and 1s in the training and testing sets are reasonably balanced, with the worst iteration being: 1 143, 0 63 Running the classification report also gives this warning, which I'm guessing is a product of the one sided classification probabilities. UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) Any ideas on why the model is so sensitive to the threshold value? I've also noticed that reducing the number of columns in Features doesn't seem to influence the T/F balance at all, it's solely determined by the threshold. Edit I toyed around with the model some more and I think I discovered the reason for the poor model performance; my data is rubbish. I tried to run a decision tree and got an accuracy of 0 on both the testing and training models. I then tried dropping some of my predicting variables and I was able to get a believable confusion matrix when only the IMDB score was included. Since I'd expect this to be more correlated with the personal Rating than any other metric, I think this means that the other variables simply have no predicting power. I'd still like to know though why this manifested as extreme probabilities depending on the threshold though!
