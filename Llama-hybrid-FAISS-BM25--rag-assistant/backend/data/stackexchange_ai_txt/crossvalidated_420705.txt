[site]: crossvalidated
[post_id]: 420705
[parent_id]: 
[tags]: 
Mechanics for combining likelihood and prior in non-trivial case

Bayes rule is simple enough on its face: $$ pr(B|A) = \frac{pr(A|B)pr(B)}{pr(A)} $$ If these things are known scalar probabilities, the answer is simple to compute. But I'm failing to understand what happens when I'm working with actual probability density functions. For example, what if my prior ( $pr(B)$ ) is $$ \mathcal{N}\left(\left[\begin{array}{c}{ 1\\ 1 }\end{array}\right], \left[\begin{array}{cc} 1 & .5 \\ .5 & 1 \end{array}\right] \right)$$ Then I collect some new data that looks like this: library(MASS) set.seed(8675309) N The likelihood is also normal, with the mean being the coef vector > coef(m) X1 X2 0.4975087 2.5471237 and the variance being > vcov(m) *sigma(m) X1 X2 X1 16.4070280 -0.5447927 X2 -0.5447927 1.4990280 How is one to plug this into Bayes rule? This is what I've been struggling to understand. Do I simply sample from each of these bivariate normals, multiplying and dividing? Do I have to worry about the fact that thir elements are correlated? In other words, am I just doing a hadamard product for multiplying the prior by the likelihood? And then the same thing in inverse to scale? A simple simulation makes it seem like that's not the case; these results are weird: AB And where does MCMC come in? Is that just a way to figure out where in the distributions I should be sampling from? Would appreciate an answer that could help me fill in the gaps! EDIT I think that what I needed was for someone to tell me "do it over a grid" or "do it over the domain of your data." And "multiply the densities, point by point, not the values" Here's a univariate example: G But I must be doing something wrong here: why is the posterior pointier than the prior and the likelihood in this case? Or is it that the normalization something more than a factor to scale the PDF to sum to 1? But I must be doing something wrong here: why is the posterior pointier than the prior and the likelihood in this case? Or is it that the normalization something more than a factor to scale the PDF to sum to 1?
