[site]: crossvalidated
[post_id]: 498293
[parent_id]: 497964
[tags]: 
Books have been written to answer your questions, as your questions cover a wide statistical ground. Your last question is the easiest one to answer. Basing a sample size on achieving a given precision (margin of error or half width of a confidence interval, etc.) is extremely reasonable and is often recommended over using traditional frequentist statistics power calculations. This also relates to one of your earlier questions. You immediately conceptualized the analysis in terms of a hypothesis test, but that is not the only way to go. In frequentist statistics you could stop with a confidence interval and with Bayesian statistics a posterior distribution. Next comes the subtle part related to your first questions. You envision the data as arriving "all at once" and you have taken the typical approach of doing a sample size calculation to find out how long to wait before analyzing the data (until you expect the power to cross an arbitrary threshold). Sample size calculations are needed in some cases if you have a fixed budget, and they are needed when doing sequential frequentist analysis because of it's need to "spend $\alpha$ " which is the type I assertion probability. If using Bayesian or likelihoodist inference, the sample size is not an intrinsic part of the design and you can experiment until you've learned enough and analyze the data as frequently as desired. Sequential learning without a firm sample size has many advantages. One of the biggest advantages is that you may be at what you had hoped was the final sample size but you see either more variability in $Y$ than you had planned for, or you see an equivocal result about the treatment effect. You decide it is worth randomizing a few more subjects to see if you can obtain a more definitive result. Contrast that with the rigid design you (and so many others) use in which a very common outcome is something like p=0.1 at the target sample size and a large margin of error, with the conclusion being "we don't know anything now that we didn't know before the experiment was done", i.e., we just know we spent the time and money. There are at least three more fundamental points that arise from your questions. First, you don't use effects observed in previous studies in power calculations. Power is computed based on an effect you don't want to miss, not on an effect that someone else observed. Second, in your "imagine the drug reduced by 0.4" question, you have to note that the 0.4 is a point estimate with a lot of uncertainty (non-zero margin of error). You can say that the drug on net worked better for subjects in your sample but you can't say the drug worked better in general . Finally you ask about switching to a one-sided test if your (noisy) point estimate is in the favorable direction. It is better to think of that pre-data when formulating the design. But for $\alpha=0.05$ you can think of this as validly allowing for two one-sided $\alpha=0.025$ tests. Again note that an uncertainty interval rather than hypothesis testing would be helpful. Also it is very important to note that you are taking for granted that the traditional frequentist statistical paradigm is the one to use. If you were using Bayesian posterior probability distribution (one that can be updated each time a new observation is collected), most of the probabilities you compute are already directional. For example you can compute the probability the drug is working in the right direction or the probability that the drug is working better than a 0.2 effect. With Bayes there is no multiplicity correction because you might also be asking whether there is evidence that the drug works in the wrong direction.
