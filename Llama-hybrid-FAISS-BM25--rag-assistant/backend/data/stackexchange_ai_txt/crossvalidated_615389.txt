[site]: crossvalidated
[post_id]: 615389
[parent_id]: 615360
[tags]: 
There are many possible solutions to this issue. The use of GANs, as always, comes with the caveat that the model is difficult to train. In my application, each of the two classes has many labels. This was a deterrent to the use of discriminators. I considered the use of the loss identified in Supervised Contrastive Learning ( https://arxiv.org/abs/2004.11362 ). The issue here is that we need many labels of each type of class to be orthogonal to each other. The adoption of this loss function was a part of the final solution but proved to be insufficient in of itself. I used the results in https://jmlr.org/papers/volume16/qiu15a/qiu15a.pdf . The addition to the loss function uses the dot product (similar to Supervised Constrastive Learning) but with a min and no logit-like formulation. The combination of 2 and 3 proved sufficient in my application. They did lead to a need to use higher dimensionality subspaces, likely because the subspaces are forced to be orthogonal, but was an easy compromise to make in my data.
