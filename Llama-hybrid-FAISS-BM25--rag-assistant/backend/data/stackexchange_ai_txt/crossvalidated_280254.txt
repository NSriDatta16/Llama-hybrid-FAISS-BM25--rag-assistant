[site]: crossvalidated
[post_id]: 280254
[parent_id]: 
[tags]: 
Vanishing gradient and learning rate (neural networks)

I can imagine that by setting a learning rate per layer (in the gradient descent update rule) it would be possible to manage the vanishing gradient problem better than when using a single learning rate. Are there any specific techniques dealing with this? I've had a look on google scholar but couldn't find any particular techniques solely designed for this purpose.
