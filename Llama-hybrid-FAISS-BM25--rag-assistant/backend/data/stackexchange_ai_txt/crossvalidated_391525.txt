[site]: crossvalidated
[post_id]: 391525
[parent_id]: 391504
[tags]: 
The short answer is no , bagging - as a process - is not directly tied to Random Forest's way of splitting a node. The purpose of bagging is to reduce overall model variance, whereas Random Forest's additional random feature selection is meant to produce uncorrelated predictions. Bootstrap aggregation (bagging) can be described as random subsampling with replacement. Every subsample is used to train a weak learner and, in a separate step, all weak learners are aggregated to produce a strong learner; i.e. one that produces predictions with reduced variance. Bagging is a generic concept in ensemble learning and does not specifically involve Random Forest's feature selection. Without random feature selection, instead of Random Forest you will get Bagged Decision Trees . Strictly speaking, bagging is not really tied to decision trees at all. Bagging generally works better with an ensemble of weak learners with high variance (prone to overfitting), and simple CART models happen to fall under that category. However, in theory, any weak learner that uses greedy stochastic search or simply produces high variance predictions, should be a good candidate for bagging.
