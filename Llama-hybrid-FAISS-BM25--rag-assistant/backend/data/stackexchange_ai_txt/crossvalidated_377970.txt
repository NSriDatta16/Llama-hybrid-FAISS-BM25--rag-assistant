[site]: crossvalidated
[post_id]: 377970
[parent_id]: 
[tags]: 
Regarding the quantics tensor train (QTT) format

I originally posted this question in Data Science Stack Exchange, however, I think this forum may be better for this question. I believe I have a fair understanding of the tensor train (TT) format, and am able to use the Python implementation of the tensor train toolbox to perform TT decompositions. However, my mathematical background isn’t strong enough to adequately follow the QTT paper by Khoromskij . While my tests with the TT library work as expected, when I attempt to perform QTT decompositions, it provides a different output then I was hoping for. When I convert data into the TT format via the function tt.vector I get compression as expected. Then, as there appears to be no built in function in the TT toolbox to convert an existing tensor into the QTT format, I have attempted to do this manually, though I may have done so incorrectly. The conversion I performed involved a $2^n$ × $2^n$ matrix and reshaping it into a 2 1 ×2 2 ×…×2 2n-1 ×2 2n tensor, and calling the tt.vector function on the reshaped object. While this appears to work just fine at first, the TT decomposition is consistently outperforming the QTT decomposition in terms of data storage complexity. Yet, according to the following chart from this paper, I should expect a logarithmic improvement in compression with QTT over TT. One area I have identified as a potential cause for concern is the nature of the data which I wish to represent in the QTT format. Are there any special requirements on the nature of the data which I wish to represent? The reason for this concern arises from the following passages of this paper. And also: Two important notes: I have taken into consideration the error parameter epsilon as well as the TT rounding procedure when calculating the TT and QTT representations of tensors. I have tested over many different error parameters and have used the TT rounding procedure to reduce the TT/QTT ranks as much as possible, however the TT still outperforms the QTT in terms of data storage complexity. I wish to compress data that already exists. In other words, I don’t want to represent a random tensor, or a tensor of all 1s in the QTT format; I want to take a tensor that I already have, and represent it using the QTT format. Question: What limitations, if any, does QTT have for compression? Does the data need a strong repetitious quality? If so, I’m hoping for a pythonic/mathematical “high-level” description, more geared towards a computer scientist/engineer familiar with TT but not QTT.
