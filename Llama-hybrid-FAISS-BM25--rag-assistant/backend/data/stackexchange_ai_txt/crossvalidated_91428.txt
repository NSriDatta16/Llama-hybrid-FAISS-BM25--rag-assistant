[site]: crossvalidated
[post_id]: 91428
[parent_id]: 91338
[tags]: 
$P(D)$ is necessary if you characterise the full posterior. For example, if you want to do a maximum-a-posteriori (MAP) estimate of your parameters, then you do not need to worry about the normaliser as you are only trying to maximise the posterior probability of the parameters given the observation i.e. $$ P(\theta|D) \propto P(D|\theta) P(\theta) $$ So, you do not need to worry about the denominator ($P(D)$) as it does not affect finding the $\theta$ that maximizes the posterior. However, MAP gives you a point estimate and you ignore the rich information that posterior distribution may convey. However, if you want to quantify the uncertainty, do model comparison (see Bayes factor) and probably other things, then you need to also compute or approximate $P(D)$. I also suggest reading Chris Bishop's book. He explains a lot of these things in an amazing way! The book is called "Pattern Recognition and Machine Learning" by Christopher Bishop. He also has some amazing lectures on probabilistic graphical models and Bayesian inferencing that can be found in the following links: https://www.youtube.com/watch?v=ju1Grt2hdko https://www.youtube.com/watch?v=c0AWH5UFyOk https://www.youtube.com/watch?v=QJSEQeH40hM
