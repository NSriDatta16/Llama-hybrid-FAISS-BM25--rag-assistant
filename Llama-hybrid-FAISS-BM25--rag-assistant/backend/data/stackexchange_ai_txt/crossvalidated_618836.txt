[site]: crossvalidated
[post_id]: 618836
[parent_id]: 618833
[tags]: 
As you can learn from their documentation, scikit-learn uses libsvm (of liblinear in linear SVM) for sitting the models. Since scikit-learn is open-source, you can check the source code on GitHub . As you can see, this is a dense C++ code, so it would be unlikely helpful if you are just starting to learn about it. I search for SVM tutorials, I found some. But, in all of them, training is done for example for 1000 iteration. The number of iterations is usually a poor stopping criterion as you never know how many iterations you need for a given problem. If you read section 4.1.2 of the libsvm paper by Chih-Chung Chang and Chih-Jen Lin (2022) , they describe there the stopping criterion that they used in detail. But in sklearn, there is no limit for it. Still, SVM in sklearn perform much faster and sooner its over. How is it possible? Because they use a high-quality, highly optimized implementation of the algorithms that are written in C++. The toy implementations from tutorials nearly never can reach a similar performance to such libraries. The point of tutorials is to show code that is easy to understand, while the point of the library is that it is efficient, those two are not necessarily the same. Can some one give me a link to exact code that sklearn used for it, to compare and see why it is faster? I already linked it above, but as I said, I doubt it would be useful at this stage. The production-level code would need to handle many edge cases, be fast, memory-efficient, etc so it would be much more complicated than the toy implementation. Reading it may be a good learning experience though. Keep in mind that those implementations would usually be in C++ or Fortran, so you would need to know those languages at least to some degree.
