[site]: datascience
[post_id]: 85923
[parent_id]: 
[tags]: 
creating cv folds from a dataset that has to be grouped on two featues and also time series data

Edit Dec 3 2020: updated the code Edit: Following are the two changes 0a. uses sets to check there are duplicate random numbers generated 0b. We are now going from minority class to majority class to maintain class ditribution I am working on the Kaggle home credit dataset. I am trying to work with installment_payment.csv. I joined this with application_train.csv to get the TARGET variable into installment_payment.csv. Now, my data looks like this SK_ID_CURR SK_ID_PREV DAYS_INSTALMENT DAYS_ENTRY_PAYMENT TARGET AMT_ANNUITY DAYS_DECISION ... 1 1 .... .... 0 0.1 1 2 .... .... 0 0.2 1 3 .... .... 0 0.1 2 4 .... .... 1 0.3 2 5 .... .... 1 0.4 3 6 .... .... 0 0.2 4 7 .... .... 1 0.1 4 8 .... .... 1 0.3 4 9 .... .... 1 0.5 To create a cross validation dataset that has not any data leak I need to create a dataset that does the following Combination of SK_ID_CURR and SK_ID_PREV should exist only in one of the train folds and certainly not exist in both train and test folds. Distribution of class labels should be in the same in all the folds DAYS_INSTALLMENT and DAYS_ENTRY_PAYMENT are to be used as time series markers def strtfied_grp_kfld(self,fnme,nfolds): """ sklearn does not have GroupkFoldStratified refer github.com/scikit-learn/scikit-learn/issues/13621 Even in that the groups cannot be of multiple levels But in this case study I needed grouping on two columns hence the birth of this function 1. I have tried to ensure that the a group occurs only in one of the folds either train\test 2. Due to class imbalance and to keep the same distribution of minor/major classes. I compute class weights of major and minor classes. For every majority class I create the corresponding number of minority samples. I hope this will maintain the same distribution of major and minor classses across the folds. 3. the main disadvantage is the total numner of rows will be less than x_train.shape[0] / number of folds """ #total number of rows in the dataset nrows = self.X_train.shape[0] print('no of rows',nrows) #number of rows in each fold nrowsfld = int(nrows/nfolds) #lst_cols = ['SK_ID_CURR','TARGET'] #attach TARGET to taining data #self.X_train['TARGET'] = self.y_train pmyname_cv = fnme[:-16] #print(df_tgt_1) #compute the class weights since this is an im balanced dataset print('Computing Class weights...') clas_wts_arr = class_weight.compute_class_weight('balanced',np.unique(self.y_train),self.y_train) wt_clas_maj = clas_wts_arr[0] wt_clas_min = int(clas_wts_arr[1]) print('wt clas maj',wt_clas_maj,wt_clas_min) #set upper bound for random number generation rndm_endrange_tgt0 = self.df_tgt_0.shape[0]-1 rndm_endrange_tgt1 = self.df_tgt_1.shape[0]-1 no_rndm_gen_tgt1 = int(rndm_endrange_tgt1/ nfolds) print('tgt0 tgt1',rndm_endrange_tgt0,rndm_endrange_tgt1) #housekeeping initializations fld = 0 itr_cntot_trn = 0 itr_rndm_nos_trn = None tot_row_cnt = 0 lst_trn_idx_0 = [] lst_trn_idx_1 = [] all_trn_idx = [] trn_idx = [] tst_idx = [] set_trn_idx_0 = set() set_trn_idx_1 = set() #set seed for the random number generation np.random.seed(84) print('n rows per fold',nrowsfld ) #main loop to control number of folds while (fld The problem I am facing is that I am getting an ROC_AUC_SCORE of 1 for my test dataset. I think this is because the time related rows are mixed between the folds. My question How should I include the time related features into the above code so that future rows are not present in the test dataset Should the same criteria be followed for rows between the folds Can logistic regression be used as a machine learning model for this type of data thanks for your replies
