[site]: crossvalidated
[post_id]: 1911
[parent_id]: 1883
[tags]: 
In 1960 most people doing statistics were calculating with a four-function manual calculator or a slide rule or by hand; mainframe computers were just beginning to run some programs in Algol and Fortran; graphical output devices were rare and crude. Because of these limitations, Bayesian analysis was considered formidably difficult due to the calculations required. Databases were managed on punch cards and computer tape drives limited to a few megabytes. Statistical education focused initially on learning formulas for t-testing and ANOVA. Statistical practice usually did not go beyond such routine hypothesis testing (although some brilliant minds had just begun to exploit computers for deeper analysis, as exemplified by Mosteller & Wallace's book on the Federalist papers, for instance). I recounted this well-known history as a reminder that all of statistics has undergone a revolution due to the rise and spread of computing power during this last half century, a revolution that has made possible almost every other innovation in statistics during that time (with the notable exception of Tukey's pencil-and-paper EDA methods, as Thylacoleo has already observed).
