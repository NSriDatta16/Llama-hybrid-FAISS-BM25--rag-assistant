[site]: crossvalidated
[post_id]: 174407
[parent_id]: 174137
[tags]: 
In a time series setting with a lagged dependent variable included as a regressor, the OLS estimator will be consistent but biased. The reason for this is that in order to show unbiasedness of the OLS estimator we need strict exogeneity, $E\left[\varepsilon_{t}\left|x_{1},\, x_{2,},\,\ldots,\, x_{T}\right.\right] $, i.e. that the error term, $\varepsilon_{t} $, in period $t $ is uncorrelated with all the regressors in all time periods. However, in order to show consistency of the OLS estimator we only need contemporanous exogeneity, $E\left[\varepsilon_{t}\left|x_{t}\right.\right] $, i.e. that the error term, $\varepsilon_{t} $, in period $t $ is uncorrelated with the regressors, $x_{t} $ in period $t $. Consider the AR(1) model: $y_{t}=\rho y_{t-1}+\varepsilon_{t},\;\varepsilon_{t}\sim N\left(0,\:\sigma_{\varepsilon}^{2}\right)$ with $x_{t}=y_{t-1} $ from now on. First I show that strict exogeneity does not hold in a model with a lagged dependent variable included as a regressor. Let's look at the correlation between $\varepsilon_{t} $ and $x_{t+1}=y_{t} $ $$E\left[\varepsilon_{t}x_{t+1}\right]=E\left[\varepsilon_{t}y_{t}\right]=E\left[\varepsilon_{t}\left(\rho y_{t-1}+\varepsilon_{t}\right)\right] $$ $$=\rho E\left(\varepsilon_{t}y_{t-1}\right)+E\left(\varepsilon_{t}^{2}\right) $$ $$=E\left(\varepsilon_{t}^{2}\right)=\sigma_{\varepsilon}^{2}>0 \ (Eq. (1)).$$ If we assume sequential exogeneity, $E\left[\varepsilon_{t}\mid y_{1},\: y_{2},\:\ldots\ldots,y_{t-1}\right]=0 $, i.e. that the error term, $\varepsilon_{t} $, in period $t $ is uncorrelated with all the regressors in previous time periods and the current then the first term above, $\rho E\left(\varepsilon_{t}y_{t-1}\right) $, will dissapear. What is clear from above is that unless we have strict exogeneity the expectation $E\left[\varepsilon_{t}x_{t+1}\right]=E\left[\varepsilon_{t}y_{t}\right]\neq0 $. However, it should be clear that contemporaneous exogeneity, $E\left[\varepsilon_{t}\left|x_{t}\right.\right] $, does hold. Now let's look at the bias of the OLS estimator when estimating the AR(1) model specified above. The OLS estimator of $\rho $, $\hat{\rho} $ is given as: $$\hat{\rho}=\frac{\frac{1}{T}\sum_{t=1}^{T}y_{t}y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\frac{\frac{1}{T}\sum_{t=1}^{T}\left(\rho y_{t-1}+\varepsilon_{t}\right)y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}\varepsilon_{t}y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}} \ (Eq. (2))$$ Then take conditional expectation on all previous, contemporaneous and future values, $E\left[\varepsilon_{t}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right] $, of $Eq. (2)$: $$E\left[\hat{\rho}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]=\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}\left[\varepsilon_{t}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}} $$ However, we know from $Eq. (1)$ that $E\left[\varepsilon_{t}y_{t}\right]=E\left(\varepsilon_{t}^{2}\right) $ such that $\left[\varepsilon_{t}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]\neq0 $ meaning that $\frac{\frac{1}{T}\sum_{t=1}^{T}\left[\varepsilon_{t}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}\neq0 $ and hence $E\left[\hat{\rho}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]\neq\rho $ but is biased: $E\left[\hat{\rho}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]=\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}\left[\varepsilon_{t}\left|y_{1},\, y_{2,},\,\ldots,\, y_{T-1}\right.\right]y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}E\left(\varepsilon_{t}^{2}\right)y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=$$\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}\sigma_{\varepsilon}^{2}y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}} $. All I assume to show consistency of the OLS estimator in the AR(1) model is contemporanous exogeneity, $E\left[\varepsilon_{t}\left|x_{t}\right.\right]=E\left[\varepsilon_{t}\left|y_{t-1}\right.\right]=0 $ which leads to the moment condition, $E\left[\varepsilon_{t}x_{t}\right]=0 $ with $x_{t}=y_{t-1} $. As before, we have that the OLS estimator of $\rho $, $\hat{\rho} $ is given as: $$\hat{\rho}=\frac{\frac{1}{T}\sum_{t=1}^{T}y_{t}y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\frac{\frac{1}{T}\sum_{t=1}^{T}\left(\rho y_{t-1}+\varepsilon_{t}\right)y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\rho+\frac{\frac{1}{T}\sum_{t=1}^{T}\varepsilon_{t}y_{t-1}}{\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}} $$ Now assume that $plim\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}=\sigma_{y}^{2} $ and $\sigma_{y}^{2} $ is positive and finite, $0 Then, as $T\rightarrow\infty $ and as long as a law of large numbers (LLN) applies we have that $p\lim\frac{1}{T}\sum_{t=1}^{T}\varepsilon_{t}y_{t-1}=E\left[\varepsilon_{t}y_{t-1}\right]=0 $. Using this result we have: $$\underset{T\rightarrow\infty}{p\lim\hat{\rho}}=\rho+\frac{p\lim\frac{1}{T}\sum_{t=1}^{T}\varepsilon_{t}y_{t-1}}{p\lim\frac{1}{T}\sum_{t=1}^{T}y_{t}^{2}}=\rho+\frac{0}{\sigma_{y}^{2}}=\rho $$ Thereby it has been shown that the OLS estimator of $p $, $\hat{\rho} $ in the AR(1) model is biased but consistent. Note that this result holds for all regressions where the lagged dependent variable is included as a regressor.
