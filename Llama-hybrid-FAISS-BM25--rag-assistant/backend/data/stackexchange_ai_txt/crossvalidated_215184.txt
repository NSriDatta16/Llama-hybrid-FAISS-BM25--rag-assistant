[site]: crossvalidated
[post_id]: 215184
[parent_id]: 215154
[tags]: 
In terms of prediction, you probably need to think of the question of how quickly the model learns the important features. Even thinking of OLS, this will give you something like model selection given enough data. But we know that it doesn't converge to this solution quickly enough - so we search for something better. Most methods are making an assumption about the kind of betas/coefficients that are going to be encountered (like a prior distribution in a bayesian model). They work best when these assumptions hold. For example, ridge/lasso regression assumes most betas are on the same scale with most near zero. They won't work as well for the "needles in a haystack" regressions where most betas are zero, and some betas are very large (i.e. scales are very different). Feature selection may work better here - lasso can get stuck in between shrinking noise and leaving signal untouched. Feature selection is more fickle - an effect is either "signal" or "noise". In terms of deciding - you need to have some idea of what sort of predictor variables you have. Do you have a few really good ones? Or all variables are weak? This will drive the profile of betas you will have. And which penalty/selection methods you use (horses for courses and all that). Feature selection is also not bad but some of the older approximations due to computational restrictions are no longer good (stepwise, forward). Model averaging using feature selection (all 1 var models, 2 var models, etc weighted by their performance) will do a pretty good job at prediction. But these are essentially penalising the betas through the weight given to models with that variable excluded - just not directly - and not in a convex optimisation problem sort of way.
