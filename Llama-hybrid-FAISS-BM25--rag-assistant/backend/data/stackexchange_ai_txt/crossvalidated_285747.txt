[site]: crossvalidated
[post_id]: 285747
[parent_id]: 
[tags]: 
Finding the optimal hyperparameters

I'm a little bit confused when estimating the hyperparameter in a problem of regression with gaussian process. In "Gaussian process for machine learning" (Rasmussen and Williams, MIT Press, 2006), the hyperparameters are estimated through an optimisation procedure, involving the partial derivatives of the log marginal likelihood with regard the hyperparameters (e.g. with the conjugate gradient). In several papers I've red that the same is made through the maximum a posteriori, finding the mode of the posterior distribution, incorporating a prior over the hyperparameters (MAP). E.g. di Sciascio and Amicarelli (Computers and Chemical Engineering 32 (2008) 3264–3273). Now, I thought both approaches are the same. But, in Rasmussen and Williams (2006) is said: "Note, however, that in the Bayesian setting the MAP estimate plays no special rôle" (p. 10). So, can somebody enlighten me about finding the optimal set of hyperparameters in a regression with gaussian processes? By the way, I'm using GPML toolbox of Rasmussen to make regression.
