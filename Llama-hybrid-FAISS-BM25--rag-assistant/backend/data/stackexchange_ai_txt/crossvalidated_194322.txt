[site]: crossvalidated
[post_id]: 194322
[parent_id]: 
[tags]: 
Why does SGD and back propagation work with ReLUs?

ReLUs are not differentiable at the origin. However, they are widely used in Deep Learning together with Stochastic Gradient Descent algorithms and Backpropagation, where the gradients of the loss function are calculated with the chain rule. How do these algorithms calculate derivatives given that ReLUs are not differentiable at x=0 ?
