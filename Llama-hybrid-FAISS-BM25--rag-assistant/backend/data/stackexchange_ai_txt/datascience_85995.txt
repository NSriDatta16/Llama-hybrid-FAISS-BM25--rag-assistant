[site]: datascience
[post_id]: 85995
[parent_id]: 52039
[tags]: 
You could pass the attention and the encoder hidden states over to whichever place on the decoder you want and let it do the same weighted sum there. At the end of the day the weighted sum (context) has to be derived before the prediction. The real question is - Can we make any other use of these attention weights. Luong et al came up with a very nice idea of passing these attention weights to the subsequent timesteps - the idea being that past history of attention could help the model in subsequent time steps. This turned out to be a good insight and makes a lot of difference
