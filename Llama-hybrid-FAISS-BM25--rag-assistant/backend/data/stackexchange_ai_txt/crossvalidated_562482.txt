[site]: crossvalidated
[post_id]: 562482
[parent_id]: 
[tags]: 
Language model gives different results when Bayes' theorem is applied

Say, the following example is our corpus: a quick brown fox jumps over the lazy dog. Here, there are 9 tokens in the corpus. If we find the unigram probability for "a" it will be P("a") = 1 / 9 and also P("quick") = 1 / 9 . Similarly if we take the bigram probability of P("quick"|"a") : P("quick"|"a") = CountOf(How many times "quick" followed by "a") / CountOf("a") = 1 / 1 = 1 But for P("a"|"quick") it is 0 . Now we want to make an n-gram language model (n = 2) and want to find the joint distribution of the phrase "a quick" , then according to the chain rule of probability: P("a", "quick") = P("a")P("quick"|"a") = (1 / 9) * 1 = 1 / 9 Here, P("quick"|"a") means the probability of finding "quick" after "a". But from Bayes' theorem we know: P("a")P("quick"|"a") = P("quick")P("a"|"quick") = (1 / 9) * 0 = 0 But P("quick")P("a"|"quick") will give us 0 as P("a"|"quick") is 0 . Whereas, P("a")P("quick"|"a") is not 0 . But mathematically they should be equal, but it is not happening. Moreover no matter how large corpus we take, the formulation found from Bayes' theorem will unlikely be exactly equal. How we can describe this behavior? Is joint distribution order sensitive?
