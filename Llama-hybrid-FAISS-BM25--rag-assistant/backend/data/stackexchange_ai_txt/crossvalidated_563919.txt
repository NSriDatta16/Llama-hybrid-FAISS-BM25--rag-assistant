[site]: crossvalidated
[post_id]: 563919
[parent_id]: 
[tags]: 
Formula to compute approximate memory requirements of Transformer models

I would like to roughly estimate the memory requirement of training an arbitrary Transformer model $M$ , with $l$ layers, $h$ attention heads, an embedding dimension of $d$ , and an input dimension of $t$ tokens. What is the formula to compute this estimate? If you happen to know the formula for a specific architecture (say BERT, or GPT) that would also be fine. Note: I am not interested in precisely knowing how many bytes will be used on a specific GPU by a specific implementation with some library. I would just like a general formula to get a sense of the dimensions that is more principled than just "changing the batch size until it fits".
