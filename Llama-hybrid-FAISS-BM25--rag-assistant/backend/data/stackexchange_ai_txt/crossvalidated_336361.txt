[site]: crossvalidated
[post_id]: 336361
[parent_id]: 336347
[tags]: 
No, $y_j$ is not a vector of length 8. It is a single value. $y_j$ describes the Q-value that agent has got by performing action $a_t$ in the state $s_t$. Note : If you only want answer of your question, you can directly look at the last part of the answer. I've written initial part for explaining Deep Q Leaning algorithm. So, to describe the entire scenario, Agent is currently in state $s_t$ (State $s_t$ is one of many combinations possible from your 64 input neurons). Now Agent has to select an action from given state $s_t$. Here the situation is called explore-exploit dilemma. By selecting exploitation, Agent chooses best action it has learnt, and by choosing exploration, Agent randomly pick any action out of 8 possible actions. This action is $a_t$. By performing this action $a_t$ in a state $s_t$, Agent receives a reward $r_t$ from the environment and moves to the next step $s_t$$_+$$_1$. This is one step of RL process. Now, based on this move we need to update Q-value. $y_t$ is the Q-value as mentioned earlier, which stores Q-value of the action $a_t$ in state $s_t$. We are using Bellman equation for updating Q-value. As shown in the algorithm, if next state(means $s_t$$_+$$_1$) is a terminal state than Q-value is simply the environment reward $r_t$. But if it is a non-terminal state, then according to Bellman equation, we need to choose best action from all the possible actions in next state $s_t$$_+$$_1$ (This is the meaning of that equation). For doing this, we need to supply state $s_t$$_+$$_1$ as the input in ANN ($s_t$$_+$$_1$ is again one of many combinations possible from your 64 input neurons), ANN will give values for all 8 actions, we need to pick highest value from those 8 values. Then multiply this highest value with discount factor gamma. And finally add environment reward $r_t$ in it. This gives the value for $y_t$ for non-terminal state. Remember this Q-value $y_t$ is for only the action $a_t$ in state $s_t$, and it is a single value, not a vector. So, Finally to answer your question, You have Q-values of all the 8 actions from state $s_t$$_+$$_1$ , which are [0.2 0.4 0.2 0.1 0.02 0.02 0.02 0.1] As per the Bellman equation, pick highest value, which is 0.4. So, $y_j$ = $r_j$ + gamma * $\text{max}_{a'}Q(s_{j+1},a';\theta)$ $y_j$ = 1 + gamma * 0.4 (gamma value of your choice such that 0= Note : Please treat $y_j$ and $y_t$ same throughout the answer.
