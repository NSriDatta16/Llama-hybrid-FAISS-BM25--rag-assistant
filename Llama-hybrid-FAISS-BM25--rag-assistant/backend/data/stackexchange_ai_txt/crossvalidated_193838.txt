[site]: crossvalidated
[post_id]: 193838
[parent_id]: 193692
[tags]: 
The degrees of freedom in a linear regression model with Student-t errors are not fixed neither in the classical nor in the Bayesian approach. You are mixing up inference with hypothesis tests. The formulation is as follows. You have $n$ response variables $y_1,\dots,y_n$, $n$ covariate vectors ${\bf x}_1,\dots,{\bf x}_n$ and $p+1$ regression parameters $\beta_0,\beta_1,\dots,\beta_p$. The regression model is: $$y_j=\beta_0 +\beta_1 x_{j1}\dots + \beta_p x_{jp} + \epsilon_j,$$ where $\epsilon_j$ is distributed according to a Student t distribution with $\nu>0$ degrees of freedom and scale parameter $\sigma>0$. Then, the likelihood function is: $$L(\beta_0,\dots,\beta_p,\sigma,\nu) \propto \prod_{j=1}^n \dfrac{1}{\sigma}f\left(\dfrac{y_j-{\bf x}^{\top}_j\beta}{\sigma}\Bigg\vert \nu\right),$$ where $f(\cdot\vert \nu)$ is the Student t density with $\nu$ degrees of freedom. The maximum likelihood estimator is obtained by maximising the likelihood function with respect to all the parameters. In order to obtain Bayesian inference, you need a prior for the parameters in order to construct the posterior distribution in the usual way: $Posterior \propto Likehood \times Prior$. Typically, people use a Normal distribution for ${\bf \beta}$, an inverse gamma for $\sigma$, and for $\nu$ several noninformative priors have been proposed. Here is a list of options: Jeffreys prior: http://biomet.oxfordjournals.org/content/95/2/325.short Juarez-Steel prior: http://onlinelibrary.wiley.com/doi/10.1002/jae.1113/abstract Discrete noninformative prior: https://projecteuclid.org/euclid.ba/1393251776 Noninformative prior based on kurtosis: http://projecteuclid.org/euclid.ejs/1440680330 Penalised complexity prior: http://arxiv.org/abs/1403.4630 Which one is better (and in what sense?)? That is an open question.
