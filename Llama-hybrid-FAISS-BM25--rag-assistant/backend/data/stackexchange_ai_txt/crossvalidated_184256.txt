[site]: crossvalidated
[post_id]: 184256
[parent_id]: 184082
[tags]: 
Here is an approach that seems natural to me. Introduction Suppose there are $n$ observations of the following form: \begin{equation} y_i = (y_{i1}, \ldots, y_{iJ}), \end{equation} where $y_{ij} \in \{0,1\}$. Let \begin{equation} Y = (Y_1, \ldots, Y_J), \end{equation} where \begin{equation} Y_j = \sum_{i=1}^n y_{ij}. \end{equation} The problem Consider the collection of subsets of $\{1,\ldots,J\}$, each containing $K A Bayesian framework Think of $Y$ as being generated from a multinomial distribution: \begin{equation} p(Y|\theta) \propto \prod_{j=1}^J \theta_j^{Y_j}, \end{equation} where $\theta = (\theta_1, \ldots, \theta_J) \in \Delta^{J-1}$, where $\Delta^{J-1}$ denotes the $J-1$-dimensional simplex. (In other words, $\theta_j \ge 0$ and $\sum_{j=1}^J \theta_j = 1$.) This provides a likelihood for $\theta$. Let the prior distribution for $\theta$ be given by the Dirichlet distribution: \begin{equation} p(\theta) = \textsf{Dirichlet}(\theta|\alpha) \propto \prod_{j=1}^J \theta_j^{\alpha_j-1}, \end{equation} where $\alpha = (\alpha_1, \ldots, \alpha_J)$ and $\alpha_j > 0$. The mean of this distribution is $\alpha/\alpha_0$, where $\alpha_0 = \sum_{j=1}^J \alpha_j$. (The variance is inversely related to $\alpha_0$, which is called the concentration parameter.) Note that if $\alpha_j = 1$ for all $j$, then $p(\theta) \propto 1$, a flat distribution. The Dirichlet distribution is a conjugate prior for the multinomial likelihood. The posterior distribution for $\theta$ is given by \begin{equation} p(\theta|Y) \propto p(Y|\theta)\,p(\theta) \propto \prod_{j=1}^J \theta_j^{(\alpha_j + Y_j) - 1} \propto \textsf{Dirichlet}(\theta|\alpha'), \end{equation} where $\alpha' = \alpha + Y$. The posterior mean for $\theta$ is \begin{equation} E[\theta|Y] = \alpha'/\alpha_0', \end{equation} where $\alpha_0' = \sum_{j=1}^J (\alpha_j + Y_j)$. Solution to the problem The posterior distribution for $\theta$ summarizes what is known about the categories $(1, \ldots, J)$. With this in mind, switch perspective from the elements of $Y$ to the corresponding elements of $\theta$ and use $p(\theta|Y)$ to solve the problem posed above. Let $m_\ell$ denote the sum of the elements in $\{\theta_j : \theta_j \in S_\ell\}$: \begin{equation} m_\ell = \sum_{j\in S_\ell} \theta_j. \end{equation} The joint posterior distribution for $(m_1, \ldots, m_L)$ can be computed from the posterior distribution for $\theta$. The posterior mean is one feature of the distribution, \begin{equation} E[m_\ell|Y] = \sum_{j \in S_\ell} E[\theta_j|Y] = \frac{1}{\alpha_0'} \sum_{j\in S_\ell} \alpha_j'. \end{equation} We wish to compute the posterior probability that $m_\ell$ is the maximal element: \begin{equation} \pi_\ell = \text{Pr}[\max(m_1, \ldots, m_L) = m_\ell\,|\,Y] \qquad\text{for $\ell = 1, \ldots, L$}. \end{equation} These probabilities will depend on the variance of $p(\theta|Y)$ as well as its mean. An analytical solution may be possible, but here I describe a numerical solution that is simple to implement. Make $R$ draws from $\textsf{Dirichelt}(\theta|\alpha')$ and let $r \in \{1,\ldots, R\}$ index the draws. For each draw compute \begin{equation} z^{(r)} = \arg\!\max_{\ell}\ (m_1^{(r)}, \ldots, m_L^{(r)}). \end{equation} Note $z^{(r)} \in \{1, \ldots, L\}$. Then \begin{equation} \pi_\ell \approx \frac{1}{R} \sum_{r=1}^R [z^{(r)} = \ell], \end{equation} where $[\,\cdot\,]$ is the Iverson bracket defined as \begin{equation} [x] = \begin{cases} 1 & \text{$x$ is true} \\ 0 & \text{$x$ is false} \end{cases} . \end{equation} The larger $R$ is, the better the approximation. (One can compute a numerical standard error to assess the accuracy of the approximation.) Example Let $J = 9$ and \begin{equation} Y = (467,\ 483,\ 127,\ 30,\ 76,\ 47,\ 34,\ 32,\ 20). \end{equation} In addition, let $\alpha_j = 1$ for all $j \in \{1, \ldots, 9\}$. (Note $\alpha_0' = 1325$.) The mean of the posterior distribution is \begin{equation} E[\theta|Y] \approx (\text{0.353},\text{ 0.365},\text{ 0.097},\text{ 0.023},\text{ 0.058},\text{ 0.036},\text{ 0.026},\text{ 0.025},\text{ 0.016}). \end{equation} Let $K = 3$, in which case $L = 84$. Let $S_1 = \{1,2,3\}$ and $S_3 = \{1,2,5\}$. Then $E[m_1|Y] \approx 0.815$ and $E[m_3|Y] \approx 0.777$. In $R = 10^6$ draws, only $m_1$ and $m_3$ appeared as the maximum, with $m_3$ appearing 170 times. Therefore $\pi_1 \approx 0.998$, $\pi_3 \approx 0.002$, and $\pi_\ell \approx 0$ for all other $\ell$.
