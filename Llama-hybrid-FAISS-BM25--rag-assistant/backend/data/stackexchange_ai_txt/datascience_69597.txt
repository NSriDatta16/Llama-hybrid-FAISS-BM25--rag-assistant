[site]: datascience
[post_id]: 69597
[parent_id]: 69277
[tags]: 
Think about how you would verify the model on your own. You could train it on data in the past, stopping before the present, and then ask the model to predict for a period that you already had data for. That way you can check how accurate it is (either by eye or with error metrics such as MAE, MAPE, RMSE, etc), and adjust accordingly. cross_validation just automates this process. The first parameter you give is your trained model m (not the data). You then also give the prediction horizon - how frequently you want to predict (in your case '15min' , assuming Python). You may then give an initial (how long to train before starting the tests) and a period (how frequently to stop and do a prediction). If you don't give them, Prophet will assign defaults of initial = 3 * horizon, and cutoffs every half a horizon. You then have a long running series of validations, each time predicting forward and calculating the error (you can use the other fbprophet.diagnostics tool, performance_metrics for this). This is somewhat akin to k-fold cross-validation in non-time-series machine learning. The documentation on this is actually pretty good. See also this blog post for an example in Python.
