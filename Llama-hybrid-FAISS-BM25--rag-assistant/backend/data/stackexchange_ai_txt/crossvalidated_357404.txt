[site]: crossvalidated
[post_id]: 357404
[parent_id]: 61783
[tags]: 
Although this question is rather old, I would like to add an additional answer because I think it is worth clarifying this a bit more. My question is partly motivated by this thread: Optimal number of folds in K-fold cross-validation: is leave-one-out CV always the best choice? . The answer there suggests that models learned with leave-one-out cross-validation have higher variance than those learned with regular K-fold cross-validation, making leave-one-out CV a worse choice. That answer does not suggest that, and it should not. Let's review the answer provided there: Leave-one-out cross-validation does not generally lead to better performance than K-fold, and is more likely to be worse, as it has a relatively high variance (i.e. its value changes more for different samples of data than the value for k-fold cross-validation). It is talking about performance . Here performance must be understood as the performance of the model error estimator . What you are estimating with k-fold or LOOCV is model performance, both when using these techniques for choosing the model and for providing an error estimate in itself. This is NOT the model variance, it is the variance of the estimator of the error (of the model). See the example (*) bellow. However, my intuition tells me that in leave-one-out CV one should see relatively lower variance between models than in the K-fold CV, since we are only shifting one data point across folds and therefore the training sets between folds overlap substantially. Indeed, there is lower variance between models, They are trained with datasets that have $n-2$ observations in common! As $n$ increases, they become virtually the same model (Assuming no stochasticity). It is precisely this lower variance and higher correlation between models what makes the estimator I talk about above have more variance, because that estimator is the mean of these correlated quantities, and the variance of the mean of correlated data is higher than that of uncorrelated data. Here it is shown why: variance of the mean of correlated and uncorrelated data . Or going in the other direction, if K is low in the K-fold CV, the training sets would be quite different across folds, and the resulting models are more likely to be different (hence higher variance). Indeed. If the above argument is right, why would models learned with leave-one-out CV have higher variance? The above argument is right. Now, the question is wrong. The variance of the model is a whole different topic. There is a variance where there is a random variable. In machine learning you deal with lots of random variables, in particular and not restricted to: each observation is a random variable; the sample is a random variable; the model, since it is trained from a random variable, is a random variable; the estimator of the error that your model will produce when faced to the population is a random variable; and last but not least, the error of the model is a random variable, since there is likely to be noise in the population (this is called irreducible error). There can also be more randomness if there is stochasticity involved in the model learning process. It is of paramount importance to distinguish between all these variables. (*) Example : Suppose you have a model with a real error $err$ , where you should understand $err$ as the error that the model produces over the entire population. Since you have a sample drawn from this population, you use Cross validation techniques over that sample to compute an estimate of $E$ , which we can name $\tilde{err}$ . As every estimator, $\tilde{err}$ is a random variable, meaning it has its own variance, $var(\tilde{err})$ , and its own bias, $E(\tilde{err}-err)$ . $var(\tilde{err})$ is precisely what is higher when employing LOOCV. While LOOCV is a less biased estimator than $k-fold$ with $k , it has more variance. To further understand why a compromise between bias and variance is desired , suppose $err = 10$ , and that you have two estimators: $\tilde{err}_1$ and $\tilde{err}_2$ . The first one is producing this output $$\tilde{err}_1 = 0,5,10,20,15,5,20,0,10,15...$$ whereas the second one is producing $$ \tilde{err}_2 = 8.5,9.5,8.5,9.5,8.75,9.25,8.8,9.2...$$ The last one, although it has more bias, should be preferred, as it has a lot less variance and a acceptable bias, i.e. a compromise ( bias-variance trade-off ). Please do note that you neither want very low variance if that entails a high bias! Additional note : In this answer I try to clarify (what I think are) the misconceptions that surround this topic and, in particular, tries to answer point by point and precisely the doubts the asker has. In particular, I try to make clear which variance we are talking about , which is what it is essentially asked here. I.e. I explain the answer which is linked by the OP. That being said, while I provide the theoretical reasoning behind the claim, we have not found, yet, conclusive empirical evidence that supports it. So please be very careful. Ideally, you should read this post first and then refer to the answer by Xavier Bourret Sicotte, which provides an insightful discussion about the empirical aspects. Last but not least, something else must be taken into account: Even if variance as you increase $k$ remains flat (as we haven't empirically proved otherwise), $k-fold$ with $k$ small enough allows for repetition ( repeated k-fold ), which definitely should be done, e.g. $10 \ \times \ 10-fold$ . This effectively reduces variance, and is not an option when performing LOOCV.
