[site]: crossvalidated
[post_id]: 461549
[parent_id]: 
[tags]: 
Neural network backpropagation to update inputs, not weights (e.g. fine-tuning embeddings)?

I recently re-read Stanford CS231N lecture notes on computer vision and backpropagation, and I came across this passage ( emphasis mine): Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples $x_i$ , in practice we usually only compute the gradient for the parameters (e.g. $W$ , $b$ ) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on $x_i$ can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing. Here are my specific questions: By backpropagation to compute the gradient on the input examples , does that mean computing the partial derivative of the loss $J$ with respect to input $x$ (i.e. computing ${\partial J} / {\partial x}$ ) rather than the usual partial derivative with respect to the weights (i.e. computing $\partial{J} / \partial{W}$ )? In natural language processing, there is the concept of pre-trained word embeddings , which are vector representations of words. One can also fine-tune the word embeddings by further training on a specific task and letting the embeddings be updated. Is this embedding fine-tuning an example of backpropagation to compute the gradient on the input examples ? Thank you for any information.
