[site]: datascience
[post_id]: 88367
[parent_id]: 88287
[tags]: 
Most of the code/references available out there use test_data during training. test_data wasn't part of my training. While this is the way we should do it but stuff like Encoding must be done holistically. In your case, you have called the pre_process separately for Test and Train. So, the words are converted to Numbers independently. This should not happen . tokenizer.texts_to_sequences(test) Above Tokenizer should be the one that was fit on train data. If I randomly print token with key 101 for train, test. This is the result print(train_tokn.index_word[101]) print(test_tokn.index_word[101]) think characters I think you should use the train_tokn for the test data and it should improve. I believe a very simple LSTM can achieve 85% on this dataset Or, manually embed both Train, Test using the GloVe embedding. A simple example for the issue from keras.preprocessing.text import Tokenizer train = ['I am sorry'] test = ['I am very sorry'] max_words = 10 # Train tokenizer = Tokenizer(num_words=max_words) tokenizer.fit_on_texts(train) tokenizer.index_word # {1: 'i', 2: 'am', 3: 'sorry'} # Test tokenizer = Tokenizer(num_words=max_words) tokenizer.fit_on_texts(test) tokenizer.index_word # {1: 'i', 2: 'am', 3: 'very', 4: 'sorry'}
