[site]: crossvalidated
[post_id]: 464606
[parent_id]: 464601
[tags]: 
I would like to use logistic regression as an example to explain what is happening when algorithm did not converge . We know that for perfect separation case, logistic regression without regularization will not converge. You may review here: Is there any intuitive explanation of why logistic regression will not work for perfect separation case? And why adding regularization will fix it? Suppose we have a perfect separation in logistic regression, the algorithm is trying hard to find a solution, that can minimize the logistic loss. However, because we can make the loss even by increase the parameter value. The algorithm will keep doing it, until it exceeds the max number that can be represented by a computer. Therefore, the algorithm will end somewhere, in most cases, it will end with the max iteration. The ending may not be bad, i.e., the parameters still can minimize the loss to some level, this is why you will see, even the algorithm is not converge but the model is still working. Here is an example, from similar to my previous answer, that you can see, for perfect separation, the algorithm does not converge but we are still getting a "meaningful" output set.seed(0) d=mlbench::mlbench.2dnormals(100, 2, r=3) fit = glm(d $classes~d$ x, family = binomial()) plot(d) abline(0, -fit $coefficients[2]/fit$ coefficients[1], col='blue', lwd=2)
