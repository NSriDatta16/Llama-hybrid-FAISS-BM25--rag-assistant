[site]: crossvalidated
[post_id]: 560106
[parent_id]: 
[tags]: 
What is the relationship between model uncertianty and model parameter count?

I'm looking for references, information and/or existing theory behind the relationship between the uncertainty in a given model vs its complexity/parameter count. The situation I have in mind is using Bayesian-ish methods like MC Dropout, bootstrapping or Laplace approximation to estimate credible/uncertainty intervals for deep neural networks. For example, I would imagine that if I trained a very deep network and a relatively shallow network, assuming they yield identical accuracy, the deeper network would have wider intervals than the shallow network. I'm not sure if my instincts here are accurate, and am curious if the community is aware of any existing relationships (theoretical or empirical) that establish relationships between uncertainty and model size. Thanks in advance!
