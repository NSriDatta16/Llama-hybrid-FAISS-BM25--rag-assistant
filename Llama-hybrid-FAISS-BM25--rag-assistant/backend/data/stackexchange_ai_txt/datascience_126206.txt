[site]: datascience
[post_id]: 126206
[parent_id]: 
[tags]: 
Why doesn't CLIP use a pretrained large language model as the text encoder?

CLIP: https://openai.com/research/clip They use a small text encoder and suggest that the simpler the model the better. Is there any reason why that is? Has anyone tried using a pretrained LLM as the text encoder and then fine tuning? I would assume the LLM would learn a lot better representation of the meaning of the text.
