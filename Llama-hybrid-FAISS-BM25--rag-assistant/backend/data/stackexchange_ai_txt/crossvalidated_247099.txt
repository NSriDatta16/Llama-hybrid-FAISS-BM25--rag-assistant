[site]: crossvalidated
[post_id]: 247099
[parent_id]: 247094
[tags]: 
If you can get access to a very precise clock, you can extract the decimal part of the current time and turn it into a uniform, from which you can derive a normal simulation by the Box-Müller transform:$$X=\sqrt{-2\log U_1}\,\cos(2\pi U_2)$$(and even two since $Y=\sqrt{-2\log U_1}\,\sin(2\pi U_2)$ is another normal variate independent from $X$). For instance, on my Linux OS, I can check $ date +%s.%N 1479733744.077762986 $ date +%s.%N 1479733980.615056616 hence set$$U_1=.077762986,\ U_2=.615056616$$and $X$ as > sqrt(-2*log(.077762986))*cos(2*pi*.615056616) [1] -1.694815 Addendum: since computing logarithms and cosines may be deemed not manual enough, there exists a variant to Box-Müller that avoids using those transcendental functions (see Exercise 2.9 in our book Monte Carlo Statistical Methods ): Now, one can argue against this version because of the Exponential variates. But there also exists a very clever way of simulating those variates without a call to transcendental functions , due to von Neumann, as summarised in this algorithm reproduced from Luc Devroye's Non-uniform Random Variates : Admittedly, it requires the computation of 1/e, but only once. If you do not have access to this clock, you can replace this uniform generator by a mechanistic uniform generator , like throwing a dart on a surface with a large number of unit squares $(0,1)^2$ or rolling a ball on a unit interval $(0,1)$ with enough bounces [as in Thomas Bayes' conceptual billiard experiment] or yet throwing matches on a wooden floor with unit width planks and counting the distance to the nearest leftmost separation [as in Buffon's experiment] or yet further to start a roulette wheel with number 1 the lowest and turn the resulting angle of 1 with its starting orientation into a uniform $(0,2\pi)$ draw. Using the CLT to approximate normality is certainly not a method I would ever advise as (1) you still need other variates to feed the average, so may as well use uniforms in the Box-Müller algorithm, and (2) the accuracy grows quite slowly with the number of simulations. Especially if using a discrete random variable like the result of a dice, even with more than six faces . To quote from Thomas et al. (2007), a survey on the pros and cons of Gaussian random generators: The central limit theorem of course is an example of an “approximate” method—even if perfect arithmetic is used, for finite K the output will not be Gaussian. Here is a quick experiment to illustrate the problem: I generated 100 times the average of 30 die outcomes: dies=apply(matrix(sample(1:6,30*100,rep=TRUE),ncol=30),1,mean) then normalised those averages into mean zero - variance one variates stdies=(dies-3.5)/sqrt(35/12/30) and looked at the normal fit [or lack thereof] of this sample: First, the fit is not great, especially in the tails, and second, rather obviously, the picture confirms that the number of values taken by the sample is embarrassingly finite. (In this particular experiment, there were only 34 different values taken by dies , between 76/30 and 122/30 .) By comparison, if I exploit the very same 3000 die outcomes $D_i$ to create enough digits of a pseudo-uniform as$$U=\sum_{i=1}^k \dfrac{D_i-1}{6^i}$$with $k=15$ (note that 6¹⁵>10¹¹, hence I generate more than 11 truly random digits), and then apply the above Box-Müller transform to turn pairs of uniforms into pairs of N(0,1) variates, dies=matrix(apply(matrix(sample(0:5,15*200,rep=TRUE),nrow=15)/6^(1:15),2,sum),ncol=2) norma=sqrt(-2*log(dies[,1]))*c(cos(2*pi*dies[,2]),sin(2*pi*dies[,2])) the fit is as good as can be expected for a Normal sample of size 200 (just plot another one for a true normal sample, norma=rnorm(100) ): as further shown by a Kolmogorov-Smirnov test: > ks.test(norma,pnorm) One-sample Kolmogorov-Smirnov test data: norma D = 0.06439, p-value = 0.3783 alternative hypothesis: two-sided
