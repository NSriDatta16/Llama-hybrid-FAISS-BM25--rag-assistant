[site]: datascience
[post_id]: 126691
[parent_id]: 126679
[tags]: 
Most ML methods assume that you properly preprocessed the data, which includes mean removal and scaling. It is so, because the underlying optimization algorthms are indeed sensitive to scaling. So you should preprocess your data, which would undo the scaling operation you applied. You can find more about standard preprocessing techniques in the sklearn docs: https://scikit-learn.org/stable/modules/preprocessing.html EDIT: As you commented you are dealing with categorical features, which are not easy in general for ML algorithms. The problem is that there might be no meaningful measure among the categories, while after encoding there is, and in fact ML algorithms try to use this measure to come up with predictions. One way to remove this measure is to use one-hot encoding (see sklearn.preprocessing.OneHotEncoder ), which separates the features into binary feature vectors of size as the number of categories. But that rapidly becomes intractable as the number of categories and the number of categorical features grow. Beyond this, your results will depend on the encoding you use pretty much for any ML algorithm. When you do the encoding, you should try to assign closer numbers to the categories which are in some sense more similar if possible. Although the results will vary for different encodings for all ML algorithms I know, that does not mean all ML algorithms are sensitive the same way to categorical variables. Linear models (like logistic regression) perform poorly, so you should try decision trees (especially random forests, gradient boosting) which will mostly use the encoding implied measure for node splitting and are able to separate the similarly encoded categorical variables which influence the prediction differently. This is what a linear model cannot do as there a weight value simply multiplies the encoding value, so closer encoding values will produce closer prediction values too. One final note, there is no point of uniformly scaling the whole encoding as you will only introduce scaling problems as I mentioned above, and make the underlying optimization perform worse, at least for logistic regression. In fact, decision trees would be unaffected by such scaling. In both of your example, the encodings provide the same order among the categories, and that is all that matters. Of course if it make sense to say that 'b' is more similar to 'a' than to 'c', then it might make sense to reflect that in the numbers.
