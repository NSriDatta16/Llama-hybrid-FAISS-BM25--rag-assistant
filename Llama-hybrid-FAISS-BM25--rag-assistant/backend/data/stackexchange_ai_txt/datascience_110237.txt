[site]: datascience
[post_id]: 110237
[parent_id]: 110053
[tags]: 
interesting a practical problem. As suggested by others below, you may be interested in "time series anomaly detection" and "causality". So you already got a response to your 1st answer. However, you'll soon realize that in the literature "anomaly detection" is often an UNsupervised task (meaning that yes, you can spot anomalies, but if you are not really aware of how the underlying system works it's difficult to define if the "weird" condition is strictly an anomaly). Let me try providing some food for thought: Understanding whether a spike has occurred : I guess you have a practical feeling of acceptable "Level Of Service" for each metric. For instance, your machines could be sized so that you get an average response time of 50ms. A second step would be getting the distribution of the response time for each given hour , i.e. you may want to serve 95% of users in 35ms. That could be an example of simple statistics to build monitoring. Warlax called this "spikiness", but what it's worth stressing is that it should come from your level of service. Comparing the trends between different time-series data to be able to conclude if one is the cause of the other . A general solution to this kind of problem is not straightforward. Once again it may be useful to think about engineering considerations: if you get a ton of pages to serve => CPU load increases (and that's ok). The other way round may be untrue: if CPU load increases in low traffic conditions you may just have a bug. Documenting these simple rules may be an excellent starting point for consultants or for the evaluation of more sophisticated methods . Even if the solution can't find the right match but it would still be a win if it can narrow down the areas for users to look into . A good starter is the "Level Of Service" mentioned in (1). Another keyword to search could be "KPI". E.g. think of defining 3 indicators (avg response time in the last hour, avg CPU load, avg failed requests): if they are systematically high you might place a red background to some part of your time histories. Ignoring minor fluctuations to understand the overall trend . Iterating the process above you may learn if your statistics are good enough to filter out "false alarms" and guarantee a good service or if you really need to design and engineer "Data Science" solutions.
