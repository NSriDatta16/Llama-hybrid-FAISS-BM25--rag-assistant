[site]: crossvalidated
[post_id]: 339017
[parent_id]: 272607
[tags]: 
Note that MNIST is a much simpler problem set than CIFAR-10, and you can get 98% from a fully-connected (non-convolutional) NNet with very little difficulty. A very simple CNN with just one or two convolutional layers can likewise get to the same level of accuracy. I'm not sure about your NNet architecture, but I can get you to 78% test accuracy on CIFAR-10 with the following architecture (which is comparatively simpler and has fewer weights). No special initialization or handholding was required, using vanilla defaults and Adam optimizer: model = Sequential() model.add(Conv2D(input_shape=trainX[0,:,:,:].shape, filters=96, kernel_size=(3,3))) model.add(Activation('relu')) model.add(Conv2D(filters=96, kernel_size=(3,3), strides=2)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Conv2D(filters=192, kernel_size=(3,3))) model.add(Activation('relu')) model.add(Conv2D(filters=192, kernel_size=(3,3), strides=2)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Flatten()) model.add(BatchNormalization()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dense(n_classes, activation="softmax")) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) This architecture is pretty simple, and is loosely based on https://arxiv.org/pdf/1412.6806.pdf . Training this model thusly: n_epochs = 25 batch_size = 256 callbacks_list = None H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=n_epochs, batch_size=batch_size, callbacks=callbacks_list) print('Done!!!') Gives the following, which you can see gets to almost 77% by the 25th epoch and more or less flattens out from there (but has enough regularization from dropout to prevent it from degrading due to overfitting, at least over the tested number of iterations). Train on 50000 samples, validate on 10000 samples Epoch 1/50 50000/50000 [==============================] - 19s 390us/step - loss: 1.6058 - acc: 0.4150 - val_loss: 1.5285 - val_acc: 0.4669 Epoch 2/50 50000/50000 [==============================] - 19s 371us/step - loss: 1.2563 - acc: 0.5477 - val_loss: 1.1447 - val_acc: 0.5901 Epoch 3/50 50000/50000 [==============================] - 19s 373us/step - loss: 1.0784 - acc: 0.6163 - val_loss: 1.1577 - val_acc: 0.6002 ... Epoch 25/50 50000/50000 [==============================] - 19s 374us/step - loss: 0.3188 - acc: 0.8857 - val_loss: 0.7493 - val_acc: 0.7680 ... Epoch 50/50 50000/50000 [==============================] - 19s 373us/step - loss: 0.1928 - acc: 0.9329 - val_loss: 0.8718 - val_acc: 0.7751 Done!!! Here's an even simpler and much smaller architecture that can get to 70% pretty quickly with the same training regimen (no BatchNormalization or pooling layers): # CNN architecture with Keras model = Sequential() model.add(Conv2D(input_shape=trainX[0,:,:,:].shape, filters=32, use_bias=True, kernel_size=(3,3))) model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(Conv2D(filters=64, use_bias=False, kernel_size=(5,5), strides=2)) model.add(Activation('relu')) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128)) model.add(Activation('relu')) model.add(Dropout(0.3)) model.add(Dense(n_classes, activation="softmax")) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics= ['accuracy']) It's worth noting that the architectures that get to best-published accuracy on CIFAR-10 (currently in the 90-96% range) are generally more complicated and take many hours to train on GPU hardware. But I've been able to get to the 70-80% range with fairly simple architectures that train in minutes, which is what I'd recommend prior to going for best-published results which usually require more complicated architectures, longer training periods, sometimes special handholding/training regimens or data augmentation, and hours of train time. UPDATE: Based on the updated plots in the question, the most obvious problem that is demonstrated is overfitting. This is evidenced by the divergence of the train-test data after about the 15th epoch which demonstrates insufficient regularization for this architecture, for this data set. You will be unlikely to get improvement from tuning any other hyperparameters (normalization strategies, learning rates, etc.) unless the overfitting is addressed. In using NNets, I recommend the following: Start from architectures that mimic or replicate those known to produce good results Verify performance on your data set, with a particular eye to over-fitting in the network (evidenced by significant divergence of train-test errors) Add additional regularization (increase dropout rates) when overfitting is observed (you're looking for "just enough" to prevent overfitting - too much will result in under-fitting) Experiment with structure, training approaches, and hyper-parameters to find avenues of improvement Prescriptions regarding the latter are actually quite hard to come by, because there is little theoretical foundation for how structure, training, or hyper-parameters interact to yield performance on any given data set. That the approaches employed by published architectures achieving similarly-high levels of performance on benchmark data sets vary so greatly is evidence of this. Batchnormalization has been found to significantly improve some architectures, but others can do quite well without it (or are indifferent to its presence). The only real guidance to provide here is to try it and see if it helps. Fine-tuning learning rates should generally be avoided unless you are an advanced practitioner with a deep understanding of ConvNets and the corresponding ability to read the tea leaves with regard to inter-epoch incremental performance during training. Customized learning rates and other specialized training regimens can in some cases can help networks navigate around local minima and find better overall solutions, but unless you have a lot of time and the know-how to diagnose the convergence behavior of the network, this is not a good place to start. Most of us should use an optimizer like Adam which will outperform novice attempts at hand-tuned learning rates in the vast majority of cases. Data augmentation via image preprocessing can sometimes yield significant performance improvements (in general, the more varied the input data the better the model will generalize - data preprocessing adds variation to the input space which can improve out-of-sample accuracy and may afford reduction in regularization requirements - hypothetically, with infinite training data we wouldn't need any regularization at all, but in the image processing space we are unlikely to approach that asymptote). This can significantly increase training time and slow convergence rates though, and introduces a whole other set of hyperparameters relating to input image permutation techniques (rotation, cropping, scaling, noising, etc. etc.). Because this path can increase training times and require additional experiments to tune results, some general advice would be to drive for best accuracy in your network without augmentation first, then see if some modest augmentation yields improvement. If it does, it may warrant further experimentation. For any and all tuning experiments, you will need to keep an eye out for changes in over- and under-fitting behavior. Changing network architecture, training regimens, or hyperparameters may require additional tuning of dropout regularization. The ability to readily ascertain over- and under-fitting behavior from train/test performance is arguably the most important baseline skill in working with NNets, and this becomes more intuitive with experience. This is the candle by which all your efforts will be guided. The candle can only dimly illuminate the path, but without it you'll be stumbling around in the dark. If your network is badly over- or under-fitting, that should be addressed before attempting random permutations of network structure or hyperparameters. The comparatively simple architectures with vanilla training regimens included in this answer demonstrate a reality of working with NNET architectures on hard problems like image classification: attaining a "pretty good" result based on approaches that are known to work well is not difficult, but incremental improvement is increasingly costly. Achieving best-published results via experimentation is going to be beyond the abilities or time availability of many (although it is possible, with enough time and effort, to follow the cookbook recipes of published approaches to replicate their results - but even this is by no means trivial). Attaining incremental improvement from a "pretty good" starting point can be a very time-consuming process of trial-and-error, and many experiments will not yield any significant improvement. This is not meant to dissuade anyone from attempting to learn, but only to make clear that there is a significant investment required to master the (ever expanding) toolset in the NNet bag of tricks, and driving improvements through trial-and-error can require dozens (or hundreds) of experiments over days or weeks of dedicated-GPU training. The time, skill, and resources (dedicated GPU) required to train networks to very high levels of performance explain in part the popularity of pre-trained networks.
