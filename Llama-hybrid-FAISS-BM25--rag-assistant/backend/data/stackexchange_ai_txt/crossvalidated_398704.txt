[site]: crossvalidated
[post_id]: 398704
[parent_id]: 398700
[tags]: 
Unbiasedness refers to that fact that some estimator, say the arithmetic mean, has the property that the average difference between the true mean (mu) and the arithmetic mean (x-bar) is zero; that is E(mu-xbar)=0. This isn't dependent on the sample size. Consistency of an estimator means that as the sample size tends toward infinity, the standard error for that estimator tends to get smaller and tends toward zero (meaning if we had the population, the variability in the estimator is zero). This implies the sampling distribution of the statistic becomes less variable and eventually shrinks to a single point and means that the probability tends to 1 that any random value of an estimate (not estimator) falls arbitrarily close to the true value (i.e. the probability that |mu - xbar| being less than some arbitrarily small value becomes more certain as the sample size grows). This is arguably the bare minimum property we desire when we choose a sample statistic as an estimator of a true value-- that our estimate is more likely to be close to the real value as we get more data. There is a related idea of asymptotic unbiasedness but above are the basics of unbiasedness and consistency of an estimator.
