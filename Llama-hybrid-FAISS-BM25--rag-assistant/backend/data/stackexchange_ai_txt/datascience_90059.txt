[site]: datascience
[post_id]: 90059
[parent_id]: 
[tags]: 
How exactly does DQN learn?

I created my custom environment in gym , which is a maze. I use a DQN model with BoltzmannQPolicy . It trains fine with the following variables: position of the agent distance from the endpoint position of the endpoint which directions can it move to So I don't give it an image or anything. If I train and test it in the same environment (the same maze, without changing the position of walls) it can solve it easily. But if I introduce it to a completely different environment (maze) without the training then it doesn't know what to do. I don't know if the problem is with my code, or DQN is just for solving the same environment. Which algorithm should I use instead?
