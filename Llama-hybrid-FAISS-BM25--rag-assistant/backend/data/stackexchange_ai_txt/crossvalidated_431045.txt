[site]: crossvalidated
[post_id]: 431045
[parent_id]: 
[tags]: 
Confusion with softmax

I was wondering if someone could explain why, if I do softmax on [683, 861, 981, 834] I get [3.80403403e-130 7.66764807e-053 1.00000000e+000 1.44115655e-064] But if I take a factor of 100 out: [6.832, 8.61, 9.81, 8.34] then I get this: [0.03217071 0.19038654 0.63210557 0.14533718] Which is more inline with what I'd expect. Clearly I don't understand softmax, I was wondering if someone could explain? I'm using the output of the softmax as probabilities to select actions to take in a neural network, but because the largest output is equal to 1 then it's always selecting the largest action without any probability of others being selected. Perhaps I should use a function that sums the entries and bases probabilities on the proportion each entry makes up of the sum...? I'm using the softmax function from a Python programming language called sklearn: def softmax(X, copy=True): if copy: X = np.copy(X) max_prob = np.max(X, axis=1).reshape((-1, 1)) X -= max_prob np.exp(X, X) sum_prob = np.sum(X, axis=1).reshape((-1, 1)) X /= sum_prob return X
