[site]: crossvalidated
[post_id]: 543590
[parent_id]: 116969
[tags]: 
I believe I can answer that, although it is an old one: Boosted Trees are immune to multicollinearity: https://datascience.stackexchange.com/questions/12554/does-xgboost-handle-multicollinearity-by-itself See also the newest implementation of Boosted Trees with EBM from Microsoft: https://interpret.ml/docs/ebm.html The boosting procedure is carefully restricted to train on one feature at a time in round-robin fashion using a very low learning rate so that feature order does not matter. It round-robin cycles through features to mitigate the effects of co-linearity and to learn the best feature function for each feature to show how each feature contributes to the modelâ€™s prediction for the problem. But! As you can see from the first link. The second answer there highlights, that boosted trees can not work out multicollinearity when it comes to inference or feature importance. Boosted Trees do not know , if you for example have added a second feature which is just perfectly linearly dependent from another. The Trees will just say that both features (the original one and the artifical one) are now important maybe they will share the feature importance. just make a simple experiment on that. You will see they can not deal with multicoll. in terms of yeah lets say causality. If you would want such a thing you first need to aggregate features or do a regularization method. Update 2022/1/17 I made an experiment examining the explanatory part of the multicollinearity in boostes trees and gams and decision trees. While for prediction, multicollinearity has no effect, the explanatory part is highly influenced by it. So far, only the EBM offers a handling of multicollinearity due to its round robin procedure. See my other post: How shap values behave in terms of multicollinearity in Trees, Ensemble, GradientBoosting and GAM/Boosting
