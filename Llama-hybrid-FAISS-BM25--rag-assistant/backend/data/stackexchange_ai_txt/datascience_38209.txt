[site]: datascience
[post_id]: 38209
[parent_id]: 38191
[tags]: 
First of all, having this situation isn't very common. If would imply that your training and test data do not originate from the same underlying distribution! That being said, here are a few other thoughts on the matter: If the volatile training accuracies and the good test accuracy occur consistently i.e. they are reproducible - then I would probably be happy with that. Then start adding different types of regularisation to smooth the training loss curves a little. This could be in the form of batch normalisation or dropout, for example. If the test accuracies are all over the place, also varying dramatically, then I would definitely want to sort out the training curves first. Try more data pre-processing, different batch-sizes (perhaps yours is too smnall, hence the volatility) as well as adding regularisation as mentioned above. Another approach might be to try some form of dimensionality reduction of your input data. Mapping the features you have to a latent space, where the information may be more dense and (hopefully) more uniform. Which method to try will greatly depend on your type of data: text, images, videos, sound, temperature, stock-prices and so on. Have a look at things like t-SNE , which will work for most data types, or Word2Vec for text.
