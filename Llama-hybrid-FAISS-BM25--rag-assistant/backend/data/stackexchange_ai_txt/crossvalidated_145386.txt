[site]: crossvalidated
[post_id]: 145386
[parent_id]: 
[tags]: 
Expected ratio of probabilities--is there a term for it?

I recently came across the following quantity when I played around with some information theoretic quantities and Bayesian learning. Given three probability distributions $q(z), p(z)$ and $p(z|x)$. $$\int_z q \log {p(z) \over p(z|x)} dz,$$ where I have sloppily introduced $q = q(z)$ as a shorthand. I wonder if anyone can point me to applications or interpretations of that. Here is what I derived myself. It is relatively straight forward to recognize this as the difference of two cross-entropies: $$\int_z q \log {p(z) \over p(z|x)} dz = \mathbb{H}(q, p(z|x)) - \mathbb{H}(q, p(z)).$$ That is the average number of excess bits needed if $p(z|x)$ is used to encode events of $q$ instead of $p(z)$. E.g. it is a measure of how good $p(z|x)$ is at encoding events from $q$ in comparison to $p(z)$.
