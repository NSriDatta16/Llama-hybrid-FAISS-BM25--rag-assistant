[site]: crossvalidated
[post_id]: 553752
[parent_id]: 391692
[tags]: 
This is an old good question that has not received a complete answer. I believe this is not only about the "kernel trick." I believe this is more about a theoretical result called "the Representer Theorem" [1; Theorem 16.1] of kernel methods which I find rather deep. Short answer Is there an easy way to explain how that works? Namely how you can have a fixed feature map but still retain the same representational degrees of freedom? Yes. There is this fun fact. Let's say the input is two-dimensional: $x = (x_1, x_2)$ as in the question's example. Consider the linear model $f(x) = \phi(x)\theta$ , where $\phi(x) = (1, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2}x_1x_2, x_1^2, x_2^2)$ , and $\theta$ is a column vector taking values in $\Re^6$ . Then, the "optimal" parameter $\hat \theta$ : $$ \hat \theta \in {\arg \min}_\theta \sum_{i=1}^n (y_i - f(x_i))^2 $$ turns out to take the form of $$ \hat \theta = \sum_{i=1}^n w_i \phi(x_i)^\top $$ for some coefficients $w \in \Re^n$ . That is, the "optimal $\theta$ " is actually a weighted sum of just $\phi(x_1)^\top, \ldots, \phi(x_n)^\top$ (negative coefficients are allowed, of course). Replacing this in the definition of $f$ , the "optimal" function looks like $$ \hat f(\cdot) = \phi(\cdot) \hat \theta = \sum_{i=1}^n w_i \phi(\cdot) \phi(x_i)^\top = \sum_{i=1}^n w_i k(\cdot, x_i), $$ where $k(x, y) = \phi(x) \phi(y)^\top = (1 + x^\top y)^2$ . So, even if we start from the usual linear model, as far as we are concerned with such an "optimal" function estimated by minimizing an objective function based on the data, the solution coincides with a kernel linear model. This is an example of a theoretical result (rather deep, I think) called the "Representer Theorem" of reproducing kernel Hilbert spaces. The same holds in much more general setups, where $\phi$ may be infinite dimensional, the objective function maybe something else (it holds pretty much for any loss function, and it does not have to be a sum), and there may be a "regularization" term. Confirmation Let us denote $$ Y = \begin{pmatrix}y_1 \\ \vdots \\ y_n\end{pmatrix}, \quad \Phi = \begin{pmatrix} \phi(x_1) \\ \vdots \\ \phi(x_n)\end{pmatrix}. $$ The optimal parameter of a linear model $f(x) = \phi(x)\theta$ is $$ \hat \theta \in {\arg \min}_\theta \|Y - \Phi \theta\|^2 \\ \qquad\qquad= {\arg \min}_\theta \left\|\begin{pmatrix}y_1 \\ \vdots \\ y_n\end{pmatrix} - \begin{pmatrix}\phi(x_1) \theta \\ \vdots \\ \phi(x_n)\theta\end{pmatrix}\right\|^2. $$ Now, from the first-order condition, $$ \hat \theta = (\Phi^\top \Phi)^{-1}\Phi^\top Y. $$ Inserting $(\Phi \Phi^\top)(\Phi \Phi^\top)^{-1}$ (i.e., the identity matrix), $$ \hat \theta = (\Phi^\top \Phi)^{-1} \Phi^\top [\Phi \Phi^\top (\Phi \Phi^\top)^{-1}] Y = \Phi^\top (\Phi\Phi^\top)^{-1}Y \\ = (\phi(x_1)^\top \cdots \phi(x_n)^\top) \begin{pmatrix}w_1 \\ \vdots \\ w_n\end{pmatrix} = \sum_{i=1}^n w_i \phi(x_i)^\top, $$ where $(w_1, \ldots, w_n)^\top = (\Phi\Phi^\top)^{-1}Y$ . [1] Shalev-Shwartz, S., & Ben-David, S. (2014). Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press.
