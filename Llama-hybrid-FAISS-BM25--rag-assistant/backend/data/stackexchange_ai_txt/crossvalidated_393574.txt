[site]: crossvalidated
[post_id]: 393574
[parent_id]: 393572
[tags]: 
The code is a single autoencoder: three layers of encoding and three layers of decoding. "Stacking" is to literally feed the output of one block to the input of the next block, so if you took this code, repeated it and linked outputs to inputs that would be a stacked autoencoder. Stacked Denoising Autoencoders are a thing for unsupervised/semisupervised learning, I believe. In answer to your comment below, remember that any deep network is created by stacking layers. It's true that if there were no non-linearities in the layers you could collapse the entire network to a single layer, but there are non-linearities and you can't. "Stacking" isn't generally used to describe connecting simple layers, but that's what it is, and stacking autoencoders -- or other blocks of layers -- is just a way of making more complex networks. (For example, it's common in CNN's to have two convolutional layers followed by a pooling layer. These convolutional blocks are stacked.)
