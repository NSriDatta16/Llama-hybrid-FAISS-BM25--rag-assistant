[site]: crossvalidated
[post_id]: 579580
[parent_id]: 579041
[tags]: 
This is a multidimensional problem in which the predictor variables $v_{ijn}$ have three indices: $i$ for subjects/individuals; $j$ for variables/measurements; $n$ for tissue samples. The variable to predict $y_i$ has a single index (subject). I propose a method to collapse the $n$ index. For each subject $i$ and each variable/measurement index $j$ , compute some features of the empirical distribution of the sample of 2000 observations $$S_{ij}=\left\{v_{ijn}: n=1,\ldots,2000\right\}$$ For example, you could compute the minimum and maximum values of the empirical distribution, its mean and standard deviation, some quantiles (e.g., the 0.01 0.05 0.10 0.25 0.5 0.75 0.90 0.95 0.99 quantiles), the proportion of observations that exceed a certain threshold. You repeat the computation of the characteristics of the empirical distribution for each $i$ and each $j$ . By doing so, you obtain a vector of predictors $$X_i=\left[X_{i1} \ldots X_{iK}\right]$$ for each subject. The number $K$ of predictors is equal to the number of characteristics you compute for each empirical distribution times the number of measurements per tissue sample. Then, you can use any standard model that allows you to predict a binary variable $y_i$ (pass/fail) conditional on a vector of predictors $X_i$ (e.g., logit models, XGBoost, random forests, etc.). However, given the relatively small number of subjects, it would seem appropriate to use methods that allow to effectively fight over-fitting. My choice would be averaging of many XGboost or LightGBM runs performed by randomizing along various dimensions (hyperparameters, bagging, feature bagging).
