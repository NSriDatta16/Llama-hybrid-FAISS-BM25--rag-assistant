[site]: crossvalidated
[post_id]: 174413
[parent_id]: 174390
[tags]: 
The goal of maximum likelihood fitting is to determine the parameters of some distribution that best fit the data - and more generally, how said parameters may vary with covariates. In the case of GLMs, we want to determine the parameters $\theta$ of some exponential family distribution, and how they are a function of some covariates $X$. For any probability distribution in the overdispersed exponential family, the mean $\mu$ is guaranteed to be related to the canonical exponential family parameter $\mathbf{\theta}$ through the canonical link function, $\theta = g(\mu)$. We can even determine a general formula for $g$, and typically $g$ is invertible as well. If we simply set $\mu = g^{-1}(\theta)$ and $\theta = X\beta$, we automatically get a model for how $\mu$ and $\theta$ vary with $X$, no matter what distribution we are dealing with, and that model can be easily and reliably fit to data by convex optimization . Matt's answer shows how it works for the Bernoulli distribution, but the real magic is that it works for every distribution in the family. The mode does not enjoy these properties. In fact, as Cliff AB points out, the mode may not even have a bijective relationship with the distribution parameter, so inference from the mode is of very limited power. Take the Bernoulli distribution, for example. Its mode is either 0 or 1, and knowing the mode only tells you whether $p$, the probability of 1, is greater or less than 1/2. In contrast, the mean tells you exactly what $p$ is. Now, to clarify some confusion in the question: maximum likelihood is not about finding the mode of a distribution, because the likelihood is not the same function as the distribution. The likelihood involves your model distribution in its formula, but that's where the similarities end. The likelihood function $L(\theta)$ takes a parameter value $\theta$ as input, and tells you how "likely" your entire dataset is, given the model distribution has that $\theta$. The model distribution $f_\theta(y)$ depends on $\theta$, but as a function, it takes a value $y$ as input and tells you how often a random sample from that distribution will equal $y$. The maximum of $L(\theta)$ and the mode of $f_\theta(y)$ are not the same thing. Maybe it helps to see the likelihood's formula. In the case of IID data $y_1,y_2,\ldots,y_n$, we have $$L(\theta) = \prod_{i=1}^n f_\theta(y_i)$$ The values of $y_i$ are all fixed - they are the values from your data. Maximum likelihood is finding the $\theta$ that maximizes $L(\theta)$. Finding the mode of the distribution would be finding the $y$ that maximizes $f_\theta(y)$, which is not what we want: $y$ is fixed in the likelihood, not a variable. So finding the maximum of the likelihood function is not, in general, the same as finding the mode of the model distribution. (It is the mode of another distribution, if you ask an objective Bayesian, but that's a very different story!)
