[site]: crossvalidated
[post_id]: 293980
[parent_id]: 293978
[tags]: 
Implementations of logistic regression typically use some form of gradient descent for optimization. Conceivably, therefore, you can perform updates by "remember" the state of the descent, and updating as new inputs come along. See, e.g., Lazy Sparse Stochastic Gradient Descent for Regularized Mutlinomial Logistic Regression . This is not the most mainstream approach to online classification, though. E.g., in sklearn , different online classifiers are supported . These work differently. They rely on some aggregating statistics that are relatively small, do not depend on the dataset size, and are amenable for online updates. For example, Naive Bayes just needs the statistics of empirical distributions for the classes.
