[site]: datascience
[post_id]: 97274
[parent_id]: 
[tags]: 
Dataset format for Transformer text-generation

I'm trying to find some tutorials on training Transformer for generating comments on articles. So far, I found an article showing how to train GPT2 as a chat-bot. Input files in that example are given in this format: PERSON1: Something that this person says. PERSON2: Something that p2 answers. PERSON1: Some completely new conversation. PERSON2: This is not related to conversation in line above. And then, when model is trained, and we want to generate some text by using GPT2, we just feed model with PERSON1: Sentence we want to generate reply to. PERSON2: and GPT will then generate the rest of the sentence (after PERSON2: label) that we can use as our reply. Now, in order to generate comments related to articles, the only thing that comes on my mind (if using this format) is to use something like: ARTICLE: Some longer article text COMMENT: some comment posted by person A ARTICLE: Some longer article text COMMENT: some comment posted by person B ARTICLE: Some longer article text COMMENT: some comment posted by person C The problem I'm having here is that ARTICLE text is repeated over and over again (it should be I guess same text, and that will make training dataset too big). Also, what if I have some comments that are reply to some other comment. Should I use the same logic here? ARTICLE: Some longer article text COMMENT: some comment posted by person A COMMENT: reply to A1 by X1 ARTICLE: Some longer article text COMMENT: some comment posted by person A COMMENT: reply to A1 by Y1 As you can see, test data size is growing too fast. Is there any other input format I could use for training the model? I would like for model to be able to post comments to the article itself, or to other comments that are already posted on some article.
