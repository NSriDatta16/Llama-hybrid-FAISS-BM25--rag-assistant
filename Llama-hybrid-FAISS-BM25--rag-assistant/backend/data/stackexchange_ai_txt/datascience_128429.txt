[site]: datascience
[post_id]: 128429
[parent_id]: 128349
[tags]: 
What you are seeing makes total sense. Lasso, Ridge Regression and other regularization techniques are also sensitive to the degrees of freedom of your dataset, as well as to multi-collinearity issues (details below). If you say that you have 1000 features and 2000 samples, I think you may need to explore other solutions for unsupervised feature reduction. Some simple examples are removing features according to a correlation matrix (drop the highly correlated ones) or do a Principal Components Analysis (PCA) to keep the dimensions that explain the majority of the variance. Why does Lasso struggle with this particular problem? If you think about it, when you have many correlated variables, Lasso tends to keep one and discard the rest (sparsity). There may be influence of the combination of the correlated features on the target variable, and Lasso is hence killing it because its optimization space favors sparsity. This would improve if for instance you apply a PCA, keep all components, and rerun the regression; because then the features would be orthogonal (non-correlated) and hence there would not be multi-collinearity. Think about the extreme example where you have 2000 examples and >= 2000 features. In such case, any regular supervised learning model would struggle. The reasons are: Geometrical: there will always be a 2000-dimensional hyperplane that passes through all your 2000 points, so the training loss would be 0. Statistical: there are too few degrees of freedom Algebraic: think about this problem like a system of 2000 linear equations with 2000 variables. If you do not have examples or features which are linear combinations of the others, then you have an exact solution to the problem! Which means, from the data science point of view, total overfitting to the training set. Lasso can handle these cases up to some extent. However it may require carefully tuning the regularization weight, and you may also find difficulties to deal with overfitting.
