[site]: datascience
[post_id]: 24518
[parent_id]: 24510
[tags]: 
This would generally only be a part of a network, used for some task like classification, entity recognition or machine translation. The loss used for this task will guide the learning of the embedding space. This is different from approaches like word2vec or GLoVe where you use context windows to learn an embedding, which is an unsupervised (or maybe self-supervised) approach. The approach that you mention here could still be used in these scenarios, by structuring it with a similar loss as word2vec or GLoVe. One advantage would be that you can leverage word similarity (words that end with -ly have a similar function). However these models are usually trained on incredibly large corpora where these character based similarities are not that relevant, and the computational cost increases quite a bit by incorporating bigger models like LSTMs.
