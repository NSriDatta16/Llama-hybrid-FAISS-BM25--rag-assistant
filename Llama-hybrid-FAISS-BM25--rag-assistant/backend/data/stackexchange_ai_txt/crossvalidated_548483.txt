[site]: crossvalidated
[post_id]: 548483
[parent_id]: 
[tags]: 
Dimension Reduction for High Dimensional Probability Distributions

I have often seen dimension reduction techniques being performed on datasets, such as the famous Iris Flower dataset (below: common visualization techniques applied on a medical dataset): However, I have always wondered if these kinds of dimension reduction techniques (e.g. Principal Component Analysis (PCA), TSNE (T Distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation)) can be used (i.e. are meant to be used) to visualize high dimensional probability distributions . To illustrate my example, I created some some high dimensional data, applied these dimension reduction techniques (using the R programming language) as follows: First, I created the data (5 Dimensional Data): library(Rtsne) library(umap) library(factoextra) library(ggplot2) library(plotly) library(bivariate) var_1 = rnorm(1000,10,10) var_2 = rnorm(1000,10,5) var_3 = rnorm(1000,10,8) var_4 = rnorm(1000,2,2) var_5 = rnorm(1000, 6,1) my_data = data.frame(var_1, var_2, var_3, var_4, var_5) Next, I applied the dimension reduction techniques: 1) PCA (Principal Component Analysis) res.pca 2) TSNE (T Stochastic Neighbor Embedding) tsne 3) UMAP (Uniform Manifold Approximation) umap Next, I performed different visualizations on these dimension reduction techniques: A) Fit Bivariate Normal Distributions (Assume No Covariances): #PCA plot_pca $PC1), mean.X2=mean(pca_out$ PC2), sd.X1=sd(pca_out $PC1) , sd.X2=sd(pca_out$ PC2), ) plot(plot_pca) #TSNE plot_tsne $X1), mean.X2=mean(tsne_out$ X2), sd.X1=sd(tsne_out $X1) , sd.X2=sd(tsne_out$ X2), ) plot(plot_tsne) #UMAP plot_umap $var_1), mean.X2=mean(umap_out$ var_2), sd.X1=sd(umap_out $var_1) , sd.X2=sd(umap_out$ var_2), ) plot(plot_umap) B) Contour Plots ggplot(pca_out, aes(x = pca_out $PC1, y = pca_out$ PC2)) + geom_density_2d_filled() + ggtitle("PCA Plot") ggplot(tsne_out, aes(x = tsne_out $X1, y = tsne_out$ X2)) + geom_density_2d_filled() + ggtitle("TSNE plot") ggplot(umap_out, aes(x = umap_out $var_1, y = umap_out$ var_2)) + geom_density_2d_filled() + ggtitle("UMAP plot") C) Kernel Density Estimates #pca kd $PC1, pca_out$ PC2, n = 50)) plot_ly(x = kd $x, y = kd$ y, z = kd$z) %>% add_surface() %>% layout(scene = list(xaxis = list(title = "PC1"), yaxis = list(title = "PC2"))) %>% layout(title = "KDE of Principal Component Analysis") #TSNE kd $X1, tsne_out$ X1, n = 50)) plot_ly(x = kd $x, y = kd$ y, z = kd$z) %>% add_surface() %>% layout(scene = list(xaxis = list(title = "TSNE1"), yaxis = list(title = "TSNE2"))) %>% layout(title = "KDE of TSNE") #UMAP kd $var_1, umap_out$ var_2, n = 50)) plot_ly(x = kd $x, y = kd$ y, z = kd$z) %>% add_surface() %>% layout(scene = list(xaxis = list(title = "UMAP1"), yaxis = list(title = "UMAP2"))) %>% layout(title = "KDE of UMAP") D) Fit Gaussian Mixture Models to the Reduced Data library(Mclust) #PCA mod1 If you look at anyone of these outputs (mod1, mod2, mod3 - these are all in the same format, I will just show one of them for the sake of brevity) : summary(mod1, parameters = TRUE) ---------------------------------------------------- Gaussian finite mixture model fitted by EM algorithm ---------------------------------------------------- Mclust EII (spherical, equal volume) model with 3 components: log-likelihood n df BIC ICL -2892.281 1000 9 -5846.733 -6978.427 Clustering table: 1 2 3 694 187 119 Mixing probabilities: 1 2 3 0.5167668 0.2738747 0.2093585 Means: [,1] [,2] [,3] PC1 -0.001083778 -0.4748532 0.6238598 PC2 0.319327849 -0.3277565 -0.3594495 Variances: [,,1] PC1 PC2 PC1 0.929556 0.000000 PC2 0.000000 0.929556 [,,2] PC1 PC2 PC1 0.929556 0.000000 PC2 0.000000 0.929556 [,,3] PC1 PC2 PC1 0.929556 0.000000 PC2 0.000000 0.929556 This means that on the PCA Reduced Data, the following multivariate gaussian mixture model was fit to the data: 0.51 * Bivariate_Normal_Distribution_1 [mean = (-0.001, 0.319), cov = (0.929, 0.929)] + 0.27 * Bivariate_Normal_Distribution_2 [mean = (-0.47, 0.319), cov = (0.929, -0.327)] + Bivariate_Normal_Distribution_3 [mean = (0.62, 0.35), cov = (0.929, -0.327)] The likelihood and BIC of this model fit is also recorded (which can be used to judge how well the model fits the data): -2892.281 and -5846.733 My Question Is all this work that I have done unnecessary and meaningless? In the real world, do researchers ever try to reduce the dimensionality of the data, and then try to fit probability distributions to the reduced data? Or is too much information lost during the dimensionality reduction which makes fitting probability distributions to reduced data meaningless? I would have thought that maybe fitting probability distributions to reduced data might reduce computational time - but even if computational time is reduced (i.e. algorithms like TSNE are known to take a long time to reduce the dimensionality of large datasets), if too much information is lost during the dimensionality reduction - is all this still irrelevant? Another disadvantage of fitting probability distributions to reduced data is the loss of interpretability when performing statistical inference on the probability distribution of this reduced data (e.g. how exactly does the original data "map" to the reduced data?) - and the fact that it might be complicated to "map" new data as well (e.g. it's not advised to do this with TSNE). At the end of the day, is it simply better to some Bayesian/MCMC (Markov Chain Monte Carlo) algorithm to fit probability distributions to the original data instead of the reduced data? Thanks! Note: If all else fails, I hope the R code that I have used in this question can at least serve as a general tutorial for other people in the future who want to try similar techniques.
