[site]: crossvalidated
[post_id]: 113055
[parent_id]: 
[tags]: 
Bayes' factor vs. Bayes' Discriminant Rule

When we are comparing two models against some data, will we obtain the same (set of) posterior odds for the models both when we use the Bayes' factor and when we use the discriminant rule? If not, which one is considered more "accurate" and why is the other being used? I have found that the Bayes' factor is described also as: $B_{i,j} = \frac{\pi_{data}(D|M_i)}{\pi_{data}(D|M_j)} = \frac{L_{max}^{(i)}}{L_{max}^{(j)}} \frac{W_i}{W_j}$ where $W_i$ is the Ochham's factor: $W_i = \int_{V_{\theta_{i}}} \pi_{prior}(\theta_i|M_i) \frac{L_{(i)}(\theta_i)}{L_{max}^{(i)}} d\theta_j$ Is $\frac{L_{max}^{(i)}}{L_{max}^{(j)}} \frac{W_i}{W_j}$ anyhow relevant to the ratio of 'allocations' at each model ($\frac{allocations-to-model_i}{allocations-to-model_j}$), made by the Bayes' Discriminant rule? Edit-: After the response of conjugateprior, I would like to add this: I am wondering whether a discriminant analysis (such as ML discriminant rule i.e. Bayesian discr. for priors 1/2 and 1/2) is an acceptable way of doing model selection when we are comparing two models from which we inferred posterior information about some parameter/hyperparameter via Bayes' theorem. I.e. although we assume that the data are generated by one model, if you do discriminant analysis, won't the percentage of the mixture reveal the model that the Bayesians' describe as 'more likely'?
