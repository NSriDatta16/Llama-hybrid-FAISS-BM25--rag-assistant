[site]: crossvalidated
[post_id]: 204613
[parent_id]: 
[tags]: 
Random Forests with modified partitioning criteria

Here is the context of my question : I'm doing binary classification with unbalanced classes. The measure of performance I'd like to maximise is a modified F-measure : $$ F_{\alpha} = \frac{1}{\frac{\alpha}{P}+\frac{1-\alpha}{R}} = \frac{1}{\frac{\alpha}{\frac{TP}{TP+FP}}+\frac{1-\alpha}{\frac{TP}{TP+FN}}}=\frac{TP}{VP+\alpha FP+(1-\alpha)FN}$$ with $\alpha = 0.25$. P=precision ; R=recall ; TP = true positive ; FN = False negative and FP = false positive. This criteria is not to be discussed. Consider it as given. So in order to classify my observations I'm using random forests. In this model, in order to partition the space of predictors two common criterias are used : either the accuracy or the gini indice. At a given step we are looking for the potential predictor which once divided minimizes the most the misclassification error rate or the gini indice (or sometimes the cross entropy). But since my final objective is the $F_{0.25}$ measure, why shouldn't I use it directly as the criteria for partitioning ? That is to say at a given step, looking for the predictor which once divided maximises the $F_{0.25}$. Then two questions : has someone had that kink of idea ? Is there some already implemented variants of random forests with different criteria of partitioning then the usual ones ? Or is it completely a stupid idea..? Hope my explanations are clear enough. Cheers.
