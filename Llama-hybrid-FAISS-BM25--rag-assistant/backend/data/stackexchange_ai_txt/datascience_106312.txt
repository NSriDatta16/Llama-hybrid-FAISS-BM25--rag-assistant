[site]: datascience
[post_id]: 106312
[parent_id]: 
[tags]: 
Why do I get different results at inference time even with fixed seed?

I am a very beginner in deep learning and am playing with voice cloning project. I trained my dataset and used the trained model to synthesize some sentences and was surprised to get a very different output each time I ran the synthesis (output ranging for very good quality to very poor with unintelligible content). I understood that this was due to the initial state of the model that was set up randomly thanks to a random seed, but in the project I use the seed is fixed to 1234 and used to initialize the random generators. Can it be that with the same initial seed the outputs differ at inference time for the same input ? Does it have to do with a bad dataset ? What are the reasons for that ? Thank you
