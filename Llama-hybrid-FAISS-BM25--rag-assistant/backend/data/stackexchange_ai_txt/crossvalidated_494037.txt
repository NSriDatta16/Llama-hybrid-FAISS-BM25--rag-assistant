[site]: crossvalidated
[post_id]: 494037
[parent_id]: 481324
[tags]: 
K_V takes the word (or word-part, but let's assume whole words from this discussion) in embedded space, and changes it. Learning K_V means learning how to change this embedded representation into another embedded representation. In a sense, what you are asking is "why should we learn how to change the embedding of the words? Why can't we use the original embedding space?" A fair question. Consider the following: After the first layer, we don't have the original words, we have words attended to by the attention mechanism (courtesy of the keys and queries, like you wrote). So we no longer have the word 'This', we have a weird hybrid of, let's say, 0.8* this , 0.15* forum and 0.05* is . We don't want the word that has this value in the original embedded space (if any), we want to make sense of this combination that is relevant to question (i.e. to what we are training this to do). Please note that we actually have multiple heads of attention, which means that the original value is broken down and is re-concatenated after attention is applied. In order to make use of the information from the different attention heads we need to let the different parts of the value (of the specific word) to effect one another. This is carried out by the Position-wise Feed-Forward layer, but K_V allows more adaptability of the values, which serves their processing in the FF. While the basic embedding (taking place before the first attention mechanism layer) can be learned specifically to better serve the target question, having learnable W_Vs allows more adaptability to different tasks, as well as allowing you to use the same basic embedding for different tasks (you don't retrain the basic embedding layer when fine-tuning, after all).
