[site]: crossvalidated
[post_id]: 330016
[parent_id]: 330001
[tags]: 
$P(\theta)$ is a prior , the assumed distribution of the parameter $\theta$ . It is not something that you estimate 1 , but something that describes what you know, or assume, about the parameter before seeing the data. If you refuse to make prior assumptions, then you can use so-called "uninformative" priors (e.g. uniform), but no prior is truly uninformative (check here for examples of different "uninformative" priors, that lead to slightly different results). Bayesian would also argue that you always have and can make some prior assumptions . You choose the prior according to your best knowledge, e.g. you know that it should be continuous, positive-valued and skewed, so you choose gamma distribution with parameters that fit your assumption (most of the probability mass is centered on the values you consider most likely etc.). As about choosing your priors, check for example the Eliciting priors from experts thread. It is not true that for continuous random variables the prior is zero, the probability is zero , but the probability density function is non-zero. 1 Unless you are using empirical Bayes approach, where the priors are estimated from the data.
