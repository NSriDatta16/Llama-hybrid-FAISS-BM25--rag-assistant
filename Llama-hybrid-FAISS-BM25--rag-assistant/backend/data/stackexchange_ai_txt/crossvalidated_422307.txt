[site]: crossvalidated
[post_id]: 422307
[parent_id]: 
[tags]: 
Feature Engineering for Multiple Regression

My goal: I want to predict a single output value from multiple inputs, some of which are numerical and some of which are categorical. To do this, I plan on building a multiple-regression model (probably elastic net). In building the model, I think it makes sense to try to capture the individual linear effects of , and non-linear interactions between , my predictors. Let's say I have 2 numerical inputs, a and b . I have an intuition that both a and b has some independent predictive value for my output, y . I also think that perhaps a-b or a/b (or some other algebraic expression) has some predictive value for my output, y . My question: Does it make sense to engineer these potential features and include them in the training dataset when building my model? That is, should I create columns for a-b and/or a/b in the training data, or is it a total waste of computational power since the model already accounts for these potential effects? Analogous case using variables we ought to be familiar with: If I want to predict lifespan from height and weight , does it make sense for me to engineer in the feature of BMI (essentially a ratio of height to weight with an adjustment), or is this redundant with building the model on the component effects of height and weight as well as the interaction effect of height by weight ? Note: If I am using any terminology incorrectly, please forgive my naivety and notify me so that I may fix the question ASAP.
