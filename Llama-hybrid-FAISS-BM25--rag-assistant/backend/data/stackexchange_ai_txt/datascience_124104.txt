[site]: datascience
[post_id]: 124104
[parent_id]: 124020
[tags]: 
Note : this answer assumes that the question is about how to use the Transformer model at inference if there is no output to use At training time, we have the expected output of the model, so there is not a problem, because on the decoder input we use the expected output prefixed with a special (beginning of sentence) token. At inference time, we have no output data. Here, we decode one token at a time: first, we pass as decoder input just the token, and the model generates a single output, which is the prediction for the first token of the output sequence; let's call is $P_1$ . Then, we concatenate such predicted token with the sequence used previously as input to the decoder (i.e. [ ]), obtaining [ $P_1$ , ], and we use it as new input to the decoder. In this second decoding step, the decoder generates 2 predictions: [ $P_1$ , $P_2$ ]. We take $P_2$ and concatenate it to the previous input, obtaining [ , $P_1$ , $P_2$ ]. We repeat the procedure until we obtain as prediction the (end of sequence) token, which marks the end of the predicted sequence.
