[site]: crossvalidated
[post_id]: 316205
[parent_id]: 
[tags]: 
Rigorous threshold determination for heavy-tailed data?

Overview: I'm trying to design a change-point detection system for univariate, non-normal, skewed, heavy-tailed data that I believe is generated by a stable random process (i.e. stable-distributed with typical $\alpha \in (0.6,2]$, $ \beta = 1, -1$ for my data samples.) Question: Is there are theoretically rigorous way to robustly approach threshold ($\tau$) determination for skewed and heavy-tailed data? - I have a couple of ideas so far (listed at the end) based on reading books and publications over the last few days, but they don't strike me as amazing or clearly the "best" answer. - I'd appreciate some guidance in any form (Papers, book references, or straight-up recommendations), because I feel like I'm not looking in the right places. Two sample data cases: A. $H_0$ (i.e., regular process) B. $H_1$ (i.e., abnormal process present in addition to regular process) Constraints: 1. I'm going for lightweight algorithms, so I'm seeking to avoid fitting distributions to the data and Bayesian approaches. 2. Given the known distribution of the data, Gaussian parameters such as standard deviation and variance (and methods of determining thresholds based on these parameters) are non-starters. 3. The lightweight constraint also eliminates the idea of transforming the data to facilitate more traditional Gaussian threshold approaches (which aligns with my assumption and primitive analysis indicating the data is not normal or close to normal). 4. Since the data is heavy-tailed, there will be overlap in nearly all cases. 5. In my opinion, it isn't appropriate to trim outliers because they aren't invalid or corrupted samples. Sample Algorithm: Assume I use a location estimator such as mode or median $M$ for each sample. (Though I will examine MED/MAD and other estimators, and am open to recommendations, that isn't the point of this request.) 1. Determine $M_t$ 2. Determine $\Delta_M = M_t - M_{t-1}$ for each new sample 3. If $\Delta_M 4. Else, label as change Background: 1. I acknowledge that whatever method I start with, the threshold will then be adjusted to optimize performance. So my question remains: What is an appropriate (for the data) theoretical approach that gets me in the initial ballpark? 2. I've spent the last two days searching this site, scholar searches on "robust detection thresholds", "outlier detection", etc. Also read 1 and 2 . Great books and an idea or two, but I didn't recognize anything as directly applicable other than potentially Approach 2. 3. I understand outlier detection is NOT what I'm doing, but thought that thresholds for labeling outliers might be translatable to my problem. Haven't found anything there. 4. There are previous posts regarding heavy-tailed and skewed data, as well as robust methods on this site, but none seem to cover how to theoretically approach thresholds. Approaches I've found or thought of to try thus far: 1. Accept a $P_{fa}$ of 2% and set $\tau$ at the boundary of the largest 2% of the data 2. Apply the Hoeffdinger Inequality 2 to determine $\tau$ - Requires the 1st moment of the data, which is undefined should $\alpha - However, this approach could be forced to work since my data is bounded and I could substitute the median for the mean So, neither of the above seem "great". Hoeffdinger is the only rigorous one I've found. What am I missing? References: 1 "Robust Regression and Outlier Detection," Peter Rousseeuw and Annick Leroy, Wiley 1987. 2 "Outlier Analysis", C.C. Aggarwal, Springer 2013.
