[site]: crossvalidated
[post_id]: 495701
[parent_id]: 
[tags]: 
Why do we not use continuously defined losses in NLP?

I understand that various problems in optimization in NLP which do not exist on continuous tasks such as vision, arise in NLP because we do not have continuous data to predict, but one-hot vectors over a vocabulary, which do not by themselves yield gradients, or, phrased differently, have no information about the similarity between words in the vocabulary. Since we have continous representations of words via word embeddings, why do we not define a loss function on these? For example, we could let our output layer produce the real valued embeddings of the target space, define the loss to be for example the negative dot product (or some other, perhaps more sophisticated metric) with the target embedding. To produce tokens in the vocabulary, we only need to choose the nearest neighbor during inference, for example. I might try this myself but the idea feels so basic that I wonder if somebody has done this?
