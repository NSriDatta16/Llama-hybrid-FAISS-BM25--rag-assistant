[site]: crossvalidated
[post_id]: 168367
[parent_id]: 
[tags]: 
Clustering algorithm advice for extracting key features in sparse data

I have the following dataset: consider a dataset $X$ of $1400 \times 600$. The rows represent households at time $1 \leq t \leq 14$. So I have $100$ households. The columns represent the programs that they have watched on that day and for how long (in minutes). My question is how to "aggregate" the data. The following few ideas came to mind: Take the mean over the days and then aggregate to household level. Take the sum over the days and then aggregate the data. (*) Cluster the household days and take the largest cluster as the most representative viewing statement of that household. I would like to do (*) method, however the question is which algorithm is the most suitable to do this? I have thought up the following trivial algorithm: Take the first vector of the household and then calculate its euler distance with each of the 13 remaining vectors. Aggregate those vectors whose distance is less than $\epsilon$ with the first vector and repeat this process with the remaining vectors. Take the vector that has been aggregated with the most as the representative vector. The problem is choosing an appropriate $\epsilon$. Not sure whether this is a real clustering algorithm (maybe someone can give its name?). My question is now: what existing algorithm can I use for the cluster step? Note: I cannot normalize the data, as far as I know, because even though this data set is rather small. For big data sets, the zero values will become non-zero and we need to have a special way to store it. Note: with sparse I mean lots of zeroes. I looked into the ideas of BellKor's Pragmatic Chaos of the Netflix prize and it seems that they first removed the individual effects and then the program specific effects. I was wondering how they did without having the sparse matrix of netflix scores transformed into a dense matrix.
