[site]: datascience
[post_id]: 117412
[parent_id]: 117411
[tags]: 
Variance part: Despite the somewhat popular notion, data augmentations won't magically fix the lack of data (see Is SMOTE any good at creating new points? ). They could possibly decrease variance somewhat if a model is already doing well. This becomes rather obvious if you try visualizing a bunch of rare observations and potential decision boundaries in a multidimensional feature space. Bias part: The exact method is not relevant as much as the proportion. Resampling violates the major concept of train set having the same distribution as the production one, thus giving your model a bias aimed to offset the bias of the logistic function. There's no rule of thumb for this as far as I'm aware, simply equalizing the populations is not guaranteed to perfectly offset such a bias. See Logistic Regression in Rare Events Data for more info. TL;DR: Data augmentations aren't really there to combat imbalance. If you pursue the metric of certain class on a certain set, that's trial and error (computationally expensive, often yielding an insignificant improvement and prone to drifting in production). If you need a statistically correct model, avoid resampling/weighting when doing augmentations and evaluate using metrics that operate on scores rather than predictions (and aren't class specific). The logistic bias can be offset by selecting the decision threshold, which is most often a single vectorized operation.
