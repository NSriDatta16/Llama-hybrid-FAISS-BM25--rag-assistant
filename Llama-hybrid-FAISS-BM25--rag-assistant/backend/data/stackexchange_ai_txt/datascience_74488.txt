[site]: datascience
[post_id]: 74488
[parent_id]: 
[tags]: 
Tuning parameters for gradient boosting/xgboost

In practice, which parameter do you typically tune first? Do you tune the learning rate (or step size) first? and then tune the total number of iterations? And how do you go about tuning these parameters?
