[site]: crossvalidated
[post_id]: 20914
[parent_id]: 20912
[tags]: 
Check out Leon Bouttou's averaged stochastic gradient. It's all on his site . There is also a good video lecture on it. What he is saying is that in your setting SGD is the best you can do. If you are uncertain about the hyperparameters, run several different configurations on a small (a few thousand examples) subset of your data and check which work best. Having said that, I want to point out that there is a form of Online LBFGS and LBFGS even works if you don't use all the data every step (just 1000 examples for example). Maybe this is 'online enough'. Then there is resilient propagation (which, as opposed to common belief, not only works with neural networks but is an ordinary optimizer). From my experience, you should try out everything. You can never reliably say which method works best a priori.
