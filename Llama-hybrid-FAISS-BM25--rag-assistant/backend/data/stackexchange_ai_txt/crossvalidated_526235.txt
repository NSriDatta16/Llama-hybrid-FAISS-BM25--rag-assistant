[site]: crossvalidated
[post_id]: 526235
[parent_id]: 
[tags]: 
Regression coefficients for (un)known $\sigma_x^2$

I'am currently reading an article A Paradoxical Result in Estimating Regression Coefficients (can be found here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4302277/#S2title ) and at the end of the second paragraph ( 2 Estimating the Slope When the Variance is Known ) there is written: What is driving the superiority of the least-squares estimate over the estimate that uses more information, is that the least-squares estimate is exploiting the dependence between $s_{xy}$ and to obtain a more efficient estimate of $Î²_1$ than the one obtained by using only $s_{xy}$ and knowing $\sigma_x$ . And as I think of it I cannot really find a good explanation why this is the case, precisely why in this case least-square estimate is superior?
