[site]: crossvalidated
[post_id]: 494464
[parent_id]: 268638
[tags]: 
I think that the word "effective" in the accepted answer should be deleted. Because due to the different number of effective parameters, as Aksakal pointed out , the accepted answer implies that Ridge and Lasso are non-parametric, but it doesn't seem to be true. Effective parameters (effective degrees of freedom) are characteristics of a learning algorithm, but not a model itself. In a machine learning problem we have three things: Data generation model. It describes our assumptions about the probabilistic distribution that generated our data. From mathematical statistics we know that data generation model can be parametric or non-parametric . As Glen_b pointed out , in parametric data generation model this distribution is defined by a fixed number of parameters. In nonparametric data generation model we don't have a distribution with a fixed number of parameters, we have milder assumptions about it, like continuity or symmetry. Algorithm (hypothesis) . It is a function $h: \mathcal{X} \to \mathcal{Y}$ from some hypothesis space $\mathcal{H}$ . This function tries to predict the true target value on any sample $x$ . Hypothesis space (model) can be parametric or non-parametric. In parametric hypothesis space (parametric model) every algorithm is uniquely defined by a fixed number of parameters (this number is the same for all algorithms from this space). Simple examples of parametric models are linear regression model: $\mathcal{H} = \{h(x;w,b) = \langle w, x \rangle + b \mid w \in \mathbb{R}^d, b \in \mathbb{R} \}$ and linear (binary) classification model: $\mathcal{H} = \{h(x; w,b) = \mathrm{sign}(\langle w, x \rangle + b) \mid w \in \mathbb{R}^d, b \in \mathbb{R}\}$ . In non-parametric models we can't describe every algorithm with the vector of parameters of the same constant size for all algorithms. Usually the number of parameters grows with the size of a training set. For example in the case of decision trees $\mathcal{H} = \{T(x; \Theta) \mid \Theta\}$ , where $\Theta = \{J, \, \{R_j, \gamma_j\}_{j=1}^J\}$ is a vector of tree's parameters: $J$ is a number of terminal nodes in the tree, $R_j$ are subregions of the input space $\mathcal{X}$ corresponding to these nodes, and $\gamma_j$ are the predictions in these nodes. Trees can have different number of leaves and different number of internal nodes, so the space of decision trees is non-parametric (dimension of $\Theta$ will be different for different trees, if we train them on the datasets generated from the same distribution, that is, with the same number of features $d$ , but with different number of observations in each dataset). Method (learning algorithm). We can formalize it as a function $\mu: D \to \mathcal{H}$ . It uses training set $D$ to fit some hypothesis $h \in \mathcal{H}$ . If $\mathcal{H}$ is parametric we call $\mu$ parametric method. If $\mathcal{H}$ is non-parametric we call $\mu$ non-parametric method. For example, OLS, Ridge and Lasso are all parametric methods because they all use exactly the same parametric model $\mathcal{H} = \{h(x;w,b) = \langle w, x \rangle + b \mid w \in \mathbb{R}^d, b \in \mathbb{R} \}$ (as I said above, we call it "linear regression model"). Despite the fact that these methods have different number of effective parameters. Keep in mind, that we can use parametric data generation model and non-parametric learning algorithm (or vice-versa). For example, we can have gaussian data generation model $Y = f(X) + \varepsilon$ , where $\varepsilon \in \mathcal{N}(0, \sigma^2)$ . Obviously, this is a parametric data generation model. But we can always fit non-parametric method (for example, kNN regression) on the training set $D$ , generated by this model. Similarly, we can fit parametric OLS method without any parametric assumptions about data generation process. In this case this method simply will not be equivalent to Maximum Likelihood Estimation. There is a useful list of parametric and non-parametric methods from Murphy's MLaPP book: Note that non-linear SVM (it is listed in the table) is a non-parametric method, whereas linear SVM (it is not listed in the table) is a parametric method because it fits linear classification model (linear classifier).
