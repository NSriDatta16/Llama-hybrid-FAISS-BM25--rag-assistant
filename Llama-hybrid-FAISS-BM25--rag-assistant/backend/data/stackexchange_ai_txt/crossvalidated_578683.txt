[site]: crossvalidated
[post_id]: 578683
[parent_id]: 578425
[tags]: 
An important concept that should be mentioned in the context of this discussion is that of the Likelihood principle (See also Berger & Wolpert's book ). The likelihood principle, which is one of the foundations of Bayesian statistics, states that all the evidence relevant to a model parameter $\theta$ is contained in the likelihood function $\mathcal L(\theta | x) = f_X(x|\theta)$ . The precise statement of the likelihood principle is that if two experiments about the same parameters $\theta$ are proportional to each other, namely if $$f_X(x|\theta) = cf_Y(y|\theta)$$ with $c$ some positive constant, then the evidence on $\theta$ from the experiments is identical. In this sense, the likelihood is only meaningful (as evidence ) up to a normalizing constant. Indeed in Bayesian statistics those two likelihoods will lead to the same posterior probability of $\theta$ (given the same prior), hence Bayesian statistics 'automatically' respects the likelihood principle. Frequentists statistics in general violates the likelihood principle, so discussion of the meaning of likelihood from a frequentist perspective is somewhat meaningless by itself. But practically most if not all frequentist uses of the likelihood function are via quantities (such as likelihood ratios or the maximum likelihood estimator) that are also invariant under scaling,
