[site]: crossvalidated
[post_id]: 589644
[parent_id]: 
[tags]: 
Finding likelihood given uniform data

I'm using this past paper to pre-study for a module in Bayesian Statistics. It has a question, A precision weighing device yields unbiased measurements within half a gramme, which can be modelled as $\text{Un}(x|\theta-1/2,\theta+1/2)$ , where $\theta$ is the unknown weight. A priori, it is believed $\theta\sim\text{Un}(10,20)$ . Using $\textbf{x} = \{11, 11.5, 11.7, 11.1, 11.4, 10.9\}$ , a set of six independent measurements, find the posterior distribution of $\theta$ . I'd like to reason that \begin{align} f(\textbf{x}|\theta) & = \prod_{i=1}^6f(x_i|\theta) \\ & = \begin{cases} \prod_{i=1}^6\frac{1}{(\theta+1/2)-(\theta-1/2)} = 1 & \text{if } \theta \in \bigcap_{i=1}^6[x_i-1/2,x_i+1/2] = [11.2,11.4], \\ 0 & \text{otherwise;} \end{cases} \end{align} and then if the prior and the normalising constant happen to cancel we'd have something like $\theta|\textbf{x}\sim\text{Un}(11.2,11.4)$ ; but I think this $f(\textbf{x}|\theta)$ only integrates to $0.2$ , so isn't a pdf. Could someone point me in the right direction? Edit: On reflection, I now think the relationship $f(\textbf{x}) = \prod_{i=1}^nf(x_i)$ only holds for ordered data, e.g. $$p((H,T))=p((T,H))=0.5\times0.5=0.25\neq0.5=p((H,T))+p((T,H))=p(\{H,T\}).$$ But eliminating ordering by dividing by $n!$ seems like overkill in the weighing example.
