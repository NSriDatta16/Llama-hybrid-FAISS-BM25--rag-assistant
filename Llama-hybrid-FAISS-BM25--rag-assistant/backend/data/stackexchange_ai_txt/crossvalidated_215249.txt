[site]: crossvalidated
[post_id]: 215249
[parent_id]: 214961
[tags]: 
In practice, I always use cross-validation or a simple trainâ€“test split rather than AIC (or BIC). I'm not too familiar with the theory behind AIC, but two chief concerns lead me to prefer more direct estimates of predictive accuracy: The number itself doesn't tell you much about how accurate a model is. AIC can provide evidence as to which of several models is the most accurate, but it doesn't tell you how accurate the model is in units of the DV. I'm almost always interested in concrete accuracy estimates of this kind, because it tells me how useful a model is in absolute terms, and also how much more accurate it is than a comparison model. AIC, like BIC, needs for each model a parameter count or some other value that measures the model's complexity. It isn't clear what you should do for this in the case of less traditional predictive methods like nearest-neighbor classification, random forests, or the wacky new ensemble method you scribbled onto a cocktail napkin midway through last month's bender. By contrast, accuracy estimates can be produced for any predictive model, and in the same way.
