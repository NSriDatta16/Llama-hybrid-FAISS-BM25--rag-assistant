[site]: crossvalidated
[post_id]: 599165
[parent_id]: 
[tags]: 
Probability Calibration, what causes simple linear models like logistic regression to be over-confident and diverge from the true class probabilities?

I've recently studied probability calibration and have investigated many examples and revisited old models of mine to find that they are all poorly calibrated. The idea of stacking also has been cast in a new light. However intuitive the "calibration" process is, I find it hard to understand why even very simple models become so over-confident. I'd be super grateful if you can share some insights into how this happens, as I would have guessed that many simpler (especially linear) models would reflect the true class probabilities. Many thanks in advance!
