[site]: datascience
[post_id]: 124043
[parent_id]: 
[tags]: 
Preprocessing overheads in Machine Learning

Meta reports that data preprocessing overheads is fast becoming a bottleneck to machine learning training ( https://engineering.fb.com/2022/09/19/ml-applications/data-ingestion-machine-learning-training-meta/ ). While there are several benchmarks for ML training, we haven't found a reliable benchmark for ML ingestion. The open-source ML repositories either perform most of their preprocessing offline, or don't use heavy pre-processing operations for the ingestion to dominate the processing costs. Machine learning practitioners out there: Are there specific ML models where the pre-processing overheads dominate training that you recommend looking at? How much of the overheads is from data storage (data access, decoding, decrypting) vs data transformation operations (cropping images, ngram encoding, etc.)? Is there any specific reason why the data ingestion cannot be done offline? How severe is this problem for inference?
