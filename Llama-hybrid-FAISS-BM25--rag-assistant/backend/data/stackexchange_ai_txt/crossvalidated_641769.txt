[site]: crossvalidated
[post_id]: 641769
[parent_id]: 641703
[tags]: 
Indeed as you rightly conceived that the problem of learning a policy $\pi$ to choose actions is similar in some respects to the function approximation problems usually dealt with supervised learning. In RL the input is a (current) state $s$ in the state space and the output is an action $a$ in the action space. Thus in order to put RL into SL framework each training example would be a pair of the form $(s, \pi(s))$ as you also suggested. However, RL differs from SL in several important respects: In RL training data is not available in this form, instead the environment only provides a sequence of immediate rewards as the agent executes its sequence of actions. Therefore it faces the problem of temporal credit assignment determining which of the actions in its sequence are to be credited with producing the eventual rewards. This cannot be fully captured by a supervised classification model which typically treats each training example independently . By above RL's temporally dependent nature, the agent influences the distribution of training examples fed to itself by the action sequence it chooses, thus the learner faces a tradeoff in choosing whether to favor exploration of unknown states and actions or exploitation of states and actions that it has already learned yielding high cumulative reward. SL learner doesn't have this self-imposed problem at all and if it just chooses exploitation based on current cumulative reward, then it's just similar to overfitting and won't generalize well to unseen situations. RL agents often work with a state space that might be partially observable or have a high-dimensional representation. If the RL agent relies on features or patterns in the state that are not easily captured by the supervised classifier architecture, the classifier might struggle to achieve the same level of performance. For instance a robot only can see what's before its camera at any time step, your SL feature is thus very limited and rigid, while RL can also flexibly use its previous observations and learn a policy to improve its observability via changing angles. Having said that, some imitation learning such as behavior cloning as a branch of RL does use SL framework as referenced here , where the input feature is the learned policy and the target is expert demonstrations. RL also suffer from several drawbacks. First, determining an appropriate reward function that can accurately represent the true performance objectives can be challenging. Second, rewards may be sparse, which makes the learning process expensive in terms of both the required amount of data and in the number of failures that may be experienced when exploring with a suboptimal policy... This chapter will first introduce two classical approaches to imitation learning (behavior cloning and the DAgger algorithm) that focus on directly imitating the policy... Behavior cloning approaches use a set of expert demonstrations... This can be accomplished through supervised learning techniques, where the difference between the learned policy and expert demonstrations are minimized with respect to some metric
