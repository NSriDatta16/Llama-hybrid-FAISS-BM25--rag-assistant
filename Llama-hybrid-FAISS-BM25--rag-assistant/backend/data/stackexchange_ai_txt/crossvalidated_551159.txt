[site]: crossvalidated
[post_id]: 551159
[parent_id]: 551108
[tags]: 
Is it true to some extent that statisticians are usually more concerned about the model's goodness-of-fit and the corresponding metrics of significance, and not that much about model's generalization capability, and vice versa for the ML scientists? No. Measuring generalisation capability is a large portion of statistics practice. Cross validation and bootstrapping techniques addresses the question of how well statistical models generalise and selecting a model in Occam's sense: A survey of cross-validation procedures for model selection . One thing seems to diverge significantly with modern Machine Learning (ML) community, unfortunately, that ML community stop practicing Occam's razor. This is partially because deep learning defy the core paradigms, such as phenomenon of double deep descent . ML community try to establish generalisation via test-train curves on detecting overfitting, i.e., single holdout only. Actually in overfitting, we require two models to compare, not only a holdout on a single model. This is best described by Andrew Gelman , see What is overfitting : Overfitting is when you have a complicated model that gives worse predictions, on average, than a simpler model. However, there is now a significant research activity in deep learning as well to introduce Occam's razor indirectly via Neural Architecture Search (NAS) . It doesn't aim at directly overfitting rather a model compression, but it is actually form of prevention of overfitting in Gelman's definition.
