[site]: crossvalidated
[post_id]: 129998
[parent_id]: 129997
[tags]: 
I was wondering what happens when the number of samples is much more (x200000 times more) than the number of features? Is there any recommended way of reducing the samples' dimensionality? If by 'reduce the samples' dimensionality' you mean 'reduce the number of samples', the simplest thing to do is just pick a random subset of the data and work with that. If you really did mean 'reduce the samples' dimensionality', you might want to look at what're called "online methods" for feature extraction - things like incremental probabilistic PCA or SGD-trained autoencoders.
