[site]: crossvalidated
[post_id]: 285646
[parent_id]: 279740
[tags]: 
After some searching outside of my field, I came across Hedecker and Gibbons 2006 ( http://rtksa.com/library1/wp-content/uploads/2015/11/523.pdf ), who suggest that using a sufficiently large number of polynomials to model time will effectively yield an average over time for the intercept, but this is not the same thing as the mean over all the time values, but, as I suspected, the mean over the x-zero crossings (um, y-intercept, if you like), which will be evenly spaced for each orthogonal polynomial. If you have very many polynomial terms, this is likely very similar to a mean, but for the only three terms that I am interested in, it is too susceptible to random fluctuations, and will change based on which timepoint the zero-crossings happen to be at (such as I observe if I shift the center to the right or left by 20 ms). They suggest, in order to get a mean over time, to create an orthogonal constant (1*sqrt(n)) over all time, to replace the intercept term, which must be held to zero, so as to not have two factors predicting the same thing (and in fact, lmer will not run with both, but always drops one). So I have changed my analyses to reflect that, as such: lmer(looks~0+constant+(poly1+poly2+poly3)*condition*gender*accuracy+(1|stimulus)+(1|subject) and the result gives me essentially the same effects in the interactions, but additionally significant effects where they ought to have appeared for main effect coefficients, if they had represented a mean over time. Does anyone know if this seems right, or am I just taking it upon myself to revise my field's understanding of mixed effects models using orthogonal polynomials? Any additional citions would be helpful, as I feel I am stepping out into uncharted territory, at least as far as my field has gotten.
