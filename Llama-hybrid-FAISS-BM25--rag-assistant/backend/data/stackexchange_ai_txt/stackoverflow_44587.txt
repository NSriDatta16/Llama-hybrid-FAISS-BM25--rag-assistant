[site]: stackoverflow
[post_id]: 44587
[parent_id]: 44569
[tags]: 
From Wikipedia At the time when octal originally became widely used in computing, systems such as the IBM mainframes employed 24-bit (or 36-bit) words. Octal was an ideal abbreviation of binary for these machines because eight (or twelve) digits could concisely display an entire machine word (each octal digit covering three binary digits). It also cut costs by allowing Nixie tubes, seven-segment displays, and calculators to be used for the operator consoles; where binary displays were too complex to use, decimal displays needed complex hardware to convert radixes, and hexadecimal displays needed to display letters. All modern computing platforms, however, use 16-, 32-, or 64-bit words, with eight bits making up a byte. On such systems three octal digits would be required, with the most significant octal digit inelegantly representing only two binary digits (and in a series the same octal digit would represent one binary digit from the next byte). Hence hexadecimal is more commonly used in programming languages today, since a hexadecimal digit covers four binary digits and all modern computing platforms have machine words that are evenly divisible by four. Some platforms with a power-of-two word size still have instruction subwords that are more easily understood if displayed in octal; this includes the PDP-11. The modern-day ubiquitous x86 architecture belongs to this category as well, but octal is almost never used on this platform. -Adam
