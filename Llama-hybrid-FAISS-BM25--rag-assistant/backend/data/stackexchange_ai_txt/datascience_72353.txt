[site]: datascience
[post_id]: 72353
[parent_id]: 72347
[tags]: 
This is a design architecture choice and falls under the popular problem of hyperparameter choice in machine learning. Typically the number of neurons increases as the network depth increases before decreasing again. The motivation behind this is to capture "local features" in the smaller layers before capturing more complex features of the data in the larger layers. Here's an example of the famous AlexNet that was one of the first neural networks to achieve human performance for image recognition: In general "why" and "how" the number of neurons were chosen is really up to the designer and typically decided through empirical testing. The choice of them remains an open question in the field of machine learning.
