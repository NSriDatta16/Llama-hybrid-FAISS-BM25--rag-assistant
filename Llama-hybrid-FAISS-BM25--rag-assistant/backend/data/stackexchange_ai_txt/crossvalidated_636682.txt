[site]: crossvalidated
[post_id]: 636682
[parent_id]: 
[tags]: 
Math problem help: Neural Networks

I am solving this problem from Bishop's 1995 book on Neural Networks: In a multi-layer perceptron a hidden unit has a constant activation for input vectors which lie on a hyperplanar surface in input space given by $w^Tx + w_0 = const.$ , while for a radial basis function network with basis functions given by the equation below, a hidden unit has a constant activation on a hyperspherical surface defined by $\|x - \mu\|^{2} = const.$ . Show that, for suitable choices of the parameters, these surfaces coincide if the input vectors are normalized to unit length, so that $\|x\| = 1$ . The basis functions for the RBF network are of the form, \begin{equation} \label{eu_eqn} \phi_j = exp\left(-\frac{\|x - \mu_j\|^2}{2\sigma_j^2}\right) \end{equation} On the surface, this problem seems quite straightforward to solve. However, I am stuck in the beginning itself, probably because of the language used to express the problem. How is it possible for a sphere and plane to coincide? What am I missing? Would appreciate any help on this problem.
