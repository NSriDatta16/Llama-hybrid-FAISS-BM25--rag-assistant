[site]: crossvalidated
[post_id]: 288278
[parent_id]: 
[tags]: 
Bayesian Linear Regression Example in the Deeplearning Book

I'm reading the deeplearning book authored by Ian Goodfellow about the Bayesian linear regression in Section 5.6 . I have two questions. Please advise me. Thanks! Question 1. Why $p({\boldsymbol {w| X, y}}) \propto p({\boldsymbol { y|X,w}})p({\boldsymbol w})$ as in equation (5.74)? I tried to derive the proportional relationship as such. \begin{align} p({\boldsymbol {w|X,y}}) = \frac{p({\boldsymbol {w, X, y}})}{p({\boldsymbol {X,y}})} = \frac{p({\boldsymbol {y|X,w}})p({\boldsymbol {X,w}})}{p({\boldsymbol {X,y}})} = \frac{p({\boldsymbol {y|X,w}})p({\boldsymbol {X|w}})p(\boldsymbol w)}{p({\boldsymbol{X,y}})} \end{align} That $p({\boldsymbol {w| X, y}}) \propto p({\boldsymbol { y|X,w}})p({\boldsymbol w})$ implies that $\frac{p({\boldsymbol {X|w}})}{p({\boldsymbol {X,y}})}$ is a constant (correct me if I'm wrong). But why whould $\frac{p({\boldsymbol {X|w}})}{p({\boldsymbol {X,y}})}$ be a constant? I could not figure it out. Question 2. Looking at equation (5.76), how on earth does one come up with the definition of $\boldsymbol \Lambda_m$ and $\boldsymbol \mu_m$ and rewriting equation (5.76) into (5.77)? The substitution does not look intuitive nor straightforward to me. I tried to derive equation (5.77) from (5.76) by myself, as follows. \begin{align} {\boldsymbol { -2y^{\top}Xw + w^{\top}X^{\top}Xw + w^{\top}\Lambda_{0}^{-1}w - 2\mu_{0}^{\top}\Lambda_{0}^{-1}w }} ={\boldsymbol { w^{\top}(X^{\top}X + \Lambda_{0}^{-1})w - 2(y^{\top}X + \mu_{0}^{\top}\Lambda_{0}^{-1})w }} \end{align} I am kind of stuck here. How to go any further? The $({\boldsymbol {X^{\top}X + \Lambda_{0}^{-1} }})$ looks like $\boldsymbol \Lambda_m$ but not exactly $\boldsymbol \Lambda_m$. It is the inverse of $\boldsymbol \Lambda_m$. And how about $\boldsymbol \mu_m$? Where on earth does it come from? Below gives the excerpt of the chapter from the book.
