[site]: crossvalidated
[post_id]: 168974
[parent_id]: 
[tags]: 
Why it is popular to use stochastic gradient descent in neural networks rather than the BFGS algorithm?

I have made two solvers to implement neural networks, one is based on stochastic gradient descent (SGD) while the other is based on the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm. I have read a lot of material and find it is common to use SGD rather BFGS, but I have found that BFGS performs better than SGD. Can anyone can tell me why people prefer SGD to BFGS?
