[site]: datascience
[post_id]: 74964
[parent_id]: 
[tags]: 
Neural Net Backprop Weight updating Pseudo code help please

Here is my code for Backpropagation weight updating. It's a simple network with 1 hidden layer and 1 output neuron. The activation function of both hidden and output layer uses tanh. I propagate the error using the gamma variables in the code. The problem is I get very small change in the average error after the first iteration. I'm using initial weights randomly between -0.1 and 0.1 and learning rate of 0.03. Inputs are roughly between -0.001 and 0.001. Target output is between -0.5 and 0.5. I just want to check if the code is correct. I've tried higher values for learning rate and initial weights and the error actually goes up and settles around 1.0. Now is the code wrong or do I just need to fine tune the initial weights and learning rate? Many thanks for your replies! ERROR = 0.5 * MathPow( (TARGET - output), 2 ); double gamma = ERROR * ( 1 - ( output * output ) ); // HIDDEN-OUTPUT LAYER UPDATE... for ( int i=0; i // OUTPUT CODE: // Calculate value of each hidden... for ( int i=0; i
