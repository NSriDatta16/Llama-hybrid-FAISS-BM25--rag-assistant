[site]: crossvalidated
[post_id]: 214163
[parent_id]: 
[tags]: 
Bayesian inference via approximate data likelihood

Suppose that we have a very large i.i.d. sample $x_1,...,x_n$ and a data likelihood defined by $$p(x | \theta,\beta) = \prod_ip(x_i | \theta,\beta)$$. Further suppose that $\theta$ is the parameter of interest and that we have a very informative prior on it $p(\theta)$. $\beta$ are a set of nuisance parameters for which we have no great intuition about what the prior $p(\beta)$ should be, though it is reasonable to consider the prior to be diffuse. One could of course perform Gibbs sampling on the full model to get a sample from the posterior, however I've been thinking about alternatives since computational complexity is a concern. Consider instead observing $$ z(\theta) = \hat{I}^{1/2}(\hat{\theta} - \theta) \sim N(0,1) $$ where $\hat{\theta}$ is the MLE and $\hat{I}$ is the estimated Fisher information. We then have $$ p(\theta | z) \propto p(z(\theta) | \theta) p(\theta). $$ If $p(\theta)$ is distributed normally, then the posterior has a closed form normal solution. Further, the Bayesian central limit theorem assures us that the posterior using $z$ approaches the full data posterior. I've been looking in the literature for a reference doing something like this but have come up empty. Can anyone point me to either a reference or a flaw in my thinking?
