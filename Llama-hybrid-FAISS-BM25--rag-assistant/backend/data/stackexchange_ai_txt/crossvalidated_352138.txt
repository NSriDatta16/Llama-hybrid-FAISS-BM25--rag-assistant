[site]: crossvalidated
[post_id]: 352138
[parent_id]: 263539
[tags]: 
I would like to provide a somewhat dissenting opinion to the well argued (+1) and highly upvoted answer by @ErichSchubert. Erich does not recommend clustering on the t-SNE output, and shows some toy examples where it can be misleading. His suggestion is to apply clustering to the original data instead. use t-SNE for visualization (and try different parameters to get something visually pleasing!), but rather do not run clustering afterwards, in particular do not use distance- or density based algorithms, as this information was intentionally (!) lost. I am well aware of the ways in which t-SNE output may be misleading (see https://distill.pub/2016/misread-tsne/ ) and I agree that it can produce weird results in some situations. But let us consider some real high-dimensional data. Take MNIST data : 70000 single-digit images. We know that there are 10 classes in the data. These classes appear well-separated to a human observer. However, clustering MNIST data into 10 clusters is a very difficult problem. I am not aware of any clustering algorithm that would correctly cluster the data into 10 clusters; more importantly, I am not aware of any clustering heuristic that would indicate that there are 10 (not more and not less) clusters in the data. I am certain that most common approaches would not be able to indicate that. But let's do t-SNE instead. (One can find many figures of t-SNE applied to MNIST online, but they are often suboptimal. In my experience, it's necessary to run early exaggeration for quite some time to get good results. Below I am using perplexity=50, max_iter=2000, early_exag_coeff=12, stop_lying_iter=1000 ). Here is what I get, on the left unlabeled, and on the right colored according to the ground truth: I would argue that the unlabeled t-SNE representation does suggest 10 clusters. Applying a good density based clustering algorithm such as HDBSCAN with carefully selected parameters will allow to cluster these 2D data into 10 clusters. In case somebody will doubt that the left plot above indeed suggests 10 clusters, here is what I get with the "late exaggeration" trick where I additionally run max_iter=200 iterations with exaggeration=4 (this trick is suggested in this great paper: https://arxiv.org/abs/1712.09005 ): Now it should be very obvious that there are 10 clusters. I encourage everybody who thinks clustering after t-SNE is a bad idea to show a clustering algorithm that would achieve comparably good result. And now even more real data. In the MNIST case we know the ground truth. Consider now some data with unknown ground truth. Clustering and t-SNE are routinely used to describe cell variability in single cell RNA-seq data. E.g. Shekhar et al. 2016 tried to identify clusters among 27000 retinal cells (there are around 20k genes in the mouse genome so dimensionality of the data is in principle about 20k; however one usually starts with reducing dimensionality with PCA down to 50 or so). They do t-SNE and they separately do clustering (a complicated clustering pipeline followed by some cluster merges etc.). The final result looks pleasing: The reason it looks so pleasing is that t-SNE produces clearly distinct clusters and clustering algorithm yields exactly the same clusters. Nice. However, if you look in the supplementaries you will see that the authors tried many different clustering approaches. Many of them look awful on the t-SNE plot because e.g. the big central cluster gets split into many sub-clusters: So what do you believe: the output of your favourite clustering algorithm together with your favourite heuristic for identifying the number of clusters, or what you see on the t-SNE plot? To be honest, despite all the shortcomings of t-SNE, I tend to believe t-SNE more. Or in any case, I don't see why I should believe it less .
