[site]: crossvalidated
[post_id]: 570752
[parent_id]: 
[tags]: 
Good classifier for related (or higher level) features?

I'm trying to find good classifier to handle features that related to each other. For example: Features: gender, age, weight, height. Label: healthy or not. (Here I made up the example but hope you understand the idea) I'm using Decision Tree or Random Forest, but looks like training process can't see the relationship between features, e.g. weight and height. Performance is not good. If I add a higher level feature: BMI (Body Mass Index), constructed from weight and height, then the decision tree works very well. So my question is: Does decision tree-typed model works well with related features? What is best algorithm to deal with related data? EDIT: adding an example to demonstrate #### Prepare randomized: w,h; label is is_square df = pd.DataFrame() df['w'] = np.arange(1, 1000) df['h'] = df.apply(lambda row: row.w if random.random() > 0.5 else random.randrange(1, 1000), axis = 1) df['ratio'] = df.w / df.h # constructed feature df['is_square'] = df.w == df.h #### Train base on raw params: w,h X = df[['w','h']] y = df['is_square'] X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3) clf = tree.DecisionTreeClassifier() clf.fit(X_train, y_train) clf.score(X_test, y_test) # score here is just: 0.91 #### Train base on constructed params: ratio=w/h X = df[['ratio']] y = df['is_square'] X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3) clf = tree.DecisionTreeClassifier() clf.fit(X_train, y_train) clf.score(X_test, y_test) # score here is 1.00 You can see if I construct param 'ratio', the performance is better. I guess the construction of 'ratio' required domain knowledge, and it help a lot. My concern is that despite of the data is clean, the decision tree failed to recognize the relationship between w and h (which is 'ratio' in this case)
