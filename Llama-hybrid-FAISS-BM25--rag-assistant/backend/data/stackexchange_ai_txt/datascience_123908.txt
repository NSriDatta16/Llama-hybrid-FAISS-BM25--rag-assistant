[site]: datascience
[post_id]: 123908
[parent_id]: 
[tags]: 
How to get Llama-2 Rotary Embeddings?

I want to get the Llama-2 rotary embeddings. I do print(model) and get the following output: In the picture I highlight the rotary embeddings. How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings? It's possible to get the embeddings as the first hidden-state hidden_state[0] and I want to know, which hidden-state represents the rotary embeddings. Am I right, that there are several rotary embeddings? Thanks in forward. Best regards.
