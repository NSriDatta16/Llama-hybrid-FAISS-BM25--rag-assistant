[site]: crossvalidated
[post_id]: 421429
[parent_id]: 421395
[tags]: 
You've more-or-less described a learning curve : a plot of the (average) performance of a model against varying amounts of training data. As you've suggested, at a certain point, additional data is not going to help you extract more signal from whatever phenomenon you're studying. Learning curves can be useful to make inferences about how much data is required for a specific problem before you reach a point of diminishing returns. Learning curves do not include a step of "overfitting" the data, though. The way that a learning curve works is that for each size of training data, you repeat your procedure of model fitting (e.g. cross-validation to select regularization parameters) and report the best out-of-sample performance of the model selected by your procedure. You may repeat this step several times and take the average, using different training data each time. Then you repeat this process, for all training data sizes of interest. Learning curves are described briefly in Elements of Statistical Learning and How large a training set is needed?
