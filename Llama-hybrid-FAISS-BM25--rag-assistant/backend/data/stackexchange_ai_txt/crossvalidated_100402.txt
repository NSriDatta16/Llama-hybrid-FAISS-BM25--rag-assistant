[site]: crossvalidated
[post_id]: 100402
[parent_id]: 100358
[tags]: 
As @NickStauner and you have surmised , this is due to separation . It is always worth looking at your data! When your data are binary, this is less obvious, but you can see a lot with table() . For example, another problem that causes SEs to expand is multicollinearity (which we think of with continuous variables, but can happen with binary covariates as well). Here's a quick check to see if A is collinear with B : summary(my.data) # X A B # 0:17 1:26 no :23 # 1:32 2:23 yes:26 with(my.data, table(A, B)) # B # A no yes # 1 10 16 # 2 13 10 So, we don't see anything suspicious there. Now we can check for separation: with(my.data, table(A, X, B)) # , , B = no # # X # A 0 1 # 1 0 10 # 2 5 8 # # , , B = yes # # X # A 0 1 # 1 5 11 # 2 7 3 The culprit is that there are no instances of X = 0 when A = 1 and B = "no" . To check, we can add such an observation and re-run the analysis: my.data.a = rbind(my.data, c(0, 1, "no")) tail(my.data.a) # X A B # 45 1 1 yes # 46 0 1 yes # 47 1 2 no # 48 1 1 no # 49 1 1 yes # 50 0 1 no The fake observation shows up in the 50th row. Lets run the analysis and compare the output: model4a |z|) # (Intercept) 18.57 2062.64 0.009 0.993 # A2 -18.10 2062.64 -0.009 0.993 # Byes -17.78 2062.64 -0.009 0.993 # A2:Byes 16.46 2062.64 0.008 0.994 # ... # # Number of Fisher Scoring iterations: 17 summary(model4a) # ... # Coefficients: # Estimate Std. Error z value Pr(>|z|) # (Intercept) 2.3026 1.0486 2.196 0.0281 * # A2 -1.8326 1.1935 -1.535 0.1247 # Byes -1.5141 1.1792 -1.284 0.1991 # A2:Byes 0.1968 1.4804 0.133 0.8942 # --- # Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # ... # # Number of Fisher Scoring iterations: 4 With the fake observation added in, there is no separation in that combination of factor levels, and the SEs look normal. Another indication that separation was to blame is that the Number of Fisher Scoring iterations was very high (17), whereas 4 is more typical of the Newton-Raphson search algorithm . It just kept going further and further out looking for the minimum deviance. Because of the separation, there is no minimum, but eventually the rate of decrease drops below some threshold and the algorithm stops. In that region, the deviance is very flat, so you get very large SEs. Remember that adding a fake observation is not a valid analysis, so throw model4a away! There is an excellent answer discussing how to deal with separation here: How to deal with perfect separation in logistic regression?
