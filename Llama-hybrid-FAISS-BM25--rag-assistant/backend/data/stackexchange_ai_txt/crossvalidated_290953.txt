[site]: crossvalidated
[post_id]: 290953
[parent_id]: 
[tags]: 
Model has high ROC Value in Cross Validation but performs poorly on unseen data

Im running a Random Forest to classify a binary outcome in R. I use k fold Cross Validation to determine the best model features (mtry) and choose the best model based off the highest ROC value. My ROC values are high, see below: mtry ROC Sens Spec 1 0.7874624 0.4661538 0.9264744 2 0.8798629 0.6280128 0.9872436 3 0.9658186 0.7065385 1.0000000 4 0.9788579 0.8607051 1.0000000 5 0.9837602 0.8835256 1.0000000 6 0.9851584 0.8886538 1.0000000 When i run my model with 6 mtry as suggested by the k fold Cross validation and test this model on the test set i get very poor performance, see below confusion mmatrix: actual predictions No Yes No 139 19 Yes 27 2 I thought k fold Cross Validation is a method that can be used to reduce the overfitting issue and guide you in choosing the correct model. However when i pick the best model suggested by k fold Cross Validation the model is extremly poor at predicting unseen data. I have two questions: 1) Is the model not predicting unseen data because the model is overfit to the train data 2) What is the point in k fold Cross Validation if overfitting is still an issue - i.e. why not just use the traditional method of hold one out with a train/test split? Addition: Here is my code whcih shows the variables i am using. modelrf3 enter code here variables: Blue, Green are categorical - colour Mesh and Twine are continuos and measure the size of a fishing net in mm NE and SW are categorical and represent two different seasons (North East and South West Monsoon)
