[site]: datascience
[post_id]: 104140
[parent_id]: 
[tags]: 
Is PositionalEncoding needed for using Transformer models correctly?

I am trying to make a model that uses a Transformer to see the relationship between several data vectors but the order of the data is not relevant in this case, so I am not using the PositionalEncoding . Since the performance of models using Transformers is quite improved with the use of this part do you think that if I remove that part I am breaking the potential of Transformers or is it correct to do so?
