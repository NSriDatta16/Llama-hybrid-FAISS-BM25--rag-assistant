[site]: crossvalidated
[post_id]: 482164
[parent_id]: 
[tags]: 
The effect of multiplying the weighs of a trained neural net with a scalar

Can somebody give us some intuition on why (sometimes) multiplying the weights of a trained neural net with a scalar $s \geq 1$ doesn't changes the accuracy, but multiplying the weights by an scalar $s has a huge impact on the accuracy? For example, here I show results of the scalar vs. accuracy for two tasks: MNIST classification and IMDB sentiment classification. Both models are a combination of CNN+FNN (from https://keras.io/examples/ ) After training --> $W_{new} = scalar*W_{old}$ For example, to run MNIST experiment, run this notebook and after training use this code: original_w = model.get_weights() for scalar in [0.001, 0.05, 0.1, .25, 0.5, .75, 0.9, 1., 5., 10., 50., 100.]: new_w = [scalar*w for w in original_w] model.set_weights(new_w) print("Scallar {} --> test accuracy {}".format(scalar, model.evaluate(x_test, y_test, verbose=0)[1])) You get an output like this: Scallar 0.001 --> test accuracy 0.11349999904632568 Scallar 0.05 --> test accuracy 0.11349999904632568 Scallar 0.1 --> test accuracy 0.3792000114917755 Scallar 0.25 --> test accuracy 0.9836999773979187 Scallar 0.5 --> test accuracy 0.9919999837875366 Scallar 0.75 --> test accuracy 0.9919000267982483 Scallar 0.9 --> test accuracy 0.9918000102043152 Scallar 1.0 --> test accuracy 0.9919000267982483 Scallar 5.0 --> test accuracy 0.9918000102043152 Scallar 10.0 --> test accuracy 0.9918000102043152 Scallar 50.0 --> test accuracy 0.9919000267982483 Scallar 100.0 --> test accuracy 0.9919000267982483 Update: This is not a general case! It's an observation on two classifiers. It seems to me one reason is that as usually learning algorithms encourage the network to keep the weights small. So, the trained model might be close to the smallest possible solution. Hence, multiplying with s > 1 maps it another solution with bigger weights. But, multiplying with s
