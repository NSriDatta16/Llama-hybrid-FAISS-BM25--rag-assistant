[site]: datascience
[post_id]: 86940
[parent_id]: 86856
[tags]: 
Answer Thought I'd elaborate a little on my comments with a demo of different curves. The main thing is that short of fitting a curve backed by theory (which we've discussed and I know isn't an option), I don't think there's any one method which is more 'valid' than the others. With just a single value at each wavelength, using a notion such as error doesn't help, as we can just choose splines so that they pass through the original data points. You could try to use theory to suggest the curve should be smooth (if that is appropriate) but each of the interpolation methods I demonstrate below provide smooth curves, so this is unlikely to be fruitful as you would need to argue some degree of smoothness and it does not sound like that argument can sensibly be made. Illustration of Interpolation Curves To illustrate some of the options for interpolating this data, I've taken the first few data points provided in the question and added a few estimated from the provided plot. Using that data, I fitted a smoothing spline ( smooth.spline ), loess regression ( loess ), cubic spline ( spline ) and polynomial ( lm with poly ). The result can be seen in this plot: I haven't tried to tune any of the methods but you can still see the main differences. The splines are all quite similar. Obviously, they have the advantage of passing through the given data points. You could argue that the 'anchoring' of the spline curves would make interpolating at values close to the original wavelengths more likely to be closer to the 'true' value (but there is no guarantee that would be the case unless some theory can be applied to explain why the curve should be smooth like that). On the other hand, you could consider the polynomial to give some degree of 'regularisation' and so may postulate that, were many data points to be taken, the true values may 'average out' to a smoother curve of this shape. Without more data, it really is a case of choosing the assumptions that you wish to make rather than choosing an objectively better way to interpolate this data. It sounds like gathering more data (repeat readings to average and/or intermediate values) would be the only way to really hone in on a method but I realise that may well not be practicable. Below is the R code I used to generate the plot, in case it's of use. library(ggplot2) library(magrittr) library(tidyr) theme_set(theme_light()) wavelen $wavelen, dat$ coeff) spline_pred $wavelen, y = dat$ coeff, method = "fmm", xout = new_wavelen) # Plot results plot_dat $y, loess_pred, poly_pred, cubic_spline$ y) colnames(plot_dat) % as.data.frame() %>% pivot_longer(-wavelen, names_to = "Method", values_to = "Coeff") dat$Method
