[site]: crossvalidated
[post_id]: 178158
[parent_id]: 178139
[tags]: 
Your question has several unspoken threads. Your question is really about the design of the choice setting leading to a latent class model, not the modeling itself. The fact is that a single, classic, "pat" answer to your question doesn't exist. This is a highly flexible class of models which can be adapted to a huge range of choice situations from observational, "paper and pencil"-type surveys to huge, CATI, conjoint studies involving large numbers of features where the response sets can be everything from pair-wise tradeoffs to multinomial classes. Next, latent class models can involve choice but are not limited to situations involving choice. Choice models comprise a wide class of models of which latent class models are only one topic. (For instance, see Jordan Louvier's paper Discrete Choice Experiments Are Not Conjoint Analysis for a pretty thorough review of the choice modeling field and its distinctions). http://www.sciencedirect.com/science/article/pii/S1755534513700149 That said, your question reflects the confusion, particularly among marketing scientists, about what latent class modeling really is. This confusion is due to a lack of awareness of the earliest work done in the field. The original work stems from Paul Lazarsfeld, at Columbia, back in the 50s while the second probably originated with James Heckman, the Nobel Laureate, in the 80s. Heckman-type methods are what many today think of as the only real use and meaning for the term "latent class modeling." Your question reflects this bias. While both streams deal with replications or repeated measures, Lazarsfeld's creates latent classes from the unsupervised, static cross-classification of categorical factors such as demographics whereas Heckman's approach is more like a finite mixture, supervised regression model which leverages repeated measures for the same subject as well as integrating both categorical and continuously distributed measures. In both cases the key to understanding the LC approach is that it's two-stage: in stage one, a model is built containing an error term or residual. Then, in stage two, any heterogeneity in the residual is partitioned or clustered to create the "latent classes." Variations in how these partitions are created constitute the bulk of the distinctions between different LC algorithms. Lazarsfeld's framework was furthered by the mathematical and educational sociologist, James Coleman, in the 60s but probably saw its greatest extensions in the 80s in the work of the late Clifford Clogg, who developed the software program MLLSA, the first "freeware" version of latent class modeling software. Bill Dillon's work, in the late 80s, on latent discriminant analysis and his software program, LADI (written in Gauss), was next. LADI is still being used by the market research vendor, MAPS, as the methodological basis for their approach to market segmentation. More recently, and other than David Rindskopf, another educational sociologist at CUNY, there has been little in the way of new developments in the "Lazarsfeld" stream. On the other hand, Heckman's framework continues to see wide use and development. Probably the best example of this being the work of the guys at Statistical Innovations including Jorgen Vermunt and Jay Magidson, vendors of the excellent and inexpensive latent class software tool Latent Gold which includes, in its latest release, a latent class approach to modeling hidden markov chains and transition matrices. http://www.statisticalinnovations.com/ (See the "Resources & Support" tab for a bibliography of their many pubs) All of this is probably too roundabout an explanation for your question regarding choice. Hope it helps.
