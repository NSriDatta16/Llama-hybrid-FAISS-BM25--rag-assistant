[site]: datascience
[post_id]: 5998
[parent_id]: 5209
[tags]: 
http://en.wikipedia.org/wiki/Named-entity_recognition#Formal_evaluation : To evaluate the quality of a NER system's output, several measures have been defined. While accuracy on the token level is one possibility, it suffers from two problems: the vast majority of tokens in real-world text are not part of entity names as usually defined, so the baseline accuracy (always predict "not an entity") is extravagantly high, typically >90%; and mispredicting the full span of an entity name is not properly penalized (finding only a person's first name when their last name follows is scored as Â½ accuracy). In academic conferences such as CoNLL, a variant of the F1 score has been defined as follows: Precision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was required, precision for the predicted name is zero. Precision is then averaged over all predicted entity names. Recall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions. F1 score is the harmonic mean of these two. It follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, "scores no points", i.e. does not contribute to either precision or recall.
