[site]: crossvalidated
[post_id]: 475612
[parent_id]: 475611
[tags]: 
I'm going to assume that by multicollinearity you mean a perfect linear correlation between predictors, and the goal of the modelling task is prediction (not causal inference), because that's my first guess based on the wording of your question. In that case, multicollinearity may not be a problem in decision tree or random forest, even if it is a problem in regression. The reason is that decision trees and random forests select variables one at a time. And even if there are multiple variables that are equally good (e.g. same amount of information gain), the algorithm can simply choose randomly between them. However, if the goal is causal inference (or estimating the strength of correlation between individual predictors and the dependent variable itself), then multicollinearity could be a problem. Because variables could be perfect linear combinations of each other, when one variable is chosen for a tree split, it doesn't mean that the other variables were weaker predictors. It could just be that the variable was chosen randomly among multiple variables that had equal predictive power.
