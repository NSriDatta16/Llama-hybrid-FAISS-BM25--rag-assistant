[site]: crossvalidated
[post_id]: 354839
[parent_id]: 354780
[tags]: 
Is this usually a problem, that is worth adressing one way or the other? Am I maybe missing something here? No, it is not a problem, because the Q value is not stored in the replay memory. You store $s, a, r, s'$ When you read from the memory, you re-calculate the target Q value in the update step, as a maximum over possible $a'$ using your current Q-value estimator (in DQN this is typically a snapshot of an earlier learning network). So all the replay memory is giving you is a sample of rewards and next states in order to calculate the new estimates. There is an issue with keeping very old replay memories: They won't necessarily have the same distribution of states and actions as a later policy generates. This is important for neural networks, because they learn to predict on statistics of input and output data. So it is a good idea to limit the amount of replay memory, although the "best" balance between keeping old transition data around and forgetting it is not 100% clear.
