[site]: stackoverflow
[post_id]: 130574
[parent_id]: 
[tags]: 
Decoding letters ('a' .. 'z') from a bit sequence without waste

I seek an algorithm that will let me represent an incoming sequence of bits as letters ('a' .. 'z' ), in a minimal matter such that the stream of bits can be regenerated from the letters, without ever holding the entire sequence in memory. That is, given an external bit source (each read returns a practically random bit), and user input of a number of bits, I would like to print out the minimal number of characters that can represent those bits. Ideally there should be a parameterization - how much memory versus maximum bits before some waste is necessary. Efficiency Goal - The same number of characters as the base-26 representation of the bits. Non-solutions: If sufficient storage was present, store the entire sequence and use a big-integer MOD 26 operation. Convert every 9 bits to 2 characters - This seems suboptimal, wasting 25% of information capacity of the letters output.
