[site]: crossvalidated
[post_id]: 315376
[parent_id]: 194844
[tags]: 
There are several approaches you may use, depending on what assumptions are you willing to make and how much data do you have at hand. I present three possible methods with explantion and code. Note that I calculated only the "probability of returning book on day 25" and "the probability of returning book after noon on day 25", but you can choose arbitrary limits (even a second). Non Parametric Estimation In general, the function you wrote as example is the empirical distribution function $$ \hat{F}_n\left( x \right) = \frac{1}{n}\sum_{i=1}^{n} 1_{\left\lbrace X_i def F(data, x) return np.mean(data Note however that $ \hat{F}_n\left( x \right) $ is still quite discrete, and it is possible to estimate the probabilities to a finer resolution using kernel density estimation . This approach approximates the derivative of $ \hat{F}_n\left( x \right) $ and is quite similar to producing histograms. In essense, you calculate $$ \hat{f}_h \left(x\right) = \frac{1}{n}\sum_{i=1}^{n} \frac{1_{\left\lbrace \left|x_i - x\right| kernel , which affect the smoothness of the resulting function. This method is more complex, and required more data to achieve the same performance as $ \hat{F}_n $. The actual rate actually depends on the underline distribution. Although more complex, this estimator has an implementation in sklearn : from sklearn.neighbors import KernelDensity from scipy import integrate clf = KernelDensity(bandwidth=0.5, kernel="tophat") clf.fit(data) # clf.score returns the log density density = lambda x: np.exp(clf.score(x)) # Probability of returning book on day 25 probability, error = integrate.quad(density, 24, 25) # Probability of returning book after noon on day 25 probability, error = integrate.quad(density, 24.5, 25) Note that you might want to play around with the parameters, and that score returns the log of the probability. Parametric Estimation Now we are ready to assume a model. It's a stronger assumption, but works better for smaller amounts of data (assuming you choose your model wisely). Two possibilities for such simple model are exponential distribution and gamma distribution . Both distribution are modeling continuous waiting times with small differences (exponential models the time until the first event, gamma until the $\alpha$'s th event). Our goal is to find the rate in which the event happens, i.e., how long until the costumer returned the book. (There might be better models, so don't get stuck on this one if it fails to predict properly). Exponential distribution has density $$ f\left(x ; \lambda \right) = \lambda e^{-\lambda x}$$ where $x$ is non-negative. Note that given data points that were generated independently, $$ f\left(x_1,\ldots, x_n ; \lambda \right) = \prod_{i=1}^{n} \lambda e^{-\lambda x} = \lambda^n e^{-\lambda \sum_{i=1}^{n} x}$$ A common estimator for $ \lambda $ using the data is the maximum likelihood estimator (MLE) , that is, the paramter $\lambda$ that maximizes the probability density for the given data. In our case, $$ \hat{\lambda} = \frac{n}{\sum_{i=1}^{n} x_i }$$ After estimating the rate, we can calculate the required probability using the density function. scipy actually provides it for us: from scipy.stats import expon # Note that expon uses scale = 1 / lambda scale = mean(data) # Probability of returning book on day 25 expon.cdf(25) - expon.cdf(24) # Probability of returning book after noon on day 25 expon.cdf(25) - expon.cdf(24.5) Final Remark There is no one size fit all. After estimating, you should test to see if the estimation makes sense. A few advised steps: Plot the histogram to know what to expect Plot the estimated density vs. the density of exponential with the estimated parameter Check the average error between points and their estimation. Preferably, do it on dataset that wasn't used for the estimation task (test set) If high resolutions are important, it might be advisable to convert the data to seconds.
