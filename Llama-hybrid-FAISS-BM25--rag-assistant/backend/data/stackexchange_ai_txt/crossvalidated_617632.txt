[site]: crossvalidated
[post_id]: 617632
[parent_id]: 617626
[tags]: 
Some of the trouble with going by the VIF is that there is more to the story than how the standard errors get inflated by multicollinearity, which is what VIF captures. You could drop some of the variables that cause this variance inflation, but this puts you at a risk of increasing the residual variance. Since the coefficient standard error is a function of both the VIF and the residual variance, it is not a given that taking steps to lower the VIF actually works toward your goal of lowering the standard error; your steps to lower the VIF might be successful but lead to such an increase in the residual variance that the coefficient standard error winds up higher, despite the lower VIF. Another issue to consider is omitted-variable bias. When you drop variables from a regression in order to lower multicollinearity, you risk biasing the estimates of the remaining coefficients. Thus, you can wind up with a low estimation variance but a high estimation bias. You might not want this. You might prefer more variance with less bias to little variance with considerable bias. Consequently, there is much more to the story than just a VIF threshold of, say, $5$ or $10$ like sometimes get recommended in introductory courses. Finally, the usual variance-inflation factor from OLS linear regression does not describe how the standard errors are inflated by multicollinearity in a logistic regression. Generalized variance-inflation factors would be considered for generalized linear models.
