[site]: crossvalidated
[post_id]: 313070
[parent_id]: 
[tags]: 
MSE formula in Neural Network applications

In Neural Network examples that I have seen online - sometimes the Mean Square Error is presented as $$ MSE = \frac{{1}}{2n} \sum_{i}^{n} ( \widehat{y_i} -y_i)^2 \quad (1) $$ and other times $$ MSE = \frac{{1}}{2} \sum_{i}^{n} ( \widehat{y_i} -y_i)^2 \quad (2) $$ Where I guess $n$ is the number of output nodes. Which one is the correct formula to use? Are we minimizing the total error or the "average" error between each output as in linear regression? Also, bonus question: Do we really need to multiply by $1/2$? In my opinion, it is not that much more convenient than multiplying by $2$ when we take the derivative. Thank you Update based on feedback below $$ MSE = \frac{{1}}{n} \sum_{i}^{n} (y_i - \widehat{y_i})^2 \quad (3) $$ Where $y_i$ is the desired Neural Network output, and $\widehat{y_i}$ is the neural network output.
