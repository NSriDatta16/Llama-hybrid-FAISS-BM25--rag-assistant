[site]: datascience
[post_id]: 26267
[parent_id]: 26265
[tags]: 
If you are asking how to reproduce those plots, the following technique works to visualize the decision boundaries for any model: Generate a dense grid of coordinates that fill your plotting area Score each point on your grid Plot your results, coloring observations based on your model's predictions. This serves as the background. Overlay your test data as a scatter plot In your link, they generate the grid using np.meshgrid and constrtuct the background as a contour plot. The lines representing the decision boundaries of each respective 1-vs-all classifier is plotted using a closed form solution for logistic decision boundaries. EDIT: To answer the question in your comment, you don't have a single $X$ dimension, and consequently your model output doesn't correspond to a curve as simple as this: it's a 3D surface. The simplest solution would be to just apply the strategy I suggested earlier (and which is also described in your link) but instead of calling .predict() to construct the background coloration, you'd call .predict_proba() and use a sequential color map (e.g. the default veridis) so color intensity corresponds to your class likelihood, giving you something which should look like this . Alternatively, you could plot surface curves . Both of these solutions are projections of the $Y$ axis, $P(Y|x1,x2)$, onto the $X_1$-$X_2$ plane. If that isn't satisfactory, you could pass scored results through PCA to combine your $X$ dimensions into a single $X$ feature -- call this $PC_1$ -- and then plot $PC_1$ vs. $P(Y|x1,x2)$. Personally I think this is significantly less informative, but it would give you an x vs. y plot.
