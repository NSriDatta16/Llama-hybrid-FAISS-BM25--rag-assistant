[site]: crossvalidated
[post_id]: 510383
[parent_id]: 509691
[tags]: 
The response is said to be the percentage of errors in a test. That being so, it is bounded by 0 and 100. The lower zero bound is biting, meaning evident in the data, but the upper bound of someone getting every question wrong is still there in principle. Regardless of that, by scaling to a percentage the fact that the original data are discrete is being ignored. It's not obvious to me that a negative binomial fits the bill at all. Also, you are focusing on the marginal distribution of the response when the distribution of the response given the predictors is the main deal. This situation of possible zero inflation often arises, one common reason being that the sample mixes different kinds (here, kinds of people). There isn't a simple solution that catches all. If you're studying alcohol consumption in a Western society on a particular day people like me might score zero on that day, but our average isn't zero: ideally you need data on who never drinks to be sure of the right modelling decision. If you're studying tobacco consumption on any day it might be easier to tell the non-smokers apart from the smokers. Here you're closer to the alcohol example, it seems. As far as showing us the data is concerned, I'd prefer a histogram showing the number of questions wrong (0, 1, ...). Yours is too coarse to show well the detail that is of interest. As far as modelling the data is concerned, I would start with treating the number of questions right as a binomial response. To me zero inflation needs to be really obvious to justify the more awkward models it entails, but zero-inflated binomials do exist. (As a teacher, I am more familiar with the opposite: several students get full marks, and you can't tell who has a really excellent understanding or who is just very good.)
