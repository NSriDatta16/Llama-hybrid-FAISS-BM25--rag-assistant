[site]: crossvalidated
[post_id]: 500226
[parent_id]: 500223
[tags]: 
You train your neural network with gradient descent, hence you will find only a LOCAL minimum for your choice of loss. Every time your run your neural network you typically initialize your net differently. Hence it is likely that you will end up in a different local minimum, which may be better or worse than the previous one... This phenomenon just indicates that your optimization problem is not convex, hence has not a single minimum. You can try using a classifier that has a convex objective as e.g. Support Vector Machines.
