[site]: crossvalidated
[post_id]: 337771
[parent_id]: 337677
[tags]: 
Consider a vector $y$ of dimension $k$ and its fitted value $\hat y$. The squared error (SE) is $$ \text{SE}(y)=\sum_{i=1}^k (y_i-\hat y_i)^2 $$ where $y_i$ is the $i$-th component of $y$ and $\hat y_i$ is the $i$-th component of $\hat y$. Now add some more dimensions to $y$, i.e. increase the dimension from $k$ to $k+h$ for some integer $h>0$. Name the variable $z$ and its fitted value $\hat z$. The first $k$ elements of $z$ constitute $y$ but the last $h$ elements are new. If the fitted value of the first $k$ elements of $z$ is $\hat y$, you get \begin{aligned} \text{SE}(z) &= \sum_{i=1}^{k+h} (z_i-\hat z_i)^2 \\ &=\sum_{i=1}^k (z_i-\hat z_i)^2+\sum_{i=k+1}^{k+h} (z_i-\hat z_i)^2 \\ &=\sum_{i=1}^k (y_i-\hat y_i)^2+\sum_{i=k+1}^{k+h} (z_i-\hat z_i)^2 \\ & \geq\sum_{i=1}^k (y_i-\hat y_i)^2 \end{aligned} simply because each square in these sums is nonngegative. This is for one realization of $y$ and $z$ and their fitted values. If you have multiple realizations and their corresponding fitted values, you would take a simple average of $\text{SE}(y)$ vs. a simple average of $\text{SE}(z)$, i.e. $\text{MSE}(y)$ vs. $\text{MSE}(z)$. Since each $\text{SE}(z)$ is at least as great as the corresponding $\text{SE}(y)$, the average (the $\text{MSE}$) will also be at least as great. This holds if the fitted value of $y$ is the same as the fitted value of the first $k$ elements of $z$. (The realizations are exactly equal by the definitions of $y$ and $z$, but the fitted values need not be since they are generated by different VAR models.) But if the fitted value accuracy for particular dimensions increases much when the overall dimension is increased (i.e. the new $h$ dimensions help predict the old $k$ dimensions very well and the new dimensions themselves are not too hard to predict), then it is possible that $\text{MSE}(z)
