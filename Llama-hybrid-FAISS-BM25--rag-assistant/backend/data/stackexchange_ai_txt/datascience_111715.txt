[site]: datascience
[post_id]: 111715
[parent_id]: 101847
[tags]: 
It might depend on your downstream task. For classification I have experienced negligible reduction in the performance metric (F1 macro in my case) while improving the speed of the downstream classifier model. There is a similar report on using PCA on top of transformer based embeddings . You may loose the dimensions (terms) that do not add much variance to your data (PCA finds a projection to maximize variance), but are crucial for your downstream task (because they are showing up in the minority classes, for example). PCA may also help you capture correlations between terms (words that happen in the same document often) without the load of using an n-gram tf-idf.
