[site]: datascience
[post_id]: 26218
[parent_id]: 26209
[tags]: 
As shimao said, that's about what you'd expect. Despite not having many layers, an input size of 512x512 is a large image to be convolving over. The large computation time is likely more due to convolving 64 filters over the large image, instead of the fully connected layers. The network you've put together has a funny information bottleneck in it though. You start off with 64 filters on the original sized image, only decreasing as your image size reduces. As the image passes through your network, the features you're learning become more and more abstracted and complex. Your Conv2D(32, (3, 3)) layer essentially limits the network to learning a 128x128 map of 32 features. Most network architectures double the number of features every time they pool, and most recent imagenet architectures actually ditch the fully connected layers in favour of an average pool over the final feature map, and basically perform logistic regression on the output of that pool. Try starting off with fewer filters, say 16 in your first convolution layer, doubling each time you stride or pool. Do this a few more times than you are, to increase receptive field and decrease feature map size. Do this down to 64x64 or 32x32, which would be 128 or 256 filters. You can use Keras' Global Avg or Max pooling to eliminate fully connected layers as well. That should about double the speed of the network, and I would expect an increase in accuracy at the same time.
