[site]: crossvalidated
[post_id]: 303853
[parent_id]: 
[tags]: 
Cross validation accuracy improved, while test accuracy went down

I did two submissions to Kaggle's Titanic competition. In both models I used Random Forest with Caret's cross validation. During the first run, according to CV, by best model scored 82% accuracy and RF's OOB error was 15%. With Kaggle's hidden data I scored 0.79%. Later, I added a new predictor to my model and run the whole thing again. CV gave me 83%, RF's OOB gave me 14% (both improved by 1%), but when I submitted to Kaggle I got a lower score of 0.77%. Now, I assume I had an overfit with the second model thus scored lower in Kaggle, but why the cross validation's accuracy did the opposite?
