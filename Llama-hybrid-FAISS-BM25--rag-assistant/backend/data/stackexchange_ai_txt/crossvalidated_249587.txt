[site]: crossvalidated
[post_id]: 249587
[parent_id]: 63968
[tags]: 
A couple of ways you could parse the quote (in statistics), assuming optimization refers to (data-driven) model selection: If you care about prediction, you may be better off with model averaging instead of selecting a single model. If you select a model on the same dataset used to fit the model, it will wreak havoc on the usual inference tools/procedures that assume you had chosen the model a priori . (Say you do stepwise regression, choosing the model size by cross-validation. For a Frequentist analysis, the usual p-values or CIs for the chosen model will be incorrect. I'm sure there are corresponding problems for Bayesian analyses that involve model selection.) If your dataset is large enough compared to the family of models you consider, overfitting might not even be a problem and model selection may be unnecessary. (Say you're going to fit a linear regression using a dataset with few variables and very many observations. Any spurious variables should get coefficients estimated close to 0 anyway, so perhaps you needn't even bother selecting a smaller model.) If your dataset is small enough, you might not have enough data to fit the "true" or "best" model for the problem. What does it even mean to do model-selection well, in that case? (Back to linear regression: Should you aim to select the "true" model with the right variables, even if you don't have enough data to measure them all adequately? Should you just pick the largest model for which you do have enough data?) Finally, even when it's clear you can and should do model selection, cross-validation is not a panacea. It has many variants and even its own tuning parameter (number of folds, or train:test ratio) which impacts its properties. So don't trust it blindly.
