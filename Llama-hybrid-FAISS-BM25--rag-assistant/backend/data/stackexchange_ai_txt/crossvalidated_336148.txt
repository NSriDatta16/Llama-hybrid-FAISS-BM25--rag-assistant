[site]: crossvalidated
[post_id]: 336148
[parent_id]: 336112
[tags]: 
So there are a couple of issues here. As correctly pointed out above, you have calculation issues with the errors in the Frequentist model. But you also have another issue, it appears you may be interpreting the Bayesian interval incorrectly. That the interval contains zero is very important in Frequentist models but doesn't mean the same thing in Bayesian models. The Frequentist hypothesis test is $\Pr(data|\beta=0)$. The Bayesian test cannot be $\Pr(\beta=0|data)$ because it is a continuous variable. The $\Pr(\beta=0)=0$ because it is a countable point in a continuum, so it has zero measure. Instead, there needs to be an hypothesized region so that you are actually looking at $\Pr(-\epsilon\le{0}\le\epsilon) Let us imagine that the interval you saw was $(-.05,7)$. This includes zero, but it also includes $-.04$. It could be $-.04$. It could also be $.01$. Let us assume the posterior is denser at $.01$ than $0$. This implies that while zero is a possible solution, it is less likely than $.01$. The Bayesian interval does not say there is a 95% chance the solution may be zero. It says that there is a 95% chance the true value is inside the interval. This substantially differs from the Frequentist interval. The Frequentist interval implies that if you were to repeat the experiment an infinite number of times with an infinitely large population, at least 95% of the intervals created would contain the true value of the parameter. It says nothing at all about this current data set. So the Frequentist interval that included zero, because the null is that it is zero, would imply that you cannot reject the null and therefore should behave as if it is zero. (Using Frequentist decision theory instead of Frequentist inference which could differ in interpretation.) A second issue is how Bayesian and Frequentist tools handle assumptions violations. Frequentist methods, if correctly specified, are optimal methods ex ante. That is they are optimal prior to seeing the data. They may be a terrible method for the particular set of data you have, but on average you cannot do better. This is because they average over the sample space given the null that all slopes are equal to zero. They condition on the null hypothesis; and as such, can become fragile when there is a violation. Some things, like a $\chi^2$ test can handle substantial departures, other things cannot. Bayesian methods, if correctly specified, are optimal methods ex post. They optimally extract information from the specific data set, without making a statement of how they would have performed under another set. It could have poor Frequentist properties. This is because they average over the parameter space, but condition on the data itself and not the model. You actually did not perform a Bayesian hypothesis test. A proper Bayesian test would have considered model selection. The Bayesian equivalent of the "no effect" hypothesis, where $\beta_1=\beta_2=0$ isn't to see if the intervals contain zero, but to run separate regressions for all combinations of possible variables. If the best regression excludes variable B, then variable B has a stated probability of having no effect. Because the Bayesian method is averaging over the parameter space, it is averaging over all possible values for $\sigma^2$. It is sort of creating a composite variance. The issue here isn't so much the estimator as the probabilities. Bayesian probabilities are affirmative probabilities and not conditioned on a null. It becomes an interpretation problem because you have incorrectly specified the model and so incorrectly specified the construction of the probability statements. In the case of heteroskedasticity, the Bayesian solution is to run two classes of regression, one with homoskedasticity and one with heteroskedasticity, though you must specify in the likelihood function how that heteroskedasticity is created. For example, if the heteroskedasticity were a function of time, then the variance would have to be a function of time. With Frequentist methods there is no concern with why heteroskedasticity is present, only if it is or it is not. "Why" is not a Frequentist issue.
