[site]: datascience
[post_id]: 66431
[parent_id]: 49468
[tags]: 
Here's the list of difference that I know about attention (AT) and self-attention (SA). In neural networks you have inputs before layers, activations (outputs) of the layers and in RNN you have states of the layers. If AT is used at some layer - the attention looks to (i.e. takes input from) the activations or states of some other layer. If SA is applied - the attention looks at the inputs of the same layer where it's applied. AT is often applied to transfer information from encoder to decoder. I.e. decoder neurons receive addition input (via AT) from the encoder states/activations. So in this case AT connects 2 different components - encoder and decoder. If SA is applied - it doesn't connect 2 different components, it's applied within one component. There may be no decoder at all if you use SA, as for example in BERT architecture. SA may be applied many times independently within a single model (e.g. 18 times in Transformer, 12 times in BERT BASE) while AT is usually applied once in the model and connects some 2 components (e.g. encoder and decoder). SA is good at modeling dependencies between different parts of the sequence. For example - understand the syntactic function between words in the sentence. AT on the other hand models only the dependencies between 2 different sequences (for example, the original text and the translation of the text). While still the SA may be very good in translation task (see Transformer) AT can connect 2 different modalities (i.e. text and image). SA is usually applied within a single modality but you still can join activations from 2 modalities into a single sequence and apply SA on it. Generally SA mechanism looks like a more general to me as it can do more than AT. You can simulate AT with SA by just replacing/concatenating the input sequence with the target sequence that you want your attention to look at. Some more notes The term Multi Head Attention is often used with SA. But theoretically you can apply Multi Head approach to AT also. The following terms: content-base attention, additive attention, location base attention, general attention, dot-product attention, scaled dot-product attention - are used to describe different mechanisms of how inputs are multiplied/added together to get the attention score. All these mechanisms may be applied both to AT and SA. Key/Query/Value approach of attention calculation is usually applied to SA. But you can use it for AT also.
