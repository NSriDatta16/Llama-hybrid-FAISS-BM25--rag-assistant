[site]: crossvalidated
[post_id]: 446618
[parent_id]: 446484
[tags]: 
For averaging , it doesn't matter: the result is the same because you have the same number of folds (splits) in each repetition. This situation is different from the question whether to average per-fold-figures-of-merit or pool the cases from all folds first because individual folds may have slightly different numbers of cases. There's nothing fundamental that would suggest a case that accidentally happend to end up in a slightly larger fold (split) should have less weight for the final result. I therefore avoid per-fold of figures of merit unless I need them for some specific reason*. Per-fold estimates are subject to both variance due to the tested cases and variance due to model instability whereas per-repetition estimates are subject only to variance due to model instability (exactly the same cases are tested exactly once in each repetition). Any difference you see in the figure of merit between the repetitions is due to instability in the training. Nevertheless, looking at the individual predictions is more sensitive to this instability than looking at the aggregate over $n$ cases. Also, not all figures of merit aggregate by pure averaging. * e.g. for hyperparamter optimiziation, both model instability and uncertainty in the performace estimates due to the limited number of tested cases is important. Variance between folds is often used as a heuristic to gauge these uncertainties. I think we can do better by considering both variances separately, and getting rid of the (unnecessary) confounder split size/number of folds. (I'm working on this, but it's not yet ready for pubication.)
