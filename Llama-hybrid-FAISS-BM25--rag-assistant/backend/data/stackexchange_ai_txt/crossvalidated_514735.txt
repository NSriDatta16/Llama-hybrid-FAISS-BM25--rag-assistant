[site]: crossvalidated
[post_id]: 514735
[parent_id]: 514733
[tags]: 
There's nothing invalid about defining a threshold for classification. Such procedure, however, is non-differentiable, so therefore you won't be able to train that network using that setup (see that you can adopt it for an already trained network if it suits your task). This is simply because the output becomes $$\mathcal F(\mathcal N (X)) = \mathbb 1_{\mathcal N (X)>0.5}= \begin{cases}0, \quad \text{if} \quad \mathcal N (X)\leq 0.5\\1, \quad \text{if} \quad \mathcal N (X)>0.5\end{cases}$$ And the derivative regarding the inputs of $\mathcal F$ are zero everywhere except at the threshold, where it is undefined. If you need to train such a network and don't actually require the probabilities for anything afterwards, just the classification, you can ditch the sigmoid and instead adopt a hinge-loss, Ã  la SVMs. This will give you a differentiable dichotomized output.
