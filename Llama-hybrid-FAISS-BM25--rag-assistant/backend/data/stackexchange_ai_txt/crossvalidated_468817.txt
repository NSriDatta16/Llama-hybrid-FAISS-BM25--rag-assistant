[site]: crossvalidated
[post_id]: 468817
[parent_id]: 468803
[tags]: 
Why the means are different For many functions you have that they do not commute . That means, if you switch the order of the operations then you do not get the same result: $$f(g(x)) \neq g(f(x))$$ This is also the case for the operation/functions that are 'taking reciprocal' and the 'computing the mean'. Another example that you may probably already know is 'the mean of the squares is not equal to the square of the mean'. Sometimes it helps to gain intuition by making some exaggerated example. For instance, let the data be 0.5 and 1.5: $$1=\underbrace{\left(\frac{0.5 + 1.5}{2}\right)^{-1}}_{\text{reciprocal of the mean}} \neq \underbrace{\frac{(0.5)^{-1} + (1.5)^{-1}}{2}}_{\text{mean of reciprocals}} = \frac{4}{3} $$ Why the standard deviation is such a difference You have your number $ a\pm b$ . When you are taking the reciprocal then you do not get (the reciprocal is not an operation with additivity ) $$ \frac{1}{a \pm b} \neq \frac{1}{a} \pm \frac{1}{b}$$ but you could approximately it with a Taylor series . $$ \frac{60}{a \pm b} \approx \frac{60}{a} \pm b \frac{60}{a^2} + \dots {\Tiny \text{higher order terms}}$$ So then you would get for the times $0.981 \pm 0.239$ a translation into roughly: $61.2 \pm 14.9$ . An alternative view from the Taylor series is that you consider the coefficient of variation . That is, the relative variation is $\pm \frac{0.239}{0.981} \cdot 100 \% = \pm 24 \%$ . And when you take the reciprocal then you assume that this coefficient of variation stays roughly the same. (note that with the coefficient of variation you are actually doing the same as the approximation with the Taylor series, and you can only use the coefficient of variation when the assumptions of the Taylor series hold, which are that the coefficient of variation is small and the higher order terms can be neglected). Some more mathematics Let $T_i$ be the $i-th$ time interval and let $R_i= 1/T_i$ be the rate. Then we can show the discrepancy between those mean values based on a Taylor expansion for the time interval terms $T_i$ around the point $\bar{T}$ which results in a polynomial expression in terms of powers of $(T_i-\bar{T})$ $$\begin{array}{} \bar{R} &=& &&\frac{1}{n} \sum\limits_{1 \leq i\leq n} R_i \\ &\approx & &&\frac{1}{n} \sum\limits_{1 \leq i \leq n} \frac{1}{\bar{T}}& - &(T_i - \bar{T_i}) \frac{1}{\bar{T}^2}& +& (T_i - \bar{T_i})^2 \frac{1}{\bar{T}^3} + {\Tiny \text{higher order terms}} \\ &\approx& \frac{1}{\bar{T}}&+ & \frac{1}{n} \sum\limits_{1 \leq i \leq n} &- &(T_i - \bar{T_i}) \frac{1}{\bar{T}^2}& +&(T_i - \bar{T_i})^2 \frac{1}{\bar{T}^3} + {\Tiny \text{higher order terms}} \\ &\approx& \frac{1}{\bar{T}}&+ & \frac{1}{n} \sum\limits_{1 \leq i \leq n} &- && +&(T_i - \bar{T_i})^2 \frac{1}{\bar{T}^3} + {\Tiny \text{higher order terms}} \end{array}$$ So you could see $\bar{R} \approx 1/\bar{T}$ as a sort of mean of a zero order approximation $R_i = 1/T_i \approx 1/\bar{T}$ . A first order approximation will be the same since the sum over the first order terms will equal zero. A second order approximation will be: $$\bar{R} \approx 1/\bar{T} + \sigma_T^2/\bar{T}^3$$ In your case $60÷0.981+60÷0.981^3×0.239^2 \approx 64.8$ only brings you a bit closer. I guess that this is because you have some few but large outliers, which requires the inclusion of higher order moments. You would have to flesh this a bit more out depending on how you wish to take the average. E.g. this average is an average over the number of measurements, but maybe you want an average over time. Some more about your algorithm Whether you use the one method or the other is not so very much relevant. If it is just about computing the average heart rate then I would use the formula $$ \text{average heart rate} = \frac{\text{total heart beats}}{\text{total time}}$$ and this corresponds to the first method $1/\text{average time}$ . But you have probably three issues that are reasons for doing more: One issue is that you might be missing heart rates due to the measuring apparatus not recording accurately. Looking at the interval times which are sometimes nearly the double of the other times, I assume that this is not an irregularity in the heart rate but more like a pulse that had not been recorded. I am not sure about this, I do not know whether the heart has such clear skipping of beats at exact intervals. But if this is not normal behavior and you know that such large irregularities should not be there, then you can augment the data with heart beats at the places where you know there should be beats. In any case, these large variations in heart beat times makes that you might want to consider more than just the variance of the heart beats, since the variance of the times/rates is very much dominated by these skipped beats and you might want to separate it and study the variance in the non-skipped beats separately. The second issue is that probably you are not just interested in the average heart rate, but also in the variation of the heart rate (because it might signify things like state of condition, health and stress). For this purpose you can use whatever suits you best, either the frequency or the time. I would choose what is most intuitive. If you are interested into studying variations of the heart beats then you might want to study a map of the times plotted as $t_n$ versus $t_{n+1}$ (I guess that this could be called a recurrence or lag plot or Poincaré plot, although I remember a heated discussion on this site where this was contested). Here is an example for your small table, it doesn't look so great yet, and you might want to increase the number of data, but it gives the idea: Potentially there might be patterns in consecutive heart beats that could provide you with information (one thing is also that the neighboring interval times will have a negative correlation when the time of measurement is not so accurate, because a signal that is delayed will cause a longer and a shorter time interval next to each other). More ways to analyse this is with an autocorrelation function, but I guess that this plot should be sufficient. A third point is that you may not want to use the simple formula, which uses all the data together, and instead you want to create something like a heart rate monitor that provides an instantaneous number. For this case you could fit the heart rate (the inverse of time) with some smooth function (and be sure to correctly take care of the missing beats) and use the value from the fit to display the current rate. An example is also occurring in speed meters for ergometers where the pace of a rotating wheel is measured by pulses. Those pulses may be variable due to variations in transmission/measuement of the pulses, but the speed can be expected to be some reasonably smooth curve. So then we can fitted the pulses with a smooth curve from which the actual speed/pace/rate is computed. An open source example can be found in the Strokedatas::fit function here: https://sourceforge.net/p/chironrowing/code/HEAD/tree/src/calculation/stroke.cpp#l256 (it is an old piece of rusty code) Rate versus time intervals As previous mentioned. If the variations are small then the inverse of mean time intervals is not so different from the mean rate. But they are different. So, if you have larger variations, then you better match the use of mean rate/interval to the goal of the analysis. For example to take the sports as example. Often people make use of time per distance. (e.g. lap times or some other reference distance) The total time for a larger distance is in that case related to the average time. However, if some other parameter is of interest, for instance total amount of used energy, then the use of rate/velocity (or some derivative value, the cube of velocity) might be better to use.
