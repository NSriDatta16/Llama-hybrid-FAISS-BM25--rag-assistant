[site]: datascience
[post_id]: 36056
[parent_id]: 
[tags]: 
How do I feed three-dimensional input layer into a Neural Network?

I have training data where each example is a 12x3 two-dimensional array. Let's say I have 1000 of these training examples so my input matrix is a three-dimensional 1000x12x3 matrix. I am trying to feed this into a Neural Network (similar to the one outlined in the diagram below). However the first layer of the neural net is expecting the input matrix to be two dimensional since the hidden layer with which it will be multiplied is also two-dimensional. How do I resolve this? Should I force each input example to be single dimensional, by flattening it out into a 1x36 array? Or should the hidden layers themselves be three-dimensional in order to match the input?
