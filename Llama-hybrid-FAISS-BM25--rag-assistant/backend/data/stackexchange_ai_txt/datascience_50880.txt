[site]: datascience
[post_id]: 50880
[parent_id]: 50873
[tags]: 
Yes, you can have a perfect autoencoder if the number of hidden units is the same as the input. In this case, you could simply make the hidden neuron h_k consider the input i_k and ignore all other inputs (i.e., simply pass forward input i_k), and with linear activation functions you would get a perfect autoencoder. It seems to me, though, that your problem is more related to the training process. As the weights are randomly initialized, it looks like the training may be getting stuck in a local minimum. I think we would need to know more details about the training parameters you are using. Update: I'm thinking now that this looks like a convex problem, so there should be no local minimum. What optimizer and training parameters are you using?
