[site]: datascience
[post_id]: 64941
[parent_id]: 24263
[tags]: 
One reason for the poor word embeddings could be the small number of documents. I think it's worth trying pre-trained word vectors (e.g. google news vectors: https://github.com/mmihaltz/word2vec-GoogleNews-vectors ). You do mention a large number of domain-specific words, which could be an issue, but these vectors are trained with a very large vocabulary. Otherwise I would second the idea of using something simpler like tf-idf scaled word frequencies, maybe limiting the vocabulary and reducing dimensionality.
