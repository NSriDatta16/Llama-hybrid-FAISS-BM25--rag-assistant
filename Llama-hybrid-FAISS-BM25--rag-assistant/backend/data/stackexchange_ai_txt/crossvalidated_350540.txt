[site]: crossvalidated
[post_id]: 350540
[parent_id]: 40876
[tags]: 
The answers above have already covered what I want to say. Just to clarify a few points as a researcher of machine learning: link function is nothing but the inverse of the activation function. For example, logit is the inverse of sigmoid, probit is the inverse of the cumulative distribution function of Gaussian. If we take the parameter of the generalized linear model to only depend on $w^T x$, with $w$ being the weight vector and $x$ as the input, then the link function is called canonical. The discussion above has nothing to do with exponential family, but a nice discussion can be found in Christopher Bishop's PRML book Chapter 4.3.6.
