[site]: crossvalidated
[post_id]: 191886
[parent_id]: 
[tags]: 
Confusion about difference between policy gradients of trajectories under stochastic/deterministic policies

(If this isn't the right SE I'm sorry! Please point me in the correct direction) I'm reading this survey on policy search in robotics , but I'm having a tough time with one part. I'm going to assume the notation is self-explanatory here, but you can check the survey if things don't make sense. As the paper states, under a stochastic policy the probability of a trajectory $\tau$ is $$ p_\theta(\tau) = \prod_t p(x_{t+1}|x_t, u_t) \pi_\theta(u_t|x_t, t) $$ So compute the gradient of the log gives $$ \nabla_\theta \log p_\theta(\tau) = \sum_t \nabla_\theta \log \pi_\theta(u_t|x_t, t) $$ Here the terms $\log p(x_{t+1}|x_t, u_t)$ don't depend on $\theta$ so they drop out, meaning we don't have to worry about the system dynamics. Whereas for a deterministic policy we have $$ p_\theta(\tau) = \prod_t p(x_{t+1}|x_t, \pi_\theta(x_t, t)) $$ Now we have a terms like $$ \nabla_\theta \log p_\theta(x_{t+1}|x_t, \pi_\theta(x_t, t)) = \left. \frac{\partial \log p_\theta(x_{t+1}|x_t, u_t)}{\partial u_t} \frac{\partial u_t}{\partial \theta} \right |_{u_t = \pi_\theta(x_t,t)} $$ So we have to worry about the system dynamics. This makes sense on the surface to me, but digging deeper I'm not really sure... Couldn't we imagine a "stochastic" policy which only has probability mass at one $u_t$ (and is therefore equivalently deterministic)? Also, as I've been relooking at it, I'm not sure why the system terms actually fall out in the stochastic case. Why is $\frac{\partial u_t}{\partial \theta} = 0$ here only? Changing $\theta$ certainly affects the draws of $u_t$. I'm assuming there's some issue with the difference between random/constant variables here that's throwing me off.
