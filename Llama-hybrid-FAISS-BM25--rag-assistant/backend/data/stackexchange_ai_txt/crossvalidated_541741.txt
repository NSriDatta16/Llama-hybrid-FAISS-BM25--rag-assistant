[site]: crossvalidated
[post_id]: 541741
[parent_id]: 541738
[tags]: 
There are many reasons for using squared differences, including A relationship to the Gaussian distribution where squared differences play a central role (the normal distribution's probability density function is the anti-log of a scaled squared difference) If a distribution is not too heavy tailed, it is a good idea to give a lot of weight to values far from the mean (e.g., by squaring the difference from the mean), and other weighting schemes will result in relative inefficient estimators $r^2$ is scaled from 0 to 1 and in the case where there are multiple predictors in a linear model, semipartial $R^2$ is tied to the partial sum of squares explained by each predictor. In other words, sums of squares can be partitioned into component effects that "add up". In the case of uncorrelated predictors, regression sums of squares and semipartial $R^2$ add up to the whole amount of explained variation due to the combination of predictors. There is no corresponding partitioning of explained variation on the absolute difference scale. Variances and covariances have a long history and many probability models are stated in terms of them (e.g., the multivariate normal distribution). As I discuss in Regression Modeling Strategies an absolute difference measure $g$ based on Gini's mean difference (average absolute difference over all possible pairs of values) can be used to quantify strength of association, and I believe that average absolute differences are easier to interpret. But they don't partition. The $g$ -index for a subset of predictors can be larger than the $g$ -index for the entire linear predictor $X\hat{\beta}$ . On the other hand semipartial $R^2$ for a subset of a multivariable model has to be no greater than the overall model $R^2$ .
