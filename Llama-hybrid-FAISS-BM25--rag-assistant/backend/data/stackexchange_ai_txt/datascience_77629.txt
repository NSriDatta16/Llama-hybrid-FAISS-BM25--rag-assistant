[site]: datascience
[post_id]: 77629
[parent_id]: 77422
[tags]: 
I think that @mirimo's idea of having a regularized model as an offset is very interesting. My proposal is a slight variation where you ensure you don't overfit. The idea is, to obtain the model for group $j$ , train a model with all groups except $j$ and use that model as an offset to the model for group $j$ . This way, we can have a complex model for the general behavior and still not train on the same target twice, thus having a more stable model. The downside is that this is way slower, as, if there are $J$ groups, it takes around $J$ times more than the regular training. Edit On top of @Carlos Mougan proposal, we can: Train a global model Train a specific model for each country Ensemble both models The ensemble can have some shrinkage, like: $$y_{final} = \frac{y_{global} \cdot m + y_{country} \cdot n_{country}}{m + n_{country}} $$ where $y_{country}$ is the prediction of the country-specific model, $y_{global}$ the global prediction, $n_{country}$ the number of samples in a country and $m$ an hyperparameter to tune, the higher the $m$ the more we trust on the global model. I think this shrinkage is very relevant to the problem.
