[site]: datascience
[post_id]: 81521
[parent_id]: 
[tags]: 
Dealing with high dimensionality datasets

I have data of dimensionality (25000, 100, 500) i.e. 25000 rows each consisting of a 2 dimensional 100 X 500 matrix. Currently I am only applying CNN for classification purpose. Is there any other way I can model this data? I flattened the 2d matrices into 1d arrays with shape (50000, ) and applied PCA on the (25000, 50000) dataset, I am getting around 85% explained variance for 400 components. Are PCA and LDA still good for such high dimensional datasets? (Other similar questions on stack do not address this problem) Is there any other way I can extract 1 dimensional features so I can apply models like SVM, XGBoost? (for example: CNN to extract flattened 1-d features is one way)
