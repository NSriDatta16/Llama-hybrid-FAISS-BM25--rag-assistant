[site]: crossvalidated
[post_id]: 575402
[parent_id]: 574457
[tags]: 
To facilitate the analysis, let $\mathbf{x} = (\mathbf{x}_1,...,\mathbf{x}_n)$ denote the matrix of observed sample vectors and let $\mathbf{x}_i = (x_{i,1},...,x_{i,K})$ denote the individual sample vectors. We will also let $\dot{\lambda} \equiv \sum_{k=1}^K \lambda_k$ denote the parameter sum. The likelihood function in this case is: $$\begin{align} L_\mathbf{x}(\boldsymbol{\lambda}) &= \prod_{i=1}^n \text{Dir-Mu}(\mathbf{x}_i | \boldsymbol{\lambda}) \\[6pt] &\propto \frac{\Gamma(\dot{\lambda})}{\Gamma(n + \dot{\lambda})} \prod_{i=1}^n \prod_{k=1}^K \frac{\Gamma(x_{i,k} + \lambda_k)}{\Gamma(\lambda_k)}, \\[6pt] &\propto \frac{\Gamma(\dot{\lambda})}{\Gamma(n + \dot{\lambda})} \prod_{k=1}^K \frac{1}{\Gamma(\lambda_k)} \prod_{i=1}^n \Gamma(x_{i,k} + \lambda_k), \\[6pt] \end{align}$$ and the prior kernel is: $$\begin{align} p(\boldsymbol{\lambda}) &= \prod_{k=1}^K \text{Gamma}(\lambda_k | \alpha, \beta) \quad \quad \quad \quad \quad \quad \ \ \\[6pt] &\propto \prod_{k=1}^K \lambda_k^{\alpha-1} \exp(-\beta \lambda_k). \\[6pt] \end{align}$$ Consequently, the posterior kernel is: $$\begin{align} p(\boldsymbol{\lambda} | \mathbf{x}_1,...,\mathbf{x}_1) &\propto L_\mathbf{x}(\boldsymbol{\lambda}) \cdot p(\boldsymbol{\lambda}) \\[6pt] &\propto \bigg[ \frac{\Gamma(\dot{\lambda})}{\Gamma(n + \dot{\lambda})} \prod_{k=1}^K \frac{1}{\Gamma(\lambda_k)} \prod_{i=1}^n \Gamma(x_{i,k} + \lambda_k) \bigg] \bigg[ \prod_{k=1}^K \lambda_k^{\alpha-1} \exp(-\beta \lambda_k) \bigg] \\[6pt] &\propto \frac{\Gamma(\dot{\lambda})}{\Gamma(n + \dot{\lambda})} \prod_{k=1}^K \frac{\lambda_k^{\alpha-1} \exp(-\beta \lambda_k)}{\Gamma(\lambda_k)} \prod_{i=1}^n \Gamma(x_{i,k} + \lambda_k). \\[6pt] \end{align}$$ This is a nasty-looking posterior kernel, and the corresponding scaling value leading to the posterior density does not have a closed form. There are a number of ways you could estimate the scaling value to get the posterior density (e.g., importance sampling, numerical integration methods, etc.), or create algorithms to sample directly from the posterior (e.g., MCMC methods).
