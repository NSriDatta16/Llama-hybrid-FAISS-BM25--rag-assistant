[site]: crossvalidated
[post_id]: 80982
[parent_id]: 80943
[tags]: 
Measures such as WCSS (aka SSQ, variance), Silhouette etc. must only be used to compare results of multiple runs of the same algorithm. Comparing different algorithms with such an internal measure is misleading . The issue here is that you are more likely to measure which algorithm is more closely related to the concept of evaluation , not necessary which produces the better clustering result! This is best seen by trying to compare $k$-means performance for different $k$. You'll notice that when increasing $k$, the WCSS values only get better and better. But in practise, the result doesn't really improve anymore, but it degenerates to the trivial solution "every object is its own cluster" (which has WCSS 0, and is "optimal" for this measure!) Which is why Silhouette comes into play. It's an orthogonal measure, that $k$-means doesn't optimize. Since different $k$-means runs don't use this measure, and it doesn't just trivially improve with $k$, none will have an unfair advantage, and you can reasonably use Silhouette to compare different $k$-means even with different $k$. But next, you'll have someone make an "improved $k$-means algorithm" that always returns the result with the best silhouette, and thus appear to always outperform $k$-means; but not so much because the results are of higher quality, but because they know how you measure "quality". Try different normalization , and their effect on your results and scores. You will be surprised how sensitive both internal evaluation and algorithms are wrt. normalization. Are you sure that none of the algorithms performs implicit normalization? Some k-means implementation will first perform PCA to "whiten" your data set (rescale and rotate your data to have unit variance and no covariances) Furhtermore, DBSCAN for example has the concept of noise . In many implementations, this means it produces a "cluster" (which isn't actually a cluster) containing all the elements DBSCAN considers to be "outliers". Using a measure such as WCSS, Silhouette etc. on the noise "cluster" is just plain wrong . Of course this cluster will have a massive variance, because it has all the outliers! An alternate "interpretation" is to make each element in this cluster its own one-element-cluster for evaluation. But then you have hundreds or thousands of clusters, and a really low WCSS socre; but this also does not reflect reality. Cluster evaluation is an art , and I would strongly advise against trusting a single mathematical number to capture this. Instead, you should evaluate whether the result is useful in practise . In particular, make sure that: The measures you use are appropriate for the algorithm (neither WCSS nor Silhouette are appropriate for DBSCAN, for example, and at least WCSS is also a bad idea to use with hierarchical clustering) They can capture all formal concepts of the algorithms (for example "noise" as returned by DBSCAN, or soft assignments as produced by EM and Fuzzy C-means) The measures are not related to the algorithms. $k$-means optimizes WCSS, so you must not use WCSS to compare other algorithms to $k$-means (or compare different $k$). $k$-means by definition must win (algorithms may get stuck in a local minimum; but the $k$-means problem is defined as finding the best WCSS partitioning). Don't use a measure to evaluate DBSCAN which does not have explicit "noise" treatment. Don't assume "noise" is a cluster; it's not, even when many implementations return a noise partition (often the first or last "cluster" is the noise). Similarly, for EM and fuzzy c-means, use an evaluation procedure which allows "fuzzy" aka "soft" assignments (partial cluster membership). Try different normalizations; and make sure the algorithms you evaluate don't perform an automatic normalization which makes it unfairly harder for them to find anything. For example, some algorithms rescale the data to $[0;1]$. Putting a single far-away outlier will make this normalization to behave really badly, removing the outlier will substantially improve results. You may want to read the Wikipedia article, which has a section on Evaluation . P.S. instead of Weka, you may want to use ELKI which is much more powerful for clustering. If you have labeled data, it will automatically produce a number of common evaluation measures (but you'll mostly see that there are too many measures). It currently does not seem to include internal measures such as WCSS or Silhouette.
