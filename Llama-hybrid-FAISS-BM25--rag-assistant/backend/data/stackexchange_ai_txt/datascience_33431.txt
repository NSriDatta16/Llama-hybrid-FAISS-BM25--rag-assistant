[site]: datascience
[post_id]: 33431
[parent_id]: 32873
[tags]: 
DQN suffers intrinsically from instability. In the original implementation, multiple techniques are employed to improve stability: a target network is used with parameters that lag behind the trained model; rewards are clipped to the range [-1, 1]; gradients are clipped to the range [-1, 1] (using something like Huber Loss or gradient clipping); and most relevant to your question, a large replay buffer is used to store transitions. Continuing on point 4, using fully random samples from a large replay buffer helps to decorrelate the samples, because it's equally likely to sample transitions from hundreds of thousands of episodes in the past as it is to sample new ones. But when priority sampling is added into the mix, purely random sampling is abandoned: there's obviously a bias toward high-priority samples. To correct for this bias, the weights corresponding to high-priority samples are adjusted very little, whereas those corresponding to low-priority samples are relativity unchanged. Intuitively this should make sense. Samples that have high priority are likely to be used in training many times. Reducing the weights on these oft-seen samples basically tells the network, "Train on these samples, but without much emphasis; they'll be seen again soon." Conversely, when a low-priority sample is seen, the IS weights basically tell the network, "This sample will likely never be seen again, so fully update." Keep in mind that these low-priority samples have a low TD-error anyway, and so there's probably not much to be learned from them; however, they're still valuable for stability purposes. In practice, the beta parameter is annealed up to 1 over the duration of training. The alpha parameter can be annealed simultaneously, thereby making prioritized sampling more aggressive while at the same time more strongly correcting the weights. And in practice, from the paper you linked, keeping a fixed alpha (.6) while annealing the beta from .4 to 1 seems to be the sweet-spot for priority-based sampling (page 14). As a side note, from my own personal experience, simply ignoring the IS weights (i.e. not correcting at all) results in a network that trains well at first, but then the network appears to overfit, forgets what it's learned (a.k.a. catastrophic forgetting), and tanks. On Atari Breakout, for example, the averages increase during the first 50 million or so frames, then the averages completely tank. The paper you linked discusses this a bit, and provides some charts.
