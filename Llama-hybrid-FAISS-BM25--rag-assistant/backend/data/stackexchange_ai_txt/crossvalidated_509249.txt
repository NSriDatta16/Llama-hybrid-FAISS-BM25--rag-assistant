[site]: crossvalidated
[post_id]: 509249
[parent_id]: 
[tags]: 
Comparison between two times series

I am looking at time series data, comparing the velocity with which different groups of animals move in a new environment. In its raw form, the data consists of trajectories from individual fish that swim together - here you see a sample of about 10 seconds (individual lines correspond to individual fish, the red dotted line is the movement of the entire group - this is what I want to look at in detail). When I plot the distance the animals traverse over time in a cumulative plot, I see the following picture emerge: the thin lines are individual replicates and the thick ones represent the mean of the corresponding group (color-coded). It seems to me that the curves are quite different and I tried to quantify this by calculating the area between the curves and comparing this with a distribution of values I receive by randomly shuffling the categories of the individual replicates. The result of this would be the following plot: The histogram shows the distribution of areas between the curves I could in principle get from the observed replicates, the horizontal line is the value I observe with the actual categories. This strikes me as quite different from the null, but how can I quantify this difference so that a classical frequentist would be happy? In the end, I have only one observation (the actual area between the curves), so a statistical test would be difficult. Does my basic idea make sense at all? Any input is greatly appreciated! Edit I am aware that on the plot there are negative values shown for the area between the curves - I deliberately did not go for absolute values to keep the distribution normal.
