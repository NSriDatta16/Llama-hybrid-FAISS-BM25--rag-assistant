[site]: crossvalidated
[post_id]: 469550
[parent_id]: 372327
[tags]: 
I would suggest not changing the (calibrated) predicted probabilities. Some further points: While calibrated probabilities appearing "low" might be counter-intuitive, it might also be more realistic given the nature of the problem. Especially when operating in an imbalanced setting, predicting that a particular user/person has a very high absolute probability of being in the very rare positive class might be misleading/over-confident. I am not 100% clear from your post how the calibration was done. Assuming we did repeated-CV $2$ times $5$ -fold cross-validation: Within each of the 10 executions should use a separate say $K$ -fold internal cross-validation with ( $K-1$ ) folds for learning the model and $1$ for fitting the calibration map. Then $K$ calibrated classifiers are generated within each execution and the outputs of them are averaged to provide predictions on the test fold. (Platt's original paper Probabilities for SV Machines uses $K=3$ throughout but that is not a hard rule.) Given we are calibrating the probabilities of our classifier it would make sense to use proper scoring rule metrics like Brier score , Continuous Ranked Probability Score (CRPS), Logarithmic score too (the latter assuming we do not have any $0$ or $1$ probabilities being predicted). After we have decided the threshold $T$ for our probabilistic classifier, we are good to explain what it does. Indeeed, the risk classification might suggest to " treat any person with risk higher than $0.03$ "; that is fine if we can relate it to the relevant misclassification costs. Similarly, if misclassification costs are unavailable, if we use a proper scoring rule like Brier, we are still good; we have calibrated probabilistic predictions, anyway.
