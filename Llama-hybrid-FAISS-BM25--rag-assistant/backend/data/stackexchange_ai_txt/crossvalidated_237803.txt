[site]: crossvalidated
[post_id]: 237803
[parent_id]: 
[tags]: 
Likelihood in bayes and likelihood function

Bayesian inference is very confusing. I want to clarify about the notation in bayes theorem and the use of the likelihood function(or is it not?) as a part of it. From my early frequentist days, I thought I understood the concept of maximum likelihood quite clearly Given some $X$ is distributed such $\theta$ is the parameter(or I like to call it "acting" known value), the notation of pdf can be written as $f(x| \theta)$. This can be expressed as $f$ being a function of $x$. $f(x|\theta)$ can be re-interpreted as $l(\theta|x)$ when we want to find the maximum likelihood. $l(\theta|x)$ can be expressed as a function of $\theta$ as oppose to $x$ and hence, we can do things like differentiating in terms of $\theta$ because now $\theta$ is the variable. I want to draw a parallel to the likelihood function which is used inside the bayes theorem. Consider bayes theorem as below, $$\pi(\theta|x)=\frac{l(x|\theta)p(\theta)} {l(x)}$$ The likelihood function in this case is $l(x|\theta)$. My question is: Is the likelihood the same in both cases? If so, why, in the bayes case, do they write $l(x|\theta)$ instead of $l(\theta|x)$? Another example is when I want to find the fisher information. $$I(\theta|x)=E_x[\frac{\partial{}}{\partial{\theta}}log(l(x|\theta)) ]^2$$ Like in this case,if we are differentitating wrt to $\theta$, why would you write $l(x|\theta)$ and not $l(\theta|x)$? Also, why would you write a partial derivative if $x$ is known? I am so confused and my guess is that some mathematical conventions have to thought about carefully.
