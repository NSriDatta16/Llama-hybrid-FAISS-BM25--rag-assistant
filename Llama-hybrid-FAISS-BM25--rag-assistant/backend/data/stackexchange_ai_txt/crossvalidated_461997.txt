[site]: crossvalidated
[post_id]: 461997
[parent_id]: 461423
[tags]: 
The main problem you will run into here is not with the speed of the computation, but the ability of R to store matrices of sufficient size for what you want to do. A numeric $N \times N$ matrix is stored as a vector with an additional set of dimensions. The numeric vector takes up about $N^2 \times 8 \text{ bytes}$ , so a numeric matrix of the size you want is beyond the storage limits in R . This problem gets worse if you want to compute the singular value decomposition of the matrix, because you then have several matrices of this size. In any case, setting this problem aside, the base package of R has a function svd to compute the singular value decomposition of a matrix. It should be possible to use this function to compute the Moore-Penrose pseudo-inverse of a fairly large matrix. Below I create a function to do this and test it on an $N \times N$ random matrix where I have set $N=5000$ . The computation time on my laptop computer is $$, although it may be faster or slower on other machines. #Create function for computing Moore-Penrose pseudo-inverse via SVD MPI $d)); for (i in 1:length(DDD)) { DDD[i] v %*% diag(DDD) %*% t(SVD$u); } #Compute the Moore-Penrose pseudo-inverse for an N x N random matrix set.seed(75134903); N Note that the computation involves some rounding error, so you will have to ensure that it is sufficiently accurate for your purposes. In the above simulation the computation is sufficiently accurate that the product of the matrix and the computed Moore-Penrose pseudo-inverse is close enough to the identity matrix that the zapsmall function makes them identical. If you can find a way to compute and store the relevant matrices within the memory then it should be possible to obtain the Moore-Penrose pseudo-inverse of the matrix you are dealing with. In my view, the main challenge will be dealing with memory issues rather than computation speed.
