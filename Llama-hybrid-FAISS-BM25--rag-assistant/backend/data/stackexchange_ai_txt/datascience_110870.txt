[site]: datascience
[post_id]: 110870
[parent_id]: 109234
[tags]: 
For question one; if you look at Q-learning for example the next state is retrieved from the replay buffer and used in value estimation / loss calculation during critic training: next_action = actor_target(next_state) + noise target_Q = self.critic_target(next_state, next_action) current_Q = self.critic(state, action) critic_loss = F.mse_loss(current_Q, target_Q) This may vary from algorithm to algorithm, so it would depend on your specific use-case. For question two you are correct. You need something in the experience replay buffer before you can use it for training, so you must at least partially fill it with the state-action transitions of some initial agent or agents. Whether these agents are randomly initialized, or partially pre-optimized (with random search, for example) is up to you. For question three; similar to question one, the necessary content of the replay buffer depends on the specific algorithm that you are using. Personally I have not come across any instances where a replay buffer was implemented without the next_state, but someone with more experience might know more. Edit: also, this may help: state-action-reward-new state: confusion of terms
