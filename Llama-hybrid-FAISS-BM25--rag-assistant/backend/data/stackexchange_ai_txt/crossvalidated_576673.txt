[site]: crossvalidated
[post_id]: 576673
[parent_id]: 
[tags]: 
Is there a mathematical way to show why exactly adjustment for baseline helps in longitudinal randomized studies and NOT in non-RCTs?

How exactly adjustment for baseline helps to minimize the bias and increase effectiveness in randomized trials? I heard about: it minimizes the unexplained variance - but how? it minimizes the differences at baseline - how? Randomization makes any baseline differences in RCTs just statistical garbage, even if observed, of no importance. it improves precision - how? By minimizing the unexplained variance? Why unexplained? What's unexplained that can be fixed by adjustment for the baseline? it addresses somehow the regressio-to-mean- issue - but how? And the contrary - why the adjustment for baseline in non-randomized studies is discouraged both by industry guidelines and many, many articles? They only show that it "worked bad", but why? I heard, that: the differences at baselines are real here (oppositely to RCTs), so nothing to "minimize" - but even if I wanted to do this irrational thing - how exactly the adjustment "minimizes the baseline differences"? the regression-to-mean works independently starting from 2 different baselines - OK, but even if it did, how adjustment for baseline adresses the regression-to-mean? The magic "adjustment for baseline" is everywhere in the industries, like medicine, pharmaceutical, epidemiology, psychology, actuary, econometrics - but HOW does it work? Can this be shown mathematically?
