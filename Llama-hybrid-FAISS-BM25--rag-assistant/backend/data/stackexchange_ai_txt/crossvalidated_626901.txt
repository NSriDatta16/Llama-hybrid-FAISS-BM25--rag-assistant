[site]: crossvalidated
[post_id]: 626901
[parent_id]: 
[tags]: 
Convergence in Logistic Regression

Hey I'm taking a deeper dive into logistic regression. Specifically the following loss function with L2 regularization, $$l(w)=\frac{1}{n}\sum_n \log(1+\exp(-y_i \cdot x_i^Tw))+\frac{\lambda}{2}||w||^2$$ Where $y_i$ is a binary label, x and w have the same shape After laboriously working things out, I got a Lipschitz constant of $$\boxed{L=\frac{1}{4n}\sum_{i=1}^n||x_i||^2+\lambda}$$ I'm not sure about the 1/4 constant in front. In any case, doesn't this suggest that the smoothness is determined by the magnitude of the inputs? In such a case, if the inputs were arbitrarily large, how could GD converge if we require $\alpha L \leq 1$ for step size $\alpha$ ? Further, how would I go about proving convergence for some step size $\alpha$ in SGD, given that $\alpha$ converges for GD?
