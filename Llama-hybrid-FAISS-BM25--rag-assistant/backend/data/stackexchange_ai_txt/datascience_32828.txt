[site]: datascience
[post_id]: 32828
[parent_id]: 32694
[tags]: 
... each layer of a neural network is responsible for recognizing one feature of the input data. For example, if we build a neural network that classifies cars, buses, vans and bicycles, a layer will be responsible to recognize the tires, another one will responsible for recognizing the size of the vehicle. There are numerous kinds of neural networks and there are different differentiable components that are used inside them to achieve end-to-end learning. Convolutional layers are responsible for finding the features that are essential for reducing the error. These features are shared among different nodes and are not necessarily meaningful to us. They have to be used simultaneously due to being shared among different weights. What you are trying to say is that in convolutional networks, the first layers attempt to find low-level features like vertical and horizontal edges whilst deeper convolutional layers try to find high-level features which are more abstract. But the point here is that they attempt to recognize and the way they do that is by using weights and activations which are shared in different activation maps. Consequently, one neuron is not necessarily responsible for faces or for cars. There may be different neurons and filters which are responsible for them. Take a look at the link which has illustrated that. The other point is about fully connected layers which are responsible for classification. What they do is finding decision boundaries to classify inputs or estimating a function for regression tasks. They are not feature extractors in a way convolutional layers do. As it is illustrated in the cited link, what they try to do is to separate the input space to make it possible for generalization in the current feature space. The question is, why is this true? i.e. each layer appears to perform similar to the others and there is no special design for each one. Is there any way to assign each layer a specific feature or it is done implicitly? It was not true as mentioned. Although each layer seems similar to other layers, because of the cost function, their corresponding weights are set to decrease the cost function. Consequently, for fully connected layers all the featrues are feed to all the neurons in each layer and the training process decides how to use them. Moreover, the convolutional layers try to find similar patterns, kernels, in the input pattern.
