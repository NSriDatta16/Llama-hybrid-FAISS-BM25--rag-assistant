[site]: crossvalidated
[post_id]: 123392
[parent_id]: 
[tags]: 
Grid search for SVM parameters; is this is really how it is done?

Suppose I use nested 10-fold cross-validation with SVM. So, the inner-most loop will go around 100 times. Now, suppose I use a gaussian radial basis kernel function, which needs the parameter sigma. Moreover, I need to find the optimal C parameter for SVM (it is called 'boxconstraint' in Matlab). To find the optimal (sigma, C) pair, I would need to train 100*100 = 10000 SVMs for each fold. So in the end there would be about one million trained SVMs (performance estimation+parameter selection). Is this really done like this? What if I add feature selection. I might have to train and test 10 million SVMs. Should I apply for a super computer? What's the limit of an ordinary 8-core machine in terms of doing this kind selection?
