[site]: datascience
[post_id]: 51291
[parent_id]: 51287
[tags]: 
Yes, it is possible to represent the neural network with a single hidden layer. Computing the final result of $a_5$ : $a_5 = (\theta_9a_1+\theta_{10}a_2+\theta_{11}a_3+\theta_{12}a_4)*C$ (Equation 1) $a_1 = (\theta_1x_1+\theta_2x_2)*C$ (Equation 2) $a_2 = (\theta_3x_1+\theta_4x_2)*C$ (Equation 3) $a_3 = (\theta_5x_3+\theta_6x_4)*C$ (Equation 4) $a_4 = (\theta_7x_3+\theta_8x_4)*C$ (Equation 5) Replacing Equations 2,3,4,5 into Equation 1: $$a_5=(\theta_9*(\theta_1x_1+\theta_2x_2)*C+\theta_{10}*(\theta_3x_1+\theta_4x_2)*C+\theta_{11}*(\theta_5x_3+\theta_6x_4)*C+\theta_{12}*(\theta_7x_3+\theta_8x_4)*C)*C$$ $$a_5=(\theta_9\theta_1x_1+\theta_9\theta_2x_2+\theta_{10}\theta_3x_1+\theta_{10}\theta_4x_2+\theta_{11}\theta_5x_3+\theta_{11}\theta_6x_4+\theta_{12}\theta_7x_3+\theta_{12}\theta_8x_4)*C^2$$ 9 $$a_5 = (\theta_9\theta_1+\theta_{10}\theta_3)C^2x_1+(\theta_9\theta_2+\theta_{10}\theta_4)C^2x_2+(\theta_{11}\theta_5+\theta_{12}\theta_7)C^2x_3+(\theta_{11}\theta_6+\theta_{12}\theta_8)C^2x_4$$ Calling $F(\theta,x) = C*\sum\theta x$ the activation function (according to the definition), we reexpress the above equation with this definition: $$a_5 = F([\theta_9\theta_1+\theta_{10}\theta_3,\theta_9\theta_2+\theta_{10}\theta_4,\theta_{11}\theta_5+\theta_{12}\theta_7,\theta_{11}\theta_6+\theta_{12}\theta_8],[x_1,x_2,x_3,x_4])*C$$ Which means we can define the result of the network with only one equation which depends on the activation function $F$ . The network is (I can't draw here, sorry): $x_1$ $->$ $x_2$ $->$ O $->$ $Y$ $x_3$ $->$ $x_4$ $->$ The weight are: $C*(\theta_9\theta_1+\theta_{10}\theta_3)$ $C*(\theta_9\theta_2+\theta_{10}\theta_4)$ $C*(\theta_{11}\theta_5+\theta_{12}\theta_7)$ $C*(\theta_{11}\theta_6+\theta_{12}\theta_8)$ Activation function: $F(\theta,x) = C*\sum\theta x$ , defined as C times the sum product of two vectors, $\theta$ and $x$
