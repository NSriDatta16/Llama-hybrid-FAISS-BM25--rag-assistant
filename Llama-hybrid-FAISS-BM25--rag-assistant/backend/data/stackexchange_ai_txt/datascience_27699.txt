[site]: datascience
[post_id]: 27699
[parent_id]: 27693
[tags]: 
Is there any reason you have to use Naive Bayes? While Naive Bayes does handle multi-class modeling quite nicely, sometimes the "naive" assumption that each word in the text is independent of the others is too naive, especially given the size of your training set. While you may be able to increase the accuracy a little bit by doing more text processing, I wouldn't expect to see significant improvement, especially given the training size. Since Naive Bayes itself doesn't have much parameter tweaking, if you want to try more processing of the data itself to improve performance, you can try things like text count vectorization or TF-IDF vectorization to represent the text data more holistically than just keyword flagging. If you are able to implement other models, I would take a look at the scikit learn multi-class models for ideas. Tree based methods are also inherently multiclass, and have more parameters to tweak, so implementing something like Random Forests may suit you better. As for the training size itself, that does seem a little small. Especially if you are using text data, the more features you include, the more data you need to be able to stand a chance of detecting a signal in the data.
