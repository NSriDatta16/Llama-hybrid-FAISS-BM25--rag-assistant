[site]: crossvalidated
[post_id]: 466531
[parent_id]: 
[tags]: 
Why do model selection (AIC and LOO) outcomes differ between ML and bayesian approaches

I am interested in understanding whether my continuous data (dput code at bottom for reproducibility) are fit better by a linear model (Gaussian distribution) or a gamma distributed model. I typically use the lme4 package in R (maximum likelihood), but have been toying with the idea of using rstanarm (bayesian) a bit more. First the ML models: library(lme4) library(rstanarm) library(bayesplot) library(DHARMa) lmm plot(simulateResiduals(glmm)) Just looking at the residual plots, the lmm looks like a much better fit to the data, and the AIC output suggests the same (lower AIC = better): AIC(lmm,glmm) df AIC lmm 5 1286.038 glmm 5 1294.297 Now let's try the rstanarm package. S_lmm following the loo vignette: https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html looL All Pareto k estimates are good, so I think I am okay to go ahead and compare these two: loo_compare(looL,looGL) elpd_diff se_diff S_glmm 0.0 0.0 S_lmm -8.9 3.5 Perhaps I am misinterpreting this, but this looks like the stan version of the glmm (Gamma) model (instead of the lmm above) has the best fit to the data (higher number = better in this case). Still following the vignette (link above), comparing LOO-PIT values to generated samples: yrep $Area, yrep, lw = weights(looL$ psis_object)) The model may be a bit underdispersed here, having some excessive zeros, but without a lot of experience with these plots, I am not sure how poor this is. yrep $Area, yrep, lw = weights(looGL$ psis_object)) The lower end of the glmm looks a bit better (not sure about the hump in the middle). Am I interpreting these outputs correctly, that the lmm is a better fit with the ML methods ( lme4 ) and the glmm is a better fit with the bayesian methods ( rstanarm )? If so, why is this the case? Data for reproducible example: SPt
