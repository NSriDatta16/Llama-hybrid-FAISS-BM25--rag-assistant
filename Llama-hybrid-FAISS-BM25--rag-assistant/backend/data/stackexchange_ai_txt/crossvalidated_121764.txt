[site]: crossvalidated
[post_id]: 121764
[parent_id]: 121529
[tags]: 
First of all, I'd say that there is no single dimensionality reduction technique that is always useful: PCA for dimensionality and/or noise reduction is a heuristic that is useful if you can sensibly assume that your data has roughly the following structure: The data covers a not too large number ($n$) of "interesting" effects that cause relatively large variance, and in addition unrelated "noise" (and/or uninteresting influencing effects), which, however, has much lower magnitude (variance). In such a situation, PCA will capture the interesting factors in the first few PCs which are kept for further analysis, while noise will tend to appear in the higher PCs and is removed. One conclusion from that is that PCA is e.g. not the method of choice if you know e.g. that certain confounding factors lead to large variance compared to the variation due to the effects of interest. Also PCA produces linear combinations of the original variates. So if you'd e.g. expect your data to consist of few interesting channels among lots of noise-only channels then a dimensionality reduction based on hard feature selection could be more suitable.
