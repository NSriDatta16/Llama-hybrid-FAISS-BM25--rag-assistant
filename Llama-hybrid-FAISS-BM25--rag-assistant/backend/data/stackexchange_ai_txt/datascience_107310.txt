[site]: datascience
[post_id]: 107310
[parent_id]: 107278
[tags]: 
Generally I think we want to get rid of unnecessary/bad features to save ourselves from the curse of dimensionality -- the more features we use, the more data we need to make sure each part of the feature space has enough data to fit the model. On top of that there are concerns specific to our model choice, for example, Random Forest / GBDT. If we have 30 features and set feature_bagging to 10, it takes >= 30C10 = 30,045,015 trees to go through all possibilities. Also, features that are highly linearly correlated with one another do not add extra value to the model but are more possible to be chosen during feature bagging. Distance-based model like K-means can be volunerable to high dimensions.
