[site]: datascience
[post_id]: 65876
[parent_id]: 65721
[tags]: 
When implementing a model from a paper to reproduce their results, it is very important to pay attention to all the details. For this case there are some important differences when comparing to the CIFAR10 results of ResNet: You are using the Adam optimizer, while the ResNet paper uses SGD with a learning rate schedule. Adam is known to have issues converging to the best solution, and generally a well tuned SGD outperforms it. For reference this is the learning rate scheduler used by the paper : We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations This schedule can be implemented with the keras callback LearningRateScheduler You are not using the same data augmentation as in the ResNet paper. Quoting from the paper : We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original 32×32 image. You are using a batch size of 16, while the paper uses a batch size of 128. This is critical as batch size controls how noisy is the gradient, and can radically change the solution learned by SGD. Epochs looks okay, the paper trains for approximately 165 epochs (they use iterations which depends on batch size), so you might be overtraining the model. After fixing all of these issues you might get closer to the accuracy/error reported by the ResNet paper. For reference the Keras examples do contain a ResNet sample using CIFAR10 that gets close results here .
