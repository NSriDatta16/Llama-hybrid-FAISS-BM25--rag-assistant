[site]: datascience
[post_id]: 90109
[parent_id]: 90103
[tags]: 
That is indeed a drawback with grid search strategy, since you must know in advance each one of the possible combinations to try out, and that might be not optimal neither to get the best evaluation metric value nor in computation performance. You have other interesting strategies , not exhaustive hyperparameter search, for instance random search or based on bayesian tuning , for a more efficient search and being a "more clever" search strategy in the second option. You can have a look at HyperOpt library with several optimization algorightms (see also this link for a practical use case ), and more recently Keras released a nice keras tuner (which I love by the way). You can also have a look at this answer for a worked out example on a XGB model using Hyperopt, and this one for using keras tuner. You can also check the keras tuner wrapper for sklearn models: https://keras-team.github.io/keras-tuner/documentation/tuners/#sklearn-class
