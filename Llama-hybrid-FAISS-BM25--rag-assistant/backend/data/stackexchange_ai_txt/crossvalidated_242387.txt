[site]: crossvalidated
[post_id]: 242387
[parent_id]: 241854
[tags]: 
Conceptually it may help to think in terms of Bayesian updating : The penalty term is equivalent to a prior estimate $\beta_0$ with precision $\lambda$ (i.e. a multivariate Gaussian prior $\beta\sim\mathrm{N}_{\beta_0,\,I/\lambda}).$ In this sense a "very large" $\lambda$ does not correspond to any particular numerical value. Rather it would be a value which "dominates" the error, so numerically it must be large relative to some norm $\|X\|$ of the design matrix. So for your example we cannot say whether $\lambda=100000$ is "very large" or not, without more information. That said, why might a "very large" value be used? A common case I have seen in practice is where the actual problem is equality constrained least squares , but this is approximated using Tikhonov Regularization with a "large $\lambda$". (This is slightly more general than your case, and would correspond to a "wide" matrix $\Lambda$, such that $\Lambda(\beta-\beta_0)=0$ could be solved exactly.)
