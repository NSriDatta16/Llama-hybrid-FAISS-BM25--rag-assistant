[site]: crossvalidated
[post_id]: 620456
[parent_id]: 619777
[tags]: 
A single image doesn't have the property of information entropy. It is the probability distribution of images that has the property of information entropy. When the entropy of a single image is considered, then it can be about a probability distribution of images that are conditioned on having the same properties as that single image. This is related to physics where one considers macrostates and microstates . The latter, a microstate, has no entropy in itself, but in a way you could consider the entropy of the related macrostate and some model for it's distribution. So when you consider the entropy of an image, and consider all $k^n$ states equally possible, then an image is indeed just like a discrete uniform distribution with entropy $\Omega = \log_2 k^n = n \log_2 k$ . It is not wrong to consider the entropy of an image like that. However, one may consider also other uses of entropy (that are more useful). For example, when one considers images according to a model that entails a non-uniform distribution, where certain values are more/less likely, then one can have lossless image compression by encoding the more likely patterns with fewer bits (at the cost of the less likely patterns requiring more bits), such that on average less bits are needed to encode the image.
