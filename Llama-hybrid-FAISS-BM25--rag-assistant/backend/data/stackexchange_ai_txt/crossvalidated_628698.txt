[site]: crossvalidated
[post_id]: 628698
[parent_id]: 628687
[tags]: 
You can just see for yourself whether the math works out. See what happens when you simultaneously increase the value of two predictors by 1. I'll show you what I mean: Let's say we have our logistic regression model $$ \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 $$ This model assumes there is no interaction on the log odds scale. Let's figure out the predicted odds for some set of values of $X_1$ and $X_2$ , say $x_1$ and $x_2$ , and find the predicted odds when we increase the value of both variables by 1. Note we will hold $X_3$ constant at a value of $x_3$ . So the predicted odds when setting $X_1$ and $X_2$ to $x_1$ and $x_2$ is $$ \frac{p}{1-p}=e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3} $$ The predicted odds when setting $X_1$ and $X_2$ to $x_1 + 1$ and $x_2 + 1$ is $$ \frac{p}{1-p}=e^{\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_3} $$ When we take the ratio of these odds, we get \begin{align} \frac{e^{\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_3}}{e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3}} &=e^{(\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_3) - (\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)}\\ &= e^{\beta_1 + \beta_2} \end{align} So, yes, the odds ratio corresponding to increase both $x_1$ and $x_2$ by 1 simultaneously is the sum of the coefficients. When there is an interaction in the model, things are a little different. Let's say our model for the log odds is $$ \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 $$ (This time I'm leaving out $X_3$ since we saw its coefficient isn't relevant unless it also interacts with $X_1$ or $X_2$ .) Now we have that the predicted odds when setting $X_1$ and $X_2$ to $x_1$ and $x_2$ is $$ \frac{p}{1-p}=e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2} $$ The predicted odds when setting $X_1$ and $X_2$ to $x_1 + 1$ and $x_2 + 1$ is $$ \frac{p}{1-p}=e^{\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_1 x_2 + \beta_3 x_1 + \beta_3 x_2 + \beta_3} $$ When we take the ratio of these odds, we get \begin{align} \frac{e^{\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_1 x_2 + \beta_3 x_1 + \beta_3 x_2 + \beta_3}}{e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2}} &= e^{(\beta_0 + \beta_1 x_1 + \beta_1 + \beta_2 x_2 + \beta_2 + \beta_3 x_1 x_2 + \beta_3 x_1 + \beta_3 x_2 + \beta_3) - (\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2)} \\ &= e^{\beta_1 + \beta_2 + \beta_3 x_1 + \beta_3 x_2 + \beta_3} \end{align} This is all math you can do yourself. When in doubt, you can always just see what happens when you increase the value of two predictors by 1 instead of taking your instructor or someone else's word for it.
