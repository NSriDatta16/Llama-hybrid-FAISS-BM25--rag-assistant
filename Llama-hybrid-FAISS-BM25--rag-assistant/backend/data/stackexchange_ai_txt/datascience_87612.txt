[site]: datascience
[post_id]: 87612
[parent_id]: 87606
[tags]: 
It is not very clear what you are referring to with "number of input neurons". The input layer in BERT is an embedding layer, which is a table of vectors. Each of those vectors has dimensionality 768, and each vector is associated to one of the tokens in the vocabulary (so the number of vectors in the embedding table is the vocabulary size).
