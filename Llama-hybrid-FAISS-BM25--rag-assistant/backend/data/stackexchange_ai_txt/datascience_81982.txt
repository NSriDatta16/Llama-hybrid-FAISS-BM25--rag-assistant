[site]: datascience
[post_id]: 81982
[parent_id]: 81979
[tags]: 
Using backpropagation is nothing else than performing (stochastic) gradient descent. It computes the gradient, but it is not the "optimal" weight. The gradient is used to update the current weight (according to the gradient descent algorithm). The gradient descent algorithm needs a step size (which is called learning rate in the context of machine learning). The step size defines how "strong" the current weights are updated by the current gradient.
