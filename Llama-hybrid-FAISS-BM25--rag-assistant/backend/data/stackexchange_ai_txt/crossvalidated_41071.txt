[site]: crossvalidated
[post_id]: 41071
[parent_id]: 
[tags]: 
Why does a positive average squared cross-partial derivative indicate an interaction?

I am studying Gradient Boosted Regression Trees and am looking at the H-statistic described on pages 18-22 of Predictive Learning via Rule Ensembles (2008) . In the paper Dr. Friedman states: Interaction effects. A function $F(\mathbf{x})$ is said to exhibit an interaction between two of its variables $x_j$ and $x_k$ if the difference in the value of $F(\mathbf{x})$ as a result of changing the value of $x_j$ depends on the value of $x_k$. For numeric variables, this can be expressed as $$\mathbf{E_x}\left[\frac{\partial^2 F(\mathbf{x})}{\partial x_j \partial x_k}\right] \gt 0$$ Where does this come from? If we take a model $ F(x)=\beta_0 + \beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{3}x_{1}x_{2}$ the interaction term is $\beta_{3}$ through the cross partial derivative $\frac{\partial^2 F}{\partial x_{1}\partial x_{2}}$. It this related? Then average over all the training data (in an applied setting) and square?
