[site]: crossvalidated
[post_id]: 174498
[parent_id]: 106302
[tags]: 
You probably solved or worked around your questions long ago, but for the benefit of people finding this via search - as I did: I cannot answer all questions, but especially this one: Should I take average of corresponding speedups No, you should never take the average of normalized values, it is meaningless. For an explanation, see the following paper from 1986: How not to lie with statistics: the correct way to summarize benchmark results by Fleming and Wallace. As a simple example from the paper, consider that you have speed measurements of 20 (benchmark/sample 1) and 40 (benchmark 2) for X and 10 and 80 for Y. You compute the speedup of Y over X for each: 0.5 and 2.0 and their arithmetic mean is thus 1.25, so you would consider Y to be 25% slower than X. However, if you normalize to Y and get the speedups of X, you again get 2.0 and 0.5 resulting in an average of 1.25 - but for X this time, which is now 25% slower than Y! There are more examples of that kind in the paper and even a proof of why the geometric mean is the only useful mean of normalized values. A simple and useful thing one can do, is to compute the speedup of the average time: $(a_1 + a_2 + \ldots)/n \over (b_1 + b_2 + \ldots)/m$ or medians: $median(a_1, a_2, \ldots) \over median(b_1, b_2, \ldots)$. For a more elaborate approach to speed-up evaluation see the article The Speedup-Test: A Statistical Methodology for Program Speedup Analysis and Computation by Touati et al. They detail how to use statistical tests and confidence intervals with average and median speed-up measurements and provide flow charts to adhere to when declaring statistical significance and confidence levels.
