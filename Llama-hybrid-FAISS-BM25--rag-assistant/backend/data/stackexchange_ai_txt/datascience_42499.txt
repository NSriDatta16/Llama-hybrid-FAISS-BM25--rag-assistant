[site]: datascience
[post_id]: 42499
[parent_id]: 
[tags]: 
Does this encoder-decoder LSTM make sense for time series sequence to sequence?

TASK given $\vec x = [x_{t=-3}, x_{t=-2}, x_{t=-1}, x_{t=0}]$ predict $\vec y = [x_{t=1}, x_{t=2}]$ Whith an LSTM encoder-decoder (seq2seq) MODEL NOTE: the ? symbol in the shape of the tensors refers to batch_size, following tensorflow notation... QUESTION Is it worth trying this architecture? (I think it took me more time to draw the picture than coding it...) The difference with typical seq2seq is that in the decoder, the input for the second time step is not the output of the previous step. The input for both time steps in the decoder is the same, and it is an "encoded" version of the all hidden states of the encoder.
