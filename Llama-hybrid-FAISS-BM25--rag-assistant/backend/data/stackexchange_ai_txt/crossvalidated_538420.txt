[site]: crossvalidated
[post_id]: 538420
[parent_id]: 136997
[tags]: 
Some good points have been raised above. I'll add three ways in which non-negative constraints improve capture of interpretable factor models: Non-negativity encourages imputation of missing signal . Because signals can only be explained in one direction--additively--much more missing signal is imputed than in SVD, for example. If negative values are permitted in the models, the algorithm may be "tempted" to subtract away real data points rather than adding to missing points. It is often the case that data in the matrix to be factorized is not complete due to signal dropout or incomplete sampling, thus imputation is important. Non-negativity enforces sparsity . While unconstrained decompositions are almost never zero, non-negative models are always zero where a given factor absolutely does not contribute to a signal. Sparse representations are desirable when we want to discover distinct feature sets or sample relationships. Non-negativity ensures that factors will not cancel one another out . For instance, if one factor "overcorrects" a signal, another factor my try to "counter-correct" to balance it out. When factors are positive or zero only, they can never counter-correct, and only can explain additive signal. Generally, in a well-conditioned non-negative input matrix, an unconstrained orthogonal factorization will nearly be non-negative. However, imposing non-negativity enforces reasonable theoretical expectations and can accelerate convergence.
