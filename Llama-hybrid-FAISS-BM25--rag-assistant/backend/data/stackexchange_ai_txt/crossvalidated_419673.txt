[site]: crossvalidated
[post_id]: 419673
[parent_id]: 
[tags]: 
don't understand the derivation of logarithmic cost used in gradient descent

I am learning Machine Learning.I have seen that in logistic regression there are two cost 1) $θ_j=θ_j − \frac{\alpha}{m}\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})x^{(i)}$ 2) $Cost(h_θ(x),y) = −ylog(h_θ(x))−(1−y)log(1−h_θ(x))$ While updating weights through gradient descent we use this $(h_θ(x^{(i)})−y^{(i)})x^{(i)}$ as derivation of cost function and after updating weights we calculate cost using $−ylog(h_θ(x))−(1−y)log(1−h_θ(x))$ cost. I don't understand that $\frac{\partial}{\partial\theta}( −ylog(h_θ(x))−(1−y)log(1−h_θ(x)) ) = (h_θ(x^{(i)})−y^{(i)})x^{(i)}$ according to my knowledge cost in the gradient is the derivation of logarithmic cost
