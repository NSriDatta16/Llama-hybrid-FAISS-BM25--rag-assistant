[site]: datascience
[post_id]: 62998
[parent_id]: 62982
[tags]: 
I can think of several approaches. Simple data approach This first simple approach does not require machine learning techniques at all. Pick a value of $T$ . From your (quite large) dataset, extract observations for which the $T$ value is close enough. Among those observations, the optimal case could be the one minimizing $S$ , or, if the $T$ range is too broad, the one minimizing some combination of $S$ and $T$ (the performance/cost trade-off must be defined by someone who knows the business). This approach can work well if the dataset correctly covers the input space with sufficient density. With 1 million observations and 10 dimensions, this could not be enough if the problem is highly erratic. Learning approach Instead of using actual data to optimize $S$ , you could train models to predict both $S$ and $T$ , and evaluate them on the overall input space. This will give additional "fake" data, in both dense and sparse areas, which can be used to do the same as above. This approach can be risky, because the model is an estimator, so produces results that are expected to be good. In a large input space, it is likely that a global predictor will fail to be accurate everywhere. To limit the risk, it is possible to combine actual data with results predicted by the models. Using models which include information on their prediction error (such as Gaussian processes) could also help. Smarter but more complex: local models Here's another idea that I find smarter if the problem is rather complex, but requires more work. We start again with the full dataset and a selected value for $T$ , and filter the dataset to keep values close to $T$ (but not too close this time, because we need training data). It is likely that we select more than one area from the input space (meaning several operating modes of the machine, for instance). So the next step is to split the data into subsets corresponding to these operating modes. This can be done with unsupervised techniques; if the data is highly non linear, I would recommend using kernel-based techniques (spectral clustering for instance). The unsupervised step can include, or not, the $S$ value along with standard inputs $I_k$ . We now have several subsets, each corresponding to a machine operating mode. Based of the $S$ value distribution in each, some may be put aside. On each of the remaining subsets, it is now possible to train models to predict both $T$ and $S$ , as in the second approach; except that this time, we can use simpler models and/or expect them to be more efficient, because we are working on a very small volume of the input space, in which the phenomena should be simpler. The models built this way can then be evaluated to minimize the value of $S$ , or more likely, once again, a combination of $S$ and $T$ that makes sense in terms of business activity.
