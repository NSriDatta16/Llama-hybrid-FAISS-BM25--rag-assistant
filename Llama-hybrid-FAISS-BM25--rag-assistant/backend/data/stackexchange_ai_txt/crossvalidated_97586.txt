[site]: crossvalidated
[post_id]: 97586
[parent_id]: 
[tags]: 
Evaluating relative accuracy/error for continuous value prediction (and assessing relative average difference/error)

I was trying to understand what are good (or standard) way of evaluating relative accuracy for continuous data. Say for example, say I have some statistical algorithm S that outputs some real number from a vector of data. Something like $S:\mathbb{R}^k \mapsto \mathbb{R}$ Intuitively what I am interested in finding is a measure of how often S gets its approximation right. Though, since the value is not discrete, its not easy to compare it to the true value (we can't just count the times its not equal to the true value because then the accuracy would always be zero with very very very high probability). However, we could make the algorithm S output say $n$ predictions with value $p_i$ and each has its own true value $t_i$ and then, calculate the difference from $t_i$ for each true value $t_i$. The problem is that since they all not have the same true value $t_i$, that would be maybe kind of a weird way of calculating the "average relative distance form the true value". Instead maybe calculating an average relative distance made more sense to me but was not sure if such a measure was something reasonable. Something like: $$\delta = \frac{1}{n}\sum^{n}_{i=1}\frac{|p_i - t_i|}{t_i}$$ If you think this is not a good metric, it would be useful to point out its weaknesses and what other metrics are better and why. Also, as n approaches infinity, does that metric approaches 0? Please address those points in your answer (in as much detail as possible). I was interested in doing something of that sort and was wondering what the statistics community thought about it. On a side note, alternatively something that also seamed reasonable could be counting the number of times S is more than 10% (some percent) away from the true value.
