[site]: crossvalidated
[post_id]: 283277
[parent_id]: 
[tags]: 
Random Forest and appropriateness of Leave-One-Out

I have a classification problem (A, B or C) that appears to be handled well by Random Forest (7 explanatory variables). I've read that some people find it inappropriate to use Leave-One-Out (L-O-O) for cross-validation purposes. They say there is a minimum fraction of hold outs that are required. Okay, but my uninformed brain tells me L-O-O is attractive because besides forming an overall assessment about the generalizability this Random Forest approach, I would like to share one, final categorization for each entry as A, B, or C membership. It seems like L-O-O would be basing the calls on something closer to the "final model" compared to model(s) with 20% or other proportion held out. I'm interested in your views. Thank you!
