[site]: crossvalidated
[post_id]: 61984
[parent_id]: 
[tags]: 
Min-max vs. standard deviation when plotting error bars

I am running some experiments in which I plot the values of some variable y (averaged over 4 runs) against some independent variable x . I also compare the effect of z . In other words: horizontal axis: x vertical axis: y plots: one line for each value of z I would like to know if there are any best practices or rules of thumb to decide whether my error bars should reflect (a) min-max values, or (b) standard deviations. What characteristics of the data set should lead me to prefer one over the other? Some things that I think might be relevant: The raw values of y are integers bounded between 0 and 60. I can only perform a small number (e.g. 4-5) replications due to the time cost of generating each raw data point. There can be some pretty wild fluctuations, e.g. y could be in the high 50's for 3 of the runs, but 20 in the last one. The data does not follow any model
