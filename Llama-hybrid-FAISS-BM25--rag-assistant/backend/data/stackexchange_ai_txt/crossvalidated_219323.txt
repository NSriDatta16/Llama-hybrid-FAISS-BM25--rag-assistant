[site]: crossvalidated
[post_id]: 219323
[parent_id]: 219311
[tags]: 
It's a matter of thinking about the problem and deciding whether the model could provide a reasonably good approximation to the outcomes. As noted by Bernhard, it will (in almost all situations) only ever be an approximation, but it might be a good one - or at least one that is good enough to be useful. E.g. when we have a success/failure outcome and very few failures, linear regression will often been a very poor choice, while logistic regression could be appropriate. If we realize that data comes in batches (e.g. different production runs), we then consider whether we should take that into account in our model. Additionally, we of course look at how well our model performs - e.g. when we see that a linear regression model frequently predicts negative proabilities (or a negative number of failures), we would hopefully realise that it's a bad model, even if this had not occured to us on theoretical grounds before. If we realise that we know a lot of the underlying process through which the data arises, we may try a more complex model to reflect this knowledge. E.g. the successes and failures are actually arising from something being subjected to some stress or exposed to some risk for some period of time and the longer the time is, the higher the probability of a failure, then a time-to-event model may be a better choice than logistic regression. On the other hand, if the times are risk do not vary too much (and perhaps we do not even know them), then logistic regression may or may not still provide a good approximation, depending on what we want to do. E.g. if we want to change something about the underlying process (such as changing the time at risk in order to reduce failures to a certain level), more complex modeling that takes into account the aspects we wish to vary is needed, otherwise we cannot really say much about how we should change the process.
