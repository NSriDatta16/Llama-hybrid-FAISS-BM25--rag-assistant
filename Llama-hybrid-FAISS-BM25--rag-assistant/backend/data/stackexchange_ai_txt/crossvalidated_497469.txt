[site]: crossvalidated
[post_id]: 497469
[parent_id]: 496123
[tags]: 
I think you can use feature attention. Firstly you transform the states(n states each with d dimension) from the encoder into a fixed-shape k by d(or any dimension) matrix. In decoding your attention mechanism just pay attention to that matrix, which can not only overcome the information bottleneck of LSTM seq2seq but also speed up the attention. You'd better not employ the normal attention technique because the reconstruction error would be always very low because it learns to only pay attention to the input in the corresponding position. Inference: Linformer: Self-Attention with Linear Complexity
