[site]: crossvalidated
[post_id]: 558707
[parent_id]: 558234
[tags]: 
Even though bagging and boosting are both ensemble methods they are conceptually different. While I'm not aware of theoretical aspects of boosting, bagging ensemble methods are mainly used to reduce the variance in predictions of individual models. Consider, e.g., a random variable following a Gaussian distribution, i.e., $x \sim \mathcal{N}(0,\sigma)$ . And consider a new random variable that is defined as the average of $N$ elements sampled from $\mathcal{N}$ , i.e. $X = \frac{1}{N} \sum_{i=1}^N x_i$ where $x_i \sim \mathcal{N} \text{ for } i=1,\dots N$ . Then, one can see that the mean of the random variable $X$ is $0$ as well. But the variance changes: $$ \text{Var}(X) = \text{Var}(\frac{1}{N} \sum_{i=1}^N x_i) = \frac{1}{N^2} \sum_{i=1}^N \text{Var}(x_i) = \frac{1}{N^2} N \sigma = \frac{\sigma}{N} $$ Where Bienaym√© formula was used in the second equality as $x_i$ are uncorrelated. That is, when combining several inference models to make a prediction, one can have a final model with a reduced variance. To see the influence on the generalization error you can use the decomposition of this error into bias and variance. As for references, an introduction to statistical learning by James et al. touches on this topic in chapter 8.2 Bagging, random forests and boosting. But not with many details.
