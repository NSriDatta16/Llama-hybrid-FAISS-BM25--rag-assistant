[site]: datascience
[post_id]: 67054
[parent_id]: 66703
[tags]: 
It appear that I forgot one point of the ANN, at least forgot one of its effects : the activation function . It is true that for linear activation, multi-layer can be reduce to a single-one, but with a non-linear function, a two-layer neural network can be proven to be a universal function approximator. Sources However, it is true that I dont understand now why to use more than two hidden layers...
