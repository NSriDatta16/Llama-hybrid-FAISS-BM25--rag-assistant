[site]: crossvalidated
[post_id]: 475172
[parent_id]: 475133
[tags]: 
There is nothing special about estimating bias and variance in ensemble methods (whether bagging or boosting). It is just like estimating them for any other supervised learner. To estimate bias you start by assuming a fixed theoretical limit of accuracy, aka Bayesian Risk . Let's say this limit corresponds to 100% accuracy. Then you calculate the training error. The difference between the accuracy on training data and the the best achievable accuracy is an estimate of the bias. For example, if you get an 80% accuracy, then you have a bias problem. Afterwards you calculate the accuracy on a test set that you kept aside (i.e. you didn't train on). The difference between training error/accuracy and the test error/accuracy is an estimate of variance. More accurate estimates of the variance can be computed using k-fold cross validation.
