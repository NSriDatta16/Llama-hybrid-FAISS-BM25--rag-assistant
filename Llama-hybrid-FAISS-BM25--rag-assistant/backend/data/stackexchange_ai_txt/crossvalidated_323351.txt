[site]: crossvalidated
[post_id]: 323351
[parent_id]: 323325
[tags]: 
Sure, you can do that. If you sum up the gradient, you will minimize $$\mathcal{L} = \sum_i \ell_i$$ where the $l_i$'s are your different loss functions. The more important question isâ€“does this make sense? In some cases it does, for example if you have a probabilistic model of two factors. Then the joint log-likelihood will be a sum: $$\log p(y_1, y_2|x) = \log \left( ~p(y_1|x)p(y_2|x)~ \right) \\ = \log p(y_1|x) + \log p(y_2|x).$$ Now let $\ell_i = -\log p(y_i|x)$ et voila! Many loss functions for neural networks, especially sum of squares and the binary cross-entropy loss are actually log likelihoods of a probabilistic model.
