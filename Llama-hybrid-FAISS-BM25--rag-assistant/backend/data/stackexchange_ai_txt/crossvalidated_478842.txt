[site]: crossvalidated
[post_id]: 478842
[parent_id]: 264904
[tags]: 
TL;DR: It isn't necessary to have an off-policy method when using experience replay, but it makes your life a lot easier. When following a given policy $\pi$ , an on-policy method (for value function estimation) estimates $V^\pi$ or $Q^\pi$ (respectively), whereas an off-policy method estimates $V^*$ or $Q^*$ . The off-policy case is desirable because it guarantees that the estimate of $V^*$ or $Q^*$ will keep getting more accurate even if the policy being followed changes, i.e., following $\pi_1$ will yield $V^*$ , following $\pi_2$ will yield $V^*$ and randomly choosing between $\pi_1$ and $\pi_2$ for each step will still yield $V^*$ . (If all state-action pairs are seen often enough, ofc.) In the on-policy case, however, following $\pi_1$ will yield $V^{\pi_1}$ , following $\pi_2$ will yield $V^{\pi_2}$ and randomly choosing between the two policies at each step will yield something that is not immediately obvious - at least to me. In experience replay, the replay buffer is an amalgamation of experiences gathered by the agent following different policies $\pi_1, \dots, \pi_n$ at different times from which a random subset is drawn and used to improve the function approximation in a batch RL / supervised learning style. Off-policy methods won't have a problem with this; they will happily take the samples and improve the estimate of $V^*$ . However, as we can see from the above, this scenario is very much not ideal for on-policy methods. The policy represented by $V$ or $Q$ will be a (random) combination of the policies in the replay buffer, and who knows if that policy is at least as good as the previous policy. If it isn't we can't guarantee an improvement once we act $\epsilon$ -greedy on it. Saving and using the $(s_t, a_t, r_{t+1})$ -sequence, as you suggest, is being done by algorithms like A3C or PPO. You actually have to do this for on-policy methods, because they won't converge to $V^\pi$ or $Q^\pi$ otherwise. The problem here isn't if on-policy methods will converge when using experience replay, but rather what it is that they converge to, and if that what is still an improvement over the previous iteration. One way of addressing this problem is to stick to off-policy methods; another is to use on-policy methods, a rolling replay buffer (to "keep the experience fresh"), and carefully tuning parameters (making very small steps). Essentially, we aim to make sure that the $V^\pi$ or $Q^\pi$ we actually learn is close enough to $V^{\pi_n}$ or $Q^{\pi_n}$ (from the latest iteration's $\pi_n$ ) so that we can guarantee an improvement when acting greedily wrt. $V^\pi$ or $Q^\pi$ .
