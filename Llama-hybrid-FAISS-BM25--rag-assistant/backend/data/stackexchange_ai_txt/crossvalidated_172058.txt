[site]: crossvalidated
[post_id]: 172058
[parent_id]: 166767
[tags]: 
Why not trying the same approach as the one used for random forests ? Given a train/test set split of the data, you train your model then test it and observe the error. Now, for each column of the test set, generate a random permutation of the elements and observe the new error. If the change observed is not important, then the predictor had little impact on the forecast. For a more accurate estimation of the importance, you can perform a K-fold CV instead of just splitting the data. I never saw anyone using it with neural networks. However, it applies successfully to SVMs. The good thing with this approach is that it is completely independent of the learning method you are using - you can implement it once and for all.
