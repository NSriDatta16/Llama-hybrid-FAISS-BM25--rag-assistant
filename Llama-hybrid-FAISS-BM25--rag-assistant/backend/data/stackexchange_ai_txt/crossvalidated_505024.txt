[site]: crossvalidated
[post_id]: 505024
[parent_id]: 
[tags]: 
ABC Pseudo Marginal

Suppose, that we have observed data denoted as $y_{obs}$ , a likelihood function $l(y|\theta)$ where the parameter $\theta$ follows a prior distribution $\pi(\theta)$ . The posterior in the usual Bayesian inference framework can be derived as $$p(\theta|y_{obs})\propto \pi(\theta)l(y_{obs}|\theta)$$ However, in the case where the computation on likelihood function $l(y_{obs}|\theta)$ is either complex or intractable or time-consuming, a way of calculating or better approximating the posterior $p(\theta|y_{obs})$ is with the use of an ABC scheme. The idea is to keep parameters $\theta$ that generate data $y$ which are close enough to the observed $y_{obs}$ . Based on ABC, we define the joint posterior of $\theta$ and $y$ $$p_{ABC}(\theta,y|y_{obs})\propto \pi(\theta)\psi(y,y_{obs})l(y|\theta)$$ where the $\psi(\cdot,y_{obs})$ is a kernel density. And then approximate the posterior of parameter $\theta$ . This approximation can be done with the use of $MCMC$ and the main steps are the following $\bullet$ Generate $\theta^{0} \sim \pi(\theta)$ After $i$ steps $\bullet$ Generate a candidate $\theta^{'}\sim g(\theta^{i-1},\theta^{'})$ $\bullet$ Generate data $y^{'}\sim l(y|\theta^{'})$ $\bullet$ Accept $(\theta^{'},y^{'})$ with probability $min(1,\frac{\psi(y^{'},y_{obs})\pi(\theta^{'})g(\theta^{'},\theta^{i-1})}{\psi(y^{i-1},y_{obs})\pi(\theta^{i-1})g(\theta^{i-1},\theta^{'})})$ There is also, one alternative way of doing an ABC $MCMC$ , with the use of pseudo marginal. A way of doing that is considering the marginal ABC posterior in terms of the parameter $\theta$ $$p_{ABC}(\theta|y_{obs}) \propto \pi(\theta)\int \psi(y,y_{obs})l(y|\theta) dy$$ Then unbiasedly estimating the integral $\int \psi(y,y_{obs})l(y|\theta) dy$ and then conducting again the bullet points. So, we have two approach for doing the same thing, but what are the benefits of the later one, why should we prefer it instead of the former one??
