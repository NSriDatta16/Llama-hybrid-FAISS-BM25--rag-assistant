[site]: datascience
[post_id]: 124587
[parent_id]: 
[tags]: 
Why is my Histogram Gradient Boosting Classifier model still producing type II error? How can I reduce the type II error?

Type 2 error and how to hypertune or feature engineer a solution for it I trial and tested different techniques and kept the structure which made the most sense to me. But still my model confusion matrix keeps showing a type 2 error. How can I reduce this error and improve my model? This is my preprocessing method: Our Testing data is untouched except for 2 aspects. applying the pre-processing function which encodes our features into numerical type. There are still null values. Some models do not accept nulll value inputs. Logistic Regression for example, only allows non null numerical values. We also dropped 2 feature columns and applied additional arithmetic to the testing data. This is okay to do because in production, the inputing data can also be altered the same way as it makes sense to do so. Our Training data is preprocessed and transformed in the following ways. For *Data Engineering* , multicolinearity is dealt with creating new features using additiion arithmetics. Addition is used because of the relational sematics of the 2 correlated features. We also remove outliers of highly correlated features using IQR math applications. It is visualized with box plots. This is important for null value imputation as we are using a regressor model and removing outliers reduce bias and variance. Since the outliers are few considering the size of the dataset, we can consider omitting them. Due to high number of null values in the training data, it is imputed with the MICE algorithm . This is more accurate than using arithmetic imputation as it is a multivariate regressor. The imputed values are estimated by other numerical features. The training data also uses SMOTE to balance the weight of our target feature. This is important as unbalanced training data will affect the model evalation. There may be more false positives and more false negatives due to the bias of unweighted target feature. We use oversampling method for the default counts. This is the GitHub link to my model and the evaluation.
