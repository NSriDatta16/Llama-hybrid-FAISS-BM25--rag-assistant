[site]: datascience
[post_id]: 57619
[parent_id]: 38785
[tags]: 
This question greatly depends on the pixel-space scale of the objects, but if the scale of the objects is similar and enough data are provided to curb overfitting, then yes, it should work both ways . The Caveats: A Neural Network is merely a glorified (hierarchical) Template Matcher . If the visual appearance of multiple objects in an image is strongly correlated with the learned template, those objects will each get a high response from that convolutional kernel. Convolutional templates can achieve some degree of scale-invariance through stacking multiple layers and using a downsampling strategy such as Max Pooling , but virtually all convolutional neural networks are practically quite limited as to the scale of features that they will respond to. As a simple example, a neural network consisting of two convolutional layers of 3x3 spatial kernels each with padding of 1 separated by a 2x2 Max Pooling downsampling layer would have a receptive field of at most 8x8 pixels per output location (3x2+2), where the last 3 is the last layer's kernel width, the middle 2 is the downsampling factor, and the last 2 is the input padding reachable from the first kernel's output. Trying to classify any objects larger than this would result in a failure to match the learned template, so in fact CNNs are sensitive to the scale of their inputs , but due to weight sharing across spatial locations, a strong response trained from a single item should activate for multiple items of similar scale and (to some extent) vice-versa.
