[site]: crossvalidated
[post_id]: 583083
[parent_id]: 583057
[tags]: 
Gradient descent (GD) is an optimization technique used for solving an optimization problem that yields parameter estimates of the model. It is a non-issue here, as GD is used in the same way in all cases. (You would not be using different versions of GD when estimating the model on different folds or the entire dataset.) Use of the entire sample vs. data splitting by cross validation is an issue. As you note, fitting the same model on different subsets of the dataset will normally yield different parameter estimates. Since you are fitting a logistic regression and considering use of LR and Wald tests, you must be implicitly assuming your data are i.i.d. (The test results might not be valid if you do not assume that.) To avoid wasting power of the tests, you would run LR and Wald tests on the model estimated on the entire dataset rather than on different (combinations of) folds. On the other hand, to learn more about the validity of the i.i.d. assumption you could run LR and Wald tests on different (combinations of) folds. Keep in mind that due to the lower power resulting from fewer observations some positive test results may turn negative even if the i.i.d. assumption holds.
