[site]: crossvalidated
[post_id]: 236017
[parent_id]: 
[tags]: 
Does it "rarely make sense" to compute Kendall's $\tau$ for a large sample?

The manual page for R's cor says: Some people have noted that the code for Kendall's tau is slow for very large datasets (many more than 1000 cases). It rarely makes sense to do such a computation, but see function cor.fk in package pcaPP . Why wouldn't it make sense to compute a Kendall's $\tau$ for a large sample? Is there some reason $\tau$ is less useful or meaningful with larger samples? Or is it just that $\tau$ is hard to compute and you might as well approximate it by randomly sampling pairs of points and checking how often they agree?
