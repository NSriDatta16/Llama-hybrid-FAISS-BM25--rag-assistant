[site]: crossvalidated
[post_id]: 505350
[parent_id]: 505328
[tags]: 
You could take the log of each number, then quantize by rounding so that you end up with a reasonable amount of distinct tokens. Using log ensures that the relative difference between any two numbers which get encoded to the same token is upper bounded. Another approach is to have tokens 0 through 9 and encode large numbers as many tokens. Large language models have been able to learn basic arithmetic, even when they've not been trained to do so, which is encouraging evidence that this approach is ultimately more flexible and powerful.
