[site]: crossvalidated
[post_id]: 173218
[parent_id]: 118199
[tags]: 
A sparse coder is kind of like half an auto-encoder. An auto-encoder works like: input => neural net layer => hidden outputs => neural net layer => output For back-propagation, the error signal, the loss, is: input - output If we apply a sparsity constraint on the hidden outputs, then most will be zeros, and a few will be 1s. Then the second layer is essentially a set of linear basis functions, that are added together, according to which of the hidden outputs are 1s. In sparse coding, we only have the second half of this: codes => neural net layer => output The 'codes' is a bunch of real numbers, selecting for the basis functions represented by the weights in the neural net layer. Since in Olshausen's paper, they are applying a sparsity constraint to the codes, the codes are, just as in the sparse auto-encoder, sparse: mostly zeros with a few ones. The difference we can see now clearly: for the sparse coding, there is no first half of the neural network: the codes are not provided for us automatically by a neural net. How do we get the codes in sparse coding? We have to optimize ourselves, which we do by using gradient descent or similar, to find the set of codes that best provides output matching the input image. We have to do this for every image, including for every test image, each time.
