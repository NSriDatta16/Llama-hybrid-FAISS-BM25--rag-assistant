[site]: stackoverflow
[post_id]: 4684358
[parent_id]: 4667434
[tags]: 
On my system at least, almost all of the running time would be spent closing files. Sequential reading and writing is fast, so you can very well make several passes over the data. Here's what I'd do: Split the file into as many files as you can open at once, in such a way that a user's tweets go to the same file you keep track of which files contain more than one user Go on splitting the output files, until every file contains only one user If you can write to 200 files in parallel, after two passes over all the data you'd have 40000 files containing 150 users on average, so after the third pass you'd probably be nearly done. Here's some code that assumes the file has been preprocessed according to S.Lott's answer (collapse, filter, user_id). Note that it will delete the input file along with other intermediate files. todo = ['source'] counter = 0 while todo: infilename = todo.pop() infile = open(infilename) users = {} files = [] filenames = [] for tweet in infile: t, u, w, user = tweet.split('\t') if user not in users: users[user] = len(users) % MAX_FILES if len(files) MAX_FILES: todo += filenames[:len(users)-MAX_FILES] infile.close() os.remove(infilename)
