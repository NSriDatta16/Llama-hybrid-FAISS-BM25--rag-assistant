[site]: crossvalidated
[post_id]: 189641
[parent_id]: 189631
[tags]: 
Both in case of unbiasedness and variance-connected issues (like efficiency or heteroskedasticity), first of all, I would recommend plugging your equation on $Y$ into formulas for $\beta_{1} / \beta_{2}$. Thus (for $Y_{i} = \beta X_{i} + u_{i}$) we get: $$ \beta_{1}= \beta + \frac{\sum_{i=1}^{n}u_{i}}{\sum_{i=1}^{n}X_{i}} \\ \beta_{2}= \beta + \frac{\sum_{i=1}^{n}X_{i}u_{i}}{\sum_{i=1}^{n}X_{i}^{2}} \\ $$ Since $E(u_{i}|X_{i}) = 0$, you can see clearly (Tipp: $E(u_{i})=E(E(u_{i}|X_{i})$), why both estimators are unbiased and why there is a problem if we put $Y_{i} = \beta X_{i} + \gamma R_{i} + u_{i}$ inside $\beta_{1} / \beta_{2}$ (instead of $Y_{i} = \beta X_{i} + u_{i}$). Moreover, calculating OLS estimator ($\beta_{OLS}=(X^{'}X)^{-1}X^{'}y$) for given model, we obtain $\beta_{2} = \beta_{OLS}$. Gauss-Markov Theorem gives you that $\beta_{2}$ has lower variance (both estimators are linear in $Y$ and $\beta_{2}$ is BLUE). Alternatively, you could calculate variance of both estimators directly from definition: $$ Var(\beta_{2})=Var(\beta + \frac{\sum_{i=1}^{n}X_{i}u_{i}}{\sum_{i=1}^{n}X_{i}^{2}})=... $$ remembering that $Var(u_{i}) = E(Var(u_{i}|X_{i})) + Var(E(u_{i}|X_{i}))$. This approach might be particularly useful in c) since you can no longer use Gauss-Markov Theorem (we have heteroskedasticity!). I hope you can manage to do this; to find out which variance is lower you can use Cauchy-Schwarz inequality ( https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality ). Last but not least, heteroskedasticity has nothing with biasedness or unbiasedness of your estimators (the only thing you should provide is $E(u_{i}|X_{i})=0$). Back to b), your approach to use Breusch - Pagan test seems correct for me.
