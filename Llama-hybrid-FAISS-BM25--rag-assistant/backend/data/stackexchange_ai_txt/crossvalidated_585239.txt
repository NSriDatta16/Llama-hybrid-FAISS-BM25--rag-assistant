[site]: crossvalidated
[post_id]: 585239
[parent_id]: 585236
[tags]: 
The whole history of statistics is full of them. For example , $t$ -tests were born in Guinness brewery as means of optimizing their processes: T-Distribution, also known as Student's t-distribution, gets its name from William Sealy Gosset who first published it in English in 1908 in the scientific journal Biometrika using his pseudonym "Student" because his employer preferred staff to use pen names when publishing scientific papers instead of their real name, so he used the name "Student" to hide his identity. Gosset worked at the Guinness Brewery in Dublin, Ireland, and was interested in the problems of small samples â€“ for example, the chemical properties of barley with small sample sizes. Linear regression was discovered by Carl Friedrich Gauss to predict planetary movement in astronomy. As about Poisson distribution ... A further practical application of this distribution was made by Ladislaus Bortkiewicz in 1898 when he was given the task of investigating the number of soldiers in the Prussian army killed accidentally by horse kicks;: 23-25 this experiment introduced the Poisson distribution to the field of reliability engineering. Pierre-Simon Laplace applied Bayes theorem to estimate the mass of Saturn in the 1800s and his result was off just by 0.05%. Everything in statistics was discovered for solving practical problems and they gained popularity because of proving useful. Yes, it's not self-driving cars, but I doubt would have self-driving cars today if Gauss didn't do his research on the least squares in the 1800s. Every machine learning textbook mentions the Bayes theorem which was first studied by Thomas Bayes in the late 1700s. The examples are countless.
