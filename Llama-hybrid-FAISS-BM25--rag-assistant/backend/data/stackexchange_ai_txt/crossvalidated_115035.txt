[site]: crossvalidated
[post_id]: 115035
[parent_id]: 47058
[tags]: 
DWin's response offers the answer but little insight, so I thought it might be useful to provide some explanation. If you have two classes you are basically trying to estimate $p=P(y_i=1|X=x_i)$. This is all you need and logistic regression model assumes that: $log \frac{p}{1-p} = log \frac{P(y_i=1|X=x_i)}{P(y_i=0|X=x_i)}=\beta _0 + \beta _1 ^T x_i$ What I think you mean by the importance of the feature $j$ is how it affects $p$ or in other words what is $\frac{\partial p}{\partial x_{ij}}$. After a small transformation you can see that $p=\frac{e^{\beta _0 + \beta _1 ^T x_i}}{1+e^{\beta _0 + \beta _1 ^T x_i}}$. Once you calculate your derivative you'll see that $\frac{\partial p}{\partial x_{ij}} = \beta_j e^{\beta_0 + \beta _1 ^T x_i}$ This clearly depend on the value of all other variables. However you can observe that the SIGN of the coefficient can be interpreted the way you want: if it is negative then this feature decreases the probability p. Now in your estimation procedure you are trying to estimate $\beta$s assuming your model is correct. With regularization you introduce some bias into these estimates. For a ridge regression and independent variables you can get an closed form solution: $\hat{\beta^r} = \frac{\hat{\beta}}{\hat{\beta} + \lambda}$. As you can see this can change the sign of your coefficient so even that interpretation break apart.
