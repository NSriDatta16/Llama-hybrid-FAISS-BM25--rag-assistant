[site]: crossvalidated
[post_id]: 589713
[parent_id]: 145122
[tags]: 
Bonus: It also feels like MDP's is all about getting from one state to another, is this true? Since, MDP is about making future decisions by taking action at present, yes! it's about going from the present state to a more returning(that yields more reward) future state. To answer the comment by @Suhail Gupta: So any process that has the states, actions, transition probabilities and rewards defined would be termed as Markovian? The process to be called Markovian should also follow the Markov property along with what you have mentioned; the property says, "the future state depends upon the action taken in the present state and is not affected by the past states."
