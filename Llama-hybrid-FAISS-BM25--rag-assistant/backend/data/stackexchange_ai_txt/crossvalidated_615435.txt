[site]: crossvalidated
[post_id]: 615435
[parent_id]: 615421
[tags]: 
This is an interesting idea. However, I think you are doing less data reduction than you think (or perhaps none at all). For instance, by making a categorical feature with each original variable as a category, you now have data with size $(50,000,000\times 250,000)\times 3$ , equal to $12.5$ -trillion rows. That is with three columns, so you have $37.5$ -trillion values to handle in the new data set, as opposed to just $50,000,000\times 250,000=12.5$ -trillion in the original data. Okay, but your goal is to pass subsets of the data into an algorithm and train in batches, either explicitly with a neural network or at least in a manner that is evocative of using batches to optimize neural network parameters. Then it would seem that you can pass batches into your training, maybe $1250$ rounds of passing data of size $1$ -billion $\times 3$ . However... ...you have to do something with your column that has $50$ -million categories. A common way to handle categorical data is make indicator variables where the indicator takes $1$ if that category is represented and $0$ otherwise, meaning that you have $50$ -million new variables. Thus, while you think your data set is $12.5$ -trillion $\times 3$ , you data set is actually more like $12.5$ -trillion $\times 50$ -million, and you have made the problem $250,000$ -times worse.
