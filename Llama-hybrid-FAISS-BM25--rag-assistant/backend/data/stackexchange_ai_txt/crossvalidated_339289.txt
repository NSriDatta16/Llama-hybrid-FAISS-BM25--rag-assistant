[site]: crossvalidated
[post_id]: 339289
[parent_id]: 297749
[tags]: 
Feed-forward neural networks approximate the true class probabilities when trained properly. In 1991, Richard & Lippmann proved that feed-forward neural networks approach posterior class probabilities, when trained with {0,1} class-indicator target patterns [ Richard M. D., & Lippmann R. P. (1991). Neural network classifiers estimate bayesian a posteriori probabilities. Neural Computation, 3, 461â€“ 483. ]. In their line of proof, they use one-hidden layer feed-forward neural networks. In the mathematical annotation of Duda & Hart [ Duda R.O. & Hart P.E. (1973) Pattern Classification and Scene Analysis, Wiley ], define the feature distributions provided as input vector to the feed-forward neural network as $P({\bf{\it x}}\,\mid\,\omega_i)$, where for example the data vector equals ${\bf{\it x}}=(0.2,10.2,0,2)$, for a classification task with 4 feature-variables. The index $i$ indicates the possible $n$ classes, $i \in \{1,\ldots,n\}$. The feed-forward neural network classifier learns the posterior probabilities, ${\hat P}(\omega_i\,\mid\,{\bf{\it x}})$, when trained by gradient descent. The desired output pattern needs for example to be ${\bf {\it o}}=(0,1)$, for a two-class classification problem. The feed-forward neural network has one output node per class. The vector $(0,1)$ indicates that the observed feature-vector belongs to the 2'nd class.
