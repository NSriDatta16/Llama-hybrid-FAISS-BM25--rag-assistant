[site]: datascience
[post_id]: 114953
[parent_id]: 
[tags]: 
How does batch normalization make a model less sensitive to hyperparameter tuning?

Question 22 of 100+ Data Science Interview Questions and Answers for 2022 asks What is the benefit of batch normalization? The first bullet of the answers to this is The model is less sensitive to hyperparameter tuning. The wikipedia page batch normalization similarly claims: Furthermore, batch normalization seems to have a regularizing effect such that the network improves its generalization properties, and it is thus unnecessary to use dropout to mitigate overfitting. In both cases I suspect they are referring to improved test error, with the former involving improved test error even having done some hyperparameter tuning. Why does batch normalization have a regularizing effect? (Or does it?)
