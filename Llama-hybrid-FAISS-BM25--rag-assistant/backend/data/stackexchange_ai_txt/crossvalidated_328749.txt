[site]: crossvalidated
[post_id]: 328749
[parent_id]: 328667
[tags]: 
If I understand correctly, the question you are asking is, why bother with considering batches instead of passing the whole dataset at once. There are 2 reasons 1) If you pass the whole dataset, each iteration is much slower since each matrix multiply costs a lot more. For very large datasets (and limited RAM) it may not even be possible to pass the entire dataset. To visualize this, think of passing the information in all of wikipedia through your solver in one iteration. 2) Turns out that the batch method surprisingly works better/iteration than the naive implementation. This fact is mostly empirical. Details of the various approaches and reasons for them can be found inhttp://cs231n.github.io/optimization-1/ ============================ In some cases, the entire sentence can be passed all at once. An example of this is the use of CNN for sentence evaluation. The architecture of the word stacking is shown in figure 1 of https://arxiv.org/pdf/1408.5882.pdf . In translation/other sequence models, it becomes painful to send the entire sequence; one reason is the size of the model, and the other is the difficulty in describing a loss function. I guess the answer is very application dependent. If you pass an entire sequence, a "correct result" in a translation model I am guessing would be the entire sequence translated properly. Here, you will notice that getting the answer "correct" is much harder. In the sequence approach, if you pass in the data word by word, you can have a notion of partial wins; some percentage of the words can be classified accurately, and you will still get an improvement in accuracy. The notion of a correct answer in translation is very difficult because different translators will have different notions of "correctness" (hence metrics like BLEU). Thus a sentence which gets a label of 1 in one translation can get a label of 0 in another if you pass the entire sequence. The problem is accentuated if you pass the entire document
