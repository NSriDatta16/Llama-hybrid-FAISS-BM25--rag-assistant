[site]: crossvalidated
[post_id]: 468282
[parent_id]: 
[tags]: 
Does normalization also help to prevend the vanish/exploding gradients?

I am implementing my own neural network from scratch using numpy. I tested my code with the MNIST dataset and I forgot to normalize the images and my code did not work, because I got an error about a zero division. But after normalization, I did not get any error and my cost as well as my metrics were improving with time, so I thing my code is correct, but I cannot understand why I got that error before, So I remembered of the vanishing gradients problem and I wonder if that is the reason, what are your thoughts about it?
