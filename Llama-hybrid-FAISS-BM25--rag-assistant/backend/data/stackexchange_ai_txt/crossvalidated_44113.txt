[site]: crossvalidated
[post_id]: 44113
[parent_id]: 44070
[tags]: 
There is indeed a bias, because you have directly optimised the cross-validation error, so it will give an optimistic estimate of generalisation performance. Lie all estimators, the error of the cross-validation estimate of the test error has two components, the bias (the error caused by the CV error being systematically wrong) and the variance (the error due to the peculiarities of the sample of data on which it is evaluated). CV is (approximately) unbiased, so we don't need to worry about that too much. However it has a finite variance, which means that you would get a slightly different result each time if you evaluated the CV estimate over a different sample of data from the same underlying distribution. If you directly optimise the CV, you will to some extent be optimising it in ways that depend on the particular sample on which it is evaluated, but this is essentially just meaningless noise. This is a very common issue in machine learning, but perhaps not as widely appreciated as it should, so I wrote a paper about it, which you can find here: G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. ( www ) The problem gets worse if you have many hyper-parameters to choose, and can easily result in ending op with an "overtuned" model that generalises quite badly.
