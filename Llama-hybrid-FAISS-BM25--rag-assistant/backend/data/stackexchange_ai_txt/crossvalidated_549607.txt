[site]: crossvalidated
[post_id]: 549607
[parent_id]: 548128
[tags]: 
I think they're describing a VAE with $\pi$ and $z$ as latent variables, and $x$ and $y$ as observed variables. The prior on $z$ is normal, as usual, and the prior on $\pi$ is $\text{Dirichlet}(\alpha)$ . (As a note, I don't think they mean literally that the objective in eq. 9 will exactly fall out of this model. I think they're just suggesting that something similar looking can be derived, as is described below.) The generative process remains the same, as described by eq. 2. The evidence lower bound becomes: $$ \begin{align} \log p_\theta(x,y) &\geq \mathbb{E}_{q_\phi(z,\pi|x,y)} \left[ \log p_\theta(x,y|z,\pi) \right] - \mathcal{D}_{KL}\left[ q_\phi(z,\pi|x,y) \vert \vert p(z,\pi)\right] \\ &\geq \mathbb{E}_{q_\phi(z,\pi|x,y)} \left[ \log p_\theta(x,y|z,\pi) + \log p(z,\pi) - \log q_\phi(z,\pi|x,y) \right] \\ &\geq \mathbb{E}_{q_\phi(z,\pi|x,y)} \left[ \log p_\theta(x|y,z)+\log p(y|\pi) + \log p(z) + \log p(\pi) \\- \log q_\phi(z|x,y) - \log q_\phi(\pi|x) \right] \end{align} $$ This looks very similar to eq. 6 -- in fact, we only have three extra terms*: $\log p(y|\pi) + \log p(\pi) - \log q_\phi(\pi|x)$ . The last two terms are the KL divergence between the posterior and prior distributions over $\pi$ , which has a tractable form in the case that they're both dirichlet, or just an irrelevant constant if $q_\phi(\pi|x)$ is a degenerate point distribution. The first term $\log p(y|\pi)$ is essentially the classification loss. In eq. 9, it's written as $\log q_\phi(y|x)$ , but really it's the same thing, just written differently because the authors have $y$ as a latent, and infer it from $x$ , whereas in the proposed model, $\pi$ is the latent which is inferred from $x$ , and $\pi_k$ is the probability which the model assigns to the label $y$ being from the k-th class. Hence, we have the typical classification log-loss. *There's one additional term in eq. 6, $p_\theta(y)$ which is missing from my derivation -- the authors don't seem to define this anywhere, so I'm not sure where it came from, but I think it's just a red herring. As for $\alpha$ in eq. 9, this is probably a random hyperparameter they threw in, and not at all related to the $\alpha$ parameter of the dirichlet prior.
