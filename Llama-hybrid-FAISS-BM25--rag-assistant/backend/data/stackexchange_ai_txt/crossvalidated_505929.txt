[site]: crossvalidated
[post_id]: 505929
[parent_id]: 
[tags]: 
Best way to deal with non-continuously differentiable error functions in machine learning

Suppose you have the following set of $n=9$ numbers x = [1, 2, 3, 4, 5, 6, 7, 8, 9] and the corresponding y values [10, 2.5, 1.1, 0.6, 0.4, 0.2, 0.15, 0.12] which visualized look like this: I'd like to fit a function $f(x)=\frac{1}{\theta x^2}$ to the points making use of a mean squared error loss function to find an optimal value for $\theta$ . The loss function would be defined as $J(\theta)=\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{\theta x_i^2}-y_i)^2$ and the derivative of $J$ with respect to $\theta$ would be $\frac{\partial}{\partial \theta} J(\theta)=\frac{1}{n}\sum_{i=1}^{n}\frac{2(\theta x^2y-1)}{\theta^3x^4}$ . The problem I'm now facing is that as the function is obviously undefined at $a=0, \theta=0$ , the convergence towards the minimum of the cost function (which would be around $\theta=0.1$ ) will fail when $\theta$ is initialized with a negative value. Visualizing the cost function for the first point $x=1, y=10$ clearly shows that when $\theta=-1$ the slope at this point is positive which will cause $\theta$ to become smaller and smaller using the update rule $\theta_{new} = \theta - \alpha \frac{\partial}{\partial \theta} J(\theta)$ when in fact, $\theta$ should converge towards $0.1$ . My question is: What is the correct way to deal with this situation? In all the textbook examples I've seen, the cost function was always a continuous convex function but what to do when this is not the case?
