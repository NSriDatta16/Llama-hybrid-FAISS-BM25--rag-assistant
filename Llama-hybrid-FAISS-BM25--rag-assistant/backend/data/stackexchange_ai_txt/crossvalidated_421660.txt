[site]: crossvalidated
[post_id]: 421660
[parent_id]: 324340
[tags]: 
The standard autoencoder can be illustrated using the following graph: As stated in the previous answers it can be viewed as just a nonlinear extension of PCA. But compared to the variational autoencoder the vanilla autoencoder has the following drawback: The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where they're encoded vectors lie, may not be continuous or allow easy interpolation. That's, the encoding part in the above graph can not deal with inputs that the encoder has never seen before because different classes are clustered bluntly and those unseen inputs are encoded be to something located somewhere in the blank: To tackle this problem, the variational autoencoder was created by adding a layer containing a mean and a standard deviation for each hidden variable in the middle layer: Then even for the same input the decoded output can vary, and the encoded and clustered inputs become smooth: So to denoise or to classify(filter out dissimilar data) data, a standard autoencoder would be enough, while we'd better employ variational autoencoder for image generation. In addition, the latent vector in the variational autoencoder can be manipulated. Say, we subtract the latent vector for glasses from the latent vector of a person with glasses and decode this latent vector we can get the same person without glasses. Then for image manipulation, we should also use a variational autoencoder. Reference: Intuitively Understanding Variational Autoencoders
