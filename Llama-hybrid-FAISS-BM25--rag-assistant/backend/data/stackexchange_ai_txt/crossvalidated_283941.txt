[site]: crossvalidated
[post_id]: 283941
[parent_id]: 89809
[tags]: 
Other answers are correct, but it might help to get an intuitive grasp of the problem by seeing an example. Below, I generate a dataset that has two clear clusters, but the non-clustered dimension is much larger than the clustered dimension (note the different scales on the axes). Clustering on the non-normalised data fails. Clustering on the normalised data works very well. The same would apply with data clustered in both dimensions, but normalisation would help less. In that case, it might help to do a PCA, then normalise, but that would only help if the clusters are linearly separable and don't overlap in the PCA dimensions. (This example only works so clearly because of the low cluster count) import numpy as np import seaborn import matplotlib.pyplot as plt from sklearn.cluster import KMeans seaborn.set_theme() rnorm = np.random.randn x = rnorm(1000) / 2 y = np.concatenate([rnorm(500) + 5, rnorm(500)]) / 10 fig, axes = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(6, 8)) axes[0].scatter(x, y, s=2) axes[0].set_title("Data (note different axes scales)") km = KMeans(2) clusters = km.fit_predict(np.array([x, y]).T) axes[1].scatter(x, y, c=clusters, cmap="bwr", s=2) axes[1].set_title("non-normalised K-means") def normalise(vals): """Normalise values to fit between [0, 1]""" return (vals - min(vals)) / (max(vals) - min(vals)) xn = normalise(x) yn = normalise(y) clusters = km.fit_predict(np.array([xn, yn]).T) axes[2].scatter(xn, yn, c=clusters, cmap="bwr", s=2) axes[2].set_title("Normalised K-means") plt.tight_layout()
