[site]: crossvalidated
[post_id]: 85997
[parent_id]: 85910
[tags]: 
There is another point of view of the Shannon entropy. Imagine you want to guess through questions what the concrete value of a variable is. For simplicity, imagine that the value can only take eight different values $\left(0,1,..., 8\right)$, and all are equally probable. The most efficient way is to perform a binary search. First you ask whether is greater or less than 4. Then compare it against 2 or 6, and so on. In total you won't need more than three questions (which is the number of bits of this concrete distribution). We can carry on the analogy for the case of two variables. If they are not independent, then knowing the value of one of them helps you make better guesses (in average) for the next question (this is reflected in the results pointed out by omidi ). Hence, the entropy is lower, unless they are completely independent, where you need to guess their values independently. Saying that the entropy is lower means (for this concrete example) that you need to make less questions in average (i.e. more often than not you will make good guesses).
