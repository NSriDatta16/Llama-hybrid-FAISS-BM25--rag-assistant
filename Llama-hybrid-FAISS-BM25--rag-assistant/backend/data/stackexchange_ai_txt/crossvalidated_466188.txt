[site]: crossvalidated
[post_id]: 466188
[parent_id]: 
[tags]: 
Value of the absorbing state in a MDP and greedy policy - Why choose to go to the absorbing state if state value is 0?

I was going through an example of a Markov Decision Problem and I got the optimal value function with the value iteration algorithm described in Sutton Barto. In this algorithm I chose to initialise the value function with all zero for all states. Since the final state has no successors, the value of the final state is never updated and remains zero. At the end, when the algorithm returns the optimal value function I wanted to choose an optimal policy. But the agent would actually never choose to go to the final state, as all other reachable states have positive value. How is this fixed in general? Do I have to introduce an extra reward for finishing or is this just a sign of badly formulated problem?
