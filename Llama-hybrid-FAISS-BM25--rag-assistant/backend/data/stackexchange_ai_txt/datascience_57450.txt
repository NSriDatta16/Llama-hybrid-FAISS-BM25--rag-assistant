[site]: datascience
[post_id]: 57450
[parent_id]: 57255
[tags]: 
First, note that in logistic regression, using both an L1 and an L2 penalty is common enough to have its own name: ElasticNet. (Perhaps see https://stats.stackexchange.com/q/184029/232706 .) So using both isn't unprecedented. Second, XGBoost and LightGBM have quite a number of hyperparameters that overlap in their purpose. Tree complexity can be controlled by maximum depth, or maximum number of leaves, or minimum sample (count or weight) per leaf, or minimum criterion gain. Any combination of these might be optimal for some problem. Overfitting can also be combatted with the learning rate vs. number of trees (and early stopping), subsampling rates, and either of the regularization penalties. Finally, since L1 regularization in GBDTs is applied to leaf scores rather than directly to features as in logistic regression, it actually serves to reduce the depth of trees. This in turn will tend to reduce the impact of less-predictive features, but it isn't so dramatic as essentially removing the feature, as happens in logistic regression. You might think of L1 regularization as more aggressive against less-predictive features than L2 regularization. But then it might make sense to use both: some L1 to punish the less-predictive features, but then also some L2 to further punish large leaf scores without being so harsh on the less-predictive features. Toy example: https://github.com/bmreiniger/datascience.stackexchange/blob/master/trees_L1_reg.ipynb Possibly useful: https://github.com/dmlc/xgboost/blob/release_0.90/src/tree/split_evaluator.cc#L118 https://xgboost.readthedocs.io/en/latest/tutorials/model.html#model-complexity
