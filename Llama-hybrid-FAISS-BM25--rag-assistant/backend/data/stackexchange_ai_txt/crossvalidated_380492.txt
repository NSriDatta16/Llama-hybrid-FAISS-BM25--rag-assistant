[site]: crossvalidated
[post_id]: 380492
[parent_id]: 
[tags]: 
Two different value functions formulations, how can I reconcile them?

I've been studying Reinforcement learning for the past months, and using different sources, I've been seeing different formulations for the same thing. Specifically for value iteration: My question is this: imagine that I'm in a scenario where the reward is only dependent on the state, meaning that the reward is for example -1 for all states, with one exception, where it is 1. For the first formulation the summation will be the: $V(s)\leftarrow \max _{a}\sum_{_{s'}}p(s'|s, a)[r_{s'} + \gamma V(s')]$ , where $r_{s'}$ is the reward for reaching s' With the second formulation : $V(s)\leftarrow r_{s}+\max _{a}\sum_{_{s'}}p(s'|s, a)[ \gamma V(s')]$ , where $r_s$ is the reward of state s I'm having trouble reconciling why they are different, shouldn't they produce the same result? I tried them with a very simple RL example where the rewards follow the logic explained above. They produced different policies. Thank you for the help!
