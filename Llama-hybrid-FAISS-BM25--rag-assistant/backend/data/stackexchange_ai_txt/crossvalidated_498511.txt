[site]: crossvalidated
[post_id]: 498511
[parent_id]: 
[tags]: 
Estimating mean in the presence of serial correlation

Consider the following generating equation: \begin{equation} X_{d+1} = a X_d + b + {\cal E}_d \end{equation} where $a$ and $b$ are constants with $0 and $b > 0$ . Further let ${\cal E}_d$ be independent of everything else with $E[{\cal E}_d] = 0$ and $E[{\cal E}_d^2] = \sigma^2 ~~\forall d$ . We are interested in the values of $X_d$ only after the sequence hits "steady state" and so ignore the starting conditions (so for simplicity, assume that the equation "starts at $d=-\infty$ "). Easy to see that $E[X] = \frac{b}{1-a}$ . Now say we have $N$ consecutive samples $X_1,\cdots, X_N$ of $X$ and want to compute $E[X]$ . We have two options: Option A: Compute the simple average of $X_1, \cdots, X_N$ . So $E[X] \approx \frac{X_1+\cdots + X_N}{N}$ . Option B: Run an OLS regression using generation equation above to compute the values of $a$ and $b$ and then compute the mean using $E[X] = \frac{b}{1-a}$ . My questions: What are the pros and cons of each approach? I think the second option is superior and results in an estimator with lower variance. Is this true and if so how can I prove this?
