[site]: crossvalidated
[post_id]: 498534
[parent_id]: 498498
[tags]: 
Why does $\rho$ starts from $t+1$ in $Q(s,a)$ ? In general, when accounting for any stochastic effect in estimates of value functions in reinforcement learning, you need to pay attention to which random elements you have already conditioned on. When you are estimating $Q(s_t,a_t)$ then you are looking at all the events that happen after choosing $a_t$ . It does not matter at that point which policy chose the action in time step $t$ , so there is no need to correct for probability ratios between behaviour and target policy. This is unlike estimates for $V(s_t)$ where you do care about differences in policy affecting which action will happen on time step $t$ . The target policy could have a probability of zero for selecting $a_t$ , and you can still update your estimate of $Q(s_t,a_t)$ - if this were not possible then the agent could never learn about alternative actions to the greedy policy. Single-step Q learning is an interesting case where because you have already conditioned on the action $a_t$ at time step $t$ and are using the target policy (greedy action choice) to establish $a_{t+1}$ at time step $t+1$ , then there is no need to use importance sampling.
