[site]: crossvalidated
[post_id]: 559634
[parent_id]: 559623
[tags]: 
I don't know if I would exactly call this simple, but here is one way that works. Create one dataset that only contains one of the outcomes and all of the predictors, and create another dataset that only contains the other outcome and all of the predictors. Append the datasets longwise, adding a new variable representing which dataset each row comes from. You should have a new dataset with twice as many rows as you started with, a single dependent variable, and a new variable containing the dataset membership. From here, you fit a single logistic regression model for the outcome, but you include an interaction between dataset membership and every predictor. The coefficient on each product term represents the difference between the coefficients in the two original models (i.e., the difference between the coefficient in the model predicting one outcome and the coefficient in model predicting the other outcome). Below is how you would do this in R, with outcomes Y1 and Y2 and predictors A and B : set.seed(1) n First let's fit the models separately: fit1 $coef #> Estimate Std. Error z value Pr(>|z|) #> (Intercept) 0.9831919 0.02932530 33.52709 1.942439e-246 #> A 0.9772295 0.02877720 33.95846 9.148440e-253 #> B 0.9918479 0.06108163 16.23807 2.713229e-59 summary(fit2)$ coef #> Estimate Std. Error z value Pr(>|z|) #> (Intercept) -0.9992999 0.03395217 -29.43258 2.104052e-190 #> A 1.9364369 0.04170307 46.43392 0.000000e+00 #> B 2.9519656 0.07305699 40.40634 0.000000e+00 Now let's do the hack I described earlier: D1 Finally, we can fit the combine model. fit Estimate Std. Error z value Pr(>|z|) #> (Intercept) 0.9831919 0.02932530 33.52709 1.942439e-246 #> outcome2 -1.9824918 0.04486338 -44.18953 0.000000e+00 #> A 0.9772295 0.02877720 33.95846 9.148440e-253 #> B 0.9918479 0.06108163 16.23807 2.713229e-59 #> outcome2:A 0.9592075 0.05066827 18.93113 6.319463e-80 #> outcome2:B 1.9601176 0.09522757 20.58351 3.857119e-94 Notice the coefficients and standard errors for the terms that don't involve the outcome2 variable are identical to those in the original model for Y1 (i.e., because 1 is the reference level for outcome ). The sum of each coefficient and its interaction with outcome2 is equal to the corresponding coefficient in the model for Y2 . The interaction terms represent the differences between the coefficients. The outcome2 coefficient represents the difference between the intercepts of the two models. As @clementzach mentioned, it might be useful to add a correction for multiple comparisons using p.adjust() . For an omnibus test of whether the two models have any different coefficients, you can perform a likelihood ratio test between fit and a version of fit with the outcome variable excluded (or included if you don't want to include the possible difference in the intercept). An example below: fit_ Analysis of Deviance Table #> #> Model 1: Y ~ A + B #> Model 2: Y ~ outcome * (A + B) #> Resid. Df Resid. Dev Df Deviance Pr(>Chi) #> 1 19997 20864 #> 2 19994 18154 3 2710 --- #> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The significant 3df test indicates at least one of the coefficients differs between the two models. For multiply imputed data, you will need to extract the imputed datasets and perform this operation one by one, then combine the results of the combined model using the standard pooling rules for multiply imputed data (e.g., using mitools::MIcombine() ). Performing an LRT on multiply imputed data is not straightforward, so you may not be able to do the omnibus test.
