[site]: crossvalidated
[post_id]: 242326
[parent_id]: 
[tags]: 
Notation for estimator in Murphy's Machine Learning (Chapter 6)

I'm confused with the notation for estimators in Chapter 6 of Murphy's "Machine Learning: A Probabilistic Perspective": In section 6.2, he initially refers to an estimator as $\delta$ in the beginning of the section. ...is computed by applying an estimator $\delta$ to some data D, so $\hat{\theta} = \delta(D)$. Later in that paragraph, he refers to the estimator as the $\hat{\theta}(\cdot)$ function. Now apply the estimator $\hat{\theta}(\cdot)$ to each D... Then in section 6.2.1, he says the estimator is equal to a function f(). We could then compute our estimator from each sample, $\hat{\theta^s} = f(x_{1:N}^s)$ and... My question is why are there three different notations for the estimator? Are they all the same?
