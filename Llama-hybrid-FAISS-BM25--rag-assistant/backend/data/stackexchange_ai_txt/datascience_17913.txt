[site]: datascience
[post_id]: 17913
[parent_id]: 16640
[tags]: 
I think you should be careful as to which algorithms you tend to use. A machine learning algorithm should be structured as follows: feature extraction and then your model. These are two things that should be done separately. Feature Extraction This is the bag of words, n_grams and word2vec. These are all good choices for text examples. I think bag of words is a good choice in your case. However, if this generates a sparse matrix then maybe n_grams can be better. You can test all 3 methods. If you have a vocabulary size of 100,000 then you really need to use some extra feature extraction. The Model Theoretically, the more parameters in your model the more data you need to train it sufficiently otherwise you will retain a large amount of bias. This means a high error rate. Neural networks tend to have a very high number of parameters. Thus they require a lot of data to be trained. But, you have 8000 instances!!! Yes. But, you also have 100,000 features for each instance. This is insufficient even for shallow machine learning models. For a neural network, i usually suggest to follow this very general rule of thumb, $\#examples = 100 * \#features$. So you will need a MASSIVE amount of data to properly train your neural network model. Moreover, if you have a skewed dataset then you should expect to be using even more training examples, to show sufficient examples such that the model is capable of distinguishing the two classes. I would suggest a less intensive model. You should try to use: naive bayes, kernel-SVM or knn for text classification. These methods would do MUCH MUCH better than a neural network. Especially considering you have a skewed dataset!! My Suggestion I would start with bag of words and then use kernel-SVM. This should be a good starting point. A recurrent neural network is 0% recommended for the amount of data you have.
