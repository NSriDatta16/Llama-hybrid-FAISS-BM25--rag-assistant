[site]: crossvalidated
[post_id]: 242834
[parent_id]: 
[tags]: 
Does SVM prediction accuracy depend on a positive scaling of the kernel function?

Support vector machine (SVM) is a supervised learning algorithm. It draws hyperplanes to separate data points of different classes. The objective function involves inner products of pairs of feature vectors. The inner product can be replaced by a non-negative function $K(x,y)\ge0$ symmetric in its two variables $x$, $y$. The kernel function $K$ is a inner product in some abstract Hilbert space. In practice, we compute the similarity matrix using a kernel and the similarity matrix is symmetric and non-negative. The diagonal is the self-similarities which may or may not be constant and depend on the kernel. However, if I scale the similarity, or the kernel function by a positive constant, does it affect the classification accuracy? My intuition says no. Is it true? P.S. If the new set of parameters are used then the prediction will probably change; here let's define the accuracy as the optimal accuracy which is unique and independent of the kernel parameters. P.P.S. I have tested it on linear kernel SVM using sklearn SVC. The predictions will not change at all if rescale the dot product by $a$ and $C$ parameter by $C'=C/a$. I believe this is true for other kernels as well. So the short answer, rescaling does change the SVM classification through a trivial reparameterization.
