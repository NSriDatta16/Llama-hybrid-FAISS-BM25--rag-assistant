[site]: crossvalidated
[post_id]: 290641
[parent_id]: 212422
[tags]: 
See Figure 10.5 of Deep Learning (Goodfellow, Bengio, Courville) . The architecture itself can be adapted to fit this use case. If there are no labels for each time step (just for the last), I believe this is necessary (although you can also replicate the labels (target replication)). Target replication is performed in Lipton's 2016 paper on learning to diagnose with LSTMs. It seems to me (just an opinion) that some kind of target replication (or, if you have them, adding extra labels) might work better. In training a many-to-one, you have to send the gradient backwards a long distance, which can cause issues. More frequent targets give more immediate feedback on how adjacent-to-target weights influence the loss.
