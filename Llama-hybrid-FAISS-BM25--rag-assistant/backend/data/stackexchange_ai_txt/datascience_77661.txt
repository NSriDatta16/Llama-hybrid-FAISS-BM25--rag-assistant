[site]: datascience
[post_id]: 77661
[parent_id]: 
[tags]: 
Is the micro averaged precision/recall/f1 score for multiclass classification always the same?

I was under the impression from this post that in the micro averaging case for multi-class classification, the precision and recall are the same. This is because the number of false negatives and false positives are equal. Each classification error will cause a false positive for X and a false negative for Y . However, in both my own experiments and in some from the transformers library , the micro averages for precision and recall are different, even though it is multi-class classification. Here is an example table: precision recall f1-score support LOCderiv 0.7619 0.6154 0.6809 52 PERpart 0.8724 0.8997 0.8858 4057 OTHpart 0.9360 0.9466 0.9413 711 ORGpart 0.7015 0.6989 0.7002 269 LOCpart 0.7668 0.8488 0.8057 496 LOC 0.8745 0.9191 0.8963 235 ORGderiv 0.7723 0.8571 0.8125 91 OTHderiv 0.4800 0.6667 0.5581 18 OTH 0.5789 0.6875 0.6286 16 PERderiv 0.5385 0.3889 0.4516 18 PER 0.5000 0.5000 0.5000 2 ORG 0.0000 0.0000 0.0000 3 micro avg 0.8574 0.8862 0.8715 5968 macro avg 0.8575 0.8862 0.8713 5968 These examples seem to be calculating the precision/recall from this function . Why would the precision, recall, and F1 be different?
