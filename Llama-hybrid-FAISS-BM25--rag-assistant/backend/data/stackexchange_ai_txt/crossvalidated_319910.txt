[site]: crossvalidated
[post_id]: 319910
[parent_id]: 318463
[tags]: 
I think it is very difficult for this not to be a philosophical discussion. My answer is really a rewording of good points already mentioned here (+1s for all); I just want to point to a quote from Andrew Gelman that really spoke to me as someone who trained as a computer scientist. I have the impression that many of the people who call what they do machine learning also come from computer science. The quote is from a talk that Gelman gave at the 2017 New York R Conference called Theoretical Statistics is the Theory of Applied Statistics : Theory is scalable. Theory tells you what makes sense and what does not under certain conditions. Do we want to do thousands or tens of thousands or millions of simulations to get an idea of the truth? Do we want to do empirical comparisons on more and more benchmark datasets? It's going to take a while, and our results may still be brittle. Further, how do we know that the comparisons we do make sense? How do we know that our new Deep Learner with 99.5% accuracy is really better than the old one that had 99.1% accuracy? Some theory will help here. I'm a big fan of simulations and I use them a lot to make sense of the world (or even make sense of the theory), but theoretical machine learning is the theory of applied machine learning.
