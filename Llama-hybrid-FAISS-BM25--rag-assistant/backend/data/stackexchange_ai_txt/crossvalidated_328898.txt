[site]: crossvalidated
[post_id]: 328898
[parent_id]: 328667
[tags]: 
I'm not aware of a good algorithm that takes the whole sequence of an RNN at once when the sequence is very long. Training on partial sequences, doing backprop, and then training on the next partial sequence is called truncated back-propagation through time (TBTT). The reason that TBTT is used is that a very long sequence has two problems. A very long sequence has lots of computation required to compute both the forward and the backward pass and The gradient vanishes as you go farther back in time, so there's no real "gain" to go back very far in time. Some resources on this include Williams and Zipser (1992), Gradient-Based Learning Algorithms for Recurrent Networks and Their Computation Complexity and Williams and Peng (1990), An Efficien Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories and Ilya Sutskeverâ€™s Ph.D. thesis .
