[site]: crossvalidated
[post_id]: 565934
[parent_id]: 
[tags]: 
Why is there a meaningful relationship between probabilities of non-ground-truth classes, in the context of knowledge distillation?

In the original knowledge distillation paper Distilling the Knowledge in a Neural Network , Geoffrey Hinton, Oriol Vinyals and Jeff Dean state the following: "much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of $10^{−6}$ of being a 3 and $10^{−9}$ of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s)" From my understanding, they're basically saying that the probabilities in the softmax distribution of the classes that aren't the ground truth class give information about other potential classes that the image "looks like" (eg: the 2nd highest probability is something the image could look like ). This behavior seems intuitive, but what is the reason we can assume this to be true? Couldn't the model learn to give a high probability to the target class for an image and meaningless assorted probabilities to the others? Assume we are using a cross entropy loss to train the teacher model, the model which provides the soft targets.
