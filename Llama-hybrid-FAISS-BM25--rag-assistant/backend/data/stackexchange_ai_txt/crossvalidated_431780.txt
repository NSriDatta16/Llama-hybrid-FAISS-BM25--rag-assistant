[site]: crossvalidated
[post_id]: 431780
[parent_id]: 431751
[tags]: 
Let me slightly diverge from your exact question. You are describing your intended model, where the probability of giving the answer $y=1$ is modeled as $$ p(y_i=1) = \lambda\,0.5 + (1 - \lambda) \, p^*_i $$ Notice that the proposed model can be described in different form $$ \lambda\;0.5 + (1 - \lambda) \, p^*_i = \alpha + \gamma_i $$ in such case, you could write $$ \lambda = \frac{\alpha}{0.5}, \qquad p^*_i = \frac{\gamma_i}{1-\lambda} $$ If you think about it, then it seems that you can re-define your model to logistic regression with $\alpha$ and $\gamma$ being unbounded, real-valued parameters, that get turned into probabilities by passing them through the logistic function $\sigma(\cdot)$ , $$ p(y_i=1) = \sigma(\alpha + \gamma\,d_i) $$ where $d$ is an indicator that is equal to $1$ for the regular trials, and $0$ for the "catch" trials. In such case, probability of $y=1$ for the "catch" trial is simply $\sigma(\alpha)$ . You should be able to learn $\alpha$ from your data in a single step, without any empirical Bayesian tricks. In such case, $\alpha$ would be the "base rate" for "catch" trials, and the added effect of non-random answers would be modeled by $\gamma$ . Finally, this enables us to re-define your complete model as $$ p(y_i=1) = \sigma\big(\alpha + d \cdot [\beta_0 + u_{j0} + (\beta_1 + u_{j1}) \,x_i]\big) $$ I do not have any formal arguments to back my thesis, but for me, this formulation seems to be simpler and more flexible (e.g. $\lambda$ and $p^*$ are not constrained to unit interval). You could argue with that, but for me, such formulation is also cleaner in terms of interpretability, because you model the additive effect directly, withouth the weighting by $\lambda$ . As about priors, for the above model you could choose something like $\alpha \sim \mathcal{N}(0, \tau)$ , where $\tau$ would control the variability around $0.5$ assumed a priori for $p(y=0)$ .
