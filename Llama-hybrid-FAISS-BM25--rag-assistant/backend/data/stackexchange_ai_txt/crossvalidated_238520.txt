[site]: crossvalidated
[post_id]: 238520
[parent_id]: 17537
[tags]: 
Have a lot of practice giving lectures about standard deviation and variance to a novice audience. Lets assume, one knows about average already. By average (or e.g. median) - one gets a single value from many measurements (that is how one usually uses them). But it is very import to say, that knowing some average is not enough at all. The second half of the knowledge is what is the error of the value. Skip the next 2 paragraphs of motivation if lazy Lets say you have some measurement device, that costed 1 000 000\$. And it gives you the answer: 42. Do you think one paid 1 000 000\$ for 42? Phooey! 1 000 000 is paid for the precision of that answer. Because Value - costs nothing without knowing its Error. You pay for the error, not the value. Here is a good live example: Commonly, we use a ruler to measure a distance. The ruler provides a precision around one millimeter (if you use metric system). What if you have to go beyond and measure something with like 0.1mm precision? - You probably would use a caliper. Now, it is easy to check, that a cheap ruler with mm scale costs cents, while reliable caliper costs ~$10. Two orders of magnitude in price for one order of magnitude in precision. And that is very usual ratio of how much one pays for smaller errors. The problem. Lets say we have a thermometer (Choose a measurement device depending on what is closer to auditory). We did N measurements of the same temperature and thermometer showed us something like 36.5, 35.9, 37.0, 36.6, ... (see the pic). But we know that the real temperature was the same all the time, and values are different because in every measurement the thermometer lies to us a bit. We can calculate the average (see red line on the picture below). Can we believe it? Even after averaging, does it have enough precision for our needs? For human health estimation for example? How can one estimate how much this little scum lies to us? Max deviation - the easiest but not the best approach. We can take the farthest point, calculate the distance between it and the average (red line) and say, that this is how thermometer lies to us, because it is maximum error we see. One could guess, this estimation is too rough. If we look at the picture, most of the points are around the average, how can we decide just by one point? Actually one can practice in naming reasons why such estimation is rough and usually bad. Variance . Then... lets take all distances and calculate an average distance from the average (on picture - average distance between each point and the red line)! BTW, how to calculate a distance? When you hear the "distance" it translates to "subtract" in math. Thus we start our formula with $ (x_{i} - \bar{x})$ where $\bar{x}$ is the average (red line) and $x_{i}$ is one of the measurements (points). Then one could imagine that the formula of average distance would be summing everything and dividing by N: $$\frac{\sum(x_{i} - \bar{x})}{N} $$ But there is a problem. We can easily see, eg. that 36.4, and 36.8 are at the same distance from 36.6. but if we put the values in the formula above, we get -0.2 and +0.2, and their sum equals 0, which is not what we want. How to get rid of the sign? At this points someone usually says "Take the absolute value of each point!". Taking an absolute value is actually a way to go, but what is the other way? We can square the values! Then the formula becomes: $$\frac{\sum(x_{i} - \bar{x})^{2}}{N} $$ . This formula is called "Variance" in statistics. And it fits Much better to estimate the spread of our thermometer (or whatever) values, than taking just the maximum distance. Standard deviation . But still there is one more problem. Look at the variance formula. Squares make our measurement units... squared. If the thermometer measures the temperature in °C (or °F) then our error estimation is measured in $°C^{2}$ (or $°F^{2}$ ). How to neutralize the squares? - Use the square root! $$\sqrt{\frac{\sum(x_{i} - \bar{x})^{2}}{N}}$$ So here we come to the Standard Deviation formula which is commonly denoted as $\sigma$ . And that is the better way to estimate our device precision. Hope it was easy to understand. From this point it should be easy go to "68–95–99.7 rule", sampling and population, standard error vs standard deviation terms Etc. P.S. @whuber pointed out a good related QA - " Why square the difference instead of taking the absolute value in standard deviation? "
