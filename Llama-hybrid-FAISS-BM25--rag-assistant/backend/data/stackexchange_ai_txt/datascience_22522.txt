[site]: datascience
[post_id]: 22522
[parent_id]: 
[tags]: 
Advanced Activation Layers in Deep Neural Networks

I'm wondering about the benefits of advanced activation layers such as LeakyReLU, Parametric ReLU, and Exponential Linear Unit (ELU). What are the differences between them and how do they benefit training?
