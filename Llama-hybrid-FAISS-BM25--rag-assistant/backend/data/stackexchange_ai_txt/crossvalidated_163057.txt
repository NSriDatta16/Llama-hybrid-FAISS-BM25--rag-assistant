[site]: crossvalidated
[post_id]: 163057
[parent_id]: 102692
[tags]: 
In short, there's no complicated definition of "Bayesian logistic regression" that cannot be inferred from its parent terms. It's Bayesian in that its inferences are based on the posterior, and begin with a specified prior for model parameters; it's logistic regression in that we're fitting the $\beta$ coefficients of a logistic regression. This choice of prior is often discussed as a means of expressing existing uncertain knowledge regarding the problem; in the case of the linked paper, the Laplace prior is chosen because it yields a posterior with certain desired properties. Namely, it can be tuned for desired levels of sparsity. In inferential applications, one would base inferences on a numerical approximation of the model parameters' full posterior. Since this is a machine learning setting and predictions are of primary interest, other concerns enter, e.g. memory and efficiency, and point estimates will do. As in many Bayesian models, no analytic expression exists for this posterior. CLG the basis of the authors' approach to fitting the parameters numerically.
