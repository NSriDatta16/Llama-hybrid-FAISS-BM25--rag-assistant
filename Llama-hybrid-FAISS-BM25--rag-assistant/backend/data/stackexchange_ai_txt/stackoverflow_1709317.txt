[site]: stackoverflow
[post_id]: 1709317
[parent_id]: 112727
[tags]: 
I like this question, so I'm giving an answer, even though others have already answered it. When I was in grad school, in the MIT AI Lab, we faced this situation all the time, where we were trying to write programs to gain understanding into language, vision, learning, reasoning, etc. My impression was that those who made progress were more interested in writing programs that would do something interesting than do something fast. In fact, time spent worrying about performance was basically subtracted from time spent conceiving interesting behavior. Now I work on more prosaic stuff, but the same principle applies. If I get something working I can always make it work faster. I would caution however that the way software engineering is now taught strongly encourages making mountains out of molehills. Rather than just getting it done, folks are taught to create a class hierarchy, with as many layers of abstraction as they can make, with services, interface specifications, plugins, and everything under the sun. They are not taught to use these things as sparingly as possible . The result is monstrously overcomplicated software that is much harder to optimize because it is much more complicated to change. I think the only way to avoid this is to get a lot of experience doing performance tuning and in that way come to recognize the design approaches that lead to this overcomplication. (Such as: an over-emphasis on classes and data structure.) Here is an example of tuning an application that has been written in the way that is generally taught.
