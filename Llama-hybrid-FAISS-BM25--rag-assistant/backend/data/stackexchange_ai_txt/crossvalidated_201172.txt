[site]: crossvalidated
[post_id]: 201172
[parent_id]: 186811
[tags]: 
Many familiar techniques for avoiding overtraining are entirely relevant to recurrent neural networks. Most obviously, regularisation (typically L1 regularisation, where a penalty is proportion to the sum of absolute values of all weights and L2 regularisation, where the penalty is proportional the sum of the squares of the weights). Another neural network specific technique that is of interest is dropout. There is some evidence that dropout needs to be modified a bit to work well on RNNs (see Zaremba, Sutskever and Vinyals, ICLR 2015), specifically that it is best to avoid dropout on the recurrent connections (the ones that link across time steps). Early stopping is a form of regularisation. With this you need to store copies of the neural network (most simply, one for the network that has achieved the lowest validation set error so far). Then you simply keep this network that performed best out of sample. This is not a perfect method, but it is very simple and very fast, as you could do without cross-validation. I believe Geoffrey Hinton expressed the informed view that for the best results, you should use a network from slightly earlier in the training than the absolute low of test set error (because the actual low is partly due to good generalisation and partly due to random fluctuations in test set score which do not generalise). For completeness, I'll mention that a final form of regularisation that has proven useful in tasks with limited noisy data is to construct multiple variants of the examples you have with forms of distortion that reflect knowledge you have. For example, if you were training an RNN to write music, you might expand a dataset of tunes by transposing each to 12 different keys. In a sense you could think of this as encouraging your RNN to respect an intrinsic symmetry. See http://arxiv.org/abs/cs/0410015 and http://arxiv.org/abs/1312.4569
