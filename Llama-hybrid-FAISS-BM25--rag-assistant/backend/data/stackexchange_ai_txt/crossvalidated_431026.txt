[site]: crossvalidated
[post_id]: 431026
[parent_id]: 431011
[tags]: 
I like to think about what would happen to my conclusions if I added a bunch of additional variables that were just random noise (but had names that fit the problem) and how my conclusion may change if I do or don't adjust for multiple comparisons. If you will declare "Success" if you get any significant p-values then not adjusting will mean that you will probably declare "Success" based on one of the noise variables, but adjusting for multiple comparisons will decrease this likelihood. On the other hand, if you will discuss the significance of a specific variable whether or not other variables are significant or not, then adding noise variables and adjusting for multiple comparisons will tend to mask information about the variable of interest, so it would be better to not correct in this case. Another option instead of correcting p-values is to switch to a Bayesian analysis, more specifically a hierarchical model. Frequentist methods correct for multiple comparisons by making confidence intervals wider or p-values larger to correct for the fact that out of many comparisons a few are likely to be far from the truth by chance (but makes no adjustment to the point estimates). Bayesian hierarchical models instead shrink the point estimates (and corresponding intervals/posteriors) towards a common estimate, which I think gives better inference.
