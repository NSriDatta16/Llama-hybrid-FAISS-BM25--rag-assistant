[site]: crossvalidated
[post_id]: 540310
[parent_id]: 540294
[tags]: 
Correlation is typically what I use to screen for association between variables in a complex system with unknown characteristics. The correlation matrix is a snapshot which shows the association between all possible pairs of variables. In addition, I typically use Spearman rank correlation, since it's not-sensitive to outliers and doesn't really require central tendency, i.e., bulk of data in the middle of a histogram. Pearson correlation, on the other hand, does require central tendency and few outliers in order to be less biased than Spearman. Pearson correlation, and many hypothesis tests in statistics, assume varying degrees of normality, and therefore a unimodal distribution. If you have multimodal distributions among your variables, then Pearson is inapplicable, but Spearman may recover some informativeness. Spearman can also handle triangular distributions quite well. For the regression models you listed, you'd have to run all three and then compare fits and significance. Non-linear may be more promising than linear. Logistic makes no assumptions about the distribution of the predictors (inputs), so that may be helpful. You might also consider running PCA (principal component analysis) on the variables, and then plot the first two principal component score vectors (n-tuples) against one another using variable labels to see if variables cluster together. Last, I would consider unsupervised manifold learning to reduce dimensionality and look for patterns among variables (or objects with the data transposed) using e.g. crisp K-means cluster, fuzzy-K-means cluster, diffusion maps, self-organizing maps, localized linear embeddings, Laplacian eigenmaps, locally preserved projections, unsupervised artificial neural networks, stochastic neighbor embedding, Sammon mapping, non-negative matrix factorization, classic multidimensional scaling, non-metric multidimensional scaling, unsupervised neural gas, Gaussian mixture models, unsupervised random forests, kernel distance-based PCA, kernel Gaussian radial basis function PCA, kernel Tanimoto distance-based PCA, and hierarchical cluster analysis. The ROI will be greater if you first focus on knowledge discovery through pattern recognition among objects and features. The key point about non-linear manifold learning methods is that you will always be reducing dimensionality to make 2D and 3D plots of the major 2 or 3 dimensions obtained from the various procedures. Clustering among objects or features may become apparent with several of the manifold learning approaches.
