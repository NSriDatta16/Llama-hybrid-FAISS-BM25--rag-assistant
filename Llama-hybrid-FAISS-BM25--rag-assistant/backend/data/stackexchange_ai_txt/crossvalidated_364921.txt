[site]: crossvalidated
[post_id]: 364921
[parent_id]: 364917
[tags]: 
Without knowing too much more about your specific implementation, what I can say is that neural networks by and large employ back-propagation/gradient descent in order to tune the network and improve the output's fit to the training data. This means that the loss function is differentiable , in most cases. The function you are trying to approximate has a large jump discontinuity, which would mean that most standard loss functions used in the optimization step of your neural network are going to fail at the discontinuity, and depending on the particular loss function and the details of the optimization algorithm used, will deal with this failure in any of various ways. If you were to apply a sigmoid layer at the end, for instance, you might be able to coax behavior that reasonably approximates the two lines "far enough" away from the discontinuity, but the output would still produce a continuous function connecting the two flat lines. Other approximation techniques will run into similar problems. You might want to check out the Gibbs Phenomenon ( https://en.wikipedia.org/wiki/Gibbs_phenomenon ) from Fourier analysis and 'ringing artifacts' occurring in signal processing in general.
