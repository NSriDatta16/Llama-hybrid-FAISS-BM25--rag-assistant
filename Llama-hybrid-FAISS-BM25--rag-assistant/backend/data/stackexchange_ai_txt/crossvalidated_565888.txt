[site]: crossvalidated
[post_id]: 565888
[parent_id]: 
[tags]: 
How many type I errors have I made in my career? (I saw this posted by a LinkedIn connection.)

I saw the following post on LinkedIn and was curious to have it dissected on Cross Validated. How many type I errors have I made over the years? I've run a lot of A/B tests in my career. I was just wondering: how many Type I errors have I made thus far, where I erroneously rejected the null hypothesis? One perspective says: none; no null hypothesis is ever "true" so no rejection is ever wrong. Okay fine, then how many times have I got the wrong direction on effect size, choosing the wrong winner? If I reject at p The tests where I didn't reject the null can't possibly be Type I errors, so they are irrelevant. If the other tests all miraculously had the same p-value, p, and there are N such tests, then I expect Np errors. E.g. p=0.05. If I had N1 tests where my p-value was p1, and N2 with p2, where N1+N2=N, then I'd expect N1 p1+N2 p2 errors, and so forth. So the lower my p-values are in all these tests the better my record. P.S. this is a sloppy proof but I have new parent brain. I feel like a more elegant proof would use the U(0, 1) distribution of the p values under H0, but I can't quite see it. This seems like a decent thing for me to track so I know how worried I should be about some past decision coming back to haunt me. Sort of like a "statistical debt" metric, analogous to tech debt. What is the validity of this line of thinking? How would we begin to approach estimating how many errors we’ve made? So far, I’ve thought that the final paragraph is a form of gambler’s fallacy, but I have not decided what I think about the rest.
