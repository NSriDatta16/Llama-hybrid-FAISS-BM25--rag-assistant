[site]: datascience
[post_id]: 35728
[parent_id]: 35715
[tags]: 
I think what the author is speaking about is the time/memory complexity of algorithms that statisticians may don't care about. Make a model which is mathematically well proven may be more important to statistician eyes than making approximation to render a model feasible in real life. I encourage you to look at the complexity of frequent mathematical operations . Often operation like Singular Value Decomposition, Matrix Inversion, Matrix transpose are used and their cost is way higher than the "upper bound scalable time complexity limit" $O(n.log(n))$ which prevent any utilization on massive datasets. As an example, you can easily imagine than we are rapidly limited with time complexity because we are not able to increase computational power -- or wait -- more than approximately linearly with the increase of problem size. Take a very common $O(n^2)$ complexity, you can't afford to wait $1000000$ more longer -- or multiply your computational power by $1000000$ to keep same app duration -- when you multiply your dataset size by $1000$ (except if your operations are really fast...)
