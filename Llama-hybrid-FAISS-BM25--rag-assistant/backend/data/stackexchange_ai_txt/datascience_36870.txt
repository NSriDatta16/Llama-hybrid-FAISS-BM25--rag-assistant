[site]: datascience
[post_id]: 36870
[parent_id]: 36862
[tags]: 
The choice of a metric depends on how you rank the importance of your classes and what you value from a classifier. Let's look at your example: For example if we have a data set with 90%-10% class distribution then a baseline classifier can achieve 90% accuracy by assigning the majority class label. One minor correction is that this way you can achieve a 90% micro-averaged accuracy. If your goal is for your classifier simply to maximize its hits and minimize its misses , this would be the way to go. However, if you valued the minority class the most, you should switch to a macro-averaged accuracy, where you would only get a 50% score. This metric is insensitive to the imbalance of the classes and treats them all as equal. In many applications the latter is preferable. Imagine a classification problem aiming at diagnosing a disease that appears in 1% of the population. What good is a classifier that would always predict that the patient was healthy, even if it could achieve a 99% micro-averaged accuracy on the task? The reason why micro-averaging is prevalent is because in most tasks, we would be interested in simply maximizing the number of correct predictions the classifier makes. In these tasks no class is more important than the others.
