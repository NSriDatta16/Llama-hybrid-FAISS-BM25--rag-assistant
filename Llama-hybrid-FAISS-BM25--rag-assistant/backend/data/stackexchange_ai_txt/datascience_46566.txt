[site]: datascience
[post_id]: 46566
[parent_id]: 46565
[tags]: 
You should do Outlier Analysis of your target variable to prepare your training data for the model. Most model would perform better on noiseless data, as Outlier might skew the findings of your model in one direction. Generally, there is no need to perform normalization on target variable for model performance or accuracy. (Though it might be useful to do some analysis on target variable to get some useful business insights out of it) Reasons behind performing normalization on input variables are as follows: 1) Feature scaling improves convergence of steepest descent algorithms 2) Helps to avoid a situation when several variables dominate other variables in magnitude While if you normalize target variable, it, in turn, will normalize MSE and there will be no impact on results. Cases when you might choose to do Normalization of Target Variables: Neural Networks update weights of nodes based on the error generated by the cost function via back propagation, large errors might cause drastic change in the weights and could make learning process unstable. Results of which optimizer might not be able to settle on Optimal Minima. The only time when you might want to normalize target is the case of floating point overflow. Sometimes the number is too large or too small that CPU memory can't handle it and will turn into INF or wrap-around to the other extreme representation.
