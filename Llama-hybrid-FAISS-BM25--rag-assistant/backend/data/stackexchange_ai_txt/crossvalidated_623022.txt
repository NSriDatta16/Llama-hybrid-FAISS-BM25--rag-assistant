[site]: crossvalidated
[post_id]: 623022
[parent_id]: 
[tags]: 
Is the equation of the derivative of a loss function relative to the input to the Sigmoid (z) the same whether computed backward or forward?

I am referring to the derivative of the binary cross-entropy loss function for logistic regression. Using back-propagation, the derivative of the loss function L with respect to z is a - y , where a is the output of the sigmoid function given z . Here, z is the polynomial that takes the parameters w , data values x , and bias value b . Assume I tried to find an equation for the derivative of the loss function from left to right. I think I would still get dL/dz = a - y as if I used back-propagation. Is that correct? As a side note, how would you calculate the derivative from left to right? Is it practical or useful in some situations? This StackExchange question gives more details about how this derivative equation is made: Derivative of the loss function w.r.t to X for the backpropagation I also used this article as reference: Derivation of DL/dz
