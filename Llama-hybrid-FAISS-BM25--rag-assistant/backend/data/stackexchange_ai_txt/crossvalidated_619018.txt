[site]: crossvalidated
[post_id]: 619018
[parent_id]: 
[tags]: 
How to aggregate semantic segmentation results from a ML model?

I'm working on a semantic segmentation system using aerial imagery. To make this question concise, I'll present a toy version of the problem I want to solve. Assume we are trying to produce a semantic segmentation for a large rectangular region $M$ . We want to classify each pixel in $M$ into one of three classes from the set {field, road, forest} . As my drone flies, I capture many successive images. Each image $I_1, I_2, I_3, \dots$ covers a small rectangular portion of $M$ . My goal is to aggregate all of these individual images $I_i$ and produce a single, best classification for each pixel in $M$ . We can assume that the "world is static" from the perspective of our segmentation system - the ground that we are inspecting will not be changing semantic class throughout the data collection phase. We typically observe the same region of $M$ many times throughout our flight. However, some observations are more informative than others! One observation might have clear line of sight, whereas another observation might be occluded by smoke and low clouds etc. I have collected and labeled a large set of images. I'm able to build a UNet that can produce decent semantic segmentation results. However, I'm looking for a principled way to aggregate these predictions. The simplest thing I've tried is simply taking the most common prediction - for example, I would loop over all predictions I have for a particular world pixel in $M$ and take the most common class. Another option would be to sum the "logits" before the final prediction, and then take the class with the highest summed probability. However, both of these options don't leverage the fact that some predictions are better than others. They effectively assign "equal weight" to non-usable images where the ground is occluded by clouds etc. . I'm looking for research into problems like this but I don't even know the right words to search for etc. I'm digging into more complicated ideas like updating the ML model to predict a per-example confidence weight $w$ and changing the training loss function to "learn" a way to aggregate classifications. For example, inside a single mini-batch I could insert K images that all see the same patch of M . I would keep my existing per-image loss, but also augment with an additional new loss. I would also modify the network to output an additional per-image scalar confidence weight. The new loss would be something like: $$ | L - \frac{\sum_{j=1}^{j=K} w_j * V_j}{\sum_{j=1}^{j=K} w_j}| $$ Here $w_j$ is the new learned confidence weight for image j , $L$ is the one-hot label vector (a vector of all 0's except with a 1 in the correct class' index), $V_j$ is the Unet's output before the SoftMax. Translated into English, this loss is indicating "the weighted average of the predictions from each image should match the label" The weights $w_i$ are a learned output from the model. These learned weights are intended to describe how confident the model is in a particular classification.
