[site]: datascience
[post_id]: 81237
[parent_id]: 
[tags]: 
IIoT Sound Classification with Little Data

[Problem Statement] I have been working on a sound a classification problem with less 200 sound files (wav format) with the following imbalanced class distribution: Class1 110 Class2 50 Class3 20 Class4 12 Here are Time-Amplitude representation based on librosa.display.waveplot for a random sample from each class: At present I am either over-fitting or have a low train/test accuracy. I am aware the more training data the better esp. for Class3 and Class4, however, since data labelling for this task is an expensive process, I am exploring ways to avoid that as much as I can, and still obtain a decent model (e.g. a model with about 90% validation accuracy) with data at hand. Sounds are from a machine in a manufacturing line, thus naturally noisy and with varying lengths for each Class. [Attempts] So far I have tried the followings: Mel-Spectrogram representation: I have converted audio files to log-scale Mel-Spectrogram (basically images as shown below) and performed a multi-class image classification, as well as transfer learning. Also I have used Google AutoML Vision to conduct transfer learning. Whatever I do, I cannot do better than 60-65% accuracy. Mel-frequency cepstral coefficients (MFCCs): I have tried to extract MFCCs features using librosa as below: audio, sample_rate = librosa.load(file, res_type='kaiser_fast') mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40) mfccsscaled = np.mean(mfccs.T,axis=0) and feed extracted features into a Fully Connected NN (FCNN) or CNN. YET I have facing not-easily-solvable over-fitting (things I have looked at: model architecture e.g. very simple to complex, regularization via l1 or l2 regularizers, class weight, feature scaling, batch normalizations, weight initializers, dropout, optimizers, change of loss, learning rate more and more), still I get somewhat like: Most likely due to this over-fitting, I am facing a known only-one-class prediction (meaning NN mostly predicts one class), and strangely, this class is in fact a minority Class3! [Questions] Are there any other methods like Transfer Learning for Images but for Audios? How about WaveNet, and directly train on Audio files? I cannot seem to find an official implementation, or at least something reliable. Anyone experienced using WaveNet? Any recommendations how to make this model somehow better? I am leaning towards the fact it is within the nature of the audios, that is hard to distinguish, still I don't get it how model can be fooled on the training set up to 100% accuracy then (well accuracy might be a good indicator as well). I have looked/am still looking into many posts, examples, suggestions to tackle this problem (list is too long, I wouldn't list them here), but rather curious if there are anything that I fundamentally miss here. Maybe simply I gotta stop and collect more data at any costs (although I doubt that would solve the problem completely), what if noise is my enemy! [Last but not least, Data Collection] It may a bit biased towards certain samples/classes. Image below shows a schematic view how it is done at the moment: Blue shows a box where certain items (shown in black) are inserted. Microphone that is placed at the top side of the box records each of these black-item placement for inspection. However, it is noted that distance from Microphone varies to the items by 5-10 cm! For example, black items on the left may all belong to Class2, but still produce a slightly different sound wave due to their varying position to the Mic. This may be negligible, but cannot be just yet ruled out!
