[site]: datascience
[post_id]: 124282
[parent_id]: 124238
[tags]: 
As hinted by the comment there are several approaches but only one that really matches the accelleration of a "block of plain Python code" using a GPU/TPU. This is different to how can python code be accellerated in general as you've specifically asked for it accellerated by a GPU/TPU . If using an Nvidia GPU, python can be accellerated using Nvidia CUDA [1], which is really a set of bindings to CUDA from python and therefore not very pythonic (have to specify types and structures etc). If the majority of your manipulations are array based you can use higher level libraries such as CuPy [2] (Numpy and SciPy equivalent) but like the other elements in the RAPIDS [3] ecosystem is focused around data science uses cases (DataFrame, Networks etc). The most accesible way to use CUDA Python is via numba , for which there are several tutorials/videos that show how to convert your code into a vector format such that it can be accelerated by a GPU [4], [5]. My recommendation is understand how to use the numba @jit decorator first [6] before looking to apply GPU based accelleration. It should become apparent that the cost of using python in this way, is that it becomes less pythonic. [1] https://nvidia.github.io/cuda-python/overview.html [2] https://cupy.dev/ [3] https://rapids.ai/ [4] https://thedatafrog.com/en/articles/boost-python-gpu/ [5] https://www.kaggle.com/code/harshwalia/1-introduction-to-cuda-python-with-numba [6] https://numba.pydata.org/numba-doc/latest/user/5minguide.html
