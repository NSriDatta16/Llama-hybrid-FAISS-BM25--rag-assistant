[site]: crossvalidated
[post_id]: 327480
[parent_id]: 327448
[tags]: 
Oh yes, that can certainly happen. One advantage of random forests is that they can model nonlinearities in your data. Thus, they can in principle classify perfectly even if your data are not linearly separable. It is enough if they are nonlinearly separable. Here is an example in R: set.seed(1) xx 0.3) plot(as.numeric(yy),xx,xlab="",xaxt="n",pch=19) axis(1,1:2,levels(yy)) library(randomForest) model Note how the predictor cannot be linearly separated, but nonlinearly:
