[site]: crossvalidated
[post_id]: 579449
[parent_id]: 579441
[tags]: 
Answering the general question, using back-propagation and gradient-based optimization algorithms is not the only possible way. There is a whole family of derivative-free optimization algorithms and applying them to neural networks is an open area of research (if you search for it , you'd find many papers). Derivative-free optimization algorithms don't need the functions to be differentiable. Why then are they not commonly used? The major reason is that they are often slower in convergence time as compared to algorithms that use derivatives. In fact, neural networks are designed to be differentiable, so that we can efficiently train them. They also do not necessarily scale that well, are often not implemented out-of-the-box, and there is no hardware optimized for using them as with GPUs. If you look at the examples, derivative-free optimization can work for neural networks. Aly et al (2019) found out that LS outperformed SGD in terms of final accuracy and loss metrics. However, SGD was found to be up to 50X faster than LS. Finally, there is nothing special or magical in gradient descent that makes it "the best" algorithm for the problem, it is just popular because it proved to work well for many problems. While it's not an example of a derivative-free approach, Ali Rahimi (2017) in his Test-of-Time Award NeurIPS talk gave an example where gradient descent (and ADAM) fail miserably as compared to using Levenbergâ€“Marquardt algorithm. This shows that other approaches are possible, though less studied.
