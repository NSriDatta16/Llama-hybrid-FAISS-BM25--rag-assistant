[site]: crossvalidated
[post_id]: 630414
[parent_id]: 630316
[tags]: 
TL;DR: you almost never want to "adjust" for a false discovery rate; that's not a real thing. "Adjusted" p-values are unintuitive, mostly reviled by statisticians as an inelegant hack, and bad summary statistics (as they're deeply unintuitive). What you really want is a simultaneous estimation approach or a multilevel model . Full version: Suppose two researchers are trying to cure a disease. The first one does a trial testing one medication, gets a p-value of .01, writes up his results, and sends them off to a paper for publication. He repeats this process 10 times. The second researcher does the same experiments and gets the same results, but he does all 10 trials at the same time. Each of these trials individually has an individual p-value of .01, but he applies a Bonferroni correction. From the exact same data, we infer a p-value of 0.1 and cannot reject the null. Here's my question to you: does this make even the tiniest bit of sense? Does the human body care whether you're publishing your results in one paper or two? No, of course not! This is insane! But what about p-hacking and data dredging? If we test 100 different drugs and only one of them seems to work, chances are that one was probably a fluke. This is the intuition that misleads us into multiple comparisons. It's a good intuition, but the wrong solution. Our intuition is saying that if the first 99 drugs failed, then before the experiment, we should think that the 100th probably won't work either--i.e. we assign a low prior probability that the 100th drug will work. The correct way to resolve this is a random effects or hierarchical model. These models learn the correct prior from your data and partially pool estimates together. They're called random effects models because the drugs in each trial are no longer treated as completely independent. Instead, they're assumed to be random values that come from the same distribution; this lets us use the information from the first 99 drugs to predict how the 100th will go. This completely solves the multiple comparisons problem. Not only that, it even improves your power at the same time! It's a free lunch.
