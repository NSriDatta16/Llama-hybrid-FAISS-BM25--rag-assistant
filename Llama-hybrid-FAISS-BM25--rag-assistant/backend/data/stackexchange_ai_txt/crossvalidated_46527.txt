[site]: crossvalidated
[post_id]: 46527
[parent_id]: 46507
[tags]: 
There are many machine learning methods that do aim to estimate the conditional mean of the data, such as artificial neural networks, but also there are many that do not (such as SVMs, decision trees etc.). The motivation of the SVM is that it is better to solve the particular problem at hand directly, rather than solve a more general problem and simplify the result. So if you are only interested in a hard binary classification, in principle that ought to be easier than estimating the a-posteriori probability of class membership and then thresholding at 0.5. Whether that is true in practice is debatable, but also in my experience in practice you often do want the a-posteriori probabilities becase training set and operational class frequencies are different or variable, or equivalently the misclassification costs are not known at training time or are variable, or you need a reject option etc. So whether a particular method estimates the conditional mean of the response variable depends on what task the method intended to solve. Note for the SVM there is an alternative that does estimate the conditional mean of the data, namely kernel logistic regression for classification and kernel ridge regression for regression problems. The loss function that is minimised has a lot to do with whether the model predicts the conditional mean of the response variable, pretty much any method that minimises a sum of squared error loss (or cross-entropy for classification) will have this property, see e.g. Saerens, M., "Building cost functions minimizing to some summary statistics", IEEE Transactions on Neural Networks, volume: 11 , issue: 6, pages 1263 - 1271, 2000.
