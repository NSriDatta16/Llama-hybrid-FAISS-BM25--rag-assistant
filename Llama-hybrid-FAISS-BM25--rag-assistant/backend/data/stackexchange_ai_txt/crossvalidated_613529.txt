[site]: crossvalidated
[post_id]: 613529
[parent_id]: 613525
[tags]: 
Imagine an intercept-only logistic regression model $$ P(y=1) = E[y] = g^{-1}(\beta_0) $$ In such a model, $\beta_0$ transformed via the logistic function $g^{-1}$ is the mean of $y$ . If we add a parameter $x$ to the model, it becomes $$ P(y=1|x) = E[y|x] = g^{-1}(\beta_0 + \beta_1 x) $$ Now $\beta_0$ would correct the bias of the model. Recall that by bias we would mean here that $E[y] \ne E[g^{-1}(\beta_0 + \beta_1 x)]$ . Logistic regression is a linear model, so let's take one step back to linear equations for a while. In a linear equation, $ax + b$ changing $a$ would shift the line vertically (see the image below taken from here ). In the regression case, shifting it over the $y$ axis is used for correcting the bias since it changes the expected value of the predictions for $y$ by a constant for it to match with $E[y]$ . In the base rate correction, we are correcting for the fact that the base rate in the population that you are making the predictions for differs from the sample that was used to fit your model assuming that the relationship between $x$ and $y$ is the same in both. We are not aiming to correct $\beta_1$ , nor do we assume that it differs. Now imagine that we "corrected" $\beta_1$ . This would lead to a completely different model! You could change the parameters to arbitrary values, but then you don't need any data at all, you can just make up the parameters and plug-in into the equation. If you assume that the relationship between $x$ and $y$ differs, then you need to fit a model on a new dataset that comes from the same population as the population you want to make the predictions for.
