[site]: crossvalidated
[post_id]: 526873
[parent_id]: 
[tags]: 
How do we update the parameters (weights) in recurrent neural networks?

In RNNs, how do we update the weights? I have following understanding of RNNs: Parameters are shared across all time-steps, i.e., $$S_t = \tanh(U X_t + W S_{t-1} )$$ $$Y_k = \text{softmax}(V S_t)$$ Here $W$ , $U$ and $V$ are weights. $S_t$ and $S_{t-1}$ is passage of information at time steps $t$ and $t-1$ respectively. Compute loss function by summing over ALL time steps. compute gradient of this loss function(which is summation over all time steps) wrt to $W$ , $U$ and $V$ . Now my question is when we have these gradients (over $W$ , $U$ and $V$ ), do we update Weights and then re-compute $Y_k$ at every time step and steps 2 and 3 are repeated or re-computation of $Y_k$ is done at only certain time steps? Weights updated by gradient descent method. I have followed following link for rnn: http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/
