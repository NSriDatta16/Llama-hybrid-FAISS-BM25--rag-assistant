[site]: crossvalidated
[post_id]: 154535
[parent_id]: 154439
[tags]: 
The quoted passage seems a bit vague and generalized, but here is my take on it: Limited dependencies: Your data doesn't have to be normally distributed or follow other mathematically convenient distributions. For example, boosted trees and support vector machines (via kernels) often perform very well even when your data is messy. Limited complexity: Some algorithms, like naive bayes and linear models are simple in their interpretability while others are simple in terms of their implementation (e.g., k-nearest neighbors). This is definitely not true across the board though - large GBMs or deep neural networks are incredibly complex. Smoothness: Yes, smoothness refers to the optimization function (like generalized linear models). However, not all machine learning algorithms have smooth optimization functions, for example random forests.
