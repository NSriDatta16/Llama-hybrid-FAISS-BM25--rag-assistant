[site]: crossvalidated
[post_id]: 469748
[parent_id]: 469652
[tags]: 
A neural network $ NN(x) = \sum_{j=1}^M w_j \sigma(x\cdot b_j) $ If we can apply the central limit theorem then $NN(x) \sim \mathcal{N}(0, K)$ as $M$ tends to infinity. Assume that $w$ and $b$ are iid with zero mean and variance $s_w, s_b$ . Then $E_w[NN(x)] = 0$ and $V[NN(x)] = E[NN(x) NN(x)^T] = s^2_w ME_b[\sigma(x\cdot b) \sigma(x\cdot b)^T]$ . Make this finite by letting $s^2_w$ scale with $M$ . Thus the central limit theorem can be applied. The infinite width comes from the CLT: If the width is infinite and the assumptions from 3 hold then the output of an infinitely wide NN is simply a normally distributed variable. The expected value is zero so all we need is the covariance matrix $K$ .
