[site]: crossvalidated
[post_id]: 367173
[parent_id]: 367141
[tags]: 
The attention "weights" are not weights in the sense that the word is usually used -- they aren't trainable parameters. They are usually computed as a function of the input and some other parameters, so for question answering, the weight might be $a_t = \text{softmax}(h_t^T W q)$, where $h$ is the hidden state, $W$ is a parameter, and $q$ is the question you're trying to answer. So $a_t$ is different for different inputs, but the weight matrix $W$ is learnable and can be fixed.
