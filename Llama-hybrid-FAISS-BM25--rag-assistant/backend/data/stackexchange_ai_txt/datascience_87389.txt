[site]: datascience
[post_id]: 87389
[parent_id]: 
[tags]: 
Inference order in BERT masking task

In BERT, multiple words in a single sentence can be masked at once. Does the model infer all of those words at once or iterate over them in either left to right or some other order? For example: The dog walked in the park. Can be masked as The [mask] walked in [mask] park. In what order (if any) are these tokens predicted? If you have any further reading on the topic, I'd appreciate it as well.
