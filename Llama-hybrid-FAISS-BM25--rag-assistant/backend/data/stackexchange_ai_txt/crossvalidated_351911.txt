[site]: crossvalidated
[post_id]: 351911
[parent_id]: 
[tags]: 
Combining Gradient boosted trees after multiple imputation

Currently I am working with a gradient boosted tree model fit onto a multiple imputed dataset. For those who don't know multiple imputation: It predicts missing values and imputes that value with added noise. It does this m times and thus creates m data sets. These data sets are then analyzed separately and their results are combined into one result. Now I have used a gradient boosted tree to build a classifier. This results in m separate boosted models. However I cannot find a way to combine these. In regression one would simply average the regression coefficients. I have looked at these, but they don't give me any clues as how to pool these models. Reconciling boosted regression trees (BRT), generalized boosted models (GBM), and gradient boosting machine (GBM) https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/ Gradient in Gradient Boosting This one is about combining trees, but not in a way that is useful for my situation. Combine decision trees from GBM to reduce output The question: Is there a way to combine these gradient boosted tree based models into one?
