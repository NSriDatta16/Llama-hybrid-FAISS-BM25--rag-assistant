[site]: crossvalidated
[post_id]: 466476
[parent_id]: 466471
[tags]: 
There is no "best" activation function. If there were, all the neural network architectures would stick to the single "best" one, while what we see is something opposite: different neural networks, or even different layers of single network, use different activation functions . Some of them, like ReLU , are more popular then others, but there are cases where they don't work and you need to use different activation functions instead. People usually do what you did: start with what is currently known to be most promising solution, but if it fails, try also the alternatives.
