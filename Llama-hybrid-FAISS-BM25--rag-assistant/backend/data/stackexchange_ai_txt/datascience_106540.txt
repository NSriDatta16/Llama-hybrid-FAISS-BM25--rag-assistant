[site]: datascience
[post_id]: 106540
[parent_id]: 
[tags]: 
How can one ensure the quality of collectively created ground truth labels?

Here is the situation: I have a small tool to assign (one or multiple) labels to image data. In the tool, you can not only assign a label(s) to the images but also assign a score for the certainty of the label. I want to use this to generate a ground truth for my image data, using a group of jurors who will do the classification by hand. The question is now, how do I make sure that the labels, that the jurors chose, are correct? To me, the only obvious approach would be to have multiple people classify the same image and combine the results. However, I am not sure about the actual method of combination. Ideas I came up with are: take the most chosen label per image (which would not work for multi-label) calculate the average/min/max score for each label/image set a threshold for the number of times a label has to be chosen I would then use all of these approaches and try to find out which generates the 'best' looking results. All of these approaches are, however, just ideas (and may or may not make actual sense). I did not yet find a proper article or book chapter that covers this topic. Are there any ways this is done 'properly'? I suppose there are no other ways to ensure the quality of the ground truth since it is only as good as the jurors?
