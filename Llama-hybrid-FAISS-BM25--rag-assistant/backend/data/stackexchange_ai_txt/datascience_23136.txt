[site]: datascience
[post_id]: 23136
[parent_id]: 
[tags]: 
Early stopping and bounds

Say I am training neural networks using a train set and set aside a validation set V. I obtain models h's after each epoch along with the validation losses(0-1 loss) $\hat{L}(h_1,V)$, $\hat{L}(h_2,V)$ ... if I use the early stopping rule suggested here (top answer). Is the resulting $\hat{L}(h_*,V)$ an unbiased estimate of the true loss? How can I bound the true loss using $\hat{L}(h_*,V)$ ? I'm guessing no for the first one since the stopping rule depends on the partition of my data set. Afaik the bounds that can be applied depends on the size of my hypothesis set and I'm not entirely sure if it's finite, countable or uncountable in this case.
