[site]: crossvalidated
[post_id]: 336065
[parent_id]: 200063
[tags]: 
Adam uses the initial learning rate, or step size according to the original paper's terminology, while adaptively computing updates. Step size also gives an approximate bound for updates. In this regard, I think it is a good idea to reduce step size towards the end of training. This is also supported by a recent work from NIPS 2017: The Marginal Value of Adaptive Gradient Methods in Machine Learning . The last line in Section 4: Deep Learning Experiments says Though conventional wisdom suggests that Adam does not require tuning, we find that tuning the initial learning rate and decay scheme for Adam yields significant improvements over its default settings in all cases. Last but not least, the paper suggests that we use SGD anyways.
