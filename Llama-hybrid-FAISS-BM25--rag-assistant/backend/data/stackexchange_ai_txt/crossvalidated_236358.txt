[site]: crossvalidated
[post_id]: 236358
[parent_id]: 236328
[tags]: 
@Ferdi already provided clear answer to your question, but let's make it a little bit more formal. Let $X_1,\dots,X_n$ be your sample of independent and identically distributed random variables from distribution $F$ . You are interested in estimating unknown but fixed quantity $\theta$ , using estimator $g$ being a function of $X_1,\dots,X_n$ . Since $g$ is a function of random variables, estimate $$ \hat\theta_n = g(X_1,\dots,X_n)$$ is also a random variable. We define bias as $$ \mathrm{bias}(\hat\theta_n) = \mathbb{E}_\theta(\hat\theta_n) - \theta $$ estimator is unbiased when $\mathbb{E}_\theta(\hat\theta_n) = \theta$ . Saying it in plain English: we are dealing with random variables , so unless it's degenerate , if we took different samples, we could expect to observe different data and so different estimates. Nonetheless, we could expect that across different samples "on average" estimated $\hat\theta_n$ would be "right" if the estimator is unbiased. So it would not be always right, but "on average" it would be right. It simply cannot always be "right" because of randomness associated with the data. As others already noted, the fact that your estimate gets "closer" to estimated quantity as your sample grows, i.e. that in converges in probability $$ \hat\theta_n \overset{P}{\to} \theta $$ has to do with estimators consistency , not unbiasedness. Unbiasedness alone does not tell us anything about sample size and its relation to obtained estimates. Moreover, unbiased estimators are not always available and not always preferable over biased ones. For example, after considering bias-variance tradeoff you may be willing to consider using estimator with greater bias, but smaller variance -- so "on average" it would be farther from the true value, but more often (smaller variance) the estimates would be closer to the true value, then in case of unbiased estimator.
