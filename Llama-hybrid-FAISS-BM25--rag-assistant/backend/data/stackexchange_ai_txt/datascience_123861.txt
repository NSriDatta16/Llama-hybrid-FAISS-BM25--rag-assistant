[site]: datascience
[post_id]: 123861
[parent_id]: 123849
[tags]: 
Let's consider the main approaches: Embed each categorical feature, then LSTM on top. I think you want to opt for individual recurrent layers when the features are sort of mutually exclusive , i.e. the information contained in one feature is not beneficial for the others, and vice-versa. Also this method can be computationally slower since you have multiple recurrent layers, and maybe also more difficult to train (again, due to multiple BPTT.) The final step is then to aggregate (merge) all the features learned by the LSTMs for the last layer. Merge the various embeddings, then recurrence. Otherwise you can think of combining the features first with a dense or even linear layer. In this way you can also reduce the dimensionality of the resulting embedding, which can be beneficial also to remove some redundant information. There are multiple ways to aggregate, like addition and multiplication (assuming same dimensionality of embeddings) or the usual concatenation , which is usually followed by a dense layer. Once the embeddings have been merged, you apply the LSTM on top followed by the final layer. Here you have a single recurrence that takes in a combination of all the embedded features, learning one single joint feature space. Indeed, to find the best solution for your problem you should compare both approaches.
