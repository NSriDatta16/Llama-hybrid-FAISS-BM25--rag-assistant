[site]: crossvalidated
[post_id]: 441799
[parent_id]: 
[tags]: 
Ensembling text classification heads on top of a shared Language Model

As is practice in state-of-the-art text classification these days (BERT, XLNet and what have you), you've got a big language model trained on a huge unsupervised training set, which you then use as the backbone for a text classifier. The text classifier consists of a 'head', i.e. a layer of output neurons chained after the language model. The classifier head is then trained with supervised data. Often, but not necessarily, the language model is fine-tuned in this latter process, that is, the gradients from the head is backpropagated to the language model. Training multiple language models is prohibitively expensive, in terms of computation power and time. Training a classifier head is very cheap in comparison. I'm toying with the idea of training a single language model, and keeping it fixed while training multiple classification heads for the same task, to ensemble their predictions in order to increase performance on this one classification task. Perhaps the different heads each could be trained with a different shuffling of the data; different learning rates or dropout. Has anything like this been done before? I'm looking for guidelines to increase text classification accuracy.
