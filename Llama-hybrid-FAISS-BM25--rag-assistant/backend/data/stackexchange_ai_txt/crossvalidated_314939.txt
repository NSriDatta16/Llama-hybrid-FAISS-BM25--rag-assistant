[site]: crossvalidated
[post_id]: 314939
[parent_id]: 
[tags]: 
Why do we want (Restricted) Boltzmann Machines to be stochastic?

Boltzmann machines are stochastic recurrent neural networks with a specific architecture, and they are interesting in several contexts. They are stochastic in the sense that the sampling procedure involves repeatedly computing the activation functions for the next layer (hidden from visible or visible from hidden), sampling from the result, and using the obtained sample (as opposite to directly using the activation functions) for the next iteration. I do wonder though, why is the stochasticity useful here? If we want to compute the feature vector corresponding to a given input, do we use a sample from the hidden neurons' activations, or do we use directly the activations? Using the same architecture as the RBM, but with deterministic neurons, would seem to also fit more naturally with the probabilistic energy-based interpretation: the activations on the visible layer after a pass through the hidden layer would directly correspond to a "proper" conditional probability of getting that output given the input, as opposite to just being the conditional probability of that output given a sample obtained from the conditional probability over the hidden layer given the used input. The stochasticity seems to also make the training kind of weird. If the neurons are taken to be deterministic wouldn't the whole architecture effectively correspond to a regular feedforward NN with a given number of hidden layers (and some relations between the weights and biases of the layers), which could be trained using standard backpropagation?
