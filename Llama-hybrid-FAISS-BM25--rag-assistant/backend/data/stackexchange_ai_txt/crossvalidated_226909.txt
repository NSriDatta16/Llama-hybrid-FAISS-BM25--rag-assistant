[site]: crossvalidated
[post_id]: 226909
[parent_id]: 220485
[tags]: 
Some ideas when facing such a situation: it might happen if you have a very low number of labeled samples try training the same neural network several times on the same train/test split, to make sure that the variability of the performance you observed isn't due to the random initialization of the weight, or random creation of the mini-batches. the discussion should mention that the results are heavily impacted by the train/test split try spotting which samples result in a significant performance difference when included in the training set
