[site]: crossvalidated
[post_id]: 336226
[parent_id]: 336180
[tags]: 
There are two asymptotic approaches (they work better as $N$ gets big... towards 20 and beyond, say), depending on whether your binary outcomes ($O$) are paired (i.e. the explanatory variable ($E$) indicates two different times of measurement in the same individual , or in a pair of matched individuals ), or whether they are independent across the two values of $E$ (e.g., sampling from group $E=0$ and measuring your binary outcome in them, and independently sampling from group $E=1$ and measuring your outcome in them). Paired test for association between binary variables In the first case, using paired data, McNemar's test provides a way to test $N$ pairs of people for association between $E$ and $O$. Each pair contains one individual with $E=0$, and one individual with $E=1$. Because your data have this pairing, there are four possible kinds of pairs (the sum of all four cells equals $N$: Observed counts of pairs ------------------------------ O=0 & E=0 | O=0 & E=1 (r) --------------+--------------- O=1 & E=0 (s) | O=1 & E=1 McNemar's test works as follows: Formulate your hypotheses: $H_{0}: P(r) = P(s)$ $H_{A}: P(r) \ne P(s)$ where $r$ is the number of pairs where $O=1$ and $E=1$, and $P(r)$ is the probability of observing such a pair, and $s$ is the number of pairs where $O=1$ and $E=0$, and $P(s)$ is the probability of observing such a pair. Notice that $r$ and $s$ are discordant pairs. Calculate a test statistic from your data (the absolute value and the "$-1$" in the numerator are a continuity correction per Edwards): $\chi^{2}=\frac{\left(|r-s|-1\right)^{2}}{r+s}$ This $\chi^{2}$ test statistic has 1 degree of freedom, so you get $p = P\left(X^{2}\ge\chi^{2}_{\text{df}=1}\right)$, as a measure of how unlikely you are to observe your test statistic if $H_{0}$ is true. Decide to reject or not if $p$ is less than or equal to your preferred type I error rate $\alpha$ If you reject $H_0$ you conclude that you found evidence that $E$ is associated with $O$ (i.e. that $P(r)\ne P(s)$) at the $\alpha$ level of significance. If you fail to reject $H_0$ you conclude that you failed to find evidence that $E$ is associated with $O$ (i.e. failed to find that $P(r)\ne P(s)$). Unpaired tests for association between binary variables You have two choices that will get you to the same place: (1) a z test for difference in proportions, or (2) a 2-by-2 contingency table test. z test for proportion difference between two groups (1 and 2, say the values of $E$; $p_{1}$ is $P\left(O=1|E=1\right)$, and $N_{1}$ is the sample size of group 1, and $p_{2}$ is $P\left(O=1|E=2\right)$, and $N_{2}$ is the sample size of group 2): Formulate your hypotheses: $H_{0}: p_{1} - p_{2} = 0$ $H_{A}: p_{1} - p_{2} \ne 0$ Calculate a test statistic from your data (the $1(1/N_{1} + 1/N_{2})/2$ and absolute value bits are the continuity correction by Yates): $z= \frac{|\hat{p}_{1}-\hat{p}_{2}|-\frac{\left(\frac{1}{N_{1}}+\frac{1}{N_{2}}\right)}{2}}{\sqrt{\hat{p}\left(1-\hat{p}\right)\left(\frac{1}{N_{1}}+\frac{1}{N_{2}}\right)}}$ where $\hat{p}=\frac{\Sigma O=1}{N_{1}+N_{2}}$ (because if $H_{0}$ is true, then the best guess as to the true population probability of $O=1$ combines both groups into a single sample. This $z$ test statistic is distributed standard normal, so you get $p = P\left(|Z|\ge|z|\right)$, as a measure of how unlikely you are to observe your test statistic if $H_{0}$ is true. Decide to reject or not if $p$ is less than or equal to your preferred type I error rate $\alpha$ If you reject $H_0$ you conclude that you found evidence that $P\left(O=1\right)$ is different depending on which group you are in at the $\alpha$ level of significance. If you fail to reject $H_0$ you conclude that you failed to find evidence that $P\left(O=1\right)$ is different depending on which group you are in at the $\alpha$ level of significance. Instead you test the same unpaired data a different way using a 2-by-2 contingency table ($\chi^{2}$) test. However, you will organize your data as counts of individuals (not pairs) rather than proportions: Observed counts of individuals ------------------------------------------- O=1 & E=1 (a) | O=1 & E=2 (b) | r1 = a + b --------------+----------------+----------- O=0 & E=1 (c) | O=0 & E=2 (d) | r0 = c + d --------------|----------------+----------- c1 = a + c | c2 = b + d | N=a+b+c+d Formulate your hypotheses (can use the same ones as the z test): $H_{0}: p_{1} - p_{2} = 0$ $H_{A}: p_{1} - p_{2} \ne 0$ If $H_{0}$ is true, then you would expect the counts in a , b , c , and d to be based on the marginal totals (same marginal totals as in the observed counts table): Expected counts of individuals --------------+----------------+----------- c1(r1/N) | c2(r1/N) | r1 = a + b --------------+----------------+----------- c1(r0/N) | c2(r0/N) | r0 = c + d --------------|----------------+----------- c1 = a + c | c2 = b + d | N=a+b+c+d Calculate a test statistic from your data (the "$-1/2$" and absolute value bits are the continuity correction by Yates): $\chi^{2}= \frac{\left(|a - c1(r1/N)|-\frac{1}{2}\right)^{2}}{c1(r1/N)} + \frac{\left(|b - c2(r1/N)|-\frac{1}{2}\right)^{2}}{c2(r1/N)} + \frac{\left(|c - c1(r0/N)|-\frac{1}{2}\right)^{2}}{c1(r0/N)} + \frac{\left(|d - c2(r0/N)|-\frac{1}{2}\right)^{2}}{c2(r0/N)}$ This $\chi^{2}$ test statistic has 1 degree of freedom, so you get $p = P\left(X^{2}\ge\chi^{2}_{\text{df}=1}\right)$, as a measure of how unlikely you are to observe your test statistic if $H_{0}$ is true. Decide to reject or not if $p$ is less than or equal to your preferred type I error rate $\alpha$ If you reject $H_0$ you conclude that you found evidence that $P\left(O=1\right)$ is different depending on which group you are in at the $\alpha$ level of significance. If you fail to reject $H_0$ you conclude that you failed to find evidence that $P\left(O=1\right)$ is different depending on which group you are in at the $\alpha$ level of significance. Control variables You have two choices: (1) you can use an appropriate test as described above, and stratify your analysis on each level of a control variable(s), or you can switch to a regression format (there are several regressions for binary outcomes to choose from, the most common is, perhaps, logistic regression ) in which you would fit models of the form: $\text{logit}\left(O_{i}\right) = \beta_{0} + \beta_{E}E_{i} + \beta_{X}X_{i}$ where $X$ is a covariate, and the $\beta$s are interpreted as the natural log of the odds of $O$ ($\beta_{0}$ functioning as the intercept term for the logistic S-curve). References Edwards, A. L. (1948). Note on the “correction for continuity” in testing the significance of the difference between correlated proportions . Psychometrika , 13(3):185–187. McNemar, Q. (1947). Note on the sampling error of the difference between two correlated proportions or percentages . Psychometrika , 12(2):153–157. Yates, F. (1934). Contingency tables involving small numbers and the $\chi^{2}$ test . Supplement to the Journal of the Royal Statistical Society , 1(2):217–235.
