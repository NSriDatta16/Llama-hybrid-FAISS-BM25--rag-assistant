[site]: crossvalidated
[post_id]: 193834
[parent_id]: 193727
[tags]: 
The likelihood function of your observation that $10$ trials out of an unknown $n$ trials resulted in a success is $$L(n; 10, 0.1) = \binom{n}{10}(0.1)^{10}(0.9)^{n-10}, ~ n = 10, 11, 12, \ldots $$ To find the value of $n$ for which this likelihood function has maximum value, look at the ratio $$\frac{L(n+1; 10, 0.1)}{L(n; 10, 0.1)} = \frac{\binom{n+1}{10}(0.1)^{10}(0.9)^{n+1-10}}{\binom{n}{10}(0.1)^{10}(0.9)^{n-10}} = \frac{n+1}{n-9}0.9$$ which is Larger than $1$ (meaning that $L(n+1; 10, 0.1)>L(n; 10, 0.1)$) for $n = 10, 11, \ldots$ as long as $\displaystyle \frac{n+1}{n-9}0.9 > 1$, that is, $n Equal to $1$ (meaning that $L(n+1; 10, 0.1)= L(n; 10, 0.1)$) if $n = 99$. and Smaller than $1$ (meaning that $L(n+1; 10, 0.1) So, the likelihood function has two maxima (twin peaks, in fact), at $99$ and $100$ and either could be taken as the maximum-likelihood estimate of $n$. There is nothing Bayesian about all of the above. Let us suppose that the number of trials is $N$, an _ integer-valued random variable_ with probability mass function (pmf) $p_N(n), n = 0, 1, \ldots$. We do not make any assumptions about this (prior) pmf except to note that a discrete random variable with countably infinite support cannot possibly be uniformly distributed on the support. In this model, the conditional pmf of the observation $X$ is binomial $(n,p)$: $$p_{X\mid N}(k \mid n) = P\{X = k \mid N = n\} = \binom{n}{k} p^k(1-p)^{n-k}, k = 0, 1, \ldots n, \tag{1}$$ and the joint distribution of $X$ and $N$ is $$p_{X,N}(k,n) = p_{N}(n)\binom{n}{k} p^k(1-p)^{n-k}, k = 0, 1, \ldots n, n = k, k+1, \ldots \tag{2}$$ From this we can determine the marginal pmf $p_X(k)$ and get that the posterior pmf of $N$ given that the event $\{X=k\}$ has occurred is $$p_{N\mid X}(n\mid k) = \frac{p_N(n)\binom{n}{k} p^k(1-p)^{n-k}}{p_X(k)} = \frac{p_N(n)}{p_X(k)} \binom{n}{k} p^k(1-p)^{n-k}, n = k, k+1, \ldots\tag{3}$$ Now, the nonBayesian part comes about with the insistence that $p_{N\mid X}(n\mid k)$ must be of the form $$p_{N\mid X}(n\mid k) = M\binom{n}{k} p^k(1-p)^{n-k}, n = k, k+1, \ldots\tag{4}$$ where $M$ is a constant instead of being a function of $n$ as in $(3)$. That is, we ignore any prior information about the distribution of $N$ that we might have, or the model etc and simply insist that $(4)$ holds. So, what is the pmf in $(4)$? Well, if $Y$ is a negative binomial random variable with parameters $(k+1,p)$, then its pmf is $$p_Y(m) = \binom{m-1}{k}p^{k+1}(1-p)^{m-(k+1)}, ~ m = k+1, k+2, \ldots$$ and the pmf of $Y-1$, which takes on values $k, k+1, \ldots$ is \begin{align} p_{Y-1}(n) &= p_Y(n+1)\\ &= p\cdot \binom{n}{k}p^k(1-p)^{n-k}, ~ n = k, k+1, \ldots \tag{5} \end{align} that is, $M$ in $(4)$ has value $p$. Consequently, the conditional mean of $N$ given that $X = k$ is $$E[N\mid X = k] = E[Y-1] = \frac{k+1}{p}-1 = \frac kp + \frac{1-p}{p}.$$ When $p = 0.1$, we have that $$E[N\mid X = k] = \frac{k}{0.1} + \frac{0.9}{0.1} = \frac{k}{0.1} + 9.$$ In the revised version of the problem, the OP confesses that he has determined that E[Nt] = Ns/0.1 + 9 which matches the above answer perfectly. But, there is no Bayesian inconsistency here. Eq. $(4)$ is not the posterior distribution of $N$ that any Bayesian would find, and $\frac{k}{0.1} + 9$ is not the mean of the posterior distribution of $N$: it is the mean of the distribution of $Y-1$ as given in $(5)$.
