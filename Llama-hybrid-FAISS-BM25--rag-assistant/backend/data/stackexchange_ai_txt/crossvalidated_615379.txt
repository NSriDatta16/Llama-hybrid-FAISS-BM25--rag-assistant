[site]: crossvalidated
[post_id]: 615379
[parent_id]: 
[tags]: 
Learning a smooth, real-valued non-parametric probability distribution

I would like to train a neural network to learn a probability distribution. (Density estimation of a model-independent "true" probability density function of a given output.) That is, I have a set of training examples where a real-valued output is drawn from a certain (unknown, non-parametric) probability distribution, conditioned on a given set of input features. Given the set of input features of an unknown, I would like the neural network to output information about the probability distribution (for that set of input features), such that I could potentially do further processing on it. (e.g. randomly sample from it; note that I don't just want the neural network to predict the optimal expected value of the output, but rather the full probability density function). Given that there's a known range and effective minimum "resolution" of the output value (e.g. a difference of 1 unit is meaningful, but a difference of 0.1 unit is not), my general thoughts are to discretize the output space, then effectively do a multi-class classification of each of the output value bins, using softmax to normalize to probabilities. My understanding of the standard multiclass classification training process is that the predicted outputs should (ideally) converge to the underlying probability distribution, conditioned on the input features. (Please correct me if I'm wrong.) The hangup I have with the process is there's no ordinal coordination between the various bins. That is, each bin is considered independently of the others in such a multiclass classification process, such that the probability of bin 5 and 6 are no more correlated than bins 5 and 25. Due to the nature of the problem, I would expect there to be some correlation/smoothing of probability values at scales larger than that of the "resolution". (The probability density function is smooth and continuous, without major swings.) I'm wondering if there's any accepted loss functions or regularization procedures which would potentially help impose such a constraint in the functional form of the predicted probability distribution. (Or if there's an alternate formulation of the training which means my initial discretization idea is misguided.) I could probably ad hoc an additional loss term, but I was wondering if there might be something with a more well-supported theoretical justification.
