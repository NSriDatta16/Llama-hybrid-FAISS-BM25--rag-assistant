[site]: datascience
[post_id]: 121105
[parent_id]: 
[tags]: 
What is the point of final test set in K-fold cross-validation?

I am carrying out logistic regression for my binary classification problem, and I have validated the model with kfold cross-validation (k=10). I don't understand why I need to have a final test set, though: can't the performance of the model be evaluated based on the kfold validation (i.e. the metrics from the 10 resulting folds?). Because isn't the whole point of kfold that the model doesn't actually "see" the test data? And so a final test set seems redundant, but it also seems as if it is common practice, and I am wondering why? I've seen some mention of hyperparameter overtuning/overfitting, etc., but logistic regression doesn't have many hyperparameters to tune, anyway. So what is the point of the final test set, and do I need it, or are the performance metrics given from each of the 10 folds sufficient to evaluate the model's performance?
