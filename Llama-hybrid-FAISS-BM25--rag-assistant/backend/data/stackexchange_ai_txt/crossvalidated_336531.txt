[site]: crossvalidated
[post_id]: 336531
[parent_id]: 
[tags]: 
How to initialize policy for Mountain-Car problem?

I am trying to solve the discrete Mountain-Car problem from OpenAI gym using a simple policy gradient method. For now, my agent never actually starts making progress. In OpenAI's implementation, the agent gets a reward of -1 for every timestep, and the episodes ends when the agent reaches the top of the mountain, or when the 200 timesteps limit is reached. To only commence learning a useful policy, the agent needs to stumble on the top of the mountain at least a first time by chance. However, it seems to me that the odds of stumbling there by chance are really tiny (I ran a random agent for more than 5000 episodes and it never stumbled once on top). How do people usually initialize their policy on that problem? I don't want to cheat by giving my agent a heuristic about how to solve the problem but at the same time it feels like it could take a very long time before it only can start to learn.
