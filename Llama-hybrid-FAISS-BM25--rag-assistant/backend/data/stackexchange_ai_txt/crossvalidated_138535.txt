[site]: crossvalidated
[post_id]: 138535
[parent_id]: 138471
[tags]: 
It is possible for a confidence interval of a mean not to include the sample mean. It is not part of the definition of a CI that it must always cover the sample mean. Thus one may, in theory, construct a CI procedure that never covers the sample mean. But most people would consider that a bad procedure. Let us therefore examine CI procedures that not only have been seriously proposed, but have been studied and found to be good. One of them is the "generalized lognormal confidence interval" procedure explained and studied ( via simulation) by Ulf Olsson in the Journal of Statistics Education (Volume 13, Number 1 (2005), http://www.amstat.org/publications/jse/v13n1/olsson.html ). This procedure is a reasonable one to use when the (natural) logarithms of the $n$ data are assumed to be independent and identically distributed with a Normal distribution. Recall that when the population mean log is $\mu$ and the population standard deviation of the logarithms is $\sigma$, then the population mean is $\exp(\mu + \sigma^2/2)$. (This relationship uses the lognormal assumption.) We will obtain confidence limits for $\mu+\sigma^2/2$; exponentiating them will give confidence limits for the population mean. The procedure is based on a "generalized confidence interval" known to produce good confidence intervals for complicated combinations of parameters like this one. Calculate the mean logarithm $\bar y$ and the sample variance of the logarithms $s^2$ from the data. A symmetric confidence interval of size $\alpha$ for $\mu + \sigma^2/2$ is found by identifying the middle $100 - 100\alpha\%$ of the distribution of $$T_{2} = \bar y - Z \sqrt{A^2/n} + A^2/2$$ where $Z$ and $A^2$ are independent variates, $Z$ has a standard Normal distribution, $$A^2 = \frac{s^2}{U^2 / (n-1)},$$ and $U^2$ has a chi-squared distribution with $n-1$ degrees of freedom. Because the distribution of $T_2$ is difficult to work with analytically, we may estimate it through simulation. When exponentiated, the $\alpha/2$ and $1-\alpha/2$ quantiles of $T_2$ are the lower and upper confidence limits for $\exp(\mu+\sigma^2/2)$. Olsson's work indicates that once $n \ge 20$ or so, this procedure tends to achieve its nominal characteristics for $\alpha=0.05$. That is, about $2.5\%$ of the time it is less than $\mu+\sigma^2/2$ and $2.5\%$ of the time it is greater than $\mu+\sigma^2/2$. One does not have to look hard to find datasets that (a) appear to meet the assumptions of this test yet for which (b) the confidence interval does not include the sample mean. Here is one with $n=50$: #0.08 0.14 0.21 0.25 0.28 0.3 0.35 0.37 0.39 0.41 0.46 0.51 0.55 0.55 0.66 0.66 0.69 0.71 0.74 0.74 0.77 0.81 0.85 1.04 1.09 1.1 1.17 1.18 1.19 1.25 1.29 1.38 1.54 1.62 1.62 1.68 1.74 1.87 2.11 2.29 2.37 2.42 2.93 2.99 4.8 5.12 5.94 7.09 11.26 120 The generalized CI of $(1.56, 3.95)$ does not include the sample mean of $4.03$. (This was computed using ten million simulated values of the distribution of $T_2$, so it should be pretty accurate. Twenty independent simulations using just a million simulated values never produced an upper limit larger than $4.02$, still below the sample mean.) Although the last few data values ($11.26, 120$) may look like outliers, their logarithms are not. Here is a histogram of their logs: OK, that final value of $\log(120)$ looks a wee bit high. But the (very) powerful Shapiro-Wilk test does not strongly reject the Normal hypothesis ($p = 0.012$). This provides some insight: lognormal (and other heavy-tailed) distributions frequently produce unusually large values by their very nature. Such values can strongly influence the sample mean but should have less influence on estimates of the underlying distributional properties. We shouldn't find anything paradoxical about this. (Although this example concerns a single group, it could be generalized to compare the difference of means between two groups, at some cost in complexity of the calculations. Nothing really changes, though: we may think a CI must include the sample mean only when we become so accustomed to using Normal-theory calculations that we come to believe, through sheer repetition, that all CIs must share their properties.) The following R code will reproduce these calculations and allow you to explore the properties of the generalized lognormal confidence interval. In particular, if you are wondering whether the failure of this CI sometimes to cover the sample mean might be due to an error in coding (a possibility that always worries me!), or if it isn't really a CI for the population mean in the first place, you can reproduce a part of Olsson's work by simulating the coverage of this CI, as in set.seed(17) x.mean = x.mean) # Fraction of times covering the true mean The output of $0.949$ shows that this nominal $95\%$ interval has covered the true mean $94.9\%$ of the time, which is excellent. (I chose this particular CI procedure specifically because it is so good.) By contrast, you could check how often this interval covers (say) the geometric mean: mean(sim[1, ] = exp(mu)) # Fraction of times covering the GM The output of $0.899$ confirms it is not a $95\%$ confidence interval for the geometric mean. Here is the full code (which you will need to compile before running the preceding lines). # # Generalized confidence intervals for lognormal means. # ci.generalized
