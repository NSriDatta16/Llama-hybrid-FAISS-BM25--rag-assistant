[site]: crossvalidated
[post_id]: 213737
[parent_id]: 155194
[tags]: 
You are right that you need a new covariance matrix computation on every iteration of gradient ascent. So if the matrix computation is not feasible for your setting, then, I think, you cannot use gradient-based marginal likelihood optimization. My suggestion is to use gradient-free methods for hyperparameter tuning, such as grid search, random search, or Bayesian optimization-based search . These methods are widely used for optimization hyperparameters of other machine learning algorithms e.g. SVMs. I suggest the grid search for your first try. You basically form a table (grid) of possible hyperparameters, try every one, and look for the best validation performance (or best marginal likelihood). Grid search would yield a suboptimal set of hyperparameters, and you have to specify grid by yourself.(tip: make grid in a log scale) but far less computation is needed. (and you don't need gradient!) If you are not familiar with grid search, you can look up Wikipedia:Hyperparameter Optimization - Grid Search
