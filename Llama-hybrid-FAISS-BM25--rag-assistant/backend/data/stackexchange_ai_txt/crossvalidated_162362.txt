[site]: crossvalidated
[post_id]: 162362
[parent_id]: 162353
[tags]: 
[edited 21.7.15 8:31 AM CEST] I suppose you used RF for classification. Because in this case, the algorithm produces fully grown trees with pure terminal nodes of only one target class. predict(model, data=X_train) This line of coding is like a dog chasing [~66% of] its own tail. The prediction of any training sample is the class of the training sample itself. For regression RF stops if node has 5 or less samples in it or if node is pure. Here prediction error will be small but not 0%. In machine learning we often work with large hypothesis spaces. This means there will always be many not yet falsified hypothesis/explanations/models to data structure of our training set. In classical statistics is the hypothesis space often small and therefore the direct model-fit is informative accordingly to some assumed probability theory. In machine learning does the direct lack-of-fit relate to the bias of the model. Bias is the "inflexibility" of the model. It does not in anyway provide a approximation of generalization power(the ability to predict new events). For algorithmic models cross-validation is the best tool to approximate generalization power, as no theory is formulated. However, if model assumptions of independent sampling fail, the model may be useless anyhow, even when a well performed cross-validation suggested otherwise. In the end, the strongest proof is to satisfyingly predict a number external test-sets of various origin. Back to CV: Out-of-bag is often a accepted type of CV. I would personally hold that OOB-CV provides similar results as 5-fold-CV, but this is a very small nuisance. If to compare let's say RF to SVM, then OOB-CV is not usefull as we would normally avoid to bag SVM. Instead then both SVM and RF would be embedded in the exact same cross-validation scheme e.g. 10-fold 10-repeats with matching partitions for each repeat. Any feature engineering steps would often also be needed to be cross-validated. If to keep things clean the entire data pipe-line could be embedded in the CV. If you tune your model with your test-set(or cross-validation) you're again inflating your hypothesis space and the validated prediction performance is likely over-optimistic. Instead you will need a calibration-set(or calibration CV-loop) to tune and a test validation set(or validation CV-loop) to assess your final optimal model. In the extreme sense, your validation score will only be unbiased if your never act on this result, when you see it. This is the paradox of validation, as why would we obtain a knowledge which is only true if you do not act on it. In practice the community willingly accepts some publication bias, where those researchers who got a over-optimistic validation at random are more likely to publish, than those who unluckily good a over-pessimistic validation. Therefore sometimes why can't reproduce others models.
