[site]: datascience
[post_id]: 120855
[parent_id]: 
[tags]: 
Batch normalization getting Val accuracy of 000e00

I am currently stuck on batch normalization. I have code written out but when I do it it keeps giving me a val_accuracy of 00e00 and stops after 2 epochs (I am wanting to run 20). I am not sure what to change or add to fix it. My code is below: import tensorflow as tf from tensorflow.keras.utils import load_img, img_to_array from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, LeakyReLU, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D from tensorflow.keras.models import Model, Sequential from tensorflow.keras import optimizers from tensorflow.keras.optimizers import Adam, SGD, RMSprop # First Layer model2 = Sequential() model2.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), padding = 'same', activation = tf.keras.layers.LeakyReLU(alpha = 0.2))) #layer 1 model2.add(MaxPooling2D()) model2.add(BatchNormalization()) # Adding the bach normalization then flattening model2.add(Dropout(0.2)) model2.add(BatchNormalization()) model2.add(Flatten()) model2.add(Dense(2, activation = tf.keras.layers.LeakyReLU(alpha = 0.2))) adam = optimizers.Adam(learning_rate = 0.001) model2.summary() Model: "sequential_6" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_7 (Conv2D) (None, 64, 64, 32) 896 max_pooling2d_7 (MaxPooling (None, 32, 32, 32) 0 2D) batch_normalization_11 (Bat (None, 32, 32, 32) 128 chNormalization) dropout_4 (Dropout) (None, 32, 32, 32) 0 batch_normalization_12 (Bat (None, 32, 32, 32) 128 chNormalization) flatten_4 (Flatten) (None, 32768) 0 dense_8 (Dense) (None, 2) 65538 ================================================================= Total params: 66,690 Trainable params: 66,562 Non-trainable params: 128 ________________________________________ model2.compile(loss = "binary_crossentropy", optimizer = 'adam', metrics = ['accuracy']) callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2), ModelCheckpoint('.mdl_wts.hdf5', monitor = 'val_loss', save_best_only = True)] history2 = model2.fit(train_images, train_labels, batch_size = 32, callbacks = callbacks, validation_split = 0.2, epochs = 20, verbose = 1)
