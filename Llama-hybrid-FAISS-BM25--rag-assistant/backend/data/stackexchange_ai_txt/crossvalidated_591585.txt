[site]: crossvalidated
[post_id]: 591585
[parent_id]: 591541
[tags]: 
is there some way to gauge whether or not variables like x1 and x2 in fact increase the probability of observing a positive result when factoring in non-linear interactions When there's a significant interaction, there's no simple way to evaluate the overall associations between single predictors and outcome. The association between x1 and outcome depends of the values of x2 , and vice-versa. Even the single-predictor coefficients become tricky to interpret, as each represents the association with outcome only when its interacting predictors are at reference levels (interacting categorical predictors) or at 0 (interacting continuous predictors). Just centering a predictor can change the single-predictor coefficients of all the other predictors it interacts with. For inference on particular combinations of predictor values as you propose, you can use the formula for the variance of a sum of correlated variables to get standard errors and confidence intervals, based on the assumption of asymptotic normality of the coefficient estimates in logistic regression. That handles the "range of values" of the coefficient estimates. For inference on the overall association between outcome and a predictor involved in interactions or other nonlinear terms, you can perform a multiple-parameter Wald test on the set of coefficients involving the predictor. For those analyses you need to use the covariance matrix of the coefficient estimates, which is typically hidden from standard model summaries but is contained in the model object. There are many tools available for this type of post-modeling analysis in R, for example in the rms , emmeans , and car packages. I don't use statsmodels but I imagine that there is some similar functionality provided by Python packages.
