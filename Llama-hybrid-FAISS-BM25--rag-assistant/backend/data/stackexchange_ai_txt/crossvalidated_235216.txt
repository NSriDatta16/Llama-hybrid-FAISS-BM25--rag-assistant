[site]: crossvalidated
[post_id]: 235216
[parent_id]: 
[tags]: 
Least squares estimator in a time series $\{Y_t\}$

Let $\{Y_t\}$ be a stochastic process such that $$\begin{cases}Y_t=\beta x_t+z_t\\z_t=\varepsilon_t+\theta \varepsilon_{t-1}\\\varepsilon_t\sim WN(0,1)\end{cases}$$ where $WN$ means white noise (it's not a probability distribution) with $\mathbb{E}[\varepsilon_t]=0$ and $\text{Var}(\varepsilon_t)=1$. The $x_t$ values are constants not random. Find the least squares estimator of $\beta$ and the variance of estimator. Question: Is the following correct? Attempt: What I did is $$Q=\sum (y_t-(\beta x_t+z_t))^2=\sum y_t^2-2y_t(\beta x_t+z_t)+(\beta x_t+z_t)^2$$ $$=\sum y_t^2-2y_t\beta x_t-2y_tz_t+\beta^2x_t^2+2\beta x_tz_t+z_t^2$$ $$\frac{\partial Q}{\partial \beta}=\sum -2y_tx_t+2\beta\sum x_t^2-\sum x_tz_t=0$$ $$\Leftrightarrow \hat{\beta}=\frac{\sum y_tx_t-\sum x_tz_t}{\sum x_t^2}$$ Taking the second derivative $$\frac{\partial^2Q}{\partial\beta\partial\beta}=\sum x_t^2>0\qquad \forall t$$ then $\hat{\beta}$ is a minimum point and is the least square estimator. $$\text{Var}(\hat{\beta})=\text{Var}\left(\frac{\sum y_tx_t-\sum x_tz_t}{\sum x_t^2}\right)=\frac{1}{(\sum x_t^2)^2}\text{Var}\left(\sum y_tx_t-\sum x_tz_t\right)$$ $$=\frac{1}{(\sum x_t^2)^2}\big(\text{Var}\left(\sum y_tx_t\right)+\text{Var}\left(\sum x_tz_t\right)-2\text{Cov}\left(\sum y_tx_t,\sum x_tz_t)\right)$$ $$=\frac{1}{(\sum x_t^2)^2}\Big(\text{Var}\Big[\sum \beta x_t\Big(\varepsilon_t+\theta\varepsilon_{t-1}\Big)\Big]+\text{Var}\Big[\sum x_t\Big(\varepsilon_t+\theta\varepsilon_{t-1}\Big)\Big]-2\text{Cov}\Big[\sum \beta x_t\Big(\varepsilon_t+\theta\varepsilon_{t-1}\Big),\sum x_t\Big(\varepsilon_t+\theta\varepsilon_{t-1}\Big)\Big]\Big)$$ Is there any mistake in the estimator?
