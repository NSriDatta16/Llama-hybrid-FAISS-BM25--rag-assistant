[site]: crossvalidated
[post_id]: 66343
[parent_id]: 
[tags]: 
What is the standard procedure for evaluating a user-based CF algorithm with a dataset offline?

I have read some papers and other materials about the evaluation of recommender systems (RS). Most of them discuss the various properties of RS (e.g. accuracy, diversity, etc.), and metrics for different tasks (e.g. RMSE, precision, recall, etc.), and some protocols. But I am still not very clear about data partitioning and the detailed validation procedure. [Shani, Guy, and Asela Gunawardana. "Evaluating recommendation systems." Recommender systems handbook. Springer US, 2011. 257-297.] For example, I want to evaluate a basic user-based CF algorithm with rating feedback. The data format is user-item-rating. There're 100 users in the dataset. Now I want to use 5-fold cross validation method. This is my way: According to users, I randomly separate the original dataset into 5 folds. Each fold contains 20 users. For all users I set one same time instant, so that before and after this time instant, each user contains a number of ratings. The data of any user x before the instant is for creating user profile, the data after the instant is for validating rating prediction (user x as user being predicted) or just being hidden(user x in neighborhood selection). For each user i in all 5 folds, I create a user profile represented by rating vector{} for user i. For fold-1, for each user i in fold-1 I calculate user similarities between user i and the users in the fold-2 ~ fold-5, and set an appropriate value of k-NN to obtain the neighbourhood for each user. For each user i in fold-1, I get predicted ratings from his neighbourhood users (items no t used by the tested user) by an appropriate rating averaging equation. calculate prediction error for user i. Repeat step 2~6 for each fold (5 times all) and get the average prediction error. This is one round. Use different parameter combinations(e.g. k-NN neighborhood size), repeat step 2~7 with many times, so as to obtain the lowest prediction error (meaning best parameter combination). With the best parameter combination, I get the lowest prediction error, and this is the final evaluation result. I have two questions: Is this procedure correct for evaluating a user-based CF algorithm? If so, what are the specific data in this procedure corresponding to the concepts of "training set", "testing set" and "validation set" from Statistics?
