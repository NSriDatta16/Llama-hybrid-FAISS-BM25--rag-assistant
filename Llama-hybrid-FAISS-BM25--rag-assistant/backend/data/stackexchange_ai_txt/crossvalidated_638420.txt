[site]: crossvalidated
[post_id]: 638420
[parent_id]: 638035
[tags]: 
First, look at the data. I saved your sample data in a data frame CD45 and looked at boxplots: CD45$sample The values seem to have substantial skew, with some extreme outliers in most samples. My guess is that these data wouldn't readily meet the assumptions required for t -tests or the more complicated models that could take the grouping by samples into account. This is pretty typical of biological gene-transcript or protein expression data, for which errors tend to be approximately proportional to the values instead of independent of the values. As these aren't count data (at least after your normalization), the negative binomial model suggested in another answer isn't appropriate for what you show.* I couldn't get the suggested Gamma model to work on your data with default settings (although I think that some playing with initial parameter values would probably work). What often works in practice with such data is to use log-transformed values instead. I used a log2 transformation, so that a 1-unit difference corresponds to a doubling. The calculations now evaluate the mean values of the log2-transformed data. I also examined the distribution of values among cells within treatments. CD45[,"log2CD45"] The box plots look much better, and the distribution of log2CD45 values is clearly shifted to higher values in the Treated group. With only 10 data points here per sample, I couldn't reasonably do this further by sample. With your full data set it would make sense to examine the distribution among cells by both sample and treatment . What follows doesn't do any modeling of the potentially rich data about the distribution of values among cells within samples. In general, treatments might affect not only the mean (or mean-log) outcome values, but also their distributions among cells. We're ignoring that here. Then the question becomes what you consider the fundamental unit of observation. The most conservative is to treat the individual cells as technical replicates and the samples as the unit of observation. Find the average of the log-transformed values per sample, and then do a linear model; in this case, it's equivalent to a t -test among samples. meanLogAggregate |t|) # (Intercept) -2.5140 0.1346 -18.673 4.19e-09 # treatmentTreated 0.4887 0.1904 2.566 0.0281 Even in this most conservative analysis, there is a "statistically significant" difference in log2CD45 as a function of treatment. Apply your understanding of the subject matter to assess the practical significance of having Treated values about 1.4 times the Control values $(2^{0.4887}=1.40)$ . With your nicely balanced sample data, the following alternative approaches all provide the same estimates for (Intercept) and the treatmentTreated coefficient, the estimated treatment-associated difference in mean log2CD45 values. The alternatives can give different estimates for the standard errors of the coefficients. The least conservative analysis is to ignore the grouping by treatment and treat all single-cell observations as independent. As you sense, that might be a risky choice in general. lmIndep |t|) # (Intercept) -2.5140 0.1163 -21.623 The standard errors are lower; together with the much larger number of observations (now by cell instead of by sample) the p -values are much lower. Treating the samples as grouping factors is a reasonable compromise. There are two general ways to deal with this. One is to start with the model based on independence and to adjust the standard errors to account for the grouping. An example of this approach is to use a "sandwich" estimator, used internally by the coeftest() function in the lmtest package. library(lmtest) library(sandwich) coeftest(lmIndep, vcov.=vcovCL(lmIndep,cluster=~sample)) # Estimate Std. Error t value Pr(>|t|) # (Intercept) -2.51402 0.07004 -35.8941 Accounting for the grouping by sample this way slightly increased the standard error of the treatmentTreated coefficient. The second general way to handle grouping is to model the groups directly, in a "mixed model" with both fixed and random terms. In your case, this would mean allowing the estimated intercepts to vary among samples (the "random" effect), with the distribution of intercepts among samples usually fit to a zero-mean Gaussian distribution. The overall (Intercept) and treatment are the "fixed" effects. That's often done this way: library(lmerTest) lmer1 |t|) # (Intercept) -2.5140 0.1346 10.0000 -18.673 4.19e-09 # treatmentTreated 0.4887 0.1904 10.0000 2.566 0.0281 In this case, the standard error and p -values for these fixed effects in the mixed model are identical to those of the original model based solely on the mean values per sample! As you have 100 times as many observations as in this sample data set, there won't be much practical difference in which estimates of standard errors and degrees of freedom you use. It seems that there will be no question about the "statistical significance" of the effect of treatment . In other circumstances, however, you might need to pay attention to the different ways of adjusting for grouping of observations and their corresponding assumptions. *A negative binomial model might be appropriate for the non-normalized data, as the original data are presumably counts of protein fragments having masses mapped to CD45 fragments. That type of model is often used, for example, with RNAseq expression data.
