[site]: datascience
[post_id]: 121529
[parent_id]: 121526
[tags]: 
We should not mistake the K, Q and V vectors received by the multi-head attention block with those received by the scaled dot-product block. The K, Q and V vectors that are fed to the multi-head attention block are projected separately to a lower dimensional space for each of the attention heads so that each scaled-dot product can compute a different result. The dimension of the lower space is the original one divided by the number of heads. After the scaled dot-product, the results of the individual scaled dot-products are combined back into a single vector, recovering the original dimensionality. Only in the first attention layer, the values of the vectors fed to the multi-head attention block come from the embeddings. From the second layer on, the inputs come from the outputs of the previous layer.
