[site]: crossvalidated
[post_id]: 110708
[parent_id]: 110673
[tags]: 
Expectation of a conditional probability is the total probability, thus the expected probability would be just the probability that $x_k$ is the maximum, which in the case of continuous distribution is $1/n$, from which decreasing in $n$ immediately follows. (This was pointed out in the comments by whuber). In this answer, I also show how to formulate the expectation integral correctly and evaluate it to be $1/n$ - this is not needed for proving the result, but may be useful for learning purposes to see what went wrong in the original approach. The first integral is incorrect. The conditional probability of $x_k$ being the maximum (conditional on of $x_k$) is simply the probability that all others are less than (or equal, which does not matter in the continuous case) $x_k$: \begin{equation} \mathbb{P}(x_k = \max(x_1,\ldots,x_n) \mid x_k) = \mathbb{P}(x_1\leq x_k,\ldots,x_{k-1}\leq x_k,x_{k+1}\leq x_k,\ldots,x_n \leq x_k) \end{equation} Applying the independence property and the definition of the CDF $F_D$, we obtain \begin{equation} =\mathbb{P}(x_1\leq x_k)\,\ldots\,\mathbb{P}(x_{k-1}\leq x_k)\,\mathbb{P}(x_{k+1}\leq x_k)\ldots\,\mathbb{P}(x_n\leq x_k)=F_D(x_k)^{n-1}. \end{equation} Then, to obtain the expectation of this random variable, we average over different values of $x_k$ weighting by the PDF of $x_k$: \begin{equation} \mathbb{E}(\mathbb{P}(x_k = \max(x_1,\ldots,x_n) \mid x_k)) = \mathbb{E}(F_D(x_k)^{n-1}) = \int_{-\infty}^{\infty}f_D(x)F_D(x)^{n-1}dx. \end{equation} Using the chain rule, the derivative of $\frac{1}{n}F_D(x)^n$ w.r.t. $x$ is $\frac{1}{n}f_D(x)\,n\,F_D(x)^{n-1}$ $= f_D(x)F_D(x)^{n-1}$, and thus our last integral equals \begin{equation} = \left. \frac{1}{n}F_D(x)^n \right|_{x=-\infty}^{x=\infty} = \frac{1}{n}. \end{equation}
