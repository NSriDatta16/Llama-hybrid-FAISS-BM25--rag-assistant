[site]: datascience
[post_id]: 106646
[parent_id]: 106645
[tags]: 
Gradient descent is a numerical method used by a computer to calculate the minimum of a loss function. If that loss function is related to the likelihood function (such as negative log likelihood in logistic regression or a neural network), then the gradient descent is finding a maximum likelihood estimator of a parameter (the regression coefficients). For interested readers, the rest of this answer goes into a bit more detail. In ordinary least squares linear regression with a model matrix $X$ and observed dependent variables $y$ (the usual notation), under certain conditions, the maximum likelihood estimator of the regression coefficients is given by: $$ \hat\beta_{MLE}=(X^TX)^{-1}X^Ty $$ This is derived by calculus, and we get a closed-form solution. In other models, such logistic regression, we are not so lucky to have a closed-form solution. There is no equation to give those estimates. However, we define them implicitly in terms of the loss function $L(\hat\beta)$ , which is related to maximum likelihood estimator of the regression coefficients. $$ \underset{\hat\beta}{argmin}\text{ }L(\hat\beta) $$ Gradient descent is one approach to finding this value.
