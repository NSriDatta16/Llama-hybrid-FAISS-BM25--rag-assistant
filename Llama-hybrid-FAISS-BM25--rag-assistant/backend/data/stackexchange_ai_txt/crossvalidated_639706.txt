[site]: crossvalidated
[post_id]: 639706
[parent_id]: 639557
[tags]: 
Consider a data generating process $$Y=f(X)+\varepsilon$$ where $\varepsilon$ is independent of $x$ with $\mathbb E(\varepsilon)=0$ and $\text{Var}(\varepsilon)=\sigma^2_\varepsilon$ . According to Hastie et al. "The Elements of Statistical Learning" (2nd edition, 2009) Section 7.3 p. 223, we can derive an expression for the expected prediction error of a regression fit $\hat g(X)$ at an input point $X=x_0$ , using squared-error loss: \begin{align} \text{Err}(x_0) &=\mathbb E[(Y-\hat g(x_0))^2|X=x_0]\\ &=(\mathbb E[\hat g(x_0)−f(x_0)])^2+\mathbb E[(\hat g(x_0)−\mathbb E[\hat g(x_0)])^2]+\sigma^2_\varepsilon\\ &=\text{Bias}^2\ \ \ \quad\quad\quad\quad\quad\;\;+\text{Variance } \quad\quad\quad\quad\quad\quad+ \text{ Irreducible Error} \end{align} (where I use the notation $\text{Bias}^2$ instead of $\text{Bias}$ and $\hat g$ instead of $\hat f$ ). As noted further on p. 224 (and referred to in this post ), for linear models the averaged-over- $x_0$ bias can be further decomposed neatly into model bias and estimation bias , $$ \mathbb E_{x_0}\big[ (\mathbb E[\hat g(x_0)−f(x_0)])^2 \big] = \mathbb E_{x_0}\big[ (f(x_0)-x_0^{\top}\beta_*)^2 \big] + \mathbb E_{x_0}\big[ (x_0^{\top}\beta_* - \mathbb E_{x_0}[ x_0^{\top}\beta_\alpha ])^2 \big] $$ where $\beta_*=\arg\min_\beta \mathbb E[(f(X)-X^\top\beta)^2]$ are the parameters of the best-fitting linear approximation to $f$ and $\beta_\alpha$ are the estimated parameters. This shows that even if a model class is unbiased ( $\text{Bias}^2=0$ for $\beta_*$ ), having a biased estimator $\beta_\alpha$ will result in $\text{Bias}^2\neq 0$ . It is possible that the authors of your quote consider model class to define not only the model but also the estimator, and then they would be right, but without such a qualification I do not think they are.
