[site]: datascience
[post_id]: 48983
[parent_id]: 48969
[tags]: 
The plot shows Heaps' Law but the formula is something different, it is Zipf's Law . $f(w)$ is the relative frequency (or probability) of word $w$ . That is, given a random word, it will be $w$ with probability $f(w)$ . Therefore, if a document has $n$ words, it has on average $n\times f(w)$ occurrences of word $w$ . The formula can be re-written as follows: $$f(w)=C(r(w)-b)^{-\alpha}$$ which is a power-law distribution that shows Zipf's Law , however with a slightly different parameterization by introducing cut-off $b$ . $r(w)$ denotes the rank of word $w$ . For example, if we sort all the words in a news corpus based on their frequency, $r(\text{'the'})$ would be 1, $r(\text{'be'})$ would be 2, and so on, Cut-off $b$ ignores highly frequent words $r(w) \le b$ , effectively shifting up the rank of remaining words, $C$ is the normalizing constant, i.e. $C=\sum_{r=\left \lfloor b \right \rfloor + 1}^{\infty}(r-b)^{-\alpha}$ , which gives $\sum_{w,r(w)>b} f(w) = 1$ , and Exponent $\alpha$ denotes the rate of drop in probability when rank increases. Higher $\alpha$ , faster drop. Exponent $\alpha$ is determined by fitting the formula to some corpus, as shown in the table. Generally, lower $\alpha$ (in the case of twitter), thus slower drop, means corpus has more word diversity.
