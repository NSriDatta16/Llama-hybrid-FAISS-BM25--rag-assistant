[site]: crossvalidated
[post_id]: 212263
[parent_id]: 
[tags]: 
Combining independent observables for the same quantity, how to check independence

Let's assume we have a set of observations $\{x_i\}$ of a physical quantity $x$. We have a model $x=X(y)$, where $y$ is a free parameter. Let's assume that we have two distinct observables, $f(x)$ and $g(x)$, that can be used to measure $y$ from the observations. If the two observables $f$ and $g$ are independent, then I can just use minimization of $\chi^2$ to combine the results and get an estimate for $y$. For example: let's say that $\{x_i\}_1^N$ are drawn from a Gaussian population, then the mean, $\bar{x}=\Sigma_i \frac{x_i}{N}$, and the sample variance, $s^2=\Sigma_i\frac{(x_i - \hat{x})^2}{N-1}$, are two independent observables, that can be used to measure the parameters of the parent Gaussian distribution. On the other side, also $\Sigma_i x_i $ and $\Sigma_i x_i^2$ are independent, aren't they? Another example: if I have a set of observations $\{x_i\}_i^N$ and I consider the mean on the first 60% of the data, and then I consider the mean of the last 60% of the data, these are two distinct observables for my data set. But they are not independent observables, because they use the same information about the data twice. In principle the dataset could be more complicated, for example it may be constituted by a time series $x_i=x(t_i)$, and the observables could be Fourier transform and the other the probability density function of $x_i$. The question is: how can I know if two observables are independent? Is there a way I can combine observables that are not independent? Can anybody provide a good reference for this problem?
