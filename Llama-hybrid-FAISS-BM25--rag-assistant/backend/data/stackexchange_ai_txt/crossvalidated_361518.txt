[site]: crossvalidated
[post_id]: 361518
[parent_id]: 
[tags]: 
What is the optimal policy and value function?

I am styding reinforcement learning for my machine learning exam. We have a test exam and the image below shows one of the problems. It would be great if you could give me tips if i did any mistakes solving this exercise or if i have formal issues. My Solution By my understanding the reinforcment learning algorithm is getting an reward everytime it goes from state $s_1$ to state $s_2$. The reward in that case is 1 otherwise 0: $r(s,a) = 1$ if $s = 1$ and $a = right$ $r(s,a) = 0$ if $s = 0,2,3,4$ independent of the action $a$ that the algo takes. With that reward function i came up with the mapping of states $S$ to actions $A$ with is also called a policy $\pi : S \times A$. The optimal policy $\pi^*$ is to go as fast as possible to state $s_1$ and then use the $right$ action to get the reward (we are in $s_2$ now). After that it is best to go back to $s_1$ to repeat the process. This is my mapping for $\pi^*$: $s_0 \rightarrow right$ $s_1 \rightarrow right$ $s_2 \rightarrow left$ $s_3 \rightarrow left$ $s_4 \rightarrow right$ For the value function $V^*(s) = E[\sum_{t=0}^{\infty} \gamma^tr(s_t,\pi^*(s_t)) | s_0 = s]$ i figured out that it would maximize $V$ if the start state is $s_1$ because then the algorithm would generate a reward with the next state by going $right$. Based on the fact that $r(s, a)$ will be 0 for every odd $t$ we could further simplify the formular, but in my opinion that's it. Can you spot any mistakes in my solution? Are there any flaws writing down my results? Thanks for your time.
