[site]: datascience
[post_id]: 61939
[parent_id]: 61937
[tags]: 
Let me preface this answer by saying I am not an expert on variational autoencoders, but I think your conceptual gaps don't have anything to do with autoencoders. First, it is possible to specify a probability distribution over categorical outcomes or continuous outcomes. For example, a sample from a normal distribution is a continuous outcome, and the normal distribution describes the distribution of outcomes (or, equivalently, the relative likelihood of outcomes with various values). When you sample from a normal distribution, the probability of getting any particular numerical value (like 0, or 3.14) is infinitely small. Because of this, people typically talk about the probability density of a continuous distribution -- which can be interpreted as the likelihood of a sample being from a given (small) region. The probability value is called a probability density because it represents probability mass divided by space (or volume). Second, it is common to represent images as a vector of numbers. For a grayscale image, this might be 1 numeric value per pixel (it's slightly more complicated for color images). You can think of the vector representation of an image as a point in space. If you have a bunch of images, you can talk about the density of images in this space. Some regions of space have a lot of 'image points', and some have very few. You can approximate this density by fitting a high-dimensional probability distribution to the image points in your dataset -- like fitting a smooth curve to a binned histogram of image counts. Because of this, each image maps to a specific value for the fitted probability density. That means you can talk about p(image) -- it really means, if we look 'near' the vector representation of this image in our vector space, how many images are there, relatively speaking? Some images are in high-density regions and some are in low-density regions, which will reflected as high and low values for p(image).
