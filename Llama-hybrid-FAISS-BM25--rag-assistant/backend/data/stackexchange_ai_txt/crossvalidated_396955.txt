[site]: crossvalidated
[post_id]: 396955
[parent_id]: 396943
[tags]: 
How do I get the coefficients to the function of the reference category If you're asking for the regression coefficients related to the reference category: they don't exist. Let the outcome be $Y$ , and imagine that $Y = 1$ is the reference category. Imagine that there are only 3 categories (because I'm lazy). You are estimating (or rather, you asked R to estimate): $P(Y = 1) = \frac{exp(X\beta_1)}{exp(X\beta_1) + exp(X\beta_2) + exp(X\beta_3)}$ $P(Y = 2) = \frac{exp(X\beta_1)}{exp(X\beta_1) + exp(X\beta_2) + exp(X\beta_3)}$ And so on. The thing is, the system of equations is unidentified. There will be more than one unique solution for $\beta_1$ , $\beta_2$ , etc. We fix $\beta_1$ to 0 (or any one of the $\beta_i$ vectors, doesn't matter which). Once we do that, we have: $P(Y = 1) = \frac{1}{1 + exp(X\beta_2) + exp(X\beta_3)}$ So, the probability of being in the reference category only has covariates related to being in the other 2 categories. Side note: you can ask R to graph the probabilities of being in all the categories of interest, technique detailed at this link by UCLA . how can I decide wether a case belongs to the reference category (0) or to some other category (1-3) With respect, this question doesn't make sense to me. In plain old multinomial regression, you can predict the probability of responding in each category given the covariates using the equations above - or you can ask R to do it for you as detailed in the link above, which is much easier. If this isn't what you were asking, can you clarify? This question would make sense in the context of latent class analysis and latent class regression, but you didn't ask about that technique. I think normal regression in this case is acceptable because independent variables have been changed to continuous values from categorical values. I assume you're talking about an extension of a linear probability model (LPM). In that model, you fit a linear model to a binary dependent variable. LPMs are outdated because we have logistic regression, but truth be told, they may not always be bad. After all, in linear regression, you're trying to estimate the mean of $Y$ given some a vector of covariates $X$ . The mean of a $Y$ that takes on values 1 and 0 is going to be the probability of $Y$ . In logistic regression, you're estimating $P(Y = 1)$ as the function of some $X$ s. If you were to extend this to the multinomial case, though, I think that would be wrong. Your outcome is un-ordered categorical (or you're modeling it as such). If you fit a linear model, you'd be applying an assumption that it's ordered, for one. You'd also be assuming that the distance between 1 and 2 is the same as the distance between 2 and 3. That doesn't quite make sense, and I would guess that most reviewers would say this makes less sense than using an LPM on a binary outcome. I have normalized the independent variables (even though it is not necessary in MLR) to range between 0 and 1 according to the goodness of the variable, for example soil: 0.1 gravel, 0.3 sand, 0.5 sandy moraine, 0.8 peat, 1 high amount of clay. I'm not sure I'd recommend this. I'd treat soil type as categorical. You don't need to normalize the independent variables in any regression.
