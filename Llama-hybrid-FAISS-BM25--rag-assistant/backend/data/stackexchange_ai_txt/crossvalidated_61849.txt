[site]: crossvalidated
[post_id]: 61849
[parent_id]: 61798
[tags]: 
Some books state a sample size of size 30 or higher is necessary for the central limit theorem to give a good approximation for $\bar{X}$ . This common rule of thumb is pretty much completely useless. There are non-normal distributions for which n=2 will do okay and non-normal distributions for which much larger $n$ is insufficient - so without an explicit restriction on the circumstances, the rule is misleading. In any case, even if it were kind of true, the required $n$ would vary depending on what you were doing. Often you get good approximations near the centre of the distribution at small $n$ , but need much larger $n$ to get a decent approximation in the tail. Edit: See the answers to this question for numerous but apparently unanimous opinions on that issue, and some good links. I won't labour the point though, since you already clearly understand it. I am wanting to see some examples of distributions where even with a large sample size (maybe 100 or 1000 or higher), the distribution of the sample mean is still fairly skewed. Examples are relatively easy to construct; one easy way is to find an infinitely divisible distribution that is non-normal and divide it up. If you have one that will approach the normal when you average or sum it up, start at the boundary of 'close to normal' and divide it as much as you like. So for example: Consider a Gamma distribution with shape parameter $α$ . Take the scale as 1 (scale doesn't matter). Let's say you regard $\text{Gamma}(α_0,1)$ as just "sufficiently normal". Then a distribution for which you need to get 1000 observations to be sufficiently normal has a $\text{Gamma}(α_0/1000,1)$ distribution. So if you feel that a Gamma with $\alpha=20$ is just 'normal enough' - Then divide $\alpha=20$ by 1000, to get $\alpha = 0.02$ : The average of 1000 of those will have the shape of the first pdf (but not its scale). Edit: a warning: very small shape parameters on the gamma can lead to numerical issues, so if you push it hard you'll get problems. Try the Poisson, perhaps, which is also infinitely divisible. A convenient example that doesn't take advantage of infinite divisibility is the lognormal. Try $\sigma$ parameters in the vicinity of $3$ to $4$ for ones that need really large $n$ to work; it's usually not too difficult to pin down a $\sigma$ that gets close to the edge of some judgement of sufficiently close to normal at some given $n$ . If you instead choose an infinitely divisible distribution that doesn't approach the normal, like say the Cauchy, then there may be no sample size at which sample means have approximately normal distributions (or, in some cases, they might still approach normality, but you don't have a $\sigma/\sqrt n$ effect for the standard error). @whuber's point about contaminated distributions is a very good one; it may pay to try some simulation with that case and see how things behave across many such samples.
