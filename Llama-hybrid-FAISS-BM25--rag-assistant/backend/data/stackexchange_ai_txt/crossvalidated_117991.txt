[site]: crossvalidated
[post_id]: 117991
[parent_id]: 
[tags]: 
Precision and recall constitute a way to measure the relevance of set of retrieved instances. Precision is the proportion of correct instances retrieved out of all instances that are retrieved. Mathematically, precision is equivalent to the positive predictive value. Relevance is the proportion of true instances that exist that are retrieved. This is equivalent to sensitivity . Precision and recall are commonly used in data mining contexts to evaluate classifiers just as sensitivity and specificity are used in statistics to evaluate the discriminative ability of a logistic regression model. They can be examined individually or combined (via their harmonic mean) to create the F 1 -score : $$ F_1 = \frac{2}{\frac{1}{{\rm Precision}}+\frac{1}{{\rm Recall}}} = 2\times\frac{{\rm Precision}\times {\rm Recall}}{{\rm Precision}+{\rm Recall}} $$ ( Because precision and recall are closely related to, and easily confused with, sensitivity and specificity, the following attempts to disentangle them. ) If a classifier can call an object positive (relevant) or not, and the object can be positive or not in reality, there are four possible combinations (represented by a confusion matrix ): Reality: Positive Negative Classification: --------------------------- | | | 'positive' | TP | FP | | | | --------------------------- | | | 'negative' | FN | TN | | | | --------------------------- where TP is true positive , FP is false positive , FN is false negative , and TN is true negative . Then: $$ {\rm Precision} = \frac{TP}{TP + FP} $$ (By contrast, specificity is: $\frac{\color{red}{TN}}{\color{red}{TN} + FP}$.) and: $$ {\rm Recall} = ({\rm Sensitivity}) = \frac{TP}{TP + FN} $$ There are other ways of parsing a confusion matrix. Another is to compute the positive predictive value and negative predictive value. It may be worth noting that precision is the same as the positive predictive value. Using either precision and recall or sensitivity and specificity will provide complete information about the performance of a classifier. Which set is used is mostly a matter of convention.
