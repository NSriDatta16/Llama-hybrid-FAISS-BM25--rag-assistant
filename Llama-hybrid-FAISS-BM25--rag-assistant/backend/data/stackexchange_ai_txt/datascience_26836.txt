[site]: datascience
[post_id]: 26836
[parent_id]: 
[tags]: 
Should the different layers of deep learning models have same size or they should be changed based on a rule

I see a lot of people varying the width of each layer in a deep neural network. ie. Input->50->100->150->Output. I'm curious what, if any, are the advantages of this structure over static layer widths ie. Input->50->50->50->Output. Given the following three model structures what would be the real-world implications and differences between them and how they interpret data? Example 1: Input -> 100 -> 100 -> 100 -> Output Example 2: Input -> 50 -> 100 -> 150 -> Output Example 3: Input -> 150 -> 100 -> 50 -> Output
