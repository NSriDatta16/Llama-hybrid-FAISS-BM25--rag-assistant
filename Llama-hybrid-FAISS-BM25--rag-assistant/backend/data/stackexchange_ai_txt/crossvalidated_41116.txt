[site]: crossvalidated
[post_id]: 41116
[parent_id]: 41094
[tags]: 
The conditions under which the output of a neural net can be treated as estimates of posterior probabilities are fairly broad, I remember the following paper as being pretty interesting and informative (caveat: but I've not read it since 2002) Marco Saerens, Patrice Latinne, Christine Decaestecker: Any reasonable cost function can be used for a posteriori probability approximation. IEEE Transactions on Neural Networks 13(5): 1204-1210 (2002) I suspect it has references to the classic papers in the text as well. Prof. Saerens has written several really nice papers; I would recommend anyone seriously interested in neural nets to look up his papers in Google scholar etc. You could view the output layer of a neural net as being a logistic regression model (with outputs that represent probabilities), and the hidden layer as being a non-linear transformation of the inputs. It is the output layer that matters regarding the interpretation of the outputs, so the MLP is essentially just a non-linear logistic regression model.
