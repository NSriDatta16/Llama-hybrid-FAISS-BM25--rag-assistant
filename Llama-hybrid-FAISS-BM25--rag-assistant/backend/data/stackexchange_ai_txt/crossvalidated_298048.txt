[site]: crossvalidated
[post_id]: 298048
[parent_id]: 
[tags]: 
Is it reasonable that overfitted model be bettern than non-overfitted model?

This question is related to this one , but it is different. I tested two different learning methods on a fixed convolutional neural network structure (with the same hyper-parameters). As it is clear from the following plot, the prob method obviously overfits the training data, since its training error is nearly zero after the third epoch. But, its validation error is also lower than the naive method. My question is this: Can we say that the prob method is better than the naive one, even though it suffers more seriously than the naive method from overfitting?
