[site]: crossvalidated
[post_id]: 359466
[parent_id]: 
[tags]: 
Comparing metrics with mean and standard deviation

I have been running some experiments for a problem using different models. Those experiment run on 10 different datasets and each of them yields a value for RMSE. After all the 10 experiments are finished I get a mean value and standard deviation for the RMSE of that model. I repeat this process for all the models and end up with some mean values and std that I should compare to choose the best/bests ones. Obviously I am not interested in the result on each dataset but on the average performance of the model in all datasets. For example I may get this mean RMSE results for let's say 3 models (std in parenthesis): |---------------------|------------------|------------------| | Model 1 | Model 2 | Model 3 | |---------------------|------------------|------------------| | 5.53 (0.12) | 5.61 (0.10) | 5.69 (0.10) | |---------------------|------------------|------------------| It is clear that the model 1 gets the bests mean results (the lower the better) but when taking into account the standard deviation, model 2 is really close to it. Model 3 is a bit off. I know there must be some way of doing this statically, proving that both model 1 and model 2 get (almost) the same results. I thought about using z-test or t-test but I don't know how to fit that here. Any idea how can I proceed to select the best models taking into account the standard deviation?
