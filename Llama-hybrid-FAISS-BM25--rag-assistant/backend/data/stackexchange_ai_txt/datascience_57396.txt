[site]: datascience
[post_id]: 57396
[parent_id]: 
[tags]: 
How does the Backpropagation through time work?

I have to write a paper on LSTMs and I want to explain why LSTMs exist in the first place. According to some papers and books because usual RNNs had problems with vanishing gradients and the LSTM has not. I would like to understand that in detail, but the backpropagation through time gradient descent is really hard to understand for me. If I have a time series with the length 3 (as an simple example), I can calculate 3 outputs. Therefore I can calculate 3 Losses: L1 , L2 , L3 Let L = L1 + L2 + L3 Now, where do I start my backpropagation? Do I apply it to all outputs or only the last one? Do I always apply the accumulated loss L ? I hope you understand my question!
