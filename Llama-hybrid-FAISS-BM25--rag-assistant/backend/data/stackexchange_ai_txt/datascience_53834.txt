[site]: datascience
[post_id]: 53834
[parent_id]: 53831
[tags]: 
A machine learning model is an algorithm which learns features from the given data to produce labels which may be continuous or categorical ( regression and classification respectively ). In other words, it tries to relate the given data with its labels, just as the human brain does. Mathematical functions are used to map the features ( produced as a result of feature extraction ) to their labels. Models trained on a high amount of data can generalise themselves better. Generalisation is the ability of a model to give generalised predictions across varied or diverse data. It is not biased towards the data on which it was originally trained. Production of models has these basic steps involved: Collection of suitable data. Preprocessing of data for training the model. Training the model. Evaluating the model. Hosting the model for production. Deploying a model: This could be done in many ways. You can serve a model or run it on an IoT device. You basically want to freeze all the trainable parameters so that they are constants. The model's learning capabilities are removed so that it could only make predictions. There are some models which exhibit Online Learning . Different frameworks like TensorFlow, Keras, PyTorch etc. have their own methods of saving models. Like in Keras, we can save a NN model as a hd5f serialized file. With TensorFlow Lite, we can run a model on an IoT device. Firebase MLKit hosts the model in the cloud created with TensorFlow. The best deployment platform is determined by its usability, scalability and developer friendliness.
