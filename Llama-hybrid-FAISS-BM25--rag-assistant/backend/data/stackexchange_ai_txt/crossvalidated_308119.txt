[site]: crossvalidated
[post_id]: 308119
[parent_id]: 
[tags]: 
Why shouldn't we scale data before entering into KPCA?

In Kernel Principal Component Analysis (KPCA), data comes in as a $n\times d$ matrix $X$ where $n$ is the number of observations and $d$ is the number of features. The process has been explained in this answer . After the kernel matrix is developed, an scaling process is taken place as explained by this post . Based on this, no scaling is placed until we apply the kernel and then we scale the kernel matrix. However, don't we lose some the data this way? For example, imagine $d=3$ where we have three features. A normal data for example is $[1000, 1000000, 0.001]$. We train our KPCA model with 1000 of these normal points. When an abnormal data comes in such as $[998, 1000002, 0.009]$, the first two elements are within the acceptable range, though, the third element is 800% higher than our normal range (0.001). If we calculate the distance between this new point and our 1000 normal points, this information will be lost. For example, the distance between this new abnormal point and one of our training points is: $\mathrm{distance}=\sqrt{(998-1000)^2+(1000002-1000000)^2+(0.009-0.001)^2}$ This distance will not be very different from a new normal point because the abnormality of the third element will become irrelevant in our distance calculation since it is so small compare to other two elements. $\mathrm{distance}=\sqrt{(998-1000)^2+(1000002-1000000)^2+(0.0015-0.001)^2}$ Now, considering all this, why shouldn't we scale our raw data before calculating the kernel matrix? Reading KPCA papers, it seems that nobody does scaling before apply the kernel matrix and I wanted to know why this is not necessary. Thanks.
