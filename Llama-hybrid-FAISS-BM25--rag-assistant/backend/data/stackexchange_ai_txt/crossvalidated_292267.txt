[site]: crossvalidated
[post_id]: 292267
[parent_id]: 
[tags]: 
Should I standardize first or generate polynomials first?

Recently I am dealing a classification problem with some algorithms, say logistic regression. When I preprocess my data, I standardize all my features and generate polynomial features based on them first. from sklearn.preprocessing import PolynomialFeatures, StandardScaler and I do features = std.fit_transform(features) features = poly.fit_transform(features) After finishing training my model, the accuracy is, say about 80%. Then I invert the two line of preprocessing code to # features is my entire features dataset, labels excluded features = poly.fit_transform(features) features = std.fit_transform(features) It gives a slightly better result on accuracy. I have read this post but it seems the answers is not strong enough to help me figure it out. Should I standardize my data first or generate polynomials from original features first? Is there any kind of "best practice" should I follow in general? I understand answers to this question may vary depending on the dataset, so let's assume I am using a dataset similar to the famous Iris dataset but with about 20 features and 1000 samples.
