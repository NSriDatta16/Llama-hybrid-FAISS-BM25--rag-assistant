[site]: crossvalidated
[post_id]: 532316
[parent_id]: 
[tags]: 
Evaluating a CNN -multi class model with two separate thresholds

I have a model that outputs three classes. But here instead of one threshold, it depends on a combination of two (user input threshold). One threshold varies from 0.1 to 1.0 and the other varies from from 1 to 800 ( this is user input based on domain knowledge). How can I evaluate this model for both a) Balanced dataset b) Imbalanced dataset A normal ROC samples 1D threshold space. How can I adjust this for 2 different thresholds? Is there any other metrics that I can use that will accommodate the two thresholds? If I use sensitivity and recall..( and many others) I am not sure how to put the two threshold picture, do I have then look at the individual thresholds separately for ( example : For 25, we look at threshold ; 0/.9, 0.8, 0.7..) then for (50, we look at ...) and may be show one more case to explain how the threshold affect the positive predictions? This is basically an App that uses: outputs from a CNN + one method ( where the user enters the two threshold) to classify a video if it contains : class: Apple, Orange, Pears . In one scenarios there are very less orange and pears --making it imbalanced and in other it is balanced.Basically we evaluating how good this method that uses the outputs of the CNN to classify.We are not evaluating the neural network but the performance of the app that is partly based on the outputs of the neural network. My model is a posture analysis CNN model, so it outputs probabilities of labels ( uppart of fruit, side, bottom..). But it does not classify these fruits but just the labels. The app takes these labels and say if the user threshold is above 0.9 for the labels and the number of consecutive frames ( another threshold) 25 ( this is video fromat) then it is an Apple because apples are usually seen for less than 2 sec in the video so we can not have consecutive frames over say 50/60. Then we have pears which are usually there for say 300 frames but and then the oranges for maximum 400. Now if I start looking for labels with smaller threshold, I might find many more oranges than there is in the video but would false negative for the apple part.This threshold ( which is the probabilities of the model) is decided by the user or domain expert. So, my final model does not use probabilities from the supervised model, but my algorithm in the app uses the probabilities of the output (for each label that is between equal to/less than 1) as the threshold that the user inputs. We are here evaluating the performance of the app for classification than the model.
