[site]: crossvalidated
[post_id]: 395099
[parent_id]: 
[tags]: 
CNN training in bfloat16

Are there any efforts so far for training CNNs end-to-end with bfloat16 format? especially the convolution part, i.e. both multiplication and addition is done in bfloat16. Can this scale to large datasets such ImageNet? Intel has a published white paper about supporting bfloat16 in their next-generation FPGAs, where they claim that accumulating in FP32 format is essential to avoid losing accuracy on an application level. Would people building custom hardware for training have to stick with the FP32 accumulation?
