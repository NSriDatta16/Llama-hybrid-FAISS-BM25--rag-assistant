[site]: datascience
[post_id]: 32656
[parent_id]: 32599
[tags]: 
Neurons do not have to be only logistic regression units. Even neurons in a simple feed-forward network may not all be logistic. For example, neurons in the output layer of a network for a regression problem are usually linear. An artificial neuron is any computing unit that takes inputs and performs some computation. It can be any computation, linear, non-linear, logistic, weird, whatever. The goal of connecting multiple neurons into a network is achieving the ability to perform a complicated computation, even though each neuron can do only a very simple task. A neuron in a self-organizing map does perform computations. When an input vector is presented to the network, every neuron in a SOM computes the distance between its weight vector and the input vector. This is a computation. Then it compares this distance with the computations of other neurons. This is also a computation which is possible only because the neurons are connected in the network. The subsequent behavior of the neuron depends on the result of these computations. If it is the winner, it does another computation - modifies its weights, and one more computation - pulls the neighboring units (also a networking thing). If it is not the winner, it performs a very important action: it does nothing, which is often the best thing that one can do. The decision for a neuron to update its weights or not to update them is a non-linear operation: if an input vector changes slightly, the response of the neuron may change sharply or not to change at all. The result is that the network performs a complicated computation: it builds a discrete, topology-preserving approximation of the distribution of training samples. This is useful in many applications. When a SOM is trained, then every neuron also does a non-linear computation. It either fires or not depending on whether the input vector is closer to that neuron than to any other neuron.
