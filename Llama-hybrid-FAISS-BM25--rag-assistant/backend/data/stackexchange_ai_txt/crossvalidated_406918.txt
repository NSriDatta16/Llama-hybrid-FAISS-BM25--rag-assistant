[site]: crossvalidated
[post_id]: 406918
[parent_id]: 
[tags]: 
Understanding Approximate Dynamic Programming

I am trying to write a paper for my optimization class about Approximate Dynamic Programming. I found a few good papers but they all seem to dive straight into the material without talking about the basics so I am really lost about what it is. In class we talked a little bit about "Dynamic Programming" but I can't seem to find out the difference between regular DP and ADP. What are we approximating? One source says Many problems in these fields are described by continuous variables, whereas DP and RL can find exact solutions only in the discrete case. Therefore, approximation is essential in practical DP and RL. so I thought ADP is used for continuous time but then another paper states Approximate Dynamic Programming (ADP) is a powerful technique to solve large scale discrete time multistage stochastic control processes So is it for discrete time or continuous? Or both? Similarly, a lot of sources on ADP talk about reinforcement learning but they don't distinguish the two. Are they both both pretty much the same things or is there a distinct difference?
