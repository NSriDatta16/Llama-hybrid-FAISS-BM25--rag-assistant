[site]: crossvalidated
[post_id]: 305462
[parent_id]: 305452
[tags]: 
The way that you can use cross-validation to determine the optimal number of epochs to train with early stopping is this: suppose we were training for between 1 to 100 epochs. For each fold, train your model and record the validation error every, say, 10 epochs. Save these trajectories of validation error vs number of epochs trained and average them together over all folds. This will yield an "average test error vs epoch" curve. The stopping point to use is the number of epochs that minimizes the average test error. You can then train your network on the full training set (no cross validation) for that many epochs. The purpose of early stopping is to avoid overfitting. You use N-fold cross-validation to estimate the generalization error of your model by creating N synthetic train/test sets and (usually) averaging together the results. Hopefully, the test set (aka new real-world data) that you are given later is going to be similar enough to the synethetic test sets that you generated with CV so that the stopping point you found earlier is close to optimal given this new testing data.
