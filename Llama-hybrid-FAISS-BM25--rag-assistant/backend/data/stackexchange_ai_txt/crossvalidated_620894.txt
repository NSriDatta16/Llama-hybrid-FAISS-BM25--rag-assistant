[site]: crossvalidated
[post_id]: 620894
[parent_id]: 
[tags]: 
iid data (Bayesian) vs iid random variables (Frequentist)?

I've been pondering the differences in notation / language used in some of the resources I've read for statistics / machine learning. Warning : this might be embarrassingly obvious to any decent statistician, so apologies in advance. 1 iid data (Bayesian) I am a big fan of Pattern Recognition for Machine Learning (Bishop) so much of my foundation in basic statistical machine learning comes from there. In the first chapter (section 1.2.4, page 26), he takes the approach of defining a scenario where we have a random variable $x$ and the data $\pmb{\mathsf{x}} = \{x_1, ..., x_n\}$ are $N$ independently drawn observations of the random variable $x$ . To emphasize...a set of data is seen as a number of observations of the same random variable . My claim is that this approach to setting up the data / statistical problem is kind of Bayesian and this is corroborated (somewhat) by the fact that Bishop seems to be a Bayesian. This also seems Bayesian to me because we're setting the data in stone. That is, our data is not a set of random variables but a set of realizations of a single random variable which falls in line with Bayesian philosophy. 2 iid random variables (Frequentist) Contrast this with a more frequentist resource such as Larry Wasserman's intermediate statistics notes . Throughout the lectures, we consider instead a set of random variables $X_1, ..., X_N$ that are i.i.d in that they are independent and follow the same exact distribution. Data is seen as a set of random variables , not a set of realizations of a random variable. This lends itself to constructing "estimators" and asymptotics because we view the dataset as random. It can be constructed many times by realizing the set of random variables $X_1, ..., X_N$ simultaneously many times. Then you can do analysis of the long-run behaviour of doing an experiment many times which I believe is quite Frequentist. I suppose my question is: what are the relative merits of using either approach to denoting the data? And does this directly coincide with the merits of Bayesianism vs Frequentism? ? I am biased and prefer the Bayesian setup because (i) I'm used to it, and (ii) it allows (in my opinion) for much less "terminological" overhead that often creates abusive notation and then confusion. However, it does seem like there is a much easier segway into things like asymptotics (maybe other stuff I'm not mentioning) if we view the dataset as random. Of course, please point out any errors! Thanks. Edit : I defer to this explanation from Christian Hennig.
