[site]: crossvalidated
[post_id]: 446879
[parent_id]: 435507
[tags]: 
So will I be able to backprop succesfully? Yes, computing derivatives through backpropagation works just fine. The composition of two differentiable functions (in this case, neural networks) gives another differentiable function, so whatever deep learning framework you are using, its auto-differentiation should handle these computations. In PyTorch, your training loop for A will look like once you have already trained B. criterion = MyCustomCriterion() # e.g. CrossEntropy, MSE, etc. netA = NetA() # instance of some subclass of torch.nn.Module netB = NetB(pretrained=True) # instance of some subclass of torch.nn.Module optimizerA = torch.optim.Adam(params=netA.parameters()) for epoch in range(num_epochs): for batch, label in dataloader: netA.zero_grad() netB.zero_grad() outputA = netB(batch) outputB = netA(outputA) err = criterion(outputB, label) # Calculate gradients for all parameters in backward pass. # Keep in mind that computing gradients with respect to the parameters of A # requires computing the gradients with respect to B through the chain rule. err.backward() optimizerA.step() I think this is a very normal approach to this since you are interested in using the values for the intermediate layers. Otherwise, it might be easier (and possibly get better results) if you just used one larger neural network.
