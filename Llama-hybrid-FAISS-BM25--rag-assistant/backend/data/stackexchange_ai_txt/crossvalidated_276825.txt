[site]: crossvalidated
[post_id]: 276825
[parent_id]: 
[tags]: 
Generative Network Sampling: spherical interpolation instead of linear?

I am building my own implementation of a DCGAN at the moment and want to try different random samplings for the Generator's input (noise). Recently, I found the paper Sampling Generative Networks . I understand the author's claim that for the Generator's input $z$ (e.g. 100-dimensional, which is kind of "standard size") a standard normal distribution $\mathcal{N}(0, 1)$ is not the best choice, since it will have "tent poles" of length approx. close to 10 (see chap. 2.1). However, I struggle understanding why the spherical version solves this "issue" and why this (heavily?) improves generative networks. Additionally, the author gives the following formula for the spherical version: $$ Slerp(q_1, q_2, \mu) = \frac{\sin(1-\mu)*\theta}{\sin(\theta)} * q_1 + \frac{sin(\mu * \theta)}{sin(\theta)} * q_2 $$ ...but he does not explain what $q1, q2, \theta$ is? Can someone help? Instead of the typical: # Python code z = np.random.standard_normal((batch_size, 100)).astype(np.float32) ...which is already more stable than a uniform distribution for my network, thus I would really like to further enhance the random sampling.
