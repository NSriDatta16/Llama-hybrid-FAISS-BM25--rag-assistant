[site]: crossvalidated
[post_id]: 295097
[parent_id]: 
[tags]: 
Recurrent networks mimicking previous / current input

So I have been trying to train an LSTM to predict the values of a certain stock. The error was pretty low, so I decided to create a graph out of the test set. It looked like this: Red: actual, Black: my predition, Blue: input to get prediction So i'm training the network with datsets like: in: x t-1 , out: x t . But when ignoring the fact that the black line is much lower than the red line, you'll see that the network is actually mimicking the input to stay as close to the actual prediction. So after doing some googling , I found out that this is a common 'trap: I am sure there is one step lag between the actual time series and the predicted time series, this is the most seen "trap" if you do time series prediction like this, in which the NN will always mimic previous input of time series. But are there things I do to avoid this? In the thread I linked some solutions are pointed out, but are there any better, generic solutions? I have created a JSFiddle with the training of the neural network and the chart. View it here ( open console before opening ). Feel free to tweak around with the options to see if you get something working...
