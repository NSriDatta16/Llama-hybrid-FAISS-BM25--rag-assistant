[site]: crossvalidated
[post_id]: 61852
[parent_id]: 61851
[tags]: 
Since the answer actually depends on the relationship between $X$ and $Y$, the most you can perhaps do is try to bound the result, and the bounds aren't going to be terribly satisfactory. Here, I generate some random data (Edit: in R ) where the true relationship with each of the variables conditional on other variable is a coefficient of 1.0 and a high conditional correlation (near to 1) zz=rnorm(30) x1=rnorm(30)*.1+sqrt(1-.99)*zz x2=rnorm(30)*.1-sqrt(1-.99)*zz y1=rnorm(30)*.1+sqrt(1-.99)*zz First regression setup - x,y positively related -- lm(z1~x1+y1) Call: lm(formula = z1 ~ x1 + y1) Coefficients: (Intercept) x1 y1 5.173 1.356 2.312 second setup: x,y negatively related lm(z2~x2+y1) Call: lm(formula = z2 ~ x2 + y1) Coefficients: (Intercept) x2 y1 4.9169 1.1761 0.4482 Okay, there's a fair bit of noise in the estimates, especially of the y coefficient. Now look at the individual regressions. First setup, where x,y were positively related: > lm(z1~x1) Call: lm(formula = z1 ~ x1) Coefficients: (Intercept) x1 5.213 2.465 > lm(z1~y1) Call: lm(formula = z1 ~ y1) Coefficients: (Intercept) y1 5.204 3.179 Now second setup where they were negatively related: > lm(z2~x2) Call: lm(formula = z2 ~ x2) Coefficients: (Intercept) x2 4.934 1.033 > lm(z2~y1) Call: lm(formula = z2 ~ y1) Coefficients: (Intercept) y1 4.93185 -0.06165 How are you supposed to get something useful out of the individual relationships without any information about the relationships between the variables? Short answer: basically, you don't. At least not much. === If we can guess the correlation sign (not the value, just the sign) of X and Y (e.g. we guess ρxy is positive),it seems the above method would have some merit. Is this just wishful thinking? You can reduce the size of the uncertainty - by reducing the potential range of the correlation, there's a smaller range of possible regressions on the two variables. Secondly, I'm starting to doubt my own doubts now, because thinking some more, if ρyz is close to one and ρxz is close to zero, or very small, then taking the weighted average seems inappropriate since the YZ correlation is far superior to the XZ, so weighting the average is in effect diluting our accuracy and our certainty of getting the right value. Is this thinking valid? If you knew the thing you say you don't know (the relationship between the two linear predictors), you can work out how much extra information is available in the weaker variable, conditional on the other one - but that only adds information if you know the right coefficient to apply (which will be different from the coefficient in the univariate relationship).
