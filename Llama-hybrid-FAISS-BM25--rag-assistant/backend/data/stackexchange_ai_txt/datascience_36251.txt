[site]: datascience
[post_id]: 36251
[parent_id]: 33717
[tags]: 
There are two steps necessary, you need a trained model and you need to apply this model regularly to new data. Step 1: After training your model you need to serialize this in some way. This could be via the tooling in your deep learning framework, or by saving the parameters somewhere and the code somewhere else or by pickling the whole object. After storing this model artifact we can reuse this fitted model in step 2. Step 2: Schedule some job that loads this model artifact, loads the data from BigQuery (in chunks or the full scope of the rows for the last time period), runs these through your model and saves the results. The schedule only kicks off some script, how to properly run these predictions depends a bit on the scale. If it is a very big job you would likely have to distribute this via something like Spark. If it is managable you could just do this on a single machine, maybe by chunking the data.
