[site]: crossvalidated
[post_id]: 445327
[parent_id]: 445316
[tags]: 
TL;DR - No.Unless you want to challenge the smartest minds on the planet by your research. My exposure to NLP is moderately recent(a year). I have worked through 3 classes in grad school and a sem-long internship at amazon with main focus on NLP. Personally, my exposure to linguistics is from personal research, probably similar to your exposure. But this has not stopped me from extracting insights from text. The initial steps taken to understand text are quite statistical than conceptual. Few of the such basic methods would be Tf-Idf, dictionary based sentiment analysis and LDA. Then one'd move to contextual word capturing techniques like N-grams and text embeddings. Any other sophisticated method would be specific to the use case. For cases like conversation analysis, methods as Bi-directional word embeddings, sequence-to-sequence NN, modified RNN models and graph based topic modeling are standard. For cases of niche sentiment analysis papers like The Unsupervised Sentiment Neuron work great. So, basically, every time I have a text corpus to analyze breaking down the problem into elements that make sense to me and a thorough research on the state-of-the-art methods than can address these individual elements does the job for me. Sure, some papers or repos can be tricky to implement and thats where you make a call. Is the time worth the results the methods promise? Building up a solution starting with the most easiest implementation(probably the most dumb implementation too) and checking how well your results reflect the labor is where the work stops. This is the approach I've been suggested by profs with almost 30 years of work-ex in NLP and product managers handling AI teams. Hope it helps.
