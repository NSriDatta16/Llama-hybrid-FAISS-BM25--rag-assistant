[site]: crossvalidated
[post_id]: 541606
[parent_id]: 247551
[tags]: 
Work in Progress... There has been a lot of work on predictive intervals for neural nets going back over the years: The simplest approach (Nix and Weigend, 1994) is to train a second neural network to predict the mean-squared error of the first. Regression networks trained to minimise the mean-squared error learn the conditional mean of the target distribution, so the output of the first network is an estimate of the conditional mean of the targets and the second learns the conditional mean of the squared distance of the targets from the mean, i.e. the conditional variance. In practice, they don't have to be separate networks, you can have one network with two outputs, one for the conditional mean and one for the conditional variance. References D. A. Nix and A. S. Weigend, "Estimating the mean and variance of the target probability distribution," Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94), 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138. David A. Nix, Andreas S. Weigend, Learning Local Error Bars for Nonlinear Regression, NIPS 1994 ( pdf ) CM Bishop, CS Qazaz, Regression with input-dependent noise: A Bayesian treatment, Advances in neural information processing systems, 347-353, 1997 (pdf)
