[site]: datascience
[post_id]: 47169
[parent_id]: 47168
[tags]: 
Are both measures usable? Is only cross-entropy used? They both could be used for this special case. However, I personally prefer "entropy" because it requires less mental gymnastics. Let's first review the definitions. The most agreed upon and consistent use of entropy and cross-entropy is that entropy is a function of only one distribution, i.e. $-\sum_x P(x)\mbox{log}P(x)$ , and cross-entropy is a function of two distributions, i.e. $-\sum_x P(x)\mbox{log}Q(x)$ (integral for continuous $x$ ). The formula used in The Elements of Statistical Learning [Page 308, 9.2.3 Classification Trees] can be written as $$-\sum_k P_{m}(k) \text{log}P_{m}(k)$$ where $P_{m}(k)$ is the ratio of class $k$ in node $m$ . This could be interpreted as a function of only one (data) distribution, i.e. an entropy, that measures the impurity of node $m$ . Nonetheless, it can also be interpreted as a cross-entropy between data distribution and model estimation (based on @DrewN nice explanation), i.e. $$-\sum_k P^{\text{data}}_{m}(k) \text{log}P^{\text{model}}_{m}(k)$$ where we, hypothetically, set the model estimation to match the data distribution in node $m$ , i.e. $$P^{\text{model}}_{m}(k)=P^{\text{data}}_{m}(k) = P_{m}(k)$$ to minimize the cross-entropy. Accordingly, cross-entropy would be the same as both data entropy and model entropy in value , i.e. $$\begin{align*} \overbrace{-\sum_k P^{\text{data}}_{m}(k) \text{log}P^{\text{model}}_{m}(k)}^{\text{data-model cross entropy } H(P^{\text{data}}_{m}, P^{\text{model}}_{m})}&=\overbrace{-\sum_k P^{\text{data}}_{m}(k) \text{log}P^{\text{data}}_{m}(k)}^{\text{data entropy } H(P^{\text{data}}_{m})}\\ &=\overbrace{-\sum_k P^{\text{model}}_{m}(k) \text{log}P^{\text{model}}_{m}(k)}^{\text{model entropy } H(P^{\text{model}}_{m})} \end{align*}$$ but it is different in meaning and rightfully has a different name. I say "hypothetically" because in practice, classifier only chooses the class with maximum probability, i.e. $$P^{\text{classifier}}_{m}(k)=\left\{\begin{matrix} 1 & k=\underset{k'}{\text{argmax }}P_m(k')\\ 0 & \text{o.w.} \end{matrix}\right.$$ From another perspective, when cross-entropy is equal to entropy, it means KL divergence is zero $$\text{KL}(P^{\text{data}}_{m} \parallel P^{\text{model}}_{m}) = H(P^{\text{data}}_{m},P^{\text{model}}_{m}) - H(P^{\text{data}}_{m})=0$$ All in all, we can still confidently use "entropy" for decision trees when we talk about node splitting and node impurity . For example, a split occurs when entropy of class distribution in parent node is higher than the weighted-average of class entropies in left and right children (i.e. positive information gain). As an extra note, cross-entropy is mostly used as a loss function to bring one distribution (e.g. model estimation) closer to another one (e.g. true distribution). A well-known example is classification cross-entropy (my answer). Also, KL-divergence (cross-entropy minus entropy) is basically used for the same reason.
