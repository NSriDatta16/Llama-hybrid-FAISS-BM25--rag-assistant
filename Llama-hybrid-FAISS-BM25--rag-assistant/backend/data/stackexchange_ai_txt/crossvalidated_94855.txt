[site]: crossvalidated
[post_id]: 94855
[parent_id]: 94845
[tags]: 
Confidence intervals for SVMs are often obtained via bootstrapping or by taking a Bayesian approach. For classification, bootstrapping may be a good approach to take. This slide deck illustrates some concepts of how to estimate confidence intervals to a test error of a certain classifier based on continuous functionals. Another approach given here : Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers Bo Jiang, Xuegong Zhang, Tianxi Cai; 9(Mar):521--540, 2008. is to use perturbation-resampling (similar to bootstrapping) to obtain the desired confidence intervals. This paper is a bit more interpretable than the slide deck. To summarize, first, it proposes that the classifier be estimated using cross-validation, and that as an output one will have $\hat{D}$ the estimated prediction error (Equation (5)). They go on to point out that it is desirable to know something about the distribution of this estimated prediction error (for instance if we know something about its distribution we can try to obtain confidence intervals for it). In order to do so for a general class of distributions (a problem confronted in the slide deck as well), they detail their resampling approach in Section 3.2. This is consolidated in an algorithm that they provide in this section. You may also have luck attempting to learn something from the LS-SVM toolbox posted by this work group. They implement confidence intervals for Least Squares SVMs for regression using a Baysian confidence interval. This is not for classification, but it may be possible that you can adapt it to your problem.
