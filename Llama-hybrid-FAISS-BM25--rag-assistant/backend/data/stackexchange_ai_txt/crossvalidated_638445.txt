[site]: crossvalidated
[post_id]: 638445
[parent_id]: 638375
[tags]: 
This relates to the questions: Could a mismatch between loss functions used for fitting vs. tuning parameter selection be justified? and If the predicted value of machine learning method is E(y | x), why bother with different cost functions for y | x? The likelihood function is popular because it relates to filtering statistical noise in an efficient way. The likelihood does not need to represent some function related to the actual final goal that is to be optimized. Many different functions may be used for that. But even when it is not the same as that final cost function, by using the likelihood one obtains an estimate/fit that has good performance on that final cost function (example: when estimating the mean of a Gaussian population, minimizing the sum of squared residuals will lead to an estimate that minimizes the absolute error). In the case of a complex model like a language learning model, the parameters of the model may be very much unrelated and the fitting of all of them together in a single value of a single likelihood function may indeed not be necessary or it may be even undesirable. And I also don't think that many people take that approach, ie. for complex models more complex cost functions are incorporated into the training of such models. For example, the fitting of different aspects of the model are weighted according to desired relative importance of these aspects, in order to optimize the variance/error and performance in some final cost function. (Still, using the product in some may remains desirable because it makes computations easier. The relationship between the least squares method and the Gaussian error distribution is a good example) Also related: When combining p-values, why not just averaging?
