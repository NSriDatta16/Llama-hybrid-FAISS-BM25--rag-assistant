[site]: crossvalidated
[post_id]: 621498
[parent_id]: 
[tags]: 
Statistical significance - sample vs bootstrap

Qeestion Would using either method potentially introduce a bias in the outcome of the signifiance test? Context I have 2 sets of samples (1000+ in each); One representing before and another representing after making some change to a system. The resulting effect on the average is usually small (1 to 2%) if the change made is successful/good/effective. Bootstrapping can be used to get better estimate when sample sizes are small. Here I was considering to use it to estimate confidence intervals and statistical significance. I bootstrapped(using the mean) and did a 2 sample statistical significance test. I also repeated it with the raw samples. Due to the nature of bootstrapping means, The p-value for the bootstrapped data is usually VERY small (less than 1e-50) and the one from the data is much much larger (order of 1e-1). Update 1 (Detailed context) Some of this might be irrelavent but I am including this in case it helps. I have a set of stochastic customers who might call in for a delivery and their quanties are also stochastic. If we were to make then vendor managed and we can adjust delivery dates and quantities, what would be the potential savings? In other words, could a few customers be 'grouped' together due to some factor and have their deliveries correlated to reduce long term costs. So the baseline(before) is the set of costs arising from the demand distributions before grouping. The after case is when a subset of the customers are made into a group and their demands/deliveries are corelated based on some formula. The question then is, does the resulting new cost distribution result in significant cost savings? Here we also want the cost saving to be at least 1%. Obtaining these costs are not easy since it involves solving VRPs. So we would like to avoid taking too many samples. We sample n days with the baseline distribution and sample n other days with the new distribution. This forms our 2 sets of sample data. I read that bootstrapping can be helpful in getting better estimates when sample sizes are smaller or the underlying distribution is not known. So, I sample each dataset with replacement to get n 'new' samples and take its mean. I repeat this B times to get the bootstrap distribution for each sample data. This is done by using scipy.stats.bootstrap on python. This automatically provides confidence intervals based on your alpha. I also calculated this manually by using the raw data. The box plots and normal qq plots are shown below. Here 0, is just boxplot with a normal distribution with mean=mean(baseline) and std=std(baseline). 1.0 and 1.2 are the boxplots of the raw data of baseline and after respectively. 2.0 and 2.2 are the box plots of those from the resulting bootstrap distribution of baseline and after respectively.
