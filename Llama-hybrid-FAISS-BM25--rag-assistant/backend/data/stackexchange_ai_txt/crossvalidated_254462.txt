[site]: crossvalidated
[post_id]: 254462
[parent_id]: 
[tags]: 
Feature importance for xgboost classification of a sample

related to Feature importance for random forest classification of a sample This blog by Ando Saabas suggests a nice way to interpret a tree result for a specific sample into per-feature contributions. Basically he goes over the path to the sample and counts each delta in the prediction $pred(currentNode) - pred(parentNode)$ as a contribution for the splitting feature. Can I use a similar approach for interpreting results of gradient boosting with tree weak classifiers, specifically for the popular xgboost implementation?
