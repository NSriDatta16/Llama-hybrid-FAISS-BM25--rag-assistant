[site]: crossvalidated
[post_id]: 132550
[parent_id]: 132545
[tags]: 
L2 is not always used with neural networks, indeed for statistical pattern recognition problems the cross-entropy loss (with a softmax activation function for the output layer) is the preferred option. See chapter 6 of Bishop's excellent book on Neural Networks for Pattern Recognition . Secondly, I would say that L1 would be a maximum likelihood approach, rather than maximum a-posteriori as there is no prior distribution involved, just a likelihood. Asymptotically (in the limit of an infinite amount of data and hidden units), both L1 and L2 will give the same answer, because the minimiser of L1 is given when the output of the model is the true probability of class membership, and the minimiser of L2 is the conditional mean of the target variable, which in this case is also the true probability of class membership. The difference arises away from these asymptotic conditions (i.e. more or less every practical case), where I would suggest that L1 is probably more efficient in converging to the optimal solution in terms of the number of training samples given, but I am not confident that the practical difference is likely to be great for most problems. In practice, most reasonable loss functions are suitable for training neural networks (see Saerens et al ), however I almost always use a maximum likelihood criterion for training neural networks because this is the most theoretically sound approach as it most accurately describes the variability of the target variable around its conditional mean.
