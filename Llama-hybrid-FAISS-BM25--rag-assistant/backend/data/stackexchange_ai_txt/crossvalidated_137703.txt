[site]: crossvalidated
[post_id]: 137703
[parent_id]: 137509
[tags]: 
Word embeddings mostly help because they can be pre-trained in an unsupervised way on large amounts of text. As a result, NN learns continuous representation of words in a space where words with similar meaning are close to each other. Since NN activation function is continuous (sigmoid or simililar) such representations are inherently easier for NN to work with, and they also improve generalization to unknown words. Also "word projection layer" can save computation time, when properly implemented. If you only train NN on small supervised dataset and use random initializations, then additional layer for "embeddings" probably won't help much ( Collobert el al paper actually demonstrates this)
