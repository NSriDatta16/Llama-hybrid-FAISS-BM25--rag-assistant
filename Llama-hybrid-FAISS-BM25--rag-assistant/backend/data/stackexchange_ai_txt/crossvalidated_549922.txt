[site]: crossvalidated
[post_id]: 549922
[parent_id]: 
[tags]: 
shouldn't reinforcement learning algorithms be always based on the assumption that the given problem can be formulated as MDP?

Recently I read a paper regarding to RL-based neural architecture search, named 'Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search' ECCV2020. In the paper, the authors propose a novel formulation of GAN architecture search problem as a Markov decision process, which is partly inspired by the human-designed Progressive GAN. And the authors say this new formulation enables the use of off-policy RL methods. I now understand how the proposed formulation is different from the previous RL-based NAS methods, which is also denoted in the paper: it is essentially different to the classic RL approaches for NAS [65], which formulate the task as an optimization problem over the entire trajectory/architecture. Instead, the MDP formulation proposed here enables us to do the optimization based on the disentangled steps of cell design. I have some confusions here: As I know (it may be wrong), RL can be applied to problems which can be described as a MDP, meaning that the previous RL-based NAS methods already formulated the problem as a (one-step) MDP. Is that right? Since the previous RL-based NAS methods are based on one-step MDP formulation, they can't be solved with off-policy methods. Is that right? What is the difference between the conditions (requirements) which off-policy and on-policy RL work?
