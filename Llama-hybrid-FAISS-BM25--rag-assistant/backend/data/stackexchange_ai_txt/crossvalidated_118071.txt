[site]: crossvalidated
[post_id]: 118071
[parent_id]: 102852
[tags]: 
I am quite new at this, but I have been following this SVM guide . It says Part 2 of Sarleâ€™s Neural Networks FAQ Sarle (1997) explains the importance of this and most of considerations also apply to SVM. The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. The article it refers to quite detailed, here are some excerpts. Im not sure how many apply just to neural networks tho. Now the question is, should you do any of these things to your data? The answer is, it depends. ... In particular, scaling the inputs to [-1,1] will work better than [0,1] ... It is also bad to have the data confined to a very narraw range such as [-0.1,0.1], since most of the initial hyperplanes will miss such a small region. The SVM guide also makes the point for numerical difficulties which I believe means overflow. E.g. if you are using 32-bit floats the intermediate values might exceed ~10^6 if the initial feature values are large. Also see this video linked from this answer , which explains for the case of gradient decent why features on different scales make the algorithm slower. Finally, a point that I just thought up - If all inputs are scaled to a standard range then the training parameters will be consistent across all features. For example in the case of SVMs, gamma is a measure of the influence of each training point. If the features are scaled, then gamma will refer to the same 'relative distance' for each feature.
