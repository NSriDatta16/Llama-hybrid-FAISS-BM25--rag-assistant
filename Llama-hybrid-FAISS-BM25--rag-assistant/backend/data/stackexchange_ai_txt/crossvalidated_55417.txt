[site]: crossvalidated
[post_id]: 55417
[parent_id]: 55408
[tags]: 
Your models are missing a random component. If it's $\ldots + \epsilon$ where $\epsilon\sim\mathcal{N}(0,\sigma^2)$, then you're not counting another parameter, $\sigma$. This doesn't matter when it's in all the models you're comparing, as you just look at differences in AIC, but it does matter when it isn't, or when you're using the second-order version, AICc. But you've counted the other parameters - the $m$s - correctly. For example, if you have observations indexed $1, \ldots, i, \ldots, n$, then for each observation Model A might be $$y_i = m_1 + m_2x_i + \epsilon_i \qquad \text{where }\epsilon_i\sim\mathcal{N}(0,\sigma^2)$$ So there are three parameters to estimate: $m_1$, $m_2$, & $\sigma$. [In response to your latest edit: So each $y_i$ is the average of several observations with the same covariate pattern, the observations have Gaussian error & you've thus estimated error variances $\sigma_i$. (1) There's a distinction between "fixed" or "known" parameters, which are set according to prior knowledge, and "free" or "unknown" parameters, which are estimated from the data. If you've made an independent estimate of $\sigma_i$ for each $y_i$ it should be included in the bias correction term of the AIC. (1a) However the absolute values of AIC are of no interest, only differences between the AIC of different models. For AIC (first-order at least) that implies you only need take into account differences in the number of estimated parameters for each model. (Not that it's hard to count how many parameters you've estimated, but it explains why people can ignore variance & intercepts in casual talk about model comparison & not go wrong.) (2) It's unusual to fit independent estimates of error variance for each $y_i$. Are you quite sure (a) you want to, (b) that's what you in fact did?]
