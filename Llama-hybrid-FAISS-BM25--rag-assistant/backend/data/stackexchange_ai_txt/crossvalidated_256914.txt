[site]: crossvalidated
[post_id]: 256914
[parent_id]: 256904
[tags]: 
In general, bias is (or more precisely is defined by me as) the dfference observed value $-$ true value. I won't define "observed", but often the words "measured" or "estimated" would fit in the same place as well or better. Here "true" could mean known in some absolute sense. For example, we "know" the constant $\pi = 3.14159...$ to many, many decimal places, but there are contexts in which it might amuse us to estimate it from simulations, so several observed or estimated values of $\pi$ and one true value are easy to imagine. Perhaps more frequently, "true" could just be a polite or honorific name for our best measurement or estimate of something. Some fields talk about "gold standard" measurements, which here has nothing to do with economics or finance but just means using the best available technology, usually of some physical or chemical or biological quantity, such as the concentration of an element in a substance. There are many different ways to make that broad idea more concrete. That definition allows bias to be simply the difference between two numbers. If someone said that my age is 42, I can tell you that answer is biased, and the bias is then just the difference between 42 and my real age. In many more serious contexts, estimating or calculating bias includes taking an average over either observed or "true" values. Both those definitions could make sense, except that I'd expect averaging to imply summation first! Without context, we can't say which might be right for any particular problem.
