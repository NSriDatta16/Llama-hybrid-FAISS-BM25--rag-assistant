[site]: datascience
[post_id]: 89615
[parent_id]: 89563
[tags]: 
IsolationForest doesn't work on Euclidean distance. Hence [0,0] is almost as good as [100,100] It builds random Trees on the dataset and expects that the Outlier will singled-out very early in the Tree while the Inliers will go deep. With that logic, it can figure out the Outlier. The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. [ scikit-learn doc ] Your data is very clean and unique. If we observe here, we can differentiate [0,0] and [100,100] in a single split. So the depth of Node will be same for both cases. You may try a euclidean distance-based Model i.e. LocalOutlierFactor from sklearn.neighbors import LocalOutlierFactor model = LocalOutlierFactor(n_neighbors=10,novelty=True).fit(dataset) model.predict([[0,0],[100,100]]) Output - array([ 1, -1]) Scikit-Learn guide for Outliers/Novelty - Link
