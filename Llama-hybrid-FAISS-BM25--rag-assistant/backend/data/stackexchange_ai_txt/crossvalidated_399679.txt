[site]: crossvalidated
[post_id]: 399679
[parent_id]: 
[tags]: 
What are recommended practices to avoid overestimation of size/dispersion from small samples of negative-binomially distributed data?

I want to estimate mu and size/dispersion accurately for a large number of small samples (n = 6-8 is typical). When I try to do maximum-likelihood inference (or related, Bayesian inference with uninformative priors) I consistently overestimate the size/dispersion factor. Playing with the code below, the median estimate for the size/dispersion parameter approaches the true value slowly for large n, but consistently overestimates it. This also happens for different values of phi. Why does this happen? And what are possible ways to overcome this bias? In the end I would prefer to do Bayesian inference and could share some information across the samples through priors. R-Code: set.seed(1) N = 200 n = 6 mu = 10 phi = 2 toycounts = phi)/nrow(mlest) # consistently larger than 0.5
