[site]: crossvalidated
[post_id]: 448896
[parent_id]: 26144
[tags]: 
Disclaimer: the approach presented is not feasible for continuous values, but I do believe bears some weight in decision making for the project Smarty77 brings up a good point about utilizing a rescaled sigmoid function. Inherently, the sigmoid function produces a probability, which describes a sampling success rate (ie 95 out of 100 photos with these features are successfully 'dog'). The final outcome described is a binary one, and the training, using 'binary cross-entropy' describes a process of separating diametrically opposed outcomes, which inherently discourages results in the middle-range. The continuum of the output is merely there for scaling based on number of samples (ie a result of 0.9761 means that 9761 out of 10000 samples displaying those or similar triats are 'dog'), but each result itself must still be considered to be binary and not arbitrarily granular. As such, it should not be mistaken for and applied as one would real numbers and may not be applicable here. Though I am not sure of the utilization of the network, I would normalize the output vector w.r.t. itself. This can be done with softmax. This will also require there to be 11 linear outputs (bins) from the network (one for each output -5 to +5), one for each class. It will provide an assurance value for any one 'bin' being the correct answer. This architecture would be trainable with one-hot encoding, with the 1 indicating the correct bin. The result is interpretable then in a manner of ways, like a greedy strategy or probabilistic sampling. However, to recast it into a continuous variable, the assuredness of each index can be used as a weight to place a marker on a number-line (similar to the behavior of the sigmoid unit), but this also highlights the primary issue: if the network is fairly certain the result is -2 or +3, but absolutely certain that it is not anything else, is +1 a viable result? Thank you for your consideration. Good luck on your project.
