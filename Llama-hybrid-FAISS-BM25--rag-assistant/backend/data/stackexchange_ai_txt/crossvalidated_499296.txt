[site]: crossvalidated
[post_id]: 499296
[parent_id]: 498557
[tags]: 
This is a common problem, so I will explain the general idea (otherwise this would be off-topic here), and then code it up in Stata. This assumes that you have a decent understanding of regression and hypothesis testing. The basic idea is to take a two-sided test from a regression, which almost any software can do, and then manufacture a one-sided test out of that output. There are two ways of doing that. One is easy (divide the p-value by two), but doesn't always work (you need to flip the rules depending on the sign of the difference). The other is harder, but always works. There are also two variants of the hard one, depending on the type of two-sided test. We will start by constructing a dataset of pig weights taken in weeks 1-3, 4-6, and 7-9. These will be our 3 groups. Pigs who are weighed in later weeks are on average heavier since they are older. . /* (0) Data Step */ . webuse pig, clear (Longitudinal analysis of pig weights) . set seed 12032020 . egen group = cut(week), group(3) icodes . replace group = group + 1 (432 real changes made) . sample 1, by(id) count // sample a pig in a random week (384 observations deleted) . isid id . table group, c(min week max week mean weight count id) ------------------------------------------------------------------ group | min(week) max(week) mean(weight) N(id) ----------+------------------------------------------------------- 1 | 1 3 30.60714 14 2 | 4 6 48.52941 17 3 | 7 9 70.52941 17 ------------------------------------------------------------------ So we have 3 groups, with 48 pigs in total. Weight does seem to increase with group/age. Now that we have the data, we can fit a regression: . /* (1) Estimate Weight Model (Levels) */ . regress weight i.group Source | SS df MS Number of obs = 48 -------------+---------------------------------- F(2, 45) = 119.47 Model | 12409.2683 2 6204.63413 Prob > F = 0.0000 Residual | 2337.05987 45 51.9346639 R-squared = 0.8415 -------------+---------------------------------- Adj R-squared = 0.8345 Total | 14746.3281 47 313.751662 Root MSE = 7.2066 ------------------------------------------------------------------------------ weight | Coef. Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- group | 2 | 17.92227 2.600884 6.89 0.000 12.68382 23.16072 3 | 39.92227 2.600884 15.35 0.000 34.68382 45.16072 | _cons | 30.60714 1.926037 15.89 0.000 26.72791 34.48638 ------------------------------------------------------------------------------ . margins group, post // calculate average weight in each group Adjusted predictions Number of obs = 48 Model VCE : OLS Expression : Linear prediction, predict() ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. t P>|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- group | 1 | 30.60714 1.926037 15.89 0.000 26.72791 34.48638 2 | 48.52941 1.74785 27.77 0.000 45.00906 52.04976 3 | 70.52941 1.74785 40.35 0.000 67.00906 74.04976 ------------------------------------------------------------------------------ The margins command calculates the expect weight in each group from the regression output: group 3 is more than twice as heavy on average (70 kg) as group 1 (30 kg). Note that these averages match the simple summary statistics above. To perform one-sided tests, you first perform the corresponding two-sided Wald test. Then you can use the results to calculate the test statistic and p-value for the one-sided test. So now we do the two-sided test on the averages: group 1 minus group 2 is equal to group 1 minus group 3. This can be accomplished like this: . test (_b[1.group] - _b[2.group]) == (_b[1.group] -_b[3.group]) ( 1) - 2.group + 3.group = 0 F( 1, 45) = 79.21 Prob > F = 0.0000 We can see that the p-value is effectively zero (last row), so we reject the null that the two differences are equal. Note that Stata has simplified our hypothesis into testing that Group 3 and Group 2 have the same average weight, or that their difference is zero. When the estimated difference is positive, you can calculate the one-sided p-values directly from the test output by dividing by two like this: . display "p-value for H0: did 0 = " r(p)/2 p-value for H0: did 0 = 8.767e-12 . display "p-value for H0: did >= 0 & Ha: did = 0 & Ha: did Here were would reject the null that the difference in differences (DID) is zero because the p-value is tiny (first line), but we cannot reject the null that the $\text{DID} \ge 0$ since the p-value is large (second line). When the difference is negative, you will need to swap r(p)/2 and 1-r(p)/2 above. Division by 2 is easy, but you can also do the following. The idea is that the Wald test given above is an F test with 1 numerator degree of freedom and 45 denominator degrees of freedom. The Student’s t distribution is related to the F distribution in that the square of the Student’s t distribution with d degrees of freedom is equivalent to the F distribution with 1 numerator degree of freedom and d denominator degrees of freedom. As long as the F test has 1 numerator degree of freedom, the square root of the F statistic (79.21) is the absolute value of the t statistic for the one-sided test. To determine whether this t statistic is positive or negative, you need to determine whether the difference is positive or negative. To do this, you can use the sign() function. So now we can get all the pieces we need and do the one-sided tests: . /* Method (1) */ . scalar df = r(df_r) . scalar Fstat = r(F) . scalar sign_did = sign((_b[1.group] - _b[2.group]) - (_b[1.group] -_b[3.group]) - 0) . display "p-value for H0: did 0 = " ttail(scalar(df),scalar(sign_did)*sqrt(scalar(Fstat))) p-value for H0: did 0 = 8.767e-12 . display "p-value for H0: did >= 0 & Ha: did = 0 & Ha: did Note that this matches the division results we got above. There is a somewhat more intuitive way that also demonstrates what to do when z statistics are reported instead of t statistics. In these cases, you will get a chi-squared test instead of an F test. The relationship between the standard normal distribution and the chi-squared distribution is similar to the relationship between the Student’s t distribution and the F distribution. In fact, the square root of the chi-squared distribution with 1 degree of freedom is the standard normal distribution. Therefore, one-sided z tests can be performed similarly to one-sided t tests: . /* Method (2): More intuitive to me */ . nlcom did:(_b[1.group] - _b[2.group]) - (_b[1.group] -_b[3.group]) - 0, post did: (_b[1.group] - _b[2.group]) - (_b[1.group] -_b[3.group]) - 0 ------------------------------------------------------------------------------ | Coef. Std. Err. z P>|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- did | 22 2.471833 8.90 0.000 17.1553 26.8447 ------------------------------------------------------------------------------ . test did ( 1) did = 0 chi2( 1) = 79.21 Prob > chi2 = 0.0000 . display r(p)/2 2.785e-19 . display 1-r(p)/2 1 . scalar sign_did = sign(_b[did] - 0) . scalar chi2 = r(chi2) . display "p-value for H0: did 0 = " 1 - normal(scalar(sign_did)*sqrt(scalar(chi2))) p-value for H0: did 0 = 0 . display "p-value for H0: did >= 0 & Ha: did = 0 & Ha: did I find this a bit more intuitive, since it returns the DID as if it was an estimation command, with all the "fixin's": we can see that the DID is 22, we get its standard error of 2.5, and also confidence interval that excludes zero. We know the sign and can also just do the division by zero. For the binary outcome, you can use a probit or logit or just an LPM (a regression, but with het-robust errors). Everything after is the same as above. The output will generally be very similar. logit binary_outcome i.group margins group ... reg binary_outcome i.group, robust margins group ... Stata Code: clear cls /* (0) Data Step */ webuse pig, clear set seed 12032020 egen group = cut(week), group(3) icodes replace group = group + 1 sample 1, by(id) count // sample a pig in a random week isid id table group, c(min week max week mean weight count id) /* (1) Estimate Weight Model (Levels) */ /* Pigs who are weighed in later weeks should be heavier */ regress weight i.group margins group, post // calculate average weight in each group /* To perform one-sided tests, you can first perform the corresponding two-sided Wald test. */ /* Then you can use the results to calculate the test statistic and p-value for the one-sided test */ test (_b[1.group] - _b[2.group]) == (_b[1.group] -_b[3.group]) display "p-value for H0: did 0 = " r(p)/2 display "p-value for H0: did >= 0 & Ha: did 0 = " ttail(scalar(df),scalar(sign_did)*sqrt(scalar(Fstat))) display "p-value for H0: did >= 0 & Ha: did 0 = " 1 - normal(scalar(sign_did)*sqrt(scalar(chi2))) display "p-value for H0: did >= 0 & Ha: did
