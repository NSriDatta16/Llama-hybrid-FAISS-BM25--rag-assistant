[site]: crossvalidated
[post_id]: 369490
[parent_id]: 301285
[tags]: 
Consider the following feedforward neural network: Let $w^l_{j,k}$ be the weight for the connection from the $k^{\text{th}}$ neuron in the $(l-1)^{\text{th}}$ layer to the $j^{\text{th}}$ neuron in the $l^{\text{th}}$ layer. Let $b^l_j$ be the bias of the $j^{\text{th}}$ neuron in the $l^{\text{th}}$ layer. Let $C$ be the cost function. We consider the inputs and desired outputs of training examples as constants while we train our network, so in our simple network, $C$ is a function of the weights and biases in the network. (I.e. weights and biases of hidden layers and the output layer.) Let $\delta^l\equiv\left(\begin{gathered}\frac{\partial C}{\partial w_{1,1}^{l}}\\ \\ \frac{\partial C}{\partial w_{1,2}^{l}}\\ \\ \frac{\partial C}{\partial w_{2,1}^{l}}\\ \\ \frac{\partial C}{\partial w_{2,2}^{l}}\\ \\ \frac{\partial C}{\partial b_{1}^{l}}\\ \\ \frac{\partial C}{\partial b_{2}^{l}} \end{gathered} \right)$ be "the gradient in the $l^{\text{th}}$ layer". (I use the notation used by Michael Nielsen in the excellent chapter How the backpropagation algorithm works in the book Neural Networks and Deep Learning , except for "the gradient in the $l^{\text{th}}$ layer", which I define slightly differently.) I am not aware of a strict definition of the vanishing gradient problem, but I think Nielsen's definition (from the chapter Why are deep neural networks hard to train? in the same book) is quite clear: [...] in at least some deep neural networks, the gradient tends to get smaller as we move backward through the hidden layers. This means that neurons in the earlier layers learn much more slowly than neurons in later layers. [...] The phenomenon is known as the vanishing gradient problem. E.g. in our network, if $||\delta^2||\ll||\delta^4||\ll||\delta^6||$ , then we say we have a vanishing gradient problem. If we use Stochastic Gradient Descent , then the size of the change to every parameter $\alpha$ (e.g. a weight, a bias, or any other parameter in more sophisticated networks) in each step taken by the algorithm (we might call this size "the speed of learning of $\alpha$ ") is proportional to an approximation of $-\frac{\partial C}{\partial\alpha}$ (based on a mini-batch of training examples). Thus, in case of a vanishing gradient problem, we can say that the speed of learning of parameters of neurons becomes lower and lower, as you move to earlier layers. So it doesn't necessarily mean that gradients in earlier layers are actually zero, or that they are stuck in any manner, but their speed of learning is low enough to significantly increase the training time, which is why it is called "vanishing gradient problem ". See this answer for a more rigorous explanation of the problem.
