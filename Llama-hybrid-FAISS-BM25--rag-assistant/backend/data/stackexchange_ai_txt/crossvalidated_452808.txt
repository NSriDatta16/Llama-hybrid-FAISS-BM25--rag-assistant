[site]: crossvalidated
[post_id]: 452808
[parent_id]: 452733
[tags]: 
The question is, why do you want to do data augmentation? Of course, more data is better, but your augmented dataset is redundant: your million augmented data points are not as good as a million actual data points. An alternative way of thinking of data augmentation is in terms of teaching invariances. For example, CNNs in deep learning are translationally invariant, which is a good thing for image recognition. Unfortunately, we would wish they were invariant to rotations as well (a leaning cat is still a cat), which is not easy to do within the architecture. In summary: Data augmentation is a way to create a model that is roughly invariant with respect to a set of transformations when you cannot force that invariance elsewhere (be it the features or the model). Answering your question, the only way to determine the valid data augmentation procedures is to apply domain knowledge. How can your data points be perturbed or modified without substantially changing them? What do you want your model to learn to ignore? Let me prove that there is no general way, and there cannot be one. Consider the case of predicting the position of an object at $t=1$ given that your $(x, y)$ are the initial positions. A logical data augmentation scheme would be to displace the points microscopically, surely they will end up almost at the same position, right? But if the system is chaotic (for example, a double pendulum), the microscopical deviations would produce exponentially diverging trajectories. What data augmentation can you apply there? Maybe perturbations of the points that lie in large basins of attractions. That would bias your data since you will have fewer samples for the chaotic regimes (which is not necessarily a bad thing!). In any case, any perturbation scheme you come up with will come from a careful analysis of the problem at hand.
