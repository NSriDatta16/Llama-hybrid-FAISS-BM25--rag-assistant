[site]: crossvalidated
[post_id]: 300881
[parent_id]: 
[tags]: 
Do the components of a word vector mean anything that is useful for the model to learn?

I am using word embeddings as independent features to train a classifier (rf), and I am getting validation accuracy of 75% comes close to another built using hand engineered features. I am trying to interpret feature importances given by the modelâ€”do the components of a word vector mean anything that is useful for the model to learn, or is it just noise that it has learned? The most common way I have come across people use word vector as features is by using a similarity kind of measure between vectors but here I do not have such a setup. PS: I am using the google new word vectors of 300 dimension.
