[site]: crossvalidated
[post_id]: 405818
[parent_id]: 
[tags]: 
Cross-Validation on a multiple linear regression model, negative values?

I'm trying to demonstrate that, using a linear model with too many predictors, that the correlation can be artificially inflated, and that k-fold cross validation can expose overfitting. To do this, I create multiple timeseries of random numbers (of length 37 in this case) [the predictors] to predict a timeseries of random numbers [the predictand]. A linear model is then fit to the predictors to optimally create a timeseries to predict the predictand. Finally, the correlation of this model prediction with the predictand is taken. This process is repeated hundreds of times and the final result averaged. The result of this is shown in the blue line below. As expected, as the number of predictors increases, the correlation is inflated. I then repeat the above process but use a Leave One Out cross validation instead. As expected, it picks up the overfitting as the cross validated correlation is around zero. However, for low numbers of predictors, it is negative. Is this expected behaviour and why? Thanks in advance, apologies if the question is not clear - can attach python code if its not clear (though its quite lengthly). I know multiple linear regression always produces a positive correlation.
