[site]: crossvalidated
[post_id]: 301796
[parent_id]: 301766
[tags]: 
Your notation is confusing for me, so allow me to change it slightly in my answer. If your model is (for simplicity let's forget about $A$) $$ D = S + N $$ where $S,N,D$ are multivariate normal random variables. Here your notation becomes confusing, since you seem to use $\vec{s},\vec{n}$ as means of normally distributed random variables and $\vec{d}$ for a random variable itself. This leads to another confusion, you named $\vec{n}$ as "noise", but in fact there is a random noise around means of both random variables, so it could be re-written as $$ D = \mu_s + \varepsilon_s + \mu_n + \varepsilon_n $$ where $\mu_s,\mu_n$ are point values and $\varepsilon_s \sim \mathcal{N}(\boldsymbol{0}, \Sigma_s)$ and $\varepsilon_n \sim \mathcal{N}(\boldsymbol{0}, \Sigma_n)$, what is equivalent to saying that $S \sim \mathcal{N}(\mu_s, \Sigma_s)$ and $N \sim \mathcal{N}(\mu_n, \Sigma_n)$. So in fact you have two sources of noise (randomness) if you assume $S,N$ to be random variables. Your general mistake is that you are not using Bayes theorem at all in your equations. What you write is $$ p(S|D) \propto p(S) \,p(N) $$ and this is not a Bayes theorem, moreover it is wrong . Distribution of a sum of random variables $S+N$ is not a product of their distributions. If $S$ and $N$ were independent, then their joint distribution would be a product of their marginal distributions. Joint distribution is not the same as a distribution of sum of variables. Moreover, for this to be a Bayes theorem, on the right hand side of the equation we would need to see $p(D|S)\,p(S)$. Moreover, the idea behind this equation is wrong. Basically, what you are saying is that you want to learn about the distribution of $S$ using the distribution of $S$. If you know it, then what is the point of using it to estimate itself? The proper use of Bayes theorem would be something like below. First, we would define the likelihood function, i.e. the function of your data conditional on parameters, $$ D \sim \mathcal{N}(\mu_d, \Sigma_d) $$ In your case the parameters are $\mu_d = \mu_s + \mu_n$ and $\Sigma_d = \Sigma_s + \Sigma_n$. Since you want to conduct a simulation to assess your estimator, you would assume that you don't know $\mu_s,\mu_n,\Sigma_s,\Sigma_n$ (otherwise there is nothing to estimate, as already described). The problem in here is that you cannot separate $S$ and $N$ knowing only their sum. Fortunately, if you know that $\mu_n = \boldsymbol{0}$ and $\Sigma_n = \boldsymbol{I}$, then you also know that $\mu_d = \mu_s + \boldsymbol{0} = \mu_s$, what simplifies your problem to estimating $\mu_d$. To estimate $\mu_d,\Sigma_d$ you would need to assume some kind of priors for them and use Bayes theorem $$ p(\mu_d,\Sigma_d \mid D) \propto p(D \mid \mu_d,\Sigma_d) \; p(\mu_d) \; p(\Sigma_d) $$ where $p(\mu_d), p(\Sigma_d)$ are priors for your parameters. To estimate it, you could use optimization (if you need only the MAP estimate), conjugate priors , or MCMC. Finally, bias of the estimator would be $$ \mathrm{bias}(\hat \mu_d) = \mu_d - E(\hat \mu_d) $$ where $\hat \mu_d$ is your estimate of $\mu_d$. Yes, to estimate $E(\hat \mu_d)$ and the bias you could run a large number of simulations, and then take the empirical mean of the estimated $\hat \mu_d$'s as estimate of $E(\hat \mu_d)$.
