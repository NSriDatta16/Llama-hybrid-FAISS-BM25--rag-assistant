[site]: datascience
[post_id]: 97959
[parent_id]: 80191
[tags]: 
Your question is valid. There are couple of known issues when trying to fit BERT-large version on small datasets (small implies a couple of 1000 training data points). The number of parameters itself is not a primary source of concern. The issues chiefly are - the use of a non-standard optimizer introduces bias in the gradient estimation; the top layers of the pre-trained BERT model provide a bad initialization point for finetuning; and the use of a pre-determined , but commonly adopted number of training iterations hurts convergence. This causes identical learning processes with different random seeds result in significantly different models for such scenarios. This CAN be fixed! The following paper has good suggestions to fix all of these: https://openreview.net/pdf?id=cO1IH43yUF I quote the author's concluion here - "we show that the debiasing omission in BERTADAM is the main cause of degenerate models on small datasets commonly observed in previous work... ...Second, we observe the top layers of the pre-trained BERT provide a detrimental initialization for fine-tuning and delay learning. Simply re-initializing these layers not only speeds up learning but also leads to better model performance. Third, we demonstrate that the common one-size-fits-all three-epochs practice for BERT fine-tuning is sub-optimal and allocating more training time can stabilize fine-tuning." Another recent SOTA paper on similar lines is: https://openreview.net/pdf?id=nzpLWnVAyah
