[site]: crossvalidated
[post_id]: 550938
[parent_id]: 
[tags]: 
RL algorithms for continuing task problems

I am stuck with an RL problem where the state space is continuous and the action space is discrete. The problem is a continuing task problem; there are no episode boundaries. So far, I have tried to solve it using a policy gradient algorithm, considering that episode length equals 1, but the results are pretty bad. I guess the gradient estimation is quite bad with just one sample. I have been trying to find different RL algorithms for a continuing task problem, but almost everything that I found is suitable for an episodic task. The only algorithm that I have been able to find for a continuing task problem is on the book "RL: an introduction" from Sutton and Barto on section 13.6, which is called "Actorâ€“Critic with Eligibility Traces". Is it acceptable to use episodic task algorithms with length 1 for a continuing task problem? Is it possible to modify them to handle a continuing task? Any other algorithms I could use for an RL continuing task? Update: The problem I was trying to tackle is a contextual multi-armed bandit problem. The context is a vector on $\mathbb{R}^6$ and I have 2 different actions (either 0 or 1). Depending on the context and the action taken, the reward can be positive or negative. The main goal is to maximize the reward. I was thinking of this problem as having an agent who takes actions in discrete time intervals. For a time interval $i$ , the agent gets a new context and takes action. At the beginning of the next time interval, it receives a reward. The agent can get any context randomly. As contexts do not follow any sequence, I was thinking of this as a continuing task. I tried implementing a policy gradient, similar to https://gist.github.com/xkrishnam/d9a62d52d28eb943c3965c6cf631ad30#file-contextualpolicy-n-arm-bandit-ipynb I have also tried UCB and TS and the results were also quite bad
