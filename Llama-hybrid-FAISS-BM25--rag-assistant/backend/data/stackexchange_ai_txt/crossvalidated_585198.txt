[site]: crossvalidated
[post_id]: 585198
[parent_id]: 584166
[tags]: 
" I want the model to catch the positives and neutrals and not misclassification on negative class (aka very small False Positive on Negative class I suppose). " The results you have are probably near optimal results assuming that you have no preferences on the types of error that the model makes. If that assumption is incorrect (as it appears to be in this case), the results are not likely to be meaningful. If you have particular concerns about one class, then that is an indication that the misclassification costs for your problem/analysis are not equal. What you should probably do is to work out what the misclassification costs are and build those into your classification, using "minimum risk classification" (also known as "cost sensitive learning", "Bayesian decision theory" etc). A sort of agree with some of the comments saying not to use accuracy as a performance metric. It is based on the assumption that all misclassifications are equally bad, which is not always true, and a focus on accuracy tends to stop people from properly considering the misclassification costs, which can be of vital importance, especially for "imbalanced learning" tasks (as the minority class is often more "important" in some sense, and it is worth suffering some additional misclassifications of the majority class in order to catch more of the minority class). I disagree with not using metric based on hard classifications though. If your application is one where you must make a hard classification, then it is likely that your metric of primary interest is based on that hard classification, and if you don't use it, you will be ignoring the primary goal of the project. You do need to be aware, however, of the shortcomings of these metrics, just as you need to be aware that proper scoring rules are no panacea either (see my answer here for an example where proper scoring rules chose the wrong model). That doesn't mean that it should be the only metric you use - sometimes it is good to use a variety of metrics to get an appreciation of different aspects of the model's performance. So to summarise - work out plausible numeric costs for each type of misclassification and build that into your decision rule (rather than just picking the class with the highest probability).
