[site]: datascience
[post_id]: 99687
[parent_id]: 97980
[tags]: 
This is an interesting one and there is not a "one-fits-all" answer to it. If I break down your question into two major parts, I would say: Choosing important variables Domain expert: It is always helpful to have an idea from the domain expert on what variables matter the most, especially in your case that you have 1000 variables to choose from. Given the size of your dataset, it would really reduce the time of processing. Intelligent variable importance measures: there are so many ways you can limit your variables to the ones that really matter, starting from simple correlation and covariance metrics to a Bayesian one. There are packages out there that will do this for you. For example, if you are programming in R, Boruta is one of the ones I use a lot (read here). Having said that, you can always use the ones from sklearn . So I am sure you appreciate that this is all pre-modelling before you go through any ML modelling. Once you have an understanding of how variables matter and to what extent, I would limit the ML to the top 100 maybe? This will save you heaps of time and effort. ML modelling I like your approach on the train-test split and in fact, it is what I do most of the time. You can define a dataframe to hold the importance measures from your ML model each time you run the model and append the new ones to it. this way, at the end of say 20 rounds of modelling, you will have 20 measures for each variable which you can then average and rank. The whole process should give you an idea of what variables really matter and then I would get into "hyperparameter optimization" after I have figured what variables to use.
