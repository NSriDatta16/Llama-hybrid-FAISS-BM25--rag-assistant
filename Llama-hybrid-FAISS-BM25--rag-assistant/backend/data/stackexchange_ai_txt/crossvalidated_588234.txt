[site]: crossvalidated
[post_id]: 588234
[parent_id]: 
[tags]: 
Refitting on the whole dataset after train-validation split?

Suppose I am facing a problem that needs hyper tuning, say batch learning with different batch sizes. I am splitting the training data into (80% train + 20% validation) and will choose the optimal value of batch size according to the lowest validation error. Here is my question: do I need to refit the model (with the optimal batch size) on the entire 100% dataset? I think it's a reasonable choice by intuition, there should be no harm in doing so (except for extra training time). But I'm not sure if it's useful in practice. I've asked several friends doing research in machine learning and got two opposite responses: It is incorrect to NOT do so , you always want to use all the data; It is not necessary to do so , results from a train-validation split is close enough; Can anyone tell me which one I should follow? Thanks!
