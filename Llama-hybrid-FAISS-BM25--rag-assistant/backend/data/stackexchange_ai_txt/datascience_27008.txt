[site]: datascience
[post_id]: 27008
[parent_id]: 26972
[tags]: 
I'm answering my question. Well, I've ditched XGBoost for LightGBM (the only Microsoft product I seem to enjoy by far) but since the interface is quite similar, the same should apply to XGBoost. Apparently I don't need to apply Sigmoid to predictions. I don't know why the examples suggest otherwise. def logistic_obj(y_hat, dtrain): y = dtrain.get_label() p = y_hat # p = 1. / (1. + np.exp(-y_hat)) grad = p - y hess = p * (1. - p) grad = 4 * p * y + p - 5 * y hess = (4 * y + 1) * (p * (1.0 - p)) return grad, hess def err_rate(y_hat, dtrain): y = dtrain.get_label() # y_hat = 1.0 / (1.0 + np.exp(-y_hat)) y_hat = np.clip(y_hat, 10e-7, 1-10e-7) loss_fn = y*np.log(y_hat) loss_fp = (1.0 - y)*np.log(1.0 - y_hat) return 'error', np.sum(-(5*loss_fn+loss_fp))/len(y), False
