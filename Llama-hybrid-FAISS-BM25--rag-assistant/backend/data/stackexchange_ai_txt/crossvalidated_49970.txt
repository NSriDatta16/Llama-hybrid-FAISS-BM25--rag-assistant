[site]: crossvalidated
[post_id]: 49970
[parent_id]: 
[tags]: 
Average rate versus average period

I am working on a project where I need to do workload balancing, and to see how well my approach is working I want to compute both the average amount of time between requests for a workload and the average amount of time for a requested workload to be obtained. Here is what is bothering me. My data consists of a list of time intervals (e.g., 0.4 seconds, 1.3 seconds, etc.). From this data, I can compute either the average rate at which an event is taking place by taking the reciprocal of each time interval to turn it into an instantaneous rate and averaging over that, or I can compute the average period by directly averaging over the time interval data. Doing the first is equivalent to computing the harmonic mean of the data, and doing the second involves computing the arithmetic mean of the data, so the two analyses get different results. What confuses me is that, although the two means get different results, a rate is equal to the inverse of a period, and so the two results seems like they should be related via reciprocation, even though they are not. Another option that I have is to use the geometric mean which will obtain consistent results for the rate and the period (as reciprocation commutes with taking the root), but my limited understanding says that one should only use the geometric mean in cases where you are taking the mean of something like growth rates. So in short, it is not clear to me which of these means --- harmonic, arithmetic, geometric, or something entirely different --- should be used in this situation. Any insight would be appreciated.
