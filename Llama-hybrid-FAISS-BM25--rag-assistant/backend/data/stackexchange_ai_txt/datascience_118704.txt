[site]: datascience
[post_id]: 118704
[parent_id]: 
[tags]: 
Why, considering duale form, Soft Margin SVM is more general than Hard Margin (linear kernel)

I have a problem understanding why, considering dual form, the Soft margin SVM (linear) is more general than Hard margin SVM. The dual form of the Hard Margin consists of the finding of tuple $\alpha$ maximizing $\psi(\alpha)=\ldots$ and with the constraint that $\forall i, 0\leq\alpha_i$ and $\sum_i \alpha_i y^{(i)}=0$ . For Soft margin SVM it is the same thing but with the constraint that $0\leq\alpha_i\leq C$ . What I understand is that there will be a (unique) solution to Hard margin (ie the data is linearly separated) if there are $\alpha$ s such that $\forall i, 0\leq\alpha_i$ and $\sum_i \alpha_i y^{(i)}=0$ (so that we can after find what such $\alpha$ s maximize $\psi(\alpha)$ ). But then, if there is some solution $\alpha$ for the soft margin SVM then it verifies $\forall i, 0\leq\alpha_i\leq C$ and $\sum_i\alpha_i y^{(i)}=0$ so it verifies the less restrictive condition $\forall i,0\leq\alpha_i$ and $\sum_i\alpha_i y^{(i)}=0$ , so it will be a valid $\alpha$ for the Hard margin (not maximizing for Hard Margin I guess, so not the solution). So with this (faulty) reasoning Hard margin appears more general than Soft margin??? where is my mistake? Maybe a simple example could show me my mistake but I get lost in the translation to the dual form...
