[site]: crossvalidated
[post_id]: 293950
[parent_id]: 293915
[tags]: 
You are using cross-validation wrong. You just split all your data into 10 folds, which means in the training fold, there are events from every year , so it learns with 90% of the data of each year. When it predicts, it predicts the remaining 10% of a year from which it has already seen 90% . When you then predict future games, the classifier has not yet seen any data from that year (or course). Or think of it like this: you want to predict the temperature of a certain day for the next year. If you use data from the last 20 years and split it, it is easy of course for the classifier to predict day x in the test sample if it has already seen x-1, x-2, x+1 (the days around day x). So it learns just to predict the next/past few days. It does newer learn to predict the next year, say to use the previous years to infer on the temperature on the same day a year later. The days before/after day x are way more useful. I think as the problem is clear now, let's go to the solution. So how to do unbiased cross-validation on time series: do NOT use the same year in testing as well as training. You also don't want to use newer data as the one you will predict, because this is not a real-world case... take, let's say, the first 10 years, train, and predict year 11. Then train on the first 11 years, predict on 12 and so on. This will give you an estimation of how well the model is able to predict outcomes as well as how much it improves with more data. In the end, when you optimized your network, you can train on the full data sample. This should yield more realistic performances and help you fight the current over-fit in your model.
