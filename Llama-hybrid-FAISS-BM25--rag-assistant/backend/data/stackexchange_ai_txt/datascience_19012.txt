[site]: datascience
[post_id]: 19012
[parent_id]: 18991
[tags]: 
The initial loss is purely dependent on your weight initialization and your data normalization. Random weights = random initial loss. If you're interested in the variance of the loss, this will roughly relate to the number of layers you have and number of weights in each layer. For a good example, check out this paper: Understanding the difficulty of training deep feedforward neural networks -- Glorot, Bengio
