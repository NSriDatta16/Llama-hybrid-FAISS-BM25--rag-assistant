[site]: datascience
[post_id]: 105095
[parent_id]: 104607
[tags]: 
The Transformer-based MT typically performs better than RNN-based MT in terms of translation quality. People used to claim that RNNs are better for low-resource language pairs, however, this is not true anymore with pre-trained models such as MASS or mBART . The other advantage of Transformers is that at training time, they can be fully parallelized, whereas an RNN always processes the sentences sequentially. To compute the $n$ -th state, you always need to wait until $n-1$ -th is ready. One disadvantage of the transformer decoder is that at every step it needs to attend to all previously decoded tokens, which makes the generation quadratic in theory (in practice, this can be parallelized quite well). When efficiency is a concern, it might be a good idea to combine a Transformer encoder with an RNN decoder .
