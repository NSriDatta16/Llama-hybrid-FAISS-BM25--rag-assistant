[site]: crossvalidated
[post_id]: 386126
[parent_id]: 
[tags]: 
Can t-SNE help feature selection?

I'm training a fully connected feed forward neural network for regression. Given one training example $(x_i, y_i)$ , I need to convert the raw representation $x_i$ into an invariant representation $x_i^{'}$ of higher dimensions by some non-linear transformation $$x_i^{'} = f_{\theta}(x_i)$$ $\theta$ is the hyperparameter determines such a transformation, the dimensionalities of $x_i$ and $x_i^{'}$ are about $150$ and $4500$ respectively. This transformation step cannot be accomplished by a neural network layer because the original representation is not invariant under translation. The original representation has to be transformed by some map controlled by $\theta$ , and the map is constructed based on domain knowledge. To choose a good hyperparameter $\theta$ is not easy, so I wonder whether dimensionality reduction can help me to make a good choice. My idea is: If a resulting representation $\{x_i^{'}\}_{i = 1}^N$ preserves much of the topological structures of the raw representation $\{x_i\}_{i = 1}^{N}$ , then it is reasonable to believe it will be good for regression. The topological structure of the raw input can be probed by the domain knowledge, to probe the structure of the resulting representation, I decided to use t-SNE because of its non-linear nature. However, by playing with t-SNE on this beautiful site , I found the result given by t-SNE is quite sensitive to the choice of perplexity, iterations and learning rate, and it seems only qualitative conclusions can be made by plotting the result. Is my idea reasonable? Is there a better choice for a quick dimensionality reduction?
