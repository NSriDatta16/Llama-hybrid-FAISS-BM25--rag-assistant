[site]: datascience
[post_id]: 69428
[parent_id]: 
[tags]: 
How to encode data for a semi-supervised Adversial Autoencoder (AAE)?

I'm trying to recreate the model described in section 5 of the AAE paper . I'm having trouble understanding the architecture of the encoder so I could incorporate both the input image and the class label and get the correct latent output for both. Currently I have an encoder that simply separates the two as follows: def build_encoder(self): input_img = Input(shape=self.img_shape) input_class = Input(shape=self.num_classes,) """ Features first""" encoder_f = Flatten()(input_img) encoder_f = LSTM(512, activation=LeakyReLU(alpha=0.2))(encoder_f) encoder_f = LSTM(512, activation=LeakyReLU(alpha=0.2))(encoder_f) # For feature extraction features = Dense(self.latent_dim, activation='linear')(encoder_f) """ Then labels""" encoder_c = Flatten()(input_class) encoder_c = LSTM(8, activation=LeakyReLU(alpha=0.2))(encoder_c) encoder_c = LSTM(8, activation=LeakyReLU(alpha=0.2))(encoder_c) # For signal classification classes = Dense(self.num_classes, activation='softmax')(encoder_c) return Model([input_img, input_class], [classes, features]) But it doesn't sit quite right with me, and I don't think that's what the authors intended. Can I simply run both the flattened input image and label vector through the same encoder like imgs= Flatten()(input_img) labels = Flatten()(input_class) encoder = LSTM(512, activation=LeakyReLU(alpha=0.2))(imgs)(labels) encoder = LSTM(512, activation=LeakyReLU(alpha=0.2))(encoder) features = Dense(self.latent_dim, activation='linear')(encoder) classes = Dense(self.num_classes, activation='softmax')(encoder) return Model([input_img, input_class], [classes, features]) And get the correct latent output? EDIT: On a closer reading, section A.2.1 of the original paper describes the encoder architecture: The q(y|x) and q(z|x) share the first two 1000-unit layers of the encoder. The dimensionality of both the style and label representation is 10. On the style representation, we impose a Gaussian distribution with the standard deviation of 1. On the label representation, we impose a Categorical distribution This implies that my original implementation was not correct, but the alternative of running both inputs through the same encoding layers is.
