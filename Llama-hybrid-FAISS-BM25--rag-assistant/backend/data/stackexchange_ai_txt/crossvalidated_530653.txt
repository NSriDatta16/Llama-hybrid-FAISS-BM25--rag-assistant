[site]: crossvalidated
[post_id]: 530653
[parent_id]: 
[tags]: 
Universal Approximation: how does a neural network handle a ratio of inputs

Related questions/background info: Universal Approximation Theorem â€” Neural Networks Does the universal approximation theorem for neural networks hold for any activation function? A universal approximator is a device or technique that is capable of approximating an input to any desired accuracy, i.e. such that the error $\epsilon$ can be made as small as desired. In the context of neural networks, there are a number of "universal approximation theorems" since, in the words of Kratsios The universal approximation property of various machine learning models is currently only understood on a case-by-case basis The singular term Universal Approximation Theorem therefore refers to results that establish the density of an algorithmically generated class of functions within a given function space of interest To define the scope of this question in terms of such spaces with sufficient mathematical rigour to satisfy the demands of all is beyond my ability, so for the purposes of this question, answers may refer to any known Universal Approximation Theorem whose scope does not exclude the specific example given. Caveats and constraints arising from particulars of the function (e.g. absence of divergence, Lebesgue integrability, etc.) may be added as required. There are many nice explanations of how neural networks can be universal approximators (see e.g. A visual proof that neural nets can compute any function ) in the context of simple polynomials, but how does it work when, for example, the function to be approximated involves a ratio, e.g. (arbitrary example) $$ f(x, y, z) = {x^3 * y}/z $$ Question Given that most neural networks are constructed from nodes that multiply and add (via the weights and biases of the matrices), how can such a system be a universal approximator for functions that include other mathematical forms such as ratios? Then, given any suitable function (with the space of the UAT being discussed), are there any results that place lower bounds on network size (layers, breadth, depth, etc.), number of training examples required, etc. in order to achieve error $\epsilon$ .
