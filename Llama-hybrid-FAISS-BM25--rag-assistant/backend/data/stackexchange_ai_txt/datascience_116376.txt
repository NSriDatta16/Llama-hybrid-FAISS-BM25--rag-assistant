[site]: datascience
[post_id]: 116376
[parent_id]: 115911
[tags]: 
In your question you talk about vector embeddings or "word 2 vector representation" (word2vec was the first software to train word embeddings). It's important to understand that not all vectors are embeddings: Embeddings are short vectors made of real numbers, they were invented around 2010. There are many different types of embeddings, i.e. methods to train the embeddings from a corpus: word2vec, Glove, Elmo, Fasttext, Bert... Before this, people were also using vectors representing a "bag of words": one-hot-encoding for a single word, frequency count or TFIDF for a sentence/document. These vectors are long and sparse, i.e. they usually contain a lot of zeros. These are the most common word representation methods, but there are potentially other alternatives. For example, in Wordnet the words are nodes in a graph and relations between words are represented as edges.
