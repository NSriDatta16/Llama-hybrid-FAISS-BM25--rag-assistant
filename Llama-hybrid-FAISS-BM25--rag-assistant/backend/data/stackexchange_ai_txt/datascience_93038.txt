[site]: datascience
[post_id]: 93038
[parent_id]: 
[tags]: 
Why are convolutions still used in some Transformer networks for speech enhancement?

So I’ve read in Attention is All You Need that Transformers remove the need for recurrence and convolutions entirely. However, I’ve seen some TNNs (such as SepFormer , DPTNet , and TSTNN ) that still utilize convolutions. Is there any particular reason for this? Doesn’t that defeat the purpose of Transformers?
