[site]: crossvalidated
[post_id]: 192899
[parent_id]: 192881
[tags]: 
There has been a considerable amount of work on parallelized SGD, that has been extended to the Map-Reduce paradigm. I would suggest looking at publications like Parallelized Stochastic Gradient Descent by Zinkevich et al. or at Optimal Distributed Online Prediction Using Mini-Batches by Dekel et al. There are many approaches to take in a distributed setting. You can use strict synchronization like the papers above use, you can use async gradient calculation at each worker like the HOGWILD! paper mentioned, or you can use stale synchronous iterations, like the work coming out of Eric Xing's lab at CMU. For implementations you can look at the machine learning libraries of Apache Flink, Spark, and other distributed frameworks.
