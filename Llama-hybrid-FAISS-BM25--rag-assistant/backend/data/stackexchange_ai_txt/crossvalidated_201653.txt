[site]: crossvalidated
[post_id]: 201653
[parent_id]: 
[tags]: 
Do the concepts of regularization and overfitting apply in probability distribution fitting?

I'm from a machine learning background and trying to brush up on my stats. I've read a few papers on estimation theory. In a nutshell, estimation theory is: given some probability model, we want to approximate the parameters for this model. In machine learning, everyone is scared of overfitting and such. Why isn't overfitting brought up more in classical estimation theory? Do classical statisticians not care about overfitting?
