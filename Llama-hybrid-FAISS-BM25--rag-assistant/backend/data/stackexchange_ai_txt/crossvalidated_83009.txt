[site]: crossvalidated
[post_id]: 83009
[parent_id]: 83006
[tags]: 
The hyperplane for linear SVM can be summarized in a single inner product. This is exactly what libraries like SVMPerf and LIBLINEAR do (which exclusively work with a linear kernel). The resulting hyperplane (e.g. decision function) is exactly the same as the 'original' formulation with a set of support vectors. The decision function of an SVM model is as follows: $$f(\mathbf{x}) = \sum_i y_i \alpha_i \kappa(\mathbf{x}_i,\mathbf{x}) + b,$$ where $\mathbf{y}$ is the vector of labels, $\alpha$ the support values, $\mathbf{x}_i$ denotes the i'th support vector, $b$ is a bias term and $\kappa(\cdot,\cdot)$ is the kernel function. Classification is based on the sign of $f(\mathbf{x})$. The linear kernel is nothing more than $\kappa(\mathbf{x},\mathbf{y}) = \mathbf{x}^T\mathbf{y}$ assuming you are working with column vectors. Knowing this, it is easy to write the decision function above in terms of a single inner product with the test point: $$\begin{align} f(\mathbf{x}) &= \sum_i y_i \alpha_i \mathbf{x}_i^T\mathbf{x} + b, \\ &= \mathbf{w}^T\mathbf{x} + b, \end{align}$$ with $\mathbf{w} = \sum_i y_i\alpha_i \mathbf{x}_i$.
