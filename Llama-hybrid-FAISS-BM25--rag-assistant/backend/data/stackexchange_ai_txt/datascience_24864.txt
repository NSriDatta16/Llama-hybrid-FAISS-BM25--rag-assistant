[site]: datascience
[post_id]: 24864
[parent_id]: 
[tags]: 
Regression problem as predicting a delta from another algorithm's output

Suppose I have a supervised machine learning regression problem to predict a value that's supposed to improve upon an already-existing algorithm. I am trying to predict the distance that a small rocket will travel. We currently have an algorithm that uses ballistic trajectory equations that produces a value called physics_distance_in_km , and we measure RMSE against the true_distance label in a test set. We wanted to create a more accurate estimate for how far the small rocket will travel, so we formed a regression problem, added more features, and built models using OLS regression, random forest regression, neural network, etc. We are using physics_distance_in_km as a feature along with wind_speed , air_density , propellant_type , etc. We found that while our machine learning regression algorithm improves our RMSE slightly on average, sometimes it is quite far off from the true distance and physics_distance_in_km . As one rare example, the rocket went 15 km, physics_distance_in_km predicted 12 km, but our regression output predicted 29 km, which is possible due to the regression not having a bound. Instead of using physics_distance_in_km as a feature in our model, should we instead leave it out? Instead of the distance as a label, we could use delta_distance as the label computed as true_distance - physics_distance_in_km . Any help would be appreciated.
