[site]: crossvalidated
[post_id]: 443540
[parent_id]: 443484
[tags]: 
Mahalanobis distance only considers linear relationships between variables. As long as you are considering multivariate data that has simple relationships, with no polinomial dependency or non-linearity, it works well, but otherwise it does not. Therefore, I would strongly discourage its use in most Machine Learning applications! Example: I sampled this data in a way that it has no linear correlation, and the two variables are standardized. Therefore, the Mahalanobis distance for this sample is equivalent to the Euclidean distance. If I insert an outlier close to the origin (clearly an outlier to the human eye, and not just for its color :) ) this will have the SMALLEST mahalanobis distance of all the points, as it is the closest to the centroid (black), while points in the corners will have high scores even though they clearly follow the same pattern as the rest of the data. Indeed, the centroid means very little for the distribution when the data does not have a simple linear dependency! Isolation forest on the other hand makes no assumptions on the distribution, and gives a high score to points that are easier to isolate by randomly splitting on the variables. In general, Isolation Forest is much more reliable, even though I would not necessarily use it for particularly small datasets like the one you have shown, where other methods such as hierarchical clustering might work better. Mahalanobis Distance (also in its Robust version) should only be used if the data has very simple structure and distrbution (also, I think it needs to do the inversion of the correlation matrix, so I would avoid using it for big datasets!)
