[site]: crossvalidated
[post_id]: 27922
[parent_id]: 27913
[tags]: 
There are two basic assumptions that lead to this relationship: The stationary distribution $\pi(\cdot)$ doesn't change too quickly (i.e. it has a bounded first derivative). Most of the probability mass of $\pi(\cdot)$ is concentrated in a relatively small subset of the domain (the distribution is "peaky"). Let's consider the "small $\sigma^2$" case first. Let $x_i$ be the current state of the Markov chain and $x_j \sim \mathcal{N}(x_i, \sigma^2)$ be the proposed state. Since $\sigma^2$ is very small, we can be confident that $x_j \approx x_i$. Combining this with our first assumption, we see that $\pi(x_j) \approx \pi(x_i)$ and thus $\frac{\pi(x_j)}{\pi(x_i)} \approx 1$. The low acceptance rate with large $\sigma^2$ follows from the second assumption. Recall that approximately $95\%$ of the probability mass of a normal distribution lies within $2\sigma$ of its mean, so in our case most proposals will be generated within the window $[x_i - 2\sigma, x_i + 2\sigma]$. As $\sigma^2$ gets larger, this window expands to cover more and more of the variable's domain. The second assumption implies that the density function must be quite small over most of the domain, so when our sampling window is large $\pi(x_j)$ will frequently be very small. Now for a bit of circular reasoning: since we know the M-H sampler generates samples distributed according to the stationary distribution $\pi$, it must be the case it generates many samples in the high density regions of the domain and few samples in the low density regions. Since most samples are generated in high density regions, $\pi(x_i)$ is usually large. Thus, $\pi(x_i)$ is large and $\pi(x_j)$ is small, resulting in an acceptance rate $\frac{\pi(x_j)}{\pi(x_i)} These two assumptions are true of most distributions we're likely to be interested in, so this relationship between proposal width and acceptance rate is a useful tool for understanding the behavior of M-H samplers.
