[site]: crossvalidated
[post_id]: 395968
[parent_id]: 
[tags]: 
Likelihood ratio test and sample statistics

Given a sample $\mathbf X =(X_1,...,X_n)$ from a parent random variable $X$ , Neyman-Pearson's test for two point hypotheses $H_0$ and $H_1$ is the one defined by the critical region $$C=\left\{\mathbf x \in \mathbb R^n \; \colon\; \lambda(\mathbf x)= \prod_{i=1}^n \frac{f_X(x_i|H_0)}{f_X(x_i|H_1)} \le k_\alpha \right\}$$ where $k_\alpha$ is determined by $$\alpha = \mathbb P (\lambda(\mathbf X)\le k_\alpha)\,.$$ Now, in practice, $C$ is often defined in terms of a statistic $T=g(\mathbf X)$ . For example, if $$\begin{align}H_0 \,\colon \;X\sim N(\mu_0,\sigma^2)\\H_1 \,\colon \;X\sim N(\mu_1,\sigma^2)\end{align}$$ with $\mu_1>\mu_0 $ , then $$C=\left\{\mathbf x \in \mathbb R^n \; \colon\; \bar x = \frac{\textstyle \sum_i x_i}{n}\ge \mu_0 + z_\alpha \frac \sigma {\sqrt n} \right\}$$ where $z_\alpha$ is the $\alpha$ -quantile of the normal distribution. Here the statistic in play is the sample average $\bar X$ . I have found that we can get to the same $C$ in another way: letting $f(\cdot;\mu,\sigma^2/n)$ be the pdf of $\bar X\sim N(\mu,\sigma^2/n)$ , we can construct the likelihood ratio of $\bar X $ via $$\lambda'(\cdot)=\frac {f(\cdot;\mu_0,\sigma^2/n)}{f(\cdot;\mu_1,\sigma^2/n)}$$ and find the critical region as $$C=\left\{ \mathbf x \in \mathbb R^n \; \colon\; \lambda'(\bar x)\le k'_\alpha \right\}$$ with $$\alpha = \mathbb P (\lambda '(\bar X) \le k'_\alpha)\,.$$ Analogously, for the hypotheses $$\begin{align}H_0 \,\colon \;X\sim N(\mu,\sigma_0^2)\\H_1 \,\colon \;X\sim N(\mu,\sigma_1^2)\end{align}$$ considering the statistic $Q=\sum_i (X_i-\mu)^2$ and using the fact that, under $H_i$ , $Q/\sigma_i^2 \sim \chi^2(n)$ , we come to the same critical region as the standard Neyman-Pearson definition. So, is this just luck? Given any statistic $T=g(\mathbf X)$ , is the likelihood ratio test constructed for $T$ the same as the original one? Of course not. $g=0$ breaks this pattern... Ok, so are there always some $g$ 's for which this pattern holds? This would mean, that in the case of point hypotheses, if you're lucky enough to get the genius idea about what $g$ to use, you can use $T$ instead of $\mathbf X$ to construct the test. This doesn't seem so useful though, because Neyman-Pearson's lemma says your test can't be better than that, so you're just being as good as Neyman-Pearson. And what about the general case (non-point hypotheses)? Are there cases where the likelihood ratio test constructed by some statistic is even better than the "ordinary" likelihood ratio test?
