[site]: crossvalidated
[post_id]: 125854
[parent_id]: 125832
[tags]: 
I don't have access to the book, but I think there should be an additional caveat. The omitted variable[s] should be uncorrelated with the explanatory variables that appear in the model. If the omitted variable, on which the conditional variance depends, is correlated with the included variables, then the residual variance will vary with the model variables and the homoscedasticity assumption will be violated. On the other hand, if the omitted variable is uncorrelated, then the residual / error variance will be the same along the range of the model variables. Thus, the homoscedasticity assumption obtains in effect for that model. Sometimes it helps to look at an example or try a little simulation. Here is one I worked up in R : set.seed(9018) # this makes the example exactly reproducible x = runif(500, min=0, max=10) # x is a uniformly distributed continuous variable g = rep(c(0,1), each=250) # g is a grouping variable, which will be omitted y1 = 5 + .3*x + g + c(rnorm(250, mean=0, sd=1), # residual SD=1 when g=0 rnorm(250, mean=0, sd=2) ) # residual SD=2 when g=1 xs = sort(x) # by sorting x, I make it correlated w/ g y2 = 5 + .3*xs + g + c(rnorm(250, mean=0, sd=1), rnorm(250, mean=0, sd=2) ) uncor.m = lm(y1~x) # this is the model w/ g omitted, but uncorrelated w/ x cor.m = lm(y2~xs) # in this case, g is correlated w/ xs library(lmtest) # we use this package to run the Breusch-Pagan tests bptest(uncor.m) # studentized Breusch-Pagan test # # data: uncor.m # BP = 0.1178, df = 1, p-value = 0.7314 bptest(cor.m) # studentized Breusch-Pagan test # # data: cor.m # BP = 38.2682, df = 1, p-value = 6.166e-10 Here are the scale-location plots for the models, you can see that the uncorrelated version is flat, whereas the correlated version has higher residual variance on the right: The residual distribution for your model is the integral of the errors over the omitted variables. In the simplest case, you could have a mixture of two normals with the same mean ($0$) but different variances / SDs. This will yield what is, in effect, a single distribution with a middling variance (at least marginally). This is similar to the case I illustrate above (in the simulation, there is an effect of g on the mean as well as the variance, so the distribution will be somewhat bimodal). Typically, the error distribution marginalized over the omitted variables will not be very normal at all. The situation is directly analogous to the marginal distribution of $Y$, which integrates over the conditional distribution of $Y$ (the residuals) and the distribution of $X$. For an example it may help to read my answer here: What if residuals are normally distributed, but Y is not? Note that even if you have homoscedasticity, the normality of the errors / residuals can also effect the validity of the standard errors. With enough data, the residuals don't have to be perfectly normal for the SEs to be valid, but this requires more data the further your residuals are from normality, and the necessary $N$ can be much higher than people suspect (see @Macro's answer here: Regression when the OLS residuals are not normally distributed ). In general, if you believe this is a reasonable possibility, you would be better off just using standard errors that are robust to these issues. The Huber-White heteroscedasticity consistent 'sandwich' errors are quite convenient, and are commonly used for this reason.
