[site]: crossvalidated
[post_id]: 486906
[parent_id]: 
[tags]: 
Bagging models with different metaparameters versus cross validation?

If we have a model with a metaparameter C, the usual way to tune this parameter is via cross-validation (or building a validation set). The generalization error of the model with the optimal is evaluated on a test set. This is the approach I have always seen. Another approach could be to choose C from a distribution of values and than aggregate the results in some way, e.g. via average or stacking. Is there a reasoning to prefer the first or second approach ? Is the second approach used commonly ?
