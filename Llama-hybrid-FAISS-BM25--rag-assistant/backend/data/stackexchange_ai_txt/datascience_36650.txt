[site]: datascience
[post_id]: 36650
[parent_id]: 36646
[tags]: 
Image classification and other task can be expressed as function approximations and, in theory , neural networks can approximate (almost) any function (given few assumptions on the activation function (see Universal Approx. Theorem )). However, in practice, not all functions fulfilling these assumptions work equally well. Popular activation functions usually share some properties that allow neural networks to learn efficiently in practice , e.g. they are continuously differentiable (for gradient descent), close to the identity near zero (accelerates initial learning from small random weights) and so on. Activation functions like the sigmoid function are not directly related to image classification or any other tasks. Rather, they allow for efficient training of neural networks, which, in turn, can represent a wide variety of tasks using different architectures and cost functions.
