[site]: datascience
[post_id]: 32170
[parent_id]: 32164
[tags]: 
You didn't tell in your question what you tried when debugging, but I'll try to answer. Short answer: It looks to me you got to choose a lower learning rate since your loss is exploding after the first iteration. Explanation: You are using a standard Stochastic Gradient Descent to perform optimization. Therefore, it's an non-adaptive learning rate algorithms, which means if that latter is poorly chosen, loss can explode if the learning rate is too high. That's why when I'm running in such optimization issues with a new neural network, what I like to do is to set a very low learning rate to ensure convergence at first. Also you could use an adaptive optimizer such as AdaGrad or Adam who have both tensorflow implementations. I hope that'll solve your issue.
