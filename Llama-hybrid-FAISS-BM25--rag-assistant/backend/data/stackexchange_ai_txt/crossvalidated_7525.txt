[site]: crossvalidated
[post_id]: 7525
[parent_id]: 7523
[tags]: 
It might be a good idea to rename the question to something like "how do I introduce features and their confidence values into classifiers". I can think of two ways to do it, but if you phrase the question differently, more people will look at it and will maybe have additional suggestions. The first option is to add two features to the classifier, the first feature being the predictor itself (e.g. the success rate), and the second feature being the confidence of this predictor. As mentioned below, this confidence can be just the denominator or width of the confidence interval. This is the more principled way to do it. The problem is that this will not work with linear classifiers. Basically what you want the classifier to learn is that the predictor has high weight if the confidence is high, and low weight if the confidence is low. I.e. you want the weight for one feature to depend on another feature. Linear classifiers cannot do that. So linear NNs (perceptron) or linear SVM will not work. Decision trees are likely to work, and you might have a chance with non-linear NNs and kernel SVM. But these have their own issues. The second option is to have some kind of "prior" on your estimate of the predictor itself. This can be a rigorous prior in the Bayesian sense, or can be just something simple and hacky (since you feed it to a classifier anyway). One simple way to do it is to add some number (e.g. 10) "fabricated" observations to each trainer's data. E.g. you believe that each trainer's prior probability of winning on a particular course is 50%. So you start with 5 wins and 5 loses for every trainer. To these fabricated observations you add the real statistics. For Catterick this will give you 11 wins out of 58 total, so 19%. The more real data you have, the less significant the effect of this prior will be. On the other hand, for the Exeter data this will give you 6 wins out of 11 total, so 55% (rather than your overly confident estimate of 100%). So the second option is to compute the percentage this way, and then feed just this single percentage figure to the classifier. Depending on what you expect the classifier to do, you may vary the prior probability as needed. E.g. if you expect the classifier to heed data close to 0% or to 100% and ignore the 50% region, then starting with the 50/50 prior may be OK. If you want to get more accurate estimates of these percentages, you can change the prior according to the trainer. E.g. you have only 1/1 real statistics for the trainer at Exeter, but you know this particular trainer wins 80% of races at other courses. You can account for this by introducing 8 fabricated wins and 2 losses (rather than 5 and 5). Weighing by the denominator itself may be reasonable, especially if it represents the number of samples. This way you can easily combine percentages. E.g. in one (small) experiment you get 3 out of 10 = 30%; in another (larger) experiment you get 50 out of 100 = 50%, and the average is (30%*10 + 50%*100) / (10 + 100) = 48%. If you just want to know how good your estimate is, then try computing "confidence intervals" -- this gives you an idea of how certain your estimates are. E.g. for a small experiment you might get an interval of [25%, 75%], for a larger one you may get [49%. 51%] interval. How to compute these confidence intervals will depend on the details of your data and the model.
